arxiv-13500-1 | Adding Gradient Noise Improves Learning for Very Deep Networks | http://arxiv.org/pdf/1511.06807v1.pdf | author:Arvind Neelakantan, Luke Vilnis, Quoc V. Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, James Martens category:stat.ML cs.LG published:2015-11-21 summary:Deep feedforward and recurrent networks have achieved impressive results inmany perception and language processing applications. This success is partiallyattributed to architectural innovations such as convolutional and longshort-term memory networks. The main motivation for these architecturalinnovations is that they capture better domain knowledge, and importantly areeasier to optimize than more basic architectures. Recently, more complexarchitectures such as Neural Turing Machines and Memory Networks have beenproposed for tasks including question answering and general computation,creating a new set of optimization challenges. In this paper, we discuss alow-overhead and easy-to-implement technique of adding gradient noise which wefind to be surprisingly effective when training these very deep architectures.The technique not only helps to avoid overfitting, but also can result in lowertraining loss. This method alone allows a fully-connected 20-layer deep networkto be trained with standard gradient descent, even starting from a poorinitialization. We see consistent improvements for many complex models,including a 72% relative reduction in error rate over a carefully-tunedbaseline on a challenging question-answering task, and a doubling of the numberof accurate binary multiplication models learned across 7,000 random restarts.We encourage further application of this technique to additional complex modernarchitectures.
arxiv-13500-2 | Estimation and Inference of Heterogeneous Treatment Effects using Random Forests | http://arxiv.org/pdf/1510.04342v2.pdf | author:Stefan Wager, Susan Athey category:stat.ME math.ST stat.ML stat.TH published:2015-10-14 summary:Many scientific and engineering challenges---ranging from personalizedmedicine to customized marketing recommendations---require an understanding oftreatment effect heterogeneity. In this paper, we develop a non-parametriccausal forest for estimating heterogeneous treatment effects that extendsBreiman's widely used random forest algorithm. Given a potential outcomesframework with unconfoundedness, we show that causal forests are pointwiseconsistent for the true treatment effect, and have an asymptotically Gaussianand centered sampling distribution. We also discuss a practical method forconstructing asymptotic confidence intervals for the true treatment effect thatare centered at the causal forest estimates. Our theoretical results rely on ageneric Gaussian theory for a large family of random forest algorithms, to ourknowledge, this is the first set of results that allows any type of randomforest, including classification and regression forests, to be used forprovably valid statistical inference. In experiments, we find causal forests tobe substantially more powerful than classical methods based on nearest-neighbormatching, especially as the number of covariates increases.
arxiv-13500-3 | Conducting sparse feature selection on arbitrarily long phrases in text corpora with a focus on interpretability | http://arxiv.org/pdf/1511.06798v1.pdf | author:Luke Miratrix, Robin Ackerman category:cs.CL cs.IR stat.AP published:2015-11-20 summary:We propose a general framework for topic-specific summarization of large textcorpora, and illustrate how it can be used for analysis in two quite differentcontexts: an OSHA database of fatality and catastrophe reports (to facilitatesurveillance for patterns in circumstances leading to injury or death) andlegal decisions on workers' compensation claims (to explore relevant case law).Our summarization framework, built on sparse classification methods, is acompromise between simple word frequency based methods currently in wide use,and more heavyweight, model-intensive methods such as Latent DirichletAllocation (LDA). For a particular topic of interest (e.g., mental healthdisability, or chemical reactions), we regress a labeling of documents onto thehigh-dimensional counts of all the other words and phrases in the documents.The resulting small set of phrases found as predictive are then harvested asthe summary. Using a branch-and-bound approach, this method can be extended toallow for phrases of arbitrary length, which allows for potentially richsummarization. We discuss how focus on the purpose of the summaries can informchoices of regularization parameters and model constraints. We evaluate thistool by comparing computational time and summary statistics of the resultingword lists to three other methods in the literature. We also present a new Rpackage, textreg. Overall, we argue that sparse methods have much to offer textanalysis, and is a branch of research that should be considered further in thiscontext.
arxiv-13500-4 | The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition | http://arxiv.org/pdf/1511.06789v1.pdf | author:Jonathan Krause, Benjamin Sapp, Andrew Howard, Howard Zhou, Alexander Toshev, Tom Duerig, James Philbin, Li Fei-Fei category:cs.CV published:2015-11-20 summary:While models of fine-grained recognition have made great progress in recentyears, little work has focused on a key ingredient of making recognition work:data. We use publicly available, noisy data sources to train generic modelswhich vastly improve upon state-of-the-art on fine-grained benchmarks. First,we present an active learning system using non-expert human raters, and improveupon state-of-the-art performance without any text or other metadata associatedwith the images. Second, we show that training on publicly-available noisy webimage search results achieves even higher accuracies, without using anyexpert-annotated training data, while scaling to over ten thousand fine-grainedcategories. We analyze the behavior of our models and data and make a strongcase for the importance of data over special-purpose modeling: using only anoff-the-shelf CNN, we obtain top-1 accuracies of 92.8\% on CUB-200-2011 Birds,85.4\% on Birdsnap, 95.9\% on FGVC-Aircraft, and 82.6\% on Stanford Dogs.
arxiv-13500-5 | Training recurrent networks online without backtracking | http://arxiv.org/pdf/1507.07680v2.pdf | author:Yann Ollivier, Corentin Tallec, Guillaume Charpiat category:cs.NE cs.LG stat.ML published:2015-07-28 summary:We introduce the "NoBackTrack" algorithm to train the parameters of dynamicalsystems such as recurrent neural networks. This algorithm works in an online,memoryless setting, thus requiring no backpropagation through time, and isscalable, avoiding the large computational and memory cost of maintaining thefull gradient of the current state with respect to the parameters. The algorithm essentially maintains, at each time, a single search directionin parameter space. The evolution of this search direction is partly stochasticand is constructed in such a way to provide, at every time, an unbiased randomestimate of the gradient of the loss function with respect to the parameters.Because the gradient estimate is unbiased, on average over time the parameteris updated as it should. The resulting gradient estimate can then be fed to a lightweight Kalman-likefilter to yield an improved algorithm. For recurrent neural networks, theresulting algorithms scale linearly with the number of parameters. Small-scale experiments confirm the suitability of the approach, showing thatthe stochastic approximation of the gradient introduced in the algorithm is notdetrimental to learning. In particular, the Kalman-like version of NoBackTrackis superior to backpropagation through time (BPTT) when the time span ofdependencies in the data is longer than the truncation span for BPTT.
arxiv-13500-6 | Variational Bayes Factor Analysis for i-Vector Extraction | http://arxiv.org/pdf/1511.07422v1.pdf | author:Jesús Villalba category:stat.ML published:2015-11-20 summary:In this document we are going to derive the equations needed to implement aVariational Bayes i-vector extractor. This can be used to extract longeri-vectors reducing the risk of overfittig or to adapt an i-vector extractorfrom a database to another with scarce development data. This work is based onPatrick Kenny's joint factor analysis and Christopher Bishop's variationalprincipal components.
arxiv-13500-7 | Unsupervised Adaptation of SPLDA | http://arxiv.org/pdf/1511.07421v1.pdf | author:Jesús Villalba category:stat.ML published:2015-11-20 summary:State-of-the-art speaker recognition relays on models that need a largeamount of training data. This models are successful in tasks like NIST SREbecause there is sufficient data available. However, in real applications, weusually do not have so much data and, in many cases, the speaker labels areunknown. We present a method to adapt a PLDA model from a domain with a largeamount of labeled data to another with unlabeled data. We describe a generativemodel that produces both sets of data where the unknown labels are modeled likelatent variables. We used variational Bayes to estimate the hidden variables.Here, we derive the equations for this model. This model has been used in thepapers: "UNSUPERVISED ADAPTATION OF PLDA BY USING VARIATIONAL BAYES METHODS"publised at ICASSP 2014, "Unsupervised Training of PLDA with Variational Bayes"published at Iberspeech 2014, and "VARIATIONAL BAYESIAN PLDA FOR SPEAKERDIARIZATION IN THE MGB CHALLENGE" published at ASRU 2015.
arxiv-13500-8 | PLDA with Two Sources of Inter-session Variability | http://arxiv.org/pdf/1511.06772v1.pdf | author:Jesús Villalba category:stat.ML published:2015-11-20 summary:In some speaker recognition scenarios we find conversations recordedsimultaneously over multiple channels. That is the case of the interviews inthe NIST SRE dataset. To take advantage of that, we propose a modification ofthe PLDA model that considers two different inter-session variability terms.The first term is tied between all the recordings belonging to the sameconversation whereas the second is not. Thus, the former mainly intends tocapture the variability due to the phonetic content of the conversation whilethe latter tries to capture the channel variability. In this document, wederive the equations for this model. This model was applied in the paper"Handling Recordings Acquired Simultaneously over Multiple Channels with PLDA"published at Interspeech 2013.
arxiv-13500-9 | Bayesian SPLDA | http://arxiv.org/pdf/1511.07318v1.pdf | author:Jesús Villalba category:stat.ML published:2015-11-20 summary:In this document we are going to derive the equations needed to implement aVariational Bayes estimation of the parameters of the simplified probabilisticlinear discriminant analysis (SPLDA) model. This can be used to adapt SPLDAfrom one database to another with few development data or to implement thefully Bayesian recipe. Our approach is similar to Bishop's VB PPCA.
arxiv-13500-10 | Images Don't Lie: Transferring Deep Visual Semantic Features to Large-Scale Multimodal Learning to Rank | http://arxiv.org/pdf/1511.06746v1.pdf | author:Corey Lynch, Kamelia Aryafar, Josh Attenberg category:cs.CV cs.LG published:2015-11-20 summary:Search is at the heart of modern e-commerce. As a result, the task of rankingsearch results automatically (learning to rank) is a multibillion dollarmachine learning problem. Traditional models optimize over a fewhand-constructed features based on the item's text. In this paper, we introducea multimodal learning to rank model that combines these traditional featureswith visual semantic features transferred from a deep convolutional neuralnetwork. In a large scale experiment using data from the online marketplaceEtsy, we verify that moving to a multimodal representation significantlyimproves ranking quality. We show how image features can capture fine-grainedstyle information not available in a text-only representation. In addition, weshow concrete examples of how image information can successfully disentanglepairs of highly different items that are ranked similarly by a text-only model.
arxiv-13500-11 | Dynamics of Stochastic Gradient Algorithms | http://arxiv.org/pdf/1511.06251v2.pdf | author:Qianxiao Li, Cheng Tai, Weinan E category:cs.LG stat.ML 68W20 published:2015-11-19 summary:Stochastic gradient algorithms (SGA) are increasingly popular in machinelearning applications and have become "the algorithm" for extremely large scaleproblems. Although there are some convergence results, little is known abouttheir dynamics. In this paper, We propose the method of stochastic modifiedequations (SME) to analyze the dynamics of the SGA. Using this technique, wecan give precise characterizations for both the initial convergence speed andthe eventual oscillations, at least in some special cases. Furthermore, the SMEformalism allows us to characterize various speed-up techniques, such asintroducing momentum, adjusting the learning rate and the mini-batch sizes.Previously, these techniques relied mostly on heuristics. Besides introducingsimple examples to illustrate the SME formalism, we also apply the framework toimprove the relaxed randomized Kaczmarz method for solving linear equations.The SME framework is a precise and unifying approach to understanding andimproving the SGA, and has the potential to be applied to many more stochasticalgorithms.
arxiv-13500-12 | Efficient Learning of Ensembles with QuadBoost | http://arxiv.org/pdf/1506.02535v5.pdf | author:Louis Fortier-Dubois, François Laviolette, Mario Marchand, Louis-Emile Robitaille, Jean-Francis Roy category:cs.LG published:2015-06-08 summary:We first present a general risk bound for ensembles that depends on the Lpnorm of the weighted combination of voters which can be selected from acontinuous set. We then propose a boosting method, called QuadBoost, which isstrongly supported by the general risk bound and has very simple rules forassigning the voters' weights. Moreover, QuadBoost exhibits a rate of decreaseof its empirical error which is slightly faster than the one achieved byAdaBoost. The experimental results confirm the expectation of the theory thatQuadBoost is a very efficient method for learning ensembles.
arxiv-13500-13 | Hand Pose Estimation through Weakly-Supervised Learning of a Rich Intermediate Representation | http://arxiv.org/pdf/1511.06728v1.pdf | author:Natalia Neverova, Christian Wolf, Florian Nebout, Graham Taylor category:cs.CV cs.AI cs.LG published:2015-11-20 summary:We propose a method for hand pose estimation based on a deep regressortrained on two different kinds of input. Raw depth data is fused with anintermediate representation in the form of a segmentation of the hand intoparts. This intermediate representation contains important topologicalinformation and provides useful cues for reasoning about joint locations. Themapping from raw depth to segmentation maps is learned in asemi/weakly-supervised way from two different datasets: (i) a synthetic datasetcreated through a rendering pipeline including densely labeled ground truth(pixelwise segmentations); and (ii) a dataset with real images for which groundtruth joint positions are available, but not dense segmentations. Loss fortraining on real images is generated from a patch-wise restoration process,which aligns tentative segmentation maps with a large dictionary of syntheticposes. The underlying premise is that the domain shift between synthetic andreal data is smaller in the intermediate representation, where labels carrygeometric and topological meaning, than in the raw input domain. Experiments onthe NYU dataset show that the proposed training method decreases error onjoints over direct regression of joints from depth data by 15.7%.
arxiv-13500-14 | Machine Learning Classification of SDSS Transient Survey Images | http://arxiv.org/pdf/1407.4118v3.pdf | author:L. du Buisson, N. Sivanandam, B. A. Bassett, M. Smith category:astro-ph.IM astro-ph.CO cs.CV published:2014-07-15 summary:We show that multiple machine learning algorithms can match human performancein classifying transient imaging data from the Sloan Digital Sky Survey (SDSS)supernova survey into real objects and artefacts. This is a first step in anytransient science pipeline and is currently still done by humans, but futuresurveys such as the Large Synoptic Survey Telescope (LSST) will necessitatefully machine-enabled solutions. Using features trained from eigenimageanalysis (principal component analysis, PCA) of single-epoch g, r andi-difference images, we can reach a completeness (recall) of 96 per cent, whileonly incorrectly classifying at most 18 per cent of artefacts as real objects,corresponding to a precision (purity) of 84 per cent. In general, randomforests performed best, followed by the k-nearest neighbour and the SkyNetartificial neural net algorithms, compared to other methods such as na\"iveBayes and kernel support vector machine. Our results show that PCA-basedmachine learning can match human success levels and can naturally be extendedby including multiple epochs of data, transient colours and host galaxyinformation which should allow for significant further improvements, especiallyat low signal-to-noise.
arxiv-13500-15 | Top-N recommendations from expressive recommender systems | http://arxiv.org/pdf/1511.06718v1.pdf | author:Cyril Stark category:cs.LG stat.ML published:2015-11-20 summary:Normalized nonnegative models assign probability distributions to users andrandom variables to items; see [Stark, 2015]. Rating an item is regarded assampling the random variable assigned to the item with respect to thedistribution assigned to the user who rates the item. Models of that kind arehighly expressive. For instance, using normalized nonnegative models we canunderstand users' preferences as mixtures of interpretable user stereotypes,and we can arrange properties of users and items in a hierarchical manner.These features would not be useful if the predictive power of normalizednonnegative models was poor. Thus, we analyze here the performance ofnormalized nonnegative models for top-N recommendation and observe that theirperformance matches the performance of methods like PureSVD which wasintroduced in [Cremonesi et al., 2010]. We conclude that normalized nonnegativemodels not only provide accurate recommendations but they also deliver (forfree) representations that are interpretable. We deepen the discussion ofnormalized nonnegative models by providing further theoretical insights. Inparticular, we introduce total variational distance as an operationalsimilarity measure, we discover scenarios where normalized nonnegative modelsyield unique representations of users and items, we prove that the inference ofoptimal normalized nonnegative models is NP-hard and finally, we discuss therelationship between normalized nonnegative models and nonnegative matrixfactorization.
arxiv-13500-16 | Semantic Diversity versus Visual Diversity in Visual Dictionaries | http://arxiv.org/pdf/1511.06704v1.pdf | author:Otávio A. B. Penatti, Sandra Avila, Eduardo Valle, Ricardo da S. Torres category:cs.CV published:2015-11-20 summary:Visual dictionaries are a critical component for imageclassification/retrieval systems based on the bag-of-visual-words (BoVW) model.Dictionaries are usually learned without supervision from a training set ofimages sampled from the collection of interest. However, for large,general-purpose, dynamic image collections (e.g., the Web), obtaining arepresentative sample in terms of semantic concepts is not straightforward. Inthis paper, we evaluate the impact of semantics in the dictionary quality,aiming at verifying the importance of semantic diversity in relation visualdiversity for visual dictionaries. In the experiments, we vary the amount ofclasses used for creating the dictionary and then compute different BoVWdescriptors, using multiple codebook sizes and different coding and poolingmethods (standard BoVW and Fisher Vectors). Results for image classificationshow that as visual dictionaries are based on low-level visual appearances,visual diversity is more important than semantic diversity. Our conclusionsopen the opportunity to alleviate the burden in generating visual dictionariesas we need only a visually diverse set of images instead of the wholecollection to create a good dictionary.
arxiv-13500-17 | Single-view to Multi-view: Reconstructing Unseen Views with a Convolutional Network | http://arxiv.org/pdf/1511.06702v1.pdf | author:Maxim Tatarchenko, Alexey Dosovitskiy, Thomas Brox category:cs.CV published:2015-11-20 summary:We present a convolutional network capable of generating images of apreviously unseen object from arbitrary viewpoints given a single image of thisobject. The input to the network is a single image and the desired newviewpoint; the output is a view of the object from this desired viewpoint. Thenetwork is trained on renderings of synthetic 3D models. It learns an implicit3D representation of the object class, which allows it to transfer shapeknowledge from training instances to a new object instance. Beside the colorimage, the network can also generate the depth map of an object from arbitraryviewpoints. This allows us to predict 3D point clouds from a single image,which can be fused into a surface mesh. We experimented with cars and chairs.Even though the network is trained on artificial data, it generalizes well toobjects in natural images without any modifications.
arxiv-13500-18 | Top-k Multiclass SVM | http://arxiv.org/pdf/1511.06683v1.pdf | author:Maksim Lapin, Matthias Hein, Bernt Schiele category:stat.ML cs.CV cs.LG published:2015-11-20 summary:Class ambiguity is typical in image classification problems with a largenumber of classes. When classes are difficult to discriminate, it makes senseto allow k guesses and evaluate classifiers based on the top-k error instead ofthe standard zero-one loss. We propose top-k multiclass SVM as a direct methodto optimize for top-k performance. Our generalization of the well-knownmulticlass SVM is based on a tight convex upper bound of the top-k error. Wepropose a fast optimization scheme based on an efficient projection onto thetop-k simplex, which is of its own interest. Experiments on five datasets showconsistent improvements in top-k accuracy compared to various baselines.
arxiv-13500-19 | Deep End2End Voxel2Voxel Prediction | http://arxiv.org/pdf/1511.06681v1.pdf | author:Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, Manohar Paluri category:cs.CV published:2015-11-20 summary:Over the last few years deep learning methods have emerged as one of the mostprominent approaches for video analysis. However, so far their most successfulapplications have been in the area of video classification and detection, i.e.,problems involving the prediction of a single class label or a handful ofoutput variables per video. Furthermore, while deep networks are commonlyrecognized as the best models to use in these domains, there is a widespreadperception that in order to yield successful results they often requiretime-consuming architecture search, manual tweaking of parameters andcomputationally intensive pre-processing or post-processing methods. In this paper we challenge these views by presenting a deep 3D convolutionalarchitecture trained end to end to perform voxel-level prediction, i.e., tooutput a variable at every voxel of the video. Most importantly, we show thatthe same exact architecture can be used to achieve competitive results on threewidely different voxel-prediction tasks: video semantic segmentation, opticalflow estimation, and video coloring. The three networks learned on theseproblems are trained from raw video without any form of preprocessing and theiroutputs do not require post-processing to achieve outstanding performance.Thus, they offer an efficient alternative to traditional and much morecomputationally expensive methods in these video domains.
arxiv-13500-20 | Personalizing Human Video Pose Estimation | http://arxiv.org/pdf/1511.06676v1.pdf | author:James Charles, Tomas Pfister, Derek Magee, David Hogg, Andrew Zisserman category:cs.CV published:2015-11-20 summary:We propose a personalized ConvNet pose estimator that automatically adaptsitself to the uniqueness of a person's appearance to improve pose estimation inlong videos. We make the following contributions: (i) we show that given a fewhigh-precision pose annotations, e.g. from a generic ConvNet pose estimator,additional annotations can be generated throughout the video using acombination of image-based matching for temporally distant frames, and denseoptical flow for temporally local frames; (ii) we develop an occlusion awareself-evaluation model that is able to automatically select the high-quality andreject the erroneous additional annotations; and (iii) we demonstrate thatthese high-quality annotations can be used to fine-tune a ConvNet poseestimator and thereby personalize it to lock on to key discriminative featuresof the person's appearance. The outcome is a substantial improvement in thepose estimates for the target video using the personalized ConvNet compared tothe original generic ConvNet. Our method outperforms the state of the art(including top ConvNet methods) by a large margin on two standard benchmarks,as well as on a new challenging YouTube video dataset. Furthermore, we showthat training from the automatically generated annotations can be used toimprove the performance of a generic ConvNet on other benchmarks.
arxiv-13500-21 | Stories in the Eye: Contextual Visual Interactions for Efficient Video to Language Translation | http://arxiv.org/pdf/1511.06674v1.pdf | author:Anirudh Goyal, Marius Leordeanu category:cs.CV cs.CL published:2015-11-20 summary:Integrating higher level visual and linguistic interpretations is at theheart of human intelligence. As automatic visual category recognition in imagesis approaching human performance, the high level understanding in the dynamicspatiotemporal domain of videos and its translation into natural language isstill far from being solved. While most works on vision-to-text translationsuse pre-learned or pre-established computational linguistic models, in thispaper we present an approach that uses vision alone to efficiently learn how totranslate into language the video content. We discover, in simple form, thestory played by main actors, while using only visual cues for representingobjects and their interactions. Our method learns in a hierarchical mannerhigher level representations for recognizing subjects, actions and objectsinvolved, their relevant contextual background and their interaction to oneanother over time. We have a three stage approach: first we take inconsideration features of the individual entities at the local level ofappearance, then we consider the relationship between these objects and actionsand their video background, and third, we consider their spatiotemporalrelations as inputs to classifiers at the highest level of interpretation.Thus, our approach finds a coherent linguistic description of videos in theform of a subject, verb and object based on their role played in the overallvisual story learned directly from training data, without using a knownlanguage model. We test the efficiency of our approach on a large scale datasetcontaining YouTube clips taken in the wild and demonstrate state-of-the-artperformance, often superior to current approaches that use more complex,pre-learned linguistic knowledge.
arxiv-13500-22 | L1 logistic regression as a feature selection step for training stable classification trees for the prediction of severity criteria in imported malaria | http://arxiv.org/pdf/1511.06663v1.pdf | author:Luca Talenti, Margaux Luck, Anastasia Yartseva, Nicolas Argy, Sandrine Houzé, Cecilia Damon category:cs.LG q-bio.QM stat.AP published:2015-11-20 summary:Multivariate classification methods using explanatory and predictive modelsare necessary for characterizing subgroups of patients according to their riskprofiles. Popular methods include logistic regression and classification treeswith performances that vary according to the nature and the characteristics ofthe dataset. In the context of imported malaria, we aimed at classifyingseverity criteria based on a heterogeneous patient population. We investigatedthese approaches by implementing two different strategies: L1 logisticregression (L1LR) that models a single global solution and classification treesthat model multiple local solutions corresponding to discriminant subregions ofthe feature space. For each strategy, we built a standard model, and a sparserversion of it. As an alternative to pruning, we explore a promising approachthat first constrains the tree model with an L1LR-based feature selection, anapproach we called L1LR-Tree. The objective is to decrease its vulnerability tosmall data variations by removing variables corresponding to unstable localphenomena. Our study is twofold: i) from a methodological perspective comparingthe performances and the stability of the three previous methods, i.e L1LR,classification trees and L1LR-Tree, for the classification of severe forms ofimported malaria, and ii) from an applied perspective improving the actualclassification of severe forms of imported malaria by identifying morepersonalized profiles predictive of several clinical criteria based onvariables dismissed for the clinical definition of the disease. The mainmethodological results show that the combined method L1LR-Tree builds sparseand stable models that significantly predicts the different severity criteriaand outperforms all the other methods in terms of accuracy.
arxiv-13500-23 | Empirical Study on Deep Learning Models for Question Answering | http://arxiv.org/pdf/1510.07526v3.pdf | author:Yang Yu, Wei Zhang, Chung-Wei Hang, Bing Xiang, Bowen Zhou category:cs.CL cs.AI cs.LG published:2015-10-26 summary:In this paper we explore deep learning models with memory component orattention mechanism for question answering task. We combine and compare threemodels, Neural Machine Translation, Neural Turing Machine, and Memory Networksfor a simulated QA data set. This paper is the first one that uses NeuralMachine Translation and Neural Turing Machines for solving QA tasks. Ourresults suggest that the combination of attention and memory have potential tosolve certain QA problem.
arxiv-13500-24 | Multi-Contrast MRI Reconstruction with Structure-Guided Total Variation | http://arxiv.org/pdf/1511.06631v1.pdf | author:Matthias J. Ehrhardt, Marta M. Betcke category:math.NA cs.CV math.OC published:2015-11-20 summary:Magnetic resonance imaging (MRI) is a versatile imaging technique that allowsdifferent contrasts depending on the acquisition parameters. Many clinicalimaging studies acquire MRI data for more than one of these contrasts---such asfor instance T1 and T2 weighted images---which makes the overall scanningprocedure very time consuming. As all of these images show the same underlyinganatomy one can try to omit unnecessary measurements by taking the similarityinto account during reconstruction. We will discuss two modifications of totalvariation---based on i) location and ii) direction---that take structural apriori knowledge into account and reduce to total variation in the degeneratecase when no structural knowledge is available. We solve the resulting convexminimization problem with the alternating direction method of multipliers thatseparates the forward operator from the prior. For both priors thecorresponding proximal operator can be implemented as an extension of the fastgradient projection method on the dual problem for total variation. We testedthe priors on six data sets that are based on phantoms and real MRI images. Inall test cases exploiting the structural information from the other contrastyields better results than separate reconstruction with total variation interms of standard metrics like peak signal-to-noise ratio and structuralsimilarity index. Furthermore, we found that exploiting the two dimensionaldirectional information results in images with well defined edges, superior tothose reconstructed solely using a priori information about the edge location.
arxiv-13500-25 | Towards Arbitrary-View Face Alignment by Recommendation Trees | http://arxiv.org/pdf/1511.06627v1.pdf | author:Shizhan Zhu, Cheng Li, Chen Change Loy, Xiaoou Tang category:cs.CV published:2015-11-20 summary:Learning to simultaneously handle face alignment of arbitrary views, e.g.frontal and profile views, appears to be more challenging than we thought. Thedifficulties lay in i) accommodating the complex appearance-shape relationsexhibited in different views, and ii) encompassing the varying landmark pointsets due to self-occlusion and different landmark protocols. Most existingstudies approach this problem via training multiple viewpoint-specific models,and conduct head pose estimation for model selection. This solution isintuitive but the performance is highly susceptible to inaccurate head poseestimation. In this study, we address this shortcoming through learning anEnsemble of Model Recommendation Trees (EMRT), which is capable of selectingoptimal model configuration without prior head pose estimation. The unifiedframework seamlessly handles different viewpoints and landmark protocols, andit is trained by optimising directly on landmark locations, thus yieldingsuperior results on arbitrary-view face alignment. This is the first study thatperforms face alignment on the full AFLWdataset with faces of different viewsincluding profile view. State-of-the-art performances are also reported onMultiPIE and AFW datasets containing both frontaland profile-view faces.
arxiv-13500-26 | Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images | http://arxiv.org/pdf/1506.07365v3.pdf | author:Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, Martin Riedmiller category:cs.LG cs.CV stat.ML published:2015-06-24 summary:We introduce Embed to Control (E2C), a method for model learning and controlof non-linear dynamical systems from raw pixel images. E2C consists of a deepgenerative model, belonging to the family of variational autoencoders, thatlearns to generate image trajectories from a latent space in which the dynamicsis constrained to be locally linear. Our model is derived directly from anoptimal control formulation in latent space, supports long-term prediction ofimage sequences and exhibits strong performance on a variety of complex controlproblems.
arxiv-13500-27 | Identifying the Absorption Bump with Deep Learning | http://arxiv.org/pdf/1511.05607v2.pdf | author:Min Li, Sudeep Gaddam, Xiaolin Li, Yinan Zhao, Jingzhe Ma, Jian Ge category:cs.CV cs.LG cs.NE published:2015-11-17 summary:The pervasive interstellar dust grains provide significant insights tounderstand the formation and evolution of the stars, planetary systems, and thegalaxies, and may harbor the building blocks of life. One of the most effectiveway to analyze the dust is via their interaction with the light from backgroundsources. The observed extinction curves and spectral features carry the sizeand composition information of dust. The broad absorption bump at 2175 Angstromis the most prominent feature in the extinction curves. Traditionally,statistical methods are applied to detect the existence of the absorption bump.These methods require heavy preprocessing and the co-existence of otherreference features to alleviate the influence from the noises. In this paper,we apply Deep Learning techniques to detect the broad absorption bump. Wedemonstrate the key steps for training the selected models and their results.The success of Deep Learning based method inspires us to generalize a commonmethodology for broader science discovery problems. We present our on-goingwork to build the DeepDis system for such kind of applications.
arxiv-13500-28 | Exponential Natural Particle Filter | http://arxiv.org/pdf/1511.06603v1.pdf | author:Ghazal Zand, Mojtaba Taherkhani, Reza Safabakhsh category:cs.LG cs.NE cs.RO published:2015-11-20 summary:Particle Filter algorithm (PF) suffers from some problems such as the loss ofparticle diversity, the need for large number of particles, and the costlyselection of the importance density functions. In this paper, a novelExponential Natural Particle Filter (xNPF) is introduced to solve the aboveproblems. In this approach, a state transitional probability with the use ofnatural gradient learning is proposed which balances exploration andexploitation more robustly. The results show that xNPF converges much closer tothe true target states than the other state of the art particle filter.
arxiv-13500-29 | Polysemy in Controlled Natural Language Texts | http://arxiv.org/pdf/1511.06591v1.pdf | author:Normunds Gruzitis, Guntis Barzdins category:cs.CL published:2015-11-20 summary:Computational semantics and logic-based controlled natural languages (CNL) donot address systematically the word sense disambiguation problem of contentwords, i.e., they tend to interpret only some functional words that are crucialfor construction of discourse representation structures. We show thatmicro-ontologies and multi-word units allow integration of the rich andpolysemous multi-domain background knowledge into CNL thus providinginterpretation for the content words. The proposed approach is demonstrated byextending the Attempto Controlled English (ACE) with polysemous and proceduralconstructs resulting in a more natural CNL named PAO covering narrativemulti-domain texts.
arxiv-13500-30 | Crowd Behavior Analysis: A Review where Physics meets Biology | http://arxiv.org/pdf/1511.06586v1.pdf | author:Ven Jyn Kok, Mei Kuan Lim, Chee Seng Chan category:cs.CV cs.AI cs.NE published:2015-11-20 summary:Although the traits emerged in a mass gathering are often non-deliberative,the act of mass impulse may lead to irre- vocable crowd disasters. The two-foldincrease of carnage in crowd since the past two decades has spurred significantadvances in the field of computer vision, towards effective and proactive crowdsurveillance. Computer vision stud- ies related to crowd are observed toresonate with the understanding of the emergent behavior in physics (complexsystems) and biology (animal swarm). These studies, which are inspired bybiology and physics, share surprisingly common insights, and interestingcontradictions. However, this aspect of discussion has not been fully explored.Therefore, this survey provides the readers with a review of thestate-of-the-art methods in crowd behavior analysis from the physics andbiologically inspired perspectives. We provide insights and comprehensivediscussions for a broader understanding of the underlying prospect of blendingphysics and biology studies in computer vision.
arxiv-13500-31 | Semantic Summarization of Egocentric Photo Stream Events | http://arxiv.org/pdf/1511.00438v2.pdf | author:Aniol Lidon, Marc Bolaños, Mariella Dimiccoli, Petia Radeva, Maite Garolera, Xavier Giró-i-Nieto category:cs.CV published:2015-11-02 summary:With the rapid increase of users of wearable cameras in recent years and ofthe amount of data they produce, there is a strong need for automatic retrievaland summarization techniques. This work addresses the problem of automaticallysummarizing egocentric photo streams captured through a wearable camera bytaking an image retrieval perspective. After removing non-informative images bya new CNN-based filter, images are ranked by relevance to ensure semanticdiversity and finally re-ranked by a novelty criterion to reduce redundancy. Toassess the results, a new evaluation metric is proposed which takes intoaccount the non-uniqueness of the solution. Experimental results applied on adatabase of 7,110 images from 6 different subjects and evaluated by expertsgave 95.74% of experts satisfaction and a Mean Opinion Score of 4.57 out of5.0.
arxiv-13500-32 | Learning Adversary Behavior in Security Games: A PAC Model Perspective | http://arxiv.org/pdf/1511.00043v3.pdf | author:Arunesh Sinha, Debarun Kar, Milind Tambe category:cs.AI cs.GT cs.LG published:2015-10-30 summary:Recent applications of Stackelberg Security Games (SSG), from wildlife crimeto urban crime, have employed machine learning tools to learn and predictadversary behavior using available data about defender-adversary interactions.Given these recent developments, this paper commits to an approach of directlylearning the response function of the adversary. Using the PAC model, thispaper lays a firm theoretical foundation for learning in SSGs (e.g.,theoretically answer questions about the numbers of samples required to learnadversary behavior) and provides utility guarantees when the learned adversarymodel is used to plan the defender's strategy. The paper also aims to answerpractical questions such as how much more data is needed to improve anadversary model's accuracy. Additionally, we explain a recently observedphenomenon that prediction accuracy of learned adversary behavior is not enoughto discover the utility maximizing defender strategy. We provide four maincontributions: (1) a PAC model of learning adversary response functions inSSGs; (2) PAC-model analysis of the learning of key, existing boundedrationality models in SSGs; (3) an entirely new approach to adversary modelingbased on a non-parametric class of response functions with PAC-model analysisand (4) identification of conditions under which computing the best defenderstrategy against the learned adversary behavior is indeed the optimal strategy.Finally, we conduct experiments with real-world data from a national park inUganda, showing the benefit of our new adversary modeling approach andverification of our PAC model predictions.
arxiv-13500-33 | WIDER FACE: A Face Detection Benchmark | http://arxiv.org/pdf/1511.06523v1.pdf | author:Shuo Yang, Ping Luo, Chen Change Loy, Xiaoou Tang category:cs.CV published:2015-11-20 summary:Face detection is one of the most studied topics in the computer visioncommunity. Much of the progresses have been made by the availability of facedetection benchmark datasets. We show that there is a gap between current facedetection performance and the real world requirements. To facilitate futureface detection research, we introduce the WIDER FACE dataset, which is 10 timeslarger than existing datasets. The dataset contains rich annotations, includingocclusions, poses, event categories, and face bounding boxes. Faces in theproposed dataset are extremely challenging due to large variations in scale,pose and occlusion, as shown in Fig. 1. Furthermore, we show that WIDER FACEdataset is an effective training source for face detection. We benchmarkseveral representative detection systems, providing an overview ofstate-of-the-art performance and propose a solution to deal with large scalevariation. Finally, we discuss common failure cases that worth to be furtherinvestigated. Dataset can be downloaded at:mmlab.ie.cuhk.edu.hk/projects/WIDERFace
arxiv-13500-34 | A Distribution Adaptive Framework for Prediction Interval Estimation Using Nominal Variables | http://arxiv.org/pdf/1511.05688v2.pdf | author:Ameen Eetemadi, Ilias Tagkopoulos category:cs.LG published:2015-11-18 summary:Proposed methods for prediction interval estimation so far focus on caseswhere input variables are numerical. In datasets with solely nominal inputvariables, we observe records with the exact same input $x^u$, but differentreal valued outputs due to the inherent noise in the system. Existingprediction interval estimation methods do not use representations that canaccurately model such inherent noise in the case of nominal inputs. We proposea new prediction interval estimation method tailored for this type of data,which is prevalent in biology and medicine. We call this method DistributionAdaptive Prediction Interval Estimation given Nominal inputs (DAPIEN) and hasfour main phases. First, we select a distribution function that can bestrepresent the inherent noise of the system for all unique inputs. Then we inferthe parameters $\theta_i$ (e.g. $\theta_i=[mean_i, variance_i]$) of theselected distribution function for all unique input vectors $x^u_i$ andgenerate a new corresponding training set using pairs of $x^u_i, \theta_i$.III). Then, we train a model to predict $\theta$ given a new $x_u$. Finally, wecalculate the prediction interval for a new sample using the inverse of thecumulative distribution function once the parameters $\theta$ is predicted bythe trained model. We compared DAPIEN to the commonly used Bootstrap method onthree synthetic datasets. Our results show that DAPIEN provides tighterprediction intervals while preserving the requested coverage when compared toBootstrap. This work can facilitate broader usage of regression methods inmedicine and biology where it is necessary to provide tight predictionintervals while preserving coverage when input variables are nominal.
arxiv-13500-35 | Flattened Convolutional Neural Networks for Feedforward Acceleration | http://arxiv.org/pdf/1412.5474v4.pdf | author:Jonghoon Jin, Aysegul Dundar, Eugenio Culurciello category:cs.NE cs.LG published:2014-12-17 summary:We present flattened convolutional neural networks that are designed for fastfeedforward execution. The redundancy of the parameters, especially weights ofthe convolutional filters in convolutional neural networks has been extensivelystudied and different heuristics have been proposed to construct a low rankbasis of the filters after training. In this work, we train flattened networksthat consist of consecutive sequence of one-dimensional filters across alldirections in 3D space to obtain comparable performance as conventionalconvolutional networks. We tested flattened model on different datasets andfound that the flattened layer can effectively substitute for the 3D filterswithout loss of accuracy. The flattened convolution pipelines provide aroundtwo times speed-up during feedforward pass compared to the baseline model dueto the significant reduction of learning parameters. Furthermore, the proposedmethod does not require efforts in manual tuning or post processing once themodel is trained.
arxiv-13500-36 | Bidirectional Warping of Active Appearance Model | http://arxiv.org/pdf/1511.06494v1.pdf | author:Ali Mollahosseini, Mohammad H. Mahoor category:cs.CV published:2015-11-20 summary:Active Appearance Model (AAM) is a commonly used method for facial imageanalysis with applications in face identification and facial expressionrecognition. This paper proposes a new approach based on image alignment forAAM fitting called bidirectional warping. Previous approaches warp either theinput image or the appearance template. We propose to warp both the inputimage, using incremental update by an affine transformation, and the appearancetemplate, using an inverse compositional approach. Our experimental results onMulti-PIE face database show that the bidirectional approach outperformsstate-of-the-art inverse compositional fitting approaches in extractinglandmark points of faces with shape and pose variations.
arxiv-13500-37 | A Simple Hierarchical Pooling Data Structure for Loop Closure | http://arxiv.org/pdf/1511.06489v1.pdf | author:Xiaohan Fei, Konstantine Tsotsos, Stefano Soatto category:cs.CV cs.RO published:2015-11-20 summary:We propose a data structure obtained by hierarchically averaging bag-of-worddescriptors during a sequence of views that achieves average speedups inlarge-scale loop closure applications ranging from 4 to 20 times on benchmarkdatasets. Although simple, the method works as well as sophisticatedagglomerative schemes at a fraction of the cost with minimal loss ofperformance.
arxiv-13500-38 | Infinite Dimensional Word Embeddings | http://arxiv.org/pdf/1511.05392v2.pdf | author:Eric Nalisnick, Sachin Ravi category:stat.ML cs.CL cs.LG published:2015-11-17 summary:We describe a method for learning word embeddings with stochasticdimensionality. Our Infinite Skip-Gram (iSG) model specifies an energy-basedjoint distribution over a word vector, a context vector, and theirdimensionality. By employing the same techniques used to make the InfiniteRestricted Boltzmann Machine (Cote & Larochelle, 2015) tractable, we definevector dimensionality over a countably infinite domain, allowing vectors togrow as needed during training. After training, we find that the distributionover embedding dimensionality for a given word is highly interpretable andleads to an elegant probabilistic mechanism for word sense induction. We showqualitatively and quantitatively that the iSG produces parameter-efficientrepresentations that are robust to language's inherent ambiguity.
arxiv-13500-39 | Fast Multi-class Dictionaries Learning with Geometrical Directions in MRI Reconstruction | http://arxiv.org/pdf/1503.02945v2.pdf | author:Zhifang Zhan, Jian-Feng Cai, Di Guo, Yunsong Liu, Zhong Chen, Xiaobo Qu category:cs.CV math.OC physics.med-ph published:2015-03-10 summary:Objective: Improve the reconstructed image with fast and multi-classdictionaries learning when magnetic resonance imaging is accelerated byundersampling the k-space data. Methods: A fast orthogonal dictionary learningmethod is introduced into magnetic resonance image reconstruction to providingadaptive sparse representation of images. To enhance the sparsity, image isdivided into classified patches according to the same geometrical direction anddictionary is trained within each class. A new sparse reconstruction model withthe multi-class dictionaries is proposed and solved using a fast alternatingdirection method of multipliers. Results: Experiments on phantom and brainimaging data with acceleration factor up to 10 and various undersamplingpatterns are conducted. The proposed method is compared with state-of-the-artmagnetic resonance image reconstruction methods. Conclusion: Artifacts arebetter suppressed and image edges are better preserved than the comparedmethods. Besides, the computation of the proposed approach is much faster thanthe typical K-SVD dictionary learning method in magnetic resonance imagereconstruction. Significance: The proposed method can be exploited inundersapmled magnetic resonance imaging to reduce data acquisition time andreconstruct images with better image quality.
arxiv-13500-40 | Tract Orientation and Angular Dispersion Deviation Indicator (TOADDI): A framework for single-subject analysis in diffusion tensor imaging | http://arxiv.org/pdf/1510.02934v2.pdf | author:Cheng Guan Koay, Ping-Hong Yeh, John M. Ollinger, M. Okan İrfanoğlu, Carlo Pierpaoli, Peter J. Basser, Terrence R. Oakes, Gerard Riedy category:physics.med-ph cs.CV stat.AP stat.CO stat.ME published:2015-10-10 summary:The purpose of this work is to develop a framework for single-subjectanalysis of diffusion tensor imaging (DTI) data. This framework (termed TOADDI)is capable of testing whether an individual tract as represented by the majoreigenvector of the diffusion tensor and its corresponding angular dispersionare significantly different from a group of tracts on a voxel-by-voxel basis.This work develops two complementary statistical tests based on the ellipticalcone of uncertainty (COU), which is a model of uncertainty or dispersion of themajor eigenvector of the diffusion tensor. The orientation deviation testexamines whether the major eigenvector from a single subject is within theaverage elliptical COU formed by a collection of elliptical COUs. The shapedeviation test is based on the two-tailed Wilcoxon-Mann-Whitney two-sample testbetween the normalized shape measures (area and circumference) of theelliptical cones of uncertainty of the single subject against a group ofcontrols. The False Discovery Rate (FDR) and False Non-discovery Rate (FNR)were incorporated in the orientation deviation test. The shape deviation testuses FDR only. TOADDI was found to be numerically accurate and statisticallyeffective. Clinical data from two Traumatic Brain Injury (TBI) patients and onenon-TBI subject were tested against the data obtained from a group of 45non-TBI controls to illustrate the application of the proposed framework insingle-subject analysis. The frontal portion of the superior longitudinalfasciculus seemed to be implicated in both tests as significantly differentfrom that of the control group. The TBI patients and the single non-TBI subjectwere well separated under the shape deviation test at the chosen FDR level of0.0005. TOADDI is a simple but novel geometrically based statistical frameworkfor analyzing DTI data.
arxiv-13500-41 | Joint Inverse Covariances Estimation with Mutual Linear Structure | http://arxiv.org/pdf/1511.06462v1.pdf | author:Ilya Soloveychik, Ami Wiesel category:stat.ML stat.AP published:2015-11-20 summary:We consider the problem of joint estimation of structured inverse covariancematrices. We perform the estimation using groups of measurements with differentcovariances of the same unknown structure. Assuming the inverse covariances tospan a low dimensional linear subspace in the space of symmetric matrices, ouraim is to determine this structure. It is then utilized to improve theestimation of the inverse covariances. We propose a novel optimizationalgorithm discovering and exploiting the underlying structure and provide itsefficient implementation. Numerical simulations are presented to illustrate theperformance benefits of the proposed algorithm.
arxiv-13500-42 | Efficient Likelihood Learning of a Generic CNN-CRF Model for Semantic Segmentation | http://arxiv.org/pdf/1511.05067v2.pdf | author:Alexander Kirillov, Dmitrij Schlesinger, Walter Forkel, Anatoly Zelenin, Shuai Zheng, Philip Torr, Carsten Rother category:cs.CV published:2015-11-16 summary:Deep Models, such as Convolutional Neural Networks (CNNs), are omnipresent incomputer vision, as well as, structured models, such as Conditional RandomFields (CRFs). Combining them brings many advantages, foremost the ability toin-cooperate prior knowledge into CNNs, e.g. by explicitly modelling thedependencies between output variables. In this work we present a CRF model wereunary factors are dependent on a CNN. Our main contribution is an efficient andscalable, maximum likelihood-based, learning procedure to infer all modelparameters jointly. Previous work either concentrated on piecewise-training, ormaximum likelihood learning of restricted model families, such as Gaussian CRFsor CRFs with a few variables only. In contrast, we are the first to performmaximum likelihood learning for large-sized factor graphs with non-parametricpotentials. We have applied our model to the task of semantic labeling of bodyparts in depth images. We show that it is superior to selected competing modelsand learning strategies. Furthermore, we empirically observe that our model cancapture shape and context information of relating body parts.
arxiv-13500-43 | Why are deep nets reversible: A simple theory, with implications for training | http://arxiv.org/pdf/1511.05653v2.pdf | author:Sanjeev Arora, Yingyu Liang, Tengyu Ma category:cs.LG published:2015-11-18 summary:Generative models for deep learning are promising both to improveunderstanding of the model, and yield training methods requiring fewer labeledsamples. Recent works use generative model approaches to produce the deep net's inputgiven the value of a hidden layer several levels above. However, there is noaccompanying "proof of correctness" for the generative model, showing that thefeedforward deep net is the correct inference method for recovering the hiddenlayer given the input. Furthermore, these models are complicated. The current paper takes a more theoretical tack. It presents a very simplegenerative model for RELU deep nets, with the following characteristics: (i)The generative model is just the reverse of the feedforward net: if the forwardtransformation at a layer is $A$ then the reverse transformation is $A^T$.(This can be seen as an explanation of the old weight tying idea for denoisingautoencoders.) (ii) Its correctness can be proven under a clean theoreticalassumption: the edge weights in real-life deep nets behave like random numbers.Under this assumption ---which is experimentally tested on real-life nets likeAlexNet--- it is formally proved that feed forward net is a correct inferencemethod for recovering the hidden layer. The generative model suggests a simple modification for training: use thegenerative model to produce synthetic data with labels and include it in thetraining set. Experiments are shown to support this theory of random-like deepnets; and that it helps the training.
arxiv-13500-44 | No-Regret Learning in Bayesian Games | http://arxiv.org/pdf/1507.00418v2.pdf | author:Jason Hartline, Vasilis Syrgkanis, Eva Tardos category:cs.GT cs.LG published:2015-07-02 summary:Recent price-of-anarchy analyses of games of complete information suggestthat coarse correlated equilibria, which characterize outcomes resulting fromno-regret learning dynamics, have near-optimal welfare. This work provides twomain technical results that lift this conclusion to games of incompleteinformation, a.k.a., Bayesian games. First, near-optimal welfare in Bayesiangames follows directly from the smoothness-based proof of near-optimal welfarein the same game when the private information is public. Second, no-regretlearning dynamics converge to Bayesian coarse correlated equilibrium in theseincomplete information games. These results are enabled by interpretation of aBayesian game as a stochastic game of complete information.
arxiv-13500-45 | Deep Metric Learning via Lifted Structured Feature Embedding | http://arxiv.org/pdf/1511.06452v1.pdf | author:Hyun Oh Song, Yu Xiang, Stefanie Jegelka, Silvio Savarese category:cs.CV cs.LG published:2015-11-19 summary:Learning the distance metric between pairs of examples is of great importancefor learning and visual recognition. With the remarkable success from the stateof the art convolutional neural networks, recent works have shown promisingresults on discriminatively training the networks to learn semantic featureembeddings where similar examples are mapped close to each other and dissimilarexamples are mapped farther apart. In this paper, we describe an algorithm fortaking full advantage of the training batches in the neural network training bylifting the vector of pairwise distances within the batch to the matrix ofpairwise distances. This step enables the algorithm to learn the state of theart feature embedding by optimizing a novel structured prediction objective onthe lifted problem. Additionally, we collected Online Products dataset: 120kimages of 23k classes of online products for metric learning. Our experimentson the CUB-200-2011, CARS196, and Online Products datasets demonstratesignificant improvement over existing deep feature embedding methods on allexperimented embedding sizes with the GoogLeNet network.
arxiv-13500-46 | Learning to Represent Words in Context with Multilingual Supervision | http://arxiv.org/pdf/1511.04623v2.pdf | author:Kazuya Kawakami, Chris Dyer category:cs.CL published:2015-11-14 summary:We present a neural network architecture based on bidirectional LSTMs tocompute representations of words in the sentential contexts. Thesecontext-sensitive word representations are suitable for, e.g., distinguishingdifferent word senses and other context-modulated variations in meaning. Tolearn the parameters of our model, we use cross-lingual supervision,hypothesizing that a good representation of a word in context will be one thatis sufficient for selecting the correct translation into a second language. Weevaluate the quality of our representations as features in three downstreamtasks: prediction of semantic supersenses (which assign nouns and verbs into afew dozen semantic classes), low resource machine translation, and a lexicalsubstitution task, and obtain state-of-the-art results on all of these.
arxiv-13500-47 | Joint Word Representation Learning using a Corpus and a Semantic Lexicon | http://arxiv.org/pdf/1511.06438v1.pdf | author:Danushka Bollegala, Alsuhaibani Mohammed, Takanori Maehara, Ken-ichi Kawarabayashi category:cs.CL cs.AI published:2015-11-19 summary:Methods for learning word representations using large text corpora havereceived much attention lately due to their impressive performance in numerousnatural language processing (NLP) tasks such as, semantic similaritymeasurement, and word analogy detection. Despite their success, thesedata-driven word representation learning methods do not consider the richsemantic relational structure between words in a co-occurring context. On theother hand, already much manual effort has gone into the construction ofsemantic lexicons such as the WordNet that represent the meanings of words bydefining the various relationships that exist among the words in a language. Weconsider the question, can we improve the word representations learnt using acorpora by integrating the knowledge from semantic lexicons?. For this purpose,we propose a joint word representation learning method that simultaneouslypredicts the co-occurrences of two words in a sentence subject to therelational constrains given by the semantic lexicon. We use relations thatexist between words in the lexicon to regularize the word representationslearnt from the corpus. Our proposed method statistically significantlyoutperforms previously proposed methods for incorporating semantic lexiconsinto word representations on several benchmark datasets for semantic similarityand word analogy.
arxiv-13500-48 | AUC-maximized Deep Convolutional Neural Fields for Sequence Labeling | http://arxiv.org/pdf/1511.05265v2.pdf | author:Sheng Wang, Siqi Sun, Jinbo Xu category:stat.ML cs.LG published:2015-11-17 summary:Deep Convolutional Neural Networks (DCNN) has shown excellent performance ina variety of machine learning tasks. This manuscript presents DeepConvolutional Neural Fields (DeepCNF), a combination of DCNN with ConditionalRandom Field (CRF), for sequence labeling with highly imbalanced labeldistribution. The widely-used training methods, such as maximum-likelihood andmaximum labelwise accuracy, do not work well on highly imbalanced data. Tohandle this, we present a new training algorithm called maximum-AUC forDeepCNF. That is, we train DeepCNF by directly maximizing the empirical AreaUnder the ROC Curve (AUC), which is an unbiased measurement for imbalanceddata. To fulfill this, we formulate AUC in a pairwise ranking framework,approximate it by a polynomial function and then apply a gradient-basedprocedure to optimize it. We then test our AUC-maximized DeepCNF on three verydifferent protein sequence labeling tasks: solvent accessibility prediction,8-state secondary structure prediction, and disorder prediction. Ourexperimental results confirm that maximum-AUC greatly outperforms the other twotraining methods on 8-state secondary structure prediction and disorderprediction since their label distributions are highly imbalanced and also havesimilar performance as the other two training methods on the solventaccessibility prediction problem which has three equally-distributed labels.Furthermore, our experimental results also show that our AUC-trained DeepCNFmodels greatly outperform existing popular predictors of these three tasks.
arxiv-13500-49 | Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models | http://arxiv.org/pdf/1507.00814v3.pdf | author:Bradly C. Stadie, Sergey Levine, Pieter Abbeel category:cs.AI cs.LG stat.ML published:2015-07-03 summary:Achieving efficient and scalable exploration in complex domains poses a majorchallenge in reinforcement learning. While Bayesian and PAC-MDP approaches tothe exploration problem offer strong formal guarantees, they are oftenimpractical in higher dimensions due to their reliance on enumerating thestate-action space. Hence, exploration in complex domains is often performedwith simple epsilon-greedy methods. In this paper, we consider the challengingAtari games domain, which requires processing raw pixel inputs and delayedrewards. We evaluate several more sophisticated exploration strategies,including Thompson sampling and Boltzman exploration, and propose a newexploration method based on assigning exploration bonuses from a concurrentlylearned model of the system dynamics. By parameterizing our learned model witha neural network, we are able to develop a scalable and efficient approach toexploration bonuses that can be applied to tasks with complex, high-dimensionalstate spaces. In the Atari domain, our method provides the most consistentimprovement across a range of games that pose a major challenge for priormethods. In addition to raw game-scores, we also develop an AUC-100 metric forthe Atari Learning domain to evaluate the impact of exploration on thisbenchmark.
arxiv-13500-50 | ParseNet: Looking Wider to See Better | http://arxiv.org/pdf/1506.04579v2.pdf | author:Wei Liu, Andrew Rabinovich, Alexander C. Berg category:cs.CV published:2015-06-15 summary:We present a technique for adding global context to deep convolutionalnetworks for semantic segmentation. The approach is simple, using the averagefeature for a layer to augment the features at each location. In addition, westudy several idiosyncrasies of training, significantly increasing theperformance of baseline networks (e.g. from FCN). When we add our proposedglobal feature, and a technique for learning normalization parameters, accuracyincreases consistently even over our improved versions of the baselines. Ourproposed approach, ParseNet, achieves state-of-the-art performance on SiftFlowand PASCAL-Context with small additional computational cost over baselines, andnear current state-of-the-art performance on PASCAL VOC 2012 semanticsegmentation with a simple approach. Code is available athttps://github.com/weiliu89/caffe/tree/fcn .
arxiv-13500-51 | Canonical Autocorrelation Analysis | http://arxiv.org/pdf/1511.06419v1.pdf | author:Maria De-Arteaga, Artur Dubrawski, Peter Huggins category:stat.ML cs.LG published:2015-11-19 summary:We present an extension of sparse Canonical Correlation Analysis (CCA)designed for finding multiple-to-multiple linear correlations within a singleset of variables. Unlike CCA, which finds correlations between two sets of datawhere the rows are matched exactly but the columns represent separate sets ofvariables, the method proposed here, Canonical Autocorrelation Analysis (CAA),finds multivariate correlations within just one set of variables. This can beuseful when we look for hidden parsimonious structures in data, each involvingonly a small subset of all features. In addition, the discovered correlationsare highly interpretable as they are formed by pairs of sparse linearcombinations of the original features. We show how CAA can be of use as a toolfor anomaly detection when the expected structure of correlations is notfollowed by anomalous data. We illustrate the utility of CAA in two applicationdomains where single-class and unsupervised learning of correlation structuresare particularly relevant: breast cancer diagnosis and radiation threatdetection. When applied to the Wisconsin Breast Cancer data, single-class CAAis competitive with supervised methods used in literature. On the radiationthreat detection task, unsupervised CAA performs significantly better than anunsupervised alternative prevalent in the domain, while providing valuableadditional insights for threat analysis.
arxiv-13500-52 | Fast Parallel SAME Gibbs Sampling on General Discrete Bayesian Networks | http://arxiv.org/pdf/1511.06416v1.pdf | author:Daniel Seita, Haoyu Chen, John Canny category:cs.LG stat.ML published:2015-11-19 summary:A fundamental task in machine learning and related fields is to performinference on Bayesian networks. Since exact inference takes exponential time ingeneral, a variety of approximate methods are used. Gibbs sampling is one ofthe most accurate approaches and provides unbiased samples from the posteriorbut it has historically been too expensive for large models. In this paper, wepresent an optimized, parallel Gibbs sampler augmented with state replication(SAME or State Augmented Marginal Estimation) to decrease convergence time. Wefind that SAME can improve the quality of parameter estimates whileaccelerating convergence. Experiments on both synthetic and real data show thatour Gibbs sampler is substantially faster than the state of the art sampler,JAGS, without sacrificing accuracy. Our ultimate objective is to introduce theGibbs sampler to researchers in many fields to expand their range of feasibleinference problems.
arxiv-13500-53 | Direct Loss Minimization for Training Deep Neural Nets | http://arxiv.org/pdf/1511.06411v1.pdf | author:Yang Song, Alexander G. Schwing, Richard S. Zemel, Raquel Urtasun category:cs.LG published:2015-11-19 summary:Supervised training of deep neural nets typically relies on minimizingcross-entropy. However, in many domains, we are interested in performing wellon specific application-specific metrics. In this paper we proposed a directloss minimization approach to train deep neural networks, taking into accountthe application-specific loss functions. This can be non-trivial, when thesefunctions are non-smooth and non-decomposable. We demonstrate the effectivenessof our approach in the context of maximizing average precision for rankingproblems. Towards this goal, we propose a dynamic programming algorithm thatcan efficiently compute the weight updates. Our approach proves superior to avariety of baselines in the context of action classification and objectdetection.
arxiv-13500-54 | sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings | http://arxiv.org/pdf/1511.06388v1.pdf | author:Andrew Trask, Phil Michalak, John Liu category:cs.CL cs.LG published:2015-11-19 summary:Neural word representations have proven useful in Natural Language Processing(NLP) tasks due to their ability to efficiently model complex semantic andsyntactic word relationships. However, most techniques model only onerepresentation per word, despite the fact that a single word can have multiplemeanings or "senses". Some techniques model words by using multiple vectorsthat are clustered based on context. However, recent neural approaches rarelyfocus on the application to a consuming NLP algorithm. Furthermore, thetraining process of recent word-sense models is expensive relative tosingle-sense embedding processes. This paper presents a novel approach whichaddresses these concerns by modeling multiple embeddings for each word based onsupervised disambiguation, which provides a fast and accurate way for aconsuming NLP model to select a sense-disambiguated embedding. We demonstratethat these embeddings can disambiguate both contrastive senses such as nominaland verbal senses as well as nuanced senses such as sarcasm. We furtherevaluate Part-of-Speech disambiguated embeddings on neural dependency parsing,yielding a greater than 8% average error reduction in unlabeled attachmentscores across 6 languages.
arxiv-13500-55 | A Unified Gradient Regularization Family for Adversarial Examples | http://arxiv.org/pdf/1511.06385v1.pdf | author:Chunchuan Lyu, Kaizhu Huang, Hai-Ning Liang category:cs.LG stat.ML published:2015-11-19 summary:Adversarial examples are augmented data points generated by imperceptibleperturbation of input samples. They have recently drawn much attention with themachine learning and data mining community. Being difficult to distinguish fromreal examples, such adversarial examples could change the prediction of many ofthe best learning models including the state-of-the-art deep learning models.Recent attempts have been made to build robust models that take into accountadversarial examples. However, these methods can either lead to performancedrops or lack mathematical motivations. In this paper, we propose a unifiedframework to build robust machine learning models against adversarial examples.More specifically, using the unified framework, we develop a family of gradientregularization methods that effectively penalize the gradient of loss functionw.r.t. inputs. Our proposed framework is appealing in that it offers a unifiedview to deal with adversarial examples. It incorporates anotherrecently-proposed perturbation based approach as a special case. In addition,we present some visual effects that reveals semantic meaning in thoseperturbations, and thus support our regularization method and provide anotherexplanation for generalizability of adversarial examples. By applying thistechnique to Maxout networks, we conduct a series of experiments and achieveencouraging results on two benchmark datasets. In particular,we attain the bestaccuracy on MNIST data (without data augmentation) and competitive performanceon CIFAR-10 data.
arxiv-13500-56 | Dynamic Adaptive Network Intelligence | http://arxiv.org/pdf/1511.06379v1.pdf | author:Richard Searle, Megan Bingham-Walker category:cs.CL cs.LG published:2015-11-19 summary:Accurate representational learning of both the explicit and implicitrelationships within data is critical to the ability of machines to performmore complex and abstract reasoning tasks. We describe the efficient weaklysupervised learning of such inferences by our Dynamic Adaptive NetworkIntelligence (DANI) model. We report state-of-the-art results for DANI overquestion answering tasks in the bAbI dataset that have proved difficult forcontemporary approaches to learning representation (Weston et al., 2015).
arxiv-13500-57 | Learning Representations Using Complex-Valued Nets | http://arxiv.org/pdf/1511.06351v1.pdf | author:Andy M. Sarroff, Victor Shepardson, Michael A. Casey category:cs.LG cs.NE published:2015-11-19 summary:Complex-valued neural networks (CVNNs) are an emerging field of research inneural networks due to their potential representational properties for audio,image, and physiological signals. It is common in signal processing totransform sequences of real values to the complex domain via a set of complexbasis functions, such as the Fourier transform. We show how CVNNs can be usedto learn complex representations of real valued time-series data. We presentmethods and results using a framework that can compose holomorphic andnon-holomorphic functions in a multi-layer network using a theoretical resultcalled the Wirtinger derivative. We test our methods on a representationlearning task for real-valued signals, recurrent complex-valued networks andtheir real-valued counterparts. Our results show that recurrent complex-valuednetworks can perform as well as their real-valued counterparts while learningfilters that are representative of the domain of the data.
arxiv-13500-58 | Handcrafted Local Features are Convolutional Neural Networks | http://arxiv.org/pdf/1511.05045v2.pdf | author:Zhenzhong Lan, Shoou-I Yu, Ming Lin, Bhiksha Raj, Alexander G. Hauptmann category:cs.CV published:2015-11-16 summary:Image and video classification research has made great progress through thedevelopment of handcrafted local features and learning based features. Thesetwo architectures were proposed roughly at the same time and have flourished atoverlapping stages of history. However, they are typically viewed as distinctapproaches. In this paper, we emphasize their structural similarities and showhow such a unified view helps us in designing features that balance efficiencyand effectiveness. As an example, we study the problem of designing efficientvideo feature learning algorithms for action recognition. We approach this problem by first showing that local handcrafted features andConvolutional Neural Networks (CNNs) share the same convolution-pooling networkstructure. We then propose a two-stream Convolutional ISA (ConvISA) that adoptsthe convolution-pooling structure of the state-of-the-art handcrafted videofeature with greater modeling capacities and a cost-effective trainingalgorithm. Through custom designed network structures for pixels and opticalflow, our method also reflects distinctive characteristics of these two datasources. Our experimental results on standard action recognition benchmarks show thatby focusing on the structure of CNNs, rather than end-to-end training methods,we are able to design an efficient and powerful video feature learningalgorithm.
arxiv-13500-59 | Robust Classification by Pre-conditioned LASSO and Transductive Diffusion Component Analysis | http://arxiv.org/pdf/1511.06340v1.pdf | author:Yanwei Fu, De-An Huang, Leonid Sigal category:cs.LG cs.CV math.ST stat.ML stat.TH published:2015-11-19 summary:Modern machine learning-based recognition approaches require large-scaledatasets with large number of labelled training images. However, such datasetsare inherently difficult and costly to collect and annotate. Hence there is agreat and growing interest in automatic dataset collection methods that canleverage the web. % which are collected % in a cheap, efficient and yetunreliable way. Collecting datasets in this way, however, requires robust andefficient ways for detecting and excluding outliers that are common andprevalent. % Outliers are thus a % prominent treat of using these dataset. Sofar, there have been a limited effort in machine learning community to directlydetect outliers for robust classification. Inspired by the recent work onPre-conditioned LASSO, this paper formulates the outlier detection task usingPre-conditioned LASSO and employs \red{unsupervised} transductive diffusioncomponent analysis to both integrate the topological structure of the datamanifold, from labeled and unlabeled instances, and reduce the featuredimensionality. Synthetic experiments as well as results on two real-worldclassification tasks show that our framework can robustly detect the outliersand improve classification.
arxiv-13500-60 | Unsupervised Deep Embedding for Clustering Analysis | http://arxiv.org/pdf/1511.06335v1.pdf | author:Junyuan Xie, Ross Girshick, Ali Farhadi category:cs.LG cs.CV published:2015-11-19 summary:Clustering is central to many data-driven application domains and has beenstudied extensively in terms of distance functions and grouping algorithms.Relatively little work has focused on learning representations for clustering.In this paper, we propose Deep Embedded Clustering (DEC), a method thatsimultaneously learns feature representations and cluster assignments usingdeep neural networks. DEC learns a mapping from the data space to alower-dimensional feature space in which it iteratively optimizes a clusteringobjective. Our experimental evaluations on image and text corpora showsignificant improvement over state-of-the-art methods.
arxiv-13500-61 | Recurrent Reinforcement Learning: A Hybrid Approach | http://arxiv.org/pdf/1509.03044v2.pdf | author:Xiujun Li, Lihong Li, Jianfeng Gao, Xiaodong He, Jianshu Chen, Li Deng, Ji He category:cs.LG cs.AI cs.SY published:2015-09-10 summary:Successful applications of reinforcement learning in real-world problemsoften require dealing with partially observable states. It is in general verychallenging to construct and infer hidden states as they often depend on theagent's entire interaction history and may require substantial domainknowledge. In this work, we investigate a deep-learning approach to learningthe representation of states in partially observable tasks, with minimal priorknowledge of the domain. In particular, we propose a new family of hybridmodels that combines the strength of both supervised learning (SL) andreinforcement learning (RL), trained in a joint fashion: The SL component canbe a recurrent neural networks (RNN) or its long short-term memory (LSTM)version, which is equipped with the desired property of being able to capturelong-term dependency on history, thus providing an effective way of learningthe representation of hidden states. The RL component is a deep Q-network (DQN)that learns to optimize the control for maximizing long-term rewards. Extensiveexperiments in a direct mailing campaign problem demonstrate the effectivenessand advantages of the proposed approach, which performs the best among a set ofprevious state-of-the-art methods.
arxiv-13500-62 | face anti-spoofing based on color texture analysis | http://arxiv.org/pdf/1511.06316v1.pdf | author:Zinelabidine Boulkenafet, Jukka Komulainen, Abdenour Hadid category:cs.CV published:2015-11-19 summary:Research on face spoofing detection has mainly been focused on analyzing theluminance of the face images, hence discarding the chrominance informationwhich can be useful for discriminating fake faces from genuine ones. In thiswork, we propose a new face anti-spoofing method based on color textureanalysis. We analyze the joint color-texture information from the luminance andthe chrominance channels using a color local binary pattern descriptor. Morespecifically, the feature histograms are extracted from each image bandseparately. Extensive experiments on two benchmark datasets, namely CASIA faceanti-spoofing and Replay-Attack databases, showed excellent results compared tothe state-of-the-art. Most importantly, our inter-database evaluation depictsthat the proposed approach showed very promising generalization capabilities.
arxiv-13500-63 | An Online Sequence-to-Sequence Model Using Partial Conditioning | http://arxiv.org/pdf/1511.04868v3.pdf | author:Navdeep Jaitly, Quoc V. Le, Oriol Vinyals, Ilya Sutskever, Samy Bengio category:cs.LG cs.CL cs.NE published:2015-11-16 summary:Sequence-to-sequence models have achieved impressive results on varioustasks. However, they are unsuitable for tasks that require incrementalpredictions to be made as more data arrives. This is because they generate anoutput sequence conditioned on an entire input sequence. In this paper, wepresent a new model that can make incremental predictions as more inputarrives, without redoing the entire computation. Unlike sequence-to-sequencemodels, our method computes the next-step distribution conditioned on thepartial input sequence observed and the partial sequence generated. Itaccomplishes this goal using an encoder recurrent neural network (RNN) thatcomputes features at the same frame rate as the input, and a transducer RNNthat operates over blocks of input steps. The transducer RNN extends thesequence produced so far using a local sequence-to-sequence model. Duringtraining, our method uses alignment information to generate supervised targetsfor each block. Approximate alignment is easily available for tasks such asspeech recognition, action recognition in videos, etc. During inference(decoding), beam search is used to find the most likely output sequence for aninput sequence. This decoding is performed online - at the end of each block,the best candidates from the previous block are extended through the localsequence-to-sequence model. On TIMIT, our online method achieves 19.8% phoneerror rate (PER). For comparison with published sequence-to-sequence methods,we used a bidirectional encoder and achieved 18.7% PER compared to 17.6% fromthe best reported sequence-to-sequence model. Importantly, unlikesequence-to-sequence our model is minimally impacted by the length of theinput. On artificially created longer utterances, it achieves 20.9% with aunidirectional model, compared to 20% from the best bidirectionalsequence-to-sequence models.
arxiv-13500-64 | Why M Heads are Better than One: Training a Diverse Ensemble of Deep Networks | http://arxiv.org/pdf/1511.06314v1.pdf | author:Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David Crandall, Dhruv Batra category:cs.CV cs.LG cs.NE published:2015-11-19 summary:Convolutional Neural Networks have achieved state-of-the-art performance on awide range of tasks. Most benchmarks are led by ensembles of these powerfullearners, but ensembling is typically treated as a post-hoc procedureimplemented by averaging independently trained models with model variationinduced by bagging or random initialization. In this paper, we rigorously treatensembling as a first-class problem to explicitly address the question: whatare the best strategies to create an ensemble? We first compare a large numberof ensembling strategies, and then propose and evaluate novel strategies, suchas parameter sharing (through a new family of models we call TreeNets) as wellas training under ensemble-aware and diversity-encouraging losses. Wedemonstrate that TreeNets can improve ensemble performance and that diverseensembles can be trained end-to-end under a unified loss, achievingsignificantly higher "oracle" accuracies than classical ensembles.
arxiv-13500-65 | Good, Better, Best: Choosing Word Embedding Context | http://arxiv.org/pdf/1511.06312v1.pdf | author:James Cross, Bing Xiang, Bowen Zhou category:cs.CL published:2015-11-19 summary:We propose two methods of learning vector representations of words andphrases that each combine sentence context with structural features extractedfrom dependency trees. Using several variations of neural network classifier,we show that these combined methods lead to improved performance when used asinput features for supervised term-matching.
arxiv-13500-66 | Faster method for Deep Belief Network based Object classification using DWT | http://arxiv.org/pdf/1511.06276v1.pdf | author:Saurabh Sihag, Pranab Kumar Dutta category:cs.CV cs.LG published:2015-11-19 summary:A Deep Belief Network (DBN) requires large, multiple hidden layers with highnumber of hidden units to learn good features from the raw pixels of largeimages. This implies more training time as well as computational complexity. Byintegrating DBN with Discrete Wavelet Transform (DWT), both training time andcomputational complexity can be reduced. The low resolution images obtainedafter application of DWT are used to train multiple DBNs. The results obtainedfrom these DBNs are combined using a weighted voting algorithm. The performanceof this method is found to be competent and faster in comparison with that oftraditional DBNs.
arxiv-13500-67 | Critical Parameters in Particle Swarm Optimisation | http://arxiv.org/pdf/1511.06248v1.pdf | author:J. Michael Herrmann, Adam Erskine, Thomas Joyce category:cs.NE published:2015-11-19 summary:Particle swarm optimisation is a metaheuristic algorithm which findsreasonable solutions in a wide range of applied problems if suitable parametersare used. We study the properties of the algorithm in the framework of randomdynamical systems which, due to the quasi-linear swarm dynamics, yieldsanalytical results for the stability properties of the particles. Suchconsiderations predict a relationship between the parameters of the algorithmthat marks the edge between convergent and divergent behaviours. Comparisonwith simulations indicates that the algorithm performs best near this margin ofinstability.
arxiv-13500-68 | Gaussian Mixture Embeddings for Multiple Word Prototypes | http://arxiv.org/pdf/1511.06246v1.pdf | author:Xinchi Chen, Xipeng Qiu, Jingxiang Jiang, Xuanjing Huang category:cs.CL published:2015-11-19 summary:Recently, word representation has been increasingly focused on for itsexcellent properties in representing the word semantics. Previous works mainlysuffer from the problem of polysemy phenomenon. To address this problem, mostof previous models represent words as multiple distributed vectors. However, itcannot reflect the rich relations between words by representing words as pointsin the embedded space. In this paper, we propose the Gaussian mixture skip-gram(GMSG) model to learn the Gaussian mixture embeddings for words based onskip-gram framework. Each word can be regarded as a gaussian mixturedistribution in the embedded space, and each gaussian component represents aword sense. Since the number of senses varies from word to word, we furtherpropose the Dynamic GMSG (D-GMSG) model by adaptively increasing the sensenumber of words during training. Experiments on four benchmarks show theeffectiveness of our proposed model.
arxiv-13500-69 | Towards Open Set Deep Networks | http://arxiv.org/pdf/1511.06233v1.pdf | author:Abhijit Bendale, Terrance Boult category:cs.CV cs.LG published:2015-11-19 summary:Deep networks have produced significant gains for various visual recognitionproblems, leading to high impact academic and commercial applications. Recentwork in deep networks highlighted that it is easy to generate images thathumans would never classify as a particular object class, yet networks classifysuch images high confidence as that given class - deep network are easilyfooled with images humans do not consider meaningful. The closed set nature ofdeep networks forces them to choose from one of the known classes leading tosuch artifacts. Recognition in the real world is open set, i.e. the recognitionsystem should reject unknown/unseen classes at test time. We present amethodology to adapt deep networks for open set recognition, by introducing anew model layer, OpenMax, which estimates the probability of an input beingfrom an unknown class. A key element of estimating the unknown probability isadapting Meta-Recognition concepts to the activation patterns in thepenultimate layer of the network. OpenMax allows rejection of "fooling" andunrelated open set images presented to the system; OpenMax greatly reduces thenumber of obvious errors made by a deep network. We prove that the OpenMaxconcept provides bounded open space risk, thereby formally providing an openset recognition solution. We evaluate the resulting open set deep networksusing pre-trained networks from the Caffe Model-zoo on ImageNet 2012 validationdata, and thousands of fooling and open set images. The proposed OpenMax modelsignificantly outperforms open set recognition accuracy of basic deep networksas well as deep networks with thresholding of SoftMax probabilities.
arxiv-13500-70 | Automatically selecting inference algorithms for discrete energy minimisation | http://arxiv.org/pdf/1511.06214v1.pdf | author:Paul Henderson, Vittorio Ferrari category:cs.CV published:2015-11-19 summary:Minimisation of discrete energies defined over factors is an importantproblem in computer vision, and a vast number of MAP inference algorithms havebeen proposed. Different inference algorithms perform better on factor graphmodels (GMs) from different underlying problem classes, and in general it isdifficult to know which algorithm will yield the lowest energy for a given GM.To mitigate this difficulty, survey papers advise the practitioner on whatalgorithms perform well on what classes of models. We take the next stepforward, and present a technique to automatically select the best inferencealgorithm for an input GM. We validate our method experimentally on an extendedversion of the OpenGM2 benchmark, containing a diverse set of vision problems.On average, our method selects an inference algorithm yielding labellings with96% of variables the same as the best available algorithm.
arxiv-13500-71 | Teaching Machines to Read and Comprehend | http://arxiv.org/pdf/1506.03340v3.pdf | author:Karl Moritz Hermann, Tomáš Kočiský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom category:cs.CL cs.AI cs.NE published:2015-06-10 summary:Teaching machines to read natural language documents remains an elusivechallenge. Machine reading systems can be tested on their ability to answerquestions posed on the contents of documents that they have seen, but until nowlarge scale training and test datasets have been missing for this type ofevaluation. In this work we define a new methodology that resolves thisbottleneck and provides large scale supervised reading comprehension data. Thisallows us to develop a class of attention based deep neural networks that learnto read real documents and answer complex questions with minimal priorknowledge of language structure.
arxiv-13500-72 | Diffusion Representations | http://arxiv.org/pdf/1511.06208v1.pdf | author:Moshe Salhov, Amit Bermanis, Guy Wolf, Amir Averbuch category:stat.ML cs.LG math.SP published:2015-11-19 summary:Diffusion Maps framework is a kernel based method for manifold learning anddata analysis that defines diffusion similarities by imposing a Markovianprocess on the given dataset. Analysis by this process uncovers the intrinsicgeometric structures in the data. Recently, it was suggested to replace thestandard kernel by a measure-based kernel that incorporates information aboutthe density of the data. Thus, the manifold assumption is replaced by a moregeneral measure-based assumption. The measure-based diffusion kernel incorporates two separate independentrepresentations. The first determines a measure that correlates with a densitythat represents normal behaviors and patterns in the data. The second consistsof the analyzed multidimensional data points. In this paper, we present a representation framework for data analysis ofdatasets that is based on a closed-form decomposition of the measure-basedkernel. The proposed representation preserves pairwise diffusion distances thatdoes not depend on the data size while being invariant to scale. For astationary data, no out-of-sample extension is needed for embedding newlyarrived data points in the representation space. Several aspects of thepresented methodology are demonstrated on analytically generated data.
arxiv-13500-73 | Adjustable Bounded Rectifiers: Towards Deep Binary Representations | http://arxiv.org/pdf/1511.06201v1.pdf | author:Zhirong Wu, Dahua Lin, Xiaoou Tang category:cs.LG stat.ML published:2015-11-19 summary:Binary representation is desirable for its memory efficiency, computationspeed and robustness. In this paper, we propose adjustable bounded rectifiersto learn binary representations for deep neural networks. While hardconstraining representations across layers to be binary makes trainingunreasonably difficult, we softly encourage activations to diverge from realvalues to binary by approximating step functions. Our final representation iscompletely binary. We test our approach on MNIST, CIFAR10, and ILSVRC2012dataset, and systematically study the training dynamics of the binarizationprocess. Our approach can binarize the last layer representation without lossof performance and binarize all the layers with reasonably small degradations.The memory space that it saves may allow more sophisticated models to bedeployed, thus compensating the loss. To the best of our knowledge, this is thefirst work to report results on current deep network architectures usingcomplete binary middle representations. Given the learned representations, wefind that the firing or inhibition of a binary neuron is usually associatedwith a meaningful interpretation across different classes. This suggests thatthe semantic structure of a neural network may be manifested through a guidedbinarization process.
arxiv-13500-74 | Spherical Cap Packing Asymptotics and Rank-Extreme Detection | http://arxiv.org/pdf/1511.06198v1.pdf | author:Kai Zhang category:math.ST cs.IT math.IT stat.ME stat.ML stat.TH published:2015-11-19 summary:We study the spherical cap packing problem with a probabilistic approach.Such probabilistic considerations result in an asymptotic sharp universaluniform bound on the maximal inner product between any set of unit vectors anda stochastically independent uniformly distributed unit vector. When the set ofunit vectors are themselves independently uniformly distributed, we furtherdevelop the extreme value distribution limit of the maximal inner product,which characterizes its uncertainty around the bound. As applications of the above asymptotic results, we derive (1) an asymptoticsharp universal uniform bound on the maximal spurious correlation, as well asits uniform convergence in distribution when the explanatory variables areindependently Gaussian distributed; and (2) an asymptotic sharp universal boundon the maximum norm of a low-rank elliptically distributed vector, as well asrelated limiting distributions. With these results, we develop a fast detectionmethod for a low-rank structure in high-dimensional Gaussian data without usingthe spectrum information.
arxiv-13500-75 | Bandits and Experts in Metric Spaces | http://arxiv.org/pdf/1312.1277v2.pdf | author:Robert Kleinberg, Aleksandrs Slivkins, Eli Upfal category:cs.DS cs.LG published:2013-12-04 summary:In a multi-armed bandit problem, an online algorithm chooses from a set ofstrategies in a sequence of trials so as to maximize the total payoff of thechosen strategies. While the performance of bandit algorithms with a smallfinite strategy set is quite well understood, bandit problems with largestrategy sets are still a topic of very active investigation, motivated bypractical applications such as online auctions and web advertisement. The goalof such research is to identify broad and natural classes of strategy sets andpayoff functions which enable the design of efficient solutions. In this work we study a very general setting for the multi-armed banditproblem in which the strategies form a metric space, and the payoff functionsatisfies a Lipschitz condition with respect to the metric. We refer to thisproblem as the "Lipschitz MAB problem". We present a solution for themulti-armed bandit problem in this setting. That is, for every metric space wedefine an isometry invariant which bounds from below the performance ofLipschitz MAB algorithms for this metric space, and we present an algorithmwhich comes arbitrarily close to meeting this bound. Furthermore, our techniquegives even better results for benign payoff functions. We also address thefull-feedback ("best expert") version of the problem, where after every roundthe payoffs from all arms are revealed.
arxiv-13500-76 | Large-scale Online Feature Selection for Ultra-high Dimensional Sparse Data | http://arxiv.org/pdf/1409.7794v3.pdf | author:Yue Wu, Steven C. H. Hoi, Tao Mei, Nenghai Yu category:cs.LG cs.CV published:2014-09-27 summary:Feature selection with large-scale high-dimensional data is important yetvery challenging in machine learning and data mining. Online feature selectionis a promising new paradigm that is more efficient and scalable than batchfeature section methods, but the existing online approaches usually fall shortin their inferior efficacy as compared with batch approaches. In this paper, wepresent a novel second-order online feature selection scheme that is simple yeteffective, very fast and extremely scalable to deal with large-scale ultra-highdimensional sparse data streams. The basic idea is to improve the existingfirst-order online feature selection methods by exploiting second-orderinformation for choosing the subset of important features with high confidenceweights. However, unlike many second-order learning methods that often sufferfrom extra high computational cost, we devise a novel smart algorithm forsecond-order online feature selection using a MaxHeap-based approach, which isnot only more effective than the existing first-order approaches, but alsosignificantly more efficient and scalable for large-scale feature selectionwith ultra-high dimensional sparse data, as validated from our extensiveexperiments. Impressively, on a billion-scale synthetic dataset (1-billiondimensions, 1-billion nonzero features, and 1-million samples), our newalgorithm took only 8 minutes on a single PC, which is orders of magnitudesfaster than traditional batch approaches. \url{http://arxiv.org/abs/1409.7794}
arxiv-13500-77 | From Pose to Activity: Surveying Datasets and Introducing CONVERSE | http://arxiv.org/pdf/1511.05788v2.pdf | author:Michael Edwards, Jingjing Deng, Xianghua Xie category:cs.CV published:2015-11-18 summary:We present a review on the current state of publicly available datasetswithin the human action recognition community; highlighting the revival of posebased methods and recent progress of understanding person-person interactionmodeling. We categorize datasets regarding several key properties for usage asa benchmark dataset; including the number of class labels, ground truthsprovided, and application domain they occupy. We also consider the level ofabstraction of each dataset; grouping those that present actions, interactionsand higher level semantic activities. The survey identifies key appearance andpose based datasets, noting a tendency for simplistic, emphasized, or scriptedaction classes that are often readily definable by a stable collection ofsub-action gestures. There is a clear lack of datasets that provide closelyrelated actions, those that are not implicitly identified via a series of posesand gestures, but rather a dynamic set of interactions. We therefore propose anovel dataset that represents complex conversational interactions between twoindividuals via 3D pose. 8 pairwise interactions describing 7 separateconversation based scenarios were collected using two Kinect depth sensors. Theintention is to provide events that are constructed from numerous primitiveactions, interactions and motions, over a period of time; providing a set ofsubtle action classes that are more representative of the real world, and achallenge to currently developed recognition methodologies. We believe this isamong one of the first datasets devoted to conversational interactionclassification using 3D pose features and the attributed papers show this taskis indeed possible. The full dataset is made publicly available to the researchcommunity at www.csvision.swansea.ac.uk/converse.
arxiv-13500-78 | Coreset-Based Adaptive Tracking | http://arxiv.org/pdf/1511.06147v1.pdf | author:Abhimanyu Dubey, Nikhil Naik, Dan Raviv, Rahul Sukthankar, Ramesh Raskar category:cs.CV cs.LG published:2015-11-19 summary:We propose a method for learning from streaming visual data using a compact,constant size representation of all the data that was seen until a givenmoment. Specifically, we construct a 'coreset' representation of streaming datausing a parallelized algorithm, which is an approximation of a set withrelation to the squared distances between this set and all other points in itsambient space. We learn an adaptive object appearance model from the coresettree in constant time and logarithmic space and use it for object tracking bydetection. Our method obtains excellent results for object tracking on threestandard datasets over more than 100 videos. The ability to summarize dataefficiently makes our method ideally suited for tracking in long videos inpresence of space and time constraints. We demonstrate this ability byoutperforming a variety of algorithms on the TLD dataset with 2685 frames onaverage. This coreset based learning approach can be applied for both real-timelearning of small, varied data and fast learning of big data.
arxiv-13500-79 | Geodesic convolutional neural networks on Riemannian manifolds | http://arxiv.org/pdf/1501.06297v2.pdf | author:Jonathan Masci, Davide Boscaini, Michael M. Bronstein, Pierre Vandergheynst category:cs.CV published:2015-01-26 summary:Feature descriptors play a crucial role in a wide range of geometry analysisand processing applications, including shape correspondence, retrieval, andsegmentation. In this paper, we introduce Geodesic Convolutional NeuralNetworks (GCNN), a generalization of the convolutional networks (CNN) paradigmto non-Euclidean manifolds. Our construction is based on a local geodesicsystem of polar coordinates to extract "patches", which are then passed througha cascade of filters and linear and non-linear operators. The coefficients ofthe filters and linear combination weights are optimization variables that arelearned to minimize a task-specific cost function. We use GCNN to learninvariant shape features, allowing to achieve state-of-the-art performance inproblems such as shape description, retrieval, and correspondence.
arxiv-13500-80 | The Kernel Two-Sample Test for Brain Networks | http://arxiv.org/pdf/1511.06120v1.pdf | author:Emanuele Olivetti, Sandro Vega-Pons, Paolo Avesani category:stat.ML published:2015-11-19 summary:In clinical and neuroscientific studies, systematic differences between twopopulations of brain networks are investigated in order to characterize mentaldiseases or processes. Those networks are usually represented as graphs builtfrom neuroimaging data and studied by means of graph analysis methods. Thetypical machine learning approach to study these brain graphs creates aclassifier and tests its ability to discriminate the two populations. Incontrast to this approach, in this work we propose to directly test whether twopopulations of graphs are different or not, by using the kernel two-sample test(KTST), without creating the intermediate classifier. We claim that, ingeneral, the two approaches provides similar results and that the KTST requiresmuch less computation. Additionally, in the regime of low sample size, we claimthat the KTST has lower frequency of Type II error than the classificationapproach. Besides providing algorithmic considerations to support these claims,we show strong evidence through experiments and one simulation.
arxiv-13500-81 | Mediated Experts for Deep Convolutional Networks | http://arxiv.org/pdf/1511.06072v1.pdf | author:Sebastian Agethen, Winston H. Hsu category:cs.LG cs.NE published:2015-11-19 summary:We present a new supervised architecture termed Mediated Mixture-of-Experts(MMoE) that allows us to improve classification accuracy of Deep ConvolutionalNetworks (DCN). Our architecture achieves this with the help of expertnetworks: A network is trained on a disjoint subset of a given dataset and thenrun in parallel to other experts during deployment. A mediator is employed ifexperts contradict each other. This allows our framework to naturally supportincremental learning, as adding new classes requires (re-)training of the newexpert only. We also propose two measures to control computational complexity:An early-stopping mechanism halts experts that have low confidence in theirprediction. The system allows to trade-off accuracy and complexity withoutfurther retraining. We also suggest to share low-level convolutional layersbetween experts in an effort to avoid computation of a near-duplicate featureset. We evaluate our system on a popular dataset and report improved accuracycompared to a single model of same configuration.
arxiv-13500-82 | Structured Depth Prediction in Challenging Monocular Video Sequences | http://arxiv.org/pdf/1511.06070v1.pdf | author:Miaomiao Liu, Mathieu Salzmann, Xuming He category:cs.CV published:2015-11-19 summary:In this paper, we tackle the problem of estimating the depth of a scene froma monocular video sequence. In particular, we handle challenging scenarios,such as non-translational camera motion and dynamic scenes, where traditionalstructure from motion and motion stereo methods do not apply. To this end, wefirst study the problem of depth estimation from a single image. In thiscontext, we exploit the availability of a pool of images for which the depth isknown, and formulate monocular depth estimation as a discrete-continuousoptimization problem, where the continuous variables encode the depth of thesuperpixels in the input image, and the discrete ones represent relationshipsbetween neighboring superpixels. The solution to this discrete-continuousoptimization problem is obtained by performing inference in a graphical modelusing particle belief propagation. To handle video sequences, we then extendour single image model to a two-frame one that naturally encodes short-rangetemporal consistency and inherently handles dynamic objects. Based on theprediction of this model, we then introduce a fully-connected pairwise CRF thataccounts for longer range spatio-temporal interactions throughout a video. Wedemonstrate the effectiveness of our model in both the indoor and outdoorscenarios.
arxiv-13500-83 | Wishart Mechanism for Differentially Private Principal Components Analysis | http://arxiv.org/pdf/1511.05680v2.pdf | author:Wuxuan Jiang, Cong Xie, Zhihua Zhang category:cs.CR cs.DS stat.ML published:2015-11-18 summary:We propose a new input perturbation mechanism for publishing a covariancematrix to achieve $(\epsilon,0)$-differential privacy. Our mechanism uses aWishart distribution to generate matrix noise. In particular, We apply thismechanism to principal component analysis. Our mechanism is able to keep thepositive semi-definiteness of the published covariance matrix. Thus, ourapproach gives rise to a general publishing framework for input perturbation ofa symmetric positive semidefinite matrix. Moreover, compared with the classicLaplace mechanism, our method has better utility guarantee. To the best of ourknowledge, Wishart mechanism is the best input perturbation approach for$(\epsilon,0)$-differentially private PCA. We also compare our work withprevious exponential mechanism algorithms in the literature and provide nearoptimal bound while having more flexibility and less computationalintractability.
arxiv-13500-84 | Transfer Learning for Speech and Language Processing | http://arxiv.org/pdf/1511.06066v1.pdf | author:Dong Wang, Thomas Fang Zheng category:cs.CL cs.LG published:2015-11-19 summary:Transfer learning is a vital technique that generalizes models trained forone setting or task to other settings or tasks. For example in speechrecognition, an acoustic model trained for one language can be used torecognize speech in another language, with little or no re-training data.Transfer learning is closely related to multi-task learning (cross-lingual vs.multilingual), and is traditionally studied in the name of `model adaptation'.Recent advance in deep learning shows that transfer learning becomes mucheasier and more effective with high-level abstract features learned by deepmodels, and the `transfer' can be conducted not only between data distributionsand data types, but also between model structures (e.g., shallow nets and deepnets) or even model types (e.g., Bayesian models and neural models). Thisreview paper summarizes some recent prominent research towards this direction,particularly for speech and language processing. We also report some resultsfrom our group and highlight the potential of this very interesting researchfield.
arxiv-13500-85 | A Novel Approach for Phase Identification in Smart Grids Using Graph Theory and Principal Component Analysis | http://arxiv.org/pdf/1511.06063v1.pdf | author:Satya Jayadev P, Nirav P Bhatt, Ramkrishna Pasumarthy category:cs.LG stat.AP stat.ML published:2015-11-19 summary:In general, low load consumers like households are supplied single-phasepower by connecting their service mains to one of the phases of a distributiontransformer. Here, the distribution companies face the problem of identifyingwhich consumer is connected to which phase and many solutions have evolved inthe past years to address this problem. The exact phase connectivityinformation is important for the efficient operation and control ofdistribution system. We propose a new data driven approach to the problem basedon Graph Theory and Principal Component Analysis (PCA), using energymeasurements in short time intervals, generated from smart meters. We proposean algorithm for the noiseless case and then extend it to noisy case. Thealgorithm will be demonstrated using simulated data for phase connectivities indistribution networks.
arxiv-13500-86 | Dual Query: Practical Private Query Release for High Dimensional Data | http://arxiv.org/pdf/1402.1526v2.pdf | author:Marco Gaboardi, Emilio Jesús Gallego Arias, Justin Hsu, Aaron Roth, Zhiwei Steven Wu category:cs.DS cs.CR cs.DB cs.LG published:2014-02-06 summary:We present a practical, differentially private algorithm for answering alarge number of queries on high dimensional datasets. Like all algorithms forthis task, ours necessarily has worst-case complexity exponential in thedimension of the data. However, our algorithm packages the computationally hardstep into a concisely defined integer program, which can be solvednon-privately using standard solvers. We prove accuracy and privacy theoremsfor our algorithm, and then demonstrate experimentally that our algorithmperforms well in practice. For example, our algorithm can efficiently andaccurately answer millions of queries on the Netflix dataset, which has over17,000 attributes; this is an improvement on the state of the art by multipleorders of magnitude.
arxiv-13500-87 | What Objective Does Self-paced Learning Indeed Optimize? | http://arxiv.org/pdf/1511.06049v1.pdf | author:Deyu Meng, Qian Zhao category:cs.LG cs.CV published:2015-11-19 summary:Self-paced learning (SPL) has been attracting increasing attention in machinelearning and computer vision. Albeit empirically substantiated to be effective,the investigation on its theoretical insight is still a blank. It is evenunknown that what objective a general SPL regime converges to. To this issue,this study attempts to initially provide some new insights under this"heuristic" learning scheme. Specifically, we prove that the solving strategyon SPL exactly accords with a majorization minimization algorithm, a well knowntechnique in optimization and machine learning, implemented on a latentobjective. A more interesting finding is that, the loss function contained inthis latent objective has a similar configuration with non-convex regularizedpenalty, an attractive topic in statistics and machine learning. In particular,we show that the previous hard and linear self-paced regularizers areequivalent to the capped norm and minimax concave plus penalties, respectively,both being widely investigated in statistics. Such connections between SPL andprevious known researches enhance new insightful comprehension on SPL, likeconvergence and parameter setting rationality. The correctness of the proposedtheory is substantiated by experimental results on synthetic and UCI data sets.
arxiv-13500-88 | Stochastic gradient method with accelerated stochastic dynamics | http://arxiv.org/pdf/1511.06036v1.pdf | author:Masayuki Ohzeki category:stat.ML cs.CV published:2015-11-19 summary:In this paper, we propose a novel technique to implement stochastic gradientmethods, which are beneficial for learning from large datasets, throughaccelerated stochastic dynamics. A stochastic gradient method is based onmini-batch learning for reducing the computational cost when the amount of datais large. The stochasticity of the gradient can be mitigated by the injectionof Gaussian noise, which yields the stochastic Langevin gradient method; thismethod can be used for Bayesian posterior sampling. However, the performance ofthe stochastic Langevin gradient method depends on the mixing rate of thestochastic dynamics. In this study, we propose violating the detailed balancecondition to enhance the mixing rate. Recent studies have revealed thatviolating the detailed balance condition accelerates the convergence to astationary state and reduces the correlation time between the samplings. Weimplement this violation of the detailed balance condition in the stochasticgradient Langevin method and test our method for a simple model to demonstrateits performance.
arxiv-13500-89 | Deep filter banks for texture recognition, description, and segmentation | http://arxiv.org/pdf/1507.02620v2.pdf | author:Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Andrea Vedaldi category:cs.CV published:2015-07-09 summary:Visual textures have played a key role in image understanding because theyconvey important semantics of images, and because texture representations thatpool local image descriptors in an orderless manner have had a tremendousimpact in diverse applications. In this paper we make several contributions totexture understanding. First, instead of focusing on texture instance andmaterial category recognition, we propose a human-interpretable vocabulary oftexture attributes to describe common texture patterns, complemented by a newdescribable texture dataset for benchmarking. Second, we look at the problem ofrecognizing materials and texture attributes in realistic imaging conditions,including when textures appear in clutter, developing corresponding benchmarkson top of the recently proposed OpenSurfaces dataset. Third, we revisit classictexture representations, including bag-of-visual-words and the Fisher vectors,in the context of deep learning and show that these have excellent efficiencyand generalization properties if the convolutional layers of a deep model areused as filter banks. We obtain in this manner state-of-the-art performance innumerous datasets well beyond textures, an efficient method to apply deepfeatures to image regions, as well as benefit in transferring features from onedomain to another.
arxiv-13500-90 | Active Object Localization with Deep Reinforcement Learning | http://arxiv.org/pdf/1511.06015v1.pdf | author:Juan C. Caicedo, Svetlana Lazebnik category:cs.CV published:2015-11-18 summary:We present an active detection model for localizing objects in scenes. Themodel is class-specific and allows an agent to focus attention on candidateregions for identifying the correct location of a target object. This agentlearns to deform a bounding box using simple transformation actions, with thegoal of determining the most specific location of target objects followingtop-down reasoning. The proposed localization agent is trained using deepreinforcement learning, and evaluated on the Pascal VOC 2007 dataset. We showthat agents guided by the proposed model are able to localize a single instanceof an object after analyzing only between 11 and 25 regions in an image, andobtain the best detection results among systems that do not use objectproposals for object localization.
arxiv-13500-91 | Studying the control of non invasive prosthetic hands over large time spans | http://arxiv.org/pdf/1511.06004v1.pdf | author:Mara Graziani category:cs.LG cs.HC published:2015-11-18 summary:The electromyography (EMG) signal is the electrical manifestation of aneuromuscular activation that provides access to physiological processes whichcause the muscle to generate force and produce movement. Non invasiveprostheses use such signals detected by the electrodes placed on the user'sstump, as input to generate hand posture movements according to the intentionsof the prosthesis wearer. The aim of this pilot study is to explore therepeatability issue, i.e. the ability to classify 17 different hand postures,represented by EMG signal, across a time span of days by a control algorithm.Data collection experiments lasted four days and signals were collected fromthe forearm of a single subject. We find that Support Vector Machine (SVM)classification results are high enough to guarantee a correct classification ofmore than 10 postures in each moment of the considered time span.
arxiv-13500-92 | Deep Unsupervised Learning using Nonequilibrium Thermodynamics | http://arxiv.org/pdf/1503.03585v8.pdf | author:Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli category:cs.LG q-bio.NC stat.ML published:2015-03-12 summary:A central problem in machine learning involves modeling complex data-setsusing highly flexible families of probability distributions in which learning,sampling, inference, and evaluation are still analytically or computationallytractable. Here, we develop an approach that simultaneously achieves bothflexibility and tractability. The essential idea, inspired by non-equilibriumstatistical physics, is to systematically and slowly destroy structure in adata distribution through an iterative forward diffusion process. We then learna reverse diffusion process that restores structure in data, yielding a highlyflexible and tractable generative model of the data. This approach allows us torapidly learn, sample from, and evaluate probabilities in deep generativemodels with thousands of layers or time steps, as well as to computeconditional and posterior probabilities under the learned model. Weadditionally release an open source reference implementation of the algorithm.
arxiv-13500-93 | Unitary-Group Invariant Kernels and Features from Transformed Unlabeled Data | http://arxiv.org/pdf/1511.05943v1.pdf | author:Dipan K. Pal, Marios Savvides category:cs.LG published:2015-11-18 summary:The study of representations invariant to common transformations of the datais important to learning. Most techniques have focused on local approximateinvariance implemented within expensive optimization frameworks lackingexplicit theoretical guarantees. In this paper, we study kernels that areinvariant to the unitary group while having theoretical guarantees inaddressing practical issues such as (1) unavailability of transformed versionsof labelled data and (2) not observing all transformations. We present atheoretically motivated alternate approach to the invariant kernel SVM. Unlikeprevious approaches to the invariant SVM, the proposed formulation solves bothissues mentioned. We also present a kernel extension of a recent technique toextract linear unitary-group invariant features addressing both issues andextend some guarantees regarding invariance and stability. We presentexperiments on the UCI ML datasets to illustrate and validate our methods.
arxiv-13500-94 | On the Global Linear Convergence of Frank-Wolfe Optimization Variants | http://arxiv.org/pdf/1511.05932v1.pdf | author:Simon Lacoste-Julien, Martin Jaggi category:math.OC cs.LG stat.ML G.1.6; I.2.6 published:2015-11-18 summary:The Frank-Wolfe (FW) optimization algorithm has lately re-gained popularitythanks in particular to its ability to nicely handle the structured constraintsappearing in machine learning applications. However, its convergence rate isknown to be slow (sublinear) when the solution lies at the boundary. A simpleless-known fix is to add the possibility to take 'away steps' duringoptimization, an operation that importantly does not require a feasibilityoracle. In this paper, we highlight and clarify several variants of theFrank-Wolfe optimization algorithm that have been successfully applied inpractice: away-steps FW, pairwise FW, fully-corrective FW and Wolfe's minimumnorm point algorithm, and prove for the first time that they all enjoy globallinear convergence, under a weaker condition than strong convexity of theobjective. The constant in the convergence rate has an elegant interpretationas the product of the (classical) condition number of the function with a novelgeometric quantity that plays the role of a 'condition number' of theconstraint set. We provide pointers to where these algorithms have made adifference in practice, in particular with the flow polytope, the marginalpolytope and the base polytope for submodular optimization.
arxiv-13500-95 | Combining Neural Networks and Log-linear Models to Improve Relation Extraction | http://arxiv.org/pdf/1511.05926v1.pdf | author:Thien Huu Nguyen, Ralph Grishman category:cs.CL cs.LG published:2015-11-18 summary:The last decade has witnessed the success of the traditional feature-basedmethod on exploiting the discrete structures such as words or lexical patternsto extract relations from text. Recently, convolutional and recurrent neuralnetworks has provided very effective mechanisms to capture the hiddenstructures within sentences via continuous representations, therebysignificantly advancing the performance of relation extraction. The advantageof convolutional neural networks is their capacity to generalize theconsecutive k-grams in the sentences while recurrent neural networks areeffective to encode long ranges of sentence context. This paper proposes tocombine the traditional feature-based method, the convolutional and recurrentneural networks to simultaneously benefit from their advantages. Our systematicevaluation of different network architectures and combination methodsdemonstrates the effectiveness of this approach and results in thestate-of-the-art performance on the ACE 2005 and SemEval dataset.
arxiv-13500-96 | Collecting and Annotating the Large Continuous Action Dataset | http://arxiv.org/pdf/1511.05914v1.pdf | author:Daniel Paul Barrett, Ran Xu, Haonan Yu, Jeffrey Mark Siskind category:cs.CV published:2015-11-18 summary:We make available to the community a new dataset to supportaction-recognition research. This dataset is different from prior datasets inseveral key ways. It is significantly larger. It contains streaming video withlong segments containing multiple action occurrences that often overlap inspace and/or time. All actions were filmed in the same collection ofbackgrounds so that background gives little clue as to action class. We hadfive humans replicate the annotation of temporal extent of action occurrenceslabeled with their class and measured a surprisingly low level of intercoderagreement. A baseline experiment shows that recent state-of-the-art methodsperform poorly on this dataset. This suggests that this will be a challengingdataset to foster advances in action-recognition research. This manuscriptserves to describe the novel content and characteristics of the LCA dataset,present the design decisions made when filming the dataset, and document thenovel methods employed to annotate the dataset.
arxiv-13500-97 | Dense Human Body Correspondences Using Convolutional Networks | http://arxiv.org/pdf/1511.05904v1.pdf | author:Lingyu Wei, Qixing Huang, Duygu Ceylan, Etienne Vouga, Hao Li category:cs.CV cs.GR published:2015-11-18 summary:We propose a deep learning approach for finding dense correspondences between3D scans of people. Our method requires only partial geometric information inthe form of two depth maps or partial reconstructed surfaces, works for humansin arbitrary poses and wearing any clothing, does not require the two people tobe scanned from similar viewpoints, and runs in real time. We use a deepconvolutional neural network to train a feature descriptor on depth map pixels,but crucially, rather than training the network to solve the shapecorrespondence problem directly, we train it to solve a body regionclassification problem, modified to increase the smoothness of the learneddescriptors near region boundaries. This approach ensures that nearby points onthe human body are nearby in feature space, and vice versa, rendering thefeature descriptor suitable for computing dense correspondences between thescans. We validate our method on real and synthetic data for both clothed andunclothed humans, and show that our correspondences are more robust than ispossible with state-of-the-art unsupervised methods, and more accurate thanthose found using methods that require full watertight 3D geometry.
arxiv-13500-98 | Multilingual Image Description with Neural Sequence Models | http://arxiv.org/pdf/1510.04709v2.pdf | author:Desmond Elliott, Stella Frank, Eva Hasler category:cs.CL cs.CV cs.LG cs.NE published:2015-10-15 summary:In this paper we present an approach to multi-language image descriptionbringing together insights from neural machine translation and neural imagedescription. To create a description of an image for a given target language,our sequence generation models condition on feature vectors from the image, thedescription from the source language, and/or a multimodal vector computed overthe image and a description in the source language. In image descriptionexperiments on the IAPR-TC12 dataset of images aligned with English and Germansentences, we find significant and substantial improvements in BLEU4 and Meteorscores for models trained over multiple languages, compared to a monolingualbaseline.
arxiv-13500-99 | How Far Can You Get By Combining Change Detection Algorithms? | http://arxiv.org/pdf/1505.02921v2.pdf | author:Simone Bianco, Gianluigi Ciocca, Raimondo Schettini category:cs.CV I.4.8; G.1.6 published:2015-05-12 summary:In this paper we investigate how state-of-the-art change detection algorithmscan be combined and used to create a more robust change algorithm leveragingtheir individual peculiarities. We exploited Genetic Programming (GP) toautomatically select the best algorithms, combine them in different ways, andperform the most suitable post-processing operations on the outputs of thealgorithms. In particular, algorithms' combination and post-processingoperations are achieved with unary, binary and $n$-ary functions embedded intothe GP framework. Using different experimental settings for combining existingalgorithms we obtained different GP solutions that we termed IUTIS (In UnityThere Is Strength). These solutions are then compared against state-of-the-artchange detection algorithms on the video sequences and ground truth annotationsof the ChandeDetection.net (CDNET 2014) challenge. Results demonstrate thatusing GP, our solutions are able to outperform all the considered singlestate-of-the-art change detection algorithms, as well as other combinationstrategies.
arxiv-13500-100 | Eigenspectra optoacoustic tomography achieves quantitative blood oxygenation imaging deep in tissues | http://arxiv.org/pdf/1511.05846v1.pdf | author:Stratis Tzoumas, Antonio Nunes, Ivan Olefir, Stefan Stangl, Panagiotis Symvoulidis, Sarah Glasl, Christine Bayer, Gabriele Multhoff, Vasilis Ntziachristos category:physics.med-ph cs.CV physics.optics q-bio.QM published:2015-11-18 summary:Light propagating in tissue attains a spectrum that varies with location dueto wavelength-dependent fluence attenuation by tissue optical properties, aneffect that causes spectral corruption. Predictions of the spectral variationsof light fluence in tissue are challenging since the spatial distribution ofoptical properties in tissue cannot be resolved in high resolution or with highaccuracy by current methods. Spectral corruption has fundamentally limited thequantification accuracy of optical and optoacoustic methods and impeded thelong sought-after goal of imaging blood oxygen saturation (sO2) deep intissues; a critical but still unattainable target for the assessment ofoxygenation in physiological processes and disease. We discover a new principleunderlying light fluence in tissues, which describes the wavelength dependenceof light fluence as an affine function of a few reference base spectra,independently of the specific distribution of tissue optical properties. Thisfinding enables the introduction of a previously undocumented concept termedeigenspectra Multispectral Optoacoustic Tomography (eMSOT) that can effectivelyaccount for wavelength dependent light attenuation without explicit knowledgeof the tissue optical properties. We validate eMSOT in more than 2000simulations and with phantom and animal measurements. We find that eMSOT canquantitatively image tissue sO2 reaching in many occasions a better than10-fold improved accuracy over conventional spectral optoacoustic methods.Then, we show that eMSOT can spatially resolve sO2 in muscle and tumor;revealing so far unattainable tissue physiology patterns. Last, we relatedeMSOT readings to cancer hypoxia and found congruence between eMSOT tumor sO2images and tissue perfusion and hypoxia maps obtained by correlativehistological analysis.
arxiv-13500-101 | Using Machine Learning to Predict the Outcome of English County twenty over Cricket Matches | http://arxiv.org/pdf/1511.05837v1.pdf | author:Stylianos Kampakis, William Thomas category:stat.ML stat.AP published:2015-11-18 summary:Cricket betting is a multi-billion dollar market. Therefore, there is astrong incentive for models that can predict the outcomes of games and beat theodds provided by bookers. The aim of this study was to investigate to whatdegree it is possible to predict the outcome of cricket matches. The targetcompetition was the English twenty over county cricket cup. The originalfeatures alongside engineered features gave rise to more than 500 team andplayer statistics. The models were optimized firstly with team features onlyand then both team and player features. The performance of the models wastested over individual seasons from 2009 to 2014 having been trained overprevious season data in each case. The optimal model was a simple predictionmethod combined with complex hierarchical features and was shown tosignificantly outperform a gambling industry benchmark.
arxiv-13500-102 | Enhancements in statistical spoken language translation by de-normalization of ASR results | http://arxiv.org/pdf/1511.09392v1.pdf | author:Agnieszka Wołk, Krzysztof Wołk, Krzysztof Marasek category:cs.CL stat.ML published:2015-11-18 summary:Spoken language translation (SLT) has become very important in anincreasingly globalized world. Machine translation (MT) for automatic speechrecognition (ASR) systems is a major challenge of great interest. This researchinvestigates that automatic sentence segmentation of speech that is importantfor enriching speech recognition output and for aiding downstream languageprocessing. This article focuses on the automatic sentence segmentation ofspeech and improving MT results. We explore the problem of identifying sentenceboundaries in the transcriptions produced by automatic speech recognitionsystems in the Polish language. We also experiment with reverse normalizationof the recognized speech samples.
arxiv-13500-103 | Harvesting comparable corpora and mining them for equivalent bilingual sentences using statistical classification and analogy- based heuristics | http://arxiv.org/pdf/1511.06285v1.pdf | author:Krzysztof Wołk, Emilia Rejmund, Krzysztof Marasek category:cs.CL stat.ML published:2015-11-18 summary:Parallel sentences are a relatively scarce but extremely useful resource formany applications including cross-lingual retrieval and statistical machinetranslation. This research explores our new methodologies for mining such datafrom previously obtained comparable corpora. The task is highly practical sincenon-parallel multilingual data exist in far greater quantities than parallelcorpora, but parallel sentences are a much more useful resource. Here wepropose a web crawling method for building subject-aligned comparable corporafrom e.g. Wikipedia dumps and Euronews web page. The improvements in machinetranslation are shown on Polish-English language pair for various text domains.We also tested another method of building parallel corpora based on comparablecorpora data. It lets automatically broad existing corpus of sentences fromsubject of corpora based on analogies between them.
arxiv-13500-104 | Factorization, Inference and Parameter Learning in Discrete AMP Chain Graphs | http://arxiv.org/pdf/1501.06727v2.pdf | author:Jose M. Peña category:stat.ML cs.AI published:2015-01-27 summary:We address some computational issues that may hinder the use of AMP chaingraphs in practice. Specifically, we show how a discrete probabilitydistribution that satisfies all the independencies represented by an AMP chaingraph factorizes according to it. We show how this factorization makes itpossible to perform inference and parameter learning efficiently, by adaptingexisting algorithms for Markov and Bayesian networks. Finally, we turn ourattention to another issue that may hinder the use of AMP CGs, namely the lackof an intuitive interpretation of their edges. We provide one suchinterpretation.
arxiv-13500-105 | Labeled pupils in the wild: A dataset for studying pupil detection in unconstrained environments | http://arxiv.org/pdf/1511.05768v1.pdf | author:Marc Tonsen, Xucong Zhang, Yusuke Sugano, Andreas Bulling category:cs.CV published:2015-11-18 summary:We present labelled pupils in the wild (LPW), a novel dataset of 66high-quality, high-speed eye region videos for the development and evaluationof pupil detection algorithms. The videos in our dataset were recorded from 22participants in everyday locations at about 95 FPS using a state-of-the-artdark-pupil head-mounted eye tracker. They cover people with differentethnicities, a diverse set of everyday indoor and outdoor illuminationenvironments, as well as natural gaze direction distributions. The dataset alsoincludes participants wearing glasses, contact lenses, as well as make-up. Webenchmark five state-of-the-art pupil detection algorithms on our dataset withrespect to robustness and accuracy. We further study the influence of imageresolution, vision aids, as well as recording location (indoor, outdoor) onpupil detection performance. Our evaluations provide valuable insights into thegeneral pupil detection problem and allow us to identify key challenges forrobust pupil detection on head-mounted eye trackers.
arxiv-13500-106 | Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction | http://arxiv.org/pdf/1511.05756v1.pdf | author:Hyeonwoo Noh, Paul Hongsuck Seo, Bohyung Han category:cs.CV cs.CL cs.LG published:2015-11-18 summary:We tackle image question answering (ImageQA) problem by learning aconvolutional neural network (CNN) with a dynamic parameter layer whose weightsare determined adaptively based on questions. For the adaptive parameterprediction, we employ a separate parameter prediction network, which consistsof gated recurrent unit (GRU) taking a question as its input and afully-connected layer generating a set of candidate weights as its output.However, it is challenging to construct a parameter prediction network for alarge number of parameters in the fully-connected dynamic parameter layer ofthe CNN. We reduce the complexity of this problem by incorporating a hashingtechnique, where the candidate weights given by the parameter predictionnetwork are selected using a predefined hash function to determine individualweights in the dynamic parameter layer. The proposed network---joint networkwith the CNN for ImageQA and the parameter prediction network---is trainedend-to-end through back-propagation, where its weights are initialized using apre-trained CNN and GRU. The proposed algorithm illustrates thestate-of-the-art performance on all available public ImageQA benchmarks.
arxiv-13500-107 | backShift: Learning causal cyclic graphs from unknown shift interventions | http://arxiv.org/pdf/1506.02494v3.pdf | author:Dominik Rothenhäusler, Christina Heinze, Jonas Peters, Nicolai Meinshausen category:stat.ME stat.ML published:2015-06-08 summary:We propose a simple method to learn linear causal cyclic models in thepresence of latent variables. The method relies on equilibrium data of themodel recorded under a specific kind of interventions ("shift interventions").The location and strength of these interventions do not have to be known andcan be estimated from the data. Our method, called backShift, only uses secondmoments of the data and performs simple joint matrix diagonalization, appliedto differences between covariance matrices. We give a sufficient and necessarycondition for identifiability of the system, which is fulfilled almost surelyunder some quite general assumptions if and only if there are at least threedistinct experimental settings, one of which can be pure observational data. Wedemonstrate the performance on some simulated data and applications in flowcytometry and financial time series. The code is made available as R-packagebackShift.
arxiv-13500-108 | Sparse learning of maximum likelihood model for optimization of complex loss function | http://arxiv.org/pdf/1511.05743v1.pdf | author:Ning Zhang, Prathamesh Chandrasekar category:cs.LG published:2015-11-18 summary:Traditional machine learning methods usually minimize a simple loss functionto learn a predictive model, and then use a complex performance measure tomeasure the prediction performance. However, minimizing a simple loss functioncannot guarantee that an optimal performance. In this paper, we study theproblem of optimizing the complex performance measure directly to obtain apredictive model. We proposed to construct a maximum likelihood model for thisproblem, and to learn the model parameter, we minimize a com- plex lossfunction corresponding to the desired complex performance measure. To optimizethe loss function, we approximate the upper bound of the complex loss. We alsopropose impose the sparsity to the model parameter to obtain a sparse model. Anobjective is constructed by combining the upper bound of the loss function andthe sparsity of the model parameter, and we develop an iterative algorithm tominimize it by using the fast iterative shrinkage- thresholding algorithmframework. The experiments on optimization on three different complexperformance measures, including F-score, receiver operating characteristiccurve, and recall precision curve break even point, over three real-worldapplications, aircraft event recognition of civil aviation safety, in- trusiondetection in wireless mesh networks, and image classification, show theadvantages of the proposed method over state-of-the-art methods.
arxiv-13500-109 | A Random Forest Guided Tour | http://arxiv.org/pdf/1511.05741v1.pdf | author:Gérard Biau, Erwan Scornet category:math.ST stat.ML stat.TH published:2015-11-18 summary:The random forest algorithm, proposed by L. Breiman in 2001, has beenextremely successful as a general-purpose classification and regression method.The approach, which combines several randomized decision trees and aggregatestheir predictions by averaging, has shown excellent performance in settingswhere the number of variables is much larger than the number of observations.Moreover, it is versatile enough to be applied to large-scale problems, iseasily adapted to various ad-hoc learning tasks, and returns measures ofvariable importance. The present article reviews the most recent theoreticaland methodological developments for random forests. Emphasis is placed on themathematical forces driving the algorithm, with special attention given to theselection of parameters, the resampling mechanism, and variable importancemeasures. This review is intended to provide non-experts easy access to themain ideas.
arxiv-13500-110 | Stochastic Expectation Propagation | http://arxiv.org/pdf/1506.04132v2.pdf | author:Yingzhen Li, Jose Miguel Hernandez-Lobato, Richard E. Turner category:stat.ML cs.LG published:2015-06-12 summary:Expectation propagation (EP) is a deterministic approximation algorithm thatis often used to perform approximate Bayesian parameter learning. EPapproximates the full intractable posterior distribution through a set of localapproximations that are iteratively refined for each datapoint. EP can offeranalytic and computational advantages over other approximations, such asVariational Inference (VI), and is the method of choice for a number of models.The local nature of EP appears to make it an ideal candidate for performingBayesian learning on large models in large-scale dataset settings. However, EPhas a crucial limitation in this context: the number of approximating factorsneeds to increase with the number of data-points, N, which often entails aprohibitively large memory overhead. This paper presents an extension to EP,called stochastic expectation propagation (SEP), that maintains a globalposterior approximation (like VI) but updates it in a local way (like EP).Experiments on a number of canonical learning problems using synthetic andreal-world datasets indicate that SEP performs almost as well as full EP, butreduces the memory consumption by a factor of $N$. SEP is therefore ideallysuited to performing approximate Bayesian learning in the large model, largedataset setting.
arxiv-13500-111 | Online learning in repeated auctions | http://arxiv.org/pdf/1511.05720v1.pdf | author:Jonathan Weed, Vianney Perchet, Philippe Rigollet category:cs.GT cs.LG stat.ML published:2015-11-18 summary:Motivated by online advertising auctions, we consider repeated Vickreyauctions where goods of unknown value are sold sequentially and bidders onlylearn (potentially noisy) information about a good's value once it ispurchased. We adopt an online learning approach with bandit feedback to modelthis problem and derive bidding strategies for two models: stochastic andadversarial. In the stochastic model, the observed values of the goods arerandom variables centered around the true value of the good. In this case,logarithmic regret is achievable when competing against well behavedadversaries. In the adversarial model, the goods need not be identical and wesimply compare our performance against that of the best fixed bid in hindsight.We show that sublinear regret is also achievable in this case and provematching minimax lower bounds. To our knowledge, this is the first complete setof strategies for bidders participating in auctions of this type.
arxiv-13500-112 | GAP Safe screening rules for sparse multi-task and multi-class models | http://arxiv.org/pdf/1506.03736v2.pdf | author:Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort, Joseph Salmon category:stat.ML cs.LG math.OC stat.CO published:2015-06-11 summary:High dimensional regression benefits from sparsity promoting regularizations.Screening rules leverage the known sparsity of the solution by ignoring somevariables in the optimization, hence speeding up solvers. When the procedure isproven not to discard features wrongly the rules are said to be \emph{safe}. Inthis paper we derive new safe rules for generalized linear models regularizedwith $\ell_1$ and $\ell_1/\ell_2$ norms. The rules are based on duality gapcomputations and spherical safe regions whose diameters converge to zero. Thisallows to discard safely more variables, in particular for low regularizationparameters. The GAP Safe rule can cope with any iterative solver and weillustrate its performance on coordinate descent for multi-task Lasso, binaryand multinomial logistic regression, demonstrating significant speed ups on alltested datasets with respect to previous safe rules.
arxiv-13500-113 | Complex-Valued Gaussian Processes for Regression: A Widely Non-Linear Approach | http://arxiv.org/pdf/1511.05710v1.pdf | author:Rafael Boloix-Tortosa, Eva Arias-de-Reyna, F. Javier Payan-Somet, Juan J. Murillo-Fuentes category:cs.LG published:2015-11-18 summary:In this paper we propose a novel Bayesian kernel based solution forregression in complex fields. We develop the formulation of the Gaussianprocess for regression (GPR) to deal with complex-valued outputs. Previoussolutions for kernels methods usually assume a complexification approach, wherethe real-valued kernel is replaced by a complex-valued one. However, based onthe results in complex-valued linear theory, we prove that both a kernel and apseudo-kernel are to be included in the solution. This is the starting point todevelop the new formulation for the complex-valued GPR. The obtainedformulation resembles the one of the widely linear minimum mean-squared(WLMMSE) approach. Just in the particular case where the outputs are proper,the pseudo-kernel cancels and the solution simplifies to a real-valued GPRstructure, as the WLMMSE does into a strictly linear solution. We include somenumerical experiments to show that the novel solution, denoted as widelynon-linear complex GPR (WCGPR), outperforms a strictly complex GPR where apseudo-kernel is not included.
arxiv-13500-114 | Efficient Output Kernel Learning for Multiple Tasks | http://arxiv.org/pdf/1511.05706v1.pdf | author:Pratik Jawanpuria, Maksim Lapin, Matthias Hein, Bernt Schiele category:stat.ML cs.LG published:2015-11-18 summary:The paradigm of multi-task learning is that one can achieve bettergeneralization by learning tasks jointly and thus exploiting the similaritybetween the tasks rather than learning them independently of each other. Whilepreviously the relationship between tasks had to be user-defined in the form ofan output kernel, recent approaches jointly learn the tasks and the outputkernel. As the output kernel is a positive semidefinite matrix, the resultingoptimization problems are not scalable in the number of tasks as aneigendecomposition is required in each step. \mbox{Using} the theory ofpositive semidefinite kernels we show in this paper that for a certain class ofregularizers on the output kernel, the constraint of being positivesemidefinite can be dropped as it is automatically satisfied for the relaxedproblem. This leads to an unconstrained dual problem which can be solvedefficiently. Experiments on several multi-task and multi-class data setsillustrate the efficacy of our approach in terms of computational efficiency aswell as generalization performance.
arxiv-13500-115 | Decomposition into Low-rank plus Additive Matrices for Background/Foreground Separation: A Review for a Comparative Evaluation with a Large-Scale Dataset | http://arxiv.org/pdf/1511.01245v2.pdf | author:Thierry Bouwmans, Andrews Sobral, Sajid Javed, Soon Ki Jung, El-Hadi Zahzah category:cs.CV published:2015-11-04 summary:Recent research on problem formulations based on decomposition into low-rankplus sparse matrices shows a suitable framework to separate moving objects fromthe background. The most representative problem formulation is the RobustPrincipal Component Analysis (RPCA) solved via Principal Component Pursuit(PCP) which decomposes a data matrix in a low-rank matrix and a sparse matrix.However, similar robust implicit or explicit decompositions can be made in thefollowing problem formulations: Robust Non-negative Matrix Factorization(RNMF), Robust Matrix Completion (RMC), Robust Subspace Recovery (RSR), RobustSubspace Tracking (RST) and Robust Low-Rank Minimization (RLRM). The main goalof these similar problem formulations is to obtain explicitly or implicitly adecomposition into low-rank matrix plus additive matrices. In this context,this work aims to initiate a rigorous and comprehensive review of the similarproblem formulations in robust subspace learning and tracking based ondecomposition into low-rank plus additive matrices for testing and rankingexisting algorithms for background/foreground separation. For this, we firstprovide a preliminary review of the recent developments in the differentproblem formulations which allows us to define a unified view that we calledDecomposition into Low-rank plus Additive Matrices (DLAM). Then, we examinecarefully each method in each robust subspace learning/tracking frameworks withtheir decomposition, their loss functions, their optimization problem and theirsolvers. Furthermore, we investigate if incremental algorithms and real-timeimplementations can be achieved for background/foreground separation. Finally,experimental results on a large-scale dataset called Background ModelsChallenge (BMC 2012) show the comparative performance of 32 different robustsubspace learning/tracking methods.
arxiv-13500-116 | Budget Online Multiple Kernel Learning | http://arxiv.org/pdf/1511.04813v2.pdf | author:Jing Lu, Steven C. H. Hoi, Doyen Sahoo, Peilin Zhao category:cs.LG published:2015-11-16 summary:Online learning with multiple kernels has gained increasing interests inrecent years and found many applications. For classification tasks, OnlineMultiple Kernel Classification (OMKC), which learns a kernel based classifierby seeking the optimal linear combination of a pool of single kernelclassifiers in an online fashion, achieves superior accuracy and enjoys greatflexibility compared with traditional single-kernel classifiers. Despite beingstudied extensively, existing OMKC algorithms suffer from high computationalcost due to their unbounded numbers of support vectors. To overcome thisdrawback, we present a novel framework of Budget Online Multiple KernelLearning (BOMKL) and propose a new Sparse Passive Aggressive learning toperform effective budget online learning. Specifically, we adopt a simple yeteffective Bernoulli sampling to decide if an incoming instance should be addedto the current set of support vectors. By limiting the number of supportvectors, our method can significantly accelerate OMKC while maintainingsatisfactory accuracy that is comparable to that of the existing OMKCalgorithms. We theoretically prove that our new method achieves an optimalregret bound in expectation, and empirically found that the proposed algorithmoutperforms various OMKC algorithms and can easily scale up to large-scaledatasets.
arxiv-13500-117 | Compositional Memory for Visual Question Answering | http://arxiv.org/pdf/1511.05676v1.pdf | author:Aiwen Jiang, Fang Wang, Fatih Porikli, Yi Li category:cs.CV published:2015-11-18 summary:Visual Question Answering (VQA) emerges as one of the most fascinating topicsin computer vision recently. Many state of the art methods naively use holisticvisual features with language features into a Long Short-Term Memory (LSTM)module, neglecting the sophisticated interaction between them. This coarsemodeling also blocks the possibilities of exploring finer-grained localfeatures that contribute to the question answering dynamically over time. This paper addresses this fundamental problem by directly modeling thetemporal dynamics between language and all possible local image patches. Whentraversing the question words sequentially, our end-to-end approach explicitlyfuses the features associated to the words and the ones available at multiplelocal patches in an attention mechanism, and further combines the fusedinformation to generate dynamic messages, which we call episode. We then feedthe episodes to a standard question answering module together with thecontextual visual information and linguistic information. Motivated by recentpractices in deep learning, we use auxiliary loss functions during training toimprove the performance. Our experiments on two latest public datasets suggestthat our method has a superior performance. Notably, on the DARQUAR dataset weadvanced the state of the art by 6$\%$, and we also evaluated our approach onthe most recent MSCOCO-VQA dataset.
arxiv-13500-118 | Learning with $\ell^{0}$-Graph: $\ell^{0}$-Induced Sparse Subspace Clustering | http://arxiv.org/pdf/1510.08520v2.pdf | author:Yingzhen Yang, Jiashi Feng, Jianchao Yang, Thomas S. Huang category:cs.LG cs.CV published:2015-10-28 summary:Sparse subspace clustering methods, such as Sparse Subspace Clustering (SSC)\cite{ElhamifarV13} and $\ell^{1}$-graph \cite{YanW09,ChengYYFH10}, areeffective in partitioning the data that lie in a union of subspaces. Most ofthose methods use $\ell^{1}$-norm or $\ell^{2}$-norm with thresholding toimpose the sparsity of the constructed sparse similarity graph, and certainassumptions, e.g. independence or disjointness, on the subspaces are requiredto obtain the subspace-sparse representation, which is the key to theirsuccess. Such assumptions are not guaranteed to hold in practice and they limitthe application of sparse subspace clustering on subspaces with generallocation. In this paper, we propose a new sparse subspace clustering methodnamed $\ell^{0}$-graph. In contrast to the required assumptions on subspacesfor most existing sparse subspace clustering methods, it is proved thatsubspace-sparse representation can be obtained by $\ell^{0}$-graph forarbitrary distinct underlying subspaces almost surely under the mild i.i.d.assumption on the data generation. We develop a proximal method to obtain thesub-optimal solution to the optimization problem of $\ell^{0}$-graph withproved guarantee of convergence. Moreover, we propose a regularized$\ell^{0}$-graph that encourages nearby data to have similar neighbors so thatthe similarity graph is more aligned within each cluster and the graphconnectivity issue is alleviated. Extensive experimental results on variousdata sets demonstrate the superiority of $\ell^{0}$-graph compared to othercompeting clustering methods, as well as the effectiveness of regularized$\ell^{0}$-graph.
arxiv-13500-119 | Reversible Recursive Instance-level Object Segmentation | http://arxiv.org/pdf/1511.04517v2.pdf | author:Xiaodan Liang, Yunchao Wei, Xiaohui Shen, Zequn Jie, Jiashi Feng, Liang Lin, Shuicheng Yan category:cs.CV published:2015-11-14 summary:In this work, we propose a novel Reversible Recursive Instance-level ObjectSegmentation (R2-IOS) framework to address the challenging instance-levelobject segmentation task. R2-IOS consists of a reversible proposal refinementsub-network that predicts bounding box offsets for refining the object proposallocations, and an instance-level segmentation sub-network that generates theforeground mask of the dominant object instance in each proposal. By beingrecursive, R2-IOS iteratively optimizes the two sub-networks during jointtraining, in which the refined object proposals and improved segmentationpredictions are alternately fed into each other to progressively increase thenetwork capabilities. By being reversible, the proposal refinement sub-networkadaptively determines an optimal number of refinement iterations required foreach proposal during both training and testing. Furthermore, to handle multipleoverlapped instances within a proposal, an instance-aware denoising autoencoderis introduced into the segmentation sub-network to distinguish the dominantobject from other distracting instances. Extensive experiments on thechallenging PASCAL VOC 2012 benchmark well demonstrate the superiority ofR2-IOS over other state-of-the-art methods. In particular, the $\text{AP}^r$over $20$ classes at $0.5$ IoU achieves $66.7\%$, which significantlyoutperforms the results of $58.7\%$ by PFN~\cite{PFN} and $46.3\%$by~\cite{liu2015multi}.
arxiv-13500-120 | Accelerating Very Deep Convolutional Networks for Classification and Detection | http://arxiv.org/pdf/1505.06798v2.pdf | author:Xiangyu Zhang, Jianhua Zou, Kaiming He, Jian Sun category:cs.CV cs.LG cs.NE published:2015-05-26 summary:This paper aims to accelerate the test-time computation of convolutionalneural networks (CNNs), especially very deep CNNs that have substantiallyimpacted the computer vision community. Unlike previous methods that aredesigned for approximating linear filters or linear responses, our method takesthe nonlinear units into account. We develop an effective solution to theresulting nonlinear optimization problem without the need of stochasticgradient descent (SGD). More importantly, while previous methods mainly focuson optimizing one or two layers, our nonlinear method enables an asymmetricreconstruction that reduces the rapidly accumulated error when multiple (e.g.,>=10) layers are approximated. For the widely used very deep VGG-16 model, ourmethod achieves a whole-model speedup of 4x with merely a 0.3% increase oftop-5 error in ImageNet classification. Our 4x accelerated VGG-16 model alsoshows a graceful accuracy degradation for object detection when plugged intothe Fast R-CNN detector.
arxiv-13500-121 | Bayesian hypothesis testing for one bit compressed sensing with sensing matrix perturbation | http://arxiv.org/pdf/1511.05660v1.pdf | author:H. Zayyani, M. Korki, F. Marvasti category:stat.ML cs.IT math.IT published:2015-11-18 summary:This letter proposes a low-computational Bayesian algorithm for noisy sparserecovery in the context of one bit compressed sensing with sensing matrixperturbation. The proposed algorithm which is called BHT-MLE comprises a sparsesupport detector and an amplitude estimator. The support detector utilizesBayesian hypothesis test, while the amplitude estimator uses an ML estimatorwhich is obtained by solving a convex optimization problem. Simulation resultsshow that BHT-MLE algorithm offers more reconstruction accuracy than that of anML estimator (MLE) at a low computational cost.
arxiv-13500-122 | Watch and Learn: Optimizing from Revealed Preferences Feedback | http://arxiv.org/pdf/1504.01033v2.pdf | author:Aaron Roth, Jonathan Ullman, Zhiwei Steven Wu category:cs.DS cs.GT cs.LG published:2015-04-04 summary:A Stackelberg game is played between a leader and a follower. The leaderfirst chooses an action, then the follower plays his best response. The goal ofthe leader is to pick the action that will maximize his payoff given thefollower's best response. In this paper we present an approach to solving forthe leader's optimal strategy in certain Stackelberg games where the follower'sutility function (and thus the subsequent best response of the follower) isunknown. Stackelberg games capture, for example, the following interaction between aproducer and a consumer. The producer chooses the prices of the goods heproduces, and then a consumer chooses to buy a utility maximizing bundle ofgoods. The goal of the seller here is to set prices to maximize hisprofit---his revenue, minus the production cost of the purchased bundle. It isquite natural that the seller in this example should not know the buyer'sutility function. However, he does have access to revealed preferencefeedback---he can set prices, and then observe the purchased bundle and his ownprofit. We give algorithms for efficiently solving, in terms of bothcomputational and query complexity, a broad class of Stackelberg games in whichthe follower's utility function is unknown, using only "revealed preference"access to it. This class includes in particular the profit maximizationproblem, as well as the optimal tolling problem in nonatomic congestion games,when the latency functions are unknown. Surprisingly, we are able to solvethese problems even though the optimization problems are non-convex in theleader's actions.
arxiv-13500-123 | Coercive Region-level Registration for Multi-modal Images | http://arxiv.org/pdf/1502.07432v3.pdf | author:Yu-Hui Chen, Dennis Wei, Gregory Newstadt, Jeffrey Simmons, Alfred Hero category:cs.CV published:2015-02-26 summary:We propose a coercive approach to simultaneously register and segmentmulti-modal images which share similar spatial structure. Registration is doneat the region level to facilitate data fusion while avoiding the need forinterpolation. The algorithm performs alternating minimization of an objectivefunction informed by statistical models for pixel values in differentmodalities. Hypothesis tests are developed to determine whether to refinesegmentations by splitting regions. We demonstrate that our approach hassignificantly better performance than the state-of-the-art registration andsegmentation methods on microscopy images.
arxiv-13500-124 | Tree-Guided MCMC Inference for Normalized Random Measure Mixture Models | http://arxiv.org/pdf/1511.05650v1.pdf | author:Juho Lee, Seungjin Choi category:stat.ML cs.LG published:2015-11-18 summary:Normalized random measures (NRMs) provide a broad class of discrete randommeasures that are often used as priors for Bayesian nonparametric models.Dirichlet process is a well-known example of NRMs. Most of posterior inferencemethods for NRM mixture models rely on MCMC methods since they are easy toimplement and their convergence is well studied. However, MCMC often suffersfrom slow convergence when the acceptance rate is low. Tree-based inference isan alternative deterministic posterior inference method, where Bayesianhierarchical clustering (BHC) or incremental Bayesian hierarchical clustering(IBHC) have been developed for DP or NRM mixture (NRMM) models, respectively.Although IBHC is a promising method for posterior inference for NRMM models dueto its efficiency and applicability to online inference, its convergence is notguaranteed since it uses heuristics that simply selects the best solution aftermultiple trials are made. In this paper, we present a hybrid inferencealgorithm for NRMM models, which combines the merits of both MCMC and IBHC.Trees built by IBHC outlines partitions of data, which guidesMetropolis-Hastings procedure to employ appropriate proposals. Inheriting thenature of MCMC, our tree-guided MCMC (tgMCMC) is guaranteed to converge, andenjoys the fast convergence thanks to the effective proposals guided by trees.Experiments on both synthetic and real-world datasets demonstrate the benefitof our method.
arxiv-13500-125 | Adversarial Autoencoders | http://arxiv.org/pdf/1511.05644v1.pdf | author:Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow category:cs.LG published:2015-11-18 summary:In this paper we propose a new method for regularizing autoencoders byimposing an arbitrary prior on the latent representation of the autoencoder.Our method, named "adversarial autoencoder", uses the recently proposedgenerative adversarial networks (GAN) in order to match the aggregatedposterior of the hidden code vector of the autoencoder with an arbitrary prior.Matching the aggregated posterior to the prior ensures that there are no"holes" in the prior, and generating from any part of prior space results inmeaningful samples. As a result, the decoder of the adversarial autoencoderlearns a deep generative model that maps the imposed prior to the datadistribution. We show how adversarial autoencoders can be used to disentanglestyle and content of images and achieve competitive generative performance onMNIST, Street View House Numbers and Toronto Face datasets.
arxiv-13500-126 | A New Smooth Approximation to the Zero One Loss with a Probabilistic Interpretation | http://arxiv.org/pdf/1511.05643v1.pdf | author:Md Kamrul Hasan, Christopher J. Pal category:cs.CV cs.AI cs.IR cs.LG published:2015-11-18 summary:We examine a new form of smooth approximation to the zero one loss in whichlearning is performed using a reformulation of the widely used logisticfunction. Our approach is based on using the posterior mean of a novelgeneralized Beta-Bernoulli formulation. This leads to a generalized logisticfunction that approximates the zero one loss, but retains a probabilisticformulation conferring a number of useful properties. The approach is easilygeneralized to kernel logistic regression and easily integrated into methodsfor structured prediction. We present experiments in which we learn such modelsusing an optimization method consisting of a combination of gradient descentand coordinate descent using localized grid search so as to escape from localminima. Our experiments indicate that optimization quality is improved whenlearning meta-parameters are themselves optimized using a validation set. Ourexperiments show improved performance relative to widely used logistic andhinge loss methods on a wide variety of problems ranging from standard UCIrvine and libSVM evaluation datasets to product review predictions and avisual information extraction task. We observe that the approach: 1) is morerobust to outliers compared to the logistic and hinge losses; 2) outperformscomparable logistic and max margin models on larger scale benchmark problems;3) when combined with Gaussian- Laplacian mixture prior on parameters thekernelized version of our formulation yields sparser solutions than SupportVector Machine classifiers; and 4) when integrated into a probabilisticstructured prediction technique our approach provides more accurateprobabilities yielding improved inference and increasing information extractionperformance.
arxiv-13500-127 | Competitive Multi-scale Convolution | http://arxiv.org/pdf/1511.05635v1.pdf | author:Zhibin Liao, Gustavo Carneiro category:cs.CV cs.LG cs.NE published:2015-11-18 summary:In this paper, we introduce a new deep convolutional neural network (ConvNet)module that promotes competition among a set of multi-scale convolutionalfilters. This new module is inspired by the inception module, where we replacethe original collaborative pooling stage (consisting of a concatenation of themulti-scale filter outputs) by a competitive pooling represented by a maxoutactivation unit. This extension has the following two objectives: 1) theselection of the maximum response among the multi-scale filters prevents filterco-adaptation and allows the formation of multiple sub-networks within the samemodel, which has been shown to facilitate the training of complex learningproblems; and 2) the maxout unit reduces the dimensionality of the outputs fromthe multi-scale filters. We show that the use of our proposed module in typicaldeep ConvNets produces classification results that are either better than orcomparable to the state of the art on the following benchmark datasets: MNIST,CIFAR-10, CIFAR-100 and SVHN.
arxiv-13500-128 | MOEA/D-GM: Using probabilistic graphical models in MOEA/D for solving combinatorial optimization problems | http://arxiv.org/pdf/1511.05625v1.pdf | author:Murilo Zangari de Souza, Roberto Santana, Aurora Trinidad Ramirez Pozo, Alexander Mendiburu category:cs.NE published:2015-11-18 summary:Evolutionary algorithms based on modeling the statistical dependencies(interactions) between the variables have been proposed to solve a wide rangeof complex problems. These algorithms learn and sample probabilistic graphicalmodels able to encode and exploit the regularities of the problem. This paperinvestigates the effect of using probabilistic modeling techniques as a way toenhance the behavior of MOEA/D framework. MOEA/D is a decomposition basedevolutionary algorithm that decomposes a multi-objective optimization problem(MOP) in a number of scalar single-objective subproblems and optimizes them ina collaborative manner. MOEA/D framework has been widely used to solve severalMOPs. The proposed algorithm, MOEA/D using probabilistic Graphical Models(MOEA/D-GM) is able to instantiate both univariate and multi-variateprobabilistic models for each subproblem. To validate the introduced frameworkalgorithm, an experimental study is conducted on a multi-objective version ofthe deceptive function Trap5. The results show that the variant of theframework (MOEA/D-Tree), where tree models are learned from the matrices of themutual information between the variables, is able to capture the structure ofthe problem. MOEA/D-Tree is able to achieve significantly better results thanboth MOEA/D using genetic operators and MOEA/D using univariate probabilitymodels, in terms of the approximation to the true Pareto front.
arxiv-13500-129 | A Block Regression Model for Short-Term Mobile Traffic Forecasting | http://arxiv.org/pdf/1511.05612v1.pdf | author:Huimin Pan, Jingchu Liu, Sheng Zhou, Zhisheng Niu category:cs.NI cs.LG published:2015-11-17 summary:Accurate mobile traffic forecast is important for efficient network planningand operations. However, existing traffic forecasting models have highcomplexity, making the forecasting process slow and costly. In this paper, weanalyze some characteristics of mobile traffic such as periodicity, spatialsimilarity and short term relativity. Based on these characteristics, wepropose a \emph{Block Regression} ({BR}) model for mobile traffic forecasting.This model employs seasonal differentiation so as to take into account of thetemporally repetitive nature of mobile traffic. One of the key features of our{BR} model lies in its low complexity since it constructs a single model forall base stations. We evaluate the accuracy of {BR} model based on real trafficdata and compare it with the existing models. Results show that our {BR} modeloffers equal accuracy to the existing models but has much less complexity.
arxiv-13500-130 | Complete Dictionary Recovery over the Sphere | http://arxiv.org/pdf/1504.06785v3.pdf | author:Ju Sun, Qing Qu, John Wright category:cs.IT cs.CV cs.LG math.IT math.OC stat.ML published:2015-04-26 summary:We consider the problem of recovering a complete (i.e., square andinvertible) matrix $\mathbf A_0$, from $\mathbf Y \in \mathbb R^{n \times p}$with $\mathbf Y = \mathbf A_0 \mathbf X_0$, provided $\mathbf X_0$ issufficiently sparse. This recovery problem is central to the theoreticalunderstanding of dictionary learning, which seeks a sparse representation for acollection of input signals, and finds numerous applications in modern signalprocessing and machine learning. We give the first efficient algorithm thatprovably recovers $\mathbf A_0$ when $\mathbf X_0$ has $O(n)$ nonzeros percolumn, under suitable probability model for $\mathbf X_0$. In contrast, priorresults based on efficient algorithms provide recovery guarantees when $\mathbfX_0$ has only $O(n^{1-\delta})$ nonzeros per column for any constant $\delta\in (0, 1)$. Our algorithmic pipeline centers around solving a certain nonconvexoptimization problem with a spherical constraint, and hence is naturallyphrased in the language of manifold optimization. To show this apparently hardproblem is tractable, we first provide a geometric characterization of thehigh-dimensional objective landscape, which shows that with high probabilitythere are no "spurious" local minima. This particular geometric structureallows us to design a Riemannian trust region algorithm over the sphere thatprovably converges to one local minimizer with an arbitrary initialization,despite the presence of saddle points. The geometric approach we develop heremay also shed light on other problems arising from nonconvex recovery ofstructured signals.
arxiv-13500-131 | Combinatorial Cascading Bandits | http://arxiv.org/pdf/1507.04208v3.pdf | author:Branislav Kveton, Zheng Wen, Azin Ashkan, Csaba Szepesvari category:cs.LG stat.ML published:2015-07-15 summary:We propose combinatorial cascading bandits, a class of partial monitoringproblems where at each step a learning agent chooses a tuple of ground itemssubject to constraints and receives a reward if and only if the weights of allchosen items are one. The weights of the items are binary, stochastic, anddrawn independently of each other. The agent observes the index of the firstchosen item whose weight is zero. This observation model arises in networkrouting, for instance, where the learning agent may only observe the first linkin the routing path which is down, and blocks the path. We propose a UCB-likealgorithm for solving our problems, CombCascade; and prove gap-dependent andgap-free upper bounds on its $n$-step regret. Our proofs build on recent workin stochastic combinatorial semi-bandits but also address two novel challengesof our setting, a non-linear reward function and partial observability. Weevaluate CombCascade on two real-world problems and show that it performs welleven when our modeling assumptions are violated. We also demonstrate that oursetting requires a new learning algorithm.
arxiv-13500-132 | Articulated Motion Learning via Visual and Lingual Signals | http://arxiv.org/pdf/1511.05526v1.pdf | author:Zhengyang Wu, Mohit Bansal, Matthew R. Walter category:cs.RO cs.CL cs.CV published:2015-11-17 summary:In order for robots to operate effectively in homes and workplaces, they mustbe able to manipulate the articulated objects common to environments built forand by humans. Previous work learns kinematic models that prescribe thismanipulation from visual demonstrations. Lingual signals, such as naturallanguage descriptions and instructions, offer a complementary means ofconveying knowledge of such manipulation models and are suitable to a widerange of interactions (e.g., remote manipulation). In this paper, we present amultimodal learning framework that incorporates both visual and lingualinformation to estimate the structure and parameters that define kinematicmodels of articulated objects. The visual signal takes the form of an RGB-Dimage stream that opportunistically captures object motion in an unpreparedscene. Accompanying natural language descriptions of the motion constitute thelingual signal. We present a probabilistic language model that uses wordembeddings to associate lingual verbs with their corresponding kinematicstructures. By exploiting the complementary nature of the visual and lingualinput, our method infers correct kinematic structures for various multiple-partobjects on which the previous state-of-the-art, visual-only system fails. Weevaluate our multimodal learning framework on a dataset comprised of a varietyof household objects, and demonstrate a 36% improvement in model accuracy overthe vision-only baseline.
arxiv-13500-133 | Automatic Instrument Recognition in Polyphonic Music Using Convolutional Neural Networks | http://arxiv.org/pdf/1511.05520v1.pdf | author:Peter Li, Jiyuan Qian, Tian Wang category:cs.SD cs.IR cs.LG cs.NE published:2015-11-17 summary:Traditional methods to tackle many music information retrieval taskstypically follow a two-step architecture: feature engineering followed by asimple learning algorithm. In these "shallow" architectures, featureengineering and learning are typically disjoint and unrelated. Additionally,feature engineering is difficult, and typically depends on extensive domainexpertise. In this paper, we present an application of convolutional neural networks forthe task of automatic musical instrument identification. In this model, featureextraction and learning algorithms are trained together in an end-to-endfashion. We show that a convolutional neural network trained on raw audio canachieve performance surpassing traditional methods that rely on hand-craftedfeatures.
arxiv-13500-134 | Moral Lineage Tracing | http://arxiv.org/pdf/1511.05512v1.pdf | author:Florian Jug, Evgeny Levinkov, Corinna Blasse, Eugene W. Myers, Bjoern Andres category:cs.CV cs.DM published:2015-11-17 summary:Lineage tracing, the tracking of living cells as they move and divide, is acentral problem in biological image analysis. Solutions, called lineageforests, are key to understanding how the structure of multicellular organismsemerges. We propose an integer linear program (ILP) whose feasible solutionsdefine a decomposition of each image in a sequence into cells (segmentation),and a lineage forest of cells across images (tracing). Unlike previousformulations, we do not constrain the set of decompositions, except bycontracting pixels to superpixels. The main challenge, as we show, is toenforce the morality of lineages, i.e., the constraint that cells do not merge.To enforce morality, we introduce path-cut inequalities. To find feasiblesolutions of the NP-hard ILP, with certified bounds to the global optimum, wedefine efficient separation procedures and apply these as part of abranch-and-cut algorithm. We show the effectiveness of this approach byanalyzing feasible solutions for real microscopy data in terms of bounds andrun-time, and by their weighted edit distance to ground truth lineage foreststraced by humans.
arxiv-13500-135 | Learning the Architecture of Deep Neural Networks | http://arxiv.org/pdf/1511.05497v1.pdf | author:Suraj Srinivas, R. Venkatesh Babu category:cs.LG cs.CV cs.NE published:2015-11-17 summary:Deep neural networks with millions of parameters are at the heart of manystate of the art machine learning models today. However, recent works haveshown that models with much smaller number of parameters can also perform justas well. In this work, we introduce the problem of architecture-learning, i.e;learning the architecture of a neural network along with weights. We introducea new trainable parameter called tri-state ReLU, which helps in eliminatingunnecessary neurons. We also propose a smooth regularizer which encourages thetotal number of neurons after elimination to be small. The resulting objectiveis differentiable and simple to optimize. We experimentally validate our methodon both small and large networks, and show that it can learn models with aconsiderably small number of parameters without affecting prediction accuracy.
arxiv-13500-136 | Accelerating pseudo-marginal Metropolis-Hastings by correlating auxiliary variables | http://arxiv.org/pdf/1511.05483v1.pdf | author:Johan Dahlin, Fredrik Lindsten, Joel Kronander, Thomas B. Schön category:stat.CO stat.ML published:2015-11-17 summary:Pseudo-marginal Metropolis-Hastings (pmMH) is a powerful method for Bayesianinference in models where the posterior distribution is analytical intractableor computationally costly to evaluate directly. It operates by introducingadditional auxiliary variables into the model and form an extended targetdistribution, which then can be evaluated point-wise. In many cases, thestandard Metropolis-Hastings is then applied to sample from the extended targetand the sought posterior can be obtained by marginalisation. However, in someimplementations this approach suffers from poor mixing as the auxiliaryvariables are sampled from an independent proposal. We propose a modificationto the pmMH algorithm in which a Crank-Nicolson (CN) proposal is used instead.This results in that we introduce a positive correlation in the auxiliaryvariables. We investigate how to tune the CN proposal and its impact on themixing of the resulting pmMH sampler. The conclusion is that the proposedmodification can have a beneficial effect on both the mixing of the Markovchain and the computational cost for each iteration of the pmMH algorithm.
arxiv-13500-137 | Evaluating Real-time Anomaly Detection Algorithms - the Numenta Anomaly Benchmark | http://arxiv.org/pdf/1510.03336v4.pdf | author:Alexander Lavin, Subutai Ahmad category:cs.AI cs.LG published:2015-10-12 summary:Much of the world's data is streaming, time-series data, where anomalies givesignificant information in critical situations; examples abound in domains suchas finance, IT, security, medical, and energy. Yet detecting anomalies instreaming data is a difficult task, requiring detectors to process data inreal-time, not batches, and learn while simultaneously making predictions.There are no benchmarks to adequately test and score the efficacy of real-timeanomaly detectors. Here we propose the Numenta Anomaly Benchmark (NAB), whichattempts to provide a controlled and repeatable environment of open-sourcetools to test and measure anomaly detection algorithms on streaming data. Theperfect detector would detect all anomalies as soon as possible, trigger nofalse alarms, work with real-world time-series data across a variety ofdomains, and automatically adapt to changing statistics. Rewarding thesecharacteristics is formalized in NAB, using a scoring algorithm designed forstreaming data. NAB evaluates detectors on a benchmark dataset with labeled,real-world time-series data. We present these components, and give results andanalyses for several open source, commercially-used algorithms. The goal forNAB is to provide a standard, open source framework with which the researchcommunity can compare and evaluate different algorithms for detecting anomaliesin streaming data.
arxiv-13500-138 | Partitioning Well-Clustered Graphs: Spectral Clustering Works! | http://arxiv.org/pdf/1411.2021v2.pdf | author:Richard Peng, He Sun, Luca Zanetti category:cs.DS cs.LG published:2014-11-07 summary:In this paper we study variants of the widely used spectral clustering thatpartitions a graph into k clusters by (1) embedding the vertices of a graphinto a low-dimensional space using the bottom eigenvectors of the Laplacianmatrix, and (2) grouping the embedded points into k clusters via k-meansalgorithms. We show that, for a wide class of graphs, spectral clustering givesa good approximation of the optimal clustering. While this approach wasproposed in the early 1990s and has comprehensive applications, prior to ourwork similar results were known only for graphs generated from stochasticmodels. We also give a nearly-linear time algorithm for partitioning well-clusteredgraphs based on heat kernel embeddings and approximate nearest neighbor datastructures.
arxiv-13500-139 | Extending Gossip Algorithms to Distributed Estimation of U-Statistics | http://arxiv.org/pdf/1511.05464v1.pdf | author:Igor Colin, Aurélien Bellet, Joseph Salmon, Stéphan Clémençon category:stat.ML cs.DC cs.LG cs.SY published:2015-11-17 summary:Efficient and robust algorithms for decentralized estimation in networks areessential to many distributed systems. Whereas distributed estimation of samplemean statistics has been the subject of a good deal of attention, computationof $U$-statistics, relying on more expensive averaging over pairs ofobservations, is a less investigated area. Yet, such data functionals areessential to describe global properties of a statistical population, withimportant examples including Area Under the Curve, empirical variance, Ginimean difference and within-cluster point scatter. This paper proposes newsynchronous and asynchronous randomized gossip algorithms which simultaneouslypropagate data across the network and maintain local estimates of the$U$-statistic of interest. We establish convergence rate bounds of $O(1/t)$ and$O(\log t / t)$ for the synchronous and asynchronous cases respectively, where$t$ is the number of iterations, with explicit data and network dependentterms. Beyond favorable comparisons in terms of rate analysis, numericalexperiments provide empirical evidence the proposed algorithms surpasses thepreviously introduced approach.
arxiv-13500-140 | Simple, Fast and Accurate Photometric Estimation of Specific Star Formation Rate | http://arxiv.org/pdf/1511.05424v1.pdf | author:Kristoffer Stensbo-Smidt, Fabian Gieseke, Christian Igel, Andrew Zirm, Kim Steenstrup Pedersen category:astro-ph.IM stat.ML published:2015-11-17 summary:Large-scale surveys make huge amounts of photometric data available. Becauseof the sheer amount of objects, spectral data cannot be obtained for all ofthem. Therefore it is important to devise techniques for reliably estimatingphysical properties of objects from photometric information alone. Theseestimates are needed to automatically identify interesting objects worth afollow-up investigation as well as to produce the required data for astatistical analysis of the space covered by a survey. We argue that machinelearning techniques are suitable to compute these estimates accurately andefficiently. This study considers the task of estimating the specific starformation rate (sSFR) of galaxies. It is shown that a nearest neighboursalgorithm can produce better sSFR estimates than traditional SED fitting. Weshow that we can obtain accurate estimates of the sSFR even at high redshiftsusing only broad-band photometry based on the u, g, r, i and z filters fromSloan Digital Sky Survey (SDSS). We addtionally demonstrate that combiningmagnitudes estimated with different methods from the same photometry can leadto a further improvement in accuracy. The study highlights the generalimportance of performing proper model selection to improve the results ofmachine learning systems and how feature selection can provide insights intothe predictive relevance of particular input features. Furthermore, the use ofmassively parallel computation on graphics processing units (GPUs) for handlinglarge amounts of astronomical data is advocated.
arxiv-13500-141 | Bayesian Optimization with Dimension Scheduling: Application to Biological Systems | http://arxiv.org/pdf/1511.05385v1.pdf | author:Doniyor Ulmasov, Caroline Baroukh, Benoit Chachuat, Marc Peter Deisenroth, Ruth Misener category:stat.ML cs.LG math.OC published:2015-11-17 summary:Bayesian Optimization (BO) is a data-efficient method for global black-boxoptimization of an expensive-to-evaluate fitness function. BO typically assumesthat computation cost of BO is cheap, but experiments are time consuming orcostly. In practice, this allows us to optimize ten or fewer criticalparameters in up to 1,000 experiments. But experiments may be less expensivethan BO methods assume: In some simulation models, we may be able to conductmultiple thousands of experiments in a few hours, and the computational burdenof BO is no longer negligible compared to experimentation time. To address thischallenge we introduce a new Dimension Scheduling Algorithm (DSA), whichreduces the computational burden of BO for many experiments. The key idea isthat DSA optimizes the fitness function only along a small set of dimensions ateach iteration. This DSA strategy (1) reduces the necessary computation time,(2) finds good solutions faster than the traditional BO method, and (3) can beparallelized straightforwardly. We evaluate the DSA in the context ofoptimizing parameters of dynamic models of microalgae metabolism and showfaster convergence than traditional BO.
arxiv-13500-142 | Constant Time EXPected Similarity Estimation using Stochastic Optimization | http://arxiv.org/pdf/1511.05371v1.pdf | author:Markus Schneider, Wolfgang Ertel, Günther Palm category:cs.LG published:2015-11-17 summary:A new algorithm named EXPected Similarity Estimation (EXPoSE) was recentlyproposed to solve the problem of large-scale anomaly detection. It is anon-parametric and distribution free kernel method based on the Hilbert spaceembedding of probability measures. Given a dataset of $n$ samples, EXPoSE needsonly $\mathcal{O}(n)$ (linear time) to build a model and $\mathcal{O}(1)$(constant time) to make a prediction. In this work we improve the linearcomputational complexity and show that an $\epsilon$-accurate model can beestimated in constant time, which has significant implications for large-scalelearning problems. To achieve this goal, we cast the original EXPoSEformulation into a stochastic optimization problem. It is crucial that thisapproach allows us to determine the number of iteration based on a desiredaccuracy $\epsilon$, independent of the dataset size $n$. We will show that theproposed stochastic gradient descent algorithm works in general (possibleinfinite-dimensional) Hilbert spaces, is easy to implement and requires noadditional step-size parameters.
arxiv-13500-143 | Vertex nomination schemes for membership prediction | http://arxiv.org/pdf/1312.2638v5.pdf | author:D. E. Fishkind, V. Lyzinski, H. Pao, L. Chen, C. E. Priebe category:stat.ML math.OC stat.AP published:2013-12-10 summary:Suppose that a graph is realized from a stochastic block model where one ofthe blocks is of interest, but many or all of the vertices' block labels areunobserved. The task is to order the vertices with unobserved block labels intoa ``nomination list'' such that, with high probability, vertices from theinteresting block are concentrated near the list's beginning. We proposeseveral vertex nomination schemes. Our basic - but principled - setting anddevelopment yields a best nomination scheme (which is a Bayes-Optimalanalogue), and also a likelihood maximization nomination scheme that ispractical to implement when there are a thousand vertices, and which isempirically near-optimal when the number of vertices is small enough to allowcomparison to the best nomination scheme. We then illustrate the robustness ofthe likelihood maximization nomination scheme to the modeling challengesinherent in real data, using examples which include a social network involvinghuman trafficking, the Enron Graph, a worm brain connectome and a politicalblog network.
arxiv-13500-144 | When Naïve Bayes Nearest Neighbours Meet Convolutional Neural Networks | http://arxiv.org/pdf/1511.03853v2.pdf | author:Ilja Kuzborskij, Fabio Maria Carlucci, Barbara Caputo category:cs.CV published:2015-11-12 summary:Since Convolutional Neural Networks (CNNs) have become the leading learningparadigm in visual recognition, Naive Bayes Nearest Neighbour (NBNN)-basedclassifiers have lost momentum in the community. This is because (1) suchalgorithms cannot use CNN activations as input features; (2) they cannot beused as final layer of CNN architectures for end-to-end training , and (3) theyare generally not scalable and hence cannot handle big data. This paperproposes a framework that addresses all these issues, thus bringing back NBNNson the map. We solve the first by extracting CNN activations from local patchesat multiple scale levels, similarly to [1]. We address simultaneously thesecond and third by proposing a scalable version of Naive Bayes Non-linearLearning (NBNL, [2]). Results obtained using pre-trained CNNs on standard sceneand domain adaptation databases show the strength of our approach, opening anew season for NBNNs.
arxiv-13500-145 | Understanding learned CNN features through Filter Decoding with Substitution | http://arxiv.org/pdf/1511.05084v2.pdf | author:Ivet Rafegas, Maria Vanrell category:cs.CV published:2015-11-16 summary:In parallel with the success of CNNs to solve vision problems, there is agrowing interest in developing methodologies to understand and visualize theinternal representations of these networks. How the responses of a trained CNNencode the visual information is a fundamental question both for computer andhuman vision research. Image representations provided by the firstconvolutional layer as well as the resolution change provided by themax-polling operation are easy to understand, however, as soon as a second andfurther convolutional layers are added in the representation, any intuition islost. A usual way to deal with this problem has been to define deconvolutionalnetworks that somehow allow to explore the internal representations of the mostimportant activations towards the image space, where deconvolution is assumedas a convolution with the transposed filter. However, this assumption is notthe best approximation of an inverse convolution. In this paper we propose anew assumption based on filter substitution to reverse the encoding of aconvolutional layer. This provides us with a new tool to directly visualize anyCNN single neuron as a filter in the first layer, this is in terms of the imagespace.
arxiv-13500-146 | Aggregation of predictors for nonstationary sub-linear processes and online adaptive forecasting of time varying autoregressive processes | http://arxiv.org/pdf/1404.6769v5.pdf | author:Christophe Giraud, François Roueff, Andres Sanchez-Perez category:math.ST stat.ML stat.TH published:2014-04-27 summary:In this work, we study the problem of aggregating a finite number ofpredictors for nonstationary sub-linear processes. We provide oracleinequalities relying essentially on three ingredients: (1) a uniform bound ofthe $\ell^1$ norm of the time varying sub-linear coefficients, (2) a Lipschitzassumption on the predictors and (3) moment conditions on the noise appearingin the linear representation. Two kinds of aggregations are considered givingrise to different moment conditions on the noise and more or less sharp oracleinequalities. We apply this approach for deriving an adaptive predictor forlocally stationary time varying autoregressive (TVAR) processes. It is obtainedby aggregating a finite number of well chosen predictors, each of them enjoyingan optimal minimax convergence rate under specific smoothness conditions on theTVAR coefficients. We show that the obtained aggregated predictor achieves aminimax rate while adapting to the unknown smoothness. To prove this result, alower bound is established for the minimax rate of the prediction risk for theTVAR process. Numerical experiments complete this study. An important featureof this approach is that the aggregated predictor can be computed recursivelyand is thus applicable in an online prediction context.
arxiv-13500-147 | Optimized Linear Imputation | http://arxiv.org/pdf/1511.05309v1.pdf | author:Yehezkel S. Resheff, Daphna Weinshall category:stat.ML stat.AP stat.CO stat.ME published:2015-11-17 summary:Often in real-world datasets, especially in high dimensional data, somefeature values are missing. Since most data analysis and statistical methods donot handle gracefully missing values, the ?rst step in the analysis requiresthe imputation of missing values. Indeed, there has been a long standinginterest in methods for the imputation of missing values as a pre-processingstep. One recent and e?ective approach, the IRMI stepwise regression imputationmethod, uses a linear regression model for each real-valued feature on thebasis of all other features in the dataset. However, the proposed iterativeformulation lacks convergence guarantee. Here we propose a closely relatedmethod, stated as a single optimization problem and a block coordinate-descentsolution which is guaranteed to converge to a local minimum. Experiments showresults on both synthetic and benchmark datasets, which are comparable to theresults of the IRMI method whenever it converges. However, while in the set ofexperiments described here IRMI often does not converge, the performance of ourmethods is shown to be markedly superior in comparison with other methods.
arxiv-13500-148 | Classifying and Segmenting Microscopy Images Using Convolutional Multiple Instance Learning | http://arxiv.org/pdf/1511.05286v1.pdf | author:Oren Z. Kraus, Lei Jimmy Ba, Brendan Frey category:cs.CV q-bio.SC stat.ML published:2015-11-17 summary:Convolutional neural networks (CNN) have achieved state of the artperformance on both classification and segmentation tasks. Applying CNNs tomicroscopy images is challenging due to the lack of datasets labeled at thesingle cell level. We extend the application of CNNs to microscopy imageclassification and segmentation using multiple instance learning (MIL). Wepresent the adaptive Noisy-AND MIL pooling function, a new MIL operator that isrobust to outliers. Combining CNNs with MIL enables training CNNs using fullresolution microscopy images with global labels. We base our approach on thesimilarity between the aggregation function used in MIL and pooling layers usedin CNNs. We show that training MIL CNNs end-to-end outperforms several previousmethods on both mammalian and yeast microscopy images without requiring anysegmentation steps.
arxiv-13500-149 | Convergence rates for pretraining and dropout: Guiding learning parameters using network structure | http://arxiv.org/pdf/1506.03412v2.pdf | author:Vamsi K. Ithapu, Sathya Ravi, Vikas Singh category:cs.LG cs.CV cs.NE math.OC stat.ML published:2015-06-10 summary:Unsupervised pretraining and dropout have been well studied, especially withrespect to regularization and output consistency. However, our understandingabout the explicit convergence rates of the parameter estimates, and theirdependence on the learning (like denoising and dropout rate) and structural(like depth and layer lengths) aspects of the network is less mature. Aninteresting question in this context is to ask if the network structure could"guide" the choices of such learning parameters. In this work, we explore thesegaps between network structure, the learning mechanisms and their interactionwith parameter convergence rates. We present a way to address these issuesbased on the backpropagation convergence rates for general nonconvex objectivesusing first-order information. We then incorporate two learning mechanisms intothis general framework -- denoising autoencoder and dropout, and subsequentlyderive the convergence rates of deep networks. Building upon these bounds, weprovide insights into the choices of learning parameters and network sizes thatachieve certain levels of convergence accuracy. The results derived heresupport existing empirical observations, and we also conduct a set ofexperiments to evaluate them.
arxiv-13500-150 | Semi-supervised Collaborative Ranking with Push at Top | http://arxiv.org/pdf/1511.05266v1.pdf | author:Iman Barjasteh, Rana Forsati, Abdol-Hossein Esfahanian, Hayder Radha category:cs.LG cs.IR published:2015-11-17 summary:Existing collaborative ranking based recommender systems tend to perform bestwhen there is enough observed ratings for each user and the observation is madecompletely at random. Under this setting recommender systems can properlysuggest a list of recommendations according to the user interests. However,when the observed ratings are extremely sparse (e.g. in the case of cold-startusers where no rating data is available), and are not sampled uniformly atrandom, existing ranking methods fail to effectively leverage side informationto transduct the knowledge from existing ratings to unobserved ones. We proposea semi-supervised collaborative ranking model, dubbed \texttt{S$^2$COR}, toimprove the quality of cold-start recommendation. \texttt{S$^2$COR} mitigatesthe sparsity issue by leveraging side information about both observed andmissing ratings by collaboratively learning the ranking model. This enables itto deal with the case of missing data not at random, but to also effectivelyincorporate the available side information in transduction. We experimentallyevaluated our proposed algorithm on a number of challenging real-world datasetsand compared against state-of-the-art models for cold-start recommendation. Wereport significantly higher quality recommendations with our algorithm comparedto the state-of-the-art.
arxiv-13500-151 | Robust PCA via Nonconvex Rank Approximation | http://arxiv.org/pdf/1511.05261v1.pdf | author:Zhao Kang, Chong Peng, Qiang Cheng category:cs.CV cs.LG cs.NA stat.ML published:2015-11-17 summary:Numerous applications in data mining and machine learning require recoveringa matrix of minimal rank. Robust principal component analysis (RPCA) is ageneral framework for handling this kind of problems. Nuclear norm based convexsurrogate of the rank function in RPCA is widely investigated. Under certainassumptions, it can recover the underlying true low rank matrix with highprobability. However, those assumptions may not hold in real-worldapplications. Since the nuclear norm approximates the rank by adding allsingular values together, which is essentially a $\ell_1$-norm of the singularvalues, the resulting approximation error is not trivial and thus the resultingmatrix estimator can be significantly biased. To seek a closer approximationand to alleviate the above-mentioned limitations of the nuclear norm, wepropose a nonconvex rank approximation. This approximation to the matrix rankis tighter than the nuclear norm. To solve the associated nonconvexminimization problem, we develop an efficient augmented Lagrange multiplierbased optimization algorithm. Experimental results demonstrate that our methodoutperforms current state-of-the-art algorithms in both accuracy andefficiency.
arxiv-13500-152 | Visualizing and Understanding Recurrent Networks | http://arxiv.org/pdf/1506.02078v2.pdf | author:Andrej Karpathy, Justin Johnson, Li Fei-Fei category:cs.LG cs.CL cs.NE published:2015-06-05 summary:Recurrent Neural Networks (RNNs), and specifically a variant with LongShort-Term Memory (LSTM), are enjoying renewed interest as a result ofsuccessful applications in a wide range of machine learning problems thatinvolve sequential data. However, while LSTMs provide exceptional results inpractice, the source of their performance and their limitations remain ratherpoorly understood. Using character-level language models as an interpretabletestbed, we aim to bridge this gap by providing an analysis of theirrepresentations, predictions and error types. In particular, our experimentsreveal the existence of interpretable cells that keep track of long-rangedependencies such as line lengths, quotes and brackets. Moreover, ourcomparative analysis with finite horizon n-gram models traces the source of theLSTM improvements to long-range structural dependencies. Finally, we provideanalysis of the remaining errors and suggests areas for further study.
arxiv-13500-153 | An extension of McDiarmid's inequality | http://arxiv.org/pdf/1511.05240v1.pdf | author:Richard Combes category:cs.LG math.PR math.ST stat.TH published:2015-11-17 summary:We derive an extension of McDiarmid's inequality for functions $f$ withbounded differences on a high probability set ${\cal Y}$ (instead of almostsurely). The behavior of $f$ outside ${\cal Y}$ may be arbitrary. The proof isshort and elementary, and relies on an extension argument similar toKirszbraun's theorem.
arxiv-13500-154 | Controlling Bias in Adaptive Data Analysis Using Information Theory | http://arxiv.org/pdf/1511.05219v1.pdf | author:Daniel Russo, James Zou category:stat.ML cs.LG published:2015-11-16 summary:Modern data is messy and high-dimensional, and it is often not clear a prioriwhat are the right questions to ask. Instead, the analyst typically needs touse the data to search for interesting analyses to perform and hypotheses totest. This is an adaptive process, where the choice of analysis to be performednext depends on the results of the previous analyses on the same data. It'swidely recognized that this process, even if well-intentioned, can lead tobiases and false discoveries, contributing to the crisis of reproducibility inscience. But while adaptivity renders standard statistical theory invalid,folklore and experience suggest that not all types of adaptive analysis areequally at risk for false discoveries. In this paper, we propose a generalinformation-theoretic framework to quantify and provably bound the bias andother statistics of an arbitrary adaptive analysis process. We prove that ourmutual information based bound is tight in natural models, and then use it togive rigorous insights into when commonly used procedures do or do not lead tosubstantially biased estimation. We first consider several popular featureselection protocols, like rank selection or variance-based selection. We thenconsider the practice of adding random noise to the observations or to thereported statistics, which is advocated by related ideas from differentialprivacy and blinded data analysis. We discuss the connections between thesetechniques and our framework, and supplement our results with illustrativesimulations.
arxiv-13500-155 | Learning Expressionlets via Universal Manifold Model for Dynamic Facial Expression Recognition | http://arxiv.org/pdf/1511.05204v1.pdf | author:Mengyi Liu, Shiguang Shan, Ruiping Wang, Xilin Chen category:cs.CV published:2015-11-16 summary:Facial expression is temporally dynamic event which can be decomposed into aset of muscle motions occurring in different facial regions over various timeintervals. For dynamic expression recognition, two key issues, temporalalignment and semantics-aware dynamic representation, must be taken intoaccount. In this paper, we attempt to solve both problems via manifold modelingof videos based on a novel mid-level representation, i.e.\textbf{expressionlet}. Specifically, our method contains three key stages: 1)each expression video clip is characterized as a spatial-temporal manifold(STM) formed by dense low-level features; 2) a Universal Manifold Model (UMM)is learned over all low-level features and represented as a set of local modesto statistically unify all the STMs. 3) the local modes on each STM can beinstantiated by fitting to UMM, and the corresponding expressionlet isconstructed by modeling the variations in each local mode. With above strategy,expression videos are naturally aligned both spatially and temporally. Toenhance the discriminative power, the expressionlet-based STM representation isfurther processed with discriminant embedding. Our method is evaluated on fourpublic expression databases, CK+, MMI, Oulu-CASIA, and FERA. In all cases, ourmethod outperforms the known state-of-the-art by a large margin.
arxiv-13500-156 | Sparse-promoting Full Waveform Inversion based on Online Orthonormal Dictionary Learning | http://arxiv.org/pdf/1511.05194v1.pdf | author:Lingchen Zhu, Entao Liu, James H. McClellan category:physics.geo-ph cs.LG cs.NA math.NA published:2015-11-16 summary:Full waveform inversion (FWI) delivers high-resolution images of a subsurfacemedium model by minimizing iteratively the least-squares misfit between theobserved and simulated seismic data. Due to the limited accuracy of thestarting model and the inconsistency of the seismic waveform data, the FWIproblem is inherently ill-posed, so that regularization techniques aretypically applied to obtain better models. FWI is also a computationallyexpensive problem because modern seismic surveys cover very large areas ofinterest and collect massive volumes of data. The dimensionality of the problemand the heterogeneity of the medium both stress the need for faster algorithmsand sparse regularization techniques to accelerate and improve imaging results. This paper reaches these goals by developing a compressive sensing approachfor the FWI problem, where the sparsity of model perturbations is exploitedwithin learned dictionaries. Based on stochastic approximations, thedictionaries are updated iteratively to adapt to dynamic model perturbations.Meanwhile, the dictionaries are kept orthonormal in order to maintain thecorresponding transform in a fast and compact manner without introducing extracomputational overhead to FWI. Such a sparsity regularization on modelperturbations enables us to take randomly subsampled data for computation andthus significantly reduce the cost. Compared with other approaches that employsparsity constraints in the fixed curvelet transform domain, our approach canachieve more robust inversion results with better model fit and visual quality.
arxiv-13500-157 | Binary Classifier Calibration using an Ensemble of Near Isotonic Regression Models | http://arxiv.org/pdf/1511.05191v1.pdf | author:Mahdi Pakdaman Naeini, Gregory F. Cooper category:cs.LG stat.ML published:2015-11-16 summary:Learning accurate probabilistic models from data is crucial in many practicaltasks in data mining. In this paper we present a new non-parametric calibrationmethod called \textit{ensemble of near isotonic regression} (ENIR). The methodcan be considered as an extension of BBQ, a recently proposed calibrationmethod, as well as the commonly used calibration method based on isotonicregression. ENIR is designed to address the key limitation of isotonicregression which is the monotonicity assumption of the predictions. Similar toBBQ, the method post-processes the output of a binary classifier to obtaincalibrated probabilities. Thus it can be combined with many existingclassification models. We demonstrate the performance of ENIR on synthetic andreal datasets for the commonly used binary classification models. Experimentalresults show that the method outperforms several common binary classifiercalibration methods. In particular on the real data, ENIR commonly performsstatistically significantly better than the other methods, and never worse. Itis able to improve the calibration power of classifiers, while retaining theirdiscrimination power. The method is also computationally tractable for largescale datasets, as it is $O(N \log N)$ time, where $N$ is the number ofsamples.
arxiv-13500-158 | Neural Adaptive Sequential Monte Carlo | http://arxiv.org/pdf/1506.03338v3.pdf | author:Shixiang Gu, Zoubin Ghahramani, Richard E. Turner category:cs.LG stat.ML published:2015-06-10 summary:Sequential Monte Carlo (SMC), or particle filtering, is a popular class ofmethods for sampling from an intractable target distribution using a sequenceof simpler intermediate distributions. Like other importance sampling-basedmethods, performance is critically dependent on the proposal distribution: abad proposal can lead to arbitrarily inaccurate estimates of the targetdistribution. This paper presents a new method for automatically adapting theproposal using an approximation of the Kullback-Leibler divergence between thetrue posterior and the proposal distribution. The method is very flexible,applicable to any parameterized proposal distribution and it supports onlineand batch variants. We use the new framework to adapt powerful proposaldistributions with rich parameterizations based upon neural networks leading toNeural Adaptive Sequential Monte Carlo (NASMC). Experiments indicate that NASMCsignificantly improves inference in a non-linear state space modeloutperforming adaptive proposal methods including the Extended Kalman andUnscented Particle Filters. Experiments also indicate that improved inferencetranslates into improved parameter learning when NASMC is used as a subroutineof Particle Marginal Metropolis Hastings. Finally we show that NASMC is able totrain a latent variable recurrent neural network (LV-RNN) achieving resultsthat compete with the state-of-the-art for polymorphic music modelling. NASMCcan be seen as bridging the gap between adaptive SMC methods and the recentwork in scalable, black-box variational inference.
arxiv-13500-159 | Cross-scale predictive dictionaries | http://arxiv.org/pdf/1511.05174v1.pdf | author:Vishwanath Saragadam, Aswin Sankaranarayanan, Xin Li category:cs.CV stat.ML published:2015-11-16 summary:We propose a novel signal model, based on sparse representations, thatcaptures cross-scale features for visual signals. We show that cross-scalepredictive model enables faster solutions to sparse approximation problems.This is achieved by first solving the sparse approximation problem for thedownsampled signal and using the support of the solution to constrain thesupport at the original resolution. The speedups obtained are especiallycompelling for high-dimensional signals that require large dictionaries toprovide precise sparse approximations. We demonstrate speedups in the order of10-100x for denoising and up to 15x speedups for compressive sensing of images,videos, hyperspectral images and light-field images.
arxiv-13500-160 | Nonlinear Local Metric Learning for Person Re-identification | http://arxiv.org/pdf/1511.05169v1.pdf | author:Siyuan Huang, Jiwen Lu, Jie Zhou, Anil K. Jain category:cs.CV published:2015-11-16 summary:Person re-identification aims at matching pedestrians observed fromnon-overlapping camera views. Feature descriptor and metric learning are twosignificant problems in person re-identification. A discriminative metriclearning method should be capable of exploiting complex nonlineartransformations due to the large variations in feature space. In this paper, wepropose a nonlinear local metric learning (NLML) method to improve thestate-of-the-art performance of person re-identification on public datasets.Motivated by the fact that local metric learning has been introduced to handlethe data which varies locally and deep neural network has presented outstandingcapability in exploiting the nonlinearity of samples, we utilize the merits ofboth local metric learning and deep neural network to learn multiple sets ofnonlinear transformations. By enforcing a margin between the distances ofpositive pedestrian image pairs and distances of negative pairs in thetransformed feature subspace, discriminative information can be effectivelyexploited in the developed neural networks. Our experiments show that theproposed NLML method achieves the state-of-the-art results on the widely usedVIPeR, GRID, and CUHK 01 datasets.
arxiv-13500-161 | Resolving the Geometric Locus Dilemma for Support Vector Learning Machines | http://arxiv.org/pdf/1511.05102v1.pdf | author:Denise M. Reeves category:cs.LG stat.ML published:2015-11-16 summary:Capacity control, the bias/variance dilemma, and learning unknown functionsfrom data, are all concerned with identifying effective and consistent fits ofunknown geometric loci to random data points. A geometric locus is a curve orsurface formed by points, all of which possess some uniform property. Ageometric locus of an algebraic equation is the set of points whose coordinatesare solutions of the equation. Any given curve or surface must pass througheach point on a specified locus. This paper argues that it is impossible to fitrandom data points to algebraic equations of partially configured geometricloci that reference arbitrary Cartesian coordinate systems. It also argues thatthe fundamental curve of a linear decision boundary is actually a principaleigenaxis. It is shown that learning principal eigenaxes of linear decisionboundaries involves finding a point of statistical equilibrium for whicheigenenergies of principal eigenaxis components are symmetrically balanced witheach other. It is demonstrated that learning linear decision boundariesinvolves strong duality relationships between a statistical eigenlocus ofprincipal eigenaxis components and its algebraic forms, in primal and dual,correlated Hilbert spaces. Locus equations are introduced and developed thatdescribe principal eigen-coordinate systems for lines, planes, and hyperplanes.These equations are used to introduce and develop primal and dual statisticaleigenlocus equations of principal eigenaxes of linear decision boundaries.Important generalizations for linear decision boundaries are shown to beencoded within a dual statistical eigenlocus of principal eigenaxis components.Principal eigenaxes of linear decision boundaries are shown to encode Bayes'likelihood ratio for common covariance data and a robust likelihood ratio forall other data.
arxiv-13500-162 | How (not) to Train your Generative Model: Scheduled Sampling, Likelihood, Adversary? | http://arxiv.org/pdf/1511.05101v1.pdf | author:Ferenc Huszár category:stat.ML cs.AI cs.IT cs.LG math.IT published:2015-11-16 summary:Modern applications and progress in deep learning research have createdrenewed interest for generative models of text and of images. However, eventoday it is unclear what objective functions one should use to train andevaluate these models. In this paper we present two contributions. Firstly, we present a critique of scheduled sampling, a state-of-the-arttraining method that contributed to the winning entry to the MSCOCO imagecaptioning benchmark in 2015. Here we show that despite this impressiveempirical performance, the objective function underlying scheduled sampling isimproper and leads to an inconsistent learning algorithm. Secondly, we revisit the problems that scheduled sampling was meant toaddress, and present an alternative interpretation. We argue that maximumlikelihood is an inappropriate training objective when the end-goal is togenerate natural-looking samples. We go on to derive an ideal objectivefunction to use in this situation instead. We introduce a generalisation ofadversarial training, and show how such method can interpolate between maximumlikelihood training and our ideal training objective. To our knowledge this isthe first theoretical analysis that explains why adversarial training tends toproduce samples with higher perceived quality.
arxiv-13500-163 | Accurate estimation of influenza epidemics using Google search data via ARGO | http://arxiv.org/pdf/1505.00864v2.pdf | author:Shihao Yang, Mauricio Santillana, S. C. Kou category:stat.AP cs.SI stat.ML published:2015-05-05 summary:Accurate real-time tracking of influenza outbreaks helps public healthofficials make timely and meaningful decisions that could save lives. Wepropose an influenza tracking model, ARGO (AutoRegression with GOogle searchdata), that uses publicly available online search data. In addition to having arigorous statistical foundation, ARGO outperforms all previously availableGoogle-search-based tracking models, including the latest version of Google FluTrends, even though it uses only low-quality search data as input from publiclyavailable Google Trends and Google Correlate websites. ARGO not onlyincorporates the seasonality in influenza epidemics but also captures changesin people's online search behavior over time. ARGO is also flexible,self-correcting, robust, and scalable, making it a potentially powerful toolthat can be used for real-time tracking of other social events at multipletemporal and spatial resolutions.
arxiv-13500-164 | Convex Optimization: Algorithms and Complexity | http://arxiv.org/pdf/1405.4980v2.pdf | author:Sébastien Bubeck category:math.OC cs.CC cs.LG cs.NA stat.ML published:2014-05-20 summary:This monograph presents the main complexity theorems in convex optimizationand their corresponding algorithms. Starting from the fundamental theory ofblack-box optimization, the material progresses towards recent advances instructural optimization and stochastic optimization. Our presentation ofblack-box optimization, strongly influenced by Nesterov's seminal book andNemirovski's lecture notes, includes the analysis of cutting plane methods, aswell as (accelerated) gradient descent schemes. We also pay special attentionto non-Euclidean settings (relevant algorithms include Frank-Wolfe, mirrordescent, and dual averaging) and discuss their relevance in machine learning.We provide a gentle introduction to structural optimization with FISTA (tooptimize a sum of a smooth and a simple non-smooth term), saddle-point mirrorprox (Nemirovski's alternative to Nesterov's smoothing), and a concisedescription of interior point methods. In stochastic optimization we discussstochastic gradient descent, mini-batches, random coordinate descent, andsublinear algorithms. We also briefly touch upon convex relaxation ofcombinatorial problems and the use of randomness to round solutions, as well asrandom walks based methods.
arxiv-13500-165 | Topic Modeling of Behavioral Modes Using Sensor Data | http://arxiv.org/pdf/1511.05082v1.pdf | author:Yehezkel S. Resheff, Shay Rotics, Ran Nathan, Daphna Weinshall category:cs.LG published:2015-11-16 summary:The field of Movement Ecology, like so many other fields, is experiencing aperiod of rapid growth in availability of data. As the volume rises,traditional methods are giving way to machine learning and data science, whichare playing an increasingly large part it turning this data intoscience-driving insights. One rich and interesting source is the bio-logger.These small electronic wearable devices are attached to animals free to roam intheir natural habitats, and report back readings from multiple sensors,including GPS and accelerometer bursts. A common use of accelerometer data isfor supervised learning of behavioral modes. However, we need unsupervisedanalysis tools as well, in order to overcome the inherent difficulties ofobtaining a labeled dataset, which in some cases is either infeasible or doesnot successfully encompass the full repertoire of behavioral modes of interest.Here we present a matrix factorization based topic-model method foraccelerometer bursts, derived using a linear mixture property of patchfeatures. Our method is validated via comparison to a labeled dataset, and isfurther compared to standard clustering algorithms.
arxiv-13500-166 | Latent Dirichlet Allocation Based Organisation of Broadcast Media Archives for Deep Neural Network Adaptation | http://arxiv.org/pdf/1511.05076v1.pdf | author:Mortaza Doulaty, Oscar Saz, Raymond W. M. Ng, Thomas Hain category:cs.CL published:2015-11-16 summary:This paper presents a new method for the discovery of latent domains indiverse speech data, for the use of adaptation of Deep Neural Networks (DNNs)for Automatic Speech Recognition. Our work focuses on transcription ofmulti-genre broadcast media, which is often only categorised broadly in termsof high level genres such as sports, news, documentary, etc. However, in termsof acoustic modelling these categories are coarse. Instead, it is expected thata mixture of latent domains can better represent the complex and diversebehaviours within a TV show, and therefore lead to better and more robustperformance. We propose a new method, whereby these latent domains arediscovered with Latent Dirichlet Allocation, in an unsupervised manner. Theseare used to adapt DNNs using the Unique Binary Code (UBIC) representation forthe LDA domains. Experiments conducted on a set of BBC TV broadcasts, with morethan 2,000 shows for training and 47 shows for testing, show that the use ofLDA-UBIC DNNs reduces the error up to 13% relative compared to the baselinehybrid DNN models.
arxiv-13500-167 | An Empirical Study of Recent Face Alignment Methods | http://arxiv.org/pdf/1511.05049v1.pdf | author:Heng Yang, Xuhui Jia, Chen Change Loy, Peter Robinson category:cs.CV published:2015-11-16 summary:The problem of face alignment has been intensively studied in the past years.A large number of novel methods have been proposed and reported very goodperformance on benchmark dataset such as 300W. However, the differences in theexperimental setting and evaluation metric, missing details in the descriptionof the methods make it hard to reproduce the results reported and evaluate therelative merits. For instance, most recent face alignment methods are built ontop of face detection but from different face detectors. In this paper, wecarry out a rigorous evaluation of these methods by making the followingcontributions: 1) we proposes a new evaluation metric for face alignment on aset of images, i.e., area under error distribution curve within a threshold,AUC$_\alpha$, given the fact that the traditional evaluation measure (meanerror) is very sensitive to big alignment error. 2) we extend the 300W databasewith more practical face detections to make fair comparison possible. 3) wecarry out face alignment sensitivity analysis w.r.t. face detection, on bothsynthetic and real data, using both off-the-shelf and re-retrained models. 4)we study factors that are particularly important to achieve good performanceand provide suggestions for practical applications. Most of the conclusionsdrawn from our comparative analysis cannot be inferred from the originalpublications.
arxiv-13500-168 | A Visual Embedding for the Unsupervised Extraction of Abstract Semantics | http://arxiv.org/pdf/1507.08818v3.pdf | author:D. Garcia-Gasulla, J. Béjar, U. Cortés, E. Ayguadé, J. Labarta category:cs.CV cs.LG cs.NE published:2015-07-31 summary:Vector-space word representations obtained from neural network models havebeen shown to enable semantic operations based on vector arithmetic. In thispaper, we explore the existence of similar information on vectorrepresentations of images. For that purpose we define a methodology to obtainlarge, sparse vector representations of image classes, and generate vectorsthrough the state-of-the-art deep learning architecture GoogLeNet for 20Kimages obtained from ImageNet. We first evaluate the resultant vector-spacesemantics through its correlation with WordNet distances, and find vectordistances to be strongly correlated with linguistic semantics. We then explorethe location of images within the vector space, finding elements close inWordNet to be clustered together, regardless of significant visual variances(e.g., 118 dog types). More surprisingly, we find that the space unsupervisedlyseparates complex classes without prior knowledge (e.g., living things).Finally, we consider vector arithmetics, and find them to be related with imageconcatenation (e.g., "Horse cart - Horse = Rickshaw"), image overlap ("Panda -Brown bear = Skunk") and regularities ("Panda is to Brown bear as Soccer ballis to Helmet"). These results indicate that image vector embeddings as the oneproposed here contain rich visual semantics usable for learning and reasoningpurposes.
arxiv-13500-169 | Learning Spanish dialects through Twitter | http://arxiv.org/pdf/1511.04970v1.pdf | author:Bruno Gonçalves, David Sánchez category:stat.ML cs.CL cs.CY physics.soc-ph stat.AP published:2015-11-16 summary:We map the large-scale variation of the Spanish language by employing acorpus based on geographically tagged Twitter messages. Lexical dialects areextracted from an analysis of variants of tens of concepts. The resulting mapsshow linguistic variations on an unprecedented scale across the globe. Wediscuss the properties of the main dialects within a machine learning approachand find that varieties spoken in urban areas have an international characterin contrast to country areas where dialects show a more regional uniformity.
arxiv-13500-170 | Performance Analysis of Multiclass Support Vector Machine Classification for Diagnosis of Coronary Heart Diseases | http://arxiv.org/pdf/1511.02352v2.pdf | author:Wiharto Wiharto, Hari Kusnanto, Herianto Herianto category:cs.LG published:2015-11-07 summary:Automatic diagnosis of coronary heart disease helps the doctor to support indecision making a diagnosis. Coronary heart disease have some types or levels.Referring to the UCI Repository dataset, it divided into 4 types or levels thatare labeled numbers 1-4 (low, medium, high and serious). The diagnosis modelscan be analyzed with multiclass classification approach. One of multiclassclassification approach used, one of which is a support vector machine (SVM).The SVM use due to strong performance of SVM in binary classification. Thisresearch study multiclass performance classification support vector machine todiagnose the type or level of coronary heart disease. Coronary heart diseasepatient data taken from the UCI Repository. Stages in this study ispreprocessing, which consist of, to normalizing the data, divide the data intodata training and testing. The next stage of multiclass classification andperformance analysis. This study uses multiclass SVM algorithm, namely: BinaryTree Support Vector Machine (BTSVM), One-Against-One (OAO), One-Against-All(OAA), Decision Direct Acyclic Graph (DDAG) and Exhaustive Output ErrorCorrection Code (ECOC). Performance parameter used is recall, precision,F-measure and Overall accuracy.
arxiv-13500-171 | Learning visual biases from human imagination | http://arxiv.org/pdf/1410.4627v2.pdf | author:Carl Vondrick, Hamed Pirsiavash, Aude Oliva, Antonio Torralba category:cs.CV published:2014-10-17 summary:Although the human visual system can recognize many concepts underchallenging conditions, it still has some biases. In this paper, we investigatewhether we can extract these biases and transfer them into a machinerecognition system. We introduce a novel method that, inspired by well-knowntools in human psychophysics, estimates the biases that the human visual systemmight use for recognition, but in computer vision feature spaces. Ourexperiments are surprising, and suggest that classifiers from the human visualsystem can be transferred into a machine with some success. Since theseclassifiers seem to capture favorable biases in the human visual system, wefurther present an SVM formulation that constrains the orientation of the SVMhyperplane to agree with the bias from human visual system. Our results suggestthat transferring this human bias into machines may help object recognitionsystems generalize across datasets and perform better when very little trainingdata is available.
arxiv-13500-172 | Mini-Batch Semi-Stochastic Gradient Descent in the Proximal Setting | http://arxiv.org/pdf/1504.04407v2.pdf | author:Jakub Konečný, Jie Liu, Peter Richtárik, Martin Takáč category:cs.LG stat.ML published:2015-04-16 summary:We propose mS2GD: a method incorporating a mini-batching scheme for improvingthe theoretical complexity and practical performance of semi-stochasticgradient descent (S2GD). We consider the problem of minimizing a stronglyconvex function represented as the sum of an average of a large number ofsmooth convex functions, and a simple nonsmooth convex regularizer. Our methodfirst performs a deterministic step (computation of the gradient of theobjective function at the starting point), followed by a large number ofstochastic steps. The process is repeated a few times with the last iteratebecoming the new starting point. The novelty of our method is in introductionof mini-batching into the computation of stochastic steps. In each step,instead of choosing a single function, we sample $b$ functions, compute theirgradients, and compute the direction based on this. We analyze the complexityof the method and show that it benefits from two speedup effects. First, weprove that as long as $b$ is below a certain threshold, we can reach anypredefined accuracy with less overall work than without mini-batching. Second,our mini-batching scheme admits a simple parallel implementation, and hence issuitable for further acceleration by parallelization.
arxiv-13500-173 | Identification and Counting White Blood Cells and Red Blood Cells using Image Processing Case Study of Leukemia | http://arxiv.org/pdf/1511.04934v1.pdf | author:Esti Suryani, Wiharto Wiharto, Nizomjon Polvonov category:cs.CV published:2015-11-16 summary:Leukemia is diagnosed with complete blood counts which is by calculating allblood cells and compare the number of white blood cells (White Blood Cells /WBC) and red blood cells (Red Blood Cells / RBC). Information obtained from acomplete blood count, has become a cornerstone in the hematology laboratory fordiagnostic purposes and monitoring of hematological disorders. However, thetraditional procedure for counting blood cells manually requires effort and along time, therefore this method is one of the most expensive routine tests inlaboratory hematology clinic. Solution for such kind of time consuming task andnecessity of data tracability can be found in image processing techniques basedon blood cell morphology . This study aims to identify Acute LymphocyticLeukemia (ALL) and Acute Myeloid Leukemia type M3 (AML M3) using Fuzzy RuleBased System based on morphology of white blood cells. Characteristicparameters witch extractedare WBC Area, Nucleus and Granule Ratio of whiteblood cells. Image processing algorithms such as thresholding, Canny edgedetection and color identification filters are used.Then for identification ofALL, AML M3 and Healthy cells used Fuzzy Rule Based System with Sugeno method.In the testing process used 104 images out of which 29 ALL - Positive, 50 AMLM3 - Positive and 25 Healthy cells. Test results showed 83.65 % accuracy .
arxiv-13500-174 | Robust Gaussian Filtering | http://arxiv.org/pdf/1509.04072v2.pdf | author:Manuel Wüthrich, Cristina Garcia Cifuentes, Sebastian Trimpe, Franziska Meier, Jeannette Bohg, Jan Issac, Stefan Schaal category:stat.ML cs.SY published:2015-09-14 summary:Most widely-used state estimation algorithms, such as the Extended KalmanFilter and the Unscented Kalman Filter, belong to the family of GaussianFilters (GF). Unfortunately, GFs fail if the measurement process is modelled bya fat-tailed distribution. This is a severe limitation, because thin-tailedmeasurement models, such as the analytically-convenient and thereforewidely-used Gaussian distribution, are sensitive to outliers. In this paper, weshow that mapping the measurements into a specific feature space enables anyexisting GF algorithm to work with fat-tailed measurement models. We find afeature function which is optimal under certain conditions. Simulation resultsshow that the proposed method allows for robust filtering in both linear andnonlinear systems with measurements contaminated by fat-tailed noise.
arxiv-13500-175 | Graph-based denoising for time-varying point clouds | http://arxiv.org/pdf/1511.04902v1.pdf | author:Yann Schoenenberger, Johan Paratte, Pierre Vandergheynst category:cs.CV cs.GR I.5.4 published:2015-11-16 summary:Noisy 3D point clouds arise in many applications. They may be due to errorswhen constructing a 3D model from images or simply to imprecise depth sensors.Point clouds can be given geometrical structure using graphs created from thesimilarity information between points. This paper introduces a technique thatuses this graph structure and convex optimization methods to denoise 3D pointclouds. A short discussion presents how those methods naturally generalize totime-varying inputs such as 3D point cloud time series.
arxiv-13500-176 | Coarse-to-fine Face Alignment with Multi-Scale Local Patch Regression | http://arxiv.org/pdf/1511.04901v1.pdf | author:Zhiao Huang, Erjin Zhou, Zhimin Cao category:cs.CV published:2015-11-16 summary:Facial landmark localization plays an important role in face recognition andanalysis applications. In this paper, we give a brief introduction to acoarse-to-fine pipeline with neural networks and sequential regression. First,a global convolutional network is applied to the holistic facial image to givean initial landmark prediction. A pyramid of multi-scale local image patches isthen cropped to feed to a new network for each landmark to refine theprediction. As the refinement network outputs a more accurate positionestimation than the input, such procedure could be repeated several times untilthe estimation converges. We evaluate our system on the 300-W dataset [11] andit outperforms the recent state-of-the-arts.
arxiv-13500-177 | Fast clustering for scalable statistical analysis on structured images | http://arxiv.org/pdf/1511.04898v1.pdf | author:Bertrand Thirion, Andrés Hoyos-Idrobo, Jonas Kahn, Gael Varoquaux category:stat.ML cs.CV published:2015-11-16 summary:The use of brain images as markers for diseases or behavioral differences ischallenged by the small effects size and the ensuing lack of power, an issuethat has incited researchers to rely more systematically on large cohorts.Coupled with resolution increases, this leads to very large datasets. Astriking example in the case of brain imaging is that of the Human ConnectomeProject: 20 Terabytes of data and growing. The resulting data deluge posessevere challenges regarding the tractability of some processing steps(discriminant analysis, multivariate models) due to the memory demands posed bythese data. In this work, we revisit dimension reduction approaches, such asrandom projections, with the aim of replacing costly function evaluations bycheaper ones while decreasing the memory requirements. Specifically, weinvestigate the use of alternate schemes, based on fast clustering, that arewell suited for signals exhibiting a strong spatial structure, such asanatomical and functional brain images. Our contribution is twofold: i) wepropose a linear-time clustering scheme that bypasses the percolation issuesinherent in these algorithms and thus provides compressions nearly as good astraditional quadratic-complexity variance-minimizing clustering schemes, ii) weshow that cluster-based compression can have the virtuous effect of removinghigh-frequency noise, actually improving subsequent estimations steps. As aconsequence, the proposed approach yields very accurate models on severallarge-scale problems yet with impressive gains in computational efficiency,making it possible to analyze large datasets.
arxiv-13500-178 | Active Contextual Entropy Search | http://arxiv.org/pdf/1511.04211v2.pdf | author:Jan Hendrik Metzen category:stat.ML cs.LG published:2015-11-13 summary:Contextual policy search allows adapting robotic movement primitives todifferent situations. For instance, a locomotion primitive might be adapted todifferent terrain inclinations or desired walking speeds. Such an adaptation isoften achievable by modifying a small number of hyperparameters. However,learning, when performed on real robotic systems, is typically restricted to asmall number of trials. Bayesian optimization has recently been proposed as asample-efficient means for contextual policy search that is well suited underthese conditions. In this work, we extend entropy search, a variant of Bayesianoptimization, such that it can be used for active contextual policy searchwhere the agent selects those tasks during training in which it expects tolearn the most. Empirical results in simulation suggest that this allowslearning successful behavior with less trials.
arxiv-13500-179 | Higher-order Segmentation via Multicuts | http://arxiv.org/pdf/1305.6387v3.pdf | author:Joerg Hendrik Kappes, Markus Speth, Gerhard Reinelt, Christoph Schnoerr category:cs.CV published:2013-05-28 summary:Multicuts enable to conveniently represent discrete graphical models forunsupervised and supervised image segmentation, in the case of local energyfunctions that exhibit symmetries. The basic Potts model and natural extensionsthereof to higher-order models provide a prominent class of such objectives,that cover a broad range of segmentation problems relevant to image analysisand computer vision. We exhibit a way to systematically take into account suchhigher-order terms for computational inference. Furthermore, we present resultsof a comprehensive and competitive numerical evaluation of a variety ofdedicated cutting-plane algorithms. Our approach enables the globally optimalevaluation of a significant subset of these models, without compromisingruntime. Polynomially solvable relaxations are studied as well, along withadvanced rounding schemes for post-processing.
arxiv-13500-180 | Analyzing Stability of Convolutional Neural Networks in the Frequency Domain | http://arxiv.org/pdf/1511.03042v2.pdf | author:Elnaz J. Heravi, Hamed H. Aghdam, Domenec Puig category:cs.CV published:2015-11-10 summary:Understanding the internal process of ConvNets is commonly done usingvisualization techniques. However, these techniques do not usually provide atool for estimating the stability of a ConvNet against noise. In this paper, weshow how to analyze a ConvNet in the frequency domain using a 4-dimensionalvisualization technique. Using the frequency domain analysis, we show thereason that a ConvNet might be sensitive to a very low magnitude additivenoise. Our experiments on a few ConvNets trained on different datasets revealedthat convolution kernels of a trained ConvNet usually pass most of thefrequencies and they are not able to effectively eliminate the effect of highfrequencies. Our next experiments shows that a convolution kernel which has amore concentrated frequency response could be more stable. Finally, we showthat fine-tuning a ConvNet using a training set augmented with noisy images canproduce more stable ConvNets.
arxiv-13500-181 | Deep Learning for steganalysis is better than a Rich Model with an Ensemble Classifier, and is natively robust to the cover source-mismatch | http://arxiv.org/pdf/1511.04855v1.pdf | author:Lionel Pibre, Pasquet Jérôme, Dino Ienco, Marc Chaumont category:cs.MM cs.CV cs.LG cs.NE published:2015-11-16 summary:Since the BOSS competition, in 2010, most steganalysis approaches use alearning methodology involving two steps: feature extraction, such as the RichModels (RM), for the image representation, and use of the Ensemble Classifier(EC) for the learning step. In 2015, Qian et al. have shown that the use of adeep learning approach that jointly learns and computes the features, is verypromising for the steganalysis. In this paper, we follow-up the study of Qianet al., and show that, due to intrinsic joint minimization, the resultsobtained from a Convolutional Neural Network (CNN) or a Fully Connected NeuralNetwork (FNN), if well parameterized, surpass the conventional use of a RM withan EC. First, numerous experiments were conducted in order to find the best "shape " of the CNN. Second, experiments were carried out in the clairvoyantscenario in order to compare the CNN and FNN to an RM with an EC. The resultsshow more than 16% reduction in the classification error with our CNN or FNN.Third, experiments were also performed in a cover-source mismatch setting. Theresults show that the CNN and FNN are naturally robust to the mismatch problem.In Addition to the experiments, we provide discussions on the internalmechanisms of a CNN, and weave links with some previously stated ideas, inorder to understand the impressive results we obtained.
arxiv-13500-182 | Probabilistic Segmentation via Total Variation Regularization | http://arxiv.org/pdf/1511.04817v1.pdf | author:Matt Wytock, J. Zico Kolter category:stat.ML published:2015-11-16 summary:We present a convex approach to probabilistic segmentation and modeling oftime series data. Our approach builds upon recent advances in multivariatetotal variation regularization, and seeks to learn a separate set of parametersfor the distribution over the observations at each time point, but with anadditional penalty that encourages the parameters to remain constant over time.We propose efficient optimization methods for solving the resulting (large)optimization problems, and a two-stage procedure for estimating recurringclusters under such models, based upon kernel density estimation. Finally, weshow on a number of real-world segmentation tasks, the resulting methods oftenperform as well or better than existing latent variable models, while beingsubstantially easier to train.
arxiv-13500-183 | Detecting Interrogative Utterances with Recurrent Neural Networks | http://arxiv.org/pdf/1511.01042v2.pdf | author:Junyoung Chung, Jacob Devlin, Hany Hassan Awadalla category:cs.CL cs.LG cs.NE published:2015-11-03 summary:In this paper, we explore different neural network architectures that canpredict if a speaker of a given utterance is asking a question or making astatement. We com- pare the outcomes of regularization methods that arepopularly used to train deep neural networks and study how different contextfunctions can affect the classification performance. We also compare theefficacy of gated activation functions that are favorably used in recurrentneural networks and study how to combine multimodal inputs. We evaluate ourmodels on two multimodal datasets: MSR-Skype and CALLHOME.
arxiv-13500-184 | Learning Mid-level Words on Riemannian Manifold for Action Recognition | http://arxiv.org/pdf/1511.04808v1.pdf | author:Mengyi Liu, Ruiping Wang, Shiguang Shan, Xilin Chen category:cs.CV published:2015-11-16 summary:Human action recognition remains a challenging task due to the varioussources of video data and large intra-class variations. It thus becomes one ofthe key issues in recent research to explore effective and robustrepresentation to handle such challenges. In this paper, we propose a novelrepresentation approach by constructing mid-level words in videos and encodingthem on Riemannian manifold. Specifically, we first conduct a global alignmenton the densely extracted low-level features to build a bank of correspondingfeature groups, each of which can be statistically modeled as a mid-level wordlying on some specific Riemannian manifold. Based on these mid-level words, weconstruct intrinsic Riemannian codebooks by employing K-Karcher-meansclustering and Riemannian Gaussian Mixture Model, and consequently extend theRiemannian manifold version of three well studied encoding methods in Euclideanspace, i.e. Bag of Visual Words (BoVW), Vector of Locally AggregatedDescriptors (VLAD), and Fisher Vector (FV), to obtain the final action videorepresentations. Our method is evaluated in two tasks on four popular realisticdatasets: action recognition on YouTube, UCF50, HMDB51 databases, and actionsimilarity labeling on ASLAN database. In all cases, the reported resultsachieve very competitive performance with those most recent state-of-the-artworks.
arxiv-13500-185 | Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization | http://arxiv.org/pdf/1511.04798v1.pdf | author:Baohan Xu, Yanwei Fu, Yu-Gang Jiang, Boyang Li, Leonid Sigal category:cs.CV cs.AI cs.MM published:2015-11-16 summary:Emotional content is a key element in user-generated videos. However, it isdifficult to understand emotions conveyed in such videos due to the complex andunstructured nature of user-generated content and the sparsity of video framesthat express emotion. In this paper, for the first time, we study the problemof transferring knowledge from heterogeneous external sources, including imageand textual data, to facilitate three related tasks in video emotionunderstanding: emotion recognition, emotion attribution and emotion-orientedsummarization. Specifically, our framework (1) learns a video encoding from anauxiliary emotional image dataset in order to improve supervised video emotionrecognition, and (2) transfers knowledge from an auxiliary textual corpus forzero-shot \pl{recognition} of emotion classes unseen during training. Theproposed technique for knowledge transfer facilitates novel applications ofemotion attribution and emotion-oriented summarization. A comprehensive set ofexperiments on multiple datasets demonstrate the effectiveness of ourframework.
arxiv-13500-186 | Causal interpretation rules for encoding and decoding models in neuroimaging | http://arxiv.org/pdf/1511.04780v1.pdf | author:Sebastian Weichwald, Timm Meyer, Ozan Özdenizci, Bernhard Schölkopf, Tonio Ball, Moritz Grosse-Wentrup category:stat.ML cs.LG q-bio.NC stat.AP published:2015-11-15 summary:Causal terminology is often introduced in the interpretation of encoding anddecoding models trained on neuroimaging data. In this article, we investigatewhich causal statements are warranted and which ones are not supported byempirical evidence. We argue that the distinction between encoding and decodingmodels is not sufficient for this purpose: relevant features in encoding anddecoding models carry a different meaning in stimulus- and in response-basedexperimental paradigms. We show that only encoding models in the stimulus-basedsetting support unambiguous causal interpretations. By combining encoding anddecoding models trained on the same data, however, we obtain insights intocausal relations beyond those that are implied by each individual model type.We illustrate the empirical relevance of our theoretical findings on EEG datarecorded during a visuo-motor learning task.
arxiv-13500-187 | Expressive recommender systems through normalized nonnegative models | http://arxiv.org/pdf/1511.04775v1.pdf | author:Cyril Stark category:cs.LG stat.ML published:2015-11-15 summary:We introduce normalized nonnegative models (NNM) for explorative dataanalysis. NNMs are partial convexifications of models from probability theory.We demonstrate their value at the example of item recommendation. We show thatNNM-based recommender systems satisfy three criteria that all recommendersystems should ideally satisfy: high predictive power, computationaltractability, and expressive representations of users and items. Expressiveuser and item representations are important in practice to succinctly summarizethe pool of customers and the pool of items. In NNMs, user representations areexpressive because each user's preference can be regarded as normalized mixtureof preferences of stereotypical users. The interpretability of item and userrepresentations allow us to arrange properties of items (e.g., genres of moviesor topics of documents) or users (e.g., personality traits) hierarchically.
arxiv-13500-188 | An Iterative Reweighted Method for Tucker Decomposition of Incomplete Multiway Tensors | http://arxiv.org/pdf/1511.04695v1.pdf | author:Linxiao Yang, Jun Fang, Hongbin Li, Bing Zeng category:cs.NA cs.LG published:2015-11-15 summary:We consider the problem of low-rank decomposition of incomplete multiwaytensors. Since many real-world data lie on an intrinsically low dimensionalsubspace, tensor low-rank decomposition with missing entries has applicationsin many data analysis problems such as recommender systems and imageinpainting. In this paper, we focus on Tucker decomposition which represents anNth-order tensor in terms of N factor matrices and a core tensor viamultilinear operations. To exploit the underlying multilinear low-rankstructure in high-dimensional datasets, we propose a group-based log-sumpenalty functional to place structural sparsity over the core tensor, whichleads to a compact representation with smallest core tensor. The method forTucker decomposition is developed by iteratively minimizing a surrogatefunction that majorizes the original objective function, which results in aniterative reweighted process. In addition, to reduce the computationalcomplexity, an over-relaxed monotone fast iterative shrinkage-thresholdingtechnique is adapted and embedded in the iterative reweighted process. Theproposed method is able to determine the model complexity (i.e. multilinearrank) in an automatic way. Simulation results show that the proposed algorithmoffers competitive performance compared with other existing algorithms.
arxiv-13500-189 | Separation Surfaces in the Spectral TV Domain for Texture Decomposition | http://arxiv.org/pdf/1511.04687v1.pdf | author:Dikla Horesh, Guy Gilboa category:cs.CV math.SP published:2015-11-15 summary:In this paper we introduce a novel notion of separation surfaces for imagedecomposition. A surface is embedded in the spectral total-variation (TV) threedimensional domain and encodes a spatially-varying separation scale. The methodallows good separation of textures with gradually varying pattern-size,pattern-contrast or illumination. The recently proposed total variationspectral framework is used to decompose the image into a continuum of texturalscales. A desired texture, within a scale range, is found by fitting a surfaceto the local maximal responses in the spectral domain. A band above and belowthe surface, referred to as the \textit{Texture Stratum}, defines for eachpixel the adaptive scale-range of the texture. Based on the decomposition anapplication is proposed which can attenuate or enhance textures in the image ina very natural and visually convincing manner.
arxiv-13500-190 | Semi-Inner-Products for Convex Functionals and Their Use in Image Decomposition | http://arxiv.org/pdf/1511.04685v1.pdf | author:Guy Gilboa category:math.NA cs.CV math.SP published:2015-11-15 summary:Semi-inner-products in the sense of Lumer are extended to convex functionals.This yields a Hilbert-space like structure to convex functionals in Banachspaces. In particular, a general expression for semi-inner-products withrespect to one homogeneous functionals is given. Thus one can use the newoperator for the analysis of total variation and higher order functionals liketotal-generalized-variation (TGV). Having a semi-inner-product, an anglebetween functions can be defined in a straightforward manner. It is shown thatin the one homogeneous case the Bregman distance can be expressed in terms ofthis newly defined angle. In addition, properties of the semi-inner-product ofnonlinear eigenfunctions induced by the functional are derived. We use thisconstruction to state a sufficient condition for a perfect decomposition of twosignals and suggest numerical measures which indicate when those conditions areapproximately met.
arxiv-13500-191 | Breaking Sticks and Ambiguities with Adaptive Skip-gram | http://arxiv.org/pdf/1502.07257v2.pdf | author:Sergey Bartunov, Dmitry Kondrashkin, Anton Osokin, Dmitry Vetrov category:cs.CL published:2015-02-25 summary:Recently proposed Skip-gram model is a powerful method for learninghigh-dimensional word representations that capture rich semantic relationshipsbetween words. However, Skip-gram as well as most prior work on learning wordrepresentations does not take into account word ambiguity and maintain onlysingle representation per word. Although a number of Skip-gram modificationswere proposed to overcome this limitation and learn multi-prototype wordrepresentations, they either require a known number of word meanings or learnthem using greedy heuristic approaches. In this paper we propose the AdaptiveSkip-gram model which is a nonparametric Bayesian extension of Skip-gramcapable to automatically learn the required number of representations for allwords at desired semantic resolution. We derive efficient online variationallearning algorithm for the model and empirically demonstrate its efficiency onword-sense induction task.
arxiv-13500-192 | Uncovering Temporal Context for Video Question and Answering | http://arxiv.org/pdf/1511.04670v1.pdf | author:Linchao Zhu, Zhongwen Xu, Yi Yang, Alexander G. Hauptmann category:cs.CV published:2015-11-15 summary:In this work, we introduce Video Question Answering in temporal domain toinfer the past, describe the present and predict the future. We present anencoder-decoder approach using Recurrent Neural Networks to learn temporalstructures of videos and introduce a dual-channel ranking loss to answermultiple-choice questions. We explore approaches for finer understanding ofvideo content using question form of "fill-in-the-blank", and managed tocollect 109,895 video clips with duration over 1,000 hours from TACoS, MPII-MD,MEDTest 14 datasets, while the corresponding 390,744 questions are generatedfrom annotations. Extensive experiments demonstrate that our approachsignificantly outperforms the compared baselines.
arxiv-13500-193 | Deep Activity Recognition Models with Triaxial Accelerometers | http://arxiv.org/pdf/1511.04664v1.pdf | author:Mohammad Abu Alsheikh, Ahmed Selim, Dusit Niyato, Linda Doyle, Shaowei Lin, Hwee-Pink Tan category:cs.LG cs.HC cs.NE published:2015-11-15 summary:Despite the widespread installation of accelerometers in almost all mobilephones and wearable devices, activity recognition using accelerometers is stillimmature due to the poor recognition accuracy of existing recognition methodsand the scarcity of labeled training data. We consider the problem of humanactivity recognition using triaxial accelerometers and deep learning paradigms.This paper shows that deep activity recognition models (a) provide betterrecognition accuracy of human activities, (b) avoid the expensive design ofhandcrafted features in existing systems, and (c) utilize the massive unlabeledacceleration samples for unsupervised feature extraction. Moreover, a hybridapproach of deep learning and hidden Markov models (DL-HMM) is presented forsequential activity recognition. This hybrid approach integrates thehierarchical representations of deep activity recognition models with thestochastic modeling of temporal sequences in the hidden Markov models. We showsubstantial recognition improvement on real world datasets overstate-of-the-art methods of human activity recognition using triaxialaccelerometers.
arxiv-13500-194 | A System for Extracting Sentiment from Large-Scale Arabic Social Data | http://arxiv.org/pdf/1511.04661v1.pdf | author:Hao Wang, Vijay R. Bommireddipalli, Ayman Hanafy, Mohamed Bahgat, Sara Noeman, Ossama S. Emam category:cs.CL published:2015-11-15 summary:Social media data in Arabic language is becoming more and more abundant. Itis a consensus that valuable information lies in social media data. Mining thisdata and making the process easier are gaining momentum in the industries. Thispaper describes an enterprise system we developed for extracting sentiment fromlarge volumes of social data in Arabic dialects. First, we give an overview ofthe Big Data system for information extraction from multilingual social datafrom a variety of sources. Then, we focus on the Arabic sentiment analysiscapability that was built on top of the system including normalizing writtenArabic dialects, building sentiment lexicons, sentiment classification, andperformance evaluation. Lastly, we demonstrate the value of enriching sentimentresults with user profiles in understanding sentiments of a specific usergroup.
arxiv-13500-195 | Implementation and comparative quantitative assessment of different multispectral image pansharpening approches | http://arxiv.org/pdf/1511.04659v1.pdf | author:Shailesh Panchal, Rajesh Thakker category:cs.CV published:2015-11-15 summary:In remote sensing, images acquired by various earth observation satellitestend to have either a high spatial and low spectral resolution or vice versa.Pansharpening is a technique which aims to improve spatial resolution ofmultispectral image. The challenges involve in the pansharpening are not onlyto improve the spatial resolution but also to preserve spectral quality of themultispectral image. In this paper, various pansharpening algorithms arediscussed and classified based on approaches they have adopted. Using MATLABimage processing toolbox, several state-of-art pan-sharpening algorithms areimplemented. Quality of pansharpened images are assessed visually andquantitatively. Correlation coefficient (CC), Root mean square error (RMSE),Relative average spectral error (RASE) and Universal quality index (Q) indicesare used to easure spectral quality while to spatial-CC (SCC) quantitativeparameter is used for spatial quality measurement. Finally, the paper isconcluded with useful remarks.
arxiv-13500-196 | Word Embedding based Correlation Model for Question/Answer Matching | http://arxiv.org/pdf/1511.04646v1.pdf | author:Yikang Shen, Wenge Rong, Nan Jiang, Baolin Peng, Jie Tang, Zhang Xiong category:cs.CL cs.AI published:2015-11-15 summary:With the development of community based question answering (Q\&A) services, alarge scale of Q\&A archives have been accumulated and are an importantinformation and knowledge resource on the web. Question and answer matching hasbeen attached much importance to for its ability to reuse knowledge stored inthese systems: it can be useful in enhancing user experience with recurrentquestions. In this paper, we try to improve the matching accuracy by overcomingthe lexical gap between question and answer pairs. A Word Embedding basedCorrelation (WEC) model is proposed by integrating advantages of both thetranslation model and word embedding, given a random pair of words, WEC canscore their co-occurrence probability in Q\&A pairs and it can also leveragethe continuity and smoothness of continuous space word representation to dealwith new pairs of words that are rare in the training parallel text. Anexperimental study on Yahoo! Answers dataset and Baidu Zhidao dataset showsthis new method's promising potential.
arxiv-13500-197 | Accurate Image Super-Resolution Using Very Deep Convolutional Networks | http://arxiv.org/pdf/1511.04587v1.pdf | author:Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee category:cs.CV published:2015-11-14 summary:We present a highly accurate single-image super-resolution (SR) method. Ourmethod uses a very deep convolutional network inspired by VGG-net used forImageNet classification \cite{simonyan2015very}. We find increasing our networkdepth shows a significant improvement in accuracy. Our final model uses 20weight layers. By cascading small filters many times in a deep networkstructure, contextual information over large image regions is exploited in anefficient way. With very deep networks, however, convergence speed becomes acritical issue during training. We propose a simple yet effective trainingprocedure. We learn residuals only and use extremely high learning rates($10^4$ times higher than SRCNN \cite{dong2015image}) enabled by adjustablegradient clipping. Our proposed method performs better than existing methods inaccuracy and visual improvements in our results are easily noticeable.
arxiv-13500-198 | Character-based Neural Machine Translation | http://arxiv.org/pdf/1511.04586v1.pdf | author:Wang Ling, Isabel Trancoso, Chris Dyer, Alan W Black category:cs.CL published:2015-11-14 summary:We introduce a neural machine translation model that views the input andoutput sentences as sequences of characters rather than words. Since word-levelinformation provides a crucial source of bias, our input model composesrepresentations of character sequences into representations of words (asdetermined by whitespace boundaries), and then these are translated using ajoint attention/translation model. In the target language, the translation ismodeled as a sequence of word vectors, but each word is generated one characterat a time, conditional on the previous character generations in each word. Asthe representation and generation of words is performed at the character level,our model is capable of interpreting and generating unseen word forms. Asecondary benefit of this approach is that it alleviates much of the challengesassociated with preprocessing/tokenization of the source and target languages.We show that our model can achieve translation results that are on par withconventional word-based models.
arxiv-13500-199 | Fast Proximal Linearized Alternating Direction Method of Multiplier with Parallel Splitting | http://arxiv.org/pdf/1511.05133v1.pdf | author:Canyi Lu, Huan Li, Zhouchen Lin, Shuicheng Yan category:math.OC cs.LG cs.NA published:2015-11-14 summary:The Augmented Lagragian Method (ALM) and Alternating Direction Method ofMultiplier (ADMM) have been powerful optimization methods for general convexprogramming subject to linear constraint. We consider the convex problem whoseobjective consists of a smooth part and a nonsmooth but simple part. We proposethe Fast Proximal Augmented Lagragian Method (Fast PALM) which achieves theconvergence rate $O(1/K^2)$, compared with $O(1/K)$ by the traditional PALM. Inorder to further reduce the per-iteration complexity and handle themulti-blocks problem, we propose the Fast Proximal ADMM with Parallel Splitting(Fast PL-ADMM-PS) method. It also partially improves the rate related to thesmooth part of the objective function. Experimental results on both synthesizedand real world data demonstrate that our fast methods significantly improve theprevious PALM and ADMM.
arxiv-13500-200 | Learning Fine-grained Features via a CNN Tree for Large-scale Classification | http://arxiv.org/pdf/1511.04534v1.pdf | author:Zhenhua Wang, Xingxing Wang, Gang Wang category:cs.CV published:2015-11-14 summary:We propose a novel approach to enhance the discriminability of ConvolutionalNeural Networks (CNN). The key idea is to build a tree structure that couldprogressively learn fine-grained features to distinguish a subset of classes,by learning features only among these classes. Such features are expected to bemore discriminative, compared to features learned for all the classes. Wedevelop a new algorithm to effectively learn the tree structure among a largenumber of classes. Experiments on large-scale image classification tasksdemonstrate that our method could boost the performance of a given basic CNNmodel. Our method is quite general, hence it can potentially be used incombination with many other deep learning models.
arxiv-13500-201 | Sparse Nonlinear Regression: Parameter Estimation and Asymptotic Inference | http://arxiv.org/pdf/1511.04514v1.pdf | author:Zhuoran Yang, Zhaoran Wang, Han Liu, Yonina C. Eldar, Tong Zhang category:stat.ML cs.IT cs.LG math.IT math.OC published:2015-11-14 summary:We study parameter estimation and asymptotic inference for sparse nonlinearregression. More specifically, we assume the data are given by $y = f( x^\top\beta^* ) + \epsilon$, where $f$ is nonlinear. To recover $\beta^*$, we proposean $\ell_1$-regularized least-squares estimator. Unlike classical linearregression, the corresponding optimization problem is nonconvex because of thenonlinearity of $f$. In spite of the nonconvexity, we prove that under mildconditions, every stationary point of the objective enjoys an optimalstatistical rate of convergence. In addition, we provide an efficient algorithmthat provably converges to a stationary point. We also access the uncertaintyof the obtained estimator. Specifically, based on any stationary point of theobjective, we construct valid hypothesis tests and confidence intervals for thelow dimensional components of the high-dimensional parameter $\beta^*$.Detailed numerical results are provided to back up our theory.
arxiv-13500-202 | BING++: A Fast High Quality Object Proposal Generator at 100fps | http://arxiv.org/pdf/1511.04511v1.pdf | author:Ziming Zhang, Yun Liu, Tolga Bolukbasi, Ming-Ming Cheng, Venkatesh Saligrama category:cs.CV published:2015-11-14 summary:We are motivated by the need for an object proposal generation algorithm thatachieves a good balance between proposal localization quality, object recalland computational efficiency. We propose a novel object proposal algorithm {\emBING++} which inherits the good computational efficiency of BING\cite{BingObj2014} but significantly improves its proposal localizationquality. Central to our success is based on the observation that good boundingboxes are those that tightly cover objects. Edge features, which can becomputed efficiently, play a critical role in this context. We propose a newalgorithm that recursively improves BING's proposals by exploiting the factthat edges in images are typically associated with object boundaries. BING++improves proposals recursively by incorporating nearest edge points (toproposal boundary pixels) to obtain a tighter bounding box. This operation haslinear computational complexity in number of pixels and can be done efficientlyusing distance transform. Superpixel merging techniques are then employed aspost-processing to further improve the proposal quality. Empirically on theVOC2007 dataset, using $10^3$ proposals and IoU threshold 0.5, our methodachieves 95.3\% object detection recall (DR), 79.2\% mean average best overlap(MABO), and 68.7\% mean average precision (mAP) on object detection over 20object classes within an average time of {\bf 0.009} seconds per image.
arxiv-13500-203 | Semantic Object Parsing with Local-Global Long Short-Term Memory | http://arxiv.org/pdf/1511.04510v1.pdf | author:Xiaodan Liang, Xiaohui Shen, Donglai Xiang, Jiashi Feng, Liang Lin, Shuicheng Yan category:cs.CV published:2015-11-14 summary:Semantic object parsing is a fundamental task for understanding objects indetail in computer vision community, where incorporating multi-level contextualinformation is critical for achieving such fine-grained pixel-levelrecognition. Prior methods often leverage the contextual information throughpost-processing predicted confidence maps. In this work, we propose a noveldeep Local-Global Long Short-Term Memory (LG-LSTM) architecture to seamlesslyincorporate short-distance and long-distance spatial dependencies into thefeature learning over all pixel positions. In each LG-LSTM layer, localguidance from neighboring positions and global guidance from the whole imageare imposed on each position to better exploit complex local and globalcontextual information. Individual LSTMs for distinct spatial dimensions arealso utilized to intrinsically capture various spatial layouts of semanticparts in the images, yielding distinct hidden and memory cells of each positionfor each dimension. In our parsing approach, several LG-LSTM layers are stackedand appended to the intermediate convolutional layers to directly enhancevisual features, allowing network parameters to be learned in an end-to-endway. The long chains of sequential computation by stacked LG-LSTM layers alsoenable each pixel to sense a much larger region for inference benefiting fromthe memorization of previous dependencies in all positions along alldimensions. Comprehensive evaluations on three public datasets well demonstratethe significant superiority of our LG-LSTM over other state-of-the-art methods.
arxiv-13500-204 | Deeply-Recursive Convolutional Network for Image Super-Resolution | http://arxiv.org/pdf/1511.04491v1.pdf | author:Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee category:cs.CV published:2015-11-14 summary:We propose an image super-resolution method (SR) using a deeply-recursiveconvolutional network (DRCN). Our network has a very deep recursive layer (upto 16 recursions). Increasing recursion depth can improve performance withoutintroducing new parameters for additional convolutions. Albeit advantages,learning a DRCN is very hard with a standard gradient descent method due toexploding/vanishing gradients. To ease the difficulty of training, we proposetwo extensions: recursive-supervision and skip-connection. Our methodoutperforms previous methods by a large margin.
arxiv-13500-205 | Unsupervised Learning in Synaptic Sampling Machines | http://arxiv.org/pdf/1511.04484v1.pdf | author:Emre O. Neftci, Bruno U. Pedroni, Siddharth Joshi, Maruan Al-Shedivat, Gert Cauwenberghs category:cs.NE published:2015-11-14 summary:Recent studies have shown that synaptic unreliability is a robust andsufficient mechanism for inducing the stochasticity observed in cortex. Here,we introduce the Synaptic Sampling Machine (SSM), a stochastic neural networkmodel that uses synaptic unreliability as a means to stochasticity forsampling. Synaptic unreliability plays the dual role of an efficient mechanismfor sampling in neuromorphic hardware, and a regularizer during learning akinto DropConnect. Similar to the original formulation of Boltzmann machines, theSSM can be viewed as a stochastic counterpart of Hopfield networks, but wherestochasticity is induced by a random mask over the connections. The SSM istrained to learn generative models with a synaptic plasticity rule implementingan event-driven form of contrastive divergence. We demonstrate this by learninga model of MNIST hand-written digit dataset, and by testing it in recognitionand inference tasks. We find that SSMs outperform restricted Boltzmann machines(4.4% error rate vs. 5%), they are more robust to overfitting, and tend tolearn sparser representations. SSMs are remarkably robust to weight pruning:removal of more than 80% of the weakest connections followed by cursoryre-learning causes only a negligible performance loss on the MNIST task (4.8%error rate). These results show that SSMs offer substantial improvements interms of performance, power and complexity over existing methods forunsupervised learning in spiking neural networks, and are thus promising modelsfor machine learning in neuromorphic execution platforms.
arxiv-13500-206 | Solving Jigsaw Puzzles with Linear Programming | http://arxiv.org/pdf/1511.04472v1.pdf | author:Rui Yu, Chris Russell, Lourdes Agapito category:cs.CV published:2015-11-13 summary:We propose a novel Linear Program (LP) based formula- tion for solving jigsawpuzzles. We formulate jigsaw solving as a set of successive global convexrelaxations of the stan- dard NP-hard formulation, that can describe bothjigsaws with pieces of unknown position and puzzles of unknown po- sition andorientation. The main contribution and strength of our approach comes from theLP assembly strategy. In contrast to existing greedy methods, our LP solverexploits all the pairwise matches simultaneously, and computes the position ofeach piece/component globally. The main ad- vantages of our LP approachinclude: (i) a reduced sensi- tivity to local minima compared to greedyapproaches, since our successive approximations are global and convex and (ii)an increased robustness to the presence of mismatches in the pairwise matchesdue to the use of a weighted L1 penalty. To demonstrate the effectiveness ofour approach, we test our algorithm on public jigsaw datasets and show that itoutperforms state-of-the-art methods.
arxiv-13500-207 | Zero-Shot Action Recognition by Word-Vector Embedding | http://arxiv.org/pdf/1511.04458v1.pdf | author:Xun Xu, Timothy Hospedales, Shaogang Gong category:cs.CV published:2015-11-13 summary:The number of categories for action recognition is growing rapidly and it hasbecome increasingly hard to label sufficient training data for learningconventional models for all categories. Instead of collecting ever more dataand labelling them exhaustively for all categories, an attractive alternativeapproach is "zeroshot learning" (ZSL). To that end, in this study we constructa mapping between visual features and a semantic descriptor of each actioncategory, allowing new categories to be recognised in the absence of any visualtraining data. Existing ZSL studies focus primarily on still images, andattribute-based semantic representations. In this work, we explore word-vectorsas the shared semantic space to embed videos and category labels for ZSL actionrecognition. This is a more challenging problem than existing ZSL of stillimages and/or attributes, because the mapping between the semantic space andvideo space-time features of actions is more complex and harder to learn forthe purpose of generalising over any cross-category domain shift. To solve thisgeneralisation problem in ZSL action recognition, we investigate a series ofsynergistic improvements to the standard ZSL pipeline. First, we enhancesignificantly the semantic space mapping by proposing manifold-regularisedregression and data augmentation strategies. Second, we evaluate two existingpost processing strategies (transductive self-training and hubness correction),and show that they are complementary. We evaluate extensively our model on awide range of human action datasets including HMDB51, UCF101, OlympicSports,CCV and TRECVID MED 13. The results demonstrate that our approach achieves thestate-of-the-art zero-shot action recognition performance with a simple andefficient pipeline, and without supervised annotation of attributes.
arxiv-13500-208 | Dynamic Sum Product Networks for Tractable Inference on Sequence Data | http://arxiv.org/pdf/1511.04412v1.pdf | author:Mazen Melibari, Pascal Poupart, Prashant Doshi category:cs.LG cs.AI stat.ML published:2015-11-13 summary:Sum-Product Networks (SPN) have recently emerged as a new class of tractableprobabilistic graphical models. Unlike Bayesian networks and Markov networkswhere inference may be exponential in the size of the network, inference inSPNs is in time linear in the size of the network. Since SPNs representdistributions over a fixed set of variables only, we propose dynamic sumproduct networks (DSPNs) as a generalization of SPNs for sequence data ofvarying length. A DSPN consists of a template network that is repeated as manytimes as needed to model data sequences of any length. We present a localsearch technique to learn the structure of the template network. In contrast todynamic Bayesian networks for which inference is generally exponential in thenumber of variables per time slice, DSPNs inherit the linear inferencecomplexity of SPNs. We demonstrate the advantages of DSPNs over DBNs and othermodels on several datasets of sequence data.
arxiv-13500-209 | Scalable Gaussian Processes for Characterizing Multidimensional Change Surfaces | http://arxiv.org/pdf/1511.04408v1.pdf | author:William Herlands, Andrew Wilson, Hannes Nickisch, Seth Flaxman, Daniel Neill, Wilbert van Panhuis, Eric Xing category:stat.ML published:2015-11-13 summary:We present a scalable Gaussian process model for identifying andcharacterizing smooth multidimensional changepoints, and automatically learningchanges in expressive covariance structure. We use Random Kitchen Sink featuresto flexibly define a change surface in combination with expressive spectralmixture kernels to capture the complex statistical structure. Finally, throughthe use of novel methods for additive non-separable kernels, we can scale themodel to large datasets. We demonstrate the model on numerical and real worlddata, including a large spatio-temporal disease dataset where we identifypreviously unknown heterogeneous changes in space and time.
arxiv-13500-210 | Robust Face Alignment Using a Mixture of Invariant Experts | http://arxiv.org/pdf/1511.04404v1.pdf | author:Oncel Tuzel, Salil Tambe, Tim K. Marks category:cs.CV published:2015-11-13 summary:Face alignment, which is the task of finding the locations of a set of faciallandmark points in an image of a face, is an important problem that is usefulin widespread application areas. Face alignment is particularly challengingwhen there are large variations in pose (in-plane and out-of-plane rotations)and facial expression. To address this issue, we propose a cascade in whicheach stage consists of a mixture of regression experts. Each expert learns acustomized regression model that is specialized to a different subset of thejoint space of pose and expressions. The system is invariant to a predefinedclass of transformations (e.g., affine), because the input is transformed tomatch each expert's prototype shape before the regression is applied. We alsopresent a method to include deformation constraints within the discriminativealignment framework, which makes our algorithm more robust. Results show thatour algorithm significantly outperforms previous methods on publicly availableface alignment datasets.
arxiv-13500-211 | Combining Monte-Carlo and Hyper-heuristic methods for the Multi-mode Resource-constrained Multi-project Scheduling Problem | http://arxiv.org/pdf/1511.04387v1.pdf | author:Shahriar Asta, Daniel Karapetyan, Ahmed Kheiri, Ender Özcan, Andrew J. Parkes category:cs.DS cs.AI cs.NE published:2015-11-13 summary:Multi-mode resource and precedence-constrained project scheduling is awell-known challenging real-world optimisation problem. An important variant ofthe problem requires scheduling of activities for multiple projects consideringavailability of local and global resources while respecting a range ofconstraints. This problem has been addressed by a competition, and associatedset of benchmark instances, as a part of the MISTA 2013 conference. A criticalaspect of the benchmarks is that the primary objective is to minimise the sumof the project completion times, with the usual makespan minimisation as asecondary objective. We observe that this leads to an expected differentoverall structure of good solutions and discuss the effects this has on thealgorithm design. This paper presents the resulting competition winningapproach; it is a carefully designed hybrid of Monte-Carlo tree search, novelneighbourhood moves, memetic algorithms, and hyper-heuristic methods. Theimplementation is also engineered to increase the speed with which iterationsare performed, and to exploit the computing power of multicore machines. Theresulting information-sharing multi-component algorithm significantlyoutperformed the other approaches during the competition, producing the bestsolution for 17 out of the 20 test instances and performing the best in around90% of all the trials.
arxiv-13500-212 | Deep Reflectance Maps | http://arxiv.org/pdf/1511.04384v1.pdf | author:Konstantinos Rematas, Tobias Ritschel, Mario Fritz, Efstratios Gavves, Tinne Tuytelaars category:cs.CV published:2015-11-13 summary:Undoing the image formation process and therefore decomposing appearance intoits intrinsic properties is a challenging task due to the under-constraintnature of this inverse problem. While significant progress has been made oninferring shape, materials and illumination from images only, progress in anunconstrained setting is still limited. We propose a convolutional neuralarchitecture to estimate reflectance maps of specular materials in naturallighting conditions. We achieve this in an end-to-end learning formulation thatdirectly predicts a reflectance map from the image itself. We show how toimprove estimates by facilitating additional supervision in an indirect schemethat first predicts surface orientation and afterwards predicts the reflectancemap by a learning-based sparse data interpolation. In order to analyze performance on this difficult task, we propose a newchallenge of Specular MAterials on SHapes with complex IllumiNation (SMASHINg)using both synthetic and real images. Furthermore, we show the application ofour method to a range of image-based editing tasks on real images.
arxiv-13500-213 | Hierarchical learning of grids of microtopics | http://arxiv.org/pdf/1503.03701v3.pdf | author:Nebojsa Jojic, Alessandro Perina, Dongwoo Kim category:stat.ML cs.IR cs.LG published:2015-03-12 summary:The counting grid is a grid of microtopics, sparse word/featuredistributions. The generative model associated with the grid does not use thesemicrotopics individually. Rather, it groups them in overlapping rectangularwindows and uses these grouped microtopics as either mixture or admixturecomponents. This paper builds upon the basic counting grid model and it showsthat hierarchical reasoning helps avoid bad local minima, produces betterclassification accuracy and, most interestingly, allows for extraction of largenumbers of coherent microtopics even from small datasets. We evaluate this interms of consistency, diversity and clarity of the indexed content, as well asin a user study on word intrusion tasks. We demonstrate that these models workwell as a technique for embedding raw images and discuss interesting parallelsbetween hierarchical CG models and other deep architectures.
arxiv-13500-214 | Large Scale Artificial Neural Network Training Using Multi-GPUs | http://arxiv.org/pdf/1511.04348v1.pdf | author:Linnan Wang, Wei Wu, Jianxiong Xiao, Yang Yi category:cs.DC cs.NE published:2015-11-13 summary:This paper describes a method for accelerating large scale Artificial NeuralNetworks (ANN) training using multi-GPUs by reducing the forward and backwardpasses to matrix multiplication. We propose an out-of-core multi-GPU matrixmultiplication and integrate the algorithm with the ANN training. Theexperiments demonstrate that our matrix multiplication algorithm achieveslinear speedup on multiple inhomogeneous GPUs. The full paper of this projectcan be found at [1].
arxiv-13500-215 | Standard methods for inexpensive pollen loads authentication by means of computer vision and machine learning | http://arxiv.org/pdf/1511.04320v1.pdf | author:Manuel Chica, Pascual Campoy category:cs.CV published:2015-11-13 summary:We present a complete methodology for authenticating local bee pollen againstfraudulent samples using image processing and machine learning techniques. Theproposed standard methods do not need expensive equipment such as advancedmicroscopes and can be used for a preliminary fast rejection of unknown pollentypes. The system is able to rapidly reject the non-local pollen samples withinexpensive hardware and without the need to send the product to thelaboratory. Methods are based on the color properties of bee pollen loadsimages and the use of one-class classifiers which are appropriate to rejectunknown pollen samples when there is limited data about them. The validation ofthe method is carried out by authenticating Spanish bee pollen types.Experimentation shows that the proposed methods can obtain an overallauthentication accuracy of 94%. We finally illustrate the user interaction withthe software in some practical cases by showing the developed applicationprototype.
arxiv-13500-216 | Exploiting an Oracle that Reports AUC Scores in Machine Learning Contests | http://arxiv.org/pdf/1506.01339v2.pdf | author:Jacob Whitehill category:cs.LG published:2015-06-03 summary:In machine learning contests such as the ImageNet Large Scale VisualRecognition Challenge and the KDD Cup, contestants can submit candidatesolutions and receive from an oracle (typically the organizers of thecompetition) the accuracy of their guesses compared to the ground-truth labels.One of the most commonly used accuracy metrics for binary classification tasksis the Area Under the Receiver Operating Characteristics Curve (AUC). In thispaper we provide proofs-of-concept of how knowledge of the AUC of a set ofguesses can be used, in two different kinds of attacks, to improve the accuracyof those guesses. On the other hand, we also demonstrate the intractability ofone kind of AUC exploit by proving that the number of possible binary labelingsof $n$ examples for which a candidate solution obtains a AUC score of $c$ growsexponentially in $n$, for every $c\in (0,1)$.
arxiv-13500-217 | LOGO-Net: Large-scale Deep Logo Detection and Brand Recognition with Deep Region-based Convolutional Networks | http://arxiv.org/pdf/1511.02462v2.pdf | author:Steven C. H. Hoi, Xiongwei Wu, Hantang Liu, Yue Wu, Huiqiong Wang, Hui Xue, Qiang Wu category:cs.CV published:2015-11-08 summary:Logo detection from images has many applications, particularly for brandrecognition and intellectual property protection. Most existing studies forlogo recognition and detection are based on small-scale datasets which are notcomprehensive enough when exploring emerging deep learning techniques. In thispaper, we introduce "LOGO-Net", a large-scale logo image database for logodetection and brand recognition from real-world product images. To facilitateresearch, LOGO-Net has two datasets: (i)"logos-18" consists of 18 logo classes,10 brands, and 16,043 logo objects, and (ii) "logos-160" consists of 160 logoclasses, 100 brands, and 130,608 logo objects. We describe the ideas andchallenges for constructing such a large-scale database. Another keycontribution of this work is to apply emerging deep learning techniques forlogo detection and brand recognition tasks, and conduct extensive experimentsby exploring several state-of-the-art deep region-based convolutional networkstechniques for object detection tasks. The LOGO-net will be released athttp://logo-net.org/
arxiv-13500-218 | Recurrent Neural Network Based Hybrid Model of Gene Regulatory Network | http://arxiv.org/pdf/1408.5405v2.pdf | author:Khalid Raza, Mansaf Alam category:cs.NE cs.CE q-bio.MN published:2014-08-22 summary:Systems biology is an emerging interdisciplinary area of research thatfocuses on study of complex interactions in a biological system, such as generegulatory networks. The discovery of gene regulatory networks leads to a widerange of applications, such as pathways related to a disease that can unveil inwhat way the disease acts and provide novel tentative drug targets. Inaddition, the development of biological models from discovered networks orpathways can help to predict the responses to disease and can be much usefulfor the novel drug development and treatments. The inference of regulatorynetworks from biological data is still in its infancy stage. This paperproposes a recurrent neural network (RNN) based gene regulatory network (GRN)model hybridized with generalized extended Kalman filter for weight update inbackpropagation through time training algorithm. The RNN is a complex neuralnetwork that gives a better settlement between the biological closeness andmathematical flexibility to model GRN. The RNN is able to capture complex,non-linear and dynamic relationship among variables. Gene expression data areinherently noisy and Kalman filter performs well for estimation even in noisydata. Hence, non-linear version of Kalman filter, i.e., generalized extendedKalman filter has been applied for weight update during network training. Thedeveloped model has been applied on DNA SOS repair network, IRMA network, andtwo synthetic networks from DREAM Challenge. We compared our results with otherstate-of-the-art techniques that show superiority of our model. Further, 5%Gaussian noise has been added in the dataset and result of the proposed modelshows negligible effect of noise on the results.
arxiv-13500-219 | Volume-based Semantic Labeling with Signed Distance Functions | http://arxiv.org/pdf/1511.04242v1.pdf | author:Tommaso Cavallari, Luigi Di Stefano category:cs.CV published:2015-11-13 summary:Research works on the two topics of Semantic Segmentation and SLAM(Simultaneous Localization and Mapping) have been following separate tracks.Here, we link them quite tightly by delineating a category label fusiontechnique that allows for embedding semantic information into the dense mapcreated by a volume-based SLAM algorithm such as KinectFusion. Accordingly, ourapproach is the first to provide a semantically labeled dense reconstruction ofthe environment from a stream of RGB-D images. We validate our proposal using apublicly available semantically annotated RGB-D dataset and a) employing groundtruth labels, b) corrupting such annotations with synthetic noise, c) deployinga state of the art semantic segmentation algorithm based on ConvolutionalNeural Networks.
arxiv-13500-220 | An Adaptive Data Representation for Robust Point-Set Registration and Merging | http://arxiv.org/pdf/1511.04240v1.pdf | author:Dylan Campbell, Lars Petersson category:cs.CV published:2015-11-13 summary:This paper presents a framework for rigid point-set registration and mergingusing a robust continuous data representation. Our point-set representation isconstructed by training a one-class support vector machine with a Gaussianradial basis function kernel and subsequently approximating the output functionwith a Gaussian mixture model. We leverage the representation's sparseparametrisation and robustness to noise, outliers and occlusions in anefficient registration algorithm that minimises the L2 distance between oursupport vector--parametrised Gaussian mixtures. In contrast, existingtechniques, such as Iterative Closest Point and Gaussian mixture approaches,manifest a narrower region of convergence and are less robust to occlusions andmissing data, as demonstrated in the evaluation on a range of 2D and 3Ddatasets. Finally, we present a novel algorithm, GMMerge, that parsimoniouslyand equitably merges aligned mixture models, allowing the framework to be usedfor reconstruction and mapping.
arxiv-13500-221 | Learning to Answer Questions From Image Using Convolutional Neural Network | http://arxiv.org/pdf/1506.00333v2.pdf | author:Lin Ma, Zhengdong Lu, Hang Li category:cs.CL cs.CV cs.LG cs.NE published:2015-06-01 summary:In this paper, we propose to employ the convolutional neural network (CNN)for the image question answering (QA). Our proposed CNN provides an end-to-endframework with convolutional architectures for learning not only the image andquestion representations, but also their inter-modal interactions to producethe answer. More specifically, our model consists of three CNNs: one image CNNto encode the image content, one sentence CNN to compose the words of thequestion, and one multimodal convolution layer to learn their jointrepresentation for the classification in the space of candidate answer words.We demonstrate the efficacy of our proposed model on the DAQUAR and COCO-QAdatasets, which are two benchmark datasets for the image QA, with theperformances significantly outperforming the state-of-the-art.
arxiv-13500-222 | Large-scale probabilistic predictors with and without guarantees of validity | http://arxiv.org/pdf/1511.00213v2.pdf | author:Vladimir Vovk, Ivan Petej, Valentina Fedorova category:cs.LG 68T05 published:2015-11-01 summary:This paper studies theoretically and empirically a method of turningmachine-learning algorithms into probabilistic predictors that automaticallyenjoys a property of validity (perfect calibration) and is computationallyefficient. The price to pay for perfect calibration is that these probabilisticpredictors produce imprecise (in practice, almost precise for large data sets)probabilities. When these imprecise probabilities are merged into preciseprobabilities, the resulting predictors, while losing the theoretical propertyof perfect calibration, are consistently more accurate than the existingmethods in empirical studies.
arxiv-13500-223 | On the Optimal Sample Complexity for Best Arm Identification | http://arxiv.org/pdf/1511.03774v2.pdf | author:Lijie Chen, Jian Li category:cs.LG cs.DS published:2015-11-12 summary:We study the best arm identification (BEST-1-ARM) problem, which is definedas follows. We are given $n$ stochastic bandit arms. The $i$th arm has a rewarddistribution $D_i$ with an unknown mean $\mu_i$. Upon each play of the $i$tharm, we can get a reward, sampled i.i.d. from $D_i$. We would like to identifythe arm with largest mean with probability at least $1-\delta$, using as fewsamples as possible. We also study an important special case where there areonly two arms, which we call the sign problem. We achieve a very detailedunderstanding of the optimal sample complexity of sign, simplifying andsignificantly extending a classical result by Farrell in 1964, with acompletely new proof. Using the new lower bound for sign, we obtain the firstlower bound for BEST-1-ARM that goes beyond the classic Mannor-Tsitsiklis lowerbound, by an interesting reduction from sign to BEST-1-ARM. To complement our lower bound, we also provide a nontrivial algorithm forBEST-1-ARM, which achieves a worst case optimal sample complexity, improvingupon several prior upper bounds on the same problem.
arxiv-13500-224 | Towards Vision-Based Deep Reinforcement Learning for Robotic Motion Control | http://arxiv.org/pdf/1511.03791v2.pdf | author:Fangyi Zhang, Jürgen Leitner, Michael Milford, Ben Upcroft, Peter Corke category:cs.LG cs.CV cs.RO published:2015-11-12 summary:This paper introduces a machine learning based system for controlling arobotic manipulator with visual perception only. The capability to autonomouslylearn robot controllers solely from raw-pixel images and without any priorknowledge of configuration is shown for the first time. We build upon thesuccess of recent deep reinforcement learning and develop a system for learningtarget reaching with a three-joint robot manipulator using external visualobservation. A Deep Q Network (DQN) was demonstrated to perform target reachingafter training in simulation. Transferring the network to real hardware andreal observation in a naive approach failed, but experiments show that thenetwork works when replacing camera images with synthetic images.
arxiv-13500-225 | $k$-means: Fighting against Degeneracy in Sequential Monte Carlo with an Application to Tracking | http://arxiv.org/pdf/1511.04157v1.pdf | author:Kai Fan, Katherine Heller category:stat.ML published:2015-11-13 summary:For regular particle filter algorithm or Sequential Monte Carlo (SMC)methods, the initial weights are traditionally dependent on the proposeddistribution, the posterior distribution at the current timestamp in thesampled sequence, and the target is the posterior distribution of the previoustimestamp. This is technically correct, but leads to algorithms which usuallyhave practical issues with degeneracy, where all particles eventually collapseonto a single particle. In this paper, we propose and evaluate using $k$ meansclustering to attack and even take advantage of this degeneracy. Specifically,we propose a Stochastic SMC algorithm which initializes the set of $k$ means,providing the initial centers chosen from the collapsed particles. To fightagainst degeneracy, we adjust the regular SMC weights, mediated by clusterproportions, and then correct them to retain the same expectation as before. Weexperimentally demonstrate that our approach has better performance thanvanilla algorithms.
arxiv-13500-226 | Adaptive Affinity Matrix for Unsupervised Metric Learning | http://arxiv.org/pdf/1511.04153v1.pdf | author:Yaoyi Li, Junxuan Chen, Hongtao Lu category:cs.CV cs.LG published:2015-11-13 summary:Spectral clustering is one of the most popular clustering approaches with thecapability to handle some challenging clustering problems. Most spectralclustering methods provide a nonlinear map from the data manifold to asubspace. Only a little work focuses on the explicit linear map which can beviewed as the unsupervised distance metric learning. In practice, the selectionof the affinity matrix exhibits a tremendous impact on the unsupervisedlearning. While much success of affinity learning has been achieved in recentyears, some issues such as noise reduction remain to be addressed. In thispaper, we propose a novel method, dubbed Adaptive Affinity Matrix (AdaAM), tolearn an adaptive affinity matrix and derive a distance metric from theaffinity. We assume the affinity matrix to be positive semidefinite withability to quantify the pairwise dissimilarity. Our method is based on posingthe optimization of objective function as a spectral decomposition problem. Weyield the affinity from both the original data distribution and the widely-usedheat kernel. The provided matrix can be regarded as the optimal representationof pairwise relationship on the manifold. Extensive experiments on a number ofreal-world data sets show the effectiveness and efficiency of AdaAM.
arxiv-13500-227 | Deep Mean Maps | http://arxiv.org/pdf/1511.04150v1.pdf | author:Junier B. Oliva, Dougal J. Sutherland, Barnabás Póczos, Jeff Schneider category:stat.ML cs.CV cs.LG published:2015-11-13 summary:The use of distributions and high-level features from deep architecture hasbecome commonplace in modern computer vision. Both of these methodologies haveseparately achieved a great deal of success in many computer vision tasks.However, there has been little work attempting to leverage the power of theseto methodologies jointly. To this end, this paper presents the Deep Mean Maps(DMMs) framework, a novel family of methods to non-parametrically representdistributions of features in convolutional neural network models. DMMs are able to both classify images using the distribution of top-levelfeatures, and to tune the top-level features for performing this task. We showhow to implement DMMs using a special mean map layer composed of typical CNNoperations, making both forward and backward propagation simple. We illustrate the efficacy of DMMs at analyzing distributional patterns inimage data in a synthetic data experiment. We also show that we extendingexisting deep architectures with DMMs improves the performance of existing CNNson several challenging real-world datasets.
arxiv-13500-228 | A Continuous-time Mutually-Exciting Point Process Framework for Prioritizing Events in Social Media | http://arxiv.org/pdf/1511.04145v1.pdf | author:Mehrdad Farajtabar, Safoora Yousefi, Long Q. Tran, Le Song, Hongyuan Zha category:cs.SI cs.LG published:2015-11-13 summary:The overwhelming amount and rate of information update in online social mediais making it increasingly difficult for users to allocate their attention totheir topics of interest, thus there is a strong need for prioritizing newsfeeds. The attractiveness of a post to a user depends on many complexcontextual and temporal features of the post. For instance, the contents of thepost, the responsiveness of a third user, and the age of the post may all haveimpact. So far, these static and dynamic features has not been incorporated ina unified framework to tackle the post prioritization problem. In this paper,we propose a novel approach for prioritizing posts based on a feature modulatedmulti-dimensional point process. Our model is able to simultaneously capturetextual and sentiment features, and temporal features such as self-excitation,mutual-excitation and bursty nature of social interaction. As an evaluation, wealso curated a real-world conversational benchmark dataset crawled fromFacebook. In our experiments, we demonstrate that our algorithm is able toachieve the-state-of-the-art performance in terms of analyzing, predicting, andprioritizing events. In terms of interpretability of our method, we observethat features indicating individual user profile and linguistic characteristicsof the events work best for prediction and prioritization of new events.
arxiv-13500-229 | Deep Cascaded Regression for Face Alignment | http://arxiv.org/pdf/1510.09083v2.pdf | author:Hanjiang Lai, Shengtao Xiao, Zhen Cui, Yan Pan, Chunyan Xu, Shuicheng Yan category:cs.CV published:2015-10-30 summary:We propose a novel cascaded regression framework for face alignment based ona deep convolutional neural network (CNN). In most existing cascaded regressionmethods, the shape-indexed features are either obtained by hand-crafted visualdescriptors or by leaning from the shallow models. This setting may besuboptimal for the face alignment task. To solve this problem, we propose anend-to-end CNN architecture to learn highly discriminative shape-indexedfeatures. First, our deep architecture encodes the image into high-levelfeature maps in the same size of the image via three main operations:convolution, pooling and deconvolution. Then, we propose "Shape-IndexedPooling" to extract the deep features from these high level descriptors. Werefine the shape via sequential regressions by using the deep shape-indexedfeatures, which demonstrates outstanding performance. We also propose to learnthe probability mask for each landmark that can be used to choose theinitialization from the shape space. Extensive evaluations conducted on severalbenchmark datasets demonstrate that the proposed deep framework showssignificant improvement over the state-of-the-art methods.
arxiv-13500-230 | Going Deeper in Facial Expression Recognition using Deep Neural Networks | http://arxiv.org/pdf/1511.04110v1.pdf | author:Ali Mollahosseini, David Chan, Mohammad H. Mahoor category:cs.NE cs.CV published:2015-11-12 summary:Automated Facial Expression Recognition (FER) has remained a challenging andinteresting problem. Despite efforts made in developing various methods forFER, existing approaches traditionally lack generalizability when applied tounseen images or those that are captured in wild setting. Most of the existingapproaches are based on engineered features (e.g. HOG, LBPH, and Gabor) wherethe classifier's hyperparameters are tuned to give best recognition accuraciesacross a single database, or a small collection of similar databases.Nevertheless, the results are not significant when they are applied to noveldata. This paper proposes a deep neural network architecture to address the FERproblem across multiple well-known standard face datasets. Specifically, ournetwork consists of two convolutional layers each followed by max pooling andthen four Inception layers. The network is a single component architecture thattakes registered facial images as the input and classifies them into either ofthe six basic or the neutral expressions. We conducted comprehensiveexperiments on seven publically available facial expression databases, viz.MultiPIE, MMI, CK+, DISFA, FERA, SFEW, and FER2013. The results of proposedarchitecture are comparable to or better than the state-of-the-art methods andbetter than traditional convolutional neural networks and in both accuracy andtraining time.
arxiv-13500-231 | Deep Gaussian Conditional Random Field Network: A Model-based Deep Network for Discriminative Denoising | http://arxiv.org/pdf/1511.04067v1.pdf | author:Raviteja Vemulapalli, Oncel Tuzel, Ming-Yu Liu category:cs.CV published:2015-11-12 summary:We propose a novel deep network architecture for image\\ denoising based on aGaussian Conditional Random Field (GCRF) model. In contrast to the existingdiscriminative denoising methods that train a separate model for each noiselevel, the proposed deep network explicitly models the input noise variance andhence is capable of handling a range of noise levels. Our deep network, whichwe refer to as deep GCRF network, consists of two sub-networks: (i) a parametergeneration network that generates the pairwise potential parameters based onthe noisy input image, and (ii) an inference network whose layers perform thecomputations involved in an iterative GCRF inference procedure.\ We train theentire deep GCRF network (both parameter generation and inference networks)discriminatively in an end-to-end fashion by maximizing the peaksignal-to-noise ratio measure. Experiments on Berkeley segmentation andPASCALVOC datasets show that the proposed deep GCRF network outperformsstate-of-the-art image denoising approaches for several noise levels.
arxiv-13500-232 | Properly Learning Poisson Binomial Distributions in Almost Polynomial Time | http://arxiv.org/pdf/1511.04066v1.pdf | author:Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart category:cs.DS cs.LG math.ST stat.TH published:2015-11-12 summary:We give an algorithm for properly learning Poisson binomial distributions. APoisson binomial distribution (PBD) of order $n$ is the discrete probabilitydistribution of the sum of $n$ mutually independent Bernoulli random variables.Given $\widetilde{O}(1/\epsilon^2)$ samples from an unknown PBD $\mathbf{p}$,our algorithm runs in time $(1/\epsilon)^{O(\log \log (1/\epsilon))}$, andoutputs a hypothesis PBD that is $\epsilon$-close to $\mathbf{p}$ in totalvariation distance. The previously best known running time for properlylearning PBDs was $(1/\epsilon)^{O(\log(1/\epsilon))}$. As one of our main contributions, we provide a novel structuralcharacterization of PBDs. We prove that, for all $\epsilon >0,$ there exists anexplicit collection $\cal{M}$ of $(1/\epsilon)^{O(\log \log (1/\epsilon))}$vectors of multiplicities, such that for any PBD $\mathbf{p}$ there exists aPBD $\mathbf{q}$ with $O(\log(1/\epsilon))$ distinct parameters whosemultiplicities are given by some element of ${\cal M}$, such that $\mathbf{q}$is $\epsilon$-close to $\mathbf{p}$. Our proof combines tools from Fourieranalysis and algebraic geometry. Our approach to the proper learning problem is as follows: Starting with anaccurate non-proper hypothesis, we fit a PBD to this hypothesis. Morespecifically, we essentially start with the hypothesis computed by thecomputationally efficient non-proper learning algorithm in our recentwork~\cite{DKS15}. Our aforementioned structural characterization allows us toreduce the corresponding fitting problem to a collection of$(1/\epsilon)^{O(\log \log(1/\epsilon))}$ systems of low-degree polynomialinequalities. We show that each such system can be solved in time$(1/\epsilon)^{O(\log \log(1/\epsilon))}$, which yields the overall runningtime of our algorithm.
arxiv-13500-233 | Efficient non-greedy optimization of decision trees | http://arxiv.org/pdf/1511.04056v1.pdf | author:Mohammad Norouzi, Maxwell D. Collins, Matthew Johnson, David J. Fleet, Pushmeet Kohli category:cs.LG cs.CV published:2015-11-12 summary:Decision trees and randomized forests are widely used in computer vision andmachine learning. Standard algorithms for decision tree induction optimize thesplit functions one node at a time according to some splitting criteria. Thisgreedy procedure often leads to suboptimal trees. In this paper, we present analgorithm for optimizing the split functions at all levels of the tree jointlywith the leaf parameters, based on a global objective. We show that the problemof finding optimal linear-combination (oblique) splits for decision trees isrelated to structured prediction with latent variables, and we formulate aconvex-concave upper bound on the tree's empirical loss. The run-time ofcomputing the gradient of the proposed surrogate objective with respect to eachtraining exemplar is quadratic in the the tree depth, and thus training deeptrees is feasible. The use of stochastic gradient descent for optimizationenables effective training with large datasets. Experiments on severalclassification benchmarks demonstrate that the resulting non-greedy decisiontrees outperform greedy decision tree baselines.
arxiv-13500-234 | Newtonian Image Understanding: Unfolding the Dynamics of Objects in Static Images | http://arxiv.org/pdf/1511.04048v1.pdf | author:Roozbeh Mottaghi, Hessam Bagherinezhad, Mohammad Rastegari, Ali Farhadi category:cs.CV published:2015-11-12 summary:In this paper, we study the challenging problem of predicting the dynamics ofobjects in static images. Given a query object in an image, our goal is toprovide a physical understanding of the object in terms of the forces actingupon it and its long term motion as response to those forces. Direct andexplicit estimation of the forces and the motion of objects from a single imageis extremely challenging. We define intermediate physical abstractions calledNewtonian scenarios and introduce Newtonian Neural Network ($N^3$) that learnsto map a single image to a state in a Newtonian scenario. Our experimentalevaluations show that our method can reliably predict dynamics of a queryobject from a single image. In addition, our approach can provide physicalreasoning that supports the predicted dynamics in terms of velocity and forcevectors. To spur research in this direction we compiled Visual NewtonianDynamics (VIND) dataset that includes 6806 videos aligned with Newtonianscenarios represented using game engines, and 4516 still images with theirground truth dynamics.
arxiv-13500-235 | Human Curation and Convnets: Powering Item-to-Item Recommendations on Pinterest | http://arxiv.org/pdf/1511.04003v1.pdf | author:Dmitry Kislyuk, Yuchen Liu, David Liu, Eric Tzeng, Yushi Jing category:cs.CV published:2015-11-12 summary:This paper presents Pinterest Related Pins, an item-to-item recommendationsystem that combines collaborative filtering with content-based ranking. Wedemonstrate that signals derived from user curation, the activity of usersorganizing content, are highly effective when used in conjunction withcontent-based ranking. This paper also demonstrates the effectiveness of visualfeatures, such as image or object representations learned from convnets, inimproving the user engagement rate of our item-to-item recommendation system.
arxiv-13500-236 | Automatic Inference of the Quantile Parameter | http://arxiv.org/pdf/1511.03990v1.pdf | author:Karthikeyan Natesan Ramamurthy, Aleksandr Y. Aravkin, Jayaraman J. Thiagarajan category:stat.ML published:2015-11-12 summary:Supervised learning is an active research area, with numerous applications indiverse fields such as data analytics, computer vision, speech and audioprocessing, and image understanding. In most cases, the loss functions used inmachine learning assume symmetric noise models, and seek to estimate theunknown function parameters. However, loss functions such as quantile andquantile Huber generalize the symmetric $\ell_1$ and Huber losses to theasymmetric setting, for a fixed quantile parameter. In this paper, we proposeto jointly infer the quantile parameter and the unknown function parameters,for the asymmetric quantile Huber and quantile losses. We explore variousproperties of the quantile Huber loss and implement a convexity certificatethat can be used to check convexity in the quantile parameter. When the loss ifconvex with respect to the parameter of the function, we prove that it isbiconvex in both the function and the quantile parameters, and propose analgorithm to jointly estimate these. Results with synthetic and real datademonstrate that the proposed approach can automatically recover the quantileparameter corresponding to the noise and also provide an improved recovery offunction parameters. To illustrate the potential of the framework, we extendthe gradient boosting machines with quantile losses to automatically estimatethe quantile parameter at each iteration.
arxiv-13500-237 | Rank Centrality: Ranking from Pair-wise Comparisons | http://arxiv.org/pdf/1209.1688v4.pdf | author:Sahand Negahban, Sewoong Oh, Devavrat Shah category:cs.LG stat.ML published:2012-09-08 summary:The question of aggregating pair-wise comparisons to obtain a global rankingover a collection of objects has been of interest for a very long time: be itranking of online gamers (e.g. MSR's TrueSkill system) and chess players,aggregating social opinions, or deciding which product to sell based ontransactions. In most settings, in addition to obtaining a ranking, finding`scores' for each object (e.g. player's rating) is of interest forunderstanding the intensity of the preferences. In this paper, we propose Rank Centrality, an iterative rank aggregationalgorithm for discovering scores for objects (or items) from pair-wisecomparisons. The algorithm has a natural random walk interpretation over thegraph of objects with an edge present between a pair of objects if they arecompared; the score, which we call Rank Centrality, of an object turns out tobe its stationary probability under this random walk. To study the efficacy ofthe algorithm, we consider the popular Bradley-Terry-Luce (BTL) model(equivalent to the Multinomial Logit (MNL) for pair-wise comparisons) in whicheach object has an associated score which determines the probabilistic outcomesof pair-wise comparisons between objects. In terms of the pair-wise marginalprobabilities, which is the main subject of this paper, the MNL model and theBTL model are identical. We bound the finite sample error rates between thescores assumed by the BTL model and those estimated by our algorithm. Inparticular, the number of samples required to learn the score well with highprobability depends on the structure of the comparison graph. When theLaplacian of the comparison graph has a strictly positive spectral gap, e.g.each item is compared to a subset of randomly chosen items, this leads todependence on the number of samples that is nearly order-optimal.
arxiv-13500-238 | Prediction of the Yield of Enzymatic Synthesis of Betulinic Acid Ester Using Artificial Neural Networks and Support Vector Machine | http://arxiv.org/pdf/1511.03984v1.pdf | author:Run Wang, Qiaoli Mo, Qian Zhang, Fudi Chen, Dazuo Yang category:cs.LG cs.NE published:2015-11-12 summary:3\b{eta}-O-phthalic ester of betulinic acid is of great importance inanticancer studies. However, the optimization of its reaction conditionsrequires a large number of experimental works. To simplify the number of timesof optimization in experimental works, here, we use artificial neural network(ANN) and support vector machine (SVM) models for the prediction of yields of3\b{eta}-O-phthalic ester of betulinic acid synthesized by betulinic acid andphthalic anhydride using lipase as biocatalyst. General regression neuralnetwork (GRNN), multilayer feed-forward neural network (MLFN) and the SVMmodels were trained based on experimental data. Four indicators were set asindependent variables, including time (h), temperature (C), amount of enzyme(mg) and molar ratio, while the yield of the 3\b{eta}-O-phthalic ester ofbetulinic acid was set as the dependent variable. Results show that the GRNNand SVM models have the best prediction results during the testing process,with comparatively low RMS errors (4.01 and 4.23respectively) and shorttraining times (both 1s). The prediction accuracy of the GRNN and SVM are both100% in testing process, under the tolerance of 30%.
arxiv-13500-239 | Bayesian Analysis of Dynamic Linear Topic Models | http://arxiv.org/pdf/1511.03947v1.pdf | author:Chris Glynn, Surya T. Tokdar, David L. Banks, Brian Howard category:stat.ML cs.LG stat.ME published:2015-11-12 summary:In dynamic topic modeling, the proportional contribution of a topic to adocument depends on the temporal dynamics of that topic's overall prevalence inthe corpus. We extend the Dynamic Topic Model of Blei and Lafferty (2006) byexplicitly modeling document level topic proportions with covariates anddynamic structure that includes polynomial trends and periodicity. A MarkovChain Monte Carlo (MCMC) algorithm that utilizes Polya-Gamma data augmentationis developed for posterior inference. Conditional independencies in the modeland sampling are made explicit, and our MCMC algorithm is parallelized wherepossible to allow for inference in large corpora. To address computationalbottlenecks associated with Polya-Gamma sampling, we appeal to the CentralLimit Theorem to develop a Gaussian approximation to the Polya-Gamma randomvariable. This approximation is fast and reliable for parameter values relevantin the text mining domain. Our model and inference algorithm are validated withmultiple simulation examples, and we consider the application of modelingtrends in PubMed abstracts. We demonstrate that sharing information acrossdocuments is critical for accurately estimating document-specific topicproportions. We also show that explicitly modeling polynomial and periodicbehavior improves our ability to predict topic prevalence at future timepoints.
arxiv-13500-240 | A Multilingual FrameNet-based Grammar and Lexicon for Controlled Natural Language | http://arxiv.org/pdf/1511.03924v1.pdf | author:Normunds Gruzitis, Dana Dannélls category:cs.CL published:2015-11-12 summary:Berkeley FrameNet is a lexico-semantic resource for English based on thetheory of frame semantics. It has been exploited in a range of natural languageprocessing applications and has inspired the development of framenets for manylanguages. We present a methodological approach to the extraction andgeneration of a computational multilingual FrameNet-based grammar and lexicon.The approach leverages FrameNet-annotated corpora to automatically extract aset of cross-lingual semantico-syntactic valence patterns. Based on data fromBerkeley FrameNet and Swedish FrameNet, the proposed approach has beenimplemented in Grammatical Framework (GF), a categorial grammar formalismspecialized for multilingual grammars. The implementation of the grammar andlexicon is supported by the design of FrameNet, providing a frame semanticabstraction layer, an interlingual semantic API (application programminginterface), over the interlingual syntactic API already provided by GF ResourceGrammar Library. The evaluation of the acquired grammar and lexicon shows thefeasibility of the approach. Additionally, we illustrate how the FrameNet-basedgrammar and lexicon are exploited in two distinct multilingual controllednatural language applications. The produced resources are available under anopen source license.
arxiv-13500-241 | Private False Discovery Rate Control | http://arxiv.org/pdf/1511.03803v1.pdf | author:Cynthia Dwork, Weijie Su, Li Zhang category:math.ST cs.DS stat.ML stat.TH published:2015-11-12 summary:We provide the first differentially private algorithms for controlling thefalse discovery rate (FDR) in multiple hypothesis testing, with essentially noloss in power under certain conditions. Our general approach is to adapt awell-known variant of the Benjamini-Hochberg procedure (BHq), making each stepdifferentially private. This destroys the classical proof of FDR control. Toprove FDR control of our method, (a) we develop a new proof of the original(non-private) BHq algorithm and its robust variants -- a proof requiring onlythe assumption that the true null test statistics are independent, allowing forarbitrary correlations between the true nulls and false nulls. This assumptionis fairly weak compared to those previously shown in the vast literature onthis topic, and explains in part the empirical robustness of BHq. Then (b) werelate the FDR control properties of the differentially private version to thecontrol properties of the non-private version. \end{enumerate} We also presenta low-distortion "one-shot" differentially private primitive for "top $k$"problems, e.g., "Which are the $k$ most popular hobbies?" (which we apply to:"Which hypotheses have the $k$ most significant $p$-values?"), and use it toget a faster privacy-preserving instantiation of our general approach at littlecost in accuracy. The proof of privacy for the one-shot top~$k$ algorithmintroduces a new technique of independent interest.
arxiv-13500-242 | Nonparametric Estimation of Scale-Free Graphical Models | http://arxiv.org/pdf/1511.03796v1.pdf | author:Yuancheng Zhu, Zhe Liu, Siqi Sun category:stat.ME cs.LG stat.ML published:2015-11-12 summary:We present a nonparametric method for estimating scale-free graphical models.To avoid the usual Gaussian assumption, we restrict the graph to be a forestand build on the work of forest density estimation. The method is motivatedfrom a Bayesian perspective and is equivalent to finding the maximum spanningtree of a weighted graph with a log degree penalty. We solve the optimizationproblem via a minorize-maximization procedure with Kruskal's algorithm.Simulations show that the proposed method outperforms competing parametricmethods, and is robust to the true data distribution. It also leads toimprovement in predictive power and interpretability in two real data examples.
arxiv-13500-243 | Dynamic Belief Fusion for Object Detection | http://arxiv.org/pdf/1502.07643v3.pdf | author:Ryan Robinson category:cs.CV published:2015-02-26 summary:A novel approach for the fusion of detection scores from disparate objectdetection methods is proposed. In order to effectively integrate the outputs ofmultiple detectors, the level of ambiguity in each individual detection score(called "uncertainty") is estimated using the precision/recall relationship ofthe corresponding detector. The proposed fusion method, called Dynamic BeliefFusion (DBF), dynamically assigns basic probabilities to propositions (target,non-target, uncertain) based on confidence levels in the detection results ofindividual approaches. A joint basic probability assignment, containinginformation from all detectors, is determined using Dempster's combinationrule, and is easily reduced to a single fused detection score. Experiments onARL and PASCAL VOC 07 datasets demonstrate that the detection accuracy of DBFis considerably greater than conventional fusion approaches as well asstate-of-the-art individual detectors.
arxiv-13500-244 | Sparse Learning for Large-scale and High-dimensional Data: A Randomized Convex-concave Optimization Approach | http://arxiv.org/pdf/1511.03766v1.pdf | author:Lijun Zhang, Tianbao Yang, Rong Jin, Zhi-Hua Zhou category:cs.LG published:2015-11-12 summary:In this paper, we develop a randomized algorithm and theory for learning asparse model from large-scale and high-dimensional data, which is usuallyformulated as an empirical risk minimization problem with a sparsity-inducingregularizer. Under the assumption that there exists a (approximately) sparsesolution with high classification accuracy, we argue that the dual solution isalso sparse or approximately sparse. The fact that both primal and dualsolutions are sparse motivates us to develop a randomized approach for ageneral convex-concave optimization problem. Specifically, the proposedapproach combines the strength of random projection with that of sparselearning: it utilizes random projection to reduce the dimensionality, andintroduces $\ell_1$-norm regularization to alleviate the approximation errorcaused by random projection. Theoretical analysis shows that under favoredconditions, the randomized algorithm can accurately recover the optimalsolutions to the convex-concave optimization problem (i.e., recover both theprimal and dual solutions). Furthermore, the solutions returned by ouralgorithm are guaranteed to be approximately sparse.
arxiv-13500-245 | Random Multi-Constraint Projection: Stochastic Gradient Methods for Convex Optimization with Many Constraints | http://arxiv.org/pdf/1511.03760v1.pdf | author:Mengdi Wang, Yichen Chen, Jialin Liu, Yuantao Gu category:stat.ML cs.LG math.OC published:2015-11-12 summary:Consider convex optimization problems subject to a large number ofconstraints. We focus on stochastic problems in which the objective takes theform of expected values and the feasible set is the intersection of a largenumber of convex sets. We propose a class of algorithms that perform bothstochastic gradient descent and random feasibility updates simultaneously. Atevery iteration, the algorithms sample a number of projection points onto arandomly selected small subsets of all constraints. Three feasibility updateschemes are considered: averaging over random projected points, projecting ontothe most distant sample, projecting onto a special polyhedral set constructedbased on sample points. We prove the almost sure convergence of thesealgorithms, and analyze the iterates' feasibility error and optimality error,respectively. We provide new convergence rate benchmarks for stochasticfirst-order optimization with many constraints. The rate analysis and numericalexperiments reveal that the algorithm using the polyhedral-set projectionscheme is the most efficient one within known algorithms.
arxiv-13500-246 | Automatic Content-Aware Color and Tone Stylization | http://arxiv.org/pdf/1511.03748v1.pdf | author:Joon-Young Lee, Kalyan Sunkavalli, Zhe Lin, Xiaohui Shen, In So Kweon category:cs.CV published:2015-11-12 summary:We introduce a new technique that automatically generates diverse, visuallycompelling stylizations for a photograph in an unsupervised manner. We achievethis by learning style ranking for a given input using a large photo collectionand selecting a diverse subset of matching styles for final style transfer. Wealso propose a novel technique that transfers the global color and tone of thechosen exemplars to the input photograph while avoiding the common visualartifacts produced by the existing style transfer methods. Together, our styleselection and transfer techniques produce compelling, artifact-free results ona wide range of input photographs, and a user study shows that our results arepreferred over other techniques.
arxiv-13500-247 | Explicit Knowledge-based Reasoning for Visual Question Answering | http://arxiv.org/pdf/1511.02570v2.pdf | author:Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, Anthony Dick category:cs.CV cs.CL published:2015-11-09 summary:We describe a method for visual question answering which is capable ofreasoning about contents of an image on the basis of information extracted froma large-scale knowledge base. The method not only answers natural languagequestions using concepts not contained in the image, but can provide anexplanation of the reasoning by which it developed its answer. The method iscapable of answering far more complex questions than the predominant longshort-term memory-based approach, and outperforms it significantly in thetesting. We also provide a dataset and a protocol by which to evaluate suchmethods, thus addressing one of the key issues in general visual ques- tionanswering.
arxiv-13500-248 | The Wilson Machine for Image Modeling | http://arxiv.org/pdf/1510.07740v2.pdf | author:Saeed Saremi, Terrence J. Sejnowski category:stat.ML cs.CV cs.LG published:2015-10-27 summary:Learning the distribution of natural images is one of the hardest and mostimportant problems in machine learning. The problem remains open, because theenormous complexity of the structures in natural images spans all lengthscales. We break down the complexity of the problem and show that the hierarchyof structures in natural images fuels a new class of learning algorithms basedon the theory of critical phenomena and stochastic processes. We approach thisproblem from the perspective of the theory of critical phenomena, which wasdeveloped in condensed matter physics to address problems with infinitelength-scale fluctuations, and build a framework to integrate the criticalityof natural images into a learning algorithm. The problem is broken down bymapping images into a hierarchy of binary images, called bitplanes. In thisrepresentation, the top bitplane is critical, having fluctuations in structuresover a vast range of scales. The bitplanes below go through a gradualstochastic heating process to disorder. We turn this representation into adirected probabilistic graphical model, transforming the learning problem intothe unsupervised learning of the distribution of the critical bitplane and thesupervised learning of the conditional distributions for the remainingbitplanes. We learnt the conditional distributions by logistic regression in aconvolutional architecture. Conditioned on the critical binary image, thissimple architecture can generate large, natural-looking images, with manyshades of gray, without the use of hidden units, unprecedented in the studiesof natural images. The framework presented here is a major step in bringingcriticality and stochastic processes to machine learning and in studyingnatural image statistics.
arxiv-13500-249 | Deep Multimodal Semantic Embeddings for Speech and Images | http://arxiv.org/pdf/1511.03690v1.pdf | author:David Harwath, James Glass category:cs.CV cs.AI cs.CL published:2015-11-11 summary:In this paper, we present a model which takes as input a corpus of imageswith relevant spoken captions and finds a correspondence between the twomodalities. We employ a pair of convolutional neural networks to model visualobjects and speech signals at the word level, and tie the networks togetherwith an embedding and alignment model which learns a joint semantic space overboth modalities. We evaluate our model using image search and annotation taskson the Flickr8k dataset, which we augmented by collecting a corpus of 40,000spoken captions using Amazon Mechanical Turk.
arxiv-13500-250 | Online Principal Component Analysis in High Dimension: Which Algorithm to Choose? | http://arxiv.org/pdf/1511.03688v1.pdf | author:Hervé Cardot, David Degras category:stat.ML cs.LG stat.ME published:2015-11-11 summary:In the current context of data explosion, online techniques that do notrequire storing all data in memory are indispensable to routinely perform taskslike principal component analysis (PCA). Recursive algorithms that update thePCA with each new observation have been studied in various fields of researchand found wide applications in industrial monitoring, computer vision,astronomy, and latent semantic indexing, among others. This work providesguidance for selecting an online PCA algorithm in practice. We present the mainapproaches to online PCA, namely, perturbation techniques, incremental methods,and stochastic optimization, and compare their statistical accuracy,computation time, and memory requirements using artificial and real data.Extensions to missing data and to functional data are discussed. All studiedalgorithms are available in the R package onlinePCA on CRAN.
arxiv-13500-251 | A Size-Free CLT for Poisson Multinomials and its Applications | http://arxiv.org/pdf/1511.03641v1.pdf | author:Constantinos Daskalakis, Anindya De, Gautam Kamath, Christos Tzamos category:cs.DS cs.GT cs.LG math.PR math.ST stat.TH published:2015-11-11 summary:An $(n,k)$-Poisson Multinomial Distribution (PMD) is the distribution of thesum of $n$ independent random vectors supported on the set ${\calB}_k=\{e_1,\ldots,e_k\}$ of standard basis vectors in $\mathbb{R}^k$. We showthat any $(n,k)$-PMD is ${\rm poly}\left({k\over \sigma}\right)$-close in totalvariation distance to the (appropriately discretized) multi-dimensionalGaussian with the same first two moments, removing the dependence on $n$ fromthe Central Limit Theorem of Valiant and Valiant. Interestingly, our CLT isobtained by bootstrapping the Valiant-Valiant CLT itself through the structuralcharacterization of PMDs shown in recent work by Daskalakis, Kamath, andTzamos. In turn, our stronger CLT can be leveraged to obtain an efficient PTASfor approximate Nash equilibria in anonymous games, significantly improving thestate of the art, and matching qualitatively the running time dependence on $n$and $1/\varepsilon$ of the best known algorithm for two-strategy anonymousgames. Our new CLT also enables the construction of covers for the set of$(n,k)$-PMDs, which are proper and whose size is shown to be essentiallyoptimal. Our cover construction combines our CLT with the Shapley-Folkmantheorem and recent sparsification results for Laplacian matrices by Batson,Spielman, and Srivastava. Our cover size lower bound is based on an algebraicgeometric construction. Finally, leveraging the structural properties of theFourier spectrum of PMDs we show that these distributions can be learned from$O_k(1/\varepsilon^2)$ samples in ${\rm poly}_k(1/\varepsilon)$-time, removingthe quasi-polynomial dependence of the running time on $1/\varepsilon$ from thealgorithm of Daskalakis, Kamath, and Tzamos.
arxiv-13500-252 | A Continuous Max-Flow Approach to Cyclic Field Reconstruction | http://arxiv.org/pdf/1511.03629v1.pdf | author:John S. H. Baxter, Jonathan McLeod, Terry M. Peters category:cs.CV published:2015-11-11 summary:Reconstruction of an image from noisy data using Markov Random Field theoryhas been explored by both the graph-cuts and continuous max-flow community inthe form of the Potts and Ishikawa models. However, neither model takes intoaccount the particular cyclic topology of specific intensity types such as thehue in natural colour images, or the phase in complex valued MRI. This paperpresents \textit{cyclic continuous max-flow} image reconstruction which modelsthe intensity being reconstructed as having a fundamentally cyclic topology.This model complements the Ishikawa model in that it is designed with imagereconstruction in mind, having the topology of the intensity space inherent inthe model while being readily extendable to an arbitrary intensity resolution.
arxiv-13500-253 | The Fourier Transform of Poisson Multinomial Distributions and its Algorithmic Applications | http://arxiv.org/pdf/1511.03592v1.pdf | author:Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart category:cs.DS cs.GT cs.LG math.PR math.ST stat.TH published:2015-11-11 summary:An $(n, k)$-Poisson Multinomial Distribution (PMD) is a random variable ofthe form $X = \sum_{i=1}^n X_i$, where the $X_i$'s are independent randomvectors supported on the set of standard basis vectors in $\mathbb{R}^k.$ Inthis paper, we obtain a refined structural understanding of PMDs by analyzingtheir Fourier transform. As our core structural result, we prove that theFourier transform of PMDs is {\em approximately sparse}, i.e., roughlyspeaking, its $L_1$-norm is small outside a small set. By building on thisresult, we obtain the following applications: {\bf Learning Theory.} We design the first computationally efficient learningalgorithm for PMDs with respect to the total variation distance. Our algorithmlearns an arbitrary $(n, k)$-PMD within variation distance $\epsilon$ using anear-optimal sample size of $\widetilde{O}_k(1/\epsilon^2),$ and runs in time$\widetilde{O}_k(1/\epsilon^2) \cdot \log n.$ Previously, no algorithm with a$\mathrm{poly}(1/\epsilon)$ runtime was known, even for $k=3.$ {\bf Game Theory.} We give the first efficient polynomial-time approximationscheme (EPTAS) for computing Nash equilibria in anonymous games. For normalizedanonymous games with $n$ players and $k$ strategies, our algorithm computes awell-supported $\epsilon$-Nash equilibrium in time $n^{O(k^3)} \cdot(k/\epsilon)^{O(k^3\log(k/\epsilon)/\log\log(k/\epsilon))^{k-1}}.$ The bestprevious algorithm for this problem had running time $n^{(f(k)/\epsilon)^k},$where $f(k) = \Omega(k^{k^2})$, for any $k>2.$ {\bf Statistics.} We prove a multivariate central limit theorem (CLT) thatrelates an arbitrary PMD to a discretized multivariate Gaussian with the samemean and covariance, in total variation distance. Our new CLT strengthens theCLT of Valiant and Valiant by completely removing the dependence on $n$ in theerror bound.
arxiv-13500-254 | Bayesian group latent factor analysis with structured sparsity | http://arxiv.org/pdf/1411.2698v2.pdf | author:Shiwen Zhao, Chuan Gao, Sayan Mukherjee, Barbara E Engelhardt category:stat.ME q-bio.QM stat.ML published:2014-11-11 summary:Latent factor models are the canonical statistical tool for exploratoryanalyses of low-dimensional linear structure for an observation matrix with pfeatures across n samples. We develop a structured Bayesian group factoranalysis model that extends the factor model to multiple coupled observationmatrices; in the case of two observations, this reduces to a Bayesian model ofcanonical correlation analysis. The main contribution of this work is tocarefully define a structured Bayesian prior that encourages both element-wiseand column-wise shrinkage and leads to desirable behavior on high-dimensionaldata. In particular, our model puts a structured prior on the joint factorloading matrix, regularizing at three levels, which enables element-wisesparsity and unsupervised recovery of latent factors corresponding tostructured variance across arbitrary subsets of the observations. In addition,our structured prior allows for both dense and sparse latent factors so thatcovariation among either all features or only a subset of features can both berecovered. We use fast parameter-expanded expectation-maximization forparameter estimation in this model. We validate our method on both simulateddata with substantial structure and real data, comparing against a number ofstate-of-the-art approaches. These results illustrate useful properties of ourmodel, including i) recovering sparse signal in the presence of dense effects;ii) the ability to scale naturally to large numbers of observations; iii)flexible observation- and factor-specific regularization to recover factorswith a wide variety of sparsity levels and percentage of variance explained;and iv) tractable inference that scales to modern genomic and document datasizes.
arxiv-13500-255 | Fast, Provable Algorithms for Isotonic Regression in all $\ell_{p}$-norms | http://arxiv.org/pdf/1507.00710v2.pdf | author:Rasmus Kyng, Anup Rao, Sushant Sachdeva category:cs.LG cs.DS math.ST stat.TH published:2015-07-02 summary:Given a directed acyclic graph $G,$ and a set of values $y$ on the vertices,the Isotonic Regression of $y$ is a vector $x$ that respects the partial orderdescribed by $G,$ and minimizes $x-y,$ for a specified norm. This papergives improved algorithms for computing the Isotonic Regression for allweighted $\ell_{p}$-norms with rigorous performance guarantees. Our algorithmsare quite practical, and their variants can be implemented to run fast inpractice.
arxiv-13500-256 | One Scan 1-Bit Compressed Sensing | http://arxiv.org/pdf/1503.02346v2.pdf | author:Ping Li category:stat.ME cs.IT cs.LG math.IT published:2015-03-08 summary:Based on $\alpha$-stable random projections with small $\alpha$, we develop asimple algorithm for compressed sensing (sparse signal recovery) by utilizingonly the signs (i.e., 1-bit) of the measurements. Using only 1-bit informationof the measurements results in substantial cost reduction in collection,storage, communication, and decoding for compressed sensing. The proposedalgorithm is efficient in that the decoding procedure requires only one scan ofthe coordinates. Our analysis can precisely show that, for a $K$-sparse signalof length $N$, $12.3K\log N/\delta$ measurements (where $\delta$ is theconfidence) would be sufficient for recovering the support and the signs of thesignal. While the method is very robust against typical measurement noises, wealso provide the analysis of the scheme under random flipping of the signs ofthe measurements. \noindent Compared to the well-known work on 1-bit marginal regression (whichcan also be viewed as a one-scan method), the proposed algorithm requiresorders of magnitude fewer measurements. Compared to 1-bit Iterative HardThresholding (IHT) (which is not a one-scan algorithm), our method is stillsignificantly more accurate. Furthermore, the proposed method is reasonablyrobust against random sign flipping while IHT is known to be very sensitive tothis type of noise.
arxiv-13500-257 | DataGrinder: Fast, Accurate, Fully non-Parametric Classification Approach Using 2D Convex Hulls | http://arxiv.org/pdf/1511.03576v1.pdf | author:Mohammad Khabbaz category:cs.DB cs.CG cs.LG published:2015-11-11 summary:It has been a long time, since data mining technologies have made their waysto the field of data management. Classification is one of the most importantdata mining tasks for label prediction, categorization of objects into groups,advertisement and data management. In this paper, we focus on the standardclassification problem which is predicting unknown labels in Euclidean space.Most efforts in Machine Learning communities are devoted to methods that useprobabilistic algorithms which are heavy on Calculus and Linear Algebra. Mostof these techniques have scalability issues for big data, and are hardlyparallelizable if they are to maintain their high accuracies in their standardform. Sampling is a new direction for improving scalability, using many smallparallel classifiers. In this paper, rather than conventional sampling methods,we focus on a discrete classification algorithm with O(n) expected runningtime. Our approach performs a similar task as sampling methods. However, we usecolumn-wise sampling of data, rather than the row-wise sampling used in theliterature. In either case, our algorithm is completely deterministic. Ouralgorithm, proposes a way of combining 2D convex hulls in order to achieve highclassification accuracy as well as scalability in the same time. First, wethoroughly describe and prove our O(n) algorithm for finding the convex hull ofa point set in 2D. Then, we show with experiments our classifier model builtbased on this idea is very competitive compared with existing sophisticatedclassification algorithms included in commercial statistical applications suchas MATLAB.
arxiv-13500-258 | Federated Optimization:Distributed Optimization Beyond the Datacenter | http://arxiv.org/pdf/1511.03575v1.pdf | author:Jakub Konečný, Brendan McMahan, Daniel Ramage category:cs.LG math.OC published:2015-11-11 summary:We introduce a new and increasingly relevant setting for distributedoptimization in machine learning, where the data defining the optimization aredistributed (unevenly) over an extremely large number of \nodes, but the goalremains to train a high-quality centralized model. We refer to this setting asFederated Optimization. In this setting, communication efficiency is of utmostimportance. A motivating example for federated optimization arises when we keep thetraining data locally on users' mobile devices rather than logging it to a datacenter for training. Instead, the mobile devices are used as nodes performingcomputation on their local data in order to update a global model. We supposethat we have an extremely large number of devices in our network, each of whichhas only a tiny fraction of data available totally; in particular, we expectthe number of data points available locally to be much smaller than the numberof devices. Additionally, since different users generate data with differentpatterns, we assume that no device has a representative sample of the overalldistribution. We show that existing algorithms are not suitable for this setting, andpropose a new algorithm which shows encouraging experimental results. This workalso sets a path for future research needed in the context of federatedoptimization.
arxiv-13500-259 | Hierarchical Recurrent Neural Encoder for Video Representation with Application to Captioning | http://arxiv.org/pdf/1511.03476v1.pdf | author:Pingbo Pan, Zhongwen Xu, Yi Yang, Fei Wu, Yueting Zhuang category:cs.CV published:2015-11-11 summary:Recently, deep learning approach, especially deep Convolutional NeuralNetworks (ConvNets), have achieved overwhelming accuracy with fast processingspeed for image classification. Incorporating temporal structure with deepConvNets for video representation becomes a fundamental problem for videocontent analysis. In this paper, we propose a new approach, namely HierarchicalRecurrent Neural Encoder (HRNE), to exploit temporal information of videos.Compared to recent video representation inference approaches, this paper makesthe following three contributions. First, our HRNE is able to efficientlyexploit video temporal structure in a longer range by reducing the length ofinput information flow, and compositing multiple consecutive inputs at a higherlevel. Second, computation operations are significantly lessened whileattaining more non-linearity. Third, HRNE is able to uncover temporaltransitions between frame chunks with different granularities, i.e., it canmodel the temporal transitions between frames as well as the transitionsbetween segments. We apply the new method to video captioning where temporalinformation plays a crucial role. Experiments demonstrate that our methodoutperforms the state-of-the-art on video captioning benchmarks. Notably, evenusing a single network with only RGB stream as input, HRNE beats all the recentsystems which combine multiple inputs, such as RGB ConvNet plus 3D ConvNet.
arxiv-13500-260 | Human Pose Estimation with Iterative Error Feedback | http://arxiv.org/pdf/1507.06550v2.pdf | author:Joao Carreira, Pulkit Agrawal, Katerina Fragkiadaki, Jitendra Malik category:cs.CV cs.LG cs.NE published:2015-07-23 summary:Hierarchical feature extractors such as Convolutional Networks (ConvNets)have achieved impressive performance on a variety of classification tasks usingpurely feedforward processing. Feedforward architectures can learn richrepresentations of the input space but do not explicitly model dependencies inthe output spaces, that are quite structured for tasks such as articulatedhuman pose estimation or object segmentation. Here we propose a framework thatexpands the expressive power of hierarchical feature extractors to encompassboth input and output spaces, by introducing top-down feedback. Instead ofdirectly predicting the outputs in one go, we use a self-correcting model thatprogressively changes an initial solution by feeding back error predictions, ina process we call Iterative Error Feedback (IEF). IEF shows excellentperformance on the task of articulated pose estimation in the challenging MPIIand LSP benchmarks, matching the state-of-the-art without requiring groundtruth scale annotation.
arxiv-13500-261 | Instantaneous Modelling and Reverse Engineering of DataConsistent Prime Models in Seconds! | http://arxiv.org/pdf/1511.03472v1.pdf | author:Michael A. Idowu category:q-bio.QM nlin.AO stat.ML published:2015-11-11 summary:A theoretical framework that supports automated construction of dynamic primemodels purely from experimental time series data has been invented anddeveloped, which can automatically generate (construct) data-driven models ofany time series data in seconds. This has resulted in the formulation andformalisation of new reverse engineering and dynamic methods for automatedsystems modelling of complex systems, including complex biological, financial,control, and artificial neural network systems. The systems/model theory behindthe invention has been formalised as a new, effective and robust systemidentification strategy complementary to process-based modelling. The proposeddynamic modelling and network inference solutions often involve tacklingextremely difficult parameter estimation challenges, inferring unknownunderlying network structures, and unsupervised formulation and construction ofsmart and intelligent ODE models of complex systems. In underdeterminedconditions, i.e., cases of dealing with how best to instantaneously and rapidlyconstruct data-consistent prime models of unknown (or well-studied) complexsystem from small-sized time series data, inference of unknown underlyingnetwork of interaction is more challenging. This article reports a robuststep-by-step mathematical and computational analysis of the entire prime modelconstruction process that determines a model from data in less than a minute.
arxiv-13500-262 | A Directional Diffusion Algorithm for Inpainting | http://arxiv.org/pdf/1511.03464v1.pdf | author:Jan Deriu, Rolf Jagerman, Kai-En Tsay category:cs.CV published:2015-11-11 summary:The problem of inpainting involves reconstructing the missing areas of animage. Inpainting has many applications, such as reconstructing old damagedphotographs or removing obfuscations from images. In this paper we present thedirectional diffusion algorithm for inpainting. Typical diffusion algorithmsare bad at propagating edges from the image into the unknown masked regions.The directional diffusion algorithm improves on the regular diffusion algorithmby reconstructing edges more accurately. It scores better than regulardiffusion when reconstructing images that are obfuscated by a text mask.
arxiv-13500-263 | Granger Causality in Multi-variate Time Series using a Time Ordered Restricted Vector Autoregressive Model | http://arxiv.org/pdf/1511.03463v1.pdf | author:Elsa Siggiridou, Dimitris Kugiumtzis category:stat.ME math.ST stat.CO stat.ML stat.TH published:2015-11-11 summary:Granger causality has been used for the investigation of the inter-dependencestructure of the underlying systems of multi-variate time series. Inparticular, the direct causal effects are commonly estimated by the conditionalGranger causality index (CGCI). In the presence of many observed variables andrelatively short time series, CGCI may fail because it is based on vectorautoregressive models (VAR) involving a large number of coefficients to beestimated. In this work, the VAR is restricted by a scheme that modifies therecently developed method of backward-in-time selection (BTS) of the laggedvariables and the CGCI is combined with BTS. Further, the proposed approach iscompared favorably to other restricted VAR representations, such as thetop-down strategy, the bottom-up strategy, and the least absolute shrinkage andselection operator (LASSO), in terms of sensitivity and specificity of CGCI.This is shown by using simulations of linear and nonlinear, low andhigh-dimensional systems and different time series lengths. For nonlinearsystems, CGCI from the restricted VAR representations are compared withanalogous nonlinear causality indices. Further, CGCI in conjunction with BTSand other restricted VAR representations is applied to multi-channel scalpelectroencephalogram (EEG) recordings of epileptic patients containingepileptiform discharges. CGCI on the restricted VAR, and BTS in particular,could track the changes in brain connectivity before, during and afterepileptiform discharges, which was not possible using the full VARrepresentation.
arxiv-13500-264 | Training Deep Gaussian Processes using Stochastic Expectation Propagation and Probabilistic Backpropagation | http://arxiv.org/pdf/1511.03405v1.pdf | author:Thang D. Bui, José Miguel Hernández-Lobato, Yingzhen Li, Daniel Hernández-Lobato, Richard E. Turner category:stat.ML published:2015-11-11 summary:Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisationsof Gaussian processes (GPs) and are formally equivalent to neural networks withmultiple, infinitely wide hidden layers. DGPs are probabilistic andnon-parametric and as such are arguably more flexible, have a greater capacityto generalise, and provide better calibrated uncertainty estimates thanalternative deep models. The focus of this paper is scalable approximateBayesian learning of these networks. The paper develops a novel and efficientextension of probabilistic backpropagation, a state-of-the-art method fortraining Bayesian neural networks, that can be used to train DGPs. The newmethod leverages a recently proposed method for scaling ExpectationPropagation, called stochastic Expectation Propagation. The method is able toautomatically discover useful input warping, expansion or compression, and itis therefore is a flexible form of Bayesian kernel design. We demonstrate thesuccess of the new method for supervised learning on several real-worlddatasets, showing that it typically outperforms GP regression and is never muchworse.
arxiv-13500-265 | A GMM-Based Stair Quality Model for Human Perceived JPEG Images | http://arxiv.org/pdf/1511.03398v1.pdf | author:Sudeng Hu, Haiqiang Wang, C. -C. Jay Kuo category:cs.MM cs.CV published:2015-11-11 summary:Based on the notion of just noticeable differences (JND), a stair qualityfunction (SQF) was recently proposed to model human perception on JPEG images.Furthermore, a k-means clustering algorithm was adopted to aggregate JND datacollected from multiple subjects to generate a single SQF. In this work, wepropose a new method to derive the SQF using the Gaussian Mixture Model (GMM).The newly derived SQF can be interpreted as a way to characterize the meanviewer experience. Furthermore, it has a lower information criterion (BIC)value than the previous one, indicating that it offers a better model. Aspecific example is given to demonstrate the advantages of the new approach.
arxiv-13500-266 | Seeing Behind the Camera: Identifying the Authorship of a Photograph | http://arxiv.org/pdf/1508.05038v2.pdf | author:Christopher Thomas, Adriana Kovashka category:cs.CV published:2015-08-20 summary:We introduce the novel problem of identifying the photographer behind thephotograph. To explore the feasibility of current computer vision techniques toaddress this problem, we created a new dataset of over 180,000 images taken by41 well-known photographers. Using this dataset, we examined the effectivenessof a variety of features (low and high-level, including CNN features) atidentifying the photographer. We also trained a new deep convolutional neuralnetwork for this task. Our results show that high-level features greatlyoutperform low-level features at this task. We provide qualitative resultsusing these learned models that give insight into our method's ability todistinguish between photographers, allow us to draw interesting conclusionsabout what specific photographers shoot, and demonstrate two applications ofour method.
arxiv-13500-267 | High Performance Latent Variable Models | http://arxiv.org/pdf/1510.06143v4.pdf | author:Aaron Q. Li, Amr Ahmed, Mu Li, Vanja Josifovski category:cs.LG cs.AI published:2015-10-21 summary:Latent variable models have accumulated a considerable amount of interestfrom the industry and academia for their versatility in a wide range ofapplications. A large amount of effort has been made to develop systems that isable to extend the systems to a large scale, in the hope to make use of them onindustry scale data. In this paper, we describe a system that operates at ascale orders of magnitude higher than previous works, and an order of magnitudefaster than state-of-the-art system at the same scale, at the same time showingmore robustness and more accurate results. Our system uses a number of advances in distributed inference: highperformance in synchronization of sufficient statistics with relaxedconsistency model; fast sampling, using the Metropolis-Hastings-Walker methodto overcome dense generative models; statistical modeling, moving beyond LatentDirichlet Allocation (LDA) to Pitman-Yor distributions (PDP) and HierarchicalDirichlet Process (HDP) models; sophisticated parameter projection schemes, toresolve the conflicts within the constraint between parameters arising from therelaxed consistency model. This work significantly extends the domain of applicability of what iscommonly known as the Parameter Server. We obtain results with up to hundredsbillion oftokens, thousands of topics, and a vocabulary of a few milliontoken-types, using up to 60,000 processor cores operating on a productioncluster of a large Internet company. This demonstrates the feasibility to scaleto problems orders of magnitude larger than any previously published work.
arxiv-13500-268 | Instance Optimal Learning | http://arxiv.org/pdf/1504.05321v2.pdf | author:Gregory Valiant, Paul Valiant category:cs.LG published:2015-04-21 summary:We consider the following basic learning task: given independent draws froman unknown distribution over a discrete support, output an approximation of thedistribution that is as accurate as possible in $\ell_1$ distance (i.e. totalvariation or statistical distance). Perhaps surprisingly, it is often possibleto "de-noise" the empirical distribution of the samples to return anapproximation of the true distribution that is significantly more accurate thanthe empirical distribution, without relying on any prior assumptions on thedistribution. We present an instance optimal learning algorithm which optimallyperforms this de-noising for every distribution for which such a de-noising ispossible. More formally, given $n$ independent draws from a distribution $p$,our algorithm returns a labelled vector whose expected distance from $p$ isequal to the minimum possible expected error that could be obtained by anyalgorithm that knows the true unlabeled vector of probabilities of distribution$p$ and simply needs to assign labels, up to an additive subconstant term thatis independent of $p$ and goes to zero as $n$ gets large. One conceptualimplication of this result is that for large samples, Bayesian assumptions onthe "shape" or bounds on the tail probabilities of a distribution over discretesupport are not helpful for the task of learning the distribution. As a consequence of our techniques, we also show that given a set of $n$samples from an arbitrary distribution, one can accurately estimate theexpected number of distinct elements that will be observed in a sample of anysize up to $n \log n$. This sort of extrapolation is practically relevant,particularly to domains such as genomics where it is important to understandhow much more might be discovered given larger sample sizes, and we areoptimistic that our approach is practically viable.
arxiv-13500-269 | Multimodal MRI Neuroimaging with Motion Compensation Based on Particle Filtering | http://arxiv.org/pdf/1511.03369v1.pdf | author:Yu-Hui Chen, Roni Mittelman, Boklye Kim, Charles Meyer, Alfred Hero category:cs.CV physics.med-ph published:2015-11-11 summary:Head movement during scanning impedes activation detection in fMRI studies.Head motion in fMRI acquired using slice-based Echo Planar Imaging (EPI) can beestimated and compensated by aligning the images onto a reference volumethrough image registration. However, registering EPI images volume to volumefails to consider head motion between slices, which may lead to severely biasedhead motion estimates. Slice-to-volume registration can be used to estimatemotion parameters for each slice by more accurately representing the imageacquisition sequence. However, accurate slice to volume mapping is dependent onthe information content of the slices: middle slices are information rich,while edge slides are information poor and more prone to distortion. In thiswork, we propose a Gaussian particle filter based head motion trackingalgorithm to reduce the image misregistration errors. The algorithm uses adynamic state space model of head motion with an observation equation thatmodels continuous slice acquisition of the scanner. Under this model theparticle filter provides more accurate motion estimates and voxel positionestimates. We demonstrate significant performance improvement of the proposedapproach as compared to registration-only methods of head motion estimation andbrain activation detection.
arxiv-13500-270 | Facial Expression Detection using Patch-based Eigen-face Isomap Networks | http://arxiv.org/pdf/1511.03363v1.pdf | author:Sohini Roychowdhury category:cs.CV published:2015-11-11 summary:Automated facial expression detection problem pose two primary challengesthat include variations in expression and facial occlusions (glasses, beard,mustache or face covers). In this paper we introduce a novel automated patchcreation technique that masks a particular region of interest in the face,followed by Eigen-value decomposition of the patched faces and generation ofIsomaps to detect underlying clustering patterns among faces. The proposedmasked Eigen-face based Isomap clustering technique achieves 75% sensitivityand 66-73% accuracy in classification of faces with occlusions and smilingfaces in around 1 second per image. Also, betweenness centrality, Eigencentrality and maximum information flow can be used as network-based measuresto identify the most significant training faces for expression classificationtasks. The proposed method can be used in combination with feature-basedexpression classification methods in large data sets for improving expressionclassification accuracies.
arxiv-13500-271 | Discovery Radiomics via StochasticNet Sequencers for Cancer Detection | http://arxiv.org/pdf/1511.03361v1.pdf | author:Mohammad Javad Shafiee, Audrey G. Chung, Devinder Kumar, Farzad Khalvati, Masoom Haider, Alexander Wong category:cs.CV cs.AI published:2015-11-11 summary:Radiomics has proven to be a powerful prognostic tool for cancer detection,and has previously been applied in lung, breast, prostate, and head-and-neckcancer studies with great success. However, these radiomics-driven methods relyon pre-defined, hand-crafted radiomic feature sets that can limit their abilityto characterize unique cancer traits. In this study, we introduce a noveldiscovery radiomics framework where we directly discover custom radiomicfeatures from the wealth of available medical imaging data. In particular, weleverage novel StochasticNet radiomic sequencers for extracting custom radiomicfeatures tailored for characterizing unique cancer tissue phenotype. UsingStochasticNet radiomic sequencers discovered using a wealth of lung CT data, weperform binary classification on 42,340 lung lesions obtained from the CT scansof 93 patients in the LIDC-IDRI dataset. Preliminary results show significantimprovement over previous state-of-the-art methods, indicating the potential ofthe proposed discovery radiomics framework for improving cancer screening anddiagnosis.
arxiv-13500-272 | Principal Autoparallel Analysis: Data Analysis in Weitzenböck Space | http://arxiv.org/pdf/1511.03355v1.pdf | author:Stephen Marsland, Carole J Twining category:stat.ME cs.CV math.DG published:2015-11-11 summary:The statistical analysis of data lying on a differentiable, locallyEuclidean, manifold introduces a variety of challenges because the analogousmeasures to standard Euclidean statistics are local, that is only definedwithin a neighbourhood of each datapoint. This is because the curvature of thespace means that the connection of Riemannian geometry is path dependent. Inthis paper we transfer the problem to Weitzenb\"{o}ck space, which has torsion,but not curvature, meaning that parallel transport is path independent, andrather than considering geodesics, it is natural to consider autoparallels,which are `straight' in the sense that they follow the local basis vectors. Wedemonstrate how to generate these autoparallels in a data-driven fashion, andshow that the resulting representation of the data is a useful space in whichto perform further analysis.
arxiv-13500-273 | Fusing Multi-Stream Deep Networks for Video Classification | http://arxiv.org/pdf/1509.06086v2.pdf | author:Zuxuan Wu, Yu-Gang Jiang, Xi Wang, Hao Ye, Xiangyang Xue, Jun Wang category:cs.CV cs.MM published:2015-09-21 summary:This paper studies deep network architectures to address the problem of videoclassification. A multi-stream framework is proposed to fully utilize the richmultimodal information in videos. Specifically, we first train threeConvolutional Neural Networks to model spatial, short-term motion and audioclues respectively. Long Short Term Memory networks are then adopted to explorelong-term temporal dynamics. With the outputs of the individual streams, wepropose a simple and effective fusion method to generate the final predictions,where the optimal fusion weights are learned adaptively for each class, and thelearning process is regularized by automatically estimated class relationships.Our contributions are two-fold. First, the proposed multi-stream framework isable to exploit multimodal features that are more comprehensive than thosepreviously attempted. Second, we demonstrate that the adaptive fusion methodusing the class relationship as a regularizer outperforms traditionalalternatives that estimate the weights in a "free" fashion. Our frameworkproduces significantly better results than the state of the arts on two popularbenchmarks, 92.2\% on UCF-101 (without using audio) and 84.9\% on ColumbiaConsumer Videos.
arxiv-13500-274 | Scalable Semi-Supervised Aggregation of Classifiers | http://arxiv.org/pdf/1506.05790v2.pdf | author:Akshay Balsubramani, Yoav Freund category:cs.LG published:2015-06-18 summary:We present and empirically evaluate an efficient algorithm that learns toaggregate the predictions of an ensemble of binary classifiers. The algorithmuses the structure of the ensemble predictions on unlabeled data to yieldsignificant performance improvements. It does this without making assumptionson the structure or origin of the ensemble, without parameters, and as scalablyas linear learning. We empirically demonstrate these performance gains withrandom forests.
arxiv-13500-275 | Attention to Scale: Scale-aware Semantic Image Segmentation | http://arxiv.org/pdf/1511.03339v1.pdf | author:Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, Alan L. Yuille category:cs.CV published:2015-11-10 summary:Incorporating multi-scale features to deep convolutional neural networks(DCNNs) has been a key element to achieve state-of-art performance on semanticimage segmentation benchmarks. One way to extract multi-scale features is byfeeding several resized input images to a shared deep network and then mergethe resulting multi-scale features for pixel-wise classification. In this work,we adapt a state-of-art semantic image segmentation model with multi-scaleinput images. We jointly train the network and an attention model which learnsto softly weight the multi-scale features, and show that it outperformsaverage- or max-pooling over scales. The proposed attention model allows us todiagnostically visualize the importance of features at different positions andscales. Moreover, we show that adding extra supervision to the output of DCNNfor each scale is essential to achieve excellent performance when mergingmulti-scale features. We demonstrate the effectiveness of our model withexhaustive experiments on three challenging datasets, includingPASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014.
arxiv-13500-276 | Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform | http://arxiv.org/pdf/1511.03328v1.pdf | author:Liang-Chieh Chen, Jonathan T. Barron, George Papandreou, Kevin Murphy, Alan L. Yuille category:cs.CV published:2015-11-10 summary:Deep convolutional neural networks (CNNs) are the backbone of state-of-artsemantic image segmentation systems. Recent work has shown that complementingCNNs with fully-connected conditional random fields (CRFs) can significantlyenhance their object localization accuracy, yet dense CRF inference iscomputationally expensive. We propose replacing the fully-connected CRF withdomain transform (DT), a modern edge-preserving filtering method in which theamount of smoothing is controlled by a reference edge map. Domain transformfiltering is several times faster than dense CRF inference and we show that ityields comparable semantic segmentation results, accurately capturing objectboundaries. Importantly, our formulation allows learning the reference edge mapfrom intermediate CNN features instead of using the image gradient magnitude asin standard DT filtering. This produces task-specific edges in an end-to-endtrainable system optimizing the target semantic segmentation quality.
arxiv-13500-277 | Anchored Discrete Factor Analysis | http://arxiv.org/pdf/1511.03299v1.pdf | author:Yoni Halpern, Steven Horng, David Sontag category:stat.ML cs.LG published:2015-11-10 summary:We present a semi-supervised learning algorithm for learning discrete factoranalysis models with arbitrary structure on the latent variables. Our algorithmassumes that every latent variable has an "anchor", an observed variable withonly that latent variable as its parent. Given such anchors, we show that it ispossible to consistently recover moments of the latent variables and use thesemoments to learn complete models. We also introduce a new technique forimproving the robustness of method-of-moment algorithms by optimizing over themarginal polytope or its relaxations. We evaluate our algorithm using tworeal-world tasks, tag prediction on questions from the Stack Overflow websiteand medical diagnosis in an emergency department.
arxiv-13500-278 | The Fast Bilateral Solver | http://arxiv.org/pdf/1511.03296v1.pdf | author:Jonathan T. Barron, Ben Poole category:cs.CV published:2015-11-10 summary:We present the bilateral solver, a novel algorithm for edge-aware smoothingthat combines the flexibility and speed of simple filtering approaches with theaccuracy of domain-specific optimization algorithms. Our technique is capableof matching or improving upon state-of-the-art results on several differentcomputer vision tasks (stereo, depth superresolution, colorization, andsemantic segmentation) while being 10-1000 times faster than competingapproaches. The bilateral solver is fast, robust, straightforward to generalizeto new domains, and simple to integrate into deep learning pipelines.
arxiv-13500-279 | From Images to Sentences through Scene Description Graphs using Commonsense Reasoning and Knowledge | http://arxiv.org/pdf/1511.03292v1.pdf | author:Somak Aditya, Yezhou Yang, Chitta Baral, Cornelia Fermuller, Yiannis Aloimonos category:cs.CV cs.AI cs.CL I.2.10 published:2015-11-10 summary:In this paper we propose the construction of linguistic descriptions ofimages. This is achieved through the extraction of scene description graphs(SDGs) from visual scenes using an automatically constructed knowledge base.SDGs are constructed using both vision and reasoning. Specifically, commonsensereasoning is applied on (a) detections obtained from existing perceptionmethods on given images, (b) a "commonsense" knowledge base constructed usingnatural language processing of image annotations and (c) lexical ontologicalknowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-basedevaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in mostcases, sentences auto-constructed from SDGs obtained by our method give a morerelevant and thorough description of an image than a recent state-of-the-artimage caption based approach. Our Image-Sentence Alignment Evaluation resultsare also comparable to that of the recent state-of-the art approaches.
arxiv-13500-280 | Online Supervised Hashing for Ever-Growing Datasets | http://arxiv.org/pdf/1511.03257v1.pdf | author:Fatih Cakir, Sarah Adel Bargal, Stan Sclaroff category:cs.CV published:2015-11-10 summary:Supervised hashing methods are widely-used for nearest neighbor search incomputer vision applications. Most state-of-the-art supervised hashingapproaches employ batch-learners. Unfortunately, batch-learning strategies canbe inefficient when confronted with large training datasets. Moreover, withbatch-learners, it is unclear how to adapt the hash functions as a datasetcontinues to grow and diversify over time. Yet, in many practical scenarios thedataset grows and diversifies; thus, both the hash functions and the indexingmust swiftly accommodate these changes. To address these issues, we propose anonline hashing method that is amenable to changes and expansions of thedatasets. Since it is an online algorithm, our approach offers linearcomplexity with the dataset size. Our solution is supervised, in that weincorporate available label information to preserve the semantic neighborhood.Such an adaptive hashing method is attractive; but it requires recomputing thehash table as the hash functions are updated. If the frequency of update ishigh, then recomputing the hash table entries may cause inefficiencies in thesystem, especially for large indexes. Thus, we also propose a framework toreduce hash table updates. We compare our method to state-of-the-art solutionson two benchmarks and demonstrate significant improvements over previous work.
arxiv-13500-281 | StochasticNet: Forming Deep Neural Networks via Stochastic Connectivity | http://arxiv.org/pdf/1508.05463v4.pdf | author:Mohammad Javad Shafiee, Parthipan Siva, Alexander Wong category:cs.CV cs.LG cs.NE published:2015-08-22 summary:Deep neural networks is a branch in machine learning that has seen a meteoricrise in popularity due to its powerful abilities to represent and modelhigh-level abstractions in highly complex data. One area in deep neuralnetworks that is ripe for exploration is neural connectivity formation. Apivotal study on the brain tissue of rats found that synaptic formation forspecific functional connectivity in neocortical neural microcircuits can besurprisingly well modeled and predicted as a random formation. Motivated bythis intriguing finding, we introduce the concept of StochasticNet, where deepneural networks are formed via stochastic connectivity between neurons. As aresult, any type of deep neural networks can be formed as a StochasticNet byallowing the neuron connectivity to be stochastic. Stochastic synapticformations, in a deep neural network architecture, can allow for efficientutilization of neurons for performing specific tasks. To evaluate thefeasibility of such a deep neural network architecture, we train aStochasticNet using four different image datasets (CIFAR-10, MNIST, SVHN, andSTL-10). Experimental results show that a StochasticNet, using less than halfthe number of neural connections as a conventional deep neural network,achieves comparable accuracy and reduces overfitting on the CIFAR-10, MNIST andSVHN dataset. Interestingly, StochasticNet with less than half the number ofneural connections, achieved a higher accuracy (relative improvement in testerror rate of ~6% compared to ConvNet) on the STL-10 dataset than aconventional deep neural network. Finally, StochasticNets have fasteroperational speeds while achieving better or similar accuracy performances.
arxiv-13500-282 | What's the point: Semantic segmentation with point supervision | http://arxiv.org/pdf/1506.02106v4.pdf | author:Amy Bearman, Olga Russakovsky, Vittorio Ferrari, Li Fei-Fei category:cs.CV published:2015-06-06 summary:The semantic image segmentation task presents a trade-off between testaccuracy and the cost of obtaining training annotations. Detailed per-pixelannotations enable training accurate models but are very expensive to obtain;image-level class labels are an order of magnitude cheaper but result in lessaccurate models. We take a natural step from image-level annotation towardsstronger supervision: we ask annotators to point to an object if one exists. Wedemonstrate that this adds negligible additional annotation cost. Weincorporate this point supervision along with a novel objectness potential inthe training loss function of a state-of-the-art CNN model. The combined effectof these two extensions is a 12.9% increase in mean intersection over union onthe PASCAL VOC 2012 segmentation task compared to a CNN model trained with onlyimage-level labels.
arxiv-13500-283 | Are Slepian-Wolf Rates Necessary for Distributed Parameter Estimation? | http://arxiv.org/pdf/1508.02765v2.pdf | author:Mostafa El Gamal, Lifeng Lai category:cs.IT math.IT stat.ML published:2015-08-11 summary:We consider a distributed parameter estimation problem, in which multipleterminals send messages related to their local observations using limited ratesto a fusion center who will obtain an estimate of a parameter related toobservations of all terminals. It is well known that if the transmission ratesare in the Slepian-Wolf region, the fusion center can fully recover allobservations and hence can construct an estimator having the same performanceas that of the centralized case. One natural question is whether Slepian-Wolfrates are necessary to achieve the same estimation performance as that of thecentralized case. In this paper, we show that the answer to this question isnegative. We establish our result by explicitly constructing an asymptoticallyminimum variance unbiased estimator (MVUE) that has the same performance asthat of the optimal estimator in the centralized case while requiringinformation rates less than the conditions required in the Slepian-Wolf rateregion.
arxiv-13500-284 | Fast Algorithms for Convolutional Neural Networks | http://arxiv.org/pdf/1509.09308v2.pdf | author:Andrew Lavin, Scott Gray category:cs.NE cs.LG I.2.6; F.2.1 published:2015-09-30 summary:Deep convolutional neural networks take GPU days of compute time to train onlarge data sets. Pedestrian detection for self driving cars requires very lowlatency. Image recognition for mobile phones is constrained by limitedprocessing resources. The success of convolutional neural networks in thesesituations is limited by how fast we can compute them. Conventional FFT basedconvolution is fast for large filters, but state of the art convolutionalneural networks use small, 3x3 filters. We introduce a new class of fastalgorithms for convolutional neural networks using Winograd's minimal filteringalgorithms. The algorithms compute minimal complexity convolution over smalltiles, which makes them fast with small filters and small batch sizes. Webenchmark a GPU implementation of our algorithm with the VGG network and showstate of the art throughput at batch sizes from 1 to 64.
arxiv-13500-285 | A Distributed One-Step Estimator | http://arxiv.org/pdf/1511.01443v2.pdf | author:Cheng Huang, Xiaoming Huo category:stat.ME cs.DC stat.ML published:2015-11-04 summary:Distributed statistical inference has recently attracted enormous attention.Many existing work focuses on the averaging estimator. We propose a one-stepapproach to enhance a simple-averaging based distributed estimator. We derivethe corresponding asymptotic properties of the newly proposed estimator. Wefind that the proposed one-step estimator enjoys the same asymptotic propertiesas the centralized estimator. The proposed one-step approach merely requiresone additional round of communication in relative to the averaging estimator;so the extra communication burden is insignificant. In finite sample cases,numerical examples show that the proposed estimator outperforms the simpleaveraging estimator with a large margin in terms of the mean squared errors. Apotential application of the one-step approach is that one can use multiplemachines to speed up large scale statistical inference with little compromisein the quality of estimators. The proposed method becomes more valuable whendata can only be available at distributed machines with limited communicationbandwidth.
arxiv-13500-286 | TemplateNet for Depth-Based Object Instance Recognition | http://arxiv.org/pdf/1511.03244v1.pdf | author:Ujwal Bonde, Vijay Badrinarayanan, Roberto Cipolla, Minh-Tri Pham category:cs.CV published:2015-11-10 summary:We present a novel deep architecture termed templateNet for depth basedobject instance recognition. Using an intermediate template layer we exploitprior knowledge of an object's shape to sparsify the feature maps. This hasthree advantages: (i) the network is better regularised resulting in structuredfilters; (ii) the sparse feature maps results in intuitive features been learntwhich can be visualized as the output of the template layer and (iii) theresulting network achieves state-of-the-art performance. The network benefitsfrom this without any additional parametrization from the template layer. Wederive the weight updates needed to efficiently train this network in anend-to-end manner. We benchmark the templateNet for depth based object instancerecognition using two publicly available datasets. The datasets presentmultiple challenges of clutter, large pose variations and similar lookingdistractors. Through our experiments we show that with the addition of atemplate layer, a depth based CNN is able to outperform existingstate-of-the-art methods in the field.
arxiv-13500-287 | The Radon cumulative distribution transform and its application to image classification | http://arxiv.org/pdf/1511.03206v1.pdf | author:Soheil Kolouri, Se Rim Park, Gustavo K. Rohde category:cs.CV published:2015-11-10 summary:Invertible image representation methods (transforms) are routinely employedas low-level image processing operations based on which feature extraction andrecognition algorithms are developed. Most transforms in current use (e.g.Fourier, Wavelet, etc.) are linear transforms, and, by themselves, are unableto substantially simplify the representation of image classes forclassification. Here we describe a nonlinear, invertible, low-level imageprocessing transform based on combining the well known Radon transform forimage data, and the 1D Cumulative Distribution Transform proposed earlier. Wedescribe a few of the properties of this new transform, and with boththeoretical and experimental results show that it can often render certainproblems linearly separable in transform space.
arxiv-13500-288 | Sliced Wasserstein Kernels for Probability Distributions | http://arxiv.org/pdf/1511.03198v1.pdf | author:Soheil Kolouri, Yang Zou, Gustavo K. Rohde category:cs.LG stat.ML published:2015-11-10 summary:Optimal transport distances, otherwise known as Wasserstein distances, haverecently drawn ample attention in computer vision and machine learning as apowerful discrepancy measure for probability distributions. The recentdevelopments on alternative formulations of the optimal transport have allowedfor faster solutions to the problem and has revamped its practical applicationsin machine learning. In this paper, we exploit the widely used kernel methodsand provide a family of provably positive definite kernels based on the SlicedWasserstein distance and demonstrate the benefits of these kernels in a varietyof learning tasks. Our work provides a new perspective on the application ofoptimal transport flavored distances through kernel methods in machine learningtasks.
arxiv-13500-289 | Fame for sale: efficient detection of fake Twitter followers | http://arxiv.org/pdf/1509.04098v2.pdf | author:Stefano Cresci, Roberto Di Pietro, Marinella Petrocchi, Angelo Spognardi, Maurizio Tesconi category:cs.SI cs.CR cs.LG H.2.8 published:2015-09-14 summary:$\textit{Fake followers}$ are those Twitter accounts specifically created toinflate the number of followers of a target account. Fake followers aredangerous for the social platform and beyond, since they may alter conceptslike popularity and influence in the Twittersphere - hence impacting oneconomy, politics, and society. In this paper, we contribute along differentdimensions. First, we review some of the most relevant existing features andrules (proposed by Academia and Media) for anomalous Twitter accountsdetection. Second, we create a baseline dataset of verified human and fakefollower accounts. Such baseline dataset is publicly available to thescientific community. Then, we exploit the baseline dataset to train a set ofmachine-learning classifiers built over the reviewed rules and features. Ourresults show that most of the rules proposed by Media provide unsatisfactoryperformance in revealing fake followers, while features proposed in the past byAcademia for spam detection provide good results. Building on the mostpromising features, we revise the classifiers both in terms of reduction ofoverfitting and cost for gathering the data needed to compute the features. Thefinal result is a novel $\textit{Class A}$ classifier, general enough to thwartoverfitting, lightweight thanks to the usage of the less costly features, andstill able to correctly classify more than 95% of the accounts of the originaltraining set. We ultimately perform an information fusion-based sensitivityanalysis, to assess the global sensitivity of each of the features employed bythe classifier. The findings reported in this paper, other than being supportedby a thorough experimental methodology and interesting on their own, also pavethe way for further investigation on the novel issue of fake Twitter followers.
arxiv-13500-290 | Dynamic Belief Fusion for Object Detection | http://arxiv.org/pdf/1511.03183v1.pdf | author:Hyungtae Lee, Heesung Kwon, Ryan M. Robinson, William d. Nothwang, Amar M. Marathe category:cs.CV published:2015-11-10 summary:A novel approach for the fusion of heterogeneous object detection methods isproposed. In order to effectively integrate the outputs of multiple detectors,the level of ambiguity in each individual detection score is estimated usingthe precision/recall relationship of the corresponding detector. The maincontribution of the proposed work is a novel fusion method, called DynamicBelief Fusion (DBF), which dynamically assigns probabilities to hypotheses(target, non-target, intermediate state (target or non-target)) based onconfidence levels in the detection results conditioned on the prior performanceof individual detectors. In DBF, a joint basic probability assignment,optimally fusing information from all detectors, is determined by theDempster's combination rule, and is easily reduced to a single fused detectionscore. Experiments on ARL and PASCAL VOC 07 datasets demonstrate that thedetection accuracy of DBF is considerably greater than conventional fusionapproaches as well as individual detectors used for the fusion.
arxiv-13500-291 | Kernel Methods for Accurate UWB-Based Ranging with Reduced Complexity | http://arxiv.org/pdf/1511.04045v1.pdf | author:Vladimir Savic, Erik G. Larsson, Javier Ferrer-Coll, Peter Stenumgaard category:cs.LG cs.IT math.IT published:2015-11-10 summary:Accurate and robust positioning in multipath environments can enable manyapplications, such as search-and-rescue and asset tracking. For this problem,ultra-wideband (UWB) technology can provide the most accurate range estimates,which are required for range-based positioning. However, UWB still faces aproblem with non-line-of-sight (NLOS) measurements, in which the rangeestimates based on time-of-arrival (TOA) will typically be positively biased.There are many techniques that address this problem, mainly based on NLOSidentification and NLOS error mitigation algorithms. However, these techniquesdo not exploit all available information in the UWB channel impulse response.Kernel-based machine learning methods, such as Gaussian Process Regression(GPR), are able to make use of all information, but they may be too complex intheir original form. In this paper, we propose novel ranging methods based onkernel principal component analysis (kPCA), in which the selected channelparameters are projected onto a nonlinear orthogonal high-dimensional space,and a subset of these projections is then used as an input for ranging. Weevaluate the proposed methods using real UWB measurements obtained in abasement tunnel, and found that one of the proposed methods is able tooutperform state-of-the-art, even if little training samples are available.
arxiv-13500-292 | Asynchronous Decentralized 20 Questions for Adaptive Search | http://arxiv.org/pdf/1511.03144v1.pdf | author:Theodoros Tsiligkaridis category:cs.MA cs.IT cs.SY math.IT stat.ML published:2015-11-10 summary:This paper considers the problem of adaptively searching for an unknowntarget using multiple agents connected through a time-varying network topology.Agents are equipped with sensors capable of fast information processing, and wepropose an asynchronous decentralized algorithm for controlling their searchgiven noisy observations. Specifically, we propose asynchronous decentralizedextensions of the adaptive query-based search strategy that combines elementsfrom the 20 questions approach and social learning. Under standard assumptionson the time-varying network dynamics, we prove convergence to correct consensuson the value of the parameter as the number of iterations go to infinity. Thisframework provides a flexible and tractable mathematical model for asynchronousdecentralized parameter estimation systems based on adaptively-designedqueries. Our results establish that stability and consistency can be maintainedeven with one-way updating and randomized pairwise averaging, thus providing ascalable low complexity alternative to the synchronous decentralized estimationalgorithm studied in Tsiligkaridis et al [1]. We illustrate the effectivenessand robustness of our algorithm for random network topologies.
arxiv-13500-293 | USFD: Twitter NER with Drift Compensation and Linked Data | http://arxiv.org/pdf/1511.03088v1.pdf | author:Leon Derczynski, Isabelle Augenstein, Kalina Bontcheva category:cs.CL published:2015-11-10 summary:This paper describes a pilot NER system for Twitter, comprising the USFDsystem entry to the W-NUT 2015 NER shared task. The goal is to correctly labelentities in a tweet dataset, using an inventory of ten types. We employstructured learning, drawing on gazetteers taken from Linked Data, and onunsupervised clustering features, and attempting to compensate for stylisticand topic drift - a key challenge in social media text. Our result iscompetitive; we provide an analysis of the components of our methodology, andan examination of the target dataset in the context of this task.
arxiv-13500-294 | The CTU Prague Relational Learning Repository | http://arxiv.org/pdf/1511.03086v1.pdf | author:Jan Motl, Oliver Schulte category:cs.LG cs.DB I.2.6; H.2.8 published:2015-11-10 summary:The aim of the CTU Prague Relational Learning Repository is to supportmachine learning research with multi-relational data. The repository currentlycontains 50 SQL databases hosted on a public MySQL server located atrelational.fit.cvut.cz. A searchable meta-database provides metadata (e.g., thenumber of tables in the database, the number of rows and columns in the tables,the number of foreign key constraints between tables).
arxiv-13500-295 | Dynamics of Human Cooperation in Economic Games | http://arxiv.org/pdf/1508.05288v3.pdf | author:Martin Spanknebel, Klaus Pawelzik category:physics.soc-ph cs.GT cs.LG math.DS published:2015-08-21 summary:Human decision behaviour is quite diverse. In many games humans on average donot achieve maximal payoff and the behaviour of individual players remainsinhomogeneous even after playing many rounds. For instance, in repeatedprisoner dilemma games humans do not always optimize their mean reward andfrequently exhibit broad distributions of cooperativity. The reasons for thesefailures of maximization are not known. Here we show that the dynamicsresulting from the tendency to shift choice probabilities towards previouslyrewarding choices in closed loop interaction with the strategy of the opponentcan not only explain systematic deviations from 'rationality', but alsoreproduce the diversity of choice behaviours. As a representative example weinvestigate the dynamics of choice probabilities in prisoner dilemma games withopponents using strategies with different degrees of extortion and generosity.We find that already a simple model for human learning can account for asurprisingly wide range of human decision behaviours. It reproduces suppressionof cooperation against extortionists and increasing cooperation when playingwith generous opponents, explains the broad distributions of individual choicesin ensembles of players, and predicts the evolution of individual subjects'cooperation rates over the course of the games. We conclude that importantaspects of human decision behaviours are rooted in elementary learningmechanisms realised in the brain.
arxiv-13500-296 | Tiny Descriptors for Image Retrieval with Unsupervised Triplet Hashing | http://arxiv.org/pdf/1511.03055v1.pdf | author:Jie Lin, Olivier Morère, Julie Petta, Vijay Chandrasekhar, Antoine Veillard category:cs.IR cs.CV cs.LG 68P20 H.3.3; I.2.6 published:2015-11-10 summary:A typical image retrieval pipeline starts with the comparison of globaldescriptors from a large database to find a short list of candidate matches. Agood image descriptor is key to the retrieval pipeline and should reconcile twocontradictory requirements: providing recall rates as high as possible andbeing as compact as possible for fast matching. Following the recent successesof Deep Convolutional Neural Networks (DCNN) for large scale imageclassification, descriptors extracted from DCNNs are increasingly used in placeof the traditional hand crafted descriptors such as Fisher Vectors (FV) withbetter retrieval performances. Nevertheless, the dimensionality of a typicalDCNN descriptor --extracted either from the visual feature pyramid or thefully-connected layers-- remains quite high at several thousands of scalarvalues. In this paper, we propose Unsupervised Triplet Hashing (UTH), a fullyunsupervised method to compute extremely compact binary hashes --in the 32-256bits range-- from high-dimensional global descriptors. UTH consists of twosuccessive deep learning steps. First, Stacked Restricted Boltzmann Machines(SRBM), a type of unsupervised deep neural nets, are used to learn binaryembedding functions able to bring the descriptor size down to the desiredbitrate. SRBMs are typically able to ensure a very high compression rate at theexpense of loosing some desirable metric properties of the original DCNNdescriptor space. Then, triplet networks, a rank learning scheme based onweight sharing nets is used to fine-tune the binary embedding functions toretain as much as possible of the useful metric properties of the originalspace. A thorough empirical evaluation conducted on multiple publicly availabledataset using DCNN descriptors shows that our method is able to significantlyoutperform state-of-the-art unsupervised schemes in the target bit range.
arxiv-13500-297 | Investigating the stylistic relevance of adjective and verb simile markers | http://arxiv.org/pdf/1511.03053v1.pdf | author:Suzanne Mpouli, Jean-Gabriel Ganascia category:cs.CL published:2015-11-10 summary:Similes play an important role in literary texts not only as rhetoricaldevices and as figures of speech but also because of their evocative power,their aptness for description and the relative ease with which they can becombined with other figures of speech (Israel et al. 2004). Detecting all typesof simile constructions in a particular text therefore seems crucial whenanalysing the style of an author. Few research studies however have beendedicated to the study of less prominent simile markers in fictional prose andtheir relevance for stylistic studies. The present paper studies the frequencyof adjective and verb simile markers in a corpus of British and French novelsin order to determine which ones are really informative and worth including ina stylistic analysis. Furthermore, are those adjectives and verb simile markersused differently in both languages?
arxiv-13500-298 | Online Action Recognition based on Incremental Learning of Weighted Covariance Descriptors | http://arxiv.org/pdf/1511.03028v1.pdf | author:Chang Tang, Wanqing Li, Chunping Hou, Pichao Wang, Yonghong Hou, Jing Zhang, Philip O. Ogunbona category:cs.CV published:2015-11-10 summary:Online action recognition aims to recognize actions from unsegmented streamsof data in a continuous manner. One of the challenges in online recognition isthe accumulation of evidence for decision making. This paper presents a fastand efficient online method to recognize actions from a stream of noisyskeleton data. The method adopts a covariance descriptor calculated fromskeleton data and is based on a novel method developed for incrementallylearning the covariance descriptors, referred to as weighted covariancedescriptors, so that past frames have less contributions to the descriptor andcurrent frames and informative frames such as key frames contributes moretowards the descriptor. The online recognition is achieved using an efficientnearest neighbour search against a set of trained actions. Experimental resultson MSRC-12 Kinect Gesture dataset and our newly collocated online actionrecognition dataset have demonstrated the efficacy of the proposed method.
arxiv-13500-299 | 3D Time-lapse Reconstruction from Internet Photos | http://arxiv.org/pdf/1511.03019v1.pdf | author:Ricardo Martin-Brualla, David Gallup, Steven M. Seitz category:cs.CV published:2015-11-10 summary:Given an Internet photo collection of a landmark, we compute a 3D time-lapsevideo sequence where a virtual camera moves continuously in time and space.While previous work assumed a static camera, the addition of camera motionduring the time-lapse creates a very compelling impression of parallax.Achieving this goal, however, requires addressing multiple technicalchallenges, including solving for time-varying depth maps, regularizing 3Dpoint color profiles over time, and reconstructing high quality, hole-freeimages at every frame from the projected profiles. Our results showphotorealistic time-lapses of skylines and natural scenes over many years, withdramatic parallax effects.
arxiv-13500-300 | Deep Representation of Facial Geometric and Photometric Attributes for Automatic 3D Facial Expression Recognition | http://arxiv.org/pdf/1511.03015v1.pdf | author:Huibin Li, Jian Sun, Dong Wang, Zongben Xu, Liming Chen category:cs.CV published:2015-11-10 summary:In this paper, we present a novel approach to automatic 3D Facial ExpressionRecognition (FER) based on deep representation of facial 3D geometric and 2Dphotometric attributes. A 3D face is firstly represented by its geometric andphotometric attributes, including the geometry map, normal maps, normalizedcurvature map and texture map. These maps are then fed into a pre-trained deepconvolutional neural network to generate the deep representation. Then thefacial expression prediction is simplyachieved by training linear SVMs over thedeep representation for different maps and fusing these SVM scores. Thevisualizations show that the deep representation provides a complete and highlydiscriminative coding scheme for 3D faces. Comprehensive experiments on theBU-3DFE database demonstrate that the proposed deep representation canoutperform the widely used hand-crafted descriptors (i.e., LBP, SIFT, HOG,Gabor) and the state-of-art approaches under the same experimental protocols.
