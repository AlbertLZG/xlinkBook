arxiv-17700-1 | Towards Reduced Reference Parametric Models for Estimating Audiovisual Quality in Multimedia Services | http://arxiv.org/pdf/1604.07211v1.pdf | author:Edip Demirbilek, Jean-Charles GrÃ©goire category:cs.MM cs.LG published:2016-04-25 summary:We have developed reduced reference parametric models for estimatingperceived quality in audiovisual multimedia services. We have created 144unique configurations for audiovisual content including various application andnetwork parameters such as bitrates and distortions in terms of bandwidth,packet loss rate and jitter. To generate the data needed for model training andvalidation we have tasked 24 subjects, in a controlled environment, to rate theoverall audiovisual quality on the absolute category rating (ACR) 5-levelquality scale. We have developed models using Random Forest and Neural Networkbased machine learning methods in order to estimate Mean Opinion Scores (MOS)values. We have used information retrieved from the packet headers and sideinformation provided as network parameters for model training. Random Forestbased models have performed better in terms of Root Mean Square Error (RMSE)and Pearson correlation coefficient. The side information proved to be veryeffective in developing the model. We have found that, while the modelperformance might be improved by replacing the side information with moreaccurate bit stream level measurements, they are performing well in estimatingperceived quality in audiovisual multimedia services.
arxiv-17700-2 | Nonparametric Bayesian Negative Binomial Factor Analysis | http://arxiv.org/pdf/1604.07464v1.pdf | author:Mingyuan Zhou category:stat.ME stat.ML published:2016-04-25 summary:A common approach to analyze an attribute-instance count matrix, an elementof which represents how many times an attribute appears in an instance, is tofactorize it under the Poisson likelihood. We show its limitation in capturingthe tendency for an attribute present in an instance to both repeat itself andexcite related ones. To address this limitation, we construct negative binomialfactor analysis (NBFA) to factorize the matrix under the negative binomiallikelihood, and relate it to a Dirichlet-multinomial distribution basedmixed-membership model. To support countably infinite factors, we propose thehierarchical gamma-negative binomial process. By exploiting newly provedconnections between discrete distributions, we construct two blocked and acollapsed Gibbs sampler that all adaptively truncate their number of factors,and demonstrate that the blocked Gibbs sampler developed under a compoundPoisson representation converges fast and has low computational complexity.Example results show that NBFA has a distinct mechanism in adjusting its numberof inferred factors according to the instance lengths, and provides clearadvantages in parsimonious representation, predictive power, and computationalcomplexity over previously proposed discrete latent variable models, whicheither completely ignore burstiness, or model only the burstiness of theattributes but not that of the factors.
arxiv-17700-3 | Observing and Recommending from a Social Web with Biases | http://arxiv.org/pdf/1604.07180v1.pdf | author:Steffen Staab, Sophie Stalla-Bourdillon, Laura Carmichael category:cs.DB cs.LG K.5.0; H.2.8 published:2016-04-25 summary:The research question this report addresses is: how, and to what extent,those directly involved with the design, development and employment of aspecific black box algorithm can be certain that it is not unlawfullydiscriminating (directly and/or indirectly) against particular persons withprotected characteristics (e.g. gender, race and ethnicity)?
arxiv-17700-4 | Dynamic Pricing with Demand Covariates | http://arxiv.org/pdf/1604.07463v1.pdf | author:Sheng Qiang, Mohsen Bayati category:stat.ML published:2016-04-25 summary:We consider a firm that sells products over $T$ periods without knowing thedemand function. The firm sequentially sets prices to earn revenue and to learnthe underlying demand function simultaneously. A natural heuristic for thisproblem, commonly used in practice, is greedy iterative least squares (GILS).At each time period, GILS estimates the demand as a linear function of theprice by applying least squares to the set of prior prices and realizeddemands. Then a price that maximizes the revenue, given the estimated demandfunction, is used for the next time period. The performance is measured by theregret, which is the expected revenue loss from the optimal (oracle) pricingpolicy when the demand function is known. Recently, den Boer and Zwart (2014)and Keskin and Zeevi (2014) demonstrated that GILS is sub-optimal. Theyintroduced algorithms which integrate forced price dispersion with GILS andachieve asymptotically optimal performance. In this paper, we consider this dynamic pricing problem in a data-richenvironment. In particular, we assume that the firm knows the expected demandunder a particular price from historical data, and in each period, beforesetting the price, the firm has access to extra information (demand covariates)which may be predictive of the demand. We prove that in this setting GILSachieves asymptotically optimal regret of order $\log(T)$. We also show thefollowing surprising result: in the original dynamic pricing problem of denBoer and Zwart (2014) and Keskin and Zeevi (2014), inclusion of any set ofcovariates in GILS as potential demand covariates (even though they could carryno information) would make GILS asymptotically optimal. We validate our resultsvia extensive numerical simulations on synthetic and real data sets.
arxiv-17700-5 | Makeup like a superstar: Deep Localized Makeup Transfer Network | http://arxiv.org/pdf/1604.07102v1.pdf | author:Si Liu, Xinyu Ou, Ruihe Qian, Wei Wang, Xiaochun Cao category:cs.CV cs.AI published:2016-04-25 summary:In this paper, we propose a novel Deep Localized Makeup Transfer Network toautomatically recommend the most suitable makeup for a female and synthesis themakeup on her face. Given a before-makeup face, her most suitable makeup isdetermined automatically. Then, both the beforemakeup and the reference facesare fed into the proposed Deep Transfer Network to generate the after-makeupface. Our end-to-end makeup transfer network have several nice propertiesincluding: (1) with complete functions: including foundation, lip gloss, andeye shadow transfer; (2) cosmetic specific: different cosmetics are transferredin different manners; (3) localized: different cosmetics are applied ondifferent facial regions; (4) producing naturally looking results withoutobvious artifacts; (5) controllable makeup lightness: various results fromlight makeup to heavy makeup can be generated. Qualitative and quantitativeexperiments show that our network performs much better than the methods of [Guoand Sim, 2009] and two variants of NerualStyle [Gatys et al., 2015a].
arxiv-17700-6 | Modeling the Contribution of Central Versus Peripheral Vision in Scene, Object, and Face Recognition | http://arxiv.org/pdf/1604.07457v1.pdf | author:Panqu Wang, Garrison Cottrell category:q-bio.NC cs.CV published:2016-04-25 summary:It is commonly believed that the central visual field is important forrecognizing objects and faces, and the peripheral region is useful for scenerecognition. However, the relative importance of central versus peripheralinformation for object, scene, and face recognition is unclear. In a behavioralstudy, Larson and Loschky (2009) investigated this question by measuring thescene recognition accuracy as a function of visual angle, and demonstrated thatperipheral vision was indeed more useful in recognizing scenes than centralvision. In this work, we modeled and replicated the result of Larson andLoschky (2009), using deep convolutional neural networks. Having fit the datafor scenes, we used the model to predict future data for large-scale scenerecognition as well as for objects and faces. Our results suggest that therelative order of importance of using central visual field information is facerecognition>object recognition>scene recognition, and vice-versa for peripheralinformation.
arxiv-17700-7 | Learning Local Dependence In Ordered Data | http://arxiv.org/pdf/1604.07451v1.pdf | author:Guo Yu, Jacob Bien category:math.ST stat.CO stat.ME stat.ML stat.TH published:2016-04-25 summary:In many applications, data comes with a natural ordering. This ordering canoften induce local dependence among nearby variables. However, in complex data,the width of this dependence may vary, making simple assumptions such as aconstant neighborhood size unrealistic. We propose a framework for learningthis local dependence based on estimating the inverse of the Cholesky factor ofthe covariance matrix. Penalized maximum likelihood estimation of this matrixyields a simple regression interpretation for local dependence in whichvariables are predicted by their neighbors. Our proposed method involvessolving a convex, penalized Gaussian likelihood problem with a hierarchicalgroup lasso penalty. The problem decomposes into independent subproblems whichcan be solved efficiently in parallel using first-order methods. Our methodyields a sparse, symmetric, positive definite estimator of the precisionmatrix, encoding a Gaussian graphical model. We derive theoretical results notfound in existing methods attaining this structure. In particular, ourconditions for signed support recovery and estimation consistency rates inmultiple norms are as mild as those in a regression problem. Empirical resultsshow our method performing favorably compared to existing methods. We apply ourmethod to genomic data to flexibly model linkage disequilibrium.
arxiv-17700-8 | Towards Real-Time, Country-Level Location Classification of Worldwide Tweets | http://arxiv.org/pdf/1604.07236v1.pdf | author:Arkaitz Zubiaga, Alex Voss, Rob Procter, Maria Liakata, Bo Wang, Adam Tsakalidis category:cs.IR cs.CL cs.SI published:2016-04-25 summary:With the increase of interest in using social media as a source for research,many have tackled the task of automatically geolocating tweets, motivated bythe lack of explicit location information in the majority of tweets. Whileothers have focused on state- or city-level classification of tweets restrictedto a specific country, here we undertake the task in a broader context byclassifying global tweets at the country level, so far unexplored in areal-time scenario. We analyse the extent to which a tweet's country of origincan be determined by making use of eight tweet-inherent features forclassification using Support Vector Machines. Furthermore, we use two datasets,collected a year apart from each other, to analyse the extent to which a modeltrained from historical tweets can still be leveraged for classification of newtweets. With classification experiments on all 217 countries in our datasets,as well as on the top 25 countries, we offer some insights into the best use oftweet-inherent features for an accurate country-level classification of tweets.Among the features inherent in a tweet, we observe that the validity ofhistorical tweet content fades over time, and other metadata associated withthe tweet, such as the language of the tweet, the name of the user, or the timezone in which the user is located, lead to more accurate classification. Whileno feature set is optimal for all countries, and each country needs to betreated differently, we show that remarkably high performance values above 0.9in terms of F1 score can be achieved for countries with unique characteristicssuch as those having a language that is not spoken in many other countries or aunique time zone. However, the difficulty of achieving an accurateclassification increases for countries with multiple commonalities, especiallyfor English and Spanish speaking countries.
arxiv-17700-9 | Conversational Markers of Constructive Discussions | http://arxiv.org/pdf/1604.07407v1.pdf | author:Vlad Niculae, Cristian Danescu-Niculescu-Mizil category:cs.CL cs.AI cs.SI physics.soc-ph stat.ML published:2016-04-25 summary:Group discussions are essential for organizing every aspect of modern life,from faculty meetings to senate debates, from grant review panels to papalconclaves. While costly in terms of time and organization effort, groupdiscussions are commonly seen as a way of reaching better decisions compared tosolutions that do not require coordination between the individuals (e.g.voting)---through discussion, the sum becomes greater than the parts. However,this assumption is not irrefutable: anecdotal evidence of wasteful discussionsabounds, and in our own experiments we find that over 30% of discussions areunproductive. We propose a framework for analyzing conversational dynamics in order todetermine whether a given task-oriented discussion is worth having or not. Weexploit conversational patterns reflecting the flow of ideas and the balancebetween the participants, as well as their linguistic choices. We apply thisframework to conversations naturally occurring in an online collaborative worldexploration game developed and deployed to support this research. Using thissetting, we show that linguistic cues and conversational patterns extractedfrom the first 20 seconds of a team discussion are predictive of whether itwill be a wasteful or a productive one.
arxiv-17700-10 | Modeling the Evolution of Gene-Culture Divergence | http://arxiv.org/pdf/1604.07108v1.pdf | author:Chris Marriott, Jobran Chebib category:cs.NE cs.MA q-bio.PE published:2016-04-25 summary:We present a model for evolving agents using both genetic and culturalinheritance mechanisms. Within each agent our model maintains two distinctinformation stores we call the genome and the memome. Processes of adaptationare modeled as evolutionary processes at each level of adaptation(phylogenetic, ontogenetic, sociogenetic). We review relevant competing modelsand we show how our model improves on previous attempts to model genetic andcultural evolutionary processes. In particular we argue our model can achievedivergent gene-culture co-evolution.
arxiv-17700-11 | Neural Random Forests | http://arxiv.org/pdf/1604.07143v1.pdf | author:GÃ©rard Biau, Erwan Scornet, Johannes Welbl category:stat.ML cs.LG published:2016-04-25 summary:Given an ensemble of randomized regression trees, it is possible torestructure them as a collection of multilayered neural networks withparticular connection weights. Following this principle, we reformulate therandom forest method of Breiman (2001) into a neural network setting, and inturn propose two new hybrid procedures that we call neural random forests. Bothpredictors exploit prior knowledge of regression trees for their architecture,have less parameters to tune than standard networks, and less restrictions onthe geometry of the decision boundaries. Consistency results are proved, andsubstantial numerical evidence is provided on both synthetic and real data setsto assess the excellent performance of our methods in a large variety ofprediction problems.
arxiv-17700-12 | Balancing Appearance and Context in Sketch Interpretation | http://arxiv.org/pdf/1604.07429v1.pdf | author:Yale Song, Randall Davis, Kaichen Ma, Dana L. Penny category:cs.AI cs.CV published:2016-04-25 summary:We describe a sketch interpretation system that detects and classifies clocknumerals created by subjects taking the Clock Drawing Test, a clinical toolwidely used to screen for cognitive impairments (e.g., dementia). We describehow it balances appearance and context, and document its performance on some2,000 drawings (about 24K clock numerals) produced by a wide spectrum ofpatients. We calibrate the utility of different forms of context, describingexperiments with Conditional Random Fields trained and tested using a varietyof features. We identify context that contributes to interpreting otherwiseambiguous or incomprehensible strokes. We describe ST-slices, a novelrepresentation that enables "unpeeling" the layers of ink that result whenpeople overwrite, which often produces ink impossible to analyze if only thefinal drawing is examined. We characterize when ST-slices work, calibrate theirimpact on performance, and consider their breadth of applicability.
arxiv-17700-13 | Protein Secondary Structure Prediction Using Cascaded Convolutional and Recurrent Neural Networks | http://arxiv.org/pdf/1604.07176v1.pdf | author:Zhen Li, Yizhou Yu category:q-bio.BM cs.AI cs.LG cs.NE q-bio.QM published:2016-04-25 summary:Protein secondary structure prediction is an important problem inbioinformatics. Inspired by the recent successes of deep neural networks, inthis paper, we propose an end-to-end deep network that predicts proteinsecondary structures from integrated local and global contextual features. Ourdeep architecture leverages convolutional neural networks with different kernelsizes to extract multiscale local contextual features. In addition, consideringlong-range dependencies existing in amino acid sequences, we set up abidirectional neural network consisting of gated recurrent unit to captureglobal contextual features. Furthermore, multi-task learning is utilized topredict secondary structure labels and amino-acid solvent accessibilitysimultaneously. Our proposed deep network demonstrates its effectiveness byachieving state-of-the-art performance, i.e., 69.7% Q8 accuracy on the publicbenchmark CB513, 76.9% Q8 accuracy on CASP10 and 73.1% Q8 accuracy on CASP11.Our model and results are publicly available.
arxiv-17700-14 | Long-Term Identity-Aware Multi-Person Tracking for Surveillance Video Summarization | http://arxiv.org/pdf/1604.07468v1.pdf | author:Shoou-I Yu, Yi Yang, Xuanchong Li, Alexander G. Hauptmann category:cs.CV published:2016-04-25 summary:In multi-person tracking scenarios, gaining access to the identity of eachtracked individual is crucial for many applications such as long-termsurveillance video analysis. Therefore, we propose a long-term multi-persontracker which utilizes face recognition information to not only enhancetracking performance, but also assign identities to tracked people. As facerecognition information is not available in many frames, the proposed trackerutilizes manifold learning techniques to propagate identity information toframes without face recognition information. Our tracker is formulated as aconstrained quadratic optimization problem, which is solved with nonnegativematrix optimization techniques. Tracking experiments performed on challengingdata sets, including a 116.25 hour complex indoor tracking data set, showedthat our method is effective in tracking each individual. We further exploredthe utility of long-term identity-aware multi-person tracking output byperforming video summarization experiments based on our tracking output.Results showed that the computed trajectories were sufficient to generate areasonable visual diary (i.e. a summary of what a person did) for differentpeople, thus potentially opening the door to summarization of hundreds or eventhousands of hours of surveillance video.
arxiv-17700-15 | Weighted Spectral Cluster Ensemble | http://arxiv.org/pdf/1604.07178v1.pdf | author:Muhammad Yousefnezhad, Daoqiang Zhang category:cs.LG cs.AI stat.ML published:2016-04-25 summary:Clustering explores meaningful patterns in the non-labeled data sets. ClusterEnsemble Selection (CES) is a new approach, which can combine individualclustering results for increasing the performance of the final results.Although CES can achieve better final results in comparison with individualclustering algorithms and cluster ensemble methods, its performance can bedramatically affected by its consensus diversity metric and thresholdingprocedure. There are two problems in CES: 1) most of the diversity metrics isbased on heuristic Shannon's entropy and 2) estimating threshold values arereally hard in practice. The main goal of this paper is proposing a robustapproach for solving the above mentioned problems. Accordingly, this paperdevelops a novel framework for clustering problems, which is called WeightedSpectral Cluster Ensemble (WSCE), by exploiting some concepts from communitydetection arena and graph based clustering. Under this framework, a new versionof spectral clustering, which is called Two Kernels Spectral Clustering, isused for generating graphs based individual clustering results. Further, byusing modularity, which is a famous metric in the community detection, on thetransformed graph representation of individual clustering results, our approachprovides an effective diversity estimation for individual clustering results.Moreover, this paper introduces a new approach for combining the evaluatedindividual clustering results without the procedure of thresholding.Experimental study on varied data sets demonstrates that the prosed approachachieves superior performance to state-of-the-art methods.
arxiv-17700-16 | A Review of Co-saliency Detection Technique: Fundamentals, Applications, and Challenges | http://arxiv.org/pdf/1604.07090v2.pdf | author:Dingwen Zhang, Huazhu Fu, Junwei Han, Feng Wu category:cs.CV published:2016-04-24 summary:Co-saliency detection is a newly emerging and rapidly growing research areain computer vision community. As a novel branch of visual saliency, co-saliencydetection refers to discovery of the common and salient foregrounds existed intwo or more relevant images, and can be more widely used in many computervision tasks. The existing co-saliency detection algorithms mainly consist ofthree components: extracting effective features to represent the image regions,exploring the informative cues or factors to characterize co-saliency, anddesigning effective computational framework to formulate co-saliency. Althoughenormous methods have been developed, a deep review of the literaturesconcerning about the co-saliency detection technique is still lacking. In thispaper, we aim to provide a comprehensive review of the fundamentals,challenges, and applications in co-saliency detection area. Specifically, thispaper provides the overview of some related computer vision works, reviews thehistory of co-saliency detection briefly, summarizes and categorizes the majoralgorithms in this research area, presents the potential applications ofco-saliency detection, discusses some open issues in this research area, andfinally points out some unsolved challenges and promising future works. It isour hope that this review will be beneficial for both the fresh and seniorresearchers in this field as well as researchers working in other relevantfields to have a better understanding about what they can do with co-saliencydetection in the future.
arxiv-17700-17 | Agnostic Estimation of Mean and Covariance | http://arxiv.org/pdf/1604.06968v1.pdf | author:Kevin A. Lai, Anup B. Rao, Santosh Vempala category:cs.DS cs.LG stat.ML published:2016-04-24 summary:We consider the problem of estimating the mean and covariance of adistribution from iid samples in $\mathbb{R}^n$, in the presence of an $\eta$fraction of malicious noise; this is in contrast to much recent work where thenoise itself is assumed to be from a distribution of known type. The agnosticproblem includes many interesting special cases, e.g., learning the parametersof a single Gaussian (or finding the best-fit Gaussian) when $\eta$ fraction ofdata is adversarially corrupted, agnostically learning a mixture of Gaussians,agnostic ICA, etc. We present polynomial-time algorithms to estimate the meanand covariance with error guarantees in terms of information-theoretic lowerbounds. As a corollary, we also obtain an agnostic algorithm for Singular ValueDecomposition.
arxiv-17700-18 | Bayesian Inference of Recursive Sequences of Group Activities from Tracks | http://arxiv.org/pdf/1604.06970v1.pdf | author:Ernesto Brau, Colin Dawson, Alfredo Carrillo, David Sidi, Clayton T. Morrison category:cs.AI cs.CV published:2016-04-24 summary:We present a probabilistic generative model for inferring a description ofcoordinated, recursively structured group activities at multiple levels oftemporal granularity based on observations of individuals' trajectories. Themodel accommodates: (1) hierarchically structured groups, (2) activities thatare temporally and compositionally recursive, (3) component roles assigningdifferent subactivity dynamics to subgroups of participants, and (4) anonparametric Gaussian Process model of trajectories. We present an MCMCsampling framework for performing joint inference over recursive activitydescriptions and assignment of trajectories to groups, integrating outcontinuous parameters. We demonstrate the model's expressive power in severalsimulated and complex real-world scenarios from the VIRAT and UCLA Aerial Eventvideo data sets.
arxiv-17700-19 | Multi-Fold Gabor, PCA and ICA Filter Convolution Descriptor for Face Recognition | http://arxiv.org/pdf/1604.07057v2.pdf | author:Cheng Yaw Low, Andrew Beng Jin Teoh, Cong Jie Ng category:cs.CV published:2016-04-24 summary:This paper devises a new means of filter diversification, dubbed multi-foldfilter convolution (M-FFC), for face recognition. On the assumption that M-FFCreceives single-scale Gabor filters of varying orientations as input, thesefilters are self-cross convolved by M-fold to instantiate an offspring set. TheM-FFC flexibility also permits the self-cross convolution amongst Gabor filtersand other filter banks of profoundly dissimilar traits, e.g., principalcomponent analysis (PCA) filters, and independent component analysis (ICA)filters, in our case. A 2-FFC instance therefore yields three offspring setsfrom: (1) Gabor filters solely, (2) Gabor and PCA filters, and (3) Gabor andICA filters, to render the learning-free and the learning-based 2-FFCdescriptors. To facilitate a sensible Gabor filter selection for M-FFC, the 40multi-scale, multi-orientation Gabor filters is condensed into 8 elementaryfilters. In addition to that, an average pooling operator is used to leveragethe 2-FFC histogram features, prior to whitening PCA compression. The empiricalresults substantiate that the 2-FFC descriptors prevail over, or on par with,other face descriptors on both identification and verification tasks.
arxiv-17700-20 | Semi-supervised Vocabulary-informed Learning | http://arxiv.org/pdf/1604.07093v1.pdf | author:Yanwei Fu, Leonid Sigal category:cs.CV cs.AI cs.LG stat.AP stat.ML published:2016-04-24 summary:Despite significant progress in object categorization, in recent years, anumber of important challenges remain, mainly, ability to learn from limitedlabeled data and ability to recognize object classes within large, potentiallyopen, set of labels. Zero-shot learning is one way of addressing thesechallenges, but it has only been shown to work with limited sized classvocabularies and typically requires separation between supervised andunsupervised classes, allowing former to inform the latter but not vice versa.We propose the notion of semi-supervised vocabulary-informed learning toalleviate the above mentioned challenges and address problems of supervised,zero-shot and open set recognition using a unified framework. Specifically, wepropose a maximum margin framework for semantic manifold-based recognition thatincorporates distance constraints from (both supervised and unsupervised)vocabulary atoms, ensuring that labeled samples are projected closest to theircorrect prototypes, in the embedding space, than to others. We show thatresulting model shows improvements in supervised, zero-shot, and large open setrecognition, with up to 310K class vocabulary on AwA and ImageNet datasets.
arxiv-17700-21 | Unsupervised Representation Learning of Structured Radio Communication Signals | http://arxiv.org/pdf/1604.07078v1.pdf | author:Timothy J. O'Shea, Johnathan Corgan, T. Charles Clancy category:cs.LG published:2016-04-24 summary:We explore unsupervised representation learning of radio communicationsignals in raw sampled time series representation. We demonstrate that we canlearn modulation basis functions using convolutional autoencoders and visuallyrecognize their relationship to the analytic bases used in digitalcommunications. We also propose and evaluate quantitative met- rics for qualityof encoding using domain relevant performance metrics.
arxiv-17700-22 | Fast-and-Light Stochastic ADMM | http://arxiv.org/pdf/1604.07070v1.pdf | author:Shuai Zheng, James T. Kwok category:cs.LG stat.ML published:2016-04-24 summary:The alternating direction method of multipliers (ADMM) is a powerfuloptimization solver in machine learning. Recently, stochastic ADMM has beenintegrated with variance reduction methods for stochastic gradient, leading toSAG-ADMM and SDCA-ADMM that have fast convergence rates and low iterationcomplexities. However, their space requirements can still be high. In thispaper, we propose an integration of ADMM with the method of stochastic variancereduced gradient (SVRG). Unlike another recent integration attempt calledSCAS-ADMM, the proposed algorithm retains the fast convergence benefits ofSAG-ADMM and SDCA-ADMM, but is more advantageous in that its storagerequirement is very low, even independent of the sample size $n$. Experimentalresults demonstrate that it is as fast as SAG-ADMM and SDCA-ADMM, much fasterthan SCAS-ADMM, and can be used on much bigger data sets.
arxiv-17700-23 | Deep Learning with Eigenvalue Decay Regularizer | http://arxiv.org/pdf/1604.06985v3.pdf | author:Oswaldo Ludwig category:cs.LG published:2016-04-24 summary:This paper extends our previous work on regularization of neural networksusing Eigenvalue Decay by employing a soft approximation of the dominanteigenvalue in order to enable the calculation of its derivatives in relation tothe synaptic weights, and therefore the application of back-propagation, whichis a primary demand for deep learning. Moreover, we extend our previoustheoretical analysis to deep neural networks and multiclass classificationproblems. Our method is implemented as an additional regularizer in Keras, amodular neural networks library written in Python, and evaluated in thebenchmark data sets Reuters Newswire Topics Classification, IMDB database forbinary sentiment classification, MNIST database of handwritten digits andCIFAR-10 data set for image classification.
arxiv-17700-24 | Binary Codes for Tagging X-Ray Images via Deep De-Noising Autoencoders | http://arxiv.org/pdf/1604.07060v1.pdf | author:Antonio Sze-To, Hamid R. Tizhoosh, Andrew K. C. Wong category:cs.CV published:2016-04-24 summary:A Content-Based Image Retrieval (CBIR) system which identifies similarmedical images based on a query image can assist clinicians for more accuratediagnosis. The recent CBIR research trend favors the construction and use ofbinary codes to represent images. Deep architectures could learn the non-linearrelationship among image pixels adaptively, allowing the automatic learning ofhigh-level features from raw pixels. However, most of them require classlabels, which are expensive to obtain, particularly for medical images. Themethods which do not need class labels utilize a deep autoencoder for binaryhashing, but the code construction involves a specific training algorithm andan ad-hoc regularization technique. In this study, we explored using a deepde-noising autoencoder (DDA), with a new unsupervised training scheme usingonly backpropagation and dropout, to hash images into binary codes. Weconducted experiments on more than 14,000 x-ray images. By using class labelsonly for evaluating the retrieval results, we constructed a 16-bit DDA and a512-bit DDA independently. Comparing to other unsupervised methods, wesucceeded to obtain the lowest total error by using the 512-bit codes forretrieval via exhaustive search, and speed up 9.27 times with the use of the16-bit codes while keeping a comparable total error. We found that our newtraining scheme could reduce the total retrieval error significantly by 21.9%.To further boost the image retrieval performance, we developed RadonAutoencoder Barcode (RABC) which are learned from the Radon projections ofimages using a de-noising autoencoder. Experimental results demonstrated itssuperior performance in retrieval when it was combined with DDA binary codes.
arxiv-17700-25 | Rotation-Invariant Restricted Boltzmann Machine using shared gradient filters | http://arxiv.org/pdf/1604.07045v1.pdf | author:Mario Valerio Giuffrida, Sotirios A. Tsaftaris category:cs.CV published:2016-04-24 summary:Finding suitable features has been an essential problem in computer vision.We focus on Restricted Boltzmann Machines (RBMs), which, despite theirversatility, cannot accommodate transformations that may occur in the scene. Asresult, several approaches have been proposed that consider a set oftransformations, which are used to either augment the training set or transformthe actual learned filters. In this paper, we propose the ExplicitRotation-Invariant Restricted Boltzmann Machine, which exploits priorinformation coming from the dominant orientation of images. Our model extendsthe standard RBM, by adding a suitable number of weight matrices, associated toeach dominant gradient. We show that our approach is able to learnrotation-invariant features, comparing it with the classic formulation of RBMon the MNIST benchmark dataset. Overall, requiring less hidden units, ourmethod learns compact features, which are robust to rotations.
arxiv-17700-26 | Towards Better Analysis of Deep Convolutional Neural Networks | http://arxiv.org/pdf/1604.07043v3.pdf | author:Mengchen Liu, Jiaxin Shi, Zhen Li, Chongxuan Li, Jun Zhu, Shixia Liu category:cs.CV published:2016-04-24 summary:Deep convolutional neural networks (CNNs) have achieved breakthroughperformance in many pattern recognition tasks such as image classification.However, the development of high-quality deep models typically relies on asubstantial amount of trial-and-error, as there is still no clear understandingof when and why a deep model works. In this paper, we present a visualanalytics approach for better understanding, diagnosing, and refining deepCNNs. We formulate a deep CNN as a directed acyclic graph. Based on thisformulation, a hybrid visualization is developed to disclose the multiplefacets of each neuron and the interactions between them. In particular, weintroduce a hierarchical rectangle packing algorithm and a matrix reorderingalgorithm to show the derived features of a neuron cluster. We also propose abiclustering-based edge bundling method to reduce visual clutter caused by alarge number of connections between neurons. We evaluated our method on a setof CNNs and the results are generally favorable.
arxiv-17700-27 | Cardiac Motion Analysis by Temporal Flow Graphs | http://arxiv.org/pdf/1604.06979v1.pdf | author:V S R Veeravasarapu, Jayanthi Sivaswamy, Vishanji Karani category:cs.CV published:2016-04-24 summary:Cardiac motion analysis from B-mode ultrasound sequence is a key task inassessing the health of the heart. The paper proposes a new methodology forcardiac motion analysis based on the temporal behaviour of points of intereston the myocardium. We define a new signal called the Temporal Flow Graph (TFG)which depicts the movement of a point of interest over time. It is a graphicalrepresentation derived from a flow field and describes the temporal evolutionof a point. We prove that TFG for an object undergoing periodic motion is alsoperiodic. This principle can be utilized to derive both global and localinformation from a given sequence. We demonstrate this for detecting motionirregularities at the sequence, as well as regional levels on real andsynthetic data. A coarse localisation of anatomical landmarks such as centresof left/right cavities and valve points is also demonstrated using TFGs.
arxiv-17700-28 | Jacques Lacan's Registers of the Psychoanalytic Field, Applied using Geometric Data Analysis to Edgar Allan Poe's "The Purloined Letter" | http://arxiv.org/pdf/1604.06952v1.pdf | author:Fionn Murtagh, Giuseppe Iurato category:cs.CL stat.ML 62H25, 62H30 published:2016-04-23 summary:In a first investigation, a Lacan-motivated template of the Poe story isfitted to the data. A segmentation of the storyline is used in order to map outthe diachrony. Based on this, it will be shown how synchronous aspects,potentially related to Lacanian registers, can be sought. This demonstrates theeffectiveness of an approach based on a model template of the storylinenarrative. In a second and more comprehensive investigation, we develop anapproach for revealing, that is, uncovering, Lacanian register relationships.Objectives of this work include the wide and general application of ourmethodology. This methodology is strongly based on the "letting the data speak"Correspondence Analysis analytics platform of Jean-Paul Benz\'ecri, that isalso the geometric data analysis, both qualitative and quantitative analytics,developed by Pierre Bourdieu.
arxiv-17700-29 | An information theoretic formulation of the Dictionary Learning and Sparse Coding Problems on Statistical Manifolds | http://arxiv.org/pdf/1604.06939v1.pdf | author:Rudrasis Chakraborty, Monami Banerjee, Victoria Crawford, Baba C. Vemuri category:cs.CV published:2016-04-23 summary:In this work, we propose a novel information theoretic framework fordictionary learning (DL) and sparse coding (SC) on a statistical manifold (themanifold of probability distributions). Unlike the traditional DL and SCframework, our new formulation {\it does not explicitly incorporate anysparsity inducing norm in the cost function but yet yields SCs}. Moreover, weextend this framework to the manifold of symmetric positive definite matrices,$\mathcal{P}_n$. Our algorithm approximates the data points, which areprobability distributions, by the weighted Kullback-Leibeler center (KL-center)of the dictionary atoms. The KL-center is the minimizer of the maximumKL-divergence between the unknown center and members of the set whose center isbeing sought. Further, {\it we proved that this KL-center is a sparsecombination of the dictionary atoms}. Since, the data reside on a statisticalmanifold, the data fidelity term can not be as simple as in the case of thevector-space data. We therefore employ the geodesic distance between the dataand a sparse approximation of the data element. This cost function is minimizedusing an acceleterated gradient descent algorithm. An extensive set ofexperimental results show the effectiveness of our proposed framework. Wepresent several experiments involving a variety of classification problems inComputer Vision applications. Further, we demonstrate the performance of ouralgorithm by comparing it to several state-of-the-art methods both in terms ofclassification accuracy and sparsity.
arxiv-17700-30 | Memory and Information Processing in Recurrent Neural Networks | http://arxiv.org/pdf/1604.06929v1.pdf | author:Alireza Goudarzi, Sarah Marzen, Peter Banda, Guy Feldman, Christof Teuscher, Darko Stefanovic category:cs.NE published:2016-04-23 summary:Recurrent neural networks (RNN) are simple dynamical systems whosecomputational power has been attributed to their short-term memory. Short-termmemory of RNNs has been previously studied analytically only for the case oforthogonal networks, and only under annealed approximation, and uncorrelatedinput. Here for the first time, we present an exact solution to the memorycapacity and the task-solving performance as a function of the structure of agiven network instance, enabling direct determination of thefunction--structure relation in RNNs. We calculate the memory capacity forarbitrary networks with exponentially correlated input and further related itto the performance of the system on signal processing tasks in a supervisedlearning setup. We compute the expected error and the worst-case error bound asa function of the spectra of the network and the correlation structure of itsinputs and outputs. Our results give an explanation for learning andgeneralization of task solving using short-term memory, which is crucial forbuilding alternative computer architectures using physical phenomena based onthe short-term memory principle.
arxiv-17700-31 | On the Sample Complexity of End-to-end Training vs. Semantic Abstraction Training | http://arxiv.org/pdf/1604.06915v1.pdf | author:Shai Shalev-Shwartz, Amnon Shashua category:cs.LG published:2016-04-23 summary:We compare the end-to-end training approach to a modular approach in which asystem is decomposed into semantically meaningful components. We focus on thesample complexity aspect, in the regime where an extremely high accuracy isnecessary, as is the case in autonomous driving applications. We demonstratecases in which the number of training examples required by the end-to-endapproach is exponentially larger than the number of examples required by thesemantic abstraction approach.
arxiv-17700-32 | Why and How to Pay Different Attention to Phrase Alignments of Different Intensities | http://arxiv.org/pdf/1604.06896v1.pdf | author:Wenpeng Yin, Hinrich SchÃ¼tze category:cs.CL published:2016-04-23 summary:This work studies comparatively two typical sentence pair classificationtasks: textual entailment (TE) and answer selection (AS), observing that phrasealignments of different intensities contribute differently in these tasks. Weaddress the problems of identifying phrase alignments of flexible granularityand pooling alignments of different intensities for these tasks. Examples forflexible granularity are alignments between two single words, between a singleword and a phrase and between a short phrase and a long phrase. By intensity weroughly mean the degree of match, it ranges from identity over surface-formco-occurrence, rephrasing and other semantic relatedness to unrelated words asin lots of parenthesis text. Prior work (i) has limitations in phrasegeneration and representation, or (ii) conducts alignment at word and phraselevels by handcrafted features or (iii) utilizes a single attention mechanismover alignment intensities without considering the characteristics of specifictasks, which limits the system's effectiveness across tasks. We propose anarchitecture based on Gated Recurrent Unit that supports (i) representationlearning of phrases of arbitrary granularity and (ii) task-specific focusing ofphrase alignments between two sentences by attention pooling. Experimentalresults on TE and AS match our observation and are state-of-the-art.
arxiv-17700-33 | Text Flow: A Unified Text Detection System in Natural Scene Images | http://arxiv.org/pdf/1604.06877v1.pdf | author:Shangxuan Tian, Yifeng Pan, Chang Huang, Shijian Lu, Kai Yu, Chew Lim Tan category:cs.CV published:2016-04-23 summary:The prevalent scene text detection approach follows four sequential stepscomprising character candidate detection, false character candidate removal,text line extraction, and text line verification. However, errors occur andaccumulate throughout each of these sequential steps which often lead to lowdetection performance. To address these issues, we propose a unified scene textdetection system, namely Text Flow, by utilizing the minimum cost (min-cost)flow network model. With character candidates detected by cascade boosting, themin-cost flow network model integrates the last three sequential steps into asingle process which solves the error accumulation problem at both characterlevel and text line level effectively. The proposed technique has been testedon three public datasets, i.e, ICDAR2011 dataset, ICDAR2013 dataset and amultilingual dataset and it outperforms the state-of-the-art methods on allthree datasets with much higher recall and F-score. The good performance on themultilingual dataset shows that the proposed technique can be used for thedetection of texts in different languages.
arxiv-17700-34 | Contextual object categorization with energy-based model | http://arxiv.org/pdf/1604.06852v1.pdf | author:Changyong Ri, Duho Pak, Cholryong Choe, Suhyang Kim, Yonghak Sin category:cs.CV published:2016-04-23 summary:Object categorization is a hot issue of an image mining. Contextualinformation between objects is one of the important semantic knowledge of animage. However, the previous researches for an object categorization have notmade full use of the contextual information, especially the spatial relationsbetween objects. In addition, the object categorization methods, whichgenerally use the probabilistic graphical models to implement the incorporationof contextual information with appearance of objects, are almost inevitable toevaluate the intractable partition function for normalization. In this work, weintroduced fully-connected fuzzy spatial relations including directional,distance and topological relations between object regions, so the spatialrelational information could be fully utilized. Then, the spatial relationswere considered as well as co-occurrence and appearance of objects by usingenergy-based model, where the energy function was defined as the region-objectassociation potential and the configuration potential of objects. Minimizingthe energy function of whole image arrangement, we obtained the optimal labelset about the image regions and addressed the evaluation of intractablepartition function in conditional random fields. Experimental results show thevalidity and reliability of this proposed method.
arxiv-17700-35 | Word2VisualVec: Cross-Media Retrieval by Visual Feature Prediction | http://arxiv.org/pdf/1604.06838v1.pdf | author:Jianfeng Dong, Xirong Li, Cees G. M. Snoek category:cs.CV published:2016-04-23 summary:This paper attacks the challenging problem of cross-media retrieval. That is,given an image find the text best describing its content, or the other wayaround. Different from existing works, which either rely on a joint space, or atext space, we propose to perform cross-media retrieval in a visual space only.We contribute \textit{Word2VisualVec}, a deep neural network architecture thatlearns to predict a deep visual encoding of textual input. We discuss itsarchitecture for prediction of CaffeNet and GoogleNet features, as well as itsloss functions for learning from text/image pairs in large-scale click-throughlogs and image sentences. Experiments on the Clickture-Lite and Flickr8Kcorpora demonstrate the robustness for both Text-to-Image and Image-to-Textretrieval, outperforming the state-of-the-art on both accounts. Interestingly,an embedding in predicted visual feature space is also highly effective whensearching in text only.
arxiv-17700-36 | A Computational Model for Situated Task Learning with Interactive Instruction | http://arxiv.org/pdf/1604.06849v1.pdf | author:Shiwali Mohan, James Kirk, John Laird category:cs.AI cs.LG published:2016-04-23 summary:Learning novel tasks is a complex cognitive activity requiring the learner toacquire diverse declarative and procedural knowledge. Prior ACT-R models ofacquiring task knowledge from instruction focused on learning proceduralknowledge from declarative instructions encoded in semantic memory. In thispaper, we identify the requirements for designing compu- tational models thatlearn task knowledge from situated task- oriented interactions with an expertand then describe and evaluate a model of learning from situated interactiveinstruc- tion that is implemented in the Soar cognitive architecture.
arxiv-17700-37 | Learning rotation invariant convolutional filters for texture classification | http://arxiv.org/pdf/1604.06720v1.pdf | author:Diego Marcos, Michele Volpi, Devis Tuia category:cs.CV published:2016-04-22 summary:We present a method for learning discriminative steerable filters using ashallow Convolutional Neural Network (CNN). We encode rotation invariancedirectly in the model by tying the weights of groups of filters to severalrotated versions of the canonical filter in the group. These filters can beused to extract rotation invariant features well-suited for imageclassification. We test this learning procedure on a texture classificationbenchmark, where the orientations of the training images differ from those ofthe test images. We obtain results comparable to or better than thestate-of-the-art. Besides numerical advantages, our proposed rotation invariantCNN decreases the number of parameters to be learned, thus showing morerobustness in small training set scenarios than a standard CNN.
arxiv-17700-38 | Optimizing top precision performance measure of content-based image retrieval by learning similarity function | http://arxiv.org/pdf/1604.06620v1.pdf | author:Jingbin Wang, Lihui Shi, Haoxiang Wang, Jiandong Meng, Jim Jing-Yan Wang, Qingquan Sun, Yi Gu category:cs.CV published:2016-04-22 summary:In this paper we study the problem of content-based image retrieval. In thisproblem, the most popular performance measure is the top precision measure, andthe most important component of a retrieval system is the similarity functionused to compare a query image against a database image. However, up to now,there is no existing similarity learning method proposed to optimize the topprecision measure. To fill this gap, in this paper, we propose a novelsimilarity learning method to maximize the top precision measure. We model thisproblem as a minimization problem with an objective function as the combinationof the losses of the relevant images ranked behind the top-ranked irrelevantimage, and the squared $\ell_2$ norm of the similarity function parameter. Thisminimization problem is solved as a quadratic programming problem. Theexperiments over two benchmark data sets show the advantages of the proposedmethod over other similarity learning methods when the top precision is used asthe performance measure.
arxiv-17700-39 | K-Bit-Swap: A New Operator For Real-Coded Evolutionary Algorithms | http://arxiv.org/pdf/1604.06607v1.pdf | author:Aram Ter-Sarkisov, Stephen Marsland category:cs.NE I.2 published:2016-04-22 summary:There has been a variety of crossover operators proposed for Real-CodedGenetic Algorithms (RCGAs), which recombine values from the same location inpairs of strings. In this article we present a recombination operator for RC-GAs that selects the locations randomly in both parents, and compare it tomainstream crossover operators in a set of experiments on a range of standardmultidimensional optimization problems and a clustering problem. We present twovariants of the operator, either selecting both bits uniformly at random in thestrings, or sampling the second bit from a normal distribution centered at theselected location in the first string. While the operator is biased towardsexploitation of fitness space, the random selection of the second bit for swap-ping makes it slightly less exploitation-biased. Extensive statistical analysisusing a non-parametric test shows the advantage of the new recombinationoperators on a range of test functions.
arxiv-17700-40 | SweLL on the rise: Swedish Learner Language corpus for European Reference Level studies | http://arxiv.org/pdf/1604.06583v1.pdf | author:Elena Volodina, IldikÃ³ PilÃ¡n, Ingegerd EnstrÃ¶m, Lorena Llozhi, Peter Lundkvist, GunlÃ¶g Sundberg, Monica Sandell category:cs.CL published:2016-04-22 summary:We present a new resource for Swedish, SweLL, a corpus of Swedish Learneressays linked to learners' performance according to the Common EuropeanFramework of Reference (CEFR). SweLL consists of three subcorpora - SpIn,SW1203 and Tisus, collected from three different educational establishments.The common metadata for all subcorpora includes age, gender, native languages,time of residence in Sweden, type of written task. Depending on the subcorpus,learner texts may contain additional information, such as text genres, topics,grades. Five of the six CEFR levels are represented in the corpus: A1, A2, B1,B2 and C1 comprising in total 339 essays. C2 level is not included sincecourses at C2 level are not offered. The work flow consists of collection ofessays and permits, essay digitization and registration, meta-data annotation,automatic linguistic annotation. Inter-rater agreement is presented on thebasis of SW1203 subcorpus. The work on SweLL is still ongoing with more than100 essays waiting in the pipeline. This article both describes the resourceand the "how-to" behind the compilation of SweLL.
arxiv-17700-41 | Kernelized Covariance for Action Recognition | http://arxiv.org/pdf/1604.06582v1.pdf | author:Jacopo Cavazza, Andrea Zunino, Marco San Biagio, Vittorio Murino category:cs.CV published:2016-04-22 summary:In this paper we aim at increasing the descriptive power of the covariancematrix, limited in capturing linear mutual dependencies between variables only.We present a rigorous and principled mathematical pipeline to recover thekernel trick for computing the covariance matrix, enhancing it to model morecomplex, non-linear relationships conveyed by the raw data. To this end, wepropose Kernelized-COV, which generalizes the original covariancerepresentation without compromising the efficiency of the computation. In theexperiments, we validate the proposed framework against many previousapproaches in the literature, scoring on par or superior with respect to thestate of the art on benchmark datasets for 3D action recognition.
arxiv-17700-42 | Exploiting Deep Semantics and Compositionality of Natural Language for Human-Robot-Interaction | http://arxiv.org/pdf/1604.06721v1.pdf | author:Manfred Eppe, Sean Trott, Jerome Feldman category:cs.AI cs.CL cs.RO published:2016-04-22 summary:We develop a natural language interface for human robot interaction thatimplements reasoning about deep semantics in natural language. To realize therequired deep analysis, we employ methods from cognitive linguistics, namelythe modular and compositional framework of Embodied Construction Grammar (ECG)[Feldman, 2009]. Using ECG, robots are able to solve fine-grained referenceresolution problems and other issues related to deep semantics andcompositionality of natural language. This also includes verbal interactionwith humans to clarify commands and queries that are too ambiguous to beexecuted safely. We implement our NLU framework as a ROS package and presentproof-of-concept scenarios with different robots, as well as a survey on thestate of the art.
arxiv-17700-43 | CT-Mapper: Mapping Sparse Multimodal Cellular Trajectories using a Multilayer Transportation Network | http://arxiv.org/pdf/1604.06577v1.pdf | author:Fereshteh Asgari, Alexis Sultan, Haoyi Xiong, Vincent Gauthier, Mounim El-Yacoubi category:cs.SI cs.LG published:2016-04-22 summary:Mobile phone data have recently become an attractive source of informationabout mobility behavior. Since cell phone data can be captured in a passive wayfor a large user population, they can be harnessed to collect well-sampledmobility information. In this paper, we propose CT-Mapper, an unsupervisedalgorithm that enables the mapping of mobile phone traces over a multimodaltransport network. One of the main strengths of CT-Mapper is its capability tomap noisy sparse cellular multimodal trajectories over a multilayertransportation network where the layers have different physical properties andnot only to map trajectories associated with a single layer. Such a network ismodeled by a large multilayer graph in which the nodes correspond tometro/train stations or road intersections and edges correspond to connectionsbetween them. The mapping problem is modeled by an unsupervised HMM where theobservations correspond to sparse user mobile trajectories and the hiddenstates to the multilayer graph nodes. The HMM is unsupervised as the transitionand emission probabilities are inferred using respectively the physicaltransportation properties and the information on the spatial coverage ofantenna base stations. To evaluate CT-Mapper we collected cellular traces withtheir corresponding GPS trajectories for a group of volunteer users in Parisand vicinity (France). We show that CT-Mapper is able to accurately retrievethe real cell phone user paths despite the sparsity of the observed tracetrajectories. Furthermore our transition probability model is up to 20% moreaccurate than other naive models.
arxiv-17700-44 | Convolutional Two-Stream Network Fusion for Video Action Recognition | http://arxiv.org/pdf/1604.06573v1.pdf | author:Christoph Feichtenhofer, Axel Pinz, Andrew Zisserman category:cs.CV published:2016-04-22 summary:Recent applications of Convolutional Neural Networks (ConvNets) for humanaction recognition in videos have proposed different solutions forincorporating the appearance and motion information. We study a number of waysof fusing ConvNet towers both spatially and temporally in order to best takeadvantage of this spatio-temporal information. We make the following findings:(i) that rather than fusing at the softmax layer, a spatial and temporalnetwork can be fused at a convolution layer without loss of performance, butwith a substantial saving in parameters; (ii) that it is better to fuse suchnetworks spatially at the last convolutional layer than earlier, and thatadditionally fusing at the class prediction layer can boost accuracy; finally(iii) that pooling of abstract convolutional features over spatiotemporalneighbourhoods further boosts performance. Based on these studies we propose anew ConvNet architecture for spatiotemporal fusion of video snippets, andevaluate its performance on standard benchmarks where this architectureachieves state-of-the-art results.
arxiv-17700-45 | Bridging LSTM Architecture and the Neural Dynamics during Reading | http://arxiv.org/pdf/1604.06635v1.pdf | author:Peng Qian, Xipeng Qiu, Xuanjing Huang category:cs.CL cs.AI cs.LG cs.NE published:2016-04-22 summary:Recently, the long short-term memory neural network (LSTM) has attracted wideinterest due to its success in many tasks. LSTM architecture consists of amemory cell and three gates, which looks similar to the neuronal networks inthe brain. However, there still lacks the evidence of the cognitiveplausibility of LSTM architecture as well as its working mechanism. In thispaper, we study the cognitive plausibility of LSTM by aligning its internalarchitecture with the brain activity observed via fMRI when the subjects read astory. Experiment results show that the artificial memory vector in LSTM canaccurately predict the observed sequential brain activities, indicating thecorrelation between LSTM architecture and the cognitive process of storyreading.
arxiv-17700-46 | A Classifier-guided Approach for Top-down Salient Object Detection | http://arxiv.org/pdf/1604.06570v1.pdf | author:Hisham Cholakkal, Jubin Johnson, Deepu Rajan category:cs.CV published:2016-04-22 summary:We propose a framework for top-down salient object detection thatincorporates a tightly coupled image classification module. The classifier istrained on novel category-aware sparse codes computed on object dictionariesused for saliency modeling. A misclassification indicates that thecorresponding saliency model is inaccurate. Hence, the classifier selectsimages for which the saliency models need to be updated. The category-awaresparse coding produces better image classification accuracy as compared toconventional sparse coding with a reduced computational complexity. Asaliency-weighted max-pooling is proposed to improve image classification,which is further used to refine the saliency maps. Experimental results onGraz-02 and PASCAL VOC-07 datasets demonstrate the effectiveness of salientobject detection. Although the role of the classifier is to support salientobject detection, we evaluate its performance in image classification and alsoillustrate the utility of thresholded saliency maps for image segmentation.
arxiv-17700-47 | Dependency Parsing with LSTMs: An Empirical Evaluation | http://arxiv.org/pdf/1604.06529v1.pdf | author:Adhiguna Kuncoro, Yuichiro Sawai, Kevin Duh, Yuji Matsumoto category:cs.CL cs.LG cs.NE published:2016-04-22 summary:We propose a transition-based dependency parser using Recurrent NeuralNetworks with Long Short-Term Memory (LSTM) units. This extends the feedforwardneural network parser of Chen and Manning (2014) and enables modelling ofentire sequences of shift/reduce transition decisions. On the Google WebTreebank, our LSTM parser is competitive with the best feedforward parser onoverall accuracy and notably achieves more than 3% improvement for long-rangedependencies, which has proved difficult for previous transition-based parsersdue to error propagation and limited context information. Our findingsadditionally suggest that dropout regularisation on the embedding layer iscrucial to improve the LSTM's generalisation.
arxiv-17700-48 | $Î³$-regression: Robust and Sparse Regression based on $Î³$-divergence | http://arxiv.org/pdf/1604.06637v1.pdf | author:Takayuki Kawashima, Hironori Fujisawa category:stat.ME stat.ML published:2016-04-22 summary:In high-dimensional data, many sparse regression methods have been proposed.However, they may not be robust against outliers. Recently, the use of densitypower weight has been studied for robust parameter estimation and thecorresponding divergences have been discussed. One of such divergences is the$\gamma$-divergence and the robust estimator using the $\gamma$-divergence isknown for having a strong robustness. In this paper, we consider the robust andsparse regression based on $\gamma$-divergence. We extend the$\gamma$-divergence to the regression problem and show that it has a strongrobustness under heavy contamination even when outliers are heterogeneous. Theloss function is constructed by an empirical estimate of the$\gamma$-divergence with sparse regularization and the parameter estimate isdefined as the minimizer of the loss function. To obtain the robust and sparseestimate, we propose an efficient update algorithm which has a monotonedecreasing property of the loss function. Particularly, we discuss a linearregression problem with $L_1$ regularization in detail. In numericalexperiments, we see that the proposed method outperforms past sparse and robustmethods.
arxiv-17700-49 | Benchmarking Deep Reinforcement Learning for Continuous Control | http://arxiv.org/pdf/1604.06778v2.pdf | author:Yan Duan, Xi Chen, Rein Houthooft, John Schulman, Pieter Abbeel category:cs.LG cs.AI cs.RO published:2016-04-22 summary:Recently, researchers have made significant progress combining the advancesin deep learning for learning feature representations with reinforcementlearning. Some notable examples include training agents to play Atari gamesbased on raw pixel data and to acquire advanced manipulation skills using rawsensory inputs. However, it has been difficult to quantify progress in thedomain of continuous control due to the lack of a commonly adopted benchmark.In this work, we present a benchmark suite of continuous control tasks,including classic tasks like cart-pole swing-up, tasks with very high state andaction dimensionality such as 3D humanoid locomotion, tasks with partialobservations, and tasks with hierarchical structure. We report novel findingsbased on the systematic evaluation of a range of implemented reinforcementlearning algorithms. Both the benchmark and reference implementations arereleased open-source in order to facilitate experimental reproducibility and toencourage adoption by other researchers.
arxiv-17700-50 | An improved chromosome formulation for genetic algorithms applied to variable selection with the inclusion of interaction terms | http://arxiv.org/pdf/1604.06727v1.pdf | author:Chee Chun Gan, Gerard Learmonth category:stat.ML cs.NE published:2016-04-22 summary:Genetic algorithms are a well-known method for tackling the problem ofvariable selection. As they are non-parametric and can use a large variety offitness functions, they are well-suited as a variable selection wrapper thatcan be applied to many different models. In almost all cases, the chromosomeformulation used in these genetic algorithms consists of a binary vector oflength n for n potential variables indicating the presence or absence of thecorresponding variables. While the aforementioned chromosome formulation hasexhibited good performance for relatively small n, there are potential problemswhen the size of n grows very large, especially when interaction terms areconsidered. We introduce a modification to the standard chromosome formulationthat allows for better scalability and model sparsity when interaction termsare included in the predictor search space. Experimental results show that theindexed chromosome formulation demonstrates improved computational efficiencyand sparsity on high-dimensional datasets with interaction terms compared tothe standard chromosome formulation.
arxiv-17700-51 | Synthetic Data for Text Localisation in Natural Images | http://arxiv.org/pdf/1604.06646v1.pdf | author:Ankush Gupta, Andrea Vedaldi, Andrew Zisserman category:cs.CV published:2016-04-22 summary:In this paper we introduce a new method for text detection in natural images.The method comprises two contributions: First, a fast and scalable engine togenerate synthetic images of text in clutter. This engine overlays synthetictext to existing background images in a natural way, accounting for the local3D scene geometry. Second, we use the synthetic images to train aFully-Convolutional Regression Network (FCRN) which efficiently performs textdetection and bounding-box regression at all locations and multiple scales inan image. We discuss the relation of FCRN to the recently-introduced YOLOdetector, as well as other end-to-end object detection systems based on deeplearning. The resulting detection network significantly out performs currentmethods for text detection in natural images, achieving an F-measure of 84.2%on the standard ICDAR 2013 benchmark. Furthermore, it can process 15 images persecond on a GPU.
arxiv-17700-52 | Opt: A Domain Specific Language for Non-linear Least Squares Optimization in Graphics and Imaging | http://arxiv.org/pdf/1604.06525v1.pdf | author:Zachary DeVito, Michael Mara, Michael ZollhÃ¶fer, Gilbert Bernstein, Jonathan Ragan-Kelley, Christian Theobalt, Pat Hanrahan, Matthew Fisher, Matthias NieÃner category:cs.GR cs.CV cs.PL published:2016-04-22 summary:Many graphics and vision problems are naturally expressed as optimizationswith either linear or non-linear least squares objective functions over visualdata, such as images and meshes. The mathematical descriptions of thesefunctions are extremely concise, but their implementation in real code istedious, especially when optimized for real-time performance in interactiveapplications. We propose a new language, Opt (available underhttp://optlang.org), in which a user simply writes energy functions over image-or graph-structured unknowns, and a compiler automatically generatesstate-of-the-art GPU optimization kernels. The end result is a system in whichreal-world energy functions in graphics and vision applications are expressiblein tens of lines of code. They compile directly into highly-optimized GPUsolver implementations with performance competitive with the best publishedhand-tuned, application-specific GPU solvers, and 1-2 orders of magnitudebeyond a general-purpose auto-generated solver.
arxiv-17700-53 | Developing an ICU scoring system with interaction terms using a genetic algorithm | http://arxiv.org/pdf/1604.06730v1.pdf | author:Chee Chun Gan, Gerard Learmonth category:cs.NE cs.LG stat.ML published:2016-04-22 summary:ICU mortality scoring systems attempt to predict patient mortality usingpredictive models with various clinical predictors. Examples of such systemsare APACHE, SAPS and MPM. However, most such scoring systems do not activelylook for and include interaction terms, despite physicians intuitively takingsuch interactions into account when making a diagnosis. One barrier toincluding such terms in predictive models is the difficulty of using mostvariable selection methods in high-dimensional datasets. A genetic algorithmframework for variable selection with logistic regression models is used tosearch for two-way interaction terms in a clinical dataset of adult ICUpatients, with separate models being built for each category of diagnosis uponadmittance to the ICU. The models had good discrimination across allcategories, with a weighted average AUC of 0.84 (>0.90 for several categories)and the genetic algorithm was able to find several significant interactionterms, which may be able to provide greater insight into mortality predictionfor health practitioners. The GA selected models had improved performanceagainst stepwise selection and random forest models, and provides greaterflexibility in terms of variable selection by being able to optimize over anymodeler-defined model performance metric instead of a specific variableimportance metric.
arxiv-17700-54 | The Mean Partition Theorem of Consensus Clustering | http://arxiv.org/pdf/1604.06626v2.pdf | author:Brijnesh J. Jain category:cs.LG cs.CV stat.ML published:2016-04-22 summary:To devise efficient solutions for approximating a mean partition in consensusclustering, Dimitriadou et al. [3] presented a necessary condition ofoptimality for a consensus function based on least square distances. We showthat their result is pivotal for deriving interesting properties of consensusclustering beyond optimization. For this, we present the necessary condition ofoptimality in a slightly stronger form in terms of the Mean Partition Theoremand extend it to the Expected Partition Theorem. To underpin its versatility,we show three examples that apply the Mean Partition Theorem: (i) equivalenceof the mean partition and optimal multiple alignment, (ii) construction ofprofiles and motifs, and (iii) relationship between consensus clustering andcluster stability.
arxiv-17700-55 | Entity Embeddings of Categorical Variables | http://arxiv.org/pdf/1604.06737v1.pdf | author:Cheng Guo, Felix Berkhahn category:cs.LG published:2016-04-22 summary:We map categorical variables in a function approximation problem intoEuclidean spaces, which are the entity embeddings of the categorical variables.The mapping is learned by a neural network during the standard supervisedtraining process. Entity embedding not only reduces memory usage and speeds upneural networks compared with one-hot encoding, but more importantly by mappingsimilar values close to each other in the embedding space it reveals theintrinsic properties of the categorical variables. We applied it successfullyin a recent Kaggle competition and were able to reach the third position withrelative simple features. We further demonstrate in this paper that entityembedding helps the neural network to generalize better when the data is sparseand statistics is unknown. Thus it is especially useful for datasets with lotsof high cardinality features, where other methods tend to overfit. We alsodemonstrate that the embeddings obtained from the trained neural network boostthe performance of all tested machine learning methods considerably when usedas the input features instead. As entity embedding defines a distance measurefor categorical variables it can be used for visualizing categorical data andfor data clustering.
arxiv-17700-56 | Refining Architectures of Deep Convolutional Neural Networks | http://arxiv.org/pdf/1604.06832v1.pdf | author:Sukrit Shankar, Duncan Robertson, Yani Ioannou, Antonio Criminisi, Roberto Cipolla category:cs.CV published:2016-04-22 summary:Deep Convolutional Neural Networks (CNNs) have recently evinced immensesuccess for various image recognition tasks. However, a question of paramountimportance is somewhat unanswered in deep learning research - is the selectedCNN optimal for the dataset in terms of accuracy and model size? In this paper,we intend to answer this question and introduce a novel strategy that altersthe architecture of a given CNN for a specified dataset, to potentially enhancethe original accuracy while possibly reducing the model size. We use twooperations for architecture refinement, viz. stretching and symmetricalsplitting. Our procedure starts with a pre-trained CNN for a given dataset, andoptimally decides the stretch and split factors across the network to refinethe architecture. We empirically demonstrate the necessity of the twooperations. We evaluate our approach on two natural scenes attributes datasets,SUN Attributes and CAMIT-NSAD, with architectures of GoogleNet and VGG-11, thatare quite contrasting in their construction. We justify our choice of datasets,and show that they are interestingly distinct from each other, and togetherpose a challenge to our architectural refinement algorithm. Our resultssubstantiate the usefulness of the proposed method.
arxiv-17700-57 | Latent Contextual Bandits and their Application to Personalized Recommendations for New Users | http://arxiv.org/pdf/1604.06743v1.pdf | author:Li Zhou, Emma Brunskill category:cs.LG cs.AI published:2016-04-22 summary:Personalized recommendations for new users, also known as the cold-startproblem, can be formulated as a contextual bandit problem. Existing contextualbandit algorithms generally rely on features alone to capture user variability.Such methods are inefficient in learning new users' interests. In this paper wepropose Latent Contextual Bandits. We consider both the benefit of leveraging aset of learned latent user classes for new users, and how we can learn suchlatent classes from prior users. We show that our approach achieves a betterregret bound than existing algorithms. We also demonstrate the benefit of ourapproach using a large real world dataset and a preliminary user study.
arxiv-17700-58 | Approximation Vector Machines for Large-scale Online Learning | http://arxiv.org/pdf/1604.06518v2.pdf | author:Trung Le, Tu Dinh Nguyen, Vu Nguyen, Dinh Phung category:cs.LG published:2016-04-22 summary:One of the most challenging problems in kernel online learning is to boundthe model size and to promote the model sparsity. Sparse models not onlyimprove computation and memory usage, but also enhance the generalizationcapacity, a principle that concurs with the law of parsimony. However,inappropriate sparsity modeling may also significantly degrade the performance.In this paper, we propose Approximation Vector Machine (AVM), a model that cansimultaneously encourage the sparsity and safeguard its risk in compromisingthe performance. When an incoming instance arrives, we approximate thisinstance by one of its neighbors whose distance to it is less than a predefinedthreshold. Our key intuition is that since the newly seen instance is expressedby its nearby neighbor the optimal performance can be analytically formulatedand maintained. We develop theoretical foundations to support this intuitionand further establish an analysis to characterize the gap between theapproximation and optimal solutions. This gap crucially depends on thefrequency of approximation and the predefined threshold. We perform theconvergence analysis for a wide spectrum of loss functions including Hinge,smooth Hinge, and Logistic for classification task, and $l_1$, $l_2$, and$\epsilon$-insensitive for regression task. We conducted extensive experimentsfor classification task in batch and online modes, and regression task inonline mode over several benchmark datasets. The results show that our proposedAVM achieved a comparable predictive performance with current state-of-the-artmethods while simultaneously achieving significant computational speed-up dueto the ability of the proposed AVM in maintaining the model size.
arxiv-17700-59 | Automatic verbal aggression detection for Russian and American imageboards | http://arxiv.org/pdf/1604.06648v1.pdf | author:Denis Gordeev category:cs.CL published:2016-04-22 summary:The problem of aggression for Internet communities is rampant. Anonymousforums usually called imageboards are notorious for their aggressive anddeviant behaviour even in comparison with other Internet communities. Thisstudy is aimed at studying ways of automatic detection of verbal expression ofaggression for the most popular American (4chan.org) and Russian (2ch.hk)imageboards. A set of 1,802,789 messages was used for this study. The machinelearning algorithm word2vec was applied to detect the state of aggression. Adecent result is obtained for English (88%), the results for Russian are yet tobe improved.
arxiv-17700-60 | Non-convex Global Minimization and False Discovery Rate Control for the TREX | http://arxiv.org/pdf/1604.06815v1.pdf | author:Jacob Bien, Irina Gaynanova, Johannes Lederer, Christian MÃ¼ller category:stat.ML cs.OH stat.CO stat.ME published:2016-04-22 summary:The TREX is a recently introduced method for performing sparsehigh-dimensional regression. Despite its statistical promise as an alternativeto the lasso, square-root lasso, and scaled lasso, the TREX is computationallychallenging in that it requires solving a non-convex optimization problem. Thispaper shows a remarkable result: despite the non-convexity of the TREX problem,there exists a polynomial-time algorithm that is guaranteed to find the globalminimum. This result adds the TREX to a very short list of non-convexoptimization problems that can be globally optimized (principal componentsanalysis being a famous example). After deriving and developing this newapproach, we demonstrate that (i) the ability of the TREX heuristic to reachthe global minimum is strongly dependent on the difficulty of the underlyingstatistical problem, (ii) the polynomial-time algorithm for TREX permits anovel variable ranking and selection scheme, (iii) this scheme can beincorporated into a rule that controls the false discovery rate (FDR) ofincluded features in the model. To achieve this last aim, we provide anextension of the results of Barber & Candes (2015) to establish that theknockoff filter framework can be applied to the TREX. This investigation thusprovides both a rare case study of a heuristic for non-convex optimization anda novel way of exploiting non-convexity for statistical inference.
arxiv-17700-61 | Detecting state of aggression in sentences using CNN | http://arxiv.org/pdf/1604.06650v1.pdf | author:Rodmonga Potapova, Denis Gordeev category:cs.CL published:2016-04-22 summary:In this article we study verbal expression of aggression and its detectionusing machine learning and neural networks methods. We test our results usingour corpora of messages from anonymous imageboards. We also compare Randomforest classifier with convolutional neural network for "Movie reviews with onesentence per review" corpus.
arxiv-17700-62 | Learning a Tree-Structured Ising Model in Order to Make Predictions | http://arxiv.org/pdf/1604.06749v1.pdf | author:Guy Bresler, Mina Karzand category:math.ST cs.IT math.IT math.PR stat.ML stat.TH published:2016-04-22 summary:We study the problem of learning a tree graphical model from samples suchthat low-order marginals are accurate. We define a distance ("small set TV" orssTV) between distributions $P$ and $Q$ by taking the maximum, over all subsets$\mathcal{S}$ of a given size, of the total variation between the marginals ofP and Q on $\mathcal{S}$. Approximating a distribution to within small ssTVallows making predictions based on partial observations. Focusing on pairwisemarginals and tree-structured Ising models on $p$ nodes with maximum edgestrength $\beta$, we prove that $\max\{e^{2\beta}, \eta^{-2}\} \log p$ i.i.d.samples suffices to get a distribution (from the same class) with ssTV at most$\eta$ from the one generating the samples.
arxiv-17700-63 | Multiscale Segmentation via Bregman Distances and Nonlinear Spectral Analysis | http://arxiv.org/pdf/1604.06665v1.pdf | author:Leonie Zeune, Guus van Dalum, Leon W. M. M. Terstappen, S. A. van Gils, Christoph Brune category:math.NA cs.CV math.SP published:2016-04-22 summary:In biomedical imaging reliable segmentation of objects (e.g. from small cellsup to large organs) is of fundamental importance for automated medicaldiagnosis. New approaches for multi-scale segmentation can considerably improveperformance in case of natural variations in intensity, size and shape. Thispaper aims at segmenting objects of interest based on shape contours andautomatically finding multiple objects with different scales. The overallstrategy of this work is to combine nonlinear segmentation with scales spacesand spectral decompositions recently introduced in literature. For this wegeneralize a variational segmentation model based on total variation usingBregman distances to construct an inverse scale space. This offers the newmodel to be accomplished by a scale analysis approach based on a spectraldecomposition of the total variation. As a result we obtain a very efficient,(nearly) parameter-free multiscale segmentation method that comes with anadaptive regularization parameter choice. The added benefit of our method isdemonstrated by systematic synthetic tests and its usage in a new biomedicaltoolbox for identifying and classifying circulating tumor cells. Due to thenature of nonlinear diffusion underlying, the mathematical concepts in thiswork offer promising extensions to nonlocal classification problems.
arxiv-17700-64 | Clustering with Missing Features: A Penalized Dissimilarity Measure based approach | http://arxiv.org/pdf/1604.06602v2.pdf | author:Shounak Datta, Supritam Bhattacharjee, Swagatam Das category:cs.LG 62H30 published:2016-04-22 summary:Many real-world clustering problems are plagued by incomplete datacharacterized by missing or absent features for some or all of the datainstances. Traditional clustering methods cannot be directly applied to suchdata without preprocessing by imputation or marginalization techniques. In thisarticle, we put forth the concept of Penalized Dissimilarity Measures whichestimate the actual distance between two data points (the distance between themif they were to be fully observed) by adding a penalty to the distance due tothe observed features common to both the instances. We then propose such adissimilarity measure called the Feature Weighted Penalty based Dissimilarity(FWPD) measure. Using the proposed dissimilarity measure, we also modify thetraditional k-means clustering algorithm and the standard hierarchicalagglomerative clustering techniques so as to make them directly applicable todatasets with missing features. We present time complexity analyses for thesenew techniques and also present a detailed analysis showing that the new FWPDbased k-means algorithm converges to a local optimum within a finite number ofiterations. We have also conducted extensive experiments on various benchmarkdatasets showing that the proposed clustering techniques have generally betterresults compared to some of the popular imputation methods which are commonlyused to handle such incomplete data. We have appended a possible extension ofthe proposed dissimilarity measure to the case of absent features (where theunobserved features are known to be non-existent).
arxiv-17700-65 | evt_MNIST: A spike based version of traditional MNIST | http://arxiv.org/pdf/1604.06751v1.pdf | author:Mazdak Fatahi, Mahmood Ahmadi, Mahyar Shahsavari, Arash Ahmadi, Philippe Devienne category:cs.NE published:2016-04-22 summary:Benchmarks and datasets have important role in evaluation of machine learningalgorithms and neural network implementations. Traditional dataset for imagessuch as MNIST is applied to evaluate efficiency of different trainingalgorithms in neural networks. This demand is different in Spiking NeuralNetworks (SNN) as they require spiking inputs. It is widely believed, in thebiological cortex the timing of spikes is irregular. Poisson distributionsprovide adequate descriptions of the irregularity in generating appropriatespikes. Here, we introduce a spike-based version of MNSIT (handwritten digitsdataset),using Poisson distribution and show the Poissonian property of thegenerated streams. We introduce a new version of evt_MNIST which can be usedfor neural network evaluation.
arxiv-17700-66 | Novelty Detection in MultiClass Scenarios with Incomplete Set of Class Labels | http://arxiv.org/pdf/1604.06242v2.pdf | author:Nomi Vinokurov, Daphna Weinshall category:cs.CV published:2016-04-21 summary:We address the problem of novelty detection in multiclass scenarios wheresome class labels are missing from the training set. Our method is based on theinitial assignment of confidence values, which measure the affinity between anew test point and each known class. We first compare the values of the two topelements in this vector of confidence values. In the heart of our method liesthe training of an ensemble of classifiers, each trained to discriminate knownfrom novel classes based on some partition of the training data intopresumed-known and presumednovel classes. Our final novelty score is derivedfrom the output of this ensemble of classifiers. We evaluated our method on twodatasets of images containing a relatively large number of classes - theCaltech-256 and Cifar-100 datasets. We compared our method to 3 alternativemethods which represent commonly used approaches, including the one-class SVM,novelty based on k-NN, novelty based on maximal confidence, and the recentKNFST method. The results show a very clear and marked advantage for our methodover all alternative methods, in an experimental setup where class labels aremissing during training.
arxiv-17700-67 | Training Deep Nets with Sublinear Memory Cost | http://arxiv.org/pdf/1604.06174v2.pdf | author:Tianqi Chen, Bing Xu, Chiyuan Zhang, Carlos Guestrin category:cs.LG published:2016-04-21 summary:We propose a systematic approach to reduce the memory consumption of deepneural network training. Specifically, we design an algorithm that costsO(sqrt(n)) memory to train a n layer network, with only the computational costof an extra forward pass per mini-batch. As many of the state-of-the-art modelshit the upper bound of the GPU memory, our algorithm allows deeper and morecomplex models to be explored, and helps advance the innovations in deeplearning research. We focus on reducing the memory cost to store theintermediate feature maps and gradients during training. Computation graphanalysis is used for automatic in-place operation and memory sharingoptimizations. We show that it is possible to trade computation for memory -giving a more memory efficient training algorithm with a little extracomputation cost. In the extreme case, our analysis also shows that the memoryconsumption can be reduced to O(log n) with as little as O(n log n) extra costfor forward computation. Our experiments show that we can reduce the memorycost of a 1,000-layer deep residual network from 48G to 7G with only 30 percentadditional running time cost on ImageNet problems. Similarly, significantmemory cost reduction is observed in training complex recurrent neural networkson very long sequences.
arxiv-17700-68 | Improving Human Action Recognition by Non-action Classification | http://arxiv.org/pdf/1604.06397v2.pdf | author:Yang Wang, Minh Hoai category:cs.CV published:2016-04-21 summary:In this paper we consider the task of recognizing human actions in realisticvideo where human actions are dominated by irrelevant factors. We first studythe benefits of removing non-action video segments, which are the ones that donot portray any human action. We then learn a non-action classifier and use itto down-weight irrelevant video segments. The non-action classifier is trainedusing ActionThread, a dataset with shot-level annotation for the occurrence orabsence of a human action. The non-action classifier can be used to identifynon-action shots with high precision and subsequently used to improve theperformance of action recognition systems.
arxiv-17700-69 | Online Action Detection | http://arxiv.org/pdf/1604.06506v1.pdf | author:Roeland De Geest, Efstratios Gavves, Amir Ghodrati, Zhenyang Li, Cees Snoek, Tinne Tuytelaars category:cs.CV published:2016-04-21 summary:In online action detection, the goal is to detect the start of an action in avideo stream as soon as it happens. For instance, if a child is chasing a ball,an autonomous car should recognize what is going on and respond immediately.This is a very challenging problem for four reasons. First, only partialactions are observed. Second, there is a large variability in negative data.Third, the start of the action is unknown, so it is unclear over what timewindow the information should be integrated. Finally, in real world data, largewithin-class variability exists. This problem has been addressed before, butonly to some extent. Our contributions to online action detection are threefold. First, weintroduce a realistic dataset composed of 27 episodes from 6 popular TV series.The dataset spans over 16 hours of footage annotated with 30 action classes,totaling about 5,000 action instances. Second, we analyze and compare variousbaseline methods, showing this is a challenging problem for which none of themethods provides a good solution. Third, we analyze the change in performancewhen there is a variation in viewpoint, occlusion, truncation, etc. Weintroduce an evaluation protocol for fair comparison. The dataset, thebaselines and the models will all be made publicly available to encouragefurther research on online action detection on realistic data.
arxiv-17700-70 | Stabilized Sparse Online Learning for Sparse Data | http://arxiv.org/pdf/1604.06498v1.pdf | author:Yuting Ma, Tian Zheng category:stat.ML cs.LG published:2016-04-21 summary:Stochastic gradient descent (SGD) is commonly used for optimization inlarge-scale machine learning problems. Langford et al. (2009) introduce asparse online learning method to induce sparsity via truncated gradient. Withhigh-dimensional sparse data, however, the method suffers from slow convergenceand high variance due to the heterogeneity in feature sparsity. To mitigatethis issue, we introduce a stabilized truncated stochastic gradient descentalgorithm. We employ a soft-thresholding scheme on the weight vector where theimposed shrinkage is adaptive to the amount of information available in eachfeature. The variability in the resulted sparse weight vector is furthercontrolled by stability selection integrated with the informative truncation.To facilitate better convergence, we adopt an annealing strategy on thetruncation rate, which leads to a balanced trade-off between exploration andexploitation in learning a sparse weight vector. Numerical experiments showthat our algorithm compares favorably with the original algorithm in terms ofprediction accuracy, achieved sparsity and stability.
arxiv-17700-71 | Walk and Learn: Facial Attribute Representation Learning from Egocentric Video and Contextual Data | http://arxiv.org/pdf/1604.06433v2.pdf | author:Jing Wang, Yu Cheng, Rogerio Schmidt Feris category:cs.CV published:2016-04-21 summary:The way people look in terms of facial attributes (ethnicity, hair color,facial hair, etc.) and the clothes or accessories they wear (sunglasses, hat,hoodies, etc.) is highly dependent on geo-location and weather condition,respectively. This work explores, for the first time, the use of thiscontextual information, as people with wearable cameras walk across differentneighborhoods of a city, in order to learn a rich feature representation forfacial attribute classification, without the costly manual annotation requiredby previous methods. By tracking the faces of casual walkers on more than 40hours of egocentric video, we are able to cover tens of thousands of differentidentities and automatically extract nearly 5 million pairs of images connectedby or from different face tracks, along with their weather and locationcontext, under pose and lighting variations. These image pairs are then fedinto a deep network that preserves similarity of images connected by the sametrack, in order to capture identity-related attribute features, and optimizesfor location and weather prediction to capture additional facial attributefeatures. Finally, the network is fine-tuned with manually annotated samples.We perform an extensive experimental analysis on wearable data and two standardbenchmark datasets based on web images (LFWA and CelebA). Our methodoutperforms by a large margin a network trained from scratch. Moreover, evenwithout using manually annotated identity labels for pre-training as inprevious methods, our approach achieves results that are better than the stateof the art.
arxiv-17700-72 | Humans and deep networks largely agree on which kinds of variation make object recognition harder | http://arxiv.org/pdf/1604.06486v1.pdf | author:Saeed Reza Kheradpisheh, Masoud Ghodrati, Mohammad Ganjtabesh, TimothÃ©e Masquelier category:cs.CV q-bio.NC published:2016-04-21 summary:View-invariant object recognition is a challenging problem, which hasattracted much attention among the psychology, neuroscience, and computervision communities. Humans are notoriously good at it, even if some variationsare presumably more difficult to handle than others (e.g. 3D rotations). Humansare thought to solve the problem through hierarchical processing along theventral stream, which progressively extracts more and more invariant visualfeatures. This feed-forward architecture has inspired a new generation ofbio-inspired computer vision systems called deep convolutional neural networks(DCNN), which are currently the best algorithms for object recognition innatural images. Here, for the first time, we systematically compared humanfeed-forward vision and DCNNs at view-invariant object recognition using thesame images and controlling for both the kinds of transformation as well astheir magnitude. We used four object categories and images were rendered from3D computer models. In total, 89 human subjects participated in 10 experimentsin which they had to discriminate between two or four categories after rapidpresentation with backward masking. We also tested two recent DCNNs on the sametasks. We found that humans and DCNNs largely agreed on the relativedifficulties of each kind of variation: rotation in depth is by far the hardesttransformation to handle, followed by scale, then rotation in plane, andfinally position. This suggests that humans recognize objects mainly through 2Dtemplate matching, rather than by constructing 3D object models, and that DCNNsare not too unreasonable models of human feed-forward vision. Also, our resultsshow that the variation levels in rotation in depth and scale strongly modulateboth humans' and DCNNs' recognition performances. We thus argue that thesevariations should be controlled in the image datasets used in vision research.
arxiv-17700-73 | Visual Congruent Ads for Image Search | http://arxiv.org/pdf/1604.06481v1.pdf | author:Yannis Kalantidis, Ayman Farahat, Lyndon Kennedy, Ricardo Baeza-Yates, David A. Shamma category:cs.CV cs.HC published:2016-04-21 summary:The quality of user experience online is affected by the relevance andplacement of advertisements. We propose a new system for selecting anddisplaying visual advertisements in image search result sets. Our methodcompares the visual similarity of candidate ads to the image search results andselects the most visually similar ad to be displayed. The method furtherselects an appropriate location in the displayed image grid to minimize theperceptual visual differences between the ad and its neighbors. We conduct anexperiment with about 900 users and find that our proposed method providessignificant improvement in the users' overall satisfaction with the imagesearch experience, without diminishing the users' ability to see the ad orrecall the advertised brand.
arxiv-17700-74 | LOH and behold: Web-scale visual search, recommendation and clustering using Locally Optimized Hashing | http://arxiv.org/pdf/1604.06480v1.pdf | author:Yannis Kalantidis, Lyndon Kennedy, Huy Nguyen, Clayton Mellina, David A. Shamma category:cs.CV cs.IR cs.MM published:2016-04-21 summary:We present a multimedia system based on a novel matching signature able toperform de-duplucation, search, clustering and visual recommendations in a waythat is easily implemented in generic distributed computing environments.Starting from a state-of-the-art algorithm, we propose a novel hashing-basedmatching system that allow for fast search and is easily implemented indistributed system languages like PIG, as it only requires set intersectionsand summations to compute. We make the following contributions: a) we propose anovel hashing method for visual search using locally optimized codes thatperforms on-par with other state-of-the-art hashing approaches but offers moreflexibility in terms of ranking, b) we extend our matching framework tomultiple image queries and provide a simple and scalable solution that canefficiently produce visual recommendations for query sets of thousands ofimages and cluster collections of hundreds of millions of images, c) we showthat this same representation can be used for efficient de-duplication of imagesearch results, performing better than traditional hashing approaches, whilestill requiring only a few milliseconds to run. In this paper we displayresults on datasets of up to 100 Million images, but in practice our system canfind and rank similar images for millions of users from a search set ofhundreds of millions of images in a runtime on the order of one hour on a largeHadoop cluster.
arxiv-17700-75 | On Detection and Structural Reconstruction of Small-World Random Networks | http://arxiv.org/pdf/1604.06474v1.pdf | author:T. Tony Cai, Tengyuan Liang, Alexander Rakhlin category:math.ST cs.LG stat.TH published:2016-04-21 summary:In this paper, we study detection and fast reconstruction of the celebratedWatts-Strogatz (WS) small-world random graph model \citep{watts1998collective}which aims to describe real-world complex networks that exhibit both highclustering and short average length properties. The WS model with neighborhoodsize $k$ and rewiring probability probability $\beta$ can be viewed as acontinuous interpolation between a deterministic ring lattice graph and theErd\H{o}s-R\'{e}nyi random graph. We study both the computational andstatistical aspects of detecting the deterministic ring lattice structure (orlocal geographical links, strong ties) in the presence of random connections(or long range links, weak ties), and for its recovery. The phase diagram interms of $(k,\beta)$ is partitioned into several regions according to thedifficulty of the problem. We propose distinct methods for the various regions.
arxiv-17700-76 | Robust Estimators in High Dimensions without the Computational Intractability | http://arxiv.org/pdf/1604.06443v1.pdf | author:Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Ankur Moitra, Alistair Stewart category:cs.DS cs.IT cs.LG math.IT math.ST stat.ML stat.TH published:2016-04-21 summary:We study high-dimensional distribution learning in an agnostic setting wherean adversary is allowed to arbitrarily corrupt an $\varepsilon$-fraction of thesamples. Such questions have a rich history spanning statistics, machinelearning and theoretical computer science. Even in the most basic settings, theonly known approaches are either computationally inefficient or losedimension-dependent factors in their error guarantees. This raises thefollowing question:Is high-dimensional agnostic distribution learning evenpossible, algorithmically? In this work, we obtain the first computationally efficient algorithms withdimension-independent error guarantees for agnostically learning severalfundamental classes of high-dimensional distributions: (1) a single Gaussian,(2) a product distribution on the hypercube, (3) mixtures of two productdistributions (under a natural balancedness condition), and (4) mixtures ofspherical Gaussians. Our algorithms achieve error that is independent of thedimension, and in many cases scales nearly-linearly with the fraction ofadversarially corrupted samples. Moreover, we develop a general recipe fordetecting and correcting corruptions in high-dimensions, that may be applicableto many other problems.
arxiv-17700-77 | Analysis of the Entropy-guided Switching Trimmed Mean Deviation-based Anisotropic Diffusion filter | http://arxiv.org/pdf/1604.06427v1.pdf | author:Uche A. Nnolim category:cs.CV published:2016-04-21 summary:This report describes the experimental analysis of a proposed switchingfilter-anisotropic diffusion hybrid for the filtering of the fixed value (saltand pepper) impulse noise (FVIN). The filter works well at both low and highnoise densities though it was specifically designed for high noise densitylevels. The filter combines the switching mechanism of decision-based filtersand the partial differential equation-based formulation to yield a powerfulsystem capable of recovering the image signals at very high noise levels.Experimental results indicate that the filter surpasses other filters,especially at very high noise levels. Additionally, its adaptive nature ensuresthat the performance is guided by the metrics obtained from the noisy inputimage. The filter algorithm is of both global and local nature, where theformer is chosen to reduce computation time and complexity, while the latter isused for best results.
arxiv-17700-78 | Row-less Universal Schema | http://arxiv.org/pdf/1604.06361v1.pdf | author:Patrick Verga, Andrew McCallum category:cs.CL published:2016-04-21 summary:Universal schema jointly embeds knowledge bases and textual patterns toreason about entities and relations for automatic knowledge base constructionand information extraction. In the past, entity pairs and relations wererepresented as learned vectors with compatibility determined by a scoringfunction, limiting generalization to unseen text patterns and entities.Recently, 'column-less' versions of Universal Schema have used compositionalpattern encoders to generalize to all text patterns. In this work we take thenext step and propose a 'row-less' model of universal schema, removing explicitentity pair representations. Instead of learning vector representations foreach entity pair in our training set, we treat an entity pair as a function ofits relation types. In experimental results on the FB15k-237 benchmark wedemonstrate that we can match the performance of a comparable model withexplicit entity pair representations using a model of attention over relationtypes. We further demonstrate that the model per- forms with nearly the sameaccuracy on entity pairs never seen during training.
arxiv-17700-79 | Robust Audio Event Recognition with 1-Max Pooling Convolutional Neural Networks | http://arxiv.org/pdf/1604.06338v1.pdf | author:Huy Phan, Lars Hertel, Marco Maass, Alfred Mertins category:cs.NE cs.LG cs.SD published:2016-04-21 summary:We present in this paper a simple, yet efficient convolutional neural network(CNN) architecture for robust audio event recognition. Opposing to deep CNNarchitectures with multiple convolutional and pooling layers topped up withmultiple fully connected layers, the proposed network consists of only threelayers: convolutional, pooling, and softmax layer. It has two features to bedistinguishable from the deep architectures that have been proposed for thetask: varying-size convolutional filters at the convolutional layer and 1-maxpooling scheme at the pooling layer. In intuition, the network tends to selectthe most discriminative features from the whole audio signals for recognition.Our proposed CNN not only shows state-of-the-art performance on the standardtask of robust audio event recognition but also outperforms other deeparchitectures up to 4.5% in terms of recognition accuracy, which is equivalentto 76.3% relative error reduction.
arxiv-17700-80 | Markov models for ocular fixation locations in the presence and absence of colour | http://arxiv.org/pdf/1604.06335v1.pdf | author:Adam B. Kashlak, Eoin Devane, Helge Dietert, Henry Jackson category:stat.AP stat.ML published:2016-04-21 summary:We propose to model the fixation locations of the human eye when observing astill image by a Markovian point process in R 2 . Our approach is data drivenusing k-means clustering of the fixation locations to identify distinct salientregions of the image, which in turn correspond to the states of our Markovchain. Bayes factors are computed as model selection criterion to determine thenumber of clusters. Furthermore, we demonstrate that the behaviour of the humaneye differs from this model when colour information is removed from the givenimage.
arxiv-17700-81 | Nonextensive information theoretical machine | http://arxiv.org/pdf/1604.06153v1.pdf | author:Chaobing Song, Shu-Tao Xia category:cs.LG published:2016-04-21 summary:In this paper, we propose a new discriminative model named \emph{nonextensiveinformation theoretical machine (NITM)} based on nonextensive generalization ofShannon information theory. In NITM, weight parameters are treated as randomvariables. Tsallis divergence is used to regularize the distribution of weightparameters and maximum unnormalized Tsallis entropy distribution is used toevaluate fitting effect. On the one hand, it is showed that some well-knownmargin-based loss functions such as $\ell_{0/1}$ loss, hinge loss, squaredhinge loss and exponential loss can be unified by unnormalized Tsallis entropy.On the other hand, Gaussian prior regularization is generalized to Student-tprior regularization with similar computational complexity. The model can besolved efficiently by gradient-based convex optimization and its performance isillustrated on standard datasets.
arxiv-17700-82 | Deep Adaptive Network: An Efficient Deep Neural Network with Sparse Binary Connections | http://arxiv.org/pdf/1604.06154v1.pdf | author:Xichuan Zhou, Shengli Li, Kai Qin, Kunping Li, Fang Tang, Shengdong Hu, Shujun Liu, Zhi Lin category:cs.LG cs.CV cs.NE published:2016-04-21 summary:Deep neural networks are state-of-the-art models for understanding thecontent of images, video and raw input data. However, implementing a deepneural network in embedded systems is a challenging task, because a typicaldeep neural network, such as a Deep Belief Network using 128x128 images asinput, could exhaust Giga bytes of memory and result in bandwidth and computingbottleneck. To address this challenge, this paper presents a hardware-orienteddeep learning algorithm, named as the Deep Adaptive Network, which attempts toexploit the sparsity in the neural connections. The proposed method adaptivelyreduces the weights associated with negligible features to zero, leading tosparse feedforward network architecture. Furthermore, since the smallproportion of important weights are significantly larger than zero, they can berobustly thresholded and represented using single-bit integers (-1 and +1),leading to implementations of deep neural networks with sparse and binaryconnections. Our experiments showed that, for the application of recognizingMNIST handwritten digits, the features extracted by a two-layer Deep AdaptiveNetwork with about 25% reserved important connections achieved 97.2%classification accuracy, which was almost the same with the standard DeepBelief Network (97.3%). Furthermore, for efficient hardware implementations,the sparse-and-binary-weighted deep neural network could save about 99.3%memory and 99.9% computation units without significant loss of classificationaccuracy for pattern recognition applications.
arxiv-17700-83 | The Extended Littlestone's Dimension for Learning with Mistakes and Abstentions | http://arxiv.org/pdf/1604.06162v1.pdf | author:Chicheng Zhang, Kamalika Chaudhuri category:cs.LG published:2016-04-21 summary:This paper studies classification with an abstention option in the onlinesetting. In this setting, examples arrive sequentially, the learner is given ahypothesis class $\mathcal H$, and the goal of the learner is to either predicta label on each example or abstain, while ensuring that it does not make morethan a pre-specified number of mistakes when it does predict a label. Previous work on this problem has left open two main challenges. First, notmuch is known about the optimality of algorithms, and in particular, about whatan optimal algorithmic strategy is for any individual hypothesis class. Second,while the realizable case has been studied, the more realistic non-realizablescenario is not well-understood. In this paper, we address both challenges.First, we provide a novel measure, called the Extended Littlestone's Dimension,which captures the number of abstentions needed to ensure a certain number ofmistakes. Second, we explore the non-realizable case, and provide upper andlower bounds on the number of abstentions required by an algorithm to guaranteea specified number of mistakes.
arxiv-17700-84 | The THUMOS Challenge on Action Recognition for Videos "in the Wild" | http://arxiv.org/pdf/1604.06182v1.pdf | author:Haroon Idrees, Amir R. Zamir, Yu-Gang Jiang, Alex Gorban, Ivan Laptev, Rahul Sukthankar, Mubarak Shah category:cs.CV published:2016-04-21 summary:Automatically recognizing and localizing wide ranges of human actions hascrucial importance for video understanding. Towards this goal, the THUMOSchallenge was introduced in 2013 to serve as a benchmark for actionrecognition. Until then, video action recognition, including THUMOS challenge,had focused primarily on the classification of pre-segmented (i.e., trimmed)videos, which is an artificial task. In THUMOS 2014, we elevated actionrecognition to a more practical level by introducing temporally untrimmedvideos. These also include `background videos' which share similar scenes andbackgrounds as action videos, but are devoid of the specific actions. The threeeditions of the challenge organized in 2013--2015 have made THUMOS a commonbenchmark for action classification and detection and the annual challenge iswidely attended by teams from around the world. In this paper we describe the THUMOS benchmark in detail and give an overviewof data collection and annotation procedures. We present the evaluationprotocols used to quantify results in the two THUMOS tasks of actionclassification and temporal detection. We also present results of submissionsto the THUMOS 2015 challenge and review the participating approaches.Additionally, we include a comprehensive empirical study evaluating thedifferences in action recognition between trimmed and untrimmed videos, and howwell methods trained on trimmed videos generalize to untrimmed videos. Weconclude by proposing several directions and improvements for future THUMOSchallenges.
arxiv-17700-85 | Evolutionary Image Transition Based on Theoretical Insights of Random Processes | http://arxiv.org/pdf/1604.06187v1.pdf | author:Aneta Neumann, Bradley Alexander, Frank Neumann category:cs.NE published:2016-04-21 summary:Evolutionary algorithms have been widely studied from a theoreticalperspective. In particular, the area of runtime analysis has contributedsignificantly to a theoretical understanding and provided insights into theworking behaviour of these algorithms. We study how these insights intoevolutionary processes can be used for evolutionary art. We introduce thenotion of evolutionary image transition which transfers a given starting imageinto a target image through an evolutionary process. Combining standardmutation effects known from the optimization of the classical benchmarkfunction OneMax and different variants of random walks, we present ways ofperforming evolutionary image transition with different artistic effects.
arxiv-17700-86 | TI-POOLING: transformation-invariant pooling for feature learning in Convolutional Neural Networks | http://arxiv.org/pdf/1604.06318v1.pdf | author:Dmitry Laptev, Nikolay Savinov, Joachim M. Buhmann, Marc Pollefeys category:cs.CV published:2016-04-21 summary:In this paper we present a deep neural network topology that incorporates asimple to implement transformation invariant pooling operator (TI-POOLING).This operator is able to efficiently handle prior knowledge on nuisancevariations in the data, such as rotation or scale changes. Most current methodsusually make use of dataset augmentation to address this issue, but thisrequires larger number of model parameters and more training data, and resultsin significantly increased training time and larger chance of under- oroverfitting. The main reason for these drawbacks is that the learned modelneeds to capture adequate features for all the possible transformations of theinput. On the other hand, we formulate features in convolutional neuralnetworks to be transformation-invariant. We achieve that using parallel siamesearchitectures for the considered transformation set and applying the TI-POOLINGoperator on their outputs before the fully-connected layers. We show that thistopology internally finds the most optimal "canonical" instance of the inputimage for training and therefore limits the redundancy in learned features.This more efficient use of training data results in better performance onpopular benchmark datasets with smaller number of parameters when comparing tostandard convolutional neural networks with dataset augmentation and to otherbaselines.
arxiv-17700-87 | Dynamic matrix factorization with social influence | http://arxiv.org/pdf/1604.06194v1.pdf | author:Aleksandr Y. Aravkin, Kush R. Varshney, Liu Yang category:stat.ML cs.IR cs.SI math.OC published:2016-04-21 summary:Matrix factorization is a key component of collaborative filtering-basedrecommendation systems because it allows us to complete sparse user-by-itemratings matrices under a low-rank assumption that encodes the belief thatsimilar users give similar ratings and that similar items garner similarratings. This paradigm has had immeasurable practical success, but it is notthe complete story for understanding and inferring the preferences of people.First, peoples' preferences and their observable manifestations as ratingsevolve over time along general patterns of trajectories. Second, an individualperson's preferences evolve over time through influence of their socialconnections. In this paper, we develop a unified process model for both typesof dynamics within a state space approach, together with an efficientoptimization scheme for estimation within that model. The model combineselements from recent developments in dynamic matrix factorization, opiniondynamics and social learning, and trust-based recommendation. The estimationbuilds upon recent advances in numerical nonlinear optimization. Empiricalresults on a large-scale data set from the Epinions website demonstrateconsistent reduction in root mean squared error by consideration of the twotypes of dynamics.
arxiv-17700-88 | Articulated Hand Pose Estimation Review | http://arxiv.org/pdf/1604.06195v1.pdf | author:Emad Barsoum category:cs.CV published:2016-04-21 summary:With the increase number of companies focusing on commercializing AugmentedReality (AR), Virtual Reality (VR) and wearable devices, the need for a handbased input mechanism is becoming essential in order to make the experiencenatural, seamless and immersive. Hand pose estimation has progresseddrastically in recent years due to the introduction of commodity depth cameras. Hand pose estimation based on vision is still a challenging problem due toits complexity from self-occlusion (between fingers), close similarity betweenfingers, dexterity of the hands, speed of the pose and the high dimension ofthe hand kinematic parameters. Articulated hand pose estimation is still anopen problem and under intensive research from both academia and industry. The 2 approaches used for hand pose estimation are: discriminative andgenerative. Generative approach is a model based that tries to fit a hand modelto the observed data. Discriminative approach is appearance based, usuallyimplemented with machine learning (ML) and require a large amount of trainingdata. Recent hand pose estimation uses hybrid approach by combining bothdiscriminative and generative methods into a single hand pipeline. In this paper, we focus on reviewing recent progress of hand pose estimationfrom depth sensor. We will survey discriminative methods, generative methodsand hybrid methods. This paper is not a comprehensive review of all hand poseestimation techniques, it is a subset of some of the recent state-of-the-arttechniques.
arxiv-17700-89 | A Novel Approach to Dropped Pronoun Translation | http://arxiv.org/pdf/1604.06285v1.pdf | author:Longyue Wang, Zhaopeng Tu, Xiaojun Zhang, Hang Li, Andy Way, Qun Liu category:cs.CL published:2016-04-21 summary:Dropped Pronouns (DP) in which pronouns are frequently dropped in the sourcelanguage but should be retained in the target language are challenge in machinetranslation. In response to this problem, we propose a semi-supervised approachto recall possibly missing pronouns in the translation. Firstly, we buildtraining data for DP generation in which the DPs are automatically labelledaccording to the alignment information from a parallel corpus. Secondly, webuild a deep learning-based DP generator for input sentences in decoding whenno corresponding references exist. More specifically, the generation istwo-phase: (1) DP position detection, which is modeled as a sequentiallabelling task with recurrent neural networks; and (2) DP prediction, whichemploys a multilayer perceptron with rich features. Finally, we integrate theabove outputs into our translation system to recall missing pronouns by bothextracting rules from the DP-labelled training data and translating theDP-generated input sentences. Experimental results show that our approachachieves a significant improvement of 1.58 BLEU points in translationperformance with 66% F-score for DP generation accuracy.
arxiv-17700-90 | Chinese Song Iambics Generation with Neural Attention-based Model | http://arxiv.org/pdf/1604.06274v1.pdf | author:Qixin Wang, Tianyi Luo, Dong Wang, Chao Xing category:cs.CL published:2016-04-21 summary:Learning and generating Chinese poems is a charming yet challenging task.Traditional approaches involve various language modeling and machinetranslation techniques, however, they perform not as well when generating poemswith complex pattern constraints, for example Song iambics, a famous type ofpoems that involve variable-length sentences and strict rhythmic patterns. Thispaper applies the attention-based sequence-to-sequence model to generateChinese Song iambics. Specifically, we encode the cue sentences by abi-directional Long-Short Term Memory (LSTM) model and then predict the entireiambic with the information provided by the encoder, in the form of anattention-based LSTM that can regularize the generation process by the finestructure of the input cues. Several techniques are investigated to improve themodel, including global context integration, hybrid style training, charactervector initialization and adaptation. Both the automatic and subjectiveevaluation results show that our model indeed can learn the complex structuraland rhythmic patterns of Song iambics, and the generation is rather successful.
arxiv-17700-91 | OCR Error Correction Using Character Correction and Feature-Based Word Classification | http://arxiv.org/pdf/1604.06225v1.pdf | author:Ido Kissos, Nachum Dershowitz category:cs.IR cs.CL published:2016-04-21 summary:This paper explores the use of a learned classifier for post-OCR textcorrection. Experiments with the Arabic language show that this approach, whichintegrates a weighted confusion matrix and a shallow language model, improvesthe vast majority of segmentation and recognition errors, the most frequenttypes of error on our dataset.
arxiv-17700-92 | Automatic 3D Reconstruction of Manifold Meshes via Delaunay Triangulation and Mesh Sweeping | http://arxiv.org/pdf/1604.06258v1.pdf | author:Andrea Romanoni, AmaÃ«l Delaunoy, Marc Pollefeys, Matteo Matteucci category:cs.CV published:2016-04-21 summary:In this paper we propose a new approach to incrementally initialize amanifold surface for automatic 3D reconstruction from images. More precisely wefocus on the automatic initialization of a 3D mesh as close as possible to thefinal solution; indeed many approaches require a good initial solution forfurther refinement via multi-view stereo techniques. Our novel algorithmautomatically estimates an initial manifold mesh for surface evolvingmulti-view stereo algorithms, where the manifold property needs to be enforced.It bootstraps from 3D points extracted via Structure from Motion, then iteratesbetween a state-of-the-art manifold reconstruction step and a novel meshsweeping algorithm that looks for new 3D points in the neighborhood of thereconstructed manifold to be added in the manifold reconstruction. Theexperimental results show quantitatively that the mesh sweeping improves theresolution and the accuracy of the manifold reconstruction, allowing a betterconvergence of state-of-the-art surface evolution multi-view stereo algorithms.
arxiv-17700-93 | Evaluation of the Effect of Improper Segmentation on Word Spotting | http://arxiv.org/pdf/1604.06243v1.pdf | author:Sounak Dey, Anguelos Nicolaou, Josep Llados, Umapada Pal category:cs.CV published:2016-04-21 summary:Word spotting is an important recognition task in historical documentanalysis. In most cases methods are developed and evaluated assuming perfectword segmentations. In this paper we propose an experimental framework toquantify the effect of goodness of word segmentation has on the performanceachieved by word spotting methods in identical unbiased conditions. Theframework consists of generating systematic distortions on segmentation andretrieving the original queries from the distorted dataset. We apply theframework on the George Washington and Barcelona Marriage Dataset and onseveral established and state-of-the-art methods. The experiments allow for anestimate of the end-to-end performance of word spotting methods.
arxiv-17700-94 | Incremental Reconstruction of Urban Environments by Edge-Points Delaunay Triangulation | http://arxiv.org/pdf/1604.06232v2.pdf | author:Andrea Romanoni, Matteo Matteucci category:cs.CV cs.RO I.4.5 published:2016-04-21 summary:Urban reconstruction from a video captured by a surveying vehicle constitutesa core module of automated mapping. When computational power represents alimited resource and, a detailed map is not the primary goal, thereconstruction can be performed incrementally, from a monocular video, carvinga 3D Delaunay triangulation of sparse points; this allows online incrementalmapping for tasks such as traversability analysis or obstacle avoidance. Toexploit the sharp edges of urban landscape, we propose to use a Delaunaytriangulation of Edge-Points, which are the 3D points corresponding to imageedges. These points constrain the edges of the 3D Delaunay triangulation toreal-world edges. Besides the use of the Edge-Points, a second contribution ofthis paper is the Inverse Cone Heuristic that preemptively avoids the creationof artifacts in the reconstructed manifold surface. We force the reconstructionof a manifold surface since it makes it possible to apply computer graphics orphotometric refinement algorithms to the output mesh. We evaluated our approachon four real sequences of the public available KITTI dataset by comparing theincremental reconstruction against Velodyne measurements.
arxiv-17700-95 | Local Binary Pattern for Word Spotting in Handwritten Historical Document | http://arxiv.org/pdf/1604.05907v2.pdf | author:Sounak Dey, Anguelos Nicolaou, Josep Llados, Umapada Pal category:cs.CV published:2016-04-20 summary:Digital libraries store images which can be highly degraded and to index thiskind of images we resort to word spot- ting as our information retrievalsystem. Information retrieval for handwritten document images is morechallenging due to the difficulties in complex layout analysis, largevariations of writing styles, and degradation or low quality of historicalmanuscripts. This paper presents a simple innovative learning-free method forword spotting from large scale historical documents combining Local BinaryPattern (LBP) and spatial sampling. This method offers three advantages:firstly, it operates in completely learning free paradigm which is verydifferent from unsupervised learning methods, secondly, the computational timeis significantly low because of the LBP features which are very fast tocompute, and thirdly, the method can be used in scenarios where annotations arenot available. Finally we compare the results of our proposed retrieval methodwith the other methods in the literature.
arxiv-17700-96 | Distributed Entity Disambiguation with Per-Mention Learning | http://arxiv.org/pdf/1604.05875v1.pdf | author:Tiep Mai, Bichen Shi, Patrick K. Nicholson, Deepak Ajwani, Alessandra Sala category:cs.CL cs.IR published:2016-04-20 summary:Entity disambiguation, or mapping a phrase to its canonical representation ina knowledge base, is a fundamental step in many natural language processingapplications. Existing techniques based on global ranking models fail tocapture the individual peculiarities of the words and hence, either struggle tomeet the accuracy requirements of many real-world applications or they are toocomplex to satisfy real-time constraints of applications. In this paper, we propose a new disambiguation system that learns specializedfeatures and models for disambiguating each ambiguous phrase in the Englishlanguage. To train and validate the hundreds of thousands of learning modelsfor this purpose, we use a Wikipedia hyperlink dataset with more than 170million labelled annotations. We provide an extensive experimental evaluationto show that the accuracy of our approach compares favourably with respect tomany state-of-the-art disambiguation systems. The training required for ourapproach can be easily distributed over a cluster. Furthermore, updating oursystem for new entities or calibrating it for special ones is a computationallyfast process, that does not affect the disambiguation of the other entities.
arxiv-17700-97 | A Factorization Machine Framework for Testing Bigram Embeddings in Knowledgebase Completion | http://arxiv.org/pdf/1604.05878v1.pdf | author:Johannes Welbl, Guillaume Bouchard, Sebastian Riedel category:cs.CL cs.AI cs.NE stat.ML published:2016-04-20 summary:Embedding-based Knowledge Base Completion models have so far mostly combineddistributed representations of individual entities or relations to computetruth scores of missing links. Facts can however also be represented usingpairwise embeddings, i.e. embeddings for pairs of entities and relations. Inthis paper we explore such bigram embeddings with a flexible FactorizationMachine model and several ablations from it. We investigate the relevance ofvarious bigram types on the fb15k237 dataset and find relative improvementscompared to a compositional model.
arxiv-17700-98 | What we write about when we write about causality: Features of causal statements across large-scale social discourse | http://arxiv.org/pdf/1604.05781v2.pdf | author:Thomas C. McAndrew, Joshua C. Bongard, Christopher M. Danforth, Peter S. Dodds, Paul D. H. Hines, James P. Bagrow category:cs.CY cs.CL cs.SI published:2016-04-20 summary:Identifying and communicating relationships between causes and effects isimportant for understanding our world, but is affected by language structure,cognitive and emotional biases, and the properties of the communication medium.Despite the increasing importance of social media, much remains unknown aboutcausal statements made online. To study real-world causal attribution, weextract a large-scale corpus of causal statements made on the Twitter socialnetwork platform as well as a comparable random control corpus. We comparecausal and control statements using statistical language and sentiment analysistools. We find that causal statements have a number of significant lexical andgrammatical differences compared with controls and tend to be more negative insentiment than controls. Causal statements made online tend to focus on newsand current events, medicine and health, or interpersonal relationships, asshown by topic models. By quantifying the features and potential biases ofcausality communication, this study improves our understanding of the accuracyof information and opinions found online.
arxiv-17700-99 | Embedded all relevant feature selection with Random Ferns | http://arxiv.org/pdf/1604.06133v1.pdf | author:Miron Bartosz Kursa category:cs.LG published:2016-04-20 summary:Many machine learning methods can produce variable importance scoresexpressing the usability of each feature in context of the produced model;those scores on their own are yet not sufficient to generate feature selection,especially when an all relevant selection is required. Although there arewrapper methods aiming to solve this problem, they introduce a substantialincrease in the required computational effort. In this paper I investigate an idea of incorporating all relevant selectionwithin the training process by producing importance for implicitly generatedshadows, attributes irrelevant by design. I propose and evaluate such a methodin context of random ferns classifier. Experiment results confirm theeffectiveness of such approach, although show that fully stochastic nature ofrandom ferns limits its applicability either to small dimensions or as a partof a broader feature selection procedure.
arxiv-17700-100 | Network of Experts for Large-Scale Image Categorization | http://arxiv.org/pdf/1604.06119v1.pdf | author:Karim Ahmed, Mohammad Haris Baig, Lorenzo Torresani category:cs.CV published:2016-04-20 summary:We present a tree-structured network architecture for large-scale imageclassification. The trunk of the network contains convolutional layersoptimized over all classes. At a given depth, the trunk splits into separatebranches, each dedicated to discriminate a different subset of classes. Eachbranch acts as an expert classifying a set of categories that are difficult totell apart, while the trunk provides common knowledge to all experts in theform of shared features. The training of our "network of experts" is completelyend-to-end: the partition of categories into disjoint subsets is learnedsimultaneously with the parameters of the network trunk and the experts aretrained jointly by minimizing a single learning objective over all classes. Theproposed structure can be built from any existing convolutional neural network(CNN). We demonstrate its generality by adapting 3 popular CNNs for imagecategorization into the form of networks of experts. Our experiments onCIFAR100 and ImageNet show that in each case our method yields a substantialimprovement in accuracy over the base CNN, and gives the best reported resulton CIFAR100. Finally, the improvement in accuracy comes at little additionalcost: compared to the base network, the training time of our model is about1.5X and the number of parameters is comparable or in some cases even lower.
arxiv-17700-101 | Speaker Cluster-Based Speaker Adaptive Training for Deep Neural Network Acoustic Modeling | http://arxiv.org/pdf/1604.06113v1.pdf | author:Wei Chu, Ruxin Chen category:cs.CL published:2016-04-20 summary:A speaker cluster-based speaker adaptive training (SAT) method under deepneural network-hidden Markov model (DNN-HMM) framework is presented in thispaper. During training, speakers that are acoustically adjacent to each otherare hierarchically clustered using an i-vector based distance metric. DNNs withspeaker dependent layers are then adaptively trained for each cluster ofspeakers. Before decoding starts, an unseen speaker in test set is matched tothe closest speaker cluster through comparing i-vector based distances. Thepreviously trained DNN of the matched speaker cluster is used for decodingutterances of the test speaker. The performance of the proposed method on alarge vocabulary spontaneous speech recognition task is evaluated on a trainingset of with 1500 hours of speech, and a test set of 24 speakers with 1774utterances. Comparing to a speaker independent DNN with a baseline word errorrate of 11.6%, a relative 6.8% reduction in word error rate is observed fromthe proposed method.
arxiv-17700-102 | Estimating 3D Trajectories from 2D Projections via Disjunctive Factored Four-Way Conditional Restricted Boltzmann Machines | http://arxiv.org/pdf/1604.05865v1.pdf | author:Decebal Constantin Mocanu, Haitham Bou Ammar, Luis Puig, Eric Eaton, Antonio Liotta category:cs.CV cs.AI published:2016-04-20 summary:Estimation, recognition, and near-future prediction of 3D trajectories basedon their two dimensional projections available from one camera source is anexceptionally difficult problem due to uncertainty in the trajectories andenvironment, high dimensionality of the specific trajectory states, lack ofenough labeled data and so on. In this article, we propose a solution to solvethis problem based on a novel deep learning model dubbed Disjunctive FactoredFour-Way Conditional Restricted Boltzmann Machine (DFFW-CRBM). Our methodimproves state-of-the-art deep learning techniques for high dimensionaltime-series modeling by introducing a novel tensor factorization capable ofdriving forth order Boltzmann machines to considerably lower energy levels, atno computational costs. DFFW-CRBMs are capable of accurately estimating,recognizing, and performing near-future prediction of three-dimensionaltrajectories from their 2D projections while requiring limited amount oflabeled data. We evaluate our method on both simulated and real-world data,showing its effectiveness in predicting and classifying complex balltrajectories and human activities.
arxiv-17700-103 | Jansen-MIDAS: a multi-level photomicrograph segmentation software based on isotropic undecimated wavelets | http://arxiv.org/pdf/1604.05921v1.pdf | author:Alexandre Fioravante de Siqueira, FlÃ¡vio Camargo Cabrera, Wagner Massayuki Nakasuga, Aylton Pagamisse, Aldo Eloizo Job category:cs.CV 68T10 published:2016-04-20 summary:Image segmentation, the process of separating the elements within an image,is frequently used for obtaining information from photomicrographs. However,segmentation methods should be used with reservations: incorrect segmentationcan mislead when interpreting regions of interest (ROI), thus decreasing thesuccess rate of additional procedures. Multi-Level Starlet Segmentation (MLSS)and Multi-Level Starlet Optimal Segmentation (MLSOS) were developed to addressthe photomicrograph segmentation deficiency on general tools. These methodsgave rise to Jansen-MIDAS, an open-source software which a scientist can use toobtain a multi-level threshold segmentation of his/hers photomicrographs. Thissoftware is presented in two versions: a text-based version, for GNU Octave,and a graphical user interface (GUI) version, for MathWorks MATLAB. It can beused to process several types of images, becoming a reliable alternative to thescientist.
arxiv-17700-104 | Scene Parsing with Integration of Parametric and Non-parametric Models | http://arxiv.org/pdf/1604.05848v1.pdf | author:Bing Shuai, Zhen Zuo, Gang Wang, Bing Wang category:cs.CV published:2016-04-20 summary:We adopt Convolutional Neural Networks (CNNs) to be our parametric model tolearn discriminative features and classifiers for local patch classification.Based on the occurrence frequency distribution of classes, an ensemble of CNNs(CNN-Ensemble) are learned, in which each CNN component focuses on learningdifferent and complementary visual patterns. The local beliefs of pixels areoutput by CNN-Ensemble. Considering that visually similar pixels areindistinguishable under local context, we leverage the global scene semanticsto alleviate the local ambiguity. The global scene constraint is mathematicallyachieved by adding a global energy term to the labeling energy function, and itis practically estimated in a non-parametric framework. A large margin basedCNN metric learning method is also proposed for better global beliefestimation. In the end, the integration of local and global beliefs gives riseto the class likelihood of pixels, based on which maximum marginal inference isperformed to generate the label prediction maps. Even without anypost-processing, we achieve state-of-the-art results on the challengingSiftFlow and Barcelona benchmarks.
arxiv-17700-105 | Automatic Graphic Logo Detection via Fast Region-based Convolutional Networks | http://arxiv.org/pdf/1604.06083v1.pdf | author:GonÃ§alo Oliveira, Xavier FrazÃ£o, AndrÃ© Pimentel, Bernardete Ribeiro category:cs.CV published:2016-04-20 summary:Brand recognition is a very challenging topic with many useful applicationsin localization recognition, advertisement and marketing. In this paper wepresent an automatic graphic logo detection system that robustly handlesunconstrained imaging conditions. Our approach is based on Fast Region-basedConvolutional Networks (FRCN) proposed by Ross Girshick, which have shownstate-of-the-art performance in several generic object recognition tasks(PASCAL Visual Object Classes challenges). In particular, we use two CNN modelspre-trained with the ILSVRC ImageNet dataset and we look at the selectivesearch of windows `proposals' in the pre-processing stage and data augmentationto enhance the logo recognition rate. The novelty lies in the use of transferlearning to leverage powerful Convolutional Neural Network models trained withlarge-scale datasets and repurpose them in the context of graphic logodetection. Another benefit of this framework is that it allows for multipledetections of graphic logos using regions that are likely to have an object.Experimental results with the FlickrLogos-32 dataset show not only thepromising performance of our developed models with respect to noise and othertransformations a graphic logo can be subject to, but also its superiority overstate-of-the-art systems with hand-crafted models and features.
arxiv-17700-106 | DeepSymmetry: Joint Symmetry and Depth Estimation using Deep Neural Networks | http://arxiv.org/pdf/1604.06079v1.pdf | author:Guilin Liu, Chao Yang, Zimo Li, Duygu Ceylan, Qixing Huang category:cs.CV published:2016-04-20 summary:Due to the abundance of 2D product images from the Internet, developingefficient and scalable algorithms to recover the missing depth information iscentral to many applications. Recent works have addressed the single-view depthestimation problem by utilizing convolutional neural networks. In this paper,we show that exploring symmetry information, which is ubiquitous in man madeobjects, can significantly boost the quality of such depth predictions.Specifically, we propose a new convolutional neural network architecture tofirst estimate dense symmetric correspondences in a product image and thenpropose an optimization which utilizes this information explicitly tosignificantly improve the quality of single-view depth estimations. We haveevaluated our approach extensively, and experimental results show that thisapproach outperforms state-of-the-art depth estimation techniques.
arxiv-17700-107 | Question Answering via Integer Programming over Semi-Structured Knowledge | http://arxiv.org/pdf/1604.06076v1.pdf | author:Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Peter Clark, Oren Etzioni, Dan Roth category:cs.AI cs.CL published:2016-04-20 summary:Answering science questions posed in natural language is an important AIchallenge. Answering such questions often requires non-trivial inference andknowledge that goes beyond factoid retrieval. Yet, most systems for this taskare based on relatively shallow Information Retrieval (IR) and statisticalcorrelation techniques operating on large unstructured corpora. We propose astructured inference system for this task, formulated as an Integer LinearProgram (ILP), that answers natural language questions using a semi-structuredknowledge base derived from text, including questions requiring multi-stepinference and a combination of multiple facts. On a dataset of real, unseenscience questions, our system significantly outperforms (+14%) the bestprevious attempt at structured reasoning for this task, which used Markov LogicNetworks (MLNs). It also improves upon a previous ILP formulation by 17.7%.When combined with unstructured inference methods, the ILP system significantlyboosts overall performance (+10%). Finally, we show our approach issubstantially more robust to a simple answer perturbation compared tostatistical correlation methods.
arxiv-17700-108 | Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation | http://arxiv.org/pdf/1604.06057v1.pdf | author:Tejas D. Kulkarni, Karthik R. Narasimhan, Ardavan Saeedi, Joshua B. Tenenbaum category:cs.LG cs.AI cs.CV cs.NE stat.ML published:2016-04-20 summary:Learning goal-directed behavior in environments with sparse feedback is amajor challenge for reinforcement learning algorithms. The primary difficultyarises due to insufficient exploration, resulting in an agent being unable tolearn robust value functions. Intrinsically motivated agents can explore newbehavior for its own sake rather than to directly solve problems. Suchintrinsic behaviors could eventually help the agent solve tasks posed by theenvironment. We present hierarchical-DQN (h-DQN), a framework to integratehierarchical value functions, operating at different temporal scales, withintrinsically motivated deep reinforcement learning. A top-level value functionlearns a policy over intrinsic goals, and a lower-level function learns apolicy over atomic actions to satisfy the given goals. h-DQN allows forflexible goal specifications, such as functions over entities and relations.This provides an efficient space for exploration in complicated environments.We demonstrate the strength of our approach on two problems with very sparse,delayed feedback: (1) a complex discrete MDP with stochastic transitions, and(2) the classic ATARI game `Montezuma's Revenge'.
arxiv-17700-109 | Random Projection Estimation of Discrete-Choice Models with Large Choice Sets | http://arxiv.org/pdf/1604.06036v1.pdf | author:Khai X. Chiong, Matthew Shum category:stat.ML published:2016-04-20 summary:We introduce sparse random projection, an important dimension-reduction toolfrom machine learning, for the estimation of discrete-choice models withhigh-dimensional choice sets. Initially, high-dimensional data are compressedinto a lower-dimensional Euclidean space using random projections.Subsequently, estimation proceeds using cyclic monotonicity moment inequalitiesimplied by the multinomial choice model; the estimation procedure issemi-parametric and does not require explicit distributional assumptions to bemade regarding the random utility errors. The random projection procedure isjustified via the Johnson-Lindenstrauss Lemma -- the pairwise distances betweendata points are preserved during data compression, which we exploit to showconvergence of our estimator. The estimator works well in simulations and in anapplication to a supermarket scanner dataset.
arxiv-17700-110 | Trading-Off Cost of Deployment Versus Accuracy in Learning Predictive Models | http://arxiv.org/pdf/1604.05819v1.pdf | author:Daniel P. Robinson, Suchi Saria category:stat.ML cs.LG published:2016-04-20 summary:Predictive models are finding an increasing number of applications in manyindustries. As a result, a practical means for trading-off the cost ofdeploying a model versus its effectiveness is needed. Our work is motivated byrisk prediction problems in healthcare. Cost-structures in domains such ashealthcare are quite complex, posing a significant challenge to existingapproaches. We propose a novel framework for designing cost-sensitivestructured regularizers that is suitable for problems with complex costdependencies. We draw upon a surprising connection to boolean circuits. Inparticular, we represent the problem costs as a multi-layer boolean circuit,and then use properties of boolean circuits to define an extended featurevector and a group regularizer that exactly captures the underlying coststructure. The resulting regularizer may then be combined with a fidelityfunction to perform model prediction, for example. For the challengingreal-world application of risk prediction for sepsis in intensive care units,the use of our regularizer leads to models that are in harmony with theunderlying cost structure and thus provide an excellent prediction accuracyversus cost tradeoff.
arxiv-17700-111 | Constructive Preference Elicitation by Setwise Max-margin Learning | http://arxiv.org/pdf/1604.06020v1.pdf | author:Stefano Teso, Andrea Passerini, Paolo Viappiani category:stat.ML cs.AI cs.LG 68T05 published:2016-04-20 summary:In this paper we propose an approach to preference elicitation that issuitable to large configuration spaces beyond the reach of existingstate-of-the-art approaches. Our setwise max-margin method can be viewed as ageneralization of max-margin learning to sets, and can produce a set of"diverse" items that can be used to ask informative queries to the user.Moreover, the approach can encourage sparsity in the parameter space, in orderto favor the assessment of utility towards combinations of weights thatconcentrate on just few features. We present a mixed integer linear programmingformulation and show how our approach compares favourably with Bayesianpreference elicitation alternatives and easily scales to realistic datasets.
arxiv-17700-112 | Greedy Criterion in Orthogonal Greedy Learning | http://arxiv.org/pdf/1604.05993v1.pdf | author:Lin Xu, Shaobo Lin, Jinshan Zeng, Xia Liu, Zongben Xu category:cs.LG published:2016-04-20 summary:Orthogonal greedy learning (OGL) is a stepwise learning scheme that startswith selecting a new atom from a specified dictionary via the steepest gradientdescent (SGD) and then builds the estimator through orthogonal projection. Inthis paper, we find that SGD is not the unique greedy criterion and introduce anew greedy criterion, called "$\delta$-greedy threshold" for learning. Based onthe new greedy criterion, we derive an adaptive termination rule for OGL. Ourtheoretical study shows that the new learning scheme can achieve the existing(almost) optimal learning rate of OGL. Plenty of numerical experiments areprovided to support that the new scheme can achieve almost optimalgeneralization performance, while requiring less computation than OGL.
arxiv-17700-113 | Depth Image Inpainting: Improving Low Rank Matrix Completion with Low Gradient Regularization | http://arxiv.org/pdf/1604.05817v1.pdf | author:Hongyang Xue, Shengming Zhang, Deng Cai category:cs.CV published:2016-04-20 summary:We consider the case of inpainting single depth images. Without correspondingcolor images, previous or next frames, depth image inpainting is quitechallenging. One natural solution is to regard the image as a matrix and adoptthe low rank regularization just as inpainting color images. However, the lowrank assumption does not make full use of the properties of depth images. A shallow observation may inspire us to penalize the non-zero gradients bysparse gradient regularization. However, statistics show that though mostpixels have zero gradients, there is still a non-ignorable part of pixels whosegradients are equal to 1. Based on this specific property of depth images , wepropose a low gradient regularization method in which we reduce the penalty forgradient 1 while penalizing the non-zero gradients to allow for gradual depthchanges. The proposed low gradient regularization is integrated with the lowrank regularization into the low rank low gradient approach for depth imageinpainting. We compare our proposed low gradient regularization with sparsegradient regularization. The experimental results show the effectiveness of ourproposed approach.
arxiv-17700-114 | A topological insight into restricted Boltzmann machines | http://arxiv.org/pdf/1604.05978v1.pdf | author:Decebal Constantin Mocanu, Elena Mocanu, Phuong H. Nguyen, Madeleine Gibescu, Antonio Liotta category:cs.NE cs.AI cs.SI published:2016-04-20 summary:Restricted Boltzmann Machines (RBMs) and models derived from them have beensuccessfully used as basic building blocks in deep artificial neural networksfor automatic features extraction, unsupervised weights initialization, butalso as density estimators. Thus, their generative and discriminativecapabilities, but also their computational time are instrumental to a widerange of applications. Our main contribution is to look at RBMs from atopological perspective, bringing insights from network science. Firstly, herewe show that RBMs and Gaussian RBMs (GRBMs) are bipartite graphs whichnaturally have a small-world topology. Secondly, we demonstrate both onsynthetic and real-world databases that by constraining RBMs and GRBMs to ascale-free topology (while still considering local neighborhoods and datadistribution), we reduce the number of weights that need to be computed by afew orders of magnitude, at virtually no loss in generative performance.Thirdly, we show that, given the same number of weights, our proposed sparsemodels (which by design have a higher number of hidden neurons) achieve bettergenerative capabilities than standard fully connected RBMs and GRBMs (which bydesign have a smaller number of hidden neurons) at no additional computationalcosts.
arxiv-17700-115 | Computational Drug Repositioning Using Continuous Self-controlled Case Series | http://arxiv.org/pdf/1604.05976v1.pdf | author:Zhaobin Kuang, James Thomson, Michael Caldwell, Peggy Peissig, Ron Stewart, David Page category:stat.AP stat.ML published:2016-04-20 summary:Computational Drug Repositioning (CDR) is the task of discovering potentialnew indications for existing drugs by mining large-scale heterogeneousdrug-related data sources. Leveraging the patient-level temporal orderinginformation between numeric physiological measurements and various drugprescriptions provided in Electronic Health Records (EHRs), we propose aContinuous Self-controlled Case Series (CSCCS) model for CDR. As an initialevaluation, we look for drugs that can control Fasting Blood Glucose (FBG)level in our experiments. Applying CSCCS to the Marshfield Clinic EHR,well-known drugs that are indicated for controlling blood glucose level arerediscovered. Furthermore, some drugs with recent literature support for thepotential effect of blood glucose level control are also identified.
arxiv-17700-116 | Deep CNNs for HEp-2 Cells Classification : A Cross-specimen Analysis | http://arxiv.org/pdf/1604.05816v1.pdf | author:Hongwei Li, Jianguo Zhang, Wei-Shi Zheng category:cs.CV published:2016-04-20 summary:Automatic classification of Human Epithelial Type-2 (HEp-2) cells stainingpatterns is an important and yet a challenging problem. Although both shallowand deep methods have been applied, the study of deep convolutional networks(CNNs) on this topic is shallow to date, thus failed to claim its top positionfor this problem. In this paper, we propose a novel study of using CNNs forHEp-2 cells classification focusing on cross-specimen analysis, a keyevaluation for generalization. For the first time, our study reveals severalkey factors of using CNNs for HEp-2 cells classification. Our proposed systemachieves state-of-the-art classification accuracy on public benchmark dataset.Comparative experiments on different training data reveals that addingdifferent specimens,rather than increasing in numbers by affinetransformations, helps to train a good deep model. This opens a new avenue foradopting deep CNNs to HEp-2 cells classification.
arxiv-17700-117 | Sherlock: Sparse Hierarchical Embeddings for Visually-aware One-class Collaborative Filtering | http://arxiv.org/pdf/1604.05813v1.pdf | author:Ruining He, Chunbin Lin, Jianguo Wang, Julian McAuley category:cs.IR cs.CV published:2016-04-20 summary:Building successful recommender systems requires uncovering the underlyingdimensions that describe the properties of items as well as users' preferencestoward them. In domains like clothing recommendation, explaining users'preferences requires modeling the visual appearance of the items in question.This makes recommendation especially challenging, due to both the complexityand subtlety of people's 'visual preferences,' as well as the scale anddimensionality of the data and features involved. Ultimately, a successfulmodel should be capable of capturing considerable variance across differentcategories and styles, while still modeling the commonalities explained by`global' structures in order to combat the sparsity (e.g. cold-start),variability, and scale of real-world datasets. Here, we address thesechallenges by building such structures to model the visual dimensions acrossdifferent product categories. With a novel hierarchical embedding architecture,our method accounts for both high-level (colorfulness, darkness, etc.) andsubtle (e.g. casualness) visual characteristics simultaneously.
arxiv-17700-118 | Parametric Object Motion from Blur | http://arxiv.org/pdf/1604.05933v1.pdf | author:Jochen Gast, Anita Sellent, Stefan Roth category:cs.CV published:2016-04-20 summary:Motion blur can adversely affect a number of vision tasks, hence it isgenerally considered a nuisance. We instead treat motion blur as a usefulsignal that allows to compute the motion of objects from a single image.Drawing on the success of joint segmentation and parametric motion models inthe context of optical flow estimation, we propose a parametric object motionmodel combined with a segmentation mask to exploit localized, non-uniformmotion blur. Our parametric image formation model is differentiable w.r.t. themotion parameters, which enables us to generalize marginal-likelihoodtechniques from uniform blind deblurring to localized, non-uniform blur. Atwo-stage pipeline, first in derivative space and then in image space, allowsto estimate both parametric object motion as well as a motion segmentation froma single image alone. Our experiments demonstrate its ability to cope with verychallenging cases of object motion blur.
arxiv-17700-119 | A Deep Neural Network for Chinese Zero Pronoun Resolution | http://arxiv.org/pdf/1604.05800v1.pdf | author:Yin Qingyu, Zhang Weinan, Zhang Yu, Liu Ting category:cs.CL published:2016-04-20 summary:This paper investigates the problem of Chinese zero pronoun resolution. Mostexisting approaches are based on machine learning algorithms, usinghand-crafted features, which is labor-intensive. More- over, semanticinformation that is essential in the resolution of noun phrases has not beenaddressed enough by previous approaches on zero pronoun resolution. This isbecause that zero pronouns have no descriptive information, which makes italmost impossible to calculate semantic similarity between the zero pronoun andits candidate antecedents. To deal with these problems, we aim at exploringlearn- ing algorithms that are capable of generating semantic representationsfor zero pronouns, capturing the intricate related- ness between zero pronounsand candidate antecedents, and meanwhile less dependent on extensive featureengineering. To achieve this goal, in this paper, we propose a zeropronoun-specific neural network for Chinese zero pronoun resolution task.Experimental results show that our approach significantly outperforms thestate-of-the- art method.
arxiv-17700-120 | Multi-agent evolutionary systems for the generation of complex virtual worlds | http://arxiv.org/pdf/1604.05792v1.pdf | author:Jan Kruse, Andy M. Connor category:cs.NE published:2016-04-20 summary:Modern films, games and virtual reality applications are dependent onconvincing computer graphics. Highly complex models are a requirement for thesuccessful delivery of many scenes and environments. While workflows such asrendering, compositing and animation have been streamlined to accommodateincreasing demands, modelling complex models is still a laborious task. Thispaper introduces the computational benefits of an Interactive Genetic Algorithm(IGA) to computer graphics modelling while compensating the effects of userfatigue, a common issue with Interactive Evolutionary Computation. Anintelligent agent is used in conjunction with an IGA that offers the potentialto reduce the effects of user fatigue by learning from the choices made by thehuman designer and directing the search accordingly. This workflow acceleratesthe layout and distribution of basic elements to form complex models. Itcaptures the designer's intent through interaction, and encourages playfuldiscovery.
arxiv-17700-121 | Procedural urban environments for FPS games | http://arxiv.org/pdf/1604.05791v1.pdf | author:Jan Kruse, Ricardo Sosa, Andy M. Connor category:cs.AI cs.HC cs.NE published:2016-04-20 summary:This paper presents a novel approach to procedural generation of urban mapsfor First Person Shooter (FPS) games. A multi-agent evolutionary system isemployed to place streets, buildings and other items inside the Unity3D gameengine, resulting in playable video game levels. A computational agent istrained using machine learning techniques to capture the intent of the gamedesigner as part of the multi-agent system, and to enable a semi-automatedaesthetic selection for the underlying genetic algorithm.
arxiv-17700-122 | Labeled Multi-Bernoulli Tracking for Industrial Mobile Platform Safety | http://arxiv.org/pdf/1604.05966v2.pdf | author:Tharindu Rathnayake, Reza Hoseinnezhad, Ruwan Tennakoon, Alireza Bab-Hadiashar category:cs.CV published:2016-04-20 summary:This paper presents a track-before-detect labeled multi-Bernoulli filtertailored for industrial mobile platform safety applications. We derive twoapplication specific separable likelihood functions that capture the geometricshape and colour information of the human targets who are wearing a highvisible vest. These likelihoods are then used in a labeled multi-Bernoullifilter with a novel two step Bayesian update. Preliminary simulation resultsshow that the proposed solution can successfully track human workers wearing aluminous yellow colour vest in an industrial environment.
arxiv-17700-123 | Dialog-based Language Learning | http://arxiv.org/pdf/1604.06045v4.pdf | author:Jason Weston category:cs.CL published:2016-04-20 summary:A long-term goal of machine learning research is to build an intelligentdialog agent. Most research in natural language understanding has focused onlearning from fixed training sets of labeled data, with supervision either atthe word level (tagging, parsing tasks) or sentence level (question answering,machine translation). This kind of supervision is not realistic of how humanslearn, where language is both learned by, and used for, communication. In thiswork, we study dialog-based language learning, where supervision is givennaturally and implicitly in the response of the dialog partner during theconversation. We study this setup in two domains: the bAbI dataset of (Westonet al., 2015) and large-scale question answering from (Dodge et al., 2015). Weevaluate a set of baseline learning strategies on these tasks, and show that anovel model incorporating predictive lookahead is a promising approach forlearning from a teacher's response. In particular, a surprising result is thatit can learn to answer questions correctly without any reward-based supervisionat all.
arxiv-17700-124 | M$^2$S-Net: Multi-Modal Similarity Metric Learning based Deep Convolutional Network for Answer Selection | http://arxiv.org/pdf/1604.05519v2.pdf | author:Lingxun Meng, Yan Li category:cs.CL published:2016-04-19 summary:Recent works using artificial neural networks based on distributed wordrepresentation greatly boost performance on various natural language processingtasks, especially the answer selection problem. Nevertheless, most of theprevious works used deep learning methods (like LSTM-RNN, CNN, etc.) only tocapture semantic representation of each sentence separately, withoutconsidering the interdependence between each other. In this paper, we propose anovel end-to-end learning framework which constitutes deep convolutional neuralnetwork based on multi-modal similarity metric learning (M$^2$S-Net) onpairwise tokens. The proposed model demonstrates its performance by surpassingprevious state-of-the-art systems on the answer selection benchmark, i.e.,TREC-QA dataset, in both MAP and MRR metrics.
arxiv-17700-125 | Syntactic and semantic classification of verb arguments using dependency-based and rich semantic features | http://arxiv.org/pdf/1604.05747v1.pdf | author:Francesco Elia category:cs.CL published:2016-04-19 summary:Corpus Pattern Analysis (CPA) has been the topic of Semeval 2015 Task 15,aimed at producing a system that can aid lexicographers in their efforts tobuild a dictionary of meanings for English verbs using the CPA annotationprocess. CPA parsing is one of the subtasks which this annotation process ismade of and it is the focus of this report. A supervised machine-learningapproach has been implemented, in which syntactic features derived from parsetrees and semantic features derived from WordNet and word embeddings are used.It is shown that this approach performs well, even with the data sparsityissues that characterize the dataset, and can obtain better results than othersystem by a margin of about 4% f-score.
arxiv-17700-126 | Online Human Action Detection using Joint Classification-Regression Recurrent Neural Networks | http://arxiv.org/pdf/1604.05633v1.pdf | author:Yanghao Li, Cuiling Lan, Junliang Xing, Wenjun Zeng, Chunfeng Yuan, Jiaying Liu category:cs.CV published:2016-04-19 summary:Human action recognition from well-segmented 3D skeleton data has beenintensively studied and attracting an increasing attention. Online actiondetection goes one step further and is more challenging, which identifies theaction type and localizes the action positions on the fly from the untrimmedstream. In this paper, we study the problem of online action detection from thestreaming skeleton data. We propose a multi-task end-to-end JointClassification-Regression Recurrent Neural Network to better explore the actiontype and temporal localization information. By employing a joint classificationand regression optimization objective, this network is capable of automaticallylocalizing the start and end points of actions more accurately. Specifically,by leveraging the merits of the deep Long Short-Term Memory (LSTM) subnetwork,the proposed model automatically captures the complex long-range temporaldynamics, which naturally avoids the typical sliding window design and thusensures high computational efficiency. Furthermore, the subtask of regressionoptimization provides the ability to forecast the action prior to itsoccurrence. To evaluate our proposed model, we build a large streaming videodataset with annotations. Experimental results on our dataset and the publicG3D dataset both demonstrate very promising performance of our scheme.
arxiv-17700-127 | Right whale recognition using convolutional neural networks | http://arxiv.org/pdf/1604.05605v1.pdf | author:Andrei Polzounov, Ilmira Terpugova, Deividas Skiparis, Andrei Mihai category:cs.CV 68 published:2016-04-19 summary:We studied the feasibility of recognizing individual right whales (Eubalaenaglacialis) using convolutional neural networks. Prior studies have shown thatCNNs can be used in wide range of classification and categorization tasks suchas automated human face recognition. To test applicability of deep learning towhale recognition we have developed several models based on best practices fromliterature. Here, we describe the performance of the models. We conclude thatmachine recognition of whales is feasible and comment on the difficulty of theproblem
arxiv-17700-128 | WarpNet: Weakly Supervised Matching for Single-view Reconstruction | http://arxiv.org/pdf/1604.05592v1.pdf | author:Angjoo Kanazawa, David W. Jacobs, Manmohan Chandraker category:cs.CV published:2016-04-19 summary:We present an approach to matching images of objects in fine-grained datasetswithout using part annotations, with an application to the challenging problemof weakly supervised single-view reconstruction. This is in contrast to priorworks that require part annotations, since matching objects across class andpose variations is challenging with appearance features alone. We overcome thischallenge through a novel deep learning architecture, WarpNet, that aligns anobject in one image with a different object in another. We exploit thestructure of the fine-grained dataset to create artificial data for trainingthis network in an unsupervised-discriminative learning approach. The output ofthe network acts as a spatial prior that allows generalization at test time tomatch real images across variations in appearance, viewpoint and articulation.On the CUB-200-2011 dataset of bird categories, we improve the AP over anappearance-only network by 13.6%. We further demonstrate that our WarpNetmatches, together with the structure of fine-grained datasets, allowsingle-view reconstructions with quality comparable to using annotated pointcorrespondences.
arxiv-17700-129 | Locating a Small Cluster Privately | http://arxiv.org/pdf/1604.05590v1.pdf | author:Kobbi Nissim, Uri Stemmer, Salil Vadhan category:cs.DS cs.CR cs.LG published:2016-04-19 summary:We present a new algorithm for locating a small cluster of points withdifferential privacy [Dwork, McSherry, Nissim, and Smith, 2006]. Our algorithmhas implications to private data exploration, clustering, and removal ofoutliers. Furthermore, we use it to significantly relax the requirements of thesample and aggregate technique [Nissim, Raskhodnikova, and Smith, 2007], whichallows compiling of "off the shelf" (non-private) analyses into analyses thatpreserve differential privacy.
arxiv-17700-130 | Sketching and Neural Networks | http://arxiv.org/pdf/1604.05753v1.pdf | author:Amit Daniely, Nevena Lazic, Yoram Singer, Kunal Talwar category:cs.LG cs.AI published:2016-04-19 summary:High-dimensional sparse data present computational and statistical challengesfor supervised learning. We propose compact linear sketches for reducing thedimensionality of the input, followed by a single layer neural network. We showthat any sparse polynomial function can be computed, on nearly all sparsebinary vectors, by a single layer neural network that takes a compact sketch ofthe vector as input. Consequently, when a set of sparse binary vectors isapproximately separable using a sparse polynomial, there exists a single-layerneural network that takes a short sketch as input and correctly classifiesnearly all the points. Previous work has proposed using sketches to reducedimensionality while preserving the hypothesis class. However, the sketch sizehas an exponential dependence on the degree in the case of polynomialclassifiers. In stark contrast, our approach of using improper learning, usinga larger hypothesis class allows the sketch size to have a logarithmicdependence on the degree. Even in the linear case, our approach allows us toimprove on the pesky $O({1}/{{\gamma}^2})$ dependence of random projections, onthe margin $\gamma$. We empirically show that our approach leads to morecompact neural networks than related methods such as feature hashing at equalor better performance.
arxiv-17700-131 | Track and Transfer: Watching Videos to Simulate Strong Human Supervision for Weakly-Supervised Object Detection | http://arxiv.org/pdf/1604.05766v1.pdf | author:Krishna Kumar Singh, Fanyi Xiao, Yong Jae Lee category:cs.CV published:2016-04-19 summary:The status quo approach to training object detectors requires expensivebounding box annotations. Our framework takes a markedly different direction:we transfer tracked object boxes from weakly-labeled videos to weakly-labeledimages to automatically generate pseudo ground-truth boxes, which replacemanually annotated bounding boxes. We first mine discriminative regions in theweakly-labeled image collection that frequently/rarely appear in thepositive/negative images. We then match those regions to videos and retrievethe corresponding tracked object boxes. Finally, we design a hough transformalgorithm to vote for the best box to serve as the pseudo GT for each image,and use them to train an object detector. Together, these lead tostate-of-the-art weakly-supervised detection results on the PASCAL 2007 and2010 datasets.
arxiv-17700-132 | Using Apache Lucene to Search Vector of Locally Aggregated Descriptors | http://arxiv.org/pdf/1604.05576v1.pdf | author:Giuseppe Amato, Paolo Bolettieri, Fabrizio Falchi, Claudio Gennaro, Lucia Vadicamo category:cs.CV cs.IR published:2016-04-19 summary:Surrogate Text Representation (STR) is a profitable solution to efficientsimilarity search on metric space using conventional text search engines, suchas Apache Lucene. This technique is based on comparing the permutations of somereference objects in place of the original metric distance. However, theAchilles heel of STR approach is the need to reorder the result set of thesearch according to the metric distance. This forces to use a support databaseto store the original objects, which requires efficient random I/O on a fastsecondary memory (such as flash-based storages). In this paper, we propose toextend the Surrogate Text Representation to specifically address a class ofvisual metric objects known as Vector of Locally Aggregated Descriptors (VLAD).This approach is based on representing the individual sub-vectors forming theVLAD vector with the STR, providing a finer representation of the vector andenabling us to get rid of the reordering phase. The experiments on a publiclyavailable dataset show that the extended STR outperforms the baseline STRachieving satisfactory performance near to the one obtained with the originalVLAD vectors.
arxiv-17700-133 | An Attentive Neural Architecture for Fine-grained Entity Type Classification | http://arxiv.org/pdf/1604.05525v1.pdf | author:Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, Sebastian Riedel category:cs.CL published:2016-04-19 summary:In this work we propose a novel attention-based neural network model for thetask of fine-grained entity type classification that unlike previously proposedmodels recursively composes representations of entity mention contexts. Ourmodel achieves state-of-the-art performance with 74.94% loose micro F1-score onthe well-established FIGER dataset, a relative improvement of 2.59%. We alsoinvestigate the behavior of the attention mechanism of our model and observethat it can learn contextual linguistic expressions that indicate thefine-grained category memberships of an entity.
arxiv-17700-134 | Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss | http://arxiv.org/pdf/1604.05529v2.pdf | author:Barbara Plank, Anders SÃ¸gaard, Yoav Goldberg category:cs.CL published:2016-04-19 summary:Bidirectional long short-term memory (bi-LSTM) networks have recently provensuccessful for various NLP sequence modeling tasks, but little is known abouttheir reliance to input representations, target languages, data set size, andlabel noise. We address these issues and evaluate bi-LSTMs with word,character, and unicode byte embeddings for POS tagging. We compare bi-LSTMs totraditional POS taggers across languages and data sizes. We also present anovel bi-LSTM model, which combines the POS tagging loss function with anauxiliary loss function that accounts for rare words. The model obtainsstate-of-the-art performance across 22 languages, and works especially well formorphologically complex languages. Our analysis suggests that bi-LSTMs are lesssensitive to training data size and label corruptions (at small noise levels)than previously assumed.
arxiv-17700-135 | Exploring Segment Representations for Neural Segmentation Models | http://arxiv.org/pdf/1604.05499v1.pdf | author:Yijia Liu, Wanxiang Che, Jiang Guo, Bing Qin, Ting Liu category:cs.CL published:2016-04-19 summary:Many natural language processing (NLP) tasks can be generalized intosegmentation problem. In this paper, we combine semi-CRF with neural network tosolve NLP segmentation tasks. Our model represents a segment both by composingthe input units and embedding the entire segment. We thoroughly study differentcomposition functions and different segment embeddings. We conduct extensiveexperiments on two typical segmentation tasks: named entity recognition (NER)and Chinese word segmentation (CWS). Experimental results show that our neuralsemi-CRF model benefits from representing the entire segment and achieves thestate-of-the-art performance on CWS benchmark dataset and competitive resultson the CoNLL03 dataset.
arxiv-17700-136 | Deep Saliency with Encoded Low level Distance Map and High Level Features | http://arxiv.org/pdf/1604.05495v1.pdf | author:Gayoung Lee, Yu-Wing Tai, Junmo Kim category:cs.CV published:2016-04-19 summary:Recent advances in saliency detection have utilized deep learning to obtainhigh level features to detect salient regions in a scene. These advances havedemonstrated superior results over previous works that utilize hand-crafted lowlevel features for saliency detection. In this paper, we demonstrate thathand-crafted features can provide complementary information to enhanceperformance of saliency detection that utilizes only high level features. Ourmethod utilizes both high level and low level features for saliency detectionunder a unified deep learning framework. The high level features are extractedusing the VGG-net, and the low level features are compared with other parts ofan image to form a low level distance map. The low level distance map is thenencoded using a convolutional neural network(CNN) with multiple 1X1convolutional and ReLU layers. We concatenate the encoded low level distancemap and the high level features, and connect them to a fully connected neuralnetwork classifier to evaluate the saliency of a query region. Our experimentsshow that our method can further improve the performance of state-of-the-artdeep learning-based saliency detection methods.
arxiv-17700-137 | Understanding Rating Behaviour and Predicting Ratings by Identifying Representative Users | http://arxiv.org/pdf/1604.05468v1.pdf | author:Rahul Kamath, Masanao Ochi, Yutaka Matsuo category:cs.IR cs.AI cs.CL cs.LG published:2016-04-19 summary:Online user reviews describing various products and services are now abundanton the web. While the information conveyed through review texts and ratings iseasily comprehensible, there is a wealth of hidden information in them that isnot immediately obvious. In this study, we unlock this hidden value behind userreviews to understand the various dimensions along which users rate products.We learn a set of users that represent each of these dimensions and use theirratings to predict product ratings. Specifically, we work with restaurantreviews to identify users whose ratings are influenced by dimensions like'Service', 'Atmosphere' etc. in order to predict restaurant ratings andunderstand the variation in rating behaviour across different cuisines. Whileprevious approaches to obtaining product ratings require either a large numberof user ratings or a few review texts, we show that it is possible to predictratings with few user ratings and no review text. Our experiments show that ourapproach outperforms other conventional methods by 16-27% in terms of RMSE.
arxiv-17700-138 | An Online Structural Plasticity Rule for Generating Better Reservoirs | http://arxiv.org/pdf/1604.05459v1.pdf | author:Subhrajit Roy, Arindam Basu category:cs.NE published:2016-04-19 summary:In this article, a novel neuro-inspired low-resolution online unsupervisedlearning rule is proposed to train the reservoir or liquid of Liquid StateMachine. The liquid is a sparsely interconnected huge recurrent network ofspiking neurons. The proposed learning rule is inspired from structuralplasticity and trains the liquid through formation and elimination of synapticconnections. Hence, the learning involves rewiring of the reservoir connectionssimilar to structural plasticity observed in biological neural networks. Thenetwork connections can be stored as a connection matrix and updated in memoryby using Address Event Representation (AER) protocols which are generallyemployed in neuromorphic systems. On investigating the 'pairwise separationproperty' we find that trained liquids provide 1.36 $\pm$ 0.18 times moreinter-class separation while retaining similar intra-class separation ascompared to random liquids. Moreover, analysis of the 'linear separationproperty' reveals that trained liquids are 2.05 $\pm$ 0.27 times better thanrandom liquids. Furthermore, we show that our liquids are able to retain the'generalization' ability and 'generality' of random liquids. A memory analysisshows that trained liquids have 83.67 $\pm$ 5.79 ms longer fading memory thanrandom liquids which have shown 92.8 $\pm$ 5.03 ms fading memory for aparticular type of spike train inputs. We also throw some light on the dynamicsof the evolution of recurrent connections within the liquid. Moreover, comparedto 'Separation Driven Synaptic Modification' - a recently proposed algorithmfor iteratively refining reservoirs, our learning rule provides 9.30%, 15.21%and 12.52% more liquid separations and 2.8%, 9.1% and 7.9% betterclassification accuracies for four, eight and twelve class pattern recognitiontasks respectively.
arxiv-17700-139 | An Adaptive Learning Mechanism for Selection of Increasingly More Complex Systems | http://arxiv.org/pdf/1604.05393v1.pdf | author:Fouad Khan category:cs.IT cs.LG math.IT published:2016-04-19 summary:Recently it has been demonstrated that causal entropic forces can lead to theemergence of complex phenomena associated with human cognitive niche such astool use and social cooperation. Here I show that even more fundamental traitsassociated with human cognition such as 'self-awareness' can easily bedemonstrated to be arising out of merely a selection for 'better regulators';i.e. systems which respond comparatively better to threats to their existencewhich are internal to themselves. A simple model demonstrates how indeed theaverage self-awareness for a universe of systems continues to rise as lessself-aware systems are eliminated. The model also demonstrates however that themaximum attainable self-awareness for any system is limited by the plasticityand energy availability for that typology of systems. I argue that this rise inself-awareness may be the reason why systems tend towards greater complexity.
arxiv-17700-140 | Parts for the Whole: The DCT Norm for Extreme Visual Recovery | http://arxiv.org/pdf/1604.05451v1.pdf | author:Yunhe Wang, Chang Xu, Shan You, Dacheng Tao, Chao Xu category:cs.CV published:2016-04-19 summary:Here we study the extreme visual recovery problem, in which over 90\% ofpixel values in a given image are missing. Existing low rank-based algorithmsare only effective for recovering data with at most 90\% missing values. Thus,we exploit visual data's smoothness property to help solve this challengingextreme visual recovery problem. Based on the Discrete Cosine Transformation(DCT), we propose a novel DCT norm that involves all pixels and produces smoothestimations in any view. Our theoretical analysis shows that the totalvariation (TV) norm, which only achieves local smoothness, is a special case ofthe proposed DCT norm. We also develop a new visual recovery algorithm byminimizing the DCT and nuclear norms to achieve a more visually pleasingestimation. Experimental results on a benchmark image dataset demonstrate thatthe proposed approach is superior to state-of-the-art methods in terms of peaksignal-to-noise ratio and structural similarity.
arxiv-17700-141 | Triplet Probabilistic Embedding for Face Verification and Clustering | http://arxiv.org/pdf/1604.05417v2.pdf | author:Swami Sankaranarayanan, Azadeh Alavi, Carlos Castillo, Rama Chellappa category:cs.CV cs.LG stat.ML published:2016-04-19 summary:Despite significant progress made over the past twenty five years,unconstrained face verification remains a challenging problem. This paperproposes an approach that couples a deep CNN-based approach with alow-dimensional discriminative embedding learned using triplet probabilityconstraints to solve the unconstrained face verification problem. Aside fromyielding performance improvements, this embedding provides significantadvantages in terms of memory and for post-processing operations like subjectspecific clustering. Experiments on the challenging IJB-A dataset show that theproposed algorithm performs comparably or better than the state of the artmethods in verification and identification metrics, while requiring much lesstraining data and training time. The superior performance of the proposedmethod on the CFP dataset shows that the representation learned by our deep CNNis robust to extreme pose variation. Furthermore, we demonstrate the robustnessof the deep features to challenges including age, pose, blur and clutter byperforming simple clustering experiments on both IJB-A and LFW datasets.
arxiv-17700-142 | Streaming Label Learning for Modeling Labels on the Fly | http://arxiv.org/pdf/1604.05449v1.pdf | author:Shan You, Chang Xu, Yunhe Wang, Chao Xu, Dacheng Tao category:stat.ML cs.LG published:2016-04-19 summary:It is challenging to handle a large volume of labels in multi-label learning.However, existing approaches explicitly or implicitly assume that all thelabels in the learning process are given, which could be easily violated inchanging environments. In this paper, we define and study streaming labellearning (SLL), i.e., labels are arrived on the fly, to model newly arrivedlabels with the help of the knowledge learned from past labels. The core of SLLis to explore and exploit the relationships between new labels and past labelsand then inherit the relationship into hypotheses of labels to boost theperformance of new classifiers. In specific, we use the labelself-representation to model the label relationship, and SLL will be dividedinto two steps: a regression problem and a empirical risk minimization (ERM)problem. Both problems are simple and can be efficiently solved. We furthershow that SLL can generate a tighter generalization error bound for new labelsthan the general ERM framework with trace norm or Frobenius normregularization. Finally, we implement extensive experiments on variousbenchmark datasets to validate the new setting. And results show that SLL caneffectively handle the constantly emerging new labels and provides excellentclassification performance.
arxiv-17700-143 | Cognitive state classification using transformed fMRI data | http://arxiv.org/pdf/1604.05413v1.pdf | author:Hariharan Ramasangu, Neelam Sinha category:cs.CV published:2016-04-19 summary:One approach, for understanding human brain functioning, is to analyze thechanges in the brain while performing cognitive tasks. Towards this, FunctionalMagnetic Resonance (fMR) images of subjects performing well-defined tasks arewidely utilized for task-specific analyses. In this work, we propose aprocedure to enable classification between two chosen cognitive tasks, usingtheir respective fMR image sequences. The time series of expert-markedanatomically-mapped relevant voxels are processed and fed as input to theclassical Naive Bayesian and SVM classifiers. The processing involves use ofrandom sieve function, phase information in the data transformed using Fourierand Hilbert transformations. This processing results in improvedclassification, as against using the voxel intensities directly, asillustrated. The novelty of the proposed method lies in utilizing the phaseinformation in the transformed domain, for classifying between the cognitivetasks along with random sieve function chosen with a particular probabilitydistribution. The proposed classification procedure is applied on a publiclyavailable dataset, StarPlus data, with 6 subjects performing the two distinctcognitive tasks of watching either a picture or a sentence. The classificationaccuracy stands at an average of 65.6%(using Naive Bayes classifier) and76.4%(using SVM classifier) for raw data. The corresponding classificationaccuracy stands at 96.8% and 97.5% for Fourier transformed data. For Hilberttransformed data, it is 93.7% and 99%, for 6 subjects, on 2 cognitive tasks.
arxiv-17700-144 | Comparative Study of Instance Based Learning and Back Propagation for Classification Problems | http://arxiv.org/pdf/1604.05429v1.pdf | author:Nadia Kanwal, Erkan Bostanci category:cs.LG published:2016-04-19 summary:The paper presents a comparative study of the performance of Back Propagationand Instance Based Learning Algorithm for classification tasks. The study iscarried out by a series of experiments will all possible combinations ofparameter values for the algorithms under evaluation. The algorithm'sclassification accuracy is compared over a range of datasets and measurementslike Cross Validation, Kappa Statistics, Root Mean Squared Value and TruePositive vs False Positive rate have been used to evaluate their performance.Along with performance comparison, techniques of handling missing values havealso been compared that include Mean or Mode replacement and MultipleImputation. The results showed that parameter adjustment plays vital role inimproving an algorithm's accuracy and therefore, Back Propagation has shownbetter results as compared to Instance Based Learning. Furthermore, the problemof missing values was better handled by Multiple imputation method, however,not suitable for less amount of data.
arxiv-17700-145 | Improving Raw Image Storage Efficiency by Exploiting Similarity | http://arxiv.org/pdf/1604.05442v1.pdf | author:Binqi Zhang, Chen Wang, Bing Bing Zhou, Albert Y. Zomaya category:cs.DC cs.CV published:2016-04-19 summary:To improve the temporal and spatial storage efficiency, researchers haveintensively studied various techniques, including compression anddeduplication. Through our evaluation, we find that methods such as photo tagsor local features help to identify the content-based similar- ity between rawimages. The images can then be com- pressed more efficiently to get betterstorage space sav- ings. Furthermore, storing similar raw images togetherenables rapid data sorting, searching and retrieval if the images are stored ina distributed and large-scale envi- ronment by reducing fragmentation. In thispaper, we evaluated the compressibility by designing experiments and observingthe results. We found that on a statistical basis the higher similarity photoshave, the better com- pression results are. This research helps provide a cluefor future large-scale storage system design.
arxiv-17700-146 | End-to-End Tracking and Semantic Segmentation Using Recurrent Neural Networks | http://arxiv.org/pdf/1604.05091v2.pdf | author:Peter Ondruska, Julie Dequaire, Dominic Zeng Wang, Ingmar Posner category:cs.LG cs.AI cs.CV cs.NE cs.RO published:2016-04-18 summary:In this work we present a novel end-to-end framework for tracking andclassifying a robot's surroundings in complex, dynamic and only partiallyobservable real-world environments. The approach deploys a recurrent neuralnetwork to filter an input stream of raw laser measurements in order todirectly infer object locations, along with their identity in both visible andoccluded areas. To achieve this we first train the network using unsupervisedDeep Tracking, a recently proposed theoretical framework for end-to-end spaceoccupancy prediction. We show that by learning to track on a large amount ofunsupervised data, the network creates a rich internal representation of itsenvironment which we in turn exploit through the principle of inductivetransfer of knowledge to perform the task of it's semantic classification. As aresult, we show that only a small amount of labelled data suffices to steer thenetwork towards mastering this additional task. Furthermore we propose a novelrecurrent neural network architecture specifically tailored to tracking andsemantic classification in real-world robotics applications. We demonstrate thetracking and classification performance of the method on real-world datacollected at a busy road junction. Our evaluation shows that the proposedend-to-end framework compares favourably to a state-of-the-art, model-freetracking solution and that it outperforms a conventional one-shot trainingscheme for semantic classification.
arxiv-17700-147 | Visual Aesthetic Quality Assessment with Multi-task Deep Learning | http://arxiv.org/pdf/1604.04970v1.pdf | author:Yueying Kao, Ran He, Kaiqi Huang category:cs.CV cs.LG cs.NE published:2016-04-18 summary:This paper considers the problem of assessing visual aesthetic quality withsemantic information. We cast the assessment problem as the main task among amulti-task deep model, and argue that semantic recognition offers the key toaddressing this problem. Based on convolutional neural networks, we propose ageneral multi-task framework with four different structures. In each structure,aesthetic quality assessment task and semantic recognition task are leveraged,and different features are explored to improve the quality assessment.Moreover, an effective strategy of keeping a balanced effect between thesemantic task and aesthetic task is developed to optimize the parameters of ourframework. The correlation analysis among the tasks validates the importance ofthe semantic recognition in aesthetic quality assessment. Extensive experimentsverify the effectiveness of the proposed multi-task framework, and furthercorroborate the above proposition.
arxiv-17700-148 | RGB-D Scene Labeling with Long Short-Term Memorized Fusion Model | http://arxiv.org/pdf/1604.05000v2.pdf | author:Zhen Li, Yukang Gan, Xiaodan Liang, Yizhou Yu, Hui Cheng, Liang Lin category:cs.CV cs.NE published:2016-04-18 summary:Semantic labeling of RGB-D scenes is crucial to many intelligent applicationsincluding perceptual robotics. It generates pixelwise and fine-grained labelmaps from simultaneously sensed photometric (RGB) and depth channels. Thispaper addresses this problem by i) developing a novel Long Short-Term MemorizedFusion (LSTM-F) Model that captures and fuses contextual information frommultiple channels of photometric and depth data, and ii) incorporating thismodel into deep convolutional neural networks (CNNs) for end-to-end training.Specifically, global contexts in photometric and depth channels are,respectively, captured by stacking several convolutional layers and a longshort-term memory layer; the memory layer encodes both short-range andlong-range spatial dependencies in an image along the vertical direction.Another long short-term memorized fusion layer is set up to integrate thecontexts along the vertical direction from different channels, and performbi-directional propagation of the fused vertical contexts along the horizontaldirection to obtain true 2D global contexts. At last, the fused contextualrepresentation is concatenated with the convolutional features extracted fromthe photometric channels in order to improve the accuracy of fine-scalesemantic labeling. Our proposed model has set a new state of the art, i.e.,48.1% average class accuracy over 37 categories 11.8% improvement), on thelarge-scale SUNRGBD dataset.1
arxiv-17700-149 | Gaussian Copula Variational Autoencoders for Mixed Data | http://arxiv.org/pdf/1604.04960v1.pdf | author:Suwon Suh, Seungjin Choi category:stat.ML cs.LG published:2016-04-18 summary:The variational autoencoder (VAE) is a generative model with continuouslatent variables where a pair of probabilistic encoder (bottom-up) and decoder(top-down) is jointly learned by stochastic gradient variational Bayes. Wefirst elaborate Gaussian VAE, approximating the local covariance matrix of thedecoder as an outer product of the principal direction at a position determinedby a sample drawn from Gaussian distribution. We show that this model, referredto as VAE-ROC, better captures the data manifold, compared to the standardGaussian VAE where independent multivariate Gaussian was used to model thedecoder. Then we extend the VAE-ROC to handle mixed categorical and continuousdata. To this end, we employ Gaussian copula to model the local dependency inmixed categorical and continuous data, leading to {\em Gaussian copulavariational autoencoder} (GCVAE). As in VAE-ROC, we use the rank-oneapproximation for the covariance in the Gaussian copula, to capture the localdependency structure in the mixed data. Experiments on various datasetsdemonstrate the useful behaviour of VAE-ROC and GCVAE, compared to the standardVAE.
arxiv-17700-150 | Selective Convolutional Descriptor Aggregation for Fine-Grained Image Retrieval | http://arxiv.org/pdf/1604.04994v1.pdf | author:Xiu-Shen Wei, Jian-Hao Luo, Jianxin Wu category:cs.CV published:2016-04-18 summary:Deep convolutional models pre-trained for the ImageNet classification taskhave been successfully adopted to tasks in other domains, such as texturedescription and object proposal generation, but these tasks require annotationsfor images in the new domain. In this paper, we focus on a novel andchallenging task in the pure unsupervised setting: fine-grained imageretrieval. Even with image labels, fine-grained images are difficult toclassify, let alone the unsupervised retrieval task. We propose the SelectiveConvolutional Descriptor Aggregation (SCDA) method. SCDA firstly localizes themain object(s) in fine-grained images, a step that discards noisy backgroundand keeps useful deep descriptors. The selected descriptors are then aggregatedand dimensionality reduced into a short feature vector using the best practiceswe found. SCDA is unsupervised, using no image label or bounding boxannotation. Experiments on four fine-grained datasets confirm the effectivenessof SCDA. Visualization of the SCDA features shows that they correspond tovisual attributes (even subtle ones), which might explain SCDA's high accuracyin fine-grained retrieval.
arxiv-17700-151 | Can Boosting with SVM as Week Learners Help? | http://arxiv.org/pdf/1604.05242v2.pdf | author:Dinesh Govindaraj category:cs.CV cs.LG published:2016-04-18 summary:Object recognition in images involves identifying objects with partialocclusions, viewpoint changes, varying illumination, cluttered backgrounds.Recent work in object recognition uses machine learning techniques SVM-KNN,Local Ensemble Kernel Learning, Multiple Kernel Learning. In this paper, wewant to utilize SVM as week learners in AdaBoost. Experiments are done withclassifiers like near- est neighbor, k-nearest neighbor, Support vectormachines, Local learning(SVM- KNN) and AdaBoost. Models use Scale-Invariantdescriptors and Pyramid his- togram of gradient descriptors. AdaBoost istrained with set of week classifier as SVMs, each with kernel distance functionon different descriptors. Results shows AdaBoost with SVM outperform othermethods for Object Categorization dataset.
arxiv-17700-152 | Learning Dense Correspondence via 3D-guided Cycle Consistency | http://arxiv.org/pdf/1604.05383v1.pdf | author:Tinghui Zhou, Philipp KrÃ¤henbÃ¼hl, Mathieu Aubry, Qixing Huang, Alexei A. Efros category:cs.CV published:2016-04-18 summary:Discriminative deep learning approaches have shown impressive results forproblems where human-labeled ground truth is plentiful, but what about taskswhere labels are difficult or impossible to obtain? This paper tackles one suchproblem: establishing dense visual correspondence across different objectinstances. For this task, although we do not know what the ground-truth is, weknow it should be consistent across instances of that category. We exploit thisconsistency as a supervisory signal to train a convolutional neural network topredict cross-instance correspondences between pairs of images depictingobjects of the same category. For each pair of training images we find anappropriate 3D CAD model and render two synthetic views to link in with thepair, establishing a correspondence flow 4-cycle. We use ground-truthsynthetic-to-synthetic correspondences, provided by the rendering engine, totrain a ConvNet to predict synthetic-to-real, real-to-real andreal-to-synthetic correspondences that are cycle-consistent with theground-truth. At test time, no CAD models are required. We demonstrate that ourend-to-end trained ConvNet supervised by cycle-consistency outperformsstate-of-the-art pairwise matching methods in correspondence-related tasks.
arxiv-17700-153 | Churn analysis using deep convolutional neural networks and autoencoders | http://arxiv.org/pdf/1604.05377v1.pdf | author:Artit Wangperawong, Cyrille Brun, Olav Laudy, Rujikorn Pavasuthipaisit category:stat.ML cs.LG cs.NE published:2016-04-18 summary:Customer temporal behavioral data was represented as images in order toperform churn prediction by leveraging deep learning architectures prominent inimage classification. Supervised learning was performed on labeled data of over6 million customers using deep convolutional neural networks, which achieved anAUC of 0.743 on the test dataset using no more than 12 temporal features foreach customer. Unsupervised learning was conducted using autoencoders to betterunderstand the reasons for customer churn. Images that maximally activate thehidden units of an autoencoder trained with churned customers reveal ampleopportunities for action to be taken to prevent churn among strong data, novoice users.
arxiv-17700-154 | Clustering Comparable Corpora of Russian and Ukrainian Academic Texts: Word Embeddings and Semantic Fingerprints | http://arxiv.org/pdf/1604.05372v1.pdf | author:Andrey Kutuzov, Mikhail Kopotev, Tatyana Sviridenko, Lyubov Ivanova category:cs.CL published:2016-04-18 summary:We present our experience in applying distributional semantics (neural wordembeddings) to the problem of representing and clustering documents in abilingual comparable corpus. Our data is a collection of Russian and Ukrainianacademic texts, for which topics are their academic fields. In order to buildlanguage-independent semantic representations of these documents, we trainneural distributional models on monolingual corpora and learn the optimallinear transformation of vectors from one language to another. The resultingvectors are then used to produce `semantic fingerprints' of documents, servingas input to a clustering algorithm. The presented method is compared to several baselines including `orthographictranslation' with Levenshtein edit distance and outperforms them by a largemargin. We also show that language-independent `semantic fingerprints' aresuperior to multi-lingual clustering algorithms proposed in the previous work,at the same time requiring less linguistic resources.
arxiv-17700-155 | Uniform Coherence | http://arxiv.org/pdf/1604.05288v1.pdf | author:Scott Garrabrant, Benya Fallenstein, Abram Demski, Nate Soares category:cs.AI cs.LG math.PR published:2016-04-18 summary:While probability theory is normally applied to external environments, therehas been some recent interest in probabilistic modeling of the outputs ofcomputations that are too expensive to run. Since mathematical logic is apowerful tool for reasoning about computer programs, we consider this problemfrom the perspective of integrating probability and logic. Recent work onassigning probabilities to mathematical statements has used the concept ofcoherent distributions, which satisfy logical constraints such as theprobability of a sentence and its negation summing to one. Although there arealgorithms which converge to a coherent probability distribution in the limit,this yields only weak guarantees about finite approximations of thesedistributions. In our setting, this is a significant limitation: Coherentdistributions assign probability one to all statements provable in a specificlogical theory, such as Peano Arithmetic, which can prove what the output ofany terminating computation is; thus, a coherent distribution must assignprobability one to the output of any terminating computation. To modeluncertainty about computations, we propose to work with approximations tocoherent distributions. We introduce uniform coherence, a strengthening ofcoherence that provides appropriate constraints on finite approximations, andpropose an algorithm which satisfies this criterion.
arxiv-17700-156 | Annotation Order Matters: Recurrent Image Annotator for Arbitrary Length Image Tagging | http://arxiv.org/pdf/1604.05225v2.pdf | author:Jiren Jin, Hideki Nakayama category:cs.CV published:2016-04-18 summary:Automatic image annotation has been an important research topic infacilitating large scale image management and retrieval. Existing methods focuson learning image-tag correlation or correlation between tags to improveannotation accuracy. However, most of these methods evaluate their performanceusing top-k retrieval performance, where k is fixed. Although such settinggives convenience for comparing different methods, it is not the natural waythat humans annotate images. The number of annotated tags should depend onimage contents. Inspired by the recent progress in machine translation andimage captioning, we propose a novel Recurrent Image Annotator (RIA) model thatforms image annotation task as a sequence generation problem so that RIA cannatively predict the proper length of tags according to image contents. Weevaluate the proposed model on various image annotation datasets. In additionto comparing our model with existing methods using the conventional top-kevaluation measures, we also provide our model as a high quality baseline forthe arbitrary length image tagging task. Moreover, the results of ourexperiments show that the order of tags in training phase has a great impact onthe final annotation performance.
arxiv-17700-157 | Asymptotic Convergence in Online Learning with Unbounded Delays | http://arxiv.org/pdf/1604.05280v1.pdf | author:Scott Garrabrant, Nate Soares, Jessica Taylor category:cs.LG cs.AI math.PR published:2016-04-18 summary:We study the problem of predicting the results of computations that are tooexpensive to run, via the observation of the results of smaller computations.We model this as an online learning problem with delayed feedback, where thelength of the delay is unbounded, which we study mainly in a stochasticsetting. We show that in this setting, consistency is not possible in general,and that optimal forecasters might not have average regret going to zero.However, it is still possible to give algorithms that converge asymptoticallyto Bayes-optimal predictions, by evaluating forecasters on specific sparseindependent subsequences of their predictions. We give an algorithm that doesthis, which converges asymptotically on good behavior, and give very weakbounds on how long it takes to converge. We then relate our results back to theproblem of predicting large computations in a deterministic setting.
arxiv-17700-158 | Efficient Calculation of Bigram Frequencies in a Corpus of Short Texts | http://arxiv.org/pdf/1604.05559v1.pdf | author:Melvyn Drag, Gauthaman Vasudevan category:cs.CL published:2016-04-18 summary:We show that an efficient and popular method for calculating bigramfrequencies is unsuitable for bodies of short texts and offer a simplealternative. Our method has the same computational complexity as the old methodand offers an exact count instead of an approximation.
arxiv-17700-159 | Forecasting Volatility in Indian Stock Market using Artificial Neural Network with Multiple Inputs and Outputs | http://arxiv.org/pdf/1604.05008v1.pdf | author:Tamal Datta Chaudhuri, Indranil Ghosh category:cs.NE published:2016-04-18 summary:Volatility in stock markets has been extensively studied in the appliedfinance literature. In this paper, Artificial Neural Network models based onvarious back propagation algorithms have been constructed to predict volatilityin the Indian stock market through volatility of NIFTY returns and volatilityof gold returns. This model considers India VIX, CBOE VIX, volatility of crudeoil returns (CRUDESDR), volatility of DJIA returns (DJIASDR), volatility of DAXreturns (DAXSDR), volatility of Hang Seng returns (HANGSDR) and volatility ofNikkei returns (NIKKEISDR) as predictor variables. Three sets of experimentshave been performed over three time periods to judge the effectiveness of theapproach.
arxiv-17700-160 | Chained Gaussian Processes | http://arxiv.org/pdf/1604.05263v1.pdf | author:Alan D. Saul, James Hensman, Aki Vehtari, Neil D. Lawrence category:stat.ML cs.LG published:2016-04-18 summary:Gaussian process models are flexible, Bayesian non-parametric approaches toregression. Properties of multivariate Gaussians mean that they can be combinedlinearly in the manner of additive models and via a link function (like ingeneralized linear models) to handle non-Gaussian data. However, the linkfunction formalism is restrictive, link functions are always invertible andmust convert a parameter of interest to a linear combination of the underlyingprocesses. There are many likelihoods and models where a non-linear combinationis more appropriate. We term these more general models Chained GaussianProcesses: the transformation of the GPs to the likelihood parameters will notgenerally be invertible, and that implies that linearisation would only bepossible with multiple (localized) links, i.e. a chain. We develop anapproximate inference procedure for Chained GPs that is scalable and applicableto any factorized likelihood. We demonstrate the approximation on a range oflikelihood functions.
arxiv-17700-161 | Risk-Averse Multi-Armed Bandit Problems under Mean-Variance Measure | http://arxiv.org/pdf/1604.05257v1.pdf | author:Sattar Vakili, Qing Zhao category:cs.LG published:2016-04-18 summary:The multi-armed bandit problems have been studied mainly under the measure ofexpected total reward accrued over a horizon of length $T$. In this paper, weaddress the issue of risk in multi-armed bandit problems and develop parallelresults under the measure of mean-variance, a commonly adopted risk measure ineconomics and mathematical finance. We show that the model-specific regret andthe model-independent regret in terms of the mean-variance of the rewardprocess are lower bounded by $\Omega(\log T)$ and $\Omega(T^{2/3})$,respectively. We then show that variations of the UCB policy and the DSEEpolicy developed for the classic risk-neutral MAB achieve these lower bounds.
arxiv-17700-162 | Kernel Distribution Embeddings: Universal Kernels, Characteristic Kernels and Kernel Metrics on Distributions | http://arxiv.org/pdf/1604.05251v1.pdf | author:Carl-Johann Simon-Gabriel, Bernhard SchÃ¶lkopf category:stat.ML math.FA math.PR G.3 published:2016-04-18 summary:Kernel mean embeddings have recently attracted the attention of the machinelearning community. They map measures $\mu$ from some set $M$ to functions in areproducing kernel Hilbert space (RKHS) with kernel $k$. The RKHS distance oftwo mapped measures is a semi-metric $d_k$ over $M$. We study three questions.(I) For a given kernel, what sets $M$ can be embedded? (II) When is theembedding injective over $M$ (in which case $d_k$ is a metric)? (III) How doesthe $d_k$-induced topology compare to other topologies on $M$? The existingmachine learning literature has addressed these questions in cases where $M$ is(a subset of) the finite regular Borel measures. We unify, improve andgeneralise those results. Our approach naturally leads to continuous andpossibly even injective embeddings of (Schwartz-) distributions, i.e.,generalised measures, but the reader is free to focus on measures only. Inparticular, we systemise and extend various (partly known) equivalences betweendifferent notions of universal, characteristic and strictly positive definitekernels, and show that on an underlying locally compact Hausdorff space, $d_k$metrises the weak convergence of probability measures if and only if $k$ iscontinuous and characteristic.
arxiv-17700-163 | Finding Common Characteristics Among NBA Playoff and Championship Teams: A Machine Learning Approach | http://arxiv.org/pdf/1604.05266v3.pdf | author:Ikjyot Singh Kohli category:stat.ML stat.AP published:2016-04-18 summary:In this paper, we employ machine learning techniques to analyze sixteenseasons of NBA regular season data from every team to determine the commoncharacteristics among NBA playoff teams. Each team was characterized by 42predictor variables and one binary response variable taking on a value of"TRUE" if a team had made the playoffs, and value of "FALSE" if a team hadmissed the playoffs. After fitting an initial classification tree to thisproblem, this tree was then pruned which decreased the test error rate. Furtherto this, a random forest of classification trees was grown which provided avery accurate model from which a variable importance plot was generated todetermine which predictor variables had the greatest influence on the responsevariable. The result of this work was the conclusion that the most importantfactors in characterizing a team's playoff eligibility are the opponent fieldgoal percentage and the opponent points per game. This seems to suggest that\emph{defensive} factors as opposed to offensive factors are the most importantcharacteristics shared among NBA playoff teams. We also perform aclassification analysis to determine common characteristics among NBAchampionship teams. Using an artificial neural network structure, we show thatchampionship teams must be able to have very strong defensive characteristics,in particular, strong perimeter defense characteristics in combination with aneffective half-court offense that generates high-percentage two-point shots. Akey part of this offensive strategy must also be the ability to draw fouls.This analysis will hopefully dispel the rising notion that an offense gearedtowards shooting many three point shots is a sufficient and necessary conditionfor an NBA team to be successful in qualifying for the playoffs and winning achampionship.
arxiv-17700-164 | Learning Sparse Additive Models with Interactions in High Dimensions | http://arxiv.org/pdf/1604.05307v1.pdf | author:Hemant Tyagi, Anastasios Kyrillidis, Bernd GÃ¤rtner, Andreas Krause category:cs.LG cs.IT math.IT stat.ML published:2016-04-18 summary:A function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is referred to as aSparse Additive Model (SPAM), if it is of the form $f(\mathbf{x}) = \sum_{l \in\mathcal{S}}\phi_{l}(x_l)$, where $\mathcal{S} \subset [d]$, $\mathcal{S} \lld$. Assuming $\phi_l$'s and $\mathcal{S}$ to be unknown, the problem ofestimating $f$ from its samples has been studied extensively. In this work, weconsider a generalized SPAM, allowing for second order interaction terms. Forsome $\mathcal{S}_1 \subset [d], \mathcal{S}_2 \subset {[d] \choose 2}$, thefunction $f$ is assumed to be of the form: $$f(\mathbf{x}) = \sum_{p \in\mathcal{S}_1}\phi_{p} (x_p) + \sum_{(l,l^{\prime}) \in\mathcal{S}_2}\phi_{(l,l^{\prime})} (x_{l},x_{l^{\prime}}).$$ Assuming$\phi_{p},\phi_{(l,l^{\prime})}$, $\mathcal{S}_1$ and, $\mathcal{S}_2$ to beunknown, we provide a randomized algorithm that queries $f$ and exactlyrecovers $\mathcal{S}_1,\mathcal{S}_2$. Consequently, this also enables us toestimate the underlying $\phi_p, \phi_{(l,l^{\prime})}$. We derive samplecomplexity bounds for our scheme and also extend our analysis to include thesituation where the queries are corrupted with noise -- either stochastic, orarbitrary but bounded. Lastly, we provide simulation results on synthetic data,that validate our theoretical findings.
arxiv-17700-165 | Pieces-of-parts for supervoxel segmentation with global context: Application to DCE-MRI tumour delineation | http://arxiv.org/pdf/1604.05210v1.pdf | author:Benjamin Irving, James M Franklin, Bartlomiej W Papiez, Ewan M Anderson, Ricky A Sharma, Fergus V Gleeson, Sir Michael Brady, Julia A Schnabel category:cs.CV published:2016-04-18 summary:Rectal tumour segmentation in dynamic contrast-enhanced MRI (DCE-MRI) is achallenging task, and an automated and consistent method would be highlydesirable to improve the modelling and prediction of patient outcomes fromtissue contrast enhancement characteristics - particularly in routine clinicalpractice. A framework is developed to automate DCE-MRI tumour segmentation, byintroducing: perfusion-supervoxels to over-segment and classify DCE-MRI volumesusing the dynamic contrast enhancement characteristics; and the pieces-of-partsgraphical model, which adds global (anatomic) constraints that further refinethe supervoxel components that comprise the tumour. The framework was evaluatedon 23 DCE-MRI scans of patients with rectal adenocarcinomas, and achieved avoxelwise area-under the receiver operating characteristic curve (AUC) of 0.97compared to expert delineations. Creating a binary tumour segmentation, 21 ofthe 23 cases were segmented correctly with a median Dice similarity coefficient(DSC) of 0.63, which is close to the inter-rater variability of thischallenging task. A sec- ond study is also included to demonstrate the method'sgeneralisability and achieved a DSC of 0.71. The framework achieves promisingresults for the underexplored area of rectal tumour segmentation in DCE-MRI,and the methods have potential to be applied to other DCE-MRI and supervoxelsegmentation problems
arxiv-17700-166 | Empirical study of PROXTONE and PROXTONE$^+$ for Fast Learning of Large Scale Sparse Models | http://arxiv.org/pdf/1604.05024v1.pdf | author:Ziqiang Shi, Rujie Liu category:cs.LG cs.AI published:2016-04-18 summary:PROXTONE is a novel and fast method for optimization of large scalenon-smooth convex problem \cite{shi2015large}. In this work, we try to usePROXTONE method in solving large scale \emph{non-smooth non-convex} problems,for example training of sparse deep neural network (sparse DNN) or sparseconvolutional neural network (sparse CNN) for embedded or mobile device.PROXTONE converges much faster than first order methods, while first ordermethod is easy in deriving and controlling the sparseness of the solutions.Thus in some applications, in order to train sparse models fast, we propose tocombine the merits of both methods, that is we use PROXTONE in the firstseveral epochs to reach the neighborhood of an optimal solution, and then usethe first order method to explore the possibility of sparsity in the followingtraining. We call such method PROXTONE plus (PROXTONE$^+$). Both PROXTONE andPROXTONE$^+$ are tested in our experiments, and which demonstrate both methodsimproved convergence speed twice as fast at least on diverse sparse modellearning problems, and at the same time reduce the size to 0.5\% for DNNmodels. The source of all the algorithms is available upon request.
arxiv-17700-167 | Locally Imposing Function for Generalized Constraint Neural Networks - A Study on Equality Constraints | http://arxiv.org/pdf/1604.05198v1.pdf | author:Linlin Cao, Ran He, Bao-Gang Hu category:cs.NE cs.LG stat.ML published:2016-04-18 summary:This work is a further study on the Generalized Constraint Neural Network(GCNN) model [1], [2]. Two challenges are encountered in the study, that is, toembed any type of prior information and to select its imposing schemes. Thework focuses on the second challenge and studies a new constraint imposingscheme for equality constraints. A new method called locally imposing function(LIF) is proposed to provide a local correction to the GCNN predictionfunction, which therefore falls within Locally Imposing Scheme (LIS). Incomparison, the conventional Lagrange multiplier method is considered asGlobally Imposing Scheme (GIS) because its added constraint term exhibits aglobal impact to its objective function. Two advantages are gained from LISover GIS. First, LIS enables constraints to fire locally and explicitly in thedomain only where they need on the prediction function. Second, constraints canbe implemented within a network setting directly. We attempt to interpretseveral constraint methods graphically from a viewpoint of the localityprinciple. Numerical examples confirm the advantages of the proposed method. Insolving boundary value problems with Dirichlet and Neumann constraints, theGCNN model with LIF is possible to achieve an exact satisfaction of theconstraints.
arxiv-17700-168 | ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation | http://arxiv.org/pdf/1604.05144v1.pdf | author:Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, Jian Sun category:cs.CV published:2016-04-18 summary:Large-scale data is of crucial importance for learning semantic segmentationmodels, but annotating per-pixel masks is a tedious and inefficient procedure.We note that for the topic of interactive image segmentation, scribbles arevery widely used in academic research and commercial software, and arerecognized as one of the most user-friendly ways of interacting. In this paper,we propose to use scribbles to annotate images, and develop an algorithm totrain convolutional networks for semantic segmentation supervised by scribbles.Our algorithm is based on a graphical model that jointly propagates informationfrom scribbles to unmarked pixels and learns network parameters. We presentcompetitive object semantic segmentation results on the PASCAL VOC dataset byusing scribbles as annotations. Scribbles are also favored for annotating stuff(e.g., water, sky, grass) that has no well-defined shape, and our method showsexcellent results on the PASCAL-CONTEXT dataset thanks to extra inexpensivescribble annotations. Our scribble annotations on PASCAL VOC are available athttp://research.microsoft.com/en-us/um/people/jifdai/downloads/scribble_sup
arxiv-17700-169 | Using Self-Contradiction to Learn Confidence Measures in Stereo Vision | http://arxiv.org/pdf/1604.05132v1.pdf | author:Christian Mostegel, Markus Rumpler, Friedrich Fraundorfer, Horst Bischof category:cs.CV published:2016-04-18 summary:Learned confidence measures gain increasing importance for outlier removaland quality improvement in stereo vision. However, acquiring the necessarytraining data is typically a tedious and time consuming task that involvesmanual interaction, active sensing devices and/or synthetic scenes. To overcomethis problem, we propose a new, flexible, and scalable way for generatingtraining data that only requires a set of stereo images as input. The key ideaof our approach is to use different view points for reasoning aboutcontradictions and consistencies between multiple depth maps generated with thesame stereo algorithm. This enables us to generate a huge amount of trainingdata in a fully automated manner. Among other experiments, we demonstrate thepotential of our approach by boosting the performance of three learnedconfidence measures on the KITTI2012 dataset by simply training them on a vastamount of automatically generated training data rather than a limited amount oflaser ground truth data.
arxiv-17700-170 | Memory controls time perception and intertemporal choices | http://arxiv.org/pdf/1604.05129v1.pdf | author:Pedro A. Ortega, Naftali Tishby category:q-bio.NC cs.AI stat.ML published:2016-04-18 summary:There is a remarkable consensus that human and non-human subjects experiencetemporal distortions in many stages of their perceptual and decision-makingsystems. Similarly, intertemporal choice research has shown thatdecision-makers undervalue future outcomes relative to immediate ones. Here wecombine techniques from information theory and artificial intelligence to showhow both temporal distortions and intertemporal choice preferences can beexplained as a consequence of the coding efficiency of sensorimotorrepresentation. In particular, the model implies that interactions thatconstrain future behavior are perceived as being both longer in duration andmore valuable. Furthermore, using simulations of artificial agents, weinvestigate how memory constraints enforce a renormalization of the perceivedtimescales. Our results show that qualitatively different discount functions,such as exponential and hyperbolic discounting, arise as a consequence of anagent's probabilistic model of the world.
arxiv-17700-171 | Pixel-level Encoding and Depth Layering for Instance-level Semantic Labeling | http://arxiv.org/pdf/1604.05096v1.pdf | author:Jonas Uhrig, Marius Cordts, Uwe Franke, Thomas Brox category:cs.CV published:2016-04-18 summary:Recent approaches for instance-aware semantic labeling have augmentedconvolutional neural networks (CNNs) with complex multi-task architectures orcomputationally expensive graphical models. We present a method that leveragesa fully convolutional network (FCN) to predict semantic labels, depth and aninstance-based encoding using each pixel's direction towards its correspondinginstance center. Subsequently, we apply low-level computer vision techniques togenerate state-of-the-art instance segmentation on the street scene datasetsKITTI and Cityscapes. Our approach outperforms existing works by a large marginand can additionally predict absolute distances of individual instances from amonocular image as well as a pixel-level semantic labeling.
arxiv-17700-172 | Mastering $2048$ with Delayed Temporal Coherence Learning, Multi-State Weight Promotion, Redundant Encoding and Carousel Shaping | http://arxiv.org/pdf/1604.05085v1.pdf | author:Wojciech JaÅkowski category:cs.AI cs.LG I.2.6; I.2.8 published:2016-04-18 summary:$2048$ is an engaging single-player, nondeterministic video puzzle game,which, thanks to the simple rules and hard-to-master gameplay, has gainedmassive popularity in recent years. As $2048$ can be conveniently embedded intothe discrete-state Markov decision processes framework, we treat it as atestbed for evaluating existing and new methods in reinforcement learning. Withthe aim to develop a strong $2048$ playing program, we employ temporaldifference learning with systematic n-tuple networks. We show that this basicmethod can be significantly improved with temporal coherence learning,multi-stage function approximator with weight promotion, carousel shaping, andredundant encoding. In addition, we demonstrate how to take advantage of thecharacteristics of the n-tuple network, to improve the algorithmiceffectiveness of the learning process by i) delaying the (decayed) update andapplying lock-free optimistic parallelism to effortlessly make advantage ofmultiple CPU cores. This way, we were able to develop the best known $2048$playing program to date, which confirms the effectiveness of the introducedmethods for discrete-state Markov decision problems.
arxiv-17700-173 | Demonstrating Hybrid Learning in a Flexible Neuromorphic Hardware System | http://arxiv.org/pdf/1604.05080v1.pdf | author:Simon Friedmann, Johannes Schemmel, Andreas Gruebl, Andreas Hartel, Matthias Hock, Karlheinz Meier category:q-bio.NC cs.NE published:2016-04-18 summary:We present results from a new approach to learning and plasticity inneuromorphic hardware systems: to enable flexibility in implementable learningmechanisms while keeping high efficiency associated with neuromorphicimplementations, we combine a general-purpose processor with full-custom analogelements. This processor is operating in parallel with a fully parallel neuromorphicsystem consisting of an array of synapses connected to analog, continuous timeneuron circuits. Novel analog correlation sensor circuits process spike eventsfor each synapse in parallel and in real-time. The processor uses this pre-processing to compute new weights possibly usingadditional information following its program. Therefore, learning rules can be defined in software giving a large degree offlexibility. Synapses realize correlation detection geared towards Spike-Timing DependentPlasticity (STDP) as central computational primitive in the analog domain. Operating at a speed-up factor of 1000 compared to biological time-scale, wemeasure time-constants from tens to hundreds of micro-seconds. We analyze variability across multiple chips and demonstrate learning using amultiplicative STDP rule. We conclude, that the presented approach will enable flexible and efficientlearning as a platform for neuroscientific research and technologicalapplications.
arxiv-17700-174 | Fully Convolutional Recurrent Network for Handwritten Chinese Text Recognition | http://arxiv.org/pdf/1604.04953v1.pdf | author:Zecheng Xie, Zenghui Sun, Lianwen Jin, Ziyong Feng, Shuye Zhang category:cs.CV published:2016-04-18 summary:This paper proposes an end-to-end framework, namely fully convolutionalrecurrent network (FCRN) for handwritten Chinese text recognition (HCTR).Unlike traditional methods that rely heavily on segmentation, our FCRN istrained with online text data directly and learns to associate the pen-tiptrajectory with a sequence of characters. FCRN consists of four parts: apath-signature layer to extract signature features from the input pen-tiptrajectory, a fully convolutional network to learn informative representation,a sequence modeling layer to make per-frame predictions on the input sequenceand a transcription layer to translate the predictions into a label sequence.The FCRN is end-to-end trainable in contrast to conventional methods whosecomponents are separately trained and tuned. We also present a refined beamsearch method that efficiently integrates the language model to decode the FCRNand significantly improve the recognition results. We evaluate the performance of the proposed method on the test sets from thedatabases CASIA-OLHWDB and ICDAR 2013 Chinese handwriting recognitioncompetition, and both achieve state-of-the-art performance with correct ratesof 96.40% and 95.00%, respectively.
arxiv-17700-175 | Speed-Constrained Tuning for Statistical Machine Translation Using Bayesian Optimization | http://arxiv.org/pdf/1604.05073v1.pdf | author:Daniel Beck, AdriÃ  de Gispert, Gonzalo Iglesias, Aurelien Waite, Bill Byrne category:cs.CL published:2016-04-18 summary:We address the problem of automatically finding the parameters of astatistical machine translation system that maximize BLEU scores while ensuringthat decoding speed exceeds a minimum value. We propose the use of BayesianOptimization to efficiently tune the speed-related decoding parameters byeasily incorporating speed as a noisy constraint function. The obtainedparameter values are guaranteed to satisfy the speed constraint with anassociated confidence margin. Across three language pairs and two speedconstraint values, we report overall optimization time reduction compared togrid and random search. We also show that Bayesian Optimization can decouplespeed and BLEU measurements, resulting in a further reduction of overalloptimization time as speed is measured over a small subset of sentences.
arxiv-17700-176 | Most Likely Separation of Intensity and Warping Effects in Image Registration | http://arxiv.org/pdf/1604.05027v1.pdf | author:Line KÃ¼hnel, Stefan Sommer, Akshay Pai, Lars Lau Raket category:cs.CV published:2016-04-18 summary:This paper introduces a class of mixed-effects models for joint modeling ofspatially correlated intensity variation and warping variation in 2D images.Spatially correlated intensity variation and warp variation are modeled asrandom effects, resulting in a nonlinear mixed-effects model that enablessimultaneous estimation of template and model parameters by optimization of thelikelihood function. We propose an algorithm for fitting the model whichalternates estimation of variance parameters and image registration, thusavoiding potential estimation bias resulting from treating registration as apreprocessing step. We apply the model to datasets of facial images and 2Dbrain magnetic resonance images to illustrate the simultaneous estimation andprediction of intensity and warp effects.
arxiv-17700-177 | A Repeated Signal Difference for Recognising Patterns | http://arxiv.org/pdf/1604.05170v2.pdf | author:Kieran Greer category:cs.NE cs.AI q-bio.NC published:2016-04-18 summary:This paper describes a new mechanism that might help with defining patternsequences, by the fact that it can produce an upper bound on the ensemble valuethat can persistently oscillate with the actual values produced from eachpattern. With every firing event, a node also receives an on/off feedbackswitch. If the node fires, then it sends a feedback result depending on theinput signal strength. If the input signal is positive or larger, it can store'on' switch feedback for the next iteration. If the signal is negative orsmaller, it can store an 'off' switch feedback for the next iteration. If thenode does not fire, then it does not affect the current feedback situation andreceives the switch command produced by the last active pattern event for thesame neuron. The upper bound therefore also represents the largest or mostenclosing pattern set and the lower value is for the actual set of firingpatterns. If the pattern sequence repeats, it will oscillate between the twovalues, allowing them to be recognised and measured more easily, over time.Tests show that changing the sequence ordering can also be measured.
arxiv-17700-178 | An Initial Seed Selection Algorithm for K-means Clustering of Georeferenced Data to Improve Replicability of Cluster Assignments for Mapping Application | http://arxiv.org/pdf/1604.04893v1.pdf | author:Fouad Khan category:cs.LG cs.DS published:2016-04-17 summary:K-means is one of the most widely used clustering algorithms in variousdisciplines, especially for large datasets. However the method is known to behighly sensitive to initial seed selection of cluster centers. K-means++ hasbeen proposed to overcome this problem and has been shown to have betteraccuracy and computational efficiency than k-means. In many clustering problemsthough -such as when classifying georeferenced data for mapping applications-standardization of clustering methodology, specifically, the ability to arriveat the same cluster assignment for every run of the method i.e. replicabilityof the methodology, may be of greater significance than any perceived measureof accuracy, especially when the solution is known to be non-unique, as in thecase of k-means clustering. Here we propose a simple initial seed selectionalgorithm for k-means clustering along one attribute that draws initial clusterboundaries along the 'deepest valleys' or greatest gaps in dataset. Thus, itincorporates a measure to maximize distance between consecutive cluster centerswhich augments the conventional k-means optimization for minimum distancebetween cluster center and cluster members. Unlike existing initializationmethods, no additional parameters or degrees of freedom are introduced to theclustering algorithm. This improves the replicability of cluster assignments byas much as 100% over k-means and k-means++, virtually reducing the varianceover different runs to zero, without introducing any additional parameters tothe clustering process. Further, the proposed method is more computationallyefficient than k-means++ and in some cases, more accurate.
arxiv-17700-179 | Generating Semi-Synthetic Validation Benchmarks for Embryomics | http://arxiv.org/pdf/1604.04906v1.pdf | author:Johannes Stegmaier, Julian Arz, Benjamin Schott, Jens C. Otte, Andrei Kobitski, G. Ulrich Nienhaus, Uwe StrÃ¤hle, Peter Sanders, Ralf Mikut category:cs.CV q-bio.CB q-bio.QM published:2016-04-17 summary:Systematic validation is an essential part of algorithm development. Theenormous dataset sizes and the complexity observed in many recent time-resolved3D fluorescence microscopy imaging experiments, however, prohibit acomprehensive manual ground truth generation. Moreover, existing simulatedbenchmarks in this field are often too simple or too specialized tosufficiently validate the observed image analysis problems. We present a newsemi-synthetic approach to generate realistic 3D+t benchmarks that combineschallenging cellular movement dynamics of real embryos with simulatedfluorescent nuclei and artificial image distortions including variousparametrizable options like cell numbers, acquisition deficiencies or multiviewsimulations. We successfully applied the approach to simulate the developmentof a zebrafish embryo with thousands of cells over 14 hours of its earlyexistence.
arxiv-17700-180 | Two Points Fundamental Matrix | http://arxiv.org/pdf/1604.04848v1.pdf | author:Gil Ben-Artzi, Tavi Halperin, Michael Werman, Shmuel Peleg category:cs.CV published:2016-04-17 summary:It is well known that computing the fundamental matrix of two uncalibratedcameras requires at least seven corresponding points. We present a method tocompute the fundamental matrix between two cameras with only two pairs ofcorresponding points. Given these two points, we show how to find three pairsof corresponding epipolar lines, from which the fundamental matrix can becomputed. Two pairs of epipolar lines, incident to the two pairs of correspondingpoints, are found by maximizing stereo consistency between lines; correspondingepipolar lines yield a good stereo correspondence. These two epipolar linesintersect at the epipoles, giving a third pair of corresponding points. A thirdpair of matching epipolar lines, needed to compute the fundamental matrix, isfound from lines incident to the epipoles. We validate our method using real-world images and compare it tostate-of-the-art methods. Our approach is more accurate by a factor of fivecompared to the standard method using seven corresponding points, and itsaccuracy is comparable to the 8-points algorithm.
arxiv-17700-181 | Mahalanobis Distance Metric Learning Algorithm for Instance-based Data Stream Classification | http://arxiv.org/pdf/1604.04879v1.pdf | author:Jorge Luis Rivero Perez, Bernardete Ribeiro, Carlos Morell Perez category:cs.LG published:2016-04-17 summary:With the massive data challenges nowadays and the rapid growing oftechnology, stream mining has recently received considerable attention. Toaddress the large number of scenarios in which this phenomenon manifests itselfsuitable tools are required in various research fields. Instance-based datastream algorithms generally employ the Euclidean distance for theclassification task underlying this problem. A novel way to look into thisissue is to take advantage of a more flexible metric due to the increasedrequirements imposed by the data stream scenario. In this paper we present anew algorithm that learns a Mahalanobis metric using similarity anddissimilarity constraints in an online manner. This approach hybridizes aMahalanobis distance metric learning algorithm and a k-NN data streamclassification algorithm with concept drift detection. First, some basicaspects of Mahalanobis distance metric learning are described taking intoaccount key properties as well as online distance metric learning algorithms.Second, we implement specific evaluation methodologies and comparative metricssuch as Q statistic for data stream classification algorithms. Finally, ouralgorithm is evaluated on different datasets by comparing its results with oneof the best instance-based data stream classification algorithm of the state ofthe art. The results demonstrate that our proposal is better
arxiv-17700-182 | Subjects and Their Objects: Localizing Interactees for a Person-Centric View of Importance | http://arxiv.org/pdf/1604.04842v1.pdf | author:Chao-Yeh Chen, Kristen Grauman category:cs.CV published:2016-04-17 summary:Understanding images with people often entails understanding their\emph{interactions} with other objects or people. As such, given a novel image,a vision system ought to infer which other objects/people play an importantrole in a given person's activity. However, existing methods are limited tolearning action-specific interactions (e.g., how the pose of a tennis playerrelates to the position of his racquet when serving the ball) for improvedrecognition, making them unequipped to reason about novel interactions withactions or objects unobserved in the training data. We propose to predict the "interactee" in novel images---that is, to localizethe \emph{object} of a person's action. Given an arbitrary image with adetected person, the goal is to produce a saliency map indicating the mostlikely positions and scales where that person's interactee would be found. Tothat end, we explore ways to learn the generic, action-independent connectionsbetween (a) representations of a person's pose, gaze, and scene cues and (b)the interactee object's position and scale. We provide results on a newlycollected UT Interactee dataset spanning more than 10,000 images from SUN,PASCAL, and COCO. We show that the proposed interaction-informed saliencymetric has practical utility for four tasks: contextual object detection, imageretargeting, predicting object importance, and data-driven natural languagescene description. All four scenarios reveal the value in linking the subjectto its object in order to understand the story of an image.
arxiv-17700-183 | Some medical applications of example-based super-resolution | http://arxiv.org/pdf/1604.04926v1.pdf | author:Ramin Zabih category:cs.CV published:2016-04-17 summary:Example-based super-resolution (EBSR) reconstructs a high-resolution imagefrom a low-resolution image, given a training set of high-resolution images. Inthis note I propose some applications of EBSR to medical imaging. A particularinteresting application, which I call "x-ray voxelization", approximates theresult of a CT scan from an x-ray image.
arxiv-17700-184 | SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions | http://arxiv.org/pdf/1604.04835v1.pdf | author:Han Xiao, Minlie Huang, Xiaoyan Zhu category:cs.CL cs.LG published:2016-04-17 summary:Knowledge graph embedding represents the entities and relations as numericalvectors, and then knowledge analysis could be promoted as a numerical method.So far, most methods merely concentrate on the fact triples that are composedby the symbolic entities and relations, while the textual information which issupposed to be most critical in NLP could hardly play a reasonable role. Forthis end, this paper proposes the method SSP which jointly learns from thesymbolic triples and textual descriptions. Our model could interact both twoinformation sources by characterizing the correlations, by which means, thetextual descriptions could make effects to discover semantic relevance andoffer precise semantic embedding. Extensive experiments show our methodachieves the substantial improvements against the state-of-the-art baselines onthe tasks of knowledge graph completion and entity classification.
arxiv-17700-185 | Probabilistic Receiver Architecture Combining BP, MF, and EP for Multi-Signal Detection | http://arxiv.org/pdf/1604.04834v1.pdf | author:Daniel J. Jakubisin, R. Michael Buehrer, Claudio R. C. M. da Silva category:cs.IT math.IT stat.ML published:2016-04-17 summary:Receiver algorithms which combine belief propagation (BP) with the mean field(MF) approximation are well-suited for inference of both continuous anddiscrete random variables. In wireless scenarios involving detection ofmultiple signals, the standard construction of the combined BP-MF frameworkincludes the equalization or multi-user detection functions within the MFsubgraph. In this paper, we show that the MF approximation is not particularlyeffective for multi-signal detection. We develop a new factor graphconstruction for application of the BP-MF framework to problems involving thedetection of multiple signals. We then develop a low-complexity variant to theproposed construction in which Gaussian BP is applied to the equalizationfactors. In this case, the factor graph of the joint probability distributionis divided into three subgraphs: (i) a MF subgraph comprised of the observationfactors and channel estimation, (ii) a Gaussian BP subgraph which is applied tomulti-signal detection, and (iii) a discrete BP subgraph which is applied todemodulation and decoding. Expectation propagation is used to approximatediscrete distributions with a Gaussian distribution and links the discrete BPand Gaussian BP subgraphs. The result is a probabilistic receiver architecturewith strong theoretical justification which can be applied to multi-signaldetection.
arxiv-17700-186 | Visual saliency detection: a Kalman filter based approach | http://arxiv.org/pdf/1604.04825v1.pdf | author:Sourya Roy, Pabitra Mitra category:cs.CV published:2016-04-17 summary:In this paper we propose a Kalman filter aided saliency detection model whichis based on the conjecture that salient regions are considerably different fromour "visual expectation" or they are "visually surprising" in nature. In thiswork, we have structured our model with an immediate objective to predictsaliency in static images. However, the proposed model can be easily extendedfor space-time saliency prediction. Our approach was evaluated using twopublicly available benchmark data sets and results have been compared withother existing saliency models. The results clearly illustrate the superiorperformance of the proposed model over other approaches.
arxiv-17700-187 | Regularizing Solutions to the MEG Inverse Problem Using Space-Time Separable Covariance Functions | http://arxiv.org/pdf/1604.04931v1.pdf | author:Arno Solin, Pasi JylÃ¤nki, Jaakko KauramÃ¤ki, Tom Heskes, Marcel A. J. van Gerven, Simo SÃ¤rkkÃ¤ category:stat.AP stat.ML published:2016-04-17 summary:In magnetoencephalography (MEG) the conventional approach to sourcereconstruction is to solve the underdetermined inverse problem independentlyover time and space. Here we present how the conventional approach can beextended by regularizing the solution in space and time by a Gaussian process(Gaussian random field) model. Assuming a separable covariance function inspace and time, the computational complexity of the proposed model becomes(without any further assumptions or restrictions) $\mathcal{O}(t^3 + n^3 +m^2n)$, where $t$ is the number of time steps, $m$ is the number of sources,and $n$ is the number of sensors. We apply the method to both simulated andempirical data, and demonstrate the efficiency and generality of our Bayesiansource reconstruction approach which subsumes various classical approaches inthe literature.
arxiv-17700-188 | Structured Sparse Convolutional Autoencoder | http://arxiv.org/pdf/1604.04812v1.pdf | author:Ehsan Hosseini-Asl, Jacek M. Zurada category:cs.LG cs.NE published:2016-04-17 summary:This paper aims to improve the feature learning in Convolutional Networks(Convnet) by capturing the structure of objects. A new sparsity function isimposed on the extracted featuremap to capture the structure and shape of thelearned object, extracting interpretable features to improve the predictionperformance. The proposed algorithm is based on organizing the activationwithin and across featuremap by constraining the node activities through$\ell_{2}$ and $\ell_{1}$ normalization in a structured form.
arxiv-17700-189 | Multi-view Learning as a Nonparametric Nonlinear Inter-Battery Factor Analysis | http://arxiv.org/pdf/1604.04939v1.pdf | author:Andreas Damianou, Neil D. Lawrence, Carl Henrik Ek category:stat.ML cs.LG math.PR published:2016-04-17 summary:Factor analysis aims to determine latent factors, or traits, which summarizea given data set. Inter-battery factor analysis extends this notion to multipleviews of the data. In this paper we show how a nonlinear, nonparametric versionof these models can be recovered through the Gaussian process latent variablemodel. This gives us a flexible formalism for multi-view learning where thelatent variables can be used both for exploratory purposes and for learningrepresentations that enable efficient inference for ambiguous estimation tasks.Learning is performed in a Bayesian manner through the formulation of avariational compression scheme which gives a rigorous lower bound on the loglikelihood. Our Bayesian framework provides strong regularization duringtraining, allowing the structure of the latent space to be determinedefficiently and automatically. We demonstrate this by producing the first (toour knowledge) published results of learning from dozens of views, even whendata is scarce. We further show experimental results on several different typesof multi-view data sets and for different kinds of tasks, including exploratorydata analysis, generation, ambiguity modelling through latent priors andclassification.
arxiv-17700-190 | Global optimization of factor models using alternating minimization | http://arxiv.org/pdf/1604.04942v1.pdf | author:Lei Le, Martha White category:stat.ML cs.LG published:2016-04-17 summary:Learning new representations in machine learning is often tackled using afactorization of the data. For many such problems, including sparse coding andmatrix completion, learning these factorizations can be difficult, in terms ofefficiency and to guarantee that the solution is a global minimum. Recently, ageneral class of objectives have been introduced, called induced regularizedfactor models (RFMs), which have an induced convex form that enables globaloptimization. Though attractive theoretically, this induced form isimpractical, particularly for large or growing datasets. In this work, weinvestigate the use of a practical alternating minimization algorithms forinduced RFMs, that ensure convergence to global optima. We characterize thestationary points of these models, and, using these insights, highlightpractical choices for the objectives. We then provide theoretical and empiricalevidence that alternating minimization, from a random initialization, convergesto global minima for a large subclass of induced RFMs. In particular, we provethat induced RFMs do not have degenerate saddlepoints and that local minima areactually global minima. Finally, we provide an extensive investigation intopractical optimization choices for using alternating minimization for inducedRFMs, for both batch and stochastic gradient descent.
arxiv-17700-191 | From Incremental Meaning to Semantic Unit (phrase by phrase) | http://arxiv.org/pdf/1604.04873v1.pdf | author:Andreas Scherbakov, Ekaterina Vylomova, Fei Liu, Timothy Baldwin category:cs.CL 68T50 I.2.7 published:2016-04-17 summary:This paper describes an experimental approach to Detection of MinimalSemantic Units and their Meaning (DiMSUM), explored within the framework ofSemEval 2016 Task 10. The approach is primarily based on a combination of wordembeddings and parserbased features, and employs unidirectional incrementalcomputation of compositional embeddings for multiword expressions.
arxiv-17700-192 | DS-MLR: Exploiting Double Separability for Scaling up Distributed Multinomial Logistic Regression | http://arxiv.org/pdf/1604.04706v1.pdf | author:Parameswaran Raman, Shin Matsushima, Xinhua Zhang, Hyokun Yun, S. V. N. Vishwanathan category:cs.LG stat.ML published:2016-04-16 summary:Multinomial logistic regression is a popular tool in the arsenal of machinelearning algorithms, yet scaling it to datasets with very large number of datapoints and classes has not been trivial. This is primarily because one needs tocompute the log-partition function on every data point. This makes distributingthe computation hard. In this paper, we present a distributed stochasticgradient descent based optimization method (DS-MLR) for scaling up multinomiallogistic regression problems to very large data. Our algorithm exploitsdouble-separability, an attractive property we observe in the objectivefunctions of several models in machine learning, that allows us to distributeboth data and model parameters simultaneously across multiple machines. Inaddition to being easily parallelizable, our algorithm achieves good testaccuracy within a short period of time, with a low overall time and memoryfootprint as demonstrated by empirical results on both single and multi-machinesettings. For instance, on a dataset with 93,805 training instances and 12,294classes, we achieve close to optimal f-score in 10,000 seconds using 2 machineseach having 12 cores.
arxiv-17700-193 | Evolutionary Projection Selection for Radon Barcodes | http://arxiv.org/pdf/1604.04673v1.pdf | author:Hamid R. Tizhoosh, Shahryar Rahnamayan category:cs.CV published:2016-04-16 summary:Recently, Radon transformation has been used to generate barcodes for taggingmedical images. The under-sampled image is projected in certain directions, andeach projection is binarized using a local threshold. The concatenation of thethresholded projections creates a barcode that can be used for tagging orannotating medical images. A small number of equidistant projections, e.g., 4or 8, is generally used to generate short barcodes. However, due to the diversenature of digital images, and since we are only working with a small number ofprojections (to keep the barcode short), taking equidistant projections may notbe the best course of action. In this paper, we proposed to find $n$ optimalprojections, whereas $n\!<\!180$, in order to increase the expressiveness ofRadon barcodes. We show examples for the exhaustive search for the simple casewhen we attempt to find 4 best projections out of 16 equidistant projectionsand compare it with the evolutionary approach in order to establish the benefitof the latter when operating on a small population size as in the case ofmicro-DE. We randomly selected 10 different classes from IRMA dataset (14,400x-ray images in 58 classes) and further randomly selected 5 images per classfor our tests.
arxiv-17700-194 | Radon Features and Barcodes for Medical Image Retrieval via SVM | http://arxiv.org/pdf/1604.04675v1.pdf | author:Shujin Zhu, H. R. Tizhoosh category:cs.CV published:2016-04-16 summary:For more than two decades, research has been performed on content-based imageretrieval (CBIR). By combining Radon projections and the support vectormachines (SVM), a content-based medical image retrieval method is presented inthis work. The proposed approach employs the normalized Radon projections withcorresponding image category labels to build an SVM classifier, and the Radonbarcode database which encodes every image in a binary format is also generatedsimultaneously to tag all images. To retrieve similar images when a query imageis given, Radon projections and the barcode of the query image are generated.Subsequently, the k-nearest neighbor search method is applied to find theimages with minimum Hamming distance of the Radon barcode within the same classpredicted by the trained SVM classifier that uses Radon features. Theperformance of the proposed method is validated by using the IRMA 2009 datasetwith 14,410 x-ray images in 57 categories. The results demonstrate that ourmethod has the capacity to retrieve similar responses for the correctlyidentified query image and even for those mistakenly classified by SVM. Theapproach further is very fast and has low memory requirement.
arxiv-17700-195 | Generating Binary Tags for Fast Medical Image Retrieval Based on Convolutional Nets and Radon Transform | http://arxiv.org/pdf/1604.04676v1.pdf | author:Xinran Liu, Hamid R. Tizhoosh, Jonathan Kofman category:cs.CV published:2016-04-16 summary:Content-based image retrieval (CBIR) in large medical image archives is achallenging and necessary task. Generally, different feature extraction methodsare used to assign expressive and invariant features to each image such thatthe search for similar images comes down to feature classification and/ormatching. The present work introduces a new image retrieval method for medicalapplications that employs a convolutional neural network (CNN) with recentlyintroduced Radon barcodes. We combine neural codes for global classificationwith Radon barcodes for the final retrieval. We also examine image search basedon regions of interest (ROI) matching after image retrieval. The IRMA datasetwith more than 14,000 x-rays images is used to evaluate the performance of ourmethod. Experimental results show that our approach is superior to manypublished works.
arxiv-17700-196 | Anatomy-Aware Measurement of Segmentation Accuracy | http://arxiv.org/pdf/1604.04678v1.pdf | author:Hamid R. Tizhoosh, Ahmed A. Othman category:cs.CV published:2016-04-16 summary:Quantifying the accuracy of segmentation and manual delineation of organs,tissue types and tumors in medical images is a necessary measurement thatsuffers from multiple problems. One major shortcoming of all accuracy measuresis that they neglect the anatomical significance or relevance of differentzones within a given segment. Hence, existing accuracy metrics measure theoverlap of a given segment with a ground-truth without any anatomicaldiscrimination inside the segment. For instance, if we understand the rectalwall or urethral sphincter as anatomical zones, then current accuracy measuresignore their significance when they are applied to assess the quality of theprostate gland segments. In this paper, we propose an anatomy-aware measurementscheme for segmentation accuracy of medical images. The idea is to create a``master gold'' based on a consensus shape containing not just the outline ofthe segment but also the outlines of the internal zones if existent orrelevant. To apply this new approach to accuracy measurement, we introduce theanatomy-aware extensions of both Dice coefficient and Jaccard index andinvestigate their effect using 500 synthetic prostate ultrasound images with 20different segments for each image. We show that through anatomy-sensitivecalculation of segmentation accuracy, namely by considering relevant anatomicalzones, not only the measurement of individual users can change but also theranking of users' segmentation skills may require reordering.
arxiv-17700-197 | Sentence-Level Grammatical Error Identification as Sequence-to-Sequence Correction | http://arxiv.org/pdf/1604.04677v1.pdf | author:Allen Schmaltz, Yoon Kim, Alexander M. Rush, Stuart M. Shieber category:cs.CL published:2016-04-16 summary:We demonstrate that an attention-based encoder-decoder model can be used forsentence-level grammatical error identification for the Automated Evaluation ofScientific Writing (AESW) Shared Task 2016. The attention-based encoder-decodermodels can be used for the generation of corrections, in addition to erroridentification, which is of interest for certain end-user applications. We showthat a character-based encoder-decoder model is particularly effective,outperforming other results on the AESW Shared Task on its own, and showinggains over a word-based counterpart. Our final model--a combination of threecharacter-based encoder-decoder models, one word-based encoder-decoder model,and a sentence-level CNN--is the highest performing system on the AESW 2016binary prediction Shared Task.
arxiv-17700-198 | Subcategory-aware Convolutional Neural Networks for Object Proposals and Detection | http://arxiv.org/pdf/1604.04693v1.pdf | author:Yu Xiang, Wongun Choi, Yuanqing Lin, Silvio Savarese category:cs.CV published:2016-04-16 summary:In CNN-based object detection methods, region proposal becomes a bottleneckwhen objects exhibit significant scale variation, occlusion or truncation. Inaddition, these methods mainly focus on 2D object detection and cannot estimatedetailed properties of objects. In this paper, we propose subcategory-awareCNNs for object detection. We introduce a novel region proposal network thatuses subcategory information to guide the proposal generating process, and anew detection network for joint detection and subcategory classification. Byusing subcategories related to object pose, we achieve state-of-the-artperformance on both detection and pose estimation on commonly used benchmarks.
arxiv-17700-199 | Learning Models for Actions and Person-Object Interactions with Transfer to Question Answering | http://arxiv.org/pdf/1604.04808v1.pdf | author:Arun Mallya, Svetlana Lazebnik category:cs.CV published:2016-04-16 summary:In this paper, we propose a convolutional deep network model which utilizeslocal and global context through feature fusion to make human activity labelpredictions and achieve state-of-the-art performance on two different activityrecognition datasets, the HICO and MPII Human Pose Dataset. We use MultipleInstance Learning to handle the lack of full person instance-label supervisionand weighted loss to handle the unbalanced training data. Further, we show howexpert knowledge from these specialized datasets can be transferred to improveaccuracy on the Visual Question Answering (VQA) task, in the form of multiplechoice fill-in-the-blank questions (Visual Madlibs). Specifically, we tackletwo types of questions on person's activity and person-object relationship andshow improvements over generic features trained on the ImageNet classificationtask.
arxiv-17700-200 | Supervised and Unsupervised Ensembling for Knowledge Base Population | http://arxiv.org/pdf/1604.04802v1.pdf | author:Nazneen Fatema Rajani, Raymond J. Mooney category:cs.CL cs.LG published:2016-04-16 summary:We present results on combining supervised and unsupervised methods toensemble multiple systems for two popular Knowledge Base Population (KBP)tasks, Cold Start Slot Filling (CSSF) and Tri-lingual Entity Discovery andLinking (TEDL). We demonstrate that our combined system along with auxiliaryfeatures outperforms the best performing system for both tasks in the 2015competition, several ensembling baselines, as well as the state-of-the-artstacking approach to ensembling KBP systems. The success of our technique ontwo different and challenging problems demonstrates the power and generality ofour combined approach to ensembling.
arxiv-17700-201 | A Hierarchical Genetic Optimization of a Fuzzy Logic System for Flow Control in Micro Grids | http://arxiv.org/pdf/1604.04789v1.pdf | author:Enrico De Santis, Alireza Sadeghian, Antonello Rizzi category:cs.AI cs.NE published:2016-04-16 summary:Computational Intelligence techniques are today widely used to solve complexengineering problems. Bio-inspired algorithms like Genetic Algorithms and FuzzyInference Systems are nowadays adopted as hybrids techniques in the commercialand industrial environment. In this paper, we present an interestingapplication of the FUZZY-GA paradigm to Smart Grids. In particular, this studyfocuses on the possibility of tuning a Fuzzy Rule Base trying to discover, bymeans of a GA, a minimal fuzzy rules set in a Fuzzy Logic Controller (FLC)adopted to perform decision making for the power flow management task in amicrogrid. The RB optimization is obtained through Hierarchical GeneticAlgorithm, based on an encoding scheme inspired by Nature, applied to theoptimization of the FIS parameters. Tests show how the proposed controllerscheme is effective in maximizing the economic return when dealing with theproblem of power flows management in a microgrid, equipped with an energystorage system.
arxiv-17700-202 | ACD: Action Concept Discovery from Image-Sentence Corpora | http://arxiv.org/pdf/1604.04784v1.pdf | author:Jiyang Gao, Chen Sun, Ram Nevatia category:cs.CV published:2016-04-16 summary:Action classification in still images is an important task in computervision. It is challenging as the appearances of ac- tions may vary depending ontheir context (e.g. associated objects). Manually labeling of contextinformation would be time consuming and difficult to scale up. To address thischallenge, we propose a method to automatically discover and cluster actionconcepts, and learn their classifiers from weakly supervised image-sentencecorpora. It obtains candidate action concepts by extracting verb-object pairsfrom sentences and verifies their visualness with the associated images.Candidate action concepts are then clustered by using a multi-modalrepresentation with image embeddings from deep convolutional networks and textembeddings from word2vec. More than one hundred human action conceptclassifiers are learned from the Flickr 30k dataset with no additional humaneffort and promising classification results are obtained. We further apply theAdaBoost algorithm to automatically select and combine relevant action conceptsgiven an action query. Promising results have been shown on the PASCAL VOC 2012action classification benchmark, which has zero overlap with Flickr30k.
arxiv-17700-203 | Automatic Segmentation of Dynamic Objects from an Image Pair | http://arxiv.org/pdf/1604.04724v1.pdf | author:Sri Raghu Malireddi, Shanmuganathan Raman category:cs.CV published:2016-04-16 summary:Automatic segmentation of objects from a single image is a challengingproblem which generally requires training on large number of images. Weconsider the problem of automatically segmenting only the dynamic objects froma given pair of images of a scene captured from different positions. We exploitdense correspondences along with saliency measures in order to first localizethe interest points on the dynamic objects from the two images. We propose anovel approach based on techniques from computational geometry in order toautomatically segment the dynamic objects from both the images using a top-downsegmentation strategy. We discuss how the proposed approach is unique innovelty compared to other state-of-the-art segmentation algorithms. We showthat the proposed approach for segmentation is efficient in handling largemotions and is able to achieve very good segmentation of the objects fordifferent scenes. We analyse the results with respect to the manually markedground truth segmentation masks created using our own dataset and provide keyobservations in order to improve the work in future.
arxiv-17700-204 | Phone-based Metric as a Predictor for Basic Personality Traits | http://arxiv.org/pdf/1604.04696v1.pdf | author:Bjarke MÃ¸nsted, Anders Mollgaard, Joachim Mathiesen category:cs.SI cs.LG physics.soc-ph published:2016-04-16 summary:Basic personality traits are typically assessed through questionnaires. Herewe consider phone-based metrics as a way to asses personality traits. We usedata from smartphones with custom data-collection software distributed to 730individuals. The data includes information about location, physical motion,face-to-face contacts, online social network friends, text messages and calls.The data is further complemented by questionnaire-based data on basicpersonality traits. From the phone-based metrics, we define a set of behavioralvariables, which we use in a prediction of basic personality traits. We findthat predominantly, the Big Five personality traits extraversion and, to somedegree, neuroticism are strongly expressed in our data. As an alternative tothe Big Five, we investigate whether other linear combinations of the 44questions underlying the Big Five Inventory are more predictable. In a tertileclassification problem, basic dimensionality reduction techniques, such asindependent component analysis, increase the predictability relative to thebaseline from $11\%$ to $23\%$. Finally, from a supervised linear classifier,we were able to further improve this predictability to $33\%$. In all cases,the most predictable projections had an overweight of the questions related toextraversion and neuroticism. In addition, our findings indicate that the scoresystem underlying the Big Five Inventory disregards a part of the informationavailable in the 44 questions.
arxiv-17700-205 | Efficient Dictionary Learning with Sparseness-Enforcing Projections | http://arxiv.org/pdf/1604.04767v1.pdf | author:Markus Thom, Matthias Rapp, GÃ¼nther Palm category:cs.LG cs.CV cs.NE published:2016-04-16 summary:Learning dictionaries suitable for sparse coding instead of using engineeredbases has proven effective in a variety of image processing tasks. This paperstudies the optimization of dictionaries on image data where the representationis enforced to be explicitly sparse with respect to a smooth, normalizedsparseness measure. This involves the computation of Euclidean projections ontolevel sets of the sparseness measure. While previous algorithms for thisoptimization problem had at least quasi-linear time complexity, here the firstalgorithm with linear time complexity and constant space complexity isproposed. The key for this is the mathematically rigorous derivation of acharacterization of the projection's result based on a soft-shrinkage function.This theory is applied in an original algorithm called Easy Dictionary Learning(EZDL), which learns dictionaries with a simple and fast-to-computeHebbian-like learning rule. The new algorithm is efficient, expressive andparticularly simple to implement. It is demonstrated that despite itssimplicity, the proposed learning algorithm is able to generate a rich varietyof dictionaries, in particular a topographic organization of atoms or separableatoms. Further, the dictionaries are as expressive as those of benchmarklearning algorithms in terms of the reproduction quality on entire images, andresult in an equivalent denoising performance. EZDL learns approximately 30 %faster than the already very efficient Online Dictionary Learning algorithm,and is therefore eligible for rapid data set analysis and problems with vastquantities of learning samples.
arxiv-17700-206 | Closed loop interactions between spiking neural network and robotic simulators based on MUSIC and ROS | http://arxiv.org/pdf/1604.04764v1.pdf | author:Philipp Weidel, Mikael Djurfeldt, Renato Duarte, Abigail Morrison category:cs.NE cs.RO published:2016-04-16 summary:In order to properly assess the function and computational properties ofsimulated neural systems, it is necessary to account for the nature of thestimuli that drive the system. However, providing stimuli that are rich and yetboth reproducible and amenable to experimental manipulations is technicallychallenging, and even more so if a closed-loop scenario is required. In thiswork, we present a novel approach to solve this problem, connecting roboticsand neural network simulators. We implement a middleware solution that bridgesthe Robotic Operating System (ROS) to the Multi-Simulator Coordinator (MUSIC).This enables any robotic and neural simulators that implement the correspondinginterfaces to be efficiently coupled, allowing real-time performance for a widerange of configurations. This work extends the toolset available forresearchers in both neurorobotics and computational neuroscience, and createsthe opportunity to perform closed-loop experiments of arbitrary complexity toaddress questions in multiple areas, including embodiment, agency, andreinforcement learning.
arxiv-17700-207 | Smoothed Hierarchical Dirichlet Process: A Non-Parametric Approach to Constraint Measures | http://arxiv.org/pdf/1604.04741v1.pdf | author:Cheng Luo, Yang Xiang, Richard Yi Da Xu category:stat.ML published:2016-04-16 summary:Time-varying mixture densities occur in many scenarios, for example, thedistributions of keywords that appear in publications may evolve from year toyear, video frame features associated with multiple targets may evolve in asequence. Any models that realistically cater to this phenomenon must exhibittwo important properties: the underlying mixture densities must have an unknownnumber of mixtures, and there must be some "smoothness" constraints in placefor the adjacent mixture densities. The traditional Hierarchical DirichletProcess (HDP) may be suited to the first property, but certainly not thesecond. This is due to how each random measure in the lower hierarchies issampled independent of each other and hence does not facilitate any temporalcorrelations. To overcome such shortcomings, we proposed a new SmoothedHierarchical Dirichlet Process (sHDP). The key novelty of this model is that weplace a temporal constraint amongst the nearby discrete measures $\{G_j\}$ inthe form of symmetric Kullback-Leibler (KL) Divergence with a fixed bound $B$.Although the constraint we place only involves a single scalar value, itnonetheless allows for flexibility in the corresponding successive measures.Remarkably, it also led us to infer the model within the stick-breaking processwhere the traditional Beta distribution used in stick-breaking is now replacedby a new constraint calculated from $B$. We present the inference algorithm andelaborate on its solutions. Our experiment using NIPS keywords has shown thedesirable effect of the model.
arxiv-17700-208 | Improving the Robustness of Deep Neural Networks via Stability Training | http://arxiv.org/pdf/1604.04326v1.pdf | author:Stephan Zheng, Yang Song, Thomas Leung, Ian Goodfellow category:cs.CV cs.LG published:2016-04-15 summary:In this paper we address the issue of output instability of deep neuralnetworks: small perturbations in the visual input can significantly distort thefeature embeddings and output of a neural network. Such instability affectsmany deep architectures with state-of-the-art performance on a wide range ofcomputer vision tasks. We present a general stability training method tostabilize deep networks against small input distortions that result fromvarious types of common image processing, such as compression, rescaling, andcropping. We validate our method by stabilizing the state-of-the-art Inceptionarchitecture against these types of distortions. In addition, we demonstratethat our stabilized model gives robust state-of-the-art performance onlarge-scale near-duplicate detection, similar-image ranking, and classificationon noisy datasets.
arxiv-17700-209 | Latent Model Ensemble with Auto-localization | http://arxiv.org/pdf/1604.04333v1.pdf | author:Miao Sun, Tony X. Han, Xun Xu category:cs.CV published:2016-04-15 summary:Deep Convolutional Neural Networks (CNN) have exhibited superior performancein many visual recognition tasks including image classification, objectdetection, and scene label- ing, due to their large learning capacity andresistance to overfit. For the image classification task, most of the currentdeep CNN- based approaches take the whole size-normalized image as input andhave achieved quite promising results. Compared with the previously dominatingapproaches based on feature extraction, pooling, and classification, the deepCNN-based approaches mainly rely on the learning capability of deep CNN toachieve superior results: the burden of minimizing intra-class variation whilemaximizing inter-class difference is entirely dependent on the implicit featurelearning component of deep CNN; we rely upon the implicitly learned filters andpooling component to select the discriminative regions, which correspond to theactivated neurons. However, if the irrelevant regions constitute a largeportion of the image of interest, the classification performance of the deepCNN, which takes the whole image as input, can be heavily affected. To solvethis issue, we propose a novel latent CNN framework, which treats the mostdiscriminate region as a latent variable. We can jointly learn the global CNNwith the latent CNN to avoid the aforementioned big irrelevant region issue,and our experimental results show the evident advantage of the proposed latentCNN over traditional deep CNN: latent CNN outperforms the state-of-the-artperformance of deep CNN on standard benchmark datasets including the CIFAR-10,CIFAR- 100, MNIST and PASCAL VOC 2007 Classification dataset.
arxiv-17700-210 | The Artificial Mind's Eye: Resisting Adversarials for Convolutional Neural Networks using Internal Projection | http://arxiv.org/pdf/1604.04428v1.pdf | author:Harm Berntsen, Wouter Kuijper, Tom Heskes category:cs.LG cs.NE published:2016-04-15 summary:We introduce a novel type of artificial neural network structure and trainingprocedure that results in networks that are provably, quantitatively morerobust to adversarial samples than classical, end-to-end trained classifiers.The main idea of our approach is to force the network to make predictions onwhat the given instance of the class under consideration would look like andsubsequently test those predictions. By forcing the network to redraw therelevant parts of the image and subsequently comparing this new image to theoriginal, we are having the network give a 'proof' of the presence of theobject.
arxiv-17700-211 | ModelWizard: Toward Interactive Model Construction | http://arxiv.org/pdf/1604.04639v1.pdf | author:Dylan Hutchison category:cs.PL cs.LG published:2016-04-15 summary:Data scientists engage in model construction to discover machine learningmodels that well explain a dataset, in terms of predictiveness,understandability and generalization across domains. Questions such as "what ifwe model common cause Z" and "what if Y's dependence on X reverses" inspiremany candidate models to consider and compare, yet current tools emphasizeconstructing a final model all at once. To more naturally reflect exploration when debating numerous models, wepropose an interactive model construction framework grounded in composableoperations. Primitive operations capture core steps refining data and modelthat, when verified, form an inductive basis to prove model validity. Derived,composite operations enable advanced model families, both generic andspecialized, abstracted away from low-level details. We prototype our envisioned framework in ModelWizard, a domain-specificlanguage embedded in F# to construct Tabular models. We enumerate languagedesign and demonstrate its use through several applications, emphasizing howlanguage may facilitate creation of complex models. To future engineersdesigning data science languages and tools, we offer ModelWizard's design as anew model construction paradigm, speeding discovery of our universe'sstructure.
arxiv-17700-212 | Non-contact hemodynamic imaging reveals the jugular venous pulse waveform | http://arxiv.org/pdf/1604.05213v2.pdf | author:Robert Amelard, Richard L Hughson, Danielle K Greaves, Kaylen J Pfisterer, Jason Leung, David A Clausi, Alexander Wong category:physics.med-ph cs.CV physics.optics published:2016-04-15 summary:Cardiovascular monitoring is important to prevent diseases from progressing.The jugular venous pulse (JVP) waveform offers important clinical informationabout cardiac health, but is not routinely examined due to its invasivecatheterisation procedure. Here, we demonstrate for the first time that the JVPcan be consistently observed in a non-contact manner using a novel light-basedphotoplethysmographic imaging system, coded hemodynamic imaging (CHI). Whiletraditional monitoring methods measure the JVP at a single location, CHI'swide-field imaging capabilities were able to observe the jugular venous pulse'sspatial flow profile for the first time. The important inflection points in theJVP were observed, meaning that cardiac abnormalities can be assessed throughJVP distortions. CHI provides a new way to assess cardiac health throughnon-contact light-based JVP monitoring, and can be used in non-surgicalenvironments for cardiac assessment.
arxiv-17700-213 | Make Up Your Mind: The Price of Online Queries in Differential Privacy | http://arxiv.org/pdf/1604.04618v1.pdf | author:Mark Bun, Thomas Steinke, Jonathan Ullman category:cs.CR cs.DS cs.LG published:2016-04-15 summary:We consider the problem of answering queries about a sensitive datasetsubject to differential privacy. The queries may be chosen adversarially from alarger set Q of allowable queries in one of three ways, which we list in orderfrom easiest to hardest to answer: Offline: The queries are chosen all at once and the differentially privatemechanism answers the queries in a single batch. Online: The queries are chosen all at once, but the mechanism only receivesthe queries in a streaming fashion and must answer each query before seeing thenext query. Adaptive: The queries are chosen one at a time and the mechanism must answereach query before the next query is chosen. In particular, each query maydepend on the answers given to previous queries. Many differentially private mechanisms are just as efficient in the adaptivemodel as they are in the offline model. Meanwhile, most lower bounds fordifferential privacy hold in the offline setting. This suggests that the threemodels may be equivalent. We prove that these models are all, in fact, distinct. Specifically, we showthat there is a family of statistical queries such that exponentially morequeries from this family can be answered in the offline model than in theonline model. We also exhibit a family of search queries such thatexponentially more queries from this family can be answered in the online modelthan in the adaptive model. We also investigate whether such separations mighthold for simple queries like threshold queries over the real line.
arxiv-17700-214 | Bayesian linear regression with Student-t assumptions | http://arxiv.org/pdf/1604.04434v1.pdf | author:Chaobing Song, Shu-Tao Xia category:cs.LG stat.ML published:2016-04-15 summary:As an automatic method of determining model complexity using the trainingdata alone, Bayesian linear regression provides us a principled way to selecthyperparameters. But one often needs approximation inference if distributionassumption is beyond Gaussian distribution. In this paper, we propose aBayesian linear regression model with Student-t assumptions (BLRS), which canbe inferred exactly. In this framework, both conjugate prior and expectationmaximization (EM) algorithm are generalized. Meanwhile, we prove that themaximum likelihood solution is equivalent to the standard Bayesian linearregression with Gaussian assumptions (BLRG). The $q$-EM algorithm for BLRS isnearly identical to the EM algorithm for BLRG. It is showed that $q$-EM forBLRS can converge faster than EM for BLRG for the task of predicting onlinenews popularity.
arxiv-17700-215 | Recognition of facial expressions based on salient geometric features and support vector machines | http://arxiv.org/pdf/1604.04334v1.pdf | author:Deepak Ghimire, Joonwhoan Lee, Ze-Nian Li, Sunghwan Jeong category:cs.CV published:2016-04-15 summary:Facial expressions convey nonverbal cues which play an important role ininterpersonal relations, and are widely used in behavior interpretation ofemotions, cognitive science, and social interactions. In this paper we analyzedifferent ways of representing geometric feature and present a fully automaticfacial expression recognition (FER) system using salient geometric features. Ingeometric feature-based FER approach, the first important step is to initializeand track dense set of facial points as the expression evolves over time inconsecutive frames. In the proposed system, facial points are initialized usingelastic bunch graph matching (EBGM) algorithm and tracking is performed usingKanade-Lucas-Tomaci (KLT) tracker. We extract geometric features from point,line and triangle composed of tracking results of facial points. The mostdiscriminative line and triangle features are extracted using feature selectivemulti-class AdaBoost with the help of extreme learning machine (ELM)classification. Finally the geometric features for FER are extracted from theboosted line, and triangles composed of facial points. The recognition accuracyusing features from point, line and triangle are analyzed independently. Theperformance of the proposed FER system is evaluated on three different datasets: namely CK+, MMI and MUG facial expression data sets.
arxiv-17700-216 | Delta divergence: A novel decision cognizant measure of classifier incongruence | http://arxiv.org/pdf/1604.04451v1.pdf | author:Josef Kittler, Cemre Zor category:cs.LG cs.IT math.IT stat.ML published:2016-04-15 summary:Disagreement between two classifiers regarding the class membership of anobservation in pattern recognition can be indicative of an anomaly and itsnuance. As in general classifiers base their decision on class aposterioriprobabilities, the most natural approach to detecting classifier incongruenceis to use divergence. However, existing divergences are not particularlysuitable to gauge classifier incongruence. In this paper, we postulate theproperties that a divergence measure should satisfy and propose a noveldivergence measure, referred to as Delta divergence. In contrast to existingmeasures, it is decision cognizant. The focus in Delta divergence on thedominant hypotheses has a clutter reducing property, the significance of whichgrows with increasing number of classes. The proposed measure satisfies otherimportant properties such as symmetry, and independence of classifierconfidence. The relationship of the proposed divergence to some baselinemeasures is demonstrated experimentally, showing its superiority.
arxiv-17700-217 | Low-Rank Matrix Recovery using Gabidulin Codes in Characteristic Zero | http://arxiv.org/pdf/1604.04397v1.pdf | author:Sven MÃ¼elich, Sven Puchinger, Martin Bossert category:cs.IT cs.CV math.IT published:2016-04-15 summary:We present a new approach on low-rank matrix recovery (LRMR) based onGabidulin Codes. Since most applications of LRMR deal with matrices overinfinite fields, we use the recently introduced generalization of Gabidulincodes to fields of characterstic zero. We show that LRMR can be reduced todecoding of Gabidulin codes and discuss which field extensions can be used inthe code construction.
arxiv-17700-218 | Facial expression recognition based on local region specific features and support vector machines | http://arxiv.org/pdf/1604.04337v1.pdf | author:Deepak Ghimire, Sunghwan Jeong, Joonwhoan Lee, Sang Hyun Park category:cs.CV published:2016-04-15 summary:Facial expressions are one of the most powerful, natural and immediate meansfor human being to communicate their emotions and intensions. Recognition offacial expression has many applications including human-computer interaction,cognitive science, human emotion analysis, personality development etc. In thispaper, we propose a new method for the recognition of facial expressions fromsingle image frame that uses combination of appearance and geometric featureswith support vector machines classification. In general, appearance featuresfor the recognition of facial expressions are computed by dividing face regioninto regular grid (holistic representation). But, in this paper we extractedregion specific appearance features by dividing the whole face region intodomain specific local regions. Geometric features are also extracted fromcorresponding domain specific regions. In addition, important local regions aredetermined by using incremental search approach which results in the reductionof feature dimension and improvement in recognition accuracy. The results offacial expressions recognition using features from domain specific regions arealso compared with the results obtained using holistic representation. Theperformance of the proposed facial expression recognition system has beenvalidated on publicly available extended Cohn-Kanade (CK+) facial expressiondata sets.
arxiv-17700-219 | Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks | http://arxiv.org/pdf/1604.04382v1.pdf | author:Chuan Li, Michael Wand category:cs.CV published:2016-04-15 summary:This paper proposes Markovian Generative Adversarial Networks (MGANs), amethod for training generative neural networks for efficient texture synthesis.While deep neural network approaches have recently demonstrated remarkableresults in terms of synthesis quality, they still come at considerablecomputational costs (minutes of run-time for low-res images). Our paperaddresses this efficiency issue. Instead of a numerical deconvolution inprevious work, we precompute a feed-forward, strided convolutional network thatcaptures the feature statistics of Markovian patches and is able to directlygenerate outputs of arbitrary dimensions. Such network can directly decodebrown noise to realistic texture, or photos to artistic paintings. Withadversarial training, we obtain quality comparable to recent neural texturesynthesis methods. As no optimization is required any longer at generationtime, our run-time performance (0.25M pixel images at 25Hz) surpasses previousneural texture synthesizers by a significant margin (at least 500 timesfaster). We apply this idea to texture synthesis, style transfer, and videostylization.
arxiv-17700-220 | High-performance Semantic Segmentation Using Very Deep Fully Convolutional Networks | http://arxiv.org/pdf/1604.04339v1.pdf | author:Zifeng Wu, Chunhua Shen, Anton van den Hengel category:cs.CV published:2016-04-15 summary:We propose a method for high-performance semantic image segmentation (orsemantic pixel labelling) based on very deep residual networks, which achievesthe state-of-the-art performance. A few design factors are carefully consideredto this end. We make the following contributions. (i) First, we evaluate differentvariations of a fully convolutional residual network so as to find the bestconfiguration, including the number of layers, the resolution of feature maps,and the size of field-of-view. Our experiments show that further enlarging thefield-of-view and increasing the resolution of feature maps are typicallybeneficial, which however inevitably leads to a higher demand for GPU memories.To walk around the limitation, we propose a new method to simulate a highresolution network with a low resolution network, which can be applied duringtraining and/or testing. (ii) Second, we propose an online bootstrapping methodfor training. We demonstrate that online bootstrapping is critically importantfor achieving good accuracy. (iii) Third we apply the traditional dropout tosome of the residual blocks, which further improves the performance. (iv)Finally, our method achieves the currently best mean intersection-over-union78.3\% on the PASCAL VOC 2012 dataset, as well as on the recent datasetCityscapes.
arxiv-17700-221 | Probing the Intra-Component Correlations within Fisher Vector for Material Classification | http://arxiv.org/pdf/1604.04473v1.pdf | author:Xiaopeng Hong, Xianbiao Qi, Guoying Zhao, Matti PietikÃ¤inen category:cs.CV published:2016-04-15 summary:Fisher vector (FV) has become a popular image representation. One notableunderlying assumption of the FV framework is that local descriptors are welldecorrelated within each cluster so that the covariance matrix for eachGaussian can be simplified to be diagonal. Though the FV usually relies on thePrincipal Component Analysis (PCA) to decorrelate local features, the PCA isapplied to the entire training data and hence it only diagonalizes the\textit{universal} covariance matrix, rather than those w.r.t. the localcomponents. As a result, the local decorrelation assumption is usually notsupported in practice. To relax this assumption, this paper proposes a completed model of the Fishervector, which is termed as the Completed Fisher vector (CFV). The CFV is a moregeneral framework of the FV, since it encodes not only the variances but alsothe correlations of the whitened local descriptors. The CFV thus leads toimproved discriminative power. We take the task of material categorization asan example and experimentally show that: 1) the CFV outperforms the FV underall parameter settings; 2) the CFV is robust to the changes in the number ofcomponents in the mixture; 3) even with a relatively small visual vocabularythe CFV still works well on two challenging datasets.
arxiv-17700-222 | Composition of Deep and Spiking Neural Networks for Very Low Bit Rate Speech Coding | http://arxiv.org/pdf/1604.04383v1.pdf | author:Milos Cernak, Alexandros Lazaridis, Afsaneh Asaei, Philip N. Garner category:cs.SD cs.CL published:2016-04-15 summary:Most current very low bit rate (VLBR) speech coding systems use hidden Markovmodel (HMM) based speech recognition/synthesis techniques. This allowstransmission of information (such as phonemes) segment by segment thatdecreases the bit rate. However, the encoder based on a phoneme speechrecognition may create bursts of segmental errors. Segmental errors are furtherpropagated to optional suprasegmental (such as syllable) information coding.Together with the errors of voicing detection in pitch parametrization,HMM-based speech coding creates speech discontinuities and unnatural speechsound artefacts. In this paper, we propose a novel VLBR speech coding framework based onneural networks (NNs) for end-to-end speech analysis and synthesis withoutHMMs. The speech coding framework relies on phonological (sub-phonetic)representation of speech, and it is designed as a composition of deep andspiking NNs: a bank of phonological analysers at the transmitter, and aphonological synthesizer at the receiver, both realised as deep NNs, and aspiking NN as an incremental and robust encoder of syllable boundaries forcoding of continuous fundamental frequency (F0). A combination of phonologicalfeatures defines much more sound patterns than phonetic features defined byHMM-based speech coders, and the finer analysis/synthesis code contributes intosmoother encoded speech. Listeners significantly prefer the NN-based approachdue to fewer discontinuities and speech artefacts of the encoded speech. Asingle forward pass is required during the speech encoding and decoding. Theproposed VLBR speech coding operates at bit rate about 360 bits/sec.
arxiv-17700-223 | Bags of Local Convolutional Features for Scalable Instance Search | http://arxiv.org/pdf/1604.04653v1.pdf | author:Eva Mohedano, Amaia Salvador, Kevin McGuinness, Ferran Marques, Noel E. O'Connor, Xavier Giro-i-Nieto category:cs.CV cs.MM published:2016-04-15 summary:This work proposes a simple instance retrieval pipeline based on encoding theconvolutional features of CNN using the bag of words aggregation scheme (BoW).Assigning each local array of activations in a convolutional layer to a visualword produces an \textit{assignment map}, a compact representation that relatesregions of an image with a visual word. We use the assignment map for fastspatial reranking, obtaining object localizations that are used for queryexpansion. We demonstrate the suitability of the BoW representation based onlocal CNN features for instance retrieval, achieving competitive performance onthe Oxford and Paris buildings benchmarks. We show that our proposed system forCNN feature aggregation with BoW outperforms state-of-the-art techniques usingsum pooling at a subset of the challenging TRECVid INS benchmark.
arxiv-17700-224 | DARI: Distance metric And Representation Integration for Person Verification | http://arxiv.org/pdf/1604.04377v1.pdf | author:Guangrun Wang, Liang Lin, Shengyong Ding, Ya Li, Qing Wang category:cs.CV published:2016-04-15 summary:The past decade has witnessed the rapid development of feature representationlearning and distance metric learning, whereas the two steps are oftendiscussed separately. To explore their interaction, this work proposes anend-to-end learning framework called DARI, i.e. Distance metric AndRepresentation Integration, and validates the effectiveness of DARI in thechallenging task of person verification. Given the training images annotatedwith the labels, we first produce a large number of triplet units, and each onecontains three images, i.e. one person and the matched/mismatch references. Foreach triplet unit, the distance disparity between the matched pair and themismatched pair tends to be maximized. We solve this objective by building adeep architecture of convolutional neural networks. In particular, theMahalanobis distance matrix is naturally factorized as one top fully-connectedlayer that is seamlessly integrated with other bottom layers representing theimage feature. The image feature and the distance metric can be thussimultaneously optimized via the one-shot backward propagation. On severalpublic datasets, DARI shows very promising performance on re-identifyingindividuals cross cameras against various challenges, and outperforms otherstate-of-the-art approaches.
arxiv-17700-225 | Unsupervised single-particle deep classification via statistical manifold learning | http://arxiv.org/pdf/1604.04539v1.pdf | author:Jiayi Wu, Yongbei Ma, Charles Condgon, Bevin Brett, Shuobing Chen, Qi Ouyang, Youdong Mao category:cs.CV q-bio.QM published:2016-04-15 summary:Structural heterogeneity in single-particle images presents a major challengefor high-resolution cryo-electron microscopy (cryo-EM) structure determination.Here we introduce a statistical manifold learning approach for unsupervisedsingle-particle deep classification. When optimized for Intel high-performancecomputing (HPC) processors, our approach can generate thousands ofreference-free class averages within several hours from hundreds of thousandsof single-particle cryo-EM images. Deep classification thus assists incomputational purification of single-particle datasets for high-resolutionreconstruction.
arxiv-17700-226 | Learning Temporal Regularity in Video Sequences | http://arxiv.org/pdf/1604.04574v1.pdf | author:Mahmudul Hasan, Jonghyun Choi, Jan Neumann, Amit K. Roy-Chowdhury, Larry S. Davis category:cs.CV published:2016-04-15 summary:Perceiving meaningful activities in a long video sequence is a challengingproblem due to ambiguous definition of 'meaningfulness' as well as clutters inthe scene. We approach this problem by learning a generative model for regularmotion patterns, termed as regularity, using multiple sources with very limitedsupervision. Specifically, we propose two methods that are built upon theautoencoders for their ability to work with little to no supervision. We firstleverage the conventional handcrafted spatio-temporal local features and learna fully connected autoencoder on them. Second, we build a fully convolutionalfeed-forward autoencoder to learn both the local features and the classifiersas an end-to-end learning framework. Our model can capture the regularitiesfrom multiple datasets. We evaluate our methods in both qualitative andquantitative ways - showing the learned regularity of videos in various aspectsand demonstrating competitive performance on anomaly detection datasets as anapplication.
arxiv-17700-227 | On deterministic conditions for subspace clustering under missing data | http://arxiv.org/pdf/1604.04615v1.pdf | author:Wenqi Wang, Shuchin Aeron, Vaneet Aggarwal category:cs.IT math.IT stat.ML published:2016-04-15 summary:In this paper we present deterministic analysis of sufficient conditions forsparse subspace clustering under missing data, when data is assumed to comefrom a Union of Subspaces (UoS) model. In this context we consider two cases,namely Case I when all the points are sampled at the same co-ordinates, andCase II when points are sampled at different locations. We show that resultsfor Case I directly follow from several existing results in the literature,while results for Case II are not as straightforward and we provide a set ofdual conditions under which, perfect clustering holds true. We provideextensive set of simulation results for clustering as well as completion ofdata under missing entries, under the UoS model. Our experimental resultsindicate that in contrast to the full data case, accurate clustering does notimply accurate subspace identification and completion, indicating the naturalorder of relative hardness of these problems.
arxiv-17700-228 | Invariant feature extraction from event based stimuli | http://arxiv.org/pdf/1604.04327v2.pdf | author:Thusitha N. Chandrapala, Bertram E. Shi category:cs.CV published:2016-04-15 summary:We propose a novel architecture, the event-based GASSOM for learning andextracting invariant representations from event streams originating fromneuromorphic vision sensors. The framework is inspired by feed-forward corticalmodels for visual processing. The model, which is based on the concepts ofsparsity and temporal slowness, is able to learn feature extractors thatresemble neurons in the primary visual cortex. Layers of units in the proposedmodel can be cascaded to learn feature extractors with different levels ofcomplexity and selectivity. We explore the applicability of the framework onreal world tasks by using the learned network for object recognition. Theproposed model achieve higher classification accuracy compared to otherstate-of-the-art event based processing methods. Our results also demonstratethe generality and robustness of the method, as the recognizers for differentdata sets and different tasks all used the same set of learned featuredetectors, which were trained on data collected independently of the testingdata.
arxiv-17700-229 | A short note on extension theorems and their connection to universal consistency in machine learning | http://arxiv.org/pdf/1604.04505v1.pdf | author:Andreas Christmann, Florian Dumpert, Dao-Hong Xiang category:stat.ML cs.LG published:2016-04-15 summary:Statistical machine learning plays an important role in modern statistics andcomputer science. One main goal of statistical machine learning is to provideuniversally consistent algorithms, i.e., the estimator converges in probabilityor in some stronger sense to the Bayes risk or to the Bayes decision function.Kernel methods based on minimizing the regularized risk over a reproducingkernel Hilbert space (RKHS) belong to these statistical machine learningmethods. It is in general unknown which kernel yields optimal results for aparticular data set or for the unknown probability measure. Hence variouskernel learning methods were proposed to choose the kernel and therefore alsoits RKHS in a data adaptive manner. Nevertheless, many practitioners often usethe classical Gaussian RBF kernel or certain Sobolev kernels with good success.The goal of this short note is to offer one possible theoretical explanationfor this empirical fact.
arxiv-17700-230 | Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN | http://arxiv.org/pdf/1604.04378v1.pdf | author:Shengxian Wan, Yanyan Lan, Jun Xu, Jiafeng Guo, Liang Pang, Xueqi Cheng category:cs.CL cs.AI cs.LG cs.NE published:2016-04-15 summary:Semantic matching, which aims to determine the matching degree between twotexts, is a fundamental problem for many NLP applications. Recently, deeplearning approach has been applied to this problem and significant improvementshave been achieved. In this paper, we propose to view the generation of theglobal interaction between two texts as a recursive process: i.e. theinteraction of two texts at each position is a composition of the interactionsbetween their prefixes as well as the word level interaction at the currentposition. Based on this idea, we propose a novel deep architecture, namelyMatch-SRNN, to model the recursive matching structure. Firstly, a tensor isconstructed to capture the word level interactions. Then a spatial RNN isapplied to integrate the local interactions recursively, with importancedetermined by four types of gates. Finally, the matching score is calculatedbased on the global interaction. We show that, after degenerated to the exactmatching scenario, Match-SRNN can approximate the dynamic programming processof longest common subsequence. Thus, there exists a clear interpretation forMatch-SRNN. Our experiments on two semantic matching tasks showed theeffectiveness of Match-SRNN, and its ability of visualizing the learnedmatching structure.
arxiv-17700-231 | Positive Definite Estimation of Large Covariance Matrix Using Generalized Nonconvex Penalties | http://arxiv.org/pdf/1604.04348v1.pdf | author:Fei Wen, Yuan Yang, Peilin Liu, Robert C. Qiu category:cs.IT cs.LG math.IT stat.ML published:2016-04-15 summary:This work addresses the issue of large covariance matrix estimation inhigh-dimensional statistical analysis. Recently, improved iterative algorithmswith positive-definite guarantee have been developed. However, these algorithmscannot be directly extended to use a nonconvex penalty for sparsity inducing.Generally, a nonconvex penalty has the capability of ameliorating the biasproblem of the popular convex lasso penalty, and thus is more advantageous. Inthis work, we propose a class of positive-definite covariance estimators usinggeneralized nonconvex penalties. We develop a first-order algorithm based onthe alternating direction method (ADM) framework to solve the nonconvexoptimization problem efficiently. The convergence of the proposed algorithm hasbeen proved. Further, the statistical properties of the new estimators havebeen analyzed for generalized nonconvex penalties. Moreover, extension of theproposed algorithm to covariance estimation from sketched measurements has beenconsidered. The performances of the proposed estimators have been demonstratedby both a simulation study and a gene clustering example for tumor tissues.
arxiv-17700-232 | Tracking Human-like Natural Motion Using Deep Recurrent Neural Networks | http://arxiv.org/pdf/1604.04528v1.pdf | author:Youngbin Park, Sungphill Moon, Il Hong Suh category:cs.CV cs.LG cs.NE cs.RO published:2016-04-15 summary:Kinect skeleton tracker is able to achieve considerable human body trackingperformance in convenient and a low-cost manner. However, The tracker oftencaptures unnatural human poses such as discontinuous and vibrated motions whenself-occlusions occur. A majority of approaches tackle this problem by usingmultiple Kinect sensors in a workspace. Combination of the measurements fromdifferent sensors is then conducted in Kalman filter framework or optimizationproblem is formulated for sensor fusion. However, these methods usually requireheuristics to measure reliability of measurements observed from each Kinectsensor. In this paper, we developed a method to improve Kinect skeleton usingsingle Kinect sensor, in which supervised learning technique was employed tocorrect unnatural tracking motions. Specifically, deep recurrent neuralnetworks were used for improving joint positions and velocities of Kinectskeleton, and three methods were proposed to integrate the refined positionsand velocities for further enhancement. Moreover, we suggested a novel measureto evaluate naturalness of captured motions. We evaluated the proposed approachby comparison with the ground truth obtained using a commercial opticalmaker-based motion capture system.
arxiv-17700-233 | CNN-RNN: A Unified Framework for Multi-label Image Classification | http://arxiv.org/pdf/1604.04573v1.pdf | author:Jiang Wang, Yi Yang, Junhua Mao, Zhiheng Huang, Chang Huang, Wei Xu category:cs.CV cs.LG cs.NE published:2016-04-15 summary:While deep convolutional neural networks (CNNs) have shown a great success insingle-label image classification, it is important to note that real worldimages generally contain multiple labels, which could correspond to differentobjects, scenes, actions and attributes in an image. Traditional approaches tomulti-label image classification learn independent classifiers for eachcategory and employ ranking or thresholding on the classification results.These techniques, although working well, fail to explicitly exploit the labeldependencies in an image. In this paper, we utilize recurrent neural networks(RNNs) to address this problem. Combined with CNNs, the proposed CNN-RNNframework learns a joint image-label embedding to characterize the semanticlabel dependency as well as the image-label relevance, and it can be trainedend-to-end from scratch to integrate both information in a unified framework.Experimental results on public benchmark datasets demonstrate that the proposedarchitecture achieves better performance than the state-of-the-art multi-labelclassification model
arxiv-17700-234 | StalemateBreaker: A Proactive Content-Introducing Approach to Automatic Human-Computer Conversation | http://arxiv.org/pdf/1604.04358v1.pdf | author:Xiang Li, Lili Mou, Rui Yan, Ming Zhang category:cs.CL cs.AI cs.IR published:2016-04-15 summary:Existing open-domain human-computer conversation systems are typicallypassive: they either synthesize or retrieve a reply provided a human-issuedutterance. It is generally presumed that humans should take the role to leadthe conversation and introduce new content when a stalemate occurs, and thatthe computer only needs to "respond." In this paper, we proposeStalemateBreaker, a conversation system that can proactively introduce newcontent when appropriate. We design a pipeline to determine when, what, and howto introduce new content during human-computer conversation. We further proposea novel reranking algorithm Bi-PageRank-HITS to enable rich interaction betweenconversation context and candidate replies. Experiments show that both thecontent-introducing approach and the reranking algorithm are effective. Ourfull StalemateBreaker model outperforms a state-of-the-practice conversationsystem by +14.4% p@1 when a stalemate occurs.
arxiv-17700-235 | Accessing accurate documents by mining auxiliary document information | http://arxiv.org/pdf/1604.04558v1.pdf | author:Jinju Joby, Jyothi Korra category:cs.IR cs.AI cs.LG published:2016-04-15 summary:Earlier techniques of text mining included algorithms like k-means, NaiveBayes, SVM which classify and cluster the text document for mining relevantinformation about the documents. The need for improving the mining techniqueshas us searching for techniques using the available algorithms. This paperproposes one technique which uses the auxiliary information that is presentinside the text documents to improve the mining. This auxiliary information canbe a description to the content. This information can be either useful orcompletely useless for mining. The user should assess the worth of theauxiliary information before considering this technique for text mining. Inthis paper, a combination of classical clustering algorithms is used to minethe datasets. The algorithm runs in two stages which carry out mining atdifferent levels of abstraction. The clustered documents would then beclassified based on the necessary groups. The proposed technique is aimed atimproved results of document clustering.
arxiv-17700-236 | Long-term Temporal Convolutions for Action Recognition | http://arxiv.org/pdf/1604.04494v1.pdf | author:GÃ¼l Varol, Ivan Laptev, Cordelia Schmid category:cs.CV published:2016-04-15 summary:Typical human actions last several seconds and exhibit characteristicspatio-temporal structure. Recent methods attempt to capture this structure andlearn action representations with convolutional neural networks. Suchrepresentations, however, are typically learned at the level of a few videoframes failing to model actions at their full temporal extent. In this work welearn video representations using neural networks with long-term temporalconvolutions (LTC). We demonstrate that LTC-CNN models with increased temporalextents improve the accuracy of action recognition. We also study the impact ofdifferent low-level representations, such as raw values of video pixels andoptical flow vector fields and demonstrate the importance of high-qualityoptical flow estimation for learning accurate action models. We reportstate-of-the-art results on two challenging benchmarks for human actionrecognition UCF101 (92.7%) and HMDB51 (67.2%).
arxiv-17700-237 | A Network-based End-to-End Trainable Task-oriented Dialogue System | http://arxiv.org/pdf/1604.04562v2.pdf | author:Tsung-Hsien Wen, David Vandyke, Nikola Mrksic, Milica Gasic, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, Steve Young category:cs.CL cs.AI cs.NE stat.ML published:2016-04-15 summary:Teaching machines to accomplish tasks by conversing naturally with humans ischallenging. Currently, developing task-oriented dialogue systems requirescreating multiple components and typically this involves either a large amountof handcrafting, or acquiring labelled datasets and solving a statisticallearning problem for each component. In this work we introduce a neuralnetwork-based text-in, text-out end-to-end trainable dialogue system along witha new way of collecting task-oriented dialogue data based on a novel pipe-linedWizard-of-Oz framework. This approach allows us to develop dialogue systemseasily and without making too many assumptions about the task at hand. Theresults show that the model can converse with human subjects naturally whilsthelping them to accomplish tasks in a restaurant search domain.
arxiv-17700-238 | Parallelizing Word2Vec in Shared and Distributed Memory | http://arxiv.org/pdf/1604.04661v1.pdf | author:Shihao Ji, Nadathur Satish, Sheng Li, Pradeep Dubey category:cs.DC cs.CL stat.ML published:2016-04-15 summary:Word2Vec is a widely used algorithm for extracting low-dimensional vectorrepresentations of words. It generated considerable excitement in the machinelearning and natural language processing (NLP) communities recently due to itsexceptional performance in many NLP applications such as named entityrecognition, sentiment analysis, machine translation and question answering.State-of-the-art algorithms including those by Mikolov et al. have beenparallelized for multi-core CPU architectures but are based on vector-vectoroperations that are memory-bandwidth intensive and do not efficiently usecomputational resources. In this work, we improve reuse of various datastructures in the algorithm through the use of minibatching, hence allowing usto express the problem using matrix multiply operations. We also exploredifferent techniques to parallelize word2vec computation across nodes in acompute cluster, and demonstrate good strong scalability up to 32 nodes. Incombination, these techniques allow us to scale up the computation nearlinearly across cores and nodes, and process hundreds of millions of words persecond, which is the fastest word2vec implementation to the best of ourknowledge.
arxiv-17700-239 | Estimation of low rank density matrices: bounds in Schatten norms and other distances | http://arxiv.org/pdf/1604.04600v1.pdf | author:Dong Xia, Vladimir Koltchinskii category:stat.ML math.ST stat.TH published:2016-04-15 summary:Let ${\mathcal S}_m$ be the set of all $m\times m$ density matrices(Hermitian positively semi-definite matrices of unit trace). Consider a problemof estimation of an unknown density matrix $\rho\in {\mathcal S}_m$ based onoutcomes of $n$ measurements of observables $X_1,\dots, X_n\in {\mathbb H}_m$(${\mathbb H}_m$ being the space of $m\times m$ Hermitian matrices) for aquantum system identically prepared $n$ times in state $\rho.$ Outcomes$Y_1,\dots, Y_n$ of such measurements could be described by a trace regressionmodel in which ${\mathbb E}_{\rho}(Y_jX_j)={\rm tr}(\rho X_j), j=1,\dots, n.$The design variables $X_1,\dots, X_n$ are often sampled at random from theuniform distribution in an orthonormal basis $\{E_1,\dots, E_{m^2}\}$ of${\mathbb H}_m$ (such as Pauli basis). The goal is to estimate the unknowndensity matrix $\rho$ based on the data $(X_1,Y_1), \dots, (X_n,Y_n).$ Let $$\hat Z:=\frac{m^2}{n}\sum_{j=1}^n Y_j X_j $$ and let $\check \rho$ be theprojection of $\hat Z$ onto the convex set ${\mathcal S}_m$ of densitymatrices. It is shown that for estimator $\check \rho$ the minimax lower boundsin classes of low rank density matrices (established earlier) are attained uplogarithmic factors for all Schatten $p$-norm distances, $p\in [1,\infty]$ andfor Bures version of quantum Hellinger distance. Moreover, for a slightlymodified version of estimator $\check \rho$ the same property holds also forquantum relative entropy (Kullback-Leibler) distance between density matrices.
arxiv-17700-240 | Unsupervised Image Segmentation using the Deffuant-Weisbuch Model from Social Dynamics | http://arxiv.org/pdf/1604.04393v2.pdf | author:Subhradeep Kayal category:cs.CV published:2016-04-15 summary:Unsupervised image segmentation algorithms aim at identifying disjointhomogeneous regions in an image, and have been subject to considerableattention in the machine vision community. In this paper, a popular theoreticalmodel with it's origins in statistical physics and social dynamics, known asthe Deffuant-Weisbuch model, is applied to the image segmentation problem. TheDeffuant-Weisbuch model has been found to be useful in modelling the evolutionof a closed system of interacting agents characterised by their opinions orbeliefs, leading to the formation of clusters of agents who share a similaropinion or belief at steady state. In the context of image segmentation, thispaper considers a pixel as an agent and it's colour property as it's opinion,with opinion updates as per the Deffuant-Weisbuch model. Apart from applyingthe basic model to image segmentation, this paper incorporates adjacency andneighbourhood information in the model, which factors in the local similarityand smoothness properties of images. Convergence is reached when the number ofunique pixel opinions, i.e., the number of colour centres, matches thepre-specified number of clusters. Experiments are performed on a set of imagesfrom the Berkeley Image Segmentation Dataset and the results are analysed bothqualitatively and quantitatively, which indicate that this simple and intuitivemethod is promising for image segmentation. To the best of the knowledge of theauthor, this is the first work where a theoretical model from statisticalphysics and social dynamics has been successfully applied to image processing.
arxiv-17700-241 | The Chow Form of the Essential Variety in Computer Vision | http://arxiv.org/pdf/1604.04372v1.pdf | author:Gunnar FlÃ¸ystad, Joe Kileel, Giorgio Ottaviani category:math.AG cs.CV math.AC published:2016-04-15 summary:The Chow form of the essential variety in computer vision is calculated. Ourderivation uses secant varieties, Ulrich sheaves and representation theory.Numerical experiments show that our formula can detect noisy pointcorrespondences between two images.
arxiv-17700-242 | Estimating parameters of nonlinear systems using the elitist particle filter based on evolutionary strategies | http://arxiv.org/pdf/1604.04198v2.pdf | author:Christian Huemmer, Christian Hofmann, Roland Maas, Walter Kellermann category:stat.ML published:2016-04-14 summary:In this article, we present the elitist particle filter based on evolutionarystrategies (EPFES) as an efficient approach for nonlinear systemidentification. The EPFES is derived from the frequently-employed state-spacemodel, where the relevant information of the nonlinear system is captured by anunknown state vector. Similar to classical particle filtering, the EPFESconsists of a set of particles and respective weights which represent differentrealizations of the latent state vector and their likelihood of being thesolution of the optimization problem. As main innovation, the EPFES includes anevolutionary elitist-particle selection which combines long-term information(based on recursively-calculated particle weights) with instantaneous samplingfrom an approximated continuous posterior distribution. In this article, theEPFES is shown to be a generalization of the widely-used Gaussian particlefilter and thus evaluated with respect to the latter for two completelydifferent scenarios: First, we consider the so-called univariate nonstationarygrowth model with time-variant latent state variable, where the evolutionaryselection of elitist particles is evaluated for non-recursively calculatedparticle weights. Second, the problem of nonlinear acoustic echo cancellationis addressed in a simulated scenario with speech as input signal: By usinglong-term fitness measures, we highlight the efficacy of the well-generalizingEPFES in estimating the nonlinear system even for large search spaces. Finally,we illustrate similarities between the EPFES and evolutionary algorithms tooutline future improvements by fusing the achievements of both fields ofresearch.
arxiv-17700-243 | Multi-Source Multi-View Clustering via Discrepancy Penalty | http://arxiv.org/pdf/1604.04029v2.pdf | author:Weixiang Shao, Jiawei Zhang, Lifang He, Philip S. Yu category:cs.LG published:2016-04-14 summary:With the advance of technology, entities can be observed in multiple views.Multiple views containing different types of features can be used forclustering. Although multi-view clustering has been successfully applied inmany applications, the previous methods usually assume the complete instancemapping between different views. In many real-world applications, informationcan be gathered from multiple sources, while each source can contain multipleviews, which are more cohesive for learning. The views under the same sourceare usually fully mapped, but they can be very heterogeneous. Moreover, themappings between different sources are usually incomplete and partiallyobserved, which makes it more difficult to integrate all the views acrossdifferent sources. In this paper, we propose MMC (Multi-source Multi-viewClustering), which is a framework based on collective spectral clustering witha discrepancy penalty across sources, to tackle these challenges. MMC hasseveral advantages compared with other existing methods. First, MMC can dealwith incomplete mapping between sources. Second, it considers the disagreementsbetween sources while treating views in the same source as a cohesive set.Third, MMC also tries to infer the instance similarities across sources toenhance the clustering performance. Extensive experiments conducted onreal-world data demonstrate the effectiveness of the proposed approach.
arxiv-17700-244 | Deep Residual Networks with Exponential Linear Unit | http://arxiv.org/pdf/1604.04112v1.pdf | author:Anish Shah, Eashan Kadam, Hena Shah, Sameer Shinde category:cs.CV published:2016-04-14 summary:Very deep convolutional neural networks introduced new problems likevanishing gradient and degradation. The recent successful contributions towardssolving these problems are Residual and Highway Networks. These networksintroduce skip connections that allow the information (from the input or thoselearned in earlier layers) to flow more into the deeper layers. These very deepmodels have lead to a considerable decrease in test errors, on benchmarks likeImageNet and COCO. In this paper, we propose the use of exponential linear unitinstead of the combination of ReLU and Batch Normalization in ResidualNetworks. We show that this not only speeds up learning in Residual Networksbut also improves the accuracy as the depth increases. It improves the testerror on almost all data sets, like CIFAR-10 and CIFAR-100
arxiv-17700-245 | Filling in the details: Perceiving from low fidelity images | http://arxiv.org/pdf/1604.04125v1.pdf | author:Farahnaz Ahmed Wick, Michael L. Wick, Marc Pomplun category:cs.CV cs.LG cs.NE published:2016-04-14 summary:Humans perceive their surroundings in great detail even though most of ourvisual field is reduced to low-fidelity color-deprived (e.g. dichromatic) inputby the retina. In contrast, most deep learning architectures arecomputationally wasteful in that they consider every part of the input whenperforming an image processing task. Yet, the human visual system is able toperform visual reasoning despite having only a small fovea of high visualacuity. With this in mind, we wish to understand the extent to whichconnectionist architectures are able to learn from and reason with low acuity,distorted inputs. Specifically, we train autoencoders to generate full-detailimages from low-detail "foveations" of those images and then measure theirability to reconstruct the full-detail images from the foveated versions. Byvarying the type of foveation, we can study how well the architectures can copewith various types of distortion. We find that the autoencoder compensates forlower detail by learning increasingly global feature functions. In many cases,the learnt features are suitable for reconstructing the original full-detailimage. For example, we find that the networks accurately perceive color in theperiphery, even when 75\% of the input is achromatic.
arxiv-17700-246 | Towards Automated Melanoma Screening: Proper Computer Vision & Reliable Results | http://arxiv.org/pdf/1604.04024v3.pdf | author:Michel Fornaciali, Micael Carvalho, FlÃ¡via Vasques Bittencourt, Sandra Avila, Eduardo Valle category:cs.CV published:2016-04-14 summary:In this paper we survey, analyze and criticize current art on automatedmelanoma screening, reimplementing a baseline technique, and proposing twonovel ones. Melanoma, although highly curable when detected early, ends as oneof the most dangerous types of cancer, due to delayed diagnosis and treatment.Its incidence is soaring, much faster than the number of trained professionalsable to diagnose it. Automated screening appears as an alternative to make themost of those professionals, focusing their time on the patients at risk whilesafely discharging the other patients. However, the potential of automatedmelanoma diagnosis is currently unfulfilled, due to the emphasis of currentliterature on outdated computer vision models. Even more problematic is theirreproducibility of current art. We show how streamlined pipelines based uponcurrent Computer Vision outperform conventional models - a model based on anadvanced bags of words reaches an AUC of 84.6%, and a model based on deepneural networks reaches 89.3%, while the baseline (a classical bag of words)stays at 81.2%. We also initiate a dialog to improve reproducibility in ourcommunity
arxiv-17700-247 | An Improved Discrete Bat Algorithm for Symmetric and Asymmetric Traveling Salesman Problems | http://arxiv.org/pdf/1604.04138v1.pdf | author:Eneko Osaba, Xin-She Yang, Fernando Diaz, Pedro Lopez-Garcia, Roberto Carballedo category:cs.NE cs.AI math.OC 78M32 published:2016-04-14 summary:Bat algorithm is a population metaheuristic proposed in 2010 which is basedon the echolocation or bio-sonar characteristics of microbats. Since its firstimplementation, the bat algorithm has been used in a wide range of fields. Inthis paper, we present a discrete version of the bat algorithm to solve thewell-known symmetric and asymmetric traveling salesman problems. In addition,we propose an improvement in the basic structure of the classic bat algorithm.To prove that our proposal is a promising approximation method, we havecompared its performance in 37 instances with the results obtained by fivedifferent techniques: evolutionary simulated annealing, genetic algorithm, anisland based distributed genetic algorithm, a discrete firefly algorithm and animperialist competitive algorithm. In order to obtain fair and rigorouscomparisons, we have conducted three different statistical tests along thepaper: the Student's $t$-test, the Holm's test, and the Friedman test. We havealso compared the convergence behaviour shown by our proposal with the onesshown by the evolutionary simulated annealing, and the discrete fireflyalgorithm. The experimentation carried out in this study has shown that thepresented improved bat algorithm outperforms significantly all the otheralternatives in most of the cases.
arxiv-17700-248 | On Reducing the Number of Visual Words in the Bag-of-Features Representation | http://arxiv.org/pdf/1604.04142v1.pdf | author:Giuseppe Amato, Fabrizio Falchi, Claudio Gennaro category:cs.CV cs.IR published:2016-04-14 summary:A new class of applications based on visual search engines are emerging,especially on smart-phones that have evolved into powerful tools for processingimages and videos. The state-of-the-art algorithms for large visual contentrecognition and content based similarity search today use the "Bag of Features"(BoF) or "Bag of Words" (BoW) approach. The idea, borrowed from text retrieval,enables the use of inverted files. A very well known issue with this approachis that the query images, as well as the stored data, are described withthousands of words. This poses obvious efficiency problems when using invertedfiles to perform efficient image matching. In this paper, we propose andcompare various techniques to reduce the number of words describing an image toimprove efficiency and we study the effects of this reduction on effectivenessin landmark recognition and retrieval scenarios. We show that very relevantimprovement in performance are achievable still preserving the advantages ofthe BoF base approach.
arxiv-17700-249 | 1-bit Matrix Completion: PAC-Bayesian Analysis of a Variational Approximation | http://arxiv.org/pdf/1604.04191v1.pdf | author:Vincent Cottet, Pierre Alquier category:stat.ML published:2016-04-14 summary:Due to challenging applications such as collaborative filtering, the matrixcompletion problem has been widely studied in the past few years. Differentapproaches rely on different structure assumptions on the matrix in hand. Here,we focus on the completion of a (possibly) low-rank matrix with binary entries,the so-called 1-bit matrix completion problem. Our approach relies on toolsfrom machine learning theory: empirical risk minimization and its convexrelaxations. We propose an algorithm to compute a variational approximation ofthe pseudo-posterior. Thanks to the convex relaxation, the correspondingminimization problem is bi-convex, and thus the method behaves well inpractice. We also study the performance of this variational approximationthrough PAC-Bayesian learning bounds. On the contrary to previous works thatfocused on upper bounds on the estimation error of M with various matrix norms,we are able to derive from this analysis a PAC bound on the prediction error ofour algorithm. We focus essentially on convex relaxation through the hinge loss, for whichwe present the complete analysis, a complete simulation study and a test on theMovieLens data set. However, we also discuss a variational approximation todeal with the logistic loss.
arxiv-17700-250 | Modeling Electrical Daily Demand in Presence of PHEVs in Smart Grids with Supervised Learning | http://arxiv.org/pdf/1604.04213v1.pdf | author:Marco Pellegrini, Farshad Rassaei category:cs.LG published:2016-04-14 summary:Replacing a portion of current light duty vehicles (LDV) with plug-in hybridelectric vehicles (PHEVs) offers the possibility to reduce the dependence onpetroleum fuels together with environmental and economic benefits. The chargingactivity of PHEVs will certainly introduce new load to the power grid. In theframework of the development of a smarter grid, the primary focus of thepresent study is to propose a model for the electrical daily demand in presenceof PHEVs charging. Expected PHEV demand is modeled by the PHEV charging timeand the starting time of charge according to real world data. A normaldistribution for starting time of charge is assumed. Several distributions forcharging time are considered: uniform distribution, Gaussian with positivesupport, Rician distribution and a non-uniform distribution coming from drivingpatterns in real-world data. We generate daily demand profiles by usingreal-world residential profiles throughout 2014 in the presence of differentexpected PHEV demand models. Support vector machines (SVMs), a set ofsupervised machine learning models, are employed in order to find the bestmodel to fit the data. SVMs with radial basis function (RBF) and polynomialkernels were tested. Model performances are evaluated by means of mean squarederror (MSE) and mean absolute percentage error (MAPE). Best results areobtained with RBF kernel: maximum (worst) values for MSE and MAPE were about2.89 10-8 and 0.023, respectively.
arxiv-17700-251 | Self-taught learning of a deep invariant representation for visual tracking via temporal slowness principle | http://arxiv.org/pdf/1604.04144v1.pdf | author:Jason Kuen, Kian Ming Lim, Chin Poo Lee category:cs.CV cs.LG cs.NE published:2016-04-14 summary:Visual representation is crucial for a visual tracking method's performances.Conventionally, visual representations adopted in visual tracking rely onhand-crafted computer vision descriptors. These descriptors were developedgenerically without considering tracking-specific information. In this paper,we propose to learn complex-valued invariant representations from trackedsequential image patches, via strong temporal slowness constraint and stackedconvolutional autoencoders. The deep slow local representations are learnedoffline on unlabeled data and transferred to the observational model of ourproposed tracker. The proposed observational model retains old training samplesto alleviate drift, and collect negative samples which are coherent withtarget's motion pattern for better discriminative tracking. With the learnedrepresentation and online training samples, a logistic regression classifier isadopted to distinguish target from background, and retrained online to adapt toappearance changes. Subsequently, the observational model is integrated into aparticle filter framework to peform visual tracking. Experimental results onvarious challenging benchmark sequences demonstrate that the proposed trackerperforms favourably against several state-of-the-art trackers.
arxiv-17700-252 | Consistently Estimating Markov Chains with Noisy Aggregate Data | http://arxiv.org/pdf/1604.04182v1.pdf | author:Garrett Bernstein, Daniel Sheldon category:cs.LG stat.ML published:2016-04-14 summary:We address the problem of estimating the parameters of a time-homogeneousMarkov chain given only noisy, aggregate data. This arises when a population ofindividuals behave independently according to a Markov chain, but individualsample paths cannot be observed due to limitations of the observation processor the need to protect privacy. Instead, only population-level counts of thenumber of individuals in each state at each time step are available. When thesecounts are exact, a conditional least squares (CLS) estimator is known to beconsistent and asymptotically normal. We initiate the study of method ofmoments estimators for this problem to handle the more realistic case whenobservations are additionally corrupted by noise. We show that CLS can beinterpreted as a simple "plug-in" method of moments estimator. However, whenobservations are noisy, it is not consistent because it fails to account foradditional variance introduced by the noise. We develop a new, simpler methodof moments estimator that bypasses this problem and is consistent under noisyobservations.
arxiv-17700-253 | Optimal Rates For Regularization Of Statistical Inverse Learning Problems | http://arxiv.org/pdf/1604.04054v1.pdf | author:Gilles Blanchard, Nicole MÃ¼cke category:stat.ML published:2016-04-14 summary:We consider a statistical inverse learning problem, where we observe theimage of a function $f$ through a linear operator $A$ at i.i.d. random designpoints $X_i$, superposed with an additive noise. The distribution of the designpoints is unknown and can be very general. We analyze simultaneously the direct(estimation of $Af$) and the inverse (estimation of $f$) learning problems. Inthis general framework, we obtain strong and weak minimax optimal rates ofconvergence (as the number of observations $n$ grows large) for a large classof spectral regularization methods over regularity classes defined throughappropriate source conditions. This improves on or completes previous resultsobtained in related settings. The optimality of the obtained rates is shown notonly in the exponent in $n$ but also in the explicit dependency of the constantfactor in the variance of the noise and the radius of the source condition set.
arxiv-17700-254 | Distribution-Free Predictive Inference For Regression | http://arxiv.org/pdf/1604.04173v1.pdf | author:Jing Lei, Max G'Sell, Alessandro Rinaldo, Ryan J. Tibshirani, Larry Wasserman category:stat.ME math.ST stat.ML stat.TH published:2016-04-14 summary:We develop a general framework for distribution-free predictive inference inregression, using conformal inference. The proposed methodology allowsconstruction of prediction bands for the response variable using any estimatorof the regression function. The resulting prediction band preserves theconsistency properties of the original estimator under standard assumptions,while guaranteeing finite sample marginal coverage even when the assumptions donot hold. We analyze and compare, both empirically and theoretically, two majorvariants of our conformal procedure: the full conformal inference and splitconformal inference, along with a related jackknife method. These methods offerdifferent tradeoffs between statistical accuracy (length of resultingprediction intervals) and computational efficiency. As extensions, we develop amethod for constructing valid in-sample prediction intervals calledrank-one-out conformal inference, which has essentially the same computationalefficiency as split conformal inference. We also describe an extension of ourprocedures for producing prediction bands with varying local width, in order toadapt to heteroskedascity in the data distribution. Lastly, we propose amodel-free notion of variable importance, called leave-one-covariate-out orLOCO inference. Accompanying our paper is an R package conformalInference thatimplements all of the proposals we have introduced. In the spirit ofreproducibility, all empirical results in this paper can be easily(re)generated using this package.
arxiv-17700-255 | Learning Visual Storylines with Skipping Recurrent Neural Networks | http://arxiv.org/pdf/1604.04279v1.pdf | author:Gunnar A. Sigurdsson, Xinlei Chen, Abhinav Gupta category:cs.CV published:2016-04-14 summary:What does a typical visit to Paris look like? Do people first take photos ofthe Louvre and then the Eiffel Tower? Can we visually model a temporal eventlike "Paris Vacation" using current frameworks? In this paper, we explore howwe can automatically learn the temporal aspects, or storylines of visualconcepts from web data. Previous attempts focus on consecutive image-to-imagetransitions and are unsuccessful at recovering the long-term underlying story.Our novel Skipping Recurrent Neural Network (S-RNN) model, does not attempt topredict each and every data point in the sequence, like classic RNNs. Rather,S-RNN uses a framework that skips through the images in the photo stream toexplore the space of all ordered subsets of the albums via an efficientsampling procedure. This approach reduces the negative impact of strongshort-term correlations, and recovers the latent story more accurately. We showhow our learned storylines can be used to analyze, predict, and summarize photoalbums from Flickr. Our experimental results provide strong qualitative andquantitative evidence that S-RNN is significantly better than other candidatemethods such as LSTMs on learning long-term correlations and recovering latentstorylines. Moreover, we show how storylines can help machines betterunderstand and summarize photo streams by inferring a brief personalized storyof each individual album.
arxiv-17700-256 | Object Detection from Video Tubelets with Convolutional Neural Networks | http://arxiv.org/pdf/1604.04053v1.pdf | author:Kai Kang, Wanli Ouyang, Hongsheng Li, Xiaogang Wang category:cs.CV published:2016-04-14 summary:Deep Convolution Neural Networks (CNNs) have shown impressive performance invarious vision tasks such as image classification, object detection andsemantic segmentation. For object detection, particularly in still images, theperformance has been significantly increased last year thanks to powerful deepnetworks (e.g. GoogleNet) and detection frameworks (e.g. Regions with CNNfeatures (R-CNN)). The lately introduced ImageNet task on object detection fromvideo (VID) brings the object detection task into the video domain, in whichobjects' locations at each frame are required to be annotated with boundingboxes. In this work, we introduce a complete framework for the VID task basedon still-image object detection and general object tracking. Their relationsand contributions in the VID task are thoroughly studied and evaluated. Inaddition, a temporal convolution network is proposed to incorporate temporalinformation to regularize the detection results and shows its effectiveness forthe task.
arxiv-17700-257 | Unsupervised Nonlinear Spectral Unmixing based on a Multilinear Mixing Model | http://arxiv.org/pdf/1604.04293v1.pdf | author:Qi Wei, Marcus Chen, Jean-Yves Tourneret, Simon Godsill category:cs.CV published:2016-04-14 summary:In the community of remote sensing, nonlinear mixing models have recentlyreceived particular attention in hyperspectral image processing. In this paper,we present a novel nonlinear spectral unmixing method following the recentmultilinear mixing model of [1], which includes an infinite number of termsrelated to interactions between different endmembers. The proposed unmixingmethod is unsupervised in the sense that the endmembers are estimated jointlywith the abundances and other parameters of interest, i.e., the transitionprobability of undergoing further interactions. Non-negativity and sum-to oneconstraints are imposed on abundances while only nonnegativity is consideredfor endmembers. The resulting unmixing problem is formulated as a constrainednonlinear optimization problem, which is solved by a block coordinate descentstrategy, consisting of updating the endmembers, abundances and transitionprobability iteratively. The proposed method is evaluated and compared withlinear unmixing methods for synthetic and real hyperspectral datasets acquiredby the AVIRIS sensor. The advantage of using non-linear unmixing as opposed tolinear unmixing is clearly shown in these examples.
arxiv-17700-258 | Multi-Oriented Text Detection with Fully Convolutional Networks | http://arxiv.org/pdf/1604.04018v2.pdf | author:Zheng Zhang, Chengquan Zhang, Wei Shen, Cong Yao, Wenyu Liu, Xiang Bai category:cs.CV published:2016-04-14 summary:In this paper, we propose a novel approach for text detec- tion in naturalimages. Both local and global cues are taken into account for localizing textlines in a coarse-to-fine pro- cedure. First, a Fully Convolutional Network(FCN) model is trained to predict the salient map of text regions in a holisticmanner. Then, text line hypotheses are estimated by combining the salient mapand character components. Fi- nally, another FCN classifier is used to predictthe centroid of each character, in order to remove the false hypotheses. Theframework is general for handling text in multiple ori- entations, languagesand fonts. The proposed method con- sistently achieves the state-of-the-artperformance on three text detection benchmarks: MSRA-TD500, ICDAR2015 andICDAR2013.
arxiv-17700-259 | Understanding How Image Quality Affects Deep Neural Networks | http://arxiv.org/pdf/1604.04004v2.pdf | author:Samuel Dodge, Lina Karam category:cs.CV published:2016-04-14 summary:Image quality is an important practical challenge that is often overlooked inthe design of machine vision systems. Commonly, machine vision systems aretrained and tested on high quality image datasets, yet in practicalapplications the input images can not be assumed to be of high quality.Recently, deep neural networks have obtained state-of-the-art performance onmany machine vision tasks. In this paper we provide an evaluation of 4state-of-the-art deep neural network models for image classification underquality distortions. We consider five types of quality distortions: blur,noise, contrast, JPEG, and JPEG2000 compression. We show that the existingnetworks are susceptible to these quality distortions, particularly to blur andnoise. These results enable future work in developing deep neural networks thatare more invariant to quality distortions.
arxiv-17700-260 | Deep Feature Based Contextual Model for Object Detection | http://arxiv.org/pdf/1604.04048v1.pdf | author:Wenqing Chu, Deng Cai category:cs.CV published:2016-04-14 summary:Object detection is one of the most active areas in computer vision, whichhas made significant improvement in recent years. Current state-of-the-artobject detection methods mostly adhere to the framework of regions withconvolutional neural network (R-CNN) and only use local appearance featuresinside object bounding boxes. Since these approaches ignore the contextualinformation around the object proposals, the outcome of these detectors maygenerate a semantically incoherent interpretation of the input image. In thispaper, we propose an ensemble object detection system which incorporates thelocal appearance, the contextual information in term of relationships amongobjects and the global scene based contextual feature generated by aconvolutional neural network. The system is formulated as a fully connectedconditional random field (CRF) defined on object proposals and the contextualconstraints among object proposals are modeled as edges naturally. Furthermore,a fast mean field approximation method is utilized to inference in this CRFmodel efficiently. The experimental results demonstrate that our approachachieves a higher mean average precision (mAP) on PASCAL VOC 2007 datasetscompared to the baseline algorithm Faster R-CNN.
arxiv-17700-261 | Fast Parallel Randomized Algorithm for Nonnegative Matrix Factorization with KL Divergence for Large Sparse Datasets | http://arxiv.org/pdf/1604.04026v1.pdf | author:Duy Khuong Nguyen, Tu Bao Ho category:math.OC cs.LG cs.NA published:2016-04-14 summary:Nonnegative Matrix Factorization (NMF) with Kullback-Leibler Divergence(NMF-KL) is one of the most significant NMF problems and equivalent toProbabilistic Latent Semantic Indexing (PLSI), which has been successfullyapplied in many applications. For sparse count data, a Poisson distribution andKL divergence provide sparse models and sparse representation, which describethe random variation better than a normal distribution and Frobenius norm.Specially, sparse models provide more concise understanding of the appearanceof attributes over latent components, while sparse representation providesconcise interpretability of the contribution of latent components overinstances. However, minimizing NMF with KL divergence is much more difficultthan minimizing NMF with Frobenius norm; and sparse models, sparserepresentation and fast algorithms for large sparse datasets are stillchallenges for NMF with KL divergence. In this paper, we propose a fastparallel randomized coordinate descent algorithm having fast convergence forlarge sparse datasets to archive sparse models and sparse representation. Theproposed algorithm's experimental results overperform the current studies' onesin this problem.
arxiv-17700-262 | Variational inference for rare variant detection in deep, heterogeneous next-generation sequencing data | http://arxiv.org/pdf/1604.04280v2.pdf | author:Fan Zhang, Patrick Flaherty category:q-bio.GN stat.ML published:2016-04-14 summary:The detection of rare variants is important for understanding the geneticheterogeneity in mixed samples. Recently, next-generation sequencing (NGS)technologies have enabled the identification of single nucleotide variants(SNVs) in mixed samples with high resolution. Yet, the noise inherent in thebiological processes involved in next-generation sequencing necessitates theuse of statistical methods to identify true rare variants. We propose a novelBayesian statistical model and a variational expectation-maximization (EM)algorithm to estimate non-reference allele frequency (NRAF) and identify SNVsin heterogeneous cell populations. We demonstrate that our variational EMalgorithm has comparable sensitivity and specificity compared with a MarkovChain Monte Carlo (MCMC) sampling inference algorithm, and is morecomputationally efficient on tests of low coverage ($27\times$ and $298\times$)data. Furthermore, we show that our model with a variational EM inferencealgorithm has higher specificity than many state-of-the-art algorithms. In ananalysis of a directed evolution longitudinal yeast data set, we are able toidentify a time-series trend in non-reference allele frequency and detect novelvariants that have not yet been reported. Our model also detects the emergenceof a beneficial variant earlier than was previously shown, and a pair ofconcomitant variants.
arxiv-17700-263 | A Discrete Firefly Algorithm to Solve a Rich Vehicle Routing Problem Modelling a Newspaper Distribution System with Recycling Policy | http://arxiv.org/pdf/1604.04146v1.pdf | author:E. Osaba, Xin-She Yang, F. Diaz, E. Onieva, A. D. Masegosa, A. Perallos category:cs.NE cs.AI math.OC 78M32 published:2016-04-14 summary:A real-world newspaper distribution problem with recycling policy is tackledin this work. In order to meet all the complex restrictions contained in such aproblem, it has been modeled as a rich vehicle routing problem, which can bemore specifically considered as an asymmetric and clustered vehicle routingproblem with simultaneous pickup and deliveries, variable costs and forbiddenpaths (AC-VRP-SPDVCFP). This is the first study of such a problem in theliterature. For this reason, a benchmark composed by 15 instances has been alsoproposed. In the design of this benchmark, real geographical positions havebeen used, located in the province of Bizkaia, Spain. For the proper treatmentof this AC-VRP-SPDVCFP, a discrete firefly algorithm (DFA) has been developed.This application is the first application of the firefly algorithm to any richvehicle routing problem. To prove that the proposed DFA is a promisingtechnique, its performance has been compared with two other well-knowntechniques: an evolutionary algorithm and an evolutionary simulated annealing.Our results have shown that the DFA has outperformed these two classicmeta-heuristics.
arxiv-17700-264 | Learning to Generate Genotypes with Neural Networks | http://arxiv.org/pdf/1604.04153v1.pdf | author:Alexander W. Churchill, Siddharth Sigtia, Chrisantha Fernando category:cs.NE published:2016-04-14 summary:Neural networks and evolutionary computation have a rich intertwined history.They most commonly appear together when an evolutionary algorithm optimises theparameters and topology of a neural network for reinforcement learningproblems, or when a neural network is applied as a surrogate fitness functionto aid the evolutionary optimisation of expensive fitness functions. In thispaper we take a different approach, asking the question of whether a neuralnetwork can be used to provide a mutation distribution for an evolutionaryalgorithm, and what advantages this approach may offer? Two modern neuralnetwork models are investigated, a Denoising Autoencoder modified to producestochastic outputs and the Neural Autoregressive Distribution Estimator.Results show that the neural network approach to learning genotypes is able tosolve many difficult discrete problems, such as MaxSat and HIFF, and regularlyoutperforms other evolutionary techniques.
arxiv-17700-265 | Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex | http://arxiv.org/pdf/1604.03640v1.pdf | author:Qianli Liao, Tomaso Poggio category:cs.LG cs.NE published:2016-04-13 summary:We discuss relations between Residual Networks (ResNet), Recurrent NeuralNetworks (RNNs) and the primate visual cortex. We begin with the observationthat a shallow RNN is exactly equivalent to a very deep ResNet with weightsharing among the layers. A direct implementation of such a RNN, althoughhaving orders of magnitude fewer parameters, leads to a performance similar tothe corresponding ResNet. We propose 1) a generalization of both RNN and ResNetarchitectures and 2) the conjecture that a class of moderately deep RNNs is abiologically-plausible model of the ventral stream in visual cortex. Wedemonstrate the effectiveness of the architectures by testing them on theCIFAR-10 dataset.
arxiv-17700-266 | Online Multi-target Tracking using Recurrent Neural Networks | http://arxiv.org/pdf/1604.03635v1.pdf | author:Anton Milan, Seyed Hamid Rezatofighi, Anthony Dick, Konrad Schindler, Ian Reid category:cs.CV published:2016-04-13 summary:We present a novel approach to online multi-target tracking based onrecurrent neural networks (RNNs). Tracking multiple objects in real-worldscenes involves many challenges, including a) an a-priori unknown andtime-varying number of targets, b) a continuous state estimation of all presenttargets, and c) a discrete combinatorial problem of data association. Mostprevious methods involve complex models that require tedious tuning ofparameters. Here, we propose for the first time, a full end-to-end learningapproach for online multi-target tracking based on deep learning. Existing deeplearning methods are not designed for the above challenges and cannot betrivially applied to the task. Our solution addresses all of the above pointsin a principled way. Experiments on both synthetic and real data showcompetitive results obtained at 300 Hz on a standard CPU, and pave the waytowards future research in this direction.
arxiv-17700-267 | Quantifying mesoscale neuroanatomy using X-ray microtomography | http://arxiv.org/pdf/1604.03629v1.pdf | author:Eva L. Dyer, William Gray Roncal, Hugo L. Fernandes, Doga GÃ¼rsoy, Xianghui Xiao, Joshua T. Vogelstein, Chris Jacobsen, Konrad P. KÃ¶rding, Narayanan Kasthuri category:q-bio.QM cs.CV published:2016-04-13 summary:Common methods for imaging the 3D microstructure of the brain often requireslicing the brain, imaging these slices, and stitching the images backtogether. In contrast, X-rays allow access into centimeter-thick sampleswithout sectioning, providing an unique and largely untapped approach forproducing large 3D mesoscale brain maps. Here we demonstrate the use ofsynchrotron X-ray microtomography ($\mu$CT) for brain science and provide amuch needed toolkit for analyzing the large datasets afforded by this approach.We introduce methods for sample preparation, imaging, and analyzing theresultant neural structures. This pipeline provides methods for automated celldetection and segmentation of the vasculature, in addition to large-scaleanalyses of the spatial statistics of cells and blood vessels. We applied ourmethods to produce dense micron-scale maps of the cells and blood vessels in amillimeter-scale volume of mouse brain with $\mu$CT. Our results demonstratethat X-ray tomography promises rapid reconstructions over brain large volumes,in a way that is complementary to other brain mapping and connectomics efforts.
arxiv-17700-268 | Joint Unsupervised Learning of Deep Representations and Image Clusters | http://arxiv.org/pdf/1604.03628v1.pdf | author:Jianwei Yang, Devi Parikh, Dhruv Batra category:cs.CV cs.LG published:2016-04-13 summary:In this paper, we propose a recurrent framework for joint unsupervisedlearning of deep representations and image clusters. In our framework,successive operations in a clustering algorithm are expressed as steps in arecurrent process, stacked on top of representations output by a ConvolutionalNeural Network (CNN). During training, image clusters and representations areupdated jointly: image clustering is conducted in the forward pass, whilerepresentation learning in the backward pass. Our key idea behind thisframework is that good representations are beneficial to image clustering andclustering results provide supervisory signals to representation learning. Byintegrating two processes into a single model with a unified weighted tripletloss and optimizing it end-to-end, we can obtain not only more powerfulrepresentations, but also more precise image clusters. Extensive experimentsshow that our method outperforms the state-of-the-art on image clusteringacross a variety of image datasets. Moreover, the learned representationsgeneralize well when transferred to other tasks.
arxiv-17700-269 | Dissecting a Social Botnet: Growth, Content and Influence in Twitter | http://arxiv.org/pdf/1604.03627v1.pdf | author:Norah Abokhodair, Daisy Yoo, David W. McDonald category:cs.CY cs.CL cs.SI published:2016-04-13 summary:Social botnets have become an important phenomenon on social media. There aremany ways in which social bots can disrupt or influence online discourse, suchas, spam hashtags, scam twitter users, and astroturfing. In this paper weconsidered one specific social botnet in Twitter to understand how it growsover time, how the content of tweets by the social botnet differ from regularusers in the same dataset, and lastly, how the social botnet may haveinfluenced the relevant discussions. Our analysis is based on a qualitativecoding for approximately 3000 tweets in Arabic and English from the Syriansocial bot that was active for 35 weeks on Twitter before it was shutdown. Wefind that the growth, behavior and content of this particular botnet did notspecifically align with common conceptions of botnets. Further we identifyinteresting aspects of the botnet that distinguish it from regular users.
arxiv-17700-270 | Learning Social Affordance for Human-Robot Interaction | http://arxiv.org/pdf/1604.03692v2.pdf | author:Tianmin Shu, M. S. Ryoo, Song-Chun Zhu category:cs.RO cs.AI cs.CV cs.LG published:2016-04-13 summary:In this paper, we present an approach for robot learning of social affordancefrom human activity videos. We consider the problem in the context ofhuman-robot interaction: Our approach learns structural representations ofhuman-human (and human-object-human) interactions, describing how body-parts ofeach agent move with respect to each other and what spatial relations theyshould maintain to complete each sub-event (i.e., sub-goal). This enables therobot to infer its own movement in reaction to the human body motion, allowingit to naturally replicate such interactions. We introduce the representation of social affordance and propose a generativemodel for its weakly supervised learning from human demonstration videos. Ourapproach discovers critical steps (i.e., latent sub-events) in an interactionand the typical motion associated with them, learning what body-parts should beinvolved and how. The experimental results demonstrate that our Markov ChainMonte Carlo (MCMC) based learning algorithm automatically discoverssemantically meaningful interactive affordance from RGB-D videos, which allowsus to generate appropriate full body motion for an agent.
arxiv-17700-271 | Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks | http://arxiv.org/pdf/1604.03650v1.pdf | author:Junyuan Xie, Ross Girshick, Ali Farhadi category:cs.CV published:2016-04-13 summary:As 3D movie viewing becomes mainstream and Virtual Reality (VR) marketemerges, the demand for 3D contents is growing rapidly. Producing 3D videos,however, remains challenging. In this paper we propose to use deep neuralnetworks for automatically converting 2D videos and images to stereoscopic 3Dformat. In contrast to previous automatic 2D-to-3D conversion algorithms, whichhave separate stages and need ground truth depth map as supervision, ourapproach is trained end-to-end directly on stereo pairs extracted from 3Dmovies. This novel training scheme makes it possible to exploit orders ofmagnitude more data and significantly increases performance. Indeed, Deep3Doutperforms baselines in both quantitative and human subject evaluations.
arxiv-17700-272 | A Novel Method to Study Bottom-up Visual Saliency and its Neural Mechanism | http://arxiv.org/pdf/1604.08426v1.pdf | author:Cheng Chen, Xilin Zhang, Yizhou Wang, Fang Fang category:cs.CV q-bio.NC published:2016-04-13 summary:In this study, we propose a novel method to measure bottom-up saliency mapsof natural images. In order to eliminate the influence of top-down signals,backward masking is used to make stimuli (natural images) subjectivelyinvisible to subjects, however, the bottom-up saliency can still orient thesubjects attention. To measure this orientation/attention effect, we adopt thecueing effect paradigm by deploying discrimination tasks at each location of animage, and measure the discrimination performance variation across the image asthe attentional effect of the bottom-up saliency. Such attentional effects arecombined to construct a final bottomup saliency map. Based on the proposedmethod, we introduce a new bottom-up saliency map dataset of natural images tobenchmark computational models. We compare several state-of-the-art saliencymodels on the dataset. Moreover, the proposed paradigm is applied toinvestigate the neural basis of the bottom-up visual saliency map by analyzingpsychophysical and fMRI experimental results. Our findings suggest that thebottom-up saliency maps of natural images are constructed in V1. It provides astrong scientific evidence to resolve the long standing dispute in neuroscienceabout where the bottom-up saliency map is constructed in human brain.
arxiv-17700-273 | DENSER Cities: A System for Dense Efficient Reconstructions of Cities | http://arxiv.org/pdf/1604.03734v1.pdf | author:Michael Tanner, Pedro Pinies, Lina Maria Paz, Paul Newman category:cs.CV cs.RO published:2016-04-13 summary:This paper is about the efficient generation of dense, colored models ofcity-scale environments from range data and in particular, stereo cameras.Better maps make for better understanding; better understanding leads to betterrobots, but this comes at a cost. The computational and memory requirements oflarge dense models can be prohibitive. We provide the theory and the systemneeded to create city-scale dense reconstructions. To do so, we apply aregularizer over a compressed 3D data structure while dealing with the complexboundary conditions this induces during the data-fusion stage. We show thatonly with these considerations can we swiftly create neat, large, "wellbehaved" reconstructions. We evaluate our system using the KITTI dataset andprovide statistics for the metric errors in all surfaces created compared tothose measured with 3D laser. Our regularizer reduces the median error by 40%in 3.4 km of dense reconstructions with a median accuracy of 6 cm. Forsubjective analysis, we provide a qualitative review of 6.1 km of our densereconstructions in an attached video. These are the largest densereconstructions from a single passive camera we are aware of in the literature.
arxiv-17700-274 | A Differentiable Transition Between Additive and Multiplicative Neurons | http://arxiv.org/pdf/1604.03736v1.pdf | author:Wiebke KÃ¶pp, Patrick van der Smagt, Sebastian Urban category:cs.LG stat.ML published:2016-04-13 summary:Existing approaches to combine both additive and multiplicative neural unitseither use a fixed assignment of operations or require discrete optimization todetermine what function a neuron should perform. However, this leads to anextensive increase in the computational complexity of the training procedure. We present a novel, parameterizable transfer function based on themathematical concept of non-integer functional iteration that allows theoperation each neuron performs to be smoothly and, most importantly,differentiablely adjusted between addition and multiplication. This allows thedecision between addition and multiplication to be integrated into the standardbackpropagation training procedure.
arxiv-17700-275 | VConv-DAE: Deep Volumetric Shape Learning Without Object Labels | http://arxiv.org/pdf/1604.03755v1.pdf | author:Abhishek Sharma, Oliver Grau, Mario Fritz category:cs.CV cs.GR published:2016-04-13 summary:With the advent of affordable depth sensors, 3D capture becomes more and moreubiquitous and already has made its way into commercial products. Yet,capturing the geometry or complete shapes of everyday objects using scanningdevices (eg. Kinect) still comes with several challenges that result in noiseor even incomplete shapes. Recent success in deep learning has shown how tolearn complex shape distributions in a data-driven way from large scale 3D CADModel collections and to utilize them for 3D processing on volumetricrepresentations and thereby circumventing problems of topology andtessellation. Prior work has shown encouraging results on problems ranging fromshape completion to recognition. We provide an analysis of such approaches anddiscover that training as well as the resulting representation are strongly andunnecessarily tied to the notion of object labels. Furthermore, deep learningresearch argues ~\cite{Vincent08} that learning representation withover-complete model are more prone to overfitting compared to the approach thatlearns from noisy data. Thus, we investigate a full convolutional volumetricdenoising auto encoder that is trained in a unsupervised fashion. Itoutperforms prior work on recognition as well as more challenging tasks likedenoising and shape completion. In addition, our approach is atleast two orderof magnitude faster at test time and thus, provides a path to scaling up 3Ddeep learning.
arxiv-17700-276 | A General Distributed Dual Coordinate Optimization Framework for Regularized Loss Minimization | http://arxiv.org/pdf/1604.03763v1.pdf | author:Shun Zheng, Fen Xia, Wei Xu, Tong Zhang category:cs.LG cs.DC published:2016-04-13 summary:In modern large-scale machine learning applications, the training data areoften partitioned and stored on multiple machines. It is customary to employthe "data parallelism" approach, where the aggregated training loss isminimized without moving data across machines. In this paper, we introduce anovel distributed dual formulation for regularized loss minimization problemsthat can directly handle data parallelism under the distributed computingenvironment. This formulation allows us to systematically derive dualcoordinate optimization procedures, which we refer to as DistributedAlternating Dual Maximization (DADM). The method extends earlier studies aboutdistributed SDCA algorithms and has a rigorous theoretical analysis. Based onthe new formulation, we also develop an accelerated DADM algorithm bygeneralizing the acceleration technique from the accelerated SDCA algorithm tothe distributed setting. Our empirical studies show that our novel approachsignificantly improves the previous state-of-the-art distributed dualcoordinate optimization algorithms.
arxiv-17700-277 | Animation and Chirplet-Based Development of a PIR Sensor Array for Intruder Classification in an Outdoor Environment | http://arxiv.org/pdf/1604.03829v1.pdf | author:Raviteja Upadrashta, Tarun Choubisa, A. Praneeth, Tony G., Aswath V. S., P. Vijay Kumar, Sripad Kowshik, Hari Prasad Gokul R, T. V. Prabhakar category:cs.LG published:2016-04-13 summary:This paper presents the development of a passive infra-red sensor towerplatform along with a classification algorithm to distinguish between humanintrusion, animal intrusion and clutter arising from wind-blown vegetativemovement in an outdoor environment. The research was aimed at exploring thepotential use of wireless sensor networks as an early-warning system to helpmitigate human-wildlife conflicts occurring at the edge of a forest. There arethree important features to the development. Firstly, the sensor platformemploys multiple sensors arranged in the form of a two-dimensional array togive it a key spatial-resolution capability that aids in classification.Secondly, given the challenges of collecting data involving animal intrusion,an Animation-based Simulation tool for Passive Infra-Red sEnsor (ASPIRE) wasdeveloped that simulates signals corresponding to human and animal intrusionand some limited models of vegetative clutter. This speeded up the process ofalgorithm development by allowing us to test different hypotheses in atime-efficient manner. Finally, a chirplet-based model for intruder signal wasdeveloped that significantly helped boost classification accuracy despitedrawing data from a smaller number of sensors. An SVM-based classifier was usedwhich made use of chirplet, energy and signal cross-correlation-based features.The average accuracy obtained for intruder detection and classification onreal-world and simulated data sets was in excess of 97%.
arxiv-17700-278 | Reversible Image Merging for Low-level Machine Vision | http://arxiv.org/pdf/1604.03832v1.pdf | author:Mikhail Kharinov category:cs.CV published:2016-04-13 summary:In this paper a hierarchical model for pixel clustering and imagesegmentation is developed. In the model an image is hierarchically structured.The original image is treated as a set of nested images, which are capable toreversibly merge with each other. An object is defined as a structural elementof an image, so that, an image is regarded as a maximal object. The simulatingof none-hierarchical optimal pixel clustering by hierarchical clustering isstudied. To generate a hierarchy of optimized piecewise constant imageapproximations, estimated by the standard deviation of approximation from theimage, the conversion of any hierarchy of approximations into the hierarchydescribed in relation to the number of intensity levels by convex sequence oftotal squared errors is proposed.
arxiv-17700-279 | Hierarchical Compound Poisson Factorization | http://arxiv.org/pdf/1604.03853v1.pdf | author:Mehmet E. Basbug, Barbara Engelhardt category:cs.LG cs.AI stat.ML published:2016-04-13 summary:Non-negative matrix factorization models based on a hierarchicalGamma-Poisson structure capture user and item behavior effectively in extremelysparse data sets, making them the ideal choice for collaborative filteringapplications. Hierarchical Poisson factorization (HPF) in particular has provedsuccessful for scalable recommendation systems with extreme sparsity. HPF,however, suffers from a tight coupling of sparsity model (absence of a rating)and response model (the value of the rating), which limits the expressivenessof the latter. Here, we introduce hierarchical compound Poisson factorization(HCPF) that has the favorable Gamma-Poisson structure and scalability of HPF tohigh-dimensional extremely sparse matrices. More importantly, HCPF decouplesthe sparsity model from the response model, allowing us to choose the mostsuitable distribution for the response. HCPF can capture binary, non-negativediscrete, non-negative continuous, and zero-inflated continuous responses. Wecompare HCPF with HPF on nine discrete and three continuous data sets andconclude that HCPF captures the relationship between sparsity and responsebetter than HPF.
arxiv-17700-280 | Detangling People: Individuating Multiple Close People and Their Body Parts via Region Assembly | http://arxiv.org/pdf/1604.03880v1.pdf | author:Hao Jiang, Kristen Grauman category:cs.CV published:2016-04-13 summary:Today's person detection methods work best when people are in common uprightposes and appear reasonably well spaced out in the image. However, in many realimages, that's not what people do. People often appear quite close to eachother, e.g., with limbs linked or heads touching, and their poses are often notpedestrian-like. We propose an approach to detangle people in multi-personimages. We formulate the task as a region assembly problem. Starting from alarge set of overlapping regions from body part semantic segmentation andgeneric object proposals, our optimization approach reassembles those piecestogether into multiple person instances. It enforces that the composed bodypart regions of each person instance obey constraints on relative sizes, mutualspatial relationships, foreground coverage, and exclusive label assignmentswhen overlapping. Since optimal region assembly is a challenging combinatorialproblem, we present a Lagrangian relaxation method to accelerate the lowerbound estimation, thereby enabling a fast branch and bound solution for theglobal optimum. As output, our method produces a pixel-level map indicatingboth 1) the body part labels (arm, leg, torso, and head), and 2) which partsbelong to which individual person. Our results on three challenging datasetsshow our method is robust to clutter, occlusion, and complex poses. Itoutperforms a variety of competing methods, including existing detector CRFmethods and region CNN approaches. In addition, we demonstrate its impact on aproxemics recognition task, which demands a precise representation of "whosebody part is where" in crowded images.
arxiv-17700-281 | The Effect of Distortions on the Prediction of Visual Attention | http://arxiv.org/pdf/1604.03882v1.pdf | author:Milind S. Gide, Samuel F. Dodge, Lina J. Karam category:cs.CV published:2016-04-13 summary:Existing saliency models have been designed and evaluated for predicting thesaliency in distortion-free images. However, in practice, the image quality isaffected by a host of factors at several stages of the image processingpipeline such as acquisition, compression and transmission. Several studieshave explored the effect of distortion on human visual attention; however, noneof them have considered the performance of visual saliency models in thepresence of distortion. Furthermore, given that one potential application ofvisual saliency prediction is to aid pooling of objective visual qualitymetrics, it is important to compare the performance of existing saliency modelson distorted images. In this paper, we evaluate several state-of-the-art visualattention models over different databases consisting of distorted images withvarious types of distortions such as blur, noise and compression with varyinglevels of distortion severity. This paper also introduces new improvedperformance evaluation metrics that are shown to overcome shortcomings inexisting performance metrics. We find that the performance of most modelsimproves with moderate and high levels of distortions as compared to the neardistortion-free case. In addition, model performance is also found to decreasewith an increase in image complexity.
arxiv-17700-282 | Algorithms for stochastic optimization with expectation constraints | http://arxiv.org/pdf/1604.03887v1.pdf | author:Guanghui Lan, Zhiqiang Zhou category:math.OC stat.ML published:2016-04-13 summary:This paper considers the problem of minimizing an expectation function over aclosed convex set, coupled with functional constraints given in the form ofexpectation. We present a new stochastic approximation (SA) type algorithm,namely the alternating mirror-descent SA (AMD-SA) algorithm, to minimize theseexpectation constrained problems, and show that it exhibits the optimal rate ofconvergence when the objective and constraint functions are convex or stronglyconvex. We also present a variant of this algorithm to solve a special class ofstructured nonconvex problems and establish its rate of convergence. It isworth noting that all theses algorithms are primal methods which do not requirethe estimation of the dual variables. We also provide some promisingpreliminary numerical results for these algorithms applied to some portfoliooptimization and machine learning problems.
arxiv-17700-283 | Single-Image Depth Perception in the Wild | http://arxiv.org/pdf/1604.03901v1.pdf | author:Weifeng Chen, Zhao Fu, Dawei Yang, Jia Deng category:cs.CV cs.AI published:2016-04-13 summary:This paper studies single-image depth perception in the wild, i.e.,recovering depth from a single image taken in unconstrained settings. Weintroduce a new dataset "Depth in the Wild" consisting of images in the wildannotated with relative depth between pairs of random points. We also propose anew algorithm that learns to estimate metric depth using annotations ofrelative depth. Compared to the state of the art, our algorithm is simpler andperforms better. Experiments show that our algorithm, combined with existingRGB-D data and our new relative depth annotations, significantly improvessingle-image depth perception in the wild.
arxiv-17700-284 | Inverse Reinforcement Learning with Simultaneous Estimation of Rewards and Dynamics | http://arxiv.org/pdf/1604.03912v1.pdf | author:Michael Herman, Tobias Gindele, JÃ¶rg Wagner, Felix Schmitt, Wolfram Burgard category:cs.AI cs.LG cs.SY stat.ML published:2016-04-13 summary:Inverse Reinforcement Learning (IRL) describes the problem of learning anunknown reward function of a Markov Decision Process (MDP) from observedbehavior of an agent. Since the agent's behavior originates in its policy andMDP policies depend on both the stochastic system dynamics as well as thereward function, the solution of the inverse problem is significantlyinfluenced by both. Current IRL approaches assume that if the transition modelis unknown, additional samples from the system's dynamics are accessible, orthe observed behavior provides enough samples of the system's dynamics to solvethe inverse problem accurately. These assumptions are often not satisfied. Toovercome this, we present a gradient-based IRL approach that simultaneouslyestimates the system's dynamics. By solving the combined optimization problem,our approach takes into account the bias of the demonstrations, which stemsfrom the generating policy. The evaluation on a synthetic MDP and a transferlearning task shows improvements regarding the sample efficiency as well as theaccuracy of the estimated reward functions and transition models.
arxiv-17700-285 | Removing Clouds and Recovering Ground Observations in Satellite Image Sequences via Temporally Contiguous Robust Matrix Completion | http://arxiv.org/pdf/1604.03915v1.pdf | author:Jialei Wang, Peder A. Olsen, Andrew R. Conn, Aurelie C. Lozano category:cs.CV cs.LG published:2016-04-13 summary:We consider the problem of removing and replacing clouds in satellite imagesequences, which has a wide range of applications in remote sensing. Ourapproach first detects and removes the cloud-contaminated part of the imagesequences. It then recovers the missing scenes from the clean parts using theproposed "TECROMAC" (TEmporally Contiguous RObust MAtrix Completion) objective.The objective function balances temporal smoothness with a low rank solutionwhile staying close to the original observations. The matrix whose the rows arepixels and columnsare days corresponding to the image, has low-rank because thepixels reflect land-types such as vegetation, roads and lakes and there arerelatively few variations as a result. We provide efficient optimizationalgorithms for TECROMAC, so we can exploit images containing millions ofpixels. Empirical results on real satellite image sequences, as well assimulated data, demonstrate that our approach is able to recover underlyingimages from heavily cloud-contaminated observations.
arxiv-17700-286 | Theoretically-Grounded Policy Advice from Multiple Teachers in Reinforcement Learning Settings with Applications to Negative Transfer | http://arxiv.org/pdf/1604.03986v1.pdf | author:Yusen Zhan, Haitham Bou Ammar, Matthew E. taylor category:cs.LG published:2016-04-13 summary:Policy advice is a transfer learning method where a student agent is able tolearn faster via advice from a teacher. However, both this and otherreinforcement learning transfer methods have little theoretical analysis. Thispaper formally defines a setting where multiple teacher agents can provideadvice to a student and introduces an algorithm to leverage both autonomousexploration and teacher's advice. Our regret bounds justify the intuition thatgood teachers help while bad teachers hurt. Using our formalization, we arealso able to quantify, for the first time, when negative transfer can occurwithin such a reinforcement learning setting.
arxiv-17700-287 | Visual Storytelling | http://arxiv.org/pdf/1604.03968v1.pdf | author:Ting-Hao, Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, Lucy Vanderwende, Michel Galley, Margaret Mitchell category:cs.CL cs.AI cs.CV published:2016-04-13 summary:We introduce the first dataset for sequential vision-to-language, and explorehow this data may be used for the task of visual storytelling. The firstrelease of this dataset, SIND v.1, includes 81,743 unique photos in 20,211sequences, aligned to both descriptive (caption) and story language. Weestablish several strong baselines for the storytelling task, and motivate anautomatic metric to benchmark progress. Modelling concrete description as wellas figurative and social language, as provided in this dataset and thestorytelling task, has the potential to move artificial intelligence from basicunderstandings of typical visual scenes towards more and more human-likeunderstanding of grounded event structure and subjective expression.
arxiv-17700-288 | Efficient Algorithms for Large-scale Generalized Eigenvector Computation and Canonical Correlation Analysis | http://arxiv.org/pdf/1604.03930v1.pdf | author:Rong Ge, Chi Jin, Sham M. Kakade, Praneeth Netrapalli, Aaron Sidford category:cs.LG math.OC stat.ML published:2016-04-13 summary:This paper considers the problem of canonical-correlation analysis (CCA)(Hotelling, 1936) and, more broadly, the generalized eigenvector problem for apair of symmetric matrices. These are two fundamental problems in data analysisand scientific computing with numerous applications in machine learning andstatistics (Shi and Malik, 2000; Hardoon et al., 2004; Witten et al., 2009). We provide simple iterative algorithms, with improved runtimes, for solvingthese problems that are globally linearly convergent with moderate dependencieson the condition numbers and eigenvalue gaps of the matrices involved. We obtain our results by reducing CCA to the top-$k$ generalized eigenvectorproblem. We solve this problem through a general framework that simply requiresblack box access to an approximate linear system solver. Instantiating thisframework with accelerated gradient descent we obtain a running time of$O(\frac{z k \sqrt{\kappa}}{\rho} \log(1/\epsilon) \log\left(k\kappa/\rho\right))$ where $z$ is the total number of nonzero entries,$\kappa$ is the condition number and $\rho$ is the relative eigenvalue gap ofthe appropriate matrices. Our algorithm is linear in the input size and the number of components $k$ upto a $\log(k)$ factor. This is essential for handling large-scale matrices thatappear in practice. To the best of our knowledge this is the first suchalgorithm with global linear convergence. We hope that our results promptfurther research and ultimately improve the practical running time forperforming these important data analysis procedures on large data sets.
arxiv-17700-289 | Max-Information, Differential Privacy, and Post-Selection Hypothesis Testing | http://arxiv.org/pdf/1604.03924v1.pdf | author:Ryan Rogers, Aaron Roth, Adam Smith, Om Thakkar category:cs.LG published:2016-04-13 summary:In this paper, we initiate a principled study of how the generalizationproperties of approximate differential privacy can be used to perform adaptivehypothesis testing, while giving statistically valid $p$-value corrections. Wedo this by observing that the guarantees of algorithms with bounded approximatemax-information are sufficient to correct the $p$-values of adaptively chosenhypotheses, and then by proving that algorithms that satisfy$(\epsilon,\delta)$-differential privacy have bounded approximate maxinformation when their inputs are drawn from a product distribution. Thissubstantially extends the known connection between differential privacy andmax-information, which previously was only known to hold for (pure)$(\epsilon,0)$-differential privacy. It also extends our understanding ofmax-information as a partially unifying measure controlling the generalizationproperties of adaptive data analyses. We also show a lower bound, proving that(despite the strong composition properties of max-information), when data isdrawn from a product distribution, $(\epsilon,\delta)$-differentially privatealgorithms can come first in a composition with other algorithms satisfyingmax-information bounds, but not necessarily second if the composition isrequired to itself satisfy a nontrivial max-information bound. This, inparticular, implies that the connection between$(\epsilon,\delta)$-differential privacy and max-information holds only forinputs drawn from product distributions, unlike the connection between$(\epsilon,0)$-differential privacy and max-information.
arxiv-17700-290 | Variational Bayesian Inference of Line Spectra | http://arxiv.org/pdf/1604.03744v1.pdf | author:Mihai-Alin Badiu, Thomas Lundgaard Hansen, Bernard Henri Fleury category:cs.IT math.IT stat.ML published:2016-04-13 summary:In this paper, we address the fundamental problem of line spectral estimationin a Bayesian framework. We target model order and parameter estimation viavariational inference in a probabilistic model in which the frequencies arecontinuous-valued, i.e., not restricted to a grid; and the coefficients aregoverned by a Bernoulli-Gaussian prior model turning model order selection intobinary sequence detection. Unlike earlier works which retain only pointestimates of the frequencies, we undertake a more complete Bayesian treatmentby estimating the posterior probability density functions (pdfs) of thefrequencies and computing expectations over them. Thus, we additionally captureand operate with the uncertainty of the frequency estimates. Aiming to maximizethe model evidence, variational optimization provides analytic approximationsof the posterior pdfs and also gives estimates of the additional parameters. Wepropose an accurate representation of the pdfs of the frequencies by mixturesof von Mises pdfs, which yields closed-form expectations. We define thealgorithm VALSE in which the estimates of the pdfs and parameters areiteratively updated. VALSE is a gridless, convergent method, does not requireparameter tuning, can easily include prior knowledge about the frequencies andprovides approximate posterior pdfs based on which the uncertainty in linespectral estimation can be quantified. Simulation results show that accountingfor the uncertainty of frequency estimates, rather than computing just pointestimates, significantly improves the performance. The performance of VALSE issuperior to that of state-of-the-art methods and closely approaches theCram\'er-Rao bound computed for the true model order.
arxiv-17700-291 | Efficient Classification of Multi-Labelled Text Streams by Clashing | http://arxiv.org/pdf/1604.03200v1.pdf | author:Ricardo Ãanculef, Ilias Flaounas, Nello Cristianini category:cs.AI cs.LG published:2016-04-12 summary:We present a method for the classification of multi-labelled text documentsexplicitly designed for data stream applications that require to process avirtually infinite sequence of data using constant memory and constantprocessing time. Our method is composed of an online procedure used toefficiently map text into a low-dimensional feature space and a partition ofthis space into a set of regions for which the system extracts and keepsstatistics used to predict multi-label text annotations. Documents are fed intothe system as a sequence of words, mapped to a region of the partition, andannotated using the statistics computed from the labelled instances collidingin the same region. This approach is referred to as clashing. We illustrate themethod in real-world text data, comparing the results with those obtained usingother text classifiers. In addition, we provide an analysis about the effect ofthe representation space dimensionality on the predictive performance of thesystem. Our results show that the online embedding indeed approximates thegeometry of the full corpus-wise TF and TF-IDF space. The model obtainscompetitive F measures with respect to the most accurate methods, usingsignificantly fewer computational resources. In addition, the method achieves ahigher macro-averaged F measure than methods with similar running time.Furthermore, the system is able to learn faster than the other methods frompartially labelled streams.
arxiv-17700-292 | Privacy-Preserving Egocentric Activity Recognition from Extreme Low Resolution | http://arxiv.org/pdf/1604.03196v1.pdf | author:Michael S. Ryoo, Brandon Rothrock, Charles Fleming category:cs.CV published:2016-04-12 summary:Privacy protection from video taken by wearable cameras is an importantsocietal challenge. We desire a wearable vision system that can recognize humanactivities, yet not disclose the identity of the participants. Videoanonymization is typically handled by decimating the image to a very lowresolution. Activity recognition, however, generally requires resolution highenough that features such as faces are identifiable. In this paper, we proposea new approach to address such contradicting objectives: human activityrecognition while only using extreme low-resolution (e.g., 16x12) anonymizedvideos. We introduce the paradigm of inverse super resolution (ISR), theconcept of learning the optimal set of image transformations to generatemultiple low-resolution videos from a single video. Our ISR learns differenttypes of sub-pixel transformations optimized for the activity classification,allowing the classifier to best take advantage of existing high-resolutionvideos (e.g., YouTube videos) by generating multiple LR training videostailored for the problem. We experimentally confirm that the paradigm ofinverse super resolution is able to benefit activity recognition from extremelow-resolution videos (e.g., 16x12 and 32x24), particularly in first-personscenarios.
arxiv-17700-293 | Disfluency Detection using a Bidirectional LSTM | http://arxiv.org/pdf/1604.03209v1.pdf | author:Vicky Zayats, Mari Ostendorf, Hannaneh Hajishirzi category:cs.CL published:2016-04-12 summary:We introduce a new approach for disfluency detection using a BidirectionalLong-Short Term Memory neural network (BLSTM). In addition to the wordsequence, the model takes as input pattern match features that were developedto reduce sensitivity to vocabulary size in training, which lead to improvedperformance over the word sequence alone. The BLSTM takes advantage of explicitrepair states in addition to the standard reparandum states. The final outputleverages integer linear programming to incorporate constraints of disfluencystructure. In experiments on the Switchboard corpus, the model achievesstate-of-the-art performance for both the standard disfluency detection taskand the correction detection task. Analysis shows that the model has betterdetection of non-repetition disfluencies, which tend to be much harder todetect.
arxiv-17700-294 | Geometric Feature-Based Facial Expression Recognition in Image Sequences Using Multi-Class AdaBoost and Support Vector Machines | http://arxiv.org/pdf/1604.03225v1.pdf | author:Deepak Ghimire, Joonwhoan Lee category:cs.CV 68T01 I.4; I.5 published:2016-04-12 summary:Facial expressions are widely used in the behavioral interpretation ofemotions, cognitive science, and social interactions. In this paper, we presenta novel method for fully automatic facial expression recognition in facialimage sequences. As the facial expression evolves over time facial landmarksare automatically tracked in consecutive video frames, using displacementsbased on elastic bunch graph matching displacement estimation. Feature vectorsfrom individual landmarks, as well as pairs of landmarks tracking results areextracted, and normalized, with respect to the first frame in the sequence. Theprototypical expression sequence for each class of facial expression is formed,by taking the median of the landmark tracking results from the training facialexpression sequences. Multi-class AdaBoost with dynamic time warping similaritydistance between the feature vector of input facial expression and prototypicalfacial expression, is used as a weak classifier to select the subset ofdiscriminative feature vectors. Finally, two methods for facial expressionrecognition are presented, either by using multi-class AdaBoost with dynamictime warping, or by using support vector machine on the boosted featurevectors. The results on the Cohn-Kanade (CK+) facial expression database show arecognition accuracy of 95.17% and 97.35% using multi-class AdaBoost andsupport vector machines, respectively.
arxiv-17700-295 | Application of the Second-Order Statistics for Estimation of the Pure Spectra of Individual Components from the Visible Hyperspectral Images of Their Mixture | http://arxiv.org/pdf/1604.03193v1.pdf | author:Sung-Ho Jong, Yong-U Ri, Kye-Ryong Sin category:cs.CV published:2016-04-12 summary:The second-order statistics (SOS) can be applied in estimation of the purespectra of chemical components from the spectrum of their mixture, when SOSseems to be good at estimation of spectral patterns, but their peak directionsare opposite in some cases. In this paper, one method for judgment of the peakdirection of the pure spectra was proposed, where the base line of the purespectra was drawn by using their histograms and the peak directions were chosenso as to make all of the pure spectra located upwards over the base line.Results of the SOS analysis on the visible hyperspectral images of the mixturecomposed of two or three chemical components showed that the present methodoffered the reasonable shape and direction of the pure spectra of itscomponents.
arxiv-17700-296 | Volumetric and Multi-View CNNs for Object Classification on 3D Data | http://arxiv.org/pdf/1604.03265v2.pdf | author:Charles R. Qi, Hao Su, Matthias Niessner, Angela Dai, Mengyuan Yan, Leonidas J. Guibas category:cs.CV cs.AI published:2016-04-12 summary:3D shape models are becoming widely available and easier to capture, makingavailable 3D information crucial for progress in object classification. Currentstate-of-the-art methods rely on CNNs to address this problem. Recently, wewitness two types of CNNs being developed: CNNs based upon volumetricrepresentations versus CNNs based upon multi-view representations. Empiricalresults from these two types of CNNs exhibit a large gap, indicating thatexisting volumetric CNN architectures and approaches are unable to fullyexploit the power of 3D representations. In this paper, we aim to improve bothvolumetric CNNs and multi-view CNNs according to extensive analysis of existingapproaches. To this end, we introduce two distinct network architectures ofvolumetric CNNs. In addition, we examine multi-view CNNs, where we introducemulti-resolution filtering in 3D. Overall, we are able to outperform currentstate-of-the-art methods for both volumetric CNNs and multi-view CNNs. Weprovide extensive experiments designed to evaluate underlying design choices,thus providing a better understanding of the space of methods available forobject classification on 3D data.
arxiv-17700-297 | Recurrent Attentional Networks for Saliency Detection | http://arxiv.org/pdf/1604.03227v1.pdf | author:Jason Kuen, Zhenhua Wang, Gang Wang category:cs.CV cs.LG stat.ML published:2016-04-12 summary:Convolutional-deconvolution networks can be adopted to perform end-to-endsaliency detection. But, they do not work well with objects of multiple scales.To overcome such a limitation, in this work, we propose a recurrent attentionalconvolutional-deconvolution network (RACDNN). Using spatial transformer andrecurrent network units, RACDNN is able to iteratively attend to selected imagesub-regions to perform saliency refinement progressively. Besides tackling thescale problem, RACDNN can also learn context-aware features from pastiterations to enhance saliency refinement in future iterations. Experiments onseveral challenging saliency detection datasets validate the effectiveness ofRACDNN, and show that RACDNN outperforms state-of-the-art saliency detectionmethods.
arxiv-17700-298 | CRAFT Objects from Images | http://arxiv.org/pdf/1604.03239v1.pdf | author:Bin Yang, Junjie Yan, Zhen Lei, Stan Z. Li category:cs.CV published:2016-04-12 summary:Object detection is a fundamental problem in image understanding. One popularsolution is the R-CNN framework and its fast versions. They decompose theobject detection problem into two cascaded easier tasks: 1) generating objectproposals from images, 2) classifying proposals into various object categories.Despite that we are handling with two relatively easier tasks, they are notsolved perfectly and there's still room for improvement. In this paper, we pushthe "divide and conquer" solution even further by dividing each task into twosub-tasks. We call the proposed method "CRAFT" (Cascade Region-proposal-networkAnd FasT-rcnn), which tackles each task with a carefully designed networkcascade. We show that the cascade structure helps in both tasks: in proposalgeneration, it provides more compact and better localized object proposals; inobject classification, it reduces false positives (mainly between ambiguouscategories) by capturing both inter- and intra-category variances. CRAFTachieves consistent and considerable improvement over the state-of-the-art onobject detection benchmarks like PASCAL VOC 07/12 and ILSVRC.
arxiv-17700-299 | Scan, Attend and Read: End-to-End Handwritten Paragraph Recognition with MDLSTM Attention | http://arxiv.org/pdf/1604.03286v2.pdf | author:ThÃ©odore Bluche, JÃ©rÃ´me Louradour, Ronaldo Messina category:cs.CV published:2016-04-12 summary:We present an attention-based model for end-to-end handwriting recognition.Our system does not require any segmentation of the input paragraph. The modelis inspired by the differentiable attention models presented recently forspeech recognition, image captioning or translation. The main difference is thecovert and overt attention, implemented as a multi-dimensional LSTM network.Our principal contribution towards handwriting recognition lies in theautomatic transcription without a prior segmentation into lines, which wascrucial in previous approaches. To the best of our knowledge this is the firstsuccessful attempt of end-to-end multi-line handwriting recognition. We carriedout experiments on the well-known IAM Database. The results are encouraging andbring hope to perform full paragraph transcription in the near future.
arxiv-17700-300 | Thesis: Multiple Kernel Learning for Object Categorization | http://arxiv.org/pdf/1604.03247v1.pdf | author:Dinesh Govindaraj category:cs.CV cs.LG published:2016-04-12 summary:Object Categorization is a challenging problem, especially when the imageshave clutter background, occlusions or different lighting conditions. In thepast, many descriptors have been proposed which aid object categorization evenin such adverse conditions. Each descriptor has its own merits and de-merits.Some descriptors are invariant to transformations while the others are morediscriminative. Past research has shown that, employing multiple descriptorsrather than any single descriptor leads to better recognition. The problem oflearning the optimal combination of the available descriptors for a particularclassification task is studied. Multiple Kernel Learning (MKL) framework hasbeen developed for learning an optimal combination of descriptors for objectcategorization. Existing MKL formulations often employ block l-1 normregularization which is equivalent to selecting a single kernel from a libraryof kernels. Since essentially a single descriptor is selected, the existingformulations maybe sub- optimal for object categorization. A MKL formulationbased on block l-infinity norm regularization has been developed, which choosesan optimal combination of kernels as opposed to selecting a single kernel. AComposite Multiple Kernel Learning(CKL) formulation based on mixed l-infinityand l-1 norm regularization has been developed. These formulations end inSecond Order Cone Programs(SOCP). Other efficient alter- native algorithms forthese formulation have been implemented. Empirical results on benchmarkdatasets show significant improvement using these new MKL formulations.
