arxiv-17700-1 | WEPSAM: Weakly Pre-Learnt Saliency Model | http://arxiv.org/pdf/1605.01101v1.pdf | author:Avisek Lahiri, Sourya Roy, Anirban Santara, Pabitra Mitra, Prabir Kumar Biswas category:cs.CV published:2016-05-03 summary:Visual saliency detection tries to mimic human vision psychology whichconcentrates on sparse, important areas in natural image. Saliency predictionresearch has been traditionally based on low level features such as contrast,edge, etc. Recent thrust in saliency prediction research is to learn high levelsemantics using ground truth eye fixation datasets. In this paper we present,WEPSAM : Weakly Pre-Learnt Saliency Model as a pioneering effort of usingdomain specific pre-learing on ImageNet for saliency prediction using a lightweight CNN architecture. The paper proposes a two step hierarchical learning,in which the first step is to develop a framework for weakly pre-training on alarge scale dataset such as ImageNet which is void of human eye fixation maps.The second step refines the pre-trained model on a limited set of ground truthfixations. Analysis of loss on iSUN and SALICON datasets reveal thatpre-trained network converges much faster compared to randomly initializednetwork. WEPSAM also outperforms some recent state-of-the-art saliencyprediction models on the challenging MIT300 dataset.
arxiv-17700-2 | Iterative Instance Segmentation | http://arxiv.org/pdf/1511.08498v2.pdf | author:Ke Li, Bharath Hariharan, Jitendra Malik category:cs.CV cs.LG published:2015-11-26 summary:Existing methods for pixel-wise labelling tasks generally disregard theunderlying structure of labellings, often leading to predictions that arevisually implausible. While incorporating structure into the model shouldimprove prediction quality, doing so is challenging - manually specifying theform of structural constraints may be impractical and inference often becomesintractable even if structural constraints are given. We sidestep this problemby reducing structured prediction to a sequence of unconstrained predictionproblems and demonstrate that this approach is capable of automaticallydiscovering priors on shape, contiguity of region predictions and smoothness ofregion contours from data without any a priori specification. On the instancesegmentation task, this method outperforms the state-of-the-art, achieving amean AP^r of 63.6% at 50% overlap and 43.3% at 70% overlap.
arxiv-17700-3 | Logarithmic proximity measures outperform plain ones in graph nodes clustering | http://arxiv.org/pdf/1605.01046v1.pdf | author:Vladimir Ivashkin, Pavel Chebotarev category:cs.LG cs.DM published:2016-05-03 summary:We consider a number of graph kernels and proximity measures: commute timekernel, regularized Laplacian kernel, heat kernel, communicability, etc., andthe corresponding distances as applied to clustering nodes in random graphs.The model of generating graphs involves edge probabilities for the pairs ofnodes that belong to the same class or different classes. It turns out that inmost cases, logarithmic measures (i.e., measures resulting after takinglogarithm of the proximities) perform much better while distinguishing classesthan the "plain" measures. A direct comparison of inter-class and intra-classdistances confirms this conclusion. A possible explanation of this fact is thatmost kernels have a multiplicative nature, while the nature of distances usedin cluster algorithms is an additive one (cf. the triangle inequality). Thelogarithmic transformation is just a tool to transform one nature to another.Moreover, some distances corresponding to the logarithmic measures possess ameaningful cutpoint additivity property. In our experiments, the leader is theso-called logarithmic communicability measure, which distinctly outperforms theother measures under study.
arxiv-17700-4 | Hierarchical Bayesian Noise Inference for Robust Real-time Probabilistic Object Classification | http://arxiv.org/pdf/1605.01042v1.pdf | author:Shayegan Omidshafiei, Brett T. Lopez, Jonathan P. How, John Vian category:cs.CV published:2016-05-03 summary:Robust environment perception is essential for decision-making on robotsoperating in complex domains. Principled treatment of uncertainty sources in arobot's observation model is necessary for accurate mapping and objectdetection. This is important not only for low-level observations (e.g.,accelerometer data), but for high-level observations such as semantic objectlabels as well. This paper presents an approach for filtering sequences ofobject classification probabilities using online modeling of the noisecharacteristics of the classifier outputs. A hierarchical Bayesian approach isused to model per-class noise distributions, while simultaneously allowingsharing of high-level noise characteristics between classes. The proposedfiltering scheme, called Hierarchical Bayesian Noise Inference (HBNI), is shownto outperform classification accuracy of existing methods. The paper alsopresents real-time filtered classification hardware experiments running fullyonboard a moving quadrotor, where the proposed approach is demonstrated to workin a challenging domain where noise-agnostic filtering fails.
arxiv-17700-5 | Online Machine Learning Techniques for Predicting Operator Performance | http://arxiv.org/pdf/1605.01029v1.pdf | author:Ahmet Anil Pala category:cs.LG published:2016-05-03 summary:This thesis explores a number of online machine learning algorithms. From atheoret- ical perspective, it assesses their employability for a particularfunction approximation problem where the analytical models fall short.Furthermore, it discusses the applica- tion of theoretically suitable learningalgorithms to the function approximation problem at hand through an efficientimplementation that exploits various computational and mathematical shortcuts.Finally, this thesis work evaluates the implemented learning algorithmsaccording to various evaluation criteria through rigorous testing.
arxiv-17700-6 | Deep Deformation Network for Object Landmark Localization | http://arxiv.org/pdf/1605.01014v1.pdf | author:Xiang Yu, Feng Zhou, Manmohan Chandraker category:cs.CV published:2016-05-03 summary:We propose a novel cascaded framework, called deep deformation network (DDN),for localizing landmarks in non-rigid objects. The hallmarks of DDN are itsincorporation of geometric constraints within a convolutional neural network(CNN) framework, ease and efficiency of training, as well as generality ofapplication. A novel shape basis network (SBN) forms the first stage of thecascade, whereby landmarks are initialized by combining the benefits of CNNfeatures and a learned shape basis to reduce the complexity of the highlynonlinear pose manifold. In the second stage, a point transformer network (PTN)estimates local deformations parameterized as thin-plate spline transformationsfor a finer refinement. Our framework does not incorporate either handcraftedfeatures or part connectivity, which enables an end-to-end shape predictionpipeline during both training and testing. In contrast to prior cascadednetworks for landmark localization that learn a mapping from feature space tolandmark locations, we demonstrate that the regularization induced throughgeometric priors in the DDN makes it easier to train, yet produces superiorresults. The efficacy and generality of the architecture is demonstratedthrough state-of-the-art performances on several benchmarks for multiple taskssuch as facial landmark localization, human body pose estimation and bird partlocalization.
arxiv-17700-7 | RENOIR - A Dataset for Real Low-Light Noise Image Reduction | http://arxiv.org/pdf/1409.8230v2.pdf | author:Josue Anaya, Adrian Barbu category:cs.CV published:2014-09-29 summary:The application of noise reduction or image denoising is a very importanttopic in the field of computer vision and image processing. Many modern andpopular state of the art image denoising algorithms are trained and evaluatedusing images with added artificial noise. These trained algorithms and theirevaluations on synthetic data may lead to incorrect conclusions about theirperformances on real noise. In this paper we introduce a benchmark dataset ofuncompressed color images corrupted by natural noise due to low-lightconditions, together with spatially and intensity-aligned low noise images ofthe same scenes. The dataset contains over 100 scenes and more than 400 images,including both 16-bit RAW formatted images and 8-bit BMP pixel andintensity-aligned images from 2 digital cameras (Canon S90 and Canon T3i) and amobile phone (Xiaomi Mi3). We also introduce a method for estimating the truenoise level in each of our images, since even the low noise images contain asmall amount of noise. Finally, we exemplify the use of our dataset byevaluating three denoising algorithms: Active Random Field, BM3D, andMulti-Layer Perceptron. We show that while the Multi-Layer Perceptron algorithmworks as well as if not better than BM3D on synthetic data, it does not achievethe same results on our dataset.
arxiv-17700-8 | Personalized Risk Scoring for Critical Care Patients using Mixtures of Gaussian Process Experts | http://arxiv.org/pdf/1605.00959v1.pdf | author:Ahmed M. Alaa, Jinsung Yoon, Scott Hu, Mihaela van der Schaar category:cs.LG stat.ML published:2016-05-03 summary:We develop a personalized real time risk scoring algorithm that providestimely and granular assessments for the clinical acuity of ward patients basedon their (temporal) lab tests and vital signs. Heterogeneity of the patientspopulation is captured via a hierarchical latent class model. The proposedalgorithm aims to discover the number of latent classes in the patientspopulation, and train a mixture of Gaussian Process (GP) experts, where eachexpert models the physiological data streams associated with a specific class.Self-taught transfer learning is used to transfer the knowledge of latentclasses learned from the domain of clinically stable patients to the domain ofclinically deteriorating patients. For new patients, the posterior beliefs ofall GP experts about the patient's clinical status given her physiological datastream are computed, and a personalized risk score is evaluated as a weightedaverage of those beliefs, where the weights are learned from the patient'shospital admission information. Experiments on a heterogeneous cohort of 6,313patients admitted to Ronald Regan UCLA medical center show that our risk scoreoutperforms the currently deployed risk scores, such as MEWS and Rothmanscores.
arxiv-17700-9 | TheanoLM - An Extensible Toolkit for Neural Network Language Modeling | http://arxiv.org/pdf/1605.00942v1.pdf | author:Seppo Enarvi, Mikko Kurimo category:cs.CL cs.NE published:2016-05-03 summary:We present a new tool for training neural network language models (NNLMs),scoring sentences, and generating text. The tool has been written using Pythonlibrary Theano, which allows researcher to easily extend it and tune any aspectof the training process. Regardless of the flexibility, Theano is able togenerate extremely fast native code that can utilize a GPU or multiple CPUcores in order to parallelize the heavy numerical computations. The tool hasbeen evaluated in difficult Finnish and English conversational speechrecognition tasks, and significant improvement was obtained over our bestback-off n-gram models. The results that we obtained in the Finnish task werecompared to those from existing RNNLM and RWTHLM toolkits, and found to be asgood or better, while training times were an order of magnitude shorter.
arxiv-17700-10 | Dictionary Learning for Massive Matrix Factorization | http://arxiv.org/pdf/1605.00937v1.pdf | author:Arthur Mensch, Julien Mairal, Bertrand Thirion, Gaël Varoquaux category:stat.ML cs.LG published:2016-05-03 summary:Sparse matrix factorization is a popular tool to obtain interpretable datadecompositions, which are also effective to perform data completion ordenoising. Its applicability to large datasets has been addressed with onlineand randomized methods, that reduce the complexity in one of the matrixdimension, but not in both of them. In this paper, we tackle very largematrices in both dimensions. We propose a new factoriza-tion method that scalesgracefully to terabyte-scale datasets, that could not be processed by previousalgorithms in a reasonable amount of time. We demonstrate the efficiency of ourapproach on massive functional Magnetic Resonance Imaging (fMRI) data, and onmatrix completion problems for recommender systems, where we obtain significantspeed-ups compared to state-of-the art coordinate descent methods.
arxiv-17700-11 | Neural Simpletrons - Minimalistic Directed Generative Networks for Learning with Few Labels | http://arxiv.org/pdf/1506.08448v3.pdf | author:Dennis Forster, Abdul-Saboor Sheikh, Jörg Lücke category:stat.ML cs.LG published:2015-06-28 summary:Deep learning is intensively studied using supervised and unsupervisedlearning, and by applying probabilistic, deterministic, and bio-inspiredapproaches. Comparisons of different approaches such as generative anddiscriminative neural networks is made difficult, however, because ofdifferences in the semantics of their graphical descriptions, differentlearning methods, different benchmarking objectives and different scalability.To allow for a direct functional comparison, we here study a generativemulti-layer neural network in a form and setting as similar to standarddiscriminative networks as possible. Based on normalized Poisson mixtures, wederive a minimalistic deep neural network with local activation and learningrules. The network learns in a semi-supervised setting and can be scaled usingstandard deep learning tools for parallelized implementations. Empiricalevaluations on standard benchmarks show that for weakly labeled data thederived minimalistic network improves on all standard deep learning approachesand is competitive with their recent variants. In comparison to recentbio-inspired approaches it suggests further improvements through top-downconnections. Furthermore, we find that the studied network is the bestperforming monolithic (`non-hybrid') system for few labels, and that it can beapplied in the limit of very few labels, where no other system has beenreported to operate so far.
arxiv-17700-12 | Recurrent Convolutional Neural Network Regression for Continuous Pain Intensity Estimation in Video | http://arxiv.org/pdf/1605.00894v1.pdf | author:Jing Zhou, Xiaopeng Hong, Fei Su, Guoying Zhao category:cs.CV published:2016-05-03 summary:Automatic pain intensity estimation possesses a significant position inhealthcare and medical field. Traditional static methods prefer to extractfeatures from frames separately in a video, which would result in unstablechanges and peaks among adjacent frames. To overcome this problem, we propose areal-time regression framework based on the recurrent convolutional neuralnetwork for automatic frame-level pain intensity estimation. Given vectorsequences of AAM-warped facial images, we used a sliding-window strategy toobtain fixed-length input samples for the recurrent network. We then carefullydesign the architecture of the recurrent network to output continuous-valuedpain intensity. The proposed end-to-end pain intensity regression framework canpredict the pain intensity of each frame by considering a sufficiently largehistorical frames while limiting the scale of the parameters within the model.Our method achieves promising results regarding both accuracy and running speedon the published UNBC-McMaster Shoulder Pain Expression Archive Database.
arxiv-17700-13 | Improving Image Captioning by Concept-based Sentence Reranking | http://arxiv.org/pdf/1605.00855v1.pdf | author:Xirong Li, Qin Jin category:cs.CV cs.CL published:2016-05-03 summary:This paper describes our winning entry in the ImageCLEF 2015 image sentencegeneration task. We improve Google's CNN-LSTM model by introducingconcept-based sentence reranking, a data-driven approach which exploits thelarge amounts of concept-level annotations on Flickr. Different from previoususage of concept detection that is tailored to specific image captioningmodels, the propose approach reranks predicted sentences in terms of theirmatches with detected concepts, essentially treating the underlying model as ablack box. This property makes the approach applicable to a number of existingsolutions. We also experiment with fine tuning on the deep language model,which improves the performance further. Scoring METEOR of 0.1875 on theImageCLEF 2015 test set, our system outperforms the runner-up (METEOR of0.1687) with a clear margin.
arxiv-17700-14 | Learning to Aggregate Information for Sequential Inferences | http://arxiv.org/pdf/1508.07964v2.pdf | author:Diyan Teng, Emre Ertin category:stat.ML cs.LG published:2015-08-31 summary:We consider the problem of training a binary sequential classifier under anerror rate constraint. It is well known that for known densities, accumulatingthe likelihood ratio statistics is time optimal under a fixed error rateconstraint. For the case of unknown densities, we formulate the learning forsequential detection problem as a constrained density ratio estimation problem.Specifically, we show that the problem can be posed as a convex optimizationproblem using a Reproducing Kernel Hilbert Space representation for thelog-density ratio function. The proposed binary sequential classifier is testedon synthetic data set and UC Irvine human activity recognition data set,together with previous approaches for density ratio estimation. Our empiricalresults show that the classifier trained through the proposed techniqueachieves smaller average sampling cost than previous classifiers proposed inthe literature for the same error rate.
arxiv-17700-15 | Online Learning of Commission Avoidant Portfolio Ensembles | http://arxiv.org/pdf/1605.00788v1.pdf | author:Guy Uziel, Ran El-Yaniv category:cs.AI cs.LG published:2016-05-03 summary:We present a novel online ensemble learning strategy for portfolio selection.The new strategy controls and exploits any set of commission-obliviousportfolio selection algorithms. The strategy handles transaction costs using anovel commission avoidance mechanism. We prove a logarithmic regret bound forour strategy with respect to optimal mixtures of the base algorithms. Numericalexamples validate the viability of our method and show significant improvementover the state-of-the-art.
arxiv-17700-16 | Temporal Clustering of Time Series via Threshold Autoregressive Models: Application to Commodity Prices | http://arxiv.org/pdf/1605.00779v1.pdf | author:Sipan Aslan, Ceylan Yozgatligil, Cem Iyigun category:stat.ML stat.AP stat.ME published:2016-05-03 summary:This study aimed to find temporal clusters for several commodity prices usingthe threshold non-linear autoregressive model. It is expected that the processof determining the commodity groups that are time-dependent will advance thecurrent knowledge about the dynamics of co-moving and coherent prices, and canserve as a basis for multivariate time series analyses. The clustering ofcommodity prices was examined using the proposed clustering approach based ontime series models to incorporate the time varying properties of price seriesinto the clustering scheme. Accordingly, the primary aim in this study wasgrouping time series according to the similarity between their Data GeneratingMechanisms (DGMs) rather than comparing pattern similarities in the time seriestraces. The approximation to the DGM of each series was accomplished usingthreshold autoregressive models, which are recognized for their ability torepresent nonlinear features in time series, such as abrupt changes,time-irreversibility and regime-shifting behavior. Through the use of theproposed approach, one can determine and monitor the set of co-moving timeseries variables across the time dimension. Furthermore, generating a timevarying commodity price index and sub-indexes can become possible.Consequently, we conducted a simulation study to assess the effectiveness ofthe proposed clustering approach and the results are presented for both thesimulated and real data sets.
arxiv-17700-17 | Spatially Aware Dictionary Learning and Coding for Fossil Pollen Identification | http://arxiv.org/pdf/1605.00775v1.pdf | author:Shu Kong, Surangi Punyasena, Charless Fowlkes category:cs.CV q-bio.PE q-bio.QM published:2016-05-03 summary:We propose a robust approach for performing automatic species-levelrecognition of fossil pollen grains in microscopy images that exploits bothglobal shape and local texture characteristics in a patch-based matchingmethodology. We introduce a novel criteria for selecting meaningful anddiscriminative exemplar patches. We optimize this function during trainingusing a greedy submodular function optimization framework that gives anear-optimal solution with bounded approximation error. We use these selectedexemplars as a dictionary basis and propose a spatially-aware sparse codingmethod to match testing images for identification while maintaining globalshape correspondence. To accelerate the coding process for fast matching, weintroduce a relaxed form that uses spatially-aware soft-thresholding duringcoding. Finally, we carry out an experimental study that demonstrates theeffectiveness and efficiency of our exemplar selection and classificationmechanisms, achieving $86.13\%$ accuracy on a difficult fine-grained speciesclassification task distinguishing three types of fossil spruce pollen.
arxiv-17700-18 | Exact post-selection inference, with application to the lasso | http://arxiv.org/pdf/1311.6238v8.pdf | author:Jason D. Lee, Dennis L. Sun, Yuekai Sun, Jonathan E. Taylor category:math.ST stat.ME stat.ML stat.TH published:2013-11-25 summary:We develop a general approach to valid inference after model selection. Atthe core of our framework is a result that characterizes the distribution of apost-selection estimator conditioned on the selection event. We specialize theapproach to model selection by the lasso to form valid confidence intervals forthe selected coefficients and test whether all relevant variables have beenincluded in the model.
arxiv-17700-19 | Automatic Identification of Retinal Arteries and Veins in Fundus Images using Local Binary Patterns | http://arxiv.org/pdf/1605.00763v1.pdf | author:Nima Hatami, Michael Goldbaum category:cs.CV published:2016-05-03 summary:Artery and vein (AV) classification of retinal images is a key to necessarytasks, such as automated measurement of arteriolar-to-venular diameter ratio(AVR). This paper comprehensively reviews the state-of-the art in AVclassification methods. To improve on previous methods, a new Local Bi- naryPattern-based method (LBP) is proposed. Beside its simplicity, LBP is robustagainst low contrast and low quality fundus images; and it helps the process byincluding additional AV texture and shape information. Experimental resultscompare the performance of the new method with the state-of-the art; and alsomethods with different feature extraction and classification schemas.
arxiv-17700-20 | Efficient Distributed Estimation of Inverse Covariance Matrices | http://arxiv.org/pdf/1605.00758v1.pdf | author:Jesús Arroyo, Elizabeth Hou category:stat.ME stat.ML published:2016-05-03 summary:In distributed systems, communication is a major concern due to issues suchas its vulnerability or efficiency. In this paper, we are interested inestimating sparse inverse covariance matrices when samples are distributed intodifferent machines. We address communication efficiency by proposing a methodwhere, in a single round of communication, each machine transfers a smallsubset of the entries of the inverse covariance matrix. We show that, with thisefficient distributed method, the error rates can be comparable with estimationin a non-distributed setting, and correct model selection is still possible.Practical performance is shown through simulations.
arxiv-17700-21 | EIE: Efficient Inference Engine on Compressed Deep Neural Network | http://arxiv.org/pdf/1602.01528v2.pdf | author:Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A. Horowitz, William J. Dally category:cs.CV cs.AR published:2016-02-04 summary:State-of-the-art deep neural networks (DNNs) have hundreds of millions ofconnections and are both computationally and memory intensive, making themdifficult to deploy on embedded systems with limited hardware resources andpower budgets. While custom hardware helps the computation, fetching weightsfrom DRAM is two orders of magnitude more expensive than ALU operations, anddominates the required power. Previously proposed 'Deep Compression' makes it possible to fit large DNNs(AlexNet and VGGNet) fully in on-chip SRAM. This compression is achieved bypruning the redundant connections and having multiple connections share thesame weight. We propose an energy efficient inference engine (EIE) thatperforms inference on this compressed network model and accelerates theresulting sparse matrix-vector multiplication with weight sharing. Going fromDRAM to SRAM gives EIE 120x energy saving; Exploiting sparsity saves 10x;Weight sharing gives 8x; Skipping zero activations from ReLU saves another 3x.Evaluated on nine DNN benchmarks, EIE is 189x and 13x faster when compared toCPU and GPU implementations of the same DNN without compression. EIE has aprocessing power of 102GOPS/s working directly on a compressed network,corresponding to 3TOPS/s on an uncompressed network, and processes FC layers ofAlexNet at 1.88x10^4 frames/sec with a power dissipation of only 600mW. It is24,000x and 3,400x more energy efficient than a CPU and GPU respectively.Compared with DaDianNao, EIE has 2.9x, 19x and 3x better throughput, energyefficiency and area efficiency.
arxiv-17700-22 | A Two-Stage Shape Retrieval (TSR) Method with Global and Local Features | http://arxiv.org/pdf/1603.01942v3.pdf | author:Xiaqing Pan, Sachin Chachada, C. -C. Jay Kuo category:cs.CV published:2016-03-07 summary:A robust two-stage shape retrieval (TSR) method is proposed to address the 2Dshape retrieval problem. Most state-of-the-art shape retrieval methods arebased on local features matching and ranking. Their retrieval performance isnot robust since they may retrieve globally dissimilar shapes in high ranks. Toovercome this challenge, we decompose the decision process into two stages. Inthe first irrelevant cluster filtering (ICF) stage, we consider both global andlocal features and use them to predict the relevance of gallery shapes withrespect to the query. Irrelevant shapes are removed from the candidate shapeset. After that, a local-features-based matching and ranking (LMR) methodfollows in the second stage. We apply the proposed TSR system to MPEG-7,Kimia99 and Tari1000 three datasets and show that it outperforms all otherexisting methods. The robust retrieval performance of the TSR system isdemonstrated.
arxiv-17700-23 | Learning Attributes Equals Multi-Source Domain Generalization | http://arxiv.org/pdf/1605.00743v1.pdf | author:Chuang Gan, Tianbao Yang, Boqing Gong category:cs.CV published:2016-05-03 summary:Attributes possess appealing properties and benefit many computer visionproblems, such as object recognition, learning with humans in the loop, andimage retrieval. Whereas the existing work mainly pursues utilizing attributesfor various computer vision problems, we contend that the most basicproblem---how to accurately and robustly detect attributes from images---hasbeen left under explored. Especially, the existing work rarely explicitlytackles the need that attribute detectors should generalize well acrossdifferent categories, including those previously unseen. Noting that this isanalogous to the objective of multi-source domain generalization, if we treateach category as a domain, we provide a novel perspective to attributedetection and propose to gear the techniques in multi-source domaingeneralization for the purpose of learning cross-category generalizableattribute detectors. We validate our understanding and approach with extensiveexperiments on four challenging datasets and three different problems.
arxiv-17700-24 | VLSI Extreme Learning Machine: A Design Space Exploration | http://arxiv.org/pdf/1605.00740v1.pdf | author:Enyi Yao, Arindam Basu category:cs.LG cs.ET published:2016-05-03 summary:In this paper, we describe a compact low-power, high performance hardwareimplementation of the extreme learning machine (ELM) for machine learningapplications. Mismatch in current mirrors are used to perform the vector-matrixmultiplication that forms the first stage of this classifier and is the mostcomputationally intensive. Both regression and classification (on UCI datasets) are demonstrated and a design space trade-off between speed, power andaccuracy is explored. Our results indicate that for a wide set of problems,$\sigma V_T$ in the range of $15-25$mV gives optimal results. An input weightmatrix rotation method to extend the input dimension and hidden layer sizebeyond the physical limits imposed by the chip is also described. This allowsus to overcome a major limit imposed on most hardware machine learners. Thechip is implemented in a $0.35 \mu$m CMOS process and occupies a die area ofaround 5 mm $\times$ 5 mm. Operating from a $1$ V power supply, it achieves anenergy efficiency of $0.47$ pJ/MAC at a classification rate of $31.6$ kHz.
arxiv-17700-25 | A propagation matting method based on the Local Sampling and KNN Classification with adaptive feature space | http://arxiv.org/pdf/1605.00732v1.pdf | author:Xiao Chen, Fazhi He category:cs.CV published:2016-05-03 summary:Closed Form is a propagation based matting algorithm, functioning well onimages with good propagation . The deficiency of the Closed Form method is thatfor complex areas with poor image propagation , such as hole areas or areas oflong and narrow structures. The right results are usually hard to get. On theseareas, if certain flags are provided, it can improve the effects of matting. Inthis paper, we design a matting algorithm by local sampling and the KNNclassifier propagation based matting algorithm. First of all, build thecorresponding features space according to the different components of imagecolors to reduce the influence of overlapping between the foreground andbackground, and to improve the classification accuracy of KNN classifier.Second, adaptively use local sampling or using local KNN classifier forprocessing based on the pros and cons of the sample performance of unknownimage areas. Finally, based on different treatment methods for the unknownareas, we will use different weight for augmenting constraints to make thetreatment more effective. In this paper, by combining qualitative observationand quantitative analysis, we will make evaluation of the experimental resultsthrough online standard set of evaluation tests. It shows that on images withgood propagation , this method is as effective as the Closed Form method, whileon images in complex regions, it can perform even better than Closed Form.
arxiv-17700-26 | Data Representation and Compression Using Linear-Programming Approximations | http://arxiv.org/pdf/1511.06606v5.pdf | author:Hristo S. Paskov, John C. Mitchell, Trevor J. Hastie category:cs.LG published:2015-11-20 summary:We propose `Dracula', a new framework for unsupervised feature selection fromsequential data such as text. Dracula learns a dictionary of $n$-grams thatefficiently compresses a given corpus and recursively compresses its owndictionary; in effect, Dracula is a `deep' extension of Compressive FeatureLearning. It requires solving a binary linear program that may be relaxed to alinear program. Both problems exhibit considerable structure, their solutionpaths are well behaved, and we identify parameters which control the depth anddiversity of the dictionary. We also discuss how to derive features from thecompressed documents and show that while certain unregularized linear modelsare invariant to the structure of the compressed dictionary, this structure maybe used to regularize learning. Experiments are presented that demonstrate theefficacy of Dracula's features.
arxiv-17700-27 | Radio Transformer Networks: Attention Models for Learning to Synchronize in Wireless Systems | http://arxiv.org/pdf/1605.00716v1.pdf | author:Timothy J O'Shea, Latha Pemula, Dhruv Batra, T. Charles Clancy category:cs.LG cs.NI cs.SY published:2016-05-03 summary:We introduce learned attention models into the radio machine learning domainfor the task of modulation recognition by leveraging spatial transformernetworks and introducing new radio domain appropriate transformations. Thisattention model allows the network to learn a localization network capable ofsynchronizing and normalizing a radio signal blindly with zero knowledge of thesignals structure based on optimization of the network for classificationaccuracy, sparse representation, and regularization. Using this architecture weare able to outperform our prior results in accuracy vs signal to noise ratioagainst an identical system without attention, however we believe such anattention model has implication far beyond the task of modulation recognition.
arxiv-17700-28 | Provable Sparse Tensor Decomposition | http://arxiv.org/pdf/1502.01425v3.pdf | author:Will Wei Sun, Junwei Lu, Han Liu, Guang Cheng category:stat.ML published:2015-02-05 summary:We propose a novel sparse tensor decomposition method, namely TensorTruncated Power (TTP) method, that incorporates variable selection into theestimation of decomposition components. The sparsity is achieved via anefficient truncation step embedded in the tensor power iteration. Our methodapplies to a broad family of high dimensional latent variable models, includinghigh dimensional Gaussian mixture and mixtures of sparse regressions. Athorough theoretical investigation is further conducted. In particular, we showthat the final decomposition estimator is guaranteed to achieve a localstatistical rate, and further strengthen it to the global statistical rate byintroducing a proper initialization procedure. In high dimensional regimes, theobtained statistical rate significantly improves those shown in the existingnon-sparse decomposition methods. The empirical advantages of TTP are confirmedin extensive simulated results and two real applications of click-through rateprediction and high-dimensional gene clustering.
arxiv-17700-29 | Discovering Useful Parts for Pose Estimation in Sparsely Annotated Datasets | http://arxiv.org/pdf/1605.00707v1.pdf | author:Mikhail Breslav, Tyson L. Hedrick, Stan Sclaroff, Margrit Betke category:cs.CV published:2016-05-02 summary:Our work introduces a novel way to increase pose estimation accuracy bydiscovering parts from unannotated regions of training images. Discovered partsare used to generate more accurate appearance likelihoods for traditionalpart-based models like Pictorial Structures [13] and its derivatives. Ourexperiments on images of a hawkmoth in flight show that our proposed approachsignificantly improves over existing work [27] for this application, while alsobeing more generally applicable. Our proposed approach localizes landmarks atleast twice as accurately as a baseline based on a Mixture of PictorialStructures (MPS) model. Our unique High-Resolution Moth Flight (HRMF) datasetis made publicly available with annotations.
arxiv-17700-30 | Robust Estimation of Self-Exciting Point Process Models with Application to Neuronal Modeling | http://arxiv.org/pdf/1507.03955v2.pdf | author:Abbas Kazemipour, Min Wu, Behtash Babadi category:cs.NE cs.IT cs.SY math.IT math.OC stat.AP published:2015-07-14 summary:We consider the problem of estimating discrete self-exciting point processmodels from limited binary observations, where the history of the processserves as the covariate. We analyze the performance of two classes ofestimators, namely the $\ell_1$-regularized maximum likelihood and greedyestimators, for a canonical self-exciting point process and characterize thesampling tradeoffs required for stable recovery in the non-asymptotic regime.Our results extend those of compressed sensing for linear and generalizedlinear models with i.i.d. covariates to point processes with highlyinter-dependent covariates. We further provide simulation studies as well asapplication to real spiking data from mouse's lateral geniculate nucleus andferret's retinal ganglion cells which agree with our theoretical predictions.
arxiv-17700-31 | Predicting online extremism, content adopters, and interaction reciprocity | http://arxiv.org/pdf/1605.00659v1.pdf | author:Emilio Ferrara, Wen-Qiang Wang, Onur Varol, Alessandro Flammini, Aram Galstyan category:cs.SI cs.LG physics.soc-ph published:2016-05-02 summary:We present a machine learning framework that leverages a mixture of metadata,network, and temporal features to detect extremist users, and predict contentadopters and interaction reciprocity in social media. We exploit a uniquedataset containing millions of tweets generated by more than 25 thousand userswho have been manually identified, reported, and suspended by Twitter due totheir involvement with extremist campaigns. We also leverage millions of tweetsgenerated by a random sample of 25 thousand regular users who were exposed to,or consumed, extremist content. We carry out three forecasting tasks, (i) todetect extremist users, (ii) to estimate whether regular users will adoptextremist content, and finally (iii) to predict whether users will reciprocatecontacts initiated by extremists. All forecasting tasks are set up in twoscenarios: a post hoc (time independent) prediction task on aggregated data,and a simulated real-time prediction task. The performance of our framework isextremely promising, yielding in the different forecasting scenarios up to 93%AUC for extremist user detection, up to 80% AUC for content adoptionprediction, and finally up to 72% AUC for interaction reciprocity forecasting.We conclude by providing a thorough feature analysis that helps determine whichare the emerging signals that provide predictive power in different scenarios.
arxiv-17700-32 | Gaussian Process Domain Experts for Model Adaptation in Facial Behavior Analysis | http://arxiv.org/pdf/1604.02917v2.pdf | author:Stefanos Eleftheriadis, Ognjen Rudovic, Marc P. Deisenroth, Maja Pantic category:stat.ML cs.CV cs.LG published:2016-04-11 summary:We present a novel approach for supervised domain adaptation that is basedupon the probabilistic framework of Gaussian processes (GPs). Specifically, weintroduce domain-specific GPs as local experts for facial expressionclassification from face images. The adaptation of the classifier isfacilitated in probabilistic fashion by conditioning the target expert onmultiple source experts. Furthermore, in contrast to existing adaptationapproaches, we also learn a target expert from available target data solely.Then, a single and confident classifier is obtained by combining thepredictions from multiple experts based on their confidence. Learning of themodel is efficient and requires no retraining/reweighting of the sourceclassifiers. We evaluate the proposed approach on two publicly availabledatasets for multi-class (MultiPIE) and multi-label (DISFA) facial expressionclassification. To this end, we perform adaptation of two contextual factors:'where' (view) and 'who' (subject). We show in our experiments that theproposed approach consistently outperforms both source and target classifiers,while using as few as 30 target examples. It also outperforms thestate-of-the-art approaches for supervised domain adaptation.
arxiv-17700-33 | Algorithms for Learning Sparse Additive Models with Interactions in High Dimensions | http://arxiv.org/pdf/1605.00609v1.pdf | author:Hemant Tyagi, Anastasios Kyrillidis, Bernd Gärtner, Andreas Krause category:cs.LG cs.IT math.IT math.NA stat.ML published:2016-05-02 summary:A function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is a Sparse AdditiveModel (SPAM), if it is of the form $f(\mathbf{x}) = \sum_{l \in\mathcal{S}}\phi_{l}(x_l)$ where $\mathcal{S} \subset [d]$, $\mathcal{S} \lld$. Assuming $\phi$'s, $\mathcal{S}$ to be unknown, there exists extensive workfor estimating $f$ from its samples. In this work, we consider a generalizedversion of SPAMs, that also allows for the presence of a sparse number ofsecond order interaction terms. For some $\mathcal{S}_1 \subset [d],\mathcal{S}_2 \subset {[d] \choose 2}$, with $\mathcal{S}_1 \ll d,\mathcal{S}_2 \ll d^2$, the function $f$ is now assumed to be of the form:$\sum_{p \in \mathcal{S}_1}\phi_{p} (x_p) + \sum_{(l,l^{\prime}) \in\mathcal{S}_2}\phi_{(l,l^{\prime})} (x_l,x_{l^{\prime}})$. Assuming we have thefreedom to query $f$ anywhere in its domain, we derive efficient algorithmsthat provably recover $\mathcal{S}_1,\mathcal{S}_2$ with finite sample bounds.Our analysis covers the noiseless setting where exact samples of $f$ areobtained, and also extends to the noisy setting where the queries are corruptedwith noise. For the noisy setting in particular, we consider two noise modelsnamely: i.i.d Gaussian noise and arbitrary but bounded noise. Our main methodsfor identification of $\mathcal{S}_2$ essentially rely on estimation of sparseHessian matrices, for which we provide two novel compressed sensing basedschemes. Once $\mathcal{S}_1, \mathcal{S}_2$ are known, we show how theindividual components $\phi_p$, $\phi_{(l,l^{\prime})}$ can be estimated viaadditional queries of $f$, with uniform error bounds. Lastly, we providesimulation results on synthetic data that validate our theoretical findings.
arxiv-17700-34 | Decentralized Q-Learning for Stochastic Teams and Games | http://arxiv.org/pdf/1506.07924v2.pdf | author:Gürdal Arslan, Serdar Yüksel category:math.OC cs.GT cs.LG published:2015-06-25 summary:There are only a few learning algorithms applicable to stochastic dynamicteams and games which generalize Markov decision processes to decentralizedstochastic control problems involving possibly self-interested decision makers.Learning in games is generally difficult because of the non-stationaryenvironment in which each decision maker aims to learn its optimal decisionswith minimal information in the presence of the other decision makers who arealso learning. In stochastic dynamic games, learning is more challengingbecause, while learning, the decision makers alter the state of the system andhence the future cost. In this paper, we present decentralized Q-learningalgorithms for stochastic games, and study their convergence for the weaklyacyclic case which includes team problems as an important special case. Thealgorithm is decentralized in that each decision maker has access to only itslocal information, the state information, and the local cost realizations;furthermore, it is completely oblivious to the presence of other decisionmakers. We show that these algorithms converge to equilibrium policies almostsurely in large classes of stochastic games.
arxiv-17700-35 | Graph Clustering Bandits for Recommendation | http://arxiv.org/pdf/1605.00596v1.pdf | author:Shuai Li, Claudio Gentile, Alexandros Karatzoglou category:stat.ML cs.AI cs.IR cs.LG published:2016-05-02 summary:We investigate an efficient context-dependent clustering technique forrecommender systems based on exploration-exploitation strategies throughmulti-armed bandits over multiple users. Our algorithm dynamically groups usersbased on their observed behavioral similarity during a sequence of loggedactivities. In doing so, the algorithm reacts to the currently served user byshaping clusters around him/her but, at the same time, it explores thegeneration of clusters over users which are not currently engaged. We motivatethe effectiveness of this clustering policy, and provide an extensive empiricalanalysis on real-world datasets, showing scalability and improved predictionperformance over state-of-the-art methods for sequential clustering of users inmulti-armed bandit scenarios.
arxiv-17700-36 | The geometry of learning | http://arxiv.org/pdf/1605.00591v1.pdf | author:Gianluca Calcagni category:q-bio.QM cs.NE published:2016-05-02 summary:We establish a correspondence between classical conditioning processes andfractals. The association strength at a given training trial corresponds to apoint in a disconnected set at a given iteration level. In this way, one canrepresent a training process as a hopping on a fractal set, instead of thetraditional learning curve as a function of the trial. The main advantage ofthis novel perspective is to provide an elegant classification of associativetheories in terms of the geometric features of fractal sets. In particular, thedimension of fractals is a parameter that can both measure the efficiency of agiven conditioning model (in terms of the characteristics of the stimuli) andcompare the efficiency of different models. We illustrate the correspondencewith the examples of the Hull, Rescorla-Wagner, and Mackintosh models and showthat they are equivalent to a Cantor set. In doing so, we approximate theMackintosh model with a new formulation in terms of a nonlinear recursiveequation for the strength of association.
arxiv-17700-37 | Sequential visibility-graph motifs | http://arxiv.org/pdf/1512.00297v2.pdf | author:Jacopo Iacovacci, Lucas Lacasa category:cs.LG nlin.CD published:2015-12-01 summary:Visibility algorithms transform time series into graphs and encode dynamicalinformation in their topology, paving the way for graph-theoretical time seriesanalysis as well as building a bridge between nonlinear dynamics and networkscience. In this work we introduce and study the concept of sequentialvisibility graph motifs, smaller substructures of n consecutive nodes thatappear with characteristic frequencies. We develop a theory to compute in anexact way the motif profiles associated to general classes of deterministic andstochastic dynamics. We find that this simple property is indeed a highlyinformative and computationally efficient feature capable to distinguish amongdifferent dynamics and robust against noise contamination. We finally confirmthat it can be used in practice to perform unsupervised learning, by extractingmotif profiles from experimental heart-rate series and being able, accordingly,to disentangle meditative from other relaxation states. Applications of thisgeneral theory include the automatic classification and description ofphysical, biological, and financial time series.
arxiv-17700-38 | Comparison of Optimization Methods in Optical Flow Estimation | http://arxiv.org/pdf/1605.00572v1.pdf | author:Noranart Vesdapunt, Utkarsh Sinha category:cs.CV published:2016-05-02 summary:Optical flow estimation is a widely known problem in computer visionintroduced by Gibson, J.J(1950) to describe the visual perception of human bystimulus objects. Estimation of optical flow model can be achieved by solvingfor the motion vectors from region of interest in the the different timeline.In this paper, we assumed slightly uniform change of velocity between twonearby frames, and solve the optical flow problem by traditional method,Lucas-Kanade(1981). This method performs minimization of errors betweentemplate and target frame warped back onto the template. Solving minimizationsteps requires optimization methods which have diverse convergence rate anderror. We explored first and second order optimization methods, and comparetheir results with Gauss-Newton method in Lucas-Kanade. We generated 105 videoswith 10,500 frames by synthetic objects, and 10 videos with 1,000 frames fromreal world footage. Our experimental results could be used as tuning parametersfor Lucas-Kanade method.
arxiv-17700-39 | Parallel Wavelet Schemes for Images | http://arxiv.org/pdf/1605.00561v1.pdf | author:David Barina, Michal Kula, Pavel Zemcik category:cs.CV published:2016-05-02 summary:In this paper, we introduce several new schemes for calculation of discretewavelet transforms of images. These schemes reduce the number of steps and, asa consequence, allow to reduce the number of synchronizations on parallelarchitectures. As an additional useful property, the proposed schemes canreduce also the number of arithmetic operations. The schemes are primarilydemonstrated on CDF 5/3 and CDF 9/7 wavelets employed in JPEG 2000 imagecompression standard. However, the presented method is general and it can beapplied on any wavelet transform. As a result, our scheme requires only twomemory barriers for 2-D CDF 5/3 transform compared to four barriers in theoriginal separable form or three barriers in the non-separable scheme recentlypublished. Our reasoning is supported by exhaustive experiments on high-endgraphics cards.
arxiv-17700-40 | Joint Segmentation and Deconvolution of Ultrasound Images Using a Hierarchical Bayesian Model based on Generalized Gaussian Priors | http://arxiv.org/pdf/1412.2813v4.pdf | author:Ningning Zhao, Adrian Basarab, Denis Kouame, Jean-Yves Tourneret category:cs.CV published:2014-12-08 summary:This paper proposes a joint segmentation and deconvolution Bayesian methodfor medical ultrasound (US) images. Contrary to piecewise homogeneous images,US images exhibit heavy characteristic speckle patterns correlated with thetissue structures. The generalized Gaussian distribution (GGD) has been shownto be one of the most relevant distributions for characterizing the speckle inUS images. Thus, we propose a GGD-Potts model defined by a label map couplingUS image segmentation and deconvolution. The Bayesian estimators of the unknownmodel parameters, including the US image, the label map and all thehyperparameters are difficult to be expressed in closed form. Thus, weinvestigate a Gibbs sampler to generate samples distributed according to theposterior of interest. These generated samples are finally used to compute theBayesian estimators of the unknown parameters. The performance of the proposedBayesian model is compared with existing approaches via several experimentsconducted on realistic synthetic data and in vivo US images.
arxiv-17700-41 | NetVLAD: CNN architecture for weakly supervised place recognition | http://arxiv.org/pdf/1511.07247v3.pdf | author:Relja Arandjelović, Petr Gronat, Akihiko Torii, Tomas Pajdla, Josef Sivic category:cs.CV cs.LG published:2015-11-23 summary:We tackle the problem of large scale visual place recognition, where the taskis to quickly and accurately recognize the location of a given queryphotograph. We present the following three principal contributions. First, wedevelop a convolutional neural network (CNN) architecture that is trainable inan end-to-end manner directly for the place recognition task. The maincomponent of this architecture, NetVLAD, is a new generalized VLAD layer,inspired by the "Vector of Locally Aggregated Descriptors" image representationcommonly used in image retrieval. The layer is readily pluggable into any CNNarchitecture and amenable to training via backpropagation. Second, we develop atraining procedure, based on a new weakly supervised ranking loss, to learnparameters of the architecture in an end-to-end manner from images depictingthe same places over time downloaded from Google Street View Time Machine.Finally, we show that the proposed architecture significantly outperformsnon-learnt image representations and off-the-shelf CNN descriptors on twochallenging place recognition benchmarks, and improves over currentstate-of-the-art compact image representations on standard image retrievalbenchmarks.
arxiv-17700-42 | Tradeoffs for Space, Time, Data and Risk in Unsupervised Learning | http://arxiv.org/pdf/1605.00529v1.pdf | author:Mario Lucic, Mesrob I. Ohannessian, Amin Karbasi, Andreas Krause category:stat.ML cs.LG published:2016-05-02 summary:Faced with massive data, is it possible to trade off (statistical) risk, and(computational) space and time? This challenge lies at the heart of large-scalemachine learning. Using k-means clustering as a prototypical unsupervisedlearning problem, we show how we can strategically summarize the data (controlspace) in order to trade off risk and time when data is generated by aprobabilistic model. Our summarization is based on coreset constructions fromcomputational geometry. We also develop an algorithm, TRAM, to navigate thespace/time/data/risk tradeoff in practice. In particular, we show that for afixed risk (or data size), as the data size increases (resp. risk increases)the running time of TRAM decreases. Our extensive experiments on real data setsdemonstrate the existence and practical utility of such tradeoffs, not only fork-means but also for Gaussian Mixture Models.
arxiv-17700-43 | On the Compression of Recurrent Neural Networks with an Application to LVCSR acoustic modeling for Embedded Speech Recognition | http://arxiv.org/pdf/1603.08042v2.pdf | author:Rohit Prabhavalkar, Ouais Alsharif, Antoine Bruguier, Ian McGraw category:cs.CL cs.LG cs.NE published:2016-03-25 summary:We study the problem of compressing recurrent neural networks (RNNs). Inparticular, we focus on the compression of RNN acoustic models, which aremotivated by the goal of building compact and accurate speech recognitionsystems which can be run efficiently on mobile devices. In this work, wepresent a technique for general recurrent model compression that jointlycompresses both recurrent and non-recurrent inter-layer weight matrices. Wefind that the proposed technique allows us to reduce the size of our LongShort-Term Memory (LSTM) acoustic model to a third of its original size withnegligible loss in accuracy.
arxiv-17700-44 | Linear-time Outlier Detection via Sensitivity | http://arxiv.org/pdf/1605.00519v1.pdf | author:Mario Lucic, Olivier Bachem, Andreas Krause category:stat.ML cs.LG published:2016-05-02 summary:Outliers are ubiquitous in modern data sets. Distance-based techniques are apopular non-parametric approach to outlier detection as they require no priorassumptions on the data generating distribution and are simple to implement.Scaling these techniques to massive data sets without sacrificing accuracy is achallenging task. We propose a novel algorithm based on the intuition thatoutliers have a significant influence on the quality of divergence-basedclustering solutions. We propose sensitivity - the worst-case impact of a datapoint on the clustering objective - as a measure of outlierness. We then provethat influence, a (non-trivial) upper-bound on the sensitivity, can be computedby a simple linear time algorithm. To scale beyond a single machine, we proposea communication efficient distributed algorithm. In an extensive experimentalevaluation, we demonstrate the effectiveness and establish the statisticalsignificance of the proposed approach. In particular, it outperforms the mostpopular distance-based approaches while being several orders of magnitudefaster.
arxiv-17700-45 | Strong Coresets for Hard and Soft Bregman Clustering with Applications to Exponential Family Mixtures | http://arxiv.org/pdf/1508.05243v2.pdf | author:Mario Lucic, Olivier Bachem, Andreas Krause category:stat.ML cs.LG published:2015-08-21 summary:Coresets are efficient representations of data sets such that models trainedon the coreset are provably competitive with models trained on the originaldata set. As such, they have been successfully used to scale up clusteringmodels such as K-Means and Gaussian mixture models to massive data sets.However, until now, the algorithms and the corresponding theory were usuallyspecific to each clustering problem. We propose a single, practical algorithm to construct strong coresets for alarge class of hard and soft clustering problems based on Bregman divergences.This class includes hard clustering with popular distortion measures such asthe Squared Euclidean distance, the Mahalanobis distance, KL-divergence andItakura-Saito distance. The corresponding soft clustering problems are directlyrelated to popular mixture models due to a dual relationship between Bregmandivergences and Exponential family distributions. Our theoretical resultsfurther imply a randomized polynomial-time approximation scheme for hardclustering. We demonstrate the practicality of the proposed algorithm in anempirical evaluation.
arxiv-17700-46 | Fuzzy clustering of distribution-valued data using adaptive L2 Wasserstein distances | http://arxiv.org/pdf/1605.00513v1.pdf | author:Antonio Irpino, Francisco De Carvalho, Rosanna Verde category:stat.ML published:2016-05-02 summary:Distributional (or distribution-valued) data are a new type of data arisingfrom several sources and are considered as realizations of distributionalvariables. A new set of fuzzy c-means algorithms for data described bydistributional variables is proposed. The algorithms use the $L2$ Wasserstein distance between distributions asdissimilarity measures. Beside the extension of the fuzzy c-means algorithm fordistributional data, and considering a decomposition of the squared $L2$Wasserstein distance, we propose a set of algorithms using different automaticway to compute the weights associated with the variables as well as with theircomponents, globally or cluster-wise. The relevance weights are computed in theclustering process introducing product-to-one constraints. The relevance weights induce adaptive distances expressing the importance ofeach variable or of each component in the clustering process, acting also as avariable selection method in clustering. We have tested the proposed algorithmson artificial and real-world data. Results confirm that the proposed methodsare able to better take into account the cluster structure of the data withrespect to the standard fuzzy c-means, with non-adaptive distances.
arxiv-17700-47 | A hybrid swarm-based algorithm for single-objective optimization problems involving high-cost analyses | http://arxiv.org/pdf/1402.5830v2.pdf | author:Enrico Ampellio, Luca Vassio category:math.OC cs.AI cs.DC cs.NE published:2014-02-24 summary:In many technical fields, single-objective optimization procedures incontinuous domains involve expensive numerical simulations. In this context, animprovement of the Artificial Bee Colony (ABC) algorithm, called the Artificialsuper-Bee enhanced Colony (AsBeC), is presented. AsBeC is designed to providefast convergence speed, high solution accuracy and robust performance over awide range of problems. It implements enhancements of the ABC structure andhybridizations with interpolation strategies. The latter are inspired by thequadratic trust region approach for local investigation and by an efficientglobal optimizer for separable problems. Each modification and their combinedeffects are studied with appropriate metrics on a numerical benchmark, which isalso used for comparing AsBeC with some effective ABC variants and otherderivative-free algorithms. In addition, the presented algorithm is validatedon two recent benchmarks adopted for competitions in international conferences.Results show remarkable competitiveness and robustness for AsBeC.
arxiv-17700-48 | Methods for Sparse and Low-Rank Recovery under Simplex Constraints | http://arxiv.org/pdf/1605.00507v1.pdf | author:Ping Li, Syama Sundar Rangapuram, Martin Slawski category:stat.ME cs.LG published:2016-05-02 summary:The de-facto standard approach of promoting sparsity by means of$\ell_1$-regularization becomes ineffective in the presence of simplexconstraints, i.e.,~the target is known to have non-negative entries summing upto a given constant. The situation is analogous for the use of nuclear normregularization for low-rank recovery of Hermitian positive semidefinitematrices with given trace. In the present paper, we discuss several strategiesto deal with this situation, from simple to more complex. As a starting point,we consider empirical risk minimization (ERM). It follows from existing theorythat ERM enjoys better theoretical properties w.r.t.~prediction and$\ell_2$-estimation error than $\ell_1$-regularization. In light of this, weargue that ERM combined with a subsequent sparsification step like thresholdingis superior to the heuristic of using $\ell_1$-regularization after droppingthe sum constraint and subsequent normalization. At the next level, we show that any sparsity-promoting regularizer undersimplex constraints cannot be convex. A novel sparsity-promoting regularizationscheme based on the inverse or negative of the squared $\ell_2$-norm isproposed, which avoids shortcomings of various alternative methods from theliterature. Our approach naturally extends to Hermitian positive semidefinitematrices with given trace. Numerical studies concerning compressed sensing,sparse mixture density estimation, portfolio optimization and quantum statetomography are used to illustrate the key points of the paper.
arxiv-17700-49 | Compressive PCA for Low-Rank Matrices on Graphs | http://arxiv.org/pdf/1602.02070v3.pdf | author:Nauman Shahid, Nathanael Perraudin, Gilles Puy, Pierre Vandergheynst category:cs.LG published:2016-02-05 summary:We introduce a novel framework for an approximate recovery of data matriceswhich are low-rank on graphs, from sampled measurements. The rows and columnsof such matrices belong to the span of the first few eigenvectors of the graphsconstructed between their rows and columns. We leverage this property torecover the non-linear low-rank structures efficiently from sampled datameasurements, with a low cost (linear in $n$). First, a Resrtricted IsometryProperty (RIP) condition is introduced for efficient uniform sampling of therows and columns of such matrices based on the cumulative coherence of grapheigenvectors. Secondly, a state-of-the-art fast low-rank recovery method issuggested for the sampled data. Finally, several efficient, parallel andparameter-free decoders are presented along with their theoretical analysis fordecoding the low-rank and cluster indicators for the full data matrix. Thus, weovercome the computational limitations of the standard \textit{linear} low-rankrecovery methods for big datasets. Our method can also be seen as a major steptowards efficient recovery of non-linear low-rank structures. On a single coremachine, our method gains a speed up of $p^2/k$ over Robust PCA, where $k \llp$ is the subspace dimension. Numerically, we can recover a low-rank matrix ofsize $10304 \times 1000$ in 15 secs, which is 100 times faster than Robust PCA.
arxiv-17700-50 | Compositional Sentence Representation from Character within Large Context Text | http://arxiv.org/pdf/1605.00482v1.pdf | author:Geonmin Kim, Hwaran Lee, Jisu Choi, Soo-young Lee category:cs.CL published:2016-05-02 summary:In this work, we targeted two problems of representing a sentence on thebasis of a constituent word sequence: a data-sparsity problem innon-compositional word embedding, and no usage of inter-sentence dependency. Toimprove these two problems, we propose a Hierarchical Composition RecurrentNetwork (HCRN), which consists of a hierarchy with 3 levels of compositionalmodels: character, word and sentence. In HCRN, word representations are builtfrom characters, thus resolving the data-sparsity problem. Moreover, aninter-sentence dependency is embedded into the sentence representation at thelevel of sentence composition. In order to alleviate optimization difficulty ofend-to-end learning for the HCRN, we adopt a hierarchy-wise learning scheme.The HCRN was evaluated on a dialogue act classification task quantitatively andqualitatively. Especially, sentence representations with an inter-sentencedependency significantly improved the performance by capturing both implicitand explicit semantics of sentence. In classifying dialogue act on theSWBD-DAMSL database, our HCRN achieved state-of-the-art performance with a testerror rate of 22.7%.
arxiv-17700-51 | Fast Single Image Super-Resolution | http://arxiv.org/pdf/1510.00143v3.pdf | author:Ningning Zhao, Qi Wei, Adrian Basarab, Nicolas Dobigeon, Denis Kouame, Jean-Yves Tourneret category:cs.CV published:2015-10-01 summary:This paper addresses the problem of single image super-resolution (SR), whichconsists of recovering a high resolution image from its blurred, decimated andnoisy version. The existing algorithms for single image SR use differentstrategies to handle the decimation and blurring operators. In addition to thetraditional first-order gradient methods, recent techniques investigatesplitting-based methods dividing the SR problem into up-sampling anddeconvolution steps that can be easily solved. Instead of following thissplitting strategy, we propose to deal with the decimation and blurringoperators simultaneously by taking advantage of their particular properties inthe frequency domain, leading to a new fast SR approach. Specifically, ananalytical solution can be obtained and implemented efficiently for theGaussian prior or any other regularization that can be formulated into an$\ell_2$-regularized quadratic model, i.e., an $\ell_2$-$\ell_2$ optimizationproblem. Furthermore, the flexibility of the proposed SR scheme is shownthrough the use of various priors/regularizations, ranging from generic imagepriors to learning-based approaches. In the case of non-Gaussian priors, weshow how the analytical solution derived from the Gaussian case can be embeddedintotraditional splitting frameworks, allowing the computation cost of existingalgorithms to be decreased significantly. Simulation results conducted onseveral images with different priors illustrate the effectiveness of our fastSR approach compared with the existing techniques.
arxiv-17700-52 | Rolling Shutter Camera Relative Pose: Generalized Epipolar Geometry | http://arxiv.org/pdf/1605.00475v1.pdf | author:Yuchao Dai, Hongdong Li, Laurent Kneip category:cs.CV published:2016-05-02 summary:The vast majority of modern consumer-grade cameras employ a rolling shuttermechanism. In dynamic geometric computer vision applications such as visualSLAM, the so-called rolling shutter effect therefore needs to be properly takeninto account. A dedicated relative pose solver appears to be the first problemto solve, as it is of eminent importance to bootstrap any derivation ofmulti-view geometry. However, despite its significance, it has receivedinadequate attention to date. This paper presents a detailed investigation of the geometry of the rollingshutter relative pose problem. We introduce the rolling shutter essentialmatrix, and establish its link to existing models such as the push-broomcameras, summarized in a clean hierarchy of multi-perspective cameras. Thegeneralization of well-established concepts from epipolar geometry is completedby a definition of the Sampson distance in the rolling shutter case. The workis concluded with a careful investigation of the introduced epipolar geometryfor rolling shutter cameras on several dedicated benchmarks.
arxiv-17700-53 | Multi30K: Multilingual English-German Image Descriptions | http://arxiv.org/pdf/1605.00459v1.pdf | author:Desmond Elliott, Stella Frank, Khalil Sima'an, Lucia Specia category:cs.CL cs.CV published:2016-05-02 summary:We introduce the Multi30K dataset to stimulate multilingual multimodalresearch. Recent advances in image description have been demonstrated onEnglish-language datasets almost exclusively, but image description should notbe limited to English. This dataset extends the Flickr30K dataset with i)German translations created by professional translators over a subset of theEnglish descriptions, and ii) descriptions crowdsourced independently of theoriginal English descriptions. We outline how the data can be used formultilingual image description and multimodal machine translation, but weanticipate the data will be useful for a broader range of tasks.
arxiv-17700-54 | Fourier Analysis and q-Gaussian Functions: Analytical and Numerical Results | http://arxiv.org/pdf/1605.00452v1.pdf | author:Paulo Sérgio Silva Rodrigues, Gilson Antonio Giraldi category:cs.CV published:2016-05-02 summary:It is a consensus in signal processing that the Gaussian kernel and itspartial derivatives enable the development of robust algorithms for featuredetection. Fourier analysis and convolution theory have central role in suchdevelopment. In this paper we collect theoretical elements to follow thisavenue but using the q-Gaussian kernel that is a nonextensive generalization ofthe Gaussian one. Firstly, we review some theoretical elements behind theone-dimensional q-Gaussian and its Fourier transform. Then, we consider thetwo-dimensional q-Gaussian and we highlight the issues behind its analyticalFourier transform computation. We analyze the q-Gaussian kernel in the spaceand Fourier domains using the concepts of space window, cut-off frequency, andthe Heisenberg inequality.
arxiv-17700-55 | An Enhanced Harmony Search Method for Bangla Handwritten Character Recognition Using Region Sampling | http://arxiv.org/pdf/1605.00420v1.pdf | author:Ritesh Sarkhel, Amit K Saha, Nibaran Das category:cs.CV published:2016-05-02 summary:Identification of minimum number of local regions of a handwritten characterimage, containing well-defined discriminating features which are sufficient fora minimal but complete description of the character is a challenging task. Anew region selection technique based on the idea of an enhanced Harmony Searchmethodology has been proposed here. The powerful framework of Harmony Searchhas been utilized to search the region space and detect only the mostinformative regions for correctly recognizing the handwritten character. Theproposed method has been tested on handwritten samples of Bangla Basic,Compound and mixed (Basic and Compound characters) characters separately withSVM based classifier using a longest run based feature-set obtained from theimage subregions formed by a CG based quad-tree partitioning approach. Applyingthis methodology on the above mentioned three types of datasets, respectively43.75%, 12.5% and 37.5% gains have been achieved in terms of region reductionand 2.3%, 0.6% and 1.2% gains have been achieved in terms of recognitionaccuracy. The results show a sizeable reduction in the minimal number ofdescriptive regions as well a significant increase in recognition accuracy forall the datasets using the proposed technique. Thus the time and cost relatedto feature extraction is decreased without dampening the correspondingrecognition accuracy.
arxiv-17700-56 | Gradient Descent Converges to Minimizers: The Case of Non-Isolated Critical Points | http://arxiv.org/pdf/1605.00405v1.pdf | author:Ioannis Panageas, Georgios Piliouras category:math.DS cs.LG published:2016-05-02 summary:We prove that the set of initial conditions so that gradient descentconverges to strict saddle points has (Lebesgue) measure zero, even fornon-isolated critical points, answering an open question in [Lee, Simchowitz,Jordan, Recht, COLT2016].
arxiv-17700-57 | Simple2Complex: Global Optimization by Gradient Descent | http://arxiv.org/pdf/1605.00404v1.pdf | author:Ming Li category:cs.LG cs.NE published:2016-05-02 summary:A method named simple2complex for modeling and training deep neural networksis proposed. Simple2complex train deep neural networks by smoothly adding moreand more layers to the shallow networks, as the learning procedure going on,the network is just like growing. Compared with learning by end2end,simple2complex is with less possibility trapping into local minimal, namely,owning ability for global optimization. Cifar10 is used for verifying thesuperiority of simple2complex.
arxiv-17700-58 | Revise Saturated Activation Functions | http://arxiv.org/pdf/1602.05980v2.pdf | author:Bing Xu, Ruitong Huang, Mu Li category:cs.LG published:2016-02-18 summary:In this paper, we revise two commonly used saturated functions, the logisticsigmoid and the hyperbolic tangent (tanh). We point out that, besides the well-known non-zero centered property, slopeof the activation function near the origin is another possible reason makingtraining deep networks with the logistic function difficult to train. Wedemonstrate that, with proper rescaling, the logistic sigmoid achievescomparable results with tanh. Then following the same argument, we improve tahn by penalizing in thenegative part. We show that "penalized tanh" is comparable and even outperformsthe state-of-the-art non-saturated functions including ReLU and leaky ReLU ondeep convolution neural networks. Our results contradict to the conclusion of previous works that thesaturation property causes the slow convergence. It suggests furtherinvestigation is necessary to better understand activation functions in deeparchitectures.
arxiv-17700-59 | Revisiting Human Action Recognition: Personalization vs. Generalization | http://arxiv.org/pdf/1605.00392v1.pdf | author:Andrea Zunino, Jacopo Cavazza, Vittorio Murino category:cs.CV published:2016-05-02 summary:By thoroughly revisiting the classic human action recognition paradigm, thispaper aims at proposing a new approach for the design of effective actionclassification systems. Taking as testbed publicly available three-dimensional(MoCap) action/activity datasets, we analyzed and validated differenttraining/testing strategies. In particular, considering that each human actionin the datasets is performed several times by different subjects, we were ableto precisely quantify the effect of inter- and intra-subject variability, so asto figure out the impact of several learning approaches in terms ofclassification performance. The net result is that standard testing strategiesconsisting in cross-validating the algorithm using typical splits of the data(holdout, k-fold, or one-subject-out) is always outperformed by a"personalization" strategy which learns how a subject is performing an action.In other words, it is advantageous to customize (i.e., personalize) the methodto learn the actions carried out by each subject, rather than trying togeneralize the actions executions across subjects. Consequently, we finallypropose an action recognition framework consisting of a two-stageclassification approach where, given a test action, the subject is firstidentified before the actual recognition of the action takes place. Despite thebasic, off-the-shelf descriptors and standard classifiers adopted, we noted arelevant increase in performance with respect to standard state-of-the-artalgorithms, so motivating the usage of personalized approaches for designingeffective action recognition systems.
arxiv-17700-60 | Recovery of non-linear cause-effect relationships from linearly mixed neuroimaging data | http://arxiv.org/pdf/1605.00391v1.pdf | author:Sebastian Weichwald, Arthur Gretton, Bernhard Schölkopf, Moritz Grosse-Wentrup category:stat.ME cs.LG stat.AP stat.ML published:2016-05-02 summary:Causal inference concerns the identification of cause-effect relationshipsbetween variables. However, often only linear combinations of variablesconstitute meaningful causal variables. For example, recovering the signal of acortical source from electroencephalography requires a well-tuned combinationof signals recorded at multiple electrodes. We recently introduced the MERLiN(Mixture Effect Recovery in Linear Networks) algorithm that is able to recover,from an observed linear mixture, a causal variable that is a linear effect ofanother given variable. Here we relax the assumption of this cause-effectrelationship being linear and present an extended algorithm that can pick upnon-linear cause-effect relationships. Thus, the main contribution is analgorithm (and ready to use code) that has broader applicability and allows fora richer model class. Furthermore, a comparative analysis indicates that theassumption of linear cause-effect relationships is not restrictive in analysingelectroencephalographic data.
arxiv-17700-61 | Highly Accurate Prediction of Jobs Runtime Classes | http://arxiv.org/pdf/1605.00388v1.pdf | author:Anat Reiner-Benaim, Anna Grabarnick, Edi Shmueli category:stat.ML published:2016-05-02 summary:Separating the short jobs from the long is a known technique to improvescheduling performance. In this paper we describe a method we developed foraccurately predicting the runtimes classes of the jobs to enable thisseparation. Our method uses the fact that the runtimes can be represented as amixture of overlapping Gaussian distributions, in order to train a CARTclassifier to provide the prediction. The threshold that separates the shortjobs from the long jobs is determined during the evaluation of the classifierto maximize prediction accuracy. Our results indicate overall accuracy of 90%for the data set used in our study, with sensitivity and specificity both above90%.
arxiv-17700-62 | Compression Artifacts Removal Using Convolutional Neural Networks | http://arxiv.org/pdf/1605.00366v1.pdf | author:Pavel Svoboda, Michal Hradis, David Barina, Pavel Zemcik category:cs.CV published:2016-05-02 summary:This paper shows that it is possible to train large and deep convolutionalneural networks (CNN) for JPEG compression artifacts reduction, and that suchnetworks can provide significantly better reconstruction quality compared topreviously used smaller networks as well as to any other state-of-the-artmethods. We were able to train networks with 8 layers in a single step and inrelatively short time by combining residual learning, skip architecture, andsymmetric weight initialization. We provide further insights into convolutionnetworks for JPEG artifact reduction by evaluating three different objectives,generalization with respect to training dataset size, and generalization withrespect to JPEG quality level.
arxiv-17700-63 | Contrastive Structured Anomaly Detection for Gaussian Graphical Models | http://arxiv.org/pdf/1605.00355v1.pdf | author:Abhinav Maurya, Mark Cheung category:stat.ML published:2016-05-02 summary:Gaussian graphical models (GGMs) are probabilistic tools of choice foranalyzing conditional dependencies between variables in complex systems.Finding changepoints in the structural evolution of a GGM is thereforeessential to detecting anomalies in the underlying system modeled by the GGM.In order to detect structural anomalies in a GGM, we consider the problem ofestimating changes in the precision matrix of the corresponding Gaussiandistribution. We take a two-step approach to solving this problem:- (i)estimating a background precision matrix using system observations from thepast without any anomalies, and (ii) estimating a foreground precision matrixusing a sliding temporal window during anomaly monitoring. Our primarycontribution is in estimating the foreground precision using a novelcontrastive inverse covariance estimation procedure. In order to accuratelylearn only the structural changes to the GGM, we maximize a penalizedlog-likelihood where the penalty is the $l_1$ norm of difference between theforeground precision being estimated and the already learned backgroundprecision. We modify the alternating direction method of multipliers (ADMM)algorithm for sparse inverse covariance estimation to perform contrastiveestimation of the foreground precision matrix. Our results on simulated GGMdata show significant improvement in precision and recall for detectingstructural changes to the GGM, compared to a non-contrastive sliding windowbaseline.
arxiv-17700-64 | Predicting distributions with Linearizing Belief Networks | http://arxiv.org/pdf/1511.05622v4.pdf | author:Yann N. Dauphin, David Grangier category:cs.LG cs.CV published:2015-11-17 summary:Conditional belief networks introduce stochastic binary variables in neuralnetworks. Contrary to a classical neural network, a belief network can predictmore than the expected value of the output $Y$ given the input $X$. It canpredict a distribution of outputs $Y$ which is useful when an input can admitmultiple outputs whose average is not necessarily a valid answer. Such networksare particularly relevant to inverse problems such as image prediction fordenoising, or text to speech. However, traditional sigmoid belief networks arehard to train and are not suited to continuous problems. This work introduces anew family of networks called linearizing belief nets or LBNs. A LBN decomposesinto a deep linear network where each linear unit can be turned on or off bynon-deterministic binary latent units. It is a universal approximator ofreal-valued conditional distributions and can be trained using gradientdescent. Moreover, the linear pathways efficiently propagate continuousinformation and they act as multiplicative skip-connections that helpoptimization by removing gradient diffusion. This yields a model which trainsefficiently and improves the state-of-the-art on image denoising and facialexpression generation with the Toronto faces dataset.
arxiv-17700-65 | Revolvable Indoor Panoramas Using a Rectified Azimuthal Projection | http://arxiv.org/pdf/1206.2068v4.pdf | author:Chamberlain Fong category:cs.CV math.DG published:2012-06-10 summary:We present an algorithm for converting an indoor spherical panorama into aphotograph with a simulated overhead view. The resulting image will have anextremely wide field of view covering up to 4{\pi} steradians of the sphericalpanorama. We argue that our method complements the stereographic projectioncommonly used in the "little planet" effect. The stereographic projection workswell in creating little planets of outdoor scenes; whereas our method is awell-suited counterpart for indoor scenes. The main innovation of our method isthe introduction of a novel azimuthal map projection that can smoothly blendbetween the stereographic projection and the Lambert azimuthal equal-areaprojection. Our projection has an adjustable parameter that allows one tocontrol and compromise between distortions in shape and distortions in sizewithin the projected panorama. This extra control parameter gives ourprojection the ability to produce superior results over the stereographicprojection.
arxiv-17700-66 | Some Insights into the Geometry and Training of Neural Networks | http://arxiv.org/pdf/1605.00329v1.pdf | author:Ewout van den Berg category:cs.LG published:2016-05-02 summary:Neural networks have been successfully used for classification tasks in arapidly growing number of practical applications. Despite their popularity andwidespread use, there are still many aspects of training and classificationthat are not well understood. In this paper we aim to provide some new insightsinto training and classification by analyzing neural networks from afeature-space perspective. We review and explain the formation of decisionregions and study some of their combinatorial aspects. We place a particularemphasis on the connections between the neural network weight and bias termsand properties of decision boundaries and other regions that exhibit varyinglevels of classification confidence. We show how the error backpropagates inthese regions and emphasize the important role they have in the formation ofgradients. These findings expose the connections between scaling of the weightparameters and the density of the training samples. This sheds more light onthe vanishing gradient problem, explains the need for regularization, andsuggests an approach for subsampling training data to improve performance.
arxiv-17700-67 | Large-scale Binary Quadratic Optimization Using Semidefinite Relaxation and Applications | http://arxiv.org/pdf/1411.7564v4.pdf | author:Peng Wang, Chunhua Shen, Anton van den Hengel, Philip H. S. Torr category:cs.CV published:2014-11-27 summary:In computer vision, many problems such as image segmentation, pixellabelling, and scene parsing can be formulated as binary quadratic programs(BQPs). For submodular problems, cuts based methods can be employed toefficiently solve large-scale problems. However, general nonsubmodular problemsare significantly more challenging to solve. Finding a solution when theproblem is of large size to be of practical interest, however, typicallyrequires relaxation. Two standard relaxation methods are widely used forsolving general BQPs--spectral methods and semidefinite programming (SDP), eachwith their own advantages and disadvantages. Spectral relaxation is simple andeasy to implement, but its bound is loose. Semidefinite relaxation has atighter bound, but its computational complexity is high, especially for largescale problems. In this work, we present a new SDP formulation for BQPs, withtwo desirable properties. First, it has a similar relaxation bound toconventional SDP formulations. Second, compared with conventional SDP methods,the new SDP formulation leads to a significantly more efficient and scalabledual optimization approach, which has the same degree of complexity as spectralmethods. We then propose two solvers, namely, quasi-Newton and smoothing Newtonmethods, for the dual problem. Both of them are significantly more efficientlythan standard interior-point methods. In practice, the smoothing Newton solveris faster than the quasi-Newton solver for dense or medium-sized problems,while the quasi-Newton solver is preferable for large sparse/structuredproblems. Our experiments on a few computer vision applications includingclustering, image segmentation, co-segmentation and registration show thepotential of our SDP formulation for solving large-scale BQPs.
arxiv-17700-68 | Dominant Codewords Selection with Topic Model for Action Recognition | http://arxiv.org/pdf/1605.00324v1.pdf | author:Hirokatsu Kataoka, Masaki Hayashi, Kenji Iwata, Yutaka Satoh, Yoshimitsu Aoki, Slobodan Ilic category:cs.CV published:2016-05-01 summary:In this paper, we propose a framework for recognizing human activities thatuses only in-topic dominant codewords and a mixture of intertopic vectors.Latent Dirichlet allocation (LDA) is used to develop approximations of humanmotion primitives; these are mid-level representations, and they adaptivelyintegrate dominant vectors when classifying human activities. In LDA topicmodeling, action videos (documents) are represented by a bag-of-words (inputfrom a dictionary), and these are based on improved dense trajectories. Theoutput topics correspond to human motion primitives, such as finger moving orsubtle leg motion. We eliminate the impurities, such as missed tracking orchanging light conditions, in each motion primitive. The assembled vector ofmotion primitives is an improved representation of the action. We demonstrateour method on four different datasets.
arxiv-17700-69 | Directional Statistics in Machine Learning: a Brief Review | http://arxiv.org/pdf/1605.00316v1.pdf | author:Suvrit Sra category:stat.ML published:2016-05-01 summary:The modern data analyst must cope with data encoded in various forms,vectors, matrices, strings, graphs, or more. Consequently, statistical andmachine learning models tailored to different data encodings are important. Wefocus on data encoded as normalized vectors, so that their "direction" is moreimportant than their magnitude. Specifically, we consider high-dimensionalvectors that lie either on the surface of the unit hypersphere or on the realprojective plane. For such data, we briefly review common mathematical modelsprevalent in machine learning, while also outlining some technical aspects,software, applications, and open mathematical challenges.
arxiv-17700-70 | Shaping the Future through Innovations: From Medical Imaging to Precision Medicine | http://arxiv.org/pdf/1605.02029v1.pdf | author:Dorin Comaniciu, Klaus Engel, Bogdan Georgescu, Tommaso Mansi category:cs.CV cs.CE published:2016-05-01 summary:Medical images constitute a source of information essential for diseasediagnosis, treatment and follow-up. In addition, due to its patient-specificnature, imaging information represents a critical component required foradvancing precision medicine into clinical practice. This manuscript describesrecently developed technologies for better handling of image information:photorealistic visualization of medical images with Cinematic Rendering,artificial agents for in-depth image understanding, support for minimallyinvasive procedures, and patient-specific computational models with enhancedpredictive power. Throughout the manuscript we will analyze the capabilities ofsuch technologies and extrapolate on their potential impact to advance thequality of medical care, while reducing its cost.
arxiv-17700-71 | Multidimensional Scaling on Multiple Input Distance Matrices | http://arxiv.org/pdf/1605.00286v1.pdf | author:Song Bai, Xiang Bai, Longin Jan Latecki, Qi Tian category:cs.CV published:2016-05-01 summary:Multidimensional Scaling (MDS) is a classic technique that seeks vectorialrepresentations for data points, given the pairwise distances between them.However, in recent years, data are usually collected from diverse sources orhave multiple heterogeneous representations. How to do multidimensional scalingon multiple input distance matrices is still unsolved to our best knowledge. Inthis paper, we first define this new task formally. Then, we propose a newalgorithm called Multi-View Multidimensional Scaling (MVMDS) by consideringeach input distance matrix as one view. Our algorithm is able to learn theweights of views (i.e., distance matrices) automatically by exploring theconsensus information and complementary nature of views. Experimental resultson synthetic as well as real datasets demonstrate the effectiveness of MVMDS.We hope that our work encourages a wider consideration in many domains whereMDS is needed.
arxiv-17700-72 | Robust Elastic Net Regression | http://arxiv.org/pdf/1511.04690v2.pdf | author:Weiyang Liu, Rongmei Lin, Meng Yang category:cs.LG stat.ML published:2015-11-15 summary:We propose a robust elastic net (REN) model for high-dimensional sparseregression and give its performance guarantees (both the statistical errorbound and the optimization bound). A simple idea of trimming the inner productis applied to the elastic net model. Specifically, we robustify the covariancematrix by trimming the inner product based on the intuition that the trimmedinner product can not be significant affected by a bounded number ofarbitrarily corrupted points (outliers). The REN model can also derive twointeresting special cases: robust Lasso and robust soft thresholding.Comprehensive experimental results show that the robustness of the proposedmodel consistently outperforms the original elastic net and matches theperformance guarantees nicely.
arxiv-17700-73 | Particle Smoothing for Hidden Diffusion Processes: Adaptive Path Integral Smoother | http://arxiv.org/pdf/1605.00278v1.pdf | author:H. -Ch. Ruiz, H. J. Kappen category:cs.LG stat.CO published:2016-05-01 summary:Particle smoothing methods are used for inference of stochastic processesbased on noisy observations. Typically, the estimation of the marginalposterior distribution given all observations is cumbersome and computationalintensive. In this paper, we propose a simple algorithm based on path integralcontrol theory to estimate the smoothing distribution of continuous-timediffusion processes with partial observations. In particular, we use anadaptive importance sampling method to improve the effective sampling size ofthe posterior over processes given the observations and the reliability of theestimation of the marginals. This is achieved by estimating a feedbackcontroller to sample efficiently from the joint smoothing distributions. Wecompare the results with estimations obtained from the standard ForwardFilter/Backward Simulator for two diffusion processes of different complexity.We show that the proposed method gives more reliable estimations than thestandard FFBSi when the smoothing distribution is poorly represented by thefilter distribution.
arxiv-17700-74 | MODS: Fast and Robust Method for Two-View Matching | http://arxiv.org/pdf/1503.02619v2.pdf | author:Dmytro Mishkin, Jiri Matas, Michal Perdoch category:cs.CV published:2015-03-09 summary:A novel algorithm for wide-baseline matching called MODS - Matching On Demandwith view Synthesis - is presented. The MODS algorithm is experimentally shownto solve a broader range of wide-baseline problems than the state of the artwhile being nearly as fast as standard matchers on simple problems. Theapparent robustness vs. speed trade-off is finessed by the use of progressivelymore time-consuming feature detectors and by on-demand generation ofsynthesized images that is performed until a reliable estimate of geometry isobtained. We introduce an improved method for tentative correspondence selection,applicable both with and without view synthesis. A modification of the standardfirst to second nearest distance rule increases the number of correct matchesby 5-20% at no additional computational cost. Performance of the MODS algorithm is evaluated on several standard publiclyavailable datasets, and on a new set of geometrically challenging wide baselineproblems that is made public together with the ground truth. Experiments showthat the MODS outperforms the state-of-the-art in robustness and speed.Moreover, MODS performs well on other classes of difficult two-view problemslike matching of images from different modalities, with wide temporal baselineor with significant lighting changes.
arxiv-17700-75 | Fast Rates with Unbounded Losses | http://arxiv.org/pdf/1605.00252v1.pdf | author:Peter D. Grünwald, Nishant A. Mehta category:cs.LG stat.ML published:2016-05-01 summary:We present new excess risk bounds for randomized and deterministic estimatorsfor general unbounded loss functions including log loss and squared loss. Ourbounds are expressed in terms of the information complexity and hold under therecently introduced $v$-central condition, allowing for high-probabilitybounds, and its weakening, the $v$-pseudoprobability convexity condition,allowing for bounds in expectation even under heavy-tailed distributions. Theparameter $v$ determines the achievable rate and is akin to the exponent in theTsybakov margin condition and the Bernstein condition for bounded losses, whichthe $v$-conditions generalize; favorable $v$ in combination with smallinformation complexity leads to $\tilde{O}(1/n)$ rates. While these fast rateconditions control the lower tail of the excess loss, the upper tail iscontrolled by a new type of witness-of-badness condition which allows us toconnect the excess risk to a generalized R\'enyi divergence, generalizingprevious results connecting Hellinger distance to KL divergence.
arxiv-17700-76 | A vector-contraction inequality for Rademacher complexities | http://arxiv.org/pdf/1605.00251v1.pdf | author:Andreas Maurer category:cs.LG stat.ML published:2016-05-01 summary:The contraction inequality for Rademacher averages is extended to Lipschitzfunctions with vector-valued domains, and it is also shown that in the boundingexpression the Rademacher variables can be replaced by arbitrary iid symmetricand sub-gaussian variables. Example applications are given for multi-categorylearning, K-means clustering and learning-to-learn.
arxiv-17700-77 | Clustering Markov Decision Processes For Continual Transfer | http://arxiv.org/pdf/1311.3959v4.pdf | author:M. M. Hassan Mahmud, Majd Hawasly, Benjamin Rosman, Subramanian Ramamoorthy category:cs.AI cs.LG 68T05 I.2.6 published:2013-11-15 summary:We present algorithms to effectively represent a set of Markov decisionprocesses (MDPs), whose optimal policies have already been learned, by asmaller source subset for lifelong, policy-reuse-based transfer learning inreinforcement learning. This is necessary when the number of previous tasks islarge and the cost of measuring similarity counteracts the benefit of transfer.The source subset forms an `$\epsilon$-net' over the original set of MDPs, inthe sense that for each previous MDP $M_p$, there is a source $M^s$ whoseoptimal policy has $<\epsilon$ regret in $M_p$. Our contributions are asfollows. We present EXP-3-Transfer, a principled policy-reuse algorithm thatoptimally reuses a given source policy set when learning for a new MDP. Wepresent a framework to cluster the previous MDPs to extract a source subset.The framework consists of (i) a distance $d_V$ over MDPs to measurepolicy-based similarity between MDPs; (ii) a cost function $g(\cdot)$ that uses$d_V$ to measure how good a particular clustering is for generating usefulsource tasks for EXP-3-Transfer and (iii) a provably convergent algorithm,MHAV, for finding the optimal clustering. We validate our algorithms throughexperiments in a surveillance domain.
arxiv-17700-78 | Common-Description Learning: A Framework for Learning Algorithms and Generating Subproblems from Few Examples | http://arxiv.org/pdf/1605.00241v1.pdf | author:Basem G. El-Barashy category:cs.AI cs.LG published:2016-05-01 summary:Current learning algorithms face many difficulties in learning simplepatterns and using them to learn more complex ones. They also require moreexamples than humans do to learn the same pattern, assuming no prior knowledge.In this paper, a new learning framework is introduced that is calledcommon-description learning (CDL). This framework has been tested on 32 smallmulti-task datasets, and the results show that it was able to learn complexalgorithms from a few number of examples. The final model is perfectlyinterpretable and its depth depends on the question. What is meant by depthhere is that whenever needed, the model learns to break down the problem intosimpler subproblems and solves them using previously learned models. Finally,we explain the capabilities of our framework in discovering complex relationsin data and how it can help in improving language understanding in machines.
arxiv-17700-79 | Text-mining the NeuroSynth corpus using Deep Boltzmann Machines | http://arxiv.org/pdf/1605.00223v1.pdf | author:Ricardo Pio Monti, Romy Lorenz, Robert Leech, Christoforos Anagnostopoulos, Giovanni Montana category:cs.LG cs.CL q-bio.NC stat.ML published:2016-05-01 summary:Large-scale automated meta-analysis of neuroimaging data has recentlyestablished itself as an important tool in advancing our understanding of humanbrain function. This research has been pioneered by NeuroSynth, a databasecollecting both brain activation coordinates and associated text across a largecohort of neuroimaging research papers. One of the fundamental aspects of suchmeta-analysis is text-mining. To date, word counts and more sophisticatedmethods such as Latent Dirichlet Allocation have been proposed. In this work wepresent an unsupervised study of the NeuroSynth text corpus using DeepBoltzmann Machines (DBMs). The use of DBMs yields several advantages over theaforementioned methods, principal among which is the fact that it yields bothword and document embeddings in a high-dimensional vector space. Suchembeddings serve to facilitate the use of traditional machine learningtechniques on the text corpus. The proposed DBM model is shown to learnembeddings with a clear semantic structure.
arxiv-17700-80 | Deep Learning for Surface Material Classification Using Haptic And Visual Information | http://arxiv.org/pdf/1512.06658v2.pdf | author:Haitian Zheng, Lu Fang, Mengqi Ji, Matti Strese, Yigitcan Ozer, Eckehard Steinbach category:cs.RO cs.CV cs.LG published:2015-12-21 summary:When a user scratches a hand-held rigid tool across an object surface, anacceleration signal can be captured, which carries relevant information aboutthe surface. More importantly, such a haptic signal is complementary to thevisual appearance of the surface, which suggests the combination of bothmodalities for the recognition of the surface material. In this paper, wepresent a novel deep learning method dealing with the surface materialclassification problem based on a Fully Convolutional Network (FCN), whichtakes as input the aforementioned acceleration signal and a corresponding imageof the surface texture. Compared to previous surface material classificationsolutions, which rely on a careful design of hand-crafted domain-specificfeatures, our method automatically extracts discriminative features utilizingthe advanced deep learning methodologies. Experiments performed on the TUMsurface material database demonstrate that our method achieves state-of-the-artclassification accuracy robustly and efficiently.
arxiv-17700-81 | Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation | http://arxiv.org/pdf/1601.01343v3.pdf | author:Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, Yoshiyasu Takefuji category:cs.CL published:2016-01-06 summary:Named Entity Disambiguation (NED) refers to the task of resolving multiplenamed entity mentions in a document to their correct references in a knowledgebase (KB) (e.g., Wikipedia). In this paper, we propose a novel embedding methodspecifically designed for NED. The proposed method jointly maps words andentities into the same continuous vector space. We extend the skip-gram modelby using two models. The KB graph model learns the relatedness of entitiesusing the link structure of the KB, whereas the anchor context model aims toalign vectors such that similar words and entities occur close to one anotherin the vector space by leveraging KB anchors and their context words. Bycombining contexts based on the proposed embedding with standard NED features,we achieved state-of-the-art accuracy of 93.1% on the standard CoNLL datasetand 85.2% on the TAC 2010 dataset.
arxiv-17700-82 | Further properties of the forward-backward envelope with applications to difference-of-convex programming | http://arxiv.org/pdf/1605.00201v1.pdf | author:Tianxiang Liu, Ting Kei Pong category:math.OC stat.ML published:2016-05-01 summary:In this paper, we further study the forward-backward envelope firstintroduced in [27] and [29] for problems whose objective is the sum of a properclosed convex function and a smooth possibly nonconvex function with Lipschitzcontinuous gradient. We derive sufficient conditions on the original problemfor the corresponding forward-backward envelope to be a level-bounded andKurdyka-{\L}ojasiewicz function with an exponent of $\frac12$; these resultsare important for the efficient minimization of the forward-backward envelopeby classical optimization algorithms. In addition, we demonstrate how tominimize some difference-of-convex regularized least squares problems byminimizing a suitably constructed forward-backward envelope. Our preliminarynumerical results on randomly generated instances of large-scale $\ell_{1-2}$regularized least squares problems [36] illustrate that an implementation ofthis approach with a limited-memory BFGS scheme outperforms some standardfirst-order methods such as the nonmonotone proximal gradient method in [34].
arxiv-17700-83 | Adaptive Concentration of Regression Trees, with Application to Random Forests | http://arxiv.org/pdf/1503.06388v3.pdf | author:Stefan Wager, Guenther Walther category:math.ST stat.ML stat.TH published:2015-03-22 summary:We study the convergence of the predictive surface of regression trees andforests. To support our analysis we introduce a notion of adaptiveconcentration for regression trees. This approach breaks tree training into amodel selection phase in which we pick the tree splits, followed by a modelfitting phase where we find the best regression model consistent with thesesplits. We then show that the fitted regression tree concentrates around theoptimal predictor with the same splits: as d and n get large, the discrepancyis with high probability bounded on the order of sqrt(log(d) log(n)/k)uniformly over the whole regression surface, where d is the dimension of thefeature space, n is the number of training examples, and k is the minimum leafsize for each tree. We also provide rate-matching lower bounds for thisadaptive concentration statement. From a practical perspective, our resultenables us to prove consistency results for adaptively grown forests in highdimensions, and to carry out valid post-selection inference in the sense ofBerk et al. [2013] for subgroups defined by tree leaves.
arxiv-17700-84 | Enforcing Template Representability and Temporal Consistency for Adaptive Sparse Tracking | http://arxiv.org/pdf/1605.00170v1.pdf | author:Xue Yang, Fei Han, Hua Wang, Hao Zhang category:cs.CV published:2016-04-30 summary:Sparse representation has been widely studied in visual tracking, which hasshown promising tracking performance. Despite a lot of progress, the visualtracking problem is still a challenging task due to appearance variations overtime. In this paper, we propose a novel sparse tracking algorithm that welladdresses temporal appearance changes, by enforcing template representabilityand temporal consistency (TRAC). By modeling temporal consistency, ouralgorithm addresses the issue of drifting away from a tracking target. Byexploring the templates' long-term-short-term representability, the proposedmethod adaptively updates the dictionary using the most descriptive templates,which significantly improves the robustness to target appearance changes. Wecompare our TRAC algorithm against the state-of-the-art approaches on 12challenging benchmark image sequences. Both qualitative and quantitativeresults demonstrate that our algorithm significantly outperforms previousstate-of-the-art trackers.
arxiv-17700-85 | Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks | http://arxiv.org/pdf/1511.06390v2.pdf | author:Jost Tobias Springenberg category:stat.ML cs.LG published:2015-11-19 summary:In this paper we present a method for learning a discriminative classifierfrom unlabeled or partially labeled data. Our approach is based on an objectivefunction that trades-off mutual information between observed examples and theirpredicted categorical class distribution, against robustness of the classifierto an adversarial generative model. The resulting algorithm can either beinterpreted as a natural generalization of the generative adversarial networks(GAN) framework or as an extension of the regularized information maximization(RIM) framework to robust classification against an optimal adversary. Weempirically evaluate our method - which we dub categorical generativeadversarial networks (or CatGAN) - on synthetic data as well as on challengingimage classification tasks, demonstrating the robustness of the learnedclassifiers. We further qualitatively assess the fidelity of samples generatedby the adversarial generator that is learned alongside the discriminativeclassifier, and identify links between the CatGAN objective and discriminativeclustering algorithms (such as RIM).
arxiv-17700-86 | Look-ahead before you leap: end-to-end active recognition by forecasting the effect of motion | http://arxiv.org/pdf/1605.00164v1.pdf | author:Dinesh Jayaraman, Kristen Grauman category:cs.CV cs.AI cs.LG cs.RO published:2016-04-30 summary:Visual recognition systems mounted on autonomous moving agents face thechallenge of unconstrained data, but simultaneously have the opportunity toimprove their performance by moving to acquire new views of test data. In thiswork, we first show how a recurrent neural network-based system may be trainedto perform end-to-end learning of motion policies suited for the "activerecognition" setting. Further, we hypothesize that active vision requires anagent to have the capacity to reason about the effects of its motions on itsview of the world. To verify this hypothesis, we attempt to induce thiscapacity in our active recognition pipeline, by simultaneously learning toforecast the effects of the agent's motions on its internal representation ofits cumulative knowledge obtained from all past views. Results across twochallenging datasets confirm both that our end-to-end system successfullylearns meaningful policies for active recognition, and that "learning to lookahead" further boosts recognition performance.
arxiv-17700-87 | Kernel Balancing: A flexible non-parametric weighting procedure for estimating causal effects | http://arxiv.org/pdf/1605.00155v1.pdf | author:Chad Hazlett category:stat.ME math.ST stat.AP stat.ML stat.TH published:2016-04-30 summary:In the absence of unobserved confounders, matching and weighting methods arewidely used to estimate causal quantities including the Average TreatmentEffect on the Treated (ATT). Unfortunately, these methods do not necessarilyachieve their goal of making the multivariate distribution of covariates forthe control group identical to that of the treated, leaving some (potentiallymultivariate) functions of the covariates with different means between the twogroups. When these "imbalanced" functions influence the non-treatment potentialoutcome, the conditioning on observed covariates fails, and ATT estimates maybe biased. Kernel balancing, introduced here, targets a weaker requirement forunbiased ATT estimation, specifically, that the expected non-treatmentpotential outcome for the treatment and control groups are equal. Theconditional expectation of the non-treatment potential outcome is assumed tofall in the space of functions associated with a choice of kernel, implying aset of basis functions in which this regression surface is linear. Weights arethen chosen on the control units such that the treated and control group haveequal means on these basis functions. As a result, the expectation of thenon-treatment potential outcome must also be equal for the treated and controlgroups after weighting, allowing unbiased ATT estimation by subsequentdifference in means or an outcome model using these weights. Moreover, theweights produced are (1) precisely those that equalize a particularkernel-based approximation of the multivariate distribution of covariates forthe treated and control, and (2) equivalent to a form of stabilized inversepropensity score weighting, though it does not require assuming any model ofthe treatment assignment mechanism. An R package, KBAL, is provided toimplement this approach.
arxiv-17700-88 | Multi-Scale Context Aggregation by Dilated Convolutions | http://arxiv.org/pdf/1511.07122v3.pdf | author:Fisher Yu, Vladlen Koltun category:cs.CV published:2015-11-23 summary:State-of-the-art models for semantic segmentation are based on adaptations ofconvolutional networks that had originally been designed for imageclassification. However, dense prediction and image classification arestructurally different. In this work, we develop a new convolutional networkmodule that is specifically designed for dense prediction. The presented moduleuses dilated convolutions to systematically aggregate multi-scale contextualinformation without losing resolution. The architecture is based on the factthat dilated convolutions support exponential expansion of the receptive fieldwithout loss of resolution or coverage. We show that the presented contextmodule increases the accuracy of state-of-the-art semantic segmentationsystems. In addition, we examine the adaptation of image classificationnetworks to dense prediction and show that simplifying the adapted network canincrease accuracy.
arxiv-17700-89 | 3D Keypoint Detection Based on Deep Neural Network with Sparse Autoencoder | http://arxiv.org/pdf/1605.00129v1.pdf | author:Xinyu Lin, Ce Zhu, Qian Zhang, Yipeng Liu category:cs.CV published:2016-04-30 summary:Researchers have proposed various methods to extract 3D keypoints from thesurface of 3D mesh models over the last decades, but most of them are based ongeometric methods, which lack enough flexibility to meet the requirements forvarious applications. In this paper, we propose a new method on the basis ofdeep learning by formulating the 3D keypoint detection as a regression problemusing deep neural network (DNN) with sparse autoencoder (SAE) as our regressionmodel. Both local information and global information of a 3D mesh model inmulti-scale space are fully utilized to detect whether a vertex is a keypointor not. SAE can effectively extract the internal structure of these two kindsof information and formulate high-level features for them, which is beneficialto the regression model. Three SAEs are used to formulate the hidden layers ofthe DNN and then a logistic regression layer is trained to process thehigh-level features produced in the third SAE. Numerical experiments show thatthe proposed DNN based 3D keypoint detection algorithm outperforms current fivestate-of-the-art methods for various 3D mesh models.
arxiv-17700-90 | Computational Cost Reduction in Learned Transform Classifications | http://arxiv.org/pdf/1504.06779v2.pdf | author:Emerson Lopes Machado, Cristiano Jacques Miosso, Ricardo von Borries, Murilo Coutinho, Pedro de Azevedo Berger, Thiago Marques, Ricardo Pezzuol Jacobi category:cs.CV stat.ML published:2015-04-26 summary:We present a theoretical analysis and empirical evaluations of a novel set oftechniques for computational cost reduction of classifiers that are based onlearned transform and soft-threshold. By modifying optimization procedures fordictionary and classifier training, as well as the resulting dictionaryentries, our techniques allow to reduce the bit precision and to replace eachfloating-point multiplication by a single integer bit shift. We also show howthe optimization algorithms in some dictionary training methods can be modifiedto penalize higher-energy dictionaries. We applied our techniques with theclassifier Learning Algorithm for Soft-Thresholding, testing on the datasetsused in its original paper. Our results indicate it is feasible to use solelysums and bit shifts of integers to classify at test time with a limitedreduction of the classification accuracy. These low power operations are avaluable trade off in FPGA implementations as they increase the classificationthroughput while decrease both energy consumption and manufacturing cost.
arxiv-17700-91 | A Repeated Signal Difference for Recognising Patterns | http://arxiv.org/pdf/1604.05170v2.pdf | author:Kieran Greer category:cs.NE cs.AI q-bio.NC published:2016-04-18 summary:This paper describes a new mechanism that might help with defining patternsequences, by the fact that it can produce an upper bound on the ensemble valuethat can persistently oscillate with the actual values produced from eachpattern. With every firing event, a node also receives an on/off feedbackswitch. If the node fires, then it sends a feedback result depending on theinput signal strength. If the input signal is positive or larger, it can store'on' switch feedback for the next iteration. If the signal is negative orsmaller, it can store an 'off' switch feedback for the next iteration. If thenode does not fire, then it does not affect the current feedback situation andreceives the switch command produced by the last active pattern event for thesame neuron. The upper bound therefore also represents the largest or mostenclosing pattern set and the lower value is for the actual set of firingpatterns. If the pattern sequence repeats, it will oscillate between the twovalues, allowing them to be recognised and measured more easily, over time.Tests show that changing the sequence ordering can also be measured.
arxiv-17700-92 | Kernel-based Tests for Joint Independence | http://arxiv.org/pdf/1603.00285v2.pdf | author:Niklas Pfister, Peter Bühlmann, Bernhard Schölkopf, Jonas Peters category:math.ST stat.ML stat.TH published:2016-03-01 summary:We investigate the problem of testing whether $d$ random variables, which mayor may not be continuous, are jointly (or mutually) independent. Our methodbuilds on ideas of the two variable Hilbert-Schmidt independence criterion(HSIC) but allows for an arbitrary number of variables. We embed the$d$-dimensional joint distribution and the product of the marginals into areproducing kernel Hilbert space and define the $d$-variable Hilbert-Schmidtindependence criterion (dHSIC) as the squared distance between the embeddings.In the population case, the value of dHSIC is zero if and only if the $d$variables are jointly independent, as long as the kernel is characteristic.Based on an empirical estimate of dHSIC, we define three differentnon-parametric hypothesis tests: a permutation test, a bootstrap test and atest based on a Gamma approximation. We prove that the permutation testachieves the significance level and that the bootstrap test achieves pointwiseasymptotic significance level as well as pointwise asymptotic consistency(i.e., it is able to detect any type of fixed dependence in the large samplelimit). The Gamma approximation does not come with these guarantees; however,it is computationally very fast and for small $d$, it performs well inpractice. Finally, we apply the test to a problem in causal discovery.
arxiv-17700-93 | Application of artificial neural networks and genetic algorithms for crude fractional distillation process modeling | http://arxiv.org/pdf/1605.00097v1.pdf | author:Lukasz Pater category:cs.NE published:2016-04-30 summary:This work presents the application of the artificial neural networks, trainedand structurally optimized by genetic algorithms, for modeling of crudedistillation process at PKN ORLEN S.A. refinery. Models for the mainfractionator distillation column products were developed using historical data.Quality of the fractions were predicted based on several chosen processvariables. The performance of the model was validated using test data. Neuralnetworks used in companion with genetic algorithms proved that they canaccurately predict fractions quality shifts, reproducing the results of thestandard laboratory analysis. Simple knowledge extraction method from neuralnetwork model built was also performed. Genetic algorithms can be successfullyutilized in efficient training of large neural networks and finding theiroptimal structures.
arxiv-17700-94 | Topic Augmented Neural Network for Short Text Conversation | http://arxiv.org/pdf/1605.00090v1.pdf | author:Yu Wu, Wei Wu, Zhoujun Li, Ming Zhou category:cs.CL published:2016-04-30 summary:We consider matching input messages with proper responses for short-textconversation. The matching should be performed not only by the messages and theresponses but also by the topics of the messages. To this end, we propose atopic augmented neural network which consists of a sentence embedding layer, atopic embedding layer, and a matching layer. The sentence embedding layerembeds an input message and a response into a vector space, while the topicembedding layer forms a topic vector by a linear combination of the embeddingof the topic words whose weights are determined by both themselves and themessage vector. The message vector, the response vector, and the topic vectorare then fed to the matching layer to calculate the matching score between themessage and the response. Empirical study on large scale annotated data showsthat our model can significantly outperform state of the art matching models.
arxiv-17700-95 | Constructive neural network learning | http://arxiv.org/pdf/1605.00079v1.pdf | author:Shaobo Lin, Jinshan Zeng, Xiaoqin Zhang category:cs.LG published:2016-04-30 summary:In this paper, we aim at developing scalable neural network-type learningsystems. Motivated by the idea of "constructive neural networks" inapproximation theory, we focus on "constructing" rather than "training"feed-forward neural networks (FNNs) for learning, and propose a novel FNNslearning system called the constructive feed-forward neural network (CFN).Theoretically, we prove that the proposed method not only overcomes theclassical saturation problem for FNN approximation, but also reaches theoptimal learning rate when the regression function is smooth, while thestate-of-the-art learning rates established for traditional FNNs are only nearoptimal (up to a logarithmic factor). A series of numerical simulations areprovided to show the efficiency and feasibility of CFN via comparing with thewell-known regularized least squares (RLS) with Gaussian kernel and extremelearning machine (ELM).
arxiv-17700-96 | Deep Colorization | http://arxiv.org/pdf/1605.00075v1.pdf | author:Zezhou Cheng, Qingxiong Yang, Bin Sheng category:cs.CV published:2016-04-30 summary:This paper investigates into the colorization problem which converts agrayscale image to a colorful version. This is a very difficult problem andnormally requires manual adjustment to achieve artifact-free quality. Forinstance, it normally requires human-labelled color scribbles on the grayscaletarget image or a careful selection of colorful reference images (e.g.,capturing the same scene in the grayscale target image). Unlike the previousmethods, this paper aims at a high-quality fully-automatic colorization method.With the assumption of a perfect patch matching technique, the use of anextremely large-scale reference database (that contains sufficient colorimages) is the most reliable solution to the colorization problem. However,patch matching noise will increase with respect to the size of the referencedatabase in practice. Inspired by the recent success in deep learningtechniques which provide amazing modeling of large-scale data, this paperre-formulates the colorization problem so that deep learning techniques can bedirectly employed. To ensure artifact-free quality, a joint bilateral filteringbased post-processing step is proposed. We further develop an adaptive imageclustering technique to incorporate the global image information. Numerousexperiments demonstrate that our method outperforms the state-of-art algorithmsboth in terms of quality and speed.
arxiv-17700-97 | On b-bit min-wise hashing for large-scale regression and classification with sparse data | http://arxiv.org/pdf/1308.1269v3.pdf | author:Rajen D. Shah, Nicolai Meinshausen category:math.ST stat.ML stat.TH published:2013-08-06 summary:Large-scale regression problems where both the number of variables, $p$, andthe number of observations, $n$, may be large and in the order of millions ormore, are becoming increasingly more common. Typically the data are sparse:only a fraction of a percent of the entries in the design matrix are non-zero.Nevertheless, often the only computationally feasible approach is to performdimension reduction to obtain a new design matrix with far fewer columns, andthen work with this compressed data. $b$-bit min-wise hashing (Li and Konig, 2011) is a promising dimensionreduction scheme for sparse matrices. In this work we study the predictionerror of procedures which perform regression in the new lower-dimensional spaceafter applying the method. For both linear and logistic models we show that theaverage prediction error vanishes asymptotically as long as $q \\beta^*\_2^2/n \rightarrow 0$, where $q$ is the average number of non-zero entries in eachrow of the design matrix and $\beta^*$ is the coefficient of the linearpredictor. We also show that ordinary least squares or ridge regression applied to thereduced data in a sense amounts to a non-parametric regression and can in factallow us fit more flexible models. We obtain non-asymptotic prediction errorbounds for interaction models and for models where an unknown row normalisationmust be applied before the signal is linear in the predictors.
arxiv-17700-98 | Higher Order Recurrent Neural Networks | http://arxiv.org/pdf/1605.00064v1.pdf | author:Rohollah Soltani, Hui Jiang category:cs.NE cs.AI published:2016-04-30 summary:In this paper, we study novel neural network structures to better model longterm dependency in sequential data. We propose to use more memory units to keeptrack of more preceding states in recurrent neural networks (RNNs), which areall recurrently fed to the hidden layers as feedback through different weightedpaths. By extending the popular recurrent structure in RNNs, we provide themodels with better short-term memory mechanism to learn long term dependency insequences. Analogous to digital filters in signal processing, we call thesestructures as higher order RNNs (HORNNs). Similar to RNNs, HORNNs can also belearned using the back-propagation through time method. HORNNs are generallyapplicable to a variety of sequence modelling tasks. In this work, we haveexamined HORNNs for the language modeling task using two popular data sets,namely the Penn Treebank (PTB) and English text8 data sets. Experimentalresults have shown that the proposed HORNNs yield the state-of-the-artperformance on both data sets, significantly outperforming the regular RNNs aswell as the popular LSTMs.
arxiv-17700-99 | Visually Indicated Sounds | http://arxiv.org/pdf/1512.08512v2.pdf | author:Andrew Owens, Phillip Isola, Josh McDermott, Antonio Torralba, Edward H. Adelson, William T. Freeman category:cs.CV cs.LG cs.SD published:2015-12-28 summary:Objects make distinctive sounds when they are hit or scratched. These soundsreveal aspects of an object's material properties, as well as the actions thatproduced them. In this paper, we propose the task of predicting what sound anobject makes when struck as a way of studying physical interactions within avisual scene. We present an algorithm that synthesizes sound from silent videosof people hitting and scratching objects with a drumstick. This algorithm usesa recurrent neural network to predict sound features from videos and thenproduces a waveform from these features with an example-based synthesisprocedure. We show that the sounds predicted by our model are realistic enoughto fool participants in a "real or fake" psychophysical experiment, and thatthey convey significant information about material properties and physicalinteractions.
arxiv-17700-100 | Distributed Cell Association for Energy Harvesting IoT Devices in Dense Small Cell Networks: A Mean-Field Multi-Armed Bandit Approach | http://arxiv.org/pdf/1605.00057v1.pdf | author:Setareh Maghsudi, Ekram Hossain category:cs.NI cs.LG cs.MA published:2016-04-30 summary:The emerging Internet of Things (IoT)-driven ultra-dense small cell networks(UD-SCNs) will need to combat a variety of challenges. On one hand, massivenumber of devices sharing the limited wireless resources will rendercentralized control mechanisms infeasible due to the excessive cost ofinformation acquisition and computations. On the other hand, to reduce energyconsumption from fixed power grid and/or battery, many IoT devices may need todepend on the energy harvested from the ambient environment (e.g., from RFtransmissions, environmental sources). However, due to the opportunistic natureof energy harvesting, this will introduce uncertainty in the network operation.In this article, we study the distributed cell association problem for energyharvesting IoT devices in UD-SCNs. After reviewing the state-of-the-artresearch on the cell association problem in small cell networks, we outline themajor challenges for distributed cell association in IoT-driven UD-SCNs wherethe IoT devices will need to perform cell association in a distributed mannerin presence of uncertainty (e.g., limited knowledge on channel/network) andlimited computational capabilities. To this end, we propose an approach basedon mean-field multi-armed bandit games to solve the uplink cell associationproblem for energy harvesting IoT devices in a UD-SCN. This approach isparticularly suitable to analyze large multi-agent systems under uncertaintyand lack of information. We provide some theoretical results as well aspreliminary performance evaluation results for the proposed approach.
arxiv-17700-101 | DisturbLabel: Regularizing CNN on the Loss Layer | http://arxiv.org/pdf/1605.00055v1.pdf | author:Lingxi Xie, Jingdong Wang, Zhen Wei, Meng Wang, Qi Tian category:cs.CV published:2016-04-30 summary:During a long period of time we are combating over-fitting in the CNNtraining process with model regularization, including weight decay, modelaveraging, data augmentation, etc. In this paper, we present DisturbLabel, anextremely simple algorithm which randomly replaces a part of labels asincorrect values in each iteration. Although it seems weird to intentionallygenerate incorrect training labels, we show that DisturbLabel prevents thenetwork training from over-fitting by implicitly averaging over exponentiallymany networks which are trained with different label sets. To the best of ourknowledge, DisturbLabel serves as the first work which adds noises on the losslayer. Meanwhile, DisturbLabel cooperates well with Dropout to providecomplementary regularization functions. Experiments demonstrate competitiverecognition results on several popular image recognition datasets.
arxiv-17700-102 | InterActive: Inter-Layer Activeness Propagation | http://arxiv.org/pdf/1605.00052v1.pdf | author:Lingxi Xie, Liang Zheng, Jingdong Wang, Alan Yuille, Qi Tian category:cs.CV published:2016-04-30 summary:An increasing number of computer vision tasks can be tackled with deepfeatures, which are the intermediate outputs of a pre-trained ConvolutionalNeural Network. Despite the astonishing performance, deep features extractedfrom low-level neurons are still below satisfaction, arguably because theycannot access the spatial context contained in the higher layers. In thispaper, we present InterActive, a novel algorithm which computes the activenessof neurons and network connections. Activeness is propagated through a neuralnetwork in a top-down manner, carrying high-level context and improving thedescriptive power of low-level and mid-level neurons. Visualization indicatesthat neuron activeness can be interpreted as spatial-weighted neuron responses.We achieve state-of-the-art classification performance on a wide range of imagedatasets.
arxiv-17700-103 | Improved Sparse Low-Rank Matrix Estimation | http://arxiv.org/pdf/1605.00042v1.pdf | author:Ankit Parekh, Ivan W. Selesnick category:math.OC cs.LG stat.ML published:2016-04-29 summary:This paper addresses the problem of estimating a sparse low-rank matrix fromits noisy observation. We propose a convex objective function consisting of adata-fidelity term and two parameterized non-convex penalty functions. Thenon-convex penalty functions induce sparsity of the singular values and theentries of the matrix to be estimated. We show how to set the parameters of thenon-convex penalty functions, in order to ensure that the objective function isstrictly convex. The proposed objective function better estimates sparselow-rank matrices than the convex method which utilizes the sum of the nuclearnorm and the $\ell_1$ norm. We derive an algorithm (as an instance of ADMM) tosolve the proposed problem, and guarantee its convergence provided the scalaraugmented Lagrangian parameter is set appropriately.
arxiv-17700-104 | Deep Convolutional Neural Networks on Cartoon Functions | http://arxiv.org/pdf/1605.00031v1.pdf | author:Philipp Grohs, Thomas Wiatowski, Helmut Bölcskei category:cs.LG cs.CV math.NA stat.ML published:2016-04-29 summary:Wiatowski and B\"olcskei, 2015, proved that deformation stability andvertical translation invariance of deep convolutional neural network-basedfeature extractors are guaranteed by the network structure per se rather thanthe specific convolution kernels and non-linearities. While the translationinvariance result applies to square-integrable functions, the deformationstability bound holds for band-limited functions only. Many signals ofpractical relevance (such as natural images) exhibit, however, sharp and curveddiscontinuities and are hence not band-limited. The main contribution of thispaper is a deformation stability result that takes these structural propertiesinto account. Specifically, we establish deformation stability bounds for theclass of cartoon functions introduced by Donoho, 2001.
arxiv-17700-105 | Multi-Atlas Segmentation using Partially Annotated Data: Methods and Annotation Strategies | http://arxiv.org/pdf/1605.00029v1.pdf | author:Lisa M. Koch, Martin Rajchl, Wenjia Bai, Christian F. Baumgartner, Tong Tong, Jonathan Passerat-Palmbach, Paul Aljabar, Daniel Rueckert category:cs.CV published:2016-04-29 summary:Multi-atlas segmentation is a widely used tool in medical image analysis,providing robust and accurate results by learning from annotated atlasdatasets. However, the availability of fully annotated atlas images fortraining is limited due to the time required for the labelling task.Segmentation methods requiring only a proportion of each atlas image to belabelled could therefore reduce the workload on expert raters tasked withannotating atlas images. To address this issue, we first re-examine thelabelling problem common in many existing approaches and formulate its solutionin terms of a Markov Random Field energy minimisation problem on a graphconnecting atlases and the target image. This provides a unifying framework formulti-atlas segmentation. We then show how modifications in the graphconfiguration of the proposed framework enable the use of partially annotatedatlas images and investigate different partial annotation strategies. Theproposed method was evaluated on two Magnetic Resonance Imaging (MRI) datasetsfor hippocampal and cardiac segmentation. Experiments were performed aimed at(1) recreating existing segmentation techniques with the proposed framework and(2) demonstrating the potential of employing sparsely annotated atlas data formulti-atlas segmentation.
arxiv-17700-106 | The Information Sieve | http://arxiv.org/pdf/1507.02284v2.pdf | author:Greg Ver Steeg, Aram Galstyan category:stat.ML cs.IT cs.LG math.IT published:2015-07-08 summary:We introduce a new framework for unsupervised learning of representationsbased on a novel hierarchical decomposition of information. Intuitively, datais passed through a series of progressively fine-grained sieves. Each layer ofthe sieve recovers a single latent factor that is maximally informative aboutmultivariate dependence in the data. The data is transformed after each pass sothat the remaining unexplained information trickles down to the next layer.Ultimately, we are left with a set of latent factors explaining all thedependence in the original data and remainder information consisting ofindependent noise. We present a practical implementation of this framework fordiscrete variables and apply it to a variety of fundamental tasks inunsupervised learning including independent component analysis, lossy andlossless compression, and predicting missing values in data.
arxiv-17700-107 | deepMiRGene: Deep Neural Network based Precursor microRNA Prediction | http://arxiv.org/pdf/1605.00017v1.pdf | author:Seunghyun Park, Seonwoo Min, Hyunsoo Choi, Sungroh Yoon category:cs.LG q-bio.QM published:2016-04-29 summary:Since microRNAs (miRNAs) play a crucial role in post-transcriptional generegulation, miRNA identification is one of the most essential problems incomputational biology. miRNAs are usually short in length ranging between 20and 23 base pairs. It is thus often difficult to distinguish miRNA-encodingsequences from other non-coding RNAs and pseudo miRNAs that have a similarlength, and most previous studies have recommended using precursor miRNAsinstead of mature miRNAs for robust detection. A great number of conventionalmachine-learning-based classification methods have been proposed, but theyoften have the serious disadvantage of requiring manual feature engineering,and their performance is limited as well. In this paper, we propose a novelmiRNA precursor prediction algorithm, deepMiRGene, based on recurrent neuralnetworks, specifically long short-term memory networks. deepMiRGeneautomatically learns suitable features from the data themselves without manualfeature engineering and constructs a model that can successfully reflectstructural characteristics of precursor miRNAs. For the performance evaluationof our approach, we have employed several widely used evaluation metrics onthree recent benchmark datasets and verified that deepMiRGene deliveredcomparable performance among the current state-of-the-art tools.
arxiv-17700-108 | An efficient and expressive similarity measure for relational clustering using neighbourhood trees | http://arxiv.org/pdf/1604.08934v1.pdf | author:Sebastijan Dumancic, Hendrik Blockeel category:stat.ML cs.AI cs.LG published:2016-04-29 summary:Clustering is an underspecified task: there are no universal criteria forwhat makes a good clustering. This is especially true for relational data,where similarity can be based on the features of individuals, the relationshipsbetween them, or a mix of both. Existing methods for relational clustering havestrong and often implicit biases in this respect. In this paper, we introduce anovel similarity measure for relational data. It is the first measure toincorporate a wide variety of types of similarity, including similarity ofattributes, similarity of relational context, and proximity in a hypergraph. Weexperimentally evaluate how using this similarity affects the quality ofclustering on very different types of datasets. The experiments demonstratethat (a) using this similarity in standard clustering methods consistentlygives good results, whereas other measures work well only on datasets thatmatch their bias; and (b) on most datasets, the novel similarity outperformseven the best among the existing ones.
arxiv-17700-109 | Predicting the direction of stock market prices using random forest | http://arxiv.org/pdf/1605.00003v1.pdf | author:Luckyson Khaidem, Snehanshu Saha, Sudeepa Roy Dey category:cs.LG cs.CE published:2016-04-29 summary:Predicting trends in stock market prices has been an area of interest forresearchers for many years due to its complex and dynamic nature. Intrinsicvolatility in stock market across the globe makes the task of predictionchallenging. Forecasting and diffusion modeling, although effective can't bethe panacea to the diverse range of problems encountered in prediction,short-term or otherwise. Market risk, strongly correlated with forecastingerrors, needs to be minimized to ensure minimal risk in investment. The authorspropose to minimize forecasting error by treating the forecasting problem as aclassification problem, a popular suite of algorithms in Machine learning. Inthis paper, we propose a novel way to minimize the risk of investment in stockmarket by predicting the returns of a stock using a class of powerful machinelearning algorithms known as ensemble learning. Some of the technicalindicators such as Relative Strength Index (RSI), stochastic oscillator etc areused as inputs to train our model. The learning model used is an ensemble ofmultiple decision trees. The algorithm is shown to outperform existing algo-rithms found in the literature. Out of Bag (OOB) error estimates have beenfound to be encouraging. Key Words: Random Forest Classifier, stock priceforecasting, Exponential smoothing, feature extraction, OOB error andconvergence.
arxiv-17700-110 | Confidence driven TGV fusion | http://arxiv.org/pdf/1603.09302v2.pdf | author:Valsamis Ntouskos, Fiora Pirri category:cs.CV published:2016-03-30 summary:We introduce a novel model for spatially varying variational data fusion,driven by point-wise confidence values. The proposed model allows for the jointestimation of the data and the confidence values based on the spatial coherenceof the data. We discuss the main properties of the introduced model as well assuitable algorithms for estimating the solution of the corresponding biconvexminimization problem and their convergence. The performance of the proposedmodel is evaluated considering the problem of depth image fusion by using bothsynthetic and real data from publicly available datasets.
arxiv-17700-111 | Learning by tracking: Siamese CNN for robust target association | http://arxiv.org/pdf/1604.07866v2.pdf | author:Laura Leal-Taixé, Cristian Canton-Ferrer, Konrad Schindler category:cs.LG cs.CV published:2016-04-26 summary:This paper introduces a novel approach to the task of data association withinthe context of pedestrian tracking, by introducing a two-stage learning schemeto match pairs of detections. First, a Siamese convolutional neural network(CNN) is trained to learn descriptors encoding local spatio-temporal structuresbetween the two input image patches, aggregating pixel values and optical flowinformation. Second, a set of contextual features derived from the position andsize of the compared input patches are combined with the CNN output by means ofa gradient boosting classifier to generate the final matching probability. Thislearning approach is validated by using a linear programming based multi-persontracker showing that even a simple and efficient tracker may outperform muchmore complex models when fed with our learned matching probabilities. Resultson publicly available sequences show that our method meets state-of-the-artstandards in multiple people tracking.
arxiv-17700-112 | Faster R-CNN Features for Instance Search | http://arxiv.org/pdf/1604.08893v1.pdf | author:Amaia Salvador, Xavier Giro-i-Nieto, Ferran Marques, Shin'ichi Satoh category:cs.CV published:2016-04-29 summary:Image representations derived from pre-trained Convolutional Neural Networks(CNNs) have become the new state of the art in computer vision tasks such asinstance retrieval. This work explores the suitability for instance retrievalof image- and region-wise representations pooled from an object detection CNNsuch as Faster R-CNN. We take advantage of the object proposals learned by aRegion Proposal Network (RPN) and their associated CNN features to build aninstance search pipeline composed of a first filtering stage followed by aspatial reranking. We further investigate the suitability of Faster R-CNNfeatures when the network is fine-tuned for the same objects one wants toretrieve. We assess the performance of our proposed system with the OxfordBuildings 5k, Paris Buildings 6k and a subset of TRECVid Instance Search 2013,achieving competitive results.
arxiv-17700-113 | Deep, Convolutional, and Recurrent Models for Human Activity Recognition using Wearables | http://arxiv.org/pdf/1604.08880v1.pdf | author:Nils Y. Hammerla, Shane Halloran, Thomas Ploetz category:cs.LG cs.AI cs.HC stat.ML published:2016-04-29 summary:Human activity recognition (HAR) in ubiquitous computing is beginning toadopt deep learning to substitute for well-established analysis techniques thatrely on hand-crafted feature extraction and classification techniques. Fromthese isolated applications of custom deep architectures it is, however,difficult to gain an overview of their suitability for problems ranging fromthe recognition of manipulative gestures to the segmentation and identificationof physical activities like running or ascending stairs. In this paper werigorously explore deep, convolutional, and recurrent approaches across threerepresentative datasets that contain movement data captured with wearablesensors. We describe how to train recurrent approaches in this setting,introduce a novel regularisation approach, and illustrate how they outperformthe state-of-the-art on a large benchmark dataset. Across thousands ofrecognition experiments with randomly sampled model configurations weinvestigate the suitability of each model for different tasks in HAR, explorethe impact of hyperparameters using the fANOVA framework, and provideguidelines for the practitioner who wants to apply deep learning in theirproblem setting.
arxiv-17700-114 | Convolutional Neural Networks for Facial Attribute-based Active Authentication on Mobile Devices | http://arxiv.org/pdf/1604.08865v1.pdf | author:Pouya Samangouei, Rama Chellappa category:cs.CV published:2016-04-29 summary:We present Deep Convolutional Neural Network (DCNN) architectures for thetask of continuous authentication on mobile devices by learning intermediatefeatures to reduce the complexity of the networks. The intermediate featuresfor face images are attributes like gender, and hair color. We present amulti-task, part-based DCNN architecture for attributes detection are betterthan or comparable to state-of-the-art methods in terms of accuracy. As abyproduct of the proposed architecture, we explore the embedding space of theattributes extracted from different facial parts, such as mouth and eyes. Weshow that it is possible to discover new attributes by performing subspaceclustering of the embedded features. Furthermore, through extensiveexperimentation, we show that the attribute features extracted by our methodperforms better than previously attribute-based authentication method and thebaseline LBP method. Lastly, we deploy our architecture on a mobile device anddemonstrate the effectiveness of the proposed method.
arxiv-17700-115 | The Z-loss: a shift and scale invariant classification loss belonging to the Spherical Family | http://arxiv.org/pdf/1604.08859v1.pdf | author:Alexandre de Brébisson, Pascal Vincent category:cs.LG cs.AI stat.ML published:2016-04-29 summary:Despite being the standard loss function to train multi-class neuralnetworks, the log-softmax has two potential limitations. First, it involvescomputations that scale linearly with the number of output classes, which canrestrict the size of problems we are able to tackle with current hardware.Second, it remains unclear how close it matches the task loss such as the top-kerror rate or other non-differentiable evaluation metrics which we aim tooptimize ultimately. In this paper, we introduce an alternative classificationloss function, the Z-loss, which is designed to address these two issues.Unlike the log-softmax, it has the desirable property of belonging to thespherical loss family (Vincent et al., 2015), a class of loss functions forwhich training can be performed very efficiently with a complexity independentof the number of output classes. We show experimentally that it significantlyoutperforms the other spherical loss functions previously investigated.Furthermore, we show on a word language modeling task that it also outperformsthe log-softmax with respect to certain ranking scores, such as top-k scores,suggesting that the Z-loss has the flexibility to better match the task loss.These qualities thus makes the Z-loss an appealing candidate to train veryefficiently large output networks such as word-language models or other extremeclassification problems. On the One Billion Word (Chelba et al., 2014) dataset,we are able to train a model with the Z-loss 40 times faster than thelog-softmax and more than 4 times faster than the hierarchical softmax.
arxiv-17700-116 | Joint Sound Source Separation and Speaker Recognition | http://arxiv.org/pdf/1604.08852v1.pdf | author:Jeroen Zegers, Hugo Van hamme category:cs.SD cs.LG published:2016-04-29 summary:Non-negative Matrix Factorization (NMF) has already been applied to learnspeaker characterizations from single or non-simultaneous speech for speakerrecognition applications. It is also known for its good performance in (blind)source separation for simultaneous speech. This paper explains how NMF can beused to jointly solve the two problems in a multichannel speaker recognizer forsimultaneous speech. It is shown how state-of-the-art multichannel NMF forblind source separation can be easily extended to incorporate speakerrecognition. Experiments on the CHiME corpus show that this method outperformsthe sequential approach of first applying source separation, followed byspeaker recognition that uses state-of-the-art i-vector techniques.
arxiv-17700-117 | Improved Dense Trajectory with Cross Streams | http://arxiv.org/pdf/1604.08826v1.pdf | author:Katsunori Ohnishi, Masatoshi Hidaka, Tatsuya Harada category:cs.CV published:2016-04-29 summary:Improved dense trajectories (iDT) have shown great performance in actionrecognition, and their combination with the two-stream approach has achievedstate-of-the-art performance. It is, however, difficult for iDT to completelyremove background trajectories from video with camera shaking. Trajectories inless discriminative regions should be given modest weights in order to createmore discriminative local descriptors for action recognition. In addition, thetwo-stream approach, which learns appearance and motion information separately,cannot focus on motion in important regions when extracting features fromspatial convolutional layers of the appearance network, and vice versa. Inorder to address the above mentioned problems, we propose a new localdescriptor that pools a new convolutional layer obtained from crossing twonetworks along iDT. This new descriptor is calculated by applyingdiscriminative weights learned from one network to a convolutional layer of theother network. Our method has achieved state-of-the-art performance on ordinalaction recognition datasets, 92.3% on UCF101, and 66.2% on HMDB51.
arxiv-17700-118 | 3D Interest Point Detection Based on Geometric Measures and Sparse Refinement | http://arxiv.org/pdf/1604.08806v1.pdf | author:Xinyu Lin, Ce Zhu, Qian Zhang, Yipeng Liu category:cs.CV published:2016-04-29 summary:3-dimensional (3D) interest point detection plays a fundamental role incomputer vision. In this paper, we introduce a new method for detecting 3Dinterest points of the surface based on geometric measures and sparserefinement (GMSR). The key idea of our approach is to analyze the geometricproperties of local surface on mesh models in scale-space and utilize theseproperties to calculate the 3D saliency map. Those points with local maxima of3D saliency measure are selected as the candidates of 3D interest points.Finally, we utilize an $l_0$ norm based optimization method to refine thecandidates of 3D interest points by constraining the number of 3D interestpoints. Numerical experiments show that the proposed GMSR based 3D interestpoint detector outperforms current 6 state-of-the-art methods for differentkinds of mesh models.
arxiv-17700-119 | Effective Backscatter Approximation for Photometry in Murky Water | http://arxiv.org/pdf/1604.08789v1.pdf | author:Chourmouzios Tsiotsios, Maria E. Angelopoulou, Andrew J. Davison, Tae-Kyun Kim category:cs.CV published:2016-04-29 summary:Shading-based approaches like Photometric Stereo assume that the imageformation model can be effectively optimized for the scene normals. However, inmurky water this is a very challenging problem. The light from artificialsources is not only reflected by the scene but it is also scattered by themedium particles, yielding the backscatter component. Backscatter correspondsto a complex term with several unknown variables, and makes the problem ofnormal estimation hard. In this work, we show that instead of trying tooptimize the complex backscatter model or use previous unrealisticsimplifications, we can approximate the per-pixel backscatter signal directlyfrom the captured images. Our method is based on the observation thatbackscatter is saturated beyond a certain distance, i.e. it becomes scene-depthindependent, and finally corresponds to a smoothly varying signal which dependsstrongly on the light position with respect to each pixel. Our backscatterapproximation method facilitates imaging and scene reconstruction in murkywater when the illumination is artificial as in Photometric Stereo.Specifically, we show that it allows accurate scene normal estimation andoffers potentials like single image restoration. We evaluate our approach usingnumerical simulations and real experiments within both the controlledenvironment of a big water-tank and real murky port-waters.
arxiv-17700-120 | PHOCNet: A Deep Convolutional Neural Network for Word Spotting in Handwritten Documents | http://arxiv.org/pdf/1604.00187v2.pdf | author:Sebastian Sudholt, Gernot A. Fink category:cs.CV published:2016-04-01 summary:In recent years, deep convolutional neural networks have achieved state ofthe art performance in various computer vision task such as classification,detection or segmentation. Due to their outstanding performance, CNNs are moreand more used in the field of document image analysis as well. In this work, wepresent a CNN architecture that is trained with the recently proposed PHOCrepresentation. We show empirically that our CNN architecture is able tooutperform state of the art results for various word spotting benchmarks whileexhibiting short training and test times.
arxiv-17700-121 | Teaching natural language to computers | http://arxiv.org/pdf/1604.08781v1.pdf | author:Joseph Corneli, Miriam Corneli category:cs.CL cs.AI published:2016-04-29 summary:"Natural Language," whether spoken and attended to by humans, or processedand generated by computers, requires a series of structures and networks thatreflect creative processes in semantic, syntactic, phonetic, linguistic,social, emotional, and cultural modules. Being able to produce novel and usefulbehavior following repeated practice gets to the root of both artificialintelligence and human language. This paper investigates current modalitiesinvolved in language-like applications that computers -- and programmers -- areengaged with, and seeks ways of fine tuning the questions we ask, to betteraccount for context, self-awareness, and embodiment.
arxiv-17700-122 | Towards Conceptual Compression | http://arxiv.org/pdf/1604.08772v1.pdf | author:Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, Daan Wierstra category:stat.ML cs.CV cs.LG published:2016-04-29 summary:We introduce a simple recurrent variational auto-encoder architecture thatsignificantly improves image modeling. The system represents thestate-of-the-art in latent variable models for both the ImageNet and Omniglotdatasets. We show that it naturally separates global conceptual informationfrom lower level details, thus addressing one of the fundamentally desiredproperties of unsupervised learning. Furthermore, the possibility ofrestricting ourselves to storing only global information about an image allowsus to achieve high quality 'conceptual compression'.
arxiv-17700-123 | Argumentation Mining in User-Generated Web Discourse | http://arxiv.org/pdf/1601.02403v3.pdf | author:Ivan Habernal, Iryna Gurevych category:cs.CL published:2016-01-11 summary:The goal of argumentation mining, an evolving research field in computationallinguistics, is to design methods capable of analyzing people's argumentation.In this article, we go beyond the state of the art in several ways. (i) We dealwith actual Web data and take up the challenges given by the variety ofregisters, multiple domains, and unrestricted noisy user-generated Webdiscourse. (ii) We bridge the gap between normative argumentation theories andargumentation phenomena encountered in actual data by adapting an argumentationmodel tested in an extensive annotation study. (iii) We create a new goldstandard corpus (90k tokens in 340 documents) and experiment with severalmachine learning methods to identify argument components. We offer the data,source codes, and annotation guidelines to the community under free licenses.Our findings show that argumentation mining in user-generated Web discourse isa feasible but challenging task.
arxiv-17700-124 | MetaGrad: Faster Convergence Without Curvature in Online Convex Optimization | http://arxiv.org/pdf/1604.08740v1.pdf | author:Wouter M. Koolen, Tim van Erven category:cs.LG published:2016-04-29 summary:In online convex optimization it is well known that objective functions withcurvature are much easier than arbitrary convex functions. Here we show thatthe regret can be significantly reduced even without curvature, in cases wherethere is a stable optimum to converge to. More precisely, the regret ofexisting methods is determined by the norms of the encountered gradients, andmatching worst-case performance lower bounds tell us that this cannot beimproved uniformly. Yet we argue that this is a rather pessimistic assessmentof the complexity of the problem. We introduce a new parameter-free algorithm,called MetaGrad, for which the gradient norms in the regret are scaled down bythe distance to the (unknown) optimum. So when the optimum is reasonably stableover time, making the algorithm converge, this new scaling leads to orders ofmagnitude smaller regret even when the gradients themselves do not vanish. MetaGrad does not require any manual tuning, but instead tunes a learningrate parameter automatically for the data. Unlike all previous methods withprovable guarantees, its learning rates are not monotonically decreasing overtime, but instead are based on a novel aggregation technique. We provide twoversions of MetaGrad. The first maintains a full covariance matrix to guaranteethe sharpest bounds for problems where we can afford update time quadratic inthe dimension. The second version maintains only the diagonal. Its linear costin the dimension makes it suitable for large-scale problems.
arxiv-17700-125 | Summary Transfer: Exemplar-based Subset Selection for Video Summarization | http://arxiv.org/pdf/1603.03369v3.pdf | author:Ke Zhang, Wei-Lun Chao, Fei Sha, Kristen Grauman category:cs.CV published:2016-03-10 summary:Video summarization has unprecedented importance to help us digest, browse,and search today's ever-growing video collections. We propose a novel subsetselection technique that leverages supervision in the form of human-createdsummaries to perform automatic keyframe-based video summarization. The mainidea is to nonparametrically transfer summary structures from annotated videosto unseen test videos. We show how to extend our method to exploit semanticside information about the video's category/genre to guide the transfer processby those training videos semantically consistent with the test input. We alsoshow how to generalize our method to subshot-based summarization, which notonly reduces computational costs but also provides more flexible ways ofdefining visual similarity across subshots spanning several frames. We conductextensive evaluation on several benchmarks and demonstrate promising results,outperforming existing methods in several settings.
arxiv-17700-126 | Music transcription modelling and composition using deep learning | http://arxiv.org/pdf/1604.08723v1.pdf | author:Bob L. Sturm, João Felipe Santos, Oded Ben-Tal, Iryna Korshunova category:cs.SD cs.LG published:2016-04-29 summary:We apply deep learning methods, specifically long short-term memory (LSTM)networks, to music transcription modelling and composition. We build and trainLSTM networks using approximately 23,000 music transcriptions expressed with ahigh-level vocabulary (ABC notation), and use them to generate newtranscriptions. Our practical aim is to create music transcription modelsuseful in particular contexts of music composition. We present results fromthree perspectives: 1) at the population level, comparing descriptivestatistics of the set of training transcriptions and generated transcriptions;2) at the individual level, examining how a generated transcription reflectsthe conventions of a music practice in the training transcriptions (Celticfolk); 3) at the application level, using the system for idea generation inmusic composition. We make our datasets, software and sound examples open andavailable: \url{https://github.com/IraKorshunova/folk-rnn}.
arxiv-17700-127 | Learning Compact Structural Representations for Audio Events Using Regressor Banks | http://arxiv.org/pdf/1604.08716v1.pdf | author:Huy Phan, Marco Maass, Lars Hertel, Radoslaw Mazur, Ian McLoughlin, Alfred Mertins category:cs.SD cs.LG stat.ML published:2016-04-29 summary:We introduce a new learned descriptor for audio signals which is efficientfor event representation. The entries of the descriptor are produced byevaluating a set of regressors on the input signal. The regressors areclass-specific and trained using the random regression forests framework. Givenan input signal, each regressor estimates the onset and offset positions of thetarget event. The estimation confidence scores output by a regressor are thenused to quantify how the target event aligns with the temporal structure of thecorresponding category. Our proposed descriptor has two advantages. First, itis compact, i.e. the dimensionality of the descriptor is equal to the number ofevent classes. Second, we show that even simple linear classification models,trained on our descriptor, yield better accuracies on audio eventclassification task than not only the nonlinear baselines but also thestate-of-the-art results.
arxiv-17700-128 | Volumetric and Multi-View CNNs for Object Classification on 3D Data | http://arxiv.org/pdf/1604.03265v2.pdf | author:Charles R. Qi, Hao Su, Matthias Niessner, Angela Dai, Mengyuan Yan, Leonidas J. Guibas category:cs.CV cs.AI published:2016-04-12 summary:3D shape models are becoming widely available and easier to capture, makingavailable 3D information crucial for progress in object classification. Currentstate-of-the-art methods rely on CNNs to address this problem. Recently, wewitness two types of CNNs being developed: CNNs based upon volumetricrepresentations versus CNNs based upon multi-view representations. Empiricalresults from these two types of CNNs exhibit a large gap, indicating thatexisting volumetric CNN architectures and approaches are unable to fullyexploit the power of 3D representations. In this paper, we aim to improve bothvolumetric CNNs and multi-view CNNs according to extensive analysis of existingapproaches. To this end, we introduce two distinct network architectures ofvolumetric CNNs. In addition, we examine multi-view CNNs, where we introducemulti-resolution filtering in 3D. Overall, we are able to outperform currentstate-of-the-art methods for both volumetric CNNs and multi-view CNNs. Weprovide extensive experiments designed to evaluate underlying design choices,thus providing a better understanding of the space of methods available forobject classification on 3D data.
arxiv-17700-129 | Sparse Generalized Eigenvalue Problem: Optimal Statistical Rates via Truncated Rayleigh Flow | http://arxiv.org/pdf/1604.08697v1.pdf | author:Kean Ming Tan, Zhaoran Wang, Han Liu, Tong Zhang category:stat.ML published:2016-04-29 summary:Sparse generalized eigenvalue problem plays a pivotal role in a large familyof high-dimensional learning tasks, including sparse Fisher's discriminantanalysis, canonical correlation analysis, and sufficient dimension reduction.However, the theory of sparse generalized eigenvalue problem remains largelyunexplored. In this paper, we exploit a non-convex optimization perspective tostudy this problem. In particular, we propose the truncated Rayleigh flowmethod (Rifle) to estimate the leading generalized eigenvector and show that itconverges linearly to a solution with the optimal statistical rate ofconvergence. Our theory involves two key ingredients: (i) a new analysis of thegradient descent method on non-convex objective functions, as well as (ii) afine-grained characterization of the evolution of sparsity patterns along thesolution path. Thorough numerical studies are provided to back up our theory.Finally, we apply our proposed method in the context of sparse sufficientdimension reduction to two gene expression data sets.
arxiv-17700-130 | Single Image 3D Interpreter Network | http://arxiv.org/pdf/1604.08685v1.pdf | author:Jiajun Wu, Tianfan Xue, Joseph J. Lim, Yuandong Tian, Joshua B. Tenenbaum, Antonio Torralba, William T. Freeman category:cs.CV published:2016-04-29 summary:Understanding 3D object structure from a single image is an important butdifficult task in computer vision, mostly due to the lack of 3D objectannotations in real images. Previous work tackles this problem by eithersolving an optimization task given 2D keypoint positions, or training onsynthetic data with ground truth 3D information. In this work, we propose 3DINterpreter Network (3D-INN), an end-to-end framework which sequentiallyestimates 2D keypoint heatmaps and 3D object structure, trained on both real2D-annotated images and synthetic 3D data. This is made possible mainly by twotechnical innovations. First, we propose a Projection Layer, which projectsestimated 3D structure to 2D space, so that 3D-INN can be trained to predict 3Dstructural parameters supervised by 2D annotations on real images. Second,heatmaps of keypoints serve as an intermediate representation connecting realand synthetic data, enabling 3D-INN to benefit from the variation and abundanceof synthetic 3D objects, without suffering from the difference between thestatistics of real and synthesized images due to imperfect rendering. Thenetwork achieves state-of-the-art performance on both 2D keypoint estimationand 3D structure recovery. We also show that the recovered 3D information canbe used in other vision applications, such as 3D rendering and image retrieval.
arxiv-17700-131 | Top-push Video-based Person Re-identification | http://arxiv.org/pdf/1604.08683v1.pdf | author:Jinjie You, Ancong Wu, Xiang Li, Wei-Shi Zheng category:cs.CV published:2016-04-29 summary:Most existing person re-identification (re-id) models focus on matching stillperson images across disjoint camera views. Since only limited information canbe exploited from still images, it is hard (if not impossible) to overcome theocclusion, pose and camera-view change, and lighting variation problems. Incomparison, video-based re-id methods can utilize extra space-time information,which contains much more rich cues for matching to overcome the mentionedproblems. However, we find that when using video-based representation, someinter-class difference can be much more obscure than the one when usingstill-image based representation, because different people could not only havesimilar appearance but also have similar motions and actions which are hard toalign. To solve this problem, we propose a top-push distance learning model(TDL), in which we integrate a top-push constrain for matching video featuresof persons. The top-push constraint enforces the optimization on top-rankmatching in re-id, so as to make the matching model more effective towardsselecting more discriminative features to distinguish different persons. Ourexperiments show that the proposed video-based re-id framework outperforms thestate-of-the-art video-based re-id methods.
arxiv-17700-132 | Data-dependent Initializations of Convolutional Neural Networks | http://arxiv.org/pdf/1511.06856v2.pdf | author:Philipp Krähenbühl, Carl Doersch, Jeff Donahue, Trevor Darrell category:cs.CV cs.LG published:2015-11-21 summary:Convolutional Neural Networks spread through computer vision like a wildfire,impacting almost all visual tasks imaginable. Despite this, few researchersdare to train their models from scratch. Most work builds on one of a handfulof ImageNet pre-trained models, and fine-tunes or adapts these for specifictasks. This is in large part due to the difficulty of properly initializingthese networks from scratch. A small miscalibration of the initial weightsleads to vanishing or exploding gradients, as well as poor convergenceproperties. In this work we present a fast and simple data-dependentinitialization procedure, that sets the weights of a network such that allunits in the network train at roughly the same rate, avoiding vanishing orexploding gradients. Our initialization matches the current state-of-the-artunsupervised or self-supervised pre-training methods on standard computervision tasks, such as image classification and object detection, while beingroughly three orders of magnitude faster. When combined with pre-trainingmethods, our initialization significantly outperforms prior work, narrowing thegap between supervised and unsupervised pre-training.
arxiv-17700-133 | Attention-Based Deep Distance Metric Learning for Aspect-Phrase Grouping | http://arxiv.org/pdf/1604.08672v1.pdf | author:Shufeng Xiong, Donghong Ji category:cs.CL published:2016-04-29 summary:Aspect phrase grouping is an important task for aspect finding inaspect-level sentiment analysis and it is a challenging problem due to polysemyand its context dependency. In this paper we propose an Attention-based DeepDistance Metric Learning (ADDML) method, which is more beneficial forclustering by considering aspect phrase representation as well as contextrepresentation and their combination. First, we feed word embeddings of aspectphrases and its contexts into an attention-based network to learn featurerepresentation of contexts. Then, both of aspect phrase embedding and contextembedding as the input of a multilayer perceptron which is used to learn deepfeature subspace, under which the distance of each intra-group pair is smallerand that of each inter-group pair is bigger, respectively. After obtaining thelearned representations, we use K-means to cluster them. Experiments on fourdomain review datasets shows that the proposed method outperforms strongbaseline methods.
arxiv-17700-134 | Deep Edge Guided Recurrent Residual Learning for Image Super-Resolution | http://arxiv.org/pdf/1604.08671v1.pdf | author:Wenhan Yang, Jiashi Feng, Jianchao Yang, Fang Zhao, Jiaying Liu, Zongming Guo, Shuicheng Yan category:cs.CV published:2016-04-29 summary:In this work, we consider the image super-resolution (SR) problem. The mainchallenge of image SR is to recover high-frequency details of a low-resolution(LR) image that are important for human perception. To address this essentiallyill-posed problem, we introduce a Deep Edge Guided REcurrent rEsidual~(DEGREE)network to progressively recover the high-frequency details. Different frommost of existing methods that aim at predicting high-resolution (HR) imagesdirectly, DEGREE investigates an alternative route to recover the differencebetween a pair of LR and HR images by recurrent residual learning. DEGREEfurther augments the SR process with edge-preserving capability, namely the LRimage and its edge map can jointly infer the sharp edge details of the HR imageduring the recurrent recovery process. To speed up its training convergencerate, by-pass connections across multiple layers of DEGREE are constructed. Inaddition, we offer an understanding on DEGREE from the view-point of sub-bandfrequency decomposition on image signal and experimentally demonstrate howDEGREE can recover different frequency bands separately. Extensive experimentson three benchmark datasets clearly demonstrate the superiority of DEGREE overwell-established baselines and DEGREE also provides new state-of-the-arts onthese datasets.
arxiv-17700-135 | Crowd Counting via Weighted VLAD on Dense Attribute Feature Maps | http://arxiv.org/pdf/1604.08660v1.pdf | author:Biyun Sheng, Chunhua Shen, Guosheng Lin, Jun Li, Wankou Yang, Changyin Sun category:cs.CV published:2016-04-29 summary:Crowd counting is an important task in computer vision, which has manyapplications in video surveillance. Although the regression-based framework hasachieved great improvements for crowd counting, how to improve thediscriminative power of image representation is still an open problem.Conventional holistic features used in crowd counting often fail to capturesemantic attributes and spatial cues of the image. In this paper, we proposeintegrating semantic information into learning locality-aware feature sets foraccurate crowd counting. First, with the help of convolutional neural network(CNN), the original pixel space is mapped onto a dense attribute feature map,where each dimension of the pixel-wise feature indicates the probabilisticstrength of a certain semantic class. Then, locality-aware features (LAF) builton the idea of spatial pyramids on neighboring patches are proposed to exploremore spatial context and local information. Finally, the traditional VLADencoding method is extended to a more generalized form in which diversecoefficient weights are taken into consideration. Experimental results validatethe effectiveness of our presented method.
arxiv-17700-136 | Without-Replacement Sampling for Stochastic Gradient Methods: Convergence Results and Application to Distributed Optimization | http://arxiv.org/pdf/1603.00570v2.pdf | author:Ohad Shamir category:cs.LG math.OC stat.ML published:2016-03-02 summary:Stochastic gradient methods for machine learning and optimization problemsare usually analyzed assuming data points are sampled with replacement. Inpractice, however, sampling without replacement is very common, easier toimplement in many cases, and often performs better. In this paper, we providecompetitive convergence guarantees for without-replacement sampling, undervarious scenarios, for three types of algorithms: Any algorithm with onlineregret guarantees, stochastic gradient descent, and SVRG. A useful applicationof our SVRG analysis is a nearly-optimal algorithm for regularized leastsquares in a distributed setting, in terms of both communication complexity andruntime complexity, when the data is randomly partitioned and the conditionnumber can be as large as the data size (up to logarithmic factors). Our prooftechniques combine ideas from stochastic optimization, adversarial onlinelearning, and transductive learning theory, and can potentially be applied toother stochastic optimization and learning problems.
arxiv-17700-137 | On the representation and embedding of knowledge bases beyond binary relations | http://arxiv.org/pdf/1604.08642v1.pdf | author:Jianfeng Wen, Jianxin Li, Yongyi Mao, Shini Chen, Richong Zhang category:cs.LG cs.AI published:2016-04-28 summary:The models developed to date for knowledge base embedding are all based onthe assumption that the relations contained in knowledge bases are binary. Forthe training and testing of these embedding models, multi-fold (or n-ary)relational data are converted to triples (e.g., in FB15K dataset) andinterpreted as instances of binary relations. This paper presents a canonicalrepresentation of knowledge bases containing multi-fold relations. We show thatthe existing embedding models on the popular FB15K datasets correspond to asub-optimal modelling framework, resulting in a loss of structural information.We advocate a novel modelling framework, which models multi-fold relationsdirectly using this canonical representation. Using this framework, theexisting TransH model is generalized to a new model, m-TransH. We demonstrateexperimentally that m-TransH outperforms TransH by a large margin, therebyestablishing a new state of the art.
arxiv-17700-138 | DCTNet and PCANet for acoustic signal feature extraction | http://arxiv.org/pdf/1605.01755v1.pdf | author:Yin Xian, Andrew Thompson, Xiaobai Sun, Douglas Nowacek, Loren Nolte category:cs.SD cs.LG published:2016-04-28 summary:We introduce the use of DCTNet, an efficient approximation and alternative toPCANet, for acoustic signal classification. In PCANet, the eigenfunctions ofthe local sample covariance matrix (PCA) are used as filterbanks forconvolution and feature extraction. When the eigenfunctions are wellapproximated by the Discrete Cosine Transform (DCT) functions, each layer of ofPCANet and DCTNet is essentially a time-frequency representation. We relateDCTNet to spectral feature representation methods, such as the the short timeFourier transform (STFT), spectrogram and linear frequency spectralcoefficients (LFSC). Experimental results on whale vocalization data show thatDCTNet improves classification rate, demonstrating DCTNet's applicability tosignal processing problems such as underwater acoustics.
arxiv-17700-139 | Optimal Transport vs. Fisher-Rao distance between Copulas for Clustering Multivariate Time Series | http://arxiv.org/pdf/1604.08634v1.pdf | author:Gautier Marti, Sébastien Andler, Frank Nielsen, Philippe Donnat category:stat.ML published:2016-04-28 summary:We present a methodology for clustering N objects which are described bymultivariate time series, i.e. several sequences of real-valued randomvariables. This clustering methodology leverages copulas which aredistributions encoding the dependence structure between several randomvariables. To take fully into account the dependence information whileclustering, we need a distance between copulas. In this work, we comparerenowned distances between distributions: the Fisher-Rao geodesic distance,related divergences and optimal transport, and discuss their advantages anddisadvantages. Applications of such methodology can be found in the clusteringof financial assets. A tutorial, experiments and implementation forreproducible research can be found at www.datagrapple.com/Tech.
arxiv-17700-140 | Word Ordering Without Syntax | http://arxiv.org/pdf/1604.08633v1.pdf | author:Allen Schmaltz, Alexander M. Rush, Stuart M. Shieber category:cs.CL published:2016-04-28 summary:Recent work on word ordering has argued that syntactic structure isimportant, or even required, for effectively recovering the order of asentence. We find that, in fact, an n-gram language model with a simpleheuristic gives strong results on this task. Furthermore, we show that a longshort-term memory (LSTM) language model is comparatively effective atrecovering order, with our basic model outperforming a state-of-the-artsyntactic model by 11.5 BLEU points. Additional data and larger beams yieldfurther gains, at the expense of training and search time.
arxiv-17700-141 | Minimum Spectral Connectivity Projection Pursuit for Unsupervised Classification | http://arxiv.org/pdf/1509.01546v2.pdf | author:David P. Hofmeyr, Nicos G. Pavlidis, Idris A. Eckley category:stat.ML cs.LG 62H30 I.5.3 published:2015-09-04 summary:We study the problem of determining the optimal low dimensional projectionfor maximising the separability of a binary partition of an unlabelled dataset,as measured by spectral graph theory. This is achieved by finding projectionswhich minimise the second eigenvalue of the Laplacian matrices of the projecteddata, which corresponds to a non-convex, non-smooth optimisation problem. Weshow that the optimal univariate projection based on spectral connectivityconverges to the vector normal to the maximum margin hyperplane through thedata, as the scaling parameter is reduced to zero. This establishes aconnection between connectivity as measured by spectral graph theory andmaximal Euclidean separation. It also allows us to apply our methodology to theproblem of finding large margin linear separators. The computational costassociated with each eigen-problem is quadratic in the number of data. Tomitigate this problem, we propose an approximation method using microclusterswith provable approximation error bounds. We evaluate the performance of theproposed method on a large collection of benchmark datasets and find that itcompares favourably with existing methods for projection pursuit and dimensionreduction for unsupervised data partitioning.
arxiv-17700-142 | Continuous online sequence learning with an unsupervised neural network model | http://arxiv.org/pdf/1512.05463v2.pdf | author:Yuwei Cui, Subutai Ahmad, Jeff Hawkins category:cs.NE q-bio.NC published:2015-12-17 summary:The ability to recognize and predict temporal sequences of sensory inputs isvital for survival in natural environments. Based on many known properties ofcortical neurons, hierarchical temporal memory (HTM) sequence memory isrecently proposed as a theoretical framework for sequence learning in thecortex. In this paper, we analyze properties of HTM sequence memory and applyit to sequence learning and prediction problems with streaming data. We showthe model is able to continuously learn a large number of variable-ordertemporal sequences using an unsupervised Hebbian-like learning rule. The sparsetemporal codes formed by the model can robustly handle branching temporalsequences by maintaining multiple predictions until there is sufficientdisambiguating evidence. We compare the HTM sequence memory with other sequencelearning algorithms, including statistical methods: autoregressive integratedmoving average (ARIMA), feedforward neural networks: online sequential extremelearning machine (ELM), and recurrent neural networks: long short-term memory(LSTM) and echo-state networks (ESN), on sequence prediction problems with bothartificial and real-world data. The HTM model achieves comparable accuracy toother state-of-the-art algorithms. The model also exhibits properties that arecritical for sequence learning, including continuous online learning, theability to handle multiple predictions and branching sequences with high orderstatistics, robustness to sensor noise and fault tolerance, and goodperformance without task-specific hyper- parameters tuning. Therefore the HTMsequence memory not only advances our understanding of how the brain may solvethe sequence learning problem, but is also applicable to a wide range ofreal-world problems such as discrete and continuous sequence prediction,anomaly detection, and sequence classification.
arxiv-17700-143 | Artistic style transfer for videos | http://arxiv.org/pdf/1604.08610v1.pdf | author:Manuel Ruder, Alexey Dosovitskiy, Thomas Brox category:cs.CV published:2016-04-28 summary:In the past, manually re-drawing an image in a certain artistic stylerequired a professional artist and a long time. Doing this for a video sequencesingle-handed was beyond imagination. Nowadays computers provide newpossibilities. We present an approach that transfers the style from one image(for example, a painting) to a whole video sequence. We make use of recentadvances in style transfer in still images and propose new initializations andloss functions applicable to videos. This allows us to generate consistent andstable stylized video sequences, even in cases with large motion and strongocclusion. We show that the proposed method clearly outperforms simplerbaselines both qualitatively and quantitatively.
arxiv-17700-144 | Comparing Fifty Natural Languages and Twelve Genetic Languages Using Word Embedding Language Divergence (WELD) as a Quantitative Measure of Language Distance | http://arxiv.org/pdf/1604.08561v1.pdf | author:Ehsaneddin Asgari, Mohammad R. K. Mofrad category:cs.CL published:2016-04-28 summary:We introduce a new measure of distance between languages based on wordembedding, called word embedding language divergence (WELD). WELD is defined asdivergence between unified similarity distribution of words between languages.Using such a measure, we perform language comparison for fifty naturallanguages and twelve genetic languages. Our natural language dataset is acollection of sentence-aligned parallel corpora from bible translations forfifty languages spanning a variety of language families. Although we useparallel corpora, which guarantees having the same content in all languages,interestingly in many cases languages within the same family cluster together.In addition to natural languages, we perform language comparison for the codingregions in the genomes of 12 different organisms (4 plants, 6 animals, and twohuman subjects). Our result confirms a significant high-level difference in thegenetic language model of humans/animals versus plants. The proposed method isa step toward defining a quantitative measure of similarity between languages,with applications in languages classification, genre identification, dialectidentification, and evaluation of translations.
arxiv-17700-145 | A Probabilistic Adaptive Search System for Exploring the Face Space | http://arxiv.org/pdf/1604.08524v1.pdf | author:Andres G. Abad, Luis I. Reyes Castro category:stat.ML cs.CV published:2016-04-28 summary:Face recall is a basic human cognitive process performed routinely, e.g.,when meeting someone and determining if we have met that person before.Assisting a subject during face recall by suggesting candidate faces can bechallenging. One of the reasons is that the search space - the face space - isquite large and lacks structure. A commercial application of face recall isfacial composite systems - such as Identikit, PhotoFIT, and CD-FIT - where awitness searches for an image of a face that resembles his memory of aparticular offender. The inherent uncertainty and cost in the evaluation of theobjective function, the large size and lack of structure of the search space,and the unavailability of the gradient concept makes this problem inappropriatefor traditional optimization methods. In this paper we propose a novelevolutionary approach for searching the face space that can be used as a facialcomposite system. The approach is inspired by methods of Bayesian optimizationand differs from other applications in the use of the skew-normal distributionas its acquisition function. This choice of acquisition function providesgreater granularity, with regularized, conservative, and realistic results.
arxiv-17700-146 | Detecting "Smart" Spammers On Social Network: A Topic Model Approach | http://arxiv.org/pdf/1604.08504v1.pdf | author:Linqing Liu, Yao Lu, Ye Luo, Renxian Zhang, Laurent Itti, Jianwei Lu category:cs.CL cs.SI published:2016-04-28 summary:Spammer detection on social network is a challenging problem. The rigidanti-spam rules have resulted in emergence of "smart" spammers. They resemblelegitimate users who are difficult to identify. In this paper, we present anovel spammer classification approach based on Latent DirichletAllocation(LDA), a topic model. Our approach extracts both the local and theglobal information of topic distribution patterns, which capture the essence ofspamming. Tested on one benchmark dataset and one self-collected dataset, ourproposed method outperforms other state-of-the-art methods in terms of averagedF1-score.
arxiv-17700-147 | Robust subspace recovery by Tyler's M-estimator | http://arxiv.org/pdf/1206.1386v3.pdf | author:Teng Zhang category:stat.ML published:2012-06-07 summary:This paper considers the problem of robust subspace recovery: given a set of$N$ points in $\mathbb{R}^D$, if many lie in a $d$-dimensional subspace, thencan we recover the underlying subspace? We show that Tyler's M-estimator can beused to recover the underlying subspace, if the percentage of the inliers islarger than $d/D$ and the data points lie in general position. Empirically,Tyler's M-estimator compares favorably with other convex subspace recoveryalgorithms in both simulations and experiments on real data sets.
arxiv-17700-148 | Sparse Representation for 3D Shape Estimation: A Convex Relaxation Approach | http://arxiv.org/pdf/1509.04309v2.pdf | author:Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Kostas Daniilidis category:cs.CV published:2015-09-14 summary:We investigate the problem of estimating the 3D shape of an object defined bya set of 3D landmarks, given their 2D correspondences in a single image. Asuccessful approach to alleviating the reconstruction ambiguity is the 3Ddeformable shape model and a sparse representation is often used to capturecomplex shape variability. But the model inference is still a challenge due tothe nonconvexity in optimization resulted from joint estimation of shape andviewpoint. In contrast to prior work that relies on a alternating scheme withsolutions depending on initialization, we propose a convex approach toaddressing this challenge and develop an efficient algorithm to solve theproposed convex program. Moreover, we propose a robust model to handle grosserrors in the 2D correspondences. We demonstrate the exact recovery property ofthe proposed method, the advantage compared to the nonconvex baseline methodsand the applicability to recover 3D human poses and car models from singleimages.
arxiv-17700-149 | Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video | http://arxiv.org/pdf/1511.09439v2.pdf | author:Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Kosta Derpanis, Kostas Daniilidis category:cs.CV published:2015-11-30 summary:This paper addresses the challenge of 3D full-body human pose estimation froma monocular image sequence. Here, two cases are considered: (i) the imagelocations of the human joints are provided and (ii) the image locations ofjoints are unknown. In the former case, a novel approach is introduced thatintegrates a sparsity-driven 3D geometric prior and temporal smoothness. In thelatter case, the former case is extended by treating the image locations of thejoints as latent variables. A deep fully convolutional network is trained topredict the uncertainty maps of the 2D joint locations. The 3D pose estimatesare realized via an Expectation-Maximization algorithm over the entiresequence, where it is shown that the 2D joint location uncertainties can beconveniently marginalized out during inference. Empirical evaluation on theHuman3.6M dataset shows that the proposed approaches achieve greater 3D poseestimation accuracy over state-of-the-art baselines. Further, the proposedapproach outperforms a publicly available 2D pose estimation baseline on thechallenging PennAction dataset.
arxiv-17700-150 | Two Differentially Private Rating Collection Mechanisms for Recommender Systems | http://arxiv.org/pdf/1604.08402v1.pdf | author:Wenjie Zheng category:stat.ML cs.CR cs.IR published:2016-04-28 summary:We design two mechanisms for the recommender system to collect user ratings.One is modified Laplace mechanism, and the other is randomized responsemechanism. We prove that they are both differentially private and preserve thedata utility.
arxiv-17700-151 | Fast k-means with accurate bounds | http://arxiv.org/pdf/1602.02514v5.pdf | author:James Newling, François Fleuret category:stat.ML cs.LG published:2016-02-08 summary:We propose a novel accelerated exact k-means algorithm, which performs betterthan the current state-of-the-art low-dimensional algorithm in 18 of 22experiments, running up to 3 times faster. We also propose a generalimprovement of existing state-of-the-art accelerated exact k-means algorithmsthrough better estimates of the distance bounds used to reduce the number ofdistance calculations, and get a speedup in 36 of 44 experiments, up to 1.8times faster. We have conducted experiments with our own implementations of existingmethods to ensure homogeneous evaluation of performance, and we show that ourimplementations perform as well or better than existing availableimplementations. Finally, we propose simplified variants of standard approachesand show that they are faster than their fully-fledged counterparts in 59 of 62experiments.
arxiv-17700-152 | Convolutional Neural Networks For Automatic State-Time Feature Extraction in Reinforcement Learning Applied to Residential Load Control | http://arxiv.org/pdf/1604.08382v1.pdf | author:Bert J. Claessens, Peter Vrancx, Frederik Ruelens category:cs.LG cs.SY published:2016-04-28 summary:Direct load control of a heterogeneous cluster of residential demandflexibility sources is a high-dimensional control problem with partialobservability. This work proposes a novel approach that uses a convolutionalneural network to extract hidden state-time features to mitigate the curse ofpartial observability. More specific, a convolutional neural network is used asa function approximator to estimate the state-action value function orQ-function in the supervised learning step of fitted Q-iteration. The approachis evaluated in a qualitative simulation, comprising a cluster ofthermostatically controlled loads that only share their air temperature, whilsttheir envelope temperature remains hidden. The simulation results show that thepresented approach is able to capture the underlying hidden features andsuccessfully reduce the electricity cost the cluster.
arxiv-17700-153 | Joint Line Segmentation and Transcription for End-to-End Handwritten Paragraph Recognition | http://arxiv.org/pdf/1604.08352v1.pdf | author:Théodore Bluche category:cs.CV cs.LG cs.NE published:2016-04-28 summary:Offline handwriting recognition systems require cropped text line images forboth training and recognition. On the one hand, the annotation of position andtranscript at line level is costly to obtain. On the other hand, automatic linesegmentation algorithms are prone to errors, compromising the subsequentrecognition. In this paper, we propose a modification of the popular andefficient multi-dimensional long short-term memory recurrent neural networks(MDLSTM-RNNs) to enable end-to-end processing of handwritten paragraphs. Moreparticularly, we replace the collapse layer transforming the two-dimensionalrepresentation into a sequence of predictions by a recurrent version which canrecognize one line at a time. In the proposed model, a neural network performsa kind of implicit line segmentation by computing attention weights on theimage representation. The experiments on paragraphs of Rimes and IAM databaseyield results that are competitive with those of networks trained at linelevel, and constitute a significant step towards end-to-end transcription offull documents.
arxiv-17700-154 | An Enhanced Deep Feature Representation for Person Re-identification | http://arxiv.org/pdf/1604.07807v2.pdf | author:Shangxuan Wu, Ying-Cong Chen, Xiang Li, An-Cong Wu, Jin-Jie You, Wei-Shi Zheng category:cs.CV published:2016-04-26 summary:Feature representation and metric learning are two critical components inperson re-identification models. In this paper, we focus on the featurerepresentation and claim that hand-crafted histogram features can becomplementary to Convolutional Neural Network (CNN) features. We propose anovel feature extraction model called Feature Fusion Net (FFN) for pedestrianimage representation. In FFN, back propagation makes CNN features constrainedby the handcrafted features. Utilizing color histogram features (RGB, HSV,YCbCr, Lab and YIQ) and texture features (multi-scale and multi-orientationGabor features), we get a new deep feature representation that is morediscriminative and compact. Experiments on three challenging datasets (VIPeR,CUHK01, PRID450s) validates the effectiveness of our proposal.
arxiv-17700-155 | RATM: Recurrent Attentive Tracking Model | http://arxiv.org/pdf/1510.08660v4.pdf | author:Samira Ebrahimi Kahou, Vincent Michalski, Roland Memisevic category:cs.LG published:2015-10-29 summary:We present an attention-based modular neural framework for computer vision.The framework uses a soft attention mechanism allowing models to be trainedwith gradient descent. It consists of three modules: a recurrent attentionmodule controlling where to look in an image or video frame, afeature-extraction module providing a representation of what is seen, and anobjective module formalizing why the model learns its attentive behavior. Theattention module allows the model to focus computation on task-relatedinformation in the input. We apply the framework to several object trackingtasks and explore various design choices. We experiment with three data sets,bouncing ball, moving digits and the real-world KTH data set. The proposedRecurrent Attentive Tracking Model performs well on all three tasks and cangeneralize to related but previously unseen sequences from a challengingtracking data set.
arxiv-17700-156 | Sequential Bayesian optimal experimental design via approximate dynamic programming | http://arxiv.org/pdf/1604.08320v1.pdf | author:Xun Huan, Youssef M. Marzouk category:stat.ME math.OC stat.CO stat.ML published:2016-04-28 summary:The design of multiple experiments is commonly undertaken via suboptimalstrategies, such as batch (open-loop) design that omits feedback or greedy(myopic) design that does not account for future effects. This paper introducesnew strategies for the optimal design of sequential experiments. First, werigorously formulate the general sequential optimal experimental design (sOED)problem as a dynamic program. Batch and greedy designs are shown to result fromspecial cases of this formulation. We then focus on sOED for parameterinference, adopting a Bayesian formulation with an information theoretic designobjective. To make the problem tractable, we develop new numerical approachesfor nonlinear design with continuous parameter, design, and observation spaces.We approximate the optimal policy by using backward induction with regressionto construct and refine value function approximations in the dynamic program.The proposed algorithm iteratively generates trajectories via exploration andexploitation to improve approximation accuracy in frequently visited regions ofthe state space. Numerical results are verified against analytical solutions ina linear-Gaussian setting. Advantages over batch and greedy design are thendemonstrated on a nonlinear source inversion problem where we seek an optimalpolicy for sequential sensing.
arxiv-17700-157 | An Empirical Evaluation of Current Convolutional Architectures' Ability to Manage Nuisance Location and Scale Variability | http://arxiv.org/pdf/1505.06795v2.pdf | author:Nikolaos Karianakis, Jingming Dong, Stefano Soatto category:cs.CV cs.LG cs.NE published:2015-05-26 summary:We conduct an empirical study to test the ability of Convolutional NeuralNetworks (CNNs) to reduce the effects of nuisance transformations of the inputdata, such as location, scale and aspect ratio. We isolate factors by adoptinga common convolutional architecture either deployed globally on the image tocompute class posterior distributions, or restricted locally to compute classconditional distributions given location, scale and aspect ratios of boundingboxes determined by proposal heuristics. In theory, averaging the latter shouldyield inferior performance compared to proper marginalization. Yet empiricalevidence suggests the converse, leading us to conclude that - at the currentlevel of complexity of convolutional architectures and scale of the data setsused to train them - CNNs are not very effective at marginalizing nuisancevariability. We also quantify the effects of context on the overallclassification task and its impact on the performance of CNNs, and proposeimproved sampling techniques for heuristic proposal schemes that improveend-to-end performance to state-of-the-art levels. We test our hypothesis on aclassification task using the ImageNet Challenge benchmark and on awide-baseline matching task using the Oxford and Fischer's datasets.
arxiv-17700-158 | What value do explicit high level concepts have in vision to language problems? | http://arxiv.org/pdf/1506.01144v6.pdf | author:Qi Wu, Chunhua Shen, Lingqiao Liu, Anthony Dick, Anton van den Hengel category:cs.CV published:2015-06-03 summary:Much of the recent progress in Vision-to-Language (V2L) problems has beenachieved through a combination of Convolutional Neural Networks (CNNs) andRecurrent Neural Networks (RNNs). This approach does not explicitly representhigh-level semantic concepts, but rather seeks to progress directly from imagefeatures to text. We propose here a method of incorporating high-level conceptsinto the very successful CNN-RNN approach, and show that it achieves asignificant improvement on the state-of-the-art performance in both imagecaptioning and visual question answering. We also show that the same mechanismcan be used to introduce external semantic information and that doing sofurther improves performance. In doing so we provide an analysis of the valueof high level semantic information in V2L problems.
arxiv-17700-159 | Recognizing Activities of Daily Living with a Wrist-mounted Camera | http://arxiv.org/pdf/1511.06783v2.pdf | author:Katsunori Ohnishi, Atsushi Kanehira, Asako Kanezaki, Tatsuya Harada category:cs.CV published:2015-11-20 summary:We present a novel dataset and a novel algorithm for recognizing activitiesof daily living (ADL) from a first-person wearable camera. Handled objects arecrucially important for egocentric ADL recognition. For specific examination ofobjects related to users' actions separately from other objects in anenvironment, many previous works have addressed the detection of handledobjects in images captured from head-mounted and chest-mounted cameras.Nevertheless, detecting handled objects is not always easy because they tend toappear small in images. They can be occluded by a user's body. As describedherein, we mount a camera on a user's wrist. A wrist-mounted camera can capturehandled objects at a large scale, and thus it enables us to skip objectdetection process. To compare a wrist-mounted camera and a head-mounted camera,we also develop a novel and publicly available dataset that includes videos andannotations of daily activities captured simultaneously by both cameras.Additionally, we propose a discriminative video representation that retainsspatial and temporal information after encoding frame descriptors extracted byConvolutional Neural Networks (CNN).
arxiv-17700-160 | Streaming View Learning | http://arxiv.org/pdf/1604.08291v1.pdf | author:Chang Xu, Dacheng Tao, Chao Xu category:stat.ML cs.LG published:2016-04-28 summary:An underlying assumption in conventional multi-view learning algorithms isthat all views can be simultaneously accessed. However, due to various factorswhen collecting and pre-processing data from different views, the streamingview setting, in which views arrive in a streaming manner, is becoming morecommon. By assuming that the subspaces of a multi-view model trained over pastviews are stable, here we fine tune their combination weights such that thewell-trained multi-view model is compatible with new views. This largelyovercomes the burden of learning new view functions and updating past viewfunctions. We theoretically examine convergence issues and the influence ofstreaming views in the proposed algorithm. Experimental results on real-worlddatasets suggest that studying the streaming views problem in multi-viewlearning is significant and that the proposed algorithm can effectively handlestreaming views in different applications.
arxiv-17700-161 | Crafting Adversarial Input Sequences for Recurrent Neural Networks | http://arxiv.org/pdf/1604.08275v1.pdf | author:Nicolas Papernot, Patrick McDaniel, Ananthram Swami, Richard Harang category:cs.CR cs.LG cs.NE published:2016-04-28 summary:Machine learning models are frequently used to solve complex securityproblems, as well as to make decisions in sensitive situations like guidingautonomous vehicles or predicting financial market behaviors. Previous effortshave shown that numerous machine learning models were vulnerable to adversarialmanipulations of their inputs taking the form of adversarial samples. Suchinputs are crafted by adding carefully selected perturbations to legitimateinputs so as to force the machine learning model to misbehave, for instance byoutputting a wrong class if the machine learning task of interest isclassification. In fact, to the best of our knowledge, all previous work onadversarial samples crafting for neural network considered models used to solveclassification tasks, most frequently in computer vision applications. In thispaper, we contribute to the field of adversarial machine learning byinvestigating adversarial input sequences for recurrent neural networksprocessing sequential data. We show that the classes of algorithms introducedpreviously to craft adversarial samples misclassified by feed-forward neuralnetworks can be adapted to recurrent neural networks. In a experiment, we showthat adversaries can craft adversarial sequences misleading both categoricaland sequential recurrent neural networks.
arxiv-17700-162 | Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data | http://arxiv.org/pdf/1511.05284v2.pdf | author:Lisa Anne Hendricks, Subhashini Venugopalan, Marcus Rohrbach, Raymond Mooney, Kate Saenko, Trevor Darrell category:cs.CV cs.CL published:2015-11-17 summary:While recent deep neural network models have achieved promising results onthe image captioning task, they rely largely on the availability of corporawith paired image and sentence captions to describe objects in context. In thiswork, we propose the Deep Compositional Captioner (DCC) to address the task ofgenerating descriptions of novel objects which are not present in pairedimage-sentence datasets. Our method achieves this by leveraging large objectrecognition datasets and external text corpora and by transferring knowledgebetween semantically similar concepts. Current deep caption models can onlydescribe objects contained in paired image-sentence corpora, despite the factthat they are pre-trained with large object recognition datasets, namelyImageNet. In contrast, our model can compose sentences that describe novelobjects and their interactions with other objects. We demonstrate our model'sability to describe novel concepts by empirically evaluating its performance onMSCOCO and show qualitative results on ImageNet images of objects for which nopaired image-caption data exist. Further, we extend our approach to generatedescriptions of objects in video clips. Our results show that DCC has distinctadvantages over existing image and video captioning approaches for generatingdescriptions of new objects in context.
arxiv-17700-163 | Efficient Optimization for Rank-based Loss Functions | http://arxiv.org/pdf/1604.08269v1.pdf | author:Pritish Mohapatra, Michal Rolinek, C. V. Jawahar, Vladimir Kolmogorov, M. Pawan Kumar category:cs.CV published:2016-04-27 summary:The accuracy of information retrieval systems is often measured using complexnon-decomposable loss functions such as the average precision (AP) or thenormalized discounted cumulative gain (NDCG). Given a set of positive(relevant) and negative (non-relevant) samples, the parameters of a retrievalsystem can be estimated using a rank SVM framework, which minimizes aregularized convex upper bound on the empirical loss. However, the highcomputational complexity of loss-augmented inference, which is required tolearn a rank SVM, prohibits its use in large training datasets. To alleviatethis de?ciency, we present a novel quicksort avored algorithm for a large classof nondecomposable loss functions. We provide a complete characterization ofthe loss functions that are amenable to our algorithm. Furthermore, we provethat no comparison based algorithm can improve upon the computationalcomplexity of our approach asymptotically. We demonstrate that it is possibleto reduce the constant factors of the complexity by exploiting the specialstructure of the AP loss. Using the PASCAL VOC action recognition and objectdetection datasets, we show that our approach provides signi?cantly betterresults than baseline methods that use a simpler decomposable loss incomparable runtime.
arxiv-17700-164 | Multiview Differential Geometry of Curves | http://arxiv.org/pdf/1604.08256v1.pdf | author:Ricardo Fabbri, Benjamin Kimia category:cs.CV cs.CG cs.GR math.DG I.4.8; I.3.5 published:2016-04-27 summary:The field of multiple view geometry has seen tremendous progress inreconstruction and calibration due to methods for extracting reliable pointfeatures and key developments in projective geometry. Point features, however,are not available in certain applications and result in unstructured pointcloud reconstructions. General image curves provide a complementary featurewhen keypoints are scarce, and result in 3D curve geometry, but face challengesnot addressed by the usual projective geometry of points and algebraic curves.We address these challenges by laying the theoretical foundations of aframework based on the differential geometry of general curves, includingstationary curves, occluding contours, and non-rigid curves, aiming at stereocorrespondence, camera estimation (including calibration, pose, and multiviewepipolar geometry), and 3D reconstruction given measured image curves. Bygathering previous results into a cohesive theory, novel results were madepossible, yielding three contributions. First we derive the differentialgeometry of an image curve (tangent, curvature, curvature derivative) from thatof the underlying space curve (tangent, curvature, curvature derivative,torsion). Second, we derive the differential geometry of a space curve fromthat of two corresponding image curves. Third, the differential motion of animage curve is derived from camera motion and the differential geometry andmotion of the space curve. The availability of such a theory enables novelcurve-based multiview reconstruction and camera estimation systems to augmentexisting point-based approaches. This theory has been used to reconstruct a "3Dcurve sketch", to determine camera pose from local curve geometry, andtracking; other developments are underway.
arxiv-17700-165 | Scalable Discrete Sampling as a Multi-Armed Bandit Problem | http://arxiv.org/pdf/1506.09039v3.pdf | author:Yutian Chen, Zoubin Ghahramani category:stat.ML cs.LG published:2015-06-30 summary:Drawing a sample from a discrete distribution is one of the buildingcomponents for Monte Carlo methods. Like other sampling algorithms, discretesampling suffers from the high computational burden in large-scale inferenceproblems. We study the problem of sampling a discrete random variable with ahigh degree of dependency that is typical in large-scale Bayesian inference andgraphical models, and propose an efficient approximate solution with asubsampling approach. We make a novel connection between the discrete samplingand Multi-Armed Bandits problems with a finite reward population and providethree algorithms with theoretical guarantees. Empirical evaluations show therobustness and efficiency of the approximate algorithms in both synthetic andreal-world large-scale problems.
arxiv-17700-166 | The IBM 2016 English Conversational Telephone Speech Recognition System | http://arxiv.org/pdf/1604.08242v1.pdf | author:George Saon, Tom Sercu, Steven Rennie, Hong-Kwang J. Kuo category:cs.CL published:2016-04-27 summary:We describe a collection of acoustic and language modeling techniques thatlowered the word error rate of our English conversational telephone LVCSRsystem to a record 6.9% on the Switchboard subset of the Hub5 2000 evaluationtestset. On the acoustic side, we use a score fusion of three strong models:recurrent nets with maxout activations, very deep convolutional nets with 3x3kernels, and bidirectional long-short term memory nets which operate onbottleneck features. On the language modeling side, we use an updated model "M"and hierarchical neural network LMs.
arxiv-17700-167 | Diving deeper into mentee networks | http://arxiv.org/pdf/1604.08220v1.pdf | author:Ragav Venkatesan, Baoxin Li category:cs.LG cs.CV cs.NE published:2016-04-27 summary:Modern computer vision is all about the possession of powerful imagerepresentations. Deeper and deeper convolutional neural networks have beenbuilt using larger and larger datasets and are made publicly available. A largeswath of computer vision scientists use these pre-trained networks with varyingdegrees of successes in various tasks. Even though there is tremendous successin copying these networks, the representational space is not learnt from thetarget dataset in a traditional manner. One of the reasons for opting to use apre-trained network over a network learnt from scratch is that small datasetsprovide less supervision and require meticulous regularization, smaller andcareful tweaking of learning rates to even achieve stable learning withoutweight explosion. It is often the case that large deep networks are notportable, which necessitates the ability to learn mid-sized networks fromscratch. In this article, we dive deeper into training these mid-sized networks onsmall datasets from scratch by drawing additional supervision from a largepre-trained network. Such learning also provides better generalizationaccuracies than networks trained with common regularization techniques such asl2, l1 and dropouts. We show that features learnt thus, are more general thanthose learnt independently. We studied various characteristics of such networksand found some interesting behaviors.
arxiv-17700-168 | Amodal Instance Segmentation | http://arxiv.org/pdf/1604.08202v1.pdf | author:Ke Li, Jitendra Malik category:cs.CV published:2016-04-27 summary:We consider the problem of amodal instance segmentation, the objective ofwhich is to predict the region encompassing both visible and occluded parts ofeach object. Thus far, the lack of publicly available amodal segmentationannotations has stymied the development of amodal segmentation methods. In thispaper, we sidestep this issue by relying solely on standard modal instancesegmentation annotations to train our model. The result is a new method foramodal instance segmentation, which represents the first such method to thebest of our knowledge. We demonstrate the proposed method's effectiveness bothqualitatively and quantitatively.
arxiv-17700-169 | Learning with Differential Privacy: Stability, Learnability and the Sufficiency and Necessity of ERM Principle | http://arxiv.org/pdf/1502.06309v3.pdf | author:Yu-Xiang Wang, Jing Lei, Stephen E. Fienberg category:stat.ML cs.CR cs.LG published:2015-02-23 summary:While machine learning has proven to be a powerful data-driven solution tomany real-life problems, its use in sensitive domains has been limited due toprivacy concerns. A popular approach known as **differential privacy** offersprovable privacy guarantees, but it is often observed in practice that it couldsubstantially hamper learning accuracy. In this paper we study the learnability(whether a problem can be learned by any algorithm) under Vapnik's generallearning setting with differential privacy constraint, and reveal someintricate relationships between privacy, stability and learnability. In particular, we show that a problem is privately learnable **if an onlyif** there is a private algorithm that asymptotically minimizes the empiricalrisk (AERM). In contrast, for non-private learning AERM alone is not sufficientfor learnability. This result suggests that when searching for private learningalgorithms, we can restrict the search to algorithms that are AERM. In light ofthis, we propose a conceptual procedure that always finds a universallyconsistent algorithm whenever the problem is learnable under privacyconstraint. We also propose a generic and practical algorithm and show thatunder very general conditions it privately learns a wide class of learningproblems. Lastly, we extend some of the results to the more practical$(\epsilon,\delta)$-differential privacy and establish the existence of aphase-transition on the class of problems that are approximately privatelylearnable with respect to how small $\delta$ needs to be.
arxiv-17700-170 | Interpretable Deep Neural Networks for Single-Trial EEG Classification | http://arxiv.org/pdf/1604.08201v1.pdf | author:Irene Sturm, Sebastian Bach, Wojciech Samek, Klaus-Robert Müller category:cs.NE stat.ML published:2016-04-27 summary:Background: In cognitive neuroscience the potential of Deep Neural Networks(DNNs) for solving complex classification tasks is yet to be fully exploited.The most limiting factor is that DNNs as notorious 'black boxes' do not provideinsight into neurophysiological phenomena underlying a decision. Layer-wiseRelevance Propagation (LRP) has been introduced as a novel method to explainindividual network decisions. New Method: We propose the application of DNNswith LRP for the first time for EEG data analysis. Through LRP the single-trialDNN decisions are transformed into heatmaps indicating each data point'srelevance for the outcome of the decision. Results: DNN achieves classificationaccuracies comparable to those of CSP-LDA. In subjects with low performancesubject-to-subject transfer of trained DNNs can improve the results. Thesingle-trial LRP heatmaps reveal neurophysiologically plausible patterns,resembling CSP-derived scalp maps. Critically, while CSP patterns representclass-wise aggregated information, LRP heatmaps pinpoint neural patterns tosingle time points in single trials. Comparison with Existing Method(s): Wecompare the classification performance of DNNs to that of linear CSP-LDA on twodata sets related to motor-imaginery BCI. Conclusion: We have demonstrated thatDNN is a powerful non-linear tool for EEG analysis. With LRP a new quality ofhigh-resolution assessment of neural activity can be reached. LRP is apotential remedy for the lack of interpretability of DNNs that has limitedtheir utility in neuroscientific applications. The extreme specificity of theLRP-derived heatmaps opens up new avenues for investigating neural activityunderlying complex perception or decision-related processes.
arxiv-17700-171 | Unsupervised Classification in Hyperspectral Imagery with Nonlocal Total Variation and Primal-Dual Hybrid Gradient Algorithm | http://arxiv.org/pdf/1604.08182v1.pdf | author:Wei Zhu, Victoria Chayes, Alexandre Tiard, Stephanie Sanchez, Devin Dahlberg, Da Kuang, Andrea L. Bertozzi, Stanley Osher, Dominique Zosso category:cs.CV published:2016-04-27 summary:We propose a graph-based nonlocal total variation method (NLTV) forunsupervised classification of hyperspectral images (HSI). The variationproblem is solved by the primal-dual hybrid gradient (PDHG) algorithm. Bysquaring the labeling function and using a stable simplex clustering routine,we can implement an unsupervised clustering method with random initialization.Finally, we speed up the calculation using a $k$-d tree and approximate nearestneighbor search algorithm for calculation of the weight matrix for distancesbetween pixel signatures. The effectiveness of this proposed algorithm isillustrated on both synthetic and real-world HSI, and numerical results showthat our algorithm outperform other standard unsupervised clustering methodssuch as spherical K-means, nonnegative matrix factorization (NMF), and thegraph-based Merriman-Bence-Osher (MBO) scheme.
arxiv-17700-172 | Classifying Syntactic Regularities for Hundreds of Languages | http://arxiv.org/pdf/1603.08016v2.pdf | author:Reed Coke, Ben King, Dragomir Radev category:cs.CL published:2016-03-25 summary:This paper presents a comparison of classification methods for linguistictypology for the purpose of expanding an extensive, but sparse languageresource: the World Atlas of Language Structures (WALS) (Dryer and Haspelmath,2013). We experimented with a variety of regression and nearest-neighbormethods for use in classification over a set of 325 languages and six syntacticrules drawn from WALS. To classify each rule, we consider the typologicalfeatures of the other five rules; linguistic features extracted from aword-aligned Bible in each language; and genealogical features (genus andfamily) of each language. In general, we find that propagating the majoritylabel among all languages of the same genus achieves the best accuracy in labelpre- diction. Following this, a logistic regression model that combinestypological and linguistic features offers the next best performance.Interestingly, this model actually outperforms the majority labels among alllanguages of the same family.
arxiv-17700-173 | Influence Maximization with Bandits | http://arxiv.org/pdf/1503.00024v4.pdf | author:Sharan Vaswani, Laks. V. S. Lakshmanan, Mark Schmidt category:cs.SI cs.LG stat.ML published:2015-02-27 summary:We consider the problem of \emph{influence maximization}, the problem ofmaximizing the number of people that become aware of a product by finding the`best' set of `seed' users to expose the product to. Most prior work on thistopic assumes that we know the probability of each user influencing each otheruser, or we have data that lets us estimate these influences. However, thisinformation is typically not initially available or is difficult to obtain. Toavoid this assumption, we adopt a combinatorial multi-armed bandit paradigmthat estimates the influence probabilities as we sequentially try differentseed sets. We establish bounds on the performance of this procedure under theexisting edge-level feedback as well as a novel and more realistic node-levelfeedback. Beyond our theoretical results, we describe a practicalimplementation and experimentally demonstrate its efficiency and effectivenesson four real datasets.
arxiv-17700-174 | Classifying Options for Deep Reinforcement Learning | http://arxiv.org/pdf/1604.08153v1.pdf | author:Kai Arulkumaran, Nat Dilokthanakul, Murray Shanahan, Anil Anthony Bharath category:cs.LG cs.AI stat.ML published:2016-04-27 summary:Deep reinforcement learning is the learning of multiple levels ofhierarchical representations for reinforcement learning. Hierarchicalreinforcement learning focuses on temporal abstractions in planning andlearning, allowing temporally-extended actions to be transferred between tasks.In this paper we combine one method for hierarchical reinforcement learning -the options framework - with deep Q-networks (DQNs) through the use ofdifferent "option heads" on the policy network, and a supervisory network forchoosing between the different options. We show that in a domain where we haveprior knowledge of the mapping between states and options, our augmented DQNachieves a policy competitive with that of a standard DQN, but with much lowersample complexity. This is achieved through a straightforward architecturaladjustment to the DQN, as well as an additional supervised neural network.
arxiv-17700-175 | Walk and Learn: Facial Attribute Representation Learning from Egocentric Video and Contextual Data | http://arxiv.org/pdf/1604.06433v2.pdf | author:Jing Wang, Yu Cheng, Rogerio Schmidt Feris category:cs.CV published:2016-04-21 summary:The way people look in terms of facial attributes (ethnicity, hair color,facial hair, etc.) and the clothes or accessories they wear (sunglasses, hat,hoodies, etc.) is highly dependent on geo-location and weather condition,respectively. This work explores, for the first time, the use of thiscontextual information, as people with wearable cameras walk across differentneighborhoods of a city, in order to learn a rich feature representation forfacial attribute classification, without the costly manual annotation requiredby previous methods. By tracking the faces of casual walkers on more than 40hours of egocentric video, we are able to cover tens of thousands of differentidentities and automatically extract nearly 5 million pairs of images connectedby or from different face tracks, along with their weather and locationcontext, under pose and lighting variations. These image pairs are then fedinto a deep network that preserves similarity of images connected by the sametrack, in order to capture identity-related attribute features, and optimizesfor location and weather prediction to capture additional facial attributefeatures. Finally, the network is fine-tuned with manually annotated samples.We perform an extensive experimental analysis on wearable data and two standardbenchmark datasets based on web images (LFWA and CelebA). Our methodoutperforms by a large margin a network trained from scratch. Moreover, evenwithout using manually annotated identity labels for pre-training as inprevious methods, our approach achieves results that are better than the stateof the art.
arxiv-17700-176 | Laser light-field fusion for wide-field lensfree on-chip phase contrast nanoscopy | http://arxiv.org/pdf/1604.08145v1.pdf | author:Farnoud Kazemzadeh, Alexander Wong category:physics.optics cs.CV published:2016-04-27 summary:Wide-field lensfree on-chip microscopy, which leverages holography principlesto capture interferometric light-field encodings without lenses, is an emergingimaging modality with widespread interest given the large field-of-viewcompared to lens-based techniques. Nanoscopy is often synonymous with highequipment costs and limited FOV. In this study, we introduce the idea of laserlight-field fusion for lensfree on-chip phase contrast nanoscopy, whereinterferometric laser light-field encodings acquired using an on-chip setupwith laser pulsations at different wavelengths are fused to produce marker-freephase contrast images with resolving power below the pixel pitch of the sensorarray as well as the wavelength of the probing light source, beyond thediffraction limit. Experimental results demonstrate, for the first time, alensfree on-chip instrument successfully detecting 500 nm nanoparticles withoutany specialized or intricate sample preparation or the use of syntheticaperture- or lateral shift-based techniques.
arxiv-17700-177 | Extracting Temporal and Causal Relations between Events | http://arxiv.org/pdf/1604.08120v1.pdf | author:Paramita Mirza category:cs.CL published:2016-04-27 summary:Structured information resulting from temporal information processing iscrucial for a variety of natural language processing tasks, for instance togenerate timeline summarization of events from news documents, or to answertemporal/causal-related questions about some events. In this thesis we presenta framework for an integrated temporal and causal relation extraction system.We first develop a robust extraction component for each type of relations, i.e.temporal order and causality. We then combine the two extraction componentsinto an integrated relation extraction system, CATENA---CAusal and Temporalrelation Extraction from NAtural language texts---, by utilizing thepresumption about event precedence in causality, that causing events musthappened BEFORE resulting events. Several resources and techniques to improveour relation extraction systems are also discussed, including word embeddingsand training data expansion. Finally, we report our adaptation efforts oftemporal information processing for languages other than English, namelyItalian and Indonesian.
arxiv-17700-178 | Global-Local Face Upsampling Network | http://arxiv.org/pdf/1603.07235v2.pdf | author:Oncel Tuzel, Yuichi Taguchi, John R. Hershey category:cs.CV cs.LG published:2016-03-23 summary:Face hallucination, which is the task of generating a high-resolution faceimage from a low-resolution input image, is a well-studied problem that isuseful in widespread application areas. Face hallucination is particularlychallenging when the input face resolution is very low (e.g., 10 x 12 pixels)and/or the image is captured in an uncontrolled setting with large pose andillumination variations. In this paper, we revisit the algorithm introduced in[1] and present a deep interpretation of this framework that achievesstate-of-the-art under such challenging scenarios. In our deep networkarchitecture the global and local constraints that define a face can beefficiently modeled and learned end-to-end using training data. Conceptuallyour network design can be partitioned into two sub-networks: the first oneimplements the holistic face reconstruction according to global constraints,and the second one enhances face-specific details and enforces local patchstatistics. We optimize the deep network using a new loss function forsuper-resolution that combines reconstruction error with a learned face qualitymeasure in adversarial setting, producing improved visual results. We conductextensive experiments in both controlled and uncontrolled setups and show thatour algorithm improves the state of the art both numerically and visually.
arxiv-17700-179 | An ABC interpretation of the multiple auxiliary variable method | http://arxiv.org/pdf/1604.08102v1.pdf | author:Dennis Prangle, Richard G. Everitt category:stat.CO stat.ML published:2016-04-27 summary:We show that the auxiliary variable method (M{\o}ller et al., 2006; Murray etal., 2006) for inference of Markov random fields can be viewed as anapproximate Bayesian computation method for likelihood estimation.
arxiv-17700-180 | Local Uncertainty Sampling for Large-Scale Multi-Class Logistic Regression | http://arxiv.org/pdf/1604.08098v1.pdf | author:Lei Han, Ting Yang, Tong Zhang category:stat.CO cs.LG stat.ML published:2016-04-27 summary:A major challenge for building statistical models in the big data era is thatthe available data volume may exceed the computational capability. A commonapproach to solve this problem is to employ a subsampled dataset that can behandled by the available computational resources. In this paper, we propose ageneral subsampling scheme for large-scale multi-class logistic regression, andexamine the variance of the resulting estimator. We show that asymptotically,the proposed method always achieves a smaller variance than that of the uniformrandom sampling. Moreover, when the classes are conditional imbalanced,significant improvement over uniform sampling can be achieved. Empiricalperformance of the proposed method is compared to other methods on bothsimulated and real-world datasets, and these results confirm our theoreticalanalysis.
arxiv-17700-181 | Detecting Violence in Video using Subclasses | http://arxiv.org/pdf/1604.08088v1.pdf | author:Xirong Li, Yujia Huo, Jieping Xu, Qin Jin category:cs.MM cs.CV published:2016-04-27 summary:This paper attacks the challenging problem of violence detection in videos.Different from existing works focusing on combining multi-modal features, we goone step further by adding and exploiting subclasses visually related toviolence. We enrich the MediaEval 2015 violence dataset by \emph{manually}labeling violence videos with respect to the subclasses. Such fine-grainedannotations not only help understand what have impeded previous efforts onlearning to fuse the multi-modal features, but also enhance the generalizationability of the learned fusion to novel test data. The new subclass basedsolution, with AP of 0.303 and P100 of 0.55 on the MediaEval 2015 test set,outperforms several state-of-the-art alternatives. Notice that our solutiondoes not require fine-grained annotations on the test set, so it can bedirectly applied on novel and fully unlabeled videos. Interestingly, our studyshows that motion related features, though being essential part in previoussystems, are dispensable.
arxiv-17700-182 | UBL: an R package for Utility-based Learning | http://arxiv.org/pdf/1604.08079v1.pdf | author:Paula Branco, Rita P. Ribeiro, Luis Torgo category:cs.MS cs.LG stat.ML published:2016-04-27 summary:This document describes the R package UBL that allows the use of severalmethods for handling utility-based learning problems. Classification andregression problems that assume non-uniform costs and/or benefits pose seriouschallenges to predictive analytics tasks. In the context of meteorology,finance, medicine, ecology, among many other, specific domain informationconcerning the preference bias of the users must be taken into account toenhance the models predictive performance. To deal with this problem, a largenumber of techniques was proposed by the research community for bothclassification and regression tasks. The main goal of UBL package is tofacilitate the utility-based predictive analytics task by providing a set ofmethods to deal with this type of problems in the R environment. It is aversatile tool that provides mechanisms to handle both regression andclassification (binary and multiclass) tasks. Moreover, UBL package allows theuser to specify his domain preferences, but it also provides some automaticmethods that try to infer those preference bias from the domain, consideringsome common known settings.
arxiv-17700-183 | Unsupervised Image Segmentation using the Deffuant-Weisbuch Model from Social Dynamics | http://arxiv.org/pdf/1604.04393v2.pdf | author:Subhradeep Kayal category:cs.CV published:2016-04-15 summary:Unsupervised image segmentation algorithms aim at identifying disjointhomogeneous regions in an image, and have been subject to considerableattention in the machine vision community. In this paper, a popular theoreticalmodel with it's origins in statistical physics and social dynamics, known asthe Deffuant-Weisbuch model, is applied to the image segmentation problem. TheDeffuant-Weisbuch model has been found to be useful in modelling the evolutionof a closed system of interacting agents characterised by their opinions orbeliefs, leading to the formation of clusters of agents who share a similaropinion or belief at steady state. In the context of image segmentation, thispaper considers a pixel as an agent and it's colour property as it's opinion,with opinion updates as per the Deffuant-Weisbuch model. Apart from applyingthe basic model to image segmentation, this paper incorporates adjacency andneighbourhood information in the model, which factors in the local similarityand smoothness properties of images. Convergence is reached when the number ofunique pixel opinions, i.e., the number of colour centres, matches thepre-specified number of clusters. Experiments are performed on a set of imagesfrom the Berkeley Image Segmentation Dataset and the results are analysed bothqualitatively and quantitatively, which indicate that this simple and intuitivemethod is promising for image segmentation. To the best of the knowledge of theauthor, this is the first work where a theoretical model from statisticalphysics and social dynamics has been successfully applied to image processing.
arxiv-17700-184 | Incremental Reconstruction of Urban Environments by Edge-Points Delaunay Triangulation | http://arxiv.org/pdf/1604.06232v2.pdf | author:Andrea Romanoni, Matteo Matteucci category:cs.CV cs.RO I.4.5 published:2016-04-21 summary:Urban reconstruction from a video captured by a surveying vehicle constitutesa core module of automated mapping. When computational power represents alimited resource and, a detailed map is not the primary goal, thereconstruction can be performed incrementally, from a monocular video, carvinga 3D Delaunay triangulation of sparse points; this allows online incrementalmapping for tasks such as traversability analysis or obstacle avoidance. Toexploit the sharp edges of urban landscape, we propose to use a Delaunaytriangulation of Edge-Points, which are the 3D points corresponding to imageedges. These points constrain the edges of the 3D Delaunay triangulation toreal-world edges. Besides the use of the Edge-Points, a second contribution ofthis paper is the Inverse Cone Heuristic that preemptively avoids the creationof artifacts in the reconstructed manifold surface. We force the reconstructionof a manifold surface since it makes it possible to apply computer graphics orphotometric refinement algorithms to the output mesh. We evaluated our approachon four real sequences of the public available KITTI dataset by comparing theincremental reconstruction against Velodyne measurements.
arxiv-17700-185 | Deep Learning for Saliency Prediction in Natural Video | http://arxiv.org/pdf/1604.08010v1.pdf | author:Souad Chaabouni, Jenny Benois-Pineau, Ofer Hadar, Chokri Ben Amar category:cs.CV published:2016-04-27 summary:The purpose of this paper is the detection of salient areas in natural videoby using the new deep learning techniques. Salient patches in video frames arepredicted first. Then the predicted visual fixation maps are built upon them.We design the deep architecture on the basis of CaffeNet implemented with Caffetoolkit. We show that changing the way of data selection for optimisation ofnetwork parameters, we can save computation cost up to 12 times. We extend deeplearning approaches for saliency prediction in still images with RGB values tospecificity of video using the sensitivity of the human visual system toresidual motion. Furthermore, we complete primary colour pixel values bycontrast features proposed in classical visual attention prediction models. Theexperiments are conducted on two publicly available datasets. The first isIRCCYN video database containing 31 videos with an overall amount of 7300frames and eye fixations of 37 subjects. The second one is HOLLYWOOD2 provided2517 movie clips with the eye fixations of 19 subjects. On IRCYYN dataset, theaccuracy obtained is of 89.51%. On HOLLYWOOD2 dataset, results in prediction ofsaliency of patches show the improvement up to 2% with regard to RGB use only.The resulting accuracy of 76, 6% is obtained. The AUC metric in comparison ofpredicted saliency maps with visual fixation maps shows the increase up to 16%on a sample of video clips from this dataset.
arxiv-17700-186 | Using Filter Banks in Convolutional Neural Networks for Texture Classification | http://arxiv.org/pdf/1601.02919v3.pdf | author:Vincent Andrearczyk, Paul F. Whelan category:cs.CV cs.NE published:2016-01-12 summary:Deep learning has established many new state of the art solutions in the lastdecade in areas such as object, scene and speech recognition. In particularConvolutional Neural Network (CNN) is a category of deep learning which obtainsexcellent results in object detection and recognition tasks. Its architectureis indeed well suited to object analysis by learning and classifying complex(deep) features that represent parts of an object or the object itself.However, some of its features are very similar to texture analysis methods. CNNlayers can be thought of as filter banks of complexity increasing with thedepth. Filter banks are powerful tools to extract texture features and havebeen widely used in texture analysis. In this paper we develop a simple networkarchitecture named Texture CNN (T-CNN) which explores this observation. It isbuilt on the idea that the overall shape information extracted by the fullyconnected layers of a classic CNN is of minor importance in texture analysis.Therefore, we pool an energy measure from the last convolution layer which weconnect to a fully connected layer. We show that our approach can improve theperformance of a network while greatly reducing the memory usage andcomputation.
arxiv-17700-187 | Multi-Fold Gabor, PCA and ICA Filter Convolution Descriptor for Face Recognition | http://arxiv.org/pdf/1604.07057v2.pdf | author:Cheng Yaw Low, Andrew Beng Jin Teoh, Cong Jie Ng category:cs.CV published:2016-04-24 summary:This paper devises a new means of filter diversification, dubbed multi-foldfilter convolution (M-FFC), for face recognition. On the assumption that M-FFCreceives single-scale Gabor filters of varying orientations as input, thesefilters are self-cross convolved by M-fold to instantiate an offspring set. TheM-FFC flexibility also permits the self-cross convolution amongst Gabor filtersand other filter banks of profoundly dissimilar traits, e.g., principalcomponent analysis (PCA) filters, and independent component analysis (ICA)filters, in our case. A 2-FFC instance therefore yields three offspring setsfrom: (1) Gabor filters solely, (2) Gabor and PCA filters, and (3) Gabor andICA filters, to render the learning-free and the learning-based 2-FFCdescriptors. To facilitate a sensible Gabor filter selection for M-FFC, the 40multi-scale, multi-orientation Gabor filters is condensed into 8 elementaryfilters. In addition to that, an average pooling operator is used to leveragethe 2-FFC histogram features, prior to whitening PCA compression. The empiricalresults substantiate that the 2-FFC descriptors prevail over, or on par with,other face descriptors on both identification and verification tasks.
arxiv-17700-188 | Change Detection in Multivariate Datastreams: Likelihood and Detectability Loss | http://arxiv.org/pdf/1510.04850v3.pdf | author:Cesare Alippi, Giacomo Boracchi, Diego Carrera, Manuel Roveri category:stat.ML published:2015-10-16 summary:We address the problem of detecting changes in multivariate datastreams, andwe investigate the intrinsic difficulty that change-detection methods have toface when the data dimension scales. In particular, we consider a generalapproach where changes are detected by comparing the distribution of thelog-likelihood of the datastream over different time windows. Despite the factthat this approach constitutes the frame of several change-detection methods,its effectiveness when data dimension scales has never been investigated, whichis indeed the goal of our paper. We show that the magnitude of the change canbe naturally measured by the symmetric Kullback-Leibler divergence between thepre- and post-change distributions, and that the detectability of a change of agiven magnitude worsens when the data dimension increases. This problem, whichwe refer to as \emph{detectability loss}, is due to the linear relationshipbetween the variance of the log-likelihood and the data dimension. Weanalytically derive the detectability loss on Gaussian-distributed datastreams,and empirically demonstrate that this problem holds also on real-world datasetsand that can be harmful even at low data-dimensions (say, 10).
arxiv-17700-189 | Simultaneous Food Localization and Recognition | http://arxiv.org/pdf/1604.07953v1.pdf | author:Marc Bolaños, Petia Radeva category:cs.CV published:2016-04-27 summary:The development of automatic nutrition diaries, which would allow to keeptrack objectively of everything we eat, could enable a whole new world ofpossibilities for people concerned about their nutrition patterns. With thispurpose, in this paper we propose the first method for simultaneous foodlocalization and recognition. Our method is based on two main steps, whichconsist in, first, produce a food activation map on the input image (i.e. heatmap of probabilities) for generating bounding boxes proposals and, second,recognize each of the food types or food-related objects present in eachbounding box. We demonstrate that our proposal, compared to the most similarproblem nowadays - object localization, is able to obtain high precision andreasonable recall levels with only a few bounding boxes. Furthermore, we showthat it is applicable to both conventional and egocentric images.
arxiv-17700-190 | Zero-shot object prediction and context modeling using semantic scene knowledge | http://arxiv.org/pdf/1604.07952v1.pdf | author:Rene Grzeszick, Gernot A. Fink category:cs.CV published:2016-04-27 summary:This work will focus on the semantic relations between scenes and objects forvisual object recognition. Semantic knowledge can be a powerful source ofinformation especially in scenarios with less or no annotated training samples.These scenarios are referred to as zero-shot recognition and often build onvisual attributes. Here, instead of attributes a more direct way is pursued,relating scenes and objects. The contribution of this paper is two-fold: First,it will be shown that scene knowledge can be an important cue for predictingobjects in an unsupervised manner. This is especially useful in clutteredscenes where visual recognition may be difficult. Second, it will be shown thatthis information can easily be integrated as a context model for objectdetection in a supervised setting.
arxiv-17700-191 | Graph Laplacian Regularization for Inverse Imaging: Analysis in the Continuous Domain | http://arxiv.org/pdf/1604.07948v1.pdf | author:Jiahao Pang, Gene Cheung category:cs.CV published:2016-04-27 summary:Inverse imaging problems are inherently under-determined, and hence it isimportant to employ appropriate image priors for regularization. One recentpopular prior---the graph Laplacian regularizer---assumes that the target pixelpatch is smooth with respect to an appropriately chosen graph. However, themechanisms and implications of imposing the graph Laplacian regularizer on theoriginal inverse problem are not well understood. To address this problem, inthis paper we interpret neighborhood graphs of pixel patches as discretecounterparts of Riemannian manifolds and perform analysis in the continuousdomain, providing insights into several fundamental aspects of graph Laplacianregularization. Specifically, we first show the convergence of the graphLaplacian regularizer to a continuous-domain functional, integrating a normmeasured in a locally adaptive metric space. Focusing on image denoising, wederive an optimal metric space assuming nonlocal self-similarity of pixelpatches, leading to an optimal graph Laplacian regularizer for denoising in thediscrete domain. We then interpret graph Laplacian regularization as ananisotropic diffusion scheme to explain its behavior during iterations, e.g.,its tendency to promote piecewise smooth signals under certain settings. Toverify our analysis, an iterative image denoising algorithm is developed.Experimental results show that our algorithm performs competitively withstate-of-the-art denoising methods such as BM3D for natural images, andoutperforms them significantly for piecewise smooth images.
arxiv-17700-192 | A Richly Annotated Dataset for Pedestrian Attribute Recognition | http://arxiv.org/pdf/1603.07054v3.pdf | author:Dangwei Li, Zhang Zhang, Xiaotang Chen, Haibin Ling, Kaiqi Huang category:cs.CV published:2016-03-23 summary:In this paper, we aim to improve the dataset foundation for pedestrianattribute recognition in real surveillance scenarios. Recognition of humanattributes, such as gender, and clothes types, has great prospects in realapplications. However, the development of suitable benchmark datasets forattribute recognition remains lagged behind. Existing human attribute datasetsare collected from various sources or an integration of pedestrianre-identification datasets. Such heterogeneous collection poses a big challengeon developing high quality fine-grained attribute recognition algorithms.Furthermore, human attribute recognition are generally severely affected byenvironmental or contextual factors, such as viewpoints, occlusions and bodyparts, while existing attribute datasets barely care about them. To tacklethese problems, we build a Richly Annotated Pedestrian (RAP) dataset from realmulti-camera surveillance scenarios with long term collection, where datasamples are annotated with not only fine-grained human attributes but alsoenvironmental and contextual factors. RAP has in total 41,585 pedestriansamples, each of which is annotated with 72 attributes as well as viewpoints,occlusions, body parts information. To our knowledge, the RAP dataset is thelargest pedestrian attribute dataset, which is expected to greatly promote thestudy of large-scale attribute recognition systems. Furthermore, we empiricallyanalyze the effects of different environmental and contextual factors onpedestrian attribute recognition. Experimental results demonstrate thatviewpoints, occlusions and body parts information could assist attributerecognition a lot in real applications.
arxiv-17700-193 | DASC: Robust Dense Descriptor for Multi-modal and Multi-spectral Correspondence Estimation | http://arxiv.org/pdf/1604.07944v1.pdf | author:Seungryong Kim, Dongbo Min, Bumsub Ham, Minh N. Do, Kwanghoon Sohn category:cs.CV published:2016-04-27 summary:Establishing dense correspondences between multiple images is a fundamentaltask in many applications. However, finding a reliable correspondence inmulti-modal or multi-spectral images still remains unsolved due to theirchallenging photometric and geometric variations. In this paper, we propose anovel dense descriptor, called dense adaptive self-correlation (DASC), toestimate multi-modal and multi-spectral dense correspondences. Based on anobservation that self-similarity existing within images is robust to imagingmodality variations, we define the descriptor with a series of an adaptiveself-correlation similarity measure between patches sampled by a randomizedreceptive field pooling, in which a sampling pattern is obtained using adiscriminative learning. The computational redundancy of dense descriptors isdramatically reduced by applying fast edge-aware filtering. Furthermore, inorder to address geometric variations including scale and rotation, we proposea geometry-invariant DASC (GI-DASC) descriptor that effectively leverages theDASC through a superpixel-based representation. For a quantitative evaluationof the GI-DASC, we build a novel multi-modal benchmark as varying photometricand geometric conditions. Experimental results demonstrate the outstandingperformance of the DASC and GI-DASC in many cases of multi-modal andmulti-spectral dense correspondences.
arxiv-17700-194 | Tensor vs Matrix Methods: Robust Tensor Decomposition under Block Sparse Perturbations | http://arxiv.org/pdf/1510.04747v7.pdf | author:Animashree Anandkumar, Prateek Jain, Yang Shi, U. N. Niranjan category:cs.LG cs.IT math.IT stat.ML published:2015-10-15 summary:Robust tensor CP decomposition involves decomposing a tensor into low rankand sparse components. We propose a novel non-convex iterative algorithm withguaranteed recovery. It alternates between low-rank CP decomposition throughgradient ascent (a variant of the tensor power method), and hard thresholdingof the residual. We prove convergence to the globally optimal solution undernatural incoherence conditions on the low rank component, and bounded level ofsparse perturbations. We compare our method with natural baselines which applyrobust matrix PCA either to the {\em flattened} tensor, or to the matrix slicesof the tensor. Our method can provably handle a far greater level ofperturbation when the sparse tensor is block-structured. This naturally occursin many applications such as the activity detection task in videos. Ourexperiments validate these findings. Thus, we establish that tensor methods cantolerate a higher level of gross corruptions compared to matrix methods.
arxiv-17700-195 | Distributed Flexible Nonlinear Tensor Factorization | http://arxiv.org/pdf/1604.07928v1.pdf | author:Shandian Zhe, Pengyuan Wang, Kuang-chih Lee, Zenglin Xu, Jian Yang, Youngja Park, Yuan Qi category:cs.LG cs.AI cs.DC stat.ML I.5.1; I.5.4 published:2016-04-27 summary:Tensor factorization is an important approach to multiway data analysis.Compared with popular multilinear methods, nonlinear tensor factorizationmodels are able to capture more complex relationships in data. However, theyare computationally expensive and incapable of exploiting the data sparsity. Toovercome these limitations, we propose a new tensor factorization model. Themodel employs a Gaussian process (GP) to capture the complex nonlinearrelationships. The GP can be projected to arbitrary sets of tensor elements,and thus can avoid the expensive computation of the Kronecker product and isable to flexibly incorporate meaningful entries for training. Furthermore, toscale up the model to large data, we develop a distributed variationalinference algorithm in MapReduce framework. To this end, we derive a tractableand tight variational evidence lower bound (ELBO) that enables efficientparallel computations and high quality inferences. In addition, we design anon-key-value Map-Reduce scheme that can prevent the costly data shuffling andfully use the memory-cache mechanism in fast MapReduce systems such as SPARK.Experiments demonstrate the advantages of our method over existing approachesin terms of both predictive performance and computational efficiency. Moreover,our approach shows a promising potential in the application ofClick-Through-Rate (CTR) prediction for online advertising.
arxiv-17700-196 | Smoothing parameter estimation framework for IBM word alignment models | http://arxiv.org/pdf/1601.03650v4.pdf | author:Vuong Van Bui, Cuong Anh Le category:cs.CL published:2016-01-14 summary:IBM models are very important word alignment models in Machine Translation.Following the Maximum Likelihood Estimation principle to estimate theirparameters, the models will easily overfit the training data when the data aresparse. While smoothing is a very popular solution in Language Model, therestill lacks studies on smoothing for word alignment. In this paper, we proposea framework which generalizes the notable work Moore [2004] of applyingadditive smoothing to word alignment models. The framework allows developers tocustomize the smoothing amount for each pair of word. The added amount will bescaled appropriately by a common factor which reflects how much the frameworktrusts the adding strategy according to the performance on data. We alsocarefully examine various performance criteria and propose a smoothened versionof the error count, which generally gives the best result.
arxiv-17700-197 | Simple, Robust and Optimal Ranking from Pairwise Comparisons | http://arxiv.org/pdf/1512.08949v2.pdf | author:Nihar B. Shah, Martin J. Wainwright category:cs.LG cs.AI cs.IT math.IT stat.ML published:2015-12-30 summary:We consider data in the form of pairwise comparisons of n items, with thegoal of precisely identifying the top k items for some value of k < n, oralternatively, recovering a ranking of all the items. We analyze the Copelandcounting algorithm that ranks the items in order of the number of pairwisecomparisons won, and show it has three attractive features: (a) itscomputational efficiency leads to speed-ups of several orders of magnitude incomputation time as compared to prior work; (b) it is robust in thattheoretical guarantees impose no conditions on the underlying matrix ofpairwise-comparison probabilities, in contrast to some prior work that appliesonly to the BTL parametric model; and (c) it is an optimal method up toconstant factors, meaning that it achieves the information-theoretic limits forrecovering the top k-subset. We extend our results to obtain sharp guaranteesfor approximate recovery under the Hamming distortion metric, and moregenerally, to any arbitrary error requirement that satisfies a simple andnatural monotonicity condition.
arxiv-17700-198 | Train and Test Tightness of LP Relaxations in Structured Prediction | http://arxiv.org/pdf/1511.01419v3.pdf | author:Ofer Meshi, Mehrdad Mahdavi, Adrian Weller, David Sontag category:stat.ML cs.AI cs.LG published:2015-11-04 summary:Structured prediction is used in areas such as computer vision and naturallanguage processing to predict structured outputs such as segmentations orparse trees. In these settings, prediction is performed by MAP inference or,equivalently, by solving an integer linear program. Because of the complexscoring functions required to obtain accurate predictions, both learning andinference typically require the use of approximate solvers. We propose atheoretical explanation to the striking observation that approximations basedon linear programming (LP) relaxations are often tight on real-world instances.In particular, we show that learning with LP relaxed inference encouragesintegrality of training instances, and that tightness generalizes from train totest data.
arxiv-17700-199 | Audio Recording Device Identification Based on Deep Learning | http://arxiv.org/pdf/1602.05682v2.pdf | author:Simeng Qi, Zheng Huang, Yan Li, Shaopei Shi category:cs.SD cs.LG published:2016-02-18 summary:In this paper we present a research on identification of audio recordingdevices from background noise, thus providing a method for forensics. The audiosignal is the sum of speech signal and noise signal. Usually, people pay moreattention to speech signal, because it carries the information to deliver. So agreat amount of researches have been dedicated to getting higherSignal-Noise-Ratio (SNR). There are many speech enhancement algorithms toimprove the quality of the speech, which can be seen as reducing the noise.However, noises can be regarded as the intrinsic fingerprint traces of an audiorecording device. These digital traces can be characterized and identified bynew machine learning techniques. Therefore, in our research, we use the noiseas the intrinsic features. As for the identification, multiple classifiers ofdeep learning methods are used and compared. The identification result showsthat the method of getting feature vector from the noise of each device andidentifying them with deep learning techniques is viable, and well-preformed.
arxiv-17700-200 | Image Colorization Using a Deep Convolutional Neural Network | http://arxiv.org/pdf/1604.07904v1.pdf | author:Tung Nguyen, Kazuki Mori, Ruck Thawonmas category:cs.CV cs.LG cs.NE published:2016-04-27 summary:In this paper, we present a novel approach that uses deep learning techniquesfor colorizing grayscale images. By utilizing a pre-trained convolutionalneural network, which is originally designed for image classification, we areable to separate content and style of different images and recombine them intoa single image. We then propose a method that can add colors to a grayscaleimage by combining its content with style of a color image having semanticsimilarity with the grayscale one. As an application, to our knowledge thefirst of its kind, we use the proposed method to colorize images of ukiyo-e agenre of Japanese painting?and obtain interesting results, showing thepotential of this method in the growing field of computer assisted art.
arxiv-17700-201 | Interactive Bayesian Hierarchical Clustering | http://arxiv.org/pdf/1602.03258v3.pdf | author:Sharad Vikram, Sanjoy Dasgupta category:cs.LG published:2016-02-10 summary:Clustering is a powerful tool in data analysis, but it is often difficult tofind a grouping that aligns with a user's needs. To address this, severalmethods incorporate constraints obtained from users into clustering algorithms,but unfortunately do not apply to hierarchical clustering. We design aninteractive Bayesian algorithm that incorporates user interaction intohierarchical clustering while still utilizing the geometry of the data bysampling a constrained posterior distribution over hierarchies. We also suggestseveral ways to intelligently query a user. The algorithm, along with thequerying schemes, shows promising results on real data.
arxiv-17700-202 | Invariant feature extraction from event based stimuli | http://arxiv.org/pdf/1604.04327v2.pdf | author:Thusitha N. Chandrapala, Bertram E. Shi category:cs.CV published:2016-04-15 summary:We propose a novel architecture, the event-based GASSOM for learning andextracting invariant representations from event streams originating fromneuromorphic vision sensors. The framework is inspired by feed-forward corticalmodels for visual processing. The model, which is based on the concepts ofsparsity and temporal slowness, is able to learn feature extractors thatresemble neurons in the primary visual cortex. Layers of units in the proposedmodel can be cascaded to learn feature extractors with different levels ofcomplexity and selectivity. We explore the applicability of the framework onreal world tasks by using the learned network for object recognition. Theproposed model achieve higher classification accuracy compared to otherstate-of-the-art event based processing methods. Our results also demonstratethe generality and robustness of the method, as the recognizers for differentdata sets and different tasks all used the same set of learned featuredetectors, which were trained on data collected independently of the testingdata.
arxiv-17700-203 | Detection of epileptic seizure in EEG signals using linear least squares preprocessing | http://arxiv.org/pdf/1604.08500v1.pdf | author:Z. Roshan Zamir category:cs.LG math.OC published:2016-04-27 summary:An epileptic seizure is a transient event of abnormal excessive neuronaldischarge in the brain. This unwanted event can be obstructed by detection ofelectrical changes in the brain that happen before the seizure takes place. Theautomatic detection of seizures is necessary since the visual screening of EEGrecordings is a time consuming task and requires experts to improve thediagnosis. Four linear least squares-based preprocessing models are proposed toextract key features of an EEG signal in order to detect seizures. The firsttwo models are newly developed. The original signal (EEG) is approximated by asinusoidal curve. Its amplitude is formed by a polynomial function and comparedwith the pre developed spline function.Different statistical measures namelyclassification accuracy, true positive and negative rates, false positive andnegative rates and precision are utilized to assess the performance of theproposed models. These metrics are derived from confusion matrices obtainedfrom classifiers. Different classifiers are used over the original dataset andthe set of extracted features. The proposed models significantly reduce thedimension of the classification problem and the computational time while theclassification accuracy is improved in most cases. The first and third modelsare promising feature extraction methods. Logistic, LazyIB1, LazyIB5 and J48are the best classifiers. Their true positive and negative rates are $1$ whilefalse positive and negative rates are zero and the corresponding precisionvalues are $1$. Numerical results suggest that these models are robust andefficient for detecting epileptic seizure.
arxiv-17700-204 | Instance-Level Segmentation for Autonomous Driving with Deep Densely Connected MRFs | http://arxiv.org/pdf/1512.06735v2.pdf | author:Ziyu Zhang, Sanja Fidler, Raquel Urtasun category:cs.CV published:2015-12-21 summary:Our aim is to provide a pixel-wise instance-level labeling of a monocularimage in the context of autonomous driving. We build on recent work [Zhang etal., ICCV15] that trained a convolutional neural net to predict instancelabeling in local image patches, extracted exhaustively in a stride from animage. A simple Markov random field model using several heuristics was thenproposed in [Zhang et al., ICCV15] to derive a globally consistent instancelabeling of the image. In this paper, we formulate the global labeling problemwith a novel densely connected Markov random field and show how to encodevarious intuitive potentials in a way that is amenable to efficient mean fieldinference [Kr\"ahenb\"uhl et al., NIPS11]. Our potentials encode thecompatibility between the global labeling and the patch-level predictions,contrast-sensitive smoothness as well as the fact that separate regions formdifferent instances. Our experiments on the challenging KITTI benchmark [Geigeret al., CVPR12] demonstrate that our method achieves a significant performanceboost over the baseline [Zhang et al., ICCV15].
arxiv-17700-205 | Inverting Visual Representations with Convolutional Networks | http://arxiv.org/pdf/1506.02753v4.pdf | author:Alexey Dosovitskiy, Thomas Brox category:cs.NE cs.CV cs.LG published:2015-06-09 summary:Feature representations, both hand-designed and learned ones, are often hardto analyze and interpret, even when they are extracted from visual data. Wepropose a new approach to study image representations by inverting them with anup-convolutional neural network. We apply the method to shallow representations(HOG, SIFT, LBP), as well as to deep networks. For shallow representations ourapproach provides significantly better reconstructions than existing methods,revealing that there is surprisingly rich information contained in thesefeatures. Inverting a deep network trained on ImageNet provides severalinsights into the properties of the feature representation learned by thenetwork. Most strikingly, the colors and the rough contours of an image can bereconstructed from activations in higher network layers and even from thepredicted class probabilities.
arxiv-17700-206 | Mixtures of Sparse Autoregressive Networks | http://arxiv.org/pdf/1511.04776v4.pdf | author:Marc Goessling, Yali Amit category:stat.ML cs.LG published:2015-11-15 summary:We consider high-dimensional distribution estimation through autoregressivenetworks. By combining the concepts of sparsity, mixtures and parameter sharingwe obtain a simple model which is fast to train and which achievesstate-of-the-art or better results on several standard benchmark datasets.Specifically, we use an L1-penalty to regularize the conditional distributionsand introduce a procedure for automatic parameter sharing between mixturecomponents. Moreover, we propose a simple distributed representation whichpermits exact likelihood evaluations since the latent variables are interleavedwith the observable variables and can be easily integrated out. Our modelachieves excellent generalization performance and scales well to extremely highdimensions.
arxiv-17700-207 | Evaluating the effect of topic consideration in identifying communities of rating-based social networks | http://arxiv.org/pdf/1604.07878v1.pdf | author:Ali Reihanian, Behrouz Minaei-Bidgoli, Muhammad Yousefnezhad category:cs.SI cs.LG stat.ML published:2016-04-26 summary:Finding meaningful communities in social network has attracted the attentionsof many researchers. The community structure of complex networks reveals boththeir organization and hidden relations among their constituents. Most of theresearches in the field of community detection mainly focus on the topologicalstructure of the network without performing any content analysis. Nowadays,real world social networks are containing a vast range of information includingshared objects, comments, following information, etc. In recent years, a numberof researches have proposed approaches which consider both the contents thatare interchanged in the networks and the topological structures of the networksin order to find more meaningful communities. In this research, the effect oftopic analysis in finding more meaningful communities in social networkingsites in which the users express their feelings toward different objects (likemovies) by the means of rating is demonstrated by performing extensiveexperiments.
arxiv-17700-208 | Are Face and Object Recognition Independent? A Neurocomputational Modeling Exploration | http://arxiv.org/pdf/1604.07872v1.pdf | author:Panqu Wang, Isabel Gauthier, Garrison Cottrell category:q-bio.NC cs.CV published:2016-04-26 summary:Are face and object recognition abilities independent? Although it iscommonly believed that they are, Gauthier et al.(2014) recently showed thatthese abilities become more correlated as experience with nonface categoriesincreases. They argued that there is a single underlying visual ability, v,that is expressed in performance with both face and nonface categories asexperience grows. Using the Cambridge Face Memory Test and the VanderbiltExpertise Test, they showed that the shared variance between Cambridge FaceMemory Test and Vanderbilt Expertise Test performance increases monotonicallyas experience increases. Here, we address why a shared resource acrossdifferent visual domains does not lead to competition and to an inversecorrelation in abilities? We explain this conundrum using ourneurocomputational model of face and object processing (The Model, TM). Ourresults show that, as in the behavioral data, the correlation betweensubordinate level face and object recognition accuracy increases as experiencegrows. We suggest that different domains do not compete for resources becausethe relevant features are shared between faces and objects. The essential powerof experience is to generate a "spreading transform" for faces that generalizesto objects that must be individuated. Interestingly, when the task of thenetwork is basic level categorization, no increase in the correlation betweendomains is observed. Hence, our model predicts that it is the type ofexperience that matters and that the source of the correlation is in thefusiform face area, rather than in cortical areas that subserve basic levelcategorization. This result is consistent with our previous modelingelucidating why the FFA is recruited for novel domains of expertise (Tong etal., 2008).
arxiv-17700-209 | Entities as topic labels: Improving topic interpretability and evaluability combining Entity Linking and Labeled LDA | http://arxiv.org/pdf/1604.07809v1.pdf | author:Federico Nanni, Pablo Ruiz Fabo category:cs.CL published:2016-04-26 summary:In order to create a corpus exploration method providing topics that areeasier to interpret than standard LDA topic models, here we propose combiningtwo techniques called Entity linking and Labeled LDA. Our method identifies inan ontology a series of descriptive labels for each document in a corpus. Thenit generates a specific topic for each label. Having a direct relation betweentopics and labels makes interpretation easier; using an ontology as backgroundknowledge limits label ambiguity. As our topics are described with a limitednumber of clear-cut labels, they promote interpretability, and this may helpquantitative evaluation. We illustrate the potential of the approach byapplying it in order to define the most relevant topics addressed by each partyin the European Parliament's fifth mandate (1999-2004).
arxiv-17700-210 | Using Indirect Encoding of Multiple Brains to Produce Multimodal Behavior | http://arxiv.org/pdf/1604.07806v1.pdf | author:Jacob Schrum, Joel Lehman, Sebastian Risi category:cs.AI cs.NE published:2016-04-26 summary:An important challenge in neuroevolution is to evolve complex neural networkswith multiple modes of behavior. Indirect encodings can potentially answer thischallenge. Yet in practice, indirect encodings do not yield effectivemultimodal controllers. Thus, this paper introduces novel multimodal extensionsto HyperNEAT, a popular indirect encoding. A previous multimodal HyperNEATapproach called situational policy geometry assumes that multiple brainsbenefit from being embedded within an explicit geometric space. However,experiments here illustrate that this assumption unnecessarily constrainsevolution, resulting in lower performance. Specifically, this paper introducesHyperNEAT extensions for evolving many brains without assuming geometricrelationships between them. The resulting Multi-Brain HyperNEAT can exploithuman-specified task divisions to decide when each brain controls the agent, orcan automatically discover when brains should be used, by means of preferenceneurons. A further extension called module mutation allows evolution todiscover the number of brains, enabling multimodal behavior with even lessexpert knowledge. Experiments in several multimodal domains highlight thatmulti-brain approaches are more effective than HyperNEAT without multimodalextensions, and show that brains without a geometric relation to each otheroutperform situational policy geometry. The conclusion is that Multi-BrainHyperNEAT provides several promising techniques for evolving complex multimodalbehavior.
arxiv-17700-211 | Scale Normalization | http://arxiv.org/pdf/1604.07796v1.pdf | author:Henry Z. Lo, Kevin Amaral, Wei Ding category:cs.NE cs.LG stat.ML published:2016-04-26 summary:One of the difficulties of training deep neural networks is caused byimproper scaling between layers. Scaling issues introduce exploding / gradientproblems, and have typically been addressed by careful scale-preservinginitialization. We investigate the value of preserving scale, or isometry,beyond the initial weights. We propose two methods of maintaing isometry, oneexact and one stochastic. Preliminary experiments show that for bothdeterminant and scale-normalization effectively speeds up learning. Resultssuggest that isometry is important in the beginning of learning, andmaintaining it leads to faster learning.
arxiv-17700-212 | A Framework for Human Pose Estimation in Videos | http://arxiv.org/pdf/1604.07788v1.pdf | author:Dong Zhang, Mubarak Shah category:cs.CV published:2016-04-26 summary:In this paper, we present a method to estimate a sequence of human poses inunconstrained videos. We aim to demonstrate that by using temporal information,the human pose estimation results can be improved over image based poseestimation methods. In contrast to the commonly employed graph optimizationformulation, which is NP-hard and needs approximate solutions, we formulatethis problem into a unified two stage tree-based optimization problem for whichan efficient and exact solution exists. Although the proposed method finds anexact solution, it does not sacrifice the ability to model the spatial andtemporal constraints between body parts in the frames; in fact it models the{\em symmetric} parts better than the existing methods. The proposed method isbased on two main ideas: `Abstraction' and `Association' to enforce the intra-and inter-frame body part constraints without inducing extra computationalcomplexity to the polynomial time solution. Using the idea of `Abstraction', anew concept of `abstract body part' is introduced to conceptually combine thesymmetric body parts and model them in the tree based body part structure.Using the idea of `Association', the optimal tracklets are generated for eachabstract body part, in order to enforce the spatiotemporal constraints betweenbody parts in adjacent frames. A sequence of the best poses is inferred fromthe abstract body part tracklets through the tree-based optimization. Finally,the poses are refined by limb alignment and refinement schemes. We evaluatedthe proposed method on three publicly available video based human poseestimation datasets, and obtained dramatically improved performance compared tothe state-of-the-art methods.
arxiv-17700-213 | F-measure Maximization in Multi-Label Classification with Conditionally Independent Label Subsets | http://arxiv.org/pdf/1604.07759v1.pdf | author:Maxime Gasse, Alex AUssem category:cs.LG published:2016-04-26 summary:We discuss a method to improve the exact F-measure maximization algorithmcalled GFM, proposed in (Dembczynski et al. 2011) for multi-labelclassification, assuming the label set can be can partitioned intoconditionally independent subsets given the input features. If the labels wereall independent, the estimation of only $m$ parameters ($m$ denoting the numberof labels) would suffice to derive Bayes-optimal predictions in $O(m^2)$operations. In the general case, $m^2+1$ parameters are required by GFM, tosolve the problem in $O(m^3)$ operations. In this work, we show that the numberof parameters can be reduced further to $m^2/n$, in the best case, assuming thelabel set can be partitioned into $n$ conditionally independent subsets. Asthis label partition needs to be estimated from the data beforehand, we usefirst the procedure proposed in (Gasse et al. 2015) that finds such partitionand then infer the required parameters locally in each label subset. The latterare aggregated and serve as input to GFM to form the Bayes-optimal prediction.We show on a synthetic experiment that the reduction in the number ofparameters brings about significant benefits in terms of performance.
arxiv-17700-214 | Compressive phase-only filtering - pattern recognition at extreme compression rates | http://arxiv.org/pdf/1604.07751v1.pdf | author:David Pastor-Calle, Anna Pastuszczak, Michal Mikolajczyk, Rafal Kotynski category:cs.CV physics.optics published:2016-04-26 summary:We introduce a compressive pattern recognition method for non-adaptiveWalsh-Hadamard or discrete noiselet-based compressive measurements and showthat images measured at extremely high compression rates may still containsufficient information for pattern recognition and target localization. Wereport on a compressive pattern recognition experiment with a single-pixeldetector with which we validate the proposed method. The correlation signalsproduced with the phase-only matched filter or with the pure-phase correlationare obtained from the compressive measurements through lasso optimizationwithout the need to reconstruct the original image. This is possible owing tothe two properties of phase-only filtering: such filtering is a unitarycirculant transform, and the correlation plane it produces in patternrecognition applications is usually sparse.
arxiv-17700-215 | EgoSampling: Wide View Hyperlapse from Single and Multiple Egocentric Videos | http://arxiv.org/pdf/1604.07741v1.pdf | author:Tavi Halperin, Yair Poleg, Chetan Arora, Shmuel Peleg category:cs.CV cs.MM published:2016-04-26 summary:The possibility of sharing one's point of view makes use of wearable camerascompelling. These videos are often long, boring and coupled with extreme shakeas the camera is worn on a moving person. Fast forwarding (i.e. frame sampling)is a natural choice for faster video browsing. However, this accentuates theshake caused by natural head motion in an egocentric video, making the fastforwarded video useless. We propose EgoSampling, an adaptive frame samplingthat gives more stable, fast forwarded, hyperlapse videos. Adaptive framesampling is formulated as energy minimization, whose optimal solution can befound in polynomial time. We further turn the camera shake from a drawback intoa feature, enabling the increase of the field-of-view. This is obtained wheneach output frame is mosaiced from several input frames. Stitching multipleframes also enables the generation of a single hyperlapse video from multipleegocentric videos, allowing even faster video consumption.
arxiv-17700-216 | Neural network-based clustering using pairwise constraints | http://arxiv.org/pdf/1511.06321v5.pdf | author:Yen-Chang Hsu, Zsolt Kira category:cs.LG stat.ML published:2015-11-19 summary:This paper presents a neural network-based end-to-end clustering framework.We design a novel strategy to utilize the contrastive criteria for pushingdata-forming clusters directly from raw data, in addition to learning a featureembedding suitable for such clustering. The network is trained with weaklabels, specifically partial pairwise relationships between data instances. Thecluster assignments and their probabilities are then obtained at the outputlayer by feed-forwarding the data. The framework has the interestingcharacteristic that no cluster centers need to be explicitly specified, thusthe resulting cluster distribution is purely data-driven and no distancemetrics need to be predefined. The experiments show that the proposed approachbeats the conventional two-stage method (feature embedding with k-means) by asignificant margin. It also compares favorably to the performance of thestandard cross entropy loss for classification. Robustness analysis also showsthat the method is largely insensitive to the number of clusters. Specifically,we show that the number of dominant clusters is close to the true number ofclusters even when a large k is used for clustering.
arxiv-17700-217 | Sketching for Sequential Change-Point Detection | http://arxiv.org/pdf/1505.06770v3.pdf | author:Yang Cao, Andrew Thompson, Meng Wang, Yao Xie category:cs.LG stat.ML published:2015-05-25 summary:We study sequential change-point detection using sketches (linearprojections) of high-dimensional signal vectors, by presenting the sketchingprocedures that are derived based on the generalized likelihood ratiostatistic. We consider both fixed and time-varying projections, and derivetheoretical approximations to two fundamental performance metrics: the averagerun length (ARL) and the expected detection delay (EDD); these approximationsare shown to be highly accurate by numerical simulations. We also characterizethe performance of the procedure when the projection is a Gaussian randomprojection or a sparse 0-1 matrix (in particular, an expander graph). Finally,we demonstrate the good performance of the sketching performance usingsimulation and real-data examples on solar flare detection and failuredetection in power networks.
arxiv-17700-218 | Condorcet's Jury Theorem for Consensus Clustering | http://arxiv.org/pdf/1604.07711v1.pdf | author:Brijnesh J. Jain category:stat.ML cs.LG published:2016-04-26 summary:The goal of consensus clustering is to improve the quality of clustering bycombining a sample of partitions of a dataset to a single consensus partition.This contribution extends Condorcet's Jury Theorem to the mean partitionapproach of consensus clustering. As a consequence of the proposed result, wechallenge and reappraise the role of diversity in consensus clustering.
arxiv-17700-219 | Distributed Clustering of Linear Bandits in Peer to Peer Networks | http://arxiv.org/pdf/1604.07706v1.pdf | author:Nathan Korda, Balazs Szorenyi, Shuai Li category:cs.LG cs.AI stat.ML published:2016-04-26 summary:We provide two distributed confidence ball algorithms for solving linearbandit problems in peer to peer networks with limited communicationcapabilities. For the first, we assume that all the peers are solving the samelinear bandit problem, and prove that our algorithm achieves the optimalasymptotic regret rate of any centralised algorithm that can instantlycommunicate information between the peers. For the second, we assume that thereare clusters of peers solving the same bandit problem within each cluster, andwe prove that our algorithm discovers these clusters, while achieving theoptimal asymptotic regret rate within each one. Through experiments on severalreal-world datasets, we demonstrate the performance of proposed algorithmscompared to the state-of-the-art.
arxiv-17700-220 | Tournament selection in zeroth-level classifier systems based on average reward reinforcement learning | http://arxiv.org/pdf/1604.07704v1.pdf | author:Zhaoxiang Zang, Zhao Li, Junying Wang, Zhiping Dan category:cs.AI cs.NE I.2 published:2016-04-26 summary:As a genetics-based machine learning technique, zeroth-level classifiersystem (ZCS) is based on a discounted reward reinforcement learning algorithm,bucket-brigade algorithm, which optimizes the discounted total reward receivedby an agent but is not suitable for all multi-step problems, especiallylarge-size ones. There are some undiscounted reinforcement learning methodsavailable, such as R-learning, which optimize the average reward per time step.In this paper, R-learning is used as the reinforcement learning employed byZCS, to replace its discounted reward reinforcement learning approach, andtournament selection is used to replace roulette wheel selection in ZCS. Themodification results in classifier systems that can support long action chains,and thus is able to solve large multi-step problems.
arxiv-17700-221 | Efficient Splitting-based Method for Global Image Smoothing | http://arxiv.org/pdf/1604.07681v1.pdf | author:Youngjung Kim, Dongbo Min, Bumsub Ham, Kwanghoon Sohn category:cs.CV published:2016-04-26 summary:Edge-preserving smoothing (EPS) can be formulated as minimizing an objectivefunction that consists of data and prior terms. This global EPS approach showsbetter smoothing performance than a local one that typically has a form ofweighted averaging, at the price of high computational cost. In this paper, weintroduce a highly efficient splitting-based method for global EPS thatminimizes the objective function of ${l_2}$ data and prior terms (possiblynon-smooth and non-convex) in linear time. Different from previoussplitting-based methods that require solving a large linear system, ourapproach solves an equivalent constrained optimization problem, resulting in asequence of 1D sub-problems. This enables linear time solvers forweighted-least squares and -total variation problems. Our solver convergesquickly, and its runtime is even comparable to state-of-the-art local EPSapproaches. We also propose a family of fast iteratively re-weighted algorithmsusing a non-convex prior term. Experimental results demonstrate theeffectiveness and flexibility of our approach in a range of computer vision andimage processing tasks.
arxiv-17700-222 | Real-time Action Recognition with Enhanced Motion Vector CNNs | http://arxiv.org/pdf/1604.07669v1.pdf | author:Bowen Zhang, Limin Wang, Zhe Wang, Yu Qiao, Hanli Wang category:cs.CV published:2016-04-26 summary:The deep two-stream architecture exhibited excellent performance on videobased action recognition. The most computationally expensive step in thisapproach comes from the calculation of optical flow which prevents it to bereal-time. This paper accelerates this architecture by replacing optical flowwith motion vector which can be obtained directly from compressed videoswithout extra calculation. However, motion vector lacks fine structures, andcontains noisy and inaccurate motion patterns, leading to the evidentdegradation of recognition performance. Our key insight for relieving thisproblem is that optical flow and motion vector are inherent correlated.Transferring the knowledge learned with optical flow CNN to motion vector CNNcan significantly boost the performance of the latter. Specifically, weintroduce three strategies for this, initialization transfer, supervisiontransfer and their combination. Experimental results show that our methodachieves comparable recognition performance to the state-of-the-art, while ourmethod can process 390.7 frames per second, which is 27 times faster than theoriginal two-stream method.
arxiv-17700-223 | $\ell_p$-Box ADMM: A Versatile Framework for Integer Programming | http://arxiv.org/pdf/1604.07666v1.pdf | author:Baoyuan Wu, Bernard Ghanem category:cs.CV cs.DS published:2016-04-26 summary:This paper revisits the integer programming (IP) problem, which plays afundamental role in many computer vision and machine learning applications. Theliterature abounds with many seminal works that address this problem, somefocusing on continuous approaches (e.g. linear program relaxation) while otherson discrete ones (e.g., min-cut). However, a limited number of them aredesigned to handle the general IP form and even these methods cannot adequatelysatisfy the simultaneous requirements of accuracy, feasibility, andscalability. To this end, we propose a novel and versatile framework called$\ell_p$-box ADMM, which is based on two parts. (1) The discrete constraint isequivalently replaced by the intersection of a box and a $(n-1)$-dimensionalsphere (defined through the $\ell_p$ norm). (2) We infuse this equivalence intothe ADMM (Alternating Direction Method of Multipliers) framework to handlethese continuous constraints separately and to harness its attractiveproperties. More importantly, the ADMM update steps can lead to manageablesub-problems in the continuous domain. To demonstrate its efficacy, we consideran instance of the framework, namely $\ell_2$-box ADMM applied to binaryquadratic programming (BQP). Here, the ADMM steps are simple, computationallyefficient, and theoretically guaranteed to converge to a KKT point. Wedemonstrate the applicability of $\ell_2$-box ADMM on three importantapplications: MRF energy minimization, graph matching, and clustering. Resultsclearly show that it significantly outperforms existing generic IP solvers bothin runtime and objective. It also achieves very competitive performance vs.state-of-the-art methods specific to these applications.
arxiv-17700-224 | TEMPO: Feature-Endowed Teichmüller Extremal Mappings of Point Clouds | http://arxiv.org/pdf/1511.06624v2.pdf | author:Ting Wei Meng, Gary Pui-Tung Choi, Lok Ming Lui category:cs.CG cs.CV cs.GR math.DG published:2015-11-20 summary:In recent decades, the use of 3D point clouds has been widespread in computerindustry. The development of techniques in analyzing point clouds isincreasingly important. In particular, mapping of point clouds has been achallenging problem. In this paper, we develop a discrete analogue of theTeichm\"{u}ller extremal mappings, which guarantee uniform conformalitydistortions, on point cloud surfaces. Based on the discrete analogue, wepropose a novel method called TEMPO for computing Teichm\"{u}ller extremalmappings between feature-endowed point clouds. Using our proposed method, theTeichm\"{u}ller metric is introduced for evaluating the dissimilarity of pointclouds. Consequently, our algorithm enables accurate recognition andclassification of point clouds. Experimental results demonstrate theeffectiveness of our proposed method.
arxiv-17700-225 | Online Influence Maximization in Non-Stationary Social Networks | http://arxiv.org/pdf/1604.07638v1.pdf | author:Yixin Bao, Xiaoke Wang, Zhi Wang, Chuan Wu, Francis C. M. Lau category:cs.SI cs.DS cs.LG published:2016-04-26 summary:Social networks have been popular platforms for information propagation. Animportant use case is viral marketing: given a promotion budget, an advertisercan choose some influential users as the seed set and provide them free ordiscounted sample products; in this way, the advertiser hopes to increase thepopularity of the product in the users' friend circles by the world-of-moutheffect, and thus maximizes the number of users that information of theproduction can reach. There has been a body of literature studying theinfluence maximization problem. Nevertheless, the existing studies mostlyinvestigate the problem on a one-off basis, assuming fixed known influenceprobabilities among users, or the knowledge of the exact social networktopology. In practice, the social network topology and the influenceprobabilities are typically unknown to the advertiser, which can be varyingover time, i.e., in cases of newly established, strengthened or weakened socialties. In this paper, we focus on a dynamic non-stationary social network anddesign a randomized algorithm, RSB, based on multi-armed bandit optimization,to maximize influence propagation over time. The algorithm produces a sequenceof online decisions and calibrates its explore-exploit strategy utilizingoutcomes of previous decisions. It is rigorously proven to achieve anupper-bounded regret in reward and applicable to large-scale social networks.Practical effectiveness of the algorithm is evaluated using both synthetic andreal-world datasets, which demonstrates that our algorithm outperforms previousstationary methods under non-stationary conditions.
arxiv-17700-226 | Solving Verbal Comprehension Questions in IQ Test by Knowledge-Powered Word Embedding | http://arxiv.org/pdf/1505.07909v4.pdf | author:Huazheng Wang, Fei Tian, Bin Gao, Jiang Bian, Tie-Yan Liu category:cs.CL cs.IR cs.LG published:2015-05-29 summary:Intelligence Quotient (IQ) Test is a set of standardized questions designedto evaluate human intelligence. Verbal comprehension questions appear veryfrequently in IQ tests, which measure human's verbal ability including theunderstanding of the words with multiple senses, the synonyms and antonyms, andthe analogies among words. In this work, we explore whether such tests can besolved automatically by artificial intelligence technologies, especially thedeep learning technologies that are recently developed and successfully appliedin a number of fields. However, we found that the task was quite challenging,and simply applying existing technologies (e.g., word embedding) could notachieve a good performance, mainly due to the multiple senses of words and thecomplex relations among words. To tackle these challenges, we propose a novelframework consisting of three components. First, we build a classifier torecognize the specific type of a verbal question (e.g., analogy,classification, synonym, or antonym). Second, we obtain distributedrepresentations of words and relations by leveraging a novel word embeddingmethod that considers the multi-sense nature of words and the relationalknowledge among words (or their senses) contained in dictionaries. Third, foreach type of questions, we propose a specific solver based on the obtaineddistributed word representations and relation representations. Experimentalresults have shown that the proposed framework can not only outperform existingmethods for solving verbal comprehension questions but also exceed the averageperformance of the Amazon Mechanical Turk workers involved in the study. Theresults indicate that with appropriate uses of the deep learning technologieswe might be a further step closer to the human intelligence.
arxiv-17700-227 | An Empirical Study into Annotator Agreement, Ground Truth Estimation, and Algorithm Evaluation | http://arxiv.org/pdf/1307.0426v3.pdf | author:Thomas A. Lampert, André Stumpf, Pierre Gançarski category:cs.CV cs.AI I.4.6; I.5.4 published:2013-07-01 summary:Although agreement between annotators has been studied in the past from astatistical viewpoint, little work has attempted to quantify the extent towhich this phenomenon affects the evaluation of computer vision (CV) objectdetection algorithms. Many researchers utilise ground truth (GT) in experimentsand more often than not this GT is derived from one annotator's opinion. Howdoes the difference in opinion affect an algorithm's evaluation? Four examplesof typical CV problems are chosen, and a methodology is applied to each toquantify the inter-annotator variance and to offer insight into the mechanismsbehind agreement and the use of GT. It is found that when detecting linearobjects annotator agreement is very low. The agreement in object position,linear or otherwise, can be partially explained through basic image properties.Automatic object detectors are compared to annotator agreement and it is foundthat a clear relationship exists. Several methods for calculating GTs from anumber of annotations are applied and the resulting differences in theperformance of the object detectors are quantified. It is found that the rankof a detector is highly dependent upon the method used to form the GT. It isalso found that although the STAPLE and LSML GT estimation methods appear torepresent the mean of the performance measured using the individualannotations, when there are few annotations, or there is a large variance inthem, these estimates tend to degrade. Furthermore, one of the most commonlyadopted annotation combination methods--consensus voting--accentuates moreobvious features, which results in an overestimation of the algorithm'sperformance. Finally, it is concluded that in some datasets it may not bepossible to state with any confidence that one algorithm outperforms anotherwhen evaluating upon one GT and a method for calculating confidence bounds isdiscussed.
arxiv-17700-228 | Spot On: Action Localization from Pointly-Supervised Proposals | http://arxiv.org/pdf/1604.07602v1.pdf | author:Pascal Mettes, Jan C. van Gemert, Cees G. M. Snoek category:cs.CV published:2016-04-26 summary:We strive for spatio-temporal localization of actions in videos. Thestate-of-the-art relies on action proposals at test time and selects the bestone with a classifier demanding carefully annotated box annotations at traintime. Annotating action boxes in video is cumbersome, tedious, and error prone.Rather than annotating boxes, we propose to annotate actions in video withpoints on a sparse subset of frames only. We introduce an overlap measurebetween action proposals and points and incorporate them all into the objectiveof a non-convex Multiple Instance Learning optimization. Experimentalevaluation on the UCF Sports and UCF 101 datasets shows that (i)spatio-temporal proposals can be used to train classifiers while retaining thelocalization performance, (ii) point annotations yield results comparable tobox annotations while being significantly faster to annotate, (iii) with aminimum amount of supervision our approach is competitive to thestate-of-the-art. Finally, we introduce spatio-temporal action annotations onthe train and test videos of Hollywood2, resulting in Hollywood2Tubes,available at tinyurl.com/hollywood2tubes.
arxiv-17700-229 | A New Approach in Persian Handwritten Letters Recognition Using Error Correcting Output Coding | http://arxiv.org/pdf/1604.07554v1.pdf | author:Maziar Kazemi, Muhammad Yousefnezhad, Saber Nourian category:cs.CV cs.LG stat.ML published:2016-04-26 summary:Classification Ensemble, which uses the weighed polling of outputs, is theart of combining a set of basic classifiers for generating high-performance,robust and more stable results. This study aims to improve the results ofidentifying the Persian handwritten letters using Error Correcting OutputCoding (ECOC) ensemble method. Furthermore, the feature selection is used toreduce the costs of errors in our proposed method. ECOC is a method fordecomposing a multi-way classification problem into many binary classificationtasks; and then combining the results of the subtasks into a hypothesizedsolution to the original problem. Firstly, the image features are extracted byPrincipal Components Analysis (PCA). After that, ECOC is used foridentification the Persian handwritten letters which it uses Support VectorMachine (SVM) as the base classifier. The empirical results of applying thisensemble method using 10 real-world data sets of Persian handwritten lettersindicate that this method has better results in identifying the Persianhandwritten letters than other ensemble methods and also singleclassifications. Moreover, by testing a number of different features, thispaper found that we can reduce the additional cost in feature selection stageby using this method.
arxiv-17700-230 | Network of Bandits | http://arxiv.org/pdf/1602.03779v6.pdf | author:Raphaël Féraud category:cs.AI cs.DC cs.LG published:2016-02-11 summary:The distribution of the best arm identification task on the user's devicesoffers several advantages for application purposes: scalability, reduction ofdeployment costs and privacy. We propose a distributed version of the algorithmSuccessive Elimination using a simple architecture based on a single serverwhich synchronizes each task executed on the user's devices. We show that thisalgorithm is near optimal both in terms of transmitted number of bits and interms of number of pulls per player. Finally, we propose an extension of thisapproach to distribute the contextual bandit algorithm Bandit Forest, which isable to finely exploit the user's data while guaranteeing the privacy.
arxiv-17700-231 | A novel mutation operator based on the union of fitness and design spaces information for Differential Evolution | http://arxiv.org/pdf/1510.02513v2.pdf | author:H. Sharifi Noghabi, H. Rajabi Mashhadi, K. Shojaei category:cs.NE published:2015-10-08 summary:Differential Evolution (DE) is one of the most successful and powerfulevolutionary algorithms for global optimization problem. The most importantoperator in this algorithm is mutation operator which parents are selectedrandomly to participate in it. Recently, numerous papers are tried to make thisoperator more intelligent by selection of parents for mutation intelligently.The intelligent selection for mutation vectors is performed by applying designspace (also known as decision space) criterion or fitness space criterion,however, in both cases, half of valuable information of the problem space isdisregarded. In this article, a Universal Differential Evolution (UDE) isproposed which takes advantage of both design and fitness spaces criteria forintelligent selection of mutation vectors. The experimental analysis on UDE areperformed on CEC2005 benchmarks and the results stated that UDE significantlyimproved the performance of differential evolution in comparison with othermethods that only use one criterion for intelligent selection.
arxiv-17700-232 | Towards Miss Universe Automatic Prediction: The Evening Gown Competition | http://arxiv.org/pdf/1604.07547v1.pdf | author:Johanna Carvajal, Arnold Wiliem, Conrad Sanderson, Brian Lovell category:cs.CV cs.CY cs.MM 68T45 published:2016-04-26 summary:Can we predict the winner of Miss Universe after watching how they strodedown the catwalk during the evening gown competition? Fashion gurus say theycan! In our work, we study this question from the perspective of computervision. In particular, we want to understand whether existing computer visionapproaches can be used to automatically extract the qualities exhibited by theMiss Universe winners during their catwalk. This study could pave the waytowards new vision-based applications for the fashion industry. To this end, wepropose a novel video dataset, called the Miss Universe dataset, comprising 10years of the evening gown competition selected between 1996-2010. We furtherpropose two ranking-related problems: (1) the Miss Universe Listwise Rankingand (2) the Miss Universe Pairwise Ranking problems. In addition, we alsodevelop an approach that simultaneously addresses the two proposed problems. Todescribe the videos we employ the recently proposed Stacked Fisher Vectors inconjunction with robust local spatio-temporal features. From our evaluation wefound that although the addressed problems are extremely challenging, theproposed system is able to rank the winner in the top 3 best predicted scoresfor 5 out of 10 Miss Universe competitions.
arxiv-17700-233 | The Mean Partition Theorem of Consensus Clustering | http://arxiv.org/pdf/1604.06626v2.pdf | author:Brijnesh J. Jain category:cs.LG cs.CV stat.ML published:2016-04-22 summary:To devise efficient solutions for approximating a mean partition in consensusclustering, Dimitriadou et al. [3] presented a necessary condition ofoptimality for a consensus function based on least square distances. We showthat their result is pivotal for deriving interesting properties of consensusclustering beyond optimization. For this, we present the necessary condition ofoptimality in a slightly stronger form in terms of the Mean Partition Theoremand extend it to the Expected Partition Theorem. To underpin its versatility,we show three examples that apply the Mean Partition Theorem: (i) equivalenceof the mean partition and optimal multiple alignment, (ii) construction ofprofiles and motifs, and (iii) relationship between consensus clustering andcluster stability.
arxiv-17700-234 | Learning Deep Feature Representations with Domain Guided Dropout for Person Re-identification | http://arxiv.org/pdf/1604.07528v1.pdf | author:Tong Xiao, Hongsheng Li, Wanli Ouyang, Xiaogang Wang category:cs.CV published:2016-04-26 summary:Learning generic and robust feature representations with data from multipledomains for the same problem is of great value, especially for the problemsthat have multiple datasets but none of them are large enough to provideabundant data variations. In this work, we present a pipeline for learning deepfeature representations from multiple domains with Convolutional NeuralNetworks (CNNs). When training a CNN with data from all the domains, someneurons learn representations shared across several domains, while some othersare effective only for a specific one. Based on this important observation, wepropose a Domain Guided Dropout algorithm to improve the feature learningprocedure. Experiments show the effectiveness of our pipeline and the proposedalgorithm. Our methods on the person re-identification problem outperformstate-of-the-art methods on multiple datasets by large margins.
arxiv-17700-235 | Regularizing RNNs by Stabilizing Activations | http://arxiv.org/pdf/1511.08400v7.pdf | author:David Krueger, Roland Memisevic category:cs.NE cs.CL cs.LG stat.ML published:2015-11-26 summary:We stabilize the activations of Recurrent Neural Networks (RNNs) bypenalizing the squared distance between successive hidden states' norms. This penalty term is an effective regularizer for RNNs including LSTMs andIRNNs, improving performance on character-level language modeling and phonemerecognition, and outperforming weight noise and dropout. We achieve competitive performance (18.6\% PER) on the TIMIT phonemerecognition task for RNNs evaluated without beam search or an RNN transducer. With this penalty term, IRNN can achieve similar performance to LSTM onlanguage modeling, although adding the penalty term to the LSTM results insuperior performance. Our penalty term also prevents the exponential growth of IRNN's activationsoutside of their training horizon, allowing them to generalize to much longersequences.
arxiv-17700-236 | Semantic Change Detection with Hypermaps | http://arxiv.org/pdf/1604.07513v1.pdf | author:Hirokatsu Kataoka, Soma Shirakabe, Yudai Miyashita, Akio Nakamura, Kenji Iwata, Yutaka Satoh category:cs.CV cs.AI published:2016-04-26 summary:Change detection is the study of detecting changes between two differentimages of a scene taken at different times. This paper proposes the concept ofsemantic change detection, which involves intuitively inserting semanticmeaning into detected change areas. The problem to be solved consists of twoparts, semantic segmentation and change detection. In order to solve thisproblem and obtain a high-level of performance, we propose an improvement tothe hypercolumns representation, hereafter known as hypermaps, whicheffectively uses convolutional maps obtained from convolutional neural networks(CNNs). We also employ multi-scale feature representation captured by differentimage patches. We applied our method to the TSUNAMI Panoramic Change Detectiondataset, and re-annotated the changed areas of the dataset via semanticclasses. The results show that our multi-scale hypermaps provided outstandingperformance on the re-annotated TSUNAMI dataset.
arxiv-17700-237 | DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation | http://arxiv.org/pdf/1511.06645v2.pdf | author:Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, Peter Gehler, Bernt Schiele category:cs.CV published:2015-11-20 summary:This paper considers the task of articulated human pose estimation ofmultiple people in real world images. We propose an approach that jointlysolves the tasks of detection and pose estimation: it infers the number ofpersons in a scene, identifies occluded body parts, and disambiguates bodyparts between people in close proximity of each other. This joint formulationis in contrast to previous strategies, that address the problem by firstdetecting people and subsequently estimating their body pose. We propose apartitioning and labeling formulation of a set of body-part hypothesesgenerated with CNN-based part detectors. Our formulation, an instance of aninteger linear program, implicitly performs non-maximum suppression on the setof part candidates and groups them to form configurations of body partsrespecting geometric and appearance constraints. Experiments on four differentdatasets demonstrate state-of-the-art results for both single person and multiperson pose estimation. Models and code available athttp://pose.mpi-inf.mpg.de.
arxiv-17700-238 | Once for All: a Two-flow Convolutional Neural Network for Visual Tracking | http://arxiv.org/pdf/1604.07507v1.pdf | author:Kai Chen, Wenbing Tao category:cs.CV published:2016-04-26 summary:One of the main challenges of visual object tracking comes from the arbitraryappearance of objects. Most existing algorithms try to resolve this problem asan object-specific task, i.e., the model is trained to regenerate or classify aspecific object. As a result, the model need to be initialized and retrainedfor different objects. In this paper, we propose a more generic approachutilizing a novel two-flow convolutional neural network (named YCNN). The YCNNtakes two inputs (one is object image patch, the other is search image patch),then outputs a response map which predicts how likely the object appears in aspecific location. Unlike those object-specific approach, the YCNN is trainedto measure the similarity between two image patches. Thus it will not beconfined to any specific object. Furthermore the network can be end-to-endtrained to extract both shallow and deep convolutional features which arededicated for visual tracking. And once properly trained, the YCNN can beapplied to track all kinds of objects without further training and updating.Benefiting from the once-for-all model, our algorithm is able to run at a veryhigh speed of 45 frames-per-second. The experiments on 51 sequences also showthat our algorithm achieves an outstanding performance.
arxiv-17700-239 | Modern Physiognomy: An Investigation on Predicting Personality Traits and Intelligence from the Human Face | http://arxiv.org/pdf/1604.07499v1.pdf | author:Rizhen Qin, Wei Gao, Huarong Xu, Zhanyi Hu category:cs.CV published:2016-04-26 summary:The human behavior of evaluating other individuals with respect to theirpersonality traits and intelligence by evaluating their faces plays a crucialrole in human relations. These trait judgments might influence important socialoutcomes in our lives such as elections and court sentences. Previous studieshave reported that human can make valid inferences for at least fourpersonality traits. In addition, some studies have demonstrated that facialtrait evaluation can be learned using machine learning methods accurately. Inthis work, we experimentally explore whether self-reported personality traitsand intelligence can be predicted reliably from a facial image. Morespecifically, the prediction problem is separately cast in two parts: aclassification task and a regression task. A facial structural feature isconstructed from the relations among facial salient points, and an appearancefeature is built by five texture descriptors. In addition, a minutia-basedfingerprint feature from a fingerprint image is also explored. Theclassification results show that the personality traits "Rule-consciousness"and "Vigilance" can be predicted reliably, and that the traits of females canbe predicted more accurately than those of male. However, the regressionexperiments show that it is difficult to predict scores for individualpersonality traits and intelligence. The residual plots and the correlationresults indicate no evident linear correlation between the measured scores andthe predicted scores. Both the classification and the regression results revealthat "Rule-consciousness" and "Tension" can be reliably predicted from thefacial features, while "Social boldness" gets the worst prediction results. Theexperiments results show that it is difficult to predict intelligence fromeither the facial features or the fingerprint feature, a finding that is inagreement with previous studies.
arxiv-17700-240 | Deep Multi-fidelity Gaussian Processes | http://arxiv.org/pdf/1604.07484v1.pdf | author:Maziar Raissi, George Karniadakis category:cs.LG stat.ML published:2016-04-26 summary:We develop a novel multi-fidelity framework that goes far beyond theclassical AR(1) Co-kriging scheme of Kennedy and O'Hagan (2000). Our method canhandle general discontinuous cross-correlations among systems with differentlevels of fidelity. A combination of multi-fidelity Gaussian Processes (AR(1)Co-kriging) and deep neural networks enables us to construct a method that isimmune to discontinuities. We demonstrate the effectiveness of the newtechnology using standard benchmark problems designed to resemble the outputsof complicated high- and low-fidelity codes.
arxiv-17700-241 | Joint Semantic Segmentation and Depth Estimation with Deep Convolutional Networks | http://arxiv.org/pdf/1604.07480v1.pdf | author:Arsalan Mousavian, Hamed Pirsiavash, Jana Kosecka category:cs.CV published:2016-04-25 summary:Multi-scale deep CNNs have been used successfully for problems mapping eachpixel to a label, such as depth estimation and semantic segmentation. It hasalso been shown that such architectures are reusable and can be used formultiple tasks. These networks are typically trained independently for eachtask by varying the output layer(s) and training objective. In this work wepresent a new model for simultaneous depth estimation and semantic segmentationfrom a single RGB image. Our approach demonstrates the feasibility of trainingparts of the model for each task and then fine tuning the full, combined modelon both tasks simultaneously using a single loss function. Furthermore wecouple the deep CNN with fully connected CRF, which captures the contextualrelationships and interactions between the semantic and depth cues improvingthe accuracy of the final results. The proposed model is trained and evaluatedon NYUDepth V2 dataset outperforming the state of the art methods on semanticsegmentation and achieving comparable results on the task of depth estimation.
arxiv-17700-242 | Long-Term Identity-Aware Multi-Person Tracking for Surveillance Video Summarization | http://arxiv.org/pdf/1604.07468v1.pdf | author:Shoou-I Yu, Yi Yang, Xuanchong Li, Alexander G. Hauptmann category:cs.CV published:2016-04-25 summary:In multi-person tracking scenarios, gaining access to the identity of eachtracked individual is crucial for many applications such as long-termsurveillance video analysis. Therefore, we propose a long-term multi-persontracker which utilizes face recognition information to not only enhancetracking performance, but also assign identities to tracked people. As facerecognition information is not available in many frames, the proposed trackerutilizes manifold learning techniques to propagate identity information toframes without face recognition information. Our tracker is formulated as aconstrained quadratic optimization problem, which is solved with nonnegativematrix optimization techniques. Tracking experiments performed on challengingdata sets, including a 116.25 hour complex indoor tracking data set, showedthat our method is effective in tracking each individual. We further exploredthe utility of long-term identity-aware multi-person tracking output byperforming video summarization experiments based on our tracking output.Results showed that the computed trajectories were sufficient to generate areasonable visual diary (i.e. a summary of what a person did) for differentpeople, thus potentially opening the door to summarization of hundreds or eventhousands of hours of surveillance video.
arxiv-17700-243 | Sparse PCA via Covariance Thresholding | http://arxiv.org/pdf/1311.5179v5.pdf | author:Yash Deshpande, Andrea Montanari category:math.ST stat.ML stat.TH published:2013-11-20 summary:In sparse principal component analysis we are given noisy observations of alow-rank matrix of dimension $n\times p$ and seek to reconstruct it underadditional sparsity assumptions. In particular, we assume here each of theprincipal components $\mathbf{v}_1,\dots,\mathbf{v}_r$ has at most $s_0$non-zero entries. We are particularly interested in the high dimensional regimewherein $p$ is comparable to, or even much larger than $n$. In an influentialpaper, \cite{johnstone2004sparse} introduced a simple algorithm that estimatesthe support of the principal vectors $\mathbf{v}_1,\dots,\mathbf{v}_r$ by thelargest entries in the diagonal of the empirical covariance. This method can beshown to identify the correct support with high probability if $s_0\leK_1\sqrt{n/\log p}$, and to fail with high probability if $s_0\ge K_2\sqrt{n/\log p}$ for two constants $0<K_1,K_2<\infty$. Despite a considerableamount of work over the last ten years, no practical algorithm exists withprovably better support recovery guarantees. Here we analyze a covariance thresholding algorithm that was recentlyproposed by \cite{KrauthgamerSPCA}. On the basis of numerical simulations (forthe rank-one case), these authors conjectured that covariance thresholdingcorrectly recover the support with high probability for $s_0\le K\sqrt{n}$(assuming $n$ of the same order as $p$). We prove this conjecture, and in factestablish a more general guarantee including higher-rank as well as $n$ muchsmaller than $p$. Recent lower bounds \cite{berthet2013computational,ma2015sum} suggest that no polynomial time algorithm can do significantlybetter. The key technical component of our analysis develops new bounds on thenorm of kernel random matrices, in regimes that were not considered before.
arxiv-17700-244 | Nonparametric Bayesian Negative Binomial Factor Analysis | http://arxiv.org/pdf/1604.07464v1.pdf | author:Mingyuan Zhou category:stat.ME stat.ML published:2016-04-25 summary:A common approach to analyze an attribute-instance count matrix, an elementof which represents how many times an attribute appears in an instance, is tofactorize it under the Poisson likelihood. We show its limitation in capturingthe tendency for an attribute present in an instance to both repeat itself andexcite related ones. To address this limitation, we construct negative binomialfactor analysis (NBFA) to factorize the matrix under the negative binomiallikelihood, and relate it to a Dirichlet-multinomial distribution basedmixed-membership model. To support countably infinite factors, we propose thehierarchical gamma-negative binomial process. By exploiting newly provedconnections between discrete distributions, we construct two blocked and acollapsed Gibbs sampler that all adaptively truncate their number of factors,and demonstrate that the blocked Gibbs sampler developed under a compoundPoisson representation converges fast and has low computational complexity.Example results show that NBFA has a distinct mechanism in adjusting its numberof inferred factors according to the instance lengths, and provides clearadvantages in parsimonious representation, predictive power, and computationalcomplexity over previously proposed discrete latent variable models, whicheither completely ignore burstiness, or model only the burstiness of theattributes but not that of the factors.
arxiv-17700-245 | Dynamic Pricing with Demand Covariates | http://arxiv.org/pdf/1604.07463v1.pdf | author:Sheng Qiang, Mohsen Bayati category:stat.ML published:2016-04-25 summary:We consider a firm that sells products over $T$ periods without knowing thedemand function. The firm sequentially sets prices to earn revenue and to learnthe underlying demand function simultaneously. A natural heuristic for thisproblem, commonly used in practice, is greedy iterative least squares (GILS).At each time period, GILS estimates the demand as a linear function of theprice by applying least squares to the set of prior prices and realizeddemands. Then a price that maximizes the revenue, given the estimated demandfunction, is used for the next time period. The performance is measured by theregret, which is the expected revenue loss from the optimal (oracle) pricingpolicy when the demand function is known. Recently, den Boer and Zwart (2014)and Keskin and Zeevi (2014) demonstrated that GILS is sub-optimal. Theyintroduced algorithms which integrate forced price dispersion with GILS andachieve asymptotically optimal performance. In this paper, we consider this dynamic pricing problem in a data-richenvironment. In particular, we assume that the firm knows the expected demandunder a particular price from historical data, and in each period, beforesetting the price, the firm has access to extra information (demand covariates)which may be predictive of the demand. We prove that in this setting GILSachieves asymptotically optimal regret of order $\log(T)$. We also show thefollowing surprising result: in the original dynamic pricing problem of denBoer and Zwart (2014) and Keskin and Zeevi (2014), inclusion of any set ofcovariates in GILS as potential demand covariates (even though they could carryno information) would make GILS asymptotically optimal. We validate our resultsvia extensive numerical simulations on synthetic and real data sets.
arxiv-17700-246 | Modeling the Contribution of Central Versus Peripheral Vision in Scene, Object, and Face Recognition | http://arxiv.org/pdf/1604.07457v1.pdf | author:Panqu Wang, Garrison Cottrell category:q-bio.NC cs.CV published:2016-04-25 summary:It is commonly believed that the central visual field is important forrecognizing objects and faces, and the peripheral region is useful for scenerecognition. However, the relative importance of central versus peripheralinformation for object, scene, and face recognition is unclear. In a behavioralstudy, Larson and Loschky (2009) investigated this question by measuring thescene recognition accuracy as a function of visual angle, and demonstrated thatperipheral vision was indeed more useful in recognizing scenes than centralvision. In this work, we modeled and replicated the result of Larson andLoschky (2009), using deep convolutional neural networks. Having fit the datafor scenes, we used the model to predict future data for large-scale scenerecognition as well as for objects and faces. Our results suggest that therelative order of importance of using central visual field information is facerecognition>object recognition>scene recognition, and vice-versa for peripheralinformation.
arxiv-17700-247 | Learning Local Dependence In Ordered Data | http://arxiv.org/pdf/1604.07451v1.pdf | author:Guo Yu, Jacob Bien category:math.ST stat.CO stat.ME stat.ML stat.TH published:2016-04-25 summary:In many applications, data comes with a natural ordering. This ordering canoften induce local dependence among nearby variables. However, in complex data,the width of this dependence may vary, making simple assumptions such as aconstant neighborhood size unrealistic. We propose a framework for learningthis local dependence based on estimating the inverse of the Cholesky factor ofthe covariance matrix. Penalized maximum likelihood estimation of this matrixyields a simple regression interpretation for local dependence in whichvariables are predicted by their neighbors. Our proposed method involvessolving a convex, penalized Gaussian likelihood problem with a hierarchicalgroup lasso penalty. The problem decomposes into independent subproblems whichcan be solved efficiently in parallel using first-order methods. Our methodyields a sparse, symmetric, positive definite estimator of the precisionmatrix, encoding a Gaussian graphical model. We derive theoretical results notfound in existing methods attaining this structure. In particular, ourconditions for signed support recovery and estimation consistency rates inmultiple norms are as mild as those in a regression problem. Empirical resultsshow our method performing favorably compared to existing methods. We apply ourmethod to genomic data to flexibly model linkage disequilibrium.
arxiv-17700-248 | Universum Prescription: Regularization using Unlabeled Data | http://arxiv.org/pdf/1511.03719v6.pdf | author:Xiang Zhang, Yann LeCun category:cs.LG published:2015-11-11 summary:This paper shows that simply prescribing "none of the above" labels tounlabeled data has a beneficial regularization effect to supervised learning.We call it universum prescription by the fact that the prescribed labels cannotbe one of the supervised labels. In spite of its simplicity, universumprescription obtained competitive results in training deep convolutionalnetworks for CIFAR-10, CIFAR-100, STL-10 and ImageNet datasets. A qualitativejustification of these approaches using Rademacher complexity is presented. Theeffect of a regularization parameter -- probability of sampling from unlabeleddata -- is also studied empirically.
arxiv-17700-249 | G-CNN: an Iterative Grid Based Object Detector | http://arxiv.org/pdf/1512.07729v2.pdf | author:Mahyar Najibi, Mohammad Rastegari, Larry S. Davis category:cs.CV published:2015-12-24 summary:We introduce G-CNN, an object detection technique based on CNNs which workswithout proposal algorithms. G-CNN starts with a multi-scale grid of fixedbounding boxes. We train a regressor to move and scale elements of the gridtowards objects iteratively. G-CNN models the problem of object detection asfinding a path from a fixed grid to boxes tightly surrounding the objects.G-CNN with around 180 boxes in a multi-scale grid performs comparably to FastR-CNN which uses around 2K bounding boxes generated with a proposal technique.This strategy makes detection faster by removing the object proposal stage aswell as reducing the number of boxes to be processed.
arxiv-17700-250 | Balancing Appearance and Context in Sketch Interpretation | http://arxiv.org/pdf/1604.07429v1.pdf | author:Yale Song, Randall Davis, Kaichen Ma, Dana L. Penny category:cs.AI cs.CV published:2016-04-25 summary:We describe a sketch interpretation system that detects and classifies clocknumerals created by subjects taking the Clock Drawing Test, a clinical toolwidely used to screen for cognitive impairments (e.g., dementia). We describehow it balances appearance and context, and document its performance on some2,000 drawings (about 24K clock numerals) produced by a wide spectrum ofpatients. We calibrate the utility of different forms of context, describingexperiments with Conditional Random Fields trained and tested using a varietyof features. We identify context that contributes to interpreting otherwiseambiguous or incomprehensible strokes. We describe ST-slices, a novelrepresentation that enables "unpeeling" the layers of ink that result whenpeople overwrite, which often produces ink impossible to analyze if only thefinal drawing is examined. We characterize when ST-slices work, calibrate theirimpact on performance, and consider their breadth of applicability.
arxiv-17700-251 | Conversational Markers of Constructive Discussions | http://arxiv.org/pdf/1604.07407v1.pdf | author:Vlad Niculae, Cristian Danescu-Niculescu-Mizil category:cs.CL cs.AI cs.SI physics.soc-ph stat.ML published:2016-04-25 summary:Group discussions are essential for organizing every aspect of modern life,from faculty meetings to senate debates, from grant review panels to papalconclaves. While costly in terms of time and organization effort, groupdiscussions are commonly seen as a way of reaching better decisions compared tosolutions that do not require coordination between the individuals (e.g.voting)---through discussion, the sum becomes greater than the parts. However,this assumption is not irrefutable: anecdotal evidence of wasteful discussionsabounds, and in our own experiments we find that over 30% of discussions areunproductive. We propose a framework for analyzing conversational dynamics in order todetermine whether a given task-oriented discussion is worth having or not. Weexploit conversational patterns reflecting the flow of ideas and the balancebetween the participants, as well as their linguistic choices. We apply thisframework to conversations naturally occurring in an online collaborative worldexploration game developed and deployed to support this research. Using thissetting, we show that linguistic cues and conversational patterns extractedfrom the first 20 seconds of a team discussion are predictive of whether itwill be a wasteful or a productive one.
arxiv-17700-252 | Context Encoders: Feature Learning by Inpainting | http://arxiv.org/pdf/1604.07379v1.pdf | author:Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei A. Efros category:cs.CV cs.AI cs.GR cs.LG published:2016-04-25 summary:We present an unsupervised visual feature learning algorithm driven bycontext-based pixel prediction. By analogy with auto-encoders, we proposeContext Encoders -- a convolutional neural network trained to generate thecontents of an arbitrary image region conditioned on its surroundings. In orderto succeed at this task, context encoders need to both understand the contentof the entire image, as well as produce a plausible hypothesis for the missingpart(s). When training context encoders, we have experimented with both astandard pixel-wise reconstruction loss, as well as a reconstruction plus anadversarial loss. The latter produces much sharper results because it canbetter handle multiple modes in the output. We found that a context encoderlearns a representation that captures not just appearance but also thesemantics of visual structures. We quantitatively demonstrate the effectivenessof our learned features for CNN pre-training on classification, detection, andsegmentation tasks. Furthermore, context encoders can be used for semanticinpainting tasks, either stand-alone or as initialization for non-parametricmethods.
arxiv-17700-253 | Total variation on a tree | http://arxiv.org/pdf/1502.07770v3.pdf | author:Vladimir Kolmogorov, Thomas Pock, Michal Rolinek category:cs.CV published:2015-02-26 summary:We consider the problem of minimizing the continuous valued total variationsubject to different unary terms on trees and propose fast direct algorithmsbased on dynamic programming to solve these problems. We treat both the convexand the non-convex case and derive worst case complexities that are equal orbetter than existing methods. We show applications to total variation based 2Dimage processing and computer vision problems based on a Lagrangiandecomposition approach. The resulting algorithms are very efficient, offer ahigh degree of parallelism and come along with memory requirements which areonly in the order of the number of image pixels.
arxiv-17700-254 | Parsing Argumentation Structures in Persuasive Essays | http://arxiv.org/pdf/1604.07370v1.pdf | author:Christian Stab, Iryna Gurevych category:cs.CL published:2016-04-25 summary:In this article, we present the first end-to-end approach for parsingargumentation structures in persuasive essays. We model the argumentationstructure as a tree including several types of argument components connectedwith argumentative support and attack relations. We consider the identificationof argumentation structures in several consecutive steps. First, we segment apersuasive essay in order to identify relevant argument components. Second, wejointly model the classification of argument components and the identificationof argumentative relations using Integer Linear Programming. Third, werecognize the stance of each argument component in order to discriminatebetween argumentative support and attack relations. By evaluating the jointmodel using two corpora, we show that our approach not only considerablyimproves the identification of argument component types and argumentativerelations but also significantly outperforms a challenging heuristic baseline.In addition, we introduce a novel corpus including 402 persuasive essaysannotated with argumentation structures and show that our new annotationguideline successfully guides annotators to substantial agreement.
arxiv-17700-255 | Attributes for Improved Attributes: A Multi-Task Network for Attribute Classification | http://arxiv.org/pdf/1604.07360v1.pdf | author:Emily M. Hand, Rama Chellappa category:cs.CV published:2016-04-25 summary:Attributes, or semantic features, have gained popularity in the past fewyears in domains ranging from activity recognition in video to faceverification. Improving the accuracy of attribute classifiers is an importantfirst step in any application which uses these attributes. In most works todate, attributes have been considered to be independent. However, we know thisnot to be the case. Many attributes are very strongly related, such as heavymakeup and wearing lipstick. We propose to take advantage of attributerelationships in three ways: by using a multi-task deep convolutional neuralnetwork (MCNN) sharing the lowest layers amongst all attributes, sharing thehigher layers for related attributes, and by building an auxiliary network ontop of the MCNN which utilizes the scores from all attributes to improve thefinal classification of each attribute. We demonstrate the effectiveness of ourmethod by producing results on two challenging publicly available datasets.
arxiv-17700-256 | Fast nonlinear embeddings via structured matrices | http://arxiv.org/pdf/1604.07356v1.pdf | author:Krzysztof Choromanski, Francois Fagan category:stat.ML cs.LG G.3 published:2016-04-25 summary:We present a new paradigm for speeding up randomized computations of severalfrequently used functions in machine learning. In particular, our paradigm canbe applied for improving computations of kernels based on random embeddings.Above that, the presented framework covers multivariate randomized functions.As a byproduct, we propose an algorithmic approach that also leads to asignificant reduction of space complexity. Our method is based on carefulrecycling of Gaussian vectors into structured matrices that share properties offully random matrices. The quality of the proposed structured approach followsfrom combinatorial properties of the graphs encoding correlations between rowsof these structured matrices. Our framework covers as special cases alreadyknown structured approaches such as the Fast Johnson-Lindenstrauss Transform,but is much more general since it can be applied also to highly nonlinearembeddings. We provide strong concentration results showing the quality of thepresented paradigm.
arxiv-17700-257 | Global Convergence of a Grassmannian Gradient Descent Algorithm for Subspace Estimation | http://arxiv.org/pdf/1506.07405v2.pdf | author:Dejiao Zhang, Laura Balzano category:cs.NA math.NA stat.ML 90C52, 65Y20 G.1.6; F.2.1 published:2015-06-24 summary:It has been observed in a variety of contexts that gradient descent methodshave great success in solving low-rank matrix factorization problems, despitethe relevant problem formulation being non-convex. We tackle a particularinstance of this scenario, where we seek the $d$-dimensional subspace spannedby a streaming data matrix. We apply the natural first order incrementalgradient descent method, constraining the gradient method to the Grassmannian.In this paper, we propose an adaptive step size scheme that is greedy for thenoiseless case, that maximizes the improvement of our metric of convergence ateach data index $t$, and yields an expected improvement for the noisy case. Weshow that, with noise-free data, this method converges from any randominitialization to the global minimum of the problem. For noisy data, we providethe expected convergence rate of the proposed algorithm per iteration.
arxiv-17700-258 | Incremental Hashing with Kernels | http://arxiv.org/pdf/1604.07342v1.pdf | author:Bahadir Ozdemir, Mahyar Najibi, Larry S. Davis category:cs.CV published:2016-04-25 summary:We propose an incremental strategy for learning hash functions with kernelsfor large-scale image search. Our method is based on a two-stage classificationframework that treats binary codes as intermediate variables between thefeature space and the semantic space. In the first stage of classification,binary codes are considered as class labels by a set of binary SVMs; eachcorresponds to one bit. In the second stage, binary codes become the inputspace of a multi-class SVM. Hash functions are learned by an efficientalgorithm where the NP-hard problem of finding optimal binary codes is solvedvia cyclic coordinate descent and SVMs are trained in a parallelizedincremental manner. For modifications like adding images from an unseen class,we describe an incremental procedure for effective and efficient updates to theprevious hash functions. Experiments on three large-scale image datasetsdemonstrate the effectiveness of the proposed hashing method, SVM-based Hashing(SVM-Hash), over the state-of-the-art supervised hashing methods.
arxiv-17700-259 | Scalable Gaussian Processes for Supervised Hashing | http://arxiv.org/pdf/1604.07335v1.pdf | author:Bahadir Ozdemir, Larry S. Davis category:cs.CV published:2016-04-25 summary:We propose a flexible procedure for large-scale image search by hashfunctions with kernels. Our method treats binary codes and pairwise semanticsimilarity as latent and observed variables, respectively, in a probabilisticmodel based on Gaussian processes for binary classification. We present anefficient inference algorithm with the sparse pseudo-input Gaussian process(SPGP) model and parallelization. Experiments on three large-scale imagedataset demonstrate the effectiveness of the proposed hashing method, GaussianProcess Hashing (GPH), for short binary codes and the datasets withoutpredefined classes in comparison to the state-of-the-art supervised hashingmethods.
arxiv-17700-260 | A Convolutional Neural Network Neutrino Event Classifier | http://arxiv.org/pdf/1604.01444v2.pdf | author:A. Aurisano, A. Radovic, D. Rocco, A. Himmel, M. D. Messier, E. Niner, G. Pawloski, F. Psihas, A. Sousa, P. Vahle category:hep-ex cs.CV published:2016-04-05 summary:Convolutional neural networks (CNNs) have been widely applied in the computervision community to solve complex problems in image recognition and analysis.We describe an application of the CNN technology to the problem of identifyingparticle interactions in sampling calorimeters used commonly in high energyphysics and high energy neutrino physics in particular. Following a discussionof the core concepts of CNNs and recent innovations in CNN architecturesrelated to the field of deep learning, we outline a specific application to theNOvA neutrino detector. This algorithm, CVN (Convolutional Visual Network)identifies neutrino interactions based on their topology without the need fordetailed reconstruction and outperforms algorithms currently in use by the NOvAcollaboration.
arxiv-17700-261 | Semi-supervised Dictionary Learning Based on Hilbert-Schmidt Independence Criterion | http://arxiv.org/pdf/1604.07319v1.pdf | author:Mehrdad J. Gangeh, Safaa M. A. Bedawi, Ali Ghodsi, Fakhri Karray category:cs.CV published:2016-04-25 summary:In this paper, a novel semi-supervised dictionary learning and sparserepresentation (SS-DLSR) is proposed. The proposed method benefits from thesupervisory information by learning the dictionary in a space where thedependency between the data and class labels is maximized. This maximization isperformed using Hilbert-Schmidt independence criterion (HSIC). On the otherhand, the global distribution of the underlying manifolds were learned from theunlabeled data by minimizing the distances between the unlabeled data and thecorresponding nearest labeled data in the space of the dictionary learned. Theproposed SS-DLSR algorithm has closed-form solutions for both the dictionaryand sparse coefficients, and therefore does not have to learn the twoiteratively and alternately as is common in the literature of the DLSR. Thismakes the solution for the proposed algorithm very fast. The experimentsconfirm the improvement in classification performance on benchmark datasets byincluding the information from both labeled and unlabeled data, particularlywhen there are many unlabeled data.
arxiv-17700-262 | End to End Learning for Self-Driving Cars | http://arxiv.org/pdf/1604.07316v1.pdf | author:Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao, Karol Zieba category:cs.CV cs.LG cs.NE published:2016-04-25 summary:We trained a convolutional neural network (CNN) to map raw pixels from asingle front-facing camera directly to steering commands. This end-to-endapproach proved surprisingly powerful. With minimum training data from humansthe system learns to drive in traffic on local roads with or without lanemarkings and on highways. It also operates in areas with unclear visualguidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessaryprocessing steps such as detecting useful road features with only the humansteering angle as the training signal. We never explicitly trained it todetect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane markingdetection, path planning, and control, our end-to-end system optimizes allprocessing steps simultaneously. We argue that this will eventually lead tobetter performance and smaller systems. Better performance will result becausethe internal components self-optimize to maximize overall system performance,instead of optimizing human-selected intermediate criteria, e.g., lanedetection. Such criteria understandably are selected for ease of humaninterpretation which doesn't automatically guarantee maximum systemperformance. Smaller networks are possible because the system learns to solvethe problem with the minimal number of processing steps. We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PXself-driving car computer also running Torch 7 for determining where to drive.The system operates at 30 frames per second (FPS).
arxiv-17700-263 | Actionness Estimation Using Hybrid Fully Convolutional Networks | http://arxiv.org/pdf/1604.07279v1.pdf | author:Limin Wang, Yu Qiao, Xiaoou Tang, Luc Van Gool category:cs.CV published:2016-04-25 summary:Actionness was introduced to quantify the likelihood of containing a genericaction instance at a specific location. Accurate and efficient estimation ofactionness is important in video analysis and may benefit other relevant taskssuch as action recognition and action detection. This paper presents a new deeparchitecture for actionness estimation, called hybrid fully convolutionalnetwork (H-FCN), which is composed of appearance FCN (A-FCN) and motion FCN(M-FCN). These two FCNs leverage the strong capacity of deep models to estimateactionness maps from the perspectives of static appearance and dynamic motion,respectively. In addition, the fully convolutional nature of H-FCN allows it toefficiently process videos with arbitrary sizes. Experiments are conducted onthe challenging datasets of Stanford40, UCF Sports, and JHMDB to verify theeffectiveness of H-FCN on actionness estimation, which demonstrate that ourmethod achieves superior performance to previous ones. Moreover, we apply theestimated actionness maps on action proposal generation and action detection.Our actionness maps advance the current state-of-the-art performance of thesetasks substantially.
arxiv-17700-264 | CMA-ES for Hyperparameter Optimization of Deep Neural Networks | http://arxiv.org/pdf/1604.07269v1.pdf | author:Ilya Loshchilov, Frank Hutter category:cs.NE cs.LG published:2016-04-25 summary:Hyperparameters of deep neural networks are often optimized by grid search,random search or Bayesian optimization. As an alternative, we propose to usethe Covariance Matrix Adaptation Evolution Strategy (CMA-ES), which is knownfor its state-of-the-art performance in derivative-free optimization. CMA-EShas some useful invariance properties and is friendly to parallel evaluationsof solutions. We provide a toy example comparing CMA-ES and state-of-the-artBayesian optimization algorithms for tuning the hyperparameters of aconvolutional neural network for the MNIST dataset on 30 GPUs in parallel.
arxiv-17700-265 | Online Batch Selection for Faster Training of Neural Networks | http://arxiv.org/pdf/1511.06343v4.pdf | author:Ilya Loshchilov, Frank Hutter category:cs.LG cs.NE math.OC published:2015-11-19 summary:Deep neural networks are commonly trained using stochastic non-convexoptimization procedures, which are driven by gradient information estimated onfractions (batches) of the dataset. While it is commonly accepted that batchsize is an important parameter for offline tuning, the benefits of onlineselection of batches remain poorly understood. We investigate online batchselection strategies for two state-of-the-art methods of stochasticgradient-based optimization, AdaDelta and Adam. As the loss function to beminimized for the whole dataset is an aggregation of loss functions ofindividual datapoints, intuitively, datapoints with the greatest loss should beconsidered (selected in a batch) more frequently. However, the limitations ofthis intuition and the proper control of the selection pressure over time areopen questions. We propose a simple strategy where all datapoints are rankedw.r.t. their latest known loss value and the probability to be selected decaysexponentially as a function of rank. Our experimental results on the MNISTdataset suggest that selecting batches speeds up both AdaDelta and Adam by afactor of about 5.
arxiv-17700-266 | A Deep Hierarchical Approach to Lifelong Learning in Minecraft | http://arxiv.org/pdf/1604.07255v1.pdf | author:Chen Tessler, Shahar Givony, Tom Zahavy, Daniel J. Mankowitz, Shie Mannor category:cs.AI cs.LG published:2016-04-25 summary:The ability to reuse or transfer knowledge from one task to another inlifelong learning problems, such as Minecraft, is one of the major challengesfaced in AI. Reusing knowledge across tasks is crucial to solving tasksefficiently with lower sample complexity. We provide a Reinforcement Learningagent with the ability to transfer knowledge by learning reusable skills, atype of temporally extended action (also know as Options (Sutton et. al.1999)). The agent learns reusable skills using Deep Q Networks (Mnih et. al.2015) to solve tasks in Minecraft, a popular video game which is an unsolvedand high-dimensional lifelong learning problem. These reusable skills, which werefer to as Deep Skill Networks (DSNs), are then incorporated into our novelHierarchical Deep Reinforcement Learning Network (H-DRLN) architecture. TheH-DRLN is a hierarchical version of Deep QNetworks and learns to efficientlysolve tasks by reusing knowledge from previously learned DSNs. The H-DRLNexhibits superior performance and lower learning sample complexity (by takingadvantage of temporal extension) compared to the regular Deep Q Network (Mnihet. al. 2015) in subdomains of Minecraft. We also show the potential totransfer knowledge between related Minecraft tasks without any additionallearning.
arxiv-17700-267 | Expectation Maximization for Sum-Product Networks as Exponential Family Mixture Models | http://arxiv.org/pdf/1604.07243v1.pdf | author:Mattia Desana, Christoph Schnörr category:cs.LG published:2016-04-25 summary:Sum-Product Networks (SPNs) are a recent class of probabilistic models whichencode very large mixtures compactly by exploiting efficient reuse ofcomputation in inference. Crucially, in SPNs the cost of inference scaleslinearly with the number of edges $E$ but the encoded mixture size $C$ can beexponentially larger than $E$. In this paper we obtain an efficient ($O(E)$)implementation of Expectation Maximization (EM) for SPNs which is the first toinclude EM updates both on mixture coefficients (corresponding to SPN weights)and mixture components (corresponding to SPN leaves). In particular, the updateon mixture components translates to a weighted maximum likelihood problem onleaf distributions, and can be solved exactly when leaves are in theexponential family. This opens new application areas for SPNs, such as learninglarge mixtures of tree graphical models. We validate the algorithm on asynthetic but non trivial "soft-parity" distribution with $2^{n}$ modes encodedby a SPN with only $O(n)$ edges.
arxiv-17700-268 | Learning vector autoregressive models with focalised Granger-causality graphs | http://arxiv.org/pdf/1507.01978v2.pdf | author:Magda Gregorova, Alexandros Kalousis, Stéphane Marchand-Maillet category:cs.LG stat.ML published:2015-07-07 summary:We consider the problem of learning models for forecasting multipletime-series systems together with discovering the leading indicators that serveas good predictors for the system. We model the systems by linear vectorautoregressive models (VAR) and link the discovery of leading indicators toinferring sparse graphs of Granger-causality. We propose new problemformulations and develop two new methods to learn such models, graduallyincreasing the complexity of assumptions and approaches. While the first methodassumes common structures across the whole system, our second method uncoversmodel clusters based on the Granger-causality and leading indicators togetherwith learning the model parameters. We study the performance of our methods ona comprehensive set of experiments and confirm their efficacy and theiradvantages over state-of-the-art sparse VAR and graphical Granger learningmethods.
arxiv-17700-269 | Towards Real-Time, Country-Level Location Classification of Worldwide Tweets | http://arxiv.org/pdf/1604.07236v1.pdf | author:Arkaitz Zubiaga, Alex Voss, Rob Procter, Maria Liakata, Bo Wang, Adam Tsakalidis category:cs.IR cs.CL cs.SI published:2016-04-25 summary:With the increase of interest in using social media as a source for research,many have tackled the task of automatically geolocating tweets, motivated bythe lack of explicit location information in the majority of tweets. Whileothers have focused on state- or city-level classification of tweets restrictedto a specific country, here we undertake the task in a broader context byclassifying global tweets at the country level, so far unexplored in areal-time scenario. We analyse the extent to which a tweet's country of origincan be determined by making use of eight tweet-inherent features forclassification using Support Vector Machines. Furthermore, we use two datasets,collected a year apart from each other, to analyse the extent to which a modeltrained from historical tweets can still be leveraged for classification of newtweets. With classification experiments on all 217 countries in our datasets,as well as on the top 25 countries, we offer some insights into the best use oftweet-inherent features for an accurate country-level classification of tweets.Among the features inherent in a tweet, we observe that the validity ofhistorical tweet content fades over time, and other metadata associated withthe tweet, such as the language of the tweet, the name of the user, or the timezone in which the user is located, lead to more accurate classification. Whileno feature set is optimal for all countries, and each country needs to betreated differently, we show that remarkably high performance values above 0.9in terms of F1 score can be achieved for countries with unique characteristicssuch as those having a language that is not spoken in many other countries or aunique time zone. However, the difficulty of achieving an accurateclassification increases for countries with multiple commonalities, especiallyfor English and Spanish speaking countries.
arxiv-17700-270 | Robust Sparse Blind Source Separation | http://arxiv.org/pdf/1507.02216v2.pdf | author:Cecile Chenot, Jerome Bobin, Jeremy Rapin category:stat.AP cs.LG stat.ML published:2015-07-08 summary:Blind Source Separation is a widely used technique to analyze multichanneldata. In many real-world applications, its results can be significantlyhampered by the presence of unknown outliers. In this paper, a novel algorithmcoined rGMCA (robust Generalized Morphological Component Analysis) isintroduced to retrieve sparse sources in the presence of outliers. Itexplicitly estimates the sources, the mixing matrix, and the outliers. It alsotakes advantage of the estimation of the outliers to further implement aweighting scheme, which provides a highly robust separation procedure.Numerical experiments demonstrate the efficiency of rGMCA to estimate themixing matrix in comparison with standard BSS techniques.
arxiv-17700-271 | IDSA: Intelligent Distributed Sensor Activation Algorithm For Target Tracking With Wireless Sensor Network | http://arxiv.org/pdf/1506.00122v3.pdf | author:Mohammad Sabokrou, Mahmood Fathy, Mojtaba Hoseini category:cs.NI cs.NE published:2015-05-30 summary:One important application of the Wireless Sensor Network(WSN) is targettracking, the aim of this application is converging to an event or object in anarea. In this paper, we propose an energy-efficient distributed sensoractivation protocol based on predicted location technique, called IntelligentDistributed Sensor Activation Algorithm (IDSA). The proposed algorithm predictsthe location of target in the next time interval, by analyzing current locationand movement history of the target, this prediction is done by computationalintelligence. The fewest essential number of sensor nodes within the predictedlocation will be activated to cover the target. The results show that theproposed method outperforms the existing methods such as Na\"ive and DSA interms of energy consumption and the number of nodes that was involved intracking the target.
arxiv-17700-272 | Towards Reduced Reference Parametric Models for Estimating Audiovisual Quality in Multimedia Services | http://arxiv.org/pdf/1604.07211v1.pdf | author:Edip Demirbilek, Jean-Charles Grégoire category:cs.MM cs.LG published:2016-04-25 summary:We have developed reduced reference parametric models for estimatingperceived quality in audiovisual multimedia services. We have created 144unique configurations for audiovisual content including various application andnetwork parameters such as bitrates and distortions in terms of bandwidth,packet loss rate and jitter. To generate the data needed for model training andvalidation we have tasked 24 subjects, in a controlled environment, to rate theoverall audiovisual quality on the absolute category rating (ACR) 5-levelquality scale. We have developed models using Random Forest and Neural Networkbased machine learning methods in order to estimate Mean Opinion Scores (MOS)values. We have used information retrieved from the packet headers and sideinformation provided as network parameters for model training. Random Forestbased models have performed better in terms of Root Mean Square Error (RMSE)and Pearson correlation coefficient. The side information proved to be veryeffective in developing the model. We have found that, while the modelperformance might be improved by replacing the side information with moreaccurate bit stream level measurements, they are performing well in estimatingperceived quality in audiovisual multimedia services.
arxiv-17700-273 | Unbiased Comparative Evaluation of Ranking Functions | http://arxiv.org/pdf/1604.07209v1.pdf | author:Tobias Schnabel, Adith Swaminathan, Peter Frazier, Thorsten Joachims category:cs.IR cs.LG published:2016-04-25 summary:Eliciting relevance judgments for ranking evaluation is labor-intensive andcostly, motivating careful selection of which documents to judge. Unliketraditional approaches that make this selection deterministically,probabilistic sampling has shown intriguing promise since it enables the designof estimators that are provably unbiased even when reusing data with missingjudgments. In this paper, we first unify and extend these sampling approachesby viewing the evaluation problem as a Monte Carlo estimation task that appliesto a large number of common IR metrics. Drawing on the theoretical clarity thatthis view offers, we tackle three practical evaluation scenarios: comparing twosystems, comparing $k$ systems against a baseline, and ranking $k$ systems. Foreach scenario, we derive an estimator and a variance-optimizing samplingdistribution while retaining the strengths of sampling-based evaluation,including unbiasedness, reusability despite missing data, and ease of use inpractice. In addition to the theoretical contribution, we empirically evaluateour methods against previously used sampling heuristics and find that theygenerally cut the number of required relevance judgments at least in half.
arxiv-17700-274 | Observing and Recommending from a Social Web with Biases | http://arxiv.org/pdf/1604.07180v1.pdf | author:Steffen Staab, Sophie Stalla-Bourdillon, Laura Carmichael category:cs.DB cs.LG K.5.0; H.2.8 published:2016-04-25 summary:The research question this report addresses is: how, and to what extent,those directly involved with the design, development and employment of aspecific black box algorithm can be certain that it is not unlawfullydiscriminating (directly and/or indirectly) against particular persons withprotected characteristics (e.g. gender, race and ethnicity)?
arxiv-17700-275 | Weighted Spectral Cluster Ensemble | http://arxiv.org/pdf/1604.07178v1.pdf | author:Muhammad Yousefnezhad, Daoqiang Zhang category:cs.LG cs.AI stat.ML published:2016-04-25 summary:Clustering explores meaningful patterns in the non-labeled data sets. ClusterEnsemble Selection (CES) is a new approach, which can combine individualclustering results for increasing the performance of the final results.Although CES can achieve better final results in comparison with individualclustering algorithms and cluster ensemble methods, its performance can bedramatically affected by its consensus diversity metric and thresholdingprocedure. There are two problems in CES: 1) most of the diversity metrics isbased on heuristic Shannon's entropy and 2) estimating threshold values arereally hard in practice. The main goal of this paper is proposing a robustapproach for solving the above mentioned problems. Accordingly, this paperdevelops a novel framework for clustering problems, which is called WeightedSpectral Cluster Ensemble (WSCE), by exploiting some concepts from communitydetection arena and graph based clustering. Under this framework, a new versionof spectral clustering, which is called Two Kernels Spectral Clustering, isused for generating graphs based individual clustering results. Further, byusing modularity, which is a famous metric in the community detection, on thetransformed graph representation of individual clustering results, our approachprovides an effective diversity estimation for individual clustering results.Moreover, this paper introduces a new approach for combining the evaluatedindividual clustering results without the procedure of thresholding.Experimental study on varied data sets demonstrates that the prosed approachachieves superior performance to state-of-the-art methods.
arxiv-17700-276 | Protein Secondary Structure Prediction Using Cascaded Convolutional and Recurrent Neural Networks | http://arxiv.org/pdf/1604.07176v1.pdf | author:Zhen Li, Yizhou Yu category:q-bio.BM cs.AI cs.LG cs.NE q-bio.QM published:2016-04-25 summary:Protein secondary structure prediction is an important problem inbioinformatics. Inspired by the recent successes of deep neural networks, inthis paper, we propose an end-to-end deep network that predicts proteinsecondary structures from integrated local and global contextual features. Ourdeep architecture leverages convolutional neural networks with different kernelsizes to extract multiscale local contextual features. In addition, consideringlong-range dependencies existing in amino acid sequences, we set up abidirectional neural network consisting of gated recurrent unit to captureglobal contextual features. Furthermore, multi-task learning is utilized topredict secondary structure labels and amino-acid solvent accessibilitysimultaneously. Our proposed deep network demonstrates its effectiveness byachieving state-of-the-art performance, i.e., 69.7% Q8 accuracy on the publicbenchmark CB513, 76.9% Q8 accuracy on CASP10 and 73.1% Q8 accuracy on CASP11.Our model and results are publicly available.
arxiv-17700-277 | RGB-D Scene Labeling with Long Short-Term Memorized Fusion Model | http://arxiv.org/pdf/1604.05000v2.pdf | author:Zhen Li, Yukang Gan, Xiaodan Liang, Yizhou Yu, Hui Cheng, Liang Lin category:cs.CV cs.NE published:2016-04-18 summary:Semantic labeling of RGB-D scenes is crucial to many intelligent applicationsincluding perceptual robotics. It generates pixelwise and fine-grained labelmaps from simultaneously sensed photometric (RGB) and depth channels. Thispaper addresses this problem by i) developing a novel Long Short-Term MemorizedFusion (LSTM-F) Model that captures and fuses contextual information frommultiple channels of photometric and depth data, and ii) incorporating thismodel into deep convolutional neural networks (CNNs) for end-to-end training.Specifically, global contexts in photometric and depth channels are,respectively, captured by stacking several convolutional layers and a longshort-term memory layer; the memory layer encodes both short-range andlong-range spatial dependencies in an image along the vertical direction.Another long short-term memorized fusion layer is set up to integrate thecontexts along the vertical direction from different channels, and performbi-directional propagation of the fused vertical contexts along the horizontaldirection to obtain true 2D global contexts. At last, the fused contextualrepresentation is concatenated with the convolutional features extracted fromthe photometric channels in order to improve the accuracy of fine-scalesemantic labeling. Our proposed model has set a new state of the art, i.e.,48.1% average class accuracy over 37 categories 11.8% improvement), on thelarge-scale SUNRGBD dataset.1
arxiv-17700-278 | Real-time 2D/3D Registration via CNN Regression | http://arxiv.org/pdf/1507.07505v3.pdf | author:Shun Miao, Z. Jane Wang, Rui Liao category:cs.CV published:2015-07-27 summary:In this paper, we present a Convolutional Neural Network (CNN) regressionapproach for real-time 2-D/3-D registration. Different from optimization-basedmethods, which iteratively optimize the transformation parameters over ascalar-valued metric function representing the quality of the registration, theproposed method exploits the information embedded in the appearances of theDigitally Reconstructed Radiograph and X-ray images, and employs CNN regressorsto directly estimate the transformation parameters. The CNN regressors aretrained for local zones and applied in a hierarchical manner to break down thecomplex regression task into simpler sub-tasks that can be learned separately.Our experiment results demonstrate the advantage of the proposed method incomputational efficiency with negligible degradation of registration accuracycompared to intensity-based methods.
arxiv-17700-279 | Neural Random Forests | http://arxiv.org/pdf/1604.07143v1.pdf | author:Gérard Biau, Erwan Scornet, Johannes Welbl category:stat.ML cs.LG published:2016-04-25 summary:Given an ensemble of randomized regression trees, it is possible torestructure them as a collection of multilayered neural networks withparticular connection weights. Following this principle, we reformulate therandom forest method of Breiman (2001) into a neural network setting, and inturn propose two new hybrid procedures that we call neural random forests. Bothpredictors exploit prior knowledge of regression trees for their architecture,have less parameters to tune than standard networks, and less restrictions onthe geometry of the decision boundaries. Consistency results are proved, andsubstantial numerical evidence is provided on both synthetic and real data setsto assess the excellent performance of our methods in a large variety ofprediction problems.
arxiv-17700-280 | Benchmarking Deep Reinforcement Learning for Continuous Control | http://arxiv.org/pdf/1604.06778v2.pdf | author:Yan Duan, Xi Chen, Rein Houthooft, John Schulman, Pieter Abbeel category:cs.LG cs.AI cs.RO published:2016-04-22 summary:Recently, researchers have made significant progress combining the advancesin deep learning for learning feature representations with reinforcementlearning. Some notable examples include training agents to play Atari gamesbased on raw pixel data and to acquire advanced manipulation skills using rawsensory inputs. However, it has been difficult to quantify progress in thedomain of continuous control due to the lack of a commonly adopted benchmark.In this work, we present a benchmark suite of continuous control tasks,including classic tasks like cart-pole swing-up, tasks with very high state andaction dimensionality such as 3D humanoid locomotion, tasks with partialobservations, and tasks with hierarchical structure. We report novel findingsbased on the systematic evaluation of a range of implemented reinforcementlearning algorithms. Both the benchmark and reference implementations arereleased open-source in order to facilitate experimental reproducibility and toencourage adoption by other researchers.
arxiv-17700-281 | Modeling the Evolution of Gene-Culture Divergence | http://arxiv.org/pdf/1604.07108v1.pdf | author:Chris Marriott, Jobran Chebib category:cs.NE cs.MA q-bio.PE published:2016-04-25 summary:We present a model for evolving agents using both genetic and culturalinheritance mechanisms. Within each agent our model maintains two distinctinformation stores we call the genome and the memome. Processes of adaptationare modeled as evolutionary processes at each level of adaptation(phylogenetic, ontogenetic, sociogenetic). We review relevant competing modelsand we show how our model improves on previous attempts to model genetic andcultural evolutionary processes. In particular we argue our model can achievedivergent gene-culture co-evolution.
arxiv-17700-282 | Learning to Select Pre-Trained Deep Representations with Bayesian Evidence Framework | http://arxiv.org/pdf/1506.02565v4.pdf | author:Yong-Deok Kim, Taewoong Jang, Bohyung Han, Seungjin Choi category:cs.CV cs.LG stat.ML published:2015-06-08 summary:We propose a Bayesian evidence framework to facilitate transfer learning frompre-trained deep convolutional neural networks (CNNs). Our framework isformulated on top of a least squares SVM (LS-SVM) classifier, which is simpleand fast in both training and testing, and achieves competitive performance inpractice. The regularization parameters in LS-SVM is estimated automaticallywithout grid search and cross-validation by maximizing evidence, which is auseful measure to select the best performing CNN out of multiple candidates fortransfer learning; the evidence is optimized efficiently by employing Aitken'sdelta-squared process, which accelerates convergence of fixed point update. Theproposed Bayesian evidence framework also provides a good solution to identifythe best ensemble of heterogeneous CNNs through a greedy algorithm. OurBayesian evidence framework for transfer learning is tested on 12 visualrecognition datasets and illustrates the state-of-the-art performanceconsistently in terms of prediction accuracy and modeling efficiency.
arxiv-17700-283 | Approximation Vector Machines for Large-scale Online Learning | http://arxiv.org/pdf/1604.06518v2.pdf | author:Trung Le, Tu Dinh Nguyen, Vu Nguyen, Dinh Phung category:cs.LG published:2016-04-22 summary:One of the most challenging problems in kernel online learning is to boundthe model size and to promote the model sparsity. Sparse models not onlyimprove computation and memory usage, but also enhance the generalizationcapacity, a principle that concurs with the law of parsimony. However,inappropriate sparsity modeling may also significantly degrade the performance.In this paper, we propose Approximation Vector Machine (AVM), a model that cansimultaneously encourage the sparsity and safeguard its risk in compromisingthe performance. When an incoming instance arrives, we approximate thisinstance by one of its neighbors whose distance to it is less than a predefinedthreshold. Our key intuition is that since the newly seen instance is expressedby its nearby neighbor the optimal performance can be analytically formulatedand maintained. We develop theoretical foundations to support this intuitionand further establish an analysis to characterize the gap between theapproximation and optimal solutions. This gap crucially depends on thefrequency of approximation and the predefined threshold. We perform theconvergence analysis for a wide spectrum of loss functions including Hinge,smooth Hinge, and Logistic for classification task, and $l_1$, $l_2$, and$\epsilon$-insensitive for regression task. We conducted extensive experimentsfor classification task in batch and online modes, and regression task inonline mode over several benchmark datasets. The results show that our proposedAVM achieved a comparable predictive performance with current state-of-the-artmethods while simultaneously achieving significant computational speed-up dueto the ability of the proposed AVM in maintaining the model size.
arxiv-17700-284 | Makeup like a superstar: Deep Localized Makeup Transfer Network | http://arxiv.org/pdf/1604.07102v1.pdf | author:Si Liu, Xinyu Ou, Ruihe Qian, Wei Wang, Xiaochun Cao category:cs.CV cs.AI published:2016-04-25 summary:In this paper, we propose a novel Deep Localized Makeup Transfer Network toautomatically recommend the most suitable makeup for a female and synthesis themakeup on her face. Given a before-makeup face, her most suitable makeup isdetermined automatically. Then, both the beforemakeup and the reference facesare fed into the proposed Deep Transfer Network to generate the after-makeupface. Our end-to-end makeup transfer network have several nice propertiesincluding: (1) with complete functions: including foundation, lip gloss, andeye shadow transfer; (2) cosmetic specific: different cosmetics are transferredin different manners; (3) localized: different cosmetics are applied ondifferent facial regions; (4) producing naturally looking results withoutobvious artifacts; (5) controllable makeup lightness: various results fromlight makeup to heavy makeup can be generated. Qualitative and quantitativeexperiments show that our network performs much better than the methods of [Guoand Sim, 2009] and two variants of NerualStyle [Gatys et al., 2015a].
arxiv-17700-285 | Double Thompson Sampling for Dueling Bandits | http://arxiv.org/pdf/1604.07101v1.pdf | author:Huasen Wu, Xin Liu, R. Srikant category:cs.LG stat.ML published:2016-04-25 summary:In this paper, we propose a Double Thompson Sampling (D-TS) algorithm fordueling bandit problems. As indicated by its name, D-TS selects both the firstand the second candidates according to Thompson Sampling. Specifically, D-TSmaintains a posterior distribution for the preference matrix, and chooses thepair of arms for comparison by sampling twice from the posterior distribution.This simple algorithm applies to general Copeland dueling bandits, includingCondorcet dueling bandits as its special case. For general Copeland duelingbandits, we show that D-TS achieves $O(K^2 \log T)$ regret. For Condorcetdueling bandits, we further simplify the D-TS algorithm and show that thesimplified D-TS algorithm achieves $O(K \log T + K^2 \log \log T)$ regret.Simulation results based on both synthetic and real-world data demonstrate theefficiency of the proposed D-TS algorithm.
arxiv-17700-286 | A Minimalistic Approach to Sum-Product Network Learning for Real Applications | http://arxiv.org/pdf/1602.04259v3.pdf | author:Viktoriya Krakovna, Moshe Looks category:cs.AI cs.LG stat.ML published:2016-02-12 summary:Sum-Product Networks (SPNs) are a class of expressive yet tractablehierarchical graphical models. LearnSPN is a structure learning algorithm forSPNs that uses hierarchical co-clustering to simultaneously identifying similarentities and similar features. The original LearnSPN algorithm assumes that allthe variables are discrete and there is no missing data. We introduce apractical, simplified version of LearnSPN, MiniSPN, that runs faster and canhandle missing data and heterogeneous features common in real applications. Wedemonstrate the performance of MiniSPN on standard benchmark datasets and ontwo datasets from Google's Knowledge Graph exhibiting high missingness ratesand a mix of discrete and continuous features.
arxiv-17700-287 | Semi-supervised Vocabulary-informed Learning | http://arxiv.org/pdf/1604.07093v1.pdf | author:Yanwei Fu, Leonid Sigal category:cs.CV cs.AI cs.LG stat.AP stat.ML published:2016-04-24 summary:Despite significant progress in object categorization, in recent years, anumber of important challenges remain, mainly, ability to learn from limitedlabeled data and ability to recognize object classes within large, potentiallyopen, set of labels. Zero-shot learning is one way of addressing thesechallenges, but it has only been shown to work with limited sized classvocabularies and typically requires separation between supervised andunsupervised classes, allowing former to inform the latter but not vice versa.We propose the notion of semi-supervised vocabulary-informed learning toalleviate the above mentioned challenges and address problems of supervised,zero-shot and open set recognition using a unified framework. Specifically, wepropose a maximum margin framework for semantic manifold-based recognition thatincorporates distance constraints from (both supervised and unsupervised)vocabulary atoms, ensuring that labeled samples are projected closest to theircorrect prototypes, in the embedding space, than to others. We show thatresulting model shows improvements in supervised, zero-shot, and large open setrecognition, with up to 310K class vocabulary on AwA and ImageNet datasets.
arxiv-17700-288 | Convolutional Radio Modulation Recognition Networks | http://arxiv.org/pdf/1602.04105v2.pdf | author:Timothy J O'Shea, Johnathan Corgan, T. Charles Clancy category:cs.LG cs.CV published:2016-02-12 summary:We study the adaptation of convolutional neural networks to the complextemporal radio signal domain. We compare the efficacy of radio modulationclassification using naively learned features against using expert features,which are currently used widely and well regarded in the field and we showsignificant performance improvements. We show that blind temporal learning onlarge and densely encoded time series using deep convolutional neural networksis viable and a strong candidate approach for this task.
arxiv-17700-289 | Semi-supervised and Unsupervised Methods for Categorizing Posts in Web Discussion Forums | http://arxiv.org/pdf/1604.00119v3.pdf | author:Krish Perumal category:cs.CL cs.IR cs.LG cs.SI published:2016-04-01 summary:Web discussion forums are used by millions of people worldwide to shareinformation belonging to a variety of domains such as automotive vehicles,pets, sports, etc. They typically contain posts that fall into differentcategories such as problem, solution, feedback, spam, etc. Automaticidentification of these categories can aid information retrieval that istailored for specific user requirements. Previously, a number of supervisedmethods have attempted to solve this problem; however, these depend on theavailability of abundant training data. A few existing unsupervised andsemi-supervised approaches are either focused on identifying a single categoryor do not report category-specific performance. In contrast, this work proposesunsupervised and semi-supervised methods that require no or minimal trainingdata to achieve this objective without compromising on performance. Afine-grained analysis is also carried out to discuss their limitations. Theproposed methods are based on sequence models (specifically, Hidden MarkovModels) that can model language for each category using word and part-of-speechprobability distributions, and manually specified features. Empiricalevaluations across domains demonstrate that the proposed methods are bettersuited for this task than existing ones.
arxiv-17700-290 | Performance Limits of Online Stochastic Sub-Gradient Learning | http://arxiv.org/pdf/1511.07902v2.pdf | author:Bicheng Ying, Ali H. Sayed category:stat.ML cs.LG cs.MA published:2015-11-24 summary:This work examines the performance of stochastic sub-gradient learningstrategies under weaker conditions than usually considered in the literature.The conditions are shown to be automatically satisfied by several importantcases of interest including the construction of Linear-SVM, LASSO, andTotal-Variation denoising formulations. In comparison, these problems do notsatisfy the traditional assumptions automatically and, therefore, conclusionsderived based on these earlier assumptions are not directly applicable to theseproblems. The analysis establishes that stochastic sub-gradient strategies canattain exponential convergence rates, as opposed to sub-linear rates, to thesteady-state. A realizable exponential-weighting procedure is proposed tosmooth the intermediate iterates by the sub-gradient procedure and to guaranteethe established performance bounds in terms of convergence rate and excessiverisk performance. Both single-agent and multi-agent scenarios are studied,where the latter case assumes that a collection of agents are interconnected bya topology and can only interact locally with their neighbors. The theoreticalconclusions are illustrated by several examples and simulations, includingcomparisons with the FISTA procedure.
arxiv-17700-291 | Unsupervised Representation Learning of Structured Radio Communication Signals | http://arxiv.org/pdf/1604.07078v1.pdf | author:Timothy J. O'Shea, Johnathan Corgan, T. Charles Clancy category:cs.LG published:2016-04-24 summary:We explore unsupervised representation learning of radio communicationsignals in raw sampled time series representation. We demonstrate that we canlearn modulation basis functions using convolutional autoencoders and visuallyrecognize their relationship to the analytic bases used in digitalcommunications. We also propose and evaluate quantitative met- rics for qualityof encoding using domain relevant performance metrics.
arxiv-17700-292 | A note on the evaluation of generative models | http://arxiv.org/pdf/1511.01844v3.pdf | author:Lucas Theis, Aäron van den Oord, Matthias Bethge category:stat.ML cs.LG published:2015-11-05 summary:Probabilistic generative models can be used for compression, denoising,inpainting, texture synthesis, semi-supervised learning, unsupervised featurelearning, and other tasks. Given this wide range of applications, it is notsurprising that a lot of heterogeneity exists in the way these models areformulated, trained, and evaluated. As a consequence, direct comparison betweenmodels is often difficult. This article reviews mostly known but oftenunderappreciated properties relating to the evaluation and interpretation ofgenerative models with a focus on image models. In particular, we show thatthree of the currently most commonly used criteria---average log-likelihood,Parzen window estimates, and visual fidelity of samples---are largelyindependent of each other when the data is high-dimensional. Good performancewith respect to one criterion therefore need not imply good performance withrespect to the other criteria. Our results show that extrapolation from onecriterion to another is not warranted and generative models need to beevaluated directly with respect to the application(s) they were intended for.In addition, we provide examples demonstrating that Parzen window estimatesshould generally be avoided.
arxiv-17700-293 | Fast-and-Light Stochastic ADMM | http://arxiv.org/pdf/1604.07070v1.pdf | author:Shuai Zheng, James T. Kwok category:cs.LG stat.ML published:2016-04-24 summary:The alternating direction method of multipliers (ADMM) is a powerfuloptimization solver in machine learning. Recently, stochastic ADMM has beenintegrated with variance reduction methods for stochastic gradient, leading toSAG-ADMM and SDCA-ADMM that have fast convergence rates and low iterationcomplexities. However, their space requirements can still be high. In thispaper, we propose an integration of ADMM with the method of stochastic variancereduced gradient (SVRG). Unlike another recent integration attempt calledSCAS-ADMM, the proposed algorithm retains the fast convergence benefits ofSAG-ADMM and SDCA-ADMM, but is more advantageous in that its storagerequirement is very low, even independent of the sample size $n$. Experimentalresults demonstrate that it is as fast as SAG-ADMM and SDCA-ADMM, much fasterthan SCAS-ADMM, and can be used on much bigger data sets.
arxiv-17700-294 | Binary Codes for Tagging X-Ray Images via Deep De-Noising Autoencoders | http://arxiv.org/pdf/1604.07060v1.pdf | author:Antonio Sze-To, Hamid R. Tizhoosh, Andrew K. C. Wong category:cs.CV published:2016-04-24 summary:A Content-Based Image Retrieval (CBIR) system which identifies similarmedical images based on a query image can assist clinicians for more accuratediagnosis. The recent CBIR research trend favors the construction and use ofbinary codes to represent images. Deep architectures could learn the non-linearrelationship among image pixels adaptively, allowing the automatic learning ofhigh-level features from raw pixels. However, most of them require classlabels, which are expensive to obtain, particularly for medical images. Themethods which do not need class labels utilize a deep autoencoder for binaryhashing, but the code construction involves a specific training algorithm andan ad-hoc regularization technique. In this study, we explored using a deepde-noising autoencoder (DDA), with a new unsupervised training scheme usingonly backpropagation and dropout, to hash images into binary codes. Weconducted experiments on more than 14,000 x-ray images. By using class labelsonly for evaluating the retrieval results, we constructed a 16-bit DDA and a512-bit DDA independently. Comparing to other unsupervised methods, wesucceeded to obtain the lowest total error by using the 512-bit codes forretrieval via exhaustive search, and speed up 9.27 times with the use of the16-bit codes while keeping a comparable total error. We found that our newtraining scheme could reduce the total retrieval error significantly by 21.9%.To further boost the image retrieval performance, we developed RadonAutoencoder Barcode (RABC) which are learned from the Radon projections ofimages using a de-noising autoencoder. Experimental results demonstrated itssuperior performance in retrieval when it was combined with DDA binary codes.
arxiv-17700-295 | Optimization as Estimation with Gaussian Processes in Bandit Settings | http://arxiv.org/pdf/1510.06423v3.pdf | author:Zi Wang, Bolei Zhou, Stefanie Jegelka category:stat.ML cs.LG published:2015-10-21 summary:Recently, there has been rising interest in Bayesian optimization -- theoptimization of an unknown function with assumptions usually expressed by aGaussian Process (GP) prior. We study an optimization strategy that directlyuses an estimate of the argmax of the function. This strategy offers bothpractical and theoretical advantages: no tradeoff parameter needs to beselected, and, moreover, we establish close connections to the popular GP-UCBand GP-PI strategies. Our approach can be understood as automatically andadaptively trading off exploration and exploitation in GP-UCB and GP-PI. Weillustrate the effects of this adaptive tuning via bounds on the regret as wellas an extensive empirical evaluation on robotics and vision tasks,demonstrating the robustness of this strategy for a range of performancecriteria.
arxiv-17700-296 | Rotation-Invariant Restricted Boltzmann Machine using shared gradient filters | http://arxiv.org/pdf/1604.07045v1.pdf | author:Mario Valerio Giuffrida, Sotirios A. Tsaftaris category:cs.CV published:2016-04-24 summary:Finding suitable features has been an essential problem in computer vision.We focus on Restricted Boltzmann Machines (RBMs), which, despite theirversatility, cannot accommodate transformations that may occur in the scene. Asresult, several approaches have been proposed that consider a set oftransformations, which are used to either augment the training set or transformthe actual learned filters. In this paper, we propose the ExplicitRotation-Invariant Restricted Boltzmann Machine, which exploits priorinformation coming from the dominant orientation of images. Our model extendsthe standard RBM, by adding a suitable number of weight matrices, associated toeach dominant gradient. We show that our approach is able to learnrotation-invariant features, comparing it with the classic formulation of RBMon the MNIST benchmark dataset. Overall, requiring less hidden units, ourmethod learns compact features, which are robust to rotations.
arxiv-17700-297 | Investigating echo state networks dynamics by means of recurrence analysis | http://arxiv.org/pdf/1601.07381v2.pdf | author:Filippo Maria Bianchi, Lorenzo Livi, Cesare Alippi category:cs.LG nlin.CD published:2016-01-26 summary:In this paper, we elaborate over the well-known interpretability issue inecho state networks. The idea is to investigate the dynamics of reservoirneurons with time-series analysis techniques taken from research on complexsystems. Notably, we analyze time-series of neuron activations with RecurrencePlots (RPs) and Recurrence Quantification Analysis (RQA), which permit tovisualize and characterize high-dimensional dynamical systems. We show thatthis approach is useful in a number of ways. First, the two-dimensionalrepresentation offered by RPs provides a way for visualizing thehigh-dimensional dynamics of a reservoir. Our results suggest that, if thenetwork is stable, reservoir and input denote similar line patterns in therespective RPs. Conversely, the more unstable the ESN, the more the RP of thereservoir presents instability patterns. As a second result, we show that the$\mathrm{L_{max}}$ measure is highly correlated with the well-establishedmaximal local Lyapunov exponent. This suggests that complexity measures basedon RP diagonal lines distribution provide a valuable tool to quantify thedegree of network stability. Finally, our analysis shows that all RQA measuresfluctuate on the proximity of the so-called edge of stability, where an ESNtypically achieves maximum computational capability. We verify that thedetermination of the edge of stability provided by such RQA measures is moreaccurate than two well-known criteria based on the Jacobian matrix of thereservoir. Therefore, we claim that RPs and RQA-based analyses can be used asvaluable tools to design an effective network given a specific problem.
arxiv-17700-298 | Matrix completion with column manipulation: Near-optimal sample-robustness-rank tradeoffs | http://arxiv.org/pdf/1102.2254v2.pdf | author:Yudong Chen, Huan Xu, Constantine Caramanis, Sujay Sanghavi category:stat.ML cs.IT math.IT published:2011-02-10 summary:This paper considers the problem of matrix completion when some number of thecolumns are completely and arbitrarily corrupted, potentially by a maliciousadversary. It is well-known that standard algorithms for matrix completion canreturn arbitrarily poor results, if even a single column is corrupted. Onedirect application comes from robust collaborative filtering. Here, some numberof users are so-called manipulators who try to skew the predictions of thealgorithm by calibrating their inputs to the system. In this paper, we developan efficient algorithm for this problem based on a combination of a trimmingprocedure and a convex program that minimizes the nuclear norm and the$\ell_{1,2}$ norm. Our theoretical results show that given a vanishing fractionof observed entries, it is nevertheless possible to complete the underlyingmatrix even when the number of corrupted columns grows. Significantly, ourresults hold without any assumptions on the locations or values of the observedentries of the manipulated columns. Moreover, we show by aninformation-theoretic argument that our guarantees are nearly optimal in termsof the fraction of sampled entries on the authentic columns, the fraction ofcorrupted columns, and the rank of the underlying matrix. Our results thereforesharply characterize the tradeoffs between sample, robustness and rank inmatrix completion.
arxiv-17700-299 | M$^2$S-Net: Multi-Modal Similarity Metric Learning based Deep Convolutional Network for Answer Selection | http://arxiv.org/pdf/1604.05519v2.pdf | author:Lingxun Meng, Yan Li category:cs.CL published:2016-04-19 summary:Recent works using artificial neural networks based on distributed wordrepresentation greatly boost performance on various natural language processingtasks, especially the answer selection problem. Nevertheless, most of theprevious works used deep learning methods (like LSTM-RNN, CNN, etc.) only tocapture semantic representation of each sentence separately, withoutconsidering the interdependence between each other. In this paper, we propose anovel end-to-end learning framework which constitutes deep convolutional neuralnetwork based on multi-modal similarity metric learning (M$^2$S-Net) onpairwise tokens. The proposed model demonstrates its performance by surpassingprevious state-of-the-art systems on the answer selection benchmark, i.e.,TREC-QA dataset, in both MAP and MRR metrics.
arxiv-17700-300 | Cardiac Motion Analysis by Temporal Flow Graphs | http://arxiv.org/pdf/1604.06979v1.pdf | author:V S R Veeravasarapu, Jayanthi Sivaswamy, Vishanji Karani category:cs.CV published:2016-04-24 summary:Cardiac motion analysis from B-mode ultrasound sequence is a key task inassessing the health of the heart. The paper proposes a new methodology forcardiac motion analysis based on the temporal behaviour of points of intereston the myocardium. We define a new signal called the Temporal Flow Graph (TFG)which depicts the movement of a point of interest over time. It is a graphicalrepresentation derived from a flow field and describes the temporal evolutionof a point. We prove that TFG for an object undergoing periodic motion is alsoperiodic. This principle can be utilized to derive both global and localinformation from a given sequence. We demonstrate this for detecting motionirregularities at the sequence, as well as regional levels on real andsynthetic data. A coarse localisation of anatomical landmarks such as centresof left/right cavities and valve points is also demonstrated using TFGs.
