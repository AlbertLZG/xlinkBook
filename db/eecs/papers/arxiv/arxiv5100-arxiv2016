arxiv-5100-1 | An evaluative baseline for geo-semantic relatedness and similarity | http://arxiv.org/pdf/1402.3371v1.pdf | author:Andrea Ballatore, Michela Bertolotto, David C. Wilson category:cs.CL published:2014-02-14 summary:In geographic information science and semantics, the computation of semanticsimilarity is widely recognised as key to supporting a vast number of tasks ininformation integration and retrieval. By contrast, the role of geo-semanticrelatedness has been largely ignored. In natural language processing, semanticrelatedness is often confused with the more specific semantic similarity. Inthis article, we discuss a notion of geo-semantic relatedness based on Lehrer'ssemantic fields, and we compare it with geo-semantic similarity. We thendescribe and validate the Geo Relatedness and Similarity Dataset (GeReSiD), anew open dataset designed to evaluate computational measures of geo-semanticrelatedness and similarity. This dataset is larger than existing datasets ofthis kind, and includes 97 geographic terms combined into 50 term pairs ratedby 203 human subjects. GeReSiD is available online and can be used as anevaluation baseline to determine empirically to what degree a givencomputational model approximates geo-semantic relatedness and similarity.
arxiv-5100-2 | Active Clustering with Model-Based Uncertainty Reduction | http://arxiv.org/pdf/1402.1783v2.pdf | author:Caiming Xiong, David Johnson, Jason J. Corso category:cs.LG cs.CV stat.ML 62H30 published:2014-02-07 summary:Semi-supervised clustering seeks to augment traditional clustering methods byincorporating side information provided via human expertise in order toincrease the semantic meaningfulness of the resulting clusters. However, mostcurrent methods are \emph{passive} in the sense that the side information isprovided beforehand and selected randomly. This may require a large number ofconstraints, some of which could be redundant, unnecessary, or even detrimentalto the clustering results. Thus in order to scale such semi-supervisedalgorithms to larger problems it is desirable to pursue an \emph{active}clustering method---i.e. an algorithm that maximizes the effectiveness of theavailable human labor by only requesting human input where it will have thegreatest impact. Here, we propose a novel online framework for activesemi-supervised spectral clustering that selects pairwise constraints asclustering proceeds, based on the principle of uncertainty reduction. Using afirst-order Taylor expansion, we decompose the expected uncertainty reductionproblem into a gradient and a step-scale, computed via an application of matrixperturbation theory and cluster-assignment entropy, respectively. The resultingmodel is used to estimate the uncertainty reduction potential of each sample inthe dataset. We then present the human user with pairwise queries with respectto only the best candidate sample. We evaluate our method using three differentimage datasets (faces, leaves and dogs), a set of common UCI machine learningdatasets and a gene dataset. The results validate our decomposition formulationand show that our method is consistently superior to existing state-of-the-arttechniques, as well as being robust to noise and to unknown numbers ofclusters.
arxiv-5100-3 | Recovering Graph-Structured Activations using Adaptive Compressive Measurements | http://arxiv.org/pdf/1305.0213v3.pdf | author:Akshay Krishnamurthy, James Sharpnack, Aarti Singh category:stat.ML cs.IT math.IT published:2013-05-01 summary:We study the localization of a cluster of activated vertices in a graph, fromadaptively designed compressive measurements. We propose a hierarchicalpartitioning of the graph that groups the activated vertices into fewpartitions, so that a top-down sensing procedure can identify these partitions,and hence the activations, using few measurements. By exploiting the clusterstructure, we are able to provide localization guarantees at weaker signal tonoise ratios than in the unstructured setting. We complement this performanceguarantee with an information theoretic lower bound, providing a necessarysignal-to-noise ratio for any algorithm to successfully localize the cluster.We verify our analysis with some simulations, demonstrating the practicality ofour algorithm.
arxiv-5100-4 | Grid Topology Identification using Electricity Prices | http://arxiv.org/pdf/1312.0516v2.pdf | author:Vassilis Kekatos, Georgios B. Giannakis, Ross Baldick category:cs.LG cs.SY stat.AP stat.ML published:2013-12-02 summary:The potential of recovering the topology of a grid using solely publiclyavailable market data is explored here. In contemporary whole-sale electricitymarkets, real-time prices are typically determined by solving thenetwork-constrained economic dispatch problem. Under a linear DC model,locational marginal prices (LMPs) correspond to the Lagrange multipliers of thelinear program involved. The interesting observation here is that the matrix ofspatiotemporally varying LMPs exhibits the following property: Oncepremultiplied by the weighted grid Laplacian, it yields a low-rank and sparsematrix. Leveraging this rich structure, a regularized maximum likelihoodestimator (MLE) is developed to recover the grid Laplacian from the LMPs. Theconvex optimization problem formulated includes low rank- andsparsity-promoting regularizers, and it is solved using a scalable algorithm.Numerical tests on prices generated for the IEEE 14-bus benchmark provideencouraging topology recovery results.
arxiv-5100-5 | Hand-Eye and Robot-World Calibration by Global Polynomial Optimization | http://arxiv.org/pdf/1402.3261v1.pdf | author:Jan Heller, Didier Henrion, Tomas Pajdla category:cs.CV math.OC published:2014-02-13 summary:The need to relate measurements made by a camera to a different knowncoordinate system arises in many engineering applications. Historically, itappeared for the first time in the connection with cameras mounted on roboticsystems. This problem is commonly known as hand-eye calibration. In this paper,we present several formulations of hand-eye calibration that lead tomultivariate polynomial optimization problems. We show that the method ofconvex linear matrix inequality (LMI) relaxations can be used to effectivelysolve these problems and to obtain globally optimal solutions. Further, we showthat the same approach can be used for the simultaneous hand-eye androbot-world calibration. Finally, we validate the proposed solutions using bothsynthetic and real datasets.
arxiv-5100-6 | Gaussian Process Volatility Model | http://arxiv.org/pdf/1402.3085v1.pdf | author:Yue Wu, Jose Miguel Hernandez Lobato, Zoubin Ghahramani category:stat.ME stat.ML 62P05 published:2014-02-13 summary:The accurate prediction of time-changing variances is an important task inthe modeling of financial data. Standard econometric models are often limitedas they assume rigid functional relationships for the variances. Moreover,function parameters are usually learned using maximum likelihood, which canlead to overfitting. To address these problems we introduce a novel model fortime-changing variances using Gaussian Processes. A Gaussian Process (GP)defines a distribution over functions, which allows us to capture highlyflexible functional relationships for the variances. In addition, we develop anonline algorithm to perform inference. The algorithm has two main advantages.First, it takes a Bayesian approach, thereby avoiding overfitting. Second, itis much quicker than current offline inference procedures. Finally, our newmodel was evaluated on financial data and showed significant improvement inpredictive performance over current standard models.
arxiv-5100-7 | Software Requirement Specification Using Reverse Speech Technology | http://arxiv.org/pdf/1402.3080v1.pdf | author:Santhy Viswam, Sajeer Karattil category:cs.CL cs.SD published:2014-02-13 summary:Speech analysis had been taken to a new level with the discovery of ReverseSpeech (RS). RS is the discovery of hidden messages, referred as reversals, innormal speech. Works are in progress for exploiting the relevance of RS indifferent real world applications such as investigation, medical field etc. Inthis paper we represent an innovative method for preparing a reliable SoftwareRequirement Specification (SRS) document with the help of reverse speech. AsSRS act as the backbone for the successful completion of any project, areliable method is needed to overcome the inconsistencies. Using RS such areliable method for SRS documentation was developed.
arxiv-5100-8 | Squeezing bottlenecks: exploring the limits of autoencoder semantic representation capabilities | http://arxiv.org/pdf/1402.3070v1.pdf | author:Parth Gupta, Rafael E. Banchs, Paolo Rosso category:cs.IR cs.LG stat.ML published:2014-02-13 summary:We present a comprehensive study on the use of autoencoders for modellingtext data, in which (differently from previous studies) we focus our attentionon the following issues: i) we explore the suitability of two different modelsbDA and rsDA for constructing deep autoencoders for text data at the sentencelevel; ii) we propose and evaluate two novel metrics for better assessing thetext-reconstruction capabilities of autoencoders; and iii) we propose anautomatic method to find the critical bottleneck dimensionality for textlanguage representations (below which structural information is lost).
arxiv-5100-9 | Event Structure of Transitive Verb: A MARVS perspective | http://arxiv.org/pdf/1402.3040v1.pdf | author:Jia-Fei Hong, Kathleen Ahrens, Chu-Ren Huang category:cs.CL published:2014-02-13 summary:Module-Attribute Representation of Verbal Semantics (MARVS) is a theory ofthe representation of verbal semantics that is based on Mandarin Chinese data(Huang et al. 2000). In the MARVS theory, there are two different types ofmodules: Event Structure Modules and Role Modules. There are also two sets ofattributes: Event-Internal Attributes and Role-Internal Attributes, which arelinked to the Event Structure Module and the Role Module, respectively. In thisstudy, we focus on four transitive verbs as chi1(eat), wan2(play),huan4(change) and shao1(burn) and explore their event structures by the MARVStheory.
arxiv-5100-10 | Regularization for Multiple Kernel Learning via Sum-Product Networks | http://arxiv.org/pdf/1402.3032v1.pdf | author:Ziming Zhang category:stat.ML cs.LG published:2014-02-13 summary:In this paper, we are interested in constructing general graph-basedregularizers for multiple kernel learning (MKL) given a structure which is usedto describe the way of combining basis kernels. Such structures are representedby sum-product networks (SPNs) in our method. Accordingly we propose a newconvex regularization method for MLK based on a path-dependent kernel weightingfunction which encodes the entire SPN structure in our method. Under certainconditions and from the view of probability, this function can be considered tofollow multinomial distributions over the weights associated with product nodesin SPNs. We also analyze the convexity of our regularizer and the complexity ofour induced classifiers, and further propose an efficient wrapper algorithm tooptimize our formulation. In our experiments, we apply our method to ......
arxiv-5100-11 | Prediction with Missing Data via Bayesian Additive Regression Trees | http://arxiv.org/pdf/1306.0618v3.pdf | author:Adam Kapelner, Justin Bleich category:stat.ML cs.LG published:2013-06-03 summary:We present a method for incorporating missing data in non-parametricstatistical learning without the need for imputation. We focus on a tree-basedmethod, Bayesian Additive Regression Trees (BART), enhanced with "MissingnessIncorporated in Attributes," an approach recently proposed incorporatingmissingness into decision trees (Twala, 2008). This procedure takes advantageof the partitioning mechanisms found in tree-based models. Simulations ongenerated models and real data indicate that our proposed method can forecastwell on complicated missing-at-random and not-missing-at-random models as wellas models where missingness itself influences the response. Our procedure hashigher predictive performance and is more stable than competitors in manycases. We also illustrate BART's abilities to incorporate missingness intouncertainty intervals and to detect the influence of missingness on the modelfit.
arxiv-5100-12 | Local Optima Networks: A New Model of Combinatorial Fitness Landscapes | http://arxiv.org/pdf/1402.2959v1.pdf | author:Gabriela Ochoa, Sébastien Verel, Fabio Daolio, Marco Tomassini category:cs.NE cs.AI published:2014-02-12 summary:This chapter overviews a recently introduced network-based model ofcombinatorial landscapes: Local Optima Networks (LON). The model compresses theinformation given by the whole search space into a smaller mathematical objectthat is a graph having as vertices the local optima and as edges the possibleweighted transitions between them. Two definitions of edges have been proposed:basin-transition and escape-edges, which capture relevant topological featuresof the underlying search spaces. This network model brings a new set of metricsto characterize the structure of combinatorial landscapes, those associatedwith the science of complex networks. These metrics are described, and resultsare presented of local optima network extraction and analysis for two selectedcombinatorial landscapes: NK landscapes and the quadratic assignment problem.Network features are found to correlate with and even predict the performanceof heuristic search algorithms operating on these problems.
arxiv-5100-13 | Non-negative least squares for high-dimensional linear models: consistency and sparse recovery without regularization | http://arxiv.org/pdf/1205.0953v2.pdf | author:Martin Slawski, Matthias Hein category:math.ST stat.ML stat.TH published:2012-05-04 summary:Least squares fitting is in general not useful for high-dimensional linearmodels, in which the number of predictors is of the same or even larger orderof magnitude than the number of samples. Theory developed in recent years hascoined a paradigm according to which sparsity-promoting regularization isregarded as a necessity in such setting. Deviating from this paradigm, we showthat non-negativity constraints on the regression coefficients may be similarlyeffective as explicit regularization if the design matrix has additionalproperties, which are met in several applications of non-negative least squares(NNLS). We show that for these designs, the performance of NNLS with regard toprediction and estimation is comparable to that of the lasso. We argue furtherthat in specific cases, NNLS may have a better $\ell_{\infty}$-rate inestimation and hence also advantages with respect to support recovery whencombined with thresholding. From a practical point of view, NNLS does notdepend on a regularization parameter and is hence easier to use.
arxiv-5100-14 | Probabilistic Solutions to Differential Equations and their Application to Riemannian Statistics | http://arxiv.org/pdf/1306.0308v2.pdf | author:Philipp Hennig, Søren Hauberg category:stat.ML cs.LG math.NA published:2013-06-03 summary:We study a probabilistic numerical method for the solution of both boundaryand initial value problems that returns a joint Gaussian process posterior overthe solution. Such methods have concrete value in the statistics on Riemannianmanifolds, where non-analytic ordinary differential equations are involved invirtually all computations. The probabilistic formulation permits marginalisingthe uncertainty of the numerical solution such that statistics are lesssensitive to inaccuracies. This leads to new Riemannian algorithms for meanvalue computations and principal geodesic analysis. Marginalisation also meansresults can be less precise than point estimates, enabling a noticeablespeed-up over the state of the art. Our approach is an argument for a widerpoint that uncertainty caused by numerical calculations should be trackedthroughout the pipeline of machine learning algorithms.
arxiv-5100-15 | PR2: A Language Independent Unsupervised Tool for Personality Recognition from Text | http://arxiv.org/pdf/1402.2796v1.pdf | author:Fabio Celli, Massimo Poesio category:cs.CL published:2014-02-12 summary:We present PR2, a personality recognition system available online, thatperforms instance-based classification of Big5 personality types fromunstructured text, using language-independent features. It has been tested onEnglish and Italian, achieving performances up to f=.68.
arxiv-5100-16 | Ellipsoidal Rounding for Nonnegative Matrix Factorization Under Noisy Separability | http://arxiv.org/pdf/1309.5701v2.pdf | author:Tomohiko Mizutani category:stat.ML published:2013-09-23 summary:We present a numerical algorithm for nonnegative matrix factorization (NMF)problems under noisy separability. An NMF problem under separability can bestated as one of finding all vertices of the convex hull of data points. Theresearch interest of this paper is to find the vectors as close to the verticesas possible in a situation in which noise is added to the data points. Ouralgorithm is designed to capture the shape of the convex hull of data points byusing its enclosing ellipsoid. We show that the algorithm has correctness androbustness properties from theoretical and practical perspectives; correctnesshere means that if the data points do not contain any noise, the algorithm canfind the vertices of their convex hull; robustness means that if the datapoints contain noise, the algorithm can find the near-vertices. Finally, weapply the algorithm to document clustering, and report the experimentalresults.
arxiv-5100-17 | Relaxed Sparse Eigenvalue Conditions for Sparse Estimation via Non-convex Regularized Regression | http://arxiv.org/pdf/1306.3343v3.pdf | author:Zheng Pan, Changshui Zhang category:cs.LG cs.NA stat.ML published:2013-06-14 summary:Non-convex regularizers usually improve the performance of sparse estimationin practice. To prove this fact, we study the conditions of sparse estimationsfor the sharp concave regularizers which are a general family of non-convexregularizers including many existing regularizers. For the global solutions ofthe regularized regression, our sparse eigenvalue based conditions are weakerthan that of L1-regularization for parameter estimation and sparsenessestimation. For the approximate global and approximate stationary (AGAS)solutions, almost the same conditions are also enough. We show that the desiredAGAS solutions can be obtained by coordinate descent (CD) based methods.Finally, we perform some experiments to show the performance of CD methods ongiving AGAS solutions and the degree of weakness of the estimation conditionsrequired by the sharp concave regularizers.
arxiv-5100-18 | Multi-Task Policy Search | http://arxiv.org/pdf/1307.0813v2.pdf | author:Marc Peter Deisenroth, Peter Englert, Jan Peters, Dieter Fox category:stat.ML cs.AI cs.LG cs.RO published:2013-07-02 summary:Learning policies that generalize across multiple tasks is an important andchallenging research topic in reinforcement learning and robotics. Trainingindividual policies for every single potential task is often impractical,especially for continuous task variations, requiring more principled approachesto share and transfer knowledge among similar tasks. We present a novelapproach for learning a nonlinear feedback policy that generalizes acrossmultiple tasks. The key idea is to define a parametrized policy as a functionof both the state and the task, which allows learning a single policy thatgeneralizes across multiple known and unknown tasks. Applications of our novelapproach to reinforcement and imitation learning in real-robot experiments areshown.
arxiv-5100-19 | A Survey on Metric Learning for Feature Vectors and Structured Data | http://arxiv.org/pdf/1306.6709v4.pdf | author:Aurélien Bellet, Amaury Habrard, Marc Sebban category:cs.LG stat.ML published:2013-06-28 summary:The need for appropriate ways to measure the distance or similarity betweendata is ubiquitous in machine learning, pattern recognition and data mining,but handcrafting such good metrics for specific problems is generallydifficult. This has led to the emergence of metric learning, which aims atautomatically learning a metric from data and has attracted a lot of interestin machine learning and related fields for the past ten years. This surveypaper proposes a systematic review of the metric learning literature,highlighting the pros and cons of each approach. We pay particular attention toMahalanobis distance metric learning, a well-studied and successful framework,but additionally present a wide range of methods that have recently emerged aspowerful alternatives, including nonlinear metric learning, similarity learningand local metric learning. Recent trends and extensions, such assemi-supervised metric learning, metric learning for histogram data and thederivation of generalization guarantees, are also covered. Finally, this surveyaddresses metric learning for structured data, in particular edit distancelearning, and attempts to give an overview of the remaining challenges inmetric learning for the years to come.
arxiv-5100-20 | Bayesian Inference with Posterior Regularization and applications to Infinite Latent SVMs | http://arxiv.org/pdf/1210.1766v3.pdf | author:Jun Zhu, Ning Chen, Eric P. Xing category:cs.LG cs.AI stat.ME stat.ML published:2012-10-05 summary:Existing Bayesian models, especially nonparametric Bayesian methods, rely onspecially conceived priors to incorporate domain knowledge for discoveringimproved latent representations. While priors can affect posteriordistributions through Bayes' rule, imposing posterior regularization isarguably more direct and in some cases more natural and general. In this paper,we present regularized Bayesian inference (RegBayes), a novel computationalframework that performs posterior inference with a regularization term on thedesired post-data posterior distribution under an information theoreticalformulation. RegBayes is more flexible than the procedure that elicits expertknowledge via priors, and it covers both directed Bayesian networks andundirected Markov networks whose Bayesian formulation results in hybrid chaingraph models. When the regularization is induced from a linear operator on theposterior distributions, such as the expectation operator, we present a generalconvex-analysis theorem to characterize the solution of RegBayes. Furthermore,we present two concrete examples of RegBayes, infinite latent support vectormachines (iLSVM) and multi-task infinite latent support vector machines(MT-iLSVM), which explore the large-margin idea in combination with anonparametric Bayesian model for discovering predictive latent features forclassification and multi-task learning, respectively. We present efficientinference methods and report empirical studies on several benchmark datasets,which appear to demonstrate the merits inherited from both large-marginlearning and Bayesian nonparametrics. Such results were not available untilnow, and contribute to push forward the interface between these two importantsubfields, which have been largely treated as isolated in the community.
arxiv-5100-21 | Noise Analysis for Lensless Compressive Imaging | http://arxiv.org/pdf/1402.2720v1.pdf | author:Hong Jiang, Gang Huang, Paul Wilford category:cs.CV published:2014-02-12 summary:We analyze the signal to noise ratio (SNR) in a recently proposed lenslesscompressive imaging architecture. The architecture consists of a sensor of asingle detector element and an aperture assembly of an array of apertureelements, each of which has a programmable transmittance. This lenslesscompressive imaging architecture can be used in conjunction with compressivesensing to capture images in a compressed form of compressive measurements. Inthis paper, we perform noise analysis of this lensless compressive imagingarchitecture and compare it with pinhole aperture imaging and lens apertureimaging. We will show that the SNR in the lensless compressive imaging isindependent of the image resolution, while that in either pinhole apertureimaging or lens aperture imaging decreases as the image resolution increases.Consequently, the SNR in the lensless compressive imaging can be much higher ifthe image resolution is large enough.
arxiv-5100-22 | Sex as Gibbs Sampling: a probability model of evolution | http://arxiv.org/pdf/1402.2704v1.pdf | author:Chris Watkins, Yvonne Buttkewitz category:q-bio.PE cs.NE published:2014-02-12 summary:We show that evolutionary computation can be implemented as standardMarkov-chain Monte-Carlo (MCMC) sampling. With some care, `genetic algorithms'can be constructed that are reversible Markov chains that satisfy detailedbalance; it follows that the stationary distribution of populations is a Gibbsdistribution in a simple factorised form. For some standard and popularnonparametric probability models, we exhibit Gibbs-sampling procedures that areplausible genetic algorithms. At mutation-selection equilibrium, a populationof genomes is analogous to a sample from a Bayesian posterior, and the genomesare analogous to latent variables. We suggest this is a general, tractable, andinsightful formulation of evolutionary computation in terms of standard machinelearning concepts and techniques. In addition, we show that evolutionary processes in which selection acts bydifferences in fecundity are not reversible, and also that it is not possibleto construct reversible evolutionary models in which each child is produced byonly two parents.
arxiv-5100-23 | An Extensive Experimental Study on the Cluster-based Reference Set Reduction for speeding-up the k-NN Classifier | http://arxiv.org/pdf/1309.7750v2.pdf | author:Stefanos Ougiaroglou, Georgios Evangelidis, Dimitris A. Dervos category:cs.LG published:2013-09-30 summary:The k-Nearest Neighbor (k-NN) classification algorithm is one of the mostwidely-used lazy classifiers because of its simplicity and ease ofimplementation. It is considered to be an effective classifier and has manyapplications. However, its major drawback is that when sequential search isused to find the neighbors, it involves high computational cost. Speeding-upk-NN search is still an active research field. Hwang and Cho have recentlyproposed an adaptive cluster-based method for fast Nearest Neighbor searching.The effectiveness of this method is based on the adjustment of threeparameters. However, the authors evaluated their method by setting specificparameter values and using only one dataset. In this paper, an extensiveexperimental study of this method is presented. The results, which are based onfive real life datasets, illustrate that if the parameters of the method arecarefully defined, one can achieve even better classification performance.
arxiv-5100-24 | Real-Time Hand Shape Classification | http://arxiv.org/pdf/1402.2673v1.pdf | author:Jakub Nalepa, Michal Kawulok category:cs.CV published:2014-02-11 summary:The problem of hand shape classification is challenging since a hand ischaracterized by a large number of degrees of freedom. Numerous shapedescriptors have been proposed and applied over the years to estimate andclassify hand poses in reasonable time. In this paper we discuss our parallelframework for real-time hand shape classification applicable in real-timeapplications. We show how the number of gallery images influences theclassification accuracy and execution time of the parallel algorithm. Wepresent the speedup and efficiency analyses that prove the efficacy of theparallel implementation. Noteworthy, different methods can be used at each stepof our parallel framework. Here, we combine the shape contexts with theappearance-based techniques to enhance the robustness of the algorithm and toincrease the classification score. An extensive experimental study proves thesuperiority of the proposed approach over existing state-of-the-art methods.
arxiv-5100-25 | On Zeroth-Order Stochastic Convex Optimization via Random Walks | http://arxiv.org/pdf/1402.2667v1.pdf | author:Tengyuan Liang, Hariharan Narayanan, Alexander Rakhlin category:cs.LG stat.ML published:2014-02-11 summary:We propose a method for zeroth order stochastic convex optimization thatattains the suboptimality rate of $\tilde{\mathcal{O}}(n^{7}T^{-1/2})$ after$T$ queries for a convex bounded function $f:{\mathbb R}^n\to{\mathbb R}$. Themethod is based on a random walk (the \emph{Ball Walk}) on the epigraph of thefunction. The randomized approach circumvents the problem of gradientestimation, and appears to be less sensitive to noisy function evaluationscompared to noiseless zeroth order methods.
arxiv-5100-26 | A Fast Two Pass Multi-Value Segmentation Algorithm based on Connected Component Analysis | http://arxiv.org/pdf/1402.2606v1.pdf | author:Dibyendu Mukherjee category:cs.CV published:2014-02-11 summary:Connected component analysis (CCA) has been heavily used to label binaryimages and classify segments. However, it has not been well-exploited tosegment multi-valued natural images. This work proposes a novel multi-valuesegmentation algorithm that utilizes CCA to segment color images. A userdefined distance measure is incorporated in the proposed modified CCA toidentify and segment similar image regions. The raw output of the algorithmconsists of distinctly labelled segmented regions. The proposed algorithm has aunique design architecture that provides several benefits: 1) it can be used tosegment any multi-channel multi-valued image; 2) the distancemeasure/segmentation criteria can be application-specific and 3) an absolutelinear-time implementation allows easy extension for real-time videosegmentation. Experimental demonstrations of the aforesaid benefits arepresented along with the comparison results on multiple datasets with currentbenchmark algorithms. A number of possible application areas are alsoidentified and results on real-time video segmentation has been presented toshow the promise of the proposed method.
arxiv-5100-27 | Online Nonparametric Regression | http://arxiv.org/pdf/1402.2594v1.pdf | author:Alexander Rakhlin, Karthik Sridharan category:stat.ML cs.LG math.ST stat.TH published:2014-02-11 summary:We establish optimal rates for online regression for arbitrary classes ofregression functions in terms of the sequential entropy introduced in (Rakhlin,Sridharan, Tewari, 2010). The optimal rates are shown to exhibit a phasetransition analogous to the i.i.d./statistical learning case, studied in(Rakhlin, Sridharan, Tsybakov 2013). In the frequently encountered situationwhen sequential entropy and i.i.d. empirical entropy match, our results pointto the interesting phenomenon that the rates for statistical learning withsquared loss and online nonparametric regression are the same. In addition to a non-algorithmic study of minimax regret, we exhibit ageneric forecaster that enjoys the established optimal rates. We also provide arecipe for designing online regression algorithms that can be computationallyefficient. We illustrate the techniques by deriving existing and newforecasters for the case of finite experts and for online linear regression.
arxiv-5100-28 | Sequentially Generated Instance-Dependent Image Representations for Classification | http://arxiv.org/pdf/1312.6594v3.pdf | author:Gabriel Dulac-Arnold, Ludovic Denoyer, Nicolas Thome, Matthieu Cord, Patrick Gallinari category:cs.CV cs.LG published:2013-12-20 summary:In this paper, we investigate a new framework for image classification thatadaptively generates spatial representations. Our strategy is based on asequential process that learns to explore the different regions of any image inorder to infer its category. In particular, the choice of regions is specificto each image, directed by the actual content of previously selectedregions.The capacity of the system to handle incomplete image information aswell as its adaptive region selection allow the system to perform well inbudgeted classification tasks by exploiting a dynamicly generatedrepresentation of each image. We demonstrate the system's abilities in a seriesof image-based exploration and classification tasks that highlight its learnedexploration and inference abilities.
arxiv-5100-29 | Realtime Multilevel Crowd Tracking using Reciprocal Velocity Obstacles | http://arxiv.org/pdf/1402.2826v1.pdf | author:Aniket Bera, Dinesh Manocha category:cs.CV published:2014-02-11 summary:We present a novel, realtime algorithm to compute the trajectory of eachpedestrian in moderately dense crowd scenes. Our formulation is based on anadaptive particle filtering scheme that uses a multi-agent motion model basedon velocity-obstacles, and takes into account local interactions as well asphysical and personal constraints of each pedestrian. Our method dynamicallychanges the number of particles allocated to each pedestrian based on differentconfidence metrics. Additionally, we use a new high-definition crowd videodataset, which is used to evaluate the performance of different pedestriantracking algorithms. This dataset consists of videos of indoor and outdoorscenes, recorded at different locations with 30-80 pedestrians. We highlightthe performance benefits of our algorithm over prior techniques using thisdataset. In practice, our algorithm can compute trajectories of tens ofpedestrians on a multi-core desktop CPU at interactive rates (27-30 frames persecond). To the best of our knowledge, our approach is 4-5 times faster thanprior methods, which provide similar accuracy.
arxiv-5100-30 | Justifying Information-Geometric Causal Inference | http://arxiv.org/pdf/1402.2499v1.pdf | author:Dominik Janzing, Bastian Steudel, Naji Shajarisales, Bernhard Schölkopf category:stat.ML published:2014-02-11 summary:Information Geometric Causal Inference (IGCI) is a new approach todistinguish between cause and effect for two variables. It is based on anindependence assumption between input distribution and causal mechanism thatcan be phrased in terms of orthogonality in information space. We describe twointuitive reinterpretations of this approach that makes IGCI more accessible toa broader audience. Moreover, we show that the described independence is related to thehypothesis that unsupervised learning and semi-supervised learning only worksfor predicting the cause from the effect and not vice versa.
arxiv-5100-31 | An evaluation of keyword extraction from online communication for the characterisation of social relations | http://arxiv.org/pdf/1402.2427v1.pdf | author:Jan Hauffa, Tobias Lichtenberg, Georg Groh category:cs.SI cs.CL cs.IR published:2014-02-11 summary:The set of interpersonal relationships on a social network service or asimilar online community is usually highly heterogenous. The concept of tiestrength captures only one aspect of this heterogeneity. Since the unstructuredtext content of online communication artefacts is a salient source ofinformation about a social relationship, we investigate the utility of keywordsextracted from the message body as a representation of the relationship'scharacteristics as reflected by the conversation topics. Keyword extraction isperformed using standard natural language processing methods. Communicationdata and human assessments of the extracted keywords are obtained from Facebookusers via a custom application. The overall positive quality assessmentprovides evidence that the keywords indeed convey relevant information aboutthe relationship.
arxiv-5100-32 | Imaging with Rays: Microscopy, Medical Imaging, and Computer Vision | http://arxiv.org/pdf/1402.2426v1.pdf | author:Keith Dillon, Yeshaiahu Fainman category:cs.CV published:2014-02-11 summary:In this paper we broadly consider techniques which utilize projections onrays for data collection, with particular emphasis on optical techniques. Weformulate a variety of imaging techniques as either special cases or extensionsof tomographic reconstruction. We then consider how the techniques must beextended to describe objects containing occlusion, as with a self-occludingopaque object. We formulate the reconstruction problem as a regularizednonlinear optimization problem to simultaneously solve for object brightnessand attenuation, where the attenuation can become infinite. We demonstratevarious simulated examples for imaging opaque objects, including sparse pointsources, a conventional multiview reconstruction technique, and asuper-resolving technique which exploits occlusion to resolve an image.
arxiv-5100-33 | Kernel Least Mean Square with Adaptive Kernel Size | http://arxiv.org/pdf/1401.5899v3.pdf | author:Badong Chen, Junli Liang, Nanning Zheng, Jose C. Principe category:stat.ML cs.LG published:2014-01-23 summary:Kernel adaptive filters (KAF) are a class of powerful nonlinear filtersdeveloped in Reproducing Kernel Hilbert Space (RKHS). The Gaussian kernel isusually the default kernel in KAF algorithms, but selecting the proper kernelsize (bandwidth) is still an open important issue especially for learning withsmall sample sizes. In previous research, the kernel size was set manually orestimated in advance by Silvermans rule based on the sample distribution. Thisstudy aims to develop an online technique for optimizing the kernel size of thekernel least mean square (KLMS) algorithm. A sequential optimization strategyis proposed, and a new algorithm is developed, in which the filter weights andthe kernel size are both sequentially updated by stochastic gradient algorithmsthat minimize the mean square error (MSE). Theoretical results on convergenceare also presented. The excellent performance of the new algorithm is confirmedby simulations on static function estimation and short term chaotic time seriesprediction.
arxiv-5100-34 | Animation of 3D Human Model Using Markerless Motion Capture Applied To Sports | http://arxiv.org/pdf/1402.2363v1.pdf | author:Ashish Shingade, Archana Ghotkar category:cs.GR cs.CV published:2014-02-11 summary:Markerless motion capture is an active research in 3D virtualization. Inproposed work we presented a system for markerless motion capture for 3D humancharacter animation, paper presents a survey on motion and skeleton trackingtechniques which are developed or are under development. The paper proposed amethod to transform the motion of a performer to a 3D human character (model),the 3D human character performs similar movements as that of a performer inreal time. In the proposed work, human model data will be captured by Kinectcamera, processed data will be applied on 3D human model for animation. 3Dhuman model is created using open source software (MakeHuman). Anticipateddataset for sport activity is considered as input which can be applied to anyHCI application.
arxiv-5100-35 | Learning-assisted Theorem Proving with Millions of Lemmas | http://arxiv.org/pdf/1402.3578v1.pdf | author:Cezary Kaliszyk, Josef Urban category:cs.AI cs.DL cs.LG cs.LO published:2014-02-11 summary:Large formal mathematical libraries consist of millions of atomic inferencesteps that give rise to a corresponding number of proved statements (lemmas).Analogously to the informal mathematical practice, only a tiny fraction of suchstatements is named and re-used in later proofs by formal mathematicians. Inthis work, we suggest and implement criteria defining the estimated usefulnessof the HOL Light lemmas for proving further theorems. We use these criteria tomine the large inference graph of the lemmas in the HOL Light and Flyspecklibraries, adding up to millions of the best lemmas to the pool of statementsthat can be re-used in later proofs. We show that in combination withlearning-based relevance filtering, such methods significantly strengthenautomated theorem proving of new conjectures over large formal mathematicallibraries such as Flyspeck.
arxiv-5100-36 | Sparsity averaging for radio-interferometric imaging | http://arxiv.org/pdf/1402.2335v1.pdf | author:Rafael E. Carrillo, Jason D. McEwen, Yves Wiaux category:astro-ph.IM cs.CV published:2014-02-11 summary:We propose a novel regularization method for compressive imaging in thecontext of the compressed sensing (CS) theory with coherent and redundantdictionaries. Natural images are often complicated and several types ofstructures can be present at once. It is well known that piecewise smoothimages exhibit gradient sparsity, and that images with extended structures arebetter encapsulated in wavelet frames. Therefore, we here conjecture thatpromoting average sparsity or compressibility over multiple frames rather thansingle frames is an extremely powerful regularization prior.
arxiv-5100-37 | Modeling sequential data using higher-order relational features and predictive training | http://arxiv.org/pdf/1402.2333v1.pdf | author:Vincent Michalski, Roland Memisevic, Kishore Konda category:cs.LG cs.CV stat.ML published:2014-02-10 summary:Bi-linear feature learning models, like the gated autoencoder, were proposedas a way to model relationships between frames in a video. By minimizingreconstruction error of one frame, given the previous frame, these models learn"mapping units" that encode the transformations inherent in a sequence, andthereby learn to encode motion. In this work we extend bi-linear models byintroducing "higher-order mapping units" that allow us to encodetransformations between frames and transformations between transformations. We show that this makes it possible to encode temporal structure that is morecomplex and longer-range than the structure captured within standard bi-linearmodels. We also show that a natural way to train the model is by replacing thecommonly used reconstruction objective with a prediction objective which forcesthe model to correctly predict the evolution of the input multiple steps intothe future. Learning can be achieved by back-propagating the multi-stepprediction through time. We test the model on various temporal predictiontasks, and show that higher-order mappings and predictive training both yield asignificant improvement over bi-linear models in terms of prediction accuracy.
arxiv-5100-38 | Feature and Variable Selection in Classification | http://arxiv.org/pdf/1402.2300v1.pdf | author:Aaron Karper category:cs.LG cs.AI stat.ML published:2014-02-10 summary:The amount of information in the form of features and variables avail- ableto machine learning algorithms is ever increasing. This can lead to classifiersthat are prone to overfitting in high dimensions, high di- mensional models donot lend themselves to interpretable results, and the CPU and memory resourcesnecessary to run on high-dimensional datasets severly limit the applications ofthe approaches. Variable and feature selection aim to remedy this by finding asubset of features that in some way captures the information provided best. Inthis paper we present the general methodology and highlight some specificapproaches.
arxiv-5100-39 | Étude cognitive des processus de construction d'une requête dans un système de gestion de connaissances médicales | http://arxiv.org/pdf/1402.2562v1.pdf | author:Nathalie Chaignaud, Valérie Delavigne, Maryvonne Holzem, Jean-Philippe Kotowicz, Alain Loisel category:cs.IR cs.CL published:2014-02-10 summary:This article presents the Cogni-CISMeF project, which aims at improvingmedical information search in the CISMeF system (Catalog and Index ofFrench-language health resources) by including a conversational agent tointeract with the user in natural language. To study the cognitive processesinvolved during the information search, a bottom-up methodology was adopted.Experimentation has been set up to obtain human dialogs between a user (playingthe role of patient) dealing with medical information search and a CISMeFexpert refining the request. The analysis of these dialogs underlined the useof discursive evidence: vocabulary, reformulation, implicit or explicitexpression of user intentions, conversational sequences, etc. A model ofartificial agent is proposed. It leads the user in its information search byproposing to him examples, assistance and choices. This model was implementedand integrated in the CISMeF system. ---- Cet article d\'ecrit le projetCogni-CISMeF qui propose un module de dialogue Homme-Machine \`a int\'egrerdans le syst\`eme d'indexation de connaissances m\'edicales CISMeF (Catalogueet Index des Sites M\'edicaux Francophones). Nous avons adopt\'e une d\'emarchede mod\'elisation cognitive en proc\'edant \`a un recueil de corpus dedialogues entre un utilisateur (jouant le r\^ole d'un patient) d\'esirant uneinformation m\'edicale et un expert CISMeF af inant cette demande pourconstruire la requ\^ete. Nous avons analys\'e la structure des dialogues ainsiobtenus et avons \'etudi\'e un certain nombre d'indices discursifs :vocabulaire employ\'e, marques de reformulation, commentaires m\'eta et\'epilinguistiques, expression implicite ou explicite des intentions del'utilisateur, encha\^inement conversationnel, etc. De cette analyse, nousavons construit un mod\`ele d'agent artificiel dot\'e de capacit\'es cognitivescapables d'aider l'utilisateur dans sa t\^ache de recherche d'information. Cemod\`ele a \'et\'e impl\'ement\'e et int\'egr\'e dans le syst\`eme CISMeF.
arxiv-5100-40 | B-tests: Low Variance Kernel Two-Sample Tests | http://arxiv.org/pdf/1307.1954v3.pdf | author:Wojciech Zaremba, Arthur Gretton, Matthew Blaschko category:cs.LG stat.ML published:2013-07-08 summary:A family of maximum mean discrepancy (MMD) kernel two-sample tests isintroduced. Members of the test family are called Block-tests or B-tests, sincethe test statistic is an average over MMDs computed on subsets of the samples.The choice of block size allows control over the tradeoff between test powerand computation time. In this respect, the $B$-test family combines favorableproperties of previously proposed MMD two-sample tests: B-tests are morepowerful than a linear time test where blocks are just pairs of samples, yetthey are more computationally efficient than a quadratic time test where asingle large block incorporating all the samples is used to compute aU-statistic. A further important advantage of the B-tests is theirasymptotically Normal null distribution: this is by contrast with theU-statistic, which is degenerate under the null hypothesis, and for whichestimates of the null distribution are computationally demanding. Recentresults on kernel selection for hypothesis testing transfer seamlessly to theB-tests, yielding a means to optimize test power via kernel choice.
arxiv-5100-41 | Unsupervised edge map scoring: a statistical complexity approach | http://arxiv.org/pdf/1302.5186v2.pdf | author:Javier Gimenez, Jorge Martinez, Ana Georgina Flesia category:cs.CV stat.AP published:2013-02-21 summary:We propose a new Statistical Complexity Measure (SCM) to qualify edge mapswithout Ground Truth (GT) knowledge. The measure is the product of two indices,an \emph{Equilibrium} index $\mathcal{E}$ obtained by projecting the edge mapinto a family of edge patterns, and an \emph{Entropy} index $\mathcal{H}$,defined as a function of the Kolmogorov Smirnov (KS) statistic. This new measure can be used for performance characterization which includes:(i)~the specific evaluation of an algorithm (intra-technique process) in orderto identify its best parameters, and (ii)~the comparison of differentalgorithms (inter-technique process) in order to classify them according totheir quality. Results made over images of the South Florida and Berkeley databases showthat our approach significantly improves over Pratt's Figure of Merit (PFoM)which is the objective reference-based edge map evaluation standard, as ittakes into account more features in its evaluation.
arxiv-5100-42 | Image Search Reranking | http://arxiv.org/pdf/1402.2232v1.pdf | author:V Rajakumar, Vipeen V Bopche category:cs.IR cs.CV published:2014-02-10 summary:The existing methods for image search reranking suffer from theunfaithfulness of the assumptions under which the text-based images searchresult. The resulting images contain more irrelevant images. Hence the reranking concept arises to re rank the retrieved images based on the text aroundthe image and data of data of image and visual feature of image. A number ofmethods are differentiated for this re-ranking. The high ranked images are usedas noisy data and a k means algorithm for classification is learned to rectifythe ranking further. We are study the affect ability of the cross validationmethod to this training data. The pre eminent originality of the overall methodis in collecting text/metadata of image and visual features in order to achievean automatic ranking of the images. Supervision is initiated to learn the modelweights offline, previous to reranking process. While model learning needsmanual labeling of the results for a some limited queries, the resulting modelis query autonomous and therefore applicable to any other query .Examples aregiven for a selection of other classes like vehicles, animals and otherclasses.
arxiv-5100-43 | Characterizing the Sample Complexity of Private Learners | http://arxiv.org/pdf/1402.2224v1.pdf | author:Amos Beimel, Kobbi Nissim, Uri Stemmer category:cs.CR cs.LG published:2014-02-10 summary:In 2008, Kasiviswanathan et al. defined private learning as a combination ofPAC learning and differential privacy. Informally, a private learner is appliedto a collection of labeled individual information and outputs a hypothesiswhile preserving the privacy of each individual. Kasiviswanathan et al. gave ageneric construction of private learners for (finite) concept classes, withsample complexity logarithmic in the size of the concept class. This samplecomplexity is higher than what is needed for non-private learners, henceleaving open the possibility that the sample complexity of private learning maybe sometimes significantly higher than that of non-private learning. We give a combinatorial characterization of the sample size sufficient andnecessary to privately learn a class of concepts. This characterization isanalogous to the well known characterization of the sample complexity ofnon-private learning in terms of the VC dimension of the concept class. Weintroduce the notion of probabilistic representation of a concept class, andour new complexity measure RepDim corresponds to the size of the smallestprobabilistic representation of the concept class. We show that any private learning algorithm for a concept class C with samplecomplexity m implies RepDim(C)=O(m), and that there exists a private learningalgorithm with sample complexity m=O(RepDim(C)). We further demonstrate that asimilar characterization holds for the database size needed for privatelycomputing a large class of optimization problems and also for the well studiedproblem of private data release.
arxiv-5100-44 | Handwritten Character Recognition In Malayalam Scripts- A Review | http://arxiv.org/pdf/1402.2188v1.pdf | author:Anitha Mary M. O. Chacko, P. M Dhanya category:cs.CV published:2014-02-10 summary:Handwritten character recognition is one of the most challenging and ongoingareas of research in the field of pattern recognition. HCR research is maturedfor foreign languages like Chinese and Japanese but the problem is much morecomplex for Indian languages. The problem becomes even more complicated forSouth Indian languages due to its large character set and the presence ofvowels modifiers and compound characters. This paper provides an overview ofimportant contributions and advances in offline as well as online handwrittencharacter recognition of Malayalam scripts.
arxiv-5100-45 | An Algorithmic Framework for Computing Validation Performance Bounds by Using Suboptimal Models | http://arxiv.org/pdf/1402.2148v1.pdf | author:Yoshiki Suzuki, Kohei Ogawa, Yuki Shinmura, Ichiro Takeuchi category:stat.ML published:2014-02-10 summary:Practical model building processes are often time-consuming because manydifferent models must be trained and validated. In this paper, we introduce anovel algorithm that can be used for computing the lower and the upper boundsof model validation errors without actually training the model itself. A keyidea behind our algorithm is using a side information available from asuboptimal model. If a reasonably good suboptimal model is available, ouralgorithm can compute lower and upper bounds of many useful quantities formaking inferences on the unknown target model. We demonstrate the advantage ofour algorithm in the context of model selection for regularized learningproblems.
arxiv-5100-46 | Learning to encode motion using spatio-temporal synchrony | http://arxiv.org/pdf/1306.3162v3.pdf | author:Kishore Reddy Konda, Roland Memisevic, Vincent Michalski category:cs.CV cs.LG stat.ML published:2013-06-13 summary:We consider the task of learning to extract motion from videos. To this end,we show that the detection of spatial transformations can be viewed as thedetection of synchrony between the image sequence and a sequence of featuresundergoing the motion we wish to detect. We show that learning about synchronyis possible using very fast, local learning rules, by introducingmultiplicative "gating" interactions between hidden units across frames. Thismakes it possible to achieve competitive performance in a wide variety ofmotion estimation tasks, using a small fraction of the time required to learnfeatures, and to outperform hand-crafted spatio-temporal features by a largemargin. We also show how learning about synchrony can be viewed as performinggreedy parameter estimation in the well-known motion energy model.
arxiv-5100-47 | Signal Reconstruction Framework Based On Projections Onto Epigraph Set Of A Convex Cost Function (PESC) | http://arxiv.org/pdf/1402.2088v1.pdf | author:Mohammad Tofighi, Kivanc Kose, A. Enis Cetin category:math.OC cs.CV published:2014-02-10 summary:A new signal processing framework based on making orthogonal Projections ontothe Epigraph Set of a Convex cost function (PESC) is developed. In this way itis possible to solve convex optimization problems using the well-knownProjections onto Convex Set (POCS) approach. In this algorithm, the dimensionof the minimization problem is lifted by one and a convex set corresponding tothe epigraph of the cost function is defined. If the cost function is a convexfunction in $R^N$, the corresponding epigraph set is also a convex set inR^{N+1}. The PESC method provides globally optimal solutions fortotal-variation (TV), filtered variation (FV), L_1, L_2, and entropic costfunction based convex optimization problems. In this article, the PESC baseddenoising and compressive sensing algorithms are developed. Simulation examplesare presented.
arxiv-5100-48 | From Small-World Networks to Comparison-Based Search | http://arxiv.org/pdf/1107.3059v3.pdf | author:Amin Karbasi, Stratis Ioannidis, Laurent Massoulie category:cs.LG cs.DS cs.IT cs.SI math.IT stat.ML published:2011-07-15 summary:The problem of content search through comparisons has recently receivedconsiderable attention. In short, a user searching for a target objectnavigates through a database in the following manner: the user is asked toselect the object most similar to her target from a small list of objects. Anew object list is then presented to the user based on her earlier selection.This process is repeated until the target is included in the list presented, atwhich point the search terminates. This problem is known to be strongly relatedto the small-world network design problem. However, contrary to prior work, which focuses on cases where objects in thedatabase are equally popular, we consider here the case where the demand forobjects may be heterogeneous. We show that, under heterogeneous demand, thesmall-world network design problem is NP-hard. Given the above negative result,we propose a novel mechanism for small-world design and provide an upper boundon its performance under heterogeneous demand. The above mechanism has anatural equivalent in the context of content search through comparisons, and weestablish both an upper bound and a lower bound for the performance of thismechanism. These bounds are intuitively appealing, as they depend on theentropy of the demand as well as its doubling constant, a quantity capturingthe topology of the set of target objects. They also illustrate interestingconnections between comparison-based search to classic results from informationtheory. Finally, we propose an adaptive learning algorithm for content searchthat meets the performance guarantees achieved by the above mechanisms.
arxiv-5100-49 | A Second-order Bound with Excess Losses | http://arxiv.org/pdf/1402.2044v1.pdf | author:Pierre Gaillard, Gilles Stoltz, Tim Van Erven category:stat.ML cs.LG math.ST stat.TH published:2014-02-10 summary:We study online aggregation of the predictions of experts, and first show newsecond-order regret bounds in the standard setting, which are obtained via aversion of the Prod algorithm (and also a version of the polynomially weightedaverage algorithm) with multiple learning rates. These bounds are in terms ofexcess losses, the differences between the instantaneous losses suffered by thealgorithm and the ones of a given expert. We then demonstrate the interest ofthese bounds in the context of experts that report their confidences as anumber in the interval [0,1] using a generic reduction to the standard setting.We conclude by two other applications in the standard setting, which improvethe known bounds in case of small excess losses and show a bounded regretagainst i.i.d. sequences of losses.
arxiv-5100-50 | Approachability in unknown games: Online learning meets multi-objective optimization | http://arxiv.org/pdf/1402.2043v1.pdf | author:Shie Mannor, Vianney Perchet, Gilles Stoltz category:stat.ML cs.LG math.ST stat.TH published:2014-02-10 summary:In the standard setting of approachability there are two players and a targetset. The players play a repeated vector-valued game where one of them wants tohave the average vector-valued payoff converge to the target set which theother player tries to exclude. We revisit the classical setting and considerthe setting where the player has a preference relation between target sets: shewishes to approach the smallest ("best") set possible given the observedaverage payoffs in hindsight. Moreover, as opposed to previous works onapproachability, and in the spirit of online learning, we do not assume thatthere is a known game structure with actions for two players. Rather, theplayer receives an arbitrary vector-valued reward vector at every round. Weshow that it is impossible, in general, to approach the best target set inhindsight. We further propose a concrete strategy that approaches a non-trivialrelaxation of the best-in-hindsight given the actual rewards. Our approach doesnot require projection onto a target set and amounts to switching betweenscalar regret minimization algorithms that are performed in episodes.
arxiv-5100-51 | Deeply Coupled Auto-encoder Networks for Cross-view Classification | http://arxiv.org/pdf/1402.2031v1.pdf | author:Wen Wang, Zhen Cui, Hong Chang, Shiguang Shan, Xilin Chen category:cs.CV cs.LG cs.NE published:2014-02-10 summary:The comparison of heterogeneous samples extensively exists in manyapplications, especially in the task of image classification. In this paper, wepropose a simple but effective coupled neural network, called Deeply CoupledAutoencoder Networks (DCAN), which seeks to build two deep neural networks,coupled with each other in every corresponding layers. In DCAN, each deepstructure is developed via stacking multiple discriminative coupledauto-encoders, a denoising auto-encoder trained with maximum margin criterionconsisting of intra-class compactness and inter-class penalty. This singlelayer component makes our model simultaneously preserve the local consistencyand enhance its discriminative capability. With increasing number of layers,the coupled networks can gradually narrow the gap between the two views.Extensive experiments on cross-view image classification tasks demonstrate thesuperiority of our method over state-of-the-art methods.
arxiv-5100-52 | Genomic Prediction of Quantitative Traits using Sparse and Locally Epistatic Models | http://arxiv.org/pdf/1402.2026v1.pdf | author:Deniz Akdemir category:stat.AP stat.ML published:2014-02-10 summary:In plant and animal breeding studies a distinction is made between thegenetic value (additive + epistatic genetic effects) and the breeding value(additive genetic effects) of an individual since it is expected that some ofthe epistatic genetic effects will be lost due to recombination. In this paper,we argue that the breeder can take advantage of some of the epistatic markereffects in regions of low recombination. The models introduced here aim toestimate local epistatic line heritability by using the genetic map informationand combine the local additive and epistatic effects. To this end, we have usedsemi-parametric mixed models with multiple local genomic relationship matriceswith hierarchical designs and lasso post-processing for sparsity in the finalmodel. Our models produce good predictive performance along with goodexplanatory information.
arxiv-5100-53 | Binary Stereo Matching | http://arxiv.org/pdf/1402.2020v1.pdf | author:Kang Zhang, Jiyang Li, Yijing Li, Weidong Hu, Lifeng Sun, Shiqiang Yang category:cs.CV published:2014-02-10 summary:In this paper, we propose a novel binary-based cost computation andaggregation approach for stereo matching problem. The cost volume isconstructed through bitwise operations on a series of binary strings. Then thisapproach is combined with traditional winner-take-all strategy, resulting in anew local stereo matching algorithm called binary stereo matching (BSM). Sincecore algorithm of BSM is based on binary and integer computations, it has ahigher computational efficiency than previous methods. Experimental results onMiddlebury benchmark show that BSM has comparable performance withstate-of-the-art local stereo methods in terms of both quality and speed.Furthermore, experiments on images with radiometric differences demonstratethat BSM is more robust than previous methods under these changes, which iscommon under real illumination.
arxiv-5100-54 | Foreground segmentation based on multi-resolution and matting | http://arxiv.org/pdf/1402.2013v1.pdf | author:Xintong Yu, Xiaohan Liu, Yisong Chen category:cs.CV published:2014-02-10 summary:We propose a foreground segmentation algorithm that does foregroundextraction under different scales and refines the result by matting. First, theinput image is filtered and resampled to 5 different resolutions. Then each ofthem is segmented by adaptive figure-ground classification and the bestsegmentation is automatically selected by an evaluation score that maximizesthe difference between foreground and background. This segmentation isupsampled to the original size, and a corresponding trimap is built.Closed-form matting is employed to label the boundary region, and the result isrefined by a final figure-ground classification. Experiments show the successof our method in treating challenging images with cluttered background andadapting to loose initial bounding-box.
arxiv-5100-55 | Better Optimism By Bayes: Adaptive Planning with Rich Models | http://arxiv.org/pdf/1402.1958v1.pdf | author:Arthur Guez, David Silver, Peter Dayan category:cs.AI cs.LG stat.ML published:2014-02-09 summary:The computational costs of inference and planning have confined Bayesianmodel-based reinforcement learning to one of two dismal fates: powerfulBayes-adaptive planning but only for simplistic models, or powerful, Bayesiannon-parametric models but using simple, myopic planning strategies such asThompson sampling. We ask whether it is feasible and truly beneficial tocombine rich probabilistic models with a closer approximation to fully Bayesianplanning. First, we use a collection of counterexamples to show formal problemswith the over-optimism inherent in Thompson sampling. Then we leveragestate-of-the-art techniques in efficient Bayes-adaptive planning andnon-parametric Bayesian methods to perform qualitatively better than bothexisting conventional algorithms and Thompson sampling on two contextualbandit-like problems.
arxiv-5100-56 | Classification Tree Diagrams in Health Informatics Applications | http://arxiv.org/pdf/1402.1947v1.pdf | author:Farrukh Arslan category:cs.IR cs.CV cs.LG published:2014-02-09 summary:Health informatics deal with the methods used to optimize the acquisition,storage and retrieval of medical data, and classify information in healthcareapplications. Healthcare analysts are particularly interested in variouscomputer informatics areas such as; knowledge representation from data, anomalydetection, outbreak detection methods and syndromic surveillance applications.Although various parametric and non-parametric approaches are being proposed toclassify information from data, classification tree diagrams provide aninteractive visualization to analysts as compared to other methods. In thiswork we discuss application of classification tree diagrams to classifyinformation from medical data in healthcare applications.
arxiv-5100-57 | MCA Learning Algorithm for Incident Signals Estimation: A Review | http://arxiv.org/pdf/1402.1931v1.pdf | author:Rashid Ahmed, John A. Avaritsiotis category:cs.NE published:2014-02-09 summary:Recently there has been many works on adaptive subspace filtering in thesignal processing literature. Most of them are concerned with tracking thesignal subspace spanned by the eigenvectors corresponding to the eigenvalues ofthe covariance matrix of the signal plus noise data. Minor Component Analysis(MCA) is important tool and has a wide application in telecommunications,antenna array processing, statistical parametric estimation, etc. As animportant feature extraction technique, MCA is a statistical method ofextracting the eigenvector associated with the smallest eigenvalue of thecovariance matrix. In this paper, we will present a MCA learning algorithm toextract minor component from input signals, and the learning rate parameter isalso presented, which ensures fast convergence of the algorithm, because it hasdirect effect on the convergence of the weight vector and the error level isaffected by this value. MCA is performed to determine the estimated DOA.Simulation results will be furnished to illustrate the theoretical resultsachieved.
arxiv-5100-58 | A Hybrid Loss for Multiclass and Structured Prediction | http://arxiv.org/pdf/1402.1921v1.pdf | author:Qinfeng Shi, Mark Reid, Tiberio Caetano, Anton van den Hengel, Zhenhua Wang category:cs.LG cs.AI cs.CV published:2014-02-09 summary:We propose a novel hybrid loss for multiclass and structured predictionproblems that is a convex combination of a log loss for Conditional RandomFields (CRFs) and a multiclass hinge loss for Support Vector Machines (SVMs).We provide a sufficient condition for when the hybrid loss is Fisher consistentfor classification. This condition depends on a measure of dominance betweenlabels--specifically, the gap between the probabilities of the best label andthe second best label. We also prove Fisher consistency is necessary forparametric consistency when learning models such as CRFs. We demonstrateempirically that the hybrid loss typically performs least as well as--and oftenbetter than--both of its constituent losses on a variety of tasks, such ashuman action recognition. In doing so we also provide an empirical comparisonof the efficacy of probabilistic and margin based approaches to multiclass andstructured prediction.
arxiv-5100-59 | StructBoost: Boosting Methods for Predicting Structured Output Variables | http://arxiv.org/pdf/1302.3283v3.pdf | author:Chunhua Shen, Guosheng Lin, Anton van den Hengel category:cs.LG published:2013-02-14 summary:Boosting is a method for learning a single accurate predictor by linearlycombining a set of less accurate weak learners. Recently, structured learninghas found many applications in computer vision. Inspired by structured supportvector machines (SSVM), here we propose a new boosting algorithm for structuredoutput prediction, which we refer to as StructBoost. StructBoost supportsnonlinear structured learning by combining a set of weak structured learners.As SSVM generalizes SVM, our StructBoost generalizes standard boostingapproaches such as AdaBoost, or LPBoost to structured learning. The resultingoptimization problem of StructBoost is more challenging than SSVM in the sensethat it may involve exponentially many variables and constraints. In contrast,for SSVM one usually has an exponential number of constraints and acutting-plane method is used. In order to efficiently solve StructBoost, weformulate an equivalent $ 1 $-slack formulation and solve it using acombination of cutting planes and column generation. We show the versatilityand usefulness of StructBoost on a range of problems such as optimizing thetree loss for hierarchical multi-class classification, optimizing the Pascaloverlap criterion for robust visual tracking and learning conditional randomfield parameters for image segmentation.
arxiv-5100-60 | Collaborative Discriminant Locality Preserving Projections With its Application to Face Recognition | http://arxiv.org/pdf/1312.7469v2.pdf | author:Sheng Huang, Dan Yang, Dong Yang, Ahmed Elgammal category:cs.CV published:2013-12-28 summary:We present a novel Discriminant Locality Preserving Projections (DLPP)algorithm named Collaborative Discriminant Locality Preserving Projection(CDLPP). In our algorithm, the discriminating power of DLPP are furtherexploited from two aspects. On the one hand, the global optimum of classscattering is guaranteed via using the between-class scatter matrix to replacethe original denominator of DLPP. On the other hand, motivated by collaborativerepresentation, an $L_2$-norm constraint is imposed to the projections todiscover the collaborations of dimensions in the sample space. We apply ouralgorithm to face recognition. Three popular face databases, namely AR, ORL andLFW-A, are employed for evaluating the performance of CDLPP. Extensiveexperimental results demonstrate that CDLPP significantly improves thediscriminating power of DLPP and outperforms the state-of-the-arts.
arxiv-5100-61 | Sparse Illumination Learning and Transfer for Single-Sample Face Recognition with Image Corruption and Misalignment | http://arxiv.org/pdf/1402.1879v1.pdf | author:Liansheng Zhuang, Tsung-Han Chan, Allen Y. Yang, S. Shankar Sastry, Yi Ma category:cs.CV published:2014-02-08 summary:Single-sample face recognition is one of the most challenging problems inface recognition. We propose a novel algorithm to address this problem based ona sparse representation based classification (SRC) framework. The new algorithmis robust to image misalignment and pixel corruption, and is able to reducerequired gallery images to one sample per class. To compensate for the missingillumination information traditionally provided by multiple gallery images, asparse illumination learning and transfer (SILT) technique is introduced. Theillumination in SILT is learned by fitting illumination examples of auxiliaryface images from one or more additional subjects with a sparsely-usedillumination dictionary. By enforcing a sparse representation of the queryimage in the illumination dictionary, the SILT can effectively recover andtransfer the illumination and pose information from the alignment stage to therecognition stage. Our extensive experiments have demonstrated that the newalgorithms significantly outperform the state of the art in the single-sampleregime and with less restrictions. In particular, the single-sample facealignment accuracy is comparable to that of the well-known Deformable SRCalgorithm using multiple gallery images per class. Furthermore, the facerecognition accuracy exceeds those of the SRC and Extended SRC algorithms usinghand labeled alignment initialization.
arxiv-5100-62 | Excess Risk Bounds for Exponentially Concave Losses | http://arxiv.org/pdf/1401.4566v2.pdf | author:Mehrdad Mahdavi, Rong Jin category:cs.LG stat.ML published:2014-01-18 summary:The overarching goal of this paper is to derive excess risk bounds forlearning from exp-concave loss functions in passive and sequential learningsettings. Exp-concave loss functions encompass several fundamental problems inmachine learning such as squared loss in linear regression, logistic loss inclassification, and negative logarithm loss in portfolio management. In batchsetting, we obtain sharp bounds on the performance of empirical riskminimization performed in a linear hypothesis space and with respect to theexp-concave loss functions. We also extend the results to the online settingwhere the learner receives the training examples in a sequential manner. Wepropose an online learning algorithm that is a properly modified version ofonline Newton method to obtain sharp risk bounds. Under an additional mildassumption on the loss function, we show that in both settings we are able toachieve an excess risk bound of $O(d\log n/n)$ that holds with a highprobability.
arxiv-5100-63 | Efficient Low Dose X-ray CT Reconstruction through Sparsity-Based MAP Modeling | http://arxiv.org/pdf/1402.1801v1.pdf | author:SayedMasoud Hashemi, Soosan Beheshti, Patrick R. Gill, Narinder S. Paul, Richard S. C. Cobbold category:stat.AP cs.CV published:2014-02-08 summary:Ultra low radiation dose in X-ray Computed Tomography (CT) is an importantclinical objective in order to minimize the risk of carcinogenesis. CompressedSensing (CS) enables significant reductions in radiation dose to be achieved byproducing diagnostic images from a limited number of CT projections. However,the excessive computation time that conventional CS-based CT reconstructiontypically requires has limited clinical implementation. In this paper, we firstdemonstrate that a thorough analysis of CT reconstruction through a Maximum aPosteriori objective function results in a weighted compressive sensingproblem. This analysis enables us to formulate a low dose fan beam and helicalcone beam CT reconstruction. Subsequently, we provide an efficient solution tothe formulated CS problem based on a Fast Composite Splitting Algorithm-LatentExpected Maximization (FCSA-LEM) algorithm. In the proposed method we usepseudo polar Fourier transform as the measurement matrix in order to decreasethe computational complexity; and rebinning of the projections to parallel raysin order to extend its application to fan beam and helical cone beam scans. Theweight involved in the proposed weighted CS model, denoted by Error AdaptationWeight (EAW), is calculated based on the statistical characteristics of CTreconstruction and is a function of Poisson measurement noise and rebinninginterpolation error. Simulation results show that low computational complexityof the proposed method made the fast recovery of the CT images possible andusing EAW reduces the reconstruction error by one order of magnitude. Recoveryof a high quality 512$\times$ 512 image was achieved in less than 20 sec on adesktop computer without numerical optimizations.
arxiv-5100-64 | Binary Excess Risk for Smooth Convex Surrogates | http://arxiv.org/pdf/1402.1792v1.pdf | author:Mehrdad Mahdavi, Lijun Zhang, Rong Jin category:cs.LG stat.ML published:2014-02-07 summary:In statistical learning theory, convex surrogates of the 0-1 loss are highlypreferred because of the computational and theoretical virtues that convexitybrings in. This is of more importance if we consider smooth surrogates aswitnessed by the fact that the smoothness is further beneficial bothcomputationally- by attaining an {\it optimal} convergence rate foroptimization, and in a statistical sense- by providing an improved {\itoptimistic} rate for generalization bound. In this paper we investigate thesmoothness property from the viewpoint of statistical consistency and show howit affects the binary excess risk. We show that in contrast to optimization andgeneralization errors that favor the choice of smooth surrogate loss, thesmoothness of loss function may degrade the binary excess risk. Motivated bythis negative result, we provide a unified analysis that integratesoptimization error, generalization bound, and the error in translating convexexcess risk into a binary excess risk when examining the impact of smoothnesson the binary excess risk. We show that under favorable conditions appropriatechoice of smooth convex loss will result in a binary excess risk that is betterthan $O(1/\sqrt{n})$.
arxiv-5100-65 | How Santa Fe Ants Evolve | http://arxiv.org/pdf/1312.1858v2.pdf | author:Dominic Wilson, Devinder Kaur category:cs.NE published:2013-12-06 summary:The Santa Fe Ant model problem has been extensively used to investigate, testand evaluate Evolutionary Computing systems and methods over the past twodecades. There is however no literature on its program structures that aresystematically used for fitness improvement, the geometries of those structuresand their dynamics during optimization. This paper analyzes the Santa Fe AntProblem using a new phenotypic schema and landscape analysis based on executedinstruction sequences. For the first time we detail systematic structuralfeatures that give high fitness and the evolutionary dynamics of suchstructures. The new schema avoids variances due to introns. We develop aphenotypic variation method that tests the new understanding of the landscape.We also develop a modified function set that tests newly identifiedsynchronization constraints. We obtain favorable computational efforts comparedto those in the literature, on testing the new variation and function set onboth the Santa Fe Trail, and the more computationally demanding Los AltosTrail. Our findings suggest that for the Santa Fe Ant problem, a perspective ofprogram assembly from repetition of highly fit responses to trail conditionsleads to better analysis and performance.
arxiv-5100-66 | Alternating Minimization for Mixed Linear Regression | http://arxiv.org/pdf/1310.3745v2.pdf | author:Xinyang Yi, Constantine Caramanis, Sujay Sanghavi category:stat.ML published:2013-10-14 summary:Mixed linear regression involves the recovery of two (or more) unknownvectors from unlabeled linear measurements; that is, where each sample comesfrom exactly one of the vectors, but we do not know which one. It is a classicproblem, and the natural and empirically most popular approach to its solutionhas been the EM algorithm. As in other settings, this is prone to bad localminima; however, each iteration is very fast (alternating between guessinglabels, and solving with those labels). In this paper we provide a new initialization procedure for EM, based onfinding the leading two eigenvectors of an appropriate matrix. We then showthat with this, a re-sampled version of the EM algorithm provably converges tothe correct vectors, under natural assumptions on the sampling distribution,and with nearly optimal (unimprovable) sample complexity. This provides notonly the first characterization of EM's performance, but also much lower samplecomplexity as compared to both standard (randomly initialized) EM, and othermethods for this problem.
arxiv-5100-67 | Performance of Hull-Detection Algorithms For Proton Computed Tomography Reconstruction | http://arxiv.org/pdf/1402.1720v1.pdf | author:Blake Schultze, Micah Witt, Yair Censor, Reinhard Schulte, Keith Evan Schubert category:cs.CV physics.med-ph published:2014-02-07 summary:Proton computed tomography (pCT) is a novel imaging modality developed forpatients receiving proton radiation therapy. The purpose of this work was toinvestigate hull-detection algorithms used for preconditioning of the large andsparse linear system of equations that needs to be solved for pCT imagereconstruction. The hull-detection algorithms investigated here includedsilhouette/space carving (SC), modified silhouette/space carving (MSC), andspace modeling (SM). Each was compared to the cone-beam version of filteredbackprojection (FBP) used for hull-detection. Data for testing these algorithmsincluded simulated data sets of a digital head phantom and an experimental dataset of a pediatric head phantom obtained with a pCT scanner prototype at LomaLinda University Medical Center. SC was the fastest algorithm, exceeding thespeed of FBP by more than 100 times. FBP was most sensitive to the presence ofnoise. Ongoing work will focus on optimizing threshold parameters in order todefine a fast and efficient method for hull-detection in pCT imagereconstruction.
arxiv-5100-68 | On the Prediction Performance of the Lasso | http://arxiv.org/pdf/1402.1700v1.pdf | author:Arnak S. Dalalyan, Mohamed Hebiri, Johannes Lederer category:math.ST stat.ML stat.TH published:2014-02-07 summary:Although the Lasso has been extensively studied, the relationship between itsprediction performance and the correlations of the covariates is not fullyunderstood. In this paper, we give new insights into this relationship in thecontext of multiple linear regression. We show, in particular, that theincorporation of a simple correlation measure into the tuning parameter leadsto a nearly optimal prediction performance of the Lasso even for highlycorrelated covariates. However, we also reveal that for moderately correlatedcovariates, the prediction performance of the Lasso can be mediocreirrespective of the choice of the tuning parameter. For the illustration of ourapproach with an important application, we deduce nearly optimal rates for theleast-squares estimator with total variation penalty.
arxiv-5100-69 | A Statistical Learning Theory Framework for Supervised Pattern Discovery | http://arxiv.org/pdf/1307.0802v2.pdf | author:Jonathan H. Huggins, Cynthia Rudin category:stat.ML cs.AI published:2013-07-02 summary:This paper formalizes a latent variable inference problem we call {\emsupervised pattern discovery}, the goal of which is to find sets ofobservations that belong to a single ``pattern.'' We discuss two versions ofthe problem and prove uniform risk bounds for both. In the first version,collections of patterns can be generated in an arbitrary manner and the dataconsist of multiple labeled collections. In the second version, the patternsare assumed to be generated independently by identically distributed processes.These processes are allowed to take an arbitrary form, so observations within apattern are not in general independent of each other. The bounds for the secondversion of the problem are stated in terms of a new complexity measure, thequasi-Rademacher complexity.
arxiv-5100-70 | Evaluation of YTEX and MetaMap for clinical concept recognition | http://arxiv.org/pdf/1402.1668v1.pdf | author:John David Osborne, Binod Gyawali, Thamar Solorio category:cs.IR cs.CL 68 published:2014-02-07 summary:We used MetaMap and YTEX as a basis for the construc- tion of two separatesystems to participate in the 2013 ShARe/CLEF eHealth Task 1[9], therecognition of clinical concepts. No modifications were directly made to thesesystems, but output concepts were filtered using stop concepts, stop concepttext and UMLS semantic type. Con- cept boundaries were also adjusted using asmall collection of rules to increase precision on the strict task. OverallMetaMap had better per- formance than YTEX on the strict task, primarily due toa 20% perfor- mance improvement in precision. In the relaxed task YTEX hadbetter performance in both precision and recall giving it an overall F-Score4.6% higher than MetaMap on the test data. Our results also indicated a 1.3%higher accuracy for YTEX in UMLS CUI mapping.
arxiv-5100-71 | A Brief History of Learning Classifier Systems: From CS-1 to XCS | http://arxiv.org/pdf/1401.3607v2.pdf | author:Larry Bull category:cs.NE cs.LG published:2014-01-15 summary:Modern Learning Classifier Systems can be characterized by their use of ruleaccuracy as the utility metric for the search algorithm(s) discovering usefulrules. Such searching typically takes place within the restricted space ofco-active rules for efficiency. This paper gives an historical overview of theevolution of such systems up to XCS, and then some of the subsequentdevelopments of XCS to different types of learning.
arxiv-5100-72 | Tracking via Motion Estimation with Physically Motivated Inter-Region Constraints | http://arxiv.org/pdf/1402.1503v1.pdf | author:Omar Arif, Ganesh Sundaramoorthi, Byung-Woo Hong, Anthony Yezzi category:cs.CV published:2014-02-06 summary:In this paper, we propose a method for tracking structures (e.g., ventriclesand myocardium) in cardiac images (e.g., magnetic resonance) by propagatingforward in time a previous estimate of the structures via a new deformationestimation scheme that is motivated by physical constraints of fluid motion.The method employs within structure motion estimation (so that differingmotions among different structures are not mixed) while simultaneouslysatisfying the physical constraint in fluid motion that at the interfacebetween a fluid and a medium, the normal component of the fluid's motion mustmatch the normal component of the motion of the medium. We show how to estimatethe motion according to the previous considerations in a variational framework,and in particular, show that these conditions lead to PDEs with boundaryconditions at the interface that resemble Robin boundary conditions and inducecoupling between structures. We illustrate the use of this motion estimationscheme in propagating a segmentation across frames and show that it leads tomore accurate segmentation than traditional motion estimation that does notmake use of physical constraints. Further, the method is naturally suited tointeractive segmentation methods, which are prominently used in practice incommercial applications for cardiac analysis, where typically a segmentationfrom the previous frame is used to predict a segmentation in the next frame. Weshow that our propagation scheme reduces the amount of user interaction bypredicting more accurate segmentations than commonly used and recentinteractive commercial techniques.
arxiv-5100-73 | Near-Optimal Joint Object Matching via Convex Relaxation | http://arxiv.org/pdf/1402.1473v1.pdf | author:Yuxin Chen, Leonidas J. Guibas, Qi-Xing Huang category:cs.LG cs.CV cs.IT math.IT math.OC stat.ML published:2014-02-06 summary:Joint matching over a collection of objects aims at aggregating informationfrom a large collection of similar instances (e.g. images, graphs, shapes) toimprove maps between pairs of them. Given multiple matches computed between afew object pairs in isolation, the goal is to recover an entire collection ofmaps that are (1) globally consistent, and (2) close to the provided maps ---and under certain conditions provably the ground-truth maps. Despite recentadvances on this problem, the best-known recovery guarantees are limited to asmall constant barrier --- none of the existing methods find theoreticalsupport when more than $50\%$ of input correspondences are corrupted. Moreover,prior approaches focus mostly on fully similar objects, while it is practicallymore demanding to match instances that are only partially similar to eachother. In this paper, we develop an algorithm to jointly match multiple objects thatexhibit only partial similarities, given a few pairwise matches that aredensely corrupted. Specifically, we propose to recover the ground-truth mapsvia a parameter-free convex program called MatchLift, following a spectralmethod that pre-estimates the total number of distinct elements to be matched.Encouragingly, MatchLift exhibits near-optimal error-correction ability, i.e.in the asymptotic regime it is guaranteed to work even when a dominant fraction$1-\Theta\left(\frac{\log^{2}n}{\sqrt{n}}\right)$ of the input maps behave likerandom outliers. Furthermore, MatchLift succeeds with minimal input complexity,namely, perfect matching can be achieved as soon as the provided maps form aconnected map graph. We evaluate the proposed algorithm on various benchmarkdata sets including synthetic examples and real-world examples, all of whichconfirm the practical applicability of MatchLift.
arxiv-5100-74 | Beating the Minimax Rate of Active Learning with Prior Knowledge | http://arxiv.org/pdf/1311.4803v2.pdf | author:Lijun Zhang, Mehrdad Mahdavi, Rong Jin category:cs.LG stat.ML published:2013-11-19 summary:Active learning refers to the learning protocol where the learner is allowedto choose a subset of instances for labeling. Previous studies have shown that,compared with passive learning, active learning is able to reduce the labelcomplexity exponentially if the data are linearly separable or satisfy theTsybakov noise condition with parameter $\kappa=1$. In this paper, we propose anovel active learning algorithm using a convex surrogate loss, with the goal tobroaden the cases for which active learning achieves an exponentialimprovement. We make use of a convex loss not only because it reduces thecomputational cost, but more importantly because it leads to a tight bound forthe empirical process (i.e., the difference between the empirical estimationand the expectation) when the current solution is close to the optimal one.Under the assumption that the norm of the optimal classifier that minimizes theconvex risk is available, our analysis shows that the introduction of theconvex surrogate loss yields an exponential reduction in the label complexityeven when the parameter $\kappa$ of the Tsybakov noise is larger than $1$. Tothe best of our knowledge, this is the first work that improves the minimaxrate of active learning by utilizing certain priori knowledge.
arxiv-5100-75 | An Autoencoder Approach to Learning Bilingual Word Representations | http://arxiv.org/pdf/1402.1454v1.pdf | author:Sarath Chandar A P, Stanislas Lauly, Hugo Larochelle, Mitesh M. Khapra, Balaraman Ravindran, Vikas Raykar, Amrita Saha category:cs.CL cs.LG stat.ML published:2014-02-06 summary:Cross-language learning allows us to use training data from one language tobuild models for a different language. Many approaches to bilingual learningrequire that we have word-level alignment of sentences from parallel corpora.In this work we explore the use of autoencoder-based methods for cross-languagelearning of vectorial word representations that are aligned between twolanguages, while not relying on word-level alignments. We show that by simplylearning to reconstruct the bag-of-words representations of aligned sentences,within and between languages, we can in fact learn high-quality representationsand do without word alignments. Since training autoencoders on wordobservations presents certain computational issues, we propose and comparedifferent variations adapted to this setting. We also propose an explicitcorrelation maximizing regularizer that leads to significant improvement in theperformance. We empirically investigate the success of our approach on theproblem of cross-language test classification, where a classifier trained on agiven language (e.g., English) must learn to generalize to a different language(e.g., German). These experiments demonstrate that our approaches arecompetitive with the state-of-the-art, achieving up to 10-14 percentage pointimprovements over the best reported results on this task.
arxiv-5100-76 | A Three-Phase Search Approach for the Quadratic Minimum Spanning Tree Problem | http://arxiv.org/pdf/1402.1379v1.pdf | author:Zhang-Hua Fu, Jin-Kao Hao category:cs.DS cs.NE published:2014-02-06 summary:Given an undirected graph with costs associated with each edge as well aseach pair of edges, the quadratic minimum spanning tree problem (QMSTP)consists of determining a spanning tree of minimum total cost. This problem canbe used to model many real-life network design applications, in which bothrouting and interference costs should be considered. For this problem, wepropose a three-phase search approach named TPS, which integrates 1) adescent-based neighborhood search phase using two different move operators toreach a local optimum from a given starting solution, 2) a local optimaexploring phase to discover nearby local optima within a given regional searcharea, and 3) a perturbation-based diversification phase to jump out of thecurrent regional search area. Additionally, we introduce dedicated techniquesto reduce the neighborhood to explore and streamline the neighborhoodevaluations. Computational experiments based on hundreds of representativebenchmarks show that TPS produces highly competitive results with respect tothe best performing approaches in the literature by improving the best knownresults for 31 instances and matching the best known results for the remaininginstances only except two cases. Critical elements of the proposed algorithmsare analyzed.
arxiv-5100-77 | Quantile Representation for Indirect Immunofluorescence Image Classification | http://arxiv.org/pdf/1402.1371v1.pdf | author:David M. J. Tax, Veronika Cheplygina, Marco Loog category:cs.CV published:2014-02-06 summary:In the diagnosis of autoimmune diseases, an important task is to classifyimages of slides containing several HEp-2 cells. All cells from one slide sharethe same label, and by classifying cells from one slide independently, someinformation on the global image quality and intensity is lost. Considering onewhole slide as a collection (a bag) of feature vectors, however, poses theproblem of how to handle this bag. A simple, and surprisingly effective,approach is to summarize the bag of feature vectors by a few quantile valuesper feature. This characterizes the full distribution of all instances, therebyassuming that all instances in a bag are informative. This representation isparticularly useful when each bag contains many feature vectors, which is thecase in the classification of the immunofluorescence images. Experiments on theclassification of indirect immunofluorescence images show the usefulness ofthis approach.
arxiv-5100-78 | Real-time Pedestrian Surveillance with Top View Cumulative Grids | http://arxiv.org/pdf/1402.1359v1.pdf | author:Kai Berger, Jeyarajan Thiyagalingam category:cs.CV published:2014-02-06 summary:This manuscript presents an efficient approach to map pedestrian surveillancefootage to an aerial view for global assessment of features. The analysis ofthe footages relies on low level computer vision and enable real-timesurveillance. While we neglect object tracking, we introduce cumulative gridson top view scene flow visualization to highlight situations of interest in thefootage. Our approach is tested on multiview footage both from RGB cameras and,for the first time in the field, on RGB-D-sensors.
arxiv-5100-79 | Dissimilarity-based Ensembles for Multiple Instance Learning | http://arxiv.org/pdf/1402.1349v1.pdf | author:Veronika Cheplygina, David M. J. Tax, Marco Loog category:stat.ML cs.LG published:2014-02-06 summary:In multiple instance learning, objects are sets (bags) of feature vectors(instances) rather than individual feature vectors. In this paper we addressthe problem of how these bags can best be represented. Two standard approachesare to use (dis)similarities between bags and prototype bags, or between bagsand prototype instances. The first approach results in a relativelylow-dimensional representation determined by the number of training bags, whilethe second approach results in a relatively high-dimensional representation,determined by the total number of instances in the training set. In this papera third, intermediate approach is proposed, which links the two approaches andcombines their strengths. Our classifier is inspired by a random subspaceensemble, and considers subspaces of the dissimilarity space, defined bysubsets of instances, as prototypes. We provide guidelines for using such anensemble, and show state-of-the-art performances on a range of multipleinstance learning problems.
arxiv-5100-80 | A Cellular Automata based Optimal Edge Detection Technique using Twenty-Five Neighborhood Model | http://arxiv.org/pdf/1402.1348v1.pdf | author:Deepak Ranjan Nayak, Sumit Kumar Sahu, Jahangir Mohammed category:cs.CV published:2014-02-06 summary:Cellular Automata (CA) are common and most simple models of parallelcomputations. Edge detection is one of the crucial task in image processing,especially in processing biological and medical images. CA can be successfullyapplied in image processing. This paper presents a new method for edgedetection of binary images based on two dimensional twenty five neighborhoodcellular automata. The method considers only linear rules of CA for extractionof edges under null boundary condition. The performance of this approach iscompared with some existing edge detection techniques. This comparison showsthat the proposed method to be very promising for edge detection of binaryimages. All the algorithms and results used in this paper are prepared inMATLAB.
arxiv-5100-81 | Learning Transformations for Classification Forests | http://arxiv.org/pdf/1312.5604v2.pdf | author:Qiang Qiu, Guillermo Sapiro category:cs.CV cs.LG stat.ML published:2013-12-19 summary:This work introduces a transformation-based learner model for classificationforests. The weak learner at each split node plays a crucial role in aclassification tree. We propose to optimize the splitting objective by learninga linear transformation on subspaces using nuclear norm as the optimizationcriteria. The learned linear transformation restores a low-rank structure fordata from the same class, and, at the same time, maximizes the separationbetween different classes, thereby improving the performance of the splitfunction. Theoretical and experimental results support the proposed framework.
arxiv-5100-82 | An Estimation Method of Measuring Image Quality for Compressed Images of Human Face | http://arxiv.org/pdf/1402.1331v1.pdf | author:Abhishek Bhattacharya, Tanusree Chatterjee category:cs.CV published:2014-02-06 summary:Nowadays digital image compression and decompression techniques are very muchimportant. So our aim is to calculate the quality of face and other regions ofthe compressed image with respect to the original image. Image segmentation istypically used to locate objects and boundaries (lines, curves etc.)in images.After segmentation the image is changed into something which is more meaningfulto analyze. Using Universal Image Quality Index(Q),Structural SimilarityIndex(SSIM) and Gradient-based Structural Similarity Index(G-SSIM) it can beshown that face region is less compressed than any other region of the image.
arxiv-5100-83 | A density-sensitive hierarchical clustering method | http://arxiv.org/pdf/1210.6292v2.pdf | author:Álvaro Martínez-Pérez category:cs.LG 62H30, 68T10 published:2012-10-23 summary:We define a hierarchical clustering method: $\alpha$-unchaining singlelinkage or $SL(\alpha)$. The input of this algorithm is a finite metric spaceand a certain parameter $\alpha$. This method is sensitive to the density ofthe distribution and offers some solution to the so called chaining effect. Wealso define a modified version, $SL^*(\alpha)$, to treat the chaining throughpoints or small blocks. We study the theoretical properties of these methodsand offer some theoretical background for the treatment of chaining effects.
arxiv-5100-84 | Localized epidemic detection in networks with overwhelming noise | http://arxiv.org/pdf/1402.1263v1.pdf | author:Eli A. Meirom, Chris Milling, Constantine Caramanis, Shie Mannor, Ariel Orda, Sanjay Shakkottai category:cs.SI cs.LG published:2014-02-06 summary:We consider the problem of detecting an epidemic in a population whereindividual diagnoses are extremely noisy. The motivation for this problem isthe plethora of examples (influenza strains in humans, or computer viruses insmartphones, etc.) where reliable diagnoses are scarce, but noisy dataplentiful. In flu/phone-viruses, exceedingly few infected people/phones areprofessionally diagnosed (only a small fraction go to a doctor) but lessreliable secondary signatures (e.g., people staying home, orgreater-than-typical upload activity) are more readily available. Thesesecondary data are often plagued by unreliability: many people with the flu donot stay home, and many people that stay home do not have the flu. This paperidentifies the precise regime where knowledge of the contact network enablesfinding the needle in the haystack: we provide a distributed, efficient androbust algorithm that can correctly identify the existence of a spreadingepidemic from highly unreliable local data. Our algorithm requires onlylocal-neighbor knowledge of this graph, and in a broad array of settings thatwe describe, succeeds even when false negatives and false positives make up anoverwhelming fraction of the data available. Our results show it succeeds inthe presence of partial information about the contact network, and also whenthere is not a single "patient zero", but rather many (hundreds, in ourexamples) of initial patient-zeroes, spread across the graph.
arxiv-5100-85 | Multispectral Palmprint Encoding and Recognition | http://arxiv.org/pdf/1402.2941v1.pdf | author:Zohaib Khan, Faisal Shafait, Yiqun Hu, Ajmal Mian category:cs.CV published:2014-02-06 summary:Palmprints are emerging as a new entity in multi-modal biometrics for humanidentification and verification. Multispectral palmprint images captured in thevisible and infrared spectrum not only contain the wrinkles and ridge structureof a palm, but also the underlying pattern of veins; making them a highlydiscriminating biometric identifier. In this paper, we propose a featureencoding scheme for robust and highly accurate representation and matching ofmultispectral palmprints. To facilitate compact storage of the feature, wedesign a binary hash table structure that allows for efficient matching inlarge databases. Comprehensive experiments for both identification andverification scenarios are performed on two public datasets -- one capturedwith a contact-based sensor (PolyU dataset), and the other with a contact-freesensor (CASIA dataset). Recognition results in various experimental setups showthat the proposed method consistently outperforms existing state-of-the-artmethods. Error rates achieved by our method (0.003% on PolyU and 0.2% on CASIA)are the lowest reported in literature on both dataset and clearly indicate theviability of palmprint as a reliable and promising biometric. All source codesare publicly available.
arxiv-5100-86 | Demystifying Information-Theoretic Clustering | http://arxiv.org/pdf/1310.4210v2.pdf | author:Greg Ver Steeg, Aram Galstyan, Fei Sha, Simon DeDeo category:cs.LG cs.IT math.IT stat.ML published:2013-10-15 summary:We propose a novel method for clustering data which is grounded ininformation-theoretic principles and requires no parametric assumptions.Previous attempts to use information theory to define clusters in anassumption-free way are based on maximizing mutual information between data andcluster labels. We demonstrate that this intuition suffers from a fundamentalconceptual flaw that causes clustering performance to deteriorate as the amountof data increases. Instead, we return to the axiomatic foundations ofinformation theory to define a meaningful clustering measure based on thenotion of consistency under coarse-graining for finite data.
arxiv-5100-87 | Image Acquisition in an Underwater Vision System with NIR and VIS Illumination | http://arxiv.org/pdf/1402.1151v1.pdf | author:Wojciech Biegański, Andrzej Kasiński category:cs.CV published:2014-02-05 summary:The paper describes the image acquisition system able to capture images intwo separated bands of light, used to underwater autonomous navigation. Thechannels are: the visible light spectrum and near infrared spectrum. Thecharacteristics of natural, underwater environment were also described togetherwith the process of the underwater image creation. The results of an experimentwith comparison of selected images acquired in these channels are discussed.
arxiv-5100-88 | An enhanced neural network based approach towards object extraction | http://arxiv.org/pdf/1405.6137v1.pdf | author:S. K. Katiyar, P. V. Arun category:cs.CV cs.LG cs.NE published:2014-02-05 summary:The improvements in spectral and spatial resolution of the satellite imageshave facilitated the automatic extraction and identification of the featuresfrom satellite images and aerial photographs. An automatic object extractionmethod is presented for extracting and identifying the various objects fromsatellite images and the accuracy of the system is verified with regard to IRSsatellite images. The system is based on neural network and simulates theprocess of visual interpretation from remote sensing images and hence increasesthe efficiency of image analysis. This approach obtains the basiccharacteristics of the various features and the performance is enhanced by theautomatic learning approach, intelligent interpretation, and intelligentinterpolation. The major advantage of the method is its simplicity and that thesystem identifies the features not only based on pixel value but also based onthe shape, haralick features etc of the objects. Further the system allowsflexibility for identifying the features within the same category based on sizeand shape. The successful application of the system verified its effectivenessand the accuracy of the system were assessed by ground truth verification.
arxiv-5100-89 | Quantum Cybernetics and Complex Quantum Systems Science - A Quantum Connectionist Exploration | http://arxiv.org/pdf/1402.1141v1.pdf | author:Carlos Pedro Gonçalves category:cs.NE quant-ph published:2014-02-05 summary:Quantum cybernetics and its connections to complex quantum systems science isaddressed from the perspective of complex quantum computing systems. In thisway, the notion of an autonomous quantum computing system is introduced inregards to quantum artificial intelligence, and applied to quantum artificialneural networks, considered as autonomous quantum computing systems, whichleads to a quantum connectionist framework within quantum cybernetics forcomplex quantum computing systems. Several examples of quantum feedforwardneural networks are addressed in regards to Boolean functions' computation,multilayer quantum computation dynamics, entanglement and quantumcomplementarity. The examples provide a framework for a reflection on the roleof quantum artificial neural networks as a general framework for addressingcomplex quantum systems that perform network-based quantum computation,possible consequences are drawn regarding quantum technologies, as well asfundamental research in complex quantum systems science and quantum biology.
arxiv-5100-90 | Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition | http://arxiv.org/pdf/1402.1128v1.pdf | author:Haşim Sak, Andrew Senior, Françoise Beaufays category:cs.NE cs.CL cs.LG stat.ML published:2014-02-05 summary:Long Short-Term Memory (LSTM) is a recurrent neural network (RNN)architecture that has been designed to address the vanishing and explodinggradient problems of conventional RNNs. Unlike feedforward neural networks,RNNs have cyclic connections making them powerful for modeling sequences. Theyhave been successfully used for sequence labeling and sequence predictiontasks, such as handwriting recognition, language modeling, phonetic labeling ofacoustic frames. However, in contrast to the deep neural networks, the use ofRNNs in speech recognition has been limited to phone recognition in small scaletasks. In this paper, we present novel LSTM based RNN architectures which makemore effective use of model parameters to train acoustic models for largevocabulary speech recognition. We train and compare LSTM, RNN and DNN models atvarious numbers of parameters and configurations. We show that LSTM modelsconverge quickly and give state of the art speech recognition performance forrelatively small sized models.
arxiv-5100-91 | Linear and Parallel Learning of Markov Random Fields | http://arxiv.org/pdf/1308.6342v4.pdf | author:Yariv Dror Mizrahi, Misha Denil, Nando de Freitas category:stat.ML cs.LG published:2013-08-29 summary:We introduce a new embarrassingly parallel parameter learning algorithm forMarkov random fields with untied parameters which is efficient for a largeclass of practical models. Our algorithm parallelizes naturally over cliquesand, for graphs of bounded degree, its complexity is linear in the number ofcliques. Unlike its competitors, our algorithm is fully parallel and forlog-linear models it is also data efficient, requiring only the localsufficient statistics of the data to estimate parameters.
arxiv-5100-92 | An evolutionary computational based approach towards automatic image registration | http://arxiv.org/pdf/1405.6136v1.pdf | author:P. V. Arun, S. K. Katiyar category:cs.CV cs.NE published:2014-02-05 summary:Image registration is a key component of various image processing operationswhich involve the analysis of different image data sets. Automatic imageregistration domains have witnessed the application of many intelligentmethodologies over the past decade; however inability to properly model objectshape as well as contextual information had limited the attainable accuracy. Inthis paper, we propose a framework for accurate feature shape modeling andadaptive resampling using advanced techniques such as Vector Machines, CellularNeural Network (CNN), SIFT, coreset, and Cellular Automata. CNN has found to beeffective in improving feature matching as well as resampling stages ofregistration and complexity of the approach has been considerably reduced usingcorset optimization The salient features of this work are cellular neuralnetwork approach based SIFT feature point optimisation, adaptive resampling andintelligent object modelling. Developed methodology has been compared withcontemporary methods using different statistical measures. Investigations overvarious satellite images revealed that considerable success was achieved withthe approach. System has dynamically used spectral and spatial information forrepresenting contextual knowledge using CNN-prolog approach. Methodology alsoillustrated to be effective in providing intelligent interpretation andadaptive resampling.
arxiv-5100-93 | Cellular Automata based adaptive resampling technique for the processing of remotely sensed imagery | http://arxiv.org/pdf/1405.6135v1.pdf | author:S. K. Katiyar, P. V. Arun category:cs.CV published:2014-02-05 summary:Resampling techniques are being widely used at different stages of satelliteimage processing. The existing methodologies cannot perfectly recover featuresfrom a completely under sampled image and hence an intelligent adaptiveresampling methodology is required. We address these issues and adopt an errormetric from the available literature to define interpolation quality. We alsopropose a new resampling scheme that adapts itself with regard to the pixel andtexture variation in the image. The proposed CNN based hybrid method has beenfound to perform better than the existing methods as it adapts itself withreference to the image features.
arxiv-5100-94 | A review over the applicability of image entropy in analyses of remote sensing datasets | http://arxiv.org/pdf/1405.6133v1.pdf | author:S. K. Katiyar, P. V. Arun category:cs.CV published:2014-02-05 summary:Entropy is the measure of uncertainty in any data and is adopted formaximisation of mutual information in many remote sensing operations. Theavailability of wide entropy variations motivated us for an investigation overthe suitability preference of these versions to specific operations.
arxiv-5100-95 | An SIR Graph Growth Model for the Epidemics of Communicable Diseases | http://arxiv.org/pdf/1312.2565v2.pdf | author:Charanpal Dhanjal, Stéphan Clémençon category:stat.AP q-bio.PE stat.ML published:2013-12-09 summary:It is the main purpose of this paper to introduce a graph-valued stochasticprocess in order to model the spread of a communicable infectious disease. Themajor novelty of the SIR model we promote lies in the fact that the socialnetwork on which the epidemics is taking place is not specified in advance butevolves through time, accounting for the temporal evolution of the interactionsinvolving infective individuals. Without assuming the existence of a fixedunderlying network model, the stochastic process introduced describes, in aflexible and realistic manner, epidemic spread in non-uniformly mixing andpossibly heterogeneous populations. It is shown how to fit such a(parametrised) model by means of Approximate Bayesian Computation methods basedon graph-valued statistics. The concepts and statistical methods described inthis paper are finally applied to a real epidemic dataset, related to thespread of HIV in Cuba in presence of a contact tracing system, which permitsone to reconstruct partly the evolution of the graph of sexual partnersdiagnosed HIV positive between 1986 and 2006.
arxiv-5100-96 | Sample Complexity of Bayesian Optimal Dictionary Learning | http://arxiv.org/pdf/1301.6199v2.pdf | author:Ayaka Sakata, Yoshiyuki Kabashima category:cs.LG cs.IT math.IT published:2013-01-26 summary:We consider a learning problem of identifying a dictionary matrix D (M timesN dimension) from a sample set of M dimensional vectors Y = N^{-1/2} DX, whereX is a sparse matrix (N times P dimension) in which the density of non-zeroentries is 0<rho< 1. In particular, we focus on the minimum sample size P_c(sample complexity) necessary for perfectly identifying D of the optimallearning scheme when D and X are independently generated from certaindistributions. By using the replica method of statistical mechanics, we showthat P_c=O(N) holds as long as alpha = M/N >rho is satisfied in the limit of Nto infinity. Our analysis also implies that the posterior distribution given Yis condensed only at the correct dictionary D when the compression rate alphais greater than a certain critical value alpha_M(rho). This suggests thatbelief propagation may allow us to learn D with a low computational complexityusing O(N) samples.
arxiv-5100-97 | Patchwise Joint Sparse Tracking with Occlusion Detection | http://arxiv.org/pdf/1402.0978v1.pdf | author:Ali Zarezade, Hamid R. Rabiee, Ali Soltani-Farani, Ahmad Khajenezhad category:cs.CV published:2014-02-05 summary:This paper presents a robust tracking approach to handle challenges such asocclusion and appearance change. Here, the target is partitioned into a numberof patches. Then, the appearance of each patch is modeled using a dictionarycomposed of corresponding target patches in previous frames. In each frame, thetarget is found among a set of candidates generated by a particle filter, via alikelihood measure that is shown to be proportional to the sum ofpatch-reconstruction errors of each candidate. Since the target's appearanceoften changes slowly in a video sequence, it is assumed that the target in thecurrent frame and the best candidates of a small number of previous frames,belong to a common subspace. This is imposed using joint sparse representationto enforce the target and previous best candidates to have a common sparsitypattern. Moreover, an occlusion detection scheme is proposed that usespatch-reconstruction errors and a prior probability of occlusion, extractedfrom an adaptive Markov chain, to calculate the probability of occlusion perpatch. In each frame, occluded patches are excluded when updating thedictionary. Extensive experimental results on several challenging sequencesshows that the proposed method outperforms state-of-the-art trackers.
arxiv-5100-98 | Consistency of Causal Inference under the Additive Noise Model | http://arxiv.org/pdf/1312.5770v3.pdf | author:Samory Kpotufe, Eleni Sgouritsa, Dominik Janzing, Bernhard Schölkopf category:cs.LG stat.ML published:2013-12-19 summary:We analyze a family of methods for statistical causal inference from sampleunder the so-called Additive Noise Model. While most work on the subject hasconcentrated on establishing the soundness of the Additive Noise Model, thestatistical consistency of the resulting inference methods has received littleattention. We derive general conditions under which the given family ofinference methods consistently infers the causal direction in a nonparametricsetting.
arxiv-5100-99 | Comparative analysis of common edge detection techniques in context of object extraction | http://arxiv.org/pdf/1405.6132v1.pdf | author:S. K. Katiyar, P. V. Arun category:cs.CV published:2014-02-05 summary:Edges characterize boundaries and are therefore a problem of practicalimportance in remote sensing.In this paper a comparative study of various edgedetection techniques and band wise analysis of these algorithms in the contextof object extraction with regard to remote sensing satellite images from theIndian Remote Sensing Satellite (IRS) sensors LISS 3, LISS 4 and Cartosat1 aswell as Google Earth is presented.
arxiv-5100-100 | Learning Ordered Representations with Nested Dropout | http://arxiv.org/pdf/1402.0915v1.pdf | author:Oren Rippel, Michael A. Gelbart, Ryan P. Adams category:stat.ML cs.LG published:2014-02-05 summary:In this paper, we study ordered representations of data in which differentdimensions have different degrees of importance. To learn these representationswe introduce nested dropout, a procedure for stochastically removing coherentnested sets of hidden units in a neural network. We first present a sequence oftheoretical results in the simple case of a semi-linear autoencoder. Werigorously show that the application of nested dropout enforces identifiabilityof the units, which leads to an exact equivalence with PCA. We then extend thealgorithm to deep models and demonstrate the relevance of orderedrepresentations to a number of applications. Specifically, we use the orderedproperty of the learned codes to construct hash-based data structures thatpermit very fast retrieval, achieving retrieval in time logarithmic in thedatabase size and independent of the dimensionality of the representation. Thisallows codes that are hundreds of times longer than currently feasible forretrieval. We therefore avoid the diminished quality associated with shortcodes, while still performing retrieval that is competitive in speed withexisting methods. We also show that ordered representations are a promising wayto learn adaptive compression for efficient online data reconstruction.
arxiv-5100-101 | Discovering Latent Network Structure in Point Process Data | http://arxiv.org/pdf/1402.0914v1.pdf | author:Scott W. Linderman, Ryan P. Adams category:stat.ML cs.LG published:2014-02-04 summary:Networks play a central role in modern data analysis, enabling us to reasonabout systems by studying the relationships between their parts. Most often innetwork analysis, the edges are given. However, in many systems it is difficultor impossible to measure the network directly. Examples of latent networksinclude economic interactions linking financial instruments and patterns ofreciprocity in gang violence. In these cases, we are limited to noisyobservations of events associated with each node. To enable analysis of theseimplicit networks, we develop a probabilistic model that combinesmutually-exciting point processes with random graph models. We show how thePoisson superposition principle enables an elegant auxiliary variableformulation and a fully-Bayesian, parallel inference algorithm. We evaluatethis new model empirically on several datasets.
arxiv-5100-102 | Jointly Clustering Rows and Columns of Binary Matrices: Algorithms and Trade-offs | http://arxiv.org/pdf/1310.0512v2.pdf | author:Jiaming Xu, Rui Wu, Kai Zhu, Bruce Hajek, R. Srikant, Lei Ying category:stat.ML published:2013-10-01 summary:In standard clustering problems, data points are represented by vectors, andby stacking them together, one forms a data matrix with row or column clusterstructure. In this paper, we consider a class of binary matrices, arising inmany applications, which exhibit both row and column cluster structure, and ourgoal is to exactly recover the underlying row and column clusters by observingonly a small fraction of noisy entries. We first derive a lower bound on theminimum number of observations needed for exact cluster recovery. Then, wepropose three algorithms with different running time and compare the number ofobservations needed by them for successful cluster recovery. Our analyticalresults show smooth time-data trade-offs: one can gradually reduce thecomputational complexity when increasingly more observations are available.
arxiv-5100-103 | Learning Loosely Connected Markov Random Fields | http://arxiv.org/pdf/1204.5540v3.pdf | author:Rui Wu, R. Srikant, Jian Ni category:stat.ML published:2012-04-25 summary:We consider the structure learning problem for graphical models that we callloosely connected Markov random fields, in which the number of short pathsbetween any pair of nodes is small, and present a new conditional independencetest based algorithm for learning the underlying graph structure. The novelmaximization step in our algorithm ensures that the true edges are detectedcorrectly even when there are short cycles in the graph. The number of samplesrequired by our algorithm is C*log p, where p is the size of the graph and theconstant C depends on the parameters of the model. We show that severalpreviously studied models are examples of loosely connected Markov randomfields, and our algorithm achieves the same or lower computational complexitythan the previously designed algorithms for individual cases. We also get newresults for more general graphical models, in particular, our algorithm learnsgeneral Ising models on the Erdos-Renyi random graph G(p, c/p) correctly withrunning time O(np^5).
arxiv-5100-104 | Cognitive Aging as Interplay between Hebbian Learning and Criticality | http://arxiv.org/pdf/1402.0836v1.pdf | author:Sakyasingha Dasgupta category:nlin.AO cs.NE q-bio.NC published:2014-02-04 summary:Cognitive ageing seems to be a story of global degradation. As one ages thereare a number of physical, chemical and biological changes that take place.Therefore it is logical to assume that the brain is no exception to thisphenomenon. The principle purpose of this project is to use models of neuraldynamics and learning based on the underlying principle of self-organisedcriticality, to account for the age related cognitive effects. In this regardlearning in neural networks can serve as a model for the acquisition of skillsand knowledge in early development stages i.e. the ageing process andcriticality in the network serves as the optimum state of cognitive abilities.Possible candidate mechanisms for ageing in a neural network are loss ofconnectivity and neurons, increase in the level of noise, reduction in whitematter or more interestingly longer learning history and the competition amongseveral optimization objectives. In this paper we are primarily interested inthe affect of the longer learning history on memory and thus the optimality inthe brain. Hence it is hypothesized that prolonged learning in the form ofassociative memory patterns can destroy the state of criticality in thenetwork. We base our model on Tsodyks and Markrams [49] model of dynamicsynapses, in the process to explore the effect of combining standard Hebbianlearning with the phenomenon of Self-organised criticality. The project mainlyconsists of evaluations and simulations of networks of integrate andfire-neurons that have been subjected to various combinations of neural-levelageing effects, with the aim of establishing the primary hypothesis andunderstanding the decline of cognitive abilities due to ageing, using one ofits important characteristics, a longer learning history.
arxiv-5100-105 | Corrupted Sensing: Novel Guarantees for Separating Structured Signals | http://arxiv.org/pdf/1305.2524v2.pdf | author:Rina Foygel, Lester Mackey category:cs.IT math.IT math.OC stat.ML published:2013-05-11 summary:We study the problem of corrupted sensing, a generalization of compressedsensing in which one aims to recover a signal from a collection of corrupted orunreliable measurements. While an arbitrary signal cannot be recovered in theface of arbitrary corruption, tractable recovery is possible when both signaland corruption are suitably structured. We quantify the relationship betweensignal recovery and two geometric measures of structure, the Gaussiancomplexity of a tangent cone and the Gaussian distance to a subdifferential. Wetake a convex programming approach to disentangling signal and corruption,analyzing both penalized programs that trade off between signal and corruptioncomplexity, and constrained programs that bound the complexity of signal orcorruption when prior information is available. In each case, we provideconditions for exact signal recovery from structured corruption and stablesignal recovery from structured corruption with added unstructured noise. Oursimulations demonstrate close agreement between our theoretical recovery boundsand the sharp phase transitions observed in practice. In addition, we providenew interpretable bounds for the Gaussian complexity of sparse vectors,block-sparse vectors, and low-rank matrices, which lead to sharper guaranteesof recovery when combined with our results and those in the literature.
arxiv-5100-106 | Sequential Model-Based Ensemble Optimization | http://arxiv.org/pdf/1402.0796v1.pdf | author:Alexandre Lacoste, Hugo Larochelle, François Laviolette, Mario Marchand category:cs.LG stat.ML published:2014-02-04 summary:One of the most tedious tasks in the application of machine learning is modelselection, i.e. hyperparameter selection. Fortunately, recent progress has beenmade in the automation of this process, through the use of sequentialmodel-based optimization (SMBO) methods. This can be used to optimize across-validation performance of a learning algorithm over the value of itshyperparameters. However, it is well known that ensembles of learned modelsalmost consistently outperform a single model, even if properly selected. Inthis paper, we thus propose an extension of SMBO methods that automaticallyconstructs such ensembles. This method builds on a recently proposed ensembleconstruction paradigm known as agnostic Bayesian learning. In experiments on 22regression and 39 classification data sets, we confirm the success of thisproposed approach, which is able to outperform model selection with SMBO.
arxiv-5100-107 | One-Pass, One-Hash n-Gram Statistics Estimation | http://arxiv.org/pdf/cs/0610010v4.pdf | author:Daniel Lemire, Owen Kaser category:cs.DB cs.CL published:2006-10-03 summary:In multimedia, text or bioinformatics databases, applications query sequencesof n consecutive symbols called n-grams. Estimating the number of distinctn-grams is a view-size estimation problem. While view sizes can be estimated bysampling under statistical assumptions, we desire an unassuming algorithm withuniversally valid accuracy bounds. Most related work has focused on repeatedlyhashing the data, which is prohibitive for large data sources. We prove that aone-pass one-hash algorithm is sufficient for accurate estimates if the hashingis sufficiently independent. To reduce costs further, we investigate recursiverandom hashing algorithms and show that they are sufficiently independent inpractice. We compare our running times with exact counts using suffix arraysand show that, while we use hardly any storage, we are an order of magnitudefaster. The approach further is extended to a one-pass/one-hash computation ofn-gram entropy and iceberg counts. The experiments use a large collection ofEnglish text from the Gutenberg Project as well as synthetic data.
arxiv-5100-108 | Signal to Noise Ratio in Lensless Compressive Imaging | http://arxiv.org/pdf/1402.0785v1.pdf | author:Hong Jiang, Gang Huang, Paul Wilford category:cs.CV published:2014-02-04 summary:We analyze the signal to noise ratio (SNR) in a lensless compressive imaging(LCI) architecture. The architecture consists of a sensor of a single detectingelement and an aperture assembly of an array of programmable elements. LCI canbe used in conjunction with compressive sensing to capture images in acompressed form of compressive measurements. In this paper, we perform SNRanalysis of the LCI and compare it with imaging with a pinhole or a lens. Wewill show that the SNR in the LCI is independent of the image resolution, whilethe SNR in either pinhole aperture imaging or lens aperture imaging decreasesas the image resolution increases. Consequently, the SNR in the LCI is muchhigher if the image resolution is large enough.
arxiv-5100-109 | Microstrip Coupler Design Using Bat Algorithm | http://arxiv.org/pdf/1402.0708v1.pdf | author:Ezgi Deniz Ulker, Sadik Ulker category:cs.NE published:2014-02-04 summary:Evolutionary and swarm algorithms have found many applications in designproblems since todays computing power enables these algorithms to findsolutions to complicated design problems very fast. Newly proposed hybridalgorithm, bat algorithm, has been applied for the design of microwavemicrostrip couplers for the first time. Simulation results indicate that thebat algorithm is a very fast algorithm and it produces very reliable results.
arxiv-5100-110 | A Study of Local Binary Pattern Method for Facial Expression Detection | http://arxiv.org/pdf/1405.6130v1.pdf | author:Ms. Drashti H. Bhatt, Mr. Kirit R. Rathod, Mr. Shardul J. Agravat category:cs.CV published:2014-02-04 summary:Face detection is a basic task for expression recognition. The reliability offace detection & face recognition approach has a major role on the performanceand usability of the entire system. There are several ways to undergo facedetection & recognition. We can use Image Processing Operations, variousclassifiers, filters or virtual machines for the former. Various strategies arebeing available for Facial Expression Detection. The field of facial expressiondetection can have various applications along with its importance & can beinteracted between human being & computer. Many few options are available toidentify a face in an image in accurate & efficient manner. Local BinaryPattern (LBP) based texture algorithms have gained popularity in these years.LBP is an effective approach to have facial expression recognition & is afeature-based approach.
arxiv-5100-111 | Local Gaussian Regression | http://arxiv.org/pdf/1402.0645v1.pdf | author:Franziska Meier, Philipp Hennig, Stefan Schaal category:cs.LG cs.RO published:2014-02-04 summary:Locally weighted regression was created as a nonparametric learning methodthat is computationally efficient, can learn from very large amounts of dataand add data incrementally. An interesting feature of locally weightedregression is that it can work with spatially varying length scales, abeneficial property, for instance, in control problems. However, it does notprovide a generative model for function values and requires training and testdata to be generated identically, independently. Gaussian (process) regression,on the other hand, provides a fully generative model without significant formalrequirements on the distribution of training data, but has much highercomputational cost and usually works with one global scale per input dimension.Using a localising function basis and approximate inference techniques, we takeGaussian (process) regression to increasingly localised properties and towardthe same computational complexity class as locally weighted regression.
arxiv-5100-112 | Hypothesis Testing in High-Dimensional Regression under the Gaussian Random Design Model: Asymptotic Theory | http://arxiv.org/pdf/1301.4240v3.pdf | author:Adel Javanmard, Andrea Montanari category:stat.ME cs.IT math.IT math.ST stat.ML stat.TH published:2013-01-17 summary:We consider linear regression in the high-dimensional regime where the numberof observations $n$ is smaller than the number of parameters $p$. A verysuccessful approach in this setting uses $\ell_1$-penalized least squares(a.k.a. the Lasso) to search for a subset of $s_0< n$ parameters that bestexplain the data, while setting the other parameters to zero. Considerableamount of work has been devoted to characterizing the estimation and modelselection problems within this approach. In this paper we consider instead the fundamental, but far less understood,question of \emph{statistical significance}. More precisely, we address theproblem of computing p-values for single regression coefficients. On one hand, we develop a general upper bound on the minimax power of testswith a given significance level. On the other, we prove that this upper boundis (nearly) achievable through a practical procedure in the case of randomdesign matrices with independent entries. Our approach is based on a debiasingof the Lasso estimator. The analysis builds on a rigorous characterization ofthe asymptotic distribution of the Lasso estimator and its debiased version.Our result holds for optimal sample size, i.e., when $n$ is at least on theorder of $s_0 \log(p/s_0)$. We generalize our approach to random design matrices with i.i.d. Gaussianrows $x_i\sim N(0,\Sigma)$. In this case we prove that a similar distributionalcharacterization (termed `standard distributional limit') holds for $n$ muchlarger than $s_0(\log p)^2$. Finally, we show that for optimal sample size, $n$ being at least of order$s_0 \log(p/s_0)$, the standard distributional limit for general Gaussiandesigns can be derived from the replica heuristics in statistical physics.
arxiv-5100-113 | Scene Labeling with Contextual Hierarchical Models | http://arxiv.org/pdf/1402.0595v1.pdf | author:Mojtaba Seyedhosseini, Tolga Tasdizen category:cs.CV published:2014-02-04 summary:Scene labeling is the problem of assigning an object label to each pixel. Itunifies the image segmentation and object recognition problems. The importanceof using contextual information in scene labeling frameworks has been widelyrealized in the field. We propose a contextual framework, called contextualhierarchical model (CHM), which learns contextual information in a hierarchicalframework for scene labeling. At each level of the hierarchy, a classifier istrained based on downsampled input images and outputs of previous levels. Ourmodel then incorporates the resulting multi-resolution contextual informationinto a classifier to segment the input image at original resolution. Thistraining strategy allows for optimization of a joint posterior probability atmultiple resolutions through the hierarchy. Contextual hierarchical model ispurely based on the input image patches and does not make use of any fragmentsor shape examples. Hence, it is applicable to a variety of problems such asobject segmentation and edge detection. We demonstrate that CHM outperformsstate-of-the-art on Stanford background and Weizmann horse datasets. It alsooutperforms state-of-the-art edge detection methods on NYU depth dataset andachieves state-of-the-art on Berkeley segmentation dataset (BSDS 500).
arxiv-5100-114 | Topic Segmentation and Labeling in Asynchronous Conversations | http://arxiv.org/pdf/1402.0586v1.pdf | author:Shafiq Rayhan Joty, Giuseppe Carenini, Raymond T Ng category:cs.CL published:2014-02-04 summary:Topic segmentation and labeling is often considered a prerequisite forhigher-level conversation analysis and has been shown to be useful in manyNatural Language Processing (NLP) applications. We present two new corpora ofemail and blog conversations annotated with topics, and evaluate annotatorreliability for the segmentation and labeling tasks in these asynchronousconversations. We propose a complete computational framework for topicsegmentation and labeling in asynchronous conversations. Our approach extendsstate-of-the-art methods by considering a fine-grained structure of anasynchronous conversation, along with other conversational features by applyingrecent graph-based methods for NLP. For topic segmentation, we propose twonovel unsupervised models that exploit the fine-grained conversationalstructure, and a novel graph-theoretic supervised model that combines lexical,conversational and topic features. For topic labeling, we propose two novel(unsupervised) random walk models that respectively capture conversationspecific clues from two different sources: the leading sentences and thefine-grained conversational structure. Empirical evaluation shows that thesegmentation and the labeling performed by our best models beat thestate-of-the-art, and are highly correlated with human annotations.
arxiv-5100-115 | Natural Language Inference for Arabic Using Extended Tree Edit Distance with Subtrees | http://arxiv.org/pdf/1402.0578v1.pdf | author:Maytham Alabbas, Allan Ramsay category:cs.CL published:2014-02-04 summary:Many natural language processing (NLP) applications require the computationof similarities between pairs of syntactic or semantic trees. Many researchershave used tree edit distance for this task, but this technique suffers from thedrawback that it deals with single node operations only. We have extended thestandard tree edit distance algorithm to deal with subtree transformationoperations as well as single nodes. The extended algorithm with subtreeoperations, TED+ST, is more effective and flexible than the standard algorithm,especially for applications that pay attention to relations among nodes (e.g.in linguistic trees, deleting a modifier subtree should be cheaper than the sumof deleting its components individually). We describe the use of TED+ST forchecking entailment between two Arabic text snippets. The preliminary resultsof using TED+ST were encouraging when compared with two string-based approachesand with the standard algorithm.
arxiv-5100-116 | A Survey on Latent Tree Models and Applications | http://arxiv.org/pdf/1402.0577v1.pdf | author:Raphaël Mourad, Christine Sinoquet, Nevin L. Zhang, Tengfei Liu, Philippe Leray category:cs.LG published:2014-02-04 summary:In data analysis, latent variables play a central role because they helpprovide powerful insights into a wide variety of phenomena, ranging frombiological to human sciences. The latent tree model, a particular type ofprobabilistic graphical models, deserves attention. Its simple structure - atree - allows simple and efficient inference, while its latent variablescapture complex relationships. In the past decade, the latent tree model hasbeen subject to significant theoretical and methodological developments. Inthis review, we propose a comprehensive study of this model. First we summarizekey ideas underlying the model. Second we explain how it can be efficientlylearned from data. Third we illustrate its use within three types ofapplications: latent structure discovery, multidimensional clustering, andprobabilistic inference. Finally, we conclude and give promising directions forfuture researches in this field.
arxiv-5100-117 | Learning to Predict from Textual Data | http://arxiv.org/pdf/1402.0574v1.pdf | author:Kira Radinsky, Sagie Davidovich, Shaul Markovitch category:cs.CL cs.AI cs.IR published:2014-02-04 summary:Given a current news event, we tackle the problem of generating plausiblepredictions of future events it might cause. We present a new methodology formodeling and predicting such future news events using machine learning and datamining techniques. Our Pundit algorithm generalizes examples of causality pairsto infer a causality predictor. To obtain precisely labeled causality examples,we mine 150 years of news articles and apply semantic natural language modelingtechniques to headlines containing certain predefined causality patterns. Forgeneralization, the model uses a vast number of world knowledge ontologies.Empirical evaluation on real news articles shows that our Pundit algorithmperforms as well as non-expert humans.
arxiv-5100-118 | A Feature Subset Selection Algorithm Automatic Recommendation Method | http://arxiv.org/pdf/1402.0570v1.pdf | author:Guangtao Wang, Qinbao Song, Heli Sun, Xueying Zhang, Baowen Xu, Yuming Zhou category:cs.LG published:2014-02-04 summary:Many feature subset selection (FSS) algorithms have been proposed, but notall of them are appropriate for a given feature selection problem. At the sametime, so far there is rarely a good way to choose appropriate FSS algorithmsfor the problem at hand. Thus, FSS algorithm automatic recommendation is veryimportant and practically useful. In this paper, a meta learning based FSSalgorithm automatic recommendation method is presented. The proposed methodfirst identifies the data sets that are most similar to the one at hand by thek-nearest neighbor classification algorithm, and the distances among these datasets are calculated based on the commonly-used data set characteristics. Then,it ranks all the candidate FSS algorithms according to their performance onthese similar data sets, and chooses the algorithms with best performance asthe appropriate ones. The performance of the candidate FSS algorithms isevaluated by a multi-criteria metric that takes into account not only theclassification accuracy over the selected features, but also the runtime offeature selection and the number of selected features. The proposedrecommendation method is extensively tested on 115 real world data sets with 22well-known and frequently-used different FSS algorithms for five representativeclassifiers. The results show the effectiveness of our proposed FSS algorithmrecommendation method.
arxiv-5100-119 | Evaluating Indirect Strategies for Chinese-Spanish Statistical Machine Translation | http://arxiv.org/pdf/1402.0563v1.pdf | author:Marta R. Costa-jussà, Carlos A. Henríquez, Rafael E. Banchs category:cs.CL published:2014-02-04 summary:Although, Chinese and Spanish are two of the most spoken languages in theworld, not much research has been done in machine translation for this languagepair. This paper focuses on investigating the state-of-the-art ofChinese-to-Spanish statistical machine translation (SMT), which nowadays is oneof the most popular approaches to machine translation. For this purpose, wereport details of the available parallel corpus which are Basic TravellerExpressions Corpus (BTEC), Holy Bible and United Nations (UN). Additionally, weconduct experimental work with the largest of these three corpora to explorealternative SMT strategies by means of using a pivot language. Threealternatives are considered for pivoting: cascading, pseudo-corpus andtriangulation. As pivot language, we use either English, Arabic or French.Results show that, for a phrase-based SMT system, English is the best pivotlanguage between Chinese and Spanish. We propose a system output combinationusing the pivot strategies which is capable of outperforming the directtranslation strategy. The main objective of this work is motivating andinvolving the research community to work in this important pair of languagesgiven their demographic impact.
arxiv-5100-120 | Safe Exploration of State and Action Spaces in Reinforcement Learning | http://arxiv.org/pdf/1402.0560v1.pdf | author:Javier Garcia, Fernando Fernandez category:cs.LG cs.AI published:2014-02-04 summary:In this paper, we consider the important problem of safe exploration inreinforcement learning. While reinforcement learning is well-suited to domainswith complex transition dynamics and high-dimensional state-action spaces, anadditional challenge is posed by the need for safe and efficient exploration.Traditional exploration techniques are not particularly useful for solvingdangerous tasks, where the trial and error process may lead to the selection ofactions whose execution in some states may result in damage to the learningsystem (or any other system). Consequently, when an agent begins an interactionwith a dangerous and high-dimensional state-action space, an important questionarises; namely, that of how to avoid (or at least minimize) damage caused bythe exploration of the state-action space. We introduce the PI-SRL algorithmwhich safely improves suboptimal albeit robust behaviors for continuous stateand action control tasks and which efficiently learns from the experiencegained from the environment. We evaluate the proposed method in four complextasks: automatic car parking, pole-balancing, helicopter hovering, and businessmanagement.
arxiv-5100-121 | Parameterized Complexity Results for Exact Bayesian Network Structure Learning | http://arxiv.org/pdf/1402.0558v1.pdf | author:Sebastian Ordyniak, Stefan Szeider category:cs.AI cs.LG published:2014-02-04 summary:Bayesian network structure learning is the notoriously difficult problem ofdiscovering a Bayesian network that optimally represents a given set oftraining data. In this paper we study the computational worst-case complexityof exact Bayesian network structure learning under graph theoretic restrictionson the (directed) super-structure. The super-structure is an undirected graphthat contains as subgraphs the skeletons of solution networks. We introduce thedirected super-structure as a natural generalization of its undirectedcounterpart. Our results apply to several variants of score-based Bayesiannetwork structure learning where the score of a network decomposes into localscores of its nodes. Results: We show that exact Bayesian network structurelearning can be carried out in non-uniform polynomial time if thesuper-structure has bounded treewidth, and in linear time if in addition thesuper-structure has bounded maximum degree. Furthermore, we show that if thedirected super-structure is acyclic, then exact Bayesian network structurelearning can be carried out in quadratic time. We complement these positiveresults with a number of hardness results. We show that both restrictions(treewidth and degree) are essential and cannot be dropped without loosinguniform polynomial time tractability (subject to a complexity-theoreticassumption). Similarly, exact Bayesian network structure learning remainsNP-hard for "almost acyclic" directed super-structures. Furthermore, we showthat the restrictions remain essential if we do not search for a globallyoptimal network but aim to improve a given network by means of at most k arcadditions, arc deletions, or arc reversals (k-neighborhood local search).
arxiv-5100-122 | Generating Extractive Summaries of Scientific Paradigms | http://arxiv.org/pdf/1402.0556v1.pdf | author:Vahed Qazvinian, Dragomir R. Radev, Saif M. Mohammad, Bonnie Dorr, David Zajic, Michael Whidby, Taesun Moon category:cs.IR cs.CL published:2014-02-04 summary:Researchers and scientists increasingly find themselves in the position ofhaving to quickly understand large amounts of technical material. Our goal isto effectively serve this need by using bibliometric text mining andsummarization techniques to generate summaries of scientific literature. Weshow how we can use citations to produce automatically generated, readilyconsumable, technical extractive summaries. We first propose C-LexRank, a modelfor summarizing single scientific articles based on citations, which employscommunity detection and extracts salient information-rich sentences. Next, wefurther extend our experiments to summarize a set of papers, which cover thesame scientific topic. We generate extractive summaries of a set of QuestionAnswering (QA) and Dependency Parsing (DP) papers, their abstracts, and theircitation sentences and show that citations have unique information amenable tocreating a summary.
arxiv-5100-123 | How Does Latent Semantic Analysis Work? A Visualisation Approach | http://arxiv.org/pdf/1402.0543v1.pdf | author:Jan Koeman, William Rea category:cs.CL cs.IR published:2014-02-03 summary:By using a small example, an analogy to photographic compression, and asimple visualization using heatmaps, we show that latent semantic analysis(LSA) is able to extract what appears to be semantic meaning of words from aset of documents by blurring the distinctions between the words.
arxiv-5100-124 | Applying Supervised Learning Algorithms and a New Feature Selection Method to Predict Coronary Artery Disease | http://arxiv.org/pdf/1402.0459v1.pdf | author:Hubert Haoyang Duan category:cs.LG stat.ML published:2014-02-03 summary:From a fresh data science perspective, this thesis discusses the predictionof coronary artery disease based on genetic variations at the DNA base pairlevel, called Single-Nucleotide Polymorphisms (SNPs), collected from theOntario Heart Genomics Study (OHGS). First, the thesis explains two commonly used supervised learning algorithms,the k-Nearest Neighbour (k-NN) and Random Forest classifiers, and includes acomplete proof that the k-NN classifier is universally consistent in any finitedimensional normed vector space. Second, the thesis introduces twodimensionality reduction steps, Random Projections, a known feature extractiontechnique based on the Johnson-Lindenstrauss lemma, and a new method termedMass Transportation Distance (MTD) Feature Selection for discrete domains.Then, this thesis compares the performance of Random Projections with the k-NNclassifier against MTD Feature Selection and Random Forest, for predictingartery disease based on accuracy, the F-Measure, and area under the ReceiverOperating Characteristic (ROC) curve. The comparative results demonstrate that MTD Feature Selection with RandomForest is vastly superior to Random Projections and k-NN. The Random Forestclassifier is able to obtain an accuracy of 0.6660 and an area under the ROCcurve of 0.8562 on the OHGS genetic dataset, when 3335 SNPs are selected by MTDFeature Selection for classification. This area is considerably better than theprevious high score of 0.608 obtained by Davies et al. in 2010 on the samedataset.
arxiv-5100-125 | A Lower Bound for the Variance of Estimators for Nakagami m Distribution | http://arxiv.org/pdf/1402.0452v1.pdf | author:Rangeet Mitra, Amit Kumar Mishra, Tarun Choubisa category:cs.LG published:2014-02-03 summary:Recently, we have proposed a maximum likelihood iterative algorithm forestimation of the parameters of the Nakagami-m distribution. This techniqueperforms better than state of art estimation techniques for this distribution.This could be of particular use in low data or block based estimation problems.In these scenarios, the estimator should be able to give accurate estimates inthe mean square sense with less amounts of data. Also, the estimates shouldimprove with the increase in number of blocks received. In this paper, we seethrough our simulations, that our proposal is well designed for suchrequirements. Further, it is well known in the literature that an efficientestimator does not exist for Nakagami-m distribution. In this paper, we derivea theoretical expression for the variance of our proposed estimator. We findthat this expression clearly fits the experimental curve for the variance ofthe proposed estimator. This expression is pretty close to the cramer-rao lowerbound(CRLB).
arxiv-5100-126 | A high-reproducibility and high-accuracy method for automated topic classification | http://arxiv.org/pdf/1402.0422v1.pdf | author:Andrea Lancichinetti, M. Irmak Sirer, Jane X. Wang, Daniel Acuna, Konrad Körding, Luís A. Nunes Amaral category:stat.ML cs.IR cs.LG physics.soc-ph published:2014-02-03 summary:Much of human knowledge sits in large databases of unstructured text.Leveraging this knowledge requires algorithms that extract and record metadataon unstructured text documents. Assigning topics to documents will enableintelligent search, statistical characterization, and meaningfulclassification. Latent Dirichlet allocation (LDA) is the state-of-the-art intopic classification. Here, we perform a systematic theoretical and numericalanalysis that demonstrates that current optimization techniques for LDA oftenyield results which are not accurate in inferring the most suitable modelparameters. Adapting approaches for community detection in networks, we proposea new algorithm which displays high-reproducibility and high-accuracy, and alsohas high computational efficiency. We apply it to a large set of documents inthe English Wikipedia and reveal its hierarchical structure. Our algorithmpromises to make "big data" text analysis systems more reliable.
arxiv-5100-127 | Multidiscipinary Optimization For Gas Turbines Design | http://arxiv.org/pdf/1402.0420v1.pdf | author:Francesco Bertini, Lorenzo Dal Mas, Luca Vassio, Enrico Ampellio category:math.OC cs.NE published:2014-02-03 summary:State-of-the-art aeronautic Low Pressure gas Turbines (LPTs) are alreadycharacterized by high quality standards, thus they offer very narrow margins ofimprovement. Typical design process starts with a Concept Design (CD) phase,defined using mean-line 1D and other low-order tools, and evolves through aPreliminary Design (PD) phase, which allows the geometric definition indetails. In this framework, multidisciplinary optimization is the only way toproperly handle the complicated peculiarities of the design. The authorspresent different strategies and algorithms that have been implementedexploiting the PD phase as a real-like design benchmark to illustrate results.The purpose of this work is to describe the optimization techniques, theirsettings and how to implement them effectively in a multidisciplinaryenvironment. Starting from a basic gradient method and a semi-random secondorder method, the authors have introduced an Artificial Bee Colony-likeoptimizer, a multi-objective Genetic Diversity Evolutionary Algorithm [1] and amulti-objective response surface approach based on Artificial Neural Network,parallelizing and customizing them for the gas turbine study. Moreover, speedupand improvement arrangements are embedded in different hybrid strategies withthe aim at finding the best solutions for different kind of problems that arisein this field.
arxiv-5100-128 | Associative Memories Based on Multiple-Valued Sparse Clustered Networks | http://arxiv.org/pdf/1402.0808v1.pdf | author:Hooman Jarollahi, Naoya Onizawa, Takahiro Hanyu, Warren J. Gross category:cs.NE published:2014-02-03 summary:Associative memories are structures that store data patterns and retrievethem given partial inputs. Sparse Clustered Networks (SCNs) arerecently-introduced binary-weighted associative memories that significantlyimprove the storage and retrieval capabilities over the prior state-of-the art.However, deleting or updating the data patterns result in a significantincrease in the data retrieval error probability. In this paper, we propose analgorithm to address this problem by incorporating multiple-valued weights forthe interconnections used in the network. The proposed algorithm lowers theerror rate by an order of magnitude for our sample network with 60% deletedcontents. We then investigate the advantages of the proposed algorithm forhardware implementations.
arxiv-5100-129 | Thompson Sampling for Contextual Bandits with Linear Payoffs | http://arxiv.org/pdf/1209.3352v4.pdf | author:Shipra Agrawal, Navin Goyal category:cs.LG cs.DS stat.ML 68W40, 68Q25 F.2.0 published:2012-09-15 summary:Thompson Sampling is one of the oldest heuristics for multi-armed banditproblems. It is a randomized algorithm based on Bayesian ideas, and hasrecently generated significant interest after several studies demonstrated itto have better empirical performance compared to the state-of-the-art methods.However, many questions regarding its theoretical performance remained open. Inthis paper, we design and analyze a generalization of Thompson Samplingalgorithm for the stochastic contextual multi-armed bandit problem with linearpayoff functions, when the contexts are provided by an adaptive adversary. Thisis among the most important and widely studied versions of the contextualbandits problem. We provide the first theoretical guarantees for the contextualversion of Thompson Sampling. We prove a high probability regret bound of$\tilde{O}(d^{3/2}\sqrt{T})$ (or $\tilde{O}(d\sqrt{T \log(N)})$), which is thebest regret bound achieved by any computationally efficient algorithm availablefor this problem in the current literature, and is within a factor of$\sqrt{d}$ (or $\sqrt{\log(N)}$) of the information-theoretic lower bound forthis problem.
arxiv-5100-130 | Learning to Optimize Via Posterior Sampling | http://arxiv.org/pdf/1301.2609v5.pdf | author:Daniel Russo, Benjamin Van Roy category:cs.LG published:2013-01-11 summary:This paper considers the use of a simple posterior sampling algorithm tobalance between exploration and exploitation when learning to optimize actionssuch as in multi-armed bandit problems. The algorithm, also known as ThompsonSampling, offers significant advantages over the popular upper confidence bound(UCB) approach, and can be applied to problems with finite or infinite actionspaces and complicated relationships among action rewards. We make twotheoretical contributions. The first establishes a connection between posteriorsampling and UCB algorithms. This result lets us convert regret boundsdeveloped for UCB algorithms into Bayesian regret bounds for posteriorsampling. Our second theoretical contribution is a Bayesian regret bound forposterior sampling that applies broadly and can be specialized to many modelclasses. This bound depends on a new notion we refer to as the eluderdimension, which measures the degree of dependence among action rewards.Compared to UCB algorithm Bayesian regret bounds for specific model classes,our general bound matches the best available for linear models and is strongerthan the best available for generalized linear models. Further, our analysisprovides insight into performance advantages of posterior sampling, which arehighlighted through simulation results that demonstrate performance surpassingrecently proposed UCB algorithms.
arxiv-5100-131 | A Robust Framework for Moving-Object Detection and Vehicular Traffic Density Estimation | http://arxiv.org/pdf/1402.0289v1.pdf | author:Pranam Janney, Glenn Geers category:cs.CV cs.RO cs.SY published:2014-02-03 summary:Intelligent machines require basic information such as moving-objectdetection from videos in order to deduce higher-level semantic information. Inthis paper, we propose a methodology that uses a texture measure to detectmoving objects in video. The methodology is computationally inexpensive,requires minimal parameter fine-tuning and also is resilient to noise,illumination changes, dynamic background and low frame rate. Experimentalresults show that performance of the proposed approach is higher than those ofstate-of-the-art approaches. We also present a framework for vehicular trafficdensity estimation using the foreground object detection technique and presenta comparison between the foreground object detection-based framework and theclassical density state modelling-based framework for vehicular traffic densityestimation.
arxiv-5100-132 | Transductive Learning with Multi-class Volume Approximation | http://arxiv.org/pdf/1402.0288v1.pdf | author:Gang Niu, Bo Dai, Marthinus Christoffel du Plessis, Masashi Sugiyama category:cs.LG stat.ML published:2014-02-03 summary:Given a hypothesis space, the large volume principle by Vladimir Vapnikprioritizes equivalence classes according to their volume in the hypothesisspace. The volume approximation has hitherto been successfully applied tobinary learning problems. In this paper, we extend it naturally to a moregeneral definition which can be applied to several transductive problemsettings, such as multi-class, multi-label and serendipitous learning. Eventhough the resultant learning method involves a non-convex optimizationproblem, the globally optimal solution is almost surely unique and can beobtained in O(n^3) time. We theoretically provide stability and error analysesfor the proposed method, and then experimentally show that it is promising.
arxiv-5100-133 | Principled Graph Matching Algorithms for Integrating Multiple Data Sources | http://arxiv.org/pdf/1402.0282v1.pdf | author:Duo Zhang, Benjamin I. P. Rubinstein, Jim Gemmell category:cs.DB cs.LG stat.ML published:2014-02-03 summary:This paper explores combinatorial optimization for problems of max-weightgraph matching on multi-partite graphs, which arise in integrating multipledata sources. Entity resolution-the data integration problem of performingnoisy joins on structured data-typically proceeds by first hashing each recordinto zero or more blocks, scoring pairs of records that are co-blocked forsimilarity, and then matching pairs of sufficient similarity. In the mostcommon case of matching two sources, it is often desirable for the finalmatching to be one-to-one (a record may be matched with at most one other);members of the database and statistical record linkage communities accomplishsuch matchings in the final stage by weighted bipartite graph matching onsimilarity scores. Such matchings are intuitively appealing: they leverage anatural global property of many real-world entity stores-that of being nearlydeduped-and are known to provide significant improvements to precision andrecall. Unfortunately unlike the bipartite case, exact max-weight matching onmulti-partite graphs is known to be NP-hard. Our two-fold algorithmiccontributions approximate multi-partite max-weight matching: our firstalgorithm borrows optimization techniques common to Bayesian probabilisticinference; our second is a greedy approximation algorithm. In addition to atheoretical guarantee on the latter, we present comparisons on a real-world ERproblem from Bing significantly larger than typically found in the literature,publication data, and on a series of synthetic problems. Our results quantifysignificant improvements due to exploiting multiple sources, which are madepossible by global one-to-one constraints linking otherwise independentmatching sub-problems. We also discover that our algorithms are complementary:one being much more robust under noise, and the other being simple to implementand very fast to run.
arxiv-5100-134 | Recovery guarantees for exemplar-based clustering | http://arxiv.org/pdf/1309.3256v2.pdf | author:Abhinav Nellore, Rachel Ward category:stat.ML cs.CV cs.LG published:2013-09-12 summary:For a certain class of distributions, we prove that the linear programmingrelaxation of $k$-medoids clustering---a variant of $k$-means clustering wheremeans are replaced by exemplars from within the dataset---distinguishes pointsdrawn from nonoverlapping balls with high probability once the number of pointsdrawn and the separation distance between any two balls are sufficiently large.Our results hold in the nontrivial regime where the separation distance issmall enough that points drawn from different balls may be closer to each otherthan points drawn from the same ball; in this case, clustering by thresholdingpairwise distances between points can fail. We also exhibit numerical evidenceof high-probability recovery in a substantially more permissive regime.
arxiv-5100-135 | Learning Information Spread in Content Networks | http://arxiv.org/pdf/1312.6169v2.pdf | author:Cédric Lagnier, Simon Bourigault, Sylvain Lamprier, Ludovic Denoyer, Patrick Gallinari category:cs.LG cs.SI physics.soc-ph published:2013-12-20 summary:We introduce a model for predicting the diffusion of content information onsocial media. When propagation is usually modeled on discrete graph structures,we introduce here a continuous diffusion model, where nodes in a diffusioncascade are projected onto a latent space with the property that theirproximity in this space reflects the temporal diffusion process. We focus onthe task of predicting contaminated users for an initial initial informationsource and provide preliminary results on differents datasets.
arxiv-5100-136 | Collaborative Receptive Field Learning | http://arxiv.org/pdf/1402.0170v1.pdf | author:Shu Kong, Zhuolin Jiang, Qiang Yang category:cs.CV cs.LG cs.MM stat.ML published:2014-02-02 summary:The challenge of object categorization in images is largely due to arbitrarytranslations and scales of the foreground objects. To attack this difficulty,we propose a new approach called collaborative receptive field learning toextract specific receptive fields (RF's) or regions from multiple images, andthe selected RF's are supposed to focus on the foreground objects of a commoncategory. To this end, we solve the problem by maximizing a submodular functionover a similarity graph constructed by a pool of RF candidates. However,measuring pairwise distance of RF's for building the similarity graph is anontrivial problem. Hence, we introduce a similarity metric calledpyramid-error distance (PED) to measure their pairwise distances throughsumming up pyramid-like matching errors over a set of low-level features.Besides, in consistent with the proposed PED, we construct a simplenonparametric classifier for classification. Experimental results show that ourmethod effectively discovers the foreground objects in images, and improvesclassification performance.
arxiv-5100-137 | Extension of Sparse Randomized Kaczmarz Algorithm for Multiple Measurement Vectors | http://arxiv.org/pdf/1401.2288v3.pdf | author:Hemant Kumar Aggarwal, Angshul Majumdar category:cs.NA cs.LG stat.ML published:2014-01-10 summary:The Kaczmarz algorithm is popular for iteratively solving an overdeterminedsystem of linear equations. The traditional Kaczmarz algorithm can approximatethe solution in few sweeps through the equations but a randomized version ofthe Kaczmarz algorithm was shown to converge exponentially and independent ofnumber of equations. Recently an algorithm for finding sparse solution to alinear system of equations has been proposed based on weighted randomizedKaczmarz algorithm. These algorithms solves single measurement vector problem;however there are applications were multiple-measurements are available. Inthis work, the objective is to solve a multiple measurement vector problem withcommon sparse support by modifying the randomized Kaczmarz algorithm. We havealso modeled the problem of face recognition from video as the multiplemeasurement vector problem and solved using our proposed technique. We havecompared the proposed algorithm with state-of-art spectral projected gradientalgorithm for multiple measurement vectors on both real and synthetic datasets.The Monte Carlo simulations confirms that our proposed algorithm have betterrecovery and convergence rate than the MMV version of spectral projectedgradient algorithm under fairness constraints.
arxiv-5100-138 | Video Compressive Sensing for Dynamic MRI | http://arxiv.org/pdf/1401.7715v2.pdf | author:Jianing V. Shi, Wotao Yin, Aswin C. Sankaranarayanan, Richard G. Baraniuk category:cs.CV math.OC published:2014-01-30 summary:We present a video compressive sensing framework, termed kt-CSLDS, toaccelerate the image acquisition process of dynamic magnetic resonance imaging(MRI). We are inspired by a state-of-the-art model for video compressivesensing that utilizes a linear dynamical system (LDS) to model the motionmanifold. Given compressive measurements, the state sequence of an LDS can befirst estimated using system identification techniques. We then reconstruct theobservation matrix using a joint structured sparsity assumption. In particular,we minimize an objective function with a mixture of wavelet sparsity and jointsparsity within the observation matrix. We derive an efficient convexoptimization algorithm through alternating direction method of multipliers(ADMM), and provide a theoretical guarantee for global convergence. Wedemonstrate the performance of our approach for video compressive sensing, interms of reconstruction accuracy. We also investigate the impact of varioussampling strategies. We apply this framework to accelerate the acquisitionprocess of dynamic MRI and show it achieves the best reconstruction accuracywith the least computational time compared with existing algorithms in theliterature.
arxiv-5100-139 | Dual-to-kernel learning with ideals | http://arxiv.org/pdf/1402.0099v1.pdf | author:Franz J. Király, Martin Kreuzer, Louis Theran category:stat.ML cs.LG math.AC math.AG math.ST stat.TH published:2014-02-01 summary:In this paper, we propose a theory which unifies kernel learning and symbolicalgebraic methods. We show that both worlds are inherently dual to each other,and we use this duality to combine the structure-awareness of algebraic methodswith the efficiency and generality of kernels. The main idea lies in relatingpolynomial rings to feature space, and ideals to manifolds, then exploitingthis generative-discriminative duality on kernel matrices. We illustrate thisby proposing two algorithms, IPCA and AVICA, for simultaneous manifold andfeature learning, and test their accuracy on synthetic and real world data.
arxiv-5100-140 | DinTucker: Scaling up Gaussian process models on multidimensional arrays with billions of elements | http://arxiv.org/pdf/1311.2663v5.pdf | author:Shandian Zhe, Yuan Qi, Youngja Park, Ian Molloy, Suresh Chari category:cs.LG cs.DC stat.ML published:2013-11-12 summary:Infinite Tucker Decomposition (InfTucker) and random function prior models,as nonparametric Bayesian models on infinite exchangeable arrays, are morepowerful models than widely-used multilinear factorization methods includingTucker and PARAFAC decomposition, (partly) due to their capability of modelingnonlinear relationships between array elements. Despite their great predictiveperformance and sound theoretical foundations, they cannot handle massive datadue to a prohibitively high training time. To overcome this limitation, wepresent Distributed Infinite Tucker (DINTUCKER), a large-scale nonlinear tensordecomposition algorithm on MAPREDUCE. While maintaining the predictive accuracyof InfTucker, it is scalable on massive data. DINTUCKER is based on a newhierarchical Bayesian model that enables local training of InfTucker onsubarrays and information integration from all local training results. We usedistributed stochastic gradient descent, coupled with variational inference, totrain this model. We apply DINTUCKER to multidimensional arrays with billionsof elements from applications in the "Read the Web" project (Carlson et al.,2010) and in information security and compare it with the state-of-the-artlarge-scale tensor decomposition method, GigaTensor. On both datasets,DINTUCKER achieves significantly higher prediction accuracy with lesscomputational time.
arxiv-5100-141 | Generative Model Selection Using a Scalable and Size-Independent Complex Network Classifier | http://arxiv.org/pdf/1306.2298v3.pdf | author:Sadegh Motallebi, Sadegh Aliakbary, Jafar Habibi category:cs.SI cs.LG physics.soc-ph stat.ML published:2013-06-10 summary:Real networks exhibit nontrivial topological features such as heavy-taileddegree distribution, high clustering, and small-worldness. Researchers havedeveloped several generative models for synthesizing artificial networks thatare structurally similar to real networks. An important research problem is toidentify the generative model that best fits to a target network. In thispaper, we investigate this problem and our goal is to select the model that isable to generate graphs similar to a given network instance. By the means ofgenerating synthetic networks with seven outstanding generative models, we haveutilized machine learning methods to develop a decision tree for modelselection. Our proposed method, which is named "Generative Model Selection forComplex Networks" (GMSCN), outperforms existing methods with respect toaccuracy, scalability and size-independence.
arxiv-5100-142 | Experiments with Three Approaches to Recognizing Lexical Entailment | http://arxiv.org/pdf/1401.8269v1.pdf | author:Peter D. Turney, Saif M. Mohammad category:cs.CL cs.AI cs.LG published:2014-01-31 summary:Inference in natural language often involves recognizing lexical entailment(RLE); that is, identifying whether one word entails another. For example,"buy" entails "own". Two general strategies for RLE have been proposed: Onestrategy is to manually construct an asymmetric similarity measure for contextvectors (directional similarity) and another is to treat RLE as a problem oflearning to recognize semantic relations using supervised machine learningtechniques (relation classification). In this paper, we experiment with tworecent state-of-the-art representatives of the two general strategies. Thefirst approach is an asymmetric similarity measure (an instance of thedirectional similarity strategy), designed to capture the degree to which thecontexts of a word, a, form a subset of the contexts of another word, b. Thesecond approach (an instance of the relation classification strategy)represents a word pair, a:b, with a feature vector that is the concatenation ofthe context vectors of a and b, and then applies supervised learning to atraining set of labeled feature vectors. Additionally, we introduce a thirdapproach that is a new instance of the relation classification strategy. Thethird approach represents a word pair, a:b, with a feature vector in which thefeatures are the differences in the similarities of a and b to a set ofreference words. All three approaches use vector space models (VSMs) ofsemantics, based on word-context matrices. We perform an extensive evaluationof the three approaches using three different datasets. The proposed newapproach (similarity differences) performs significantly better than the othertwo approaches on some datasets and there is no dataset for which it issignificantly worse. Our results suggest it is beneficial to make connectionsbetween the research in lexical entailment and the research in semanticrelation classification.
arxiv-5100-143 | Principal motion components for gesture recognition using a single-example | http://arxiv.org/pdf/1310.4822v2.pdf | author:Hugo Jair Escalante, Isabelle Guyon, Vassilis Athitsos, Pat Jangyodsuk, Jun Wan category:cs.CV 68T45 published:2013-10-17 summary:This paper introduces principal motion components (PMC), a new method forone-shot gesture recognition. In the considered scenario a singletraining-video is available for each gesture to be recognized, which limits theapplication of traditional techniques (e.g., HMMs). In PMC, a 2D map of motionenergy is obtained per each pair of consecutive frames in a video. Motion mapsassociated to a video are processed to obtain a PCA model, which is used forrecognition under a reconstruction-error approach. The main benefits of theproposed approach are its simplicity, easiness of implementation, competitiveperformance and efficiency. We report experimental results in one-shot gesturerecognition using the ChaLearn Gesture Dataset; a benchmark comprising morethan 50,000 gestures, recorded as both RGB and depth video with a Kinectcamera. Results obtained with PMC are competitive with alternative methodsproposed for the same data set.
arxiv-5100-144 | Marginal and simultaneous predictive classification using stratified graphical models | http://arxiv.org/pdf/1401.8078v1.pdf | author:Henrik Nyman, Jie Xiong, Johan Pensar, Jukka Corander category:stat.ML published:2014-01-31 summary:An inductive probabilistic classification rule must generally obey theprinciples of Bayesian predictive inference, such that all observed andunobserved stochastic quantities are jointly modeled and the parameteruncertainty is fully acknowledged through the posterior predictivedistribution. Several such rules have been recently considered and theirasymptotic behavior has been characterized under the assumption that theobserved features or variables used for building a classifier are conditionallyindependent given a simultaneous labeling of both the training samples andthose from an unknown origin. Here we extend the theoretical results topredictive classifiers acknowledging feature dependencies either throughgraphical models or sparser alternatives defined as stratified graphicalmodels. We also show through experimentation with both synthetic and real datathat the predictive classifiers based on stratified graphical models haveconsistently best accuracy compared with the predictive classifiers based oneither conditionally independent features or on ordinary graphical models.
arxiv-5100-145 | Empirically Evaluating Multiagent Learning Algorithms | http://arxiv.org/pdf/1401.8074v1.pdf | author:Erik Zawadzki, Asher Lipson, Kevin Leyton-Brown category:cs.GT cs.LG published:2014-01-31 summary:There exist many algorithms for learning how to play repeated bimatrix games.Most of these algorithms are justified in terms of some sort of theoreticalguarantee. On the other hand, little is known about the empirical performanceof these algorithms. Most such claims in the literature are based on smallexperiments, which has hampered understanding as well as the development of newmultiagent learning (MAL) algorithms. We have developed a new suite of toolsfor running multiagent experiments: the MultiAgent Learning Testbed (MALT).These tools are designed to facilitate larger and more comprehensiveexperiments by removing the need to build one-off experimental code. MALT alsoprovides baseline implementations of many MAL algorithms, hopefully eliminatingor reducing differences between algorithm implementations and increasing thereproducibility of results. Using this test suite, we ran an experimentunprecedented in size. We analyzed the results according to a variety ofperformance metrics including reward, maxmin distance, regret, and severalnotions of equilibrium convergence. We confirmed several pieces of conventionalwisdom, but also discovered some surprising results. For example, we found thatsingle-agent $Q$-learning outperformed many more complicated and more modernMAL algorithms.
arxiv-5100-146 | Hallucinating optimal high-dimensional subspaces | http://arxiv.org/pdf/1401.8053v1.pdf | author:Ognjen Arandjelovic category:cs.CV published:2014-01-31 summary:Linear subspace representations of appearance variation are pervasive incomputer vision. This paper addresses the problem of robustly matching suchsubspaces (computing the similarity between them) when they are used todescribe the scope of variations within sets of images of different (possiblygreatly so) scales. A naive solution of projecting the low-scale subspace intothe high-scale image space is described first and subsequently shown to beinadequate, especially at large scale discrepancies. A successful approach isproposed instead. It consists of (i) an interpolated projection of thelow-scale subspace into the high-scale space, which is followed by (ii) arotation of this initial estimate within the bounds of the imposed``downsampling constraint''. The optimal rotation is found in the closed-formwhich best aligns the high-scale reconstruction of the low-scale subspace withthe reference it is compared to. The method is evaluated on the problem ofmatching sets of (i) face appearances under varying illumination and (ii)object appearances under varying viewpoint, using two large data sets. Incomparison to the naive matching, the proposed algorithm is shown to greatlyincrease the separation of between-class and within-class similarities, as wellas produce far more meaningful modes of common appearance on which the matchscore is based.
arxiv-5100-147 | Sparse Bayesian Unsupervised Learning | http://arxiv.org/pdf/1401.8017v1.pdf | author:Stephane Gaiffas, Bertrand Michel category:stat.ML 62H30 published:2014-01-30 summary:This paper is about variable selection, clustering and estimation in anunsupervised high-dimensional setting. Our approach is based on fittingconstrained Gaussian mixture models, where we learn the number of clusters $K$and the set of relevant variables $S$ using a generalized Bayesian posteriorwith a sparsity inducing prior. We prove a sparsity oracle inequality whichshows that this procedure selects the optimal parameters $K$ and $S$. Thisprocedure is implemented using a Metropolis-Hastings algorithm, based on aclustering-oriented greedy proposal, which makes the convergence to theposterior very fast.
arxiv-5100-148 | Support vector comparison machines | http://arxiv.org/pdf/1401.8008v1.pdf | author:Toby Dylan Hocking, Supaporn Spanurattana, Masashi Sugiyama category:stat.ML cs.LG published:2014-01-30 summary:In ranking problems, the goal is to learn a ranking function from labeledpairs of input points. In this paper, we consider the related comparisonproblem, where the label indicates which element of the pair is better, or ifthere is no significant difference. We cast the learning problem as a marginmaximization, and show that it can be solved by converting it to a standardSVM. We use simulated nonlinear patterns and a real learning to rank sushi dataset to show that our proposed SVMcompare algorithm outperforms SVMrank whenthere are equality pairs.
arxiv-5100-149 | Human Activity Recognition using Smartphone | http://arxiv.org/pdf/1401.8212v1.pdf | author:Amin Rasekh, Chien-An Chen, Yan Lu category:cs.CY cs.HC cs.LG published:2014-01-30 summary:Human activity recognition has wide applications in medical research andhuman survey system. In this project, we design a robust activity recognitionsystem based on a smartphone. The system uses a 3-dimentional smartphoneaccelerometer as the only sensor to collect time series signals, from which 31features are generated in both time and frequency domain. Activities areclassified using 4 different passive learning methods, i.e., quadraticclassifier, k-nearest neighbor algorithm, support vector machine, andartificial neural networks. Dimensionality reduction is performed through bothfeature extraction and subset selection. Besides passive learning, we alsoapply active learning algorithms to reduce data labeling expense. Experimentresults show that the classification rate of passive learning reaches 84.4% andit is robust to common positions and poses of cellphone. The results of activelearning on real data demonstrate a reduction of labeling labor to achievecomparable performance with passive learning.
arxiv-5100-150 | Maximum Margin Multiclass Nearest Neighbors | http://arxiv.org/pdf/1401.7898v1.pdf | author:Aryeh Kontorovich, Roi Weiss category:cs.LG math.ST stat.TH published:2014-01-30 summary:We develop a general framework for margin-based multicategory classificationin metric spaces. The basic work-horse is a margin-regularized version of thenearest-neighbor classifier. We prove generalization bounds that match thestate of the art in sample size $n$ and significantly improve the dependence onthe number of classes $k$. Our point of departure is a nearly Bayes-optimalfinite-sample risk bound independent of $k$. Although $k$-free, this bound isunregularized and non-adaptive, which motivates our main result: Rademacher andscale-sensitive margin bounds with a logarithmic dependence on $k$. As the bestprevious risk estimates in this setting were of order $\sqrt k$, our bound isexponentially sharper. From the algorithmic standpoint, in doubling metricspaces our classifier may be trained on $n$ examples in $O(n^2\log n)$ time andevaluated on new points in $O(\log n)$ time.
arxiv-5100-151 | Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method | http://arxiv.org/pdf/1401.7743v1.pdf | author:T. Balaji, Dr. M. Sumathi category:cs.CV published:2014-01-30 summary:Remote sensing image classification can be performed in many different waysto extract meaningful features. One common approach is to perform edgedetection. A second approach is to try and detect whole shapes, given the factthat these shapes usually tend to have distinctive properties such as objectforeground or background. To get optimal results, these two approaches can becombined. This paper adopts a combinatorial optimization method to adaptivelyselect threshold based features to improve remote sensing image. Featureselection is an important combinatorial optimization problem in the remotesensing image classification. The feature selection method has to achieve threecharacteristics: first the performance issues by facilitating data collectionand reducing storage space and classification time, second to perform semanticsanalysis helping to understand the problem, and third to improve predictionaccuracy by avoiding the curse of dimensionality. The goal of this thresholdingan image is to classify pixels as either dark or light and evaluation ofclassification results. Interactive adaptive thresholding is a form ofthresholding that takes into account spatial variations in illumination ofremote sensing image. We present a technique for remote sensing based adaptivethresholding using the interactive satellite image of the input. However, oursolution is more robust to illumination changes in the remote sensing image.Additionally, our method is simple and easy to implement but it is effectivealgorithm to classify the image pixels. This technique is suitable forpreprocessing the remote sensing image classification, making it a valuabletool for interactive remote based applications such as augmented reality of theclassification procedure.
arxiv-5100-152 | Security Evaluation of Support Vector Machines in Adversarial Environments | http://arxiv.org/pdf/1401.7727v1.pdf | author:Battista Biggio, Igino Corona, Blaine Nelson, Benjamin I. P. Rubinstein, Davide Maiorca, Giorgio Fumera, Giorgio Giacinto, and Fabio Roli category:cs.LG cs.CR published:2014-01-30 summary:Support Vector Machines (SVMs) are among the most popular classificationtechniques adopted in security applications like malware detection, intrusiondetection, and spam filtering. However, if SVMs are to be incorporated inreal-world security systems, they must be able to cope with attack patternsthat can either mislead the learning algorithm (poisoning), evade detection(evasion), or gain information about their internal parameters (privacybreaches). The main contributions of this chapter are twofold. First, weintroduce a formal general framework for the empirical evaluation of thesecurity of machine-learning systems. Second, according to our framework, wedemonstrate the feasibility of evasion, poisoning and privacy attacks againstSVMs in real-world security problems. For each attack technique, we evaluateits impact and discuss whether (and how) it can be countered through anadversary-aware design of SVMs. Our experiments are easily reproducible thanksto open-source code that we have made available, together with all the employeddatasets, on a public repository.
arxiv-5100-153 | A Generalized Probabilistic Framework for Compact Codebook Creation | http://arxiv.org/pdf/1401.7713v1.pdf | author:Lingqiao Liu, Lei Wang, Chunhua Shen category:cs.CV published:2014-01-30 summary:Compact and discriminative visual codebooks are preferred in many visualrecognition tasks. In the literature, a number of works have taken the approachof hierarchically merging visual words of an initial large-sized codebook, butimplemented this approach with different merging criteria. In this work, wepropose a single probabilistic framework to unify these merging criteria, byidentifying two key factors: the function used to model class-conditionaldistribution and the method used to estimate the distribution parameters. Moreimportantly, by adopting new distribution functions and/or parameter estimationmethods, our framework can readily produce a spectrum of novel mergingcriteria. Three of them are specifically focused in this work. In the firstcriterion, we adopt the multinomial distribution with Bayesian method; In thesecond criterion, we integrate Gaussian distribution with maximum likelihoodparameter estimation. In the third criterion, which shows the best mergingperformance, we propose a max-margin-based parameter estimation method andapply it with multinomial distribution. Extensive experimental study isconducted to systematically analyse the performance of the above three criteriaand compare them with existing ones. As demonstrated, the best criterionobtained in our framework achieves the overall best merging performance amongthe comparable merging criteria developed in the literature.
arxiv-5100-154 | Joint Inference of Multiple Label Types in Large Networks | http://arxiv.org/pdf/1401.7709v1.pdf | author:Deepayan Chakrabarti, Stanislav Funiak, Jonathan Chang, Sofus A. Macskassy category:cs.LG cs.SI stat.ML published:2014-01-30 summary:We tackle the problem of inferring node labels in a partially labeled graphwhere each node in the graph has multiple label types and each label type has alarge number of possible labels. Our primary example, and the focus of thispaper, is the joint inference of label types such as hometown, current city,and employers, for users connected by a social network. Standard labelpropagation fails to consider the properties of the label types and theinteractions between them. Our proposed method, called EdgeExplain, explicitlymodels these, while still enabling scalable inference under a distributedmessage-passing architecture. On a billion-node subset of the Facebook socialnetwork, EdgeExplain significantly outperforms label propagation for severallabel types, with lifts of up to 120% for recall@1 and 60% for recall@3.
arxiv-5100-155 | Embed and Conquer: Scalable Embeddings for Kernel k-Means on MapReduce | http://arxiv.org/pdf/1311.2334v4.pdf | author:Ahmed Elgohary, Ahmed K. Farahat, Mohamed S. Kamel, Fakhri Karray category:cs.LG published:2013-11-11 summary:The kernel $k$-means is an effective method for data clustering which extendsthe commonly-used $k$-means algorithm to work on a similarity matrix overcomplex data structures. The kernel $k$-means algorithm is howevercomputationally very complex as it requires the complete data matrix to becalculated and stored. Further, the kernelized nature of the kernel $k$-meansalgorithm hinders the parallelization of its computations on moderninfrastructures for distributed computing. In this paper, we are defining afamily of kernel-based low-dimensional embeddings that allows for scalingkernel $k$-means on MapReduce via an efficient and unified parallelizationstrategy. Afterwards, we propose two methods for low-dimensional embedding thatadhere to our definition of the embedding family. Exploiting the proposedparallelization strategy, we present two scalable MapReduce algorithms forkernel $k$-means. We demonstrate the effectiveness and efficiency of theproposed algorithms through an empirical evaluation on benchmark data sets.
arxiv-5100-156 | RES: Regularized Stochastic BFGS Algorithm | http://arxiv.org/pdf/1401.7625v1.pdf | author:Aryan Mokhtari, Alejandro Ribeiro category:cs.LG math.OC stat.ML published:2014-01-29 summary:RES, a regularized stochastic version of the Broyden-Fletcher-Goldfarb-Shanno(BFGS) quasi-Newton method is proposed to solve convex optimization problemswith stochastic objectives. The use of stochastic gradient descent algorithmsis widespread, but the number of iterations required to approximate optimalarguments can be prohibitive in high dimensional problems. Application ofsecond order methods, on the other hand, is impracticable because computationof objective function Hessian inverses incurs excessive computational cost.BFGS modifies gradient descent by introducing a Hessian approximation matrixcomputed from finite gradient differences. RES utilizes stochastic gradients inlieu of deterministic gradients for both, the determination of descentdirections and the approximation of the objective function's curvature. Sincestochastic gradients can be computed at manageable computational cost RES isrealizable and retains the convergence rate advantages of its deterministiccounterparts. Convergence results show that lower and upper bounds on theHessian egeinvalues of the sample functions are sufficient to guaranteeconvergence to optimal arguments. Numerical experiments showcase reductions inconvergence time relative to stochastic gradient descent algorithms andnon-regularized stochastic versions of BFGS. An application of RES to theimplementation of support vector machines is developed.
arxiv-5100-157 | Bayesian nonparametric comorbidity analysis of psychiatric disorders | http://arxiv.org/pdf/1401.7620v1.pdf | author:Francisco J. R. Ruiz, Isabel Valera, Carlos Blanco, Fernando Perez-Cruz category:stat.ML cs.LG published:2014-01-29 summary:The analysis of comorbidity is an open and complex research field in thebranch of psychiatry, where clinical experience and several studies suggestthat the relation among the psychiatric disorders may have etiological andtreatment implications. In this paper, we are interested in applying latentfeature modeling to find the latent structure behind the psychiatric disordersthat can help to examine and explain the relationships among them. To this end,we use the large amount of information collected in the National EpidemiologicSurvey on Alcohol and Related Conditions (NESARC) database and propose to modelthese data using a nonparametric latent model based on the Indian BuffetProcess (IBP). Due to the discrete nature of the data, we first need to adaptthe observation model for discrete random variables. We propose a generativemodel in which the observations are drawn from a multinomial-logit distributiongiven the IBP matrix. The implementation of an efficient Gibbs sampler isaccomplished using the Laplace approximation, which allows integrating out theweighting factors of the multinomial-logit likelihood model. We also provide avariational inference algorithm for this model, which provides a complementary(and less expensive in terms of computational complexity) alternative to theGibbs sampler allowing us to deal with a larger number of data. Finally, we usethe model to analyze comorbidity among the psychiatric disorders diagnosed byexperts from the NESARC database.
arxiv-5100-158 | Information quantity in a pixel of digital image | http://arxiv.org/pdf/1401.7517v1.pdf | author:M. Kharinov category:cs.CV cs.IT math.IT published:2014-01-29 summary:The paper is devoted to the problem of integer-valued estimating ofinformation quantity in a pixel of digital image. The definition of an integerestimation of information quantity based on constructing of the certain binaryhierarchy of pixel clusters is proposed. The methods for constructinghierarchies of clusters and generating of hierarchical sequences of imageapproximations that minimally differ from the image by a standard deviation aredeveloped. Experimental results on integer-valued estimation of informationquantity are compared with the results obtained by utilizing of the classicalformulas.
arxiv-5100-159 | Use HMM and KNN for classifying corneal data | http://arxiv.org/pdf/1401.7486v1.pdf | author:Payam Porkar Rezaeiye, mehrnoosh bazrafkan, ali akbar movassagh, Mojtaba Sedigh Fazli, Gholam hossein bazyari category:cs.CV published:2014-01-29 summary:These days to gain classification system with high accuracy that can classifycomplicated pattern are so useful in medicine and industry. In this article aprocess for getting the best classifier for Lasik data is suggested. However atfirst it's been tried to find the best line and curve by this classifier inorder to gain classifier fitting, and in the end by using the Markov method aclassifier for topographies is gained.
arxiv-5100-160 | The parametrized probabilistic finite-state transducer probe game player fingerprint model | http://arxiv.org/pdf/1401.7406v1.pdf | author:Jeffrey Tsang category:cs.GT cs.NE published:2014-01-29 summary:Fingerprinting operators generate functional signatures of game players andare useful for their automated analysis independent of representation orencoding. The theory for a fingerprinting operator which returns thelength-weighted probability of a given move pair occurring from playing theinvestigated agent against a general parametrized probabilistic finite-statetransducer (PFT) is developed, applicable to arbitrary iterated games. Resultsfor the distinguishing power of the 1-state opponent model, uniformapproximability of fingerprints of arbitrary players, analyticity and Lipschitzcontinuity of fingerprints for logically possible players, and equicontinuityof the fingerprints of bounded-state probabilistic transducers are derived.Algorithms for the efficient computation of special instances are given; theshortcomings of a previous model, strictly generalized here from a simpleprojection of the new model, are explained in terms of regularity conditionviolations, and the extra power and functional niceness of the new fingerprintsdemonstrated. The 2-state deterministic finite-state transducers (DFTs) arefingerprinted and pairwise distances computed; using this the structure of DFTsin strategy space is elucidated.
arxiv-5100-161 | Bounding Embeddings of VC Classes into Maximum Classes | http://arxiv.org/pdf/1401.7388v1.pdf | author:J. Hyam Rubinstein, Benjamin I. P. Rubinstein, Peter L. Bartlett category:cs.LG math.CO stat.ML published:2014-01-29 summary:One of the earliest conjectures in computational learning theory-the SampleCompression conjecture-asserts that concept classes (equivalently set systems)admit compression schemes of size linear in their VC dimension. To-date thisstatement is known to be true for maximum classes---those that possess maximumcardinality for their VC dimension. The most promising approach to positivelyresolving the conjecture is by embedding general VC classes into maximumclasses without super-linear increase to their VC dimensions, as suchembeddings would extend the known compression schemes to all VC classes. Weshow that maximum classes can be characterised by a local-connectivity propertyof the graph obtained by viewing the class as a cubical complex. This geometriccharacterisation of maximum VC classes is applied to prove a negative embeddingresult which demonstrates VC-d classes that cannot be embedded in any maximumclass of VC dimension lower than 2d. On the other hand, we show that every VC-dclass C embeds in a VC-(d+D) maximum class where D is the deficiency of C,i.e., the difference between the cardinalities of a maximum VC-d class and ofC. For VC-2 classes in binary n-cubes for 4 <= n <= 6, we give best possibleresults on embedding into maximum classes. For some special classes of Booleanfunctions, relationships with maximum classes are investigated. Finally we givea general recursive procedure for embedding VC-d classes into VC-(d+k) maximumclasses for smallest k.
arxiv-5100-162 | Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts | http://arxiv.org/pdf/1401.1974v4.pdf | author:Vu Nguyen, Dinh Phung, XuanLong Nguyen, Svetha Venkatesh, Hung Hai Bui category:cs.LG stat.ML published:2014-01-09 summary:We present a Bayesian nonparametric framework for multilevel clustering whichutilizes group-level context information to simultaneously discoverlow-dimensional structures of the group contents and partitions groups intoclusters. Using the Dirichlet process as the building block, our modelconstructs a product base-measure with a nested structure to accommodatecontent and context observations at multiple levels. The proposed modelpossesses properties that link the nested Dirichlet processes (nDP) and theDirichlet process mixture models (DPM) in an interesting way: integrating outall contents results in the DPM over contexts, whereas integrating outgroup-specific contexts results in the nDP mixture over content variables. Weprovide a Polya-urn view of the model and an efficient collapsed Gibbsinference procedure. Extensive experiments on real-world datasets demonstratethe advantage of utilizing context information via our model in both text andimage domains.
arxiv-5100-163 | Infrared face recognition: a comprehensive review of methodologies and databases | http://arxiv.org/pdf/1401.8261v1.pdf | author:Reza Shoja Ghiass, Ognjen Arandjelovic, Hakim Bendada, Xavier Maldague category:cs.CV published:2014-01-29 summary:Automatic face recognition is an area with immense practical potential whichincludes a wide range of commercial and law enforcement applications. Hence itis unsurprising that it continues to be one of the most active research areasof computer vision. Even after over three decades of intense research, thestate-of-the-art in face recognition continues to improve, benefitting fromadvances in a range of different research fields such as image processing,pattern recognition, computer graphics, and physiology. Systems based onvisible spectrum images, the most researched face recognition modality, havereached a significant level of maturity with some practical success. However,they continue to face challenges in the presence of illumination, pose andexpression changes, as well as facial disguises, all of which can significantlydecrease recognition accuracy. Amongst various approaches which have beenproposed in an attempt to overcome these limitations, the use of infrared (IR)imaging has emerged as a particularly promising research direction. This paperpresents a comprehensive and timely review of the literature on this subject.Our key contributions are: (i) a summary of the inherent properties of infraredimaging which makes this modality promising in the context of face recognition,(ii) a systematic review of the most influential approaches, with a focus onemerging common trends as well as key differences between alternativemethodologies, (iii) a description of the main databases of infrared facialimages available to the researcher, and lastly (iv) a discussion of the mostpromising avenues for future research.
arxiv-5100-164 | Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks | http://arxiv.org/pdf/1312.4461v4.pdf | author:Andrew Davis, Itamar Arel category:cs.LG published:2013-12-16 summary:Scalability properties of deep neural networks raise key research questions,particularly as the problems considered become larger and more challenging.This paper expands on the idea of conditional computation introduced by Bengio,et. al., where the nodes of a deep network are augmented by a set of gatingunits that determine when a node should be calculated. By factorizing theweight matrix into a low-rank approximation, an estimation of the sign of thepre-nonlinearity activation can be efficiently obtained. For networks usingrectified-linear hidden units, this implies that the computation of a hiddenunit with an estimated negative pre-nonlinearity can be ommitted altogether, asits value will become zero when nonlinearity is applied. For sparse neuralnetworks, this can result in considerable speed gains. Experimental resultsusing the MNIST and SVHN data sets with a fully-connected deep neural networkdemonstrate the performance robustness of the proposed scheme with respect tothe error introduced by the conditional computation process.
arxiv-5100-165 | Universal Approximation Depth and Errors of Narrow Belief Networks with Discrete Units | http://arxiv.org/pdf/1303.7461v2.pdf | author:Guido F. Montúfar category:stat.ML cs.LG math.PR published:2013-03-29 summary:We generalize recent theoretical work on the minimal number of layers ofnarrow deep belief networks that can approximate any probability distributionon the states of their visible units arbitrarily well. We relax the setting ofbinary units (Sutskever and Hinton, 2008; Le Roux and Bengio, 2008, 2010;Mont\'ufar and Ay, 2011) to units with arbitrary finite state spaces, and thevanishing approximation error to an arbitrary approximation error tolerance.For example, we show that a $q$-ary deep belief network with $L\geq2+\frac{q^{\lceil m-\delta \rceil}-1}{q-1}$ layers of width $n \leq m +\log_q(m) + 1$ for some $m\in \mathbb{N}$ can approximate any probabilitydistribution on $\{0,1,\ldots,q-1\}^n$ without exceeding a Kullback-Leiblerdivergence of $\delta$. Our analysis covers discrete restricted Boltzmannmachines and na\"ive Bayes models as special cases.
arxiv-5100-166 | Tempering by Subsampling | http://arxiv.org/pdf/1401.7145v1.pdf | author:Jan-Willem van de Meent, Brooks Paige, Frank Wood category:stat.ML published:2014-01-28 summary:In this paper we demonstrate that tempering Markov chain Monte Carlo samplersfor Bayesian models by recursively subsampling observations without replacementcan improve the performance of baseline samplers in terms of effective samplesize per computation. We present two tempering by subsampling algorithms,subsampled parallel tempering and subsampled tempered transitions. We providean asymptotic analysis of the computational cost of tempering by subsampling,verify that tempering by subsampling costs less than traditional tempering, anddemonstrate both algorithms on Bayesian approaches to learning the mean of ahigh dimensional multivariate Normal and estimating Gaussian processhyperparameters.
arxiv-5100-167 | Bayesian Properties of Normalized Maximum Likelihood and its Fast Computation | http://arxiv.org/pdf/1401.7116v1.pdf | author:Andrew Barron, Teemu Roos, Kazuho Watanabe category:cs.IT cs.LG math.IT stat.ML published:2014-01-28 summary:The normalized maximized likelihood (NML) provides the minimax regretsolution in universal data compression, gambling, and prediction, and it playsan essential role in the minimum description length (MDL) method of statisticalmodeling and estimation. Here we show that the normalized maximum likelihoodhas a Bayes-like representation as a mixture of the component models, even infinite samples, though the weights of linear combination may be both positiveand negative. This representation addresses in part the relationship betweenMDL and Bayes modeling. This representation has the advantage of speeding thecalculation of marginals and conditionals required for coding and predictionapplications.
arxiv-5100-168 | Kaldi+PDNN: Building DNN-based ASR Systems with Kaldi and PDNN | http://arxiv.org/pdf/1401.6984v1.pdf | author:Yajie Miao category:cs.LG cs.CL published:2014-01-27 summary:The Kaldi toolkit is becoming popular for constructing automated speechrecognition (ASR) systems. Meanwhile, in recent years, deep neural networks(DNNs) have shown state-of-the-art performance on various ASR tasks. Thisdocument describes our open-source recipes to implement fully-fledged DNNacoustic modeling using Kaldi and PDNN. PDNN is a lightweight deep learningtoolkit developed under the Theano environment. Using these recipes, we canbuild up multiple systems including DNN hybrid systems, convolutional neuralnetwork (CNN) systems and bottleneck feature systems. These recipes aredirectly based on the Kaldi Switchboard 110-hour setup. However, adapting themto new datasets is easy to achieve.
arxiv-5100-169 | Efficient Eigen-updating for Spectral Graph Clustering | http://arxiv.org/pdf/1301.1318v4.pdf | author:Charanpal Dhanjal, Romaric Gaudel, Stéphan Clémençon category:stat.ML published:2013-01-07 summary:Partitioning a graph into groups of vertices such that those within eachgroup are more densely connected than vertices assigned to different groups,known as graph clustering, is often used to gain insight into the organisationof large scale networks and for visualisation purposes. Whereas a large numberof dedicated techniques have been recently proposed for static graphs, thedesign of on-line graph clustering methods tailored for evolving networks is achallenging problem, and much less documented in the literature. Motivated bythe broad variety of applications concerned, ranging from the study ofbiological networks to the analysis of networks of scientific referencesthrough the exploration of communications networks such as the World Wide Web,it is the main purpose of this paper to introduce a novel, computationallyefficient, approach to graph clustering in the evolutionary context. Namely,the method promoted in this article can be viewed as an incremental eigenvaluesolution for the spectral clustering method described by Ng. et al. (2001). Theincremental eigenvalue solution is a general technique for finding theapproximate eigenvectors of a symmetric matrix given a change. As well asoutlining the approach in detail, we present a theoretical bound on the qualityof the approximate eigenvectors using perturbation theory. We then derive anovel spectral clustering algorithm called Incremental Approximate SpectralClustering (IASC). The IASC algorithm is simple to implement and its efficacyis demonstrated on both synthetic and real datasets modelling the evolution ofa HIV epidemic, a citation network and the purchase history graph of ane-commerce website.
arxiv-5100-170 | SKYNET: an efficient and robust neural network training tool for machine learning in astronomy | http://arxiv.org/pdf/1309.0790v2.pdf | author:Philip Graff, Farhan Feroz, Michael P. Hobson, Anthony N. Lasenby category:astro-ph.IM cs.LG cs.NE stat.ML published:2013-09-03 summary:We present the first public release of our generic neural network trainingalgorithm, called SkyNet. This efficient and robust machine learning tool isable to train large and deep feed-forward neural networks, includingautoencoders, for use in a wide range of supervised and unsupervised learningapplications, such as regression, classification, density estimation,clustering and dimensionality reduction. SkyNet uses a `pre-training' method toobtain a set of network parameters that has empirically been shown to be closeto a good solution, followed by further optimisation using a regularisedvariant of Newton's method, where the level of regularisation is determined andadjusted automatically; the latter uses second-order derivative information toimprove convergence, but without the need to evaluate or store the full Hessianmatrix, by using a fast approximate method to calculate Hessian-vectorproducts. This combination of methods allows for the training of complicatednetworks that are difficult to optimise using standard backpropagationtechniques. SkyNet employs convergence criteria that naturally preventoverfitting, and also includes a fast algorithm for estimating the accuracy ofnetwork outputs. The utility and flexibility of SkyNet are demonstrated byapplication to a number of toy problems, and to astronomical problems focusingon the recovery of structure from blurred and noisy images, the identificationof gamma-ray bursters, and the compression and denoising of galaxy images. TheSkyNet software, which is implemented in standard ANSI C and fully parallelisedusing MPI, is available at http://www.mrao.cam.ac.uk/software/skynet/.
arxiv-5100-171 | Computing support for advanced medical data analysis and imaging | http://arxiv.org/pdf/1401.6929v1.pdf | author:W. Wiślicki, T. Bednarski, P. Białas, E. Czerwiński, Ł. Kapłon, A. Kochanowski, G. Korcyl, J. Kowal, P. Kowalski, T. Kozik, W. Krzemień, M. Molenda, P. Moskal, S. Niedźwiecki, M. Pałka, M. Pawlik, L. Raczyński, Z. Rudy, P. Salabura, N. G. Sharma, M. Silarski, A. Słomski, J. Smyrski, A. Strzelecki, A. Wieczorek, M. Zieliński, N. Zoń category:cs.CV cs.DC physics.med-ph published:2014-01-27 summary:We discuss computing issues for data analysis and image reconstruction ofPET-TOF medical scanner or other medical scanning devices producing largevolumes of data. Service architecture based on the grid and cloud concepts fordistributed processing is proposed and critically discussed.
arxiv-5100-172 | Reversible MCMC on Markov equivalence classes of sparse directed acyclic graphs | http://arxiv.org/pdf/1209.5860v3.pdf | author:Yangbo He, Jinzhu Jia, Bin Yu category:stat.ML cs.DM stat.ME published:2012-09-26 summary:Graphical models are popular statistical tools which are used to representdependent or causal complex systems. Statistically equivalent causal ordirected graphical models are said to belong to a Markov equivalent class. Itis of great interest to describe and understand the space of such classes.However, with currently known algorithms, sampling over such classes is onlyfeasible for graphs with fewer than approximately 20 vertices. In this paper,we design reversible irreducible Markov chains on the space of Markovequivalent classes by proposing a perfect set of operators that determine thetransitions of the Markov chain. The stationary distribution of a proposedMarkov chain has a closed form and can be computed easily. Specifically, weconstruct a concrete perfect set of operators on sparse Markov equivalenceclasses by introducing appropriate conditions on each possible operator.Algorithms and their accelerated versions are provided to efficiently generateMarkov chains and to explore properties of Markov equivalence classes of sparsedirected acyclic graphs (DAGs) with thousands of vertices. We findexperimentally that in most Markov equivalence classes of sparse DAGs, (1) mostedges are directed, (2) most undirected subgraphs are small and (3) the numberof these undirected subgraphs grows approximately linearly with the number ofvertices. The article contains supplement arXiv:1303.0632,http://dx.doi.org/10.1214/13-AOS1125SUPP
arxiv-5100-173 | Safe Sample Screening for Support Vector Machines | http://arxiv.org/pdf/1401.6740v1.pdf | author:Kohei Ogawa, Yoshiki Suzuki, Shinya Suzumura, Ichiro Takeuchi category:stat.ML published:2014-01-27 summary:Sparse classifiers such as the support vector machines (SVM) are efficient intest-phases because the classifier is characterized only by a subset of thesamples called support vectors (SVs), and the rest of the samples (non SVs)have no influence on the classification result. However, the advantage of thesparsity has not been fully exploited in training phases because it isgenerally difficult to know which sample turns out to be SV beforehand. In thispaper, we introduce a new approach called safe sample screening that enables usto identify a subset of the non-SVs and screen them out prior to the trainingphase. Our approach is different from existing heuristic approaches in thesense that the screened samples are guaranteed to be non-SVs at the optimalsolution. We investigate the advantage of the safe sample screening approachthrough intensive numerical experiments, and demonstrate that it cansubstantially decrease the computational cost of the state-of-the-art SVMsolvers such as LIBSVM. In the current big data era, we believe that safesample screening would be of great practical importance since the data size canbe reduced without sacrificing the optimality of the final solution.
arxiv-5100-174 | Rank-frequency relation for Chinese characters | http://arxiv.org/pdf/1309.1536v2.pdf | author:W. B. Deng, A. E. Allahverdyan, B. Li, Q. A. Wang category:cs.CL published:2013-09-06 summary:We show that the Zipf's law for Chinese characters perfectly holds forsufficiently short texts (few thousand different characters). The scenario ofits validity is similar to the Zipf's law for words in short English texts. Forlong Chinese texts (or for mixtures of short Chinese texts), rank-frequencyrelations for Chinese characters display a two-layer, hierarchic structure thatcombines a Zipfian power-law regime for frequent characters (first layer) withan exponential-like regime for less frequent characters (second layer). Forthese two layers we provide different (though related) theoretical descriptionsthat include the range of low-frequency characters (hapax legomena). Thecomparative analysis of rank-frequency relations for Chinese characters versusEnglish words illustrates the extent to which the characters play for Chinesewriters the same role as the words for those writing within alphabeticalsystems.
arxiv-5100-175 | Painting Analysis Using Wavelets and Probabilistic Topic Models | http://arxiv.org/pdf/1401.6638v1.pdf | author:Tong Wu, Gungor Polatkan, David Steel, William Brown, Ingrid Daubechies, Robert Calderbank category:cs.CV cs.LG stat.ML published:2014-01-26 summary:In this paper, computer-based techniques for stylistic analysis of paintingsare applied to the five panels of the 14th century Peruzzi Altarpiece by Giottodi Bondone. Features are extracted by combining a dual-tree complex wavelettransform with a hidden Markov tree (HMT) model. Hierarchical clustering isused to identify stylistic keywords in image patches, and keyword frequenciesare calculated for sub-images that each contains many patches. A generativehierarchical Bayesian model learns stylistic patterns of keywords; thesepatterns are then used to characterize the styles of the sub-images; this inturn, permits to discriminate between paintings. Results suggest that suchunsupervised probabilistic topic models can be useful to distill characteristicelements of style.
arxiv-5100-176 | Ensembled Correlation Between Liver Analysis Outputs | http://arxiv.org/pdf/1401.6597v1.pdf | author:Sadi Evren Seker, Y. Unal, Z. Erdem, H. Erdinc Kocer category:stat.ML cs.CE cs.LG published:2014-01-25 summary:Data mining techniques on the biological analysis are spreading for most ofthe areas including the health care and medical information. We have appliedthe data mining techniques, such as KNN, SVM, MLP or decision trees over aunique dataset, which is collected from 16,380 analysis results for a year.Furthermore we have also used meta-classifiers to question the increasedcorrelation rate between the liver disorder and the liver analysis outputs. Theresults show that there is a correlation among ALT, AST, Billirubin Direct andBillirubin Total down to 15% of error rate. Also the correlation coefficient isup to 94%. This makes possible to predict the analysis results from each otheror disease patterns can be applied over the linear correlation of theparameters.
arxiv-5100-177 | Category theory, logic and formal linguistics: some connections, old and new | http://arxiv.org/pdf/1401.6574v1.pdf | author:Jean Gillibert, Christian Retoré category:math.CT cs.CL cs.LO math.LO published:2014-01-25 summary:We seize the opportunity of the publication of selected papers from the\emph{Logic, categories, semantics} workshop in the \emph{Journal of AppliedLogic} to survey some current trends in logic, namely intuitionistic and lineartype theories, that interweave categorical, geometrical and computationalconsiderations. We thereafter present how these rich logical frameworks canmodel the way language conveys meaning.
arxiv-5100-178 | Deverbal semantics and the Montagovian generative lexicon | http://arxiv.org/pdf/1401.6573v1.pdf | author:Livy-Maria Real-Coelho, Christian Retoré category:cs.CL cs.LO published:2014-01-25 summary:We propose a lexical account of action nominals, in particular of deverbalnominalisations, whose meaning is related to the event expressed by their baseverb. The literature about nominalisations often assumes that the semantics ofthe base verb completely defines the structure of action nominals. We arguethat the information in the base verb is not sufficient to completely determinethe semantics of action nominals. We exhibit some data from differentlanguages, especially from Romance language, which show that nominalisationsfocus on some aspects of the verb semantics. The selected aspects, however,seem to be idiosyncratic and do not automatically result from the internalstructure of the verb nor from its interaction with the morphological suffix.We therefore propose a partially lexicalist approach view of deverbal nouns. Itis made precise and computable by using the Montagovian Generative Lexicon, atype theoretical framework introduced by Bassac, Mery and Retor\'e in thisjournal in 2010. This extension of Montague semantics with a richer type systemeasily incorporates lexical phenomena like the semantics of action nominals inparticular deverbals, including their polysemy and (in)felicitouscopredications.
arxiv-5100-179 | Keyword and Keyphrase Extraction Using Centrality Measures on Collocation Networks | http://arxiv.org/pdf/1401.6571v1.pdf | author:Shibamouli Lahiri, Sagnik Ray Choudhury, Cornelia Caragea category:cs.CL cs.IR published:2014-01-25 summary:Keyword and keyphrase extraction is an important problem in natural languageprocessing, with applications ranging from summarization to semantic search todocument clustering. Graph-based approaches to keyword and keyphrase extractionavoid the problem of acquiring a large in-domain training corpus by applyingvariants of PageRank algorithm on a network of words. Although graph-basedapproaches are knowledge-lean and easily adoptable in online systems, itremains largely open whether they can benefit from centrality measures otherthan PageRank. In this paper, we experiment with an array of centralitymeasures on word and noun phrase collocation networks, and analyze theirperformance on four benchmark datasets. Not only are there centrality measuresthat perform as well as or better than PageRank, but they are much simpler(e.g., degree, strength, and neighborhood size). Furthermore, centrality-basedmethods give results that are competitive with and, in some cases, better thantwo strong unsupervised baselines.
arxiv-5100-180 | A Machine Learning Approach for the Identification of Bengali Noun-Noun Compound Multiword Expressions | http://arxiv.org/pdf/1401.6567v1.pdf | author:Vivekananda Gayen, Kamal Sarkar category:cs.CL cs.LG published:2014-01-25 summary:This paper presents a machine learning approach for identification of Bengalimultiword expressions (MWE) which are bigram nominal compounds. Our proposedapproach has two steps: (1) candidate extraction using chunk information andvarious heuristic rules and (2) training the machine learning algorithm calledRandom Forest to classify the candidates into two groups: bigram nominalcompound MWE or not bigram nominal compound MWE. A variety of associationmeasures, syntactic and linguistic clues and a set of WordNet-based similarityfeatures have been used for our MWE identification task. The approach presentedin this paper can be used to identify bigram nominal compound MWE in Bengalirunning text.
arxiv-5100-181 | A Hybrid Approach to Extract Keyphrases from Medical Documents | http://arxiv.org/pdf/1303.1441v2.pdf | author:Kamal Sarkar category:cs.IR cs.CL published:2013-03-06 summary:Keyphrases are the phrases, consisting of one or more words, representing theimportant concepts in the articles. Keyphrases are useful for a variety oftasks such as text summarization, automatic indexing,clustering/classification, text mining etc. This paper presents a hybridapproach to keyphrase extraction from medical documents. The keyphraseextraction approach presented in this paper is an amalgamation of two methods:the first one assigns weights to candidate keyphrases based on an effectivecombination of features such as position, term frequency, inverse documentfrequency and the second one assign weights to candidate keyphrases using someknowledge about their similarities to the structure and characteristics ofkeyphrases available in the memory (stored list of keyphrases). An efficientcandidate keyphrase identification method as the first component of theproposed keyphrase extraction system has also been introduced in this paper.The experimental results show that the proposed hybrid approach performs betterthan some state-of-the art keyphrase extraction approaches.
arxiv-5100-182 | The Stochastic Gradient Descent for the Primal L1-SVM Optimization Revisited | http://arxiv.org/pdf/1304.6383v2.pdf | author:Constantinos Panagiotakopoulos, Petroula Tsampouka category:cs.LG cs.AI published:2013-04-23 summary:We reconsider the stochastic (sub)gradient approach to the unconstrainedprimal L1-SVM optimization. We observe that if the learning rate is inverselyproportional to the number of steps, i.e., the number of times any trainingpattern is presented to the algorithm, the update rule may be transformed intothe one of the classical perceptron with margin in which the margin thresholdincreases linearly with the number of steps. Moreover, if we cycle repeatedlythrough the possibly randomly permuted training set the dual variables definednaturally via the expansion of the weight vector as a linear combination of thepatterns on which margin errors were made are shown to obey at the end of eachcomplete cycle automatically the box constraints arising in dual optimization.This renders the dual Lagrangian a running lower bound on the primal objectivetending to it at the optimum and makes available an upper bound on the relativeaccuracy achieved which provides a meaningful stopping criterion. In addition,we propose a mechanism of presenting the same pattern repeatedly to thealgorithm which maintains the above properties. Finally, we give experimentalevidence that algorithms constructed along these lines exhibit a considerablyimproved performance.
arxiv-5100-183 | Identification of Protein Coding Regions in Genomic DNA Using Unsupervised FMACA Based Pattern Classifier | http://arxiv.org/pdf/1401.6484v1.pdf | author:Pokkuluri Kiran Sree, Inampudi Ramesh Babu category:cs.CE cs.LG published:2014-01-25 summary:Genes carry the instructions for making proteins that are found in a cell asa specific sequence of nucleotides that are found in DNA molecules. But, theregions of these genes that code for proteins may occupy only a small region ofthe sequence. Identifying the coding regions play a vital role in understandingthese genes. In this paper we propose a unsupervised Fuzzy Multiple AttractorCellular Automata (FMCA) based pattern classifier to identify the coding regionof a DNA sequence. We propose a distinct K-Means algorithm for designing FMACAclassifier which is simple, efficient and produces more accurate classifierthan that has previously been obtained for a range of different sequencelengths. Experimental results confirm the scalability of the proposedUnsupervised FCA based classifier to handle large volume of datasetsirrespective of the number of classes, tuples and attributes. Goodclassification accuracy has been established.
arxiv-5100-184 | Co-Multistage of Multiple Classifiers for Imbalanced Multiclass Learning | http://arxiv.org/pdf/1312.6597v2.pdf | author:Luis Marujo, Anatole Gershman, Jaime Carbonell, David Martins de Matos, João P. Neto category:cs.LG cs.IR published:2013-12-23 summary:In this work, we propose two stochastic architectural models (CMC and CMC-M)with two layers of classifiers applicable to datasets with one and multipleskewed classes. This distinction becomes important when the datasets have alarge number of classes. Therefore, we present a novel solution to imbalancedmulticlass learning with several skewed majority classes, which improvesminority classes identification. This fact is particularly important for textclassification tasks, such as event detection. Our models combined withpre-processing sampling techniques improved the classification results on sixwell-known datasets. Finally, we have also introduced a new metric SG-Mean toovercome the multiplication by zero limitation of G-Mean.
arxiv-5100-185 | Multimodal Transitions for Generative Stochastic Networks | http://arxiv.org/pdf/1312.5578v4.pdf | author:Sherjil Ozair, Li Yao, Yoshua Bengio category:cs.LG stat.ML published:2013-12-19 summary:Generative Stochastic Networks (GSNs) have been recently introduced as analternative to traditional probabilistic modeling: instead of parametrizing thedata distribution directly, one parametrizes a transition operator for a Markovchain whose stationary distribution is an estimator of the data generatingdistribution. The result of training is therefore a machine that generatessamples through this Markov chain. However, the previously introduced GSNconsistency theorems suggest that in order to capture a wide class ofdistributions, the transition operator in general should be multimodal,something that has not been done before this paper. We introduce for the firsttime multimodal transition distributions for GSNs, in particular using modelsin the NADE family (Neural Autoregressive Density Estimator) as outputdistributions of the transition operator. A NADE model is related to an RBM(and can thus model multimodal distributions) but its likelihood (andlikelihood gradient) can be computed easily. The parameters of the NADE areobtained as a learned function of the previous state of the learned Markovchain. Experiments clearly illustrate the advantage of such multimodaltransition distributions over unimodal GSNs.
arxiv-5100-186 | Automatic Detection of Calibration Grids in Time-of-Flight Images | http://arxiv.org/pdf/1401.6393v1.pdf | author:Miles Hansard, Radu Horaud, Michel Amat, Georgios Evangelidis category:cs.CV published:2014-01-24 summary:It is convenient to calibrate time-of-flight cameras by established methods,using images of a chequerboard pattern. The low resolution of the amplitudeimage, however, makes it difficult to detect the board reliably. Heuristicdetection methods, based on connected image-components, perform very poorly onthis data. An alternative, geometrically-principled method is introduced here,based on the Hough transform. The projection of a chequerboard is representedby two pencils of lines, which are identified as oriented clusters in thegradient-data of the image. A projective Hough transform is applied to each ofthe two clusters, in axis-aligned coordinates. The range of each transform isproperly bounded, because the corresponding gradient vectors are approximatelyparallel. Each of the two transforms contains a series of collinear peaks; onefor every line in the given pencil. This pattern is easily detected, bysweeping a dual line through the transform. The proposed Hough-based method iscompared to the standard OpenCV detection routine, by application to severalhundred time-of-flight images. It is shown that the new method detectssignificantly more calibration boards, over a greater variety of poses, withoutany overall loss of accuracy. This conclusion is based on an analysis of bothgeometric and photometric error.
arxiv-5100-187 | Steady-state performance of non-negative least-mean-square algorithm and its variants | http://arxiv.org/pdf/1401.6376v1.pdf | author:Jie Chen, José Carlos M. Bermudez, Cédric Richard category:cs.LG published:2014-01-24 summary:Non-negative least-mean-square (NNLMS) algorithm and its variants have beenproposed for online estimation under non-negativity constraints. The transientbehavior of the NNLMS, Normalized NNLMS, Exponential NNLMS and Sign-Sign NNLMSalgorithms have been studied in our previous work. In this technical report, wederive closed-form expressions for the steady-state excess mean-square error(EMSE) for the four algorithms. Simulations results illustrate the accuracy ofthe theoretical results. This is a complementary material to our previous work.
arxiv-5100-188 | Community Detection in Networks using Graph Distance | http://arxiv.org/pdf/1401.3915v2.pdf | author:Sharmodeep Bhattacharyya, Peter J. Bickel category:stat.ML cs.SI published:2014-01-16 summary:The study of networks has received increased attention recently not only fromthe social sciences and statistics but also from physicists, computerscientists and mathematicians. One of the principal problem in networks iscommunity detection. Many algorithms have been proposed for community findingbut most of them do not have have theoretical guarantee for sparse networks andnetworks close to the phase transition boundary proposed by physicists. Thereare some exceptions but all have some incomplete theoretical basis. Here wepropose an algorithm based on the graph distance of vertices in the network. Wegive theoretical guarantees that our method works in identifying communitiesfor block models and can be extended for degree-corrected block models andblock models with the number of communities growing with number of vertices.Despite favorable simulation results, we are not yet able to conclude that ourmethod is satisfactory for worst possible case. We illustrate on a network ofpolitical blogs, Facebook networks and some other networks.
arxiv-5100-189 | The EM algorithm and the Laplace Approximation | http://arxiv.org/pdf/1401.6276v1.pdf | author:Niko Brümmer category:stat.ML published:2014-01-24 summary:The Laplace approximation calls for the computation of second derivatives atthe likelihood maximum. When the maximum is found by the EM-algorithm, there isa convenient way to compute these derivatives. The likelihood gradient can beobtained from the EM-auxiliary, while the Hessian can be obtained from thisgradient with the Pearlmutter trick.
arxiv-5100-190 | Parallel Genetic Algorithm to Solve Traveling Salesman Problem on MapReduce Framework using Hadoop Cluster | http://arxiv.org/pdf/1401.6267v1.pdf | author:Harun Rasit Er, Nadia Erdogan category:cs.DC cs.NE published:2014-01-24 summary:Traveling Salesman Problem (TSP) is one of the most common studied problemsin combinatorial optimization. Given the list of cities and distances betweenthem, the problem is to find the shortest tour possible which visits all thecities in list exactly once and ends in the city where it starts. Despite theTraveling Salesman Problem is NP-Hard, a lot of methods and solutions areproposed to the problem. One of them is Genetic Algorithm (GA). GA is a simplebut an efficient heuristic method that can be used to solve Traveling SalesmanProblem. In this paper, we will show a parallel genetic algorithmimplementation on MapReduce framework in order to solve Traveling SalesmanProblem. MapReduce is a framework used to support distributed computation onclusters of computers. We used free licensed Hadoop implementation as MapReduceframework.
arxiv-5100-191 | Is Extreme Learning Machine Feasible? A Theoretical Assessment (Part II) | http://arxiv.org/pdf/1401.6240v1.pdf | author:Shaobo Lin, Xia Liu, Jian Fang, Zongben Xu category:cs.LG 68T05 F.2.2 published:2014-01-24 summary:An extreme learning machine (ELM) can be regarded as a two stage feed-forwardneural network (FNN) learning system which randomly assigns the connectionswith and within hidden neurons in the first stage and tunes the connectionswith output neurons in the second stage. Therefore, ELM training is essentiallya linear learning problem, which significantly reduces the computationalburden. Numerous applications show that such a computation burden reductiondoes not degrade the generalization capability. It has, however, been open thatwhether this is true in theory. The aim of our work is to study the theoreticalfeasibility of ELM by analyzing the pros and cons of ELM. In the previous parton this topic, we pointed out that via appropriate selection of the activationfunction, ELM does not degrade the generalization capability in the expectationsense. In this paper, we launch the study in a different direction and showthat the randomness of ELM also leads to certain negative consequences. On onehand, we find that the randomness causes an additional uncertainty problem ofELM, both in approximation and learning. On the other hand, we theoreticallyjustify that there also exists an activation function such that thecorresponding ELM degrades the generalization capability. In particular, weprove that the generalization capability of ELM with Gaussian kernel isessentially worse than that of FNN with Gaussian kernel. To facilitate the useof ELM, we also provide a remedy to such a degradation. We find that thewell-developed coefficient regularization technique can essentially improve thegeneralization capability. The obtained results reveal the essentialcharacteristic of ELM and give theoretical guidance concerning how to use ELM.
arxiv-5100-192 | Word-length entropies and correlations of natural language written texts | http://arxiv.org/pdf/1401.6224v1.pdf | author:Maria Kalimeri, Vassilios Constantoudis, Constantinos Papadimitriou, Konstantinos Karamanos, Fotis K. Diakonos, Harris Papageorgiou category:cs.CL published:2014-01-24 summary:We study the frequency distributions and correlations of the word lengths often European languages. Our findings indicate that a) the word-lengthdistribution of short words quantified by the mean value and the entropydistinguishes the Uralic (Finnish) corpus from the others, b) the tails at longwords, manifested in the high-order moments of the distributions, differentiatethe Germanic languages (except for English) from the Romanic languages andGreek and c) the correlations between nearby word lengths measured by thecomparison of the real entropies with those of the shuffled texts are found tobe smaller in the case of Germanic and Finnish languages.
arxiv-5100-193 | Risk-sensitive Markov control processes | http://arxiv.org/pdf/1110.6317v5.pdf | author:Yun Shen, Wilhelm Stannat, Klaus Obermayer category:math.OC cs.CE math.DS stat.ML published:2011-10-28 summary:We introduce a general framework for measuring risk in the context of Markovcontrol processes with risk maps on general Borel spaces that generalize knownconcepts of risk measures in mathematical finance, operations research andbehavioral economics. Within the framework, applying weighted norm spaces toincorporate also unbounded costs, we study two types of infinite-horizonrisk-sensitive criteria, discounted total risk and average risk, and solve theassociated optimization problems by dynamic programming. For the discountedcase, we propose a new discount scheme, which is different from theconventional form but consistent with the existing literature, while for theaverage risk criterion, we state Lyapunov-like stability conditions thatgeneralize known conditions for Markov chains to ensure the existence ofsolutions to the optimality equation.
arxiv-5100-194 | Spatially regularized reconstruction of fibre orientation distributions in the presence of isotropic diffusion | http://arxiv.org/pdf/1401.6196v1.pdf | author:Q. Zhou, O. Michailovich, Y. Rathi category:cs.CV published:2014-01-23 summary:The connectivity and structural integrity of the white matter of the brain isnowadays known to be implicated into a wide range of brain-related disorders.However, it was not before the advent of diffusion Magnetic Resonance Imaging(dMRI) that researches have been able to examine the properties of white matterin vivo. Presently, among a range of various methods of dMRI, high angularresolution diffusion imaging (HARDI) is known to excel in its ability toprovide reliable information about the local orientations of neural fasciculi(aka fibre tracts). Moreover, as opposed to the more traditional diffusiontensor imaging (DTI), HARDI is capable of distinguishing the orientations ofmultiple fibres passing through a given spatial voxel. Unfortunately, theability of HARDI to discriminate between neural fibres that cross each other atacute angles is always limited, which is the main reason behind the developmentof numerous post-processing tools, aiming at the improvement of the directionalresolution of HARDI. Among such tools is spherical deconvolution (SD). Due toits ill-posed nature, however, SD standardly relies on a number of a prioriassumptions which are to render its results unique and stable. In this paper,we propose a different approach to the problem of SD in HARDI, which accountsfor the spatial continuity of neural fibres as well as the presence ofisotropic diffusion. Subsequently, we demonstrate how the proposed solution canbe used to successfully overcome the effect of partial voluming, whilepreserving the spatial coherency of cerebral diffusion at moderate-to-severenoise levels. In a series of both in silico and in vivo experiments, theperformance of the proposed method is compared with that of several availablealternatives, with the comparative results clearly supporting the viability andusefulness of our approach.
arxiv-5100-195 | Risk-sensitive Reinforcement Learning | http://arxiv.org/pdf/1311.2097v3.pdf | author:Yun Shen, Michael J. Tobia, Tobias Sommer, Klaus Obermayer category:cs.LG published:2013-11-08 summary:We derive a family of risk-sensitive reinforcement learning methods foragents, who face sequential decision-making tasks in uncertain environments. Byapplying a utility function to the temporal difference (TD) error, nonlineartransformations are effectively applied not only to the received rewards butalso to the true transition probabilities of the underlying Markov decisionprocess. When appropriate utility functions are chosen, the agents' behaviorsexpress key features of human behavior as predicted by prospect theory(Kahneman and Tversky, 1979), for example different risk-preferences for gainsand losses as well as the shape of subjective probability curves. We derive arisk-sensitive Q-learning algorithm, which is necessary for modeling humanbehavior when transition probabilities are unknown, and prove its convergence.As a proof of principle for the applicability of the new framework we apply itto quantify human behavior in a sequential investment task. We find, that therisk-sensitive variant provides a significantly better fit to the behavioraldata and that it leads to an interpretation of the subject's responses which isindeed consistent with prospect theory. The analysis of simultaneously measuredfMRI signals show a significant correlation of the risk-sensitive TD error withBOLD signal change in the ventral striatum. In addition we find a significantcorrelation of the risk-sensitive Q-values with neural activity in thestriatum, cingulate cortex and insula, which is not present if standardQ-values are used.
arxiv-5100-196 | Iterative Universal Hash Function Generator for Minhashing | http://arxiv.org/pdf/1401.6124v1.pdf | author:Fabricio Olivetti de Franca category:cs.LG cs.IR published:2014-01-23 summary:Minhashing is a technique used to estimate the Jaccard Index between two setsby exploiting the probability of collision in a random permutation. In order tospeed up the computation, a random permutation can be approximated by using anuniversal hash function such as the $h_{a,b}$ function proposed by Carter andWegman. A better estimate of the Jaccard Index can be achieved by using many ofthese hash functions, created at random. In this paper a new iterativeprocedure to generate a set of $h_{a,b}$ functions is devised that eliminatesthe need for a list of random values and avoid the multiplication operationduring the calculation. The properties of the generated hash functions remainsthat of an universal hash function family. This is possible due to the randomnature of features occurrence on sparse datasets. Results show that theuniformity of hashing the features is maintaned while obtaining a speed up ofup to $1.38$ compared to the traditional approach.
arxiv-5100-197 | Identifying Bengali Multiword Expressions using Semantic Clustering | http://arxiv.org/pdf/1401.6122v1.pdf | author:Tanmoy Chakraborty, Dipankar Das, Sivaji Bandyopadhyay category:cs.CL published:2014-01-23 summary:One of the key issues in both natural language understanding and generationis the appropriate processing of Multiword Expressions (MWEs). MWEs pose a hugeproblem to the precise language processing due to their idiosyncratic natureand diversity in lexical, syntactical and semantic properties. The semantics ofa MWE cannot be expressed after combining the semantics of its constituents.Therefore, the formalism of semantic clustering is often viewed as aninstrument for extracting MWEs especially for resource constraint languageslike Bengali. The present semantic clustering approach contributes to locateclusters of the synonymous noun tokens present in the document. These clustersin turn help measure the similarity between the constituent words of apotentially candidate phrase using a vector space model and judge thesuitability of this phrase to be a MWE. In this experiment, we apply thesemantic clustering approach for noun-noun bigram MWEs, though it can beextended to any types of MWEs. In parallel, the well known statistical models,namely Point-wise Mutual Information (PMI), Log Likelihood Ratio (LLR),Significance function are also employed to extract MWEs from the Bengalicorpus. The comparative evaluation shows that the semantic clustering approachoutperforms all other competing statistical models. As a by-product of thisexperiment, we have started developing a standard lexicon in Bengali thatserves as a productive Bengali linguistic thesaurus.
arxiv-5100-198 | Integrative Semantic Dependency Parsing via Efficient Large-scale Feature Selection | http://arxiv.org/pdf/1401.6050v1.pdf | author:Hai Zhao, Xiaotian Zhang, Chunyu Kit category:cs.CL published:2014-01-23 summary:Semantic parsing, i.e., the automatic derivation of meaning representationsuch as an instantiated predicate-argument structure for a sentence, plays acritical role in deep processing of natural language. Unlike all other topsystems of semantic dependency parsing that have to rely on a pipelineframework to chain up a series of submodels each specialized for a specificsubtask, the one presented in this article integrates everything into onemodel, in hopes of achieving desirable integrity and practicality for realapplications while maintaining a competitive performance. This integrativeapproach tackles semantic parsing as a word pair classification problem using amaximum entropy classifier. We leverage adaptive pruning of argument candidatesand large-scale feature selection engineering to allow the largest featurespace ever in use so far in this field, it achieves a state-of-the-artperformance on the evaluation data set for CoNLL-2008 shared task, on top ofall but one top pipeline system, confirming its feasibility and effectiveness.
arxiv-5100-199 | Matrix factorization with Binary Components | http://arxiv.org/pdf/1401.6024v1.pdf | author:Martin Slawski, Matthias Hein, Pavlo Lutsik category:stat.ML cs.LG published:2014-01-23 summary:Motivated by an application in computational biology, we consider low-rankmatrix factorization with $\{0,1\}$-constraints on one of the factors andoptionally convex constraints on the second one. In addition to thenon-convexity shared with other matrix factorization schemes, our problem isfurther complicated by a combinatorial constraint set of size $2^{m \cdot r}$,where $m$ is the dimension of the data points and $r$ the rank of thefactorization. Despite apparent intractability, we provide - in the line ofrecent work on non-negative matrix factorization by Arora et al. (2012) - analgorithm that provably recovers the underlying factorization in the exact casewith $O(m r 2^r + mnr + r^2 n)$ operations for $n$ datapoints. To obtain thisresult, we use theory around the Littlewood-Offord lemma from combinatorics.
arxiv-5100-200 | Reasoning about Meaning in Natural Language with Compact Closed Categories and Frobenius Algebras | http://arxiv.org/pdf/1401.5980v1.pdf | author:Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, Stephen Pulman, Bob Coecke category:cs.CL cs.AI math.CT published:2014-01-23 summary:Compact closed categories have found applications in modeling quantuminformation protocols by Abramsky-Coecke. They also provide semantics forLambek's pregroup algebras, applied to formalizing the grammatical structure ofnatural language, and are implicit in a distributional model of word meaningbased on vector spaces. Specifically, in previous work Coecke-Clark-Sadrzadehused the product category of pregroups with vector spaces and provided adistributional model of meaning for sentences. We recast this theory in termsof strongly monoidal functors and advance it via Frobenius algebras over vectorspaces. The former are used to formalize topological quantum field theories byAtiyah and Baez-Dolan, and the latter are used to model classical data inquantum protocols by Coecke-Pavlovic-Vicary. The Frobenius algebras enable usto work in a single space in which meanings of words, phrases, and sentences ofany structure live. Hence we can compare meanings of different languageconstructs and enhance the applicability of the theory. We report onexperimental results on a number of language tasks and verify the theoreticalpredictions.
arxiv-5100-201 | On Image Block Loss Restoration Using the Sparsity Pattern as Side Information | http://arxiv.org/pdf/1401.5966v1.pdf | author:Hossein Hosseini, Ali Goli, Neda Barzegar Marvasti, Masoume Azghani, Farokh Marvasti category:cs.MM cs.CV published:2014-01-23 summary:In this paper, we propose an image block loss restoration method based on thenotion of sparse representation. The sparsity pattern is exploited as sideinformation to efficiently restore block losses, by iteratively imposing theconstraints of spatial and transform domains on the corrupted image. Two novelfeatures, including a pre-interpolation and a criterion for stopping theiterations, are added for performance improvement. Besides, a scheme ispresented for no-reference quality estimation of the restored image withrespect to the original and sparse images. To the best of our knowledge, thisis the first attempt to estimate the image quality in the restoration methods.Also, to deal with practical applications, we develop a technique to transmitthe side information along with the image. In this technique, we first compressthe side information and then embed its LDPC coded version in the leastsignificant bits of the image pixels. This technique ensures the error-freetransmission of the side information, while causing only a small perturbationon the transmitted image. Mathematical analysis and extensive simulations areperformed to evaluate the method and investigate the efficiency of the proposedtechniques. The results verify that the suggested method outperforms itscounterparts for image block loss restoration.
arxiv-5100-202 | Gaussian-binary Restricted Boltzmann Machines on Modeling Natural Image Statistics | http://arxiv.org/pdf/1401.5900v1.pdf | author:Nan Wang, Jan Melchior, Laurenz Wiskott category:cs.NE cs.LG stat.ML published:2014-01-23 summary:We present a theoretical analysis of Gaussian-binary restricted Boltzmannmachines (GRBMs) from the perspective of density models. The key aspect of thisanalysis is to show that GRBMs can be formulated as a constrained mixture ofGaussians, which gives a much better insight into the model's capabilities andlimitations. We show that GRBMs are capable of learning meaningful featuresboth in a two-dimensional blind source separation task and in modeling naturalimages. Further, we show that reported difficulties in training GRBMs are dueto the failure of the training algorithm rather than the model itself. Based onour analysis we are able to propose several training recipes, which allowedsuccessful and fast training in our experiments. Finally, we discuss therelationship of GRBMs to several modifications that have been proposed toimprove the model.
arxiv-5100-203 | Learning Mid-Level Features and Modeling Neuron Selectivity for Image Classification | http://arxiv.org/pdf/1401.5535v2.pdf | author:Shu Kong, Zhuolin Jiang, Qiang Yang category:cs.CV cs.LG cs.NE cs.RO published:2014-01-22 summary:We now know that mid-level features can greatly enhance the performance ofimage learning, but how to automatically learn the image features efficientlyand in an unsupervised manner is still an open question. In this paper, wepresent a very efficient mid-level feature learning approach (MidFea), whichonly involves simple operations such as $k$-means clustering, convolution,pooling, vector quantization and random projection. We explain why this simplemethod generates the desired features, and argue that there is no need to spendmuch time in learning low-level feature extractors. Furthermore, to boost theperformance, we propose to model the neuron selectivity (NS) principle bybuilding an additional layer over the mid-level features before feeding thefeatures into the classifier. We show that the NS-layer learnscategory-specific neurons with both bottom-up inference and top-down analysis,and thus supports fast inference for a query image. We run extensiveexperiments on several public databases to demonstrate that our approach canachieve state-of-the-art performances for face recognition, genderclassification, age estimation and object categorization. In particular, wedemonstrate that our approach is more than an order of magnitude faster thansome recently proposed sparse coding based methods.
arxiv-5100-204 | Hierarchical pixel clustering for image segmentation | http://arxiv.org/pdf/1401.5891v1.pdf | author:M. Kharinov category:cs.CV published:2014-01-23 summary:In the paper a piecewise constant image approximations of sequential numberof pixel clusters or segments are treated. A majorizing of optimalapproximation sequence by hierarchical sequence of image approximations isstudied. Transition from pixel clustering to image segmentation by reducing ofsegment numbers in clusters is provided. Algorithms are proved by elementaryformulas.
arxiv-5100-205 | Asymptotic Accuracy of Bayes Estimation for Latent Variables with Redundancy | http://arxiv.org/pdf/1205.3234v5.pdf | author:Keisuke Yamazaki category:stat.ML published:2012-05-15 summary:Hierarchical parametric models consisting of observable and latent variablesare widely used for unsupervised learning tasks. For example, a mixture modelis a representative hierarchical model for clustering. From the statisticalpoint of view, the models can be regular or singular due to the distribution ofdata. In the regular case, the models have the identifiability; there isone-to-one relation between a probability density function for the modelexpression and the parameter. The Fisher information matrix is positivedefinite, and the estimation accuracy of both observable and latent variableshas been studied. In the singular case, on the other hand, the models are notidentifiable and the Fisher matrix is not positive definite. Conventionalstatistical analysis based on the inverse Fisher matrix is not applicable.Recently, an algebraic geometrical analysis has been developed and is used toelucidate the Bayes estimation of observable variables. The present paperapplies this analysis to latent-variable estimation and determines itstheoretical performance. Our results clarify behavior of the convergence of theposterior distribution. It is found that the posterior of theobservable-variable estimation can be different from the one in thelatent-variable estimation. Because of the difference, the Markov chain MonteCarlo method based on the parameter and the latent variable cannot constructthe desired posterior distribution.
arxiv-5100-206 | Towards Unsupervised Learning of Temporal Relations between Events | http://arxiv.org/pdf/1401.6427v1.pdf | author:Seyed Abolghasem Mirroshandel, Gholamreza Ghassem-Sani category:cs.LG cs.CL published:2014-01-23 summary:Automatic extraction of temporal relations between event pairs is animportant task for several natural language processing applications such asQuestion Answering, Information Extraction, and Summarization. Since mostexisting methods are supervised and require large corpora, which for manylanguages do not exist, we have concentrated our efforts to reduce the need forannotated data as much as possible. This paper presents two differentalgorithms towards this goal. The first algorithm is a weakly supervisedmachine learning approach for classification of temporal relations betweenevents. In the first stage, the algorithm learns a general classifier from anannotated corpus. Then, inspired by the hypothesis of "one type of temporalrelation per discourse, it extracts useful information from a cluster oftopically related documents. We show that by combining the global informationof such a cluster with local decisions of a general classifier, a bootstrappingcross-document classifier can be built to extract temporal relations betweenevents. Our experiments show that without any additional annotated data, theaccuracy of the proposed algorithm is higher than that of several previoussuccessful systems. The second proposed method for temporal relation extractionis based on the expectation maximization (EM) algorithm. Within EM, we useddifferent techniques such as a greedy best-first search and integer linearprogramming for temporal inconsistency removal. We think that the experimentalresults of our EM based algorithm, as a first step toward a fully unsupervisedtemporal relation extraction method, is encouraging.
arxiv-5100-207 | A Tutorial on Dual Decomposition and Lagrangian Relaxation for Inference in Natural Language Processing | http://arxiv.org/pdf/1405.5208v1.pdf | author:Alexander M. Rush, Michael Collins category:cs.CL cs.AI published:2014-01-23 summary:Dual decomposition, and more generally Lagrangian relaxation, is a classicalmethod for combinatorial optimization; it has recently been applied to severalinference problems in natural language processing (NLP). This tutorial gives anoverview of the technique. We describe example algorithms, describe formalguarantees for the method, and describe practical issues in implementing thealgorithms. While our examples are predominantly drawn from the NLP literature,the material should be of general relevance to inference problems in machinelearning. A central theme of this tutorial is that Lagrangian relaxation isnaturally applied in conjunction with a broad class of combinatorialalgorithms, allowing inference in models that go significantly beyond previouswork on Lagrangian relaxation for inference in graphical models.
arxiv-5100-208 | Automatic Aggregation by Joint Modeling of Aspects and Values | http://arxiv.org/pdf/1401.6422v1.pdf | author:Christina Sauper, Regina Barzilay category:cs.CL published:2014-01-23 summary:We present a model for aggregation of product review snippets by joint aspectidentification and sentiment analysis. Our model simultaneously identifies anunderlying set of ratable aspects presented in the reviews of a product (e.g.,sushi and miso for a Japanese restaurant) and determines the correspondingsentiment of each aspect. This approach directly enables discovery ofhighly-rated or inconsistent aspects of a product. Our generative model admitsan efficient variational mean-field inference algorithm. It is also easilyextensible, and we describe several modifications and their effects on modelstructure and inference. We test our model on two tasks, joint aspectidentification and sentiment analysis on a set of Yelp reviews and aspectidentification alone on a set of medical summaries. We evaluate the performanceof the model on aspect identification, sentiment analysis, and per-wordlabeling accuracy. We demonstrate that our model outperforms applicablebaselines by a considerable margin, yielding up to 32% relative error reductionon aspect identification and up to 20% relative error reduction on sentimentanalysis.
arxiv-5100-209 | Toward Supervised Anomaly Detection | http://arxiv.org/pdf/1401.6424v1.pdf | author:Nico Goernitz, Marius Micha Kloft, Konrad Rieck, Ulf Brefeld category:cs.LG published:2014-01-23 summary:Anomaly detection is being regarded as an unsupervised learning task asanomalies stem from adversarial or unlikely events with unknown distributions.However, the predictive performance of purely unsupervised anomaly detectionoften fails to match the required detection rates in many tasks and thereexists a need for labeled data to guide the model generation. Our firstcontribution shows that classical semi-supervised approaches, originating froma supervised classifier, are inappropriate and hardly detect new and unknownanomalies. We argue that semi-supervised anomaly detection needs to ground onthe unsupervised learning paradigm and devise a novel algorithm that meets thisrequirement. Although being intrinsically non-convex, we further show that theoptimization problem has a convex equivalent under relatively mild assumptions.Additionally, we propose an active learning strategy to automatically filtercandidates for labeling. In an empirical study on network intrusion detectiondata, we observe that the proposed learning methodology requires much lesslabeled data than the state-of-the-art, while achieving higher detectionaccuracies.
arxiv-5100-210 | Riffled Independence for Efficient Inference with Partial Rankings | http://arxiv.org/pdf/1401.6421v1.pdf | author:Jonathan Huang, Ashish Kapoor, Carlos Guestrin category:cs.LG published:2014-01-23 summary:Distributions over rankings are used to model data in a multitude of realworld settings such as preference analysis and political elections. Modelingsuch distributions presents several computational challenges, however, due tothe factorial size of the set of rankings over an item set. Some of thesechallenges are quite familiar to the artificial intelligence community, such ashow to compactly represent a distribution over a combinatorially large space,and how to efficiently perform probabilistic inference with theserepresentations. With respect to ranking, however, there is the additionalchallenge of what we refer to as human task complexity users are rarely willingto provide a full ranking over a long list of candidates, instead oftenpreferring to provide partial ranking information. Simultaneously addressingall of these challenges i.e., designing a compactly representable model whichis amenable to efficient inference and can be learned using partial rankingdata is a difficult task, but is necessary if we would like to scale toproblems with nontrivial size. In this paper, we show that the recentlyproposed riffled independence assumptions cleanly and efficiently address eachof the above challenges. In particular, we establish a tight mathematicalconnection between the concepts of riffled independence and of partialrankings. This correspondence not only allows us to then develop efficient andexact algorithms for performing inference tasks using riffled independencebased represen- tations with partial rankings, but somewhat surprisingly, alsoshows that efficient inference is not possible for riffle independent models(in a certain sense) with observations which do not take the form of partialrankings. Finally, using our inference algorithm, we introduce the first methodfor learning riffled independence based models from partially ranked data.
arxiv-5100-211 | Improving Statistical Machine Translation for a Resource-Poor Language Using Related Resource-Rich Languages | http://arxiv.org/pdf/1401.6876v1.pdf | author:Preslav Ivanov Nakov, Hwee Tou Ng category:cs.CL published:2014-01-23 summary:We propose a novel language-independent approach for improving machinetranslation for resource-poor languages by exploiting their similarity toresource-rich ones. More precisely, we improve the translation from aresource-poor source language X_1 into a resource-rich language Y given abi-text containing a limited number of parallel sentences for X_1-Y and alarger bi-text for X_2-Y for some resource-rich language X_2 that is closelyrelated to X_1. This is achieved by taking advantage of the opportunities thatvocabulary overlap and similarities between the languages X_1 and X_2 inspelling, word order, and syntax offer: (1) we improve the word alignments forthe resource-poor language, (2) we further augment it with additionaltranslation options, and (3) we take care of potential spelling differencesthrough appropriate transliteration. The evaluation for Indonesian- >Englishusing Malay and for Spanish -> English using Portuguese and pretending Spanishis resource-poor shows an absolute gain of up to 1.35 and 3.37 BLEU points,respectively, which is an improvement over the best rivaling approaches, whileusing much less additional data. Overall, our method cuts the amount ofnecessary "real training data by a factor of 2--5.
arxiv-5100-212 | Collaborative Regression | http://arxiv.org/pdf/1401.5823v1.pdf | author:Samuel M. Gross, Robert Tibshirani category:q-bio.QM stat.ML published:2014-01-22 summary:We consider the scenario where one observes an outcome variable and sets offeatures from multiple assays, all measured on the same set of samples. Oneapproach that has been proposed for dealing with this type of data is ``sparsemultiple canonical correlation analysis'' (sparse mCCA). All of the currentsparse mCCA techniques are biconvex and thus have no guarantees about reachinga global optimum. We propose a method for performing sparse supervisedcanonical correlation analysis (sparse sCCA), a specific case of sparse mCCAwhen one of the datasets is a vector. Our proposal for sparse sCCA is convexand thus does not face the same difficulties as the other methods. We deriveefficient algorithms for this problem, and illustrate their use on simulatedand real data.
arxiv-5100-213 | Node-Based Learning of Multiple Gaussian Graphical Models | http://arxiv.org/pdf/1303.5145v4.pdf | author:Karthik Mohan, Palma London, Maryam Fazel, Daniela Witten, Su-In Lee category:stat.ML cs.LG math.OC published:2013-03-21 summary:We consider the problem of estimating high-dimensional Gaussian graphicalmodels corresponding to a single set of variables under several distinctconditions. This problem is motivated by the task of recovering transcriptionalregulatory networks on the basis of gene expression data {containingheterogeneous samples, such as different disease states, multiple species, ordifferent developmental stages}. We assume that most aspects of the conditionaldependence networks are shared, but that there are some structured differencesbetween them. Rather than assuming that similarities and differences betweennetworks are driven by individual edges, we take a node-based approach, whichin many cases provides a more intuitive interpretation of the networkdifferences. We consider estimation under two distinct assumptions: (1)differences between the K networks are due to individual nodes that areperturbed across conditions, or (2) similarities among the K networks are dueto the presence of common hub nodes that are shared across all K networks.Using a row-column overlap norm penalty function, we formulate two convexoptimization problems that correspond to these two assumptions. We solve theseproblems using an alternating direction method of multipliers algorithm, and wederive a set of necessary and sufficient conditions that allows us to decomposethe problem into independent subproblems so that our algorithm can be scaled tohigh-dimensional settings. Our proposal is illustrated on synthetic data, awebpage data set, and a brain cancer gene expression data set.
arxiv-5100-214 | A Unified Approach to Universal Prediction: Generalized Upper and Lower Bounds | http://arxiv.org/pdf/1311.6396v2.pdf | author:N. Denizcan Vanli, Suleyman S. Kozat category:cs.LG published:2013-11-25 summary:We study sequential prediction of real-valued, arbitrary and unknownsequences under the squared error loss as well as the best parametric predictorout of a large, continuous class of predictors. Inspired by recent results fromcomputational learning theory, we refrain from any statistical assumptions anddefine the performance with respect to the class of general parametricpredictors. In particular, we present generic lower and upper bounds on thisrelative performance by transforming the prediction task into a parameterlearning problem. We first introduce the lower bounds on this relativeperformance in the mixture of experts framework, where we show that for anysequential algorithm, there always exists a sequence for which the performanceof the sequential algorithm is lower bounded by zero. We then introduce asequential learning algorithm to predict such arbitrary and unknown sequences,and calculate upper bounds on its total squared prediction error for everybounded sequence. We further show that in some scenarios we achieve matchinglower and upper bounds demonstrating that our algorithms are optimal in astrong minimax sense such that their performances cannot be improved further.As an interesting result we also prove that for the worst case scenario, theperformance of randomized algorithms can be achieved by sequential algorithmsso that randomized algorithms does not improve the performance.
arxiv-5100-215 | Numerical weather prediction or stochastic modeling: an objective criterion of choice for the global radiation forecasting | http://arxiv.org/pdf/1401.6002v1.pdf | author:Cyril Voyant, Gilles Notton, Christophe Paoli, Marie Laure Nivet, Marc Muselli, Kahina Dahmani category:stat.AP cs.LG published:2014-01-22 summary:Numerous methods exist and were developed for global radiation forecasting.The two most popular types are the numerical weather predictions (NWP) and thepredictions using stochastic approaches. We propose to compute a parameternoted constructed in part from the mutual information which is a quantity thatmeasures the mutual dependence of two variables. Both of these are calculatedwith the objective to establish the more relevant method between NWP andstochastic models concerning the current problem.
arxiv-5100-216 | Finding the True Frequent Itemsets | http://arxiv.org/pdf/1301.1218v3.pdf | author:Matteo Riondato, Fabio Vandin category:cs.LG cs.DB cs.DS stat.ML H.2.8 published:2013-01-07 summary:Frequent Itemsets (FIs) mining is a fundamental primitive in data mining. Itrequires to identify all itemsets appearing in at least a fraction $\theta$ ofa transactional dataset $\mathcal{D}$. Often though, the ultimate goal ofmining $\mathcal{D}$ is not an analysis of the dataset \emph{per se}, but theunderstanding of the underlying process that generated it. Specifically, inmany applications $\mathcal{D}$ is a collection of samples obtained from anunknown probability distribution $\pi$ on transactions, and by extracting theFIs in $\mathcal{D}$ one attempts to infer itemsets that are frequently (i.e.,with probability at least $\theta$) generated by $\pi$, which we call the TrueFrequent Itemsets (TFIs). Due to the inherently stochastic nature of thegenerative process, the set of FIs is only a rough approximation of the set ofTFIs, as it often contains a huge number of \emph{false positives}, i.e.,spurious itemsets that are not among the TFIs. In this work we design andanalyze an algorithm to identify a threshold $\hat{\theta}$ such that thecollection of itemsets with frequency at least $\hat{\theta}$ in $\mathcal{D}$contains only TFIs with probability at least $1-\delta$, for someuser-specified $\delta$. Our method uses results from statistical learningtheory involving the (empirical) VC-dimension of the problem at hand. Thisallows us to identify almost all the TFIs without including any false positive.We also experimentally compare our method with the direct mining of$\mathcal{D}$ at frequency $\theta$ and with techniques based on widely-usedstandard bounds (i.e., the Chernoff bounds) of the binomial distribution, andshow that our algorithm outperforms these methods and achieves even betterresults than what is guaranteed by the theoretical analysis.
arxiv-5100-217 | A new keyphrases extraction method based on suffix tree data structure for arabic documents clustering | http://arxiv.org/pdf/1401.5644v1.pdf | author:Issam Sahmoudi, Hanane Froud, Abdelmonaime Lachkar category:cs.CL cs.IR H.2.3 published:2014-01-22 summary:Document Clustering is a branch of a larger area of scientific study known asdata mining .which is an unsupervised classification using to find a structurein a collection of unlabeled data. The useful information in the documents canbe accompanied by a large amount of noise words when using Full TextRepresentation, and therefore will affect negatively the result of theclustering process. So it is with great need to eliminate the noise words andkeeping just the useful information in order to enhance the quality of theclustering results. This problem occurs with different degree for any languagesuch as English, European, Hindi, Chinese, and Arabic Language. To overcomethis problem, in this paper, we propose a new and efficient Keyphrasesextraction method based on the Suffix Tree data structure (KpST), the extractedKeyphrases are then used in the clustering process instead of Full TextRepresentation. The proposed method for Keyphrases extraction is languageindependent and therefore it may be applied to any language. In thisinvestigation, we are interested to deal with the Arabic language which is oneof the most complex languages. To evaluate our method, we conduct anexperimental study on Arabic Documents using the most popular Clusteringapproach of Hierarchical algorithms: Agglomerative Hierarchical algorithm withseven linkage techniques and a variety of distance functions and similaritymeasures to perform Arabic Document Clustering task. The obtained results showthat our method for extracting Keyphrases increases the quality of theclustering results. We propose also to study the effect of using the stemmingfor the testing dataset to cluster it with the same documents clusteringtechniques and similarity/distance measures.
arxiv-5100-218 | Causal Discovery in a Binary Exclusive-or Skew Acyclic Model: BExSAM | http://arxiv.org/pdf/1401.5636v1.pdf | author:Takanori Inazumi, Takashi Washio, Shohei Shimizu, Joe Suzuki, Akihiro Yamamoto, Yoshinobu Kawahara category:stat.ML cs.LG published:2014-01-22 summary:Discovering causal relations among observed variables in a given data set isa major objective in studies of statistics and artificial intelligence.Recently, some techniques to discover a unique causal model have been exploredbased on non-Gaussianity of the observed data distribution. However, most ofthese are limited to continuous data. In this paper, we present a novel causalmodel for binary data and propose an efficient new approach to deriving theunique causal model governing a given binary data set under skew distributionsof external binary noises. Experimental evaluation shows excellent performancefor both artificial and real world data sets.
arxiv-5100-219 | Enhancing Template Security of Face Biometrics by Using Edge Detection and Hashing | http://arxiv.org/pdf/1401.5632v1.pdf | author:Manoj Krishnaswamy, G. Hemantha Kumar category:cs.CV published:2014-01-22 summary:In this paper we address the issues of using edge detection techniques onfacial images to produce cancellable biometric templates and a novel method fortemplate verification against tampering. With increasing use of biometrics,there is a real threat for the conventional systems using face databases, whichstore images of users in raw and unaltered form. If compromised not only it isirrevocable, but can be misused for cross-matching across different databases.So it is desirable to generate and store revocable templates for the same userin different applications to prevent cross-matching and to enhance security,while maintaining privacy and ethics. By comparing different edge detectionmethods it has been observed that the edge detection based on the Roberts Crossoperator performs consistently well across multiple face datasets, in which theface images have been taken under a variety of conditions. We have proposed anovel scheme using hashing, for extra verification, in order to harden thesecurity of the stored biometric templates.
arxiv-5100-220 | Identifiability of an Integer Modular Acyclic Additive Noise Model and its Causal Structure Discovery | http://arxiv.org/pdf/1401.5625v1.pdf | author:Joe Suzuki, Takanori Inazumi, Takashi Washio, Shohei Shimizu category:stat.ML published:2014-01-22 summary:The notion of causality is used in many situations dealing with uncertainty.We consider the problem whether causality can be identified given data setgenerated by discrete random variables rather than continuous ones. Inparticular, for non-binary data, thus far it was only known that causality canbe identified except rare cases. In this paper, we present necessary andsufficient condition for an integer modular acyclic additive noise (IMAN) oftwo variables. In addition, we relate bivariate and multivariate causalidentifiability in a more explicit manner, and develop a practical algorithm tofind the order of variables and their parent sets. We demonstrate itsperformance in applications to artificial data and real world body motion datawith comparisons to conventional methods.
arxiv-5100-221 | The Gabor-Einstein Wavelet: A Model for the Receptive Fields of V1 to MT Neurons | http://arxiv.org/pdf/1401.5589v1.pdf | author:Stephen G. Odaibo category:q-bio.NC cs.CV physics.bio-ph published:2014-01-22 summary:Our visual system is astonishingly efficient at detecting moving objects.This process is mediated by the neurons which connect the primary visual cortex(V1) to the middle temporal (MT) area. Interestingly, since Kuffler'spioneering experiments on retinal ganglion cells, mathematical models have beenvital for advancing our understanding of the receptive fields of visualneurons. However, existing models were not designed to describe the mostsalient attributes of the highly specialized neurons in the V1 to MT motionprocessing stream; and they have not been able to do so. Here, we introduce theGabor-Einstein wavelet, a new family of functions for representing thereceptive fields of V1 to MT neurons. We show that the way space and time aremixed in the visual cortex is analogous to the way they are mixed in thespecial theory of relativity (STR). Hence we constrained the Gabor-Einsteinmodel by requiring: (i) relativistic-invariance of the wave carrier, and (ii)the minimum possible number of parameters. From these two constraints, the sincfunction emerged as a natural descriptor of the wave carrier. The particulardistribution of lowpass to bandpass temporal frequency filtering properties ofV1 to MT neurons (Foster et al 1985; DeAngelis et al 1993b; Hawken et al 1996)is clearly explained by the Gabor-Einstein basis. Furthermore, it does so in amanner innately representative of the motion-processing stream's neuronalhierarchy. Our analysis and computer simulations show that the distribution oftemporal frequency filtering properties along the motion processing stream is adirect effect of the way the brain jointly encodes space and time. We uncoveredthis fundamental link by demonstrating that analogous mathematical structuresunderlie STR and joint cortical spacetime encoding. This link will provide newphysiological insights into how the brain represents visual information.
arxiv-5100-222 | License Plate Recognition (LPR): A Review with Experiments for Malaysia Case Study | http://arxiv.org/pdf/1401.5559v1.pdf | author:Nuzulha Khilwani Ibrahim, Emaliana Kasmuri, Norazira A Jalil, Mohd Adili Norasikin, Sazilah Salam, Mohamad Riduwan Md Nawawi category:cs.CV published:2014-01-22 summary:Most vehicle license plate recognition use neural network techniques toenhance its computing capability. The image of the vehicle license plate iscaptured and processed to produce a textual output for further processing. Thispaper reviews image processing and neural network techniques applied atdifferent stages which are preprocessing, filtering, feature extraction,segmentation and recognition in such way to remove the noise of the image, toenhance the image quality and to expedite the computing process by convertingthe characters in the image into respective text. An exemplar experiment hasbeen done in MATLAB to show the basic process of the image processingespecially for license plate in Malaysia case study. An algorithm is adaptedinto the solution for parking management system. The solution then isimplemented as proof of concept to the algorithm.
arxiv-5100-223 | On the symmetrical Kullback-Leibler Jeffreys centroids | http://arxiv.org/pdf/1303.7286v3.pdf | author:Frank Nielsen category:cs.IT cs.LG math.IT stat.ML published:2013-03-29 summary:Due to the success of the bag-of-word modeling paradigm, clusteringhistograms has become an important ingredient of modern information processing.Clustering histograms can be performed using the celebrated $k$-meanscentroid-based algorithm. From the viewpoint of applications, it is usuallyrequired to deal with symmetric distances. In this letter, we consider theJeffreys divergence that symmetrizes the Kullback-Leibler divergence, andinvestigate the computation of Jeffreys centroids. We first prove that theJeffreys centroid can be expressed analytically using the Lambert $W$ functionfor positive histograms. We then show how to obtain a fast guaranteedapproximation when dealing with frequency histograms. Finally, we conclude withsome remarks on the $k$-means histogram clustering.
arxiv-5100-224 | Distributed Online Learning in Social Recommender Systems | http://arxiv.org/pdf/1309.6707v2.pdf | author:Cem Tekin, Simpson Zhang, Mihaela van der Schaar category:cs.SI cs.LG stat.ML published:2013-09-26 summary:In this paper, we consider decentralized sequential decision making indistributed online recommender systems, where items are recommended to usersbased on their search query as well as their specific background includinghistory of bought items, gender and age, all of which comprise the contextinformation of the user. In contrast to centralized recommender systems, inwhich there is a single centralized seller who has access to the completeinventory of items as well as the complete record of sales and userinformation, in decentralized recommender systems each seller/learner only hasaccess to the inventory of items and user information for its own products andnot the products and user information of other sellers, but can get commissionif it sells an item of another seller. Therefore the sellers must distributedlyfind out for an incoming user which items to recommend (from the set of ownitems or items of another seller), in order to maximize the revenue from ownsales and commissions. We formulate this problem as a cooperative contextualbandit problem, analytically bound the performance of the sellers compared tothe best recommendation strategy given the complete realization of userarrivals and the inventory of items, as well as the context-dependent purchaseprobabilities of each item, and verify our results via numerical examples on adistributed data set adapted based on Amazon data. We evaluate the dependenceof the performance of a seller on the inventory of items the seller has, thenumber of connections it has with the other sellers, and the commissions whichthe seller gets by selling items of other sellers to its users.
arxiv-5100-225 | Guaranteed Model Order Estimation and Sample Complexity Bounds for LDA | http://arxiv.org/pdf/1312.2646v4.pdf | author:E. D. Gutiérrez category:stat.ML published:2013-12-10 summary:The question of how to determine the number of independent latent factors(topics) in mixture models such as Latent Dirichlet Allocation (LDA) is ofgreat practical importance. In most applications, the exact number of topics isunknown, and depends on the application and the size of the data set. Bayesiannonparametric methods can avoid the problem of topic number selection, but theycan be impracticably slow for large sample sizes and are subject to localoptima. We develop a guaranteed procedure for topic number recovery that doesnot necessitate learning the model's latent parameters beforehand. Ourprocedure relies on adapting results from random matrix theory. Performance ofour topic number recovery procedure is superior to hLDA, a nonparametricmethod. We also discuss some implications of our results on the samplecomplexity and accuracy of popular spectral learning algorithms for LDA. Ourresults and procedure can be extended to spectral learning algorithms for otherexchangeable mixture models as well as Hidden Markov Models.
arxiv-5100-226 | Reaserchnig the Development of the Electrical Power System Using Systemically Evolutionary Algorithm | http://arxiv.org/pdf/1401.5789v1.pdf | author:Jerzy Tchorzewski, Emil Chyzy category:cs.NE cs.SY published:2014-01-21 summary:The paper contains the concept and the results of research concerning theevolutionary algorithm, identified based on the systems control theory, whichwas called the Systemically of Evolutionary Algorithm (SAE). Special attentionwas paid to two elements of evolutionary algorithms, which have not been fullysolved yet, i.e. to the methods used to create the initial population and themethod of creating the robustness (fitness) function. Other elements of the SEAalgorithm, i.a. cross-over, mutation, selection, etc. were also defined from asystemic point of view. Computational experiments were conducted using aselected subsystem of the Polish Electrical Power System and three programminglanguages: Java, C++ and Matlab. Selected comparative results for the SAEalgorithm in different implementations were also presented.
arxiv-5100-227 | Hilbert Space Methods for Reduced-Rank Gaussian Process Regression | http://arxiv.org/pdf/1401.5508v1.pdf | author:Arno Solin, Simo Särkkä category:stat.ML published:2014-01-21 summary:This paper proposes a novel scheme for reduced-rank Gaussian processregression. The method is based on an approximate series expansion of thecovariance function in terms of an eigenfunction expansion of the Laplaceoperator in a compact subset of $\mathbb{R}^d$. On this approximate eigenbasisthe eigenvalues of the covariance function can be expressed as simple functionsof the spectral density of the Gaussian process, which allows the GP inferenceto be solved under a computational cost scaling as $\mathcal{O}(nm^2)$(initial) and $\mathcal{O}(m^3)$ (hyperparameter learning) with $m$ basisfunctions and $n$ data points. The approach also allows for rigorous erroranalysis with Hilbert space theory, and we show that the approximation becomesexact when the size of the compact subset and the number of eigenfunctions goto infinity. The expansion generalizes to Hilbert spaces with an inner productwhich is defined as an integral over a specified input density. The method iscompared to previously proposed methods theoretically and through empiricaltests with simulated and real data.
arxiv-5100-228 | On change point detection using the fused lasso method | http://arxiv.org/pdf/1401.5408v1.pdf | author:Cristian R. Rojas, Bo Wahlberg category:math.ST stat.ML stat.TH 62G08, 62G20 published:2014-01-21 summary:In this paper we analyze the asymptotic properties of l1 penalized maximumlikelihood estimation of signals with piece-wise constant mean values and/orvariances. The focus is on segmentation of a non-stationary time series withrespect to changes in these model parameters. This change point detection andestimation problem is also referred to as total variation denoising or l1 -meanfiltering and has many important applications in most fields of science andengineering. We establish the (approximate) sparse consistency properties,including rate of convergence, of the so-called fused lasso signal approximator(FLSA). We show that this only holds if the sign of the correspondingconsecutive changes are all different, and that this estimator is otherwiseincapable of correctly detecting the underlying sparsity pattern. The key ideais to notice that the optimality conditions for this problem can be analyzedusing techniques related to brownian bridge theory.
arxiv-5100-229 | HMACA: Towards Proposing a Cellular Automata Based Tool for Protein Coding, Promoter Region Identification and Protein Structure Prediction | http://arxiv.org/pdf/1401.5364v1.pdf | author:Pokkuluri Kiran Sree, Inampudi Ramesh Babu, SSSN Usha Devi N category:cs.CE cs.LG published:2014-01-21 summary:Human body consists of lot of cells, each cell consist of DeOxaRibo NucleicAcid (DNA). Identifying the genes from the DNA sequences is a very difficulttask. But identifying the coding regions is more complex task compared to theformer. Identifying the protein which occupy little place in genes is a reallychallenging issue. For understating the genes coding region analysis plays animportant role. Proteins are molecules with macro structure that areresponsible for a wide range of vital biochemical functions, which includesacting as oxygen, cell signaling, antibody production, nutrient transport andbuilding up muscle fibers. Promoter region identification and protein structureprediction has gained a remarkable attention in recent years. Even though thereare some identification techniques addressing this problem, the approximateaccuracy in identifying the promoter region is closely 68% to 72%. We havedeveloped a Cellular Automata based tool build with hybrid multiple attractorcellular automata (HMACA) classifier for protein coding region, promoter regionidentification and protein structure prediction which predicts the protein andpromoter regions with an accuracy of 76%. This tool also predicts the structureof protein with an accuracy of 80%.
arxiv-5100-230 | Study of Neural Network Algorithm for Straight-Line Drawings of Planar Graphs | http://arxiv.org/pdf/1401.5330v1.pdf | author:Mohamed A. El-Sayed, S. Abdel-Khalek, Hanan H. Amin category:cs.CG cs.NE published:2014-01-21 summary:Graph drawing addresses the problem of finding a layout of a graph thatsatisfies given aesthetic and understandability objectives. The most importantobjective in graph drawing is minimization of the number of crossings in thedrawing, as the aesthetics and readability of graph drawings depend on thenumber of edge crossings. VLSI layouts with fewer crossings are more easilyrealizable and consequently cheaper. A straight-line drawing of a planar graphG of n vertices is a drawing of G such that each edge is drawn as astraight-line segment without edge crossings. However, a problem with currentgraph layout methods which are capable of producing satisfactory results for awide range of graphs is that they often put an extremely high demand oncomputational resources. This paper introduces a new layout method, whichnicely draws internally convex of planar graph that consumes only littlecomputational resources and does not need any heavy duty preprocessing. Here,we use two methods: The first is self organizing map known from unsupervisedneural networks which is known as (SOM) and the second method is Inverse SelfOrganized Map (ISOM).
arxiv-5100-231 | Compositional Operators in Distributional Semantics | http://arxiv.org/pdf/1401.5327v1.pdf | author:Dimitri Kartsaklis category:cs.CL cs.AI math.CT published:2014-01-21 summary:This survey presents in some detail the main advances that have been recentlytaking place in Computational Linguistics towards the unification of the twoprominent semantic paradigms: the compositional formal semantics view and thedistributional models of meaning based on vector spaces. After an introductionto these two approaches, I review the most important models that aim to providecompositionality in distributional semantics. Then I proceed and present inmore detail a particular framework by Coecke, Sadrzadeh and Clark (2010) basedon the abstract mathematical setting of category theory, as a more completeexample capable to demonstrate the diversity of techniques and scientificdisciplines that this kind of research can draw from. This paper concludes witha discussion about important open issues that need to be addressed by theresearchers in the future.
arxiv-5100-232 | Increasing Server Availability for Overall System Security: A Preventive Maintenance Approach Based on Failure Prediction | http://arxiv.org/pdf/1401.5686v1.pdf | author:Ayman M. Bahaa-Eldin, Hoda K. Mohamead, Sally S. Deraz category:cs.DC cs.NE published:2014-01-21 summary:Server Availability (SA) is an important measure of overall systems security.Important security systems rely on the availability of their hosting servers todeliver critical security services. Many of these servers offer managementinterface through web mainly using an Apache server. This paper investigatesthe increase of Server Availability by the use of Artificial Neural Networks(ANN) to predict software aging phenomenon. Several resource usage data iscollected and analyzed on a typical long-running software system (a webserver). A Multi-Layer Perceptron feed forward Artificial Neural Network wastrained on an Apache web server data-set to predict future server resourceexhaustion through uni-variate time series forecasting. The results werebenchmarked against those obtained from non-parametric statistical techniques,parametric time series models and empirical modeling techniques reported in theliterature.
arxiv-5100-233 | Genetic Algorithms and its use with back-propagation network | http://arxiv.org/pdf/1401.5246v1.pdf | author:Ayman M. Bahaa-Eldin, A. M. A. Wahdan, H. M. K. Mahdi category:cs.NE published:2014-01-21 summary:Genetic algorithms are considered as one of the most efficient searchtechniques. Although they do not offer an optimal solution, their ability toreach a suitable solution in considerably short time gives them theirrespectable role in many AI techniques. This work introduces genetic algorithmsand describes their characteristics. Then a novel method using geneticalgorithm in best training set generation and selection for a back-propagationnetwork is proposed. This work also offers a new extension to the originalgenetic algorithms
arxiv-5100-234 | Edge detection of binary images using the method of masks | http://arxiv.org/pdf/1401.5245v1.pdf | author:Ayman M Bahaa-Eldeen, Abdel-Moneim A. Wahdan, Hani M. K. Mahdi category:cs.CV published:2014-01-21 summary:In this work the method of masks, creating and using of inverted image masks,together with binary operation of image data are used in edge detection ofbinary images, monochrome images, which yields about 300 times faster thanordinary methods. The method is divided into three stages: Mask construction,Fundamental edge detection, and Edge Construction Comparison with an ordinarymethod and a fuzzy based method is carried out.
arxiv-5100-235 | Optimal Intelligent Control for Wind Turbulence Rejection in WECS Using ANNs and Genetic Fuzzy Approach | http://arxiv.org/pdf/1401.5221v1.pdf | author:Hadi kasiri, hamid reza momeni, Atiyeh Kasiri category:cs.SY cs.NE published:2014-01-21 summary:One of the disadvantages in Connection of wind energy conversion systems(WECSs) to transmission networks is plentiful turbulence of wind speed.Therefore effects of this problem must be controlled. Nowadays,pitch-controlled WECSs are increasingly used for variable speed and pitch windturbines. Megawatt class wind turbines generally turn at variable speed in windfarm. Thus turbine operation must be controlled in order to maximize theconversion efficiency below rated power and reduce loading on the drive-train.Due to random and non-linear nature of the wind turbulence and the ability ofMulti-Layer Perceptron (MLP) and Radial Basis Function (RBF) Artificial NeuralNetworks (ANNs) in the modeling and control of this turbulence, in this study,widespread changes of wind have been perused using MLP and RBF artificial NNs.In addition in this study, a new genetic fuzzy system has been successfullyapplied to identify disturbance wind in turbine input. Thus output power hasbeen regulated in optimal and nominal range by pitch angle regulation.Consequently, our proposed approaches have regulated output aerodynamic powerand torque in the nominal rang.
arxiv-5100-236 | Consistency of weighted majority votes | http://arxiv.org/pdf/1312.0451v5.pdf | author:Daniel Berend, Aryeh Kontorovich category:math.PR cs.LG stat.ML 60C05, 60F15 published:2013-12-02 summary:We revisit the classical decision-theoretic problem of weighted expert votingfrom a statistical learning perspective. In particular, we examine theconsistency (both asymptotic and finitary) of the optimal Nitzan-Paroushweighted majority and related rules. In the case of known expert competencelevels, we give sharp error estimates for the optimal rule. When the competencelevels are unknown, they must be empirically estimated. We provide frequentistand Bayesian analyses for this situation. Some of our proof techniques arenon-standard and may be of independent interest. The bounds we derive arenearly optimal, and several challenging open problems are posed. Experimentalresults are provided to illustrate the theory.
arxiv-5100-237 | Multi-GPU parallel memetic algorithm for capacitated vehicle routing problem | http://arxiv.org/pdf/1401.5216v1.pdf | author:Michał Karpiński, Maciej Pacut category:cs.DC cs.NE published:2014-01-21 summary:The goal of this paper is to propose and test a new memetic algorithm for thecapacitated vehicle routing problem in parallel computing environment. In thispaper we consider simple variation of vehicle routing problem in which the onlyparameter is the capacity of the vehicle and each client only needs onepackage. We present simple reduction to prove the existence of polynomial-timealgorithm for capacity 2. We analyze the efficiency of the algorithm usinghierarchical Parallel Random Access Machine (PRAM) model and run experimentswith code written in CUDA (for capacities larger than 2).
arxiv-5100-238 | A Unifying Framework for Typical Multi-Task Multiple Kernel Learning Problems | http://arxiv.org/pdf/1401.5136v1.pdf | author:Cong Li, Michael Georgiopoulos, Georgios C. Anagnostopoulos category:cs.LG published:2014-01-21 summary:Over the past few years, Multi-Kernel Learning (MKL) has received significantattention among data-driven feature selection techniques in the context ofkernel-based learning. MKL formulations have been devised and solved for abroad spectrum of machine learning problems, including Multi-Task Learning(MTL). Solving different MKL formulations usually involves designing algorithmsthat are tailored to the problem at hand, which is, typically, a non-trivialaccomplishment. In this paper we present a general Multi-Task Multi-Kernel Learning(Multi-Task MKL) framework that subsumes well-known Multi-Task MKLformulations, as well as several important MKL approaches on single-taskproblems. We then derive a simple algorithm that can solve the unifyingframework. To demonstrate the flexibility of the proposed framework, weformulate a new learning problem, namely Partially-Shared Common Space (PSCS)Multi-Task MKL, and demonstrate its merits through experimentation.
arxiv-5100-239 | An Identification System Using Eye Detection Based On Wavelets And Neural Networks | http://arxiv.org/pdf/1401.5108v1.pdf | author:Mohamed A. El-Sayed, Mohamed A. Khafagy category:cs.CV published:2014-01-20 summary:The randomness and uniqueness of human eye patterns is a major breakthroughin the search for quicker, easier and highly reliable forms of automatic humanidentification. It is being used extensively in security solutions. Thisincludes access control to physical facilities, security systems andinformation databases, Suspect tracking, surveillance and intrusion detectionand by various Intelligence agencies through out the world. We use theadvantage of human eye uniqueness to identify people and approve its validityas a biometric. . Eye detection involves first extracting the eye from adigital face image, and then encoding the unique patterns of the eye in such away that they can be compared with pre-registered eye patterns. The eyedetection system consists of an automatic segmentation system that is based onthe wavelet transform, and then the Wavelet analysis is used as a pre-processorfor a back propagation neural network with conjugate gradient learning. Theinputs to the neural network are the wavelet maxima neighborhood coefficientsof face images at a particular scale. The output of the neural network is theclassification of the input into an eye or non-eye region. An accuracy of 90%is observed for identifying test images under different conditions included intraining stage.
arxiv-5100-240 | Study of Efficient Technique Based On 2D Tsallis Entropy For Image Thresholding | http://arxiv.org/pdf/1401.5098v1.pdf | author:Mohamed A. El-Sayed, S. Abdel-Khalek, Eman Abdel-Aziz category:cs.CV 68U10 published:2014-01-20 summary:Thresholding is an important task in image processing. It is a main tool inpattern recognition, image segmentation, edge detection and scene analysis. Inthis paper, we present a new thresholding technique based on two-dimensionalTsallis entropy. The two-dimensional Tsallis entropy was obtained from thetwodimensional histogram which was determined by using the gray value of thepixels and the local average gray value of the pixels, the work it was applieda generalized entropy formalism that represents a recent development instatistical mechanics. The effectiveness of the proposed method is demonstratedby using examples from the real-world and synthetic images. The performanceevaluation of the proposed technique in terms of the quality of the thresholdedimages are presented. Experimental results demonstrate that the proposed methodachieve better result than the Shannon method.
arxiv-5100-241 | A Review of Verbal and Non-Verbal Human-Robot Interactive Communication | http://arxiv.org/pdf/1401.4994v1.pdf | author:Nikolaos Mavridis category:cs.RO cs.CL published:2014-01-20 summary:In this paper, an overview of human-robot interactive communication ispresented, covering verbal as well as non-verbal aspects of human-robotinteraction. Following a historical introduction, and motivation towards fluidhuman-robot communication, ten desiderata are proposed, which provide anorganizational axis both of recent as well as of future research on human-robotcommunication. Then, the ten desiderata are examined in detail, culminating toa unifying discussion, and a forward-looking conclusion.
arxiv-5100-242 | Classification of IDS Alerts with Data Mining Techniques | http://arxiv.org/pdf/1401.4872v1.pdf | author:Hany Nashat Gabra, Ayman Mohammad Bahaa-Eldin, Huda Korashy category:cs.CR cs.DB cs.LG published:2014-01-20 summary:A data mining technique to reduce the amount of false alerts within an IDSsystem is proposed. The new technique achieves an accuracy of 99% compared to97% by the current systems.
arxiv-5100-243 | Does Syntactic Knowledge help English-Hindi SMT? | http://arxiv.org/pdf/1401.4869v1.pdf | author:Taraka Rama, Karthik Gali, Avinesh PVS category:cs.CL cs.AI published:2014-01-20 summary:In this paper we explore various parameter settings of the state-of-artStatistical Machine Translation system to improve the quality of thetranslation for a `distant' language pair like English-Hindi. We proposed newtechniques for efficient reordering. A slight improvement over the baseline isreported using these techniques. We also show that a simple pre-processing stepcan improve the quality of the translation significantly.
arxiv-5100-244 | A Genetic Algorithm to Optimize a Tweet for Retweetability | http://arxiv.org/pdf/1401.4857v1.pdf | author:Ronald Hochreiter, Christoph Waldhauser category:cs.NE cs.CY cs.SI physics.soc-ph published:2014-01-20 summary:Twitter is a popular microblogging platform. When users send out messages,other users have the ability to forward these messages to their own subgraph.Most research focuses on increasing retweetability from a node's perspective.Here, we center on improving message style to increase the chance of a messagebeing forwarded. To this end, we simulate an artificial Twitter-like networkwith nodes deciding deterministically on retweeting a message or not. A geneticalgorithm is used to optimize message composition, so that the reach of amessage is increased. When analyzing the algorithm's runtime behavior across aset of different node types, we find that the algorithm consistently succeedsin significantly improving the retweetability of a message.
arxiv-5100-245 | An Evolutionary Approach towards Clustering Airborne Laser Scanning Data | http://arxiv.org/pdf/1401.4848v1.pdf | author:Ronald Hochreiter, Christoph Waldhauser category:cs.NE published:2014-01-20 summary:In land surveying, the generation of maps was greatly simplified with theintroduction of orthophotos and at a later stage with airborne LiDAR laserscanning systems. While the original purpose of LiDAR systems was to determinethe altitude of ground elevations, newer full wave systems provide additionalinformation that can be used on classifying the type of ground cover and thegeneration of maps. The LiDAR resulting point clouds are huge, multidimensionaldata sets that need to be grouped in classes of ground cover. We propose agenetic algorithm that aids in classifying these data sets and thus make themusable for map generation. A key feature are tailor-made genetic operators andfitness functions for the subject. The algorithm is compared to a traditionalk-means clustering.
arxiv-5100-246 | Smoothed Analysis of Tensor Decompositions | http://arxiv.org/pdf/1311.3651v4.pdf | author:Aditya Bhaskara, Moses Charikar, Ankur Moitra, Aravindan Vijayaraghavan category:cs.DS cs.LG stat.ML published:2013-11-14 summary:Low rank tensor decompositions are a powerful tool for learning generativemodels, and uniqueness results give them a significant advantage over matrixdecomposition methods. However, tensors pose significant algorithmic challengesand tensors analogs of much of the matrix algebra toolkit are unlikely to existbecause of hardness results. Efficient decomposition in the overcomplete case(where rank exceeds dimension) is particularly challenging. We introduce asmoothed analysis model for studying these questions and develop an efficientalgorithm for tensor decomposition in the highly overcomplete case (rankpolynomial in the dimension). In this setting, we show that our algorithm isrobust to inverse polynomial error -- a crucial property for applications inlearning since we are only allowed a polynomial number of samples. Whilealgorithms are known for exact tensor decomposition in some overcompletesettings, our main contribution is in analyzing their stability in theframework of smoothed analysis. Our main technical contribution is to show that tensor products of perturbedvectors are linearly independent in a robust sense (i.e. the associated matrixhas singular values that are at least an inverse polynomial). This key resultpaves the way for applying tensor methods to learning problems in the smoothedsetting. In particular, we use it to obtain results for learning multi-viewmodels and mixtures of axis-aligned Gaussians where there are many more"components" than dimensions. The assumption here is that the model is notadversarially chosen, formalized by a perturbation of model parameters. Webelieve this an appealing way to analyze realistic instances of learningproblems, since this framework allows us to overcome many of the usuallimitations of using tensor methods.
arxiv-5100-247 | Generalized Bhattacharyya and Chernoff upper bounds on Bayes error using quasi-arithmetic means | http://arxiv.org/pdf/1401.4788v1.pdf | author:Frank Nielsen category:cs.CV cs.IT math.IT published:2014-01-20 summary:Bayesian classification labels observations based on given prior information,namely class-a priori and class-conditional probabilities. Bayes' risk is theminimum expected classification cost that is achieved by the Bayes' test, theoptimal decision rule. When no cost incurs for correct classification and unitcost is charged for misclassification, Bayes' test reduces to the maximum aposteriori decision rule, and Bayes risk simplifies to Bayes' error, theprobability of error. Since calculating this probability of error is oftenintractable, several techniques have been devised to bound it with closed-formformula, introducing thereby measures of similarity and divergence betweendistributions like the Bhattacharyya coefficient and its associatedBhattacharyya distance. The Bhattacharyya upper bound can further be tightenedusing the Chernoff information that relies on the notion of best errorexponent. In this paper, we first express Bayes' risk using the total variationdistance on scaled distributions. We then elucidate and extend theBhattacharyya and the Chernoff upper bound mechanisms using generalizedweighted means. We provide as a byproduct novel notions of statisticaldivergences and affinity coefficients. We illustrate our technique by derivingnew upper bounds for the univariate Cauchy and the multivariate$t$-distributions, and show experimentally that those bounds are not toodistant to the computationally intractable Bayes' error.
arxiv-5100-248 | Anomaly detection in reconstructed quantum states using a machine-learning technique | http://arxiv.org/pdf/1401.4785v1.pdf | author:Satoshi Hara, Takafumi Ono, Ryo Okamoto, Takashi Washio, Shigeki Takeuchi category:quant-ph stat.AP stat.ML 81V80 published:2014-01-20 summary:The accurate detection of small deviations in given density matrices isimportant for quantum information processing. Here we propose a new methodbased on the concept of data mining. We demonstrate that the proposed methodcan more accurately detect small erroneous deviations in reconstructed densitymatrices, which contain intrinsic fluctuations due to the limited number ofsamples, than a naive method of checking the trace distance from the average ofthe given density matrices. This method has the potential to be a key tool inbroad areas of physics where the detection of small deviations of quantumstates reconstructed using a limited number of samples are essential.
arxiv-5100-249 | Revolutionary Algorithms | http://arxiv.org/pdf/1401.4714v1.pdf | author:Ronald Hochreiter, Christoph Waldhauser category:cs.NE published:2014-01-19 summary:The optimization of dynamic problems is both widespread and difficult. Whenconducting dynamic optimization, a balance between reinitialization andcomputational expense has to be found. There are multiple approaches to this.In parallel genetic algorithms, multiple sub-populations concurrently try tooptimize a potentially dynamic problem. But as the number of sub-populationincreases, their efficiency decreases. Cultural algorithms provide a frameworkthat has the potential to make optimizations more efficient. But they adaptslowly to changing environments. We thus suggest a confluence of theseapproaches: revolutionary algorithms. These algorithms seek to extend theevolutionary and cultural aspects of the former to approaches with a notion ofthe political. By modeling how belief systems are changed by means ofrevolution, these algorithms provide a framework to model and optimize dynamicproblems in an efficient fashion.
arxiv-5100-250 | Evolutionary Optimization for Decision Making under Uncertainty | http://arxiv.org/pdf/1401.4696v1.pdf | author:Ronald Hochreiter category:cs.NE published:2014-01-19 summary:Optimizing decision problems under uncertainty can be done using a variety ofsolution methods. Soft computing and heuristic approaches tend to be powerfulfor solving such problems. In this overview article, we survey EvolutionaryOptimization techniques to solve Stochastic Programming problems - both for thesingle-stage and multi-stage case.
arxiv-5100-251 | Evolving Accuracy: A Genetic Algorithm to Improve Election Night Forecasts | http://arxiv.org/pdf/1401.4674v1.pdf | author:Ronald Hochreiter, Christoph Waldhauser category:cs.NE published:2014-01-19 summary:In this paper, we apply genetic algorithms to the field of electoral studies.Forecasting election results is one of the most exciting and demanding tasks inthe area of market research, especially due to the fact that decisions have tobe made within seconds on live television. We show that the proposed methodoutperforms currently applied approaches and thereby provide an argument totighten the intersection between computer science and social science,especially political science, further. We scrutinize the performance of ouralgorithm's runtime behavior to evaluate its applicability in the field.Numerical results with real data from a local election in the Austrian provinceof Styria from 2010 substantiate the applicability of the proposed approach.
arxiv-5100-252 | Visual Tracking using Particle Swarm Optimization | http://arxiv.org/pdf/1401.4648v1.pdf | author:Rafid Siddiqui, Siamak Khatibi category:cs.CV published:2014-01-19 summary:The problem of robust extraction of visual odometry from a sequence of imagesobtained by an eye in hand camera configuration is addressed. A novel approachtoward solving planar template based tracking is proposed which performs anon-linear image alignment for successful retrieval of camera transformations.In order to obtain global optimum a bio-metaheuristic is used for optimizationof similarity among the planar regions. The proposed method is validated onimage sequences with real as well as synthetic transformations and found to beresilient to intensity variations. A comparative analysis of the varioussimilarity measures as well as various state-of-art methods reveal that thealgorithm succeeds in tracking the planar regions robustly and has goodpotential to be used in real applications.
arxiv-5100-253 | The Capacity of String-Replication Systems | http://arxiv.org/pdf/1401.4634v1.pdf | author:Farzad Farnoud, Moshe Schwartz, Jehoshua Bruck category:cs.IT cs.CL math.IT published:2014-01-19 summary:It is known that the majority of the human genome consists of repeatedsequences. Furthermore, it is believed that a significant part of the rest ofthe genome also originated from repeated sequences and has mutated to itscurrent form. In this paper, we investigate the possibility of constructing anexponentially large number of sequences from a short initial sequence andsimple replication rules, including those resembling genomic replicationprocesses. In other words, our goal is to find out the capacity, or theexpressive power, of these string-replication systems. Our results includeexact capacities, and bounds on the capacities, of four fundamentalstring-replication systems.
arxiv-5100-254 | Modelling Observation Correlations for Active Exploration and Robust Object Detection | http://arxiv.org/pdf/1401.4612v1.pdf | author:Javier Velez, Garrett Hemann, Albert S. Huang, Ingmar Posner, Nicholas Roy category:cs.RO cs.CV published:2014-01-18 summary:Today, mobile robots are expected to carry out increasingly complex tasks inmultifarious, real-world environments. Often, the tasks require a certainsemantic understanding of the workspace. Consider, for example, spokeninstructions from a human collaborator referring to objects of interest; therobot must be able to accurately detect these objects to correctly understandthe instructions. However, existing object detection, while competent, is notperfect. In particular, the performance of detection algorithms is commonlysensitive to the position of the sensor relative to the objects in the scene.This paper presents an online planning algorithm which learns an explicit modelof the spatial dependence of object detection and generates plans whichmaximize the expected performance of the detection, and by extension theoverall plan performance. Crucially, the learned sensor model incorporatesspatial correlations between measurements, capturing the fact that successivemeasurements taken at the same or nearby locations are not independent. We showhow this sensor model can be incorporated into an efficient forward searchalgorithm in the information space of detected objects, allowing the robot togenerate motion plans efficiently. We investigate the performance of ourapproach by addressing the tasks of door and text detection in indoorenvironments and demonstrate significant improvement in detection performanceduring task execution over alternative methods in simulated and real robotexperiments.
arxiv-5100-255 | Generalized Biwords for Bitext Compression and Translation Spotting | http://arxiv.org/pdf/1401.5674v1.pdf | author:Felipe Sánchez-Martínez, Rafael C. Carrasco, Miguel A. Martínez-Prieto, Joaquin Adiego category:cs.CL published:2014-01-18 summary:Large bilingual parallel texts (also known as bitexts) are usually stored ina compressed form, and previous work has shown that they can be moreefficiently compressed if the fact that the two texts are mutual translationsis exploited. For example, a bitext can be seen as a sequence of biwords---pairs of parallel words with a high probability of co-occurrence--- that canbe used as an intermediate representation in the compression process. However,the simple biword approach described in the literature can only exploitone-to-one word alignments and cannot tackle the reordering of words. Wetherefore introduce a generalization of biwords which can describe multi-wordexpressions and reorderings. We also describe some methods for the binarycompression of generalized biword sequences, and compare their performance whendifferent schemes are applied to the extraction of the biword sequence. Inaddition, we show that this generalization of biwords allows for theimplementation of an efficient algorithm to look on the compressed bitext forwords or text segments in one of the texts and retrieve their counterparttranslations in the other text ---an application usually referred to astranslation spotting--- with only some minor modifications in the compressionalgorithm.
arxiv-5100-256 | Learning to Win by Reading Manuals in a Monte-Carlo Framework | http://arxiv.org/pdf/1401.5390v1.pdf | author:S. R. K. Branavan, David Silver, Regina Barzilay category:cs.CL cs.AI cs.LG published:2014-01-18 summary:Domain knowledge is crucial for effective performance in autonomous controlsystems. Typically, human effort is required to encode this knowledge into acontrol algorithm. In this paper, we present an approach to language groundingwhich automatically interprets text in the context of a complex controlapplication, such as a game, and uses domain knowledge extracted from the textto improve control performance. Both text analysis and control strategies arelearned jointly using only a feedback signal inherent to the application. Toeffectively leverage textual information, our method automatically extracts thetext segment most relevant to the current game state, and labels it with atask-centric predicate structure. This labeled text is then used to bias anaction selection policy for the game, guiding it towards promising regions ofthe action space. We encode our model for text analysis and game playing in amulti-layer neural network, representing linguistic decisions via latentvariables in the hidden layers, and game action quality via the output layer.Operating within the Monte-Carlo Search framework, we estimate model parametersusing feedback from simulated games. We apply our approach to the complexstrategy game Civilization II using the official game manual as the text guide.Our results show that a linguistically-informed game-playing agentsignificantly outperforms its language-unaware counterpart, yielding a 34%absolute improvement and winning over 65% of games when playing against thebuilt-in AI of Civilization.
arxiv-5100-257 | Semantic Similarity Measures Applied to an Ontology for Human-Like Interaction | http://arxiv.org/pdf/1401.4603v1.pdf | author:Esperanza Albacete, Javier Calle, Elena Castro, Dolores Cuadra category:cs.AI cs.CL published:2014-01-18 summary:The focus of this paper is the calculation of similarity between two conceptsfrom an ontology for a Human-Like Interaction system. In order to facilitatethis calculation, a similarity function is proposed based on five dimensions(sort, compositional, essential, restrictive and descriptive) constituting thestructure of ontological knowledge. The paper includes a proposal for computinga similarity function for each dimension of knowledge. Later on, the similarityvalues obtained are weighted and aggregated to obtain a global similaritymeasure. In order to calculate those weights associated to each dimension, fourtraining methods have been proposed. The training methods differ in the elementto fit: the user, concepts or pairs of concepts, and a hybrid approach. Forevaluating the proposal, the knowledge base was fed from WordNet and extendedby using a knowledge editing toolkit (Cognos). The evaluation of the proposalis carried out through the comparison of system responses with those given byhuman test subjects, both providing a measure of the soundness of the procedureand revealing ways in which the proposal may be improved.
arxiv-5100-258 | The CQC Algorithm: Cycling in Graphs to Semantically Enrich and Enhance a Bilingual Dictionary | http://arxiv.org/pdf/1402.2561v1.pdf | author:Tiziano Flati, Roberto Navigli category:cs.CL published:2014-01-18 summary:Bilingual machine-readable dictionaries are knowledge resources useful inmany automatic tasks. However, compared to monolingual computational lexiconslike WordNet, bilingual dictionaries typically provide a lower amount ofstructured information, such as lexical and semantic relations, and often donot cover the entire range of possible translations for a word of interest. Inthis paper we present Cycles and Quasi-Cycles (CQC), a novel algorithm for theautomated disambiguation of ambiguous translations in the lexical entries of abilingual machine-readable dictionary. The dictionary is represented as agraph, and cyclic patterns are sought in the graph to assign an appropriatesense tag to each translation in a lexical entry. Further, we use thealgorithms output to improve the quality of the dictionary itself, bysuggesting accurate solutions to structural problems such as misalignments,partial alignments and missing entries. Finally, we successfully apply CQC tothe task of synonym extraction.
arxiv-5100-259 | Combining Evaluation Metrics via the Unanimous Improvement Ratio and its Application to Clustering Tasks | http://arxiv.org/pdf/1401.4590v1.pdf | author:Enrique Amigó, Julio Gonzalo, Javier Artiles, Felisa Verdejo category:cs.AI cs.LG published:2014-01-18 summary:Many Artificial Intelligence tasks cannot be evaluated with a single qualitycriterion and some sort of weighted combination is needed to provide systemrankings. A problem of weighted combination measures is that slight changes inthe relative weights may produce substantial changes in the system rankings.This paper introduces the Unanimous Improvement Ratio (UIR), a measure thatcomplements standard metric combination criteria (such as van Rijsbergen'sF-measure) and indicates how robust the measured differences are to changes inthe relative weights of the individual metrics. UIR is meant to elucidatewhether a perceived difference between two systems is an artifact of howindividual metrics are weighted. Besides discussing the theoretical foundations of UIR, this paper presentsempirical results that confirm the validity and usefulness of the metric forthe Text Clustering problem, where there is a tradeoff between precision andrecall based metrics and results are particularly sensitive to the weightingscheme used to combine them. Remarkably, our experiments show that UIR can beused as a predictor of how well differences between systems measured on a giventest bed will also hold in a different test bed.
arxiv-5100-260 | miRNA and Gene Expression based Cancer Classification using Self- Learning and Co-Training Approaches | http://arxiv.org/pdf/1401.4589v1.pdf | author:Rania Ibrahim, Noha A. Yousri, Mohamed A. Ismail, Nagwa M. El-Makky category:cs.CE cs.LG published:2014-01-18 summary:miRNA and gene expression profiles have been proved useful for classifyingcancer samples. Efficient classifiers have been recently sought and developed.A number of attempts to classify cancer samples using miRNA/gene expressionprofiles are known in literature. However, the use of semi-supervised learningmodels have been used recently in bioinformatics, to exploit the huge corpusesof publicly available sets. Using both labeled and unlabeled sets to trainsample classifiers, have not been previously considered when gene and miRNAexpression sets are used. Moreover, there is a motivation to integrate bothmiRNA and gene expression for a semi-supervised cancer classification as thatprovides more information on the characteristics of cancer samples. In thispaper, two semi-supervised machine learning approaches, namely self-learningand co-training, are adapted to enhance the quality of cancer sampleclassification. These approaches exploit the huge public corpuses to enrich thetraining data. In self-learning, miRNA and gene based classifiers are enhancedindependently. While in co-training, both miRNA and gene expression profilesare used simultaneously to provide different views of cancer samples. To ourknowledge, it is the first attempt to apply these learning approaches to cancerclassification. The approaches were evaluated using breast cancer,hepatocellular carcinoma (HCC) and lung cancer expression sets. Results show upto 20% improvement in F1-measure over Random Forests and SVM classifiers.Co-Training also outperforms Low Density Separation (LDS) approach by around25% improvement in F1-measure in breast cancer.
arxiv-5100-261 | Embedding Graphs under Centrality Constraints for Network Visualization | http://arxiv.org/pdf/1401.4408v1.pdf | author:Brian Baingana, Georgios B. Giannakis category:stat.ML published:2014-01-17 summary:Visual rendering of graphs is a key task in the mapping of complex networkdata. Although most graph drawing algorithms emphasize aesthetic appeal,certain applications such as travel-time maps place more importance onvisualization of structural network properties. The present paper advocates twograph embedding approaches with centrality considerations to comply with nodehierarchy. The problem is formulated first as one of constrainedmulti-dimensional scaling (MDS), and it is solved via block coordinate descentiterations with successive approximations and guaranteed convergence to a KKTpoint. In addition, a regularization term enforcing graph smoothness isincorporated with the goal of reducing edge crossings. A second approachleverages the locally-linear embedding (LLE) algorithm which assumes that thegraph encodes data sampled from a low-dimensional manifold. Closed-formsolutions to the resulting centrality-constrained optimization problems aredetermined yielding meaningful embeddings. Experimental results demonstrate theefficacy of both approaches, especially for visualizing large networks on theorder of thousands of nodes.
arxiv-5100-262 | Learning AMP Chain Graphs and some Marginal Models Thereof under Faithfulness: Extended Version | http://arxiv.org/pdf/1303.0691v3.pdf | author:Jose M. Peña category:stat.ML cs.AI cs.LG published:2013-03-04 summary:This paper deals with chain graphs under the Andersson-Madigan-Perlman (AMP)interpretation. In particular, we present a constraint based algorithm forlearning an AMP chain graph a given probability distribution is faithful to.Moreover, we show that the extension of Meek's conjecture to AMP chain graphsdoes not hold, which compromises the development of efficient and correctscore+search learning algorithms under assumptions weaker than faithfulness. We also introduce a new family of graphical models that consists ofundirected and bidirected edges. We name this new family maximalcovariance-concentration graphs (MCCGs) because it includes both covariance andconcentration graphs as subfamilies. However, every MCCG can be seen as theresult of marginalizing out some nodes in an AMP CG. We describe global, localand pairwise Markov properties for MCCGs and prove their equivalence. Wecharacterize when two MCCGs are Markov equivalent, and show that every Markovequivalence class of MCCGs has a distinguished member. We present a constraintbased algorithm for learning a MCCG a given probability distribution isfaithful to. Finally, we present a graphical criterion for reading dependencies from aMCCG of a probability distribution that satisfies the graphoid properties, weaktransitivity and composition. We prove that the criterion is sound and completein certain sense.
arxiv-5100-263 | A new class of metrics for spike trains | http://arxiv.org/pdf/1209.2918v3.pdf | author:Cătălin V. Rusu, Răzvan V. Florian category:cs.IT cs.NE math.IT q-bio.NC published:2012-09-13 summary:The distance between a pair of spike trains, quantifying the differencesbetween them, can be measured using various metrics. Here we introduce a newclass of spike train metrics, inspired by the Pompeiu-Hausdorff distance, andcompare them with existing metrics. Some of our new metrics (the modulus-metricand the max-metric) have characteristics that are qualitatively different thanthose of classical metrics like the van Rossum distance or the Victor & Purpuradistance. The modulus-metric and the max-metric are particularly suitable formeasuring distances between spike trains where information is encoded inbursts, but the number and the timing of spikes inside a burst does not carryinformation. The modulus-metric does not depend on any parameters and can becomputed using a fast algorithm, in a time that depends linearly on the numberof spikes in the two spike trains. We also introduce localized versions of thenew metrics, which could have the biologically-relevant interpretation ofmeasuring the differences between spike trains as they are perceived at aparticular moment in time by a neuron receiving these spike trains.
arxiv-5100-264 | Multiclass Data Segmentation using Diffuse Interface Methods on Graphs | http://arxiv.org/pdf/1302.3913v2.pdf | author:Cristina Garcia-Cardona, Ekaterina Merkurjev, Andrea L. Bertozzi, Arjuna Flenner, Allon Percus category:stat.ML 62-XX published:2013-02-15 summary:We present two graph-based algorithms for multiclass segmentation ofhigh-dimensional data. The algorithms use a diffuse interface model based onthe Ginzburg-Landau functional, related to total variation compressed sensingand image processing. A multiclass extension is introduced using the Gibbssimplex, with the functional's double-well potential modified to handle themulticlass case. The first algorithm minimizes the functional using a convexsplitting numerical scheme. The second algorithm is a uses a graph adaptationof the classical numerical Merriman-Bence-Osher (MBO) scheme, which alternatesbetween diffusion and thresholding. We demonstrate the performance of bothalgorithms experimentally on synthetic data, grayscale and color images, andseveral benchmark data sets such as MNIST, COIL and WebKB. We also make use offast numerical solvers for finding the eigenvectors and eigenvalues of thegraph Laplacian, and take advantage of the sparsity of the matrix. Experimentsindicate that the results are competitive with or better than the currentstate-of-the-art multiclass segmentation algorithms.
arxiv-5100-265 | Distortion-driven Turbulence Effect Removal using Variational Model | http://arxiv.org/pdf/1401.4221v1.pdf | author:Yuan Xie, Wensheng Zhang, Dacheng Tao, Wenrui Hu, Yanyun Qu, Hanzi Wang category:cs.CV published:2014-01-17 summary:It remains a challenge to simultaneously remove geometric distortion andspace-time-varying blur in frames captured through a turbulent atmosphericmedium. To solve, or at least reduce these effects, we propose a new scheme torecover a latent image from observed frames by integrating a new variationalmodel and distortion-driven spatial-temporal kernel regression. The proposedscheme first constructs a high-quality reference image from the observed framesusing low-rank decomposition. Then, to generate an improved registeredsequence, the reference image is iteratively optimized using a variationalmodel containing a new spatial-temporal regularization. The proposed fastalgorithm efficiently solves this model without the use of partial differentialequations (PDEs). Next, to reduce blur variation, distortion-drivenspatial-temporal kernel regression is carried out to fuse the registeredsequence into one image by introducing the concept of the near-stationarypatch. Applying a blind deconvolution algorithm to the fused image produces thefinal output. Extensive experimental testing shows, both qualitatively andquantitatively, that the proposed method can effectively alleviate distortionand blur and recover details of the original scene compared to state-of-the-artmethods.
arxiv-5100-266 | Entropy analysis of word-length series of natural language texts: Effects of text language and genre | http://arxiv.org/pdf/1401.4205v1.pdf | author:Maria Kalimeri, Vassilios Constantoudis, Constantinos Papadimitriou, Kostantinos Karamanos, Fotis K. Diakonos, Haris Papageorgiou category:cs.CL published:2014-01-17 summary:We estimate the $n$-gram entropies of natural language texts in word-lengthrepresentation and find that these are sensitive to text language and genre. Weattribute this sensitivity to changes in the probability distribution of thelengths of single words and emphasize the crucial role of the uniformity ofprobabilities of having words with length between five and ten. Furthermore,comparison with the entropies of shuffled data reveals the impact of wordlength correlations on the estimated $n$-gram entropies.
arxiv-5100-267 | Convex Optimization for Binary Classifier Aggregation in Multiclass Problems | http://arxiv.org/pdf/1401.4143v1.pdf | author:Sunho Park, TaeHyun Hwang, Seungjin Choi category:cs.LG published:2014-01-16 summary:Multiclass problems are often decomposed into multiple binary problems thatare solved by individual binary classifiers whose results are integrated into afinal answer. Various methods, including all-pairs (APs), one-versus-all (OVA),and error correcting output code (ECOC), have been studied, to decomposemulticlass problems into binary problems. However, little study has been madeto optimally aggregate binary problems to determine a final answer to themulticlass problem. In this paper we present a convex optimization method foran optimal aggregation of binary classifiers to estimate class membershipprobabilities in multiclass problems. We model the class membership probabilityas a softmax function which takes a conic combination of discrepancies inducedby individual binary classifiers, as an input. With this model, we formulatethe regularized maximum likelihood estimation as a convex optimization problem,which is solved by the primal-dual interior point method. Connections of ourmethod to large margin classifiers are presented, showing that the large marginformulation can be considered as a limiting case of our convex formulation.Numerical experiments on synthetic and real-world data sets demonstrate thatour method outperforms existing aggregation methods as well as direct methods,in terms of the classification accuracy and the quality of class membershipprobability estimates.
arxiv-5100-268 | Detection of Anomalous Crowd Behavior Using Spatio-Temporal Multiresolution Model and Kronecker Sum Decompositions | http://arxiv.org/pdf/1401.3291v2.pdf | author:Kristjan Greenewald, Alfred Hero category:stat.ML published:2014-01-14 summary:In this work we consider the problem of detecting anomalous spatio-temporalbehavior in videos. Our approach is to learn the normative multiframe pixeljoint distribution and detect deviations from it using a likelihood basedapproach. Due to the extreme lack of available training samples relative to thedimension of the distribution, we use a mean and covariance approach andconsider methods of learning the spatio-temporal covariance in the low-sampleregime. Our approach is to estimate the covariance using parameter reductionand sparse models. The first method considered is the representation of thecovariance as a sum of Kronecker products as in (Greenewald et al 2013), whichis found to be an accurate approximation in this setting. We propose learningalgorithms relevant to our problem. We then consider the sparse multiresolutionmodel of (Choi et al 2010) and apply the Kronecker product methods to it forfurther parameter reduction, as well as introducing modifications for enhancedefficiency and greater applicability to spatio-temporal covariance matrices. Weapply our methods to the detection of crowd behavior anomalies in theUniversity of Minnesota crowd anomaly dataset, and achieve competitive results.
arxiv-5100-269 | Towards the selection of patients requiring ICD implantation by automatic classification from Holter monitoring indices | http://arxiv.org/pdf/1401.4128v1.pdf | author:Charles-Henri Cappelaere, R. Dubois, P. Roussel, G. Dreyfus category:cs.LG stat.AP published:2014-01-16 summary:The purpose of this study is to optimize the selection of prophylacticcardioverter defibrillator implantation candidates. Currently, the maincriterion for implantation is a low Left Ventricular Ejection Fraction (LVEF)whose specificity is relatively poor. We designed two classifiers aimed topredict, from long term ECG recordings (Holter), whether a low-LVEF patient islikely or not to undergo ventricular arrhythmia in the next six months. Oneclassifier is a single hidden layer neural network whose variables are the mostrelevant features extracted from Holter recordings, and the other classifierhas a structure that capitalizes on the physiological decomposition of thearrhythmogenic factors into three disjoint groups: the myocardial substrate,the triggers and the autonomic nervous system (ANS). In this ad hoc network,the features were assigned to each group; one neural network classifier pergroup was designed and its complexity was optimized. The outputs of theclassifiers were fed to a single neuron that provided the required probabilityestimate. The latter was thresholded for final discrimination A datasetcomposed of 186 pre-implantation 30-mn Holter recordings of patients equippedwith an implantable cardioverter defibrillator (ICD) in primary prevention wasused in order to design and test this classifier. 44 out of 186 patientsunderwent at least one treated ventricular arrhythmia during the six-monthfollow-up period. Performances of the designed classifier were evaluated usinga cross-test strategy that consists in splitting the database into severalcombinations of a training set and a test set. The average arrhythmiaprediction performances of the ad-hoc classifier are NPV = 77% $\pm$ 13% andPPV = 31% $\pm$ 19% (Negative Predictive Value $\pm$ std, Positive PredictiveValue $\pm$ std). According to our study, improving prophylacticICD-implantation candidate selection by automatic classification from ECGfeatures may be possible, but the availability of a sizable dataset appears tobe essential to decrease the number of False Negatives.
arxiv-5100-270 | Revisiting loss-specific training of filter-based MRFs for image restoration | http://arxiv.org/pdf/1401.4107v1.pdf | author:Yunjin Chen, Thomas Pock, René Ranftl, Horst Bischof category:cs.CV published:2014-01-16 summary:It is now well known that Markov random fields (MRFs) are particularlyeffective for modeling image priors in low-level vision. Recent years have seenthe emergence of two main approaches for learning the parameters in MRFs: (1)probabilistic learning using sampling-based algorithms and (2) loss-specifictraining based on MAP estimate. After investigating existing trainingapproaches, it turns out that the performance of the loss-specific training hasbeen significantly underestimated in existing work. In this paper, we revisitthis approach and use techniques from bi-level optimization to solve it. Weshow that we can get a substantial gain in the final performance by solving thelower-level problem in the bi-level framework with high accuracy using ournewly proposed algorithm. As a result, our trained model is on par with highlyspecialized image denoising algorithms and clearly outperformsprobabilistically trained MRF models. Our findings suggest that for theloss-specific training scheme, solving the lower-level problem with higheraccuracy is beneficial. Our trained model comes along with the additionaladvantage, that inference is extremely efficient. Our GPU-based implementationtakes less than 1s to produce state-of-the-art performance.
arxiv-5100-271 | Learning $\ell_1$-based analysis and synthesis sparsity priors using bi-level optimization | http://arxiv.org/pdf/1401.4105v1.pdf | author:Yunjin Chen, Thomas Pock, Horst Bischof category:cs.CV published:2014-01-16 summary:We consider the analysis operator and synthesis dictionary learning problemsbased on the the $\ell_1$ regularized sparse representation model. We revealthe internal relations between the $\ell_1$-based analysis model and synthesismodel. We then introduce an approach to learn both analysis operator andsynthesis dictionary simultaneously by using a unified framework of bi-leveloptimization. Our aim is to learn a meaningful operator (dictionary) such thatthe minimum energy solution of the analysis (synthesis)-prior based model is asclose as possible to the ground-truth. We solve the bi-level optimizationproblem using the implicit differentiation technique. Moreover, we demonstratethe effectiveness of our leaning approach by applying the learned analysisoperator (dictionary) to the image denoising task and comparing its performancewith state-of-the-art methods. Under this unified framework, we can compare theperformance of the two types of priors.
arxiv-5100-272 | Reweighted message passing revisited | http://arxiv.org/pdf/1309.5655v2.pdf | author:Vladimir Kolmogorov category:cs.AI cs.CV cs.LG published:2013-09-22 summary:We propose a new family of message passing techniques for MAP estimation ingraphical models which we call {\em Sequential Reweighted Message Passing}(SRMP). Special cases include well-known techniques such as {\em Min-SumDiffusion} (MSD) and a faster {\em Sequential Tree-Reweighted Message Passing}(TRW-S). Importantly, our derivation is simpler than the original derivation ofTRW-S, and does not involve a decomposition into trees. This allows easygeneralizations. We present such a generalization for the case of higher-ordergraphical models, and test it on several real-world problems with promisingresults.
arxiv-5100-273 | Co-clustering separately exchangeable network data | http://arxiv.org/pdf/1212.4093v5.pdf | author:David Choi, Patrick J. Wolfe category:math.ST cs.SI math.CO stat.ML stat.TH published:2012-12-17 summary:This article establishes the performance of stochastic blockmodels inaddressing the co-clustering problem of partitioning a binary array intosubsets, assuming only that the data are generated by a nonparametric processsatisfying the condition of separate exchangeability. We provide oracleinequalities with rate of convergence $\mathcal{O}_P(n^{-1/4})$ correspondingto profile likelihood maximization and mean-square error minimization, and showthat the blockmodel can be interpreted in this setting as an optimalpiecewise-constant approximation to the generative nonparametric model. We alsoshow for large sample sizes that the detection of co-clusters in such dataindicates with high probability the existence of co-clusters of equal size andasymptotically equivalent connectivity in the underlying generative process.
arxiv-5100-274 | An Empirical Evaluation of Similarity Measures for Time Series Classification | http://arxiv.org/pdf/1401.3973v1.pdf | author:Joan Serrà, Josep Lluis Arcos category:cs.LG cs.CV stat.ML published:2014-01-16 summary:Time series are ubiquitous, and a measure to assess their similarity is acore part of many computational systems. In particular, the similarity measureis the most essential ingredient of time series clustering and classificationsystems. Because of this importance, countless approaches to estimate timeseries similarity have been proposed. However, there is a lack of comparativestudies using empirical, rigorous, quantitative, and large-scale assessmentstrategies. In this article, we provide an extensive evaluation of similaritymeasures for time series classification following the aforementionedprinciples. We consider 7 different measures coming from alternative measure`families', and 45 publicly-available time series data sets coming from a widevariety of scientific domains. We focus on out-of-sample classificationaccuracy, but in-sample accuracies and parameter choices are also discussed.Our work is based on rigorous evaluation methodologies and includes the use ofpowerful statistical significance tests to derive meaningful conclusions. Theobtained results show the equivalence, in terms of accuracy, of a number ofmeasures, but with one single candidate outperforming the rest. Such findings,together with the followed methodology, invite researchers on the field toadopt a more consistent evaluation criteria and a more informed decisionregarding the baseline measures to which new developments should be compared.
arxiv-5100-275 | Nonparametric Latent Tree Graphical Models: Inference, Estimation, and Structure Learning | http://arxiv.org/pdf/1401.3940v1.pdf | author:Le Song, Han Liu, Ankur Parikh, Eric Xing category:stat.ML published:2014-01-16 summary:Tree structured graphical models are powerful at expressing long range orhierarchical dependency among many variables, and have been widely applied indifferent areas of computer science and statistics. However, existing methodsfor parameter estimation, inference, and structure learning mainly rely on theGaussian or discrete assumptions, which are restrictive under manyapplications. In this paper, we propose new nonparametric methods based onreproducing kernel Hilbert space embeddings of distributions that can recoverthe latent tree structures, estimate the parameters, and perform inference forhigh dimensional continuous and non-Gaussian variables. The usefulness of theproposed methods are illustrated by thorough numerical results.
arxiv-5100-276 | Frequency Recognition in SSVEP-based BCI using Multiset Canonical Correlation Analysis | http://arxiv.org/pdf/1308.5609v2.pdf | author:Yu Zhang, Guoxu Zhou, Jing Jin, Xingyu Wang, Andrzej Cichocki category:stat.ML published:2013-08-26 summary:Canonical correlation analysis (CCA) has been one of the most popular methodsfor frequency recognition in steady-state visual evoked potential (SSVEP)-basedbrain-computer interfaces (BCIs). Despite its efficiency, a potential problemis that using pre-constructed sine-cosine waves as the required referencesignals in the CCA method often does not result in the optimal recognitionaccuracy due to their lack of features from the real EEG data. To address thisproblem, this study proposes a novel method based on multiset canonicalcorrelation analysis (MsetCCA) to optimize the reference signals used in theCCA method for SSVEP frequency recognition. The MsetCCA method learns multiplelinear transforms that implement joint spatial filtering to maximize theoverall correlation among canonical variates, and hence extracts SSVEP commonfeatures from multiple sets of EEG data recorded at the same stimulusfrequency. The optimized reference signals are formed by combination of thecommon features and completely based on training data. Experimental study withEEG data from ten healthy subjects demonstrates that the MsetCCA methodimproves the recognition accuracy of SSVEP frequency in comparison with the CCAmethod and other two competing methods (multiway CCA (MwayCCA) and phaseconstrained CCA (PCCA)), especially for a small number of channels and a shorttime window length. The superiority indicates that the proposed MsetCCA methodis a new promising candidate for frequency recognition in SSVEP-based BCIs.
arxiv-5100-277 | Centrality-as-Relevance: Support Sets and Similarity as Geometric Proximity | http://arxiv.org/pdf/1401.3908v1.pdf | author:Ricardo Ribeiro, David Martins de Matos category:cs.IR cs.CL published:2014-01-16 summary:In automatic summarization, centrality-as-relevance means that the mostimportant content of an information source, or a collection of informationsources, corresponds to the most central passages, considering a representationwhere such notion makes sense (graph, spatial, etc.). We assess the mainparadigms, and introduce a new centrality-based relevance model for automaticsummarization that relies on the use of support sets to better estimate therelevant content. Geometric proximity is used to compute semantic relatedness.Centrality (relevance) is determined by considering the whole input source (andnot only local information), and by taking into account the existence of minortopics or lateral subjects in the information sources to be summarized. Themethod consists in creating, for each passage of the input source, a supportset consisting only of the most semantically related passages. Then, thedetermination of the most relevant content is achieved by selecting thepassages that occur in the largest number of support sets. This model producesextractive summaries that are generic, and language- and domain-independent.Thorough automatic evaluation shows that the method achieves state-of-the-artperformance, both in written text, and automatically transcribed speechsummarization, including when compared to considerably more complex approaches.
arxiv-5100-278 | Policy Invariance under Reward Transformations for General-Sum Stochastic Games | http://arxiv.org/pdf/1401.3907v1.pdf | author:Xiaosong Lu, Howard M. Schwartz, Sidney N. Givigi Jr category:cs.GT cs.LG published:2014-01-16 summary:We extend the potential-based shaping method from Markov decision processesto multi-player general-sum stochastic games. We prove that the Nash equilibriain a stochastic game remains unchanged after potential-based shaping is appliedto the environment. The property of policy invariance provides a possible wayof speeding convergence when learning to play a stochastic game.
arxiv-5100-279 | Controlling Complexity in Part-of-Speech Induction | http://arxiv.org/pdf/1401.6131v1.pdf | author:João V. Graça, Kuzman Ganchev, Luisa Coheur, Fernando Pereira, Ben Taskar category:cs.CL cs.LG published:2014-01-16 summary:We consider the problem of fully unsupervised learning of grammatical(part-of-speech) categories from unlabeled text. The standardmaximum-likelihood hidden Markov model for this task performs poorly, becauseof its weak inductive bias and large model capacity. We address this problem byrefining the model and modifying the learning objective to control its capacityvia para- metric and non-parametric constraints. Our approach enforcesword-category association sparsity, adds morphological and orthographicfeatures, and eliminates hard-to-estimate parameters for rare words. We developan efficient learning algorithm that is not much more computationally intensivethan standard training. We also provide an open-source implementation of thealgorithm. Our experiments on five diverse languages (Bulgarian, Danish,English, Portuguese, Spanish) achieve significant improvements compared withprevious methods for the same task.
arxiv-5100-280 | Efficient Multi-Start Strategies for Local Search Algorithms | http://arxiv.org/pdf/1401.3894v1.pdf | author:András György, Levente Kocsis category:cs.LG cs.AI stat.ML published:2014-01-16 summary:Local search algorithms applied to optimization problems often suffer fromgetting trapped in a local optimum. The common solution for this deficiency isto restart the algorithm when no progress is observed. Alternatively, one canstart multiple instances of a local search algorithm, and allocatecomputational resources (in particular, processing time) to the instancesdepending on their behavior. Hence, a multi-start strategy has to decide(dynamically) when to allocate additional resources to a particular instanceand when to start new instances. In this paper we propose multi-startstrategies motivated by works on multi-armed bandit problems and Lipschitzoptimization with an unknown constant. The strategies continuously estimate thepotential performance of each algorithm instance by supposing a convergencerate of the local search algorithm up to an unknown constant, and in everyphase allocate resources to those instances that could converge to the optimumfor a particular range of the constant. Asymptotic bounds are given on theperformance of the strategies. In particular, we prove that at most a quadraticincrease in the number of times the target function is evaluated is needed toachieve the performance of a local search algorithm started from the attractionregion of the optimum. Experiments are provided using SPSA (SimultaneousPerturbation Stochastic Approximation) and k-means as local search algorithms,and the results indicate that the proposed strategies work well in practice,and, in all cases studied, need only logarithmically more evaluations of thetarget function as opposed to the theoretically suggested quadratic increase.
arxiv-5100-281 | Regression Conformal Prediction with Nearest Neighbours | http://arxiv.org/pdf/1401.3880v1.pdf | author:Harris Papadopoulos, Vladimir Vovk, Alex Gammerman category:cs.LG published:2014-01-16 summary:In this paper we apply Conformal Prediction (CP) to the k-Nearest NeighboursRegression (k-NNR) algorithm and propose ways of extending the typicalnonconformity measure used for regression so far. Unlike traditional regressionmethods which produce point predictions, Conformal Predictors output predictiveregions that satisfy a given confidence level. The regions produced by anyConformal Predictor are automatically valid, however their tightness andtherefore usefulness depends on the nonconformity measure used by each CP. Ineffect a nonconformity measure evaluates how strange a given example iscompared to a set of other examples based on some traditional machine learningalgorithm. We define six novel nonconformity measures based on the k-NearestNeighbours Regression algorithm and develop the corresponding CPs followingboth the original (transductive) and the inductive CP approaches. A comparisonof the predictive regions produced by our measures with those of the typicalregression measure suggests that a major improvement in terms of predictiveregion tightness is achieved by the new measures.
arxiv-5100-282 | Properties of Bethe Free Energies and Message Passing in Gaussian Models | http://arxiv.org/pdf/1401.3877v1.pdf | author:Botond Cseke, Tom Heskes category:cs.LG cs.AI stat.ML published:2014-01-16 summary:We address the problem of computing approximate marginals in Gaussianprobabilistic models by using mean field and fractional Bethe approximations.We define the Gaussian fractional Bethe free energy in terms of the momentparameters of the approximate marginals, derive a lower and an upper bound onthe fractional Bethe free energy and establish a necessary condition for thelower bound to be bounded from below. It turns out that the condition isidentical to the pairwise normalizability condition, which is known to be asufficient condition for the convergence of the message passing algorithm. Weshow that stable fixed points of the Gaussian message passing algorithm arelocal minima of the Gaussian Bethe free energy. By a counterexample, wedisprove the conjecture stating that the unboundedness of the free energyimplies the divergence of the message passing algorithm.
arxiv-5100-283 | Non-Deterministic Policies in Markovian Decision Processes | http://arxiv.org/pdf/1401.3871v1.pdf | author:Mahdi Milani Fard, Joelle Pineau category:cs.AI cs.LG published:2014-01-16 summary:Markovian processes have long been used to model stochastic environments.Reinforcement learning has emerged as a framework to solve sequential planningand decision-making problems in such environments. In recent years, attemptswere made to apply methods from reinforcement learning to construct decisionsupport systems for action selection in Markovian environments. Althoughconventional methods in reinforcement learning have proved to be useful inproblems concerning sequential decision-making, they cannot be applied in theircurrent form to decision support systems, such as those in medical domains, asthey suggest policies that are often highly prescriptive and leave little roomfor the users input. Without the ability to provide flexible guidelines, it isunlikely that these methods can gain ground with users of such systems. Thispaper introduces the new concept of non-deterministic policies to allow moreflexibility in the users decision-making process, while constraining decisionsto remain near optimal solutions. We provide two algorithms to computenon-deterministic policies in discrete domains. We study the output and runningtime of these method on a set of synthetic and real-world problems. In anexperiment with human subjects, we show that humans assisted by hints based onnon-deterministic policies outperform both human-only and computer-only agentsin a web navigation task.
arxiv-5100-284 | Learning to Make Predictions In Partially Observable Environments Without a Generative Model | http://arxiv.org/pdf/1401.3870v1.pdf | author:Erik Talvitie, Satinder Singh category:cs.LG cs.AI stat.ML published:2014-01-16 summary:When faced with the problem of learning a model of a high-dimensionalenvironment, a common approach is to limit the model to make only a restrictedset of predictions, thereby simplifying the learning problem. These partialmodels may be directly useful for making decisions or may be combined togetherto form a more complete, structured model. However, in partially observable(non-Markov) environments, standard model-learning methods learn generativemodels, i.e. models that provide a probability distribution over all possiblefutures (such as POMDPs). It is not straightforward to restrict such models tomake only certain predictions, and doing so does not always simplify thelearning problem. In this paper we present prediction profile models:non-generative partial models for partially observable systems that make only agiven set of predictions, and are therefore far simpler than generative modelsin some cases. We formalize the problem of learning a prediction profile modelas a transformation of the original model-learning problem, and showempirically that one can learn prediction profile models that make a small setof important predictions even in systems that are too complex for standardgenerative models.
arxiv-5100-285 | Narrowing the Modeling Gap: A Cluster-Ranking Approach to Coreference Resolution | http://arxiv.org/pdf/1405.5202v1.pdf | author:Altaf Rahman, Vincent Ng category:cs.CL published:2014-01-16 summary:Traditional learning-based coreference resolvers operate by training themention-pair model for determining whether two mentions are coreferent or not.Though conceptually simple and easy to understand, the mention-pair model islinguistically rather unappealing and lags far behind the heuristic-basedcoreference models proposed in the pre-statistical NLP era in terms ofsophistication. Two independent lines of recent research have attempted toimprove the mention-pair model, one by acquiring the mention-ranking model torank preceding mentions for a given anaphor, and the other by training theentity-mention model to determine whether a preceding cluster is coreferentwith a given mention. We propose a cluster-ranking approach to coreferenceresolution, which combines the strengths of the mention-ranking model and theentity-mention model, and is therefore theoretically more appealing than bothof these models. In addition, we seek to improve cluster rankers via twoextensions: (1) lexicalization and (2) incorporating knowledge of anaphoricityby jointly modeling anaphoricity determination and coreference resolution.Experimental results on the ACE data sets demonstrate the superior performanceof cluster rankers to competing approaches as well as the effectiveness of ourtwo extensions.
arxiv-5100-286 | Evaluating Temporal Graphs Built from Texts via Transitive Reduction | http://arxiv.org/pdf/1401.3865v1.pdf | author:Xavier Tannier, Philippe Muller category:cs.CL cs.IR published:2014-01-16 summary:Temporal information has been the focus of recent attention in informationextraction, leading to some standardization effort, in particular for the taskof relating events in a text. This task raises the problem of comparing twoannotations of a given text, because relations between events in a story areintrinsically interdependent and cannot be evaluated separately. A properevaluation measure is also crucial in the context of a machine learningapproach to the problem. Finding a common comparison referent at the text levelis not obvious, and we argue here in favor of a shift from event-based measuresto measures on a unique textual object, a minimal underlying temporal graph, ormore formally the transitive reduction of the graph of relations between eventboundaries. We support it by an investigation of its properties on syntheticdata and on a well-know temporal corpus.
arxiv-5100-287 | Kalman Temporal Differences | http://arxiv.org/pdf/1406.3270v1.pdf | author:Matthieu Geist, Olivier Pietquin category:cs.LG published:2014-01-16 summary:Because reinforcement learning suffers from a lack of scalability, onlinevalue (and Q-) function approximation has received increasing interest thislast decade. This contribution introduces a novel approximation scheme, namelythe Kalman Temporal Differences (KTD) framework, that exhibits the followingfeatures: sample-efficiency, non-linear approximation, non-stationarityhandling and uncertainty management. A first KTD-based algorithm is providedfor deterministic Markov Decision Processes (MDP) which produces biasedestimates in the case of stochastic transitions. Than the eXtended KTDframework (XKTD), solving stochastic MDP, is described. Convergence is analyzedfor special cases for both deterministic and stochastic transitions. Relatedalgorithms are experimented on classical benchmarks. They compare favorably tothe state of the art while exhibiting the announced features.
arxiv-5100-288 | Which Clustering Do You Want? Inducing Your Ideal Clustering with Minimal Feedback | http://arxiv.org/pdf/1401.5389v1.pdf | author:Sajib Dasgupta, Vincent Ng category:cs.IR cs.CL cs.LG published:2014-01-16 summary:While traditional research on text clustering has largely focused on groupingdocuments by topic, it is conceivable that a user may want to cluster documentsalong other dimensions, such as the authors mood, gender, age, or sentiment.Without knowing the users intention, a clustering algorithm will only groupdocuments along the most prominent dimension, which may not be the one the userdesires. To address the problem of clustering documents along the user-desireddimension, previous work has focused on learning a similarity metric from datamanually annotated with the users intention or having a human construct afeature space in an interactive manner during the clustering process. With thegoal of reducing reliance on human knowledge for fine-tuning the similarityfunction or selecting the relevant features required by these approaches, wepropose a novel active clustering algorithm, which allows a user to easilyselect the dimension along which she wants to cluster the documents byinspecting only a small number of words. We demonstrate the viability of ouralgorithm on a variety of commonly-used sentiment datasets.
arxiv-5100-289 | Cause Identification from Aviation Safety Incident Reports via Weakly Supervised Semantic Lexicon Construction | http://arxiv.org/pdf/1401.4436v1.pdf | author:Muhammad Arshad Ul Abedin, Vincent Ng, Latifur Khan category:cs.CL cs.LG published:2014-01-16 summary:The Aviation Safety Reporting System collects voluntarily submitted reportson aviation safety incidents to facilitate research work aiming to reduce suchincidents. To effectively reduce these incidents, it is vital to accuratelyidentify why these incidents occurred. More precisely, given a set of possiblecauses, or shaping factors, this task of cause identification involvesidentifying all and only those shaping factors that are responsible for theincidents described in a report. We investigate two approaches to causeidentification. Both approaches exploit information provided by a semanticlexicon, which is automatically constructed via Thelen and Riloffs Basiliskframework augmented with our linguistic and algorithmic modifications. Thefirst approach labels a report using a simple heuristic, which looks for thewords and phrases acquired during the semantic lexicon learning process in thereport. The second approach recasts cause identification as a textclassification problem, employing supervised and transductive textclassification algorithms to learn models from incident reports labeled withshaping factors and using the models to label unseen reports. Our experimentsshow that both the heuristic-based approach and the learning-based approach(when given sufficient training data) outperform the baseline systemsignificantly.
arxiv-5100-290 | Using Local Alignments for Relation Recognition | http://arxiv.org/pdf/1405.7713v1.pdf | author:Sophia Katrenko, Pieter Adriaans, Maarten van Someren category:cs.CL cs.IR cs.LG published:2014-01-16 summary:This paper discusses the problem of marrying structural similarity withsemantic relatedness for Information Extraction from text. Aiming at accuraterecognition of relations, we introduce local alignment kernels and explorevarious possibilities of using them for this task. We give a definition of alocal alignment (LA) kernel based on the Smith-Waterman score as a sequencesimilarity measure and proceed with a range of possibilities for computingsimilarity between elements of sequences. We show how distributional similaritymeasures obtained from unlabeled data can be incorporated into the learningtask as semantic knowledge. Our experiments suggest that the LA kernel yieldspromising results on various biomedical corpora outperforming two baselines bya large margin. Additional series of experiments have been conducted on thedata sets of seven general relation types, where the performance of the LAkernel is comparable to the current state-of-the-art results.
arxiv-5100-291 | An Active Learning Approach for Jointly Estimating Worker Performance and Annotation Reliability with Crowdsourced Data | http://arxiv.org/pdf/1401.3836v1.pdf | author:Liyue Zhao, Yu Zhang, Gita Sukthankar category:cs.LG cs.HC published:2014-01-16 summary:Crowdsourcing platforms offer a practical solution to the problem ofaffordably annotating large datasets for training supervised classifiers.Unfortunately, poor worker performance frequently threatens to compromiseannotation reliability, and requesting multiple labels for every instance canlead to large cost increases without guaranteeing good results. Minimizing therequired training samples using an active learning selection procedure reducesthe labeling requirement but can jeopardize classifier training by focusing onerroneous annotations. This paper presents an active learning approach in whichworker performance, task difficulty, and annotation reliability are jointlyestimated and used to compute the risk function guiding the sample selectionprocedure. We demonstrate that the proposed approach, which employs activelearning with Bayesian networks, significantly improves training accuracy andcorrectly ranks the expertise of unknown labelers in the presence of annotationnoise.
arxiv-5100-292 | Constructing Reference Sets from Unstructured, Ungrammatical Text | http://arxiv.org/pdf/1401.3832v1.pdf | author:Matthew Michelson, Craig A. Knoblock category:cs.CL cs.IR published:2014-01-16 summary:Vast amounts of text on the Web are unstructured and ungrammatical, such asclassified ads, auction listings, forum postings, etc. We call such text"posts." Despite their inconsistent structure and lack of grammar, posts arefull of useful information. This paper presents work on semi-automaticallybuilding tables of relational information, called "reference sets," byanalyzing such posts directly. Reference sets can be applied to a number oftasks such as ontology maintenance and information extraction. Ourreference-set construction method starts with just a small amount of backgroundknowledge, and constructs tuples representing the entities in the posts to forma reference set. We also describe an extension to this approach for the specialcase where even this small amount of background knowledge is impossible todiscover and use. To evaluate the utility of the machine-constructed referencesets, we compare them to manually constructed reference sets in the context ofreference-set-based information extraction. Our results show the reference setsconstructed by our method outperform manually constructed reference sets. Wealso compare the reference-set-based extraction approach using themachine-constructed reference set to supervised extraction approaches usinggeneric features. These results demonstrate that using machine-constructedreference sets outperforms the supervised methods, even though the supervisedmethods require training data.
arxiv-5100-293 | Context-based Word Acquisition for Situated Dialogue in a Virtual World | http://arxiv.org/pdf/1401.6875v1.pdf | author:Shaolin Qu, Joyce Y. Chai category:cs.CL published:2014-01-16 summary:To tackle the vocabulary problem in conversational systems, previous work hasapplied unsupervised learning approaches on co-occurring speech and eye gazeduring interaction to automatically acquire new words. Although theseapproaches have shown promise, several issues related to human languagebehavior and human-machine conversation have not been addressed. First,psycholinguistic studies have shown certain temporal regularities between humaneye movement and language production. While these regularities can potentiallyguide the acquisition process, they have not been incorporated in the previousunsupervised approaches. Second, conversational systems generally have anexisting knowledge base about the domain and vocabulary. While the existingknowledge can potentially help bootstrap and constrain the acquired new words,it has not been incorporated in the previous models. Third, eye gaze couldserve different functions in human-machine conversation. Some gaze streams maynot be closely coupled with speech stream, and thus are potentially detrimentalto word acquisition. Automated recognition of closely-coupled speech-gazestreams based on conversation context is important. To address these issues, wedeveloped new approaches that incorporate user language behavior, domainknowledge, and conversation context in word acquisition. We evaluated theseapproaches in the context of situated dialogue in a virtual world. Ourexperimental results have shown that incorporating the above three types ofcontextual information significantly improves word acquisition performance.
arxiv-5100-294 | RoxyBot-06: Stochastic Prediction and Optimization in TAC Travel | http://arxiv.org/pdf/1401.3829v1.pdf | author:Amy Greenwald, Seong Jae Lee, Victor Naroditskiy category:cs.GT cs.LG published:2014-01-16 summary:In this paper, we describe our autonomous bidding agent, RoxyBot, who emergedvictorious in the travel division of the 2006 Trading Agent Competition in aphoto finish. At a high level, the design of many successful trading agents canbe summarized as follows: (i) price prediction: build a model of market prices;and (ii) optimization: solve for an approximately optimal set of bids, giventhis model. To predict, RoxyBot builds a stochastic model of market prices bysimulating simultaneous ascending auctions. To optimize, RoxyBot relies on thesample average approximation method, a stochastic optimization technique.
arxiv-5100-295 | Training a Multilingual Sportscaster: Using Perceptual Context to Learn Language | http://arxiv.org/pdf/1405.7711v1.pdf | author:David L. Chen, Joohyun Kim, Raymond J. Mooney category:cs.CL published:2014-01-16 summary:We present a novel framework for learning to interpret and generate languageusing only perceptual context as supervision. We demonstrate its capabilitiesby developing a system that learns to sportscast simulated robot soccer gamesin both English and Korean without any language-specific prior knowledge.Training employs only ambiguous supervision consisting of a stream ofdescriptive textual comments and a sequence of events extracted from thesimulation trace. The system simultaneously establishes correspondences betweenindividual comments and the events that they describe while building atranslation model that supports both parsing and generation. We also present anovel algorithm for learning which events are worth describing. Humanevaluations of the generated commentaries indicate they are of reasonablequality and in some cases even on par with those produced by humans for ourlimited domain.
arxiv-5100-296 | Generation, Implementation and Appraisal of an N-gram based Stemming Algorithm | http://arxiv.org/pdf/1312.4824v2.pdf | author:B. P. Pande, Pawan Tamta, H. S. Dhami category:cs.IR cs.CL published:2013-12-17 summary:A language independent stemmer has always been looked for. Single N-gramtokenization technique works well, however, it often generates stems that startwith intermediate characters, rather than initial ones. We present a noveltechnique that takes the concept of N gram stemming one step ahead and compareour method with an established algorithm in the field, Porter's Stemmer.Results indicate that our N gram stemmer is not inferior to Porter's linguisticstemmer.
arxiv-5100-297 | On the Estimation of Pointwise Dimension | http://arxiv.org/pdf/1312.2298v3.pdf | author:Shohei Hidaka, Neeraj Kashyap category:math.DS nlin.CD stat.ML published:2013-12-09 summary:Our goal in this paper is to develop an effective estimator of fractaldimension. We survey existing ideas in dimension estimation, with a focus onthe currently popular method of Grassberger and Procaccia for the estimation ofcorrelation dimension. There are two major difficulties in estimation based onthis method. The first is the insensitivity of correlation dimension itself todifferences in dimensionality over data, which we term "dimension blindness".The second comes from the reliance of the method on the inference of limitingbehavior from finite data. We propose pointwise dimension as an object for estimation in response to thedimension blindness of correlation dimension. Pointwise dimension is a localquantity, and the distribution of pointwise dimensions over the data containsthe information to which correlation dimension is blind. We use a "limit-free"description of pointwise dimension to develop a new estimator. We conclude bydiscussing potential applications of our estimator as well as some challengesit raises.
arxiv-5100-298 | Structured Priors for Sparse-Representation-Based Hyperspectral Image Classification | http://arxiv.org/pdf/1401.3818v1.pdf | author:Xiaoxia Sun, Qing Qu, Nasser M. Nasrabadi, Trac D. Tran category:cs.CV cs.LG stat.ML published:2014-01-16 summary:Pixel-wise classification, where each pixel is assigned to a predefinedclass, is one of the most important procedures in hyperspectral image (HSI)analysis. By representing a test pixel as a linear combination of a smallsubset of labeled pixels, a sparse representation classifier (SRC) gives ratherplausible results compared with that of traditional classifiers such as thesupport vector machine (SVM). Recently, by incorporating additional structuredsparsity priors, the second generation SRCs have appeared in the literature andare reported to further improve the performance of HSI. These priors are basedon exploiting the spatial dependencies between the neighboring pixels, theinherent structure of the dictionary, or both. In this paper, we review andcompare several structured priors for sparse-representation-based HSIclassification. We also propose a new structured prior called the low rankgroup prior, which can be considered as a modification of the low rank prior.Furthermore, we will investigate how different structured priors improve theresult for the HSI classification.
arxiv-5100-299 | Seeded Graph Matching Via Joint Optimization of Fidelity and Commensurability | http://arxiv.org/pdf/1401.3813v1.pdf | author:Vince Lyzinski, Sancar Adali, Joshua T. Vogelstein, Youngser Park, Carey E. Priebe category:stat.ML stat.AP stat.ME published:2014-01-16 summary:We present a novel approximate graph matching algorithm that incorporatesseeded data into the graph matching paradigm. Our Joint Optimization ofFidelity and Commensurability (JOFC) algorithm embeds two graphs into a commonEuclidean space where the matching inference task can be performed. Throughreal and simulated data examples, we demonstrate the versatility of ouralgorithm in matching graphs with various characteristics--weightedness,directedness, loopiness, many-to-one and many-to-many matchings, and softseedings.
arxiv-5100-300 | Coordinate Descent with Online Adaptation of Coordinate Frequencies | http://arxiv.org/pdf/1401.3737v1.pdf | author:Tobias Glasmachers, Ürün Dogan category:stat.ML cs.LG published:2014-01-15 summary:Coordinate descent (CD) algorithms have become the method of choice forsolving a number of optimization problems in machine learning. They areparticularly popular for training linear models, including linear supportvector machine classification, LASSO regression, and logistic regression. We consider general CD with non-uniform selection of coordinates. Instead offixing selection frequencies beforehand we propose an online adaptationmechanism for this important parameter, called the adaptive coordinatefrequencies (ACF) method. This mechanism removes the need to estimate optimalcoordinate frequencies beforehand, and it automatically reacts to changingrequirements during an optimization run. We demonstrate the usefulness of our ACF-CD approach for a variety ofoptimization problems arising in machine learning contexts. Our algorithmoffers significant speed-ups over state-of-the-art training methods.
