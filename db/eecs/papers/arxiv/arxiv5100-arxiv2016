arxiv-5100-1 | Efficient coordinate-descent for orthogonal matrices through Givens rotations | http://arxiv.org/pdf/1312.0624v2.pdf | author:Uri Shalit, Gal Chechik category:cs.LG stat.ML published:2013-12-02 summary:Optimizing over the set of orthogonal matrices is a central component inproblems like sparse-PCA or tensor decomposition. Unfortunately, suchoptimization is hard since simple operations on orthogonal matrices easilybreak orthogonality, and correcting orthogonality usually costs a large amountof computation. Here we propose a framework for optimizing orthogonal matrices,that is the parallel of coordinate-descent in Euclidean spaces. It is based on{\em Givens-rotations}, a fast-to-compute operation that affects a small numberof entries in the learned matrix, and preserves orthogonality. We show twoapplications of this approach: an algorithm for tensor decomposition that isused in learning mixture models, and an algorithm for sparse-PCA. We study theparameter regime where a Givens rotation approach converges faster and achievesa superior model on a genome-wide brain-wide mRNA expression dataset.
arxiv-5100-2 | Consistency of weighted majority votes | http://arxiv.org/pdf/1312.0451v5.pdf | author:Daniel Berend, Aryeh Kontorovich category:math.PR cs.LG stat.ML 60C05, 60F15 published:2013-12-02 summary:We revisit the classical decision-theoretic problem of weighted expert votingfrom a statistical learning perspective. In particular, we examine theconsistency (both asymptotic and finitary) of the optimal Nitzan-Paroushweighted majority and related rules. In the case of known expert competencelevels, we give sharp error estimates for the optimal rule. When the competencelevels are unknown, they must be empirically estimated. We provide frequentistand Bayesian analyses for this situation. Some of our proof techniques arenon-standard and may be of independent interest. The bounds we derive arenearly optimal, and several challenging open problems are posed. Experimentalresults are provided to illustrate the theory.
arxiv-5100-3 | The Law of Total Odds | http://arxiv.org/pdf/1312.0365v5.pdf | author:Dirk Tasche category:math.PR stat.AP stat.ML 60A05, 62H30 published:2013-12-02 summary:The law of total probability may be deployed in binary classificationexercises to estimate the unconditional class probabilities if the classproportions in the training set are not representative of the population classproportions. We argue that this is not a conceptually sound approach andsuggest an alternative based on the new law of total odds. We quantify the biasof the total probability estimator of the unconditional class probabilities andshow that the total odds estimator is unbiased. The sample version of the totalodds estimator is shown to coincide with a maximum-likelihood estimator knownfrom the literature. The law of total odds can also be used for transformingthe conditional class probabilities if independent estimates of theunconditional class probabilities of the population are available. Keywords: Total probability, likelihood ratio, Bayes' formula, binaryclassification, relative odds, unbiased estimator, supervised learning, datasetshift.
arxiv-5100-4 | Sensing-Aware Kernel SVM | http://arxiv.org/pdf/1312.0512v2.pdf | author:Weicong Ding, Prakash Ishwar, Venkatesh Saligrama, W. Clem Karl category:cs.LG published:2013-12-02 summary:We propose a novel approach for designing kernels for support vector machines(SVMs) when the class label is linked to the observation through a latent stateand the likelihood function of the observation given the state (the sensingmodel) is available. We show that the Bayes-optimum decision boundary is ahyperplane under a mapping defined by the likelihood function. Combining thiswith the maximum margin principle yields kernels for SVMs that leverageknowledge of the sensing model in an optimal way. We derive the optimum kernelfor the bag-of-words (BoWs) sensing model and demonstrate its superiorperformance over other kernels in document and image classification tasks.These results indicate that such optimum sensing-aware kernel SVMs can matchthe performance of rather sophisticated state-of-the-art approaches.
arxiv-5100-5 | Inferring Regulatory Networks by Combining Perturbation Screens and Steady State Gene Expression Profiles | http://arxiv.org/pdf/1312.0335v1.pdf | author:Ali Shojaie, Alexandra Jauhiainen, Michael Kallitsis, George Michailidis category:stat.ML q-bio.MN published:2013-12-02 summary:Reconstructing transcriptional regulatory networks is an important task infunctional genomics. Data obtained from experiments that perturb genes byknockouts or RNA interference contain useful information for addressing thisreconstruction problem. However, such data can be limited in size and/or areexpensive to acquire. On the other hand, observational data of the organism insteady state (e.g. wild-type) are more readily available, but theirinformational content is inadequate for the task at hand. We develop acomputational approach to appropriately utilize both data sources forestimating a regulatory network. The proposed approach is based on a three-stepalgorithm to estimate the underlying directed but cyclic network, that uses asinput both perturbation screens and steady state gene expression data. In thefirst step, the algorithm determines causal orderings of the genes that areconsistent with the perturbation data, by combining an exhaustive search methodwith a fast heuristic that in turn couples a Monte Carlo technique with a fastsearch algorithm. In the second step, for each obtained causal ordering, aregulatory network is estimated using a penalized likelihood based method,while in the third step a consensus network is constructed from the highestscored ones. Extensive computational experiments show that the algorithmperforms well in reconstructing the underlying network and clearly outperformscompeting approaches that rely only on a single data source. Further, it isestablished that the algorithm produces a consistent estimate of the regulatorynetwork.
arxiv-5100-6 | Practical Collapsed Stochastic Variational Inference for the HDP | http://arxiv.org/pdf/1312.0412v1.pdf | author:Arnim Bleier category:cs.LG published:2013-12-02 summary:Recent advances have made it feasible to apply the stochastic variationalparadigm to a collapsed representation of latent Dirichlet allocation (LDA).While the stochastic variational paradigm has successfully been applied to anuncollapsed representation of the hierarchical Dirichlet process (HDP), noattempts to apply this type of inference in a collapsed setting ofnon-parametric topic modeling have been put forward so far. In this paper weexplore such a collapsed stochastic variational Bayes inference for the HDP.The proposed online algorithm is easy to implement and accounts for theinference of hyper-parameters. First experiments show a promising improvementin predictive performance.
arxiv-5100-7 | Precise Semidefinite Programming Formulation of Atomic Norm Minimization for Recovering d-Dimensional ($d\geq 2$) Off-the-Grid Frequencies | http://arxiv.org/pdf/1312.0485v1.pdf | author:Weiyu Xu, Jian-Feng Cai, Kumar Vijay Mishra, Myung Cho, Anton Kruger category:cs.IT math.IT math.OC stat.ML published:2013-12-02 summary:Recent research in off-the-grid compressed sensing (CS) has demonstratedthat, under certain conditions, one can successfully recover a spectrallysparse signal from a few time-domain samples even though the dictionary iscontinuous. In particular, atomic norm minimization was proposed in\cite{tang2012csotg} to recover $1$-dimensional spectrally sparse signal.However, in spite of existing research efforts \cite{chi2013compressive}, itwas still an open problem how to formulate an equivalent positive semidefiniteprogram for atomic norm minimization in recovering signals with $d$-dimensional($d\geq 2$) off-the-grid frequencies. In this paper, we settle this problem byproposing equivalent semidefinite programming formulations of atomic normminimization to recover signals with $d$-dimensional ($d\geq 2$) off-the-gridfrequencies.
arxiv-5100-8 | Bidirectional Recursive Neural Networks for Token-Level Labeling with Structure | http://arxiv.org/pdf/1312.0493v1.pdf | author:Ozan Ä°rsoy, Claire Cardie category:cs.LG cs.CL stat.ML published:2013-12-02 summary:Recently, deep architectures, such as recurrent and recursive neural networkshave been successfully applied to various natural language processing tasks.Inspired by bidirectional recurrent neural networks which use representationsthat summarize the past and future around an instance, we propose a novelarchitecture that aims to capture the structural information around an input,and use it to label instances. We apply our method to the task of opinionexpression extraction, where we employ the binary parse tree of a sentence asthe structure, and word vector representations as the initial representation ofa single token. We conduct preliminary experiments to investigate itsperformance and compare it to the sequential approach.
arxiv-5100-9 | SpeedMachines: Anytime Structured Prediction | http://arxiv.org/pdf/1312.0579v1.pdf | author:Alexander Grubb, Daniel Munoz, J. Andrew Bagnell, Martial Hebert category:cs.LG published:2013-12-02 summary:Structured prediction plays a central role in machine learning applicationsfrom computational biology to computer vision. These models requiresignificantly more computation than unstructured models, and, in manyapplications, algorithms may need to make predictions within a computationalbudget or in an anytime fashion. In this work we propose an anytime techniquefor learning structured prediction that, at training time, incorporates bothstructural elements and feature computation trade-offs that affect test-timeinference. We apply our technique to the challenging problem of sceneunderstanding in computer vision and demonstrate efficient and anytimepredictions that gradually improve towards state-of-the-art classificationperformance as the allotted time increases.
arxiv-5100-10 | Grid Topology Identification using Electricity Prices | http://arxiv.org/pdf/1312.0516v2.pdf | author:Vassilis Kekatos, Georgios B. Giannakis, Ross Baldick category:cs.LG cs.SY stat.AP stat.ML published:2013-12-02 summary:The potential of recovering the topology of a grid using solely publiclyavailable market data is explored here. In contemporary whole-sale electricitymarkets, real-time prices are typically determined by solving thenetwork-constrained economic dispatch problem. Under a linear DC model,locational marginal prices (LMPs) correspond to the Lagrange multipliers of thelinear program involved. The interesting observation here is that the matrix ofspatiotemporally varying LMPs exhibits the following property: Oncepremultiplied by the weighted grid Laplacian, it yields a low-rank and sparsematrix. Leveraging this rich structure, a regularized maximum likelihoodestimator (MLE) is developed to recover the grid Laplacian from the LMPs. Theconvex optimization problem formulated includes low rank- andsparsity-promoting regularizers, and it is solved using a scalable algorithm.Numerical tests on prices generated for the IEEE 14-bus benchmark provideencouraging topology recovery results.
arxiv-5100-11 | Families of Parsimonious Finite Mixtures of Regression Models | http://arxiv.org/pdf/1312.0518v1.pdf | author:Utkarsh J. Dang, Paul D. McNicholas category:stat.ME stat.CO stat.ML published:2013-12-02 summary:Finite mixtures of regression models offer a flexible framework forinvestigating heterogeneity in data with functional dependencies. These modelscan be conveniently used for unsupervised learning on data with clearregression relationships. We extend such models by imposing aneigen-decomposition on the multivariate error covariance matrix. Byconstraining parts of this decomposition, we obtain families of parsimoniousmixtures of regressions and mixtures of regressions with concomitant variables.These families of models account for correlations between multiple responses.An expectation-maximization algorithm is presented for parameter estimation andperformance is illustrated on simulated and real data.
arxiv-5100-12 | Phase Transitions in Community Detection: A Solvable Toy Model | http://arxiv.org/pdf/1312.0631v1.pdf | author:Greg Ver Steeg, Cristopher Moore, Aram Galstyan, Armen E. Allahverdyan category:cs.SI physics.soc-ph stat.ML published:2013-12-02 summary:Recently, it was shown that there is a phase transition in the communitydetection problem. This transition was first computed using the cavity method,and has been proved rigorously in the case of $q=2$ groups. However, analyticcalculations using the cavity method are challenging since they require us tounderstand probability distributions of messages. We study analogoustransitions in so-called "zero-temperature inference" model, where thisdistribution is supported only on the most-likely messages. Furthermore,whenever several messages are equally likely, we break the tie by choosingamong them with equal probability. While the resulting analysis does not givethe correct values of the thresholds, it does reproduce some of the qualitativefeatures of the system. It predicts a first-order detectability transitionwhenever $q > 2$, while the finite-temperature cavity method shows that this isthe case only when $q > 4$. It also has a regime analogous to the "hard butdetectable" phase, where the community structure can be partially recovered,but only when the initial messages are sufficiently accurate. Finally, we studya semisupervised setting where we are given the correct labels for a fraction$\rho$ of the nodes. For $q > 2$, we find a regime where the accuracy jumpsdiscontinuously at a critical value of $\rho$.
arxiv-5100-13 | Efficient Learning and Planning with Compressed Predictive States | http://arxiv.org/pdf/1312.0286v2.pdf | author:William L. Hamilton, Mahdi Milani Fard, Joelle Pineau category:cs.LG stat.ML published:2013-12-01 summary:Predictive state representations (PSRs) offer an expressive framework formodelling partially observable systems. By compactly representing systems asfunctions of observable quantities, the PSR learning approach avoids usinglocal-minima prone expectation-maximization and instead employs a globallyoptimal moment-based algorithm. Moreover, since PSRs do not require apredetermined latent state structure as an input, they offer an attractiveframework for model-based reinforcement learning when agents must plan withouta priori access to a system model. Unfortunately, the expressiveness of PSRscomes with significant computational cost, and this cost is a major factorinhibiting the use of PSRs in applications. In order to alleviate thisshortcoming, we introduce the notion of compressed PSRs (CPSRs). The CPSRlearning approach combines recent advancements in dimensionality reduction,incremental matrix decomposition, and compressed sensing. We show how thisapproach provides a principled avenue for learning accurate approximations ofPSRs, drastically reducing the computational costs associated with learningwhile also providing effective regularization. Going further, we propose aplanning framework which exploits these learned models. And we show that thisapproach facilitates model-learning and planning in large complex partiallyobservable domains, a task that is infeasible without the principled use ofcompression.
arxiv-5100-14 | Stochastic continuum armed bandit problem of few linear parameters in high dimensions | http://arxiv.org/pdf/1312.0232v3.pdf | author:Hemant Tyagi, Sebastian Stich, Bernd GÃ¤rtner category:stat.ML cs.LG math.OC published:2013-12-01 summary:We consider a stochastic continuum armed bandit problem where the arms areindexed by the $\ell_2$ ball $B_{d}(1+\nu)$ of radius $1+\nu$ in$\mathbb{R}^d$. The reward functions $r :B_{d}(1+\nu) \rightarrow \mathbb{R}$are considered to intrinsically depend on $k \ll d$ unknown linear parametersso that $r(\mathbf{x}) = g(\mathbf{A} \mathbf{x})$ where $\mathbf{A}$ is a fullrank $k \times d$ matrix. Assuming the mean reward function to be smooth wemake use of results from low-rank matrix recovery literature and derive anefficient randomized algorithm which achieves a regret bound of $O(C(k,d)n^{\frac{1+k}{2+k}} (\log n)^{\frac{1}{2+k}})$ with high probability. Here$C(k,d)$ is at most polynomial in $d$ and $k$ and $n$ is the number of roundsor the sampling budget which is assumed to be known beforehand.
arxiv-5100-15 | Stochastic Optimization of Smooth Loss | http://arxiv.org/pdf/1312.0048v1.pdf | author:Rong Jin category:cs.LG published:2013-11-30 summary:In this paper, we first prove a high probability bound rather than anexpectation bound for stochastic optimization with smooth loss. Furthermore,the existing analysis requires the knowledge of optimal classifier for tuningthe step size in order to achieve the desired bound. However, this informationis usually not accessible in advanced. We also propose a strategy to addressthe limitation.
arxiv-5100-16 | A Framework for Genetic Algorithms Based on Hadoop | http://arxiv.org/pdf/1312.0086v2.pdf | author:Filomena Ferrucci, M-Tahar Kechadi, Pasquale Salza, Federica Sarro category:cs.NE cs.DC published:2013-11-30 summary:Genetic Algorithms (GAs) are powerful metaheuristic techniques mostly used inmany real-world applications. The sequential execution of GAs requiresconsiderable computational power both in time and resources. Nevertheless, GAsare naturally parallel and accessing a parallel platform such as Cloud is easyand cheap. Apache Hadoop is one of the common services that can be used forparallel applications. However, using Hadoop to develop a parallel version ofGAs is not simple without facing its inner workings. Even though somesequential frameworks for GAs already exist, there is no framework supportingthe development of GA applications that can be executed in parallel. In thispaper is described a framework for parallel GAs on the Hadoop platform,following the paradigm of MapReduce. The main purpose of this framework is toallow the user to focus on the aspects of GA that are specific to the problemto be addressed, being sure that this task is going to be correctly executed onthe Cloud with a good performance. The framework has been also exploited todevelop an application for Feature Subset Selection problem. A preliminaryanalysis of the performance of the developed GA application has been performedusing three datasets and shown very promising performance.
arxiv-5100-17 | Improving Texture Categorization with Biologically Inspired Filtering | http://arxiv.org/pdf/1312.0072v1.pdf | author:Ngoc-Son Vu, Thanh Phuong Nguyen, Christophe Garcia category:cs.CV published:2013-11-30 summary:Within the domain of texture classification, a lot of effort has been spenton local descriptors, leading to many powerful algorithms. However,preprocessing techniques have received much less attention despite theirimportant potential for improving the overall classification performance. Weaddress this question by proposing a novel, simple, yet very powerfulbiologically-inspired filtering (BF) which simulates the performance of humanretina. In the proposed approach, given a texture image, after applying a DoGfilter to detect the "edges", we first split the filtered image into two "maps"alongside the sides of its edges. The feature extraction step is then carriedout on the two "maps" instead of the input image. Our algorithm has severaladvantages such as simplicity, robustness to illumination and noise, anddiscriminative power. Experimental results on three large texture databasesshow that with an extremely low computational cost, the proposed methodimproves significantly the performance of many texture classification systems,notably in noisy environments. The source codes of the proposed algorithm canbe downloaded from https://sites.google.com/site/nsonvu/code.
arxiv-5100-18 | One-Class Classification: Taxonomy of Study and Review of Techniques | http://arxiv.org/pdf/1312.0049v1.pdf | author:Shehroz S. Khan, Michael G. Madden category:cs.LG cs.AI published:2013-11-30 summary:One-class classification (OCC) algorithms aim to build classification modelswhen the negative class is either absent, poorly sampled or not well defined.This unique situation constrains the learning of efficient classifiers bydefining class boundary just with the knowledge of positive class. The OCCproblem has been considered and applied under many research themes, such asoutlier/novelty detection and concept learning. In this paper we present aunified view of the general problem of OCC by presenting a taxonomy of studyfor OCC problems, which is based on the availability of training data,algorithms used and the application domains applied. We further delve into eachof the categories of the proposed taxonomy and present a comprehensiveliterature review of the OCC algorithms, techniques and methodologies with afocus on their significance, limitations and applications. We conclude ourpaper by discussing some open research problems in the field of OCC and presentour vision for future research.
arxiv-5100-19 | A Typology of Collaboration Platform Users | http://arxiv.org/pdf/1312.0162v1.pdf | author:Anastasia Bezzubtseva, Dmitry I. Ignatov category:cs.CY cs.HC cs.SI stat.ML 68U35, 91D30 K.4.3 published:2013-11-30 summary:In this paper we present a review of the existing typologies of Internetservice users. We zoom in on social networking services including blogs andcrowdsourcing websites. Based on the results of the analysis of the consideredtypologies obtained by means of FCA we developed a new user typology of acertain class of Internet services, namely a collaboration innovation platform.Cluster analysis of data extracted from the collaboration platform Witology wasused to divide more than 500 participants into six groups based on threeactivity indicators: idea generation, commenting, and evaluation (assigningmarks) The obtained groups and their percentages appear to follow the "90 - 9 -1" rule.
arxiv-5100-20 | Statistical estimation for optimization problems on graphs | http://arxiv.org/pdf/1311.7656v1.pdf | author:Mikhail Langovoy, Suvrit Sra category:stat.ML cs.DM math.OC stat.CO stat.ME published:2013-11-29 summary:Large graphs abound in machine learning, data mining, and several relatedareas. A useful step towards analyzing such graphs is that of obtaining certainsummary statistics - e.g., or the expected length of a shortest path betweentwo nodes, or the expected weight of a minimum spanning tree of the graph, etc.These statistics provide insight into the structure of a graph, and they canhelp predict global properties of a graph. Motivated thus, we propose to studystatistical properties of structured subgraphs (of a given graph), inparticular, to estimate the expected objective function value of acombinatorial optimization problem over these subgraphs. The general task isvery difficult, if not unsolvable; so for concreteness we describe a morespecific statistical estimation problem based on spanning trees. We hope thatour position paper encourages others to also study other types of graphicalstructures for which one can prove nontrivial statistical estimates.
arxiv-5100-21 | The Power of Asymmetry in Binary Hashing | http://arxiv.org/pdf/1311.7662v1.pdf | author:Behnam Neyshabur, Payman Yadollahpour, Yury Makarychev, Ruslan Salakhutdinov, Nathan Srebro category:cs.LG cs.CV cs.IR published:2013-11-29 summary:When approximating binary similarity using the hamming distance between shortbinary hashes, we show that even if the similarity is symmetric, we can haveshorter and more accurate hashes by using two distinct code maps. I.e. byapproximating the similarity between $x$ and $x'$ as the hamming distancebetween $f(x)$ and $g(x')$, for two distinct binary codes $f,g$, rather than asthe hamming distance between $f(x)$ and $f(x')$.
arxiv-5100-22 | Combination of Diverse Ranking Models for Personalized Expedia Hotel Searches | http://arxiv.org/pdf/1311.7679v1.pdf | author:Xudong Liu, Bing Xu, Yuyu Zhang, Qiang Yan, Liang Pang, Qiang Li, Hanxiao Sun, Bin Wang category:cs.LG published:2013-11-29 summary:The ICDM Challenge 2013 is to apply machine learning to the problem of hotelranking, aiming to maximize purchases according to given hotel characteristics,location attractiveness of hotels, user's aggregated purchase history andcompetitive online travel agency information for each potential hotel choice.This paper describes the solution of team "binghsu & MLRush & BrickMover". Weconduct simple feature engineering work and train different models by eachindividual team member. Afterwards, we use listwise ensemble method to combineeach model's output. Besides describing effective model and features, we willdiscuss about the lessons we learned while using deep learning in thiscompetition.
arxiv-5100-23 | Adaptive nonparametric detection in cryo-electron microscopy | http://arxiv.org/pdf/1311.7650v1.pdf | author:Mikhail Langovoy, Michael Habeck, Bernhard Schoelkopf category:stat.AP stat.ME stat.ML published:2013-11-29 summary:Cryo-electron microscopy (cryo-EM) is an emerging experimental method tocharacterize the structure of large biomolecular assemblies. Single particlecryo-EM records 2D images (so-called micrographs) of projections of thethree-dimensional particle, which need to be processed to obtain thethree-dimensional reconstruction. A crucial step in the reconstruction processis particle picking which involves detection of particles in noisy 2Dmicrographs with low signal-to-noise ratios of typically 1:10 or even lower.Typically, each picture contains a large number of particles, and particleshave unknown irregular and nonconvex shapes.
arxiv-5100-24 | ADMM Algorithm for Graphical Lasso with an $\ell_{\infty}$ Element-wise Norm Constraint | http://arxiv.org/pdf/1311.7198v1.pdf | author:Karthik Mohan category:cs.LG math.OC stat.ML published:2013-11-28 summary:We consider the problem of Graphical lasso with an additional $\ell_{\infty}$element-wise norm constraint on the precision matrix. This problem hasapplications in high-dimensional covariance decomposition such as in\citep{Janzamin-12}. We propose an ADMM algorithm to solve this problem. Wealso use a continuation strategy on the penalty parameter to have a fastimplemenation of the algorithm.
arxiv-5100-25 | Real-time High Resolution Fusion of Depth Maps on GPU | http://arxiv.org/pdf/1311.7194v1.pdf | author:Dmitry Trifonov category:cs.GR cs.CV published:2013-11-28 summary:A system for live high quality surface reconstruction using a single movingdepth camera on a commodity hardware is presented. High accuracy and real-timeframe rate is achieved by utilizing graphics hardware computing capabilitiesvia OpenCL and by using sparse data structure for volumetric surfacerepresentation. Depth sensor pose is estimated by combining serial textureregistration algorithm with iterative closest points algorithm (ICP) aligningobtained depth map to the estimated scene model. Aligned surface is then fusedinto the scene. Kalman filter is used to improve fusion quality. Truncatedsigned distance function (TSDF) stored as block-based sparse buffer is used torepresent surface. Use of sparse data structure greatly increases accuracy ofscanned surfaces and maximum scanning area. Traditional GPU implementation ofvolumetric rendering and fusion algorithms were modified to exploit sparsity toachieve desired performance. Incorporation of texture registration for sensorpose estimation and Kalman filter for measurement integration improved accuracyand robustness of scanning process.
arxiv-5100-26 | A Novel Illumination-Invariant Loss for Monocular 3D Pose Estimation | http://arxiv.org/pdf/1311.7186v1.pdf | author:Srimal Jayawardena, Marcus Hutter, Nathan Brewer category:cs.CV published:2013-11-28 summary:The problem of identifying the 3D pose of a known object from a given 2Dimage has important applications in Computer Vision. Our proposed method ofregistering a 3D model of a known object on a given 2D photo of the object hasnumerous advantages over existing methods. It does not require prior training,knowledge of the camera parameters, explicit point correspondences or matchingfeatures between the image and model. Unlike techniques that estimate a partial3D pose (as in an overhead view of traffic or machine parts on a conveyorbelt), our method estimates the complete 3D pose of the object. It works on asingle static image from a given view under varying and unknown lightingconditions. For this purpose we derive a novel illumination-invariant distancemeasure between the 2D photo and projected 3D model, which is then minimised tofind the best pose parameters. Results for vehicle pose detection in realphotographs are presented.
arxiv-5100-27 | Learning Semantic Representations for the Phrase Translation Model | http://arxiv.org/pdf/1312.0482v1.pdf | author:Jianfeng Gao, Xiaodong He, Wen-tau Yih, Li Deng category:cs.CL published:2013-11-28 summary:This paper presents a novel semantic-based phrase translation model. A pairof source and target phrases are projected into continuous-valued vectorrepresentations in a low-dimensional latent semantic space, where theirtranslation score is computed by the distance between the pair in this newspace. The projection is performed by a multi-layer neural network whoseweights are learned on parallel training data. The learning is aimed todirectly optimize the quality of end-to-end machine translation results.Experimental evaluation has been performed on two Europarl translation tasks,English-French and German-English. The results show that the new semantic-basedphrase translation model significantly improves the performance of astate-of-the-art phrase-based statistical machine translation sys-tem, leadingto a gain of 0.7-1.0 BLEU points.
arxiv-5100-28 | An Alternate Approach for Designing a Domain Specific Image Search Prototype Using Histogram | http://arxiv.org/pdf/1401.2902v1.pdf | author:Sukanta Sinha, Rana Dattagupta, Debajyoti Mukhopadhyay category:cs.CV cs.IR published:2013-11-28 summary:Everyone knows that thousand of words are represented by a single image. As aresult image search has become a very popular mechanism for the Web searchers.Image search means, the search results are produced by the search engine shouldbe a set of images along with their Web page Unified Resource Locator. Now Websearcher can perform two types of image search, they are Text to Image andImage to Image search. In Text to Image search, search query should be a text.Based on the input text data system will generate a set of images along withtheir Web page URL as an output. On the other hand, in Image to Image search,search query should be an image and based on this image system will generate aset of images along with their Web page URL as an output. According to thecurrent scenarios, Text to Image search mechanism always not returns perfectresult. It matches the text data and then displays the corresponding images asan output, which is not always perfect. To resolve this problem, Webresearchers have introduced the Image to Image search mechanism. In this paper,we have also proposed an alternate approach of Image to Image search mechanismusing Histogram.
arxiv-5100-29 | Finding a Maximum Clique using Ant Colony Optimization and Particle Swarm Optimization in Social Networks | http://arxiv.org/pdf/1311.7213v1.pdf | author:Mohammad Soleimani-Pouri, Alireza Rezvanian, Mohammad Reza Meybodi category:cs.SI cs.NE published:2013-11-28 summary:Interaction between users in online social networks plays a key role insocial network analysis. One on important types of social group is fullconnected relation between some users, which known as clique structure.Therefore finding a maximum clique is essential for some analysis. In thispaper, we proposed a new method using ant colony optimization algorithm andparticle swarm optimization algorithm. In the proposed method, in order toattain better results, it is improved process of pheromone update by particleswarm optimization. Simulation results on popular standard social networkbenchmarks in comparison standard ant colony optimization algorithm are shown arelative enhancement of proposed algorithm.
arxiv-5100-30 | Using Multiple Samples to Learn Mixture Models | http://arxiv.org/pdf/1311.7184v1.pdf | author:Jason D Lee, Ran Gilad-Bachrach, Rich Caruana category:stat.ML cs.LG published:2013-11-28 summary:In the mixture models problem it is assumed that there are $K$ distributions$\theta_{1},\ldots,\theta_{K}$ and one gets to observe a sample from a mixtureof these distributions with unknown coefficients. The goal is to associateinstances with their generating distributions, or to identify the parameters ofthe hidden distributions. In this work we make the assumption that we haveaccess to several samples drawn from the same $K$ underlying distributions, butwith different mixing weights. As with topic modeling, having multiple samplesis often a reasonable assumption. Instead of pooling the data into one sample,we prove that it is possible to use the differences between the samples tobetter recover the underlying structure. We present algorithms that recover theunderlying structure under milder assumptions than the current state of artwhen either the dimensionality or the separation is high. The methods, whenapplied to topic modeling, allow generalization to words not present in thetraining data.
arxiv-5100-31 | Shape from Texture using Locally Scaled Point Processes | http://arxiv.org/pdf/1311.7401v1.pdf | author:Eva-Maria Didden, Thordis Linda Thorarinsdottir, Alex Lenkoski, Christoph SchnÃ¶rr category:stat.AP cs.CV published:2013-11-28 summary:Shape from texture refers to the extraction of 3D information from 2D imageswith irregular texture. This paper introduces a statistical framework to learnshape from texture where convex texture elements in a 2D image are representedthrough a point process. In a first step, the 2D image is preprocessed togenerate a probability map corresponding to an estimate of the unnormalizedintensity of the latent point process underlying the texture elements. Thelatent point process is subsequently inferred from the probability map in anon-parametric, model free manner. Finally, the 3D information is extractedfrom the point pattern by applying a locally scaled point process model wherethe local scaling function represents the deformation caused by the projectionof a 3D surface onto a 2D image.
arxiv-5100-32 | Glasgow's Stereo Image Database of Garments | http://arxiv.org/pdf/1311.7295v1.pdf | author:Gerardo Aragon-Camarasa, Susanne B. Oehler, Yuan Liu, Sun Li, Paul Cockshott, J. Paul Siebert category:cs.RO cs.CV published:2013-11-28 summary:To provide insight into cloth perception and manipulation with an activebinocular robotic vision system, we compiled a database of 80 stereo-paircolour images with corresponding horizontal and vertical disparity maps andmask annotations, for 3D garment point cloud rendering has been created andreleased. The stereo-image garment database is part of research conducted underthe EU-FP7 Clothes Perception and Manipulation (CloPeMa) project and belongs toa wider database collection released through CloPeMa (www.clopema.eu). Thisdatabase is based on 16 different off-the-shelve garments. Each garment hasbeen imaged in five different pose configurations on the project's binocularrobot head. A full copy of the database is made available for scientificresearch only at https://sites.google.com/site/ugstereodatabase/.
arxiv-5100-33 | Unobtrusive Low Cost Pupil Size Measurements using Web cameras | http://arxiv.org/pdf/1311.7327v1.pdf | author:Sergios Petridis, Theodoros Giannakopoulos, Costantine D. Spyropoulos category:cs.CV J.3; I.4.8 published:2013-11-28 summary:Unobtrusive every day health monitoring can be of important use for theelderly population. In particular, pupil size may be a valuable source ofinformation, since, apart from pathological cases, it can reveal the emotionalstate, the fatigue and the ageing. To allow for unobtrusive monitoring to gainacceptance, one should seek for efficient methods of monitoring using com- monlow-cost hardware. This paper describes a method for monitoring pupil sizesusing a common web camera in real time. Our method works by first detecting theface and the eyes area. Subsequently, optimal iris and sclera location andradius, modelled as ellipses, are found using efficient filtering. Finally, thepupil center and radius is estimated by optimal filtering within the area ofthe iris. Experimental result show both the efficiency and the effectiveness ofour approach.
arxiv-5100-34 | Spatially-Adaptive Reconstruction in Computed Tomography using Neural Networks | http://arxiv.org/pdf/1311.7251v1.pdf | author:Joseph Shtok, Michael Zibulevsky, Michael Elad category:cs.CV cs.LG cs.NE published:2013-11-28 summary:We propose a supervised machine learning approach for boosting existingsignal and image recovery methods and demonstrate its efficacy on example ofimage reconstruction in computed tomography. Our technique is based on a localnonlinear fusion of several image estimates, all obtained by applying a chosenreconstruction algorithm with different values of its control parameters.Usually such output images have different bias/variance trade-off. The fusionof the images is performed by feed-forward neural network trained on a set ofknown examples. Numerical experiments show an improvement in reconstructionquality relatively to existing direct and iterative reconstruction methods.
arxiv-5100-35 | Bayesian Inference for Gaussian Process Classifiers with Annealing and Pseudo-Marginal MCMC | http://arxiv.org/pdf/1311.7320v2.pdf | author:Maurizio Filippone category:stat.ME stat.ML published:2013-11-28 summary:Kernel methods have revolutionized the fields of pattern recognition andmachine learning. Their success, however, critically depends on the choice ofkernel parameters. Using Gaussian process (GP) classification as a workingexample, this paper focuses on Bayesian inference of covariance (kernel)parameters using Markov chain Monte Carlo (MCMC) methods. The motivation isthat, compared to standard optimization of kernel parameters, they have beensystematically demonstrated to be superior in quantifying uncertainty inpredictions. Recently, the Pseudo-Marginal MCMC approach has been proposed as apractical inference tool for GP models. In particular, it amounts in replacingthe analytically intractable marginal likelihood by an unbiased estimateobtainable by approximate methods and importance sampling. After discussing thepotential drawbacks in employing importance sampling, this paper proposes theapplication of annealed importance sampling. The results empiricallydemonstrate that compared to importance sampling, annealed importance samplingcan reduce the variance of the estimate of the marginal likelihoodexponentially in the number of data at a computational cost that scales onlypolynomially. The results on real data demonstrate that employing annealedimportance sampling in the Pseudo-Marginal MCMC approach represents a stepforward in the development of fully automated exact inference engines for GPmodels.
arxiv-5100-36 | Algorithmic Identification of Probabilities | http://arxiv.org/pdf/1311.7385v3.pdf | author:Paul M. B. Vitanyi, Nick Chater category:cs.LG published:2013-11-28 summary:TThe problem is to identify a probability associated with a set of naturalnumbers, given an infinite data sequence of elements from the set. If the givensequence is drawn i.i.d. and the probability mass function involved (thetarget) belongs to a computably enumerable (c.e.) or co-computably enumerable(co-c.e.) set of computable probability mass functions, then there is analgorithm to almost surely identify the target in the limit. The technical toolis the strong law of large numbers. If the set is finite and the elements ofthe sequence are dependent while the sequence is typical in the sense ofMartin-L\"of for at least one measure belonging to a c.e. or co-c.e. set ofcomputable measures, then there is an algorithm to identify in the limit acomputable measure for which the sequence is typical (there may be more thanone such measure). The technical tool is the theory of Kolmogorov complexity.We give the algorithms and consider the associated predictions.
arxiv-5100-37 | Dimensionality reduction for click-through rate prediction: Dense versus sparse representation | http://arxiv.org/pdf/1311.6976v2.pdf | author:Bjarne Ãrum Fruergaard, Toke Jansen Hansen, Lars Kai Hansen category:stat.ML cs.LG stat.AP stat.ME published:2013-11-27 summary:In online advertising, display ads are increasingly being placed based onreal-time auctions where the advertiser who wins gets to serve the ad. This iscalled real-time bidding (RTB). In RTB, auctions have very tight timeconstraints on the order of 100ms. Therefore mechanisms for biddingintelligently such as clickthrough rate prediction need to be sufficientlyfast. In this work, we propose to use dimensionality reduction of theuser-website interaction graph in order to produce simplified features of usersand websites that can be used as predictors of clickthrough rate. Wedemonstrate that the Infinite Relational Model (IRM) as a dimensionalityreduction offers comparable predictive performance to conventionaldimensionality reduction schemes, while achieving the most economical usage offeatures and fastest computations at run-time. For applications such asreal-time bidding, where fast database I/O and few computations are key tosuccess, we thus recommend using IRM based features as predictors to exploitthe recommender effects from bipartite graphs.
arxiv-5100-38 | Cross-Domain Sparse Coding | http://arxiv.org/pdf/1311.7080v1.pdf | author:Jim Jing-Yan Wang category:cs.CV stat.ML published:2013-11-27 summary:Sparse coding has shown its power as an effective data representation method.However, up to now, all the sparse coding approaches are limited within thesingle domain learning problem. In this paper, we extend the sparse coding tocross domain learning problem, which tries to learn from a source domain to atarget domain with significant different distribution. We impose the MaximumMean Discrepancy (MMD) criterion to reduce the cross-domain distributiondifference of sparse codes, and also regularize the sparse codes by the classlabels of the samples from both domains to increase the discriminative ability.The encouraging experiment results of the proposed cross-domain sparse codingalgorithm on two challenging tasks --- image classification of photograph andoil painting domains, and multiple user spam detection --- show the advantageof the proposed method over other cross-domain data representation methods.
arxiv-5100-39 | A novel framework for image forgery localization | http://arxiv.org/pdf/1311.6932v1.pdf | author:Davide Cozzolino, Diego Gragnaniello, Luisa Verdoliva category:cs.CV published:2013-11-27 summary:Image forgery localization is a very active and open research field for thedifficulty to handle the large variety of manipulations a malicious user canperform by means of more and more sophisticated image editing tools. Here, wepropose a localization framework based on the fusion of three very differenttools, based, respectively, on sensor noise, patch-matching, and machinelearning. The binary masks provided by these tools are finally fused based onsome suitable reliability indexes. According to preliminary experiments on thetraining set, the proposed framework provides often a very good localizationaccuracy and sometimes valuable clues for visual scrutiny.
arxiv-5100-40 | Sparse Linear Dynamical System with Its Application in Multivariate Clinical Time Series | http://arxiv.org/pdf/1311.7071v2.pdf | author:Zitao Liu, Milos Hauskrecht category:cs.AI cs.LG stat.ML published:2013-11-27 summary:Linear Dynamical System (LDS) is an elegant mathematical framework formodeling and learning multivariate time series. However, in general, it isdifficult to set the dimension of its hidden state space. A small number ofhidden states may not be able to model the complexities of a time series, whilea large number of hidden states can lead to overfitting. In this paper, westudy methods that impose an $\ell_1$ regularization on the transition matrixof an LDS model to alleviate the problem of choosing the optimal number ofhidden states. We incorporate a generalized gradient descent method into theMaximum a Posteriori (MAP) framework and use Expectation Maximization (EM) toiteratively achieve sparsity on the transition matrix of an LDS model. We showthat our Sparse Linear Dynamical System (SLDS) improves the predictiveperformance when compared to ordinary LDS on a multivariate clinical timeseries dataset.
arxiv-5100-41 | Image forgery detection based on the fusion of machine learning and block-matching methods | http://arxiv.org/pdf/1311.6934v1.pdf | author:Davide Cozzolino, Diego Gragnaniello, Luisa Verdoliva category:cs.CV published:2013-11-27 summary:Dense local descriptors and machine learning have been used with success inseveral applications, like classification of textures, steganalysis, andforgery detection. We develop a new image forgery detector building upon somedescriptors recently proposed in the steganalysis field suitably merging someof such descriptors, and optimizing a SVM classifier on the available trainingset. Despite the very good performance, very small forgeries are hardly everdetected because they contribute very little to the descriptors. Therefore wealso develop a simple, but extremely specific, copy-move detector based onregion matching and fuse decisions so as to reduce the missing detection rate.Overall results appear to be extremely encouraging.
arxiv-5100-42 | Color and Shape Content Based Image Classification using RBF Network and PSO Technique: A Survey | http://arxiv.org/pdf/1311.6881v1.pdf | author:Abhishek Pandey, Anjna Jayant Deen, Rajeev Pandey category:cs.CV cs.LG cs.NE published:2013-11-27 summary:The improvement of the accuracy of image query retrieval used imageclassification technique. Image classification is well known technique ofsupervised learning. The improved method of image classification increases theworking efficiency of image query retrieval. For the improvements ofclassification technique we used RBF neural network function for betterprediction of feature used in image retrieval.Colour content is represented bypixel values in image classification using radial base function(RBF) technique.This approach provides better result compare to SVM technique in imagerepresentation.Image is represented by matrix though RBF using pixel values ofcolour intensity of image. Firstly we using RGB colour model. In this colourmodel we use red, green and blue colour intensity values in matrix.SVM withpartical swarm optimization for image classification is implemented in contentof images which provide better Results based on the proposed approach are foundencouraging in terms of color image classification accuracy.
arxiv-5100-43 | Modeling Radiometric Uncertainty for Vision with Tone-mapped Color Images | http://arxiv.org/pdf/1311.6887v2.pdf | author:Ayan Chakrabarti, Ying Xiong, Baochen Sun, Trevor Darrell, Daniel Scharstein, Todd Zickler, Kate Saenko category:cs.CV published:2013-11-27 summary:To produce images that are suitable for display, tone-mapping is widely usedin digital cameras to map linear color measurements into narrow gamuts withlimited dynamic range. This introduces non-linear distortion that must beundone, through a radiometric calibration process, before computer visionsystems can analyze such photographs radiometrically. This paper considers theinherent uncertainty of undoing the effects of tone-mapping. We observe thatthis uncertainty varies substantially across color space, making some pixelsmore reliable than others. We introduce a model for this uncertainty and amethod for fitting it to a given camera or imaging pipeline. Once fit, themodel provides for each pixel in a tone-mapped digital photograph a probabilitydistribution over linear scene colors that could have induced it. Wedemonstrate how these distributions can be useful for visual inference byincorporating them into estimation algorithms for a representative set ofvision tasks.
arxiv-5100-44 | Universal Codes from Switching Strategies | http://arxiv.org/pdf/1311.6536v1.pdf | author:Wouter M. Koolen, Steven de Rooij category:cs.IT cs.LG math.IT published:2013-11-26 summary:We discuss algorithms for combining sequential prediction strategies, a taskwhich can be viewed as a natural generalisation of the concept of universalcoding. We describe a graphical language based on Hidden Markov Models fordefining prediction strategies, and we provide both existing and new models asexamples. The models include efficient, parameterless models for switchingbetween the input strategies over time, including a model for the case whereswitches tend to occur in clusters, and finally a new model for the scenariowhere the prediction strategies have a known relationship, and where jumps aretypically between strongly related ones. This last model is relevant for codingtime series data where parameter drift is expected. As theoretical ontributionswe introduce an interpolation construction that is useful in the developmentand analysis of new algorithms, and we establish a new sophisticated lemma foranalysing the individual sequence regret of parameterised models.
arxiv-5100-45 | A Novel Family of Adaptive Filtering Algorithms Based on The Logarithmic Cost | http://arxiv.org/pdf/1311.6809v1.pdf | author:Muhammed O. Sayin, N. Denizcan Vanli, Suleyman S. Kozat category:cs.LG published:2013-11-26 summary:We introduce a novel family of adaptive filtering algorithms based on arelative logarithmic cost. The new family intrinsically combines the higher andlower order measures of the error into a single continuous update based on theerror amount. We introduce important members of this family of algorithms suchas the least mean logarithmic square (LMLS) and least logarithmic absolutedifference (LLAD) algorithms that improve the convergence performance of theconventional algorithms. However, our approach and analysis are generic suchthat they cover other well-known cost functions as described in the paper. TheLMLS algorithm achieves comparable convergence performance with the least meanfourth (LMF) algorithm and extends the stability bound on the step size. TheLLAD and least mean square (LMS) algorithms demonstrate similar convergenceperformance in impulse-free noise environments while the LLAD algorithm isrobust against impulsive interferences and outperforms the sign algorithm (SA).We analyze the transient, steady state and tracking performance of theintroduced algorithms and demonstrate the match of the theoretical analyzes andsimulation results. We show the extended stability bound of the LMLS algorithmand analyze the robustness of the LLAD algorithm against impulsiveinterferences. Finally, we demonstrate the performance of our algorithms indifferent scenarios through numerical examples.
arxiv-5100-46 | A Mixture of Generalized Hyperbolic Factor Analyzers | http://arxiv.org/pdf/1311.6530v3.pdf | author:Cristina Tortora, Paul D. McNicholas, Ryan P. Browne category:stat.ME stat.ML published:2013-11-26 summary:Model-based clustering imposes a finite mixture modelling structure on datafor clustering. Finite mixture models assume that the population is a convexcombination of a finite number of densities, the distribution within eachpopulation is a basic assumption of each particular model. Among alldistributions that have been tried, the generalized hyperbolic distribution hasthe advantage that is a generalization of several other methods, such as theGaussian distribution, the skew t-distribution, etc. With specific parameters,it can represent either a symmetric or a skewed distribution. While itsinherent flexibility is an advantage in many ways, it means the estimation ofmore parameters than its special and limiting cases. The aim of this work is topropose a mixture of generalized hyperbolic factor analyzers to introduceparsimony and extend the method to high dimensional data. This work can be seenas an extension of the mixture of factor analyzers model to generalizedhyperbolic mixtures. The performance of our generalized hyperbolic factoranalyzers is illustrated on real data, where it performs favourably compared toits Gaussian analogue.
arxiv-5100-47 | Double Ramp Loss Based Reject Option Classifier | http://arxiv.org/pdf/1311.6556v2.pdf | author:Naresh Manwani, Kalpit Desai, Sanand Sasidharan, Ramasubramanian Sundararajan category:cs.LG published:2013-11-26 summary:We consider the problem of learning reject option classifiers. The goodnessof a reject option classifier is quantified using $0-d-1$ loss function whereina loss $d \in (0,.5)$ is assigned for rejection. In this paper, we propose {\emdouble ramp loss} function which gives a continuous upper bound for $(0-d-1)$loss. Our approach is based on minimizing regularized risk under the doubleramp loss using {\em difference of convex (DC) programming}. We show theeffectiveness of our approach through experiments on synthetic and benchmarkdatasets. Our approach performs better than the state of the art reject optionclassification approaches.
arxiv-5100-48 | A Blockwise Descent Algorithm for Group-penalized Multiresponse and Multinomial Regression | http://arxiv.org/pdf/1311.6529v1.pdf | author:Noah Simon, Jerome Friedman, Trevor Hastie category:stat.CO stat.ML published:2013-11-26 summary:In this paper we purpose a blockwise descent algorithm for group-penalizedmultiresponse regression. Using a quasi-newton framework we extend this togroup-penalized multinomial regression. We give a publicly availableimplementation for these in R, and compare the speed of this algorithm to acompeting algorithm --- we show that our implementation is an order ofmagnitude faster than its competitor, and can solve gene-expression-sizedproblems in real time.
arxiv-5100-49 | Auto-adaptative Laplacian Pyramids for High-dimensional Data Analysis | http://arxiv.org/pdf/1311.6594v2.pdf | author:Ãngela FernÃ¡ndez, Neta Rabin, Dalia Fishelov, JosÃ© R. Dorronsoro category:cs.AI cs.LG stat.ML published:2013-11-26 summary:Non-linear dimensionality reduction techniques such as manifold learningalgorithms have become a common way for processing and analyzinghigh-dimensional patterns that often have attached a target that corresponds tothe value of an unknown function. Their application to new points consists intwo steps: first, embedding the new data point into the low dimensional spaceand then, estimating the function value on the test point from its neighbors inthe embedded space. However, finding the low dimension representation of a test point, while easyfor simple but often not powerful enough procedures such as PCA, can be muchmore complicated for methods that rely on some kind of eigenanalysis, such asSpectral Clustering (SC) or Diffusion Maps (DM). Similarly, when a targetfunction is to be evaluated, averaging methods like nearest neighbors may giveunstable results if the function is noisy. Thus, the smoothing of the targetfunction with respect to the intrinsic, low-dimensional representation thatdescribes the geometric structure of the examined data is a challenging task. In this paper we propose Auto-adaptive Laplacian Pyramids (ALP), an extensionof the standard Laplacian Pyramids model that incorporates a modified LOOCVprocedure that avoids the large cost of the standard one and offers thefollowing advantages: (i) it selects automatically the optimal functionresolution (stopping time) adapted to the data and its noise, (ii) it is easyto apply as it does not require parameterization, (iii) it does not overfit thetraining set and (iv) it adds no extra cost compared to other classicalinterpolation methods. We illustrate numerically ALP's behavior on a syntheticproblem and apply it to the computation of the DM projection of new patternsand to the extension to them of target function values on a radiationforecasting problem over very high dimensional patterns.
arxiv-5100-50 | Practical Inexact Proximal Quasi-Newton Method with Global Complexity Analysis | http://arxiv.org/pdf/1311.6547v4.pdf | author:Katya Scheinberg, Xiaocheng Tang category:cs.LG math.OC stat.ML published:2013-11-26 summary:Recently several methods were proposed for sparse optimization which makecareful use of second-order information [10, 28, 16, 3] to improve localconvergence rates. These methods construct a composite quadratic approximationusing Hessian information, optimize this approximation using a first-ordermethod, such as coordinate descent and employ a line search to ensuresufficient descent. Here we propose a general framework, which includesslightly modified versions of existing algorithms and also a new algorithm,which uses limited memory BFGS Hessian approximations, and provide a novelglobal convergence rate analysis, which covers methods that solve subproblemsvia coordinate descent.
arxiv-5100-51 | Recommending with an Agenda: Active Learning of Private Attributes using Matrix Factorization | http://arxiv.org/pdf/1311.6802v2.pdf | author:Smriti Bhagat, Udi Weinsberg, Stratis Ioannidis, Nina Taft category:cs.LG cs.CY published:2013-11-26 summary:Recommender systems leverage user demographic information, such as age,gender, etc., to personalize recommendations and better place their targetedads. Oftentimes, users do not volunteer this information due to privacyconcerns, or due to a lack of initiative in filling out their online profiles.We illustrate a new threat in which a recommender learns private attributes ofusers who do not voluntarily disclose them. We design both passive and activeattacks that solicit ratings for strategically selected items, and could thusbe used by a recommender system to pursue this hidden agenda. Our methods arebased on a novel usage of Bayesian matrix factorization in an active learningsetting. Evaluations on multiple datasets illustrate that such attacks areindeed feasible and use significantly fewer rated items than static inferencemethods. Importantly, they succeed without sacrificing the quality ofrecommendations to users.
arxiv-5100-52 | Brains and pseudorandom generators | http://arxiv.org/pdf/1311.6531v3.pdf | author:VaÅ¡ek ChvÃ¡tal, Mark Goldsmith, Nan Yang category:math.DS cs.CR cs.NE math.NA published:2013-11-26 summary:In a pioneering classic, Warren McCulloch and Walter Pitts proposed a modelof the central nervous system; motivated by EEG recordings of normal brainactivity, Chv\' atal and Goldsmith asked whether or not this model can beengineered to provide pseudorandom number generators. We supply evidencesuggesting that the answer is negative.
arxiv-5100-53 | Semi-Supervised Sparse Coding | http://arxiv.org/pdf/1311.6834v2.pdf | author:Jim Jing-Yan Wang, Xin Gao category:stat.ML cs.LG published:2013-11-26 summary:Sparse coding approximates the data sample as a sparse linear combination ofsome basic codewords and uses the sparse codes as new presentations. In thispaper, we investigate learning discriminative sparse codes by sparse coding ina semi-supervised manner, where only a few training samples are labeled. Byusing the manifold structure spanned by the data set of both labeled andunlabeled samples and the constraints provided by the labels of the labeledsamples, we learn the variable class labels for all the samples. Furthermore,to improve the discriminative ability of the learned sparse codes, we assumethat the class labels could be predicted from the sparse codes directly using alinear classifier. By solving the codebook, sparse codes, class labels andclassifier parameters simultaneously in a unified objective function, wedevelop a semi-supervised sparse coding algorithm. Experiments on tworeal-world pattern recognition problems demonstrate the advantage of theproposed methods over supervised sparse coding methods on partially labeleddata sets.
arxiv-5100-54 | Learning Prices for Repeated Auctions with Strategic Buyers | http://arxiv.org/pdf/1311.6838v1.pdf | author:Kareem Amin, Afshin Rostamizadeh, Umar Syed category:cs.LG cs.GT published:2013-11-26 summary:Inspired by real-time ad exchanges for online display advertising, weconsider the problem of inferring a buyer's value distribution for a good whenthe buyer is repeatedly interacting with a seller through a posted-pricemechanism. We model the buyer as a strategic agent, whose goal is to maximizeher long-term surplus, and we are interested in mechanisms that maximize theseller's long-term revenue. We define the natural notion of strategic regret--- the lost revenue as measured against a truthful (non-strategic) buyer. Wepresent seller algorithms that are no-(strategic)-regret when the buyerdiscounts her future surplus --- i.e. the buyer prefers showing advertisementsto users sooner rather than later. We also give a lower bound on strategicregret that increases as the buyer's discounting weakens and shows, inparticular, that any seller algorithm will suffer linear strategic regret ifthere is no discounting.
arxiv-5100-55 | Score-based Causal Learning in Additive Noise Models | http://arxiv.org/pdf/1311.6359v3.pdf | author:Christopher Nowzohour, Peter BÃ¼hlmann category:stat.ML published:2013-11-25 summary:Given data sampled from a number of variables, one is often interested in theunderlying causal relationships in the form of a directed acyclic graph. In thegeneral case, without interventions on some of the variables it is onlypossible to identify the graph up to its Markov equivalence class. However, insome situations one can find the true causal graph just from observationaldata, for example in structural equation models with additive noise andnonlinear edge functions. Most current methods for achieving this rely onnonparametric independence tests. One of the problems there is that the nullhypothesis is independence, which is what one would like to get evidence for.We take a different approach in our work by using a penalized likelihood as ascore for model selection. This is practically feasible in many settings andhas the advantage of yielding a natural ranking of the candidate models. Whenmaking smoothness assumptions on the probability density space, we proveconsistency of the penalized maximum likelihood estimator. We also presentempirical results for simulated scenarios and real two-dimensional data sets(cause-effect pairs) where we obtain similar results as other state-of-the-artmethods.
arxiv-5100-56 | Exact post-selection inference, with application to the lasso | http://arxiv.org/pdf/1311.6238v8.pdf | author:Jason D. Lee, Dennis L. Sun, Yuekai Sun, Jonathan E. Taylor category:math.ST stat.ME stat.ML stat.TH published:2013-11-25 summary:We develop a general approach to valid inference after model selection. Atthe core of our framework is a result that characterizes the distribution of apost-selection estimator conditioned on the selection event. We specialize theapproach to model selection by the lasso to form valid confidence intervals forthe selected coefficients and test whether all relevant variables have beenincluded in the model.
arxiv-5100-57 | A Unified Approach to Universal Prediction: Generalized Upper and Lower Bounds | http://arxiv.org/pdf/1311.6396v2.pdf | author:N. Denizcan Vanli, Suleyman S. Kozat category:cs.LG published:2013-11-25 summary:We study sequential prediction of real-valued, arbitrary and unknownsequences under the squared error loss as well as the best parametric predictorout of a large, continuous class of predictors. Inspired by recent results fromcomputational learning theory, we refrain from any statistical assumptions anddefine the performance with respect to the class of general parametricpredictors. In particular, we present generic lower and upper bounds on thisrelative performance by transforming the prediction task into a parameterlearning problem. We first introduce the lower bounds on this relativeperformance in the mixture of experts framework, where we show that for anysequential algorithm, there always exists a sequence for which the performanceof the sequential algorithm is lower bounded by zero. We then introduce asequential learning algorithm to predict such arbitrary and unknown sequences,and calculate upper bounds on its total squared prediction error for everybounded sequence. We further show that in some scenarios we achieve matchinglower and upper bounds demonstrating that our algorithms are optimal in astrong minimax sense such that their performances cannot be improved further.As an interesting result we also prove that for the worst case scenario, theperformance of randomized algorithms can be achieved by sequential algorithmsso that randomized algorithms does not improve the performance.
arxiv-5100-58 | On Approximate Inference for Generalized Gaussian Process Models | http://arxiv.org/pdf/1311.6371v3.pdf | author:Lifeng Shang, Antoni B. Chan category:stat.ML cs.CV cs.LG published:2013-11-25 summary:A generalized Gaussian process model (GGPM) is a unifying framework thatencompasses many existing Gaussian process (GP) models, such as GP regression,classification, and counting. In the GGPM framework, the observation likelihoodof the GP model is itself parameterized using the exponential familydistribution (EFD). In this paper, we consider efficient algorithms forapproximate inference on GGPMs using the general form of the EFD. A particularGP model and its associated inference algorithms can then be formed by changingthe parameters of the EFD, thus greatly simplifying its creation fortask-specific output domains. We demonstrate the efficacy of this framework bycreating several new GP models for regressing to non-negative reals and to realintervals. We also consider a closed-form Taylor approximation for efficientinference on GGPMs, and elaborate on its connections with other model-specificheuristic closed-form approximations. Finally, we present a comprehensive setof experiments to compare approximate inference algorithms on a wide variety ofGGPMs.
arxiv-5100-59 | A Comprehensive Approach to Universal Piecewise Nonlinear Regression Based on Trees | http://arxiv.org/pdf/1311.6392v2.pdf | author:N. Denizcan Vanli, Suleyman S. Kozat category:cs.LG stat.ML published:2013-11-25 summary:In this paper, we investigate adaptive nonlinear regression and introducetree based piecewise linear regression algorithms that are highly efficient andprovide significantly improved performance with guaranteed upper bounds in anindividual sequence manner. We use a tree notion in order to partition thespace of regressors in a nested structure. The introduced algorithms adapt notonly their regression functions but also the complete tree structure whileachieving the performance of the "best" linear mixture of a doubly exponentialnumber of partitions, with a computational complexity only polynomial in thenumber of nodes of the tree. While constructing these algorithms, we also avoidusing any artificial "weighting" of models (with highly data dependentparameters) and, instead, directly minimize the final regression error, whichis the ultimate performance goal. The introduced methods are generic such thatthey can readily incorporate different tree construction methods such as randomtrees in their framework and can use different regressor or partitioningfunctions as demonstrated in the paper.
arxiv-5100-60 | Synchronous Context-Free Grammars and Optimal Linear Parsing Strategies | http://arxiv.org/pdf/1311.6421v1.pdf | author:Pierluigi Crescenzi, Daniel Gildea, Andrea Marino, Gianluca Rossi, Giorgio Satta category:cs.FL cs.CL published:2013-11-25 summary:Synchronous Context-Free Grammars (SCFGs), also known as syntax-directedtranslation schemata, are unlike context-free grammars in that they do not havea binary normal form. In general, parsing with SCFGs takes space and timepolynomial in the length of the input strings, but with the degree of thepolynomial depending on the permutations of the SCFG rules. We consider linearparsing strategies, which add one nonterminal at a time. We show that for agiven input permutation, the problems of finding the linear parsing strategywith the minimum space and time complexity are both NP-hard.
arxiv-5100-61 | Learning Reputation in an Authorship Network | http://arxiv.org/pdf/1311.6334v1.pdf | author:Charanpal Dhanjal, StÃ©phan ClÃ©menÃ§on category:cs.SI cs.IR cs.LG stat.ML published:2013-11-25 summary:The problem of searching for experts in a given academic field is hugelyimportant in both industry and academia. We study exactly this issue withrespect to a database of authors and their publications. The idea is to useLatent Semantic Indexing (LSI) and Latent Dirichlet Allocation (LDA) to performtopic modelling in order to find authors who have worked in a query field. Wethen construct a coauthorship graph and motivate the use of influencemaximisation and a variety of graph centrality measures to obtain a ranked listof experts. The ranked lists are further improved using a Markov Chain-basedrank aggregation approach. The complete method is readily scalable to largedatasets. To demonstrate the efficacy of the approach we report on an extensiveset of computational simulations using the Arnetminer dataset. An improvementin mean average precision is demonstrated over the baseline case of simplyusing the order of authors found by the topic models.
arxiv-5100-62 | Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching | http://arxiv.org/pdf/1311.6425v1.pdf | author:Marcelo Fiori, Pablo Sprechmann, Joshua Vogelstein, Pablo MusÃ©, Guillermo Sapiro category:math.OC cs.LG stat.ML published:2013-11-25 summary:Graph matching is a challenging problem with very important applications in awide range of fields, from image and video analysis to biological andbiomedical problems. We propose a robust graph matching algorithm inspired insparsity-related techniques. We cast the problem, resembling group orcollaborative sparsity formulations, as a non-smooth convex optimizationproblem that can be efficiently solved using augmented Lagrangian techniques.The method can deal with weighted or unweighted graphs, as well as multimodaldata, where different graphs represent different types of data. The proposedapproach is also naturally integrated with collaborative graph inferencetechniques, solving general network inference problems where the observedvariables, possibly coming from different modalities, are not incorrespondence. The algorithm is tested and compared with state-of-the-artgraph matching techniques in both synthetic and real graphs. We also presentresults on multimodal graphs and applications to collaborative inference ofbrain connectivity from alignment-free functional magnetic resonance imaging(fMRI) data. The code is publicly available.
arxiv-5100-63 | Are all training examples equally valuable? | http://arxiv.org/pdf/1311.6510v1.pdf | author:Agata Lapedriza, Hamed Pirsiavash, Zoya Bylinskii, Antonio Torralba category:cs.CV cs.LG stat.ML published:2013-11-25 summary:When learning a new concept, not all training examples may prove equallyuseful for training: some may have higher or lower training value than others.The goal of this paper is to bring to the attention of the vision community thefollowing considerations: (1) some examples are better than others for trainingdetectors or classifiers, and (2) in the presence of better examples, someexamples may negatively impact performance and removing them may be beneficial.In this paper, we propose an approach for measuring the training value of anexample, and use it for ranking and greedily sorting examples. We test ourmethods on different vision tasks, models, datasets and classifiers. Ourexperiments show that the performance of current state-of-the-art detectors andclassifiers can be improved when training on a subset, rather than the wholetraining set.
arxiv-5100-64 | Novelty Detection Under Multi-Instance Multi-Label Framework | http://arxiv.org/pdf/1311.6211v1.pdf | author:Qi Lou, Raviv Raich, Forrest Briggs, Xiaoli Z. Fern category:cs.LG published:2013-11-25 summary:Novelty detection plays an important role in machine learning and signalprocessing. This paper studies novelty detection in a new setting where thedata object is represented as a bag of instances and associated with multipleclass labels, referred to as multi-instance multi-label (MIML) learning.Contrary to the common assumption in MIML that each instance in a bag belongsto one of the known classes, in novelty detection, we focus on the scenariowhere bags may contain novel-class instances. The goal is to determine, for anygiven instance in a new bag, whether it belongs to a known class or a novelclass. Detecting novelty in the MIML setting captures many real-world phenomenaand has many potential applications. For example, in a collection of taggedimages, the tag may only cover a subset of objects existing in the images.Discovering an object whose class has not been previously tagged can be usefulfor the purpose of soliciting a label for the new object class. To address thisnovel problem, we present a discriminative framework for detecting new classinstances. Experiments demonstrate the effectiveness of our proposed method,and reveal that the presence of unlabeled novel instances in training bags ishelpful to the detection of such instances in testing stage.
arxiv-5100-65 | Off-policy reinforcement learning for $ H_\infty $ control design | http://arxiv.org/pdf/1311.6107v3.pdf | author:Biao Luo, Huai-Ning Wu, Tingwen Huang category:cs.SY cs.LG math.OC stat.ML published:2013-11-24 summary:The $H_\infty$ control design problem is considered for nonlinear systemswith unknown internal system model. It is known that the nonlinear $ H_\infty $control problem can be transformed into solving the so-calledHamilton-Jacobi-Isaacs (HJI) equation, which is a nonlinear partialdifferential equation that is generally impossible to be solved analytically.Even worse, model-based approaches cannot be used for approximately solving HJIequation, when the accurate system model is unavailable or costly to obtain inpractice. To overcome these difficulties, an off-policy reinforcement leaning(RL) method is introduced to learn the solution of HJI equation from realsystem data instead of mathematical system model, and its convergence isproved. In the off-policy RL method, the system data can be generated witharbitrary policies rather than the evaluating policy, which is extremelyimportant and promising for practical systems. For implementation purpose, aneural network (NN) based actor-critic structure is employed and a least-squareNN weight update algorithm is derived based on the method of weightedresiduals. Finally, the developed NN-based off-policy RL method is tested on alinear F16 aircraft plant, and further applied to a rotational/translationalactuator system.
arxiv-5100-66 | Bounding the Test Log-Likelihood of Generative Models | http://arxiv.org/pdf/1311.6184v4.pdf | author:Yoshua Bengio, Li Yao, Kyunghyun Cho category:cs.LG published:2013-11-24 summary:Several interesting generative learning algorithms involve a complexprobability distribution over many random variables, involving intractablenormalization constants or latent variable normalization. Some of them may evennot have an analytic expression for the unnormalized probability function andno tractable approximation. This makes it difficult to estimate the quality ofthese models, once they have been trained, or to monitor their quality (e.g.for early stopping) while training. A previously proposed method is based onconstructing a non-parametric density estimator of the model's probabilityfunction from samples generated by the model. We revisit this idea, propose amore efficient estimator, and prove that it provides a lower bound on the truetest log-likelihood, and an unbiased estimator as the number of generatedsamples goes to infinity, although one that incorporates the effect of poormixing. We further propose a biased variant of the estimator that can be usedreliably with a finite number of samples for the purpose of model comparison.
arxiv-5100-67 | A Primal-Dual Method for Training Recurrent Neural Networks Constrained by the Echo-State Property | http://arxiv.org/pdf/1311.6091v3.pdf | author:Jianshu Chen, Li Deng category:cs.LG cs.NE published:2013-11-24 summary:We present an architecture of a recurrent neural network (RNN) with afully-connected deep neural network (DNN) as its feature extractor. The RNN isequipped with both causal temporal prediction and non-causal look-ahead, viaauto-regression (AR) and moving-average (MA), respectively. The focus of thispaper is a primal-dual training method that formulates the learning of the RNNas a formal optimization problem with an inequality constraint that provides asufficient condition for the stability of the network dynamics. Experimentalresults demonstrate the effectiveness of this new method, which achieves 18.86%phone recognition error on the TIMIT benchmark for the core test set. Theresult approaches the best result of 17.7%, which was obtained by using RNNwith long short-term memory (LSTM). The results also show that the proposedprimal-dual training method produces lower recognition errors than the popularRNN methods developed earlier based on the carefully tuned threshold parameterthat heuristically prevents the gradient from exploding.
arxiv-5100-68 | Detection of Partially Visible Objects | http://arxiv.org/pdf/1311.6758v1.pdf | author:Patrick Ott, Mark Everingham, Jiri Matas category:cs.CV published:2013-11-24 summary:An "elephant in the room" for most current object detection and localizationmethods is the lack of explicit modelling of partial visibility due toocclusion by other objects or truncation by the image boundary. Based on asliding window approach, we propose a detection method which explicitly modelspartial visibility by treating it as a latent variable. A novel non-maximumsuppression scheme is proposed which takes into account the inferred partialvisibility of objects while providing a globally optimal solution. The methodgives more detailed scene interpretations than conventional detectors in thatwe are able to identify the visible parts of an object. We report improvedaverage precision on the PASCAL VOC 2010 dataset compared to a baselinedetector.
arxiv-5100-69 | Sparse CCA via Precision Adjusted Iterative Thresholding | http://arxiv.org/pdf/1311.6186v1.pdf | author:Mengjie Chen, Chao Gao, Zhao Ren, Harrison H. Zhou category:math.ST stat.ME stat.ML stat.TH published:2013-11-24 summary:Sparse Canonical Correlation Analysis (CCA) has received considerableattention in high-dimensional data analysis to study the relationship betweentwo sets of random variables. However, there has been remarkably littletheoretical statistical foundation on sparse CCA in high-dimensional settingsdespite active methodological and applied research activities. In this paper,we introduce an elementary sufficient and necessary characterization such thatthe solution of CCA is indeed sparse, propose a computationally efficientprocedure, called CAPIT, to estimate the canonical directions, and show thatthe procedure is rate-optimal under various assumptions on nuisance parameters.The procedure is applied to a breast cancer dataset from The Cancer GenomeAtlas project. We identify methylation probes that are associated with genes,which have been previously characterized as prognosis signatures of themetastasis of breast cancer.
arxiv-5100-70 | Local Similarities, Global Coding: An Algorithm for Feature Coding and its Applications | http://arxiv.org/pdf/1311.6079v2.pdf | author:Amirreza Shaban, Hamid R. Rabiee, Mahyar Najibi category:cs.CV cs.AI published:2013-11-24 summary:Data coding as a building block of several image processing algorithms hasbeen received great attention recently. Indeed, the importance of the localityassumption in coding approaches is studied in numerous works and severalmethods are proposed based on this concept. We probe this assumption and claimthat taking the similarity between a data point and a more global set of anchorpoints does not necessarily weaken the coding method as long as the underlyingstructure of the anchor points are taken into account. Based on this fact, wepropose to capture this underlying structure by assuming a random walker overthe anchor points. We show that our method is a fast approximate learningalgorithm based on the diffusion map kernel. The experiments on variousdatasets show that making different state-of-the-art coding algorithms aware ofthis structure boosts them in different learning tasks.
arxiv-5100-71 | Robust Low-rank Tensor Recovery: Models and Algorithms | http://arxiv.org/pdf/1311.6182v1.pdf | author:Donald Goldfarb, Zhiwei Qin category:stat.ML published:2013-11-24 summary:Robust tensor recovery plays an instrumental role in robustifying tensordecompositions for multilinear data analysis against outliers, grosscorruptions and missing values and has a diverse array of applications. In thispaper, we study the problem of robust low-rank tensor recovery in a convexoptimization framework, drawing upon recent advances in robust PrincipalComponent Analysis and tensor completion. We propose tailored optimizationalgorithms with global convergence guarantees for solving both the constrainedand the Lagrangian formulations of the problem. These algorithms are based onthe highly efficient alternating direction augmented Lagrangian and acceleratedproximal gradient methods. We also propose a nonconvex model that can oftenimprove the recovery results from the convex models. We investigate theempirical recoverability properties of the convex and nonconvex formulationsand compare the computational performance of the algorithms on simulated data.We demonstrate through a number of real applications the practicaleffectiveness of this convex optimization framework for robust low-rank tensorrecovery.
arxiv-5100-72 | No Free Lunch Theorem and Bayesian probability theory: two sides of the same coin. Some implications for black-box optimization and metaheuristics | http://arxiv.org/pdf/1311.6041v3.pdf | author:Loris Serafino category:cs.LG published:2013-11-23 summary:Challenging optimization problems, which elude acceptable solution viaconventional calculus methods, arise commonly in different areas of industrialdesign and practice. Hard optimization problems are those who manifest thefollowing behavior: a) high number of independent input variables; b) verycomplex or irregular multi-modal fitness; c) computational expensive fitnessevaluation. This paper will focus on some theoretical issues that have strongimplications for practice. I will stress how an interpretation of the No FreeLunch theorem leads naturally to a general Bayesian optimization framework. Thechoice of a prior over the space of functions is a critical and inevitable stepin every black-box optimization.
arxiv-5100-73 | Robust Vertex Classification | http://arxiv.org/pdf/1311.5954v2.pdf | author:Li Chen, Cencheng Shen, Joshua Vogelstein, Carey Priebe category:stat.ML published:2013-11-23 summary:For random graphs distributed according to stochastic blockmodels, a specialcase of latent position graphs, adjacency spectral embedding followed byappropriate vertex classification is asymptotically Bayes optimal; but thisapproach requires knowledge of and critically depends on the model dimension.In this paper, we propose a sparse representation vertex classifier which doesnot require information about the model dimension. This classifier represents atest vertex as a sparse combination of the vertices in the training set anduses the recovered coefficients to classify the test vertex. We proveconsistency of our proposed classifier for stochastic blockmodels, anddemonstrate that the sparse representation classifier can predict vertex labelswith higher accuracy than adjacency spectral embedding approaches via bothsimulation studies and real data experiments. Our results demonstrate therobustness and effectiveness of our proposed vertex classifier when the modeldimension is unknown.
arxiv-5100-74 | Fast Training of Effective Multi-class Boosting Using Coordinate Descent Optimization | http://arxiv.org/pdf/1311.5947v1.pdf | author:Guosheng Lin, Chunhua Shen, Anton van den Hengel, David Suter category:cs.CV cs.LG stat.CO published:2013-11-23 summary:Wepresentanovelcolumngenerationbasedboostingmethod for multi-classclassification. Our multi-class boosting is formulated in a single optimizationproblem as in Shen and Hao (2011). Different from most existing multi-classboosting methods, which use the same set of weak learners for all the classes,we train class specified weak learners (i.e., each class has a different set ofweak learners). We show that using separate weak learner sets for each classleads to fast convergence, without introducing additional computationaloverhead in the training procedure. To further make the training more efficientand scalable, we also propose a fast co- ordinate descent method for solvingthe optimization problem at each boosting iteration. The proposed coordinatedescent method is conceptually simple and easy to implement in that it is aclosed-form solution for each coordinate update. Experimental results on avariety of datasets show that, compared to a range of existing multi-classboosting meth- ods, the proposed method has much faster convergence rate andbetter generalization performance in most cases. We also empirically show thatthe proposed fast coordinate descent algorithm needs less training time thanthe MultiBoost algorithm in Shen and Hao (2011).
arxiv-5100-75 | Brain Tumor Detection Based On Symmetry Information | http://arxiv.org/pdf/1401.6127v1.pdf | author:Narkhede Sachin G, Vaishali Khairnar category:cs.CV published:2013-11-23 summary:Advances in computing technology have allowed researchers across many fieldsof endeavor to collect and maintain vast amounts of observational statisticaldata such as clinical data, biological patient data, data regarding access ofweb sites, financial data, and the like. This paper addresses some of thechallenging issues on brain magnetic resonance (MR) image tumor segmentationcaused by the weak correlation between magnetic resonance imaging (MRI)intensity and anatomical meaning. With the objective of utilizing moremeaningful information to improve brain tumor segmentation, an approach whichemploys bilateral symmetry information as an additional feature forsegmentation is proposed. This is motivated by potential performanceimprovement in the general automatic brain tumor segmentation systems which areimportant for many medical and scientific applications
arxiv-5100-76 | Dynamic Model of Facial Expression Recognition based on Eigen-face Approach | http://arxiv.org/pdf/1311.6007v1.pdf | author:Nikunj Bajaj, Aurobinda Routray, S L Happy category:cs.CV published:2013-11-23 summary:Emotions are best way of communicating information; and sometimes it carrymore information than words. Recently, there has been a huge interest inautomatic recognition of human emotion because of its wide spread applicationin security, surveillance, marketing, advertisement, and human-computerinteraction. To communicate with a computer in a natural way, it will bedesirable to use more natural modes of human communication based on voice,gestures and facial expressions. In this paper, a holistic approach for facialexpression recognition is proposed which captures the variation in facialfeatures in temporal domain and classifies the sequence of images in differentemotions. The proposed method uses Haar-like features to detect face in animage. The dimensionality of the eigenspace is reduced using PrincipalComponent Analysis (PCA). By projecting the subsequent face images intoprincipal eigen directions, the variation pattern of the obtained weight vectoris modeled to classify it into different emotions. Owing to the variations ofexpressions for different people and its intensity, a person specific methodfor emotion recognition is followed. Using the gray scale images of the frontalface, the system is able to classify four basic emotions such as happiness,sadness, surprise, and anger.
arxiv-5100-77 | Skin Texture Recognition Using Neural Networks | http://arxiv.org/pdf/1311.6049v1.pdf | author:Nidhal K. El Abbadi, Nazar Dahir, Zaid Abd Alkareem category:cs.CV published:2013-11-23 summary:Skin recognition is used in many applications ranging from algorithms forface detection, hand gesture analysis, and to objectionable image filtering. Inthis work a skin recognition system was developed and tested. While many skinsegmentation algorithms relay on skin color, our work relies on both skin colorand texture features (features derives from the GLCM) to give a better and moreefficient recognition accuracy of skin textures. We used feed forward neuralnetworks to classify input textures images to be skin or non skin textures. Thesystem gave very encouraging results during the neural network generalizationface.
arxiv-5100-78 | Build Electronic Arabic Lexicon | http://arxiv.org/pdf/1311.6045v1.pdf | author:Nidhal El-Abbadi, Ahmed Nidhal Khdhair, Adel Al-Nasrawi category:cs.CL published:2013-11-23 summary:There are many known Arabic lexicons organized on different ways, each ofthem has a different number of Arabic words according to its organization way.This paper has used mathematical relations to count a number of Arabic words,which proofs the number of Arabic words presented by Al Farahidy. The paperalso presents new way to build an electronic Arabic lexicon by using a hashfunction that converts each word (as input) to correspond a unique integernumber (as output), these integer numbers will be used as an index to a lexiconentry.
arxiv-5100-79 | On the Design and Analysis of Multiple View Descriptors | http://arxiv.org/pdf/1311.6048v1.pdf | author:Jingming Dong, Jonathan Balzer, Damek Davis, Joshua Hernandez, Stefano Soatto category:cs.CV published:2013-11-23 summary:We propose an extension of popular descriptors based on gradient orientationhistograms (HOG, computed in a single image) to multiple views. It hinges oninterpreting HOG as a conditional density in the space of sampled images, wherethe effects of nuisance factors such as viewpoint and illumination aremarginalized. However, such marginalization is performed with respect to a verycoarse approximation of the underlying distribution. Our extension leverages onthe fact that multiple views of the same scene allow separating intrinsic fromnuisance variability, and thus afford better marginalization of the latter. Theresult is a descriptor that has the same complexity of single-view HOG, and canbe compared in the same manner, but exploits multiple views to better trade offinsensitivity to nuisance variability with specificity to intrinsicvariability. We also introduce a novel multi-view wide-baseline matchingdataset, consisting of a mixture of real and synthetic objects with groundtruthed camera motion and dense three-dimensional geometry.
arxiv-5100-80 | A Short Introduction to NILE | http://arxiv.org/pdf/1311.6063v4.pdf | author:Sheng Yu, Tianxi Cai category:cs.CL published:2013-11-23 summary:In this paper, we briefly introduce the Narrative Information LinearExtraction (NILE) system, a natural language processing library for clinicalnarratives. NILE is an experiment of our ideas on efficient and effectivemedical language processing. We introduce the overall design of NILE and itsmajor components, and show the performance of it in real projects.
arxiv-5100-81 | Finding sparse solutions of systems of polynomial equations via group-sparsity optimization | http://arxiv.org/pdf/1311.5871v2.pdf | author:Fabien Lauer, Henrik Ohlsson category:cs.IT cs.LG math.IT math.OC stat.ML published:2013-11-22 summary:The paper deals with the problem of finding sparse solutions to systems ofpolynomial equations possibly perturbed by noise. In particular, we show howthese solutions can be recovered from group-sparse solutions of a derivedsystem of linear equations. Then, two approaches are considered to find thesegroup-sparse solutions. The first one is based on a convex relaxation resultingin a second-order cone programming formulation which can benefit from efficientreweighting techniques for sparsity enhancement. For this approach, sufficientconditions for the exact recovery of the sparsest solution to the polynomialsystem are derived in the noiseless setting, while stable recovery results areobtained for the noisy case. Though lacking a similar analysis, the secondapproach provides a more computationally efficient algorithm based on a greedystrategy adding the groups one-by-one. With respect to previous work, theproposed methods recover the sparsest solution in a very short computing timewhile remaining at least as accurate in terms of the probability of success.This probability is empirically analyzed to emphasize the relationship betweenthe ability of the methods to solve the polynomial system and the sparsity ofthe solution.
arxiv-5100-82 | Gradient Hard Thresholding Pursuit for Sparsity-Constrained Optimization | http://arxiv.org/pdf/1311.5750v2.pdf | author:Xiao-Tong Yuan, Ping Li, Tong Zhang category:cs.LG cs.NA stat.ML published:2013-11-22 summary:Hard Thresholding Pursuit (HTP) is an iterative greedy selection procedurefor finding sparse solutions of underdetermined linear systems. This method hasbeen shown to have strong theoretical guarantee and impressive numericalperformance. In this paper, we generalize HTP from compressive sensing to ageneric problem setup of sparsity-constrained convex optimization. The proposedalgorithm iterates between a standard gradient descent step and a hardthresholding step with or without debiasing. We prove that our method enjoysthe strong guarantees analogous to HTP in terms of rate of convergence andparameter estimation accuracy. Numerical evidences show that our method issuperior to the state-of-the-art greedy selection methods in sparse logisticregression and sparse precision matrix estimation tasks.
arxiv-5100-83 | Learning Non-Linear Feature Maps | http://arxiv.org/pdf/1311.5636v1.pdf | author:Dimitrios Athanasakis, John Shawe-Taylor, Delmiro Fernandez-Reyes category:cs.LG published:2013-11-22 summary:Feature selection plays a pivotal role in learning, particularly in areaswere parsimonious features can provide insight into the underlying process,such as biology. Recent approaches for non-linear feature selection employinggreedy optimisation of Centred Kernel Target Alignment(KTA), while exhibitingstrong results in terms of generalisation accuracy and sparsity, can becomecomputationally prohibitive for high-dimensional datasets. We propose randSel,a randomised feature selection algorithm, with attractive scaling properties.Our theoretical analysis of randSel provides strong probabilistic guaranteesfor the correct identification of relevant features. Experimental results onreal and artificial data, show that the method successfully identifieseffective features, performing better than a number of competitive approaches.
arxiv-5100-84 | Automated and Weighted Self-Organizing Time Maps | http://arxiv.org/pdf/1311.5763v1.pdf | author:Peter Sarlin category:cs.NE cs.HC published:2013-11-22 summary:This paper proposes schemes for automated and weighted Self-Organizing TimeMaps (SOTMs). The SOTM provides means for a visual approach to evolutionaryclustering, which aims at producing a sequence of clustering solutions. Thistask we denote as visual dynamic clustering. The implication of an automatedSOTM is not only a data-driven parametrization of the SOTM, but also thefeature of adjusting the training to the characteristics of the data at eachtime step. The aim of the weighted SOTM is to improve learning from moretrustworthy or important data with an instance-varying weight. The schemes forautomated and weighted SOTMs are illustrated on two real-world datasets: (i)country-level risk indicators to measure the evolution of global imbalances,and (ii) credit applicant data to measure the evolution of firm-level creditrisks.
arxiv-5100-85 | Automatic Ranking of MT Outputs using Approximations | http://arxiv.org/pdf/1311.5836v1.pdf | author:Pooja Gupta, Nisheeth Joshi, Iti Mathur category:cs.CL published:2013-11-22 summary:Since long, research on machine translation has been ongoing. Still, we donot get good translations from MT engines so developed. Manual ranking of theseoutputs tends to be very time consuming and expensive. Identifying which one isbetter or worse than the others is a very taxing task. In this paper, we showan approach which can provide automatic ranks to MT outputs (translations)taken from different MT Engines and which is based on N-gram approximations. Weprovide a solution where no human intervention is required for ranking systems.Further we also show the evaluations of our results which show equivalentresults as that of human ranking.
arxiv-5100-86 | Dictionary-Learning-Based Reconstruction Method for Electron Tomography | http://arxiv.org/pdf/1311.5830v1.pdf | author:Baodong Liu, Hengyong Yu, Scott S. Verbridge, Lizhi Sun, Ge Wang category:cs.CV physics.med-ph published:2013-11-22 summary:Electron tomography usually suffers from so called missing wedge artifactscaused by limited tilt angle range. An equally sloped tomography (EST)acquisition scheme (which should be called the linogram sampling scheme) wasrecently applied to achieve 2.4-angstrom resolution. On the other hand, acompressive sensing-inspired reconstruction algorithm, known as adaptivedictionary based statistical iterative reconstruction (ADSIR), has beenreported for x-ray computed tomography. In this paper, we evaluate the EST,ADSIR and an ordered-subset simultaneous algebraic reconstruction technique(OS-SART), and compare the ES and equally angled (EA) data acquisition modes.Our results show that OS-SART is comparable to EST, and the ADSIR outperformsEST and OS-SART. Furthermore, the equally sloped projection data acquisitionmode has no advantage over the conventional equally angled mode in the context.
arxiv-5100-87 | A Unified SVM Framework for Signal Estimation | http://arxiv.org/pdf/1311.5406v1.pdf | author:JosÃ© Luis Rojo-Ãlvarez, Manel MartÃ­nez-RamÃ³n, Jordi MuÃ±oz-MarÃ­, Gustavo Camps-Valls category:stat.ML stat.AP published:2013-11-21 summary:This paper presents a unified framework to tackle estimation problems inDigital Signal Processing (DSP) using Support Vector Machines (SVMs). The useof SVMs in estimation problems has been traditionally limited to its mere useas a black-box model. Noting such limitations in the literature, we takeadvantage of several properties of Mercer's kernels and functional analysis todevelop a family of SVM methods for estimation in DSP. Three types of signalmodel equations are analyzed. First, when a specific time-signal structure isassumed to model the underlying system that generated the data, the linearsignal model (so called Primal Signal Model formulation) is first stated andanalyzed. Then, non-linear versions of the signal structure can be readilydeveloped by following two different approaches. On the one hand, the signalmodel equation is written in reproducing kernel Hilbert spaces (RKHS) using thewell-known RKHS Signal Model formulation, and Mercer's kernels are readily usedin SVM non-linear algorithms. On the other hand, in the alternative and not socommon Dual Signal Model formulation, a signal expansion is made by using anauxiliary signal model equation given by a non-linear regression of each timeinstant in the observed time series. These building blocks can be used togenerate different novel SVM-based methods for problems of signal estimation,and we deal with several of the most important ones in DSP. We illustrate theusefulness of this methodology by defining SVM algorithms for linear andnon-linear system identification, spectral analysis, nonuniform interpolation,sparse deconvolution, and array processing. The performance of the developedSVM methods is compared to standard approaches in all these settings. Theexperimental results illustrate the generality, simplicity, and capabilities ofthe proposed SVM framework for DSP.
arxiv-5100-88 | Adaptive Learning of Region-based pLSA Model for Total Scene Annotation | http://arxiv.org/pdf/1311.5590v1.pdf | author:Yuzhu Zhou, Le Li, Honggang Zhang category:cs.CV published:2013-11-21 summary:In this paper, we present a region-based pLSA model to accomplish the task oftotal scene annotation. To be more specific, we not only properly generate alist of tags for each image, but also localizing each region with itscorresponding tag. We integrate advantages of different existing region-basedworks: employ efficient and powerful JSEG algorithm for segmentation so thateach region can easily express meaningful object information; the introductionof pLSA model can help better capturing semantic information behind thelow-level features. Moreover, we also propose an adaptive padding mechanism toautomatically choose the optimal padding strategy for each region, whichdirectly increases the overall system performance. Finally we conduct 3experiments to verify our ideas on Corel database and demonstrate theeffectiveness and accuracy of our system.
arxiv-5100-89 | PANDA: Pose Aligned Networks for Deep Attribute Modeling | http://arxiv.org/pdf/1311.5591v2.pdf | author:Ning Zhang, Manohar Paluri, Marc'Aurelio Ranzato, Trevor Darrell, Lubomir Bourdev category:cs.CV published:2013-11-21 summary:We propose a method for inferring human attributes (such as gender, hairstyle, clothes style, expression, action) from images of people under largevariation of viewpoint, pose, appearance, articulation and occlusion.Convolutional Neural Nets (CNN) have been shown to perform very well on largescale object recognition problems. In the context of attribute classification,however, the signal is often subtle and it may cover only a small part of theimage, while the image is dominated by the effects of pose and viewpoint.Discounting for pose variation would require training on very large labeleddatasets which are not presently available. Part-based models, such as poseletsand DPM have been shown to perform well for this problem but they are limitedby shallow low-level features. We propose a new method which combinespart-based models and deep learning by training pose-normalized CNNs. We showsubstantial improvement vs. state-of-the-art methods on challenging attributeclassification tasks in unconstrained settings. Experiments confirm that ourmethod outperforms both the best part-based methods on this problem andconventional CNNs trained on the full bounding box of the person.
arxiv-5100-90 | Clustering and Relational Ambiguity: from Text Data to Natural Data | http://arxiv.org/pdf/1311.5401v2.pdf | author:Nicolas Turenne category:cs.CL cs.IR published:2013-11-21 summary:Text data is often seen as "take-away" materials with little noise and easyto process information. Main questions are how to get data and transform theminto a good document format. But data can be sensitive to noise oftenly calledambiguities. Ambiguities are aware from a long time, mainly because polysemy isobvious in language and context is required to remove uncertainty. I claim inthis paper that syntactic context is not suffisant to improve interpretation.In this paper I try to explain that firstly noise can come from natural datathemselves, even involving high technology, secondly texts, seen as verifiedbut meaningless, can spoil content of a corpus; it may lead to contradictionsand background noise.
arxiv-5100-91 | Bayesian Discovery of Threat Networks | http://arxiv.org/pdf/1311.5552v3.pdf | author:Steven T. Smith, Edward K. Kao, Kenneth D. Senne, Garrett Bernstein, Scott Philips category:cs.SI cs.LG math.ST physics.soc-ph stat.ML stat.TH published:2013-11-21 summary:A novel unified Bayesian framework for network detection is developed, underwhich a detection algorithm is derived based on random walks on graphs. Thealgorithm detects threat networks using partial observations of their activity,and is proved to be optimum in the Neyman-Pearson sense. The algorithm isdefined by a graph, at least one observation, and a diffusion model for threat.A link to well-known spectral detection methods is provided, and theequivalence of the random walk and harmonic solutions to the Bayesianformulation is proven. A general diffusion model is introduced that utilizesspatio-temporal relationships between vertices, and is used for a specificspace-time formulation that leads to significant performance improvements oncoordinated covert networks. This performance is demonstrated using a newhybrid mixed-membership blockmodel introduced to simulate random covertnetworks with realistic properties.
arxiv-5100-92 | Texture descriptor combining fractal dimension and artificial crawlers | http://arxiv.org/pdf/1311.5290v1.pdf | author:Wesley Nunes GonÃ§alves, Bruno Brandoli Machado, Odemir Martinez Bruno category:cs.CV published:2013-11-21 summary:Texture is an important visual attribute used to describe images. There aremany methods available for texture analysis. However, they do not capture thedetails richness of the image surface. In this paper, we propose a new methodto describe textures using the artificial crawler model. This model assumesthat each agent can interact with the environment and each other. Since thisswarm system alone does not achieve a good discrimination, we developed a newmethod to increase the discriminatory power of artificial crawlers, togetherwith the fractal dimension theory. Here, we estimated the fractal dimension bythe Bouligand-Minkowski method due to its precision in quantifying structuralproperties of images. We validate our method on two texture datasets and theexperimental results reveal that our method leads to highly discriminativetextural features. The results indicate that our method can be used indifferent texture applications.
arxiv-5100-93 | Compressive Measurement Designs for Estimating Structured Signals in Structured Clutter: A Bayesian Experimental Design Approach | http://arxiv.org/pdf/1311.5599v1.pdf | author:Swayambhoo Jain, Akshay Soni, Jarvis Haupt category:stat.ML cs.LG published:2013-11-21 summary:This work considers an estimation task in compressive sensing, where the goalis to estimate an unknown signal from compressive measurements that arecorrupted by additive pre-measurement noise (interference, or clutter) as wellas post-measurement noise, in the specific setting where some (perhaps limited)prior knowledge on the signal, interference, and noise is available. Thespecific aim here is to devise a strategy for incorporating this priorinformation into the design of an appropriate compressive measurement strategy.Here, the prior information is interpreted as statistics of a priordistribution on the relevant quantities, and an approach based on BayesianExperimental Design is proposed. Experimental results on synthetic datademonstrate that the proposed approach outperforms traditional randomcompressive measurement designs, which are agnostic to the prior information,as well as several other knowledge-enhanced sensing matrix designs based onmore heuristic notions.
arxiv-5100-94 | Learning Pairwise Graphical Models with Nonlinear Sufficient Statistics | http://arxiv.org/pdf/1311.5479v2.pdf | author:Xiao-Tong Yuan, Ping Li, Tong Zhang category:stat.ML published:2013-11-21 summary:We investigate a generic problem of learning pairwise exponential familygraphical models with pairwise sufficient statistics defined by a globalmapping function, e.g., Mercer kernels. This subclass of pairwise graphicalmodels allow us to flexibly capture complex interactions among variables beyondpairwise product. We propose two $\ell_1$-norm penalized maximum likelihoodestimators to learn the model parameters from i.i.d. samples. The first one isa joint estimator which estimates all the parameters simultaneously. The secondone is a node-wise conditional estimator which estimates the parametersindividually for each node. For both estimators, we show that under properconditions the extra flexibility gained in our model comes at almost no cost ofstatistical and computational efficiency. We demonstrate the advantages of ourmodel over state-of-the-art methods on synthetic and real datasets.
arxiv-5100-95 | Neural Network Application on Foliage Plant Identification | http://arxiv.org/pdf/1311.5829v1.pdf | author:Abdul Kadir, Lukito Edi Nugroho, Adhi Susanto, Paulus Insap Santosa category:cs.CV cs.NE published:2013-11-20 summary:Several researches in leaf identification did not include color informationas features. The main reason is caused by a fact that they used green coloredleaves as samples. However, for foliage plants, plants with colorful leaves,fancy patterns in their leaves, and interesting plants with unique shape, colorand also texture could not be neglected. For example, Epipremnum pinnatum'Aureum' and Epipremnum pinnatum 'Marble Queen' have similar patterns, sameshape, but different colors. Combination of shape, color, texture features, andother attribute contained on the leaf is very useful in leaf identification. Inthis research, Polar Fourier Transform and three kinds of geometric featureswere used to represent shape features, color moments that consist of mean,standard deviation, skewness were used to represent color features, texturefeatures are extracted from GLCMs, and vein features were added to improveperformance of the identification system. The identification system usesProbabilistic Neural Network (PNN) as a classifier. The result shows that thesystem gives average accuracy of 93.0833% for 60 kinds of foliage plants.
arxiv-5100-96 | Leaf Classification Using Shape, Color, and Texture Features | http://arxiv.org/pdf/1401.4447v1.pdf | author:Abdul Kadir, Lukito Edi Nugroho, Adhi Susanto, Paulus Insap Santosa category:cs.CV cs.CY published:2013-11-20 summary:Several methods to identify plants have been proposed by several researchers.Commonly, the methods did not capture color information, because color was notrecognized as an important aspect to the identification. In this research,shape and vein, color, and texture features were incorporated to classify aleaf. In this case, a neural network called Probabilistic Neural network (PNN)was used as a classifier. The experimental result shows that the method forclassification gives average accuracy of 93.75% when it was tested on Flaviadataset, that contains 32 kinds of plant leaves. It means that the method givesbetter performance compared to the original work.
arxiv-5100-97 | Comparative Study Of Image Edge Detection Algorithms | http://arxiv.org/pdf/1311.4963v2.pdf | author:Shubham Saini, Bhavesh Kasliwal, Shraey Bhatia category:cs.CV published:2013-11-20 summary:Since edge detection is in the forefront of image processing for objectdetection, it is crucial to have a good understanding of edge detectionalgorithms. The reason for this is that edges form the outline of an object. Anedge is the boundary between an object and the background, and indicates theboundary between overlapping objects. This means that if the edges in an imagecan be identified accurately, all of the objects can be located and basicproperties such as area, perimeter, and shape can be measured. Since computervision involves the identification and classification of objects in an image,edge detection is an essential tool. We tested two edge detectors that usedifferent methods for detecting edges and compared their results under avariety of situations to determine which detector was preferable underdifferent sets of conditions.
arxiv-5100-98 | Experiments of Distance Measurements in a Foliage Plant Retrieval System | http://arxiv.org/pdf/1401.3584v1.pdf | author:Abdul Kadir, Lukito Edi Nugroho, Adhi Susanto, Paulus Insap Santosa category:cs.CV published:2013-11-20 summary:One of important components in an image retrieval system is selecting adistance measure to compute rank between two objects. In this paper, severaldistance measures were researched to implement a foliage plant retrievalsystem. Sixty kinds of foliage plants with various leaf color and shape wereused to test the performance of 7 different kinds of distance measures: cityblock distance, Euclidean distance, Canberra distance, Bray-Curtis distance, x2statistics, Jensen Shannon divergence and Kullback Leibler divergence. Theresults show that city block and Euclidean distance measures gave the bestperformance among the others.
arxiv-5100-99 | Extended Formulations for Online Linear Bandit Optimization | http://arxiv.org/pdf/1311.5022v3.pdf | author:Shaona Ghosh, Adam Prugel-Bennett category:cs.LG cs.DS published:2013-11-20 summary:On-line linear optimization on combinatorial action sets (d-dimensionalactions) with bandit feedback, is known to have complexity in the order of thedimension of the problem. The exponential weighted strategy achieves the bestknown regret bound that is of the order of $d^{2}\sqrt{n}$ (where $d$ is thedimension of the problem, $n$ is the time horizon). However, such strategiesare provably suboptimal or computationally inefficient. The complexity isattributed to the combinatorial structure of the action set and the dearth ofefficient exploration strategies of the set. Mirror descent with entropicregularization function comes close to solving this problem by enforcing ameticulous projection of weights with an inherent boundary condition. Entropicregularization in mirror descent is the only known way of achieving alogarithmic dependence on the dimension. Here, we argue otherwise and recoverthe original intuition of exponential weighting by borrowing a technique fromdiscrete optimization and approximation algorithms called `extendedformulation'. Such formulations appeal to the underlying geometry of the setwith a guaranteed logarithmic dependence on the dimension underpinned by aninformation theoretic entropic analysis.
arxiv-5100-100 | Gromov-Hausdorff stability of linkage-based hierarchical clustering methods | http://arxiv.org/pdf/1311.5068v1.pdf | author:A. MartÃ­nez-PÃ©rez category:cs.LG published:2013-11-20 summary:A hierarchical clustering method is stable if small perturbations on the dataset produce small perturbations in the result. These perturbations are measuredusing the Gromov-Hausdorff metric. We study the problem of stability onlinkage-based hierarchical clustering methods. We obtain that, under some basicconditions, standard linkage-based methods are semi-stable. This means thatthey are stable if the input data is close enough to an ultrametric space. Weprove that, apart from exotic examples, introducing any unchaining condition inthe algorithm always produces unstable methods.
arxiv-5100-101 | Analyzing Evolutionary Optimization in Noisy Environments | http://arxiv.org/pdf/1311.4987v1.pdf | author:Chao Qian, Yang Yu, Zhi-Hua Zhou category:cs.AI cs.NE published:2013-11-20 summary:Many optimization tasks have to be handled in noisy environments, where wecannot obtain the exact evaluation of a solution but only a noisy one. Fornoisy optimization tasks, evolutionary algorithms (EAs), a kind of stochasticmetaheuristic search algorithm, have been widely and successfully applied.Previous work mainly focuses on empirical studying and designing EAs for noisyoptimization, while, the theoretical counterpart has been little investigated.In this paper, we investigate a largely ignored question, i.e., whether anoptimization problem will always become harder for EAs in a noisy environment.We prove that the answer is negative, with respect to the measurement of theexpected running time. The result implies that, for optimization tasks thathave already been quite hard to solve, the noise may not have a negativeeffect, and the easier a task the more negatively affected by the noise. On arepresentative problem where the noise has a strong negative effect, we examinetwo commonly employed mechanisms in EAs dealing with noise, the re-evaluationand the threshold selection strategies. The analysis discloses that the twostrategies, however, both are not effective, i.e., they do not make the EA morenoise tolerant. We then find that a small modification of the thresholdselection allows it to be proven as an effective strategy for dealing with thenoise in the problem.
arxiv-5100-102 | Robust Compressed Sensing Under Matrix Uncertainties | http://arxiv.org/pdf/1311.4924v4.pdf | author:Yipeng Liu category:cs.IT cs.CV math.IT math.RT stat.AP stat.ML published:2013-11-20 summary:Compressed sensing (CS) shows that a signal having a sparse or compressiblerepresentation can be recovered from a small set of linear measurements. Inclassical CS theory, the sampling matrix and representation matrix are assumedto be known exactly in advance. However, uncertainties exist due to samplingdistortion, finite grids of the parameter space of dictionary, etc. In thispaper, we take a generalized sparse signal model, which simultaneouslyconsiders the sampling and representation matrix uncertainties. Based on thenew signal model, a new optimization model for robust sparse signalreconstruction is proposed. This optimization model can be deduced withstochastic robust approximation analysis. Both convex relaxation and greedyalgorithms are used to solve the optimization problem. For the convexrelaxation method, a sufficient condition for recovery by convex relaxation isgiven; For the greedy algorithm, it is realized by the introduction of apre-processing of the sensing matrix and the measurements. In numericalexperiments, both simulated data and real-life ECG data based results show thatthe proposed method has a better performance than the current methods.
arxiv-5100-103 | Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis | http://arxiv.org/pdf/1311.5422v2.pdf | author:Nikhil Rao, Christopher Cox, Robert Nowak, Timothy Rogers category:cs.LG stat.ML published:2013-11-20 summary:Multitask learning can be effective when features useful in one task are alsouseful for other tasks, and the group lasso is a standard method for selectinga common subset of features. In this paper, we are interested in a lessrestrictive form of multitask learning, wherein (1) the available features canbe organized into subsets according to a notion of similarity and (2) featuresuseful in one task are similar, but not necessarily identical, to the featuresbest suited for other tasks. The main contribution of this paper is a newprocedure called Sparse Overlapping Sets (SOS) lasso, a convex optimizationthat automatically selects similar features for related learning tasks. Errorbounds are derived for SOSlasso and its consistency is established for squarederror loss. In particular, SOSlasso is motivated by multi- subject fMRI studiesin which functional activity is classified using brain voxels as features.Experiments with real and synthetic data demonstrate the advantages of SOSlassocompared to the lasso and group lasso.
arxiv-5100-104 | Sparse PCA via Covariance Thresholding | http://arxiv.org/pdf/1311.5179v5.pdf | author:Yash Deshpande, Andrea Montanari category:math.ST stat.ML stat.TH published:2013-11-20 summary:In sparse principal component analysis we are given noisy observations of alow-rank matrix of dimension $n\times p$ and seek to reconstruct it underadditional sparsity assumptions. In particular, we assume here each of theprincipal components $\mathbf{v}_1,\dots,\mathbf{v}_r$ has at most $s_0$non-zero entries. We are particularly interested in the high dimensional regimewherein $p$ is comparable to, or even much larger than $n$. In an influentialpaper, \cite{johnstone2004sparse} introduced a simple algorithm that estimatesthe support of the principal vectors $\mathbf{v}_1,\dots,\mathbf{v}_r$ by thelargest entries in the diagonal of the empirical covariance. This method can beshown to identify the correct support with high probability if $s_0\leK_1\sqrt{n/\log p}$, and to fail with high probability if $s_0\ge K_2\sqrt{n/\log p}$ for two constants $0<K_1,K_2<\infty$. Despite a considerableamount of work over the last ten years, no practical algorithm exists withprovably better support recovery guarantees. Here we analyze a covariance thresholding algorithm that was recentlyproposed by \cite{KrauthgamerSPCA}. On the basis of numerical simulations (forthe rank-one case), these authors conjectured that covariance thresholdingcorrectly recover the support with high probability for $s_0\le K\sqrt{n}$(assuming $n$ of the same order as $p$). We prove this conjecture, and in factestablish a more general guarantee including higher-rank as well as $n$ muchsmaller than $p$. Recent lower bounds \cite{berthet2013computational,ma2015sum} suggest that no polynomial time algorithm can do significantlybetter. The key technical component of our analysis develops new bounds on thenorm of kernel random matrices, in regimes that were not considered before.
arxiv-5100-105 | Complexity measurement of natural and artificial languages | http://arxiv.org/pdf/1311.5427v2.pdf | author:Gerardo Febres, Klaus Jaffe, Carlos Gershenson category:cs.CL cs.IT math.IT nlin.AO physics.soc-ph published:2013-11-20 summary:We compared entropy for texts written in natural languages (English, Spanish)and artificial languages (computer software) based on a simple expression forthe entropy as a function of message length and specific word diversity. Codetext written in artificial languages showed higher entropy than text of similarlength expressed in natural languages. Spanish texts exhibit more symbolicdiversity than English ones. Results showed that algorithms based on complexitymeasures differentiate artificial from natural languages, and that textanalysis based on complexity measures allows the unveiling of important aspectsof their nature. We propose specific expressions to examine entropy relatedaspects of tests and estimate the values of entropy, emergence,self-organization and complexity based on specific diversity and messagelength.
arxiv-5100-106 | Gaussian Process Optimization with Mutual Information | http://arxiv.org/pdf/1311.4825v3.pdf | author:Emile Contal, Vianney Perchet, Nicolas Vayatis category:stat.ML cs.LG published:2013-11-19 summary:In this paper, we analyze a generic algorithm scheme for sequential globaloptimization using Gaussian processes. The upper bounds we derive on thecumulative regret for this generic algorithm improve by an exponential factorthe previously known bounds for algorithms like GP-UCB. We also introduce thenovel Gaussian Process Mutual Information algorithm (GP-MI), whichsignificantly improves further these upper bounds for the cumulative regret. Weconfirm the efficiency of this algorithm on synthetic and real tasks againstthe natural competitor, GP-UCB, and also the Expected Improvement heuristic.
arxiv-5100-107 | Beating the Minimax Rate of Active Learning with Prior Knowledge | http://arxiv.org/pdf/1311.4803v2.pdf | author:Lijun Zhang, Mehrdad Mahdavi, Rong Jin category:cs.LG stat.ML published:2013-11-19 summary:Active learning refers to the learning protocol where the learner is allowedto choose a subset of instances for labeling. Previous studies have shown that,compared with passive learning, active learning is able to reduce the labelcomplexity exponentially if the data are linearly separable or satisfy theTsybakov noise condition with parameter $\kappa=1$. In this paper, we propose anovel active learning algorithm using a convex surrogate loss, with the goal tobroaden the cases for which active learning achieves an exponentialimprovement. We make use of a convex loss not only because it reduces thecomputational cost, but more importantly because it leads to a tight bound forthe empirical process (i.e., the difference between the empirical estimationand the expectation) when the current solution is close to the optimal one.Under the assumption that the norm of the optimal classifier that minimizes theconvex risk is available, our analysis shows that the introduction of theconvex surrogate loss yields an exponential reduction in the label complexityeven when the parameter $\kappa$ of the Tsybakov noise is larger than $1$. Tothe best of our knowledge, this is the first work that improves the minimaxrate of active learning by utilizing certain priori knowledge.
arxiv-5100-108 | Face Verification Using Kernel Principle Component Analysis | http://arxiv.org/pdf/1401.6108v1.pdf | author:V. Karthikeyan, Manjupriya, C. K. Chithra, M. Divya category:cs.CV published:2013-11-19 summary:In the beginning stage, face verification is done using easy method ofgeometric algorithm models, but the verification route has now developed into ascientific progress of complicated geometric representation and matchingprocess. In modern time the skill have enhanced face detection system into thevigorous focal point. Researchers currently undergoing strong research onfinding face recognition system for wider area information taken underhysterical elucidation dissimilarity. The proposed face recognition systemconsists of a narrative exposition indiscreet preprocessing method, a hybridFourier-based facial feature extraction and a score fusion scheme. We take inconventional the face detection in unlike cheer up circumstances and at unusualsetting. Image processing, Image detection, Feature removal and Face detectionare the methods used for Face Verification System . This paper focuses mainlyon the issue of toughness to lighting variations. The proposed system hasobtained an average of verification rate on Two-Dimensional images underdifferent lightening conditions.
arxiv-5100-109 | Post-Proceedings of the First International Workshop on Learning and Nonmonotonic Reasoning | http://arxiv.org/pdf/1311.4639v1.pdf | author:Katsumi Inoue, Chiaki Sakama category:cs.AI cs.LG cs.LO published:2013-11-19 summary:Knowledge Representation and Reasoning and Machine Learning are two importantfields in AI. Nonmonotonic logic programming (NMLP) and Answer Set Programming(ASP) provide formal languages for representing and reasoning with commonsenseknowledge and realize declarative problem solving in AI. On the other side,Inductive Logic Programming (ILP) realizes Machine Learning in logicprogramming, which provides a formal background to inductive learning and thetechniques have been applied to the fields of relational learning and datamining. Generally speaking, NMLP and ASP realize nonmonotonic reasoning whilelack the ability of learning. By contrast, ILP realizes inductive learningwhile most techniques have been developed under the classical monotonic logic.With this background, some researchers attempt to combine techniques in thecontext of nonmonotonic ILP. Such combination will introduce a learningmechanism to programs and would exploit new applications on the NMLP side,while on the ILP side it will extend the representation language and enable usto use existing solvers. Cross-fertilization between learning and nonmonotonicreasoning can also occur in such as the use of answer set solvers for ILP,speed-up learning while running answer set solvers, learning action theories,learning transition rules in dynamical systems, abductive learning, learningbiological networks with inhibition, and applications involving default andnegation. This workshop is the first attempt to provide an open forum for theidentification of problems and discussion of possible collaborations amongresearchers with complementary expertise. The workshop was held on September15th of 2013 in Corunna, Spain. This post-proceedings contains five technicalpapers (out of six accepted papers) and the abstract of the invited talk by LucDe Raedt.
arxiv-5100-110 | Domain Adaptation of Majority Votes via Perturbed Variation-based Label Transfer | http://arxiv.org/pdf/1311.4833v1.pdf | author:Emilie Morvant category:stat.ML cs.LG published:2013-11-19 summary:We tackle the PAC-Bayesian Domain Adaptation (DA) problem. This arrives whenone desires to learn, from a source distribution, a good weighted majority vote(over a set of classifiers) on a different target distribution. In thiscontext, the disagreement between classifiers is known crucial to control. Innon-DA supervised setting, a theoretical bound - the C-bound - involves thisdisagreement and leads to a majority vote learning algorithm: MinCq. In thiswork, we extend MinCq to DA by taking advantage of an elegant divergencebetween distribution called the Perturbed Varation (PV). Firstly, justified bya new formulation of the C-bound, we provide to MinCq a target sample labeledthanks to a PV-based self-labeling focused on regions where the source andtarget marginal distributions are closer. Secondly, we propose an originalprocess for tuning the hyperparameters. Our framework shows very promisingresults on a toy problem.
arxiv-5100-111 | Hilditchs Algorithm Based Tamil Character Recognition | http://arxiv.org/pdf/1311.6740v1.pdf | author:V. Karthikeyan category:cs.CV published:2013-11-19 summary:Character identification plays a vital role in the contemporary world ofImage processing. It can solve many composite problems and makes humans workeasier. An instance is Handwritten Character detection. Handwritten recognitionis not a novel expertise, but it has not gained community notice until Now. Theeventual aim of designing Handwritten Character recognition structure with anaccurateness rate of 100% is pretty illusionary. Tamil Handwritten Characterrecognition system uses the Neural Networks to distinguish them. Neural Networkand structural characteristics are used to instruct and recognize writtencharacters. After training and testing the exactness rate reached 99%. Thiscorrectness rate is extremely high. In this paper we are exploring imageprocessing through the Hilditch algorithm foundation and structuralcharacteristics of a character in the image. And we recognized some characterof the Tamil language, and we are trying to identify all the character of TamilIn our future works.
arxiv-5100-112 | Image enhancement using fusion by wavelet transform and laplacian pyramid | http://arxiv.org/pdf/1401.6129v1.pdf | author:S. M. Mukane, Y. S. Ghodake, P. S. Khandagle category:cs.CV published:2013-11-19 summary:The idea of combining multiple image modalities to provide a single, enhancedimage is well established different fusion methods have been proposed inliterature. This paper is based on image fusion using laplacian pyramid andwavelet transform method. Images of same size are used for experimentation.Images used for the experimentation are standard images and averaging filter isused of equal weights in original images to burl. Performance of image fusiontechnique is measured by mean square error, normalized absolute error and peaksignal to noise ratio. From the performance analysis it has been observed thatMSE is decreased in case of both the methods where as PSNR increased, NAEdecreased in case of laplacian pyramid where as constant for wavelet transformmethod.
arxiv-5100-113 | Near-Optimal Entrywise Sampling for Data Matrices | http://arxiv.org/pdf/1311.4643v1.pdf | author:Dimitris Achlioptas, Zohar Karnin, Edo Liberty category:cs.LG cs.IT cs.NA math.IT stat.ML published:2013-11-19 summary:We consider the problem of selecting non-zero entries of a matrix $A$ inorder to produce a sparse sketch of it, $B$, that minimizes $\A-B\_2$. Forlarge $m \times n$ matrices, such that $n \gg m$ (for example, representing $n$observations over $m$ attributes) we give sampling distributions that exhibitfour important properties. First, they have closed forms computable fromminimal information regarding $A$. Second, they allow sketching of matriceswhose non-zeros are presented to the algorithm in arbitrary order as a stream,with $O(1)$ computation per non-zero. Third, the resulting sketch matrices arenot only sparse, but their non-zero entries are highly compressible. Lastly,and most importantly, under mild assumptions, our distributions are provablycompetitive with the optimal offline distribution. Note that the probabilitiesin the optimal offline distribution may be complex functions of all the entriesin the matrix. Therefore, regardless of computational complexity, the optimaldistribution might be impossible to compute in the streaming model.
arxiv-5100-114 | Face Verification System based on Integral Normalized Gradient Image(INGI) | http://arxiv.org/pdf/1401.6112v1.pdf | author:V. Karthikeyan, M. Divya, C. K. Chithra, K. Manju Priya category:cs.CV published:2013-11-19 summary:Character identification plays a vital role in the contemporary world ofImage processing. It can solve many composite problems and makes humans workeasier. An instance is Handwritten Character detection. Handwritten recognitionis not a novel expertise, but it has not gained community notice until Now. Theeventual aim of designing Handwritten Character recognition structure with anaccurateness rate of 100% is pretty illusionary. Tamil Handwritten Characterrecognition system uses the Neural Networks to distinguish them. Neural Networkand structural characteristics are used to instruct and recognize writtencharacters. After training and testing the exactness rate reached 99%. Thiscorrectness rate is extremely high. In this paper we are exploring imageprocessing through the Hilditch algorithm foundation and structuralcharacteristics of a character in the image. And we recognized some characterof the Tamil language, and we are trying to identify all the character of TamilIn our future works.
arxiv-5100-115 | Asymptotically Exact, Embarrassingly Parallel MCMC | http://arxiv.org/pdf/1311.4780v2.pdf | author:Willie Neiswanger, Chong Wang, Eric Xing category:stat.ML cs.DC cs.LG stat.CO published:2013-11-19 summary:Communication costs, resulting from synchronization requirements duringlearning, can greatly slow down many parallel machine learning algorithms. Inthis paper, we present a parallel Markov chain Monte Carlo (MCMC) algorithm inwhich subsets of data are processed independently, with very littlecommunication. First, we arbitrarily partition data onto multiple machines.Then, on each machine, any classical MCMC method (e.g., Gibbs sampling) may beused to draw samples from a posterior distribution given the data subset.Finally, the samples from each machine are combined to form samples from thefull posterior. This embarrassingly parallel algorithm allows each machine toact independently on a subset of the data (without communication) until thefinal combination stage. We prove that our algorithm generates asymptoticallyexact samples and empirically demonstrate its ability to parallelize burn-inand sampling in several models.
arxiv-5100-116 | Analysis of Farthest Point Sampling for Approximating Geodesics in a Graph | http://arxiv.org/pdf/1311.4665v1.pdf | author:Pegah Kamousi, Sylvain Lazard, Anil Maheshwari, Stefanie Wuhrer category:cs.CG cs.CV cs.GR published:2013-11-19 summary:A standard way to approximate the distance between any two vertices $p$ and$q$ on a mesh is to compute, in the associated graph, a shortest path from $p$to $q$ that goes through one of $k$ sources, which are well-chosen vertices.Precomputing the distance between each of the $k$ sources to all vertices ofthe graph yields an efficient computation of approximate distances between anytwo vertices. One standard method for choosing $k$ sources, which has been usedextensively and successfully for isometry-invariant surface processing, is theso-called Farthest Point Sampling (FPS), which starts with a random vertex asthe first source, and iteratively selects the farthest vertex from the alreadyselected sources. In this paper, we analyze the stretch factor $\mathcal{F}_{FPS}$ ofapproximate geodesics computed using FPS, which is the maximum, over all pairsof distinct vertices, of their approximated distance over their geodesicdistance in the graph. We show that $\mathcal{F}_{FPS}$ can be bounded in termsof the minimal value $\mathcal{F}^*$ of the stretch factor obtained using anoptimal placement of $k$ sources as $\mathcal{F}_{FPS}\leq 2 r_e^2\mathcal{F}^*+ 2 r_e^2 + 8 r_e + 1$, where $r_e$ is the ratio of the lengths ofthe longest and the shortest edges of the graph. This provides some evidenceexplaining why farthest point sampling has been used successfully forisometry-invariant shape processing. Furthermore, we show that it isNP-complete to find $k$ sources that minimize the stretch factor.
arxiv-5100-117 | Nonparametric Bayes dynamic modeling of relational data | http://arxiv.org/pdf/1311.4669v1.pdf | author:Daniele Durante, David B. Dunson category:stat.ML published:2013-11-19 summary:Symmetric binary matrices representing relations among entities are commonlycollected in many areas. Our focus is on dynamically evolving binary relationalmatrices, with interest being in inference on the relationship structure andprediction. We propose a nonparametric Bayesian dynamic model, which reducesdimensionality in characterizing the binary matrix through a lower-dimensionallatent space representation, with the latent coordinates evolving in continuoustime via Gaussian processes. By using a logistic mapping function from theprobability matrix space to the latent relational space, we obtain a flexibleand computational tractable formulation. Employing P\`olya-Gamma dataaugmentation, an efficient Gibbs sampler is developed for posteriorcomputation, with the dimension of the latent space automatically inferred. Weprovide some theoretical results on flexibility of the model, and illustrateperformance via simulation experiments. We also consider an application toco-movements in world financial markets.
arxiv-5100-118 | Reflection methods for user-friendly submodular optimization | http://arxiv.org/pdf/1311.4296v1.pdf | author:Stefanie Jegelka, Francis Bach, Suvrit Sra category:cs.LG cs.NA cs.RO math.OC published:2013-11-18 summary:Recently, it has become evident that submodularity naturally captures widelyoccurring concepts in machine learning, signal processing and computer vision.Consequently, there is need for efficient optimization procedures forsubmodular functions, especially for minimization problems. While generalsubmodular minimization is challenging, we propose a new method that exploitsexisting decomposability of submodular functions. In contrast to previousapproaches, our method is neither approximate, nor impractical, nor does itneed any cumbersome parameter tuning. Moreover, it is easy to implement andparallelize. A key component of our method is a formulation of the discretesubmodular minimization problem as a continuous best approximation problem thatis solved through a sequence of reflections, and its solution can be easilythresholded to obtain an optimal discrete solution. This method solves both thecontinuous and discrete formulations of the problem, and therefore hasapplications in learning, inference, and reconstruction. In our experiments, weillustrate the benefits of our method on two image segmentation tasks.
arxiv-5100-119 | Contour polygonal approximation using shortest path in networks | http://arxiv.org/pdf/1311.4252v1.pdf | author:AndrÃ© Ricardo Backes, Dalcimar Casanova, Odemir Martinez Bruno category:cs.CV published:2013-11-18 summary:Contour polygonal approximation is a simplified representation of a contourby line segments, so that the main characteristics of the contour remain in asmall number of line segments. This paper presents a novel method for polygonalapproximation based on the Complex Networks theory. We convert each point ofthe contour into a vertex, so that we model a regular network. Then wetransform this network into a Small-World Complex Network by applying sometransformations over its edges. By analyzing of network properties, especiallythe geodesic path, we compute the polygonal approximation. The paper presentsthe main characteristics of the method, as well as its functionality. Weevaluate the proposed method using benchmark contours, and compare its resultswith other polygonal approximation methods.
arxiv-5100-120 | On the definition of a general learning system with user-defined operators | http://arxiv.org/pdf/1311.4235v1.pdf | author:Fernando MartÃ­nez-Plumed, CÃ¨sar Ferri, JosÃ© HernÃ¡ndez-Orallo, MarÃ­a-JosÃ© RamÃ­rez-Quintana category:cs.LG published:2013-11-18 summary:In this paper, we push forward the idea of machine learning systems whoseoperators can be modified and fine-tuned for each problem. This allows us topropose a learning paradigm where users can write (or adapt) their operators,according to the problem, data representation and the way the informationshould be navigated. To achieve this goal, data instances, backgroundknowledge, rules, programs and operators are all written in the same functionallanguage, Erlang. Since changing operators affect how the search space needs tobe explored, heuristics are learnt as a result of a decision process based onreinforcement learning where each action is defined as a choice of operator andrule. As a result, the architecture can be seen as a 'system for writingmachine learning systems' or to explore new operators where the policy reuse(as a kind of transfer learning) is allowed. States and actions are representedin a Q matrix which is actually a table, from which a supervised model islearnt. This makes it possible to have a more flexible mapping between old andnew problems, since we work with an abstraction of rules and actions. Weinclude some examples sharing reuse and the application of the system gErl toIQ problems. In order to evaluate gErl, we will test it against some structuredproblems: a selection of IQ test tasks and some experiments on some structuredprediction problems (list patterns).
arxiv-5100-121 | Discriminative Density-ratio Estimation | http://arxiv.org/pdf/1311.4486v2.pdf | author:Yun-Qian Miao, Ahmed K. Farahat, Mohamed S. Kamel category:cs.LG published:2013-11-18 summary:The covariate shift is a challenging problem in supervised learning thatresults from the discrepancy between the training and test distributions. Aneffective approach which recently drew a considerable attention in the researchcommunity is to reweight the training samples to minimize that discrepancy. Inspecific, many methods are based on developing Density-ratio (DR) estimationtechniques that apply to both regression and classification problems. Althoughthese methods work well for regression problems, their performance onclassification problems is not satisfactory. This is due to a key observationthat these methods focus on matching the sample marginal distributions withoutpaying attention to preserving the separation between classes in the reweightedspace. In this paper, we propose a novel method for DiscriminativeDensity-ratio (DDR) estimation that addresses the aforementioned problem andaims at estimating the density-ratio of joint distributions in a class-wisemanner. The proposed algorithm is an iterative procedure that alternatesbetween estimating the class information for the test data and estimating newdensity ratio for each class. To incorporate the estimated class information ofthe test data, a soft matching technique is proposed. In addition, we employ aneffective criterion which adopts mutual information as an indicator to stop theiterative procedure while resulting in a decision boundary that lies in asparse region. Experiments on synthetic and benchmark datasets demonstrate thesuperiority of the proposed method in terms of both accuracy and robustness.
arxiv-5100-122 | Minimum $n$-Rank Approximation via Iterative Hard Thresholding | http://arxiv.org/pdf/1311.4291v2.pdf | author:Min Zhang, Lei Yang, Zheng-Hai Huang category:math.OC stat.ML published:2013-11-18 summary:The problem of recovering a low $n$-rank tensor is an extension of sparserecovery problem from the low dimensional space (matrix space) to the highdimensional space (tensor space) and has many applications in computer visionand graphics such as image inpainting and video inpainting. In this paper, weconsider a new tensor recovery model, named as minimum $n$-rank approximation(MnRA), and propose an appropriate iterative hard thresholding algorithm withgiving the upper bound of the $n$-rank in advance. The convergence analysis ofthe proposed algorithm is also presented. Particularly, we show that for thenoiseless case, the linear convergence with rate $\frac{1}{2}$ can be obtainedfor the proposed algorithm under proper conditions. Additionally, combining aneffective heuristic for determining $n$-rank, we can also apply the proposedalgorithm to solve MnRA when $n$-rank is unknown in advance. Some preliminarynumerical results on randomly generated and real low $n$-rank tensor completionproblems are reported, which show the efficiency of the proposed algorithms.
arxiv-5100-123 | From Maxout to Channel-Out: Encoding Information on Sparse Pathways | http://arxiv.org/pdf/1312.1909v1.pdf | author:Qi Wang, Joseph JaJa category:cs.NE cs.CV cs.LG stat.ML published:2013-11-18 summary:Motivated by an important insight from neural science, we propose a newframework for understanding the success of the recently proposed "maxout"networks. The framework is based on encoding information on sparse pathways andrecognizing the correct pathway at inference time. Elaborating further on thisinsight, we propose a novel deep network architecture, called "channel-out"network, which takes a much better advantage of sparse pathway encoding. Inchannel-out networks, pathways are not only formed a posteriori, but they arealso actively selected according to the inference outputs from the lowerlayers. From a mathematical perspective, channel-out networks can represent awider class of piece-wise continuous functions, thereby endowing the networkwith more expressive power than that of maxout networks. We test ourchannel-out networks on several well-known image classification benchmarks,setting new state-of-the-art performance on CIFAR-100 and STL-10, whichrepresent some of the "harder" image classification benchmarks.
arxiv-5100-124 | On Nonrigid Shape Similarity and Correspondence | http://arxiv.org/pdf/1311.5595v1.pdf | author:Alon Shtern, Ron Kimmel category:cs.CV cs.GR published:2013-11-18 summary:An important operation in geometry processing is finding the correspondencesbetween pairs of shapes. The Gromov-Hausdorff distance, a measure ofdissimilarity between metric spaces, has been found to be highly useful fornonrigid shape comparison. Here, we explore the applicability of related shapesimilarity measures to the problem of shape correspondence, adopting spectraltype distances. We propose to evaluate the spectral kernel distance, thespectral embedding distance and the novel spectral quasi-conformal distance,comparing the manifolds from different viewpoints. By matching the shapes inthe spectral domain, important attributes of surface structure are beingaligned. For the purpose of testing our ideas, we introduce a fully automaticframework for finding intrinsic correspondence between two shapes. The proposedmethod achieves state-of-the-art results on the Princeton isometric shapematching protocol applied, as usual, to the TOSCA and SCAPE benchmarks.
arxiv-5100-125 | Stochastic processes and feedback-linearisation for online identification and Bayesian adaptive control of fully-actuated mechanical systems | http://arxiv.org/pdf/1311.4468v3.pdf | author:Jan-Peter Calliess, Antonis Papachristodoulou, Stephen J. Roberts category:cs.LG cs.SY stat.ML published:2013-11-18 summary:This work proposes a new method for simultaneous probabilistic identificationand control of an observable, fully-actuated mechanical system. Identificationis achieved by conditioning stochastic process priors on observations ofconfigurations and noisy estimates of configuration derivatives. In contrast toprevious work that has used stochastic processes for identification, weleverage the structural knowledge afforded by Lagrangian mechanics and learnthe drift and control input matrix functions of the control-affine systemseparately. We utilise feedback-linearisation to reduce, in expectation, theuncertain nonlinear control problem to one that is easy to regulate in adesired manner. Thereby, our method combines the flexibility of nonparametricBayesian learning with epistemological guarantees on the expected closed-looptrajectory. We illustrate our method in the context of torque-actuated pendulawhere the dynamics are learned with a combination of normal and log-normalprocesses.
arxiv-5100-126 | Confidence Intervals for Random Forests: The Jackknife and the Infinitesimal Jackknife | http://arxiv.org/pdf/1311.4555v2.pdf | author:Stefan Wager, Trevor Hastie, Bradley Efron category:stat.ML stat.CO stat.ME published:2013-11-18 summary:We study the variability of predictions made by bagged learners and randomforests, and show how to estimate standard errors for these methods. Our workbuilds on variance estimates for bagging proposed by Efron (1992, 2012) thatare based on the jackknife and the infinitesimal jackknife (IJ). In practice,bagged predictors are computed using a finite number B of bootstrap replicates,and working with a large B can be computationally expensive. Directapplications of jackknife and IJ estimators to bagging require B on the orderof n^{1.5} bootstrap replicates to converge, where n is the size of thetraining set. We propose improved versions that only require B on the order ofn replicates. Moreover, we show that the IJ estimator requires 1.7 times lessbootstrap replicates than the jackknife to achieve a given accuracy. Finally,we study the sampling distributions of the jackknife and IJ variance estimatesthemselves. We illustrate our findings with multiple experiments and simulationstudies.
arxiv-5100-127 | Ranking Algorithms by Performance | http://arxiv.org/pdf/1311.4319v1.pdf | author:Lars Kotthoff category:cs.AI cs.LG published:2013-11-18 summary:A common way of doing algorithm selection is to train a machine learningmodel and predict the best algorithm from a portfolio to solve a particularproblem. While this method has been highly successful, choosing only a singlealgorithm has inherent limitations -- if the choice was bad, no remedial actioncan be taken and parallelism cannot be exploited, to name but a few problems.In this paper, we investigate how to predict the ranking of the portfolioalgorithms on a particular problem. This information can be used to choose thesingle best algorithm, but also to allocate resources to the algorithmsaccording to their rank. We evaluate a range of approaches to predict theranking of a set of algorithms on a problem. We furthermore introduce aframework for categorizing ranking predictions that allows to judge theexpressiveness of the predictive output. Our experimental evaluationdemonstrates on a range of data sets from the literature that it is beneficialto consider the relationship between algorithms when predicting rankings. Wefurthermore show that relatively naive approaches deliver rankings of goodquality already.
arxiv-5100-128 | A Component Lasso | http://arxiv.org/pdf/1311.4472v2.pdf | author:Nadine Hussami, Robert Tibshirani category:stat.ML cs.LG 62J07 published:2013-11-18 summary:We propose a new sparse regression method called the component lasso, basedon a simple idea. The method uses the connected-components structure of thesample covariance matrix to split the problem into smaller ones. It then solvesthe subproblems separately, obtaining a coefficient vector for each one. Then,it uses non-negative least squares to recombine the different vectors into asingle solution. This step is useful in selecting and reweighting componentsthat are correlated with the response. Simulated and real data examples showthat the component lasso can outperform standard regression methods such as thelasso and elastic net, achieving a lower mean squared error as well as bettersupport recovery.
arxiv-5100-129 | Unsupervised Learning of Invariant Representations in Hierarchical Architectures | http://arxiv.org/pdf/1311.4158v5.pdf | author:Fabio Anselmi, Joel Z. Leibo, Lorenzo Rosasco, Jim Mutch, Andrea Tacchetti, Tomaso Poggio category:cs.CV cs.LG published:2013-11-17 summary:The present phase of Machine Learning is characterized by supervised learningalgorithms relying on large sets of labeled examples ($n \to \infty$). The nextphase is likely to focus on algorithms capable of learning from very fewlabeled examples ($n \to 1$), like humans seem able to do. We propose anapproach to this problem and describe the underlying theory, based on theunsupervised, automatic learning of a ``good'' representation for supervisedlearning, characterized by small sample complexity ($n$). We consider the caseof visual object recognition though the theory applies to other domains. Thestarting point is the conjecture, proved in specific cases, that imagerepresentations which are invariant to translations, scaling and othertransformations can considerably reduce the sample complexity of learning. Weprove that an invariant and unique (discriminative) signature can be computedfor each image patch, $I$, in terms of empirical distributions of thedot-products between $I$ and a set of templates stored during unsupervisedlearning. A module performing filtering and pooling, like the simple andcomplex cells described by Hubel and Wiesel, can compute such estimates.Hierarchical architectures consisting of this basic Hubel-Wiesel moduli inheritits properties of invariance, stability, and discriminability while capturingthe compositional organization of the visual world in terms of wholes andparts. The theory extends existing deep learning convolutional architecturesfor image and speech recognition. It also suggests that the main computationalgoal of the ventral stream of visual cortex is to provide a hierarchicalrepresentation of new objects/images which is invariant to transformations,stable, and discriminative for recognition---and that this representation maybe continuously learned in an unsupervised way during development and visualexperience.
arxiv-5100-130 | Towards Big Topic Modeling | http://arxiv.org/pdf/1311.4150v1.pdf | author:Jian-Feng Yan, Jia Zeng, Zhi-Qiang Liu, Yang Gao category:cs.LG cs.DC cs.IR stat.ML published:2013-11-17 summary:To solve the big topic modeling problem, we need to reduce both time andspace complexities of batch latent Dirichlet allocation (LDA) algorithms.Although parallel LDA algorithms on the multi-processor architecture have lowtime and space complexities, their communication costs among processors oftenscale linearly with the vocabulary size and the number of topics, leading to aserious scalability problem. To reduce the communication complexity amongprocessors for a better scalability, we propose a novel communication-efficientparallel topic modeling architecture based on power law, which consumes ordersof magnitude less communication time when the number of topics is large. Wecombine the proposed communication-efficient parallel architecture with theonline belief propagation (OBP) algorithm referred to as POBP for big topicmodeling tasks. Extensive empirical results confirm that POBP has the followingadvantages to solve the big topic modeling problem: 1) high accuracy, 2)communication-efficient, 3) fast speed, and 4) constant memory usage whencompared with recent state-of-the-art parallel LDA algorithms on themulti-processor architecture.
arxiv-5100-131 | A hybrid decision support system : application on healthcare | http://arxiv.org/pdf/1311.4086v1.pdf | author:Abdelhak Mansoul, Baghdad Atmani, Sofia Benbelkacem category:cs.AI cs.LG published:2013-11-16 summary:Many systems based on knowledge, especially expert systems for medicaldecision support have been developed. Only systems are based on productionrules, and cannot learn and evolve only by updating them. In addition, takinginto account several criteria induces an exorbitant number of rules to beinjected into the system. It becomes difficult to translate medical knowledgeor a support decision as a simple rule. Moreover, reasoning based on genericcases became classic and can even reduce the range of possible solutions. Toremedy that, we propose an approach based on using a multi-criteria decisionguided by a case-based reasoning (CBR) approach.
arxiv-5100-132 | The Optimization of Running Queries in Relational Databases Using ANT-Colony Algorithm | http://arxiv.org/pdf/1311.4088v1.pdf | author:Adel Alinezhad Kolaei, Marzieh Ahmadzadeh category:cs.DB cs.NE published:2013-11-16 summary:The issue of optimizing queries is a cost-sensitive process and with respectto the number of associated tables in a query, its number of permutations growsexponentially. On one hand, in comparison with other operators in relationaldatabase, join operator is the most difficult and complicated one in terms ofoptimization for reducing its runtime. Accordingly, various algorithms have sofar been proposed to solve this problem. On the other hand, the success of anydatabase management system (DBMS) means exploiting the query model. In thecurrent paper, the heuristic ant algorithm has been proposed to solve thisproblem and improve the runtime of join operation. Experiments and observedresults reveal the efficiency of this algorithm compared to its similaralgorithms.
arxiv-5100-133 | Blind Deconvolution with Non-local Sparsity Reweighting | http://arxiv.org/pdf/1311.4029v2.pdf | author:Dilip Krishnan, Joan Bruna, Rob Fergus category:cs.CV published:2013-11-16 summary:Blind deconvolution has made significant progress in the past decade. Mostsuccessful algorithms are classified either as Variational or Maximuma-Posteriori ($MAP$). In spite of the superior theoretical justification ofvariational techniques, carefully constructed $MAP$ algorithms have provenequally effective in practice. In this paper, we show that all successful $MAP$and variational algorithms share a common framework, relying on the followingkey principles: sparsity promotion in the gradient domain, $l_2$ regularizationfor kernel estimation, and the use of convex (often quadratic) cost functions.Our observations lead to a unified understanding of the principles required forsuccessful blind deconvolution. We incorporate these principles into a novelalgorithm that improves significantly upon the state of the art.
arxiv-5100-134 | A Comparative Study of Histogram Equalization Based Image Enhancement Techniques for Brightness Preservation and Contrast Enhancement | http://arxiv.org/pdf/1311.4033v1.pdf | author:Omprakash Patel, Yogendra P. S. Maravi, Sanjeev Sharma category:cs.CV published:2013-11-16 summary:Histogram Equalization is a contrast enhancement technique in the imageprocessing which uses the histogram of image. However histogram equalization isnot the best method for contrast enhancement because the mean brightness of theoutput image is significantly different from the input image. There are severalextensions of histogram equalization has been proposed to overcome thebrightness preservation challenge. Contrast enhancement using brightnesspreserving bi-histogram equalization (BBHE) and Dualistic sub image histogramequalization (DSIHE) which divides the image histogram into two parts based onthe input mean and median respectively then equalizes each sub histogramindependently. This paper provides review of different popular histogramequalization techniques and experimental study based on the absolute meanbrightness error (AMBE), peak signal to noise ratio (PSNR), Structuresimilarity index (SSI) and Entropy.
arxiv-5100-135 | Signal Recovery from Pooling Representations | http://arxiv.org/pdf/1311.4025v3.pdf | author:Joan Bruna, Arthur Szlam, Yann LeCun category:stat.ML published:2013-11-16 summary:In this work we compute lower Lipschitz bounds of $\ell_p$ pooling operatorsfor $p=1, 2, \infty$ as well as $\ell_p$ pooling operators preceded byhalf-rectification layers. These give sufficient conditions for the design ofinvertible neural network layers. Numerical experiments on MNIST and imagepatches confirm that pooling layers can be inverted with phase recoveryalgorithms. Moreover, the regularity of the inverse pooling, controlled by thelower Lipschitz constant, is empirically verified with a nearest neighborregression.
arxiv-5100-136 | Can a biologically-plausible hierarchy effectively replace face detection, alignment, and recognition pipelines? | http://arxiv.org/pdf/1311.4082v3.pdf | author:Qianli Liao, Joel Z Leibo, Youssef Mroueh, Tomaso Poggio category:cs.CV published:2013-11-16 summary:The standard approach to unconstrained face recognition in naturalphotographs is via a detection, alignment, recognition pipeline. While thatapproach has achieved impressive results, there are several reasons to bedissatisfied with it, among them is its lack of biological plausibility. Arecent theory of invariant recognition by feedforward hierarchical networks,like HMAX, other convolutional networks, or possibly the ventral stream,implies an alternative approach to unconstrained face recognition. Thisapproach accomplishes detection and alignment implicitly by storingtransformations of training images (called templates) rather than explicitlydetecting and aligning faces at test time. Here we propose a particularlocality-sensitive hashing based voting scheme which we call "consensus ofcollisions" and show that it can be used to approximate the full 3-layerhierarchy implied by the theory. The resulting end-to-end system forunconstrained face recognition operates on photographs of faces taken undernatural conditions, e.g., Labeled Faces in the Wild (LFW), without aligning orcropping them, as is normally done. It achieves a drastic improvement in thestate of the art on this end-to-end task, reaching the same level ofperformance as the best systems operating on aligned, closely cropped images(no outside training data). It also performs well on two newer datasets,similar to LFW, but more difficult: LFW-jittered (new here) and SUFR-W.
arxiv-5100-137 | Ensemble Relational Learning based on Selective Propositionalization | http://arxiv.org/pdf/1311.3735v1.pdf | author:Nicola Di Mauro, Floriana Esposito category:cs.LG cs.AI published:2013-11-15 summary:Dealing with structured data needs the use of expressive representationformalisms that, however, puts the problem to deal with the computationalcomplexity of the machine learning process. Furthermore, real world domainsrequire tools able to manage their typical uncertainty. Many statisticalrelational learning approaches try to deal with these problems by combining theconstruction of relevant relational features with a probabilistic tool. Whenthe combination is static (static propositionalization), the constructedfeatures are considered as boolean features and used offline as input to astatistical learner; while, when the combination is dynamic (dynamicpropositionalization), the feature construction and probabilistic tool arecombined into a single process. In this paper we propose a selectivepropositionalization method that search the optimal set of relational featuresto be used by a probabilistic learner in order to minimize a loss function. Thenew propositionalization approach has been combined with the random subspaceensemble method. Experiments on real-world datasets shows the validity of theproposed method.
arxiv-5100-138 | Periodicity Extraction using Superposition of Distance Matching Function and One-dimensional Haar Wavelet Transform | http://arxiv.org/pdf/1311.3808v1.pdf | author:V. Asha, N. U. Bhajantri, P. Nagabhushan category:cs.CV I.0; I.2.7 published:2013-11-15 summary:Periodicity of a texture is one of the important visual characteristics andis often used as a measure for textural discrimination at the structural level.Knowledge about periodicity of a texture is very essential in the field oftexture synthesis and texture compression and also in the design of frieze andwall papers. In this paper, we propose a method of periodicity extraction fromnoisy images based on superposition of distance matching function (DMF) andwavelet decomposition without de-noising the test images. Overall DMFs aresubjected to single-level Haar wavelet decomposition to obtain approximate anddetailed coefficients. Extracted coefficients help in determination ofperiodicities in row and column directions. We illustrate the usefulness andthe effectiveness of the proposed method in a texture synthesis application.
arxiv-5100-139 | On Estimating Many Means, Selection Bias, and the Bootstrap | http://arxiv.org/pdf/1311.3709v1.pdf | author:Noah Simon, Richard Simon category:stat.ML stat.ME published:2013-11-15 summary:With recent advances in high throughput technology, researchers often findthemselves running a large number of hypothesis tests (thousands+) and esti-mating a large number of effect-sizes. Generally there is particular interestin those effects estimated to be most extreme. Unfortunately naive estimates ofthese effect-sizes (even after potentially accounting for multiplicity in atesting procedure) can be severely biased. In this manuscript we explore thisbias from a frequentist perspective: we give a formal definition, and show thatan oracle estimator using this bias dominates the naive maximum likelihoodestimate. We give a resampling estimator to approximate this oracle, and showthat it works well on simulated data. We also connect this to ideas inempirical Bayes.
arxiv-5100-140 | Mapping cognitive ontologies to and from the brain | http://arxiv.org/pdf/1311.3859v2.pdf | author:Yannick Schwartz, Bertrand Thirion, GaÃ«l Varoquaux category:stat.ML cs.LG q-bio.NC published:2013-11-15 summary:Imaging neuroscience links brain activation maps to behavior and cognitionvia correlational studies. Due to the nature of the individual experiments,based on eliciting neural response from a small number of stimuli, this link isincomplete, and unidirectional from the causal point of view. To come toconclusions on the function implied by the activation of brain regions, it isnecessary to combine a wide exploration of the various brain functions and someinversion of the statistical inference. Here we introduce a methodology foraccumulating knowledge towards a bidirectional link between observed brainactivity and the corresponding function. We rely on a large corpus of imagingstudies and a predictive engine. Technically, the challenges are to findcommonality between the studies without denaturing the richness of the corpus.The key elements that we contribute are labeling the tasks performed with acognitive ontology, and modeling the long tail of rare paradigms in the corpus.To our knowledge, our approach is the first demonstration of predicting thecognitive content of completely new brain images. To that end, we propose amethod that predicts the experimental paradigms across different studies.
arxiv-5100-141 | HEVAL: Yet Another Human Evaluation Metric | http://arxiv.org/pdf/1311.3961v1.pdf | author:Nisheeth Joshi, Iti Mathur, Hemant Darbari, Ajai Kumar category:cs.CL published:2013-11-15 summary:Machine translation evaluation is a very important activity in machinetranslation development. Automatic evaluation metrics proposed in literatureare inadequate as they require one or more human reference translations tocompare them with output produced by machine translation. This does not alwaysgive accurate results as a text can have several different translations. Humanevaluation metrics, on the other hand, lacks inter-annotator agreement andrepeatability. In this paper we have proposed a new human evaluation metricwhich addresses these issues. Moreover this metric also provides solid groundsfor making sound assumptions on the quality of the text produced by a machinetranslation.
arxiv-5100-142 | Mixing Energy Models in Genetic Algorithms for On-Lattice Protein Structure Prediction | http://arxiv.org/pdf/1311.3840v1.pdf | author:Mahmood A. Rashid, M. A. Hakim Newton, Md. Tamjidul Hoque, Abdul Sattar category:cs.CE cs.NE published:2013-11-15 summary:Protein structure prediction (PSP) is computationally a very challengingproblem. The challenge largely comes from the fact that the energy functionthat needs to be minimised in order to obtain the native structure of a givenprotein is not clearly known. A high resolution 20x20 energy model could bettercapture the behaviour of the actual energy function than a low resolutionenergy model such as hydrophobic polar. However, the fine grained details ofthe high resolution interaction energy matrix are often not very informativefor guiding the search. In contrast, a low resolution energy model couldeffectively bias the search towards certain promising directions. In thispaper, we develop a genetic algorithm that mainly uses a high resolution energymodel for protein structure evaluation but uses a low resolution HP energymodel in focussing the search towards exploring structures that havehydrophobic cores. We experimentally show that this mixing of energy modelsleads to significant lower energy structures compared to the state-of-the-artresults.
arxiv-5100-143 | Compressed Sensing for Energy-Efficient Wireless Telemonitoring: Challenges and Opportunities | http://arxiv.org/pdf/1311.3995v2.pdf | author:Zhilin Zhang, Bhaskar D. Rao, Tzyy-Ping Jung category:cs.IT math.IT stat.ML published:2013-11-15 summary:As a lossy compression framework, compressed sensing has drawn much attentionin wireless telemonitoring of biosignals due to its ability to reduce energyconsumption and make possible the design of low-power devices. However, thenon-sparseness of biosignals presents a major challenge to compressed sensing.This study proposes and evaluates a spatio-temporal sparse Bayesian learningalgorithm, which has the desired ability to recover such non-sparse biosignals.It exploits both temporal correlation in each individual biosignal andinter-channel correlation among biosignals from different channels. Theproposed algorithm was used for compressed sensing of multichannelelectroencephalographic (EEG) signals for estimating vehicle drivers'drowsiness. Results showed that the drowsiness estimation was almost unaffectedeven if raw EEG signals (containing various artifacts) were compressed by 90%.
arxiv-5100-144 | Recognizing Image Style | http://arxiv.org/pdf/1311.3715v3.pdf | author:Sergey Karayev, Matthew Trentacoste, Helen Han, Aseem Agarwala, Trevor Darrell, Aaron Hertzmann, Holger Winnemoeller category:cs.CV published:2013-11-15 summary:The style of an image plays a significant role in how it is viewed, but stylehas received little attention in computer vision research. We describe anapproach to predicting style of images, and perform a thorough evaluation ofdifferent image features for these tasks. We find that features learned in amulti-layer network generally perform best -- even when trained with objectclass (not style) labels. Our large-scale learning methods results in the bestpublished performance on an existing dataset of aesthetic ratings andphotographic style annotations. We present two novel datasets: 80K Flickrphotographs annotated with 20 curated style labels, and 85K paintings annotatedwith 25 style/genre labels. Our approach shows excellent classificationperformance on both datasets. We use the learned classifiers to extendtraditional tag-based image search to consider stylistic constraints, anddemonstrate cross-dataset understanding of style.
arxiv-5100-145 | Deterministic Bayesian Information Fusion and the Analysis of its Performance | http://arxiv.org/pdf/1311.3755v4.pdf | author:Gaurav Thakur category:math.ST stat.ML stat.TH published:2013-11-15 summary:This paper develops a mathematical and computational framework for analyzingthe expected performance of Bayesian data fusion, or joint statisticalinference, within a sensor network. We use variational techniques to obtain theposterior expectation as the optimal fusion rule under a deterministicconstraint and a quadratic cost, and study the smoothness and other propertiesof its classification performance. For a certain class of fusion problems, weprove that this fusion rule is also optimal in a much wider sense and satisfiesstrong asymptotic convergence results. We show how these results apply to avariety of examples with Gaussian, exponential and other statistics, anddiscuss computational methods for determining the fusion system's performancein more general, large-scale problems. These results are motivated by studyingthe performance of fusing multi-modal radar and acoustic sensors for detectingexplosive substances, but have broad applicability to other Bayesian decisionproblems.
arxiv-5100-146 | Clustering Markov Decision Processes For Continual Transfer | http://arxiv.org/pdf/1311.3959v4.pdf | author:M. M. Hassan Mahmud, Majd Hawasly, Benjamin Rosman, Subramanian Ramamoorthy category:cs.AI cs.LG 68T05 I.2.6 published:2013-11-15 summary:We present algorithms to effectively represent a set of Markov decisionprocesses (MDPs), whose optimal policies have already been learned, by asmaller source subset for lifelong, policy-reuse-based transfer learning inreinforcement learning. This is necessary when the number of previous tasks islarge and the cost of measuring similarity counteracts the benefit of transfer.The source subset forms an `$\epsilon$-net' over the original set of MDPs, inthe sense that for each previous MDP $M_p$, there is a source $M^s$ whoseoptimal policy has $<\epsilon$ regret in $M_p$. Our contributions are asfollows. We present EXP-3-Transfer, a principled policy-reuse algorithm thatoptimally reuses a given source policy set when learning for a new MDP. Wepresent a framework to cluster the previous MDPs to extract a source subset.The framework consists of (i) a distance $d_V$ over MDPs to measurepolicy-based similarity between MDPs; (ii) a cost function $g(\cdot)$ that uses$d_V$ to measure how good a particular clustering is for generating usefulsource tasks for EXP-3-Transfer and (iii) a provably convergent algorithm,MHAV, for finding the optimal clustering. We validate our algorithms throughexperiments in a surveillance domain.
arxiv-5100-147 | The STONE Transform: Multi-Resolution Image Enhancement and Real-Time Compressive Video | http://arxiv.org/pdf/1311.3405v2.pdf | author:Tom Goldstein, Lina Xu, Kevin F. Kelly, Richard Baraniuk category:cs.CV published:2013-11-14 summary:Compressed sensing enables the reconstruction of high-resolution signals fromunder-sampled data. While compressive methods simplify data acquisition, theyrequire the solution of difficult recovery problems to make use of theresulting measurements. This article presents a new sensing framework thatcombines the advantages of both conventional and compressive sensing. Using theproposed \stone transform, measurements can be reconstructed instantly atNyquist rates at any power-of-two resolution. The same data can then be"enhanced" to higher resolutions using compressive methods that leveragesparsity to "beat" the Nyquist limit. The availability of a fast directreconstruction enables compressive measurements to be processed on smallembedded devices. We demonstrate this by constructing a real-time compressivevideo camera.
arxiv-5100-148 | Scalable Influence Estimation in Continuous-Time Diffusion Networks | http://arxiv.org/pdf/1311.3669v1.pdf | author:Nan Du, Le Song, Manuel Gomez Rodriguez, Hongyuan Zha category:cs.SI cs.LG H.2.8 published:2013-11-14 summary:If a piece of information is released from a media site, can it spread, in 1month, to a million web pages? This influence estimation problem is verychallenging since both the time-sensitive nature of the problem and the issueof scalability need to be addressed simultaneously. In this paper, we propose arandomized algorithm for influence estimation in continuous-time diffusionnetworks. Our algorithm can estimate the influence of every node in a networkwith V nodes and E edges to an accuracy of $\varepsilon$ using$n=O(1/\varepsilon^2)$ randomizations and up to logarithmic factorsO(nE+nV) computations. When used as a subroutine in a greedy influencemaximization algorithm, our proposed method is guaranteed to find a set ofnodes with an influence of at least (1-1/e)OPT-2$\varepsilon$, where OPT is theoptimal value. Experiments on both synthetic and real-world data show that theproposed method can easily scale up to networks of millions of nodes whilesignificantly improves over previous state-of-the-arts in terms of the accuracyof the estimated influence and the quality of the selected nodes in maximizingthe influence.
arxiv-5100-149 | Big Data and Cross-Document Coreference Resolution: Current State and Future Opportunities | http://arxiv.org/pdf/1311.3987v1.pdf | author:Seyed-Mehdi-Reza Beheshti, Srikumar Venugopal, Seung Hwan Ryu, Boualem Benatallah, Wei Wang category:cs.CL cs.DC cs.IR published:2013-11-14 summary:Information Extraction (IE) is the task of automatically extractingstructured information from unstructured/semi-structured machine-readabledocuments. Among various IE tasks, extracting actionable intelligence fromever-increasing amount of data depends critically upon Cross-DocumentCoreference Resolution (CDCR) - the task of identifying entity mentions acrossmultiple documents that refer to the same underlying entity. Recently, documentdatasets of the order of peta-/tera-bytes has raised many challenges forperforming effective CDCR such as scaling to large numbers of mentions andlimited representational power. The problem of analysing such datasets iscalled "big data". The aim of this paper is to provide readers with anunderstanding of the central concepts, subtasks, and the currentstate-of-the-art in CDCR process. We provide assessment of existingtools/techniques for CDCR subtasks and highlight big data challenges in each ofthem to help readers identify important and outstanding issues for furtherinvestigation. Finally, we provide concluding remarks and discuss possibledirections for future work.
arxiv-5100-150 | Describing Textures in the Wild | http://arxiv.org/pdf/1311.3618v2.pdf | author:Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, Andrea Vedaldi category:cs.CV published:2013-11-14 summary:Patterns and textures are defining characteristics of many natural objects: ashirt can be striped, the wings of a butterfly can be veined, and the skin ofan animal can be scaly. Aiming at supporting this analytical dimension in imageunderstanding, we address the challenging problem of describing textures withsemantic attributes. We identify a rich vocabulary of forty-seven texture termsand use them to describe a large dataset of patterns collected in the wild.Theresulting Describable Textures Dataset (DTD) is the basis to seek for the besttexture representation for recognizing describable texture attributes inimages. We port from object recognition to texture recognition the ImprovedFisher Vector (IFV) and show that, surprisingly, it outperforms specializedtexture descriptors not only on our problem, but also in established materialrecognition datasets. We also show that the describable attributes areexcellent texture descriptors, transferring between datasets and tasks; inparticular, combined with IFV, they significantly outperform thestate-of-the-art by more than 8 percent on both FMD and KTHTIPS-2b benchmarks.We also demonstrate that they produce intuitive descriptions of materials andInternet images.
arxiv-5100-151 | Smoothed Analysis of Tensor Decompositions | http://arxiv.org/pdf/1311.3651v4.pdf | author:Aditya Bhaskara, Moses Charikar, Ankur Moitra, Aravindan Vijayaraghavan category:cs.DS cs.LG stat.ML published:2013-11-14 summary:Low rank tensor decompositions are a powerful tool for learning generativemodels, and uniqueness results give them a significant advantage over matrixdecomposition methods. However, tensors pose significant algorithmic challengesand tensors analogs of much of the matrix algebra toolkit are unlikely to existbecause of hardness results. Efficient decomposition in the overcomplete case(where rank exceeds dimension) is particularly challenging. We introduce asmoothed analysis model for studying these questions and develop an efficientalgorithm for tensor decomposition in the highly overcomplete case (rankpolynomial in the dimension). In this setting, we show that our algorithm isrobust to inverse polynomial error -- a crucial property for applications inlearning since we are only allowed a polynomial number of samples. Whilealgorithms are known for exact tensor decomposition in some overcompletesettings, our main contribution is in analyzing their stability in theframework of smoothed analysis. Our main technical contribution is to show that tensor products of perturbedvectors are linearly independent in a robust sense (i.e. the associated matrixhas singular values that are at least an inverse polynomial). This key resultpaves the way for applying tensor methods to learning problems in the smoothedsetting. In particular, we use it to obtain results for learning multi-viewmodels and mixtures of axis-aligned Gaussians where there are many more"components" than dimensions. The assumption here is that the model is notadversarially chosen, formalized by a perturbation of model parameters. Webelieve this an appealing way to analyze realistic instances of learningproblems, since this framework allows us to overcome many of the usuallimitations of using tensor methods.
arxiv-5100-152 | Anytime Belief Propagation Using Sparse Domains | http://arxiv.org/pdf/1311.3368v1.pdf | author:Sameer Singh, Sebastian Riedel, Andrew McCallum category:stat.ML cs.AI cs.LG published:2013-11-14 summary:Belief Propagation has been widely used for marginal inference, however it isslow on problems with large-domain variables and high-order factors. Previouswork provides useful approximations to facilitate inference on such models, butlacks important anytime properties such as: 1) providing accurate andconsistent marginals when stopped early, 2) improving the approximation whenrun longer, and 3) converging to the fixed point of BP. To this end, we proposea message passing algorithm that works on sparse (partially instantiated)domains, and converges to consistent marginals using dynamic messagescheduling. The algorithm grows the sparse domains incrementally, selecting thenext value to add using prioritization schemes based on the gradients of themarginal inference objective. Our experiments demonstrate local anytimeconsistency and fast convergence, providing significant speedups over BP toobtain low-error marginals: up to 25 times on grid models, and up to 6 times ona real-world natural language processing task.
arxiv-5100-153 | Reproducing kernel Hilbert space based estimation of systems of ordinary differential equations | http://arxiv.org/pdf/1311.3576v2.pdf | author:Javier GonzÃ¡lez, Ivan VujaÄiÄ, Ernst Wit category:stat.ME stat.ML published:2013-11-14 summary:Non-linear systems of differential equations have attracted the interest infields like system biology, ecology or biochemistry, due to their flexibilityand their ability to describe dynamical systems. Despite the importance of suchmodels in many branches of science they have not been the focus of systematicstatistical analysis until recently. In this work we propose a general approachto estimate the parameters of systems of differential equations measured withnoise. Our methodology is based on the maximization of the penalized likelihoodwhere the system of differential equations is used as a penalty. To do so, weuse a Reproducing Kernel Hilbert Space approach that allows to formulate theestimation problem as an unconstrained numeric maximization problem easy tosolve. The proposed method is tested with synthetically simulated data and itis used to estimate the unobserved transcription factor CdaR in Steptomyescoelicolor using gene expression data of the genes it regulates.
arxiv-5100-154 | High-dimensional learning of linear causal networks via inverse covariance estimation | http://arxiv.org/pdf/1311.3492v1.pdf | author:Po-Ling Loh, Peter BÃ¼hlmann category:stat.ML math.ST stat.TH 62F12 published:2013-11-14 summary:We establish a new framework for statistical estimation of directed acyclicgraphs (DAGs) when data are generated from a linear, possibly non-Gaussianstructural equation model. Our framework consists of two parts: (1) inferringthe moralized graph from the support of the inverse covariance matrix; and (2)selecting the best-scoring graph amongst DAGs that are consistent with themoralized graph. We show that when the error variances are known or estimatedto close enough precision, the true DAG is the unique minimizer of the scorecomputed using the reweighted squared l_2-loss. Our population-level resultshave implications for the identifiability of linear SEMs when the errorcovariances are specified up to a constant multiple. On the statistical side,we establish rigorous conditions for high-dimensional consistency of ourtwo-part algorithm, defined in terms of a "gap" between the true DAG and thenext best candidate. Finally, we demonstrate that dynamic programming may beused to select the optimal DAG in linear time when the treewidth of themoralized graph is bounded.
arxiv-5100-155 | Fundamental Limits of Online and Distributed Algorithms for Statistical Learning and Estimation | http://arxiv.org/pdf/1311.3494v6.pdf | author:Ohad Shamir category:cs.LG stat.ML published:2013-11-14 summary:Many machine learning approaches are characterized by information constraintson how they interact with the training data. These include memory andsequential access constraints (e.g. fast first-order methods to solvestochastic optimization problems); communication constraints (e.g. distributedlearning); partial access to the underlying data (e.g. missing features andmulti-armed bandits) and more. However, currently we have little understandinghow such information constraints fundamentally affect our performance,independent of the learning problem semantics. For example, are there learningproblems where any algorithm which has small memory footprint (or can use anybounded number of bits from each example, or has certain communicationconstraints) will perform worse than what is possible without such constraints?In this paper, we describe how a single set of results implies positive answersto the above, for several different settings.
arxiv-5100-156 | Evolutionary perspectives on collective decision making: Studying the implications of diversity and social network structure with agent-based simulations | http://arxiv.org/pdf/1311.3674v1.pdf | author:Hiroki Sayama, Shelley D. Dionne, Francis J. Yammarino category:cs.MA cs.NE cs.SI physics.soc-ph published:2013-11-14 summary:Collective, especially group-based, managerial decision making is crucial inorganizations. Using an evolutionary theory approach to collective decisionmaking, agent-based simulations were conducted to investigate how collectivedecision making would be affected by the agents' diversity in problemunderstanding and/or behavior in discussion, as well as by their social networkstructure. Simulation results indicated that groups with consistent problemunderstanding tended to produce higher utility values of ideas and displayedbetter decision convergence, but only if there was no group-level bias incollective problem understanding. Simulation results also indicated theimportance of balance between selection-oriented (i.e., exploitative) andvariation-oriented (i.e., explorative) behaviors in discussion to achievequality final decisions. Expanding the group size and introducing non-trivialsocial network structure generally improved the quality of ideas at the cost ofdecision convergence. Simulations with different social network topologiesrevealed that collective decision making on small-world networks with highlocal clustering tended to achieve highest decision quality more often than onrandom or scale-free networks. Implications of this evolutionary theory andsimulation approach for future managerial research on collective, group, andmulti-level decision making are discussed.
arxiv-5100-157 | Architecture of an Ontology-Based Domain-Specific Natural Language Question Answering System | http://arxiv.org/pdf/1311.3175v1.pdf | author:Athira P. M., Sreeja M., P. C. Reghu Raj category:cs.CL cs.IR published:2013-11-13 summary:Question answering (QA) system aims at retrieving precise information from alarge collection of documents against a query. This paper describes thearchitecture of a Natural Language Question Answering (NLQA) system for aspecific domain based on the ontological information, a step towards semanticweb question answering. The proposed architecture defines four basic modulessuitable for enhancing current QA capabilities with the ability of processingcomplex questions. The first module was the question processing, which analysesand classifies the question and also reformulates the user query. The secondmodule allows the process of retrieving the relevant documents. The next moduleprocesses the retrieved documents, and the last module performs the extractionand generation of a response. Natural language processing techniques are usedfor processing the question and documents and also for answer extraction.Ontology and domain knowledge are used for reformulating queries andidentifying the relations. The aim of the system is to generate short andspecific answer to the question that is asked in the natural language in aspecific domain. We have achieved 94 % accuracy of natural language questionanswering in our implementation.
arxiv-5100-158 | Stochastic inference with deterministic spiking neurons | http://arxiv.org/pdf/1311.3211v1.pdf | author:Mihai A. Petrovici, Johannes Bill, Ilja Bytschok, Johannes Schemmel, Karlheinz Meier category:q-bio.NC cs.NE physics.bio-ph 92-08 C.1.3; I.5.1 published:2013-11-13 summary:The seemingly stochastic transient dynamics of neocortical circuits observedin vivo have been hypothesized to represent a signature of ongoing stochasticinference. In vitro neurons, on the other hand, exhibit a highly deterministicresponse to various types of stimulation. We show that an ensemble ofdeterministic leaky integrate-and-fire neurons embedded in a spiking noisyenvironment can attain the correct firing statistics in order to sample from awell-defined target distribution. We provide an analytical derivation of theactivation function on the single cell level; for recurrent networks, weexamine convergence towards stationarity in computer simulations anddemonstrate sample-based Bayesian inference in a mixed graphical model. Thisestablishes a rigorous link between deterministic neuron models and functionalstochastic dynamics on the network level.
arxiv-5100-159 | Nonparametric Estimation of Multi-View Latent Variable Models | http://arxiv.org/pdf/1311.3287v2.pdf | author:Le Song, Animashree Anandkumar, Bo Dai, Bo Xie category:cs.LG stat.ML published:2013-11-13 summary:Spectral methods have greatly advanced the estimation of latent variablemodels, generating a sequence of novel and efficient algorithms with strongtheoretical guarantees. However, current spectral algorithms are largelyrestricted to mixtures of discrete or Gaussian distributions. In this paper, wepropose a kernel method for learning multi-view latent variable models,allowing each mixture component to be nonparametric. The key idea of the methodis to embed the joint distribution of a multi-view latent variable into areproducing kernel Hilbert space, and then the latent parameters are recoveredusing a robust tensor power method. We establish that the sample complexity forthe proposed method is quadratic in the number of latent components and is alow order polynomial in the other relevant parameters. Thus, our non-parametrictensor approach to learning latent variable models enjoys good sample andcomputational efficiencies. Moreover, the non-parametric tensor power methodcompares favorably to EM algorithm and other existing spectral algorithms inour experiments.
arxiv-5100-160 | Learning Input and Recurrent Weight Matrices in Echo State Networks | http://arxiv.org/pdf/1311.2987v1.pdf | author:Hamid Palangi, Li Deng, Rabab K Ward category:cs.LG published:2013-11-13 summary:Echo State Networks (ESNs) are a special type of the temporally deep networkmodel, the Recurrent Neural Network (RNN), where the recurrent matrix iscarefully designed and both the recurrent and input matrices are fixed. An ESNuses the linearity of the activation function of the output units to simplifythe learning of the output matrix. In this paper, we devise a special techniquethat take advantage of this linearity in the output units of an ESN, to learnthe input and recurrent matrices. This has not been done in earlier ESNs due totheir well known difficulty in learning those matrices. Compared to thetechnique of BackPropagation Through Time (BPTT) in learning general RNNs, ourproposed method exploits linearity of activation function in the output unitsto formulate the relationships amongst the various matrices in an RNN. Theserelationships results in the gradient of the cost function having an analyticalform and being more accurate. This would enable us to compute the gradientsinstead of obtaining them by recursion as in BPTT. Experimental results onphone state classification show that learning one or both the input andrecurrent matrices in an ESN yields superior results compared to traditionalESNs that do not learn these matrices, especially when longer time steps areused.
arxiv-5100-161 | Compressive Nonparametric Graphical Model Selection For Time Series | http://arxiv.org/pdf/1311.3257v2.pdf | author:Alexander Jung, Reinhard Heckel, Helmut BÃ¶lcskei, Franz Hlawatsch category:stat.ML published:2013-11-13 summary:We propose a method for inferring the conditional indepen- dence graph (CIG)of a high-dimensional discrete-time Gaus- sian vector random process fromfinite-length observations. Our approach does not rely on a parametric model(such as, e.g., an autoregressive model) for the vector random process; rather,it only assumes certain spectral smoothness proper- ties. The proposedinference scheme is compressive in that it works for sample sizes that are(much) smaller than the number of scalar process components. We provideanalytical conditions for our method to correctly identify the CIG with highprobability.
arxiv-5100-162 | smart application for AMS using Face Recognition | http://arxiv.org/pdf/1401.6130v1.pdf | author:MuthuKalyani. K, VeeraMuthu. A category:cs.CY cs.CV published:2013-11-13 summary:Attendance Management System (AMS) can be made into smarter way by using facerecognition technique, where we use a CCTV camera to be fixed at the entrypoint of a classroom, which automatically captures the image of the person andchecks the observed image with the face database using android enhanced smartphone. It is typically used for two purposes. Firstly, marking attendance forstudent by comparing the face images produced recently and secondly,recognition of human who are strange to the environment i.e. an unauthorizedperson For verification of image, a newly emerging trend 3D Face Recognition isused which claims to provide more accuracy in matching the image databases andhas an ability to recognize a subject at different view angles.
arxiv-5100-163 | On a non-local spectrogram for denoising one-dimensional signals | http://arxiv.org/pdf/1311.3269v1.pdf | author:Gonzalo Galiano, JuliÃ¡n Velasco category:cs.CV published:2013-11-13 summary:In previous works, we investigated the use of local filters based on partialdifferential equations (PDE) to denoise one-dimensional signals through theimage processing of time-frequency representations, such as the spectrogram. Inthis image denoising algorithms, the particularity of the image was hardlytaken into account. We turn, in this paper, to study the performance ofnon-local filters, like Neighborhood or Yaroslavsky filters, in the sameproblem. We show that, for certain iterative schemes involving the Neighborhoodfilter, the computational time is drastically reduced with respect toYaroslavsky or nonlinear PDE based filters, while the outputs of the filteringprocesses are similar. This is heuristically justified by the connectionbetween the (fast) Neighborhood filter applied to a spectrogram and thecorresponding Nonlocal Means filter (accurate) applied to the Wigner-Villedistribution of the signal. This correspondence holds only for time-frequencyrepresentations of one-dimensional signals, not to usual images, and in thissense the particularity of the image is exploited. We compare though a seriesof experiments on synthetic and biomedical signals the performance of local andnon-local filters.
arxiv-5100-164 | Sparse Matrix Factorization | http://arxiv.org/pdf/1311.3315v3.pdf | author:Behnam Neyshabur, Rina Panigrahy category:cs.LG stat.ML published:2013-11-13 summary:We investigate the problem of factorizing a matrix into several sparsematrices and propose an algorithm for this under randomness and sparsityassumptions. This problem can be viewed as a simplification of the deeplearning problem where finding a factorization corresponds to finding edges indifferent layers and values of hidden units. We prove that under certainassumptions for a sparse linear deep network with $n$ nodes in each layer, ouralgorithm is able to recover the structure of the network and values of toplayer hidden units for depths up to $\tilde O(n^{1/6})$. We further discuss therelation among sparse matrix factorization, deep learning, sparse recovery anddictionary learning.
arxiv-5100-165 | Informed Source Separation: A Bayesian Tutorial | http://arxiv.org/pdf/1311.3001v1.pdf | author:Kevin H. Knuth category:stat.ML cs.LG published:2013-11-13 summary:Source separation problems are ubiquitous in the physical sciences; anysituation where signals are superimposed calls for source separation toestimate the original signals. In this tutorial I will discuss the Bayesianapproach to the source separation problem. This approach has a specificadvantage in that it requires the designer to explicitly describe the signalmodel in addition to any other information or assumptions that go into theproblem description. This leads naturally to the idea of informed sourceseparation, where the algorithm design incorporates relevant information aboutthe specific problem. This approach promises to enable researchers to designtheir own high-quality algorithms that are specifically tailored to the problemat hand.
arxiv-5100-166 | UW SPF: The University of Washington Semantic Parsing Framework | http://arxiv.org/pdf/1311.3011v1.pdf | author:Yoav Artzi, Luke Zettlemoyer category:cs.CL published:2013-11-13 summary:The University of Washington Semantic Parsing Framework (SPF) is a learningand inference framework for mapping natural language to formal representationof its meaning.
arxiv-5100-167 | A Study of Actor and Action Semantic Retention in Video Supervoxel Segmentation | http://arxiv.org/pdf/1311.3318v1.pdf | author:Chenliang Xu, Richard F. Doell, Stephen JosÃ© Hanson, Catherine Hanson, Jason J. Corso category:cs.CV published:2013-11-13 summary:Existing methods in the semantic computer vision community seem unable todeal with the explosion and richness of modern, open-source and social videocontent. Although sophisticated methods such as object detection orbag-of-words models have been well studied, they typically operate on low levelfeatures and ultimately suffer from either scalability issues or a lack ofsemantic meaning. On the other hand, video supervoxel segmentation has recentlybeen established and applied to large scale data processing, which potentiallyserves as an intermediate representation to high level video semanticextraction. The supervoxels are rich decompositions of the video content: theycapture object shape and motion well. However, it is not yet known if thesupervoxel segmentation retains the semantics of the underlying video content.In this paper, we conduct a systematic study of how well the actor and actionsemantics are retained in video supervoxel segmentation. Our study has humanobservers watching supervoxel segmentation videos and trying to discriminateboth actor (human or animal) and action (one of eight everyday actions). Wegather and analyze a large set of 640 human perceptions over 96 videos in 3different supervoxel scales. Furthermore, we conduct machine recognitionexperiments on a feature defined on supervoxel segmentation, called supervoxelshape context, which is inspired by the higher order processes in humanperception. Our ultimate findings suggest that a significant amount ofsemantics have been well retained in the video supervoxel segmentation and canbe used for further video analysis.
arxiv-5100-168 | An Efficient Method for Recognizing the Low Quality Fingerprint Verification by Means of Cross Correlation | http://arxiv.org/pdf/1311.3076v1.pdf | author:V. Karthikeyan, V. J. Vijayalakshmi category:cs.CV published:2013-11-13 summary:In this paper, we propose an efficient method to provide personalidentification using fingerprint to get better accuracy even in noisycondition. The fingerprint matching based on the number of correspondingminutia pairings, has been in use for a long time, which is not very efficientfor recognizing the low quality fingerprints. To overcome this problem,correlation technique is used. The correlation-based fingerprint verificationsystem is capable of dealing with low quality images from which no minutiae canbe extracted reliably and with fingerprints that suffer from non-uniform shapedistortions, also in case of damaged and partial images. Orientation FieldMethodology (OFM) has been used as a preprocessing module, and it converts theimages into a field pattern based on the direction of the ridges, loops andbifurcations in the image of a fingerprint. The input image is then CrossCorrelated (CC) with all the images in the cluster and the highest correlatedimage is taken as the output. The result gives a good recognition rate, as theproposed scheme uses Cross Correlation of Field Orientation (CCFO = OFM + CC)for fingerprint identification.
arxiv-5100-169 | Spectral Clustering via the Power Method -- Provably | http://arxiv.org/pdf/1311.2854v3.pdf | author:Christos Boutsidis, Alex Gittens, Prabhanjan Kambadur category:cs.LG cs.NA published:2013-11-12 summary:Spectral clustering is one of the most important algorithms in data miningand machine intelligence; however, its computational complexity limits itsapplication to truly large scale data analysis. The computational bottleneck inspectral clustering is computing a few of the top eigenvectors of the(normalized) Laplacian matrix corresponding to the graph representing the datato be clustered. One way to speed up the computation of these eigenvectors isto use the "power method" from the numerical linear algebra literature.Although the power method has been empirically used to speed up spectralclustering, the theory behind this approach, to the best of our knowledge,remains unexplored. This paper provides the \emph{first} such rigoroustheoretical justification, arguing that a small number of power iterationssuffices to obtain near-optimal partitionings using the approximateeigenvectors. Specifically, we prove that solving the $k$-means clusteringproblem on the approximate eigenvectors obtained via the power method gives anadditive-error approximation to solving the $k$-means problem on the optimaleigenvectors.
arxiv-5100-170 | Hypothesis Testing for Automated Community Detection in Networks | http://arxiv.org/pdf/1311.2694v2.pdf | author:Peter J. Bickel, Purnamrita Sarkar category:stat.ML cs.LG cs.SI physics.soc-ph published:2013-11-12 summary:Community detection in networks is a key exploratory tool with applicationsin a diverse set of areas, ranging from finding communities in social andbiological networks to identifying link farms in the World Wide Web. Theproblem of finding communities or clusters in a network has received muchattention from statistics, physics and computer science. However, mostclustering algorithms assume knowledge of the number of clusters k. In thispaper we propose to automatically determine k in a graph generated from aStochastic Blockmodel. Our main contribution is twofold; first, wetheoretically establish the limiting distribution of the principal eigenvalueof the suitably centered and scaled adjacency matrix, and use that distributionfor our hypothesis test. Secondly, we use this test to design a recursivebipartitioning algorithm. Using quantifiable classification tasks on real worldnetworks with ground truth, we show that our algorithm outperforms existingprobabilistic models for learning overlapping clusters, and on unlabelednetworks, we show that we uncover nested community structure.
arxiv-5100-171 | Learning Mixtures of Discrete Product Distributions using Spectral Decompositions | http://arxiv.org/pdf/1311.2972v2.pdf | author:Prateek Jain, Sewoong Oh category:stat.ML cs.CC cs.IT cs.LG math.IT published:2013-11-12 summary:We study the problem of learning a distribution from samples, when theunderlying distribution is a mixture of product distributions over discretedomains. This problem is motivated by several practical applications such ascrowd-sourcing, recommendation systems, and learning Boolean functions. Theexisting solutions either heavily rely on the fact that the number ofcomponents in the mixtures is finite or have sample/time complexity that isexponential in the number of components. In this paper, we introduce apolynomial time/sample complexity method for learning a mixture of $r$ discreteproduct distributions over $\{1, 2, \dots, \ell\}^n$, for general $\ell$ and$r$. We show that our approach is statistically consistent and further providefinite sample guarantees. We use techniques from the recent work on tensor decompositions forhigher-order moment matching. A crucial step in these moment matching methodsis to construct a certain matrix and a certain tensor with low-rank spectraldecompositions. These tensors are typically estimated directly from thesamples. The main challenge in learning mixtures of discrete productdistributions is that these low-rank tensors cannot be obtained directly fromthe sample moments. Instead, we reduce the tensor estimation problem to: $a$)estimating a low-rank matrix using only off-diagonal block elements; and $b$)estimating a tensor using a small number of linear measurements. Leveraging onrecent developments in matrix completion, we give an alternating minimizationbased method to estimate the low-rank matrix, and formulate the tensorcompletion problem as a least-squares problem.
arxiv-5100-172 | A PAC-Bayesian bound for Lifelong Learning | http://arxiv.org/pdf/1311.2838v2.pdf | author:Anastasia Pentina, Christoph H. Lampert category:stat.ML cs.LG 68T05 published:2013-11-12 summary:Transfer learning has received a lot of attention in the machine learningcommunity over the last years, and several effective algorithms have beendeveloped. However, relatively little is known about their theoreticalproperties, especially in the setting of lifelong learning, where the goal isto transfer information to tasks for which no data have been observed so far.In this work we study lifelong learning from a theoretical perspective. Ourmain result is a PAC-Bayesian generalization bound that offers a unified viewon existing paradigms for transfer learning, such as the transfer of parametersor the transfer of low-dimensional representations. We also use the bound toderive two principled lifelong learning algorithms, and we show that theseyield results comparable with existing methods.
arxiv-5100-173 | DinTucker: Scaling up Gaussian process models on multidimensional arrays with billions of elements | http://arxiv.org/pdf/1311.2663v5.pdf | author:Shandian Zhe, Yuan Qi, Youngja Park, Ian Molloy, Suresh Chari category:cs.LG cs.DC stat.ML published:2013-11-12 summary:Infinite Tucker Decomposition (InfTucker) and random function prior models,as nonparametric Bayesian models on infinite exchangeable arrays, are morepowerful models than widely-used multilinear factorization methods includingTucker and PARAFAC decomposition, (partly) due to their capability of modelingnonlinear relationships between array elements. Despite their great predictiveperformance and sound theoretical foundations, they cannot handle massive datadue to a prohibitively high training time. To overcome this limitation, wepresent Distributed Infinite Tucker (DINTUCKER), a large-scale nonlinear tensordecomposition algorithm on MAPREDUCE. While maintaining the predictive accuracyof InfTucker, it is scalable on massive data. DINTUCKER is based on a newhierarchical Bayesian model that enables local training of InfTucker onsubarrays and information integration from all local training results. We usedistributed stochastic gradient descent, coupled with variational inference, totrain this model. We apply DINTUCKER to multidimensional arrays with billionsof elements from applications in the "Read the Web" project (Carlson et al.,2010) and in information security and compare it with the state-of-the-artlarge-scale tensor decomposition method, GigaTensor. On both datasets,DINTUCKER achieves significantly higher prediction accuracy with lesscomputational time.
arxiv-5100-174 | The More, the Merrier: the Blessing of Dimensionality for Learning Large Gaussian Mixtures | http://arxiv.org/pdf/1311.2891v3.pdf | author:Joseph Anderson, Mikhail Belkin, Navin Goyal, Luis Rademacher, James Voss category:cs.LG cs.DS stat.ML published:2013-11-12 summary:In this paper we show that very large mixtures of Gaussians are efficientlylearnable in high dimension. More precisely, we prove that a mixture with knownidentical covariance matrices whose number of components is a polynomial of anyfixed degree in the dimension n is polynomially learnable as long as a certainnon-degeneracy condition on the means is satisfied. It turns out that thiscondition is generic in the sense of smoothed complexity, as soon as thedimensionality of the space is high enough. Moreover, we prove that no suchcondition can possibly exist in low dimension and the problem of learning theparameters is generically hard. In contrast, much of the existing work onGaussian Mixtures relies on low-dimensional projections and thus hits anartificial barrier. Our main result on mixture recovery relies on a new"Poissonization"-based technique, which transforms a mixture of Gaussians to alinear map of a product distribution. The problem of learning this map can beefficiently solved using some recent results on tensor decompositions andIndependent Component Analysis (ICA), thus giving an algorithm for recoveringthe mixture. In addition, we combine our low-dimensional hardness results forGaussian mixtures with Poissonization to show how to embed difficult instancesof low-dimensional Gaussian mixtures into the ICA setting, thus establishingexponential information-theoretic lower bounds for underdetermined ICA in lowdimension. To the best of our knowledge, this is the first such result in theliterature. In addition to contributing to the problem of Gaussian mixturelearning, we believe that this work is among the first steps toward betterunderstanding the rare phenomenon of the "blessing of dimensionality" in thecomputational aspects of statistical inference.
arxiv-5100-175 | Authorship Attribution Using Word Network Features | http://arxiv.org/pdf/1311.2978v1.pdf | author:Shibamouli Lahiri, Rada Mihalcea category:cs.CL published:2013-11-12 summary:In this paper, we explore a set of novel features for authorship attributionof documents. These features are derived from a word network representation ofnatural language text. As has been noted in previous studies, natural languagetends to show complex network structure at word level, with low degrees ofseparation and scale-free (power law) degree distribution. There has also beenwork on authorship attribution that incorporates ideas from complex networks.The goal of our paper is to explore properties of these complex networks thatare suitable as features for machine-learning-based authorship attribution ofdocuments. We performed experiments on three different datasets, and obtainedpromising results.
arxiv-5100-176 | Approximate Inference in Continuous Determinantal Point Processes | http://arxiv.org/pdf/1311.2971v1.pdf | author:Raja Hafiz Affandi, Emily B. Fox, Ben Taskar category:stat.ML cs.LG stat.ME published:2013-11-12 summary:Determinantal point processes (DPPs) are random point processes well-suitedfor modeling repulsion. In machine learning, the focus of DPP-based models hasbeen on diverse subset selection from a discrete and finite base set. Thisdiscrete setting admits an efficient sampling algorithm based on theeigendecomposition of the defining kernel matrix. Recently, there has beengrowing interest in using DPPs defined on continuous spaces. While thediscrete-DPP sampler extends formally to the continuous case, computationally,the steps required are not tractable in general. In this paper, we present twoefficient DPP sampling schemes that apply to a wide range of kernel functions:one based on low rank approximations via Nystrom and random Fourier featuretechniques and another based on Gibbs sampling. We demonstrate the utility ofcontinuous DPPs in repulsive mixture modeling and synthesizing human posesspanning activity spaces.
arxiv-5100-177 | Multiple Closed-Form Local Metric Learning for K-Nearest Neighbor Classifier | http://arxiv.org/pdf/1311.3157v1.pdf | author:Jianbo Ye category:cs.LG published:2013-11-12 summary:Many researches have been devoted to learn a Mahalanobis distance metric,which can effectively improve the performance of kNN classification. Mostapproaches are iterative and computational expensive and linear rigidity stillcritically limits metric learning algorithm to perform better. We proposed acomputational economical framework to learn multiple metrics in closed-form.
arxiv-5100-178 | Visualizing and Understanding Convolutional Networks | http://arxiv.org/pdf/1311.2901v3.pdf | author:Matthew D Zeiler, Rob Fergus category:cs.CV published:2013-11-12 summary:Large Convolutional Network models have recently demonstrated impressiveclassification performance on the ImageNet benchmark. However there is no clearunderstanding of why they perform so well, or how they might be improved. Inthis paper we address both issues. We introduce a novel visualization techniquethat gives insight into the function of intermediate feature layers and theoperation of the classifier. We also perform an ablation study to discover theperformance contribution from different model layers. This enables us to findmodel architectures that outperform Krizhevsky \etal on the ImageNetclassification benchmark. We show our ImageNet model generalizes well to otherdatasets: when the softmax classifier is retrained, it convincingly beats thecurrent state-of-the-art results on Caltech-101 and Caltech-256 datasets.
arxiv-5100-179 | Sampling Based Approaches to Handle Imbalances in Network Traffic Dataset for Machine Learning Techniques | http://arxiv.org/pdf/1311.2677v1.pdf | author:Raman Singh, Harish Kumar, R. K. Singla category:cs.NI cs.CR cs.LG published:2013-11-12 summary:Network traffic data is huge, varying and imbalanced because various classesare not equally distributed. Machine learning (ML) algorithms for trafficanalysis uses the samples from this data to recommend the actions to be takenby the network administrators as well as training. Due to imbalances indataset, it is difficult to train machine learning algorithms for trafficanalysis and these may give biased or false results leading to seriousdegradation in performance of these algorithms. Various techniques can beapplied during sampling to minimize the effect of imbalanced instances. In thispaper various sampling techniques have been analysed in order to compare thedecrease in variation in imbalances of network traffic datasets sampled forthese algorithms. Various parameters like missing classes in samples,probability of sampling of the different instances have been considered forcomparison.
arxiv-5100-180 | Aggregation of Affine Estimators | http://arxiv.org/pdf/1311.2799v1.pdf | author:Dong Dai, Philippe Rigollet, Lucy Xia, Tong Zhang category:math.ST cs.LG stat.TH 62G08 published:2013-11-12 summary:We consider the problem of aggregating a general collection of affineestimators for fixed design regression. Relevant examples include some commonlyused statistical estimators such as least squares, ridge and robust leastsquares estimators. Dalalyan and Salmon (2012) have established that, for thisproblem, exponentially weighted (EW) model selection aggregation leads to sharporacle inequalities in expectation, but similar bounds in deviation were notpreviously known. While results indicate that the same aggregation scheme maynot satisfy sharp oracle inequalities with high probability, we prove that aweaker notion of oracle inequality for EW that holds with high probability.Moreover, using a generalization of the newly introduced $Q$-aggregation schemewe also prove sharp oracle inequalities that hold with high probability.Finally, we apply our results to universal aggregation and show that ourproposed estimator leads simultaneously to all the best known bounds foraggregation, including $\ell_q$-aggregation, $q \in (0,1)$, with highprobability.
arxiv-5100-181 | Verifiable Source Code Documentation in Controlled Natural Language | http://arxiv.org/pdf/1311.2702v1.pdf | author:Tobias Kuhn, Alexandre Bergel category:cs.SE cs.AI cs.CL cs.HC cs.LO H.5.2; D.2.7 published:2013-11-12 summary:Writing documentation about software internals is rarely considered arewarding activity. It is highly time-consuming and the resulting documentationis fragile when the software is continuously evolving in a multi-developersetting. Unfortunately, traditional programming environments poorly support thewriting and maintenance of documentation. Consequences are severe as the lackof documentation on software structure negatively impacts the overall qualityof the software product. We show that using a controlled natural language witha reasoner and a query engine is a viable technique for verifying theconsistency and accuracy of documentation and source code. Using ACE, astate-of-the-art controlled natural language, we present positive results onthe comprehensibility and the general feasibility of creating and verifyingdocumentation. As a case study, we used automatic documentation verification toidentify and fix severe flaws in the architecture of a non-trivial piece ofsoftware. Moreover, a user experiment shows that our language is faster andeasier to learn and understand than other formal languages for softwaredocumentation.
arxiv-5100-182 | When Does More Regularization Imply Fewer Degrees of Freedom? Sufficient Conditions and Counter Examples from Lasso and Ridge Regression | http://arxiv.org/pdf/1311.2791v1.pdf | author:Shachar Kaufman, Saharon Rosset category:math.ST stat.ML stat.TH published:2013-11-12 summary:Regularization aims to improve prediction performance of a given statisticalmodeling approach by moving to a second approach which achieves worse trainingerror but is expected to have fewer degrees of freedom, i.e., better agreementbetween training and prediction error. We show here, however, that thisexpected behavior does not hold in general. In fact, counter examples are giventhat show regularization can increase the degrees of freedom in simplesituations, including lasso and ridge regression, which are the most commonregularization approaches in use. In such situations, the regularizationincreases both training error and degrees of freedom, and is thus inherentlywithout merit. On the other hand, two important regularization scenarios aredescribed where the expected reduction in degrees of freedom is indeedguaranteed: (a) all symmetric linear smoothers, and (b) linear regressionversus convex constrained linear regression (as in the constrained variant ofridge regression and lasso).
arxiv-5100-183 | Deep neural networks for single channel source separation | http://arxiv.org/pdf/1311.2746v1.pdf | author:Emad M. Grais, Mehmet Umut Sen, Hakan Erdogan category:cs.NE cs.LG published:2013-11-12 summary:In this paper, a novel approach for single channel source separation (SCSS)using a deep neural network (DNN) architecture is introduced. Unlike previousstudies in which DNN and other classifiers were used for classifyingtime-frequency bins to obtain hard masks for each source, we use the DNN toclassify estimated source spectra to check for their validity duringseparation. In the training stage, the training data for the source signals areused to train a DNN. In the separation stage, the trained DNN is utilized toaid in estimation of each source in the mixed signal. Single channel sourceseparation problem is formulated as an energy minimization problem where eachsource spectra estimate is encouraged to fit the trained DNN model and themixed signal spectrum is encouraged to be written as a weighted sum of theestimated source spectra. The proposed approach works regardless of the energyscale differences between the source signals in the training and separationstages. Nonnegative matrix factorization (NMF) is used to initialize the DNNestimate for each source. The experimental results show that using DNNinitialized by NMF for source separation improves the quality of the separatedsignal compared with using NMF for source separation.
arxiv-5100-184 | Toward a unified theory of sparse dimensionality reduction in Euclidean space | http://arxiv.org/pdf/1311.2542v4.pdf | author:Jean Bourgain, Sjoerd Dirksen, Jelani Nelson category:cs.DS cs.CG cs.IT math.IT math.PR stat.ML published:2013-11-11 summary:Let $\Phi\in\mathbb{R}^{m\times n}$ be a sparse Johnson-Lindenstrausstransform [KN14] with $s$ non-zeroes per column. For a subset $T$ of the unitsphere, $\varepsilon\in(0,1/2)$ given, we study settings for $m,s$ required toensure $$ \mathop{\mathbb{E}}_\Phi \sup_{x\in T} \left\\Phi x\_2^2 - 1\right < \varepsilon , $$ i.e. so that $\Phi$ preserves the norm of every$x\in T$ simultaneously and multiplicatively up to $1+\varepsilon$. Weintroduce a new complexity parameter, which depends on the geometry of $T$, andshow that it suffices to choose $s$ and $m$ such that this parameter is small.Our result is a sparse analog of Gordon's theorem, which was concerned with adense $\Phi$ having i.i.d. Gaussian entries. We qualitatively unify severalresults related to the Johnson-Lindenstrauss lemma, subspace embeddings, andFourier-based restricted isometries. Our work also implies new results in usingthe sparse Johnson-Lindenstrauss transform in numerical linear algebra,classical and model-based compressed sensing, manifold learning, andconstrained least squares problems such as the Lasso.
arxiv-5100-185 | Rich feature hierarchies for accurate object detection and semantic segmentation | http://arxiv.org/pdf/1311.2524v5.pdf | author:Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik category:cs.CV published:2013-11-11 summary:Object detection performance, as measured on the canonical PASCAL VOCdataset, has plateaued in the last few years. The best-performing methods arecomplex ensemble systems that typically combine multiple low-level imagefeatures with high-level context. In this paper, we propose a simple andscalable detection algorithm that improves mean average precision (mAP) by morethan 30% relative to the previous best result on VOC 2012---achieving a mAP of53.3%. Our approach combines two key insights: (1) one can apply high-capacityconvolutional neural networks (CNNs) to bottom-up region proposals in order tolocalize and segment objects and (2) when labeled training data is scarce,supervised pre-training for an auxiliary task, followed by domain-specificfine-tuning, yields a significant performance boost. Since we combine regionproposals with CNNs, we call our method R-CNN: Regions with CNN features. Wealso compare R-CNN to OverFeat, a recently proposed sliding-window detectorbased on a similar CNN architecture. We find that R-CNN outperforms OverFeat bya large margin on the 200-class ILSVRC2013 detection dataset. Source code forthe complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.
arxiv-5100-186 | An Empirical Evaluation of Sequence-Tagging Trainers | http://arxiv.org/pdf/1311.2378v1.pdf | author:P. Balamurugan, Shirish Shevade, S. Sundararajan, S. S Keerthi category:cs.LG published:2013-11-11 summary:The task of assigning label sequences to a set of observed sequences iscommon in computational linguistics. Several models for sequence labeling havebeen proposed over the last few years. Here, we focus on discriminative modelsfor sequence labeling. Many batch and online (updating model parameters aftervisiting each example) learning algorithms have been proposed in theliterature. On large datasets, online algorithms are preferred as batchlearning methods are slow. These online algorithms were designed to solveeither a primal or a dual problem. However, there has been no systematiccomparison of these algorithms in terms of their speed, generalizationperformance (accuracy/likelihood) and their ability to achieve steady stategeneralization performance fast. With this aim, we compare different algorithmsand make recommendations, useful for a practitioner. We conclude that theselection of an algorithm for sequence labeling depends on the evaluationcriterion used and its implementation simplicity.
arxiv-5100-187 | Program Evaluation and Causal Inference with High-Dimensional Data | http://arxiv.org/pdf/1311.2645v6.pdf | author:Alexandre Belloni, Victor Chernozhukov, Ivan FernÃ¡ndez-Val, Chris Hansen category:math.ST stat.ME stat.ML stat.TH published:2013-11-11 summary:In this paper, we provide efficient estimators and honest confidence bandsfor a variety of treatment effects including local average (LATE) and localquantile treatment effects (LQTE) in data-rich environments. We can handle verymany control variables, endogenous receipt of treatment, heterogeneoustreatment effects, and function-valued outcomes. Our framework covers thespecial case of exogenous receipt of treatment, either conditional on controlsor unconditionally as in randomized control trials. In the latter case, ourapproach produces efficient estimators and honest bands for (functional)average treatment effects (ATE) and quantile treatment effects (QTE). To makeinformative inference possible, we assume that key reduced form predictiverelationships are approximately sparse. This assumption allows the use ofregularization and selection methods to estimate those relations, and weprovide methods for post-regularization and post-selection inference that areuniformly valid (honest) across a wide-range of models. We show that a keyingredient enabling honest inference is the use of orthogonal or doubly robustmoment conditions in estimating certain reduced form functional parameters. Weillustrate the use of the proposed methods with an application to estimatingthe effect of 401(k) eligibility and participation on accumulated assets.
arxiv-5100-188 | Learning Mixtures of Linear Classifiers | http://arxiv.org/pdf/1311.2547v4.pdf | author:Yuekai Sun, Stratis Ioannidis, Andrea Montanari category:cs.LG stat.ML published:2013-11-11 summary:We consider a discriminative learning (regression) problem, whereby theregression function is a convex combination of k linear classifiers. Existingapproaches are based on the EM algorithm, or similar techniques, withoutprovable guarantees. We develop a simple method based on spectral techniquesand a `mirroring' trick, that discovers the subspace spanned by theclassifiers' parameter vectors. Under a probabilistic assumption on the featurevector distribution, we prove that this approach has nearly optimal statisticalefficiency.
arxiv-5100-189 | Global Sensitivity Analysis with Dependence Measures | http://arxiv.org/pdf/1311.2483v1.pdf | author:SÃ©bastien Da Veiga category:math.ST cs.LG stat.ML stat.TH published:2013-11-11 summary:Global sensitivity analysis with variance-based measures suffers from severaltheoretical and practical limitations, since they focus only on the variance ofthe output and handle multivariate variables in a limited way. In this paper,we introduce a new class of sensitivity indices based on dependence measureswhich overcomes these insufficiencies. Our approach originates from the idea tocompare the output distribution with its conditional counterpart when one ofthe input variables is fixed. We establish that this comparison yieldspreviously proposed indices when it is performed with Csiszar f-divergences, aswell as sensitivity indices which are well-known dependence measures betweenrandom variables. This leads us to investigate completely new sensitivityindices based on recent state-of-the-art dependence measures, such as distancecorrelation and the Hilbert-Schmidt independence criterion. We also emphasizethe potential of feature selection techniques relying on such dependencemeasures as alternatives to screening in high dimension.
arxiv-5100-190 | Performing edge detection by difference of Gaussians using q-Gaussian kernels | http://arxiv.org/pdf/1311.2561v2.pdf | author:Lucas Assirati, NÃºbia R. da Silva, Lilian Berton, Alneu de A. Lopes, Odemir M. Bruno category:cs.CV published:2013-11-11 summary:In image processing, edge detection is a valuable tool to perform theextraction of features from an image. This detection reduces the amount ofinformation to be processed, since the redundant information (considered lessrelevant) can be unconsidered. The technique of edge detection consists ofdetermining the points of a digital image whose intensity changes sharply. Thischanges are due to the discontinuities of the orientation on a surface forexample. A well known method of edge detection is the Difference of Gaussians(DoG). The method consists of subtracting two Gaussians, where a kernel has astandard deviation smaller than the previous one. The convolution between thesubtraction of kernels and the input image results in the edge detection ofthis image. This paper introduces a method of extracting edges using DoG withkernels based on the q-Gaussian probability distribution, derived from theq-statistic proposed by Constantino Tsallis. To demonstrate the method'spotential, we compare the introduced method with the traditional DoG usingGaussians kernels. The results showed that the proposed method can extractedges with more accurate details.
arxiv-5100-191 | Notes on Elementary Spectral Graph Theory. Applications to Graph Clustering Using Normalized Cuts | http://arxiv.org/pdf/1311.2492v1.pdf | author:Jean Gallier category:cs.CV published:2013-11-11 summary:These are notes on the method of normalized graph cuts and its applicationsto graph clustering. I provide a fairly thorough treatment of this deeplyoriginal method due to Shi and Malik, including complete proofs. I include thenecessary background on graphs and graph Laplacians. I then explain in detailhow the eigenvectors of the graph Laplacian can be used to draw a graph. Thisis an attractive application of graph Laplacians. The main thrust of this paperis the method of normalized cuts. I give a detailed account for K = 2 clusters,and also for K > 2 clusters, based on the work of Yu and Shi. Three points thatdo not appear to have been clearly articulated before are elaborated: 1. The solutions of the main optimization problem should be viewed as tuplesin the K-fold cartesian product of projective space RP^{N-1}. 2. When K > 2, the solutions of the relaxed problem should be viewed aselements of the Grassmannian G(K,N). 3. Two possible Riemannian distances are available to compare the closenessof solutions: (a) The distance on (RP^{N-1})^K. (b) The distance on theGrassmannian. I also clarify what should be the necessary and sufficient conditions for amatrix to represent a partition of the vertices of a graph to be clustered.
arxiv-5100-192 | The Noisy Power Method: A Meta Algorithm with Applications | http://arxiv.org/pdf/1311.2495v4.pdf | author:Moritz Hardt, Eric Price category:cs.DS cs.LG published:2013-11-11 summary:We provide a new robust convergence analysis of the well-known power methodfor computing the dominant singular vectors of a matrix that we call the noisypower method. Our result characterizes the convergence behavior of thealgorithm when a significant amount noise is introduced after eachmatrix-vector multiplication. The noisy power method can be seen as ameta-algorithm that has recently found a number of important applications in abroad range of machine learning problems including alternating minimization formatrix completion, streaming principal component analysis (PCA), andprivacy-preserving spectral analysis. Our general analysis subsumes severalexisting ad-hoc convergence bounds and resolves a number of open problems inmultiple applications including streaming PCA and privacy-preserving singularvector computation.
arxiv-5100-193 | Predictable Feature Analysis | http://arxiv.org/pdf/1311.2503v1.pdf | author:Stefan Richthofer, Laurenz Wiskott category:cs.LG stat.ML published:2013-11-11 summary:Every organism in an environment, whether biological, robotic or virtual,must be able to predict certain aspects of its environment in order to surviveor perform whatever task is intended. It needs a model that is capable ofestimating the consequences of possible actions, so that planning, control, anddecision-making become feasible. For scientific purposes, such models areusually created in a problem specific manner using differential equations andother techniques from control- and system-theory. In contrast to that, we aimfor an unsupervised approach that builds up the desired model in aself-organized fashion. Inspired by Slow Feature Analysis (SFA), our approachis to extract sub-signals from the input, that behave as predictable aspossible. These "predictable features" are highly relevant for modeling,because predictability is a desired property of the neededconsequence-estimating model by definition. In our approach, we measurepredictability with respect to a certain prediction model. We focus here on thesolution of the arising optimization problem and present a tractable algorithmbased on algebraic methods which we call Predictable Feature Analysis (PFA). Weprove that the algorithm finds the globally optimal signal, if this signal canbe predicted with low error. To deal with cases where the optimal signal has asignificant prediction error, we provide a robust, heuristically motivatedvariant of the algorithm and verify it empirically. Additionally, we giveformal criteria a prediction-model must meet to be suitable for measuringpredictability in the PFA setting and also provide a suitable default-modelalong with a formal proof that it meets these criteria.
arxiv-5100-194 | Embed and Conquer: Scalable Embeddings for Kernel k-Means on MapReduce | http://arxiv.org/pdf/1311.2334v4.pdf | author:Ahmed Elgohary, Ahmed K. Farahat, Mohamed S. Kamel, Fakhri Karray category:cs.LG published:2013-11-11 summary:The kernel $k$-means is an effective method for data clustering which extendsthe commonly-used $k$-means algorithm to work on a similarity matrix overcomplex data structures. The kernel $k$-means algorithm is howevercomputationally very complex as it requires the complete data matrix to becalculated and stored. Further, the kernelized nature of the kernel $k$-meansalgorithm hinders the parallelization of its computations on moderninfrastructures for distributed computing. In this paper, we are defining afamily of kernel-based low-dimensional embeddings that allows for scalingkernel $k$-means on MapReduce via an efficient and unified parallelizationstrategy. Afterwards, we propose two methods for low-dimensional embedding thatadhere to our definition of the embedding family. Exploiting the proposedparallelization strategy, we present two scalable MapReduce algorithms forkernel $k$-means. We demonstrate the effectiveness and efficiency of theproposed algorithms through an empirical evaluation on benchmark data sets.
arxiv-5100-195 | The Infinite Degree Corrected Stochastic Block Model | http://arxiv.org/pdf/1311.2520v3.pdf | author:Tue Herlau, Mikkel N. Schmidt, Morten MÃ¸rup category:stat.ML published:2013-11-11 summary:In Stochastic blockmodels, which are among the most prominent statisticalmodels for cluster analysis of complex networks, clusters are defined as groupsof nodes with statistically similar link probabilities within and betweengroups. A recent extension by Karrer and Newman incorporates a node degreecorrection to model degree heterogeneity within each group. Although thisdemonstrably leads to better performance on several networks it is not obviouswhether modelling node degree is always appropriate or necessary. We formulatethe degree corrected stochastic blockmodel as a non-parametric Bayesian model,incorporating a parameter to control the amount of degree correction which canthen be inferred from data. Additionally, our formulation yields principledways of inferring the number of groups as well as predicting missing links inthe network which can be used to quantify the model's predictive performance.On synthetic data we demonstrate that including the degree correction yieldsbetter performance both on recovering the true group structure and predictingmissing links when degree heterogeneity is present, whereas performance is onpar for data with no degree heterogeneity within clusters. On seven realnetworks (with no ground truth group structure available) we show thatpredictive performance is about equal whether or not degree correction isincluded; however, for some networks significantly fewer clusters arediscovered when correcting for degree indicating that the data can be morecompactly explained by clusters of heterogenous degree nodes.
arxiv-5100-196 | Volumetric Reconstruction Applied to Perceptual Studies of Size and Weight | http://arxiv.org/pdf/1311.2642v1.pdf | author:J. Balzer, M. Peters, S. Soatto category:cs.CV published:2013-11-11 summary:We explore the application of volumetric reconstruction from structured-lightsensors in cognitive neuroscience, specifically in the quantification of thesize-weight illusion, whereby humans tend to systematically perceive smallerobjects as heavier. We investigate the performance of two commercialstructured-light scanning systems in comparison to one we developedspecifically for this application. Our method has two main distinct features:First, it only samples a sparse series of viewpoints, unlike other systems suchas the Kinect Fusion. Second, instead of building a distance field for thepurpose of points-to-surface conversion directly, we pursue a first-orderapproach: the distance function is recovered from its gradient by a screenedPoisson reconstruction, which is very resilient to noise and yet preserveshigh-frequency signal components. Our experiments show that the quality ofmetric reconstruction from structured light sensors is subject to systematicbiases, and highlights the factors that influence it. Our main performanceindex rates estimates of volume (a proxy of size), for which we review awell-known formula applicable to incomplete meshes. Our code and data will bemade publicly available upon completion of the anonymous review process.
arxiv-5100-197 | Second-order Shape Optimization for Geometric Inverse Problems in Vision | http://arxiv.org/pdf/1311.2626v5.pdf | author:J. Balzer, S. Soatto category:cs.CV published:2013-11-11 summary:We develop a method for optimization in shape spaces, i.e., sets of surfacesmodulo re-parametrization. Unlike previously proposed gradient flows, weachieve superlinear convergence rates through a subtle approximation of theshape Hessian, which is generally hard to compute and suffers from a series ofdegeneracies. Our analysis highlights the role of mean curvature motion incomparison with first-order schemes: instead of surface area, our approachpenalizes deformation, either by its Dirichlet energy or total variation.Latter regularizer sparks the development of an alternating direction method ofmultipliers on triangular meshes. Therein, a conjugate-gradients solver enablesus to bypass formation of the Gaussian normal equations appearing in the courseof the overall optimization. We combine all of the aforementioned ideas in aversatile geometric variation-regularized Levenberg-Marquardt-type methodapplicable to a variety of shape functionals, depending on intrinsic propertiesof the surface such as normal field and curvature as well as its embedding intospace. Promising experimental results are reported.
arxiv-5100-198 | Determining Leishmania Infection Levels by Automatic Analysis of Microscopy Images | http://arxiv.org/pdf/1311.2621v1.pdf | author:P. A Nogueira category:cs.CV published:2013-11-11 summary:Analysis of microscopy images is one important tool in many fields ofbiomedical research, as it allows the quantification of a multitude ofparameters at the cellular level. However, manual counting of these images isboth tiring and unreliable and ultimately very time-consuming for biomedicalresearchers. Not only does this slow down the overall research process, it alsointroduces counting errors due to a lack of objectivity and consistencyinherent to the researchers own human nature. This thesis addresses this issue by automatically determining infectionindexes of macrophages parasite by Leishmania in microscopy images usingcomputer vision and pattern recognition methodologies. Initially images aresubmitted to a pre-processing stage that consists in a normalization ofillumination conditions. Three algorithms are then applied in parallel to eachimage. Algorithm A intends to detect macrophage nuclei and consists ofsegmentation via adaptive multi-threshold, and classification of resultingregions using a set of collected features. Algorithm B intends to detectparasites and is similar to Algorithm A but the adaptive multi-threshold isparameterized with a different constraints vector. Algorithm C intends todetect the macrophages and parasites cytoplasm and consists of a cut-offversion of the previous two algorithms, where the classification step isskipped. Regions with multiple nuclei or parasites are processed by a votingsystem that employs both a Support Vector Machine and a set of region featuresfor determining the number of objects present in each region. The previous voteis then taken into account as the number of mixtures to be used in a GaussianMixture Model to decluster the said region. Finally each parasite is assignedto, at most, a single macrophage using minimum Euclidean distance to a cellnucleus, thus quantifying Leishmania infection levels.
arxiv-5100-199 | Stitched Panoramas from Toy Airborne Video Cameras | http://arxiv.org/pdf/1311.6500v1.pdf | author:Camille Goudeseune category:cs.CV published:2013-11-11 summary:Effective panoramic photographs are taken from vantage points that are high.High vantage points have recently become easier to reach as the cost ofquadrotor helicopters has dropped to nearly disposable levels. Although camerascarried by such aircraft weigh only a few grams, their low-quality video can beconverted into panoramas of high quality and high resolution. Also, the smallsize of these aircraft vastly reduces the risks inherent to flight.
arxiv-5100-200 | Motility at the origin of life: Its characterization and a model | http://arxiv.org/pdf/1311.2531v1.pdf | author:Tom Froese, Nathaniel Virgo, Takashi Ikegami category:cs.AI cs.NE nlin.AO nlin.PS q-bio.PE 35K57 published:2013-11-11 summary:Due to recent advances in synthetic biology and artificial life, the originof life is currently a hot topic of research. We review the literature andargue that the two traditionally competing "replicator-first" and"metabolism-first" approaches are merging into one integrated theory ofindividuation and evolution. We contribute to the maturation of this moreinclusive approach by highlighting some problematic assumptions that still leadto an impoverished conception of the phenomenon of life. In particular, weargue that the new consensus has so far failed to consider the relevance ofintermediate timescales. We propose that an adequate theory of life mustaccount for the fact that all living beings are situated in at least fourdistinct timescales, which are typically associated with metabolism, motility,development, and evolution. On this view, self-movement, adaptive behavior andmorphological changes could have already been present at the origin of life. Inorder to illustrate this possibility we analyze a minimal model of life-likephenomena, namely of precarious, individuated, dissipative structures that canbe found in simple reaction-diffusion systems. Based on our analysis we suggestthat processes in intermediate timescales could have already been operative inprebiotic systems. They may have facilitated and constrained changes occurringin the faster- and slower-paced timescales of chemical self-individuation andevolution by natural selection, respectively.
arxiv-5100-201 | From average case complexity to improper learning complexity | http://arxiv.org/pdf/1311.2272v2.pdf | author:Amit Daniely, Nati Linial, Shai Shalev-Shwartz category:cs.LG cs.CC published:2013-11-10 summary:The basic problem in the PAC model of computational learning theory is todetermine which hypothesis classes are efficiently learnable. There ispresently a dearth of results showing hardness of learning problems. Moreover,the existing lower bounds fall short of the best known algorithms. The biggest challenge in proving complexity results is to establish hardnessof {\em improper learning} (a.k.a. representation independent learning).Thedifficulty in proving lower bounds for improper learning is that the standardreductions from $\mathbf{NP}$-hard problems do not seem to apply in thiscontext. There is essentially only one known approach to proving lower boundson improper learning. It was initiated in (Kearns and Valiant 89) and relies oncryptographic assumptions. We introduce a new technique for proving hardness of improper learning, basedon reductions from problems that are hard on average. We put forward a (fairlystrong) generalization of Feige's assumption (Feige 02) about the complexity ofrefuting random constraint satisfaction problems. Combining this assumptionwith our new technique yields far reaching implications. In particular, 1. Learning $\mathrm{DNF}$'s is hard. 2. Agnostically learning halfspaces with a constant approximation ratio ishard. 3. Learning an intersection of $\omega(1)$ halfspaces is hard.
arxiv-5100-202 | FuSSO: Functional Shrinkage and Selection Operator | http://arxiv.org/pdf/1311.2234v2.pdf | author:Junier B. Oliva, Barnabas Poczos, Timothy Verstynen, Aarti Singh, Jeff Schneider, Fang-Cheng Yeh, Wen-Yih Tseng category:stat.ML cs.LG math.ST stat.TH published:2013-11-10 summary:We present the FuSSO, a functional analogue to the LASSO, that efficientlyfinds a sparse set of functional input covariates to regress a real-valuedresponse against. The FuSSO does so in a semi-parametric fashion, making noparametric assumptions about the nature of input functional covariates andassuming a linear form to the mapping of functional covariates to the response.We provide a statistical backing for use of the FuSSO via proof of asymptoticsparsistency under various conditions. Furthermore, we observe good results onboth synthetic and real-world data.
arxiv-5100-203 | Fast Distribution To Real Regression | http://arxiv.org/pdf/1311.2236v2.pdf | author:Junier B. Oliva, Willie Neiswanger, Barnabas Poczos, Jeff Schneider, Eric Xing category:stat.ML cs.LG math.ST stat.TH published:2013-11-10 summary:We study the problem of distribution to real-value regression, where one aimsto regress a mapping $f$ that takes in a distribution input covariate $P\in\mathcal{I}$ (for a non-parametric family of distributions $\mathcal{I}$) andoutputs a real-valued response $Y=f(P) + \epsilon$. This setting was recentlystudied, and a "Kernel-Kernel" estimator was introduced and shown to have apolynomial rate of convergence. However, evaluating a new prediction with theKernel-Kernel estimator scales as $\Omega(N)$. This causes the difficultsituation where a large amount of data may be necessary for a low estimationrisk, but the computation cost of estimation becomes infeasible when thedata-set is too large. To this end, we propose the Double-Basis estimator,which looks to alleviate this big data problem in two ways: first, theDouble-Basis estimator is shown to have a computation complexity that isindependent of the number of of instances $N$ when evaluating new predictionsafter training; secondly, the Double-Basis estimator is shown to have a fastrate of convergence for a general class of mappings $f\in\mathcal{F}$.
arxiv-5100-204 | Learning Gaussian Graphical Models with Observed or Latent FVSs | http://arxiv.org/pdf/1311.2241v1.pdf | author:Ying Liu, Alan S. Willsky category:cs.LG stat.ML published:2013-11-10 summary:Gaussian Graphical Models (GGMs) or Gauss Markov random fields are widelyused in many applications, and the trade-off between the modeling capacity andthe efficiency of learning and inference has been an important researchproblem. In this paper, we study the family of GGMs with small feedback vertexsets (FVSs), where an FVS is a set of nodes whose removal breaks all thecycles. Exact inference such as computing the marginal distributions and thepartition function has complexity $O(k^{2}n)$ using message-passing algorithms,where k is the size of the FVS, and n is the total number of nodes. We proposeefficient structure learning algorithms for two cases: 1) All nodes areobserved, which is useful in modeling social or flight networks where the FVSnodes often correspond to a small number of high-degree nodes, or hubs, whilethe rest of the networks is modeled by a tree. Regardless of the maximumdegree, without knowing the full graph structure, we can exactly compute themaximum likelihood estimate in $O(kn^2+n^2\log n)$ if the FVS is known or inpolynomial time if the FVS is unknown but has bounded size. 2) The FVS nodesare latent variables, where structure learning is equivalent to decomposing ainverse covariance matrix (exactly or approximately) into the sum of atree-structured matrix and a low-rank matrix. By incorporating efficientinference into the learning steps, we can obtain a learning algorithm usingalternating low-rank correction with complexity $O(kn^{2}+n^{2}\log n)$ periteration. We also perform experiments using both synthetic data as well asreal data of flight delays to demonstrate the modeling capacity with FVSs ofvarious sizes.
arxiv-5100-205 | Semantic Sort: A Supervised Approach to Personalized Semantic Relatedness | http://arxiv.org/pdf/1311.2252v1.pdf | author:Ran El-Yaniv, David Yanay category:cs.CL cs.LG published:2013-11-10 summary:We propose and study a novel supervised approach to learning statisticalsemantic relatedness models from subjectively annotated training examples. Theproposed semantic model consists of parameterized co-occurrence statisticsassociated with textual units of a large background knowledge corpus. Wepresent an efficient algorithm for learning such semantic models from atraining sample of relatedness preferences. Our method is corpus independentand can essentially rely on any sufficiently large (unstructured) collection ofcoherent texts. Moreover, the approach facilitates the fitting of semanticmodels for specific users or groups of users. We present the results ofextensive range of experiments from small to large scale, indicating that theproposed method is effective and competitive with the state-of-the-art.
arxiv-5100-206 | More data speeds up training time in learning halfspaces over sparse vectors | http://arxiv.org/pdf/1311.2271v1.pdf | author:Amit Daniely, Nati Linial, Shai Shalev Shwartz category:cs.LG published:2013-11-10 summary:The increased availability of data in recent years has led several authors toask whether it is possible to use data as a {\em computational} resource. Thatis, if more data is available, beyond the sample complexity limit, is itpossible to use the extra examples to speed up the computation time required toperform the learning task? We give the first positive answer to this question for a {\em naturalsupervised learning problem} --- we consider agnostic PAC learning ofhalfspaces over $3$-sparse vectors in $\{-1,1,0\}^n$. This class isinefficiently learnable using $O\left(n/\epsilon^2\right)$ examples. Our maincontribution is a novel, non-cryptographic, methodology for establishingcomputational-statistical gaps, which allows us to show that, under a widelybelieved assumption that refuting random $\mathrm{3CNF}$ formulas is hard, itis impossible to efficiently learn this class using only$O\left(n/\epsilon^2\right)$ examples. We further show that under strongerhardness assumptions, even $O\left(n^{1.499}/\epsilon^2\right)$ examples do notsuffice. On the other hand, we show a new algorithm that learns this classefficiently using $\tilde{\Omega}\left(n^2/\epsilon^2\right)$ examples. Thisformally establishes the tradeoff between sample and computational complexityfor a natural supervised learning problem.
arxiv-5100-207 | A Quantitative Evaluation Framework for Missing Value Imputation Algorithms | http://arxiv.org/pdf/1311.2276v1.pdf | author:Vinod Nair, Rahul Kidambi, Sundararajan Sellamanickam, S. Sathiya Keerthi, Johannes Gehrke, Vijay Narayanan category:cs.LG published:2013-11-10 summary:We consider the problem of quantitatively evaluating missing value imputationalgorithms. Given a dataset with missing values and a choice of severalimputation algorithms to fill them in, there is currently no principled way torank the algorithms using a quantitative metric. We develop a framework basedon treating imputation evaluation as a problem of comparing two distributionsand show how it can be used to compute quantitative metrics. We present anefficient procedure for applying this framework to practical datasets,demonstrate several metrics derived from the existing literature on comparingdistributions, and propose a new metric called Neighborhood-based DissimilarityScore which is fast to compute and provides similar results. Results are shownon several datasets, metrics, and imputations algorithms.
arxiv-5100-208 | Large Margin Semi-supervised Structured Output Learning | http://arxiv.org/pdf/1311.2139v1.pdf | author:P. Balamurugan, Shirish Shevade, Sundararajan Sellamanickam category:cs.LG published:2013-11-09 summary:In structured output learning, obtaining labelled data for real-worldapplications is usually costly, while unlabelled examples are available inabundance. Semi-supervised structured classification has been developed tohandle large amounts of unlabelled structured data. In this work, we considersemi-supervised structural SVMs with domain constraints. The optimizationproblem, which in general is not convex, contains the loss terms associatedwith the labelled and unlabelled examples along with the domain constraints. Wepropose a simple optimization approach, which alternates between solving asupervised learning problem and a constraint matching problem. Solving theconstraint matching problem is difficult for structured prediction, and wepropose an efficient and effective hill-climbing method to solve it. Thealternating optimization is carried out within a deterministic annealingframework, which helps in effective constraint matching, and avoiding localminima which are not very useful. The algorithm is simple to implement andachieves comparable generalization performance on benchmark datasets.
arxiv-5100-209 | Fast large-scale optimization by unifying stochastic gradient and quasi-Newton methods | http://arxiv.org/pdf/1311.2115v7.pdf | author:Jascha Sohl-Dickstein, Ben Poole, Surya Ganguli category:cs.LG 90C26 G.1.6 published:2013-11-09 summary:We present an algorithm for minimizing a sum of functions that combines thecomputational efficiency of stochastic gradient descent (SGD) with the secondorder curvature information leveraged by quasi-Newton methods. We unify thesedisparate approaches by maintaining an independent Hessian approximation foreach contributing function in the sum. We maintain computational tractabilityand limit memory requirements even for high dimensional optimization problemsby storing and manipulating these quadratic approximations in a shared, timeevolving, low dimensional subspace. Each update step requires only a singlecontributing function or minibatch evaluation (as in SGD), and each step isscaled using an approximate inverse Hessian and little to no adjustment ofhyperparameters is required (as is typical for quasi-Newton methods). Thisalgorithm contrasts with earlier stochastic second order techniques that treatthe Hessian of each contributing function as a noisy approximation to the fullHessian, rather than as a target for direct estimation. We experimentallydemonstrate improved convergence on seven diverse optimization problems. Thealgorithm is released as open source Python and MATLAB packages.
arxiv-5100-210 | Neighborhood filters and the decreasing rearrangement | http://arxiv.org/pdf/1311.2191v2.pdf | author:Gonzalo Galiano, JuliÃ¡n Velasco category:cs.CV 68U10 published:2013-11-09 summary:Nonlocal filters are simple and powerful techniques for image denoising. Inthis paper, we give new insights into the analysis of one kind of them, theNeighborhood filter, by using a classical although not very commontransformation: the decreasing rearrangement of a function (the image).Independently of the dimension of the image, we reformulate the Neighborhoodfilter and its iterative variants as an integral operator defined in aone-dimensional space. The simplicity of this formulation allows to perform adetailed analysis of its properties. Among others, we prove that the filterbehaves asymptotically as a shock filter combined with a border diffusive term,responsible for the staircaising effect and the loss of contrast.
arxiv-5100-211 | Pattern-Coupled Sparse Bayesian Learning for Recovery of Block-Sparse Signals | http://arxiv.org/pdf/1311.2150v1.pdf | author:Jun Fang, Yanning Shen, Hongbin Li, Pu Wang category:cs.IT cs.LG math.IT stat.ML published:2013-11-09 summary:We consider the problem of recovering block-sparse signals whose structuresare unknown \emph{a priori}. Block-sparse signals with nonzero coefficientsoccurring in clusters arise naturally in many practical scenarios. However, theknowledge of the block structure is usually unavailable in practice. In thispaper, we develop a new sparse Bayesian learning method for recovery ofblock-sparse signals with unknown cluster patterns. Specifically, apattern-coupled hierarchical Gaussian prior model is introduced to characterizethe statistical dependencies among coefficients, in which a set ofhyperparameters are employed to control the sparsity of signal coefficients.Unlike the conventional sparse Bayesian learning framework in which eachindividual hyperparameter is associated independently with each coefficient, inthis paper, the prior for each coefficient not only involves its ownhyperparameter, but also the hyperparameters of its immediate neighbors. Indoing this way, the sparsity patterns of neighboring coefficients are relatedto each other and the hierarchical model has the potential to encouragestructured-sparse solutions. The hyperparameters, along with the sparse signal,are learned by maximizing their posterior probability via anexpectation-maximization (EM) algorithm. Numerical results show that theproposed algorithm presents uniform superiority over other existing methods ina series of experiments.
arxiv-5100-212 | A Structured Prediction Approach for Missing Value Imputation | http://arxiv.org/pdf/1311.2137v1.pdf | author:Rahul Kidambi, Vinod Nair, Sundararajan Sellamanickam, S. Sathiya Keerthi category:cs.LG published:2013-11-09 summary:Missing value imputation is an important practical problem. There is a largebody of work on it, but there does not exist any work that formulates theproblem in a structured output setting. Also, most applications haveconstraints on the imputed data, for example on the distribution associatedwith each variable. None of the existing imputation methods use theseconstraints. In this paper we propose a structured output approach for missingvalue imputation that also incorporates domain constraints. We focus on largemargin models, but it is easy to extend the ideas to probabilistic models. Wedeal with the intractable inference step in learning via a piecewise trainingtechnique that is simple, efficient, and effective. Comparison with existingstate-of-the-art and baseline imputation methods shows that our method givessignificantly improved performance on the Hamming loss measure.
arxiv-5100-213 | Optimization, Learning, and Games with Predictable Sequences | http://arxiv.org/pdf/1311.1869v1.pdf | author:Alexander Rakhlin, Karthik Sridharan category:cs.LG cs.GT published:2013-11-08 summary:We provide several applications of Optimistic Mirror Descent, an onlinelearning algorithm based on the idea of predictable sequences. First, werecover the Mirror Prox algorithm for offline optimization, prove an extensionto Holder-smooth functions, and apply the results to saddle-point typeproblems. Next, we prove that a version of Optimistic Mirror Descent (which hasa close relation to the Exponential Weights algorithm) can be used by twostrongly-uncoupled players in a finite zero-sum matrix game to converge to theminimax equilibrium at the rate of O((log T)/T). This addresses a question ofDaskalakis et al 2011. Further, we consider a partial information version ofthe problem. We then apply the results to convex programming and exhibit asimple algorithm for the approximate Max Flow problem.
arxiv-5100-214 | Logique mathÃ©matique et linguistique formelle | http://arxiv.org/pdf/1311.1897v1.pdf | author:Christian RetorÃ© category:math.LO cs.CL published:2013-11-08 summary:As the etymology of the word shows, logic is intimately related to language,as exemplified by the work of philosophers from Antiquity and from theMiddle-Age. At the beginning of the XX century, the crisis of the foundationsof mathematics invented mathematical logic and imposed logic as alanguage-based foundation for mathematics. How did the relations between logicand language evolved in this newly defined mathematical framework? After asurvey of the history of the relation between logic and linguistics,traditionally focused on semantics, we focus on some present issues: 1) grammaras a deductive system 2) the transformation of the syntactic structure of asentence to a logical formula representing its meaning 3) taking into accountthe context when interpreting words. This lecture shows that type theoryprovides a convenient framework both for natural language syntax and for theinterpretation of any of tis level (words, sentences, discourse).
arxiv-5100-215 | Moment-based Uniform Deviation Bounds for $k$-means and Friends | http://arxiv.org/pdf/1311.1903v1.pdf | author:Matus Telgarsky, Sanjoy Dasgupta category:cs.LG stat.ML published:2013-11-08 summary:Suppose $k$ centers are fit to $m$ points by heuristically minimizing the$k$-means cost; what is the corresponding fit over the source distribution?This question is resolved here for distributions with $p\geq 4$ boundedmoments; in particular, the difference between the sample cost and distributioncost decays with $m$ and $p$ as $m^{\min\{-1/4, -1/2+2/p\}}$. The essentialtechnical contribution is a mechanism to uniformly control deviations in theface of unbounded parameter sets, cost functions, and source distributions. Tofurther demonstrate this mechanism, a soft clustering variant of $k$-means costis also considered, namely the log likelihood of a Gaussian mixture, subject tothe constraint that all covariance matrices have bounded spectrum. Lastly, arate with refined constants is provided for $k$-means instances possessing somecluster structure.
arxiv-5100-216 | Visualizing the Effects of a Changing Distance on Data Using Continuous Embeddings | http://arxiv.org/pdf/1311.1911v2.pdf | author:Gina Gruenhage, Manfred Opper, Simon Barthelme category:stat.ML published:2013-11-08 summary:Most ML methods, from clustering to classification, rely on a distancefunction to describe relationships between datapoints. For complex datasets itis hard to avoid making some arbitrary choices when defining a distancefunction. To compare images, one must choose a spatial scale, for signals, atemporal scale. The right scale is hard to pin down and it is preferable whenresults do not depend too tightly on the exact value one picked. Topologicaldata analysis seeks to address this issue by focusing on the notion ofneighbourhood instead of distance. Here, we show that in some cases a simplersolution is available. One can check how strongly distance relationships dependon a hyperparameter using dimensionality reduction. We formulate a variant ofdynamical multi-dimensional scaling (MDS), which embeds datapoints as curves.The resulting algorithm is based on the Concave-Convex Procedure (CCCP) andprovides a simple and efficient way of visualizing changes and invariances indistance patterns as a hyperparameter is varied. We also present a variant toanalyze the dependence on multiple hyperparameters. We provide a cMDS algorithmthat is straightforward to implement, use and extend. To illustrate thepossibilities of cMDS, we apply cMDS to several real-world data sets.
arxiv-5100-217 | Risk-sensitive Reinforcement Learning | http://arxiv.org/pdf/1311.2097v3.pdf | author:Yun Shen, Michael J. Tobia, Tobias Sommer, Klaus Obermayer category:cs.LG published:2013-11-08 summary:We derive a family of risk-sensitive reinforcement learning methods foragents, who face sequential decision-making tasks in uncertain environments. Byapplying a utility function to the temporal difference (TD) error, nonlineartransformations are effectively applied not only to the received rewards butalso to the true transition probabilities of the underlying Markov decisionprocess. When appropriate utility functions are chosen, the agents' behaviorsexpress key features of human behavior as predicted by prospect theory(Kahneman and Tversky, 1979), for example different risk-preferences for gainsand losses as well as the shape of subjective probability curves. We derive arisk-sensitive Q-learning algorithm, which is necessary for modeling humanbehavior when transition probabilities are unknown, and prove its convergence.As a proof of principle for the applicability of the new framework we apply itto quantify human behavior in a sequential investment task. We find, that therisk-sensitive variant provides a significantly better fit to the behavioraldata and that it leads to an interpretation of the subject's responses which isindeed consistent with prospect theory. The analysis of simultaneously measuredfMRI signals show a significant correlation of the risk-sensitive TD error withBOLD signal change in the ventral striatum. In addition we find a significantcorrelation of the risk-sensitive Q-values with neural activity in thestriatum, cingulate cortex and insula, which is not present if standardQ-values are used.
arxiv-5100-218 | Fast Tracking via Spatio-Temporal Context Learning | http://arxiv.org/pdf/1311.1939v1.pdf | author:Kaihua Zhang, Lei Zhang, Ming-Hsuan Yang, David Zhang category:cs.CV published:2013-11-08 summary:In this paper, we present a simple yet fast and robust algorithm whichexploits the spatio-temporal context for visual tracking. Our approachformulates the spatio-temporal relationships between the object of interest andits local context based on a Bayesian framework, which models the statisticalcorrelation between the low-level features (i.e., image intensity and position)from the target and its surrounding regions. The tracking problem is posed bycomputing a confidence map, and obtaining the best target location bymaximizing an object location likelihood function. The Fast Fourier Transformis adopted for fast learning and detection in this work. Implemented in MATLABwithout code optimization, the proposed tracker runs at 350 frames per secondon an i7 machine. Extensive experimental results show that the proposedalgorithm performs favorably against state-of-the-art methods in terms ofefficiency, accuracy and robustness.
arxiv-5100-219 | Submodularization for Quadratic Pseudo-Boolean Optimization | http://arxiv.org/pdf/1311.1856v2.pdf | author:Lena Gorelick, Yuri Boykov, Olga Veksler, Ismail Ben Ayed, Andrew Delong category:cs.CV published:2013-11-08 summary:Many computer vision problems require optimization of binary non-submodularenergies. We propose a general optimization framework based on local submodularapproximations (LSA). Unlike standard LP relaxation methods that linearize thewhole energy globally, our approach iteratively approximates the energieslocally. On the other hand, unlike standard local optimization methods (e.g.gradient descent or projection techniques) we use non-linear submodularapproximations and optimize them without leaving the domain of integersolutions. We discuss two specific LSA algorithms based on "trust region" and"auxiliary function" principles, LSA-TR and LSA-AUX. These methods obtainstate-of-the-art results on a wide range of applications outperforming manystandard techniques such as LBP, QPBO, and TRWS. While our paper is focused onpairwise energies, our ideas extend to higher-order problems. The code isavailable online (http://vision.csd.uwo.ca/code/).
arxiv-5100-220 | A new stopping criterion for the mean shift iterative algorithm | http://arxiv.org/pdf/1311.2014v1.pdf | author:Roberto RodrÃ­guez, Esley Torres, Yasel GarcÃ©s, Osvaldo Pereira, Humberto Sossa category:cs.CV published:2013-11-08 summary:The mean shift iterative algorithm was proposed in 2006, for using theentropy as a stopping criterion. From then on, a theoretical base has beendeveloped and a group of applications has been carried out using thisalgorithm. This paper proposes a new stopping criterion for the mean shiftiterative algorithm, where stopping threshold via entropy is used now, but inanother way. Many segmentation experiments were carried out by utilizingstandard images and it was verified that a better segmentation was reached, andthat the algorithm had better stability. An analysis on the convergence,through a theorem, with the new stopping criterion was carried out. The goal ofthis paper is to compare the new stopping criterion with the old criterion. Forthis reason, the obtained results were not compared with other segmentationapproaches, since with the old stopping criterion were previously carried out.
arxiv-5100-221 | Nonparametric Multi-group Membership Model for Dynamic Networks | http://arxiv.org/pdf/1311.2079v1.pdf | author:Myunghwan Kim, Jure Leskovec category:cs.SI physics.soc-ph stat.ML published:2013-11-08 summary:Relational data-like graphs, networks, and matrices-is often dynamic, wherethe relational structure evolves over time. A fundamental problem in theanalysis of time-varying network data is to extract a summary of the commonstructure and the dynamics of the underlying relations between the entities.Here we build on the intuition that changes in the network structure are drivenby the dynamics at the level of groups of nodes. We propose a nonparametricmulti-group membership model for dynamic networks. Our model contains threemain components: We model the birth and death of individual groups with respectto the dynamics of the network structure via a distance dependent Indian BuffetProcess. We capture the evolution of individual node group memberships via aFactorial Hidden Markov model. And, we explain the dynamics of the networkstructure by explicitly modeling the connectivity structure of groups. Wedemonstrate our model's capability of identifying the dynamics of latent groupsin a number of different types of network data. Experimental results show thatour model provides improved predictive performance over existing dynamicnetwork models on future network forecasting and missing link prediction.
arxiv-5100-222 | An Experimental Comparison of Trust Region and Level Sets | http://arxiv.org/pdf/1311.2102v1.pdf | author:Lena Gorelick, Ismail BenAyed, Frank R. Schmidt, Yuri Boykov category:cs.CV published:2013-11-08 summary:High-order (non-linear) functionals have become very popular in segmentation,stereo and other computer vision problems. Level sets is a well establishedgeneral gradient descent framework, which is directly applicable tooptimization of such functionals and widely used in practice. Recently, anothergeneral optimization approach based on trust region methodology was proposedfor regional non-linear functionals. Our goal is a comprehensive experimentalcomparison of these two frameworks in regard to practical efficiency,robustness to parameters, and optimality. We experiment on a wide range ofproblems with non-linear constraints on segment volume, appearance and shape.
arxiv-5100-223 | Curvature and Optimal Algorithms for Learning and Minimizing Submodular Functions | http://arxiv.org/pdf/1311.2110v1.pdf | author:Rishabh Iyer, Stefanie Jegelka, Jeff Bilmes category:cs.DS cs.DM cs.LG published:2013-11-08 summary:We investigate three related and important problems connected to machinelearning: approximating a submodular function everywhere, learning a submodularfunction (in a PAC-like setting [53]), and constrained minimization ofsubmodular functions. We show that the complexity of all three problems dependson the 'curvature' of the submodular function, and provide lower and upperbounds that refine and improve previous results [3, 16, 18, 52]. Our prooftechniques are fairly generic. We either use a black-box transformation of thefunction (for approximation and learning), or a transformation of algorithms touse an appropriate surrogate function (for minimization). Curiously, curvaturehas been known to influence approximations for submodular maximization [7, 55],but its effect on minimization, approximation and learning has hitherto beenopen. We complete this picture, and also support our theoretical claims byempirical results.
arxiv-5100-224 | Constructing Time Series Shape Association Measures: Minkowski Distance and Data Standardization | http://arxiv.org/pdf/1311.1958v3.pdf | author:Ildar Batyrshin category:cs.LG published:2013-11-07 summary:It is surprising that last two decades many works in time series data miningand clustering were concerned with measures of similarity of time series butnot with measures of association that can be used for measuring possible directand inverse relationships between time series. Inverse relationships can existbetween dynamics of prices and sell volumes, between growth patterns ofcompetitive companies, between well production data in oilfields, between windvelocity and air pollution concentration etc. The paper develops a theoreticalbasis for analysis and construction of time series shape association measures.Starting from the axioms of time series shape association measures it studiesthe methods of construction of measures satisfying these axioms. Severalgeneral methods of construction of such measures suitable for measuring timeseries shape similarity and shape association are proposed. Time series shapeassociation measures based on Minkowski distance and data standardizationmethods are considered. The cosine similarity and the Pearsons correlationcoefficient are obtained as particular cases of the proposed general methodsthat can be used also for construction of new association measures in dataanalysis.
arxiv-5100-225 | The Maximum Entropy Relaxation Path | http://arxiv.org/pdf/1311.1644v1.pdf | author:Moshe Dubiner, Matan Gavish, Yoram Singer category:cs.LG math.OC stat.ML published:2013-11-07 summary:The relaxed maximum entropy problem is concerned with finding a probabilitydistribution on a finite set that minimizes the relative entropy to a givenprior distribution, while satisfying relaxed max-norm constraints with respectto a third observed multinomial distribution. We study the entire relaxationpath for this problem in detail. We show existence and a geometric descriptionof the relaxation path. Specifically, we show that the maximum entropyrelaxation path admits a planar geometric description as an increasing,piecewise linear function in the inverse relaxation parameter. We derive fastalgorithms for tracking the path. In various realistic settings, our algorithmsrequire $O(n\log(n))$ operations for probability distributions on $n$ points,making it possible to handle large problems. Once the path has been recovered,we show that given a validation set, the family of admissible models is reducedfrom an infinite family to a small, discrete set. We demonstrate the merits ofour approach in experiments with synthetic data and discuss its potential forthe estimation of compact n-gram language models.
arxiv-5100-226 | Stochastic blockmodel approximation of a graphon: Theory and consistent estimation | http://arxiv.org/pdf/1311.1731v2.pdf | author:Edoardo M Airoldi, Thiago B Costa, Stanley H Chan category:stat.ME cs.LG cs.SI stat.ML published:2013-11-07 summary:Non-parametric approaches for analyzing network data based on exchangeablegraph models (ExGM) have recently gained interest. The key object that definesan ExGM is often referred to as a graphon. This non-parametric perspective onnetwork modeling poses challenging questions on how to make inference on thegraphon underlying observed network data. In this paper, we propose acomputationally efficient procedure to estimate a graphon from a set ofobserved networks generated from it. This procedure is based on a stochasticblockmodel approximation (SBA) of the graphon. We show that, by approximatingthe graphon with a stochastic block model, the graphon can be consistentlyestimated, that is, the estimation error vanishes as the size of the graphapproaches infinity.
arxiv-5100-227 | An Inexact Proximal Path-Following Algorithm for Constrained Convex Minimization | http://arxiv.org/pdf/1311.1756v2.pdf | author:Quoc Tran Dinh, Anastasios Kyrillidis, Volkan Cevher category:math.OC stat.ML published:2013-11-07 summary:Many scientific and engineering applications feature nonsmooth convexminimization problems over convex sets. In this paper, we address an importantinstance of this broad class where we assume that the nonsmooth objective isequipped with a tractable proximity operator and that the convex constraint setaffords a self-concordant barrier. We provide a new joint treatment of proximaland self-concordant barrier concepts and illustrate that such problems can beefficiently solved, without the need of lifting the problem dimensions, as indisciplined convex optimization approach. We propose an inexact path-followingalgorithmic framework and theoretically characterize the worst-case analyticalcomplexity of this framework when the proximal subproblems are solvedinexactly. To show the merits of our framework, we apply its instances to bothsynthetic and real-world applications, where it shows advantages over standardinterior point methods. As a by-product, we describe how our framework canobtain points on the Pareto frontier of regularized problems withself-concordant objectives in a tuning free fashion.
arxiv-5100-228 | Biometric Signature Processing & Recognition Using Radial Basis Function Network | http://arxiv.org/pdf/1311.1694v1.pdf | author:Ankit Chadha, Neha Satam, Vibha Wali category:cs.CV published:2013-11-07 summary:Automatic recognition of signature is a challenging problem which hasreceived much attention during recent years due to its many applications indifferent fields. Signature has been used for long time for verification andauthentication purpose. Earlier methods were manual but nowadays they aregetting digitized. This paper provides an efficient method to signaturerecognition using Radial Basis Function Network. The network is trained withsample images in database. Feature extraction is performed before using themfor training. For testing purpose, an image is made to undergorotation-translation-scaling correction and then given to network. The networksuccessfully identifies the original image and gives correct output for storeddatabase images also. The method provides recognition rate of approximately 80%for 200 samples.
arxiv-5100-229 | Exploring Deep and Recurrent Architectures for Optimal Control | http://arxiv.org/pdf/1311.1761v1.pdf | author:Sergey Levine category:cs.LG cs.AI cs.NE cs.RO cs.SY published:2013-11-07 summary:Sophisticated multilayer neural networks have achieved state of the artresults on multiple supervised tasks. However, successful applications of suchmultilayer networks to control have so far been limited largely to theperception portion of the control pipeline. In this paper, we explore theapplication of deep and recurrent neural networks to a continuous,high-dimensional locomotion task, where the network is used to represent acontrol policy that maps the state of the system (represented by joint angles)directly to the torques at each joint. By using a recent reinforcement learningalgorithm called guided policy search, we can successfully train neural networkcontrollers with thousands of parameters, allowing us to compare a variety ofarchitectures. We discuss the differences between the locomotion control taskand previous supervised perception tasks, present experimental resultscomparing various architectures, and discuss future directions in theapplication of techniques from deep learning to the problem of optimal control.
arxiv-5100-230 | Efficient Regularization of Squared Curvature | http://arxiv.org/pdf/1311.1838v2.pdf | author:Claudia Nieuwenhuis, Eno Toeppe, Lena Gorelick, Olga Veksler, Yuri Boykov category:cs.CV published:2013-11-07 summary:Curvature has received increased attention as an important alternative tolength based regularization in computer vision. In contrast to length, itpreserves elongated structures and fine details. Existing approaches are eitherinefficient, or have low angular resolution and yield results with strong blockartifacts. We derive a new model for computing squared curvature based onintegral geometry. The model counts responses of straight line triple cliques.The corresponding energy decomposes into submodular and supermodular pairwisepotentials. We show that this energy can be efficiently minimized even for highangular resolutions using the trust region framework. Our results confirm thatwe obtain accurate and visually pleasing solutions without strong artifacts atreasonable run times.
arxiv-5100-231 | Scalable Recommendation with Poisson Factorization | http://arxiv.org/pdf/1311.1704v3.pdf | author:Prem Gopalan, Jake M. Hofman, David M. Blei category:cs.IR cs.AI cs.LG stat.ML published:2013-11-07 summary:We develop a Bayesian Poisson matrix factorization model for formingrecommendations from sparse user behavior data. These data are large user/itemmatrices where each user has provided feedback on only a small subset of items,either explicitly (e.g., through star ratings) or implicitly (e.g., throughviews or purchases). In contrast to traditional matrix factorizationapproaches, Poisson factorization implicitly models each user's limitedattention to consume items. Moreover, because of the mathematical form of thePoisson likelihood, the model needs only to explicitly consider the observedentries in the matrix, leading to both scalable computation and good predictiveperformance. We develop a variational inference algorithm for approximateposterior inference that scales up to massive data sets. This is an efficientalgorithm that iterates over the observed entries and adjusts an approximateposterior over the user/item representations. We apply our method to largereal-world user data containing users rating movies, users listening to songs,and users reading scientific papers. In all these settings, Bayesian Poissonfactorization outperforms state-of-the-art matrix factorization methods.
arxiv-5100-232 | Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks | http://arxiv.org/pdf/1311.1780v7.pdf | author:Caglar Gulcehre, Kyunghyun Cho, Razvan Pascanu, Yoshua Bengio category:cs.NE cs.LG stat.ML published:2013-11-07 summary:In this paper we propose and investigate a novel nonlinear unit, called $L_p$unit, for deep neural networks. The proposed $L_p$ unit receives signals fromseveral projections of a subset of units in the layer below and computes anormalized $L_p$ norm. We notice two interesting interpretations of the $L_p$unit. First, the proposed unit can be understood as a generalization of anumber of conventional pooling operators such as average, root-mean-square andmax pooling widely used in, for instance, convolutional neural networks (CNN),HMAX models and neocognitrons. Furthermore, the $L_p$ unit is, to a certaindegree, similar to the recently proposed maxout unit (Goodfellow et al., 2013)which achieved the state-of-the-art object recognition results on a number ofbenchmark datasets. Secondly, we provide a geometrical interpretation of theactivation function based on which we argue that the $L_p$ unit is moreefficient at representing complex, nonlinear separating boundaries. Each $L_p$unit defines a superelliptic boundary, with its exact shape defined by theorder $p$. We claim that this makes it possible to model arbitrarily shaped,curved boundaries more efficiently by combining a few $L_p$ units of differentorders. This insight justifies the need for learning different orders for eachunit in the model. We empirically evaluate the proposed $L_p$ units on a numberof datasets and show that multilayer perceptrons (MLP) consisting of the $L_p$units achieve the state-of-the-art results on a number of benchmark datasets.Furthermore, we evaluate the proposed $L_p$ unit on the recently proposed deeprecurrent neural networks (RNN).
arxiv-5100-233 | How to Center Binary Deep Boltzmann Machines | http://arxiv.org/pdf/1311.1354v3.pdf | author:Jan Melchior, Asja Fischer, Laurenz Wiskott category:stat.ML cs.LG published:2013-11-06 summary:This work analyzes centered binary Restricted Boltzmann Machines (RBMs) andbinary Deep Boltzmann Machines (DBMs), where centering is done by subtractingoffset values from visible and hidden variables. We show analytically that (i)centering results in a different but equivalent parameterization for artificialneural networks in general, (ii) the expected performance of centered binaryRBMs/DBMs is invariant under simultaneous flip of data and offsets, for anyoffset value in the range of zero to one, (iii) centering can be reformulatedas a different update rule for normal binary RBMs/DBMs, and (iv) using theenhanced gradient is equivalent to setting the offset values to the averageover model and data mean. Furthermore, numerical simulations suggest that (i)optimal generative performance is achieved by subtracting mean values fromvisible as well as hidden variables, (ii) centered RBMs/DBMs reachsignificantly higher log-likelihood values than normal binary RBMs/DBMs, (iii)centering variants whose offsets depend on the model mean, like the enhancedgradient, suffer from severe divergence problems, (iv) learning is stabilizedif an exponentially moving average over the batch means is used for the offsetvalues instead of the current batch mean, which also prevents the enhancedgradient from diverging, (v) centered RBMs/DBMs reach higher LL values thannormal RBMs/DBMs while having a smaller norm of the weight matrix, (vi)centering leads to an update direction that is closer to the natural gradientand that the natural gradient is extremly efficient for training RBMs, (vii)centering dispense the need for greedy layer-wise pre-training of DBMs, (viii)furthermore we show that pre-training often even worsen the resultsindependently whether centering is used or not, and (ix) centering is alsobeneficial for auto encoders.
arxiv-5100-234 | Structural Learning for Template-free Protein Folding | http://arxiv.org/pdf/1311.1422v2.pdf | author:Feng Zhao category:cs.LG cs.CE q-bio.QM published:2013-11-06 summary:The thesis is aimed to solve the template-free protein folding problem bytackling two important components: efficient sampling in vast conformationspace, and design of knowledge-based potentials with high accuracy. We haveproposed the first-order and second-order CRF-Sampler to sample structures fromthe continuous local dihedral angles space by modeling the lower and higherorder conditional dependency between neighboring dihedral angles given theprimary sequence information. A framework combining the Conditional RandomFields and the energy function is introduced to guide the local conformationsampling using long range constraints with the energy function. The relationship between the sequence profile and the local dihedral angledistribution is nonlinear. Hence we proposed the CNF-Folder to model thiscomplex relationship by applying a novel machine learning model ConditionalNeural Fields which utilizes the structural graphical model with the neuralnetwork. CRF-Samplers and CNF-Folder perform very well in CASP8 and CASP9. Further, a novel pairwise distance statistical potential (EPAD) is designedto capture the dependency of the energy profile on the positions of theinteracting amino acids as well as the types of those amino acids, opposing thecommon assumption that this energy profile depends only on the types of aminoacids. EPAD has also been successfully applied in the CASP 10 Free Modelingexperiment with CNF-Folder, especially outstanding on some uncommon structuredtargets.
arxiv-5100-235 | Vision-Guided Robot Hearing | http://arxiv.org/pdf/1311.2460v2.pdf | author:Xavier Alameda-Pineda, Radu Horaud category:cs.RO cs.CV published:2013-11-06 summary:Natural human-robot interaction in complex and unpredictable environments isone of the main research lines in robotics. In typical real-world scenarios,humans are at some distance from the robot and the acquired signals arestrongly impaired by noise, reverberations and other interfering sources. Inthis context, the detection and localisation of speakers plays a key role sinceit is the pillar on which several tasks (e.g.: speech recognition and speakertracking) rely. We address the problem of how to detect and localize peoplethat are both seen and heard by a humanoid robot. We introduce a hybriddeterministic/probabilistic model. Indeed, the deterministic component allowsus to map the visual information into the auditory space. By means of theprobabilistic component, the visual features guide the grouping of the auditoryfeatures in order to form AV objects. The proposed model and the associatedalgorithm are implemented in real-time (17 FPS) using a stereoscopic camerapair and two microphones embedded into the head of the humanoid robot NAO. Weperformed experiments on (i) synthetic data, (ii) a publicly available data setand (iii) data acquired using the robot. The results we obtained validate theapproach and encourage us to further investigate how vision can help robothearing.
arxiv-5100-236 | Category-Theoretic Quantitative Compositional Distributional Models of Natural Language Semantics | http://arxiv.org/pdf/1311.1539v1.pdf | author:Edward Grefenstette category:cs.CL cs.LG math.CT math.LO I.2.7 published:2013-11-06 summary:This thesis is about the problem of compositionality in distributionalsemantics. Distributional semantics presupposes that the meanings of words area function of their occurrences in textual contexts. It models words asdistributions over these contexts and represents them as vectors in highdimensional spaces. The problem of compositionality for such models concernsitself with how to produce representations for larger units of text bycomposing the representations of smaller units of text. This thesis focuses on a particular approach to this compositionalityproblem, namely using the categorical framework developed by Coecke, Sadrzadeh,and Clark, which combines syntactic analysis formalisms with distributionalsemantic representations of meaning to produce syntactically motivatedcomposition operations. This thesis shows how this approach can betheoretically extended and practically implemented to produce concretecompositional distributional models of natural language semantics. Itfurthermore demonstrates that such models can perform on par with, or betterthan, other competing approaches in the field of natural language processing. There are three principal contributions to computational linguistics in thisthesis. The first is to extend the DisCoCat framework on the syntactic frontand semantic front, incorporating a number of syntactic analysis formalisms andproviding learning procedures allowing for the generation of concretecompositional distributional models. The second contribution is to evaluate themodels developed from the procedures presented here, showing that theyoutperform other compositional distributional models present in the literature.The third contribution is to show how using category theory to solve linguisticproblems forms a sound basis for research, illustrated by examples of work onthis topic, that also suggest directions for future research.
arxiv-5100-237 | Exploration in Interactive Personalized Music Recommendation: A Reinforcement Learning Approach | http://arxiv.org/pdf/1311.6355v1.pdf | author:Xinxi Wang, Yi Wang, David Hsu, Ye Wang category:cs.MM cs.IR cs.LG H.3.3; H.5.5 published:2013-11-06 summary:Current music recommender systems typically act in a greedy fashion byrecommending songs with the highest user ratings. Greedy recommendation,however, is suboptimal over the long term: it does not actively gatherinformation on user preferences and fails to recommend novel songs that arepotentially interesting. A successful recommender system must balance the needsto explore user preferences and to exploit this information for recommendation.This paper presents a new approach to music recommendation by formulating thisexploration-exploitation trade-off as a reinforcement learning task called themulti-armed bandit. To learn user preferences, it uses a Bayesian model, whichaccounts for both audio content and the novelty of recommendations. Apiecewise-linear approximation to the model and a variational inferencealgorithm are employed to speed up Bayesian inference. One additional benefitof our approach is a single unified model for both music recommendation andplaylist generation. Both simulation results and a user study indicate strongpotential for the new approach.
arxiv-5100-238 | Face Recognition via Globality-Locality Preserving Projections | http://arxiv.org/pdf/1311.1279v1.pdf | author:Sheng Huang, Dan Yang, Fei Yang, Yongxin Ge, Xiaohong Zhang, Jiwen Lu category:cs.CV published:2013-11-06 summary:We present an improved Locality Preserving Projections (LPP) method, namedGloablity-Locality Preserving Projections (GLPP), to preserve both the globaland local geometric structures of data. In our approach, an additionalconstraint of the geometry of classes is imposed to the objective function ofconventional LPP for respecting some more global manifold structures. Moreover,we formulate a two-dimensional extension of GLPP (2D-GLPP) as an example toshow how to extend GLPP with some other statistical techniques. We apply ourworks to face recognition on four popular face databases, namely ORL, Yale,FERET and LFW-A databases, and extensive experimental results demonstrate thatthe considered global manifold information can significantly improve theperformance of LPP and the proposed face recognition methods outperform thestate-of-the-arts.
arxiv-5100-239 | Delay Learning Architectures for Memory and Classification | http://arxiv.org/pdf/1311.1294v2.pdf | author:Shaista Hussain, Arindam Basu, R. Wang, Tara Julia Hamilton category:cs.NE q-bio.NC published:2013-11-06 summary:We present a neuromorphic spiking neural network, the DELTRON, that canremember and store patterns by changing the delays of every connection asopposed to modifying the weights. The advantage of this architecture overtraditional weight based ones is simpler hardware implementation withoutmultipliers or digital-analog converters (DACs) as well as being suited totime-based computing. The name is derived due to similarity in the learningrule with an earlier architecture called Tempotron. The DELTRON can remembermore patterns than other delay-based networks by modifying a few delays toremember the most 'salient' or synchronous part of every spike pattern. Wepresent simulations of memory capacity and classification ability of theDELTRON for different random spatio-temporal spike patterns. The memorycapacity for noisy spike patterns and missing spikes are also shown. Finally,we present SPICE simulation results of the core circuits involved in areconfigurable mixed signal implementation of this architecture.
arxiv-5100-240 | Quality Assessment of Pixel-Level ImageFusion Using Fuzzy Logic | http://arxiv.org/pdf/1311.1223v1.pdf | author:Srinivasa Rao Dammavalam, Seetha Maddala, M. H. M. Krishna Prasad category:cs.CV published:2013-11-05 summary:Image fusion is to reduce uncertainty and minimize redundancy in the outputwhile maximizing relevant information from two or more images of a scene into asingle composite image that is more informative and is more suitable for visualperception or processing tasks like medical imaging, remote sensing, concealedweapon detection, weather forecasting, biometrics etc. Image fusion combinesregistered images to produce a high quality fused image with spatial andspectral information. The fused image with more information will improve theperformance of image analysis algorithms used in different applications. Inthis paper, we proposed a fuzzy logic method to fuse images from differentsensors, in order to enhance the quality and compared proposed method with twoother methods i.e. image fusion using wavelet transform and weighted averagediscrete wavelet transform based image fusion using genetic algorithm (hereonwards abbreviated as GA) along with quality evaluation parameters imagequality index (IQI), mutual information measure (MIM), root mean square error(RMSE), peak signal to noise ratio (PSNR), fusion factor (FF), fusion symmetry(FS) and fusion index (FI) and entropy. The results obtained from proposedfuzzy based image fusion approach improves quality of fused image as comparedto earlier reported methods, wavelet transform based image fusion and weightedaverage discrete wavelet transform based image fusion using genetic algorithm.
arxiv-5100-241 | Large Margin Distribution Machine | http://arxiv.org/pdf/1311.0989v2.pdf | author:Teng Zhang, Zhi-Hua Zhou category:cs.LG published:2013-11-05 summary:Support vector machine (SVM) has been one of the most popular learningalgorithms, with the central idea of maximizing the minimum margin, i.e., thesmallest distance from the instances to the classification boundary. Recenttheoretical results, however, disclosed that maximizing the minimum margin doesnot necessarily lead to better generalization performances, and instead, themargin distribution has been proven to be more crucial. In this paper, wepropose the Large margin Distribution Machine (LDM), which tries to achieve abetter generalization performance by optimizing the margin distribution. Wecharacterize the margin distribution by the first- and second-order statistics,i.e., the margin mean and variance. The LDM is a general learning approachwhich can be used in any place where SVM can be applied, and its superiority isverified both theoretically and empirically in this paper.
arxiv-5100-242 | Identifying Purpose Behind Electoral Tweets | http://arxiv.org/pdf/1311.1194v1.pdf | author:Saif M. Mohammad, Svetlana Kiritchenko, Joel Martin category:cs.CL published:2013-11-05 summary:Tweets pertaining to a single event, such as a national election, can numberin the hundreds of millions. Automatically analyzing them is beneficial in manydownstream natural language applications such as question answering andsummarization. In this paper, we propose a new task: identifying the purposebehind electoral tweets--why do people post election-oriented tweets? We showthat identifying purpose is correlated with the related phenomenon of sentimentand emotion detection, but yet significantly different. Detecting purpose has anumber of applications including detecting the mood of the electorate,estimating the popularity of policies, identifying key issues of contention,and predicting the course of events. We create a large dataset of electoraltweets and annotate a few thousand tweets for purpose. We develop a system thatautomatically classifies electoral tweets as per their purpose, obtaining anaccuracy of 43.56% on an 11-class task and an accuracy of 73.91% on a 3-classtask (both accuracies well above the most-frequent-class baseline). Finally, weshow that resources developed for emotion detection are also helpful fordetecting purpose.
arxiv-5100-243 | Statistical Inference in Hidden Markov Models using $k$-segment Constraints | http://arxiv.org/pdf/1311.1189v1.pdf | author:Michalis K. Titsias, Christopher Yau, Christopher C. Holmes category:stat.ME cs.LG stat.ML published:2013-11-05 summary:Hidden Markov models (HMMs) are one of the most widely used statisticalmethods for analyzing sequence data. However, the reporting of output from HMMshas largely been restricted to the presentation of the most-probable (MAP)hidden state sequence, found via the Viterbi algorithm, or the sequence of mostprobable marginals using the forward-backward (F-B) algorithm. In this article,we expand the amount of information we could obtain from the posteriordistribution of an HMM by introducing linear-time dynamic programmingalgorithms that, we collectively call $k$-segment algorithms, that allow us toi) find MAP sequences, ii) compute posterior probabilities and iii) simulatesample paths conditional on a user specified number of segments, i.e.contiguous runs in a hidden state, possibly of a particular type. We illustratethe utility of these methods using simulated and real examples and highlightthe application of prospective and retrospective use of these methods forfitting HMMs or exploring existing model fits.
arxiv-5100-244 | Using Robust PCA to estimate regional characteristics of language use from geo-tagged Twitter messages | http://arxiv.org/pdf/1311.1169v1.pdf | author:DÃ¡niel Kondor, IstvÃ¡n Csabai, LÃ¡szlÃ³ Dobos, JÃ¡nos SzÃ¼le, Norbert Barankai, TamÃ¡s Hanyecz, TamÃ¡s SebÅk, ZsÃ³fia Kallus, GÃ¡bor Vattay category:cs.CL published:2013-11-05 summary:Principal component analysis (PCA) and related techniques have beensuccessfully employed in natural language processing. Text mining applicationsin the age of the online social media (OSM) face new challenges due toproperties specific to these use cases (e.g. spelling issues specific to textsposted by users, the presence of spammers and bots, service announcements,etc.). In this paper, we employ a Robust PCA technique to separate typicaloutliers and highly localized topics from the low-dimensional structure presentin language use in online social networks. Our focus is on identifyinggeospatial features among the messages posted by the users of the Twittermicroblogging service. Using a dataset which consists of over 200 milliongeolocated tweets collected over the course of a year, we investigate whetherthe information present in word usage frequencies can be used to identifyregional features of language use and topics of interest. Using the PCA pursuitmethod, we are able to identify important low-dimensional features, whichconstitute smoothly varying functions of the geographic location.
arxiv-5100-245 | Dropout improves Recurrent Neural Networks for Handwriting Recognition | http://arxiv.org/pdf/1312.4569v2.pdf | author:Vu Pham, ThÃ©odore Bluche, Christopher Kermorvant, JÃ©rÃ´me Louradour category:cs.CV cs.LG cs.NE published:2013-11-05 summary:Recurrent neural networks (RNNs) with Long Short-Term memory cells currentlyhold the best known results in unconstrained handwriting recognition. We showthat their performance can be greatly improved using dropout - a recentlyproposed regularization method for deep architectures. While previous worksshowed that dropout gave superior performance in the context of convolutionalnetworks, it had never been applied to RNNs. In our approach, dropout iscarefully used in the network so that it does not affect the recurrentconnections, hence the power of RNNs in modeling sequence is preserved.Extensive experiments on a broad range of handwritten databases confirm theeffectiveness of dropout on deep architectures even when the network mainlyconsists of recurrent and shared connections.
arxiv-5100-246 | Nonparametric Bayesian models of hierarchical structure in complex networks | http://arxiv.org/pdf/1311.1033v2.pdf | author:Mikkel N. Schmidt, Tue Herlau, Morten MÃ¸rup category:stat.ML published:2013-11-05 summary:Analyzing and understanding the structure of complex relational data isimportant in many applications including analysis of the connectivity in thehuman brain. Such networks can have prominent patterns on different scales,calling for a hierarchically structured model. We propose two non-parametricBayesian hierarchical network models based on Gibbs fragmentation tree priors,and demonstrate their ability to capture nested patterns in simulated networks.On real networks we demonstrate detection of hierarchical structure and showpredictive performance on par with the state of the art. We envision that ourmethods can be employed in exploratory analysis of large scale complex networksfor example to model human brain connectivity.
arxiv-5100-247 | Polyhedrons and Perceptrons Are Functionally Equivalent | http://arxiv.org/pdf/1311.1090v1.pdf | author:Daniel Crespin category:cs.NE 68T01 C.1.3; I.2.6 published:2013-11-05 summary:Mathematical definitions of polyhedrons and perceptron networks arediscussed. The formalization of polyhedrons is done in a rather traditionalway. For networks, previously proposed systems are developed. Perceptronnetworks in disjunctive normal form (DNF) and conjunctive normal forms (CNF)are introduced. The main theme is that single output perceptron neural networksand characteristic functions of polyhedrons are one and the same class offunctions. A rigorous formulation and proof that three layers suffice isobtained. The various constructions and results are among several stepsrequired for algorithms that replace incremental and statistical learning withmore efficient, direct and exact geometric methods for calculation ofperceptron architecture and weights.
arxiv-5100-248 | Event-Driven Contrastive Divergence for Spiking Neuromorphic Systems | http://arxiv.org/pdf/1311.0966v3.pdf | author:Emre Neftci, Srinjoy Das, Bruno Pedroni, Kenneth Kreutz-Delgado, Gert Cauwenberghs category:cs.NE q-bio.NC published:2013-11-05 summary:Restricted Boltzmann Machines (RBMs) and Deep Belief Networks have beendemonstrated to perform efficiently in a variety of applications, such asdimensionality reduction, feature learning, and classification. Theirimplementation on neuromorphic hardware platforms emulating large-scalenetworks of spiking neurons can have significant advantages from theperspectives of scalability, power dissipation and real-time interfacing withthe environment. However the traditional RBM architecture and the commonly usedtraining algorithm known as Contrastive Divergence (CD) are based on discreteupdates and exact arithmetics which do not directly map onto a dynamical neuralsubstrate. Here, we present an event-driven variation of CD to train a RBMconstructed with Integrate & Fire (I&F) neurons, that is constrained by thelimitations of existing and near future neuromorphic hardware platforms. Ourstrategy is based on neural sampling, which allows us to synthesize a spikingneural network that samples from a target Boltzmann distribution. The recurrentactivity of the network replaces the discrete steps of the CD algorithm, whileSpike Time Dependent Plasticity (STDP) carries out the weight updates in anonline, asynchronous fashion. We demonstrate our approach by training an RBMcomposed of leaky I&F neurons with STDP synapses to learn a generative model ofthe MNIST hand-written digit dataset, and by testing it in recognition,generation and cue integration tasks. Our results contribute to a machinelearning-driven approach for synthesizing networks of spiking neurons capableof carrying out practical, high-level functionality.
arxiv-5100-249 | Fifth-order canonical polyadic decomposition with partial symmetry via joint diagonalization for combined independent component analysis and canonical / Parallel factor analysis | http://arxiv.org/pdf/1311.1040v1.pdf | author:Xiao-Feng Gong, Cheng-Yuan Wang, Ya-Na Hao, Qiu-Hua Lin category:stat.ML cs.LG published:2013-11-05 summary:Recently, there has been a trend to combine independent component analysisand canonical / parallel factor analysis (ICA-CPA) for an enhanced robustnessfor the computation of CPA, and ICA-CPA could be further converted into theproblem of canonical polyadic decomposition (CPD) of a 5th-order partiallysymmetric tensor, by calculating the 4th-order cumulant of a trilinear mixture.In this study, we propose a new 5th-order CPD algorithm constrained withpartial symmetry using joint diagonalization. As the main steps involved in theproposed algorithm undergo no updating iterations for the loading matrices, itis much faster than the existing algorithm based on alternating least squaresand enhanced line search, and therefore could be used as a nice initializationfor the latter. Simulation results are given to examine the performance of theproposed algorithm.
arxiv-5100-250 | Motion and audio analysis in mobile devices for remote monitoring of physical activities and user authentication | http://arxiv.org/pdf/1311.1132v1.pdf | author:Hamed Ketabdar, Jalaluddin Qureshi, Pan Hui category:cs.HC cs.CV published:2013-11-05 summary:In this article we propose the use of accelerometer embedded by default insmartphone as a cost-effective, reliable and efficient way to provide remotephysical activity monitoring for the elderly and people requiring healthcareservice. Mobile phones are regularly carried by users during their day-to-daywork routine, physical movement information can be captured by the mobile phoneaccelerometer, processed and sent to a remote server for monitoring. Theacceleration pattern can deliver information related to the pattern of physicalactivities the user is engaged in. We further show how this technique can beextended to provide implicit real-time security by analysing unexpectedmovements captured by the phone accelerometer, and automatically locking thephone in such situation to prevent unauthorised access. This technique is alsoshown to provide implicit continuous user authentication, by capturing regularuser movements such as walking, and requesting for re-authentication wheneverit detects a non-regular movement.
arxiv-5100-251 | A Parallel SGD method with Strong Convergence | http://arxiv.org/pdf/1311.0636v1.pdf | author:Dhruv Mahajan, S. Sathiya Keerthi, S. Sundararajan, Leon Bottou category:cs.LG cs.DC published:2013-11-04 summary:This paper proposes a novel parallel stochastic gradient descent (SGD) methodthat is obtained by applying parallel sets of SGD iterations (each setoperating on one node using the data residing in it) for finding the directionin each iteration of a batch descent method. The method has strong convergenceproperties. Experiments on datasets with high dimensional feature spaces showthe value of this method.
arxiv-5100-252 | A Parallel Compressive Imaging Architecture for One-Shot Acquisition | http://arxiv.org/pdf/1311.0646v1.pdf | author:Tomas BjÃ¶rklund, Enrico Magli category:cs.CV astro-ph.IM published:2013-11-04 summary:A limitation of many compressive imaging architectures lies in the sequentialnature of the sensing process, which leads to long sensing times. In this paperwe present a novel architecture that uses fewer detectors than the number ofreconstructed pixels and is able to acquire the image in a single acquisition.This paves the way for the development of video architectures that acquireseveral frames per second. We specifically address the diffraction problem,showing that deconvolution normally used to recover diffraction blur can bereplaced by convolution of the sensing matrix, and how measurements of a 0/1physical sensing matrix can be converted to -1/1 compressive sensing matrixwithout any extra acquisitions. Simulations of our architecture show that theimage quality is comparable to that of a classic Compressive Imaging camera,whereas the proposed architecture avoids long acquisition times due tosequential sensing. This one-shot procedure also allows to employ a fixedsensing matrix instead of a complex device such as a Digital Micro Mirror arrayor Spatial Light Modulator. It also enables imaging at bandwidths where theseare not efficient.
arxiv-5100-253 | Distributed Exploration in Multi-Armed Bandits | http://arxiv.org/pdf/1311.0800v1.pdf | author:Eshcar Hillel, Zohar Karnin, Tomer Koren, Ronny Lempel, Oren Somekh category:cs.LG published:2013-11-04 summary:We study exploration in Multi-Armed Bandits in a setting where $k$ playerscollaborate in order to identify an $\epsilon$-optimal arm. Our motivationcomes from recent employment of bandit algorithms in computationally intensive,large-scale applications. Our results demonstrate a non-trivial tradeoffbetween the number of arm pulls required by each of the players, and the amountof communication between them. In particular, our main result shows that byallowing the $k$ players to communicate only once, they are able to learn$\sqrt{k}$ times faster than a single player. That is, distributing learning to$k$ players gives rise to a factor $\sqrt{k}$ parallel speed-up. We complementthis result with a lower bound showing this is in general the best possible. Onthe other extreme, we present an algorithm that achieves the ideal factor $k$speed-up in learning performance, with communication only logarithmic in$1/\epsilon$.
arxiv-5100-254 | TOP-SPIN: TOPic discovery via Sparse Principal component INterference | http://arxiv.org/pdf/1311.1406v1.pdf | author:Martin TakÃ¡Ä, Selin Damla AhipaÅaoÄlu, Ngai-Man Cheung, Peter RichtÃ¡rik category:cs.CV cs.IR cs.LG published:2013-11-04 summary:We propose a novel topic discovery algorithm for unlabeled images based onthe bag-of-words (BoW) framework. We first extract a dictionary of visual wordsand subsequently for each image compute a visual word occurrence histogram. Weview these histograms as rows of a large matrix from which we extract sparseprincipal components (PCs). Each PC identifies a sparse combination of visualwords which co-occur frequently in some images but seldom appear in others.Each sparse PC corresponds to a topic, and images whose interference with thePC is high belong to that topic, revealing the common parts possessed by theimages. We propose to solve the associated sparse PCA problems using anAlternating Maximization (AM) method, which we modify for purpose ofefficiently extracting multiple PCs in a deflation scheme. Our approach attacksthe maximization problem in sparse PCA directly and is scalable tohigh-dimensional data. Experiments on automatic topic discovery and categoryprediction demonstrate encouraging performance of our approach.
arxiv-5100-255 | On Fast Dropout and its Applicability to Recurrent Networks | http://arxiv.org/pdf/1311.0701v7.pdf | author:Justin Bayer, Christian Osendorfer, Daniela Korhammer, Nutan Chen, Sebastian Urban, Patrick van der Smagt category:stat.ML cs.LG cs.NE published:2013-11-04 summary:Recurrent Neural Networks (RNNs) are rich models for the processing ofsequential data. Recent work on advancing the state of the art has been focusedon the optimization or modelling of RNNs, mostly motivated by adressing theproblems of the vanishing and exploding gradients. The control of overfittinghas seen considerably less attention. This paper contributes to that byanalyzing fast dropout, a recent regularization method for generalized linearmodels and neural networks from a back-propagation inspired perspective. Weshow that fast dropout implements a quadratic form of an adaptive,per-parameter regularizer, which rewards large weights in the light ofunderfitting, penalizes them for overconfident predictions and vanishes atminima of an unregularized training loss. The derivatives of that regularizerare exclusively based on the training error signal. One consequence of this isthe absense of a global weight attractor, which is particularly appealing forRNNs, since the dynamics are not biased towards a certain regime. We positivelytest the hypothesis that this improves the performance of RNNs on four musicaldata sets.
arxiv-5100-256 | A Comparative Study on Linguistic Feature Selection in Sentiment Polarity Classification | http://arxiv.org/pdf/1311.0833v1.pdf | author:Zitao Liu category:cs.CL published:2013-11-04 summary:Sentiment polarity classification is perhaps the most widely studied topic.It classifies an opinionated document as expressing a positive or negativeopinion. In this paper, using movie review dataset, we perform a comparativestudy with different single kind linguistic features and the combinations ofthese features. We find that the classic topic-based classifier(Naive Bayes andSupport Vector Machine) do not perform as well on sentiment polarityclassification. And we find that with some combination of different linguisticfeatures, the classification accuracy can be boosted a lot. We give somereasonable explanations about these boosting outcomes.
arxiv-5100-257 | A Divide-and-Conquer Solver for Kernel Support Vector Machines | http://arxiv.org/pdf/1311.0914v1.pdf | author:Cho-Jui Hsieh, Si Si, Inderjit S. Dhillon category:cs.LG published:2013-11-04 summary:The kernel support vector machine (SVM) is one of the most widely usedclassification methods; however, the amount of computation required becomes thebottleneck when facing millions of samples. In this paper, we propose andanalyze a novel divide-and-conquer solver for kernel SVMs (DC-SVM). In thedivision step, we partition the kernel SVM problem into smaller subproblems byclustering the data, so that each subproblem can be solved independently andefficiently. We show theoretically that the support vectors identified by thesubproblem solution are likely to be support vectors of the entire kernel SVMproblem, provided that the problem is partitioned appropriately by kernelclustering. In the conquer step, the local solutions from the subproblems areused to initialize a global coordinate descent solver, which converges quicklyas suggested by our analysis. By extending this idea, we develop a multilevelDivide-and-Conquer SVM algorithm with adaptive clustering and early predictionstrategy, which outperforms state-of-the-art methods in terms of trainingspeed, testing accuracy, and memory usage. As an example, on the covtypedataset with half-a-million samples, DC-SVM is 7 times faster than LIBSVM inobtaining the exact SVM solution (to within $10^{-6}$ relative error) whichachieves 96.15% prediction accuracy. Moreover, with our proposed earlyprediction strategy, DC-SVM achieves about 96% accuracy in only 12 minutes,which is more than 100 times faster than LIBSVM.
arxiv-5100-258 | Stochastic Dual Coordinate Ascent with Alternating Direction Multiplier Method | http://arxiv.org/pdf/1311.0622v1.pdf | author:Taiji Suzuki category:stat.ML published:2013-11-04 summary:We propose a new stochastic dual coordinate ascent technique that can beapplied to a wide range of regularized learning problems. Our method is basedon Alternating Direction Multiplier Method (ADMM) to deal with complexregularization functions such as structured regularizations. Although theoriginal ADMM is a batch method, the proposed method offers a stochastic updaterule where each iteration requires only one or few sample observations.Moreover, our method can naturally afford mini-batch update and it gives speedup of convergence. We show that, under mild assumptions, our method convergesexponentially. The numerical experiments show that our method actually performsefficiently.
arxiv-5100-259 | Q-Gaussian Swarm Quantum Particle Intelligence on Predicting Global Minimum of Potential Energy Function | http://arxiv.org/pdf/1311.0598v1.pdf | author:Hiqmet Kamberaj category:cs.NE published:2013-11-04 summary:We present a newly developed -Gaussian Swarm Quantum-like ParticleOptimization (q-GSQPO) algorithm to determine the global minimum of thepotential energy function. Swarm Quantum-like Particle Optimization (SQPO)algorithms have been derived using different attractive potential fields torepresent swarm particles moving in a quantum environment, where the one whichuses a harmonic oscillator potential as attractive field is considered as animproved version. In this paper, we propose a new SQPO that uses -Gaussianprobability density function for the attractive potential field (q-GSQPO)rather than Gaussian one (GSQPO) which corresponds to harmonic potential. Theperformance of the q-GSQPO is compared against the GSQPO. The new algorithmoutperforms the GSQPO on most of the time in convergence to the global optimumby increasing the efficiency of sampling the phase space and avoiding thepremature convergence to local minima. Moreover, the computational efforts werecomparable for both algorithms. We tested the algorithm to determine the lowestenergy configurations of a particle moving in a 2, 5, 10, and 50 dimensionalspaces.
arxiv-5100-260 | Generative Modelling for Unsupervised Score Calibration | http://arxiv.org/pdf/1311.0707v3.pdf | author:Niko BrÃ¼mmer, Daniel Garcia-Romero category:stat.ML cs.LG published:2013-11-04 summary:Score calibration enables automatic speaker recognizers to makecost-effective accept / reject decisions. Traditional calibration requiressupervised data, which is an expensive resource. We propose a 2-component GMMfor unsupervised calibration and demonstrate good performance relative to asupervised baseline on NIST SRE'10 and SRE'12. A Bayesian analysis demonstratesthat the uncertainty associated with the unsupervised calibration parameterestimates is surprisingly small.
arxiv-5100-261 | Particle filter-based Gaussian process optimisation for parameter inference | http://arxiv.org/pdf/1311.0689v2.pdf | author:Johan Dahlin, Fredrik Lindsten category:stat.CO stat.ML published:2013-11-04 summary:We propose a novel method for maximum likelihood-based parameter inference innonlinear and/or non-Gaussian state space models. The method is an iterativeprocedure with three steps. At each iteration a particle filter is used toestimate the value of the log-likelihood function at the current parameteriterate. Using these log-likelihood estimates, a surrogate objective functionis created by utilizing a Gaussian process model. Finally, we use a heuristicprocedure to obtain a revised parameter iterate, providing an automatictrade-off between exploration and exploitation of the surrogate model. Themethod is profiled on two state space models with good performance bothconsidering accuracy and computational cost.
arxiv-5100-262 | Oracle Inequalities for High Dimensional Vector Autoregressions | http://arxiv.org/pdf/1311.0811v2.pdf | author:Anders Bredahl Kock, Laurent A. F. Callot category:math.ST stat.ML stat.TH published:2013-11-04 summary:This paper establishes non-asymptotic oracle inequalities for the predictionerror and estimation accuracy of the LASSO in stationary vector autoregressivemodels. These inequalities are used to establish consistency of the LASSO evenwhen the number of parameters is of a much larger order of magnitude than thesample size. We also give conditions under which no relevant variables areexcluded. Next, non-asymptotic probabilities are given for the Adaptive LASSO to selectthe correct sparsity pattern. We then give conditions under which the AdaptiveLASSO reveals the correct sparsity pattern asymptotically. We establish thatthe estimates of the non-zero coefficients are asymptotically equivalent to theoracle assisted least squares estimator. This is used to show that the rate ofconvergence of the estimates of the non-zero coefficients is identical to theone of least squares only including the relevant covariates.
arxiv-5100-263 | The Squared-Error of Generalized LASSO: A Precise Analysis | http://arxiv.org/pdf/1311.0830v2.pdf | author:Samet Oymak, Christos Thrampoulidis, Babak Hassibi category:cs.IT math.IT math.OC stat.ML published:2013-11-04 summary:We consider the problem of estimating an unknown signal $x_0$ from noisylinear observations $y = Ax_0 + z\in R^m$. In many practical instances, $x_0$has a certain structure that can be captured by a structure inducing convexfunction $f(\cdot)$. For example, $\ell_1$ norm can be used to encourage asparse solution. To estimate $x_0$ with the aid of $f(\cdot)$, we consider thewell-known LASSO method and provide sharp characterization of its performance.We assume the entries of the measurement matrix $A$ and the noise vector $z$have zero-mean normal distributions with variances $1$ and $\sigma^2$respectively. For the LASSO estimator $x^*$, we attempt to calculate theNormalized Square Error (NSE) defined as $\frac{\x^*-x_0\_2^2}{\sigma^2}$ asa function of the noise level $\sigma$, the number of observations $m$ and thestructure of the signal. We show that, the structure of the signal $x_0$ andchoice of the function $f(\cdot)$ enter the error formulae through the summaryparameters $D(cone)$ and $D(\lambda)$, which are defined as the Gaussiansquared-distances to the subdifferential cone and to the $\lambda$-scaledsubdifferential, respectively. The first LASSO estimator assumes a-prioriknowledge of $f(x_0)$ and is given by $\arg\min_{x}\{{\y-Ax\_2}~\text{subjectto}~f(x)\leq f(x_0)\}$. We prove that its worst case NSE is achieved when$\sigma\rightarrow 0$ and concentrates around $\frac{D(cone)}{m-D(cone)}$.Secondly, we consider $\arg\min_{x}\{\y-Ax\_2+\lambda f(x)\}$, for some$\lambda\geq 0$. This time the NSE formula depends on the choice of $\lambda$and is given by $\frac{D(\lambda)}{m-D(\lambda)}$. We then establish a mappingbetween this and the third estimator $\arg\min_{x}\{\frac{1}{2}\y-Ax\_2^2+\lambda f(x)\}$. Finally, for a number of important structured signal classes,we translate our abstract formulae to closed-form upper bounds on the NSE.
arxiv-5100-264 | Particle Metropolis-Hastings using gradient and Hessian information | http://arxiv.org/pdf/1311.0686v4.pdf | author:Johan Dahlin, Fredrik Lindsten, Thomas B. SchÃ¶n category:stat.CO stat.ML published:2013-11-04 summary:Particle Metropolis-Hastings (PMH) allows for Bayesian parameter inference innonlinear state space models by combining Markov chain Monte Carlo (MCMC) andparticle filtering. The latter is used to estimate the intractable likelihood.In its original formulation, PMH makes use of a marginal MCMC proposal for theparameters, typically a Gaussian random walk. However, this can lead to a poorexploration of the parameter space and an inefficient use of the generatedparticles. We propose a number of alternative versions of PMH that incorporate gradientand Hessian information about the posterior into the proposal. This informationis more or less obtained as a byproduct of the likelihood estimation. Indeed,we show how to estimate the required information using a fixed-lag particlesmoother, with a computational cost growing linearly in the number ofparticles. We conclude that the proposed methods can: (i) decrease the lengthof the burn-in phase, (ii) increase the mixing of the Markov chain at thestationary phase, and (iii) make the proposal distribution scale invariantwhich simplifies tuning.
arxiv-5100-265 | An Adaptive Amoeba Algorithm for Shortest Path Tree Computation in Dynamic Graphs | http://arxiv.org/pdf/1311.0460v1.pdf | author:Xiaoge Zhang, Qi Liu, Yong Hu, Felix T. S. Chan, Sankaran Mahadevan, Zili Zhang, Yong Deng category:cs.NE published:2013-11-03 summary:This paper presents an adaptive amoeba algorithm to address the shortest pathtree (SPT) problem in dynamic graphs. In dynamic graphs, the edge weightupdates consists of three categories: edge weight increases, edge weightdecreases, the mixture of them. Existing work on this problem solve this issuethrough analyzing the nodes influenced by the edge weight updates and recomputethese affected vertices. However, when the network becomes big, the processwill become complex. The proposed method can overcome the disadvantages of theexisting approaches. The most important feature of this algorithm is itsadaptivity. When the edge weight changes, the proposed algorithm can recognizethe affected vertices and reconstruct them spontaneously. To evaluate theproposed adaptive amoeba algorithm, we compare it with the Label Settingalgorithm and Bellman-Ford algorithm. The comparison results demonstrate theeffectiveness of the proposed method.
arxiv-5100-266 | Thompson Sampling for Complex Bandit Problems | http://arxiv.org/pdf/1311.0466v1.pdf | author:Aditya Gopalan, Shie Mannor, Yishay Mansour category:stat.ML cs.LG published:2013-11-03 summary:We consider stochastic multi-armed bandit problems with complex actions overa set of basic arms, where the decision maker plays a complex action ratherthan a basic arm in each round. The reward of the complex action is somefunction of the basic arms' rewards, and the feedback observed may notnecessarily be the reward per-arm. For instance, when the complex actions aresubsets of the arms, we may only observe the maximum reward over the chosensubset. Thus, feedback across complex actions may be coupled due to the natureof the reward function. We prove a frequentist regret bound for Thompsonsampling in a very general setting involving parameter, action and observationspaces and a likelihood function over them. The bound holds fordiscretely-supported priors over the parameter space and without additionalstructural properties such as closed-form posteriors, conjugate prior structureor independence across arms. The regret bound scales logarithmically with timebut, more importantly, with an improved constant that non-trivially capturesthe coupling across complex actions due to the structure of the rewards. Asapplications, we derive improved regret bounds for classes of complex banditproblems involving selecting subsets of arms, including the first nontrivialregret bounds for nonlinear MAX reward feedback from subsets.
arxiv-5100-267 | Thompson Sampling for Online Learning with Linear Experts | http://arxiv.org/pdf/1311.0468v1.pdf | author:Aditya Gopalan category:stat.ML cs.LG published:2013-11-03 summary:In this note, we present a version of the Thompson sampling algorithm for theproblem of online linear generalization with full information (i.e., theexperts setting), studied by Kalai and Vempala, 2005. The algorithm uses aGaussian prior and time-varying Gaussian likelihoods, and we show that itessentially reduces to Kalai and Vempala's Follow-the-Perturbed-Leaderstrategy, with exponentially distributed noise replaced by Gaussian noise. Thisimplies sqrt(T) regret bounds for Thompson sampling (with time-varyinglikelihood) for online learning with full information.
arxiv-5100-268 | Data-based approximate policy iteration for nonlinear continuous-time optimal control design | http://arxiv.org/pdf/1311.0396v1.pdf | author:Biao Luo, Huai-Ning Wu, Tingwen Huang, Derong Liu category:cs.SY math.OC stat.ML published:2013-11-02 summary:This paper addresses the model-free nonlinear optimal problem withgeneralized cost functional, and a data-based reinforcement learning techniqueis developed. It is known that the nonlinear optimal control problem relies onthe solution of the Hamilton-Jacobi-Bellman (HJB) equation, which is anonlinear partial differential equation that is generally impossible to besolved analytically. Even worse, most of practical systems are too complicatedto establish their accurate mathematical model. To overcome these difficulties,we propose a data-based approximate policy iteration (API) method by using realsystem data rather than system model. Firstly, a model-free policy iterationalgorithm is derived for constrained optimal control problem and itsconvergence is proved, which can learn the solution of HJB equation and optimalcontrol policy without requiring any knowledge of system mathematical model.The implementation of the algorithm is based on the thought of actor-criticstructure, where actor and critic neural networks (NNs) are employed toapproximate the control policy and cost function, respectively. To update theweights of actor and critic NNs, a least-square approach is developed based onthe method of weighted residuals. The whole data-based API method includes twoparts, where the first part is implemented online to collect real systeminformation, and the second part is conducting offline policy iteration tolearn the solution of HJB equation and the control policy. Then, the data-basedAPI algorithm is simplified for solving unconstrained optimal control problemof nonlinear and linear systems. Finally, we test the efficiency of thedata-based API control design method on a simple nonlinear system, and furtherapply it to a rotational/translational actuator system. The simulation resultsdemonstrate the effectiveness of the proposed method.
arxiv-5100-269 | Multivariate Generalized Gaussian Process Models | http://arxiv.org/pdf/1311.0360v1.pdf | author:Antoni B. Chan category:stat.ML published:2013-11-02 summary:We propose a family of multivariate Gaussian process models for correlatedoutputs, based on assuming that the likelihood function takes the generic formof the multivariate exponential family distribution (EFD). We denote this modelas a multivariate generalized Gaussian process model, and derive Taylor andLaplace algorithms for approximate inference on the generic model. Byinstantiating the EFD with specific parameter functions, we obtain two novel GPmodels (and corresponding inference algorithms) for correlated outputs: 1) aVon-Mises GP for angle regression; and 2) a Dirichlet GP for regressing on themultinomial simplex.
arxiv-5100-270 | Nearly Optimal Sample Size in Hypothesis Testing for High-Dimensional Regression | http://arxiv.org/pdf/1311.0274v1.pdf | author:Adel Javanmard, Andrea Montanari category:math.ST cs.IT cs.LG math.IT stat.ME stat.TH published:2013-11-01 summary:We consider the problem of fitting the parameters of a high-dimensionallinear regression model. In the regime where the number of parameters $p$ iscomparable to or exceeds the sample size $n$, a successful approach uses an$\ell_1$-penalized least squares estimator, known as Lasso. Unfortunately,unlike for linear estimators (e.g., ordinary least squares), nowell-established method exists to compute confidence intervals or p-values onthe basis of the Lasso estimator. Very recently, a line of work\cite{javanmard2013hypothesis, confidenceJM, GBR-hypothesis} has addressed thisproblem by constructing a debiased version of the Lasso estimator. In thispaper, we study this approach for random design model, under the assumptionthat a good estimator exists for the precision matrix of the design. Ouranalysis improves over the state of the art in that it establishes nearlyoptimal \emph{average} testing power if the sample size $n$ asymptoticallydominates $s_0 (\log p)^2$, with $s_0$ being the sparsity level (number ofnon-zero coefficients). Earlier work obtains provable guarantees only for muchlarger sample size, namely it requires $n$ to asymptotically dominate $(s_0\log p)^2$. In particular, for random designs with a sparse precision matrix we show thatan estimator thereof having the required properties can be computedefficiently. Finally, we evaluate this approach on synthetic data and compareit with earlier proposals.
arxiv-5100-271 | Reinforcement Learning for Matrix Computations: PageRank as an Example | http://arxiv.org/pdf/1311.2889v1.pdf | author:Vivek S. Borkar, Adwaitvedant S. Mathkar category:cs.LG cs.SI stat.ML published:2013-11-01 summary:Reinforcement learning has gained wide popularity as a technique forsimulation-driven approximate dynamic programming. A less known aspect is thatthe very reasons that make it effective in dynamic programming can also beleveraged for using it for distributed schemes for certain matrix computationsinvolving non-negative matrices. In this spirit, we propose a reinforcementlearning algorithm for PageRank computation that is fashioned after analogousschemes for approximate dynamic programming. The algorithm has the advantage ofease of distributed implementation and more importantly, of being model-free,i.e., not dependent on any specific assumptions about the transitionprobabilities in the random web-surfer model. We analyze its convergence andfinite time behavior and present some supporting numerical experiments.
arxiv-5100-272 | Parsimonious Shifted Asymmetric Laplace Mixtures | http://arxiv.org/pdf/1311.0317v1.pdf | author:Brian C. Franczak, Paul D. McNicholas, Ryan P. Browne, Paula M. Murray category:stat.ME stat.CO stat.ML published:2013-11-01 summary:A family of parsimonious shifted asymmetric Laplace mixture models isintroduced. We extend the mixture of factor analyzers model to the shiftedasymmetric Laplace distribution. Imposing constraints on the constitute partsof the resulting decomposed component scale matrices leads to a family ofparsimonious models. An explicit two-stage parameter estimation procedure isdescribed, and the Bayesian information criterion and the integrated completedlikelihood are compared for model selection. This novel family of models isapplied to real data, where it is compared to its Gaussian analogue withinclustering and classification paradigms.
arxiv-5100-273 | Iterative Bilateral Filtering of Polarimetric SAR Data | http://arxiv.org/pdf/1311.0162v1.pdf | author:Olivier D'Hondt, StÃ©phane Guillaso, Olaf Hellwich category:cs.CV published:2013-11-01 summary:In this paper, we introduce an iterative speckle filtering method forpolarimetric SAR (PolSAR) images based on the bilateral filter. To locallyadapt to the spatial structure of images, this filter relies on pixelsimilarities in both spatial and radiometric domains. To deal with polarimetricdata, we study the use of similarities based on a statistical distance calledKullback-Leibler divergence as well as two geodesic distances on Riemannianmanifolds. To cope with speckle, we propose to progressively refine the resultthanks to an iterative scheme. Experiments are run over synthetic andexperimental data. First, simulations are generated to study the effects offiltering parameters in terms of polarimetric reconstruction error, edgepreservation and smoothing of homogeneous areas. Comparison with other methodsshows that our approach compares well to other state of the art methods in theextraction of polarimetric information and shows superior performance for edgerestoration and noise smoothing. The filter is then applied to experimentaldata sets from ESAR and FSAR sensors (DLR) at L-band and S-band, respectively.These last experiments show the ability of the filter to restore structuressuch as buildings and roads and to preserve boundaries between regions whileachieving a high amount of smoothing in homogeneous areas.
arxiv-5100-274 | Reconstruction of Complex-Valued Fractional Brownian Motion Fields Based on Compressive Sampling and Its Application to PSF Interpolation in Weak Lensing Survey | http://arxiv.org/pdf/1311.0124v1.pdf | author:Andriyan B. Suksmono category:cs.CV astro-ph.CO published:2013-11-01 summary:A new reconstruction method of complex-valued fractional Brownian motion(CV-fBm) field based on Compressive Sampling (CS) is proposed. The decayproperty of Fourier coefficients magnitude of the fBm signals/ fields indicatesthat fBms are compressible. Therefore, a few numbers of samples will besufficient for a CS based method to reconstruct the full field. Theeffectiveness of the proposed method is showed by simulating, random sampling,and reconstructing CV-fBm fields. Performance evaluation shows advantages ofthe proposed method over boxcar filtering and thin plate methods. It is alsofound that the reconstruction performance depends on both of the fBm's Hurstparameter and the number of samples, which in fact is consistent with the CSreconstruction theory. In contrast to other fBm or fractal interpolationmethods, the proposed CS based method does not require the knowledge of fractalparameters in the reconstruction process; the inherent sparsity is justsufficient for the CS to do the reconstruction. Potential applicability of theproposed method in weak gravitational lensing survey, particularly forinterpolating non-smooth PSF (Point Spread Function) distribution representingdistortion by a turbulent field is also discussed.
arxiv-5100-275 | Joint Estimation of Multiple Graphical Models from High Dimensional Time Series | http://arxiv.org/pdf/1311.0219v2.pdf | author:Huitong Qiu, Fang Han, Han Liu, Brian Caffo category:stat.ML published:2013-11-01 summary:In this manuscript we consider the problem of jointly estimating multiplegraphical models in high dimensions. We assume that the data are collected fromn subjects, each of which consists of T possibly dependent observations. Thegraphical models of subjects vary, but are assumed to change smoothlycorresponding to a measure of closeness between subjects. We propose a kernelbased method for jointly estimating all graphical models. Theoretically, undera double asymptotic framework, where both (T,n) and the dimension d canincrease, we provide the explicit rate of convergence in parameter estimation.It characterizes the strength one can borrow across different individuals andimpact of data dependence on parameter estimation. Empirically, experiments onboth synthetic and real resting state functional magnetic resonance imaging(rs-fMRI) data illustrate the effectiveness of the proposed method.
arxiv-5100-276 | Structure-preserving color transformations using Laplacian commutativity | http://arxiv.org/pdf/1311.0119v1.pdf | author:Davide Eynard, Artiom Kovnatsky, Michael M. Bronstein category:cs.CV cs.GR math.SP published:2013-11-01 summary:Mappings between color spaces are ubiquitous in image processing problemssuch as gamut mapping, decolorization, and image optimization for color-blindpeople. Simple color transformations often result in information loss andambiguities (for example, when mapping from RGB to grayscale), and one wishesto find an image-specific transformation that would preserve as much aspossible the structure of the original image in the target color space. In thispaper, we propose Laplacian colormaps, a generic framework forstructure-preserving color transformations between images. We use the imageLaplacian to capture the structural information, and show that if the colortransformation between two images preserves the structure, the respectiveLaplacians have similar eigenvectors, or in other words, are approximatelyjointly diagonalizable. Employing the relation between joint diagonalizabilityand commutativity of matrices, we use Laplacians commutativity as a criterionof color mapping quality and minimize it w.r.t. the parameters of a colortransformation to achieve optimal structure preservation. We show numerousapplications of our approach, including color-to-gray conversion, gamutmapping, multispectral image fusion, and image optimization for color deficientviewers.
arxiv-5100-277 | Bayesian inference as iterated random functions with applications to sequential inference in graphical models | http://arxiv.org/pdf/1311.0072v1.pdf | author:Arash A. Amini, XuanLong Nguyen category:stat.ML math.ST stat.ME stat.TH published:2013-11-01 summary:We propose a general formalism of iterated random functions with semigroupproperty, under which exact and approximate Bayesian posterior updates can beviewed as specific instances. A convergence theory for iterated randomfunctions is presented. As an application of the general theory we analyzeconvergence behaviors of exact and approximate message-passing algorithms thatarise in a sequential change point detection problem formulated via a latentvariable directed graphical model. The sequential inference algorithm and itssupporting theory are illustrated by simulated examples.
arxiv-5100-278 | Online Learning with Multiple Operator-valued Kernels | http://arxiv.org/pdf/1311.0222v2.pdf | author:Julien Audiffren, Hachem Kadri category:cs.LG stat.ML published:2013-11-01 summary:We consider the problem of learning a vector-valued function f in an onlinelearning setting. The function f is assumed to lie in a reproducing Hilbertspace of operator-valued kernels. We describe two online algorithms forlearning f while taking into account the output structure. A first contributionis an algorithm, ONORMA, that extends the standard kernel-based online learningalgorithm NORMA from scalar-valued to operator-valued setting. We report acumulative error bound that holds both for classification and regression. Wethen define a second algorithm, MONORMA, which addresses the limitation ofpre-defining the output structure in ONORMA by learning sequentially a linearcombination of operator-valued kernels. Our experiments show that the proposedalgorithms achieve good performance results with low computational cost.
arxiv-5100-279 | Deep AutoRegressive Networks | http://arxiv.org/pdf/1310.8499v2.pdf | author:Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, Daan Wierstra category:cs.LG stat.ML published:2013-10-31 summary:We introduce a deep, generative autoencoder capable of learning hierarchiesof distributed representations from data. Successive deep stochastic hiddenlayers are equipped with autoregressive connections, which enable the model tobe sampled from quickly and exactly via ancestral sampling. We derive anefficient approximate parameter estimation method based on the minimumdescription length (MDL) principle, which can be seen as maximising avariational lower bound on the log-likelihood, with a feedforward neuralnetwork implementing approximate inference. We demonstrate state-of-the-artgenerative performance on a number of classic data sets: several UCI data sets,MNIST and Atari 2600 games.
arxiv-5100-280 | Multilabel Classification through Random Graph Ensembles | http://arxiv.org/pdf/1310.8428v2.pdf | author:Hongyu Su, Juho Rousu category:cs.LG published:2013-10-31 summary:We present new methods for multilabel classification, relying on ensemblelearning on a collection of random output graphs imposed on the multilabel anda kernel-based structured output learner as the base classifier. For ensemblelearning, differences among the output graphs provide the required baseclassifier diversity and lead to improved performance in the increasing size ofthe ensemble. We study different methods of forming the ensemble prediction,including majority voting and two methods that perform inferences over thegraph structures before or after combining the base models into the ensemble.We compare the methods against the state-of-the-art machine learning approacheson a set of heterogeneous multilabel benchmark problems, including multilabelAdaBoost, convex multitask feature learning, as well as single target learningapproaches represented by Bagging and SVM. In our experiments, the random graphensembles are very competitive and robust, ranking first or second on most ofthe datasets. Overall, our results show that random graph ensembles are viablealternatives to flat multilabel and multitask learners.
arxiv-5100-281 | Spatial statistics, image analysis and percolation theory | http://arxiv.org/pdf/1310.8574v1.pdf | author:Mikhail Langovoy, Michael Habeck, Bernhard SchÃ¶lkopf category:stat.AP stat.ML published:2013-10-31 summary:We develop a novel method for detection of signals and reconstruction ofimages in the presence of random noise. The method uses results frompercolation theory. We specifically address the problem of detection ofmultiple objects of unknown shapes in the case of nonparametric noise. Thenoise density is unknown and can be heavy-tailed. The objects of interest haveunknown varying intensities. No boundary shape constraints are imposed on theobjects, only a set of weak bulk conditions is required. We view the objectdetection problem as a multiple hypothesis testing for discrete statisticalinverse problems. We present an algorithm that allows to detect greyscaleobjects of various shapes in noisy images. We prove results on consistency andalgorithmic complexity of our procedures. Applications to cryo-electronmicroscopy are presented.
arxiv-5100-282 | A Preadapted Universal Switch Distribution for Testing Hilberg's Conjecture | http://arxiv.org/pdf/1310.8511v2.pdf | author:Åukasz DÄbowski category:cs.IT cs.CL math.IT 68P30, 94A45 E.4 published:2013-10-31 summary:Hilberg's conjecture about natural language states that the mutualinformation between two adjacent long blocks of text grows like a power of theblock length. The exponent in this statement can be upper bounded using thepointwise mutual information estimate computed for a carefully chosen code. Thebound is the better, the lower the compression rate is but there is arequirement that the code be universal. So as to improve a received upper boundfor Hilberg's exponent, in this paper, we introduce two novel universal codes,called the plain switch distribution and the preadapted switch distribution.Generally speaking, switch distributions are certain mixtures of adaptiveMarkov chains of varying orders with some additional communication to avoid socalled catch-up phenomenon. The advantage of these distributions is that theyboth achieve a low compression rate and are guaranteed to be universal. Usingthe switch distributions we obtain that a sample of a text in English isnon-Markovian with Hilberg's exponent being $\le 0.83$, which improves over theprevious bound $\le 0.94$ obtained using the Lempel-Ziv code.
arxiv-5100-283 | Reinforcement Learning Framework for Opportunistic Routing in WSNs | http://arxiv.org/pdf/1310.8467v1.pdf | author:G. Srinivas Rao, A. V. Ramana category:cs.NI cs.LG published:2013-10-31 summary:Routing packets opportunistically is an essential part of multihop ad hocwireless sensor networks. The existing routing techniques are not adaptiveopportunistic. In this paper we have proposed an adaptive opportunistic routingscheme that routes packets opportunistically in order to ensure that packetloss is avoided. Learning and routing are combined in the framework thatexplores the optimal routing possibilities. In this paper we implemented thisReinforced learning framework using a customer simulator. The experimentalresults revealed that the scheme is able to exploit the opportunistic tooptimize routing of packets even though the network structure is unknown.
arxiv-5100-284 | Convergence analysis of kernel LMS algorithm with pre-tuned dictionary | http://arxiv.org/pdf/1310.8618v1.pdf | author:Jie Chen, Wei Gao, CÃ©dric Richard, Jose-Carlos M. Bermudez category:stat.ML published:2013-10-31 summary:The kernel least-mean-square (KLMS) algorithm is an appealing tool for onlineidentification of nonlinear systems due to its simplicity and robustness. Inaddition to choosing a reproducing kernel and setting filter parameters,designing a KLMS adaptive filter requires to select a so-called dictionary inorder to get a finite-order model. This dictionary has a significant impact onperformance, and requires careful consideration. Theoretical analysis of KLMSas a function of dictionary setting has rarely, if ever, been addressed in theliterature. In an analysis previously published by the authors, the dictionaryelements were assumed to be governed by the same probability density functionof the input data. In this paper, we modify this study by considering thedictionary as part of the filter parameters to be set. This theoreticalanalysis paves the way for future investigations on KLMS dictionary design.
arxiv-5100-285 | Parameterless Optimal Approximate Message Passing | http://arxiv.org/pdf/1311.0035v1.pdf | author:Ali Mousavi, Arian Maleki, Richard G. Baraniuk category:cs.IT math.IT math.ST stat.ML stat.TH published:2013-10-31 summary:Iterative thresholding algorithms are well-suited for high-dimensionalproblems in sparse recovery and compressive sensing. The performance of thisclass of algorithms depends heavily on the tuning of certain thresholdparameters. In particular, both the final reconstruction error and theconvergence rate of the algorithm crucially rely on how the threshold parameteris set at each step of the algorithm. In this paper, we propose aparameter-free approximate message passing (AMP) algorithm that sets thethreshold parameter at each iteration in a fully automatic way without eitherhaving an information about the signal to be reconstructed or needing anytuning from the user. We show that the proposed method attains both the minimumreconstruction error and the highest convergence rate. Our method is based onapplying the Stein unbiased risk estimate (SURE) along with a modified gradientdescent to find the optimal threshold in each iteration. Motivated by theconnections between AMP and LASSO, it could be employed to find the solution ofthe LASSO for the optimal regularization parameter. To the best of ourknowledge, this is the first work concerning parameter tuning that obtains thefastest convergence rate with theoretical guarantees.
arxiv-5100-286 | An efficient distributed learning algorithm based on effective local functional approximations | http://arxiv.org/pdf/1310.8418v4.pdf | author:Dhruv Mahajan, Nikunj Agrawal, S. Sathiya Keerthi, S. Sundararajan, Leon Bottou category:cs.LG published:2013-10-31 summary:Scalable machine learning over big data is an important problem that isreceiving a lot of attention in recent years. On popular distributedenvironments such as Hadoop running on a cluster of commodity machines,communication costs are substantial and algorithms need to be designed suitablyconsidering those costs. In this paper we give a novel approach to thedistributed training of linear classifiers (involving smooth losses and L2regularization) that is designed to reduce the total communication costs. Ateach iteration, the nodes minimize locally formed approximate objectivefunctions; then the resulting minimizers are combined to form a descentdirection to move. Our approach gives a lot of freedom in the formation of theapproximate objective function as well as in the choice of methods to solvethem. The method is shown to have $O(log(1/\epsilon))$ time convergence. Themethod can be viewed as an iterative parameter mixing method. A specialinstantiation yields a parallel stochastic gradient descent method with strongconvergence. When communication times between nodes are large, our method ismuch faster than the Terascale method (Agarwal et al., 2011), which is a stateof the art distributed solver based on the statistical query model (Chuet al.,2006) that computes function and gradient values in a distributed fashion. Wealso evaluate against other recent distributed methods and demonstrate superiorperformance of our method.
arxiv-5100-287 | Robust Compressed Sensing and Sparse Coding with the Difference Map | http://arxiv.org/pdf/1311.0053v2.pdf | author:Will Landecker, Rick Chartrand, Simon DeDeo category:cs.CV stat.ML published:2013-10-31 summary:In compressed sensing, we wish to reconstruct a sparse signal $x$ fromobserved data $y$. In sparse coding, on the other hand, we wish to find arepresentation of an observed signal $y$ as a sparse linear combination, withcoefficients $x$, of elements from an overcomplete dictionary. While manyalgorithms are competitive at both problems when $x$ is very sparse, it can bechallenging to recover $x$ when it is less sparse. We present the DifferenceMap, which excels at sparse recovery when sparseness is lower and noise ishigher. The Difference Map out-performs the state of the art withreconstruction from random measurements and natural image reconstruction viasparse coding.
arxiv-5100-288 | Nonlinear unmixing of hyperspectral images using a semiparametric model and spatial regularization | http://arxiv.org/pdf/1310.8612v1.pdf | author:Jie Chen, CÃ©dric Richard, Alfred O. Hero III category:stat.ML published:2013-10-31 summary:Incorporating spatial information into hyperspectral unmixing procedures hasbeen shown to have positive effects, due to the inherent spatial-spectralduality in hyperspectral scenes. Current research works that consider spatialinformation are mainly focused on the linear mixing model. In this paper, weinvestigate a variational approach to incorporating spatial correlation into anonlinear unmixing procedure. A nonlinear algorithm operating in reproducingkernel Hilbert spaces, associated with an $\ell_1$ local variation norm as thespatial regularizer, is derived. Experimental results, with both synthetic andreal data, illustrate the effectiveness of the proposed scheme.
arxiv-5100-289 | Safe and Efficient Screening For Sparse Support Vector Machine | http://arxiv.org/pdf/1310.8320v1.pdf | author:Zheng Zhao, Jun Liu category:cs.LG stat.ML published:2013-10-30 summary:Screening is an effective technique for speeding up the training process of asparse learning model by removing the features that are guaranteed to beinactive the process. In this paper, we present a efficient screening techniquefor sparse support vector machine based on variational inequality. Thetechnique is both efficient and safe.
arxiv-5100-290 | Para-active learning | http://arxiv.org/pdf/1310.8243v1.pdf | author:Alekh Agarwal, Leon Bottou, Miroslav Dudik, John Langford category:cs.LG stat.ML published:2013-10-30 summary:Training examples are not all equally informative. Active learning strategiesleverage this observation in order to massively reduce the number of examplesthat need to be labeled. We leverage the same observation to build a genericstrategy for parallelizing learning algorithms. This strategy is effectivebecause the search for informative examples is highly parallelizable andbecause we show that its performance does not deteriorate when the siftingprocess relies on a slightly outdated model. Parallel active learning isparticularly attractive to train nonlinear models with non-linearrepresentations because there are few practical parallel learning algorithmsfor such models. We report preliminary experiments using both kernel SVMs andSGD-trained neural networks.
arxiv-5100-291 | A U-statistic estimator for the variance of resampling-based error estimators | http://arxiv.org/pdf/1310.8203v2.pdf | author:Mathias Fuchs, Roman Hornung, Riccardo De Bin, Anne-Laure Boulesteix category:math.ST stat.ML stat.TH published:2013-10-30 summary:We revisit resampling procedures for error estimation in binaryclassification in terms of U-statistics. In particular, we exploit the factthat the error rate estimator involving all learning-testing splits is aU-statistic. Thus, it has minimal variance among all unbiased estimators and isasymptotically normally distributed. Moreover, there is an unbiased estimatorfor this minimal variance if the total sample size is at least the doublelearning set size plus two. In this case, we exhibit such an estimator which isanother U-statistic. It enjoys, again, various optimality properties and yieldsan asymptotically exact hypothesis test of the equality of error rates when twolearning algorithms are compared. Our statements apply to any deterministiclearning algorithms under weak non-degeneracy assumptions.
arxiv-5100-292 | Tracking Deformable Parts via Dynamic Conditional Random Fields | http://arxiv.org/pdf/1311.0262v1.pdf | author:Suofei Zhang, Zhixin Sun, Xu Cheng, Zhenyang Wu category:cs.CV cs.MM published:2013-10-30 summary:Despite the success of many advanced tracking methods in this area, trackingtargets with drastic variation of appearance such as deformation, view changeand partial occlusion in video sequences is still a challenge in practicalapplications. In this letter, we take these serious tracking problems intoaccount simultaneously, proposing a dynamic graph based model to track objectand its deformable parts at multiple resolutions. The method introduces welllearned structural object detection models into object tracking applications asprior knowledge to deal with deformation and view change. Meanwhile, itexplicitly formulates partial occlusion by integrating spatial potentials andtemporal potentials with an unparameterized occlusion handling mechanism in thedynamic conditional random field framework. Empirical results demonstrate thatthe method outperforms state-of-the-art trackers on different challenging videosequences.
arxiv-5100-293 | Learning Sparsely Used Overcomplete Dictionaries via Alternating Minimization | http://arxiv.org/pdf/1310.7991v2.pdf | author:Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth Netrapalli category:cs.LG math.OC stat.ML published:2013-10-30 summary:We consider the problem of sparse coding, where each sample consists of asparse linear combination of a set of dictionary atoms, and the task is tolearn both the dictionary elements and the mixing coefficients. Alternatingminimization is a popular heuristic for sparse coding, where the dictionary andthe coefficients are estimated in alternate steps, keeping the other fixed.Typically, the coefficients are estimated via $\ell_1$ minimization, keepingthe dictionary fixed, and the dictionary is estimated through least squares,keeping the coefficients fixed. In this paper, we establish local linearconvergence for this variant of alternating minimization and establish that thebasin of attraction for the global optimum (corresponding to the truedictionary and the coefficients) is $\order{1/s^2}$, where $s$ is the sparsitylevel in each sample and the dictionary satisfies RIP. Combined with the recentresults of approximate dictionary estimation, this yields provable guaranteesfor exact recovery of both the dictionary elements and the coefficients, whenthe dictionary elements are incoherent.
arxiv-5100-294 | Necessary and Sufficient Conditions for Novel Word Detection in Separable Topic Models | http://arxiv.org/pdf/1310.7994v1.pdf | author:Weicong Ding, Prakash Ishwar, Mohammad H. Rohban, Venkatesh Saligrama category:cs.LG cs.IR stat.ML published:2013-10-30 summary:The simplicial condition and other stronger conditions that imply it haverecently played a central role in developing polynomial time algorithms withprovable asymptotic consistency and sample complexity guarantees for topicestimation in separable topic models. Of these algorithms, those that relysolely on the simplicial condition are impractical while the practical onesneed stronger conditions. In this paper, we demonstrate, for the first time,that the simplicial condition is a fundamental, algorithm-independent,information-theoretic necessary condition for consistent separable topicestimation. Furthermore, under solely the simplicial condition, we present apractical quadratic-complexity algorithm based on random projections whichconsistently detects all novel words of all topics using only up tosecond-order empirical word moments. This algorithm is amenable to distributedimplementation making it attractive for 'big-data' scenarios involving anetwork of large distributed databases.
arxiv-5100-295 | Description and Evaluation of Semantic Similarity Measures Approaches | http://arxiv.org/pdf/1310.8059v1.pdf | author:Thabet Slimani category:cs.CL published:2013-10-30 summary:In recent years, semantic similarity measure has a great interest in SemanticWeb and Natural Language Processing (NLP). Several similarity measures havebeen developed, being given the existence of a structured knowledgerepresentation offered by ontologies and corpus which enable semanticinterpretation of terms. Semantic similarity measures compute the similaritybetween concepts/terms included in knowledge sources in order to performestimations. This paper discusses the existing semantic similarity methodsbased on structure, information content and feature approaches. Additionally,we present a critical evaluation of several categories of semantic similarityapproaches based on two standard benchmarks. The aim of this paper is to givean efficient evaluation of all these measures which help researcher andpractitioners to select the measure that best fit for their requirements.
arxiv-5100-296 | Online Ensemble Learning for Imbalanced Data Streams | http://arxiv.org/pdf/1310.8004v1.pdf | author:Boyu Wang, Joelle Pineau category:cs.LG stat.ML published:2013-10-30 summary:While both cost-sensitive learning and online learning have been studiedextensively, the effort in simultaneously dealing with these two issues islimited. Aiming at this challenge task, a novel learning framework is proposedin this paper. The key idea is based on the fusion of online ensemblealgorithms and the state of the art batch mode cost-sensitive bagging/boostingalgorithms. Within this framework, two separately developed research areas arebridged together, and a batch of theoretically sound online cost-sensitivebagging and online cost-sensitive boosting algorithms are first proposed.Unlike other online cost-sensitive learning algorithms lacking theoreticalanalysis of asymptotic properties, the convergence of the proposed algorithmsis guaranteed under certain conditions, and the experimental evidence withbenchmark data sets also validates the effectiveness and efficiency of theproposed methods.
arxiv-5100-297 | Wavelet and Fast Fourier Transform based analysis of Solar Image | http://arxiv.org/pdf/1311.6799v2.pdf | author:Sabyasachi Mukhopadhyay, Debadatta Dash, Swapnil Barmase, Prasanta K Panigrahi category:cs.CV cs.CE published:2013-10-30 summary:Both of Wavelet and Fast Fourier Transform are strong signal processing toolsin the field of Data Analysis. In this paper fast fourier transform (FFT) andWavelet Transform are employed to observe some important features of Solarimage (December, 2004). We have tried to find out the periodicity and coherenceof different sections of the solar image. We plotted the distribution of energyin solar surface by analyzing the solar image with scalograms and3D-coefficient plots.
arxiv-5100-298 | The Information Geometry of Mirror Descent | http://arxiv.org/pdf/1310.7780v2.pdf | author:Garvesh Raskutti, Sayan Mukherjee category:stat.ML cs.LG published:2013-10-29 summary:Information geometry applies concepts in differential geometry to probabilityand statistics and is especially useful for parameter estimation in exponentialfamilies where parameters are known to lie on a Riemannian manifold.Connections between the geometric properties of the induced manifold andstatistical properties of the estimation problem are well-established. Howeverdeveloping first-order methods that scale to larger problems has been less of afocus in the information geometry community. The best known algorithm thatincorporates manifold structure is the second-order natural gradient descentalgorithm introduced by Amari. On the other hand, stochastic approximationmethods have led to the development of first-order methods for optimizing noisyobjective functions. A recent generalization of the Robbins-Monro algorithmknown as mirror descent, developed by Nemirovski and Yudin is a first ordermethod that induces non-Euclidean geometries. However current analysis ofmirror descent does not precisely characterize the induced non-Euclideangeometry nor does it consider performance in terms of statistical relativeefficiency. In this paper, we prove that mirror descent induced by Bregmandivergences is equivalent to the natural gradient descent algorithm on the dualRiemannian manifold. Using this equivalence, it follows that (1) mirror descentis the steepest descent direction along the Riemannian manifold of theexponential family; (2) mirror descent with log-likelihood loss applied toparameter estimation in exponential families asymptotically achieves theclassical Cram\'er-Rao lower bound and (3) natural gradient descent formanifolds corresponding to exponential families can be implemented as afirst-order method through mirror descent.
arxiv-5100-299 | Structured Optimal Transmission Control in Network-coded Two-way Relay Channels | http://arxiv.org/pdf/1310.7679v1.pdf | author:Ni Ding, Parastoo Sadeghi, Rodney A. Kennedy category:cs.SY stat.ML published:2013-10-29 summary:This paper considers a transmission control problem in network-coded two-wayrelay channels (NC-TWRC), where the relay buffers random symbol arrivals fromtwo users, and the channels are assumed to be fading. The problem is modeled bya discounted infinite horizon Markov decision process (MDP). The objective isto find a transmission control policy that minimizes the symbol delay, bufferoverflow and transmission power consumption and error rate simultaneously andin the long run. By using the concepts of submodularity, multimodularity andL-natural convexity, we study the structure of the optimal policy searched bydynamic programming (DP) algorithm. We show that the optimal transmissionpolicy is nondecreasing in queue occupancies or/and channel states undercertain conditions such as the chosen values of parameters in the MDP model,channel modeling method, modulation scheme and the preservation of stochasticdominance in the transitions of system states. The results derived in thispaper can be used to relieve the high complexity of DP and facilitate real-timecontrol.
arxiv-5100-300 | Automatic Classification of Variable Stars in Catalogs with missing data | http://arxiv.org/pdf/1310.7868v1.pdf | author:Karim Pichara, Pavlos Protopapas category:astro-ph.IM cs.LG stat.ML published:2013-10-29 summary:We present an automatic classification method for astronomical catalogs withmissing data. We use Bayesian networks, a probabilistic graphical model, thatallows us to perform inference to pre- dict missing values given observed dataand dependency relationships between variables. To learn a Bayesian networkfrom incomplete data, we use an iterative algorithm that utilises samplingmethods and expectation maximization to estimate the distributions andprobabilistic dependencies of variables from data with missing values. To testour model we use three catalogs with missing data (SAGE, 2MASS and UBVI) andone complete catalog (MACHO). We examine how classification accuracy changeswhen information from missing data catalogs is included, how our methodcompares to traditional missing data approaches and at what computational cost.Integrating these catalogs with missing data we find that classification ofvariable objects improves by few percent and by 15% for quasar detection whilekeeping the computational cost the same.
