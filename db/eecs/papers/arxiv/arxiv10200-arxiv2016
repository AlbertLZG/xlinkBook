arxiv-1502-06922 | Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval |  http://arxiv.org/abs/1502.06922  | author:Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying Song, Rabab Ward category:cs.CL cs.IR cs.LG cs.NE published:2015-02-24 summary:This paper develops a model that addresses sentence embedding, a hot topic incurrent natural language processing research, using recurrent neural networkswith Long Short-Term Memory (LSTM) cells. Due to its ability to capture longterm memory, the LSTM-RNN accumulates increasingly richer information as itgoes through the sentence, and when it reaches the last word, the hidden layerof the network provides a semantic representation of the whole sentence. Inthis paper, the LSTM-RNN is trained in a weakly supervised manner on userclick-through data logged by a commercial web search engine. Visualization andanalysis are performed to understand how the embedding process works. The modelis found to automatically attenuate the unimportant words and detects thesalient keywords in the sentence. Furthermore, these detected keywords arefound to automatically activate different cells of the LSTM-RNN, where wordsbelonging to a similar topic activate the same cell. As a semanticrepresentation of the sentence, the embedding vector can be used in manydifferent applications. These automatic keyword detection and topic allocationabilities enabled by the LSTM-RNN allow the network to perform documentretrieval, a difficult language processing task, where the similarity betweenthe query and documents can be measured by the distance between theircorresponding sentence embedding vectors computed by the LSTM-RNN. On a websearch task, the LSTM-RNN embedding is shown to significantly outperformseveral existing state of the art methods. We emphasize that the proposed modelgenerates sentence embedding vectors that are specially useful for web documentretrieval tasks. A comparison with a well known general sentence embeddingmethod, the Paragraph Vector, is performed. The results show that the proposedmethod in this paper significantly outperforms it for web document retrievaltask.
arxiv-1502-06807 | Hands Deep in Deep Learning for Hand Pose Estimation |  http://arxiv.org/abs/1502.06807  | author:Markus Oberweger, Paul Wohlhart, Vincent Lepetit category:cs.CV published:2015-02-24 summary:We introduce and evaluate several architectures for Convolutional NeuralNetworks to predict the 3D joint locations of a hand given a depth map. Wefirst show that a prior on the 3D pose can be easily introduced andsignificantly improves the accuracy and reliability of the predictions. We alsoshow how to use context efficiently to deal with ambiguities between fingers.These two contributions allow us to significantly outperform thestate-of-the-art on several challenging benchmarks, both in terms of accuracyand computation times.
arxiv-1502-06895 | On the consistency theory of high dimensional variable screening |  http://arxiv.org/abs/1502.06895  | author:Xiangyu Wang, Chenlei Leng, David B. Dunson category:math.ST cs.LG stat.ML stat.TH published:2015-02-24 summary:Variable screening is a fast dimension reduction technique for assisting highdimensional feature selection. As a preselection method, it selects a moderatesize subset of candidate variables for further refining via feature selectionto produce the final model. The performance of variable screening depends onboth computational efficiency and the ability to dramatically reduce the numberof variables without discarding the important ones. When the data dimension $p$is substantially larger than the sample size $n$, variable screening becomescrucial as 1) Faster feature selection algorithms are needed; 2) Conditionsguaranteeing selection consistency might fail to hold. This article studies aclass of linear screening methods and establishes consistency theory for thisspecial class. In particular, we prove the restricted diagonally dominant (RDD)condition is a necessary and sufficient condition for strong screeningconsistency. As concrete examples, we show two screening methods $SIS$ and$HOLP$ are both strong screening consistent (subject to additional constraints)with large probability if $n > O((\rho s + \sigma/\tau)^2\log p)$ under randomdesigns. In addition, we relate the RDD condition to the irrepresentablecondition, and highlight limitations of $SIS$.
arxiv-1505-05819 | New HSL Distance Based Colour Clustering Algorithm |  http://arxiv.org/abs/1505.05819  | author:Vasile Patrascu category:cs.CV published:2015-02-24 summary:In this paper, we define a distance for the HSL colour system. Next, theproposed distance is used for a fuzzy colour clustering algorithm construction.The presented algorithm is related to the well-known fuzzy c-means algorithm.Finally, the clustering algorithm is used as colour reduction method. Theobtained experimental results are presented to demonstrate the effectiveness ofour approach.
arxiv-1503-00587 | Personalising Mobile Advertising Based on Users Installed Apps |  http://arxiv.org/abs/1503.00587  | author:Jenna Reps, Uwe Aickelin, Jonathan Garibaldi, Chris Damski category:cs.CY cs.LG published:2015-02-24 summary:Mobile advertising is a billion pound industry that is rapidly expanding. Thesuccess of an advert is measured based on how users interact with it. In thispaper we investigate whether the application of unsupervised learning andassociation rule mining could be used to enable personalised targeting ofmobile adverts with the aim of increasing the interaction rate. Over May andJune 2014 we recorded advert interactions such as tapping the advert orwatching the whole advert video along with the set of apps a user has installedat the time of the interaction. Based on the apps that the users have installedwe applied k-means clustering to profile the users into one of ten classes. Dueto the large number of apps considered we implemented dimension reduction toreduced the app feature space by mapping the apps to their iTunes category andclustered users based on the percentage of their apps that correspond to eachiTunes app category. The clustering was externally validated by investigatingdifferences between the way the ten profiles interact with the various advertsgenres (lifestyle, finance and entertainment adverts). In addition associationrule mining was performed to find whether the time of the day that the advertis served and the number of apps a user has installed makes certain profilesmore likely to interact with the advert genres. The results showed there wereclear differences in the way the profiles interact with the different advertgenres and the results of this paper suggest that mobile advert targeting wouldimprove the frequency that users interact with an advert.
arxiv-1502-06796 | Online Tracking by Learning Discriminative Saliency Map with Convolutional Neural Network |  http://arxiv.org/abs/1502.06796  | author:Seunghoon Hong, Tackgeun You, Suha Kwak, Bohyung Han category:cs.CV published:2015-02-24 summary:We propose an online visual tracking algorithm by learning discriminativesaliency map using Convolutional Neural Network (CNN). Given a CNN pre-trainedon a large-scale image repository in offline, our algorithm takes outputs fromhidden layers of the network as feature descriptors since they show excellentrepresentation performance in various general visual recognition problems. Thefeatures are used to learn discriminative target appearance models using anonline Support Vector Machine (SVM). In addition, we construct target-specificsaliency map by backpropagating CNN features with guidance of the SVM, andobtain the final tracking result in each frame based on the appearance modelgeneratively constructed with the saliency map. Since the saliency mapvisualizes spatial configuration of target effectively, it improves targetlocalization accuracy and enable us to achieve pixel-level target segmentation.We verify the effectiveness of our tracking algorithm through extensiveexperiment on a challenging benchmark, where our method illustrates outstandingperformance compared to the state-of-the-art tracking algorithms.
arxiv-1502-06665 | Reified Context Models |  http://arxiv.org/abs/1502.06665  | author:Jacob Steinhardt, Percy Liang category:cs.LG published:2015-02-24 summary:A classic tension exists between exact inference in a simple model andapproximate inference in a complex model. The latter offers expressivity andthus accuracy, but the former provides coverage of the space, an importantproperty for confidence estimation and learning with indirect supervision. Inthis work, we introduce a new approach, reified context models, to reconcilethis tension. Specifically, we let the amount of context (the arity of thefactors in a graphical model) be chosen "at run-time" by reifying it---that is,letting this choice itself be a random variable inside the model. Empirically,we show that our approach obtains expressivity and coverage on three naturallanguage tasks.
arxiv-1502-06800 | On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions |  http://arxiv.org/abs/1502.06800  | author:Francis Bach category:cs.LG math.NA stat.ML published:2015-02-24 summary:We show that kernel-based quadrature rules for computing integrals can beseen as a special case of random feature expansions for positive definitekernels, for a particular decomposition that always exists for such kernels. Weprovide a theoretical analysis of the number of required samples for a givenapproximation error, leading to both upper and lower bounds that are basedsolely on the eigenvalues of the associated integral operator and match up tologarithmic terms. In particular, we show that the upper bound may be obtainedfrom independent and identically distributed samples from a specificnon-uniform distribution, while the lower bound if valid for any set of points.Applying our results to kernel-based quadrature, while our results are fairlygeneral, we recover known upper and lower bounds for the special cases ofSobolev spaces. Moreover, our results extend to the more general problem offull function approximations (beyond simply computing an integral), withresults in L2- and L$\infty$-norm that match known results for special cases.Applying our results to random features, we show an improvement of the numberof random features needed to preserve the generalization guarantees forlearning with Lipschitz-continuous losses.
arxiv-1502-06811 | A Review of Audio Features and Statistical Models Exploited for Voice Pattern Design |  http://arxiv.org/abs/1502.06811  | author:Ngoc Q. K. Duong, Hien-Thanh Duong category:cs.SD stat.ML published:2015-02-24 summary:Audio fingerprinting, also named as audio hashing, has been well-known as apowerful technique to perform audio identification and synchronization. Itbasically involves two major steps: fingerprint (voice pattern) design andmatching search. While the first step concerns the derivation of a robust andcompact audio signature, the second step usually requires knowledge aboutdatabase and quick-search algorithms. Though this technique offers a wide rangeof real-world applications, to the best of the authors' knowledge, acomprehensive survey of existing algorithms appeared more than eight years ago.Thus, in this paper, we present a more up-to-date review and, for emphasizingon the audio signal processing aspect, we focus our state-of-the-art survey onthe fingerprint design step for which various audio features and theirtractable statistical models are discussed.
arxiv-1502-06648 | Recognizing Fine-Grained and Composite Activities using Hand-Centric Features and Script Data |  http://arxiv.org/abs/1502.06648  | author:Marcus Rohrbach, Anna Rohrbach, Michaela Regneri, Sikandar Amin, Mykhaylo Andriluka, Manfred Pinkal, Bernt Schiele category:cs.CV published:2015-02-23 summary:Activity recognition has shown impressive progress in recent years. However,the challenges of detecting fine-grained activities and understanding how theyare combined into composite activities have been largely overlooked. In thiswork we approach both tasks and present a dataset which provides detailedannotations to address them. The first challenge is to detect fine-grainedactivities, which are defined by low inter-class variability and are typicallycharacterized by fine-grained body motions. We explore how human pose and handscan help to approach this challenge by comparing two pose-based and twohand-centric features with state-of-the-art holistic features. To attack thesecond challenge, recognizing composite activities, we leverage the fact thatthese activities are compositional and that the essential components of theactivities can be obtained from textual descriptions or scripts. We show thebenefits of our hand-centric approach for fine-grained activity classificationand detection. For composite activity recognition we find that decompositioninto attributes allows sharing information across composites and is essentialto attack this hard task. Using script data we can recognize novel compositeswithout having training data for them.
arxiv-1502-06590 | Improved Sum-of-Squares Lower Bounds for Hidden Clique and Hidden Submatrix Problems |  http://arxiv.org/abs/1502.06590  | author:Yash Deshpande, Andrea Montanari category:cs.CC cs.IT math.IT math.ST stat.ML stat.TH published:2015-02-23 summary:Given a large data matrix $A\in\mathbb{R}^{n\times n}$, we consider theproblem of determining whether its entries are i.i.d. with some known marginaldistribution $A_{ij}\sim P_0$, or instead $A$ contains a principal submatrix$A_{{\sf Q},{\sf Q}}$ whose entries have marginal distribution $A_{ij}\simP_1\neq P_0$. As a special case, the hidden (or planted) clique problemrequires to find a planted clique in an otherwise uniformly random graph. Assuming unbounded computational resources, this hypothesis testing problemis statistically solvable provided ${\sf Q}\ge C \log n$ for a suitableconstant $C$. However, despite substantial effort, no polynomial time algorithmis known that succeeds with high probability when ${\sf Q} = o(\sqrt{n})$.Recently Meka and Wigderson \cite{meka2013association}, proposed a method toestablish lower bounds within the Sum of Squares (SOS) semidefinite hierarchy. Here we consider the degree-$4$ SOS relaxation, and study the construction of\cite{meka2013association} to prove that SOS fails unless $k\ge C\,n^{1/3}/\log n$. An argument presented by Barak implies that this lower boundcannot be substantially improved unless the witness construction is changed inthe proof. Our proof uses the moments method to bound the spectrum of a certainrandom association scheme, i.e. a symmetric random matrix whose rows andcolumns are indexed by the edges of an Erd\"os-Renyi random graph.
arxiv-1502-06309 | Learning with Differential Privacy: Stability, Learnability and the Sufficiency and Necessity of ERM Principle |  http://arxiv.org/abs/1502.06309  | author:Yu-Xiang Wang, Jing Lei, Stephen E. Fienberg category:stat.ML cs.CR cs.LG published:2015-02-23 summary:While machine learning has proven to be a powerful data-driven solution tomany real-life problems, its use in sensitive domains has been limited due toprivacy concerns. A popular approach known as **differential privacy** offersprovable privacy guarantees, but it is often observed in practice that it couldsubstantially hamper learning accuracy. In this paper we study the learnability(whether a problem can be learned by any algorithm) under Vapnik's generallearning setting with differential privacy constraint, and reveal someintricate relationships between privacy, stability and learnability. In particular, we show that a problem is privately learnable **if an onlyif** there is a private algorithm that asymptotically minimizes the empiricalrisk (AERM). In contrast, for non-private learning AERM alone is not sufficientfor learnability. This result suggests that when searching for private learningalgorithms, we can restrict the search to algorithms that are AERM. In light ofthis, we propose a conceptual procedure that always finds a universallyconsistent algorithm whenever the problem is learnable under privacyconstraint. We also propose a generic and practical algorithm and show thatunder very general conditions it privately learns a wide class of learningproblems. Lastly, we extend some of the results to the more practical$(\epsilon,\delta)$-differential privacy and establish the existence of aphase-transition on the class of problems that are approximately privatelylearnable with respect to how small $\delta$ needs to be.
arxiv-1502-06362 | Contextual Dueling Bandits |  http://arxiv.org/abs/1502.06362  | author:Miroslav Dudík, Katja Hofmann, Robert E. Schapire, Aleksandrs Slivkins, Masrour Zoghi category:cs.LG published:2015-02-23 summary:We consider the problem of learning to choose actions using contextualinformation when provided with limited feedback in the form of relativepairwise comparisons. We study this problem in the dueling-bandits framework ofYue et al. (2009), which we extend to incorporate context. Roughly, thelearner's goal is to find the best policy, or way of behaving, in some space ofpolicies, although "best" is not always so clearly defined. Here, we propose anew and natural solution concept, rooted in game theory, called a von Neumannwinner, a randomized policy that beats or ties every other policy. We show thatthis notion overcomes important limitations of existing solutions, particularlythe Condorcet winner which has typically been used in the past, but whichrequires strong and often unrealistic assumptions. We then present threeefficient algorithms for online learning in our setting, and for approximatinga von Neumann winner from batch-like data. The first of these algorithmsachieves particularly low regret, even when data is adversarial, although itstime and space requirements are linear in the size of the policy space. Theother two algorithms require time and space only logarithmic in the size of thepolicy space when provided access to an oracle for solving classificationproblems on the space.
arxiv-1502-06344 | Convolutional Patch Networks with Spatial Prior for Road Detection and Urban Scene Understanding |  http://arxiv.org/abs/1502.06344  | author:Clemens-Alexander Brust, Sven Sickert, Marcel Simon, Erik Rodner, Joachim Denzler category:cs.CV published:2015-02-23 summary:Classifying single image patches is important in many different applications,such as road detection or scene understanding. In this paper, we presentconvolutional patch networks, which are convolutional networks learned todistinguish different image patches and which can be used for pixel-wiselabeling. We also show how to incorporate spatial information of the patch asan input to the network, which allows for learning spatial priors for certaincategories jointly with an appearance model. In particular, we focus on roaddetection and urban scene understanding, two application areas where we areable to achieve state-of-the-art results on the KITTI as well as on theLabelMeFacade dataset. Furthermore, our paper offers a guideline for people working in the area anddesperately wandering through all the painstaking details that render trainingCNs on image patches extremely difficult.
arxiv-1502-06531 | Scalable Variational Inference in Log-supermodular Models |  http://arxiv.org/abs/1502.06531  | author:Josip Djolonga, Andreas Krause category:cs.LG stat.ML published:2015-02-23 summary:We consider the problem of approximate Bayesian inference in log-supermodularmodels. These models encompass regular pairwise MRFs with binary variables, butallow to capture high-order interactions, which are intractable for existingapproximate inference techniques such as belief propagation, mean field, andvariants. We show that a recently proposed variational approach to inference inlog-supermodular models -L-FIELD- reduces to the widely-studied minimum normproblem for submodular minimization. This insight allows to leverage powerfulexisting tools, and hence to solve the variational problem orders of magnitudemore efficiently than previously possible. We then provide another naturalinterpretation of L-FIELD, demonstrating that it exactly minimizes a specifictype of R\'enyi divergence measure. This insight sheds light on the nature ofthe variational approximations produced by L-FIELD. Furthermore, we show how toperform parallel inference as message passing in a suitable factor graph at alinear convergence rate, without having to sum up over all the configurationsof the factor. Finally, we apply our approach to a challenging imagesegmentation task. Our experiments confirm scalability of our approach, highquality of the marginals, and the benefit of incorporating higher-orderpotentials.
arxiv-1502-06626 | Optimal Sparse Linear Auto-Encoders and Sparse PCA |  http://arxiv.org/abs/1502.06626  | author:Malik Magdon-Ismail, Christos Boutsidis category:cs.LG cs.AI cs.IT math.IT stat.CO stat.ML published:2015-02-23 summary:Principal components analysis (PCA) is the optimal linear auto-encoder ofdata, and it is often used to construct features. Enforcing sparsity on theprincipal components can promote better generalization, while improving theinterpretability of the features. We study the problem of constructing optimalsparse linear auto-encoders. Two natural questions in such a setting are: i)Given a level of sparsity, what is the best approximation to PCA that can beachieved? ii) Are there low-order polynomial-time algorithms which canasymptotically achieve this optimal tradeoff between the sparsity and theapproximation quality? In this work, we answer both questions by giving efficient low-orderpolynomial-time algorithms for constructing asymptotically \emph{optimal}linear auto-encoders (in particular, sparse features with near-PCAreconstruction error) and demonstrate the performance of our algorithms on realdata.
arxiv-1502-06354 | First-order regret bounds for combinatorial semi-bandits |  http://arxiv.org/abs/1502.06354  | author:Gergely Neu category:cs.LG stat.ML published:2015-02-23 summary:We consider the problem of online combinatorial optimization undersemi-bandit feedback, where a learner has to repeatedly pick actions from acombinatorial decision set in order to minimize the total losses associatedwith its decisions. After making each decision, the learner observes the lossesassociated with its action, but not other losses. For this problem, there areseveral learning algorithms that guarantee that the learner's expected regretgrows as $\widetilde{O}(\sqrt{T})$ with the number of rounds $T$. In thispaper, we propose an algorithm that improves this scaling to$\widetilde{O}(\sqrt{{L_T^*}})$, where $L_T^*$ is the total loss of the bestaction. Our algorithm is among the first to achieve such guarantees in apartial-feedback scheme, and the first one to do so in a combinatorial setting.
arxiv-1502-06470 | Approximate Message Passing with Restricted Boltzmann Machine Priors |  http://arxiv.org/abs/1502.06470  | author:Eric W. Tramel, Angélique Drémeau, Florent Krzakala category:cs.IT math.IT stat.ML published:2015-02-23 summary:Approximate Message Passing (AMP) has been shown to be an excellentstatistical approach to signal inference and compressed sensing problem. TheAMP framework provides modularity in the choice of signal prior; here wepropose a hierarchical form of the Gauss-Bernouilli prior which utilizes aRestricted Boltzmann Machine (RBM) trained on the signal support to pushreconstruction performance beyond that of simple iid priors for signals whosesupport can be well represented by a trained binary RBM. We present and analyzetwo methods of RBM factorization and demonstrate how these affect signalreconstruction performance within our proposed algorithm. Finally, using theMNIST handwritten digit dataset, we show experimentally that using an RBMallows AMP to approach oracle-support performance.
arxiv-1502-06464 | Rectified Factor Networks |  http://arxiv.org/abs/1502.06464  | author:Djork-Arné Clevert, Andreas Mayr, Thomas Unterthiner, Sepp Hochreiter category:cs.LG cs.CV cs.NE stat.ML published:2015-02-23 summary:We propose rectified factor networks (RFNs) to efficiently construct verysparse, non-linear, high-dimensional representations of the input. RFN modelsidentify rare and small events in the input, have a low interference betweencode units, have a small reconstruction error, and explain the data covariancestructure. RFN learning is a generalized alternating minimization algorithmderived from the posterior regularization method which enforces non-negativeand normalized posterior means. We proof convergence and correctness of the RFNlearning algorithm. On benchmarks, RFNs are compared to other unsupervisedmethods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast toprevious sparse coding methods, RFNs yield sparser codes, capture the data'scovariance structure more precisely, and have a significantly smallerreconstruction error. We test RFNs as pretraining technique for deep networkson different vision datasets, where RFNs were superior to RBMs andautoencoders. On gene expression data from two pharmaceutical drug discoverystudies, RFNs detected small and rare gene modules that revealed highlyrelevant new biological insights which were so far missed by other unsupervisedmethods.
arxiv-1502-06398 | Bandit Convex Optimization: sqrt{T} Regret in One Dimension |  http://arxiv.org/abs/1502.06398  | author:Sébastien Bubeck, Ofer Dekel, Tomer Koren, Yuval Peres category:cs.LG math.OC published:2015-02-23 summary:We analyze the minimax regret of the adversarial bandit convex optimizationproblem. Focusing on the one-dimensional case, we prove that the minimax regretis $\widetilde\Theta(\sqrt{T})$ and partially resolve a decade-old openproblem. Our analysis is non-constructive, as we do not present a concretealgorithm that attains this regret rate. Instead, we use minimax duality toreduce the problem to a Bayesian setting, where the convex loss functions aredrawn from a worst-case distribution, and then we solve the Bayesian version ofthe problem with a variant of Thompson Sampling. Our analysis features a noveluse of convexity, formalized as a "local-to-global" property of convexfunctions, that may be of independent interest.
arxiv-1502-06644 | On The Identifiability of Mixture Models from Grouped Samples |  http://arxiv.org/abs/1502.06644  | author:Robert A. Vandermeulen, Clayton D. Scott category:stat.ML cs.LG math.ST stat.TH published:2015-02-23 summary:Finite mixture models are statistical models which appear in many problems instatistics and machine learning. In such models it is assumed that data aredrawn from random probability measures, called mixture components, which arethemselves drawn from a probability measure P over probability measures. Whenestimating mixture models, it is common to make assumptions on the mixturecomponents, such as parametric assumptions. In this paper, we make noassumption on the mixture components, and instead assume that observations fromthe mixture model are grouped, such that observations in the same group areknown to be drawn from the same component. We show that any mixture of mprobability measures can be uniquely identified provided there are 2m-1observations per group. Moreover we show that, for any m, there exists amixture of m probability measures that cannot be uniquely identified whengroups have 2m-2 observations. Our results hold for any sample space with morethan one element.
arxiv-1502-06557 | Iteratively reweighted adaptive lasso for conditional heteroscedastic time series with applications to AR-ARCH type processes |  http://arxiv.org/abs/1502.06557  | author:Florian Ziel category:stat.ME q-fin.CP stat.AP stat.CO stat.ML published:2015-02-23 summary:Shrinkage algorithms are of great importance in almost every area ofstatistics due to the increasing impact of big data. Especially time seriesanalysis benefits from efficient and rapid estimation techniques such as thelasso. However, currently lasso type estimators for autoregressive time seriesmodels still focus on models with homoscedastic residuals. Therefore, aniteratively reweighted adaptive lasso algorithm for the estimation of timeseries models under conditional heteroscedasticity is presented in ahigh-dimensional setting. The asymptotic behaviour of the resulting estimatoris analysed. It is found that the proposed estimation procedure performssubstantially better than its homoscedastic counterpart. A special case of thealgorithm is suitable to compute the estimated multivariate AR-ARCH type modelsefficiently. Extensions to the model like periodic AR-ARCH, threshold AR-ARCHor ARMA-GARCH are discussed. Finally, different simulation results andapplications to electricity market data and returns of metal prices are shown.
arxiv-1502-06556 | Shannon, Tsallis and Kaniadakis entropies in bi-level image thresholding |  http://arxiv.org/abs/1502.06556  | author:Amelia Carolina Sparavigna category:cs.CV published:2015-02-23 summary:The maximum entropy principle is often used for bi-level or multi-levelthresholding of images. For this purpose, some methods are available based onShannon and Tsallis entropies. In this paper, we discuss them and propose amethod based on Kaniadakis entropy.
arxiv-1502-06197 | On Online Control of False Discovery Rate |  http://arxiv.org/abs/1502.06197  | author:Adel Javanmard, Andrea Montanari category:stat.ME cs.LG math.ST stat.AP stat.TH published:2015-02-22 summary:Multiple hypotheses testing is a core problem in statistical inference andarises in almost every scientific field. Given a sequence of null hypotheses$\mathcal{H}(n) = (H_1,..., H_n)$, Benjamini and Hochberg\cite{benjamini1995controlling} introduced the false discovery rate (FDR)criterion, which is the expected proportion of false positives among rejectednull hypotheses, and proposed a testing procedure that controls FDR below apre-assigned significance level. They also proposed a different criterion,called mFDR, which does not control a property of the realized set of tests;rather it controls the ratio of expected number of false discoveries to theexpected number of discoveries. In this paper, we propose two procedures for multiple hypotheses testing thatwe will call "LOND" and "LORD". These procedures control FDR and mFDR in an\emph{online manner}. Concretely, we consider an ordered --possibly infinite--sequence of null hypotheses $\mathcal{H} = (H_1,H_2,H_3,...)$ where, at eachstep $i$, the statistician must decide whether to reject hypothesis $H_i$having access only to the previous decisions. To the best of our knowledge, ourwork is the first that controls FDR in this setting. This model was introducedby Foster and Stine \cite{alpha-investing} whose alpha-investing rule onlycontrols mFDR in online manner. In order to compare different procedures, we develop lower bounds on thetotal discovery rate under the mixture model and prove that both LOND and LORDhave nearly linear number of discoveries. We further propose adjustment to LONDto address arbitrary correlation among the $p$-values. Finally, we evaluate theperformance of our procedures on both synthetic and real data comparing themwith alpha-investing rule, Benjamin-Hochberg method and a Bonferroni procedure.
arxiv-1502-06177 | SDCA without Duality |  http://arxiv.org/abs/1502.06177  | author:Shai Shalev-Shwartz category:cs.LG published:2015-02-22 summary:Stochastic Dual Coordinate Ascent is a popular method for solving regularizedloss minimization for the case of convex losses. In this paper we show how avariant of SDCA can be applied for non-convex losses. We prove linearconvergence rate even if individual loss functions are non-convex as long asthe expected loss is convex.
arxiv-1502-06189 | Two-stage Sampling, Prediction and Adaptive Regression via Correlation Screening (SPARCS) |  http://arxiv.org/abs/1502.06189  | author:Hamed Firouzi, Bala Rajaratnam, Alfred Hero category:stat.ML cs.LG published:2015-02-22 summary:This paper proposes a general adaptive procedure for budget-limited predictordesign in high dimensions called two-stage Sampling, Prediction and AdaptiveRegression via Correlation Screening (SPARCS). SPARCS can be applied to highdimensional prediction problems in experimental science, medicine, finance, andengineering, as illustrated by the following. Suppose one wishes to run asequence of experiments to learn a sparse multivariate predictor of a dependentvariable $Y$ (disease prognosis for instance) based on a $p$ dimensional set ofindependent variables $\mathbf X=[X_1,\ldots, X_p]^T$ (assayed biomarkers).Assume that the cost of acquiring the full set of variables $\mathbf X$increases linearly in its dimension. SPARCS breaks the data collection into twostages in order to achieve an optimal tradeoff between sampling cost andpredictor performance. In the first stage we collect a few ($n$) expensivesamples $\{y_i,\mathbf x_i\}_{i=1}^n$, at the full dimension $p\gg n$ of$\mathbf X$, winnowing the number of variables down to a smaller dimension $l <p$ using a type of cross-correlation or regression coefficient screening. Inthe second stage we collect a larger number $(t-n)$ of cheaper samples of the$l$ variables that passed the screening of the first stage. At the secondstage, a low dimensional predictor is constructed by solving the standardregression problem using all $t$ samples of the selected variables. SPARCS isan adaptive online algorithm that implements false positive control on theselected variables, is well suited to small sample sizes, and is scalable tohigh dimensions. We establish asymptotic bounds for the Familywise Error Rate(FWER), specify high dimensional convergence rates for support recovery, andestablish optimal sample allocation rules to the first and second stages.
arxiv-1502-06219 | Video Text Localization with an emphasis on Edge Features |  http://arxiv.org/abs/1502.06219  | author:B. H. Shekar, Smitha M. L. category:cs.CV published:2015-02-22 summary:The text detection and localization plays a major role in video analysis andunderstanding. The scene text embedded in video consist of high-level semanticsand hence contributes significantly to visual content analysis and retrieval.This paper proposes a novel method to robustly localize the texts in naturalscene images and videos based on sobel edge emphasizing approach. The inputimage is preprocessed and edge emphasis is done to detect the text clusters.Further, a set of rules have been devised using morphological operators forfalse positive elimination and connected component analysis is performed todetect the text regions and hence text localization is performed. Theexperimental results obtained on publicly available standard datasetsillustrate that the proposed method can detect and localize the texts ofvarious sizes, fonts and colors.
arxiv-1502-06161 | Using NLP to measure democracy |  http://arxiv.org/abs/1502.06161  | author:Thiago Marzagão category:cs.CL cs.IR cs.LG stat.ML published:2015-02-22 summary:This paper uses natural language processing to create the first machine-codeddemocracy index, which I call Automated Democracy Scores (ADS). The ADS arebased on 42 million news articles from 6,043 different sources and cover allindependent countries in the 1993-2012 period. Unlike the democracy indices wehave today the ADS are replicable and have standard errors small enough toactually distinguish between cases. The ADS are produced with supervised learning. Three approaches are tried: a)a combination of Latent Semantic Analysis and tree-based regression methods; b)a combination of Latent Dirichlet Allocation and tree-based regression methods;and c) the Wordscores algorithm. The Wordscores algorithm outperforms thealternatives, so it is the one on which the ADS are based. There is a web application where anyone can change the training set and seehow the results change: democracy-scores.org
arxiv-1502-06260 | Compressive Hyperspectral Imaging with Side Information |  http://arxiv.org/abs/1502.06260  | author:Xin Yuan, Tsung-Han Tsai, Ruoyu Zhu, Patrick Llull, David Brady, Lawrence Carin category:cs.CV published:2015-02-22 summary:A blind compressive sensing algorithm is proposed to reconstructhyperspectral images from spectrally-compressed measurements.Thewavelength-dependent data are coded and then superposed, mapping thethree-dimensional hyperspectral datacube to a two-dimensional image. Theinversion algorithm learns a dictionary {\em in situ} from the measurements viaglobal-local shrinkage priors. By using RGB images as side information of thecompressive sensing system, the proposed approach is extended to learn acoupled dictionary from the joint dataset of the compressed measurements andthe corresponding RGB images, to improve reconstruction quality. A prototypecamera is built using a liquid-crystal-on-silicon modulator. Experimentalreconstructions of hyperspectral datacubes from both simulated and realcompressed measurements demonstrate the efficacy of the proposed inversionalgorithm, the feasibility of the camera and the benefit of side information.
arxiv-1502-06254 | The fundamental nature of the log loss function |  http://arxiv.org/abs/1502.06254  | author:Vladimir Vovk category:cs.LG stat.ME published:2015-02-22 summary:The standard loss functions used in the literature on probabilisticprediction are the log loss function, the Brier loss function, and thespherical loss function; however, any computable proper loss function can beused for comparison of prediction algorithms. This note shows that the log lossfunction is most selective in that any prediction algorithm that is optimal fora given data sequence (in the sense of the algorithmic theory of randomness)under the log loss function will be optimal under any computable proper mixableloss function; on the other hand, there is a data sequence and a predictionalgorithm that is optimal for that sequence under either of the two otherstandard loss functions but not under the log loss function.
arxiv-1502-06236 | Some enumerations of binary digital images |  http://arxiv.org/abs/1502.06236  | author:P. Christopher Staecker category:math.CO cs.CV math.GN I.4.m published:2015-02-22 summary:The topology of digital images has been studied much in recent years, but noattempt has been made to exhaustively catalog the structure of binary images ofsmall numbers of points. We produce enumerations of several classes of digitalimages up to isomorphism and decide which among them are homotopy equivalent toone another. Noting some patterns in the results, we make some conjecturesabout digital images which are irreducible but not rigid.
arxiv-1502-06220 | Boosting of Image Denoising Algorithms |  http://arxiv.org/abs/1502.06220  | author:Yaniv Romano, Michael Elad category:cs.CV cs.NA published:2015-02-22 summary:In this paper we propose a generic recursive algorithm for improving imagedenoising methods. Given the initial denoised image, we suggest repeating thefollowing "SOS" procedure: (i) (S)trengthen the signal by adding the previousdenoised image to the degraded input image, (ii) (O)perate the denoising methodon the strengthened image, and (iii) (S)ubtract the previous denoised imagefrom the restored signal-strengthened outcome. The convergence of this processis studied for the K-SVD image denoising and related algorithms. Still in thecontext of K-SVD image denoising, we introduce an interesting interpretation ofthe SOS algorithm as a technique for closing the gap between the localpatch-modeling and the global restoration task, thereby leading to improvedperformance. In a quest for the theoretical origin of the SOS algorithm, weprovide a graph-based interpretation of our method, where the SOS recursiveupdate effectively minimizes a penalty function that aims to denoise the image,while being regularized by the graph Laplacian. We demonstrate the SOS boostingalgorithm for several leading denoising methods (K-SVD, NLM, BM3D, and EPLL),showing tendency to further improve denoising performance.
arxiv-1502-06208 | Nearly optimal classification for semimetrics |  http://arxiv.org/abs/1502.06208  | author:Lee-Ad Gottlieb, Aryeh Kontorovich category:cs.LG cs.CC cs.DS published:2015-02-22 summary:We initiate the rigorous study of classification in semimetric spaces, whichare point sets with a distance function that is non-negative and symmetric, butneed not satisfy the triangle inequality. For metric spaces, the doublingdimension essentially characterizes both the runtime and sample complexity ofclassification algorithms --- yet we show that this is not the case forsemimetrics. Instead, we define the {\em density dimension} and discover thatit plays a central role in the statistical and algorithmic feasibility oflearning in semimetric spaces. We present nearly optimal sample compressionalgorithms and use these to obtain generalization guarantees, including fastrates. The latter hold for general sample compression schemes and may be ofindependent interest.
arxiv-1502-06256 | Spaced seeds improve k-mer-based metagenomic classification |  http://arxiv.org/abs/1502.06256  | author:Karel Brinda, Maciej Sykulski, Gregory Kucherov category:q-bio.GN cs.CE cs.LG published:2015-02-22 summary:Metagenomics is a powerful approach to study genetic content of environmentalsamples that has been strongly promoted by NGS technologies. To cope withmassive data involved in modern metagenomic projects, recent tools [4, 39] relyon the analysis of k-mers shared between the read to be classified and sampledreference genomes. Within this general framework, we show in this work thatspaced seeds provide a significant improvement of classification accuracy asopposed to traditional contiguous k-mers. We support this thesis through aseries a different computational experiments, including simulations oflarge-scale metagenomic projects. Scripts and programs used in this study, aswell as supplementary material, are available fromhttp://github.com/gregorykucherov/spaced-seeds-for-metagenomics.
arxiv-1502-06235 | Spatio-temporal Video Parsing for Abnormality Detection |  http://arxiv.org/abs/1502.06235  | author:Borislav Antić, Björn Ommer category:cs.CV published:2015-02-22 summary:Abnormality detection in video poses particular challenges due to theinfinite size of the class of all irregular objects and behaviors. Thus no (orby far not enough) abnormal training samples are available and we need to findabnormalities in test data without actually knowing what they are.Nevertheless, the prevailing concept of the field is to directly search forindividual abnormal local patches or image regions independent of another. Toaddress this problem, we propose a method for joint detection of abnormalitiesin videos by spatio-temporal video parsing. The goal of video parsing is tofind a set of indispensable normal spatio-temporal object hypotheses thatjointly explain all the foreground of a video, while, at the same time, beingsupported by normal training samples. Consequently, we avoid a direct detectionof abnormalities and discover them indirectly as those hypotheses which areneeded for covering the foreground without finding an explanation forthemselves by normal samples. Abnormalities are localized by MAP inference in agraphical model and we solve it efficiently by formulating it as a convexoptimization problem. We experimentally evaluate our approach on severalchallenging benchmark sets, improving over the state-of-the-art on all standardbenchmarks both in terms of abnormality classification and localization.
arxiv-1502-06187 | Teaching and compressing for low VC-dimension |  http://arxiv.org/abs/1502.06187  | author:Shay Moran, Amir Shpilka, Avi Wigderson, Amir Yehudayoff category:cs.LG published:2015-02-22 summary:In this work we study the quantitative relation between VC-dimension and twoother basic parameters related to learning and teaching. We present relativelyefficient constructions of {\em sample compression schemes} and {\em teachingsets} for classes of low VC-dimension. Let $C$ be a finite boolean conceptclass of VC-dimension $d$. Set $k = O(d 2^d \log \log C)$. We construct sample compression schemes of size $k$ for $C$, with additionalinformation of $k \log(k)$ bits. Roughly speaking, given any list of$C$-labelled examples of arbitrary length, we can retain only $k$ labeledexamples in a way that allows to recover the labels of all others examples inthe list. We also prove that there always exists a concept $c$ in $C$ with a teachingset (i.e. a list of $c$-labelled examples uniquely identifying $c$) of size$k$. Equivalently, we prove that the recursive teaching dimension of $C$ is atmost $k$. The question of constructing sample compression schemes for classes of smallVC-dimension was suggested by Littlestone and Warmuth (1986), and the problemof constructing teaching sets for classes of small VC-dimension was suggestedby Kuhlmann (1999). Previous constructions for general concept classes yieldedsize $O(\log C)$ for both questions, even when the VC-dimension is constant.
arxiv-1502-06105 | Regularization and Kernelization of the Maximin Correlation Approach |  http://arxiv.org/abs/1502.06105  | author:Taehoon Lee, Taesup Moon, Seung Jean Kim, Sungroh Yoon category:cs.CV cs.LG published:2015-02-21 summary:Robust classification becomes challenging when each class consists ofmultiple subclasses. Examples include multi-font optical character recognitionand automated protein function prediction. In correlation-basednearest-neighbor classification, the maximin correlation approach (MCA)provides the worst-case optimal solution by minimizing the maximummisclassification risk through an iterative procedure. Despite the optimality,the original MCA has drawbacks that have limited its wide applicability inpractice. That is, the MCA tends to be sensitive to outliers, cannoteffectively handle nonlinearities in datasets, and suffers from having highcomputational complexity. To address these limitations, we propose an improvedsolution, named regularized maximin correlation approach (R-MCA). We firstreformulate MCA as a quadratically constrained linear programming (QCLP)problem, incorporate regularization by introducing slack variables in theprimal problem of the QCLP, and derive the corresponding Lagrangian dual. Thedual formulation enables us to apply the kernel trick to R-MCA so that it canbetter handle nonlinearities. Our experimental results demonstrate that theregularization and kernelization make the proposed R-MCA more robust andaccurate for various classification tasks than the original MCA. Furthermore,when the data size or dimensionality grows, R-MCA runs substantially faster bysolving either the primal or dual (whichever has a smaller variable dimension)of the QCLP.
arxiv-1502-06134 | Learning with Square Loss: Localization through Offset Rademacher Complexity |  http://arxiv.org/abs/1502.06134  | author:Tengyuan Liang, Alexander Rakhlin, Karthik Sridharan category:stat.ML cs.LG math.ST stat.TH published:2015-02-21 summary:We consider regression with square loss and general classes of functionswithout the boundedness assumption. We introduce a notion of offset Rademachercomplexity that provides a transparent way to study localization both inexpectation and in high probability. For any (possibly non-convex) class, theexcess loss of a two-step estimator is shown to be upper bounded by this offsetcomplexity through a novel geometric inequality. In the convex case, theestimator reduces to an empirical risk minimizer. The method recovers theresults of \citep{RakSriTsy15} for the bounded case while also providingguarantees without the boundedness assumption.
arxiv-1502-06076 | A Heat-Map-based Algorithm for Recognizing Group Activities in Videos |  http://arxiv.org/abs/1502.06076  | author:Weiyao Lin, Hang Chu, Jianxin Wu, Bin Sheng, Zhenzhong Chen category:cs.CV published:2015-02-21 summary:In this paper, a new heat-map-based (HMB) algorithm is proposed for groupactivity recognition. The proposed algorithm first models human trajectories asseries of "heat sources" and then applies a thermal diffusion process to createa heat map (HM) for representing the group activities. Based on this heat map,a new key-point based (KPB) method is used for handling the alignments amongheat maps with different scales and rotations. And a surface-fitting (SF)method is also proposed for recognizing group activities. Our proposed HMfeature can efficiently embed the temporal motion information of the groupactivities while the proposed KPB and SF methods can effectively utilize thecharacteristics of the heat map for activity recognition. Experimental resultsdemonstrate the effectiveness of our proposed algorithms.
arxiv-1502-06080 | Intra-and-Inter-Constraint-based Video Enhancement based on Piecewise Tone Mapping |  http://arxiv.org/abs/1502.06080  | author:Yuanzhe Chen, Weiyao Lin, Chongyang Zhang, Zhenzhong Chen, Ning Xu, Jun Xie category:cs.CV cs.MM published:2015-02-21 summary:Video enhancement plays an important role in various video applications. Inthis paper, we propose a new intra-and-inter-constraint-based video enhancementapproach aiming to 1) achieve high intra-frame quality of the entire picturewhere multiple region-of-interests (ROIs) can be adaptively and simultaneouslyenhanced, and 2) guarantee the inter-frame quality consistencies among videoframes. We first analyze features from different ROIs and create a piecewisetone mapping curve for the entire frame such that the intra-frame quality of aframe can be enhanced. We further introduce new inter-frame constraints toimprove the temporal quality consistency. Experimental results show that theproposed algorithm obviously outperforms the state-of-the-art algorithms.
arxiv-1502-06081 | Study of a Robust Algorithm Applied in the Optimal Position Tuning for the Camera Lens in Automated Visual Inspection Systems |  http://arxiv.org/abs/1502.06081  | author:Radu Arsinte category:cs.CV published:2015-02-21 summary:This paper present the mathematical fundaments and experimental study of analgorithm used to find the optimal position for the camera lens to obtain amaximum of details. This information can be further applied to a appropriatesystem to automatically correct this position. The algorithm is based on theevaluation of a so called resolution function who calculates the maximum ofgradient in a certain zone of the image. The paper also presents alternativeforms of the function, results of measurements and set up a set of practicalrules for the right application of the algorithm.
arxiv-1502-06096 | Reinforcement Learning in a Neurally Controlled Robot Using Dopamine Modulated STDP |  http://arxiv.org/abs/1502.06096  | author:Richard Evans category:cs.NE cs.RO published:2015-02-21 summary:Recent work has shown that dopamine-modulated STDP can solve many of theissues associated with reinforcement learning, such as the distal rewardproblem. Spiking neural networks provide a useful technique in implementingreinforcement learning in an embodied context as they can deal with continuousparameter spaces and as such are better at generalizing the correct behaviourto perform in a given context. In this project we implement a version of DA-modulated STDP in an embodiedrobot on a food foraging task. Through simulated dopaminergic neurons we showhow the robot is able to learn a sequence of behaviours in order to achieve afood reward. In tests the robot was able to learn food-attraction behaviour,and subsequently unlearn this behaviour when the environment changed, in all 50trials. Moreover we show that the robot is able to operate in an environmentwhereby the optimal behaviour changes rapidly and so the agent must constantlyrelearn. In a more complex environment, consisting of food-containers, therobot was able to learn food-container attraction in 95% of trials, despite thelarge temporal distance between the correct behaviour and the reward. This isachieved by shifting the dopamine response from the primary stimulus (food) tothe secondary stimulus (food-container). Our work provides insights into the reasons behind some observed biologicalphenomena, such as the bursting behaviour observed in dopaminergic neurons. Aswell as demonstrating how spiking neural network controlled robots are able tosolve a range of reinforcement learning tasks.
arxiv-1502-06132 | Universal Memory Architectures for Autonomous Machines |  http://arxiv.org/abs/1502.06132  | author:Dan P. Guralnik, Daniel E. Koditschek category:cs.AI cs.LG cs.RO math.MG published:2015-02-21 summary:We propose a self-organizing memory architecture for perceptual experience,capable of supporting autonomous learning and goal-directed problem solving inthe absence of any prior information about the agent's environment. Thearchitecture is simple enough to ensure (1) a quadratic bound (in the number ofavailable sensors) on space requirements, and (2) a quadratic bound on thetime-complexity of the update-execute cycle. At the same time, it issufficiently complex to provide the agent with an internal representation whichis (3) minimal among all representations of its class which account for everysensory equivalence class subject to the agent's belief state; (4) capable, inprinciple, of recovering the homotopy type of the system's state space; (5)learnable with arbitrary precision through a random application of theavailable actions. The provable properties of an effectively trained memorystructure exploit a duality between weak poc sets -- a symbolic (discrete)representation of subset nesting relations -- and non-positively curved cubicalcomplexes, whose rich convexity theory underlies the planning cycle of theproposed architecture.
arxiv-1502-06144 | Detection of Planted Solutions for Flat Satisfiability Problems |  http://arxiv.org/abs/1502.06144  | author:Quentin Berthet, Jordan S. Ellenberg category:math.ST cs.CC cs.LG stat.TH published:2015-02-21 summary:We study the detection problem of finding planted solutions in randominstances of flat satisfiability problems, a generalization of booleansatisfiability formulas. We describe the properties of random instances of flatsatisfiability, as well of the optimal rates of detection of the associatedhypothesis testing problem. We also study the performance of an algorithmicallyefficient testing procedure. We introduce a modification of our model, thelight planting of solutions, and show that it is as hard as the problem oflearning parity with noise. This hints strongly at the difficulty of detectingplanted flat satisfiability for a wide class of tests.
arxiv-1502-06073 | Study on Sparse Representation based Classification for Biometric Verification |  http://arxiv.org/abs/1502.06073  | author:Zengxi Huang, Yiguang Liu, Xiaoming Wang, Jinrong Hu category:cs.CV published:2015-02-21 summary:In this paper, we propose a multimodal verification system integrating faceand ear based on sparse representation based classification (SRC). The face andear query samples are first encoded separately to derive sparsity-based matchscores, and which are then combined with sum-rule fusion for verification.Apart from validating the encouraging performance of SRC-based multimodalverification, this paper also dedicates to provide a clear understanding aboutthe characteristics of SRC-based biometric verification. To this end, twosparsity-based metrics, i.e. spare coding error (SCE) and sparse contributionrate (SCR), are involved, together with face and ear unimodal SRC-basedverification. As for the issue that SRC-based biometric verification may sufferfrom heavy computational burden and verification accuracy degradation withincrease of enrolled subjects, we argue that it could be properly resolved byexploiting small random dictionary for sparsity-based score computation, whichconsists of training samples from a limited number of randomly selectedsubjects. Experimental results demonstrate the superiority of SRC-basedmultimodal verification compared to the state-of-the-art multimodal methodslike likelihood ratio (LLR), support vector machine (SVM), and the sum-rulefusion methods using cosine similarity, meanwhile the idea of using smallrandom dictionary is feasible in both effectiveness and efficiency.
arxiv-1502-06094 | Positive Neural Networks in Discrete Time Implement Monotone-Regular Behaviors |  http://arxiv.org/abs/1502.06094  | author:Tom J. Ameloot, Jan Van den Bussche category:cs.NE published:2015-02-21 summary:We study the expressive power of positive neural networks. The model usespositive connection weights and multiple input neurons. Different behaviors canbe expressed by varying the connection weights. We show that in discrete time,and in absence of noise, the class of positive neural networks captures theso-called monotone-regular behaviors, that are based on regular languages. Afiner picture emerges if one takes into account the delay by which amonotone-regular behavior is implemented. Each monotone-regular behavior can beimplemented by a positive neural network with a delay of one time unit. Somemonotone-regular behaviors can be implemented with zero delay. And,interestingly, some simple monotone-regular behaviors can not be implementedwith zero delay.
arxiv-1502-06108 | Don't Just Listen, Use Your Imagination: Leveraging Visual Common Sense for Non-Visual Tasks |  http://arxiv.org/abs/1502.06108  | author:Xiao Lin, Devi Parikh category:cs.CV published:2015-02-21 summary:Artificial agents today can answer factual questions. But they fall short onquestions that require common sense reasoning. Perhaps this is because mostexisting common sense databases rely on text to learn and represent knowledge.But much of common sense knowledge is unwritten - partly because it tends notto be interesting enough to talk about, and partly because some common sense isunnatural to articulate in text. While unwritten, it is not unseen. In thispaper we leverage semantic common sense knowledge learned from images - i.e.visual common sense - in two textual tasks: fill-in-the-blank and visualparaphrasing. We propose to "imagine" the scene behind the text, and leveragevisual cues from the "imagined" scenes in addition to textual cues whileanswering these questions. We imagine the scenes as a visual abstraction. Ourapproach outperforms a strong text-only baseline on these tasks. Our proposedtasks can serve as benchmarks to quantitatively evaluate progress in solvingtasks that go "beyond recognition". Our code and datasets are publiclyavailable.
arxiv-1502-06064 | MILJS : Brand New JavaScript Libraries for Matrix Calculation and Machine Learning |  http://arxiv.org/abs/1502.06064  | author:Ken Miura, Tetsuaki Mano, Atsushi Kanehira, Yuichiro Tsuchiya, Tatsuya Harada category:stat.ML cs.LG cs.MS published:2015-02-21 summary:MILJS is a collection of state-of-the-art, platform-independent, scalable,fast JavaScript libraries for matrix calculation and machine learning. Our corelibrary offering a matrix calculation is called Sushi, which exhibits farbetter performance than any other leading machine learning libraries written inJavaScript. Especially, our matrix multiplication is 177 times faster than thefastest JavaScript benchmark. Based on Sushi, a machine learning library calledTempura is provided, which supports various algorithms widely used in machinelearning research. We also provide Soba as a visualization library. Theimplementations of our libraries are clearly written, properly documented andthus can are easy to get started with, as long as there is a web browser. Theselibraries are available from http://mil-tokyo.github.io/ under the MIT license.
arxiv-1502-06075 | A new network-based algorithm for human activity recognition in video |  http://arxiv.org/abs/1502.06075  | author:Weiyao Lin, Yuanzhe Chen, Jianxin Wu, Hanli Wang, Bin Sheng, Hongxiang Li category:cs.CV published:2015-02-21 summary:In this paper, a new network-transmission-based (NTB) algorithm is proposedfor human activity recognition in videos. The proposed NTB algorithm models theentire scene as an error-free network. In this network, each node correspondsto a patch of the scene and each edge represents the activity correlationbetween the corresponding patches. Based on this network, we further modelpeople in the scene as packages while human activities can be modeled as theprocess of package transmission in the network. By analyzing these specific"package transmission" processes, various activities can be effectivelydetected. The implementation of our NTB algorithm into abnormal activitydetection and group activity recognition are described in detail in the paper.Experimental results demonstrate the effectiveness of our proposed algorithm.
arxiv-1502-05911 | A Data Mining framework to model Consumer Indebtedness with Psychological Factors |  http://arxiv.org/abs/1502.05911  | author:Alexandros Ladas, Eamonn Ferguson, Uwe Aickelin, Jon Garibaldi category:cs.LG cs.CE published:2015-02-20 summary:Modelling Consumer Indebtedness has proven to be a problem of complex nature.In this work we utilise Data Mining techniques and methods to explore themultifaceted aspect of Consumer Indebtedness by examining the contribution ofPsychological Factors, like Impulsivity to the analysis of Consumer Debt. Ourresults confirm the beneficial impact of Psychological Factors in modellingConsumer Indebtedness and suggest a new approach in analysing Consumer Debt,that would take into consideration more Psychological characteristics ofconsumers and adopt techniques and practices from Data Mining.
arxiv-1502-05925 | Feature-Budgeted Random Forest |  http://arxiv.org/abs/1502.05925  | author:Feng Nan, Joseph Wang, Venkatesh Saligrama category:stat.ML cs.LG published:2015-02-20 summary:We seek decision rules for prediction-time cost reduction, where completedata is available for training, but during prediction-time, each feature canonly be acquired for an additional cost. We propose a novel random forestalgorithm to minimize prediction error for a user-specified {\it average}feature acquisition budget. While random forests yield strong generalizationperformance, they do not explicitly account for feature costs and furthermorerequire low correlation among trees, which amplifies costs. Our random forestgrows trees with low acquisition cost and high strength based on greedy minimaxcost-weighted-impurity splits. Theoretically, we establish near-optimalacquisition cost guarantees for our algorithm. Empirically, on a number ofbenchmark datasets we demonstrate superior accuracy-cost curves againststate-of-the-art prediction-time algorithms.
arxiv-1502-05928 | Supervised Dictionary Learning and Sparse Representation-A Review |  http://arxiv.org/abs/1502.05928  | author:Mehrdad J. Gangeh, Ahmed K. Farahat, Ali Ghodsi, Mohamed S. Kamel category:cs.CV published:2015-02-20 summary:Dictionary learning and sparse representation (DLSR) is a recent andsuccessful mathematical model for data representation that achievesstate-of-the-art performance in various fields such as pattern recognition,machine learning, computer vision, and medical imaging. The originalformulation for DLSR is based on the minimization of the reconstruction errorbetween the original signal and its sparse representation in the space of thelearned dictionary. Although this formulation is optimal for solving problemssuch as denoising, inpainting, and coding, it may not lead to optimal solutionin classification tasks, where the ultimate goal is to make the learneddictionary and corresponding sparse representation as discriminative aspossible. This motivated the emergence of a new category of techniques, whichis appropriately called supervised dictionary learning and sparserepresentation (S-DLSR), leading to more optimal dictionary and sparserepresentation in classification tasks. Despite many research efforts forS-DLSR, the literature lacks a comprehensive view of these techniques, theirconnections, advantages and shortcomings. In this paper, we address this gapand provide a review of the recently proposed algorithms for S-DLSR. We firstpresent a taxonomy of these algorithms into six categories based on theapproach taken to include label information into the learning of the dictionaryand/or sparse representation. For each category, we draw connections betweenthe algorithms in this category and present a unified framework for them. Wethen provide guidelines for applied researchers on how to represent and learnthe building blocks of an S-DLSR solution based on the problem at hand. Thisreview provides a broad, yet deep, view of the state-of-the-art methods forS-DLSR and allows for the advancement of research and development in thisemerging area of research.
arxiv-1502-05934 | Achieving All with No Parameters: Adaptive NormalHedge |  http://arxiv.org/abs/1502.05934  | author:Haipeng Luo, Robert E. Schapire category:cs.LG published:2015-02-20 summary:We study the classic online learning problem of predicting with expertadvice, and propose a truly parameter-free and adaptive algorithm that achievesseveral objectives simultaneously without using any prior information. The maincomponent of this work is an improved version of the NormalHedge.DT algorithm(Luo and Schapire, 2014), called AdaNormalHedge. On one hand, this newalgorithm ensures small regret when the competitor has small loss and almostconstant regret when the losses are stochastic. On the other hand, thealgorithm is able to compete with any convex combination of the expertssimultaneously, with a regret in terms of the relative entropy of the prior andthe competitor. This resolves an open problem proposed by Chaudhuri et al.(2009) and Chernov and Vovk (2010). Moreover, we extend the results to thesleeping expert setting and provide two applications to illustrate the power ofAdaNormalHedge: 1) competing with time-varying unknown competitors and 2)predicting almost as well as the best pruning tree. Our results on theseapplications significantly improve previous work from different aspects, and aspecial case of the first application resolves another open problem proposed byWarmuth and Koolen (2014) on whether one can simultaneously achieve optimalshifting regret for both adversarial and stochastic losses.
arxiv-1502-05943 | Refining Adverse Drug Reactions using Association Rule Mining for Electronic Healthcare Data |  http://arxiv.org/abs/1502.05943  | author:Jenna M. Reps, Uwe Aickelin, Jiangang Ma, Yanchun Zhang category:cs.DB cs.CE cs.LG published:2015-02-20 summary:Side effects of prescribed medications are a common occurrence. Electronichealthcare databases present the opportunity to identify new side effectsefficiently but currently the methods are limited due to confounding (i.e. whenan association between two variables is identified due to them both beingassociated to a third variable). In this paper we propose a proof of concept method that learns commonassociations and uses this knowledge to automatically refine side effectsignals (i.e. exposure-outcome associations) by removing instances of theexposure-outcome associations that are caused by confounding. This leaves thesignal instances that are most likely to correspond to true side effectoccurrences. We then calculate a novel measure termed the confounding-adjustedrisk value, a more accurate absolute risk value of a patient experiencing theoutcome within 60 days of the exposure. Tentative results suggest that the method works. For the four signals (i.e.exposure-outcome associations) investigated we are able to correctly filter themajority of exposure-outcome instances that were unlikely to correspond to trueside effects. The method is likely to improve when tuning the association rulemining parameters for specific health outcomes. This paper shows that it may be possible to filter signals at a patient levelbased on association rules learned from considering patients' medicalhistories. However, additional work is required to develop a way to automatethe tuning of the method's parameters.
arxiv-1502-05957 | Web Similarity |  http://arxiv.org/abs/1502.05957  | author:Andrew R. Cohen, Paul M. B. Vitanyi category:cs.IR cs.CL cs.CV published:2015-02-20 summary:Normalized web distance (NWD) is a similarity or normalized semantic distancebased on the World Wide Web or any other large electronic database, forinstance Wikipedia, and a search engine that returns reliable aggregate pagecounts. For sets of search terms the NWD gives a similarity on a scale from 0(identical) to 1 (completely different). The NWD approximates the similarityaccording to all (upper semi)computable properties. We develop the theory andgive applications. The derivation of the NWD method is based on Kolmogorovcomplexity.
arxiv-1502-05774 | Low-Cost Learning via Active Data Procurement |  http://arxiv.org/abs/1502.05774  | author:Jacob Abernethy, Yiling Chen, Chien-Ju Ho, Bo Waggoner category:cs.GT cs.AI cs.LG stat.ML J.4; I.2.6 published:2015-02-20 summary:We design mechanisms for online procurement of data held by strategic agentsfor machine learning tasks. The challenge is to use past data to actively pricefuture data and give learning guarantees even when an agent's cost forrevealing her data may depend arbitrarily on the data itself. We achieve thisgoal by showing how to convert a large class of no-regret algorithms intoonline posted-price and learning mechanisms. Our results in a sense parallelclassic sample complexity guarantees, but with the key resource being moneyrather than quantity of data: With a budget constraint $B$, we give robust risk(predictive error) bounds on the order of $1/\sqrt{B}$. Because we use anactive approach, we can often guarantee to do significantly better byleveraging correlations between costs and data. Our algorithms and analysis go through a model of no-regret learning with $T$arriving pairs (cost, data) and a budget constraint of $B$. Our regret boundsfor this model are on the order of $T/\sqrt{B}$ and we give lower bounds on thesame order.
arxiv-1502-05908 | Learning Descriptors for Object Recognition and 3D Pose Estimation |  http://arxiv.org/abs/1502.05908  | author:Paul Wohlhart, Vincent Lepetit category:cs.CV published:2015-02-20 summary:Detecting poorly textured objects and estimating their 3D pose reliably isstill a very challenging problem. We introduce a simple but powerful approachto computing descriptors for object views that efficiently capture both theobject identity and 3D pose. By contrast with previous manifold-basedapproaches, we can rely on the Euclidean distance to evaluate the similaritybetween descriptors, and therefore use scalable Nearest Neighbor search methodsto efficiently handle a large number of objects under a large range of poses.To achieve this, we train a Convolutional Neural Network to compute thesedescriptors by enforcing simple similarity and dissimilarity constraintsbetween the descriptors. We show that our constraints nicely untangle theimages from different objects and different views into clusters that are notonly well-separated but also structured as the corresponding sets of poses: TheEuclidean distance between descriptors is large when the descriptors are fromdifferent objects, and directly related to the distance between the poses whenthe descriptors are from the same object. These important properties allow usto outperform state-of-the-art object views representations on challenging RGBand RGB-D data.
arxiv-1502-05803 | Visual object tracking performance measures revisited |  http://arxiv.org/abs/1502.05803  | author:Luka Čehovin, Aleš Leonardis, Matej Kristan category:cs.CV published:2015-02-20 summary:The problem of visual tracking evaluation is sporting a large variety ofperformance measures, and largely suffers from lack of consensus about whichmeasures should be used in experiments. This makes the cross-paper trackercomparison difficult. Furthermore, as some measures may be less effective thanothers, the tracking results may be skewed or biased towards particulartracking aspects. In this paper we revisit the popular performance measures andtracker performance visualizations and analyze them theoretically andexperimentally. We show that several measures are equivalent from the point ofinformation they provide for tracker comparison and, crucially, that some aremore brittle than the others. Based on our analysis we narrow down the set ofpotential measures to only two complementary ones, describing accuracy androbustness, thus pushing towards homogenization of the tracker evaluationmethodology. These two measures can be intuitively interpreted and visualizedand have been employed by the recent Visual Object Tracking (VOT) challenges asthe foundation for the evaluation methodology.
arxiv-1502-05886 | On predictability of rare events leveraging social media: a machine learning perspective |  http://arxiv.org/abs/1502.05886  | author:Lei Le, Emilio Ferrara, Alessandro Flammini category:cs.SI cs.LG physics.soc-ph published:2015-02-20 summary:Information extracted from social media streams has been leveraged toforecast the outcome of a large number of real-world events, from politicalelections to stock market fluctuations. An increasing amount of studiesdemonstrates how the analysis of social media conversations provides cheapaccess to the wisdom of the crowd. However, extents and contexts in which suchforecasting power can be effectively leveraged are still unverified at least ina systematic way. It is also unclear how social-media-based predictions compareto those based on alternative information sources. To address these issues,here we develop a machine learning framework that leverages social mediastreams to automatically identify and predict the outcomes of soccer matches.We focus in particular on matches in which at least one of the possibleoutcomes is deemed as highly unlikely by professional bookmakers. We argue thatsport events offer a systematic approach for testing the predictive power ofsocial media, and allow to compare such power against the rigorous baselinesset by external sources. Despite such strict baselines, our framework yieldsabove 8% marginal profit when used to inform simple betting strategies. Thesystem is based on real-time sentiment analysis and exploits data collectedimmediately before the games, allowing for informed bets. We discuss therationale behind our approach, describe the learning framework, its predictionperformance and the return it provides as compared to a set of bettingstrategies. To test our framework we use both historical Twitter data from the2014 FIFA World Cup games, and real-time Twitter data collected by monitoringthe conversations about all soccer matches of four major European tournaments(FA Premier League, Serie A, La Liga, and Bundesliga), and the 2014 UEFAChampions League, during the period between Oct. 25th 2014 and Nov. 26th 2014.
arxiv-1502-05840 | A General Multi-Graph Matching Approach via Graduated Consistency-regularized Boosting |  http://arxiv.org/abs/1502.05840  | author:Junchi Yan, Minsu Cho, Hongyuan Zha, Xiaokang Yang, Stephen Chu category:cs.CV published:2015-02-20 summary:This paper addresses the problem of matching $N$ weighted graphs referring toan identical object or category. More specifically, matching the common nodecorrespondences among graphs. This multi-graph matching problem involves twoingredients affecting the overall accuracy: i) the local pairwise matchingaffinity score among graphs; ii) the global matching consistency that measuresthe uniqueness of the pairwise matching results by different chaining orders.Previous studies typically either enforce the matching consistency constraintsin the beginning of iterative optimization, which may propagate matching errorboth over iterations and across graph pairs; or separate affinity optimizingand consistency regularization in two steps. This paper is motivated by theobservation that matching consistency can serve as a regularizer in theaffinity objective function when the function is biased due to noises orinappropriate modeling. We propose multi-graph matching methods to incorporatethe two aspects by boosting the affinity score, meanwhile gradually infusingthe consistency as a regularizer. Furthermore, we propose a node-wiseconsistency/affinity-driven mechanism to elicit the common inlier nodes out ofthe irrelevant outliers. Extensive results on both synthetic and public imagedatasets demonstrate the competency of the proposed algorithms.
arxiv-1502-05832 | A provably convergent alternating minimization method for mean field inference |  http://arxiv.org/abs/1502.05832  | author:Pierre Baqué, Jean-Hubert Hours, François Fleuret, Pascal Fua category:cs.LG math.OC published:2015-02-20 summary:Mean-Field is an efficient way to approximate a posterior distribution incomplex graphical models and constitutes the most popular class of Bayesianvariational approximation methods. In most applications, the mean fielddistribution parameters are computed using an alternate coordinateminimization. However, the convergence properties of this algorithm remainunclear. In this paper, we show how, by adding an appropriate penalizationterm, we can guarantee convergence to a critical point, while keeping a closedform update at each step. A convergence rate estimate can also be derived basedon recent results in non-convex optimization.
arxiv-1502-05777 | Spike Event Based Learning in Neural Networks |  http://arxiv.org/abs/1502.05777  | author:James A. Henderson, TingTing A. Gibson, Janet Wiles category:cs.NE cs.LG published:2015-02-20 summary:A scheme is derived for learning connectivity in spiking neural networks. Thescheme learns instantaneous firing rates that are conditional on the activityin other parts of the network. The scheme is independent of the choice ofneuron dynamics or activation function, and network architecture. It involvestwo simple, online, local learning rules that are applied only in response tooccurrences of spike events. This scheme provides a direct method fortransferring ideas between the fields of deep learning and computationalneuroscience. This learning scheme is demonstrated using a layered feedforwardspiking neural network trained self-supervised on a prediction andclassification task for moving MNIST images collected using a Dynamic VisionSensor.
arxiv-1502-05767 | Automatic differentiation in machine learning: a survey |  http://arxiv.org/abs/1502.05767  | author:Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, Jeffrey Mark Siskind category:cs.SC cs.LG G.1.4; I.2.6 published:2015-02-20 summary:Derivatives, mostly in the form of gradients and Hessians, are ubiquitous inmachine learning. Automatic differentiation (AD) is a technique for calculatingderivatives of numeric functions expressed as computer programs efficiently andaccurately, used in fields such as computational fluid dynamics, nuclearengineering, and atmospheric sciences. Despite its advantages and use in otherfields, machine learning practitioners have been little influenced by AD andmake scant use of available tools. We survey the intersection of AD and machinelearning, cover applications where AD has the potential to make a big impact,and report on some recent developments in the adoption of this technique. Weaim to dispel some misconceptions that we contend have impeded the use of ADwithin the machine learning community.
arxiv-1502-05890 | Efficient Contextual Semi-Bandit Learning |  http://arxiv.org/abs/1502.05890  | author:Akshay Krishnamurthy, Alekh Agarwal, Miroslav Dudik category:cs.LG stat.ML published:2015-02-20 summary:We study a variant of the contextual bandit problem, where on each round, thelearner plays a sequence of actions, receives a feature for each individualaction, and reward that is linearly related to these features. This setting hasapplications to network routing, crowd-sourcing, personalized search, and manyother domains. If the linear transformation is known, we analyze an algorithmthat is structurally similar to the algorithm of Agarwal et a. [2014] and showthat it enjoys a regret bound between $\tilde{O}(\sqrt{KLT \ln N})$ and$\tilde{O}(L\sqrt{KT \ln N})$, where $K$ is the number of actions, $L$ is thelength of each action sequence, $T$ is the number of rounds, and $N$ is thenumber of policies. If the linear transformation is unknown, we show that analgorithm that first explores to learn the unknown weights via linearregression and thereafter uses the estimated weights can achieve$\tilde{O}(\w\_1(KT)^{3/4} \sqrt{\ln N})$ regret, where $w$ is the true(unknown) weight vector. Both algorithms use an optimization oracle to avoidexplicit enumeration of the policies and consequently are computationallyefficient whenever an efficient algorithm for the fully supervised setting isavailable.
arxiv-1502-05742 | Application of Independent Component Analysis Techniques in Speckle Noise Reduction of Retinal OCT Images |  http://arxiv.org/abs/1502.05742  | author:Ahmadreza Baghaie, Roshan M. D'souza, Zeyun Yu category:cs.CV published:2015-02-19 summary:Optical Coherence Tomography (OCT) is an emerging technique in the field ofbiomedical imaging, with applications in ophthalmology, dermatology, coronaryimaging etc. OCT images usually suffer from a granular pattern, called specklenoise, which restricts the process of interpretation. Therefore the need forspeckle noise reduction techniques is of high importance. To the best of ourknowledge, use of Independent Component Analysis (ICA) techniques has neverbeen explored for speckle reduction of OCT images. Here, a comparative study ofseveral ICA techniques (InfoMax, JADE, FastICA and SOBI) is provided for noisereduction of retinal OCT images. Having multiple B-scans of the same location,the eye movements are compensated using a rigid registration technique. Then,different ICA techniques are applied to the aggregated set of B-scans forextracting the noise-free image. Signal-to-Noise-Ratio (SNR),Contrast-to-Noise-Ratio (CNR) and Equivalent-Number-of-Looks (ENL), as well asanalysis on the computational complexity of the methods, are considered asmetrics for comparison. The results show that use of ICA can be beneficial,especially in case of having fewer number of B-scans.
arxiv-1502-05503 | Classification and Bayesian Optimization for Likelihood-Free Inference |  http://arxiv.org/abs/1502.05503  | author:Michael U. Gutmann, Jukka Corander, Ritabrata Dutta, Samuel Kaski category:stat.CO stat.ME stat.ML published:2015-02-19 summary:Some statistical models are specified via a data generating process for whichthe likelihood function cannot be computed in closed form. Standardlikelihood-based inference is then not feasible but the model parameters can beinferred by finding the values which yield simulated data that resemble theobserved data. This approach faces at least two major difficulties: The firstdifficulty is the choice of the discrepancy measure which is used to judgewhether the simulated data resemble the observed data. The second difficulty isthe computationally efficient identification of regions in the parameter spacewhere the discrepancy is low. We give here an introduction to our recent workwhere we tackle the two difficulties through classification and Bayesianoptimization.
arxiv-1502-05534 | NeuroSVM: A Graphical User Interface for Identification of Liver Patients |  http://arxiv.org/abs/1502.05534  | author:Kalyan Nagaraj, Amulyashree Sridhar category:cs.LG cs.HC published:2015-02-19 summary:Diagnosis of liver infection at preliminary stage is important for bettertreatment. In todays scenario devices like sensors are used for detection ofinfections. Accurate classification techniques are required for automaticidentification of disease samples. In this context, this study utilizes datamining approaches for classification of liver patients from healthyindividuals. Four algorithms (Naive Bayes, Bagging, Random forest and SVM) wereimplemented for classification using R platform. Further to improve theaccuracy of classification a hybrid NeuroSVM model was developed using SVM andfeed-forward artificial neural network (ANN). The hybrid model was tested forits performance using statistical parameters like root mean square error (RMSE)and mean absolute percentage error (MAPE). The model resulted in a predictionaccuracy of 98.83%. The results suggested that development of hybrid modelimproved the accuracy of prediction. To serve the medicinal community forprediction of liver disease among patients, a graphical user interface (GUI)has been developed using R. The GUI is deployed as a package in localrepository of R platform for users to perform prediction.
arxiv-1502-05556 | Robust Active Ranking from Sparse Noisy Comparisons |  http://arxiv.org/abs/1502.05556  | author:Lucas Maystre, Matthias Grossglauser category:stat.ML cs.LG published:2015-02-19 summary:From sporting events to sociological surveys, ranking from pairwisecomparisons is a tool of choice for many applications. When certain pairs ofitems are difficult to compare, outcomes can be noisy, and it is necessary todevelop robust strategies. In this work, we show how a simple active samplingscheme that uses a standard black box sorting algorithm enables the efficientrecovery of the ranking, achieving low error with sparse samples. Both intheory and practice, this active strategy performs systematically better thanselecting comparisons at random. As a detour, we show a link between RankCentrality, a recently proposed algorithm for rank aggregation, and the MLestimator for the Bradley-Terry model. This enables us to develop a new,provably convergent iterative algorithm for computing the ML estimate.
arxiv-1502-05565 | Multi-valued Color Representation Based on Frank t-norm Properties |  http://arxiv.org/abs/1502.05565  | author:Vasile Patrascu category:cs.CV published:2015-02-19 summary:In this paper two knowledge representation models are proposed, FP4 and FP6.Both combine ideas from fuzzy sets and four-valued and hexa-valued logics. Bothrepresent imprecise properties whose accomplished degree is unknown orcontradictory for some objects. A possible application in the color analysisand color image processing is discussed.
arxiv-1502-05571 | Finding Dantzig selectors with a proximity operator based fixed-point algorithm |  http://arxiv.org/abs/1502.05571  | author:Ashley Prater, Lixin Shen, Bruce W. Suter category:math.NA stat.ML published:2015-02-19 summary:In this paper, we study a simple iterative method for finding the Dantzigselector, which was designed for linear regression problems. The methodconsists of two main stages. The first stage is to approximate the Dantzigselector through a fixed-point formulation of solutions to the Dantzig selectorproblem. The second stage is to construct a new estimator by regressing dataonto the support of the approximated Dantzig selector. We compare our method toan alternating direction method, and present the results of numericalsimulations using both the proposed method and the alternating direction methodon synthetic and real data sets. The numerical simulations demonstrate that thetwo methods produce results of similar quality, however the proposed methodtends to be significantly faster.
arxiv-1502-05675 | NP-Hardness and Inapproximability of Sparse PCA |  http://arxiv.org/abs/1502.05675  | author:Malik Magdon-Ismail category:cs.LG cs.CC cs.DS math.CO stat.ML published:2015-02-19 summary:We give a reduction from {\sc clique} to establish that sparse PCA isNP-hard. The reduction has a gap which we use to exclude an FPTAS for sparsePCA (unless P=NP). Under weaker complexity assumptions, we also excludepolynomial constant-factor approximation algorithms.
arxiv-1502-05752 | Pairwise Constraint Propagation: A Survey |  http://arxiv.org/abs/1502.05752  | author:Zhenyong Fu, Zhiwu Lu category:cs.CV cs.LG stat.ML published:2015-02-19 summary:As one of the most important types of (weaker) supervised information inmachine learning and pattern recognition, pairwise constraint, which specifieswhether a pair of data points occur together, has recently received significantattention, especially the problem of pairwise constraint propagation. At leasttwo reasons account for this trend: the first is that compared to the datalabel, pairwise constraints are more general and easily to collect, and thesecond is that since the available pairwise constraints are usually limited,the constraint propagation problem is thus important. This paper provides an up-to-date critical survey of pairwise constraintpropagation research. There are two underlying motivations for us to write thissurvey paper: the first is to provide an up-to-date review of the existingliterature, and the second is to offer some insights into the studies ofpairwise constraint propagation. To provide a comprehensive survey, we not onlycategorize existing propagation techniques but also present detaileddescriptions of representative methods within each category.
arxiv-1502-05461 | Visualizing Object Detection Features |  http://arxiv.org/abs/1502.05461  | author:Carl Vondrick, Aditya Khosla, Hamed Pirsiavash, Tomasz Malisiewicz, Antonio Torralba category:cs.CV published:2015-02-19 summary:We introduce algorithms to visualize feature spaces used by object detectors.Our method works by inverting a visual feature back to multiple natural images.We found that these visualizations allow us to analyze object detection systemsin new ways and gain new insight into the detector's failures. For example,when we visualize the features for high scoring false alarms, we discoveredthat, although they are clearly wrong in image space, they do look deceptivelysimilar to true positives in feature space. This result suggests that many ofthese false alarms are caused by our choice of feature space, and supports thatcreating a better learning algorithm or building bigger datasets is unlikely tocorrect these errors. By visualizing feature spaces, we can gain a moreintuitive understanding of recognition systems.
arxiv-1502-05680 | Finding One Community in a Sparse Graph |  http://arxiv.org/abs/1502.05680  | author:Andrea Montanari category:stat.ML cs.SI published:2015-02-19 summary:We consider a random sparse graph with bounded average degree, in which asubset of vertices has higher connectivity than the background. In particular,the average degree inside this subset of vertices is larger than outside (butstill bounded). Given a realization of such graph, we aim at identifying thehidden subset of vertices. This can be regarded as a model for the problem offinding a tightly knitted community in a social network, or a cluster in arelational dataset. In this paper we present two sets of contributions: $(i)$ We use the cavitymethod from spin glass theory to derive an exact phase diagram for thereconstruction problem. In particular, as the difference in edge probabilityincreases, the problem undergoes two phase transitions, a static phasetransition and a dynamic one. $(ii)$ We establish rigorous bounds on thedynamic phase transition and prove that, above a certain threshold, a localalgorithm (belief propagation) correctly identify most of the hidden set. Belowthe same threshold \emph{no local algorithm} can achieve this goal. However, inthis regime the subset can be identified by exhaustive search. For small hidden sets and large average degree, the phase transition forlocal algorithms takes an intriguingly simple form. Local algorithms succeedwith high probability for ${\rm deg}_{\rm in} - {\rm deg}_{\rm out} >\sqrt{{\rm deg}_{\rm out}/e}$ and fail for ${\rm deg}_{\rm in} - {\rm deg}_{\rmout} < \sqrt{{\rm deg}_{\rm out}/e}$ (with ${\rm deg}_{\rm in}$, ${\rmdeg}_{\rm out}$ the average degrees inside and outside the community). We arguethat spectral algorithms are also ineffective in the latter regime. It is an open problem whether any polynomial time algorithms might succeedfor ${\rm deg}_{\rm in} - {\rm deg}_{\rm out} < \sqrt{{\rm deg}_{\rm out}/e}$.
arxiv-1502-05700 | Scalable Bayesian Optimization Using Deep Neural Networks |  http://arxiv.org/abs/1502.05700  | author:Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Md. Mostofa Ali Patwary, Prabhat, Ryan P. Adams category:stat.ML published:2015-02-19 summary:Bayesian optimization is an effective methodology for the global optimizationof functions with expensive evaluations. It relies on querying a distributionover functions defined by a relatively cheap surrogate model. An accurate modelfor this distribution over functions is critical to the effectiveness of theapproach, and is typically fit using Gaussian processes (GPs). However, sinceGPs scale cubically with the number of observations, it has been challenging tohandle objectives whose optimization requires many evaluations, and as such,massively parallelizing the optimization. In this work, we explore the use of neural networks as an alternative to GPsto model distributions over functions. We show that performing adaptive basisfunction regression with a neural network as the parametric form performscompetitively with state-of-the-art GP-based approaches, but scales linearlywith the number of data rather than cubically. This allows us to achieve apreviously intractable degree of parallelism, which we apply to large scalehyperparameter optimization, rapidly finding competitive models on benchmarkobject recognition tasks using convolutional networks, and image captiongeneration using neural language models.
arxiv-1502-05577 | Adaptive system optimization using random directions stochastic approximation |  http://arxiv.org/abs/1502.05577  | author:Prashanth L. A., Shalabh Bhatnagar, Michael Fu, Steve Marcus category:math.OC cs.LG published:2015-02-19 summary:We present novel algorithms for simulation optimization using randomdirections stochastic approximation (RDSA). These include first-order(gradient) as well as second-order (Newton) schemes. We incorporate bothcontinuous-valued as well as discrete-valued perturbations into both ouralgorithms. The former are chosen to be independent and identically distributed(i.i.d.) symmetric, uniformly distributed random variables (r.v.), while thelatter are i.i.d., asymmetric, Bernoulli r.v.s. Our Newton algorithm, with anovel Hessian estimation scheme, requires N-dimensional perturbations and threeloss measurements per iteration, whereas the simultaneous perturbation Newtonsearch algorithm of [1] requires 2N-dimensional perturbations and four lossmeasurements per iteration. We prove the unbiasedness of both gradient andHessian estimates and asymptotic (strong) convergence for both first-order andsecond-order schemes. We also provide asymptotic normality results, which inparticular establish that the asymmetric Bernoulli variant of Newton RDSAmethod is better than 2SPSA of [1]. Numerical experiments are used to validatethe theoretical results.
arxiv-1502-05744 | Scale-Free Algorithms for Online Linear Optimization |  http://arxiv.org/abs/1502.05744  | author:Francesco Orabona, David Pal category:cs.LG math.OC published:2015-02-19 summary:We design algorithms for online linear optimization that have optimal regretand at the same time do not need to know any upper or lower bounds on the normof the loss vectors. We achieve adaptiveness to norms of loss vectors by scaleinvariance, i.e., our algorithms make exactly the same decisions if thesequence of loss vectors is multiplied by any positive constant. Our algorithmswork for any decision set, bounded or unbounded. For unbounded decisions sets,these are the first truly adaptive algorithms for online linear optimization.
arxiv-1502-05472 | On the Effects of Low-Quality Training Data on Information Extraction from Clinical Reports |  http://arxiv.org/abs/1502.05472  | author:Diego Marcheggiani, Fabrizio Sebastiani category:cs.LG cs.CL cs.IR published:2015-02-19 summary:In the last five years there has been a flurry of work on informationextraction from clinical documents, i.e., on algorithms capable of extracting,from the informal and unstructured texts that are generated during everydayclinical practice, mentions of concepts relevant to such practice. Most of thisliterature is about methods based on supervised learning, i.e., methods fortraining an information extraction system from manually annotated examples.While a lot of work has been devoted to devising learning methods that generatemore and more accurate information extractors, no work has been devoted toinvestigating the effect of the quality of training data on the learningprocess. Low quality in training data often derives from the fact that theperson who has annotated the data is different from the one against whosejudgment the automatically annotated data must be evaluated. In this paper wetest the impact of such data quality issues on the accuracy of informationextraction systems as applied to the clinical domain. We do this by comparingthe accuracy deriving from training data annotated by the authoritative coder(i.e., the one who has also annotated the test data, and by whose judgment wemust abide), with the accuracy deriving from training data annotated by adifferent coder. The results indicate that, although the disagreement betweenthe two coders (as measured on the training set) is substantial, the differenceis (surprisingly enough) not always statistically significant.
arxiv-1502-05698 | Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks |  http://arxiv.org/abs/1502.05698 | author:Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M. Rush, Bart van Merriënboer, Armand Joulin, Tomas Mikolov category:cs.AI cs.CL stat.ML published:2015-02-19 summary:One long-term goal of machine learning research is to produce methods thatare applicable to reasoning and natural language, in particular building anintelligent dialogue agent. To measure progress towards that goal, we argue forthe usefulness of a set of proxy tasks that evaluate reading comprehension viaquestion answering. Our tasks measure understanding in several ways: whether asystem is able to answer questions via chaining facts, simple induction,deduction and many more. The tasks are designed to be prerequisites for anysystem that aims to be capable of conversing with a human. We believe manyexisting learning systems can currently not solve them, and hence our aim is toclassify these tasks into skill sets, so that researchers can identify (andthen rectify) the failings of their systems. We also extend and improve therecently introduced Memory Networks model, and show it is able to solve some,but not all, of the tasks.
arxiv-1502-05696 | Approval Voting and Incentives in Crowdsourcing |  http://arxiv.org/abs/1502.05696  | author:Nihar B. Shah, Dengyong Zhou, Yuval Peres category:cs.GT cs.AI cs.LG cs.MA published:2015-02-19 summary:The growing need for labeled training data has made crowdsourcing animportant part of machine learning. The quality of crowdsourced labels is,however, adversely affected by three factors: (1) the workers are not experts;(2) the incentives of the workers are not aligned with those of the requesters;and (3) the interface does not allow workers to convey their knowledgeaccurately, by forcing them to make a single choice among a set of options. Inthis paper, we address these issues by introducing approval voting to utilizethe expertise of workers who have partial knowledge of the true answer, andcoupling it with a ("strictly proper") incentive-compatible compensationmechanism. We show rigorous theoretical guarantees of optimality of ourmechanism together with a simple axiomatic characterization. We also conductpreliminary empirical studies on Amazon Mechanical Turk which validate ourapproach.
arxiv-1502-05678 | VIP: Finding Important People in Images |  http://arxiv.org/abs/1502.05678  | author:Clint Solomon Mathialagan, Andrew C. Gallagher, Dhruv Batra category:cs.CV published:2015-02-19 summary:People preserve memories of events such as birthdays, weddings, or vacationsby capturing photos, often depicting groups of people. Invariably, someindividuals in the image are more important than others given the context ofthe event. This paper analyzes the concept of the importance of individuals ingroup photographs. We address two specific questions -- Given an image, who arethe most important individuals in it? Given multiple images of a person, whichimage depicts the person in the most important role? We introduce a measure ofimportance of people in images and investigate the correlation betweenimportance and visual saliency. We find that not only can we automaticallypredict the importance of people from purely visual cues, incorporating thispredicted importance results in significant improvement in applications such asim2text (generating sentences that describe images of groups of people).
arxiv-1502-05477 | Trust Region Policy Optimization |  http://arxiv.org/abs/1502.05477  | author:John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel category:cs.LG published:2015-02-19 summary:In this article, we describe a method for optimizing control policies, withguaranteed monotonic improvement. By making several approximations to thetheoretically-justified scheme, we develop a practical algorithm, called TrustRegion Policy Optimization (TRPO). This algorithm is effective for optimizinglarge nonlinear policies such as neural networks. Our experiments demonstrateits robust performance on a wide variety of tasks: learning simulated roboticswimming, hopping, and walking gaits; and playing Atari games using images ofthe screen as input. Despite its approximations that deviate from the theory,TRPO tends to give monotonic improvement, with little tuning ofhyperparameters.
arxiv-1502-05491 | Optimizing Text Quantifiers for Multivariate Loss Functions |  http://arxiv.org/abs/1502.05491  | author:Andrea Esuli, Fabrizio Sebastiani category:cs.LG cs.IR published:2015-02-19 summary:We address the problem of \emph{quantification}, a supervised learning taskwhose goal is, given a class, to estimate the relative frequency (or\emph{prevalence}) of the class in a dataset of unlabelled items.Quantification has several applications in data and text mining, such asestimating the prevalence of positive reviews in a set of reviews of a givenproduct, or estimating the prevalence of a given support issue in a dataset oftranscripts of phone calls to tech support. So far, quantification has beenaddressed by learning a general-purpose classifier, counting the unlabelleditems which have been assigned the class, and tuning the obtained countsaccording to some heuristics. In this paper we depart from the tradition ofusing general-purpose classifiers, and use instead a supervised learning modelfor \emph{structured prediction}, capable of generating classifiers directlyoptimized for the (multivariate and non-linear) function used for evaluatingquantification accuracy. The experiments that we have run on 5500 binaryhigh-dimensional datasets (averaging more than 14,000 documents each) show thatthis method is more accurate, more stable, and more efficient than existing,state-of-the-art quantification methods.
arxiv-1502-05689 | Unsupervised Network Pretraining via Encoding Human Design |  http://arxiv.org/abs/1502.05689  | author:Ming-Yu Liu, Arun Mallya, Oncel C. Tuzel, Xi Chen category:cs.CV published:2015-02-19 summary:Over the years, computer vision researchers have spent an immense amount ofeffort on designing image features for the visual object recognition task. Wepropose to incorporate this valuable experience to guide the task of trainingdeep neural networks. Our idea is to pretrain the network through the task ofreplicating the process of hand-designed feature extraction. By learning toreplicate the process, the neural network integrates previous researchknowledge and learns to model visual objects in a way similar to thehand-designed features. In the succeeding finetuning step, it further learnsobject-specific representations from labeled data and this boosts itsclassification power. We pretrain two convolutional neural networks where onereplicates the process of histogram of oriented gradients feature extraction,and the other replicates the process of region covariance feature extraction.After finetuning, we achieve substantially better performance than the baselinemethods.
arxiv-1502-05441 | Rule-and Dictionary-based Solution for Variations in Written Arabic Names in Social Networks, Big Data, Accounting Systems and Large Databases |  http://arxiv.org/abs/1502.05441  | author:Ahmad B. A. Hassanat, Ghada Awad Altarawneh category:cs.DB cs.CL cs.IR published:2015-02-18 summary:This paper investigates the problem that some Arabic names can be written inmultiple ways. When someone searches for only one form of a name, neither exactnor approximate matching is appropriate for returning the multiple variants ofthe name. Exact matching requires the user to enter all forms of the name forthe search, and approximate matching yields names not among the variations ofthe one being sought. In this paper, we attempt to solve the problem with adictionary of all Arabic names mapped to their different (alternative) writingforms. We generated alternatives based on rules we derived from reviewing thefirst names of 9.9 million citizens and former citizens of Jordan. Thisdictionary can be used for both standardizing the written form when inserting anew name into a database and for searching for the name and all its alternativewritten forms. Creating the dictionary automatically based on rules resulted inat least 7% erroneous acceptance errors and 7.9% erroneous rejection errors. Weaddressed the errors by manually editing the dictionary. The dictionary can beof help to real world-databases, with the qualification that manual editingdoes not guarantee 100% correctness.
arxiv-1502-05111 | CSAL: Self-adaptive Labeling based Clustering Integrating Supervised Learning on Unlabeled Data |  http://arxiv.org/abs/1502.05111  | author:Fangfang Li, Guandong Xu, Longbing Cao category:cs.LG published:2015-02-18 summary:Supervised classification approaches can predict labels for unknown databecause of the supervised training process. The success of classification isheavily dependent on the labeled training data. Differently, clustering iseffective in revealing the aggregation property of unlabeled data, but theperformance of most clustering methods is limited by the absence of labeleddata. In real applications, however, it is time-consuming and sometimesimpossible to obtain labeled data. The combination of clustering andclassification is a promising and active approach which can largely improve theperformance. In this paper, we propose an innovative and effective clusteringframework based on self-adaptive labeling (CSAL) which integrates clusteringand classification on unlabeled data. Clustering is first employed to partitiondata and a certain proportion of clustered data are selected by our proposedlabeling approach for training classifiers. In order to refine the trainedclassifiers, an iterative process of Expectation-Maximization algorithm isdevised into the proposed clustering framework CSAL. Experiments are conductedon publicly data sets to test different combinations of clustering algorithmsand classification models as well as various training data labeling methods.The experimental results show that our approach along with the self-adaptivemethod outperforms other methods.
arxiv-1502-05090 | Real time clustering of time series using triangular potentials |  http://arxiv.org/abs/1502.05090  | author:Aldo Pacchiano, Oliver Williams category:cs.LG published:2015-02-18 summary:Motivated by the problem of computing investment portfolio weightings weinvestigate various methods of clustering as alternatives to traditionalmean-variance approaches. Such methods can have significant benefits from apractical point of view since they remove the need to invert a samplecovariance matrix, which can suffer from estimation error and will almostcertainly be non-stationary. The general idea is to find groups of assets whichshare similar return characteristics over time and treat each group as a singlecomposite asset. We then apply inverse volatility weightings to these newcomposite assets. In the course of our investigation we devise a method ofclustering based on triangular potentials and we present associated theoreticalresults as well as various examples based on synthetic data.
arxiv-1502-05137 | Prediction of Search Targets From Fixations in Open-World Settings |  http://arxiv.org/abs/1502.05137  | author:Hosnieh Sattar, Sabine Müller, Mario Fritz, Andreas Bulling category:cs.CV published:2015-02-18 summary:Previous work on predicting the target of visual search from human fixationsonly considered closed-world settings in which training labels are availableand predictions are performed for a known set of potential targets. In thiswork we go beyond the state of the art by studying search target prediction inan open-world setting in which we no longer assume that we have fixation datato train for the search targets. We present a dataset containing fixation dataof 18 users searching for natural images from three image categories withinsynthesised image collages of about 80 images. In a closed-world baselineexperiment we show that we can predict the correct target image out of acandidate set of five images. We then present a new problem formulation forsearch target prediction in the open-world setting that is based on learningcompatibilities between fixations and potential targets.
arxiv-1502-05435 | Fusion of Image Segmentation Algorithms using Consensus Clustering |  http://arxiv.org/abs/1502.05435  | author:Mete Ozay, Fatos T. Yarman Vural, Sanjeev R. Kulkarni, H. Vincent Poor category:cs.CV published:2015-02-18 summary:A new segmentation fusion method is proposed that ensembles the output ofseveral segmentation algorithms applied on a remotely sensed image. Thecandidate segmentation sets are processed to achieve a consensus segmentationusing a stochastic optimization algorithm based on the Filtered Stochastic BOEM(Best One Element Move) method. For this purpose, Filtered Stochastic BOEM isreformulated as a segmentation fusion problem by designing a new distancelearning approach. The proposed algorithm also embeds the computation of theoptimum number of clusters into the segmentation fusion problem.
arxiv-1502-05197 | Analysis and approximation of some Shape-from-Shading models for non-Lambertian surfaces |  http://arxiv.org/abs/1502.05197  | author:Silvia Tozza, Maurizio Falcone category:math.NA cs.CV cs.NA math.AP published:2015-02-18 summary:The reconstruction of a 3D object or a scene is a classical inverse problemin Computer Vision. In the case of a single image this is called theShape-from-Shading (SfS) problem and it is known to be ill-posed even in asimplified version like the vertical light source case. A huge number of worksdeals with the orthographic SfS problem based on the Lambertian reflectancemodel, the most common and simplest model which leads to an eikonal typeequation when the light source is on the vertical axis. In this paper we wantto study non-Lambertian models since they are more realistic and suitablewhenever one has to deal with different kind of surfaces, rough or specular. Wewill present a unified mathematical formulation of some popular orthographicnon-Lambertian models, considering vertical and oblique light directions aswell as different viewer positions. These models lead to more complexstationary nonlinear partial differential equations of Hamilton-Jacobi typewhich can be regarded as the generalization of the classical eikonal equationcorresponding to the Lambertian case. However, all the equations correspondingto the models considered here (Oren-Nayar and Phong) have a similar structureso we can look for weak solutions to this class in the viscosity solutionframework. Via this unified approach, we are able to develop a semi-Lagrangianapproximation scheme for the Oren-Nayar and the Phong model and to prove ageneral convergence result. Numerical simulations on synthetic and real imageswill illustrate the effectiveness of this approach and the main features of thescheme, also comparing the results with previous results in the literature.
arxiv-1502-05375 | On learning k-parities with and without noise |  http://arxiv.org/abs/1502.05375  | author:Arnab Bhattacharyya, Ameet Gadekar, Ninad Rajgopal category:cs.DS cs.DM cs.LG published:2015-02-18 summary:We first consider the problem of learning $k$-parities in the on-linemistake-bound model: given a hidden vector $x \in \{0,1\}^n$ with $x=k$ and asequence of "questions" $a_1, a_2, ...\in \{0,1\}^n$, where the algorithm mustreply to each question with $< a_i, x> \pmod 2$, what is the best tradeoffbetween the number of mistakes made by the algorithm and its time complexity?We improve the previous best result of Buhrman et al. by an $\exp(k)$ factor inthe time complexity. Second, we consider the problem of learning $k$-parities in the presence ofclassification noise of rate $\eta \in (0,1/2)$. A polynomial time algorithmfor this problem (when $\eta > 0$ and $k = \omega(1)$) is a longstandingchallenge in learning theory. Grigorescu et al. showed an algorithm running intime ${n \choose k/2}^{1 + 4\eta^2 +o(1)}$. Note that this algorithm inherentlyrequires time ${n \choose k/2}$ even when the noise rate $\eta$ is polynomiallysmall. We observe that for sufficiently small noise rate, it is possible tobreak the $n \choose k/2$ barrier. In particular, if for some function $f(n) =\omega(1)$ and $\alpha \in [1/2, 1)$, $k = n/f(n)$ and $\eta = o(f(n)^{-\alpha}/\log n)$, then there is an algorithm for the problem with running time$poly(n)\cdot {n \choose k}^{1-\alpha} \cdot e^{-k/4.01}$.
arxiv-1502-05113 | Temporal Embedding in Convolutional Neural Networks for Robust Learning of Abstract Snippets |  http://arxiv.org/abs/1502.05113  | author:Jiajun Liu, Kun Zhao, Brano Kusy, Ji-rong Wen, Raja Jurdak category:cs.LG cs.NE published:2015-02-18 summary:The prediction of periodical time-series remains challenging due to varioustypes of data distortions and misalignments. Here, we propose a novel modelcalled Temporal embedding-enhanced convolutional neural Network (TeNet) tolearn repeatedly-occurring-yet-hidden structural elements in periodicaltime-series, called abstract snippets, for predicting future changes. Our modeluses convolutional neural networks and embeds a time-series with its potentialneighbors in the temporal domain for aligning it to the dominant patterns inthe dataset. The model is robust to distortions and misalignments in thetemporal domain and demonstrates strong prediction power for periodicaltime-series. We conduct extensive experiments and discover that the proposed model showssignificant and consistent advantages over existing methods on a variety ofdata modalities ranging from human mobility to household power consumptionrecords. Empirical results indicate that the model is robust to various factorssuch as number of samples, variance of data, numerical ranges of data etc. Theexperiments also verify that the intuition behind the model can be generalizedto multiple data types and applications and promises significant improvement inprediction performances across the datasets studied.
arxiv-1502-05241 | NEFI: Network Extraction From Images |  http://arxiv.org/abs/1502.05241  | author:Michael Dirnberger, Adrian Neumann, Tim Kehl category:cs.CV cs.SE published:2015-02-18 summary:Networks and network-like structures are amongst the central building blocksof many technological and biological systems. Given a mathematical graphrepresentation of a network, methods from graph theory enable a preciseinvestigation of its properties. Software for the analysis of graphs is widelyavailable and has been applied to graphs describing large scale networks suchas social networks, protein-interaction networks, etc. In these applications,graph acquisition, i.e., the extraction of a mathematical graph from a network,is relatively simple. However, for many network-like structures, e.g. leafvenations, slime molds and mud cracks, data collection relies on images wheregraph extraction requires domain-specific solutions or even manual. Here weintroduce Network Extraction From Images, NEFI, a software tool thatautomatically extracts accurate graphs from images of a wide range of networksoriginating in various domains. While there is previous work on graphextraction from images, theoretical results are fully accessible only to anexpert audience and ready-to-use implementations for non-experts are rarelyavailable or insufficiently documented. NEFI provides a novel platform allowingpractitioners from many disciplines to easily extract graph representationsfrom images by supplying flexible tools from image processing, computer visionand graph theory bundled in a convenient package. Thus, NEFI constitutes ascalable alternative to tedious and error-prone manual graph extraction andspecial purpose tools. We anticipate NEFI to enable the collection of largerdatasets by reducing the time spent on graph extraction. The analysis of thesenew datasets may open up the possibility to gain new insights into thestructure and function of various types of networks. NEFI is open source andavailable http://nefi.mpi-inf.mpg.de.
arxiv-1502-05312 | Predictive Entropy Search for Bayesian Optimization with Unknown Constraints |  http://arxiv.org/abs/1502.05312  | author:José Miguel Hernández-Lobato, Michael A. Gelbart, Matthew W. Hoffman, Ryan P. Adams, Zoubin Ghahramani category:stat.ML published:2015-02-18 summary:Unknown constraints arise in many types of expensive black-box optimizationproblems. Several methods have been proposed recently for performing Bayesianoptimization with constraints, based on the expected improvement (EI)heuristic. However, EI can lead to pathologies when used with constraints. Forexample, in the case of decoupled constraints---i.e., when one canindependently evaluate the objective or the constraints---EI can encounter apathology that prevents exploration. Additionally, computing EI requires acurrent best solution, which may not exist if none of the data collected so farsatisfy the constraints. By contrast, information-based approaches do notsuffer from these failure modes. In this paper, we present a newinformation-based method called Predictive Entropy Search with Constraints(PESC). We analyze the performance of PESC and show that it compares favorablyto EI-based approaches on synthetic and benchmark problems, as well as severalreal-world examples. We demonstrate that PESC is an effective algorithm thatprovides a promising direction towards a unified solution for constrainedBayesian optimization.
arxiv-1502-05336 | Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks |  http://arxiv.org/abs/1502.05336  | author:José Miguel Hernández-Lobato, Ryan P. Adams category:stat.ML published:2015-02-18 summary:Large multilayer neural networks trained with backpropagation have recentlyachieved state-of-the-art results in a wide range of problems. However, usingbackprop for neural net learning still has some disadvantages, e.g., having totune a large number of hyperparameters to the data, lack of calibratedprobabilistic predictions, and a tendency to overfit the training data. Inprinciple, the Bayesian approach to learning neural networks does not havethese problems. However, existing Bayesian techniques lack scalability to largedataset and network sizes. In this work we present a novel scalable method forlearning Bayesian neural networks, called probabilistic backpropagation (PBP).Similar to classical backpropagation, PBP works by computing a forwardpropagation of probabilities through the network and then doing a backwardcomputation of gradients. A series of experiments on ten real-world datasetsshow that PBP is significantly faster than other techniques, while offeringcompetitive predictive abilities. Our experiments also show that PBP providesaccurate estimates of the posterior variance on the network weights.
arxiv-1502-05213 | F0 Modeling In Hmm-Based Speech Synthesis System Using Deep Belief Network |  http://arxiv.org/abs/1502.05213  | author:Sankar Mukherjee, Shyamal Kumar Das Mandal category:cs.LG cs.NE published:2015-02-18 summary:In recent years multilayer perceptrons (MLPs) with many hid- den layers DeepNeural Network (DNN) has performed sur- prisingly well in many speech tasks,i.e. speech recognition, speaker verification, speech synthesis etc. Althoughin the context of F0 modeling these techniques has not been ex- ploitedproperly. In this paper, Deep Belief Network (DBN), a class of DNN family hasbeen employed and applied to model the F0 contour of synthesized speech whichwas generated by HMM-based speech synthesis system. The experiment was done onBengali language. Several DBN-DNN architectures ranging from four to sevenhidden layers and up to 200 hid- den units per hidden layer was presented andevaluated. The results were compared against clustering tree techniques pop-ularly found in statistical parametric speech synthesis. We show that fromtextual inputs DBN-DNN learns a high level structure which in turn improves F0contour in terms of ob- jective and subjective tests.
arxiv-1502-05167 | Dengue disease prediction using weka data mining tool |  http://arxiv.org/abs/1502.05167  | author:Kashish Ara Shakil, Shadma Anis, Mansaf Alam category:cs.CY cs.LG published:2015-02-18 summary:Dengue is a life threatening disease prevalent in several developed as wellas developing countries like India.In this paper we discuss various algorithmapproaches of data mining that have been utilized for dengue diseaseprediction. Data mining is a well known technique used by health organizationsfor classification of diseases such as dengue, diabetes and cancer inbioinformatics research. In the proposed approach we have used WEKA with 10cross validation to evaluate data and compare results. Weka has an extensivecollection of different machine learning and data mining algorithms. In thispaper we have firstly classified the dengue data set and then compared thedifferent data mining techniques in weka through Explorer, knowledge flow andExperimenter interfaces. Furthermore in order to validate our approach we haveused a dengue dataset with 108 instances but weka used 99 rows and 18attributes to determine the prediction of disease and their accuracy usingclassifications of different algorithms to find out the best performance. Themain objective of this paper is to classify data and assist the users inextracting useful information from data and easily identify a suitablealgorithm for accurate predictive model from it. From the findings of thispaper it can be concluded that Na\"ive Bayes and J48 are the best performancealgorithms for classified accuracy because they achieved maximum accuracy= 100%with 99 correctly classified instances, maximum ROC = 1, had least meanabsolute error and it took minimum time for building this model throughExplorer and Knowledge flow results
arxiv-1502-05212 | IAT - Image Annotation Tool: Manual |  http://arxiv.org/abs/1502.05212  | author:Gianluigi Ciocca, Paolo Napoletano, Raimondo Schettini category:cs.CV published:2015-02-18 summary:The annotation of image and video data of large datasets is a fundamentaltask in multimedia information retrieval and computer vision applications. Inorder to support the users during the image and video annotation process,several software tools have been developed to provide them with a graphicalenvironment which helps drawing object contours, handling tracking informationand specifying object metadata. Here we introduce a preliminary version of theimage annotation tools developed at the Imaging and Vision Laboratory.
arxiv-1502-05224 | Cross-Modality Hashing with Partial Correspondence |  http://arxiv.org/abs/1502.05224  | author:Yun Gu, Haoyang Xue, Jie Yang category:cs.CV published:2015-02-18 summary:Learning a hashing function for cross-media search is very desirable due toits low storage cost and fast query speed. However, the data crawled fromInternet cannot always guarantee good correspondence among different modalitieswhich affects the learning for hashing function. In this paper, we focus oncross-modal hashing with partially corresponded data. The data without fullcorrespondence are made in use to enhance the hashing performance. Theexperiments on Wiki and NUS-WIDE datasets demonstrates that the proposed methodoutperforms some state-of-the-art hashing approaches with fewer correspondenceinformation.
arxiv-1502-05134 | Supervised cross-modal factor analysis for multiple modal data classification |  http://arxiv.org/abs/1502.05134  | author:Jingbin Wang, Yihua Zhou, Kanghong Duan, Jim Jing-Yan Wang, Halima Bensmail category:cs.LG published:2015-02-18 summary:In this paper we study the problem of learning from multiple modal data forpurpose of document classification. In this problem, each document is composedtwo different modals of data, i.e., an image and a text. Cross-modal factoranalysis (CFA) has been proposed to project the two different modals of data toa shared data space, so that the classification of a image or a text can beperformed directly in this space. A disadvantage of CFA is that it has ignoredthe supervision information. In this paper, we improve CFA by incorporating thesupervision information to represent and classify both image and text modals ofdocuments. We project both image and text data to a shared data space by factoranalysis, and then train a class label predictor in the shared space to use theclass label information. The factor analysis parameter and the predictorparameter are learned jointly by solving one single objective function. Withthis objective function, we minimize the distance between the projections ofimage and text of the same document, and the classification error of theprojection measured by hinge loss function. The objective function is optimizedby an alternate optimization strategy in an iterative algorithm. Experiments intwo different multiple modal document data sets show the advantage of theproposed algorithm over other CFA methods.
arxiv-1502-05313 | Variational Optimization of Annealing Schedules |  http://arxiv.org/abs/1502.05313  | author:Taichi Kiwaki category:stat.ML published:2015-02-18 summary:Annealed importance sampling (AIS) is a common algorithm to estimatepartition functions of useful stochastic models. One important problem forobtaining accurate AIS estimates is the selection of an annealing schedule.Conventionally, an annealing schedule is often determined heuristically or issimply set as a linearly increasing sequence. In this paper, we propose analgorithm for the optimal schedule by deriving a functional that dominates theAIS estimation error and by numerically minimizing this functional. Weexperimentally demonstrate that the proposed algorithm mostly outperformsconventional scheduling schemes with large quantization numbers.
arxiv-1502-05056 | On Sex, Evolution, and the Multiplicative Weights Update Algorithm |  http://arxiv.org/abs/1502.05056  | author:Reshef Meir, David Parkes category:cs.LG cs.GT published:2015-02-17 summary:We consider a recent innovative theory by Chastain et al. on the role of sexin evolution [PNAS'14]. In short, the theory suggests that the evolutionaryprocess of gene recombination implements the celebrated multiplicative weightsupdates algorithm (MWUA). They prove that the population dynamics induced bysexual reproduction can be precisely modeled by genes that use MWUA as theirlearning strategy in a particular coordination game. The result holds in theenvironments of \emph{weak selection}, under the assumption that the populationfrequencies remain a product distribution. We revisit the theory, eliminating both the requirement of weak selection andany assumption on the distribution of the population. Removing the assumptionof product distributions is crucial, since as we show, this assumption isinconsistent with the population dynamics. We show that the marginal alleledistributions induced by the population dynamics precisely match the marginalsinduced by a multiplicative weights update algorithm in this general setting,thereby affirming and substantially generalizing these earlier results. We further revise the implications for convergence and utility or fitnessguarantees in coordination games. In contrast to the claim of Chastain etal.[PNAS'14], we conclude that the sexual evolutionary dynamics does not entailany property of the population distribution, beyond those already implied byconvergence.
arxiv-1502-04824 | Randomized LU decomposition: An Algorithm for Dictionaries Construction |  http://arxiv.org/abs/1502.04824  | author:Aviv Rotbart, Gil Shabat, Yaniv Shmueli, Amir Averbuch category:cs.CV I.5 published:2015-02-17 summary:In recent years, distinctive-dictionary construction has gained importancedue to his usefulness in data processing. Usually, one or more dictionaries areconstructed from a training data and then they are used to classify signalsthat did not participate in the training process. A new dictionary constructionalgorithm is introduced. It is based on a low-rank matrix factorization beingachieved by the application of the randomized LU decomposition to a trainingdata. This method is fast, scalable, parallelizable, consumes low memory,outperforms SVD in these categories and works also extremely well on largesparse matrices. In contrast to existing methods, the randomized LUdecomposition constructs an under-complete dictionary, which simplifies boththe construction and the classification processes of newly arrived signals. Thedictionary construction is generic and general that fits differentapplications. We demonstrate the capabilities of this algorithm for file typeidentification, which is a fundamental task in digital security arena,performed nowadays for example by sandboxing mechanism, deep packet inspection,firewalls and anti-virus systems. We propose a content-based method thatdetects file types that neither depend on file extension nor on metadata. Suchapproach is harder to deceive and we show that only a few file fragments from awhole file are needed for a successful classification. Based on the constructeddictionaries, we show that the proposed method can effectively identifyexecution code fragments in PDF files. $\textbf{Keywords.}$ Dictionary construction, classification, LUdecomposition, randomized LU decomposition, content-based file detection,computer security.
arxiv-1502-04972 | Measuring and Understanding Sensory Representations within Deep Networks Using a Numerical Optimization Framework |  http://arxiv.org/abs/1502.04972  | author:Chuan-Yung Tsai, David D. Cox category:cs.NE q-bio.NC published:2015-02-17 summary:A central challenge in sensory neuroscience is describing how the activity ofpopulations of neurons can represent useful features of the externalenvironment. However, while neurophysiologists have long been able to recordthe responses of neurons in awake, behaving animals, it is another matterentirely to say what a given neuron does. A key problem is that in many sensorydomains, the space of all possible stimuli that one might encounter iseffectively infinite; in vision, for instance, natural scenes arecombinatorially complex, and an organism will only encounter a tiny fraction ofpossible stimuli. As a result, even describing the response properties ofsensory neurons is difficult, and investigations of neuronal functions arealmost always critically limited by the number of stimuli that can beconsidered. In this paper, we propose a closed-loop, optimization-basedexperimental framework for characterizing the response properties of sensoryneurons, building on past efforts in closed-loop experimental methods, andleveraging recent advances in artificial neural networks to serve as as aproving ground for our techniques. Specifically, using deep convolutionalneural networks, we asked whether modern black-box optimization techniques canbe used to interrogate the "tuning landscape" of an artificial neuron in adeep, nonlinear system, without imposing significant constraints on the spaceof stimuli under consideration. We introduce a series of measures to quantifythe tuning landscapes, and show how these relate to the performances of thenetworks in an object recognition task. To the extent that deep convolutionalneural networks increasingly serve as de facto working hypotheses forbiological vision, we argue that developing a unified approach for studyingboth artificial and biological systems holds great potential to advance bothfields together.
arxiv-1502-04868 | Proper Complex Gaussian Processes for Regression |  http://arxiv.org/abs/1502.04868  | author:Rafael Boloix-Tortosa, F. Javier Payán-Somet, Eva Arias-de-Reyna, Juan José Murillo-Fuentes category:cs.LG stat.ML published:2015-02-17 summary:Complex-valued signals are used in the modeling of many systems inengineering and science, hence being of fundamental interest. Often, randomcomplex-valued signals are considered to be proper. A proper complex randomvariable or process is uncorrelated with its complex conjugate. This assumptionis a good model of the underlying physics in many problems, and simplifies thecomputations. While linear processing and neural networks have been widelystudied for these signals, the development of complex-valued nonlinear kernelapproaches remains an open problem. In this paper we propose Gaussian processesfor regression as a framework to develop 1) a solution for propercomplex-valued kernel regression and 2) the design of the reproducing kernelfor complex-valued inputs, using the convolutional approach forcross-covariances. In this design we pay attention to preserve, in the complexdomain, the measure of similarity between near inputs. The hyperparameters ofthe kernel are learned maximizing the marginal likelihood using Wirtingerderivatives. Besides, the approach is connected to the multiple output learningscenario. In the experiments included, we first solve a proper complex Gaussianprocess where the cross-covariance does not cancel, a challenging scenario whendealing with proper complex signals. Then we successfully use these novelresults to solve some problems previously proposed in the literature asbenchmarks, reporting a remarkable improvement in the estimation error.
arxiv-1502-04874 | Regret bounds for Narendra-Shapiro bandit algorithms |  http://arxiv.org/abs/1502.04874  | author:Sébastien Gadat, Fabien Panloup, Sofiane Saadane category:math.PR math.ST stat.ML stat.TH published:2015-02-17 summary:Narendra-Shapiro (NS) algorithms are bandit-type algorithms that have beenintroduced in the sixties (with a view to applications in Psychology orlearning automata), whose convergence has been intensively studied in thestochastic algorithm literature. In this paper, we adress the followingquestion: are the Narendra-Shapiro (NS) bandit algorithms competitive from a\textit{regret} point of view? In our main result, we show that somecompetitive bounds can be obtained for such algorithms in their penalizedversion (introduced in \cite{Lamberton_Pages}). More precisely, up to anover-penalization modification, the pseudo-regret $\bar{R}_n$ related to thepenalized two-armed bandit algorithm is uniformly bounded by $C \sqrt{n}$(where $C$ is made explicit in the paper). \noindent We also generalizeexisting convergence and rates of convergence results to the multi-armed caseof the over-penalized bandit algorithm, including the convergence toward theinvariant measure of a Piecewise Deterministic Markov Process (PDMP) after asuitable renormalization. Finally, ergodic properties of this PDMP are given inthe multi-armed case.
arxiv-1502-04983 | Context Tricks for Cheap Semantic Segmentation |  http://arxiv.org/abs/1502.04983  | author:Thanapong Intharah, Gabriel J. Brostow category:cs.CV published:2015-02-17 summary:Accurate semantic labeling of image pixels is difficult because intra-classvariability is often greater than inter-class variability. In turn, fastsemantic segmentation is hard because accurate models are usually toocomplicated to also run quickly at test-time. Our experience with building andrunning semantic segmentation systems has also shown a reasonably obviousbottleneck on model complexity, imposed by small training datasets. Wetherefore propose two simple complementary strategies that leverage context togive better semantic segmentation, while scaling up or down to train ondifferent-sized datasets. As easy modifications for existing semantic segmentation algorithms, weintroduce Decorrelated Semantic Texton Forests, and the Context Sensitive ImageLevel Prior. The proposed modifications are tested using a Semantic TextonForest (STF) system, and the modifications are validated on two standardbenchmark datasets, MSRC-21 and PascalVOC-2010. In Python based comparisons,our system is insignificantly slower than STF at test-time, yet producessuperior semantic segmentations overall, with just push-button training.
arxiv-1502-04956 | The Linearization of Pairwise Markov Networks |  http://arxiv.org/abs/1502.04956  | author:Wolfgang Gatterbauer category:cs.AI cs.LG cs.SI published:2015-02-17 summary:Belief Propagation (BP) allows to approximate exact probabilistic inferencein graphical models, such as Markov networks (also called Markov random fields,or undirected graphical models). However, no exact convergence guarantees forBP are known, in general. Recent work has proposed to approximate BP bylinearizing the update equations around default values for the special casewhen all edges in the Markov network carry the same symmetric, doublystochastic potential. This linearization has led to exact convergenceguarantees, considerable speed-up, while maintaining high quality results innetwork-based classification (i.e. when we only care about the most likelylabel or class for each node and not the exact probabilities). The presentpaper generalizes our prior work on Linearized Belief Propagation (LinBP) withan approach that approximates Loopy Belief Propagation on any pairwise Markovnetwork with the problem of solving a linear equation system.
arxiv-1502-04754 | 3D Pose from Detections |  http://arxiv.org/abs/1502.04754  | author:Cosimo Rubino, Marco Crocco, Alessandro Perina, Vittorio Murino, Alessio Del Bue category:cs.CV published:2015-02-17 summary:We present a novel method to infer, in closed-form, a general 3D spatialoccupancy and orientation of a collection of rigid objects given 2D imagedetections from a sequence of images. In particular, starting from 2D ellipsesfitted to bounding boxes, this novel multi-view problem can be reformulated asthe estimation of a quadric (ellipsoid) in 3D. We show that an efficientsolution exists in the dual-space using a minimum of three views while asolution with two views is possible through the use of regularization. However,this algebraic solution can be negatively affected in the presence of grossinaccuracies in the bounding boxes estimation. To this end, we also propose arobust ellipse fitting algorithm able to improve performance in the presence oferrors in the detected objects. Results on synthetic tests and on differentreal datasets, involving real challenging scenarios, demonstrate theapplicability and potential of our method.
arxiv-1502-04981 | Semi-supervised Segmentation Fusion of Multi-spectral and Aerial Images |  http://arxiv.org/abs/1502.04981  | author:Mete Ozay category:cs.CV published:2015-02-17 summary:A Semi-supervised Segmentation Fusion algorithm is proposed using consensusand distributed learning. The aim of Unsupervised Segmentation Fusion (USF) isto achieve a consensus among different segmentation outputs obtained fromdifferent segmentation algorithms by computing an approximate solution to theNP problem with less computational complexity. Semi-supervision is incorporatedin USF using a new algorithm called Semi-supervised Segmentation Fusion (SSSF).In SSSF, side information about the co-occurrence of pixels in the same ordifferent segments is formulated as the constraints of a convex optimizationproblem. The results of the experiments employed on artificial and real-worldbenchmark multi-spectral and aerial images show that the proposed algorithmsperform better than the individual state-of-the art segmentation algorithms.
arxiv-1502-05023 | A New Sampling Technique for Tensors |  http://arxiv.org/abs/1502.05023  | author:Srinadh Bhojanapalli, Sujay Sanghavi category:stat.ML cs.DS cs.IT cs.LG math.IT published:2015-02-17 summary:In this paper we propose new techniques to sample arbitrary third-ordertensors, with an objective of speeding up tensor algorithms that have recentlygained popularity in machine learning. Our main contribution is a new way toselect, in a biased random way, only $O(n^{1.5}/\epsilon^2)$ of the possible$n^3$ elements while still achieving each of the three goals: \\ {\em (a)tensor sparsification}: for a tensor that has to be formed from arbitrarysamples, compute very few elements to get a good spectral approximation, andfor arbitrary orthogonal tensors {\em (b) tensor completion:} recover anexactly low-rank tensor from a small number of samples via alternating leastsquares, or {\em (c) tensor factorization:} approximating factors of a low-ranktensor corrupted by noise. \\ Our sampling can be used along with existingtensor-based algorithms to speed them up, removing the computational bottleneckin these methods.
arxiv-1502-04938 | A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena |  http://arxiv.org/abs/1502.04938  | author:Arianna Bisazza, Marcello Federico category:cs.CL published:2015-02-17 summary:Word reordering is one of the most difficult aspects of statistical machinetranslation (SMT), and an important factor of its quality and efficiency.Despite the vast amount of research published to date, the interest of thecommunity in this problem has not decreased, and no single method appears to bestrongly dominant across language pairs. Instead, the choice of the optimalapproach for a new translation task still seems to be mostly driven byempirical trials. To orientate the reader in this vast and complex researcharea, we present a comprehensive survey of word reordering viewed as astatistical modeling challenge and as a natural language phenomenon. The surveydescribes in detail how word reordering is modeled within differentstring-based and tree-based SMT frameworks and as a stand-alone task, includingsystematic overviews of the literature in advanced reordering modeling. We thenquestion why some approaches are more successful than others in differentlanguage pairs. We argue that, besides measuring the amount of reordering, itis important to understand which kinds of reordering occur in a given languagepair. To this end, we conduct a qualitative analysis of word reorderingphenomena in a diverse sample of language pairs, based on a large collection oflinguistic knowledge. Empirical results in the SMT literature are shown tosupport the hypothesis that a few linguistic facts can be very useful toanticipate the reordering characteristics of a language pair and to select theSMT framework that best suits them.
arxiv-1502-05082 | What makes for effective detection proposals? |  http://arxiv.org/abs/1502.05082  | author:Jan Hosang, Rodrigo Benenson, Piotr Dollár, Bernt Schiele category:cs.CV published:2015-02-17 summary:Current top performing object detectors employ detection proposals to guidethe search for objects, thereby avoiding exhaustive sliding window searchacross images. Despite the popularity and widespread use of detectionproposals, it is unclear which trade-offs are made when using them duringobject detection. We provide an in-depth analysis of twelve proposal methodsalong with four baselines regarding proposal repeatability, ground truthannotation recall on PASCAL, ImageNet, and MS COCO, and their impact on DPM,R-CNN, and Fast R-CNN detection performance. Our analysis shows that for objectdetection improving proposal localisation accuracy is as important as improvingrecall. We introduce a novel metric, the average recall (AR), which rewardsboth high recall and good localisation and correlates surprisingly well withdetection performance. Our findings show common strengths and weaknesses ofexisting methods, and provide insights and metrics for selecting and tuningproposal methods.
arxiv-1502-05243 | SA-CNN: Dynamic Scene Classification using Convolutional Neural Networks |  http://arxiv.org/abs/1502.05243  | author:Aalok Gangopadhyay, Shivam Mani Tripathi, Ishan Jindal, Shanmuganathan Raman category:cs.CV I.5.4; I.4.8 published:2015-02-17 summary:The task of classifying videos of natural dynamic scenes into appropriateclasses has gained lot of attention in recent years. The problem especiallybecomes challenging when the camera used to capture the video is dynamic. Inthis paper, we analyse the performance of statistical aggregation (SA)techniques on various pre-trained convolutional neural network(CNN) models toaddress this problem. The proposed approach works by extracting CNN activationfeatures for a number of frames in a video and then uses an aggregation schemein order to obtain a robust feature descriptor for the video. We show throughresults that the proposed approach performs better than the-state-of-the artsfor the Maryland and YUPenn dataset. The final descriptor obtained is powerfulenough to distinguish among dynamic scenes and is even capable of addressingthe scenario where the camera motion is dominant and the scene dynamics arecomplex. Further, this paper shows an extensive study on the performance ofvarious aggregation methods and their combinations. We compare the proposedapproach with other dynamic scene classification algorithms on two publiclyavailable datasets - Maryland and YUPenn to demonstrate the superiorperformance of the proposed approach.
arxiv-1502-04993 | Reconstruction of recurrent synaptic connectivity of thousands of neurons from simulated spiking activity |  http://arxiv.org/abs/1502.04993  | author:Yury V. Zaytsev, Abigail Morrison, Moritz Deger category:q-bio.NC q-bio.QM stat.ML published:2015-02-17 summary:Dynamics and function of neuronal networks are determined by their synapticconnectivity. Current experimental methods to analyze synaptic networkstructure on the cellular level, however, cover only small fractions offunctional neuronal circuits, typically without a simultaneous record ofneuronal spiking activity. Here we present a method for the reconstruction oflarge recurrent neuronal networks from thousands of parallel spike trainrecordings. We employ maximum likelihood estimation of a generalized linearmodel of the spiking activity in continuous time. For this model the pointprocess likelihood is concave, such that a global optimum of the parameters canbe obtained by gradient ascent. Previous methods, including those of the sameclass, did not allow recurrent networks of that order of magnitude to bereconstructed due to prohibitive computational cost and numericalinstabilities. We describe a minimal model that is optimized for large networksand an efficient scheme for its parallelized numerical optimization on genericcomputing clusters. For a simulated balanced random network of 1000 neurons,synaptic connectivity is recovered with a misclassification error rate of lessthan 1% under ideal conditions. We show that the error rate remains low in aseries of example cases under progressively less ideal conditions. Finally, wesuccessfully reconstruct the connectivity of a hidden synfire chain that isembedded in a random network, which requires clustering of the networkconnectivity to reveal the synfire groups. Our results demonstrate how synapticconnectivity could potentially be inferred from large-scale parallel spiketrain recordings.
arxiv-1502-04843 | Generalized Gradient Learning on Time Series under Elastic Transformations |  http://arxiv.org/abs/1502.04843  | author:Brijnesh Jain category:cs.LG published:2015-02-17 summary:The majority of machine learning algorithms assumes that objects arerepresented as vectors. But often the objects we want to learn on are morenaturally represented by other data structures such as sequences and timeseries. For these representations many standard learning algorithms areunavailable. We generalize gradient-based learning algorithms to time seriesunder dynamic time warping. To this end, we introduce elastic functions, whichextend functions on time series to matrix spaces. Necessary conditions arepresented under which generalized gradient learning on time series isconsistent. We indicate how results carry over to arbitrary elastic distancefunctions and to sequences consisting of symbolic elements. Specifically, fourlinear classifiers are extended to time series under dynamic time warping andapplied to benchmark datasets. Results indicate that generalized gradientlearning via elastic functions have the potential to complement thestate-of-the-art in statistical pattern recognition on time series.
arxiv-1502-04837 | Nonparametric Nearest Neighbor Descent Clustering based on Delaunay Triangulation |  http://arxiv.org/abs/1502.04837  | author:Teng Qiu, Yongjie Li category:stat.ML cs.CV cs.LG published:2015-02-17 summary:In our physically inspired in-tree (IT) based clustering algorithm and theseries after it, there is only one free parameter involved in computing thepotential value of each point. In this work, based on the DelaunayTriangulation or its dual Voronoi tessellation, we propose a nonparametricprocess to compute potential values by the local information. This computation,though nonparametric, is relatively very rough, and consequently, many localextreme points will be generated. However, unlike those gradient-based methods,our IT-based methods are generally insensitive to those local extremes. Thispositively demonstrates the superiority of these parametric (previous) andnonparametric (in this work) IT-based methods.
arxiv-1502-04617 | Deep Transform: Error Correction via Probabilistic Re-Synthesis |  http://arxiv.org/abs/1502.04617  | author:Andrew J. R. Simpson category:cs.LG 68Txx published:2015-02-16 summary:Errors in data are usually unwelcome and so some means to correct them isuseful. However, it is difficult to define, detect or correct errors in anunsupervised way. Here, we train a deep neural network to re-synthesize itsinputs at its output layer for a given class of data. We then exploit the factthat this abstract transformation, which we call a deep transform (DT),inherently rejects information (errors) existing outside of the abstractfeature space. Using the DT to perform probabilistic re-synthesis, wedemonstrate the recovery of data that has been subject to extreme degradation.
arxiv-1502-04492 | Towards Building Deep Networks with Bayesian Factor Graphs |  http://arxiv.org/abs/1502.04492  | author:Amedeo Buonanno, Francesco A. N. Palmieri category:cs.CV cs.LG published:2015-02-16 summary:We propose a Multi-Layer Network based on the Bayesian framework of theFactor Graphs in Reduced Normal Form (FGrn) applied to a two-dimensionallattice. The Latent Variable Model (LVM) is the basic building block of aquadtree hierarchy built on top of a bottom layer of random variables thatrepresent pixels of an image, a feature map, or more generally a collection ofspatially distributed discrete variables. The multi-layer architectureimplements a hierarchical data representation that, via belief propagation, canbe used for learning and inference. Typical uses are pattern completion,correction and classification. The FGrn paradigm provides great flexibility andmodularity and appears as a promising candidate for building deep networks: thesystem can be easily extended by introducing new and different (in cardinalityand in type) variables. Prior knowledge, or supervised information, can beintroduced at different scales. The FGrn paradigm provides a handy way forbuilding all kinds of architectures by interconnecting only three types ofunits: Single Input Single Output (SISO) blocks, Sources and Replicators. Thenetwork is designed like a circuit diagram and the belief messages flowbidirectionally in the whole system. The learning algorithms operate onlylocally within each block. The framework is demonstrated in this paper in athree-layer structure applied to images extracted from a standard data set.
arxiv-1502-04499 | Color Image Enhancement Using the lrgb Coordinates in the Context of Support Fuzzification |  http://arxiv.org/abs/1502.04499  | author:Vasile Patrascu category:cs.CV published:2015-02-16 summary:Image enhancement is an important stage in the image-processing domain. Themost known image enhancement method is the histogram equalization. This methodis an automated one, and realizes a simultaneous modification for brightnessand contrast in the case of monochrome images and for brightness, contrast,saturation and hue in the case of color images. Simple and efficient methodscan be obtained if affine transforms within logarithmic models are used. A veryimportant thing in the affine transform determination for color images is thecoordinate system that is used for color space representation. Thus, the usingof the RGB coordinates leads to a simultaneous modification of luminosity andsaturation. In this paper using the lrgb perceptual coordinates one can defineaffine transforms, which allow a separated modification of luminosity l andsaturation s (saturation being calculated with the component rgb in thechromatic plane). Better results can be obtained if partitions are defined onthe image support and then the pixels are separately processed in each windowbelonging to the defined partition. Classical partitions frequently lead to theappearance of some discontinuities at the boundaries between these windows. Inorder to avoid all these drawbacks the classical partitions may be replaced byfuzzy partitions. Their elements will be fuzzy windows and in each of themthere will be defined an affine transform induced by parameters using the fuzzymean, fuzzy variance and fuzzy saturation computed for the pixels that belongto the analyzed window. The final image is obtained by summing up in a weightway the images of every fuzzy window.
arxiv-1502-04502 | Clustering by Descending to the Nearest Neighbor in the Delaunay Graph Space |  http://arxiv.org/abs/1502.04502  | author:Teng Qiu, Yongjie Li category:stat.ML cs.CV cs.LG published:2015-02-16 summary:In our previous works, we proposed a physically-inspired rule to organize thedata points into an in-tree (IT) structure, in which some undesired edges areallowed to occur. By removing those undesired or redundant edges, this ITstructure is divided into several separate parts, each representing onecluster. In this work, we seek to prevent the undesired edges from arising atthe source. Before using the physically-inspired rule, data points are at firstorganized into a proximity graph which restricts each point to select theoptimal directed neighbor just among its neighbors. Consequently, separatedin-trees or clusters automatically arise, without redundant edges requiring tobe removed.
arxiv-1502-04585 | The Ladder: A Reliable Leaderboard for Machine Learning Competitions |  http://arxiv.org/abs/1502.04585  | author:Avrim Blum, Moritz Hardt category:cs.LG published:2015-02-16 summary:The organizer of a machine learning competition faces the problem ofmaintaining an accurate leaderboard that faithfully represents the quality ofthe best submission of each competing team. What makes this estimation problemparticularly challenging is its sequential and adaptive nature. As participantsare allowed to repeatedly evaluate their submissions on the leaderboard, theymay begin to overfit to the holdout data that supports the leaderboard. Fewtheoretical results give actionable advice on how to design a reliableleaderboard. Existing approaches therefore often resort to poorly understoodheuristics such as limiting the bit precision of answers and the rate ofre-submission. In this work, we introduce a notion of "leaderboard accuracy" tailored to theformat of a competition. We introduce a natural algorithm called "the Ladder"and demonstrate that it simultaneously supports strong theoretical guaranteesin a fully adaptive model of estimation, withstands practical adversarialattacks, and achieves high utility on real submission files from an actualcompetition hosted by Kaggle. Notably, we are able to sidestep a powerful recent hardness result foradaptive risk estimation that rules out algorithms such as ours under aseemingly very similar notion of accuracy. On a practical note, we provide acompletely parameter-free variant of our algorithm that can be deployed in areal competition with no tuning required whatsoever.
arxiv-1502-04742 | On the Predictive Properties of Binary Link Functions |  http://arxiv.org/abs/1502.04742  | author:Necla Gunduz, Ernest Fokoue category:stat.ML stat.ME published:2015-02-16 summary:This paper provides a theoretical and computational justification of the longheld claim that of the similarity of the probit and logit link functions oftenused in binary classification. Despite this widespread recognition of thestrong similarities between these two link functions, very few (if any)researchers have dedicated time to carry out a formal study aimed atestablishing and characterizing firmly all the aspects of the similarities anddifferences. This paper proposes a definition of both structural and predictiveequivalence of link functions-based binary regression models, and explores thevarious ways in which they are either similar or dissimilar. From a predictiveanalytics perspective, it turns out that not only are probit and logitperfectly predictively concordant, but the other link functions like cauchitand complementary log log enjoy very high percentage of predictive equivalence.Throughout this paper, simulated and real life examples demonstrate all theequivalence results that we prove theoretically.
arxiv-1502-04726 | ICR: Iterative Convex Refinement for Sparse Signal Recovery Using Spike and Slab Priors |  http://arxiv.org/abs/1502.04726  | author:Hojjat S. Mousavi, Vishal Monga, Trac D. Tran category:stat.ML cs.CV math.OC published:2015-02-16 summary:In this letter, we address sparse signal recovery using spike and slabpriors. In particular, we focus on a Bayesian framework where sparsity isenforced on reconstruction coefficients via probabilistic priors. Theoptimization resulting from spike and slab prior maximization is known to be ahard non-convex problem, and existing solutions involve simplifying assumptionsand/or relaxations. We propose an approach called Iterative Convex Refinement(ICR) that aims to solve the aforementioned optimization problem directlyallowing for greater generality in the sparse structure. Essentially, ICRsolves a sequence of convex optimization problems such that sequence ofsolutions converges to a sub-optimal solution of the original hard optimizationproblem. We propose two versions of our algorithm: a.) an unconstrainedversion, and b.) with a non-negativity constraint on sparse coefficients, whichmay be required in some real-world problems. Experimental validation isperformed on both synthetic data and for a real-world image recovery problem,which illustrates merits of ICR over state of the art alternatives.
arxiv-1502-04622 | Particle Gibbs for Bayesian Additive Regression Trees |  http://arxiv.org/abs/1502.04622  | author:Balaji Lakshminarayanan, Daniel M. Roy, Yee Whye Teh category:stat.ML cs.LG stat.CO published:2015-02-16 summary:Additive regression trees are flexible non-parametric models and popularoff-the-shelf tools for real-world non-linear regression. In applicationdomains, such as bioinformatics, where there is also demand for probabilisticpredictions with measures of uncertainty, the Bayesian additive regressiontrees (BART) model, introduced by Chipman et al. (2010), is increasinglypopular. As data sets have grown in size, however, the standardMetropolis-Hastings algorithms used to perform inference in BART are provinginadequate. In particular, these Markov chains make local changes to the treesand suffer from slow mixing when the data are high-dimensional or the bestfitting trees are more than a few layers deep. We present a novel sampler forBART based on the Particle Gibbs (PG) algorithm (Andrieu et al., 2010) and atop-down particle filtering algorithm for Bayesian decision trees(Lakshminarayanan et al., 2013). Rather than making local changes to individualtrees, the PG sampler proposes a complete tree to fit the residual. Experimentsshow that the PG sampler outperforms existing samplers in many settings.
arxiv-1502-04658 | HEp-2 Cell Classification via Fusing Texture and Shape Information |  http://arxiv.org/abs/1502.04658  | author:Xianbiao Qi, Guoying Zhao, Chun-Guang Li, Jun Guo, Matti Pietikäinen category:cs.CV published:2015-02-16 summary:Indirect Immunofluorescence (IIF) HEp-2 cell image is an effective evidencefor diagnosis of autoimmune diseases. Recently computer-aided diagnosis ofautoimmune diseases by IIF HEp-2 cell classification has attracted greatattention. However the HEp-2 cell classification task is quite challenging dueto large intra-class variation and small between-class variation. In this paperwe propose an effective and efficient approach for the automatic classificationof IIF HEp-2 cell image by fusing multi-resolution texture information andricher shape information. To be specific, we propose to: a) capture themulti-resolution texture information by a novel Pairwise Rotation InvariantCo-occurrence of Local Gabor Binary Pattern (PRICoLGBP) descriptor, b) depictthe richer shape information by using an Improved Fisher Vector (IFV) modelwith RootSIFT features which are sampled from large image patches in multiplescales, and c) combine them properly. We evaluate systematically the proposedapproach on the IEEE International Conference on Pattern Recognition (ICPR)2012, IEEE International Conference on Image Processing (ICIP) 2013 and ICPR2014 contest data sets. The experimental results for the proposed methodssignificantly outperform the winners of ICPR 2012 and ICIP 2013 contest, andachieve comparable performance with the winner of the newly released ICPR 2014contest.
arxiv-1502-04652 | Inferring 3D Object Pose in RGB-D Images |  http://arxiv.org/abs/1502.04652  | author:Saurabh Gupta, Pablo Arbeláez, Ross Girshick, Jitendra Malik category:cs.CV published:2015-02-16 summary:The goal of this work is to replace objects in an RGB-D scene withcorresponding 3D models from a library. We approach this problem by firstdetecting and segmenting object instances in the scene using the approach fromGupta et al. [13]. We use a convolutional neural network (CNN) to predict thepose of the object. This CNN is trained using pixel normals in imagescontaining rendered synthetic objects. When tested on real data, it outperformsalternative algorithms trained on real data. We then use this coarse poseestimate along with the inferred pixel support to align a small number ofprototypical models to the data, and place the model that fits the best intothe scene. We observe a 48% relative improvement in performance at the task of3D detection over the current state-of-the-art [33], while being an order ofmagnitude faster at the same time.
arxiv-1502-04569 | Image Specificity |  http://arxiv.org/abs/1502.04569  | author:Mainak Jas, Devi Parikh category:cs.CV published:2015-02-16 summary:For some images, descriptions written by multiple people are consistent witheach other. But for other images, descriptions across people vary considerably.In other words, some images are specific $-$ they elicit consistentdescriptions from different people $-$ while other images are ambiguous.Applications involving images and text can benefit from an understanding ofwhich images are specific and which ones are ambiguous. For instance, considertext-based image retrieval. If a query description is moderately similar to thecaption (or reference description) of an ambiguous image, that query may beconsidered a decent match to the image. But if the image is very specific, amoderate similarity between the query and the reference description may not besufficient to retrieve the image. In this paper, we introduce the notion of image specificity. We present twomechanisms to measure specificity given multiple descriptions of an image: anautomated measure and a measure that relies on human judgement. We analyzeimage specificity with respect to image content and properties to betterunderstand what makes an image specific. We then train models to automaticallypredict the specificity of an image from image features alone without requiringtextual descriptions of the image. Finally, we show that modeling imagespecificity leads to improvements in a text-based image retrieval application.
arxiv-1502-04423 | Exploring Transfer Function Nonlinearity in Echo State Networks |  http://arxiv.org/abs/1502.04423  | author:Alireza Goudarzi, Alireza Shabani, Darko Stefanovic category:cs.NE published:2015-02-16 summary:Supralinear and sublinear pre-synaptic and dendritic integration isconsidered to be responsible for nonlinear computation power of biologicalneurons, emphasizing the role of nonlinear integration as opposed to nonlinearoutput thresholding. How, why, and to what degree the transfer functionnonlinearity helps biologically inspired neural network models is not fullyunderstood. Here, we study these questions in the context of echo statenetworks (ESN). ESN is a simple neural network architecture in which a fixedrecurrent network is driven with an input signal, and the output is generatedby a readout layer from the measurements of the network states. ESNarchitecture enjoys efficient training and good performance on certainsignal-processing tasks, such as system identification and time seriesprediction. ESN performance has been analyzed with respect to the connectivitypattern in the network structure and the input bias. However, the effects ofthe transfer function in the network have not been studied systematically.Here, we use an approach tanh on the Taylor expansion of a frequently usedtransfer function, the hyperbolic tangent function, to systematically study theeffect of increasing nonlinearity of the transfer function on the memory,nonlinear capacity, and signal processing performance of ESN. Interestingly, wefind that a quadratic approximation is enough to capture the computationalpower of ESN with tanh function. The results of this study apply to bothsoftware and hardware implementation of ESN.
arxiv-1502-04623 | DRAW: A Recurrent Neural Network For Image Generation |  http://arxiv.org/abs/1502.04623  | author:Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, Daan Wierstra category:cs.CV cs.LG cs.NE published:2015-02-16 summary:This paper introduces the Deep Recurrent Attentive Writer (DRAW) neuralnetwork architecture for image generation. DRAW networks combine a novelspatial attention mechanism that mimics the foveation of the human eye, with asequential variational auto-encoding framework that allows for the iterativeconstruction of complex images. The system substantially improves on the stateof the art for generative models on MNIST, and, when trained on the Street ViewHouse Numbers dataset, it generates images that cannot be distinguished fromreal data with the naked eye.
arxiv-1502-04635 | Parameter estimation in softmax decision-making models with linear objective functions |  http://arxiv.org/abs/1502.04635  | author:Paul Reverdy, Naomi E. Leonard category:math.OC cs.LG stat.ML 93E10 published:2015-02-16 summary:With an eye towards human-centered automation, we contribute to thedevelopment of a systematic means to infer features of human decision-makingfrom behavioral data. Motivated by the common use of softmax selection inmodels of human decision-making, we study the maximum likelihood parameterestimation problem for softmax decision-making models with linear objectivefunctions. We present conditions under which the likelihood function is convex.These allow us to provide sufficient conditions for convergence of theresulting maximum likelihood estimator and to construct its asymptoticdistribution. In the case of models with nonlinear objective functions, we showhow the estimator can be applied by linearizing about a nominal parametervalue. We apply the estimator to fit the stochastic UCL (Upper Credible Limit)model of human decision-making to human subject data. We show statisticallysignificant differences in behavior across related, but distinct, tasks.
arxiv-1502-04500 | Bi-Level Image Thresholding obtained by means of Kaniadakis Entropy |  http://arxiv.org/abs/1502.04500  | author:Amelia Carolina Sparavigna category:cs.CV published:2015-02-16 summary:In this paper we are proposing the use of Kaniadakis entropy in the bi-levelthresholding of images, in the framework of a maximum entropy principle. Wediscuss the role of its entropic index in determining the threshold and indriving an "image transition", that is, an abrupt transition in the appearanceof the corresponding bi-level image. Some examples are proposed to illustratethe method and for comparing it to the approach which is using the Tsallisentropy.
arxiv-1502-04689 | Exact tensor completion using t-SVD |  http://arxiv.org/abs/1502.04689  | author:Zemin Zhang, Shuchin Aeron category:cs.LG cs.NA stat.ML published:2015-02-16 summary:In this paper we focus on the problem of completion of multidimensionalarrays (also referred to as tensors) from limited sampling. Our approach isbased on a recently proposed tensor-Singular Value Decomposition (t-SVD) [1].Using this factorization one can derive notion of tensor rank, referred to asthe tensor tubal rank, which has optimality properties similar to that ofmatrix rank derived from SVD. As shown in [2] some multidimensional data, suchas panning video sequences exhibit low tensor tubal rank and we look at theproblem of completing such data under random sampling of the data cube. We showthat by solving a convex optimization problem, which minimizes the tensornuclear norm obtained as the convex relaxation of tensor tubal rank, one canguarantee recovery with overwhelming probability as long as samples inproportion to the degrees of freedom in t-SVD are observed. In this sense ourresults are order-wise optimal. The conditions under which this result holdsare very similar to the incoherency conditions for the matrix completion,albeit we define incoherency under the algebraic set-up of t-SVD. We show theperformance of the algorithm on some real data sets and compare it with otherexisting approaches based on tensor flattening and Tucker decomposition.
arxiv-1502-04416 | Random Subspace Learning Approach to High-Dimensional Outliers Detection |  http://arxiv.org/abs/1502.04416  | author:Bohan Liu, Ernest Fokoue category:stat.ML 62H25, 62H30 published:2015-02-16 summary:We introduce and develop a novel approach to outlier detection based onadaptation of random subspace learning. Our proposed method handles bothhigh-dimension low-sample size and traditional low-dimensional high-sample sizedatasets. Essentially, we avoid the computational bottleneck of techniques likeminimum covariance determinant (MCD) by computing the needed determinants andassociated measures in much lower dimensional subspaces. Both theoretical andcomputational development of our approach reveal that it is computationallymore efficient than the regularized methods in high-dimensional low-samplesize, and often competes favorably with existing methods as far as thepercentage of correct outlier detection is concerned.
arxiv-1502-04631 | Clustering and Inference From Pairwise Comparisons |  http://arxiv.org/abs/1502.04631  | author:Rui Wu, Jiaming Xu, R. Srikant, Laurent Massoulié, Marc Lelarge, Bruce Hajek category:stat.ML published:2015-02-16 summary:Given a set of pairwise comparisons, the classical ranking problem computes asingle ranking that best represents the preferences of all users. In thispaper, we study the problem of inferring individual preferences, arising in thecontext of making personalized recommendations. In particular, we assume thatthere are $n$ users of $r$ types; users of the same type provide similarpairwise comparisons for $m$ items according to the Bradley-Terry model. Wepropose an efficient algorithm that accurately estimates the individualpreferences for almost all users, if there are $r \max \{m, n\}\log m \log^2 n$pairwise comparisons per type, which is near optimal in sample complexity when$r$ only grows logarithmically with $m$ or $n$. Our algorithm has three steps:first, for each user, compute the \emph{net-win} vector which is a projectionof its $\binom{m}{2}$-dimensional vector of pairwise comparisons onto an$m$-dimensional linear subspace; second, cluster the users based on the net-winvectors; third, estimate a single preference for each cluster separately. Thenet-win vectors are much less noisy than the high dimensional vectors ofpairwise comparisons and clustering is more accurate after the projection asconfirmed by numerical experiments. Moreover, we show that, when a cluster isonly approximately correct, the maximum likelihood estimation for theBradley-Terry model is still close to the true preference.
arxiv-1502-04681 | Unsupervised Learning of Video Representations using LSTMs |  http://arxiv.org/abs/1502.04681  | author:Nitish Srivastava, Elman Mansimov, Ruslan Salakhutdinov category:cs.LG cs.CV cs.NE published:2015-02-16 summary:We use multilayer Long Short Term Memory (LSTM) networks to learnrepresentations of video sequences. Our model uses an encoder LSTM to map aninput sequence into a fixed length representation. This representation isdecoded using single or multiple decoder LSTMs to perform different tasks, suchas reconstructing the input sequence, or predicting the future sequence. Weexperiment with two kinds of input sequences - patches of image pixels andhigh-level representations ("percepts") of video frames extracted using apretrained convolutional net. We explore different design choices such aswhether the decoder LSTMs should condition on the generated output. We analyzethe outputs of the model qualitatively to see how well the model canextrapolate the learned video representation into the future and into the past.We try to visualize and interpret the learned features. We stress test themodel by running it on longer time scales and on out-of-domain data. We furtherevaluate the representations by finetuning them for a supervised learningproblem - human action recognition on the UCF-101 and HMDB-51 datasets. We showthat the representations help improve classification accuracy, especially whenthere are only a few training examples. Even models pretrained on unrelateddatasets (300 hours of YouTube videos) can help action recognition performance.
arxiv-1502-04469 | Classification and its applications for drug-target interaction identification |  http://arxiv.org/abs/1502.04469  | author:Jian-Ping Mei, Chee-Keong Kwoh, Peng Yang, Xiao-Li Li category:cs.LG q-bio.MN q-bio.QM published:2015-02-16 summary:Classification is one of the most popular and widely used supervised learningtasks, which categorizes objects into predefined classes based on knownknowledge. Classification has been an important research topic in machinelearning and data mining. Different classification methods have been proposedand applied to deal with various real-world problems. Unlike unsupervisedlearning such as clustering, a classifier is typically trained with labeleddata before being used to make prediction, and usually achieves higher accuracythan unsupervised one. In this paper, we first define classification and then review severalrepresentative methods. After that, we study in details the application ofclassification to a critical problem in drug discovery, i.e., drug-targetprediction, due to the challenges in predicting possible interactions betweendrugs and targets.
arxiv-1502-04434 | Invariant backpropagation: how to train a transformation-invariant neural network |  http://arxiv.org/abs/1502.04434  | author:Sergey Demyanov, James Bailey, Ramamohanarao Kotagiri, Christopher Leckie category:stat.ML cs.LG cs.NE published:2015-02-16 summary:In many classification problems a classifier should be robust to smallvariations in the input vector. This is a desired property not only forparticular transformations, such as translation and rotation in imageclassification problems, but also for all others for which the change is smallenough to retain the object perceptually indistinguishable. We propose twoextensions of the backpropagation algorithm that train a neural network to berobust to variations in the feature vector. While the first of them enforcesrobustness of the loss function to all variations, the second method trains thepredictions to be robust to a particular variation which changes the lossfunction the most. The second methods demonstrates better results, but isslightly slower. We analytically compare the proposed algorithm with two themost similar approaches (Tangent BP and Adversarial Training), and proposetheir fast versions. In the experimental part we perform comparison of allalgorithms in terms of classification accuracy and robustness to noise on MNISTand CIFAR-10 datasets. Additionally we analyze how the performance of theproposed algorithm depends on the dataset size and data augmentation.
arxiv-1502-04390 | Equilibrated adaptive learning rates for non-convex optimization |  http://arxiv.org/abs/1502.04390  | author:Yann N. Dauphin, Harm de Vries, Yoshua Bengio category:cs.LG cs.NA published:2015-02-15 summary:Parameter-specific adaptive learning rate methods are computationallyefficient ways to reduce the ill-conditioning problems encountered whentraining large deep networks. Following recent work that strongly suggests thatmost of the critical points encountered when training such networks are saddlepoints, we find how considering the presence of negative eigenvalues of theHessian could help us design better suited adaptive learning rate schemes. Weshow that the popular Jacobi preconditioner has undesirable behavior in thepresence of both positive and negative curvature, and present theoretical andempirical evidence that the so-called equilibration preconditioner iscomparatively better suited to non-convex problems. We introduce a noveladaptive learning rate scheme, called ESGD, based on the equilibrationpreconditioner. Our experiments show that ESGD performs as well or better thanRMSProp in terms of convergence speed, always clearly improving over plainstochastic gradient descent.
arxiv-1502-04269 | Supersparse Linear Integer Models for Optimized Medical Scoring Systems |  http://arxiv.org/abs/1502.04269  | author:Berk Ustun, Cynthia Rudin category:stat.ML cs.DM cs.LG stat.AP stat.ME published:2015-02-15 summary:Scoring systems are linear classification models that only require users toadd, subtract and multiply a few small numbers in order to make a prediction.These models are in widespread use by the medical community, but are difficultto learn from data because they need to be accurate and sparse, have coprimeinteger coefficients, and satisfy multiple operational constraints. We presenta new method for creating data-driven scoring systems called a SupersparseLinear Integer Model (SLIM). SLIM scoring systems are built by solving aninteger program that directly encodes measures of accuracy (the 0-1 loss) andsparsity (the $\ell_0$-seminorm) while restricting coefficients to coprimeintegers. SLIM can seamlessly incorporate a wide range of operationalconstraints related to accuracy and sparsity, and can produce highly tailoredmodels without parameter tuning. We provide bounds on the testing and trainingaccuracy of SLIM scoring systems, and present a new data reduction techniquethat can improve scalability by eliminating a portion of the training databeforehand. Our paper includes results from a collaboration with theMassachusetts General Hospital Sleep Laboratory, where SLIM was used to createa highly tailored scoring system for sleep apnea screening
arxiv-1502-04315 | Fast and Memory-Efficient Significant Pattern Mining via Permutation Testing |  http://arxiv.org/abs/1502.04315  | author:Felipe Llinares López, Mahito Sugiyama, Laetitia Papaxanthos, Karsten M. Borgwardt category:stat.ML published:2015-02-15 summary:We present a novel algorithm, Westfall-Young light, for detecting patterns,such as itemsets and subgraphs, which are statistically significantly enrichedin one of two classes. Our method corrects rigorously for multiple hypothesistesting and correlations between patterns through the Westfall-Youngpermutation procedure, which empirically estimates the null distribution ofpattern frequencies in each class via permutations. In our experiments,Westfall-Young light dramatically outperforms the current state-of-the-artapproach in terms of both runtime and memory efficiency on popular real-worldbenchmark datasets for pattern mining. The key to this efficiency is thatunlike all existing methods, our algorithm neither needs to solve theunderlying frequent itemset mining problem anew for each permutation nor needsto store the occurrence list of all frequent patterns. Westfall-Young lightopens the door to significant pattern mining on large datasets that previouslyled to prohibitive runtime or memory costs.
arxiv-1502-04275 | segDeepM: Exploiting Segmentation and Context in Deep Neural Networks for Object Detection |  http://arxiv.org/abs/1502.04275  | author:Yukun Zhu, Raquel Urtasun, Ruslan Salakhutdinov, Sanja Fidler category:cs.CV published:2015-02-15 summary:In this paper, we propose an approach that exploits object segmentation inorder to improve the accuracy of object detection. We frame the problem asinference in a Markov Random Field, in which each detection hypothesis scoresobject appearance as well as contextual information using Convolutional NeuralNetworks, and allows the hypothesis to choose and score a segment out of alarge pool of accurate object segmentation proposals. This enables the detectorto incorporate additional evidence when it is available and thus results inmore accurate detections. Our experiments show an improvement of 4.1% in mAPover the R-CNN baseline on PASCAL VOC 2010, and 3.4% over the currentstate-of-the-art, demonstrating the power of our approach.
arxiv-1502-04272 | Spatial Stimuli Gradient Sketch Model |  http://arxiv.org/abs/1502.04272  | author:Joshin John Mathew, Alex Pappachen James category:cs.CV published:2015-02-15 summary:The inability of automated edge detection methods inspired from primal sketchmodels to accurately calculate object edges under the influence of pixel noiseis an open problem. Extending the principles of image perception i.e.Weber-Fechner law, and Sheperd similarity law, we propose a new edge detectionmethod and formulation that use perceived brightness and neighbourhoodsimilarity calculations in the determination of robust object edges. Therobustness of the detected edges is benchmark against Sobel, SIS, Kirsch, andPrewitt edge detection methods in an example face recognition problem showingstatistically significant improvement in recognition accuracy and pixel noisetolerance.
arxiv-1502-04383 | A Comprehensive Survey on Pose-Invariant Face Recognition |  http://arxiv.org/abs/1502.04383  | author:Changxing Ding, Dacheng Tao category:cs.CV published:2015-02-15 summary:The capacity to recognize faces under varied poses is a fundamental humanability that presents a unique challenge for computer vision systems. Comparedto frontal face recognition, which has been intensively studied and hasgradually matured in the past few decades, pose-invariant face recognition(PIFR) remains a largely unsolved problem. However, PIFR is crucial torealizing the full potential of face recognition for real-world applications,since face recognition is intrinsically a passive biometric technology forrecognizing uncooperative subjects. In this paper, we discuss the inherentdifficulties in PIFR and present a comprehensive review of establishedtechniques. Existing PIFR methods can be grouped into four categories, i.e.,pose-robust feature extraction approaches, multi-view subspace learningapproaches, face synthesis approaches, and hybrid approaches. The motivations,strategies, pros/cons, and performance of representative approaches aredescribed and compared. Moreover, promising directions for future research arediscussed.
arxiv-1502-04204 | Gray-Level Image Transitions Driven by Tsallis Entropic Index |  http://arxiv.org/abs/1502.04204  | author:Amelia Carolina Sparavigna category:cs.CV published:2015-02-14 summary:The maximum entropy principle is largely used in thresholding andsegmentation of images. Among the several formulations of this principle, themost effectively applied is that based on Tsallis non-extensive entropy. Here,we discuss the role of its entropic index in determining the thresholds. Whenthis index is spanning the interval (0,1), for some images, the values ofthresholds can have large leaps. In this manner, we observe abrupt transitionsin the appearance of corresponding bi-level or multi-level images. Thesegray-level image transitions are analogous to order or texture transitionsobserved in physical systems, transitions which are driven by the temperatureor by other physical quantities.
arxiv-1502-04168 | Nonparametric regression using needlet kernels for spherical data |  http://arxiv.org/abs/1502.04168  | author:Shaobo Lin category:cs.LG stat.ML 68T05, 62J02 F.2.2 published:2015-02-14 summary:Needlets have been recognized as state-of-the-art tools to tackle sphericaldata, due to their excellent localization properties in both spacial andfrequency domains. This paper considers developing kernel methods associated with the needletkernel for nonparametric regression problems whose predictor variables aredefined on a sphere. Due to the localization property in the frequency domain,we prove that the regularization parameter of the kernel ridge regressionassociated with the needlet kernel can decrease arbitrarily fast. A naturalconsequence is that the regularization term for the kernel ridge regression isnot necessary in the sense of rate optimality. Based on the excellentlocalization property in the spacial domain further, we also prove that all the$l^{q}$ $(01\leq q < \infty)$ kernel regularization estimates associated withthe needlet kernel, including the kernel lasso estimate and the kernel bridgeestimate, possess almost the same generalization capability for a large rangeof regularization parameters in the sense of rate optimality. This finding tentatively reveals that, if the needlet kernel is utilized,then the choice of $q$ might not have a strong impact in terms of thegeneralization capability in some modeling contexts. From this perspective, $q$can be arbitrarily specified, or specified merely by other no generalizationcriteria like smoothness, computational complexity, sparsity, etc..
arxiv-1502-04174 | Probabilistic Models for High-Order Projective Dependency Parsing |  http://arxiv.org/abs/1502.04174  | author:Xuezhe Ma, Hai Zhao category:cs.CL published:2015-02-14 summary:This paper presents generalized probabilistic models for high-orderprojective dependency parsing and an algorithmic framework for learning thesestatistical models involving dependency trees. Partition functions andmarginals for high-order dependency trees can be computed efficiently, byadapting our algorithms which extend the inside-outside algorithm tohigher-order cases. To show the effectiveness of our algorithms, we performexperiments on three languages---English, Chinese and Czech, using maximumconditional likelihood estimation for model training and L-BFGS for parameterestimation. Our methods achieve competitive performance for English, andoutperform all previously reported dependency parsers for Chinese and Czech.
arxiv-1502-04156 | Towards Biologically Plausible Deep Learning |  http://arxiv.org/abs/1502.04156  | author:Yoshua Bengio, Dong-Hyun Lee, Jorg Bornschein, Zhouhan Lin category:cs.LG published:2015-02-14 summary:Neuroscientists have long criticised deep learning algorithms as incompatiblewith current knowledge of neurobiology. We explore more biologically plausibleversions of deep representation learning, focusing here mostly on unsupervisedlearning but developing a learning mechanism that could account for supervised,unsupervised and reinforcement learning. The starting point is that the basiclearning rule believed to govern synaptic weight updates(Spike-Timing-Dependent Plasticity) can be interpreted as gradient descent onsome objective function so long as the neuronal dynamics push firing ratestowards better values of the objective function (be it supervised,unsupervised, or reward-driven). The second main idea is that this correspondsto a form of the variational EM algorithm, i.e., with approximate rather thanexact posteriors, implemented by neural dynamics. Another contribution of thispaper is that the gradients required for updating the hidden states in theabove variational interpretation can be estimated using an approximation thatonly requires propagating activations forward and backward, with pairs oflayers learning to form a denoising auto-encoder. Finally, we extend the theoryabout the probabilistic interpretation of auto-encoders to justify improvedsampling schemes based on the generative interpretation of denoisingauto-encoders, and we validate all these ideas on generative learning tasks.
arxiv-1502-04163 | A Distributional Representation Model For Collaborative Filtering |  http://arxiv.org/abs/1502.04163  | author:Zhang Junlin, Cai Heng, Huang Tongwen, Xue Huiping category:cs.IR cs.NE published:2015-02-14 summary:In this paper, we propose a very concise deep learning approach forcollaborative filtering that jointly models distributional representation forusers and items. The proposed framework obtains better performance whencompared against current state-of-art algorithms and that made thedistributional representation model a promising direction for further researchin the collaborative filtering.
arxiv-1502-04252 | Cardiac MR Image Segmentation Techniques: an overview |  http://arxiv.org/abs/1502.04252  | author:Tizita Nesibu Shewaye category:cs.CV published:2015-02-14 summary:Broadly speaking, the objective in cardiac image segmentation is to delineatethe outer and inner walls of the heart to segment out either the entire orparts of the organ boundaries. This paper will focus on MR images as they arethe most widely used in cardiac segmentation -- as a result of the accuratemorphological information and better soft tissue contrast they provide. Thiscardiac segmentation information is very useful as it eases physicalmeasurements that provides useful metrics for cardiac diagnosis such asinfracted volumes, ventricular volumes, ejection fraction, myocardial mass,cardiac movement, and the like. But, this task is difficult due to theintensity and texture similarities amongst the different cardiac and backgroundstructures on top of some noisy artifacts present in MR images. Thus far,various researchers have proposed different techniques to solve some of thepressing issues. This seminar paper presents an overview of representativemedical image segmentation techniques. The paper also highlights preferredapproaches for segmentation of the four cardiac chambers: the left ventricle(LV), right ventricle (RV), left atrium (LA) and right atrium (RA), on shortaxis image planes.
arxiv-1502-04248 | Asymptotic Justification of Bandlimited Interpolation of Graph signals for Semi-Supervised Learning |  http://arxiv.org/abs/1502.04248  | author:Aamir Anis, Aly El Gamal, A. Salman Avestimehr, Antonio Ortega category:cs.LG cs.IT math.IT published:2015-02-14 summary:Graph-based methods play an important role in unsupervised andsemi-supervised learning tasks by taking into account the underlying geometryof the data set. In this paper, we consider a statistical setting forsemi-supervised learning and provide a formal justification of the recentlyintroduced framework of bandlimited interpolation of graph signals. Ouranalysis leads to the interpretation that, given enough labeled data, thismethod is very closely related to a constrained low density separation problemas the number of data points tends to infinity. We demonstrate the practicalutility of our results through simple experiments.
arxiv-1502-04187 | Application of Deep Neural Network in Estimation of the Weld Bead Parameters |  http://arxiv.org/abs/1502.04187  | author:Soheil Keshmiri, Xin Zheng, Chee Meng Chew, Chee Khiang Pang category:cs.LG published:2015-02-14 summary:We present a deep learning approach to estimation of the bead parameters inwelding tasks. Our model is based on a four-hidden-layer neural networkarchitecture. More specifically, the first three hidden layers of thisarchitecture utilize Sigmoid function to produce their respective intermediateoutputs. On the other hand, the last hidden layer uses a linear transformationto generate the final output of this architecture. This transforms our deepnetwork architecture from a classifier to a non-linear regression model. Wecompare the performance of our deep network with a selected number of resultsin the literature to show a considerable improvement in reducing the errors inestimation of these values. Furthermore, we show its scalability on estimatingthe weld bead parameters with same level of accuracy on combination of datasetsthat pertain to different welding techniques. This is a nontrivial result thatis counter-intuitive to the general belief in this field of research.
arxiv-1502-03913 | Skeleton Matching based approach for Text Localization in Scene Images |  http://arxiv.org/abs/1502.03913  | author:B. H. Shekar, Smitha M. L category:cs.CV published:2015-02-13 summary:In this paper, we propose a skeleton matching based approach which aids intext localization in scene images. The input image is preprocessed andsegmented into blocks using connected component analysis. We obtain theskeleton of the segmented block using morphology based approach. Theskeletonized images are compared with the trained templates in the database tocategorize into text and non-text blocks. Further, the newly designedgeometrical rules and morphological operations are employed on the detectedtext blocks for scene text localization. The experimental results obtained onpublicly available standard datasets illustrate that the proposed method candetect and localize the texts of various sizes, fonts and colors.
arxiv-1502-03879 | Semi-supervised Data Representation via Affinity Graph Learning |  http://arxiv.org/abs/1502.03879  | author:Weiya Ren category:cs.LG cs.CV 68T10 I.4.2; I.4.7 published:2015-02-13 summary:We consider the general problem of utilizing both labeled and unlabeled datato improve data representation performance. A new semi-supervised learningframework is proposed by combing manifold regularization and datarepresentation methods such as Non negative matrix factorization and sparsecoding. We adopt unsupervised data representation methods as the learningmachines because they do not depend on the labeled data, which can improvemachine's generation ability as much as possible. The proposed framework formsthe Laplacian regularizer through learning the affinity graph. We incorporatethe new Laplacian regularizer into the unsupervised data representation tosmooth the low dimensional representation of data and make use of labelinformation. Experimental results on several real benchmark datasets indicatethat our semi-supervised learning framework achieves encouraging resultscompared with state-of-art methods.
arxiv-1502-03918 | Gradient Difference based approach for Text Localization in Compressed domain |  http://arxiv.org/abs/1502.03918  | author:B. H. Shekar, Smitha M. L category:cs.CV published:2015-02-13 summary:In this paper, we propose a gradient difference based approach to textlocalization in videos and scene images. The input video frame/ image is firstcompressed using multilevel 2-D wavelet transform. The edge information of thereconstructed image is found which is further used for finding the maximumgradient difference between the pixels and then the boundaries of the detectedtext blocks are computed using zero crossing technique. We perform logical ANDoperation of the text blocks obtained by gradient difference and the zerocrossing technique followed by connected component analysis to eliminate thefalse positives. Finally, the morphological dilation operation is employed onthe detected text blocks for scene text localization. The experimental resultsobtained on publicly available standard datasets illustrate that the proposedmethod can detect and localize the texts of various sizes, fonts and colors.
arxiv-1502-03919 | Policy Gradient for Coherent Risk Measures |  http://arxiv.org/abs/1502.03919  | author:Aviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, Shie Mannor category:cs.AI cs.LG stat.ML published:2015-02-13 summary:Several authors have recently developed risk-sensitive policy gradientmethods that augment the standard expected cost minimization problem with ameasure of variability in cost. These studies have focused on specificrisk-measures, such as the variance or conditional value at risk (CVaR). Inthis work, we extend the policy gradient method to the whole class of coherentrisk measures, which is widely accepted in finance and operations research,among other fields. We consider both static and time-consistent dynamic riskmeasures. For static risk measures, our approach is in the spirit of policygradient algorithms and combines a standard sampling approach with convexprogramming. For dynamic risk measures, our approach is actor-critic style andinvolves explicit approximation of value function. Most importantly, ourcontribution presents a unified approach to risk-sensitive reinforcementlearning that generalizes and extends previous results.
arxiv-1502-04081 | A Linear Dynamical System Model for Text |  http://arxiv.org/abs/1502.04081  | author:David Belanger, Sham Kakade category:stat.ML cs.CL cs.LG published:2015-02-13 summary:Low dimensional representations of words allow accurate NLP models to betrained on limited annotated data. While most representations ignore words'local context, a natural way to induce context-dependent representations is toperform inference in a probabilistic latent-variable sequence model. Given therecent success of continuous vector space word representations, we provide suchan inference procedure for continuous states, where words' representations aregiven by the posterior mean of a linear dynamical system. Here, efficientinference can be performed using Kalman filtering. Our learning algorithm isextremely scalable, operating on simple cooccurrence counts for both parameterinitialization using the method of moments and subsequent iterations of EM. Inour experiments, we employ our inferred word embeddings as features in standardtagging tasks, obtaining significant accuracy improvements. Finally, the Kalmanfilter updates can be seen as a linear recurrent neural network. We demonstratethat using the parameters of our model to initialize a non-linear recurrentneural network language model reduces its training time by a day and yieldslower perplexity.
arxiv-1502-03939 | Polynomial-Chaos-based Kriging |  http://arxiv.org/abs/1502.03939  | author:R. Schoebi, B. Sudret, J. Wiart category:stat.CO stat.ME stat.ML published:2015-02-13 summary:Computer simulation has become the standard tool in many engineering fieldsfor designing and optimizing systems, as well as for assessing theirreliability. To cope with demanding analysis such as optimization andreliability, surrogate models (a.k.a meta-models) have been increasinglyinvestigated in the last decade. Polynomial Chaos Expansions (PCE) and Krigingare two popular non-intrusive meta-modelling techniques. PCE surrogates thecomputational model with a series of orthonormal polynomials in the inputvariables where polynomials are chosen in coherency with the probabilitydistributions of those input variables. On the other hand, Kriging assumes thatthe computer model behaves as a realization of a Gaussian random process whoseparameters are estimated from the available computer runs, i.e. input vectorsand response values. These two techniques have been developed more or less inparallel so far with little interaction between the researchers in the twofields. In this paper, PC-Kriging is derived as a new non-intrusivemeta-modeling approach combining PCE and Kriging. A sparse set of orthonormalpolynomials (PCE) approximates the global behavior of the computational modelwhereas Kriging manages the local variability of the model output. An adaptivealgorithm similar to the least angle regression algorithm determines theoptimal sparse set of polynomials. PC-Kriging is validated on various benchmarkanalytical functions which are easy to sample for reference results. From thenumerical investigations it is concluded that PC-Kriging performs better thanor at least as good as the two distinct meta-modeling techniques. A larger gainin accuracy is obtained when the experimental design has a limited size, whichis an asset when dealing with demanding computational models.
arxiv-1502-04149 | Joint Optimization of Masks and Deep Recurrent Neural Networks for Monaural Source Separation |  http://arxiv.org/abs/1502.04149  | author:Po-Sen Huang, Minje Kim, Mark Hasegawa-Johnson, Paris Smaragdis category:cs.SD cs.AI cs.LG cs.MM published:2015-02-13 summary:Monaural source separation is important for many real world applications. Itis challenging because, with only a single channel of information available,without any constraints, an infinite number of solutions are possible. In thispaper, we explore joint optimization of masking functions and deep recurrentneural networks for monaural source separation tasks, including monaural speechseparation, monaural singing voice separation, and speech denoising. The jointoptimization of the deep recurrent neural networks with an extra masking layerenforces a reconstruction constraint. Moreover, we explore a discriminativecriterion for training neural networks to further enhance the separationperformance. We evaluate the proposed system on the TSP, MIR-1K, and TIMITdatasets for speech separation, singing voice separation, and speech denoisingtasks, respectively. Our approaches achieve 2.30--4.98 dB SDR gain compared toNMF models in the speech separation task, 2.30--2.48 dB GNSDR gain and4.32--5.42 dB GSIR gain compared to existing models in the singing voiceseparation task, and outperform NMF and DNN baselines in the speech denoisingtask.
arxiv-1502-04137 | Non-Adaptive Learning a Hidden Hipergraph |  http://arxiv.org/abs/1502.04137  | author:Hasan Abasi, Nader H. Bshouty, Hanna Mazzawi category:cs.LG published:2015-02-13 summary:We give a new deterministic algorithm that non-adaptively learns a hiddenhypergraph from edge-detecting queries. All previous non-adaptive algorithmseither run in exponential time or have non-optimal query complexity. We givethe first polynomial time non-adaptive learning algorithm for learninghypergraph that asks almost optimal number of queries.
arxiv-1502-04110 | Modeling Brain Circuitry over a Wide Range of Scales |  http://arxiv.org/abs/1502.04110  | author:Pascal Fua, Graham Knott category:cs.CV published:2015-02-13 summary:If we are ever to unravel the mysteries of brain function at its mostfundamental level, we will need a precise understanding of how its componentneurons connect to each other. Electron Microscopes (EM) can now provide thenanometer resolution that is needed to image synapses, and thereforeconnections, while Light Microscopes (LM) see at the micrometer resolutionrequired to model the 3D structure of the dendritic network. Since both thetopology and the connection strength are integral parts of the brain's wiringdiagram, being able to combine these two modalities is critically important. In fact, these microscopes now routinely produce high-resolution imagery insuch large quantities that the bottleneck becomes automated processing andinterpretation, which is needed for such data to be exploited to its fullpotential. In this paper, we briefly review the Computer Vision techniques wehave developed at EPFL to address this need. They include delineating dendriticarbors from LM imagery, segmenting organelles from EM, and combining the twointo a consistent representation.
arxiv-1502-04042 | Abstract Learning via Demodulation in a Deep Neural Network |  http://arxiv.org/abs/1502.04042  | author:Andrew J. R. Simpson category:cs.LG cs.NE 68Txx published:2015-02-13 summary:Inspired by the brain, deep neural networks (DNN) are thought to learnabstract representations through their hierarchical architecture. However, atpresent, how this happens is not well understood. Here, we demonstrate that DNNlearn abstract representations by a process of demodulation. We introduce abiased sigmoid activation function and use it to show that DNN learn andperform better when optimized for demodulation. Our findings constitute thefirst unambiguous evidence that DNN perform abstract learning in practical use.Our findings may also explain abstract learning in the human brain.
arxiv-1502-04148 | A Pseudo-Euclidean Iteration for Optimal Recovery in Noisy ICA |  http://arxiv.org/abs/1502.04148  | author:James Voss, Mikhail Belkin, Luis Rademacher category:cs.LG stat.ML published:2015-02-13 summary:Independent Component Analysis (ICA) is a popular model for blind signalseparation. The ICA model assumes that a number of independent source signalsare linearly mixed to form the observed signals. We propose a new algorithm,PEGI (for pseudo-Euclidean Gradient Iteration), for provable model recovery forICA with Gaussian noise. The main technical innovation of the algorithm is touse a fixed point iteration in a pseudo-Euclidean (indefinite "inner product")space. The use of this indefinite "inner product" resolves technical issuescommon to several existing algorithms for noisy ICA. This leads to an algorithmwhich is conceptually simple, efficient and accurate in testing. Our second contribution is combining PEGI with the analysis of objectives foroptimal recovery in the noisy ICA model. It has been observed that the directapproach of demixing with the inverse of the mixing matrix is suboptimal forsignal recovery in terms of the natural Signal to Interference plus Noise Ratio(SINR) criterion. There have been several partial solutions proposed in the ICAliterature. It turns out that any solution to the mixing matrix reconstructionproblem can be used to construct an SINR-optimal ICA demixing, despite the factthat SINR itself cannot be computed from data. That allows us to obtain apractical and provably SINR-optimal recovery method for ICA with arbitraryGaussian noise.
arxiv-1502-04049 | How essential are unstructured clinical narratives and information fusion to clinical trial recruitment? |  http://arxiv.org/abs/1502.04049  | author:Preethi Raghavan, James L. Chen, Eric Fosler-Lussier, Albert M. Lai category:cs.CY cs.AI cs.CL published:2015-02-13 summary:Electronic health records capture patient information using structuredcontrolled vocabularies and unstructured narrative text. While structured datatypically encodes lab values, encounters and medication lists, unstructureddata captures the physician's interpretation of the patient's condition,prognosis, and response to therapeutic intervention. In this paper, wedemonstrate that information extraction from unstructured clinical narrativesis essential to most clinical applications. We perform an empirical study tovalidate the argument and show that structured data alone is insufficient inresolving eligibility criteria for recruiting patients onto clinical trials forchronic lymphocytic leukemia (CLL) and prostate cancer. Unstructured data isessential to solving 59% of the CLL trial criteria and 77% of the prostatecancer trial criteria. More specifically, for resolving eligibility criteriawith temporal constraints, we show the need for temporal reasoning andinformation integration with medical events within and across unstructuredclinical narratives and structured data.
arxiv-1502-04132 | Long-short Term Motion Feature for Action Classification and Retrieval |  http://arxiv.org/abs/1502.04132  | author:Zhenzhong Lan, Xuanchong Li, Ming Lin, Alexander G. Hauptmann category:cs.CV published:2015-02-13 summary:We propose a method for representing motion information for videoclassification and retrieval. We improve upon local descriptor based methodsthat have been among the most popular and successful models for representingvideos. The desired local descriptors need to satisfy two requirements: 1) tobe representative, 2) to be discriminative. Therefore, they need to occurfrequently enough in the videos and to be be able to tell the difference amongdifferent types of motions. To generate such local descriptors, the videoblocks they are based on must contain just the right amount of motioninformation. However, current state-of-the-art local descriptor methods usevideo blocks with a single fixed size, which is insufficient for coveringactions with varying speeds. In this paper, we introduce a long-short termmotion feature that generates descriptors from video blocks with multiplelengths, thus covering motions with large speed variance. Experimental resultsshow that, albeit simple, our model achieves state-of-the-arts results onseveral benchmark datasets.
arxiv-1502-04033 | The Responsibility Weighted Mahalanobis Kernel for Semi-Supervised Training of Support Vector Machines for Classification |  http://arxiv.org/abs/1502.04033  | author:Tobias Reitmaier, Bernhard Sick category:cs.LG stat.ML published:2015-02-13 summary:Kernel functions in support vector machines (SVM) are needed to assess thesimilarity of input samples in order to classify these samples, for instance.Besides standard kernels such as Gaussian (i.e., radial basis function, RBF) orpolynomial kernels, there are also specific kernels tailored to considerstructure in the data for similarity assessment. In this article, we willcapture structure in data by means of probabilistic mixture density models, forexample Gaussian mixtures in the case of real-valued input spaces. From thedistance measures that are inherently contained in these models, e.g.,Mahalanobis distances in the case of Gaussian mixtures, we derive a new kernel,the responsibility weighted Mahalanobis (RWM) kernel. Basically, this kernelemphasizes the influence of model components from which any two samples thatare compared are assumed to originate (that is, the "responsible" modelcomponents). We will see that this kernel outperforms the RBF kernel and otherkernels capturing structure in data (such as the LAP kernel in Laplacian SVM)in many applications where partially labeled data are available, i.e., forsemi-supervised training of SVM. Other key advantages are that the RWM kernelcan easily be used with standard SVM implementations and training algorithmssuch as sequential minimal optimization, and heuristics known for theparametrization of RBF kernels in a C-SVM can easily be transferred to this newkernel. Properties of the RWM kernel are demonstrated with 20 benchmark datasets and an increasing percentage of labeled samples in the training data.
arxiv-1502-03496 | Spectral Sparsification of Random-Walk Matrix Polynomials |  http://arxiv.org/abs/1502.03496  | author:Dehua Cheng, Yu Cheng, Yan Liu, Richard Peng, Shang-Hua Teng category:cs.DS cs.DM cs.LG cs.SI stat.ML published:2015-02-12 summary:We consider a fundamental algorithmic question in spectral graph theory:Compute a spectral sparsifier of random-walk matrix-polynomial$$L_\alpha(G)=D-\sum_{r=1}^d\alpha_rD(D^{-1}A)^r$$ where $A$ is the adjacencymatrix of a weighted, undirected graph, $D$ is the diagonal matrix of weighteddegrees, and $\alpha=(\alpha_1...\alpha_d)$ are nonnegative coefficients with$\sum_{r=1}^d\alpha_r=1$. Recall that $D^{-1}A$ is the transition matrix ofrandom walks on the graph. The sparsification of $L_\alpha(G)$ appears to bealgorithmically challenging as the matrix power $(D^{-1}A)^r$ is defined by allpaths of length $r$, whose precise calculation would be prohibitivelyexpensive. In this paper, we develop the first nearly linear time algorithm for thissparsification problem: For any $G$ with $n$ vertices and $m$ edges, $d$coefficients $\alpha$, and $\epsilon > 0$, our algorithm runs in time$O(d^2m\log^2n/\epsilon^{2})$ to construct a Laplacian matrix$\tilde{L}=D-\tilde{A}$ with $O(n\log n/\epsilon^{2})$ non-zeros such that$\tilde{L}\approx_{\epsilon}L_\alpha(G)$. Matrix polynomials arise in mathematical analysis of matrix functions as wellas numerical solutions of matrix equations. Our work is particularly motivatedby the algorithmic problems for speeding up the classic Newton's method inapplications such as computing the inverse square-root of the precision matrixof a Gaussian random field, as well as computing the $q$th-root transition (for$q\geq1$) in a time-reversible Markov model. The key algorithmic step for bothapplications is the construction of a spectral sparsifier of a constant degreerandom-walk matrix-polynomials introduced by Newton's method. Our algorithm canalso be used to build efficient data structures for effective resistances formulti-step time-reversible Markov models, and we anticipate that it could beuseful for other tasks in network analysis.
arxiv-1502-03581 | Web spam classification using supervised artificial neural network algorithms |  http://arxiv.org/abs/1502.03581  | author:Ashish Chandra, Mohammad Suaib, Dr. Rizwan Beg category:cs.NE cs.LG published:2015-02-12 summary:Due to the rapid growth in technology employed by the spammers, there is aneed of classifiers that are more efficient, generic and highly adaptive.Neural Network based technologies have high ability of adaption as well asgeneralization. As per our knowledge, very little work has been done in thisfield using neural network. We present this paper to fill this gap. This paperevaluates performance of three supervised learning algorithms of artificialneural network by creating classifiers for the complex problem of latest webspam pattern classification. These algorithms are Conjugate Gradient algorithm,Resilient Backpropagation learning, and Levenberg-Marquardt algorithm.
arxiv-1502-03532 | An equalised global graphical model-based approach for multi-camera object tracking |  http://arxiv.org/abs/1502.03532  | author:Lijun Cao, Weihua Chen, Xiaotang Chen, Shuai Zheng, Kaiqi Huang category:cs.CV published:2015-02-12 summary:Multi-camera non-overlapping visual object tracking system typically consistsof two tasks: single camera object tracking and inter-camera object tracking.Since the state-of-the-art approaches are yet not perform perfectly in realscenes, the errors in single camera object tracking module would propagate intothe module of inter-camera object tracking, resulting much lower overallperformance. In order to address this problem, we develop an approach thatjointly optimise the single camera object tracking and inter-camera objecttracking in an equalised global graphical model. Such an approach has theadvantage of guaranteeing a good overall tracking performance even when thereare limited amount of false tracking in single camera object tracking. Besides,the similarity metrics used in our approach improve the compatibility of themetrics used in the two different tasks. Results show that our approach achievethe state-of-the-art results in multi-camera non-overlapping tracking datasets.
arxiv-1502-03536 | Speeding up Permutation Testing in Neuroimaging |  http://arxiv.org/abs/1502.03536  | author:Chris Hinrichs, Vamsi K Ithapu, Qinyuan Sun, Sterling C Johnson, Vikas Singh category:stat.CO cs.AI stat.ML published:2015-02-12 summary:Multiple hypothesis testing is a significant problem in nearly allneuroimaging studies. In order to correct for this phenomena, we require areliable estimate of the Family-Wise Error Rate (FWER). The well knownBonferroni correction method, while simple to implement, is quite conservative,and can substantially under-power a study because it ignores dependenciesbetween test statistics. Permutation testing, on the other hand, is an exact,non-parametric method of estimating the FWER for a given $\alpha$-threshold,but for acceptably low thresholds the computational burden can be prohibitive.In this paper, we show that permutation testing in fact amounts to populatingthe columns of a very large matrix ${\bf P}$. By analyzing the spectrum of thismatrix, under certain conditions, we see that ${\bf P}$ has a low-rank plus alow-variance residual decomposition which makes it suitable for highlysub--sampled --- on the order of $0.5\%$ --- matrix completion methods. Basedon this observation, we propose a novel permutation testing methodology whichoffers a large speedup, without sacrificing the fidelity of the estimated FWER.Our evaluations on four different neuroimaging datasets show that acomputational speedup factor of roughly $50\times$ can be achieved whilerecovering the FWER distribution up to very high accuracy. Further, we showthat the estimated $\alpha$-threshold is also recovered faithfully, and isstable.
arxiv-1502-03505 | Supervised LogEuclidean Metric Learning for Symmetric Positive Definite Matrices |  http://arxiv.org/abs/1502.03505  | author:Florian Yger, Masashi Sugiyama category:cs.LG published:2015-02-12 summary:Metric learning has been shown to be highly effective to improve theperformance of nearest neighbor classification. In this paper, we address theproblem of metric learning for Symmetric Positive Definite (SPD) matrices suchas covariance matrices, which arise in many real-world applications. Naivelyusing standard Mahalanobis metric learning methods under the Euclidean geometryfor SPD matrices is not appropriate, because the difference of SPD matrices canbe a non-SPD matrix and thus the obtained solution can be uninterpretable. Tocope with this problem, we propose to use a properly parameterized LogEuclideandistance and optimize the metric with respect to kernel-target alignment, whichis a supervised criterion for kernel learning. Then the resulting non-trivialoptimization problem is solved by utilizing the Riemannian geometry. Finally,we experimentally demonstrate the usefulness of our LogEuclidean metriclearning algorithm on real-world classification tasks for EEG signals andtexture patches.
arxiv-1502-03509 | MADE: Masked Autoencoder for Distribution Estimation |  http://arxiv.org/abs/1502.03509  | author:Mathieu Germain, Karol Gregor, Iain Murray, Hugo Larochelle category:cs.LG cs.NE stat.ML published:2015-02-12 summary:There has been a lot of recent interest in designing neural network models toestimate a distribution from a set of examples. We introduce a simplemodification for autoencoder neural networks that yields powerful generativemodels. Our method masks the autoencoder's parameters to respect autoregressiveconstraints: each input is reconstructed only from previous inputs in a givenordering. Constrained this way, the autoencoder outputs can be interpreted as aset of conditional probabilities, and their product, the full jointprobability. We can also train a single network that can decompose the jointprobability in multiple different orderings. Our simple framework can beapplied to multiple architectures, including deep ones. Vectorizedimplementations, such as on GPUs, are simple and fast. Experiments demonstratethat this approach is competitive with state-of-the-art tractable distributionestimators. At test time, the method is significantly faster and scales betterthan other autoregressive estimators.
arxiv-1502-03537 | Convergence of gradient based pre-training in Denoising autoencoders |  http://arxiv.org/abs/1502.03537  | author:Vamsi K Ithapu, Sathya Ravi, Vikas Singh category:cs.LG cs.CV math.OC published:2015-02-12 summary:The success of deep architectures is at least in part attributed to thelayer-by-layer unsupervised pre-training that initializes the network. Variouspapers have reported extensive empirical analysis focusing on the design andimplementation of good pre-training procedures. However, an understandingpertaining to the consistency of parameter estimates, the convergence oflearning procedures and the sample size estimates is still unavailable in theliterature. In this work, we study pre-training in classical and distributeddenoising autoencoders with these goals in mind. We show that the gradientconverges at the rate of $\frac{1}{\sqrt{N}}$ and has a sub-linear dependenceon the size of the autoencoder network. In a distributed setting where disjointsections of the whole network are pre-trained synchronously, we show that theconvergence improves by at least $\tau^{3/4}$, where $\tau$ corresponds to thesize of the sections. We provide a broad set of experiments to empiricallyevaluate the suggested behavior.
arxiv-1502-03656 | Quasi-Newton particle Metropolis-Hastings |  http://arxiv.org/abs/1502.03656  | author:Johan Dahlin, Fredrik Lindsten, Thomas B. Schön category:stat.CO q-fin.CP stat.ML published:2015-02-12 summary:Particle Metropolis-Hastings enables Bayesian parameter inference in generalnonlinear state space models (SSMs). However, in many implementations a randomwalk proposal is used and this can result in poor mixing if not tuned correctlyusing tedious pilot runs. Therefore, we consider a new proposal inspired byquasi-Newton algorithms that may achieve similar (or better) mixing with lesstuning. An advantage compared to other Hessian based proposals, is that it onlyrequires estimates of the gradient of the log-posterior. A possible applicationis parameter inference in the challenging class of SSMs with intractablelikelihoods. We exemplify this application and the benefits of the new proposalby modelling log-returns of future contracts on coffee by a stochasticvolatility model with $\alpha$-stable observations.
arxiv-1502-03571 | Weighted SGD for $\ell_p$ Regression with Randomized Preconditioning |  http://arxiv.org/abs/1502.03571  | author:Jiyan Yang, Yin-Lam Chow, Christopher Ré, Michael W. Mahoney category:math.OC stat.ML published:2015-02-12 summary:In recent years, stochastic gradient descent (SGD) methods and randomizedlinear algebra (RLA) algorithms have been applied to many large-scale problemsin machine learning and data analysis. We aim to bridge the gap between thesetwo methods in solving constrained overdetermined linear regressionproblems---e.g., $\ell_2$ and $\ell_1$ regression problems. We propose a hybridalgorithm named pwSGD that uses RLA techniques for preconditioning andconstructing an importance sampling distribution, and then performs an SGD-likeiterative process with weighted sampling on the preconditioned system. We provethat pwSGD inherits faster convergence rates that only depend on the lowerdimension of the linear system, while maintaining low computation complexity.Particularly, when solving $\ell_1$ regression with size $n$ by $d$, pwSGDreturns an approximate solution with $\epsilon$ relative error in the objectivevalue in $\mathcal{O}(\log n \cdot \text{nnz}(A) + \text{poly}(d)/\epsilon^2)$time. This complexity is uniformly better than that of RLA methods in terms ofboth $\epsilon$ and $d$ when the problem is unconstrained. For $\ell_2$regression, pwSGD returns an approximate solution with $\epsilon$ relativeerror in the objective value and the solution vector measured in predictionnorm in $\mathcal{O}(\log n \cdot \text{nnz}(A) + \text{poly}(d)\log(1/\epsilon) /\epsilon)$ time. We also provide lower bounds on the coresetcomplexity for more general regression problems, indicating that still newideas will be needed to extend similar RLA preconditioning ideas to weightedSGD algorithms for more general regression problems. Finally, the effectivenessof such algorithms is illustrated numerically on both synthetic and realdatasets.
arxiv-1502-03696 | Monte Carlo Planning method estimates planning horizons during interactive social exchange |  http://arxiv.org/abs/1502.03696  | author:Andreas Hula, P. Read Montague, Peter Dayan category:stat.ML published:2015-02-12 summary:Reciprocating interactions represent a central feature of all humanexchanges. They have been the target of various recent experiments, withhealthy participants and psychiatric populations engaging as dyads inmulti-round exchanges such as a repeated trust task. Behaviour in suchexchanges involves complexities related to each agent's preference for equitywith their partner, beliefs about the partner's appetite for equity, beliefsabout the partner's model of their partner, and so on. Agents may also plandifferent numbers of steps into the future. Providing a computationally preciseaccount of the behaviour is an essential step towards understanding whatunderlies choices. A natural framework for this is that of an interactivepartially observable Markov decision process (IPOMDP). However, the variouscomplexities make IPOMDPs inordinately computationally challenging. Here, weshow how to approximate the solution for the multi-round trust task using avariant of the Monte-Carlo tree search algorithm. We demonstrate that thealgorithm is efficient and effective, and therefore can be used to invertobservations of behavioural choices. We use generated behaviour to elucidatethe richness and sophistication of interactive inference.
arxiv-1502-03655 | Newton-based maximum likelihood estimation in nonlinear state space models |  http://arxiv.org/abs/1502.03655  | author:Manon Kok, Johan Dahlin, Thomas B. Schön, Adrian Wills category:stat.CO stat.ML published:2015-02-12 summary:Maximum likelihood (ML) estimation using Newton's method in nonlinear statespace models (SSMs) is a challenging problem due to the analyticalintractability of the log-likelihood and its gradient and Hessian. We estimatethe gradient and Hessian using Fisher's identity in combination with asmoothing algorithm. We explore two approximations of the log-likelihood and ofthe solution of the smoothing problem. The first is a linearizationapproximation which is computationally cheap, but the accuracy typically variesbetween models. The second is a sampling approximation which is asymptoticallyvalid for any SSM but is more computationally costly. We demonstrate ourapproach for ML parameter estimation on simulated data from two different SSMswith encouraging results.
arxiv-1502-03596 | Towards zero-configuration condition monitoring based on dictionary learning |  http://arxiv.org/abs/1502.03596  | author:Sergio Martin-del-Campo, Fredrik Sandin category:cs.CV published:2015-02-12 summary:Condition-based predictive maintenance can significantly improve overallequipment effectiveness provided that appropriate monitoring methods are used.Online condition monitoring systems are customized to each type of machine andneed to be reconfigured when conditions change, which is costly and requiresexpert knowledge. Basic feature extraction methods limited to signaldistribution functions and spectra are commonly used, making it difficult toautomatically analyze and compare machine conditions. In this paper, weinvestigate the possibility to automate the condition monitoring process bycontinuously learning a dictionary of optimized shift-invariant feature vectorsusing a well-known sparse approximation method. We study how the featurevectors learned from a vibration signal evolve over time when a fault developswithin a ball bearing of a rotating machine. We quantify the adaptation rate oflearned features and find that this quantity changes significantly in thetransitions between normal and faulty states of operation of the ball bearing.
arxiv-1502-03601 | A Predictive System for detection of Bankruptcy using Machine Learning techniques |  http://arxiv.org/abs/1502.03601  | author:Kalyan Nagaraj, Amulyashree Sridhar category:cs.LG published:2015-02-12 summary:Bankruptcy is a legal procedure that claims a person or organization as adebtor. It is essential to ascertain the risk of bankruptcy at initial stagesto prevent financial losses. In this perspective, different soft computingtechniques can be employed to ascertain bankruptcy. This study proposes abankruptcy prediction system to categorize the companies based on extent ofrisk. The prediction system acts as a decision support tool for detection ofbankruptcy Keywords: Bankruptcy, soft computing, decision support tool
arxiv-1502-03630 | Ordering-sensitive and Semantic-aware Topic Modeling |  http://arxiv.org/abs/1502.03630  | author:Min Yang, Tianyi Cui, Wenting Tu category:cs.LG cs.CL cs.IR published:2015-02-12 summary:Topic modeling of textual corpora is an important and challenging problem. Inmost previous work, the "bag-of-words" assumption is usually made which ignoresthe ordering of words. This assumption simplifies the computation, but itunrealistically loses the ordering information and the semantic of words in thecontext. In this paper, we present a Gaussian Mixture Neural Topic Model(GMNTM) which incorporates both the ordering of words and the semantic meaningof sentences into topic modeling. Specifically, we represent each topic as acluster of multi-dimensional vectors and embed the corpus into a collection ofvectors generated by the Gaussian mixture model. Each word is affected not onlyby its topic, but also by the embedding vector of its surrounding words and thecontext. The Gaussian mixture components and the topic of documents, sentencesand words can be learnt jointly. Extensive experiments show that our model canlearn better topics and more accurate word distributions for each topic.Quantitatively, comparing to state-of-the-art topic modeling approaches, GMNTMobtains significantly better performance in terms of perplexity, retrievalaccuracy and classification accuracy.
arxiv-1502-03508 | Adding vs. Averaging in Distributed Primal-Dual Optimization |  http://arxiv.org/abs/1502.03508  | author:Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I. Jordan, Peter Richtárik, Martin Takáč category:cs.LG 90C25, 68W15 G.1.6; C.1.4 published:2015-02-12 summary:Distributed optimization methods for large-scale machine learning suffer froma communication bottleneck. It is difficult to reduce this bottleneck whilestill efficiently and accurately aggregating partial work from differentmachines. In this paper, we present a novel generalization of the recentcommunication-efficient primal-dual framework (CoCoA) for distributedoptimization. Our framework, CoCoA+, allows for additive combination of localupdates to the global parameters at each iteration, whereas previous schemeswith convergence guarantees only allow conservative averaging. We give stronger(primal-dual) convergence rate guarantees for both CoCoA as well as our newvariants, and generalize the theory for both methods to cover non-smooth convexloss functions. We provide an extensive experimental comparison that shows themarkedly improved performance of CoCoA+ on several real-world distributeddatasets, especially when scaling up the number of machines.
arxiv-1502-03648 | Over-Sampling in a Deep Neural Network |  http://arxiv.org/abs/1502.03648  | author:Andrew J. R. Simpson category:cs.LG cs.NE 68Txx published:2015-02-12 summary:Deep neural networks (DNN) are the state of the art on many engineeringproblems such as computer vision and audition. A key factor in the success ofthe DNN is scalability - bigger networks work better. However, the reason forthis scalability is not yet well understood. Here, we interpret the DNN as adiscrete system, of linear filters followed by nonlinear activations, that issubject to the laws of sampling theory. In this context, we demonstrate thatover-sampled networks are more selective, learn faster and learn more robustly.Our findings may ultimately generalize to the human brain.
arxiv-1502-03520 | RAND-WALK: A Latent Variable Model Approach to Word Embeddings |  http://arxiv.org/abs/1502.03520  | author:Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, Andrej Risteski category:cs.LG cs.CL stat.ML published:2015-02-12 summary:Semantic word embeddings represent the meaning of a word via a vector, andare created by diverse methods such as Latent Semantic Analysis (LSA),generative text models such as topic models, matrix factorization, neural nets,and energy-based models. Many methods use nonlinear operations ---such asPairwise Mutual Information or PMI--- on co-occurrence statistics, and havehand-tuned hyperparameters and reweightings. Often a {\em generative model} can help provide theoretical insight into suchmodeling choices, but there appears to be no such model to "explain" the abovenonlinear models. For example, we know of no generative model for which thecorrect solution is the usual (dimension-restricted) PMI model. This paper gives a new generative model, a dynamic version of the loglineartopic model of \citet{mnih2007three}. The methodological novelty is to use theprior to compute {\em closed form} expressions for word statistics. Theseprovide an explanation for nonlinear models like PMI, {\bf word2vec}, andGloVe, as well as some hyperparameter choices. Experimental support is provided for the generative model assumptions, themost important of which is that latent word vectors are fairly uniformlydispersed ("isotropic") in space. The model also helps explain why low-dimensional semantic embeddings containlinear algebraic structure that allows solution of word analogies, as shownby~\citet{mikolov2013efficient} and many subsequent papers.
arxiv-1502-03529 | Scalable Stochastic Alternating Direction Method of Multipliers |  http://arxiv.org/abs/1502.03529  | author:Shen-Yi Zhao, Wu-Jun Li, Zhi-Hua Zhou category:cs.LG published:2015-02-12 summary:Stochastic alternating direction method of multipliers (ADMM), which visitsonly one sample or a mini-batch of samples each time, has recently been provedto achieve better performance than batch ADMM. However, most stochastic methodscan only achieve a convergence rate $O(1/\sqrt T)$ on general convexproblems,where T is the number of iterations. Hence, these methods are notscalable with respect to convergence rate (computation cost). There exists onlyone stochastic method, called SA-ADMM, which can achieve convergence rate$O(1/T)$ on general convex problems. However, an extra memory is needed forSA-ADMM to store the historic gradients on all samples, and thus it is notscalable with respect to storage cost. In this paper, we propose a novelmethod, called scalable stochastic ADMM(SCAS-ADMM), for large-scaleoptimization and learning problems. Without the need to store the historicgradients, SCAS-ADMM can achieve the same convergence rate $O(1/T)$ as the beststochastic method SA-ADMM and batch ADMM on general convex problems.Experiments on graph-guided fused lasso show that SCAS-ADMM can achievestate-of-the-art performance in real applications
arxiv-1502-03682 | Applying deep learning techniques on medical corpora from the World Wide Web: a prototypical system and evaluation |  http://arxiv.org/abs/1502.03682  | author:Jose Antonio Miñarro-Giménez, Oscar Marín-Alonso, Matthias Samwald category:cs.CL cs.IR cs.LG cs.NE published:2015-02-12 summary:BACKGROUND: The amount of biomedical literature is rapidly growing and it isbecoming increasingly difficult to keep manually curated knowledge bases andontologies up-to-date. In this study we applied the word2vec deep learningtoolkit to medical corpora to test its potential for identifying relationshipsfrom unstructured text. We evaluated the efficiency of word2vec in identifyingproperties of pharmaceuticals based on mid-sized, unstructured medical textcorpora available on the web. Properties included relationships to diseases('may treat') or physiological processes ('has physiological effect'). Wecompared the relationships identified by word2vec with manually curatedinformation from the National Drug File - Reference Terminology (NDF-RT)ontology as a gold standard. RESULTS: Our results revealed a maximum accuracyof 49.28% which suggests a limited ability of word2vec to capture linguisticregularities on the collected medical corpora compared with other publishedresults. We were able to document the influence of different parameter settingson result accuracy and found and unexpected trade-off between ranking qualityand accuracy. Pre-processing corpora to reduce syntactic variability proved tobe a good strategy for increasing the utility of the trained vector models.CONCLUSIONS: Word2vec is a very efficient implementation for computing vectorrepresentations and for its ability to identify relationships in textual datawithout any prior domain knowledge. We found that the ranking and retrievedresults generated by word2vec were not of sufficient quality for automaticpopulation of knowledge bases and ontologies, but could serve as a startingpoint for further manual curation.
arxiv-1502-03699 | Analysis of Solution Quality of a Multiobjective Optimization-based Evolutionary Algorithm for Knapsack Problem |  http://arxiv.org/abs/1502.03699  | author:Jun He, Yong Wang, Yuren Zhou category:cs.NE published:2015-02-12 summary:Multi-objective optimisation is regarded as one of the most promising waysfor dealing with constrained optimisation problems in evolutionaryoptimisation. This paper presents a theoretical investigation of amulti-objective optimisation evolutionary algorithm for solving the 0-1knapsack problem. Two initialisation methods are considered in the algorithm:local search initialisation and greedy search initialisation. Then the solutionquality of the algorithm is analysed in terms of the approximation ratio.
arxiv-1502-03723 | Simulation of Color Blindness and a Proposal for Using Google Glass as Color-correcting Tool |  http://arxiv.org/abs/1502.03723  | author:H. M. de Oliveira, J. Ranhel, R. B. A. Alves category:cs.HC cs.CV published:2015-02-12 summary:The human visual color response is driven by specialized cells called cones,which exist in three types, viz. R, G, and B. Software is developed to simulatehow color images are displayed for different types of color blindness.Specified the default color deficiency associated with a user, it generates apreview of the rainbow (in the visible range, from red to violet) and shows up,side by side with a colorful image provided as input, the display correspondentcolorblind. The idea is to provide an image processing after image acquisitionto enable a better perception ofcolors by the color blind. Examples ofpseudo-correction are shown for the case of Protanopia (red blindness). Thesystem is adapted into a screen of an i-pad or a cellphone in which thecolorblind observe the camera, the image processed with color detail previouslyimperceptible by his naked eye. As prospecting, wearable computer glasses couldbe manufactured to provide a corrected image playback. The approach can alsoprovide augmented reality for human vision by adding the UV or IR responses asa new feature of Google Glass.
arxiv-1502-03752 | A new hybrid metric for verifying parallel corpora of Arabic-English |  http://arxiv.org/abs/1502.03752  | author:Saad Alkahtani, Wei Liu, William J. Teahan category:cs.CL published:2015-02-12 summary:This paper discusses a new metric that has been applied to verify the qualityin translation between sentence pairs in parallel corpora of Arabic-English.This metric combines two techniques, one based on sentence length and the otherbased on compression code length. Experiments on sample test parallelArabic-English corpora indicate the combination of these two techniquesimproves accuracy of the identification of satisfactory and unsatisfactorysentence pairs compared to sentence length and compression code length alone.The new method proposed in this research is effective at filtering noise andreducing mis-translations resulting in greatly improved quality.
arxiv-1502-03851 | Discovering Human Interactions in Videos with Limited Data Labeling |  http://arxiv.org/abs/1502.03851  | author:Mehran Khodabandeh, Arash Vahdat, Guang-Tong Zhou, Hossein Hajimirsadeghi, Mehrsan Javan Roshtkhari, Greg Mori, Stephen Se category:cs.CV published:2015-02-12 summary:We present a novel approach for discovering human interactions in videos.Activity understanding techniques usually require a large number of labeledexamples, which are not available in many practical cases. Here, we focus onrecovering semantically meaningful clusters of human-human and human-objectinteraction in an unsupervised fashion. A new iterative solution is introducedbased on Maximum Margin Clustering (MMC), which also accepts user feedback torefine clusters. This is achieved by formulating the whole process as a unifiedconstrained latent max-margin clustering problem. Extensive experiments havebeen carried out over three challenging datasets, Collective Activity, VIRAT,and UT-interaction. Empirical results demonstrate that the proposed algorithmcan efficiently discover perfect semantic clusters of human interactions withonly a small amount of labeling effort.
arxiv-1502-03671 | Phrase-based Image Captioning |  http://arxiv.org/abs/1502.03671  | author:Rémi Lebret, Pedro O. Pinheiro, Ronan Collobert category:cs.CL published:2015-02-12 summary:Generating a novel textual description of an image is an interesting problemthat connects computer vision and natural language processing. In this paper,we present a simple model that is able to generate descriptive sentences givena sample image. This model has a strong focus on the syntax of thedescriptions. We train a purely bilinear model that learns a metric between animage representation (generated from a previously trained Convolutional NeuralNetwork) and phrases that are used to described them. The system is then ableto infer phrases from a given image sample. Based on caption syntax statistics,we propose a simple language model that can produce relevant descriptions for agiven test image using the phrases inferred. Our approach, which isconsiderably simpler than state-of-the-art models, achieves comparable resultsin two popular datasets for the task: Flickr30k and the recently proposedMicrosoft COCO.
arxiv-1502-03322 | Boost Phrase-level Polarity Labelling with Review-level Sentiment Classification |  http://arxiv.org/abs/1502.03322  | author:Yongfeng Zhang, Min Zhang, Yiqun Liu, Shaoping Ma category:cs.CL cs.AI published:2015-02-11 summary:Sentiment analysis on user reviews helps to keep track of user reactionstowards products, and make advices to users about what to buy. State-of-the-artreview-level sentiment classification techniques could give pretty goodprecisions of above 90%. However, current phrase-level sentiment analysisapproaches might only give sentiment polarity labelling precisions of around70%~80%, which is far from satisfaction and restricts its application in manypractical tasks. In this paper, we focus on the problem of phrase-levelsentiment polarity labelling and attempt to bridge the gap between phrase-leveland review-level sentiment analysis. We investigate the inconsistency betweenthe numerical star ratings and the sentiment orientation of textual userreviews. Although they have long been treated as identical, which serves as abasic assumption in previous work, we find that this assumption is notnecessarily true. We further propose to leverage the results of review-levelsentiment classification to boost the performance of phrase-level polaritylabelling using a novel constrained convex optimization framework. Besides, theframework is capable of integrating various kinds of information sources andheuristics, while giving the global optimal solution due to its convexity.Experimental results on both English and Chinese reviews show that ourframework achieves high labelling precisions of up to 89%, which is asignificant improvement from current approaches.
arxiv-1502-03296 | Statistical laws in linguistics |  http://arxiv.org/abs/1502.03296  | author:Eduardo G. Altmann, Martin Gerlach category:physics.soc-ph cs.LG published:2015-02-11 summary:Zipf's law is just one out of many universal laws proposed to describestatistical regularities in language. Here we review and critically discuss howthese laws can be statistically interpreted, fitted, and tested (falsified).The modern availability of large databases of written text allows for testswith an unprecedent statistical accuracy and also a characterization of thefluctuations around the typical behavior. We find that fluctuations are usuallymuch larger than expected based on simplifying statistical assumptions (e.g.,independence and lack of correlations between observations).Thesesimplifications appear also in usual statistical tests so that the largefluctuations can be erroneously interpreted as a falsification of the law.Instead, here we argue that linguistic laws are only meaningful (falsifiable)if accompanied by a model for which the fluctuations can be computed (e.g., agenerative model of the text). The large fluctuations we report show that theconstraints imposed by linguistic laws on the creativity process of textgeneration are not as tight as one could expect.
arxiv-1502-03175 | Proximal Algorithms in Statistics and Machine Learning |  http://arxiv.org/abs/1502.03175  | author:Nicholas G. Polson, James G. Scott, Brandon T. Willard category:stat.ML cs.LG stat.ME published:2015-02-11 summary:In this paper we develop proximal methods for statistical learning. Proximalpoint algorithms are useful in statistics and machine learning for obtainingoptimization solutions for composite functions. Our approach exploitsclosed-form solutions of proximal operators and envelope representations basedon the Moreau, Forward-Backward, Douglas-Rachford and Half-Quadratic envelopes.Envelope representations lead to novel proximal algorithms for statisticaloptimisation of composite objective functions which include both non-smooth andnon-convex objectives. We illustrate our methodology with regularized Logisticand Poisson regression and non-convex bridge penalties with a fused lasso norm.We provide a discussion of convergence of non-descent algorithms withacceleration and for non-convex functions. Finally, we provide directions forfuture research.
arxiv-1502-03365 | Reconstruction in the Labeled Stochastic Block Model |  http://arxiv.org/abs/1502.03365  | author:Marc Lelarge, Laurent Massoulié, Jiaming Xu category:stat.ML published:2015-02-11 summary:The labeled stochastic block model is a random graph model representingnetworks with community structure and interactions of multiple types. In itssimplest form, it consists of two communities of approximately equal size, andthe edges are drawn and labeled at random with probability depending on whethertheir two endpoints belong to the same community or not. It has been conjectured in \cite{Heimlicher12} that correlated reconstruction(i.e.\ identification of a partition correlated with the true partition intothe underlying communities) would be feasible if and only if a model parameterexceeds a threshold. We prove one half of this conjecture, i.e., reconstructionis impossible when below the threshold. In the positive direction, we introducea weighted graph to exploit the label information. With a suitable choice ofweight function, we show that when above the threshold by a specific constant,reconstruction is achieved by (1) minimum bisection, (2) a semidefiniterelaxation of minimum bisection, and (3) a spectral method combined withremoval of edges incident to vertices of high degree. Furthermore, we show thathypothesis testing between the labeled stochastic block model and the labeledErd\H{o}s-R\'enyi random graph model exhibits a phase transition at theconjectured reconstruction threshold.
arxiv-1502-03409 | Large-Scale Deep Learning on the YFCC100M Dataset |  http://arxiv.org/abs/1502.03409  | author:Karl Ni, Roger Pearce, Kofi Boakye, Brian Van Essen, Damian Borth, Barry Chen, Eric Wang category:cs.LG cs.CV published:2015-02-11 summary:We present a work-in-progress snapshot of learning with a 15 billionparameter deep learning network on HPC architectures applied to the largestpublicly available natural image and video dataset released to-date. Recentadvancements in unsupervised deep neural networks suggest that scaling up suchnetworks in both model and training dataset size can yield significantimprovements in the learning of concepts at the highest layers. We train ourthree-layer deep neural network on the Yahoo! Flickr Creative Commons 100Mdataset. The dataset comprises approximately 99.2 million images and 800,000user-created videos from Yahoo's Flickr image and video sharing platform.Training of our network takes eight days on 98 GPU nodes at the HighPerformance Computing Center at Lawrence Livermore National Laboratory.Encouraging preliminary results and future research directions are presentedand discussed.
arxiv-1502-03465 | Variable and Fixed Interval Exponential Smoothing |  http://arxiv.org/abs/1502.03465  | author:Javier R. Movellan category:stat.ML math.OC published:2015-02-11 summary:Exponential smoothers are a simple and memory efficient way to computerunning averages of time series. Here we define and describe practicalproperties of exponential smoothers for signals observed at constant andvariable intervals.
arxiv-1502-03466 | Dependent Matérn Processes for Multivariate Time Series |  http://arxiv.org/abs/1502.03466  | author:Alexander Vandenberg-Rodes, Babak Shahbaba category:stat.ML published:2015-02-11 summary:For the challenging task of modeling multivariate time series, we propose anew class of models that use dependent Mat\'ern processes to capture theunderlying structure of data, explain their interdependencies, and predicttheir unknown values. Although similar models have been proposed in theeconometric, statistics, and machine learning literature, our approach hasseveral advantages that distinguish it from existing methods: 1) it is flexibleto provide high prediction accuracy, yet its complexity is controlled to avoidoverfitting; 2) its interpretability separates it from black-box methods; 3)finally, its computational efficiency makes it scalable for high-dimensionaltime series. In this paper, we use several simulated and real data sets toillustrate these advantages. We will also briefly discuss some extensions ofour model.
arxiv-1502-03491 | How to show a probabilistic model is better |  http://arxiv.org/abs/1502.03491  | author:Mithun Chakraborty, Sanmay Das, Allen Lavoie category:stat.ML cs.LG published:2015-02-11 summary:We present a simple theoretical framework, and corresponding practicalprocedures, for comparing probabilistic models on real data in a traditionalmachine learning setting. This framework is based on the theory of properscoring rules, but requires only basic algebra and probability theory tounderstand and verify. The theoretical concepts presented are well-studied,primarily in the statistics literature. The goal of this paper is to advocatetheir wider adoption for performance evaluation in empirical machine learning.
arxiv-1502-03473 | Collaborative Filtering Bandits |  http://arxiv.org/abs/1502.03473  | author:Shuai Li, Alexandros Karatzoglou, Claudio Gentile category:cs.LG cs.AI stat.ML published:2015-02-11 summary:Classical collaborative filtering, and content-based filtering methods try tolearn a static recommendation model given training data. These approaches arefar from ideal in highly dynamic recommendation domains such as newsrecommendation and computational advertisement, where the set of items andusers is very fluid. In this work, we investigate an adaptive clusteringtechnique for content recommendation based on exploration-exploitationstrategies in contextual multi-armed bandit settings. Our algorithm takes intoaccount the collaborative effects that arise due to the interaction of theusers with the items, by dynamically grouping users based on the items underconsideration and, at the same time, grouping items based on the similarity ofthe clusterings induced over the users. The resulting algorithm thus takesadvantage of preference patterns in the data in a way akin to collaborativefiltering methods. We provide an empirical analysis on medium-size real-worlddatasets, showing scalability and increased prediction performance (as measuredby click-through rate) over state-of-the-art methods for clustering bandits. Wealso provide a regret analysis within a standard linear stochastic noisesetting.
arxiv-1502-03240 | Conditional Random Fields as Recurrent Neural Networks |  http://arxiv.org/abs/1502.03240  | author:Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, Philip H. S. Torr category:cs.CV published:2015-02-11 summary:Pixel-level labelling tasks, such as semantic segmentation, play a centralrole in image understanding. Recent approaches have attempted to harness thecapabilities of deep learning techniques for image recognition to tacklepixel-level labelling tasks. One central issue in this methodology is thelimited capacity of deep learning techniques to delineate visual objects. Tosolve this problem, we introduce a new form of convolutional neural networkthat combines the strengths of Convolutional Neural Networks (CNNs) andConditional Random Fields (CRFs)-based probabilistic graphical modelling. Tothis end, we formulate mean-field approximate inference for the ConditionalRandom Fields with Gaussian pairwise potentials as Recurrent Neural Networks.This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain adeep network that has desirable properties of both CNNs and CRFs. Importantly,our system fully integrates CRF modelling with CNNs, making it possible totrain the whole deep network end-to-end with the usual back-propagationalgorithm, avoiding offline post-processing methods for object delineation. Weapply the proposed method to the problem of semantic image segmentation,obtaining top results on the challenging Pascal VOC 2012 segmentationbenchmark.
arxiv-1502-03391 | Fast Embedding for JOFC Using the Raw Stress Criterion |  http://arxiv.org/abs/1502.03391  | author:Vince Lyzinski, Youngser Park, Carey E. Priebe, Michael W. Trosset category:stat.ML stat.ME published:2015-02-11 summary:The Joint Optimization of Fidelity and Commensurability (JOFC) manifoldmatching methodology embeds an omnibus dissimilarity matrix consisting ofmultiple dissimilarities on the same set of objects. One approach to thisembedding optimizes the preservation of fidelity to each individualdissimilarity matrix together with commensurability of each given observationacross modalities via iterative majorizations of a raw stress error criterionby successive Guttman transforms. In this paper, we exploit the specialstructure inherent to JOFC to exactly and efficiently compute the successiveGuttman transforms, and as a result we are able to greatly speed up andparallelize the JOFC procedure for both in-sample and out-of-sample embedding.We demonstrate the scalability of our implementation on both real and simulateddata examples.
arxiv-1502-03255 | Off-policy evaluation for MDPs with unknown structure |  http://arxiv.org/abs/1502.03255  | author:Assaf Hallak, François Schnitzler, Timothy Mann, Shie Mannor category:stat.ML cs.LG published:2015-02-11 summary:Off-policy learning in dynamic decision problems is essential for providingstrong evidence that a new policy is better than the one in use. But how can weprove superiority without testing the new policy? To answer this question, weintroduce the G-SCOPE algorithm that evaluates a new policy based on datagenerated by the existing policy. Our algorithm is both computationally andsample efficient because it greedily learns to exploit factored structure inthe dynamics of the environment. We present a finite sample analysis of ourapproach and show through experiments that the algorithm scales well onhigh-dimensional problems with few samples.
arxiv-1502-03302 | Using Distance Estimation and Deep Learning to Simplify Calibration in Food Calorie Measurement |  http://arxiv.org/abs/1502.03302  | author:Pallavi Kuhad, Abdulsalam Yassine, Shervin Shirmohammadi category:cs.CY cs.HC cs.LG published:2015-02-11 summary:High calorie intake in the human body on the one hand, has proved harmful innumerous occasions leading to several diseases and on the other hand, astandard amount of calorie intake has been deemed essential by dieticians tomaintain the right balance of calorie content in human body. As such,researchers have proposed a variety of automatic tools and systems to assistusers measure their calorie in-take. In this paper, we consider the category ofthose tools that use image processing to recognize the food, and we propose amethod for fully automatic and user-friendly calibration of the dimension ofthe food portion sizes, which is needed in order to measure food portion weightand its ensuing amount of calories. Experimental results show that our method,which uses deep learning, mobile cloud computing, distance estimation and sizecalibration inside a mobile device, leads to an accuracy improvement to 95% onaverage compared to previous work
arxiv-1502-03492 | Gradient-based Hyperparameter Optimization through Reversible Learning |  http://arxiv.org/abs/1502.03492  | author:Dougal Maclaurin, David Duvenaud, Ryan P. Adams category:stat.ML cs.LG published:2015-02-11 summary:Tuning hyperparameters of learning algorithms is hard because gradients areusually unavailable. We compute exact gradients of cross-validation performancewith respect to all hyperparameters by chaining derivatives backwards throughthe entire training procedure. These gradients allow us to optimize thousandsof hyperparameters, including step-size and momentum schedules, weightinitialization distributions, richly parameterized regularization schemes, andneural network architectures. We compute hyperparameter gradients by exactlyreversing the dynamics of stochastic gradient descent with momentum.
arxiv-1502-03215 | A Hybrid Approach for Improved Content-based Image Retrieval using Segmentation |  http://arxiv.org/abs/1502.03215  | author:Smarajit Bose, Amita Pal, Jhimli Mallick, Sunil Kumar, Pratyaydipta Rudra category:cs.IR cs.CV stat.ME published:2015-02-11 summary:The objective of Content-Based Image Retrieval (CBIR) methods is essentiallyto extract, from large (image) databases, a specified number of images similarin visual and semantic content to a so-called query image. To bridge thesemantic gap that exists between the representation of an image by low-levelfeatures (namely, colour, shape, texture) and its high-level semantic contentas perceived by humans, CBIR systems typically make use of the relevancefeedback (RF) mechanism. RF iteratively incorporates user-given inputsregarding the relevance of retrieved images, to improve retrieval efficiency.One approach is to vary the weights of the features dynamically via featurereweighting. In this work, an attempt has been made to improve retrievalaccuracy by enhancing a CBIR system based on color features alone, throughimplicit incorporation of shape information obtained through prior segmentationof the images. Novel schemes for feature reweighting as well as forinitialization of the relevant set for improved relevance feedback, have alsobeen proposed for boosting performance of RF- based CBIR. At the same time, newmeasures for evaluation of retrieval accuracy have been suggested, to overcomethe limitations of existing measures in the RF context. Results of extensiveexperiments have been presented to illustrate the effectiveness of the proposedapproaches.
arxiv-1502-03211 | An Extreme-Value Approach for Testing the Equality of Large U-Statistic Based Correlation Matrices |  http://arxiv.org/abs/1502.03211  | author:Cheng Zhou, Fang Han, Xinsheng Zhang, Han Liu category:math.ST stat.ML stat.TH published:2015-02-11 summary:There has been an increasing interest in testing the equality of largePearson's correlation matrices. However, in many applications it is moreimportant to test the equality of large rank-based correlation matrices sincethey are more robust to outliers and nonlinearity. Unlike the Pearson's case,testing the equality of large rank-based statistics has not been well exploredand requires us to develop new methods and theory. In this paper, we provide aframework for testing the equality of two large U-statistic based correlationmatrices, which include the rank-based correlation matrices as special cases.Our approach exploits extreme value statistics and the Jackknife estimator foruncertainty assessment and is valid under a fully nonparametric model.Theoretically, we develop a theory for testing the equality of U-statisticbased correlation matrices. We then apply this theory to study the problem oftesting large Kendall's tau correlation matrices and demonstrate itsoptimality. For proving this optimality, a novel construction of leastfavourable distributions is developed for the correlation matrix comparison.
arxiv-1502-03163 | Gaussian Process Models for HRTF based Sound-Source Localization and Active-Learning |  http://arxiv.org/abs/1502.03163  | author:Yuancheng Luo, Dmitry N. Zotkin, Ramani Duraiswami category:cs.SD cs.LG stat.ML 60G15 published:2015-02-11 summary:From a machine learning perspective, the human ability localize sounds can bemodeled as a non-parametric and non-linear regression problem between binauralspectral features of sound received at the ears (input) and their sound-sourcedirections (output). The input features can be summarized in terms of theindividual's head-related transfer functions (HRTFs) which measure the spectralresponse between the listener's eardrum and an external point in $3$D. Based onthese viewpoints, two related problems are considered: how can one achieve anoptimal sampling of measurements for training sound-source localization (SSL)models, and how can SSL models be used to infer the subject's HRTFs inlistening tests. First, we develop a class of binaural SSL models based onGaussian process regression and solve a \emph{forward selection} problem thatfinds a subset of input-output samples that best generalize to all SSLdirections. Second, we use an \emph{active-learning} approach that updates anonline SSL model for inferring the subject's SSL errors via headphones and agraphical user interface. Experiments show that only a small fraction of HRTFsare required for $5^{\circ}$ localization accuracy and that the learned HRTFsare localized closer to their intended directions than non-individualizedHRTFs.
arxiv-1502-03273 | Image denoising based on improved data-driven sparse representation |  http://arxiv.org/abs/1502.03273  | author:Dai-Qiang Chen category:cs.CV published:2015-02-11 summary:Sparse representation of images under certain transform domain has beenplaying a fundamental role in image restoration tasks. One such representativemethod is the widely used wavelet tight frame systems. Instead of adoptingfixed filters for constructing a tight frame to sparsely model any input image,a data-driven tight frame was proposed for the sparse representation of images,and shown to be very efficient for image denoising very recently. However, inthis method the number of framelet filters used for constructing a tight frameis the same as the length of filters. In fact, through further investigation itis found that part of these filters are unnecessary and even harmful to therecovery effect due to the influence of noise. Therefore, an improveddata-driven sparse representation systems constructed with much less number offilters are proposed. Numerical results on denoising experiments demonstratethat the proposed algorithm overall outperforms the original data-driven tightframe construction scheme on both the recovery quality and computational time.
arxiv-1502-03475 | Combinatorial Bandits Revisited |  http://arxiv.org/abs/1502.03475  | author:Richard Combes, M. Sadegh Talebi, Alexandre Proutiere, Marc Lelarge category:cs.LG math.OC stat.ML published:2015-02-11 summary:This paper investigates stochastic and adversarial combinatorial multi-armedbandit problems. In the stochastic setting under semi-bandit feedback, wederive a problem-specific regret lower bound, and discuss its scaling with thedimension of the decision space. We propose ESCB, an algorithm that efficientlyexploits the structure of the problem and provide a finite-time analysis of itsregret. ESCB has better performance guarantees than existing algorithms, andsignificantly outperforms these algorithms in practice. In the adversarialsetting under bandit feedback, we propose \textsc{CombEXP}, an algorithm withthe same regret scaling as state-of-the-art algorithms, but with lowercomputational complexity for some combinatorial problems.
arxiv-1502-03436 | An exploration of parameter redundancy in deep networks with circulant projections |  http://arxiv.org/abs/1502.03436  | author:Yu Cheng, Felix X. Yu, Rogerio S. Feris, Sanjiv Kumar, Alok Choudhary, Shih-Fu Chang category:cs.CV published:2015-02-11 summary:We explore the redundancy of parameters in deep neural networks by replacingthe conventional linear projection in fully-connected layers with the circulantprojection. The circulant structure substantially reduces memory footprint andenables the use of the Fast Fourier Transform to speed up the computation.Considering a fully-connected neural network layer with d input nodes, and doutput nodes, this method improves the time complexity from O(d^2) to O(dlogd)and space complexity from O(d^2) to O(d). The space savings are particularlyimportant for modern deep convolutional neural network architectures, wherefully-connected layers typically contain more than 90% of the networkparameters. We further show that the gradient computation and optimization ofthe circulant projections can be performed very efficiently. Our experiments onthree standard datasets show that the proposed approach achieves thissignificant gain in storage and efficiency with minimal increase in error ratecompared to neural networks with unstructured projections.
arxiv-1502-03167 | Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift |  http://arxiv.org/abs/1502.03167  | author:Sergey Ioffe, Christian Szegedy category:cs.LG published:2015-02-11 summary:Training Deep Neural Networks is complicated by the fact that thedistribution of each layer's inputs changes during training, as the parametersof the previous layers change. This slows down the training by requiring lowerlearning rates and careful parameter initialization, and makes it notoriouslyhard to train models with saturating nonlinearities. We refer to thisphenomenon as internal covariate shift, and address the problem by normalizinglayer inputs. Our method draws its strength from making normalization a part ofthe model architecture and performing the normalization for each trainingmini-batch. Batch Normalization allows us to use much higher learning rates andbe less careful about initialization. It also acts as a regularizer, in somecases eliminating the need for Dropout. Applied to a state-of-the-art imageclassification model, Batch Normalization achieves the same accuracy with 14times fewer training steps, and beats the original model by a significantmargin. Using an ensemble of batch-normalized networks, we improve upon thebest published result on ImageNet classification: reaching 4.9% top-5validation error (and 4.8% test error), exceeding the accuracy of human raters.
arxiv-1502-03121 | Fast Fusion of Multi-Band Images Based on Solving a Sylvester Equation |  http://arxiv.org/abs/1502.03121  | author:Qi Wei, Nicolas Dobigeon, Jean-Yves Tourneret category:cs.CV published:2015-02-10 summary:This paper proposes a fast multi-band image fusion algorithm, which combinesa high-spatial low-spectral resolution image and a low-spatial high-spectralresolution image. The well admitted forward model is explored to form thelikelihoods of the observations. Maximizing the likelihoods leads to solving aSylvester equation. By exploiting the properties of the circulant anddownsampling matrices associated with the fusion problem, a closed-formsolution for the corresponding Sylvester equation is obtained explicitly,getting rid of any iterative update step. Coupled with the alternatingdirection method of multipliers and the block coordinate descent method, theproposed algorithm can be easily generalized to incorporate prior informationfor the fusion problem, allowing a Bayesian estimator. Simulation results showthat the proposed algorithm achieves the same performance as existingalgorithms with the advantage of significantly decreasing the computationalcomplexity of these algorithms.
arxiv-1502-02965 | Video Primal Sketch: A Unified Middle-Level Representation for Video |  http://arxiv.org/abs/1502.02965  | author:Zhi Han, Zongben Xu, Song-Chun Zhu category:cs.CV published:2015-02-10 summary:This paper presents a middle-level video representation named Video PrimalSketch (VPS), which integrates two regimes of models: i) sparse coding modelusing static or moving primitives to explicitly represent moving corners,lines, feature points, etc., ii) FRAME /MRF model reproducing featurestatistics extracted from input video to implicitly represent textured motion,such as water and fire. The feature statistics include histograms ofspatio-temporal filters and velocity distributions. This paper makes threecontributions to the literature: i) Learning a dictionary of video primitivesusing parametric generative models; ii) Proposing the Spatio-Temporal FRAME(ST-FRAME) and Motion-Appearance FRAME (MA-FRAME) models for modeling andsynthesizing textured motion; and iii) Developing a parsimonious hybrid modelfor generic video representation. Given an input video, VPS selects the propermodels automatically for different motion patterns and is compatible withhigh-level action representations. In the experiments, we synthesize a numberof textured motion; reconstruct real videos using the VPS; report a series ofhuman perception experiments to verify the quality of reconstructed videos;demonstrate how the VPS changes over the scale transition in videos; andpresent the close connection between VPS and high-level action models.
arxiv-1502-03126 | Kernel Task-Driven Dictionary Learning for Hyperspectral Image Classification |  http://arxiv.org/abs/1502.03126  | author:Soheil Bahrampour, Nasser M. Nasrabadi, Asok Ray, Kenneth W. Jenkins category:stat.ML cs.CV cs.LG published:2015-02-10 summary:Dictionary learning algorithms have been successfully used in bothreconstructive and discriminative tasks, where the input signal is representedby a linear combination of a few dictionary atoms. While these methods areusually developed under $\ell_1$ sparsity constrain (prior) in the inputdomain, recent studies have demonstrated the advantages of sparserepresentation using structured sparsity priors in the kernel domain. In thispaper, we propose a supervised dictionary learning algorithm in the kerneldomain for hyperspectral image classification. In the proposed formulation, thedictionary and classifier are obtained jointly for optimal classificationperformance. The supervised formulation is task-driven and provides learnedfeatures from the hyperspectral data that are well suited for theclassification task. Moreover, the proposed algorithm uses a joint($\ell_{12}$) sparsity prior to enforce collaboration among the neighboringpixels. The simulation results illustrate the efficiency of the proposeddictionary learning algorithm.
arxiv-1502-02905 | Real Time Implementation of Spatial Filtering On FPGA |  http://arxiv.org/abs/1502.02905  | author:Chaitannya Supe category:cs.CV published:2015-02-10 summary:Field Programmable Gate Array (FPGA) technology has gained vital importancemainly because of its parallel processing hardware which makes it ideal forimage and video processing. In this paper, a step by step approach to apply alinear spatial filter on real time video frame sent by Omnivision OV7670 camerausing Zynq Evaluation and Development board based on Xilinx XC7Z020 has beendiscussed. Face detection application was chosen to explain above procedure.This procedure is applicable to most of the complex image processing algorithmswhich needs to be implemented using FPGA.
arxiv-1502-02772 | A HMAX with LLC for visual recognition |  http://arxiv.org/abs/1502.02772  | author:Kean Hong Lau, Yong Haur Tay, Fook Loong Lo category:cs.CV published:2015-02-10 summary:Today's high performance deep artificial neural networks (ANNs) rely heavilyon parameter optimization, which is sequential in nature and even with apowerful GPU, would have taken weeks to train them up for solving challengingtasks [22]. HMAX [17] has demonstrated that a simple high performing networkcould be obtained without heavy optimization. In this paper, we had improved onthe existing best HMAX neural network [12] in terms of structural simplicityand performance. Our design replaces the L1 minimization sparse coding (SC)with a locality-constrained linear coding (LLC) [20] which has a lowercomputational demand. We also put the simple orientation filter bank back intothe front layer of the network replacing PCA. Our system's performance hasimproved over the existing architecture and reached 79.0% on the challengingCaltech-101 [7] dataset, which is state-of-the-art for ANNs (without transferlearning). From our empirical data, the main contributors to our system'sperformance include an introduction of partial signal whitening, a spotdetector, and a spatial pyramid matching (SPM) [14] layer.
arxiv-1502-02871 | Talk to the Hand: Generating a 3D Print from Photographs |  http://arxiv.org/abs/1502.02871  | author:Edward Aboufadel, Sylvanna V. Krawczyk, Melissa Sherman-Bennett category:math.HO cs.CV published:2015-02-10 summary:This manuscript presents a linear algebra-based technique that only requirestwo unique photographs from a digital camera to mathematically construct a 3Dsurface representation which can then be 3D printed. Basic computer visiontheory and manufacturing principles are also briefly discussed.
arxiv-1502-02860 | Gaussian Processes for Data-Efficient Learning in Robotics and Control |  http://arxiv.org/abs/1502.02860  | author:Marc Peter Deisenroth, Dieter Fox, Carl Edward Rasmussen category:stat.ML cs.LG cs.RO cs.SY published:2015-02-10 summary:Autonomous learning has been a promising direction in control and roboticsfor more than a decade since data-driven learning allows to reduce the amountof engineering knowledge, which is otherwise required. However, autonomousreinforcement learning (RL) approaches typically require many interactions withthe system to learn controllers, which is a practical limitation in realsystems, such as robots, where many interactions can be impractical and timeconsuming. To address this problem, current learning approaches typicallyrequire task-specific knowledge in form of expert demonstrations, realisticsimulators, pre-shaped policies, or specific knowledge about the underlyingdynamics. In this article, we follow a different approach and speed up learningby extracting more information from data. In particular, we learn aprobabilistic, non-parametric Gaussian process transition model of the system.By explicitly incorporating model uncertainty into long-term planning andcontroller learning our approach reduces the effects of model errors, a keyproblem in model-based learning. Compared to state-of-the art RL ourmodel-based policy search method achieves an unprecedented speed of learning.We demonstrate its applicability to autonomous learning in real robot andcontrol tasks.
arxiv-1502-02793 | The Benefit of Sex in Noisy Evolutionary Search |  http://arxiv.org/abs/1502.02793  | author:Tobias Friedrich, Timo Kötzing, Martin Krejca, Andrew M. Sutton category:cs.NE published:2015-02-10 summary:The benefit of sexual recombination is one of the most fundamental questionsboth in population genetics and evolutionary computation. It is widely believedthat recombination helps solving difficult optimization problems. We presentthe first result, which rigorously proves that it is beneficial to use sexualrecombination in an uncertain environment with a noisy fitness function. Forthis, we model sexual recombination with a simple estimation of distributionalgorithm called the Compact Genetic Algorithm (cGA), which we compare with theclassical $\mu+1$ EA. For a simple noisy fitness function with additiveGaussian posterior noise $\mathcal{N}(0,\sigma^2)$, we prove that themutation-only $\mu+1$ EA typically cannot handle noise in polynomial time for$\sigma^2$ large enough while the cGA runs in polynomial time as long as thepopulation size is not too small. This shows that in this uncertain environmentsexual recombination is provably beneficial. We observe the same behavior in asmall empirical study.
arxiv-1502-02761 | Generative Moment Matching Networks |  http://arxiv.org/abs/1502.02761  | author:Yujia Li, Kevin Swersky, Richard Zemel category:cs.LG cs.AI stat.ML published:2015-02-10 summary:We consider the problem of learning deep generative models from data. Weformulate a method that generates an independent sample via a singlefeedforward pass through a multilayer perceptron, as in the recently proposedgenerative adversarial networks (Goodfellow et al., 2014). Training agenerative adversarial network, however, requires careful optimization of adifficult minimax program. Instead, we utilize a technique from statisticalhypothesis testing known as maximum mean discrepancy (MMD), which leads to asimple objective that can be interpreted as matching all orders of statisticsbetween a dataset and samples from the model, and can be trained bybackpropagation. We further boost the performance of this approach by combiningour generative network with an auto-encoder network, using MMD to learn togenerate codes that can then be decoded to produce samples. We show that thecombination of these techniques yields excellent generative models compared tobaseline approaches as measured on MNIST and the Toronto Face Database.
arxiv-1502-03042 | Functional Gaussian Process Model for Bayesian Nonparametric Analysis |  http://arxiv.org/abs/1502.03042  | author:Leo L. Duan, Xia Wang, Rhonda D. Szczesniak category:stat.ML stat.CO stat.ME published:2015-02-10 summary:Gaussian process is a theoretically appealing model for nonparametricanalysis, but its computational cumbersomeness hinders its use in large scaleand the existing reduced-rank solutions are usually heuristic. In this work, wepropose a novel construction of Gaussian process as a projection from fixeddiscrete frequencies to any continuous location. This leads to a validstochastic process that has a theoretic support with the reduced rank in thespectral density, as well as a high-speed computing algorithm. Our methodprovides accurate estimates for the covariance parameters and concise form ofpredictive distribution for spatial prediction. For non-stationary data, weadopt the mixture framework with a customized spectral dependency structure.This enables clustering based on local stationarity, while maintains the jointGaussianness. Our work is directly applicable in solving some of the challengesin the spatial data, such as large scale computation, anisotropic covariance,spatio-temporal modeling, etc. We illustrate the uses of the model viasimulations and an application on a massive dataset.
arxiv-1502-03044 | Show, Attend and Tell: Neural Image Caption Generation with Visual Attention |  http://arxiv.org/abs/1502.03044  | author:Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio category:cs.LG cs.CV published:2015-02-10 summary:Inspired by recent work in machine translation and object detection, weintroduce an attention based model that automatically learns to describe thecontent of images. We describe how we can train this model in a deterministicmanner using standard backpropagation techniques and stochastically bymaximizing a variational lower bound. We also show through visualization howthe model is able to automatically learn to fix its gaze on salient objectswhile generating the corresponding words in the output sequence. We validatethe use of attention with state-of-the-art performance on three benchmarkdatasets: Flickr8k, Flickr30k and MS COCO.
arxiv-1502-03032 | Implementing Randomized Matrix Algorithms in Parallel and Distributed Environments |  http://arxiv.org/abs/1502.03032  | author:Jiyan Yang, Xiangrui Meng, Michael W. Mahoney category:cs.DC cs.DS math.NA stat.ML published:2015-02-10 summary:In this era of large-scale data, distributed systems built on top of clustersof commodity hardware provide cheap and reliable storage and scalableprocessing of massive data. Here, we review recent work on developing andimplementing randomized matrix algorithms in large-scale parallel anddistributed environments. Randomized algorithms for matrix problems havereceived a great deal of attention in recent years, thus far typically eitherin theory or in machine learning applications or with implementations on asingle machine. Our main focus is on the underlying theory and practicalimplementation of random projection and random sampling algorithms for verylarge very overdetermined (i.e., overconstrained) $\ell_1$ and $\ell_2$regression problems. Randomization can be used in one of two related ways:either to construct sub-sampled problems that can be solved, exactly orapproximately, with traditional numerical methods; or to constructpreconditioned versions of the original full problem that are easier to solvewith traditional iterative algorithms. Theoretical results demonstrate that innear input-sparsity time and with only a few passes through the data one canobtain very strong relative-error approximate solutions, with high probability.Empirical results highlight the importance of various trade-offs (e.g., betweenthe time to construct an embedding and the conditioning quality of theembedding, between the relative importance of computation versus communication,etc.) and demonstrate that $\ell_1$ and $\ell_2$ regression problems can besolved to low, medium, or high precision in existing distributed systems on upto terabyte-sized data.
arxiv-1502-02763 | Cascading Bandits: Learning to Rank in the Cascade Model |  http://arxiv.org/abs/1502.02763  | author:Branislav Kveton, Csaba Szepesvari, Zheng Wen, Azin Ashkan category:cs.LG stat.ML published:2015-02-10 summary:A search engine usually outputs a list of $K$ web pages. The user examinesthis list, from the first web page to the last, and chooses the firstattractive page. This model of user behavior is known as the cascade model. Inthis paper, we propose cascading bandits, a learning variant of the cascademodel where the objective is to identify $K$ most attractive items. Weformulate our problem as a stochastic combinatorial partial monitoring problem.We propose two algorithms for solving it, CascadeUCB1 and CascadeKL-UCB. Wealso prove gap-dependent upper bounds on the regret of these algorithms andderive a lower bound on the regret in cascading bandits. The lower boundmatches the upper bound of CascadeKL-UCB up to a logarithmic factor. Weexperiment with our algorithms on several problems. The algorithms performsurprisingly well even when our modeling assumptions are violated.
arxiv-1502-02843 | Distributed Gaussian Processes |  http://arxiv.org/abs/1502.02843  | author:Marc Peter Deisenroth, Jun Wei Ng category:stat.ML published:2015-02-10 summary:To scale Gaussian processes (GPs) to large data sets we introduce the robustBayesian Committee Machine (rBCM), a practical and scalable product-of-expertsmodel for large-scale distributed GP regression. Unlike state-of-the-art sparseGP approximations, the rBCM is conceptually simple and does not rely oninducing or variational parameters. The key idea is to recursively distributecomputations to independent computational units and, subsequently, recombinethem to form an overall result. Efficient closed-form inference allows forstraightforward parallelisation and distributed computations with a smallmemory footprint. The rBCM is independent of the computational graph and can beused on heterogeneous computing infrastructures, ranging from laptops toclusters. With sufficient computing resources our distributed GP model canhandle arbitrarily large data sets.
arxiv-1502-02846 | Probabilistic Line Searches for Stochastic Optimization |  http://arxiv.org/abs/1502.02846  | author:Maren Mahsereci, Philipp Hennig category:cs.LG math.OC stat.ML published:2015-02-10 summary:In deterministic optimization, line searches are a standard tool ensuringstability and efficiency. Where only stochastic gradients are available, nodirect equivalent has so far been formulated, because uncertain gradients donot allow for a strict sequence of decisions collapsing the search space. Weconstruct a probabilistic line search by combining the structure of existingdeterministic methods with notions from Bayesian optimization. Our methodretains a Gaussian process surrogate of the univariate optimization objective,and uses a probabilistic belief over the Wolfe conditions to monitor thedescent. The algorithm has very low computational cost, and no user-controlledparameters. Experiments show that it effectively removes the need to define alearning rate for stochastic gradient descent.
arxiv-1502-02766 | Multi-view Face Detection Using Deep Convolutional Neural Networks |  http://arxiv.org/abs/1502.02766  | author:Sachin Sudhakar Farfade, Mohammad Saberian, Li-Jia Li category:cs.CV I.4 published:2015-02-10 summary:In this paper we consider the problem of multi-view face detection. Whilethere has been significant research on this problem, current state-of-the-artapproaches for this task require annotation of facial landmarks, e.g. TSM [25],or annotation of face poses [28, 22]. They also require training dozens ofmodels to fully capture faces in all orientations, e.g. 22 models in HeadHuntermethod [22]. In this paper we propose Deep Dense Face Detector (DDFD), a methodthat does not require pose/landmark annotation and is able to detect faces in awide range of orientations using a single model based on deep convolutionalneural networks. The proposed method has minimal complexity; unlike otherrecent deep learning object detection methods [9], it does not requireadditional components such as segmentation, bounding-box regression, or SVMclassifiers. Furthermore, we analyzed scores of the proposed face detector forfaces in different orientations and found that 1) the proposed method is ableto detect faces from different angles and can handle occlusion to some extent,2) there seems to be a correlation between dis- tribution of positive examplesin the training set and scores of the proposed face detector. The lattersuggests that the proposed methods performance can be further improved by usingbetter sampling strategies and more sophisticated data augmentation techniques.Evaluations on popular face detection benchmark datasets show that oursingle-model face detector algorithm has similar or better performance comparedto the previous methods, which are more complex and require annotations ofeither different poses or facial landmarks.
arxiv-1502-02791 | Learning Transferable Features with Deep Adaptation Networks |  http://arxiv.org/abs/1502.02791  | author:Mingsheng Long, Yue Cao, Jianmin Wang, Michael I. Jordan category:cs.LG published:2015-02-10 summary:Recent studies reveal that a deep neural network can learn transferablefeatures which generalize well to novel tasks for domain adaptation. However,as deep features eventually transition from general to specific along thenetwork, the feature transferability drops significantly in higher layers withincreasing domain discrepancy. Hence, it is important to formally reduce thedataset bias and enhance the transferability in task-specific layers. In thispaper, we propose a new Deep Adaptation Network (DAN) architecture, whichgeneralizes deep convolutional neural network to the domain adaptationscenario. In DAN, hidden representations of all task-specific layers areembedded in a reproducing kernel Hilbert space where the mean embeddings ofdifferent domain distributions can be explicitly matched. The domaindiscrepancy is further reduced using an optimal multi-kernel selection methodfor mean embedding matching. DAN can learn transferable features withstatistical guarantees, and can scale linearly by unbiased estimate of kernelembedding. Extensive empirical evidence shows that the proposed architectureyields state-of-the-art image classification error rates on standard domainadaptation benchmarks.
arxiv-1502-02606 | The Power of Randomization: Distributed Submodular Maximization on Massive Datasets |  http://arxiv.org/abs/1502.02606  | author:Rafael da Ponte Barbosa, Alina Ene, Huy L. Nguyen, Justin Ward category:cs.LG cs.AI cs.DC published:2015-02-09 summary:A wide variety of problems in machine learning, including exemplarclustering, document summarization, and sensor placement, can be cast asconstrained submodular maximization problems. Unfortunately, the resultingsubmodular optimization problems are often too large to be solved on a singlemachine. We develop a simple distributed algorithm that is embarrassinglyparallel and it achieves provable, constant factor, worst-case approximationguarantees. In our experiments, we demonstrate its efficiency in large problemswith different kinds of constraints with objective values always close to whatis achievable in the centralized setting.
arxiv-1502-02613 | Projected Nesterov's Proximal-Gradient Algorithm for Sparse Signal Reconstruction with a Convex Constraint |  http://arxiv.org/abs/1502.02613  | author:Renliang Gu, Aleksandar Dogandžić category:stat.CO stat.ML published:2015-02-09 summary:We develop a projected Nesterov's proximal-gradient (PNPG) approach forsparse signal reconstruction that combines adaptive step size with Nesterov'smomentum acceleration. The objective function that we wish to minimize is thesum of a convex differentiable data-fidelity (negative log-likelihood (NLL))term and a convex regularization term. We apply sparse signal regularizationwhere the signal belongs to a closed convex set within the closure of thedomain of the NLL; the convex-set constraint facilitates flexible NLL domainsand accurate signal recovery. Signal sparsity is imposed using the$\ell_1$-norm penalty on the signal's linear transform coefficients or gradientmap, respectively. The PNPG approach employs projected Nesterov's accelerationstep with restart and an inner iteration to compute the proximal mapping. Wepropose an adaptive step-size selection scheme to obtain a good localmajorizing function of the NLL and reduce the time spent backtracking. Thanksto step-size adaptation, PNPG does not require Lipschitz continuity of thegradient of the NLL. We present an integrated derivation of the momentumacceleration and its $\mathcal{O}(k^{-2})$ convergence-rate and iterateconvergence proofs, which account for adaptive step-size selection, inexactnessof the iterative proximal mapping, and the convex-set constraint. The tuning ofPNPG is largely application-independent. Tomographic and compressed-sensingreconstruction experiments with Poisson generalized linear and Gaussian linearmeasurement models demonstrate the performance of the proposed approach.
arxiv-1502-02643 | Random Coordinate Descent Methods for Minimizing Decomposable Submodular Functions |  http://arxiv.org/abs/1502.02643  | author:Alina Ene, Huy L. Nguyen category:cs.LG cs.AI published:2015-02-09 summary:Submodular function minimization is a fundamental optimization problem thatarises in several applications in machine learning and computer vision. Theproblem is known to be solvable in polynomial time, but general purposealgorithms have high running times and are unsuitable for large-scale problems.Recent work have used convex optimization techniques to obtain very practicalalgorithms for minimizing functions that are sums of ``simple" functions. Inthis paper, we use random coordinate descent methods to obtain algorithms withfaster linear convergence rates and cheaper iteration costs. Compared toalternating projection methods, our algorithms do not rely on full-dimensionalvector operations and they converge in significantly fewer iterations.
arxiv-1502-02651 | Optimal and Adaptive Algorithms for Online Boosting |  http://arxiv.org/abs/1502.02651  | author:Alina Beygelzimer, Satyen Kale, Haipeng Luo category:cs.LG published:2015-02-09 summary:We study online boosting, the task of converting any weak online learner intoa strong online learner. Based on a novel and natural definition of weak onlinelearnability, we develop two online boosting algorithms. The first algorithm isan online version of boost-by-majority. By proving a matching lower bound, weshow that this algorithm is essentially optimal in terms of the number of weaklearners and the sample complexity needed to achieve a specified accuracy. Thisoptimal algorithm is not adaptive however. Using tools from online lossminimization, we derive an adaptive online boosting algorithm that is alsoparameter-free, but not optimal. Both algorithms work with base learners thatcan handle example importance weights directly, as well as by rejectionsampling examples with probability defined by the booster. Results arecomplemented with an extensive experimental study.
arxiv-1502-02704 | Learning Reductions that Really Work |  http://arxiv.org/abs/1502.02704  | author:Alina Beygelzimer, Hal Daumé III, John Langford, Paul Mineiro category:cs.LG published:2015-02-09 summary:We provide a summary of the mathematical and computational techniques thathave enabled learning reductions to effectively address a wide class ofproblems, and show that this approach to solving machine learning problems canbe broadly useful.
arxiv-1502-02710 | Scalable Multilabel Prediction via Randomized Methods |  http://arxiv.org/abs/1502.02710  | author:Nikos Karampatziakis, Paul Mineiro category:cs.LG published:2015-02-09 summary:Modeling the dependence between outputs is a fundamental challenge inmultilabel classification. In this work we show that a generic regularizednonlinearity mapping independent predictions to joint predictions is sufficientto achieve state-of-the-art performance on a variety of benchmark problems.Crucially, we compute the joint predictions without ever obtaining anyindependent predictions, while incorporating low-rank and smoothnessregularization. We achieve this by leveraging randomized algorithms for matrixdecomposition and kernel approximation. Furthermore, our techniques areapplicable to the multiclass setting. We apply our method to a variety ofmulticlass and multilabel data sets, obtaining state-of-the-art results.
arxiv-1502-02362 | Counterfactual Risk Minimization: Learning from Logged Bandit Feedback |  http://arxiv.org/abs/1502.02362  | author:Adith Swaminathan, Thorsten Joachims category:cs.LG stat.ML published:2015-02-09 summary:We develop a learning principle and an efficient algorithm for batch learningfrom logged bandit feedback. This learning setting is ubiquitous in onlinesystems (e.g., ad placement, web search, recommendation), where an algorithmmakes a prediction (e.g., ad ranking) for a given input (e.g., query) andobserves bandit feedback (e.g., user clicks on presented ads). We first addressthe counterfactual nature of the learning problem through propensity scoring.Next, we prove generalization error bounds that account for the variance of thepropensity-weighted empirical risk estimator. These constructive bounds giverise to the Counterfactual Risk Minimization (CRM) principle. We show how CRMcan be used to derive a new learning method -- called Policy Optimizer forExponential Models (POEM) -- for learning stochastic linear rules forstructured output prediction. We present a decomposition of the POEM objectivethat enables efficient stochastic gradient optimization. POEM is evaluated onseveral multi-label classification problems showing substantially improvedrobustness and generalization performance compared to the state-of-the-art.
arxiv-1502-02367 | Gated Feedback Recurrent Neural Networks |  http://arxiv.org/abs/1502.02367  | author:Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio category:cs.NE cs.LG stat.ML published:2015-02-09 summary:In this work, we propose a novel recurrent neural network (RNN) architecture.The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach ofstacking multiple recurrent layers by allowing and controlling signals flowingfrom upper recurrent layers to lower layers using a global gating unit for eachpair of layers. The recurrent signals exchanged between layers are gatedadaptively based on the previous hidden states and the current input. Weevaluated the proposed GF-RNN with different types of recurrent units, such astanh, long short-term memory and gated recurrent units, on the tasks ofcharacter-level language modeling and Python program evaluation. Our empiricalevaluation of different RNN units, revealed that in both tasks, the GF-RNNoutperforms the conventional approaches to build deep stacked RNNs. We suggestthat the improvement arises because the GF-RNN can adaptively assign differentlayers to different timescales and layer-to-layer interactions (including thetop-down ones which are not usually present in a stacked RNN) by learning togate these interactions.
arxiv-1502-02444 | On the Dynamics of a Recurrent Hopfield Network |  http://arxiv.org/abs/1502.02444  | author:Rama Garimella, Berkay Kicanaoglu, Moncef Gabbouj category:cs.NE published:2015-02-09 summary:In this research paper novel real/complex valued recurrent Hopfield NeuralNetwork (RHNN) is proposed. The method of synthesizing the energy landscape ofsuch a network and the experimental investigation of dynamics of RecurrentHopfield Network is discussed. Parallel modes of operation (other than fullyparallel mode) in layered RHNN is proposed. Also, certain potentialapplications are proposed.
arxiv-1502-02551 | Deep Learning with Limited Numerical Precision |  http://arxiv.org/abs/1502.02551  | author:Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, Pritish Narayanan category:cs.LG cs.NE stat.ML published:2015-02-09 summary:Training of large-scale deep neural networks is often constrained by theavailable computational resources. We study the effect of limited precisiondata representation and computation on neural network training. Within thecontext of low-precision fixed-point computations, we observe the roundingscheme to play a crucial role in determining the network's behavior duringtraining. Our results show that deep networks can be trained using only 16-bitwide fixed-point number representation when using stochastic rounding, andincur little to no degradation in the classification accuracy. We alsodemonstrate an energy-efficient hardware accelerator that implementslow-precision fixed-point arithmetic with stochastic rounding.
arxiv-1502-02330 | Tensor Canonical Correlation Analysis for Multi-view Dimension Reduction |  http://arxiv.org/abs/1502.02330  | author:Yong Luo, Dacheng Tao, Yonggang Wen, Kotagiri Ramamohanarao, Chao Xu category:stat.ML cs.CV cs.LG published:2015-02-09 summary:Canonical correlation analysis (CCA) has proven an effective tool fortwo-view dimension reduction due to its profound theoretical foundation andsuccess in practical applications. In respect of multi-view learning, however,it is limited by its capability of only handling data represented by two-viewfeatures, while in many real-world applications, the number of views isfrequently many more. Although the ad hoc way of simultaneously exploring allpossible pairs of features can numerically deal with multi-view data, itignores the high order statistics (correlation information) which can only bediscovered by simultaneously exploring all features. Therefore, in this work, we develop tensor CCA (TCCA) which straightforwardlyyet naturally generalizes CCA to handle the data of an arbitrary number ofviews by analyzing the covariance tensor of the different views. TCCA aims todirectly maximize the canonical correlation of multiple (more than two) views.Crucially, we prove that the multi-view canonical correlation maximizationproblem is equivalent to finding the best rank-1 approximation of the datacovariance tensor, which can be solved efficiently using the well-knownalternating least squares (ALS) algorithm. As a consequence, the high ordercorrelation information contained in the different views is explored and thus amore reliable common subspace shared by all features can be obtained. Inaddition, a non-linear extension of TCCA is presented. Experiments on variouschallenge tasks, including large scale biometric structure prediction, internetadvertisement classification and web image annotation, demonstrate theeffectiveness of the proposed method.
arxiv-1502-02513 | Evaluation of modelling approaches for predicting the spatial distribution of soil organic carbon stocks at the national scale |  http://arxiv.org/abs/1502.02513  | author:M. P. Martin, T. G. Orton, E. Lacarce, J. Meersmans, N. P. A. Saby, J. B. Paroissien, C. Jolivet, L. Boulonne, D. Arrouays category:stat.AP stat.ML published:2015-02-09 summary:Soil organic carbon (SOC) plays a major role in the global carbon budget. Itcan act as a source or a sink of atmospheric carbon, thereby possiblyinfluencing the course of climate change. Improving the tools that model thespatial distributions of SOC stocks at national scales is a priority, both formonitoring changes in SOC and as an input for global carbon cycles studies. Inthis paper, we compare and evaluate two recent and promising modellingapproaches. First, we considered several increasingly complex boostedregression trees (BRT), a convenient and efficient multiple regression modelfrom the statistical learning field. Further, we considered a robustgeostatistical approach coupled to the BRT models. Testing the differentapproaches was performed on the dataset from the French Soil MonitoringNetwork, with a consistent cross-validation procedure. We showed that when alimited number of predictors were included in the BRT model, the standalone BRTpredictions were significantly improved by robust geostatistical modelling ofthe residuals. However, when data for several SOC drivers were included, thestandalone BRT model predictions were not significantly improved bygeostatistical modelling. Therefore, in this latter situation, the BRTpredictions might be considered adequate without the need for geostatisticalmodelling, provided that i) care is exercised in model fitting and validating,and ii) the dataset does not allow for modelling of local spatialautocorrelations, as is the case for many national systematic sampling schemes.
arxiv-1502-02478 | Efficient batchwise dropout training using submatrices |  http://arxiv.org/abs/1502.02478  | author:Ben Graham, Jeremy Reizenstein, Leigh Robinson category:cs.NE cs.CV published:2015-02-09 summary:Dropout is a popular technique for regularizing artificial neural networks.Dropout networks are generally trained by minibatch gradient descent with adropout mask turning off some of the units---a different pattern of dropout isapplied to every sample in the minibatch. We explore a very simple alternativeto the dropout mask. Instead of masking dropped out units by setting them tozero, we perform matrix multiplication using a submatrix of the weightmatrix---unneeded hidden units are never calculated. Performing dropoutbatchwise, so that one pattern of dropout is used for each sample in aminibatch, we can substantially reduce training times. Batchwise dropout can beused with fully-connected and convolutional neural networks.
arxiv-1502-02322 | Rademacher Observations, Private Data, and Boosting |  http://arxiv.org/abs/1502.02322  | author:Richard Nock, Giorgio Patrini, Arik Friedman category:cs.LG 68Q32 E.4; I.2.6 published:2015-02-09 summary:The minimization of the logistic loss is a popular approach to batchsupervised learning. Our paper starts from the surprising observation that,when fitting linear (or kernelized) classifiers, the minimization of thelogistic loss is \textit{equivalent} to the minimization of an exponential\textit{rado}-loss computed (i) over transformed data that we call Rademacherobservations (rados), and (ii) over the \textit{same} classifier as the one ofthe logistic loss. Thus, a classifier learnt from rados can be\textit{directly} used to classify \textit{observations}. We provide a learningalgorithm over rados with boosting-compliant convergence rates on the\textit{logistic loss} (computed over examples). Experiments on domains with upto millions of examples, backed up by theoretical arguments, display thatlearning over a small set of random rados can challenge the state of the artthat learns over the \textit{complete} set of examples. We show that radoscomply with various privacy requirements that make them good candidates formachine learning in a privacy framework. We give several algebraic, geometricand computational hardness results on reconstructing examples from rados. Wealso show how it is possible to craft, and efficiently learn from, rados in adifferential privacy framework. Tests reveal that learning from differentiallyprivate rados can compete with learning from random rados, and hence with batchlearning from examples, achieving non-trivial privacy vs accuracy tradeoffs.
arxiv-1502-02506 | Predicting Alzheimer's disease: a neuroimaging study with 3D convolutional neural networks |  http://arxiv.org/abs/1502.02506  | author:Adrien Payan, Giovanni Montana category:cs.CV cs.LG stat.AP stat.ML published:2015-02-09 summary:Pattern recognition methods using neuroimaging data for the diagnosis ofAlzheimer's disease have been the subject of extensive research in recentyears. In this paper, we use deep learning methods, and in particular sparseautoencoders and 3D convolutional neural networks, to build an algorithm thatcan predict the disease status of a patient, based on an MRI scan of the brain.We report on experiments using the ADNI data set involving 2,265 historicalscans. We demonstrate that 3D convolutional neural networks outperform severalother classifiers reported in the literature and produce state-of-art results.
arxiv-1502-02410 | Out-of-sample generalizations for supervised manifold learning for classification |  http://arxiv.org/abs/1502.02410  | author:Elif Vural, Christine Guillemot category:cs.CV cs.LG published:2015-02-09 summary:Supervised manifold learning methods for data classification map data samplesresiding in a high-dimensional ambient space to a lower-dimensional domain in astructure-preserving way, while enhancing the separation between differentclasses in the learned embedding. Most nonlinear supervised manifold learningmethods compute the embedding of the manifolds only at the initially availabletraining points, while the generalization of the embedding to novel points,known as the out-of-sample extension problem in manifold learning, becomesespecially important in classification applications. In this work, we propose asemi-supervised method for building an interpolation function that provides anout-of-sample extension for general supervised manifold learning algorithmsstudied in the context of classification. The proposed algorithm computes aradial basis function (RBF) interpolator that minimizes an objective functionconsisting of the total embedding error of unlabeled test samples, defined astheir distance to the embeddings of the manifolds of their own class, as wellas a regularization term that controls the smoothness of the interpolationfunction in a direction-dependent way. The class labels of test data and theinterpolation function parameters are estimated jointly with a progressiveprocedure. Experimental results on face and object images demonstrate thepotential of the proposed out-of-sample extension algorithm for theclassification of manifold-modeled data sets.
arxiv-1502-02398 | Towards a Learning Theory of Cause-Effect Inference |  http://arxiv.org/abs/1502.02398  | author:David Lopez-Paz, Krikamol Muandet, Bernhard Schölkopf, Ilya Tolstikhin category:stat.ML math.PR math.ST stat.TH published:2015-02-09 summary:We pose causal inference as the problem of learning to classify probabilitydistributions. In particular, we assume access to a collection$\{(S_i,l_i)\}_{i=1}^n$, where each $S_i$ is a sample drawn from theprobability distribution of $X_i \times Y_i$, and $l_i$ is a binary labelindicating whether "$X_i \to Y_i$" or "$X_i \leftarrow Y_i$". Given these data,we build a causal inference rule in two steps. First, we featurize each $S_i$using the kernel mean embedding associated with some characteristic kernel.Second, we train a binary classifier on such embeddings to distinguish betweencausal directions. We present generalization bounds showing the statisticalconsistency and learning rates of the proposed approach, and provide a simpleimplementation that achieves state-of-the-art cause-effect inference.Furthermore, we extend our ideas to infer causal relationships between morethan two variables.
arxiv-1502-02445 | Deep Neural Networks for Anatomical Brain Segmentation |  http://arxiv.org/abs/1502.02445  | author:Alexandre de Brebisson, Giovanni Montana category:cs.CV cs.LG stat.AP stat.ML published:2015-02-09 summary:We present a novel approach to automatically segment magnetic resonance (MR)images of the human brain into anatomical regions. Our methodology is based ona deep artificial neural network that assigns each voxel in an MR image of thebrain to its corresponding anatomical region. The inputs of the network captureinformation at different scales around the voxel of interest: 3D and orthogonal2D intensity patches capture the local spatial context while large, compressed2D orthogonal patches and distances to the regional centroids enforce globalspatial consistency. Contrary to commonly used segmentation methods, ourtechnique does not require any non-linear registration of the MR images. Tobenchmark our model, we used the dataset provided for the MICCAI 2012 challengeon multi-atlas labelling, which consists of 35 manually segmented MR images ofthe brain. We obtained competitive results (mean dice coefficient 0.725, errorrate 0.163) showing the potential of our approach. To our knowledge, ourtechnique is the first to tackle the anatomical segmentation of the whole brainusing deep neural networks.
arxiv-1502-02609 | Efficient model-based reinforcement learning for approximate online optimal |  http://arxiv.org/abs/1502.02609  | author:Rushikesh Kamalapurkar, Joel A. Rosenfeld, Warren E. Dixon category:cs.SY cs.LG math.OC published:2015-02-09 summary:In this paper the infinite horizon optimal regulation problem is solvedonline for a deterministic control-affine nonlinear dynamical system using thestate following (StaF) kernel method to approximate the value function. Unliketraditional methods that aim to approximate a function over a large compactset, the StaF kernel method aims to approximate a function in a smallneighborhood of a state that travels within a compact set. Simulation resultsdemonstrate that stability and approximate optimality of the control system canbe achieved with significantly fewer basis functions than may be required forglobal approximation methods.
arxiv-1502-02512 | The Adaptive Mean-Linkage Algorithm: A Bottom-Up Hierarchical Cluster Technique |  http://arxiv.org/abs/1502.02512  | author:H. M. de Oliveira category:stat.ME cs.LG stat.AP published:2015-02-09 summary:In this paper a variant of the classical hierarchical cluster analysis isreported. This agglomerative (bottom-up) cluster technique is referred to asthe Adaptive Mean-Linkage Algorithm. It can be interpreted as a linkagealgorithm where the value of the threshold is conveniently up-dated at eachinteraction. The superiority of the adaptive clustering with respect to theaverage-linkage algorithm follows because it achieves a good compromise onthreshold values: Thresholds based on the cut-off distance are sufficientlysmall to assure the homogeneity and also large enough to guarantee at least apair of merging sets. This approach is applied to a set of possiblesubstituents in a chemical series.
arxiv-1502-02344 | Regularization Path of Cross-Validation Error Lower Bounds |  http://arxiv.org/abs/1502.02344  | author:Atsushi Shibagaki, Yoshiki Suzuki, Masayuki Karasuyama, Ichiro Takeuchi category:stat.ML published:2015-02-09 summary:Careful tuning of a regularization parameter is indispensable in many machinelearning tasks because it has a significant impact on generalizationperformances. Nevertheless, current practice of regularization parameter tuningis more of an art than a science, e.g., it is hard to tell how many grid-pointswould be needed in cross-validation (CV) for obtaining a solution withsufficiently small CV error. In this paper we propose a novel framework forcomputing a lower bound of the CV errors as a function of the regularizationparameter, which we call regularization path of CV error lower bounds. Theproposed framework can be used for providing a theoretical approximationguarantee on a set of solutions in the sense that how far the CV error of thecurrent best solution could be away from best possible CV error in the entirerange of the regularization parameters. We demonstrate through numericalexperiments that a theoretically guaranteed a choice of regularizationparameter in the above sense is possible with reasonable computational costs.
arxiv-1502-02599 | Adaptive Random SubSpace Learning (RSSL) Algorithm for Prediction |  http://arxiv.org/abs/1502.02599  | author:Mohamed Elshrif, Ernest Fokoue category:cs.LG published:2015-02-09 summary:We present a novel adaptive random subspace learning algorithm (RSSL) forprediction purpose. This new framework is flexible where it can be adapted withany learning technique. In this paper, we tested the algorithm for regressionand classification problems. In addition, we provide a variety of weightingschemes to increase the robustness of the developed algorithm. These differentwighting flavors were evaluated on simulated as well as on real-world data setsconsidering the cases where the ratio between features (attributes) andinstances (samples) is large and vice versa. The framework of the new algorithmconsists of many stages: first, calculate the weights of all features on thedata set using the correlation coefficient and F-statistic statisticalmeasurements. Second, randomly draw n samples with replacement from the dataset. Third, perform regular bootstrap sampling (bagging). Fourth, draw withoutreplacement the indices of the chosen variables. The decision was taken basedon the heuristic subspacing scheme. Fifth, call base learners and build themodel. Sixth, use the model for prediction purpose on test set of the data. Theresults show the advancement of the adaptive RSSL algorithm in most of thecases compared with the synonym (conventional) machine learning algorithms.
arxiv-1502-02407 | A Social Spider Algorithm for Global Optimization |  http://arxiv.org/abs/1502.02407  | author:James J. Q. Yu, Victor O. K. Li category:cs.NE published:2015-02-09 summary:The growing complexity of real-world problems has motivated computerscientists to search for efficient problem-solving methods. Metaheuristicsbased on evolutionary computation and swarm intelligence are outstandingexamples of nature-inspired solution techniques. Inspired by the socialspiders, we propose a novel Social Spider Algorithm to solve globaloptimization problems. This algorithm is mainly based on the foraging strategyof social spiders, utilizing the vibrations on the spider web to determine thepositions of preys. Different from the previously proposed swarm intelligencealgorithms, we introduce a new social animal foraging strategy model to solveoptimization problems. In addition, we perform preliminary parametersensitivity analysis for our proposed algorithm, developing guidelines forchoosing the parameter values. The Social Spider Algorithm is evaluated by aseries of widely-used benchmark functions, and our proposed algorithm hassuperior performance compared with other state-of-the-art metaheuristics.
arxiv-1502-02590 | Analysis of classifiers' robustness to adversarial perturbations |  http://arxiv.org/abs/1502.02590  | author:Alhussein Fawzi, Omar Fawzi, Pascal Frossard category:cs.LG cs.CV stat.ML published:2015-02-09 summary:The goal of this paper is to analyze an intriguing phenomenon recentlydiscovered in deep networks, namely their instability to adversarialperturbations (Szegedy et. al., 2014). We provide a theoretical framework foranalyzing the robustness of classifiers to adversarial perturbations, and showfundamental upper bounds on the robustness of classifiers. Specifically, weestablish a general upper bound on the robustness of classifiers to adversarialperturbations, and then illustrate the obtained upper bound on the families oflinear and quadratic classifiers. In both cases, our upper bound depends on adistinguishability measure that captures the notion of difficulty of theclassification task. Our results for both classes imply that in tasks involvingsmall distinguishability, no classifier in the considered set will be robust toadversarial perturbations, even if a good accuracy is achieved. Our theoreticalframework moreover suggests that the phenomenon of adversarial instability isdue to the low flexibility of classifiers, compared to the difficulty of theclassification task (captured by the distinguishability). Moreover, we show theexistence of a clear distinction between the robustness of a classifier torandom noise and its robustness to adversarial perturbations. Specifically, theformer is shown to be larger than the latter by a factor that is proportionalto \sqrt{d} (with d being the signal dimension) for linear classifiers. Thisresult gives a theoretical explanation for the discrepancy between the tworobustness properties in high dimensional problems, which was empiricallyobserved in the context of neural networks. To the best of our knowledge, ourresults provide the first theoretical work that addresses the phenomenon ofadversarial instability recently observed for deep networks. Our analysis iscomplemented by experimental results on controlled and real-world data.
arxiv-1502-02558 | K2-ABC: Approximate Bayesian Computation with Kernel Embeddings |  http://arxiv.org/abs/1502.02558  | author:Mijung Park, Wittawat Jitkrittum, Dino Sejdinovic category:stat.ML cs.LG published:2015-02-09 summary:Complicated generative models often result in a situation where computing thelikelihood of observed data is intractable, while simulating from theconditional density given a parameter value is relatively easy. ApproximateBayesian Computation (ABC) is a paradigm that enables simulation-basedposterior inference in such cases by measuring the similarity between simulatedand observed data in terms of a chosen set of summary statistics. However,there is no general rule to construct sufficient summary statistics for complexmodels. Insufficient summary statistics will "leak" information, which leads toABC algorithms yielding samples from an incorrect (partial) posterior. In thispaper, we propose a fully nonparametric ABC paradigm which circumvents the needfor manually selecting summary statistics. Our approach, K2-ABC, uses maximummean discrepancy (MMD) as a dissimilarity measure between the distributionsover observed and simulated data. MMD is easily estimated as the squareddifference between their empirical kernel embeddings. Experiments on asimulated scenario and a real-world biological problem illustrate theeffectiveness of the proposed algorithm.
arxiv-1502-02536 | Nested Sequential Monte Carlo Methods |  http://arxiv.org/abs/1502.02536  | author:Christian A. Naesseth, Fredrik Lindsten, Thomas B. Schön category:stat.CO stat.ME stat.ML published:2015-02-09 summary:We propose nested sequential Monte Carlo (NSMC), a methodology to sample fromsequences of probability distributions, even where the random variables arehigh-dimensional. NSMC generalises the SMC framework by requiring onlyapproximate, properly weighted, samples from the SMC proposal distribution,while still resulting in a correct SMC algorithm. Furthermore, NSMC can initself be used to produce such properly weighted samples. Consequently, oneNSMC sampler can be used to construct an efficient high-dimensional proposaldistribution for another NSMC sampler, and this nesting of the algorithm can bedone to an arbitrary degree. This allows us to consider complex andhigh-dimensional models using SMC. We show results that motivate the efficacyof our approach on several filtering problems with dimensions in the order of100 to 1 000.
arxiv-1502-02734 | Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation |  http://arxiv.org/abs/1502.02734  | author:George Papandreou, Liang-Chieh Chen, Kevin Murphy, Alan L. Yuille category:cs.CV published:2015-02-09 summary:Deep convolutional neural networks (DCNNs) trained on a large number ofimages with strong pixel-level annotations have recently significantly pushedthe state-of-art in semantic image segmentation. We study the more challengingproblem of learning DCNNs for semantic image segmentation from either (1)weakly annotated training data such as bounding boxes or image-level labels or(2) a combination of few strongly labeled and many weakly labeled images,sourced from one or multiple datasets. We develop Expectation-Maximization (EM)methods for semantic image segmentation model training under these weaklysupervised and semi-supervised settings. Extensive experimental evaluationshows that the proposed techniques can learn models delivering competitiveresults on the challenging PASCAL VOC 2012 image segmentation benchmark, whilerequiring significantly less annotation effort. We share source codeimplementing the proposed system athttps://bitbucket.org/deeplab/deeplab-public.
arxiv-1502-02355 | High dimensional errors-in-variables models with dependent measurements |  http://arxiv.org/abs/1502.02355  | author:Mark Rudelson, Shuheng Zhou category:math.ST stat.ML stat.TH published:2015-02-09 summary:Suppose that we observe $y \in \mathbb{R}^f$ and $X \in \mathbb{R}^{f \timesm}$ in the following errors-in-variables model: \begin{eqnarray*} y & = & X_0\beta^* + \epsilon \\ X & = & X_0 + W \end{eqnarray*} where $X_0$ is a $f\times m$ design matrix with independent subgaussian row vectors, $\epsilon \in\mathbb{R}^f$ is a noise vector and $W$ is a mean zero $f \times m$ randomnoise matrix with independent subgaussian column vectors, independent of $X_0$and $\epsilon$. This model is significantly different from those analyzed inthe literature in the sense that we allow the measurement error for eachcovariate to be a dependent vector across its $f$ observations. Such errorstructures appear in the science literature when modeling the trial-to-trialfluctuations in response strength shared across a set of neurons. Under sparsity and restrictive eigenvalue type of conditions, we show thatone is able to recover a sparse vector $\beta^* \in \mathbb{R}^m$ from themodel given a single observation matrix $X$ and the response vector $y$. Weestablish consistency in estimating $\beta^*$ and obtain the rates ofconvergence in the $\ell_q$ norm, where $q = 1, 2$ for the Lasso-typeestimator, and for $q \in [1, 2]$ for a Dantzig-type conic programmingestimator. We show error bounds which approach that of the regular Lasso andthe Dantzig selector in case the errors in $W$ are tending to 0.
arxiv-1502-02476 | An Infinite Restricted Boltzmann Machine |  http://arxiv.org/abs/1502.02476  | author:Marc-Alexandre Côté, Hugo Larochelle category:cs.LG published:2015-02-09 summary:We present a mathematical construction for the restricted Boltzmann machine(RBM) that doesn't require specifying the number of hidden units. In fact, thehidden layer size is adaptive and can grow during training. This is obtained byfirst extending the RBM to be sensitive to the ordering of its hidden units.Then, thanks to a carefully chosen definition of the energy function, we showthat the limit of infinitely many hidden units is well defined. As with RBM,approximate maximum likelihood training can be performed, resulting in analgorithm that naturally and adaptively adds trained hidden units duringlearning. We empirically study the behaviour of this infinite RBM, showing thatits performance is competitive to that of the RBM, while not requiring thetuning of a hidden layer size.
arxiv-1502-02347 | Local and Global Inference for High Dimensional Nonparanormal Graphical Models |  http://arxiv.org/abs/1502.02347  | author:Quanquan Gu, Yuan Cao, Yang Ning, Han Liu category:stat.ML published:2015-02-09 summary:This paper proposes a unified framework to quantify local and globalinferential uncertainty for high dimensional nonparanormal graphical models. Inparticular, we consider the problems of testing the presence of a single edgeand constructing a uniform confidence subgraph. Due to the presence of unknownmarginal transformations, we propose a pseudo likelihood based inferentialapproach. In sharp contrast to the existing high dimensional score test method,our method is free of tuning parameters given an initial estimator, and extendsthe scope of the existing likelihood based inferential framework. Furthermore,we propose a U-statistic multiplier bootstrap method to construct theconfidence subgraph. We show that the constructed subgraph is contained in thetrue graph with probability greater than a given nominal level. Compared withexisting methods for constructing confidence subgraphs, our method does notrely on Gaussian or sub-Gaussian assumptions. The theoretical properties of theproposed inferential methods are verified by thorough numerical experiments andreal data analysis.
arxiv-1502-02377 | Sparse Coding with Earth Mover's Distance for Multi-Instance Histogram Representation |  http://arxiv.org/abs/1502.02377  | author:Mohua Zhang, Jianhua Peng, Xuejie Liu, Jim Jing-Yan Wang category:cs.LG stat.ML published:2015-02-09 summary:Sparse coding (Sc) has been studied very well as a powerful datarepresentation method. It attempts to represent the feature vector of a datasample by reconstructing it as the sparse linear combination of some basicelements, and a $L_2$ norm distance function is usually used as the lossfunction for the reconstruction error. In this paper, we investigate using Scas the representation method within multi-instance learning framework, where asample is given as a bag of instances, and further represented as a histogramof the quantized instances. We argue that for the data type of histogram, using$L_2$ norm distance is not suitable, and propose to use the earth mover'sdistance (EMD) instead of $L_2$ norm distance as a measure of thereconstruction error. By minimizing the EMD between the histogram of a sampleand the its reconstruction from some basic histograms, a novel sparse codingmethod is developed, which is refereed as SC-EMD. We evaluate its performancesas a histogram representation method in tow multi-instance learning problems--- abnormal image detection in wireless capsule endoscopy videos, and proteinbinding site retrieval. The encouraging results demonstrate the advantages ofthe new method over the traditional method using $L_2$ norm distance.
arxiv-1502-02259 | Contextual Markov Decision Processes |  http://arxiv.org/abs/1502.02259  | author:Assaf Hallak, Dotan Di Castro, Shie Mannor category:stat.ML cs.LG published:2015-02-08 summary:We consider a planning problem where the dynamics and rewards of theenvironment depend on a hidden static parameter referred to as the context. Theobjective is to learn a strategy that maximizes the accumulated reward acrossall contexts. The new model, called Contextual Markov Decision Process (CMDP),can model a customer's behavior when interacting with a website (the learner).The customer's behavior depends on gender, age, location, device, etc. Based onthat behavior, the website objective is to determine customer characteristics,and to optimize the interaction between them. Our work focuses on one basicscenario--finite horizon with a small known number of possible contexts. Wesuggest a family of algorithms with provable guarantees that learn theunderlying models and the latent contexts, and optimize the CMDPs. Bounds areobtained for specific naive implementations, and extensions of the frameworkare discussed, laying the ground for future research.
arxiv-1502-02233 | Hierarchical Dirichlet process for tracking complex topical structure evolution and its application to autism research literature |  http://arxiv.org/abs/1502.02233  | author:Adham Beykikhoshk, Ognjen Arandjelovic, Dinh Phung, Svetha Venkatesh category:cs.IR cs.CL published:2015-02-08 summary:In this paper we describe a novel framework for the discovery of the topicalcontent of a data corpus, and the tracking of its complex structural changesacross the temporal dimension. In contrast to previous work our model does notimpose a prior on the rate at which documents are added to the corpus nor doesit adopt the Markovian assumption which overly restricts the type of changesthat the model can capture. Our key technical contribution is a framework basedon (i) discretization of time into epochs, (ii) epoch-wise topic discoveryusing a hierarchical Dirichlet process-based model, and (iii) a temporalsimilarity graph which allows for the modelling of complex topic changes:emergence and disappearance, evolution, and splitting and merging. The power ofthe proposed framework is demonstrated on the medical literature corpusconcerned with the autism spectrum disorder (ASD) - an increasingly importantresearch subject of significant social and healthcare importance. In additionto the collected ASD literature corpus which we will make freely available, ourcontributions also include two free online tools we built as aids to ASDresearchers. These can be used for semantically meaningful navigation andsearching, as well as knowledge discovery from this large and rapidly growingcorpus of literature.
arxiv-1502-02215 | Real World Applications of Machine Learning Techniques over Large Mobile Subscriber Datasets |  http://arxiv.org/abs/1502.02215  | author:Jobin Wilson, Chitharanj Kachappilly, Rakesh Mohan, Prateek Kapadia, Arun Soman, Santanu Chaudhury category:cs.LG cs.CY cs.SE published:2015-02-08 summary:Communication Service Providers (CSPs) are in a unique position to utilizetheir vast transactional data assets generated from interactions of subscriberswith network elements as well as with other subscribers. CSPs could leverageits data assets for a gamut of applications such as service personalization,predictive offer management, loyalty management, revenue forecasting, networkcapacity planning, product bundle optimization and churn management to gainsignificant competitive advantage. However, due to the sheer data volume,variety, velocity and veracity of mobile subscriber datasets, sophisticateddata analytics techniques and frameworks are necessary to derive actionableinsights in a useable timeframe. In this paper, we describe our journey from arelational database management system (RDBMS) based campaign managementsolution which allowed data scientists and marketers to use hand-written rulesfor service personalization and targeted promotions to a distributed Big DataAnalytics platform, capable of performing large scale machine learning and datamining to deliver real time service personalization, predictive modelling andproduct optimization. Our work involves a careful blend of technology,processes and best practices, which facilitate man-machine collaboration andcontinuous experimentation to derive measurable economic value from data. Ourplatform has a reach of more than 500 million mobile subscribers worldwide,delivering over 1 billion personalized recommendations annually, processing atotal data volume of 64 Petabytes, corresponding to 8.5 trillion events.
arxiv-1502-02268 | SDNA: Stochastic Dual Newton Ascent for Empirical Risk Minimization |  http://arxiv.org/abs/1502.02268  | author:Zheng Qu, Peter Richtárik, Martin Takáč, Olivier Fercoq category:cs.LG published:2015-02-08 summary:We propose a new algorithm for minimizing regularized empirical loss:Stochastic Dual Newton Ascent (SDNA). Our method is dual in nature: in eachiteration we update a random subset of the dual variables. However, unlikeexisting methods such as stochastic dual coordinate ascent, SDNA is capable ofutilizing all curvature information contained in the examples, which leads tostriking improvements in both theory and practice - sometimes by orders ofmagnitude. In the special case when an L2-regularizer is used in the primal,the dual problem is a concave quadratic maximization problem plus a separableterm. In this regime, SDNA in each step solves a proximal subproblem involvinga random principal submatrix of the Hessian of the quadratic function; whencethe name of the method. If, in addition, the loss functions are quadratic, ourmethod can be interpreted as a novel variant of the recently introducedIterative Hessian Sketch.
arxiv-1502-02251 | From Pixels to Torques: Policy Learning with Deep Dynamical Models |  http://arxiv.org/abs/1502.02251  | author:Niklas Wahlström, Thomas B. Schön, Marc Peter Deisenroth category:stat.ML cs.LG cs.RO cs.SY published:2015-02-08 summary:Data-efficient learning in continuous state-action spaces using veryhigh-dimensional observations remains a key challenge in developing fullyautonomous systems. In this paper, we consider one instance of this challenge,the pixels to torques problem, where an agent must learn a closed-loop controlpolicy from pixel information only. We introduce a data-efficient, model-basedreinforcement learning algorithm that learns such a closed-loop policy directlyfrom pixel information. The key ingredient is a deep dynamical model that usesdeep auto-encoders to learn a low-dimensional embedding of images jointly witha predictive model in this low-dimensional feature space. Joint learningensures that not only static but also dynamic properties of the data areaccounted for. This is crucial for long-term predictions, which lie at the coreof the adaptive model predictive control strategy that we use for closed-loopcontrol. Compared to state-of-the-art reinforcement learning methods forcontinuous states and actions, our approach learns quickly, scales tohigh-dimensional state spaces and is an important step toward fully autonomouslearning from pixels to torques.
arxiv-1502-02277 | Improving Term Frequency Normalization for Multi-topical Documents, and Application to Language Modeling Approaches |  http://arxiv.org/abs/1502.02277  | author:Seung-Hoon Na, In-Su Kang, Jong-Hyeok Lee category:cs.IR cs.CL H.3.3 published:2015-02-08 summary:Term frequency normalization is a serious issue since lengths of documentsare various. Generally, documents become long due to two different reasons -verbosity and multi-topicality. First, verbosity means that the same topic isrepeatedly mentioned by terms related to the topic, so that term frequency ismore increased than the well-summarized one. Second, multi-topicality indicatesthat a document has a broad discussion of multi-topics, rather than singletopic. Although these document characteristics should be differently handled,all previous methods of term frequency normalization have ignored thesedifferences and have used a simplified length-driven approach which decreasesthe term frequency by only the length of a document, causing an unreasonablepenalization. To attack this problem, we propose a novel TF normalizationmethod which is a type of partially-axiomatic approach. We first formulate twoformal constraints that the retrieval model should satisfy for documents havingverbose and multi-topicality characteristic, respectively. Then, we modifylanguage modeling approaches to better satisfy these two constraints, andderive novel smoothing methods. Experimental results show that the proposedmethod increases significantly the precision for keyword queries, andsubstantially improves MAP (Mean Average Precision) for verbose queries.
arxiv-1502-02309 | Measuring the functional connectome "on-the-fly": towards a new control signal for fMRI-based brain-computer interfaces |  http://arxiv.org/abs/1502.02309  | author:Ricardo Pio Monti, Romy Lorenz, Christoforos Anagnostopoulos, Robert Leech, Giovanni Montana category:stat.ML published:2015-02-08 summary:There has been an explosion of interest in functional Magnetic ResonanceImaging (MRI) during the past two decades. Naturally, this has been accompaniedby many major advances in the understanding of the human connectome. Theseadvances have served to pose novel challenges as well as open new avenues forresearch. One of the most promising and exciting of such avenues is the studyof functional MRI in real-time. Such studies have recently gained momentum andhave been applied in a wide variety of settings; ranging from training ofhealthy subjects to self-regulate neuronal activity to being suggested aspotential treatments for clinical populations. To date, the vast majority ofthese studies have focused on a single region at a time. This is due in part tothe many challenges faced when estimating dynamic functional connectivitynetworks in real-time. In this work we propose a novel methodology with whichto accurately track changes in functional connectivity networks in real-time.We adapt the recently proposed SINGLE algorithm for estimating sparse andtemporally homo- geneous dynamic networks to be applicable in real-time. Theproposed method is applied to motor task data from the Human Connectome Projectas well as to real-time data ob- tained while exploring a virtual environment.We show that the algorithm is able to estimate signi?cant task-related changesin network structure quickly enough to be useful in future brain-computerinterface applications.
arxiv-1502-02206 | Learning to Search Better Than Your Teacher |  http://arxiv.org/abs/1502.02206  | author:Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daumé III, John Langford category:cs.LG stat.ML published:2015-02-08 summary:Methods for learning to search for structured prediction typically imitate areference policy, with existing theoretical guarantees demonstrating low regretcompared to that reference. This is unsatisfactory in many applications wherethe reference policy is suboptimal and the goal of learning is to improve uponit. Can learning to search work even when the reference is poor? We provide a new learning to search algorithm, LOLS, which does well relativeto the reference policy, but additionally guarantees low regret compared todeviations from the learned policy: a local-optimality guarantee. Consequently,LOLS can improve upon the reference policy, unlike previous algorithms. Thisenables us to develop structured contextual bandits, a partial informationstructured prediction setting with many potential applications.
arxiv-1502-02160 | A Survey on Hough Transform, Theory, Techniques and Applications |  http://arxiv.org/abs/1502.02160  | author:Allam Shehata Hassanein, Sherien Mohammad, Mohamed Sameer, Mohammad Ehab Ragab category:cs.CV published:2015-02-07 summary:For more than half a century, the Hough transform is ever-expanding for newfrontiers. Thousands of research papers and numerous applications have evolvedover the decades. Carrying out an all-inclusive survey is hardly possible andenormously space-demanding. What we care about here is emphasizing some of themost crucial milestones of the transform. We describe its variationselaborating on the basic ones such as the line and circle Hough transforms. Thehigh demand for storage and computation time is clarified with differentsolution approaches. Since most uses of the transform take place on binaryimages, we have been concerned with the work done directly on gray or colorimages. The myriad applications of the standard transform and its variationshave been classified highlighting the up-to-date and the unconventional ones.Due to its merits such as noise-immunity and expandability, the transform hasan excellent history, and a bright future as well.
arxiv-1502-02182 | Comparison of Algorithms for Compressed Sensing of Magnetic Resonance Images |  http://arxiv.org/abs/1502.02182  | author:Jelena Badnjar category:cs.CV published:2015-02-07 summary:Magnetic resonance imaging (MRI) is an essential medical tool with inherentlyslow data acquisition process. Slow acquisition process requires patient to belong time exposed to scanning apparatus. In recent years significant effortsare made towards the applying Compressive Sensing technique to the acquisitionprocess of MRI and biomedical images. Compressive Sensing is an emerging theoryin signal processing. It aims to reduce the amount of acquired data requiredfor successful signal reconstruction. Reducing the amount of acquired imagecoefficients leads to lower acquisition time, i.e. time of exposition to theMRI apparatus. Using optimization algorithms, satisfactory image quality can beobtained from the small set of acquired samples. A number of optimizationalgorithms for the reconstruction of the biomedical images is proposed in theliterature. In this paper, three commonly used optimization algorithms arecompared and results are presented on the several MRI images.
arxiv-1502-02655 | An investigation into language complexity of World-of-Warcraft game-external texts |  http://arxiv.org/abs/1502.02655  | author:Simon Šuster category:cs.CL published:2015-02-07 summary:We present a language complexity analysis of World of Warcraft (WoW)community texts, which we compare to texts from a general corpus of webEnglish. Results from several complexity types are presented, including lexicaldiversity, density, readability and syntactic complexity. The language of WoWtexts is found to be comparable to the general corpus on some complexitymeasures, yet more specialized on other measures. Our findings can be used byeducators willing to include game-related activities into school curricula.
arxiv-1502-02158 | Learning Parametric-Output HMMs with Two Aliased States |  http://arxiv.org/abs/1502.02158  | author:Roi Weiss, Boaz Nadler category:cs.LG published:2015-02-07 summary:In various applications involving hidden Markov models (HMMs), some of thehidden states are aliased, having identical output distributions. Theminimality, identifiability and learnability of such aliased HMMs have beenlong standing problems, with only partial solutions provided thus far. In thispaper we focus on parametric-output HMMs, whose output distributions come froma parametric family, and that have exactly two aliased states. For this class,we present a complete characterization of their minimality and identifiability.Furthermore, for a large family of parametric output distributions, we derivecomputationally efficient and statistically consistent algorithms to detect thepresence of aliasing and learn the aliased HMM transition and emissionparameters. We illustrate our theoretical analysis by several simulations.
arxiv-1502-02092 | Reflectance Hashing for Material Recognition |  http://arxiv.org/abs/1502.02092  | author:Hang Zhang, Kristin Dana, Ko Nishino category:cs.CV published:2015-02-07 summary:We introduce a novel method for using reflectance to identify materials.Reflectance offers a unique signature of the material but is challenging tomeasure and use for recognizing materials due to its high-dimensionality. Inthis work, one-shot reflectance is captured using a unique optical camerameasuring {\it reflectance disks} where the pixel coordinates correspond tosurface viewing angles. The reflectance has class-specific stucture and angulargradients computed in this reflectance space reveal the material class. These reflectance disks encode discriminative information for efficient andaccurate material recognition. We introduce a framework called reflectancehashing that models the reflectance disks with dictionary learning and binaryhashing. We demonstrate the effectiveness of reflectance hashing for materialrecognition with a number of real-world materials.
arxiv-1502-02089 | Discriminative training for Convolved Multiple-Output Gaussian processes |  http://arxiv.org/abs/1502.02089  | author:Sebastián Gómez-González, Mauricio A. Álvarez, Hernán Felipe García category:stat.ML published:2015-02-07 summary:Multi-output Gaussian processes (MOGP) are probability distributions overvector-valued functions, and have been previously used for multi-outputregression and for multi-class classification. A less explored facet of themulti-output Gaussian process is that it can be used as a generative model forvector-valued random fields in the context of pattern recognition. As agenerative model, the multi-output GP is able to handle vector-valued functionswith continuous inputs, as opposed, for example, to hidden Markov models. Italso offers the ability to model multivariate random functions with highdimensional inputs. In this report, we use a discriminative training criteriaknown as Minimum Classification Error to fit the parameters of a multi-outputGaussian process. We compare the performance of generative training anddiscriminative training of MOGP in emotion recognition, activity recognition,and face recognition. We also compare the proposed methodology against hiddenMarkov models trained in a generative and in a discriminative way.
arxiv-1502-02125 | Contextual Online Learning for Multimedia Content Aggregation |  http://arxiv.org/abs/1502.02125  | author:Cem Tekin, Mihaela van der Schaar category:cs.MM cs.LG cs.MA published:2015-02-07 summary:The last decade has witnessed a tremendous growth in the volume as well asthe diversity of multimedia content generated by a multitude of sources (newsagencies, social media, etc.). Faced with a variety of content choices,consumers are exhibiting diverse preferences for content; their preferencesoften depend on the context in which they consume content as well as variousexogenous events. To satisfy the consumers' demand for such diverse content,multimedia content aggregators (CAs) have emerged which gather content fromnumerous multimedia sources. A key challenge for such systems is to accuratelypredict what type of content each of its consumers prefers in a certaincontext, and adapt these predictions to the evolving consumers' preferences,contexts and content characteristics. We propose a novel, distributed, onlinemultimedia content aggregation framework, which gathers content generated bymultiple heterogeneous producers to fulfill its consumers' demand for content.Since both the multimedia content characteristics and the consumers'preferences and contexts are unknown, the optimal content aggregation strategyis unknown a priori. Our proposed content aggregation algorithm is able tolearn online what content to gather and how to match content and users byexploiting similarities between consumer types. We prove bounds for ourproposed learning algorithms that guarantee both the accuracy of thepredictions as well as the learning speed. Importantly, our algorithms operateefficiently even when feedback from consumers is missing or content andpreferences evolve over time. Illustrative results highlight the merits of theproposed content aggregation system in a variety of settings.
arxiv-1502-02127 | Hyperparameter Search in Machine Learning |  http://arxiv.org/abs/1502.02127  | author:Marc Claesen, Bart De Moor category:cs.LG stat.ML published:2015-02-07 summary:We introduce the hyperparameter search problem in the field of machinelearning and discuss its main challenges from an optimization perspective.Machine learning methods attempt to build models that capture some element ofinterest based on given data. Most common learning algorithms feature a set ofhyperparameters that must be determined before training commences. The choiceof hyperparameters can significantly affect the resulting model's performance,but determining good values can be complex; hence a disciplined, theoreticallysound search strategy is essential.
arxiv-1502-02171 | Person Re-identification Meets Image Search |  http://arxiv.org/abs/1502.02171  | author:Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jiahao Bu, Qi Tian category:cs.CV published:2015-02-07 summary:For long time, person re-identification and image search are two separatelystudied tasks. However, for person re-identification, the effectiveness oflocal features and the "query-search" mode make it well posed for image searchtechniques. In the light of recent advances in image search, this paper proposes to treatperson re-identification as an image search problem. Specifically, this paperclaims two major contributions. 1) By designing an unsupervised Bag-of-Wordsrepresentation, we are devoted to bridging the gap between the two tasks byintegrating techniques from image search in person re-identification. We showthat our system sets up an effective yet efficient baseline that is amenable tofurther supervised/unsupervised improvements. 2) We contribute a new highquality dataset which uses DPM detector and includes a number of distractorimages. Our dataset reaches closer to realistic settings, and new perspectivesare provided. Compared with approaches that rely on feature-feature match, our method isfaster by over two orders of magnitude. Moreover, on three datasets, we reportcompetitive results compared with the state-of-the-art methods.
arxiv-1502-01988 | Computational and Statistical Boundaries for Submatrix Localization in a Large Noisy Matrix |  http://arxiv.org/abs/1502.01988  | author:T. Tony Cai, Tengyuan Liang, Alexander Rakhlin category:math.ST stat.ML stat.TH published:2015-02-06 summary:The interplay between computational efficiency and statistical accuracy inhigh-dimensional inference has drawn increasing attention in the literature. Inthis paper, we study computational and statistical boundaries for submatrixlocalization. Given one observation of (one or multiple non-overlapping) signalsubmatrix (of magnitude $\lambda$ and size $k_m \times k_n$) contaminated witha noise matrix (of size $m \times n$), we establish two transition thresholdsfor the signal to noise $\lambda/\sigma$ ratio in terms of $m$, $n$, $k_m$, and$k_n$. The first threshold, $\sf SNR_c$, corresponds to the computationalboundary. Below this threshold, it is shown that no polynomial time algorithmcan succeed in identifying the submatrix, under the \textit{hidden cliquehypothesis}. We introduce adaptive linear time spectral algorithms thatidentify the submatrix with high probability when the signal strength is abovethe threshold $\sf SNR_c$. The second threshold, $\sf SNR_s$, captures thestatistical boundary, below which no method can succeed with probability goingto one in the minimax sense. The exhaustive search method successfully findsthe submatrix above this threshold. The results show an interesting phenomenonthat $\sf SNR_c$ is always significantly larger than $\sf SNR_s$, which impliesan essential gap between statistical optimality and computational efficiencyfor submatrix localization.
arxiv-1502-01953 | A Generalization of the Borkar-Meyn Theorem for Stochastic Recursive Inclusions |  http://arxiv.org/abs/1502.01953  | author:Arunselvan Ramaswamy, Shalabh Bhatnagar category:cs.SY math.DS stat.ML published:2015-02-06 summary:In this paper the stability theorem of Borkar and Meyn is extended to includethe case when the mean field is a differential inclusion. Two different sets ofsufficient conditions are presented that guarantee the stability andconvergence of stochastic recursive inclusions. Our work builds on the works ofBenaim, Hofbauer and Sorin as well as Borkar and Meyn. As a corollary to one ofthe main theorems, a natural generalization of the Borkar and Meyn Theoremfollows. In addition, the original theorem of Borkar and Meyn is shown to holdunder slightly relaxed assumptions. Finally, as an application to one of themain theorems we discuss a solution to the approximate drift problem.
arxiv-1502-01908 | Marginalizing Gaussian Process Hyperparameters using Sequential Monte Carlo |  http://arxiv.org/abs/1502.01908  | author:Andreas Svensson, Johan Dahlin, Thomas B. Schön category:stat.ML stat.CO published:2015-02-06 summary:Gaussian process regression is a popular method for non-parametricprobabilistic modeling of functions. The Gaussian process prior ischaracterized by so-called hyperparameters, which often have a large influenceon the posterior model and can be difficult to tune. This work provides amethod for numerical marginalization of the hyperparameters, relying on therigorous framework of sequential Monte Carlo. Our method is well suited foronline problems, and we demonstrate its ability to handle real-world problemswith several dimensions and compare it to other marginalization methods. Wealso conclude that our proposed method is a competitive alternative to thecommonly used point estimates maximizing the likelihood, both in terms ofcomputational load and its ability to handle multimodal posteriors.
arxiv-1502-01880 | A Fingerprint-based Access Control using Principal Component Analysis and Edge Detection |  http://arxiv.org/abs/1502.01880  | author:E. F. Melo, H. M. de Oliveira category:cs.CV cs.CR stat.AP published:2015-02-06 summary:This paper presents a novel approach for deciding on the appropriateness ornot of an acquired fingerprint image into a given database. The process beginswith the assembly of a training base in an image space constructed by combiningPrincipal Component Analysis (PCA) and edge detection. Then, the parameter H, anew feature that helps in the decision making about the relevance of afingerprint image in databases, is derived from a relationship betweenEuclidean and Mahalanobian distances. This procedure ends with the lifting ofthe curve of the Receiver Operating Characteristic (ROC), where the thresholdsdefined on the parameter H are chosen according to the acceptable rates offalse positives and false negatives.
arxiv-1502-01943 | Active Function Cross-Entropy Clustering |  http://arxiv.org/abs/1502.01943  | author:P. Spurek, J. Tabor, P. Markowicz category:stat.ML published:2015-02-06 summary:Gaussian Mixture Models (GMM) have found many applications in densityestimation and data clustering. However, the model does not adapt well tocurved and strongly nonlinear data. Recently there appeared an improvementcalled AcaGMM (Active curve axis Gaussian Mixture Model), which fits Gaussiansalong curves using an EM-like (Expectation Maximization) approach. Using the ideas standing behind AcaGMM, we build an alternative activefunction model of clustering, which has some advantages over AcaGMM. Inparticular it is naturally defined in arbitrary dimensions and enables an easyadaptation to clustering of complicated datasets along the predefined family offunctions. Moreover, it does not need external methods to determine the numberof clusters as it automatically reduces the number of groups on-line.
arxiv-1502-02072 | Massively Multitask Networks for Drug Discovery |  http://arxiv.org/abs/1502.02072  | author:Bharath Ramsundar, Steven Kearnes, Patrick Riley, Dale Webster, David Konerding, Vijay Pande category:stat.ML cs.LG cs.NE published:2015-02-06 summary:Massively multitask neural architectures provide a learning framework fordrug discovery that synthesizes information from many distinct biologicalsources. To train these architectures at scale, we gather large amounts of datafrom public sources to create a dataset of nearly 40 million measurementsacross more than 200 biological targets. We investigate several aspects of themultitask framework by performing a series of empirical studies and obtain someinteresting results: (1) massively multitask networks obtain predictiveaccuracies significantly better than single-task methods, (2) the predictivepower of multitask networks improves as additional tasks and data are added,(3) the total amount of data and the total number of tasks both contributesignificantly to multitask improvement, and (4) multitask networks affordlimited transferability to tasks not in the training set. Our resultsunderscore the need for greater data sharing and further algorithmic innovationto accelerate the drug discovery process.
arxiv-1502-02077 | Quantum Energy Regression using Scattering Transforms |  http://arxiv.org/abs/1502.02077  | author:Matthew Hirn, Nicolas Poilvert, Stéphane Mallat category:cs.LG cs.CV quant-ph published:2015-02-06 summary:We present a novel approach to the regression of quantum mechanical energiesbased on a scattering transform of an intermediate electron densityrepresentation. A scattering transform is a deep convolution network computedwith a cascade of multiscale wavelet transforms. It possesses appropriateinvariant and stability properties for quantum energy regression. This newframework removes fundamental limitations of Coulomb matrix based energyregressions, and numerical experiments give state-of-the-art accuracy overplanar molecules.
arxiv-1502-01852 | Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification |  http://arxiv.org/abs/1502.01852  | author:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun category:cs.CV cs.AI cs.LG published:2015-02-06 summary:Rectified activation units (rectifiers) are essential for state-of-the-artneural networks. In this work, we study rectifier neural networks for imageclassification from two aspects. First, we propose a Parametric RectifiedLinear Unit (PReLU) that generalizes the traditional rectified unit. PReLUimproves model fitting with nearly zero extra computational cost and littleoverfitting risk. Second, we derive a robust initialization method thatparticularly considers the rectifier nonlinearities. This method enables us totrain extremely deep rectified models directly from scratch and to investigatedeeper or wider network architectures. Based on our PReLU networks(PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012classification dataset. This is a 26% relative improvement over the ILSVRC 2014winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpasshuman-level performance (5.1%, Russakovsky et al.) on this visual recognitionchallenge.
arxiv-1502-01956 | Stochastic recursive inclusion in two timescales with an application to the Lagrangian dual problem |  http://arxiv.org/abs/1502.01956  | author:Arunselvan Ramaswamy, Shalabh Bhatnagar category:cs.SY math.DS stat.ML published:2015-02-06 summary:In this paper we present a framework to analyze the asymptotic behavior oftwo timescale stochastic approximation algorithms including those withset-valued mean fields. This paper builds on the works of Borkar and Perkins &Leslie. The framework presented herein is more general as compared to thesynchronous two timescale framework of Perkins \& Leslie, however theassumptions involved are easily verifiable. As an application, we use thisframework to analyze the two timescale stochastic approximation algorithmcorresponding to the Lagrangian dual problem in optimization theory.
arxiv-1502-01827 | Hierarchical Maximum-Margin Clustering |  http://arxiv.org/abs/1502.01827  | author:Guang-Tong Zhou, Sung Ju Hwang, Mark Schmidt, Leonid Sigal, Greg Mori category:cs.LG cs.CV published:2015-02-06 summary:We present a hierarchical maximum-margin clustering method for unsuperviseddata analysis. Our method extends beyond flat maximum-margin clustering, andperforms clustering recursively in a top-down manner. We propose an effectivegreedy splitting criteria for selecting which cluster to split next, and employregularizers that enforce feature sharing/competition for capturing datasemantics. Experimental results obtained on four standard datasets show thatour method outperforms flat and hierarchical clustering baselines, whileforming clean and semantically meaningful cluster hierarchies.
arxiv-1502-01823 | Unsupervised Fusion Weight Learning in Multiple Classifier Systems |  http://arxiv.org/abs/1502.01823  | author:Anurag Kumar, Bhiksha Raj category:cs.LG cs.CV published:2015-02-06 summary:In this paper we present an unsupervised method to learn the weights withwhich the scores of multiple classifiers must be combined in classifier fusionsettings. We also introduce a novel metric for ranking instances based on anindex which depends upon the rank of weighted scores of test points among theweighted scores of training points. We show that the optimized index can beused for computing measures such as average precision. Unlike most classifierfusion methods where a single weight is learned to weigh all examples ourmethod learns instance-specific weights. The problem is formulated as learningthe weight which maximizes a clarity index; subsequently the index itself andthe learned weights both are used separately to rank all the test points. Ourmethod gives an unsupervised method of optimizing performance on actual testdata, unlike the well known stacking-based methods where optimization is doneover a labeled training set. Moreover, we show that our method is tolerant tonoisy classifiers and can be used for selecting N-best classifiers.
arxiv-1502-01783 | Learning Efficient Anomaly Detectors from $K$-NN Graphs |  http://arxiv.org/abs/1502.01783  | author:Jing Qian, Jonathan Root, Venkatesh Saligrama category:cs.LG stat.ML published:2015-02-06 summary:We propose a non-parametric anomaly detection algorithm for high dimensionaldata. We score each datapoint by its average $K$-NN distance, and rank themaccordingly. We then train limited complexity models to imitate these scoresbased on the max-margin learning-to-rank framework. A test-point is declared asan anomaly at $\alpha$-false alarm level if the predicted score is in the$\alpha$-percentile. The resulting anomaly detector is shown to beasymptotically optimal in that for any false alarm rate $\alpha$, its decisionregion converges to the $\alpha$-percentile minimum volume level set of theunknown underlying density. In addition, we test both the statisticalperformance and computational efficiency of our algorithm on a number ofsynthetic and real-data experiments. Our results demonstrate the superiority ofour algorithm over existing $K$-NN based anomaly detection algorithms, withsignificant computational savings.
arxiv-1502-01853 | Generalized Inpainting Method for Hyperspectral Image Acquisition |  http://arxiv.org/abs/1502.01853  | author:K. Degraux, V. Cambareri, L. Jacques, B. Geelen, C. Blanch, G. Lafruit category:cs.CV published:2015-02-06 summary:A recently designed hyperspectral imaging device enables multiplexedacquisition of an entire data volume in a single snapshot thanks tomonolithically-integrated spectral filters. Such an agile imaging techniquecomes at the cost of a reduced spatial resolution and the need for ademosaicing procedure on its interleaved data. In this work, we address bothissues and propose an approach inspired by recent developments in compressedsensing and analysis sparse models. We formulate our superresolution anddemosaicing task as a 3-D generalized inpainting problem. Interestingly, thetarget spatial resolution can be adjusted for mitigating the compression levelof our sensing. The reconstruction procedure uses a fast greedy method calledPseudo-inverse IHT. We also show on simulations that a random arrangement ofthe spectral filters on the sensor is preferable to regular mosaic layout as itimproves the quality of the reconstruction. The efficiency of our technique isdemonstrated through numerical experiments on both synthetic and real data asacquired by the snapshot imager.
arxiv-1502-01782 | Multi-Action Recognition via Stochastic Modelling of Optical Flow and Gradients |  http://arxiv.org/abs/1502.01782  | author:Johanna Carvajal, Conrad Sanderson, Chris McCool, Brian C. Lovell category:cs.CV published:2015-02-06 summary:In this paper we propose a novel approach to multi-action recognition thatperforms joint segmentation and classification. This approach models eachaction using a Gaussian mixture using robust low-dimensional action features.Segmentation is achieved by performing classification on overlapping temporalwindows, which are then merged to produce the final result. This approach isconsiderably less complicated than previous methods which use dynamicprogramming or computationally expensive hidden Markov models (HMMs). Initialexperiments on a stitched version of the KTH dataset show that the proposedapproach achieves an accuracy of 78.3%, outperforming a recent HMM-basedapproach which obtained 71.2%.
arxiv-1502-01812 | Crowded Scene Analysis: A Survey |  http://arxiv.org/abs/1502.01812  | author:Teng Li, Huan Chang, Meng Wang, Bingbing Ni, Richang Hong, Shuicheng Yan category:cs.CV 68-02 published:2015-02-06 summary:Automated scene analysis has been a topic of great interest in computervision and cognitive science. Recently, with the growth of crowd phenomena inthe real world, crowded scene analysis has attracted much attention. However,the visual occlusions and ambiguities in crowded scenes, as well as the complexbehaviors and scene semantics, make the analysis a challenging task. In thepast few years, an increasing number of works on crowded scene analysis havebeen reported, covering different aspects including crowd motion patternlearning, crowd behavior and activity analysis, and anomaly detection incrowds. This paper surveys the state-of-the-art techniques on this topic. Wefirst provide the background knowledge and the available features related tocrowded scenes. Then, existing models, popular algorithms, evaluationprotocols, as well as system performance are provided corresponding todifferent aspects of crowded scene analysis. We also outline the availabledatasets for performance evaluation. Finally, some research problems andpromising future directions are presented with discussions.
arxiv-1502-02063 | Visual Recognition by Counting Instances: A Multi-Instance Cardinality Potential Kernel |  http://arxiv.org/abs/1502.02063  | author:Hossein Hajimirsadeghi, Wang Yan, Arash Vahdat, Greg Mori category:cs.CV published:2015-02-06 summary:Many visual recognition problems can be approached by counting instances. Todetermine whether an event is present in a long internet video, one could counthow many frames seem to contain the activity. Classifying the activity of agroup of people can be done by counting the actions of individual people.Encoding these cardinality relationships can reduce sensitivity to clutter, inthe form of irrelevant frames or individuals not involved in a group activity.Learned parameters can encode how many instances tend to occur in a class ofinterest. To this end, this paper develops a powerful and flexible framework toinfer any cardinality relation between latent labels in a multi-instance model.Hard or soft cardinality relations can be encoded to tackle diverse levels ofambiguity. Experiments on tasks such as human activity recognition, video eventdetection, and video summarization demonstrate the effectiveness of usingcardinality relations for improving recognition results.
arxiv-1502-01684 | On Anomaly Ranking and Excess-Mass Curves |  http://arxiv.org/abs/1502.01684  | author:Nicolas Goix, Anne Sabourin, Stéphan Clémençon category:stat.ML math.PR published:2015-02-05 summary:Learning how to rank multivariate unlabeled observations depending on theirdegree of abnormality/novelty is a crucial problem in a wide range ofapplications. In practice, it generally consists in building a real valued"scoring" function on the feature space so as to quantify to which extentobservations should be considered as abnormal. In the 1-d situation,measurements are generally considered as "abnormal" when they are remote fromcentral measures such as the mean or the median. Anomaly detection then relieson tail analysis of the variable of interest. Extensions to the multivariatesetting are far from straightforward and it is precisely the main purpose ofthis paper to introduce a novel and convenient (functional) criterion formeasuring the performance of a scoring function regarding the anomaly rankingtask, referred to as the Excess-Mass curve (EM curve). In addition, an adaptivealgorithm for building a scoring function based on unlabeled data X1 , . . . ,Xn with a nearly optimal EM is proposed and is analyzed from a statisticalperspective.
arxiv-1502-01682 | Use of Modality and Negation in Semantically-Informed Syntactic MT |  http://arxiv.org/abs/1502.01682  | author:Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr, Chris Callison-Burch, Nathaniel W. Filardo, Christine Piatko, Lori Levin, Scott Miller category:cs.CL cs.LG stat.ML published:2015-02-05 summary:This paper describes the resource- and system-building efforts of aneight-week Johns Hopkins University Human Language Technology Center ofExcellence Summer Camp for Applied Language Exploration (SCALE-2009) onSemantically-Informed Machine Translation (SIMT). We describe a newmodality/negation (MN) annotation scheme, the creation of a (publiclyavailable) MN lexicon, and two automated MN taggers that we built using theannotation scheme and lexicon. Our annotation scheme isolates three componentsof modality and negation: a trigger (a word that conveys modality or negation),a target (an action associated with modality or negation) and a holder (anexperiencer of modality). We describe how our MN lexicon was semi-automaticallyproduced and we demonstrate that a structure-based MN tagger results inprecision around 86% (depending on genre) for tagging of a standard LDC dataset. We apply our MN annotation scheme to statistical machine translation using asyntactic framework that supports the inclusion of semantic annotations.Syntactic tags enriched with semantic annotations are assigned to parse treesin the target-language training texts through a process of tree grafting. Whilethe focus of our work is modality and negation, the tree grafting procedure isgeneral and supports other types of semantic information. We exploit thiscapability by including named entities, produced by a pre-existing tagger, inaddition to the MN elements produced by the taggers described in this paper.The resulting system significantly outperformed a linguistically naive baselinemodel (Hiero), and reached the highest scores yet reported on the NIST 2009Urdu-English test set. This finding supports the hypothesis that both syntacticand semantic information can improve translation quality.
arxiv-1502-01425 | Provable Sparse Tensor Decomposition |  http://arxiv.org/abs/1502.01425  | author:Will Wei Sun, Junwei Lu, Han Liu, Guang Cheng category:stat.ML published:2015-02-05 summary:We propose a novel sparse tensor decomposition method, namely TensorTruncated Power (TTP) method, that incorporates variable selection into theestimation of decomposition components. The sparsity is achieved via anefficient truncation step embedded in the tensor power iteration. Our methodapplies to a broad family of high dimensional latent variable models, includinghigh dimensional Gaussian mixture and mixtures of sparse regressions. Athorough theoretical investigation is further conducted. In particular, we showthat the final decomposition estimator is guaranteed to achieve a localstatistical rate, and further strengthen it to the global statistical rate byintroducing a proper initialization procedure. In high dimensional regimes, theobtained statistical rate significantly improves those shown in the existingnon-sparse decomposition methods. The empirical advantages of TTP are confirmedin extensive simulated results and two real applications of click-through rateprediction and high-dimensional gene clustering.
arxiv-1502-01664 | Estimating Optimal Active Learning via Model Retraining Improvement |  http://arxiv.org/abs/1502.01664  | author:Lewis P. G. Evans, Niall M. Adams, Christoforos Anagnostopoulos category:stat.ML cs.LG published:2015-02-05 summary:A central question for active learning (AL) is: "what is the optimalselection?" Defining optimality by classifier loss produces a newcharacterisation of optimal AL behaviour, by treating expected loss reductionas a statistical target for estimation. This target forms the basis of modelretraining improvement (MRI), a novel approach providing a statisticalestimation framework for AL. This framework is constructed to address thecentral question of AL optimality, and to motivate the design of estimationalgorithms. MRI allows the exploration of optimal AL behaviour, and theexamination of AL heuristics, showing precisely how they make sub-optimalselections. The abstract formulation of MRI is used to provide a new guaranteefor AL, that an unbiased MRI estimator should outperform random selection. ThisMRI framework reveals intricate estimation issues that in turn motivate theconstruction of new statistical AL algorithms. One new algorithm in particularperforms strongly in a large-scale experimental study, compared to standard ALmethods. This competitive performance suggests that practical efforts tominimise estimation bias may be important for AL applications.
arxiv-1502-01659 | Learning Articulated Motions From Visual Demonstration |  http://arxiv.org/abs/1502.01659  | author:Sudeep Pillai, Matthew R. Walter, Seth Teller category:cs.RO cs.CV published:2015-02-05 summary:Many functional elements of human homes and workplaces consist of rigidcomponents which are connected through one or more sliding or rotatinglinkages. Examples include doors and drawers of cabinets and appliances;laptops; and swivel office chairs. A robotic mobile manipulator would benefitfrom the ability to acquire kinematic models of such objects from observation.This paper describes a method by which a robot can acquire an object model bycapturing depth imagery of the object as a human moves it through its range ofmotion. We envision that in future, a machine newly introduced to anenvironment could be shown by its human user the articulated objects particularto that environment, inferring from these "visual demonstrations" enoughinformation to actuate each object independently of the user. Our method employs sparse (markerless) feature tracking, motion segmentation,component pose estimation, and articulation learning; it does not require priorobject models. Using the method, a robot can observe an object being exercised,infer a kinematic model incorporating rigid, prismatic and revolute joints,then use the model to predict the object's motion from a novel vantage point.We evaluate the method's performance, and compare it to that of a previouslypublished technique, for a variety of household objects.
arxiv-1502-01418 | RELEAF: An Algorithm for Learning and Exploiting Relevance |  http://arxiv.org/abs/1502.01418  | author:Cem Tekin, Mihaela van der Schaar category:cs.LG stat.ML published:2015-02-05 summary:Recommender systems, medical diagnosis, network security, etc., requireon-going learning and decision-making in real time. These -- and many others --represent perfect examples of the opportunities and difficulties presented byBig Data: the available information often arrives from a variety of sources andhas diverse features so that learning from all the sources may be valuable butintegrating what is learned is subject to the curse of dimensionality. Thispaper develops and analyzes algorithms that allow efficient learning anddecision-making while avoiding the curse of dimensionality. We formalize theinformation available to the learner/decision-maker at a particular time as acontext vector which the learner should consider when taking actions. Ingeneral the context vector is very high dimensional, but in many settings, themost relevant information is embedded into only a few relevant dimensions. Ifthese relevant dimensions were known in advance, the problem would be simple --but they are not. Moreover, the relevant dimensions may be different fordifferent actions. Our algorithm learns the relevant dimensions for eachaction, and makes decisions based in what it has learned. Formally, we build onthe structure of a contextual multi-armed bandit by adding and exploiting arelevance relation. We prove a general regret bound for our algorithm whosetime order depends only on the maximum number of relevant dimensions among allthe actions, which in the special case where the relevance relation issingle-valued (a function), reduces to $\tilde{O}(T^{2(\sqrt{2}-1)})$; in theabsence of a relevance relation, the best known contextual bandit algorithmsachieve regret $\tilde{O}(T^{(D+1)/(D+2)})$, where $D$ is the full dimension ofthe context vector.
arxiv-1502-01687 | Graph Partitioning for Independent Sets |  http://arxiv.org/abs/1502.01687  | author:Sebastian Lamm, Peter Sanders, Christian Schulz category:cs.DS cs.NE published:2015-02-05 summary:Computing maximum independent sets in graphs is an important problem incomputer science. In this paper, we develop an evolutionary algorithm to tacklethe problem. The core innovations of the algorithm are very natural combineoperations based on graph partitioning and local search algorithms. Moreprecisely, we employ a state-of-the-art graph partitioner to derive operationsthat enable us to quickly exchange whole blocks of given independent sets. Toenhance newly computed offsprings we combine our operators with a local searchalgorithm. Our experimental evaluation indicates that we are able to outperformstate-of-the-art algorithms on a variety of instances.
arxiv-1502-01446 | Beyond Word-based Language Model in Statistical Machine Translation |  http://arxiv.org/abs/1502.01446  | author:Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, Chengqing Zong category:cs.CL published:2015-02-05 summary:Language model is one of the most important modules in statistical machinetranslation and currently the word-based language model dominants thiscommunity. However, many translation models (e.g. phrase-based models) generatethe target language sentences by rendering and compositing the phrases ratherthan the words. Thus, it is much more reasonable to model dependency betweenphrases, but few research work succeed in solving this problem. In this paper,we tackle this problem by designing a novel phrase-based language model whichattempts to solve three key sub-problems: 1, how to define a phrase in languagemodel; 2, how to determine the phrase boundary in the large-scale monolingualdata in order to enlarge the training set; 3, how to alleviate the datasparsity problem due to the huge vocabulary size of phrases. By carefullyhandling these issues, the extensive experiments on Chinese-to-Englishtranslation show that our phrase-based language model can significantly improvethe translation quality by up to +1.47 absolute BLEU score.
