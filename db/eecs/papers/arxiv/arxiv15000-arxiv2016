arxiv-15000-1 | Towards the Application of Linear Programming Methods For Multi-Camera Pose Estimation | http://arxiv.org/pdf/1512.02357v1.pdf | author:Masoud Aghamohamadian-Sharbaf, Ahmadreza Heravi, Hamidreza Pourreza category:cs.CV published:2015-12-08 summary:We presented a separation based optimization algorithm which, rather thanoptimization the entire variables altogether, This would allow us to employ: 1)a class of nonlinear functions with three variables and 2) a convex quadraticmultivariable polynomial, for minimization of reprojection error. Neglectingthe inversion required to minimize the nonlinear functions, in this paper wedemonstrate how separation allows eradication of matrix inversion.
arxiv-15000-2 | Speeding Up Distributed Machine Learning Using Codes | http://arxiv.org/pdf/1512.02673v2.pdf | author:Kangwook Lee, Maximilian Lam, Ramtin Pedarsani, Dimitris Papailiopoulos, Kannan Ramchandran category:cs.DC cs.IT cs.LG cs.PF math.IT published:2015-12-08 summary:Codes are widely used in many engineering applications to offer some form ofreliability and fault tolerance. The high-level idea of coding is to exploitresource redundancy to deliver higher robustness against system noise. Inlarge-scale systems there are several types of "noise" that can affect theperformance of distributed machine learning algorithms: straggler nodes, systemfailures, or communication bottlenecks. Moreover, redundancy is abundant: aplethora of nodes, a lot of spare storage, etc. In this work, scratching the surface of "codes for distributed computation,"we provide theoretical insights on how coded solutions can achieve significantgains compared to uncoded ones. We focus on two of the most basic buildingblocks of distributed learning algorithms: matrix multiplication and datashuffling. For matrix multiplication, we use codes to leverage the plethora ofnodes and alleviate the effects of stragglers. We show that if the number ofworkers is $n$, and the runtime of each subtask has an exponential tail, theoptimal coded matrix multiplication is $\Theta(\log n)$ times faster than theuncoded matrix multiplication. In data shuffling, we use codes to exploit theexcess in storage and reduce communication bottlenecks. We show that when$\alpha$ is the fraction of the data matrix that can be cached at each worker,and $n$ is the number of workers, coded shuffling reduces the communicationcost by a factor $\Theta(\alpha \gamma(n))$ compared to uncoded shuffling,where $\gamma(n)$ is the ratio of the cost of unicasting $n$ messages to $n$users to broadcasting a common message (of the same size) to $n$ users. Oursynthetic and Open MPI experiments on Amazon EC2 show that coded distributedalgorithms can achieve significant speedups of up to 40% compared to uncodeddistributed algorithms.
arxiv-15000-3 | Fine-grained Image Classification by Exploring Bipartite-Graph Labels | http://arxiv.org/pdf/1512.02665v2.pdf | author:Feng Zhou, Yuanqing Lin category:cs.CV published:2015-12-08 summary:Given a food image, can a fine-grained object recognition engine tell "whichrestaurant which dish" the food belongs to? Such ultra-fine grained imagerecognition is the key for many applications like search by images, but it isvery challenging because it needs to discern subtle difference between classeswhile dealing with the scarcity of training data. Fortunately, the ultra-finegranularity naturally brings rich relationships among object classes. Thispaper proposes a novel approach to exploit the rich relationships throughbipartite-graph labels (BGL). We show how to model BGL in an overallconvolutional neural networks and the resulting system can be optimized throughback-propagation. We also show that it is computationally efficient ininference thanks to the bipartite structure. To facilitate the study, weconstruct a new food benchmark dataset, which consists of 37,885 food imagescollected from 6 restaurants and totally 975 menus. Experimental results onthis new food and three other datasets demonstrates BGL advances previous worksin fine-grained object recognition. An online demo is available athttp://www.f-zhou.com/fg_demo/.
arxiv-15000-4 | Online Crowdsourcing | http://arxiv.org/pdf/1512.02393v1.pdf | author:Changbo Zhu, Huan Xu, Shuicheng Yan category:cs.LG published:2015-12-08 summary:With the success of modern internet based platform, such as Amazon MechanicalTurk, it is now normal to collect a large number of hand labeled samples fromnon-experts. The Dawid- Skene algorithm, which is based on Expectation-Maximization update, has been widely used for inferring the true labels fromnoisy crowdsourced labels. However, Dawid-Skene scheme requires all the data toperform each EM iteration, and can be infeasible for streaming data or largescale data. In this paper, we provide an online version of Dawid- Skenealgorithm that only requires one data frame for each iteration. Further, weprove that under mild conditions, the online Dawid-Skene scheme with projectionconverges to a stationary point of the marginal log-likelihood of the observeddata. Our experiments demonstrate that the online Dawid- Skene scheme achievesstate of the art performance comparing with other methods based on the Dawid-Skene scheme.
arxiv-15000-5 | Fast spectral algorithms from sum-of-squares proofs: tensor decomposition and planted sparse vectors | http://arxiv.org/pdf/1512.02337v2.pdf | author:Samuel B. Hopkins, Tselil Schramm, Jonathan Shi, David Steurer category:cs.DS cs.CC cs.LG stat.ML published:2015-12-08 summary:We consider two problems that arise in machine learning applications: theproblem of recovering a planted sparse vector in a random linear subspace andthe problem of decomposing a random low-rank overcomplete 3-tensor. For bothproblems, the best known guarantees are based on the sum-of-squares method. Wedevelop new algorithms inspired by analyses of the sum-of-squares method. Ouralgorithms achieve the same or similar guarantees as sum-of-squares for theseproblems but the running time is significantly faster. For the planted sparse vector problem, we give an algorithm with running timenearly linear in the input size that approximately recovers a planted sparsevector with up to constant relative sparsity in a random subspace of $\mathbbR^n$ of dimension up to $\tilde \Omega(\sqrt n)$. These recovery guaranteesmatch the best known ones of Barak, Kelner, and Steurer (STOC 2014) up tologarithmic factors. For tensor decomposition, we give an algorithm with running time close tolinear in the input size (with exponent $\approx 1.086$) that approximatelyrecovers a component of a random 3-tensor over $\mathbb R^n$ of rank up to$\tilde \Omega(n^{4/3})$. The best previous algorithm for this problem due toGe and Ma (RANDOM 2015) works up to rank $\tilde \Omega(n^{3/2})$ but requiresquasipolynomial time.
arxiv-15000-6 | Gibbs-type Indian buffet processes | http://arxiv.org/pdf/1512.02543v1.pdf | author:Creighton Heaukulani, Daniel M. Roy category:stat.ML published:2015-12-08 summary:We investigate a class of feature allocation models that generalize theIndian buffet process and are parameterized by Gibbs-type random measures. Twoexisting classes are contained as special cases: the original two-parameterIndian buffet process, corresponding to the Dirichlet process, and the stable(or three-parameter) Indian buffet process, corresponding to the Pitman-Yorprocess. Asymptotic behavior of the Gibbs-type partitions, such as power lawsholding for the number of latent clusters, translates into analogouscharacteristics for this class of Gibbs-type feature allocation models. Despitecontaining several different distinct subclasses, the properties of Gibbs-typepartitions allow us to develop a black-box procedure for posterior inferencewithin any subclass of models. Through numerical experiments, we compare andcontrast a few of these subclasses and highlight the utility of varyingpower-law behaviors in the latent features.
arxiv-15000-7 | SSD: Single Shot MultiBox Detector | http://arxiv.org/pdf/1512.02325v2.pdf | author:Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg category:cs.CV published:2015-12-08 summary:We present a method for detecting objects in images using a single deepneural network. Our approach, named SSD, discretizes the output space ofbounding boxes into a set of default boxes over different aspect ratios andscales per feature map location. At prediction time, the network generatesscores for the presence of each object category in each default box andproduces adjustments to the box to better match the object shape. Additionally,the network combines predictions from multiple feature maps with differentresolutions to naturally handle objects of various sizes. Our SSD model issimple relative to methods that require object proposals because it completelyeliminates proposal generation and subsequent pixel or feature resampling stageand encapsulates all computation in a single network. This makes SSD easy totrain and straightforward to integrate into systems that require a detectioncomponent. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasetsconfirm that SSD has comparable accuracy to methods that utilize an additionalobject proposal step and is much faster, while providing a unified frameworkfor both training and inference. Compared to other single stage methods, SSDhas much better accuracy, even with a smaller input image size. For $300\times300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia TitanX and for $500\times 500$ input, SSD achieves 75.1% mAP, outperforming acomparable state of the art Faster R-CNN model. Code is available at\url{https://github.com/weiliu89/caffe/tree/ssd} .
arxiv-15000-8 | Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views | http://arxiv.org/pdf/1512.02497v2.pdf | author:Francisco Massa, Bryan Russell, Mathieu Aubry category:cs.CV cs.LG cs.NE published:2015-12-08 summary:This paper presents an end-to-end convolutional neural network (CNN) for2D-3D exemplar detection. We demonstrate that the ability to adapt the featuresof natural images to better align with those of CAD rendered views is criticalto the success of our technique. We show that the adaptation can be learned bycompositing rendered views of textured object models on natural images. Ourapproach can be naturally incorporated into a CNN detection pipeline andextends the accuracy and speed benefits from recent advances in deep learningto 2D-3D exemplar detection. We applied our method to two tasks: instancedetection, where we evaluated on the IKEA dataset, and object categorydetection, where we out-perform Aubry et al. for "chair" detection on a subsetof the Pascal VOC dataset.
arxiv-15000-9 | An Explicit Rate Bound for the Over-Relaxed ADMM | http://arxiv.org/pdf/1512.02063v2.pdf | author:Guilherme França, José Bento category:stat.ML math.OC published:2015-12-07 summary:The framework of Integral Quadratic Constraints of Lessard et al. (2014)reduces the computation of upper bounds on the convergence rate of severaloptimization algorithms to semi-definite programming (SDP). Followup work byNishihara et al. (2015) applies this technique to the entire family ofover-relaxed Alternating Direction Method of Multipliers (ADMM). Unfortunately,they only provide an explicit error bound for sufficiently large values of someof the parameters of the problem, leaving the computation for the general caseas a numerical optimization problem. In this paper we provide an exactanalytical solution to this SDP and obtain a general and explicit upper boundon the convergence rate of the entire family of over-relaxed ADMM. Furthermore,we demonstrate that it is not possible to extract from this SDP a general boundbetter than ours. We end with a few numerical illustrations of our result and acomparison between the convergence rate we obtain for the ADMM with knownconvergence rates for the Gradient Descent.
arxiv-15000-10 | Level-Based Analysis of Genetic Algorithms for Combinatorial Optimization | http://arxiv.org/pdf/1512.02047v2.pdf | author:Duc-Cuong Dang, Anton V. Eremeev, Per Kristian Lehre category:cs.NE published:2015-12-07 summary:The paper is devoted to upper bounds on run-time of Non-Elitist GeneticAlgorithms until some target subset of solutions is visited for the first time.In particular, we consider the sets of optimal solutions and the sets of localoptima as the target subsets. Previously known upper bounds are improved bymeans of drift analysis. Finally, we propose conditions ensuring that aNon-Elitist Genetic Algorithm efficiently finds approximate solutions withconstant approximation ratio on the class of combinatorial optimizationproblems with guaranteed local optima (GLO).
arxiv-15000-11 | Risk Minimization in Structured Prediction using Orbit Loss | http://arxiv.org/pdf/1512.02033v2.pdf | author:Danny Karmon, Joseph Keshet category:cs.LG published:2015-12-07 summary:We introduce a new surrogate loss function called orbit loss in thestructured prediction framework, which has good theoretical and practicaladvantages. While the orbit loss is not convex, it has a simple analyticalgradient and a simple perceptron-like learning rule. We analyze the new losstheoretically and state a PAC-Bayesian generalization bound. We also prove thatthe new loss is consistent in the strong sense; namely, the risk achieved bythe set of the trained parameters approaches the infimum risk achievable by anylinear decoder over the given features. Methods that are aimed at riskminimization, such as the structured ramp loss, the structured probit loss andthe direct loss minimization require at least two inference operations pertraining iteration. In this sense, the orbit loss is more efficient as itrequires only one inference operation per training iteration, while yieldssimilar performance. We conclude the paper with an empirical comparison of theproposed loss function to the structured hinge loss, the structured ramp loss,the structured probit loss and the direct loss minimization method on severalbenchmark datasets and tasks.
arxiv-15000-12 | THCHS-30 : A Free Chinese Speech Corpus | http://arxiv.org/pdf/1512.01882v2.pdf | author:Dong Wang, Xuewei Zhang category:cs.CL cs.SD published:2015-12-07 summary:Speech data is crucially important for speech recognition research. There arequite some speech databases that can be purchased at prices that are reasonablefor most research institutes. However, for young people who just start researchactivities or those who just gain initial interest in this direction, the costfor data is still an annoying barrier. We support the `free data' movement inspeech recognition: research institutes (particularly supported by publicfunds) publish their data freely so that new researchers can obtain sufficientdata to kick of their career. In this paper, we follow this trend and release afree Chinese speech database THCHS-30 that can be used to build a full- edgedChinese speech recognition system. We report the baseline system establishedwith this database, including the performance under highly noisy conditions.
arxiv-15000-13 | Optimal strategies for the control of autonomous vehicles in data assimilation | http://arxiv.org/pdf/1512.02271v1.pdf | author:Damon McDougall, Richard Moore category:math.OC stat.CO stat.ML published:2015-12-07 summary:We propose a method to compute optimal control paths for autonomous vehiclesdeployed for the purpose of inferring a velocity field. In addition to beingadvected by the flow, the vehicles are able to effect a fixed relative speedwith arbitrary control over direction. It is this direction that is used as thebasis for the locally optimal control algorithm presented here, with objectiveformed from the variance trace of the expected posterior distribution. Wepresent results for linear flows near hyperbolic fixed points.
arxiv-15000-14 | How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies | http://arxiv.org/pdf/1512.02011v2.pdf | author:Vincent François-Lavet, Raphael Fonteneau, Damien Ernst category:cs.LG cs.AI published:2015-12-07 summary:Using deep neural nets as function approximator for reinforcement learningtasks have recently been shown to be very powerful for solving problemsapproaching real-world complexity. Using these results as a benchmark, wediscuss the role that the discount factor may play in the quality of thelearning process of a deep Q-network (DQN). When the discount factorprogressively increases up to its final value, we empirically show that it ispossible to significantly reduce the number of learning steps. When used inconjunction with a varying learning rate, we empirically show that itoutperforms original DQN on several experiments. We relate this phenomenon withthe instabilities of neural networks when they are used in an approximateDynamic Programming setting. We also describe the possibility to fall within alocal optimum during the learning process, thus connecting our discussion withthe exploration/exploitation dilemma.
arxiv-15000-15 | Using SVM to pre-classify government purchases | http://arxiv.org/pdf/1601.02680v1.pdf | author:Thiago Marzagão category:cs.LG published:2015-12-07 summary:The Brazilian government often misclassifies the goods it buys. That makes ithard to audit government expenditures. We cannot know whether the price paidfor a ballpoint pen (code #7510) was reasonable if the pen was misclassified asa technical drawing pen (code #6675) or as any other good. This paper shows howwe can use machine learning to reduce misclassification. I trained a supportvector machine (SVM) classifier that takes a product description as input andreturns the most likely category codes as output. I trained the classifierusing 20 million goods purchased by the Brazilian government between 1999-04-01and 2015-04-02. In 83.3% of the cases the correct category code was one of thethree most likely category codes identified by the classifier. I used thetrained classifier to develop a web app that might help the government reducemisclassification. I open sourced the code on GitHub; anyone can use and modifyit.
arxiv-15000-16 | Sparsifying Neural Network Connections for Face Recognition | http://arxiv.org/pdf/1512.01891v1.pdf | author:Yi Sun, Xiaogang Wang, Xiaoou Tang category:cs.CV published:2015-12-07 summary:This paper proposes to learn high-performance deep ConvNets with sparseneural connections, referred to as sparse ConvNets, for face recognition. Thesparse ConvNets are learned in an iterative way, each time one additional layeris sparsified and the entire model is re-trained given the initial weightslearned in previous iterations. One important finding is that directly trainingthe sparse ConvNet from scratch failed to find good solutions for facerecognition, while using a previously learned denser model to properlyinitialize a sparser model is critical to continue learning effective featuresfor face recognition. This paper also proposes a new neural correlation-basedweight selection criterion and empirically verifies its effectiveness inselecting informative connections from previously learned models in eachiteration. When taking a moderately sparse structure (26%-76% of weights in thedense model), the proposed sparse ConvNet model significantly improves the facerecognition performance of the previous state-of-the-art DeepID2+ models giventhe same training data, while it keeps the performance of the baseline modelwith only 12% of the original parameters.
arxiv-15000-17 | Bounds on bilinear inverse forms via Gaussian quadrature with applications | http://arxiv.org/pdf/1512.01904v1.pdf | author:Chengtao Li, Suvrit Sra, Stefanie Jegelka category:stat.ML cs.NA published:2015-12-07 summary:We address quadrature-based approximations of the bilinear inverse form$u^\top A^{-1} u$, where $A$ is a real symmetric positive definite matrix, andanalyze properties of the Gauss, Gauss-Radau, and Gauss-Lobatto quadrature. Inparticular, we establish monotonicity of the bounds given by these quadraturerules, compare the tightness of these bounds, and derive associated convergencerates. To our knowledge, this is the first work to establish these propertiesof Gauss-type quadrature for computing bilinear inverse forms, thus filling atheoretical gap regarding this classical topic. We illustrate the empiricalbenefits of our theoretical results by applying quadrature to speed up twoMarkov Chain sampling procedures for (discrete) determinantal point processes.
arxiv-15000-18 | Recognition from Hand Cameras | http://arxiv.org/pdf/1512.01881v3.pdf | author:Cheng-Sheng Chan, Shou-Zhong Chen, Pei-Xuan Xie, Chiung-Chih Chang, Min Sun category:cs.CV published:2015-12-07 summary:We revisit the study of a wrist-mounted camera system (referred to asHandCam) for recognizing activities of hands. HandCam has two unique propertiesas compared to egocentric systems (referred to as HeadCam): (1) it avoids theneed to detect hands; (2) it more consistently observes the activities ofhands. By taking advantage of these properties, we propose adeep-learning-based method to recognize hand states (free v.s. active hands,hand gestures, object categories), and discover object categories. Moreover, wepropose a novel two-streams deep network to further take advantage of bothHandCam and HeadCam. We have collected a new synchronized HandCam and HeadCamdataset with 20 videos captured in three scenes for hand states recognition.Experiments show that our HandCam system consistently outperforms adeep-learning-based HeadCam method (with estimated manipulation regions) and adense-trajectory-based HeadCam method in all tasks. We also show that HandCamvideos captured by different users can be easily aligned to improve free v.s.active recognition accuracy (3.3% improvement) in across-scenes use case.Moreover, we observe that finetuning Convolutional Neural Network consistentlyimproves accuracy. Finally, our novel two-streams deep network combiningHandCam and HeadCam features achieves the best performance in four out of fivetasks. With more data, we believe a joint HandCam and HeadCam system canrobustly log hand states in daily life.
arxiv-15000-19 | Rademacher Complexity of the Restricted Boltzmann Machine | http://arxiv.org/pdf/1512.01914v1.pdf | author:Xiao Zhang category:cs.LG published:2015-12-07 summary:Boltzmann machine, as a fundamental construction block of deep belief networkand deep Boltzmann machines, is widely used in deep learning community andgreat success has been achieved. However, theoretical understanding of manyaspects of it is still far from clear. In this paper, we studied the Rademachercomplexity of both the asymptotic restricted Boltzmann machine and thepractical implementation with single-step contrastive divergence (CD-1)procedure. Our results disclose the fact that practical implementation trainingprocedure indeed increased the Rademacher complexity of restricted Boltzmannmachines. A further research direction might be the investigation of the VCdimension of a compositional function used in the CD-1 procedure.
arxiv-15000-20 | Thinking Required | http://arxiv.org/pdf/1512.01926v1.pdf | author:Kamil Rocki category:cs.LG cs.AI cs.CL published:2015-12-07 summary:There exists a theory of a single general-purpose learning algorithm whichcould explain the principles its operation. It assumes the initial rougharchitecture, a small library of simple innate circuits which are prewired atbirth. and proposes that all significant mental algorithms are learned. Givencurrent understanding and observations, this paper reviews and lists theingredients of such an algorithm from architectural and functionalperspectives.
arxiv-15000-21 | Fast Optimization Algorithm on Riemannian Manifolds and Its Application in Low-Rank Representation | http://arxiv.org/pdf/1512.01927v1.pdf | author:Haoran Chen, Yanfeng Sun, Junbin Gao, Yongli Hu category:cs.NA cs.CV cs.LG published:2015-12-07 summary:The paper addresses the problem of optimizing a class of composite functionson Riemannian manifolds and a new first order optimization algorithm (FOA) witha fast convergence rate is proposed. Through the theoretical analysis for FOA,it has been proved that the algorithm has quadratic convergence. Theexperiments in the matrix completion task show that FOA has better performancethan other first order optimization methods on Riemannian manifolds. A fastsubspace pursuit method based on FOA is proposed to solve the low-rankrepresentation model based on augmented Lagrange method on the low rank matrixvariety. Experimental results on synthetic and real data sets are presented todemonstrate that both FOA and SP-RPRG(ALM) can achieve superior performance interms of faster convergence and higher accuracy.
arxiv-15000-22 | New Design Criteria for Robust PCA and a Compliant Bayesian-Inspired Algorithm | http://arxiv.org/pdf/1512.02188v1.pdf | author:Tae-Hyun Oh, David Wipf, Yasuyuki Matsushita, In So Kweon category:cs.CV cs.LG stat.ML published:2015-12-07 summary:Commonly used in computer vision and other applications, robust PCArepresents an algorithmic attempt to reduce the sensitivity of classical PCA tooutliers. The basic idea is to learn a decomposition of some data matrix ofinterest into low rank and sparse components, the latter representing unwantedoutliers. Although the resulting optimization problem is typically NP-hard,convex relaxations provide a computationally-expedient alternative withtheoretical support. However, in practical regimes performance guarantees breakdown and a variety of non-convex alternatives, including Bayesian-inspiredmodels, have been proposed to boost estimation quality. Unfortunately though,without additional a priori knowledge none of these methods can significantlyexpand the critical operational range such that exact principal subspacerecovery is possible. Into this mix we propose a novel pseudo-Bayesianalgorithm that explicitly compensates for design weaknesses in many existingnon-convex approaches leading to state-of-the-art performance with a soundanalytical foundation.
arxiv-15000-23 | Simple Baseline for Visual Question Answering | http://arxiv.org/pdf/1512.02167v2.pdf | author:Bolei Zhou, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, Rob Fergus category:cs.CV cs.CL published:2015-12-07 summary:We describe a very simple bag-of-words baseline for visual questionanswering. This baseline concatenates the word features from the question andCNN features from the image to predict the answer. When evaluated on thechallenging VQA dataset [2], it shows comparable performance to many recentapproaches using recurrent neural networks. To explore the strength andweakness of the trained model, we also provide an interactive web demo andopen-source code. .
arxiv-15000-24 | Learning population and subject-specific brain connectivity networks via Mixed Neighborhood Selection | http://arxiv.org/pdf/1512.01947v1.pdf | author:Ricardo Pio Monti, Christoforos Anagnostopoulos, Giovanni Montana category:stat.ML published:2015-12-07 summary:In neuroimaging data analysis, Gaussian graphical models are often used tomodel statistical dependencies across spatially remote brain regions known asfunctional connectivity. Typically, data is collected across a cohort ofsubjects and the scientific objectives consist of estimating population andsubject-specific graphical models. A third objective that is often overlookedinvolves quantifying inter-subject variability and thus identifying regions orsub-networks that demonstrate heterogeneity across subjects. Such informationis fundamental in order to thoroughly understand the human connectome. Wepropose Mixed Neighborhood Selection in order to simultaneously address thethree aforementioned objectives. By recasting covariance selection as aneighborhood selection problem we are able to efficiently learn the topology ofeach node. We introduce an additional mixed effect component to neighborhoodselection in order to simultaneously estimate a graphical model for thepopulation of subjects as well as for each individual subject. The proposedmethod is validated empirically through a series of simulations and applied toresting state data for healthy subjects taken from the ABIDE consortium.
arxiv-15000-25 | Hyperspectral Chemical Plume Detection Algorithms Based On Multidimensional Iterative Filtering Decomposition | http://arxiv.org/pdf/1512.01979v1.pdf | author:Antonio Cicone, Jingfang Liu, Haomin Zhou category:math.NA cs.CV published:2015-12-07 summary:Chemicals released in the air can be extremely dangerous for human beings andthe environment. Hyperspectral images can be used to identify chemical plumes,however the task can be extremely challenging. Assuming we know a priori thatsome chemical plume, with a known frequency spectrum, has been photographedusing a hyperspectral sensor, we can use standard techniques like the so calledmatched filter or adaptive cosine estimator, plus a properly chosen thresholdvalue, to identify the position of the chemical plume. However, due to noiseand sensors fault, the accurate identification of chemical pixels is not easyeven in this apparently simple situation. In this paper we present apost-processing tool that, in a completely adaptive and data driven fashion,allows to improve the performance of any classification methods in identifyingthe boundaries of a plume. This is done using the Multidimensional IterativeFiltering (MIF) algorithm (arXiv:1411.6051, arXiv:1507.07173), which is anon-stationary signal decomposition method like the pioneering Empirical ModeDecomposition (EMD) method. Moreover, based on the MIF technique, we proposealso a pre-processing method that allows to decorrelate and mean-center ahyperspectral dataset. The Cosine Similarity measure, which often fails inpractice, appears to become a successful and outperforming classifier whenequipped with such pre-processing method. We show some examples of the proposedmethods when applied to real life problems.
arxiv-15000-26 | The Teaching Dimension of Linear Learners | http://arxiv.org/pdf/1512.02181v1.pdf | author:Ji Liu, Xiaojin Zhu category:cs.LG published:2015-12-07 summary:Teaching dimension is a learning theoretic quantity that specifies theminimum training set size to teach a target model to a learner. Previousstudies on teaching dimension focused on version-space learners which maintainall hypotheses consistent with the training data, and cannot be applied tomodern machine learners which select a specific hypothesis via optimization.This paper presents the first known teaching dimension for ridge regression,support vector machines, and logistic regression. We also exhibit optimaltraining sets that match these teaching dimensions. Our approach generalizes toother linear learners.
arxiv-15000-27 | Visualizing Deep Convolutional Neural Networks Using Natural Pre-Images | http://arxiv.org/pdf/1512.02017v3.pdf | author:Aravindh Mahendran, Andrea Vedaldi category:cs.CV 68T45 published:2015-12-07 summary:Image representations, from SIFT and bag of visual words to ConvolutionalNeural Networks (CNNs) are a crucial component of almost all computer visionsystems. However, our understanding of them remains limited. In this paper westudy several landmark representations, both shallow and deep, by a number ofcomplementary visualization techniques. These visualizations are based on theconcept of "natural pre-image", namely a natural-looking image whoserepresentation has some notable property. We study in particular three suchvisualizations: inversion, in which the aim is to reconstruct an image from itsrepresentation, activation maximization, in which we search for patterns thatmaximally stimulate a representation component, and caricaturization, in whichthe visual patterns that a representation detects in an image are exaggerated.We pose these as a regularized energy-minimization framework and demonstrateits generality and effectiveness. In particular, we show that this method caninvert representations such as HOG more accurately than recent alternativeswhile being applicable to CNNs too. Among our findings, we show that severallayers in CNNs retain photographically accurate information about the image,with different degrees of geometric and photometric invariance.
arxiv-15000-28 | A Novel Approach to Distributed Multi-Class SVM | http://arxiv.org/pdf/1512.01993v1.pdf | author:Aruna Govada, Shree Ranjani, Aditi Viswanathan, S. K. Sahay category:cs.LG cs.DC published:2015-12-07 summary:With data sizes constantly expanding, and with classical machine learningalgorithms that analyze such data requiring larger and larger amounts ofcomputation time and storage space, the need to distribute computation andmemory requirements among several computers has become apparent. Althoughsubstantial work has been done in developing distributed binary SVM algorithmsand multi-class SVM algorithms individually, the field of multi-classdistributed SVMs remains largely unexplored. This research proposes a novelalgorithm that implements the Support Vector Machine over a multi-class datasetand is efficient in a distributed environment (here, Hadoop). The idea is todivide the dataset into half recursively and thus compute the optimal SupportVector Machine for this half during the training phase, much like a divide andconquer approach. While testing, this structure has been effectively exploitedto significantly reduce the prediction time. Our algorithm has shown bettercomputation time during the prediction phase than the traditional sequentialSVM methods (One vs. One, One vs. Rest) and out-performs them as the size ofthe dataset grows. This approach also classifies the data with higher accuracythan the traditional multi-class algorithms.
arxiv-15000-29 | Jointly Modeling Topics and Intents with Global Order Structure | http://arxiv.org/pdf/1512.02009v1.pdf | author:Bei Chen, Jun Zhu, Nan Yang, Tian Tian, Ming Zhou, Bo Zhang category:cs.CL cs.IR cs.LG published:2015-12-07 summary:Modeling document structure is of great importance for discourse analysis andrelated applications. The goal of this research is to capture the documentintent structure by modeling documents as a mixture of topic words andrhetorical words. While the topics are relatively unchanged through onedocument, the rhetorical functions of sentences usually change followingcertain orders in discourse. We propose GMM-LDA, a topic modeling basedBayesian unsupervised model, to analyze the document intent structurecooperated with order information. Our model is flexible that has the abilityto combine the annotations and do supervised learning. Additionally, entropicregularization can be introduced to model the significant divergence betweentopics and intents. We perform experiments in both unsupervised and supervisedsettings, results show the superiority of our model over severalstate-of-the-art baselines.
arxiv-15000-30 | Scalable domain adaptation of convolutional neural networks | http://arxiv.org/pdf/1512.02013v1.pdf | author:Adrian Popescu, Etienne Gadeski, Hervé Le Borgne category:cs.CV published:2015-12-07 summary:Convolutional neural networks (CNNs) tend to become a standard approach tosolve a wide array of computer vision problems. Besides important theoreticaland practical advances in their design, their success is built on the existenceof manually labeled visual resources, such as ImageNet. The creation of suchdatasets is cumbersome and here we focus on alternatives to manual labeling. Wehypothesize that new resources are of uttermost importance in domains which arenot or weakly covered by ImageNet, such as tourism photographs. We firstcollect noisy Flickr images for tourist points of interest and apply automaticor weakly-supervised reranking techniques to reduce noise. Then, we learndomain adapted models with a standard CNN architecture and compare them to ageneric model obtained from ImageNet. Experimental validation is conducted withpublicly available datasets, including Oxford5k, INRIA Holidays and Div150Cred.Results show that low-cost domain adaptation improves results compared to theuse of generic models but also compared to strong non-CNN baselines such astriangulation embedding.
arxiv-15000-31 | Discriminative Nonparametric Latent Feature Relational Models with Data Augmentation | http://arxiv.org/pdf/1512.02016v1.pdf | author:Bei Chen, Ning Chen, Jun Zhu, Jiaming Song, Bo Zhang category:cs.LG stat.ML published:2015-12-07 summary:We present a discriminative nonparametric latent feature relational model(LFRM) for link prediction to automatically infer the dimensionality of latentfeatures. Under the generic RegBayes (regularized Bayesian inference)framework, we handily incorporate the prediction loss with probabilisticinference of a Bayesian model; set distinct regularization parameters fordifferent types of links to handle the imbalance issue in real networks; andunify the analysis of both the smooth logistic log-loss and the piecewiselinear hinge loss. For the nonconjugate posterior inference, we present asimple Gibbs sampler via data augmentation, without making restrictingassumptions as done in variational methods. We further develop an approximatesampler using stochastic gradient Langevin dynamics to handle large networkswith hundreds of thousands of entities and millions of links, orders ofmagnitude larger than what existing LFRM models can process. Extensive studieson various real networks show promising performance.
arxiv-15000-32 | On The Continuous Steering of the Scale of Tight Wavelet Frames | http://arxiv.org/pdf/1512.02072v1.pdf | author:Zsuzsanna Püspöki, John Paul Ward, Daniel Sage, Michael Unser category:cs.CV published:2015-12-07 summary:In analogy with steerable wavelets, we present a general construction ofadaptable tight wavelet frames, with an emphasis on scaling operations. Inparticular, the derived wavelets can be "dilated" by a procedure comparable tothe operation of steering steerable wavelets. The fundamental aspects of theconstruction are the same: an admissible collection of Fourier multipliers isused to extend a tight wavelet frame, and the "scale" of the wavelets isadapted by scaling the multipliers. As an application, the proposed waveletscan be used to improve the frequency localization. Importantly, the localizedfrequency bands specified by this construction can be scaled efficiently usingmatrix multiplication.
arxiv-15000-33 | Clustering by Deep Nearest Neighbor Descent (D-NND): A Density-based Parameter-Insensitive Clustering Method | http://arxiv.org/pdf/1512.02097v1.pdf | author:Teng Qiu, Yongjie Li category:stat.ML cs.CV cs.LG stat.CO stat.ME published:2015-12-07 summary:Most density-based clustering methods largely rely on how well the underlyingdensity is estimated. However, density estimation itself is also a challengingproblem, especially the determination of the kernel bandwidth. A largebandwidth could lead to the over-smoothed density estimation in which thenumber of density peaks could be less than the true clusters, while a smallbandwidth could lead to the under-smoothed density estimation in which spuriousdensity peaks, or called the "ripple noise", would be generated in theestimated density. In this paper, we propose a density-based hierarchicalclustering method, called the Deep Nearest Neighbor Descent (D-NND), whichcould learn the underlying density structure layer by layer and capture thecluster structure at the same time. The over-smoothed density estimation couldbe largely avoided and the negative effect of the under-estimated cases couldbe also largely reduced. Overall, D-NND presents not only the strong capabilityof discovering the underlying cluster structure but also the remarkablereliability due to its insensitivity to parameters.
arxiv-15000-34 | Digital Genesis: Computers, Evolution and Artificial Life | http://arxiv.org/pdf/1512.02100v1.pdf | author:Tim Taylor, Alan Dorin, Kevin Korb category:cs.NE published:2015-12-07 summary:The application of evolution in the digital realm, with the goal of creatingartificial intelligence and artificial life, has a history as long as that ofthe digital computer itself. We illustrate the intertwined history of theseideas, starting with the early theoretical work of John von Neumann and thepioneering experimental work of Nils Aall Barricelli. We argue thatevolutionary thinking and artificial life will continue to play an integralrole in the future development of the digital world.
arxiv-15000-35 | In-situ multi-scattering tomography | http://arxiv.org/pdf/1512.02110v1.pdf | author:Vadim Holodovsky, Yoav Y. Schechner, Anat Levin, Aviad Levis, Amit Aides category:cs.CV published:2015-12-07 summary:To recover the three dimensional (3D) volumetric distribution of matter in anobject, images of the object are captured from multiple directions andlocations. Using these images tomographic computations extract thedistribution. In highly scattering media and constrained, natural irradiance,tomography must explicitly account for off-axis scattering. Furthermore, thetomographic model and recovery must function when imaging is done in-situ, asoccurs in medical imaging and ground-based atmospheric sensing. We formulatetomography that handles arbitrary orders of scattering, using a monte-carlomodel. Moreover, the model is highly parallelizable in our formulation. Thisenables large scale rendering and recovery of volumetric scenes having a largenumber of variables. We solve stability and conditioning problems that stemfrom radiative transfer (RT) modeling in-situ.
arxiv-15000-36 | A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation | http://arxiv.org/pdf/1512.02134v1.pdf | author:Nikolaus Mayer, Eddy Ilg, Philip Häusser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, Thomas Brox category:cs.CV cs.LG stat.ML published:2015-12-07 summary:Recent work has shown that optical flow estimation can be formulated as asupervised learning task and can be successfully solved with convolutionalnetworks. Training of the so-called FlowNet was enabled by a largesynthetically generated dataset. The present paper extends the concept ofoptical flow estimation via convolutional networks to disparity and scene flowestimation. To this end, we propose three synthetic stereo video datasets withsufficient realism, variation, and size to successfully train large networks.Our datasets are the first large-scale datasets to enable training andevaluating scene flow methods. Besides the datasets, we present a convolutionalnetwork for real-time disparity estimation that provides state-of-the-artresults. By combining a flow and disparity estimation network and training itjointly, we demonstrate the first scene flow estimation with a convolutionalnetwork.
arxiv-15000-37 | Rank Pooling for Action Recognition | http://arxiv.org/pdf/1512.01848v2.pdf | author:Basura Fernando, Efstratios Gavves, Jose Oramas, Amir Ghodrati, Tinne Tuytelaars category:cs.CV published:2015-12-06 summary:We propose a function-based temporal pooling method that captures the latentstructure of the video sequence data - e.g. how frame-level features evolveover time in a video. We show how the parameters of a function that has beenfit to the video data can serve as a robust new video representation. As aspecific example, we learn a pooling function via ranking machines. By learningto rank the frame-level features of a video in chronological order, we obtain anew representation that captures the video-wide temporal dynamics of a video,suitable for action recognition. Other than ranking functions, we exploredifferent parametric models that could also explain the temporal changes invideos. The proposed functional pooling methods, and rank pooling inparticular, is easy to interpret and implement, fast to compute and effectivein recognizing a wide variety of actions. We evaluate our method on variousbenchmarks for generic action, fine-grained action and gesture recognition.Results show that rank pooling brings an absolute improvement of 7-10 averagepooling baseline. At the same time, rank pooling is compatible with andcomplementary to several appearance and local motion based methods andfeatures, such as improved trajectories and deep learning features.
arxiv-15000-38 | Vanishing point attracts gaze in free-viewing and visual search tasks | http://arxiv.org/pdf/1512.01722v2.pdf | author:Ali Borji, Mengyang Feng category:cs.CV published:2015-12-06 summary:To investigate whether the vanishing point (VP) plays a significant role ingaze guidance, we ran two experiments. In the first one, we recorded fixationsof 10 observers (4 female; mean age 22; SD=0.84) freely viewing 532 images, outof which 319 had VP (shuffled presentation; each image for 4 secs). We foundthat the average number of fixations at a local region (80x80 pixels) centeredat the VP is significantly higher than the average fixations at randomlocations (t-test; n=319; p=1.8e-35). To address the confounding factor ofsaliency, we learned a combined model of bottom-up saliency and VP. AUC scoreof our model (0.85; SD=0.01) is significantly higher than the original saliencymodel (e.g., 0.8 using AIM model by Bruce & Tsotsos (2009), t-test; p=3.14e-16) and the VP-only model (0.64, t-test; p= 4.02e-22). In the secondexperiment, we asked 14 subjects (4 female, mean age 23.07, SD=1.26) to searchfor a target character (T or L) placed randomly on a 3x3 imaginary gridoverlaid on top of an image. Subjects reported their answers by pressing one oftwo keys. Stimuli consisted of 270 color images (180 with a single VP, 90without). The target happened with equal probability inside each cell (15 timesL, 15 times T). We found that subjects were significantly faster (and moreaccurate) when target happened inside the cell containing the VP compared tocells without VP (median across 14 subjects 1.34 sec vs. 1.96; Wilcoxonrank-sum test; p = 0.0014). Response time at VP cells were also significantlylower than response time on images without VP (median 2.37; p= 4.77e-05). Thesefindings support the hypothesis that vanishing point, similar to face and text(Cerf et al., 2009) as well as gaze direction (Borji et al., 2014) attractsattention in free-viewing and visual search.
arxiv-15000-39 | Want Answers? A Reddit Inspired Study on How to Pose Questions | http://arxiv.org/pdf/1512.01768v1.pdf | author:Danish, Yogesh Dahiya, Partha Talukdar category:cs.CL published:2015-12-06 summary:Questions form an integral part of our everyday communication, both offlineand online. Getting responses to our questions from others is fundamental tosatisfying our information need and in extending our knowledge boundaries. Aquestion may be represented using various factors such as social, syntactic,semantic, etc. We hypothesize that these factors contribute with varyingdegrees towards getting responses from others for a given question. We performa thorough empirical study to measure effects of these factors using a novelquestion and answer dataset from the website Reddit.com. To the best of ourknowledge, this is the first such analysis of its kind on this important topic.We also use a sparse nonnegative matrix factorization technique toautomatically induce interpretable semantic factors from the question dataset.We also document various patterns on response prediction we observe during ouranalysis in the data. For instance, we found that preference-probing questionsare scantily answered. Our method is robust to capture such latent responsefactors. We hope to make our code and datasets publicly available uponpublication of the paper.
arxiv-15000-40 | Image reconstruction from dense binary pixels | http://arxiv.org/pdf/1512.01774v1.pdf | author:Or Litany, Tal Remez, Alex Bronstein category:cs.CV published:2015-12-06 summary:Recently, the dense binary pixel Gigavision camera had been introduced,emulating a digital version of the photographic film. While seems to be apromising solution for HDR imaging, its output is not directly usable andrequires an image reconstruction process. In this work, we formulate thisproblem as the minimization of a convex objective combining amaximum-likelihood term with a sparse synthesis prior. We present MLNet - anovel feed-forward neural network, producing acceptable output quality at afixed complexity and is two orders of magnitude faster than iterativealgorithms. We present state of the art results in the abstract.
arxiv-15000-41 | Fixation prediction with a combined model of bottom-up saliency and vanishing point | http://arxiv.org/pdf/1512.01858v1.pdf | author:Mengyang Feng, Ali Borji, Huchuan Lu category:cs.CV published:2015-12-06 summary:By predicting where humans look in natural scenes, we can understand how theyperceive complex natural scenes and prioritize information for furtherhigh-level visual processing. Several models have been proposed for thispurpose, yet there is a gap between best existing saliency models and humanperformance. While many researchers have developed purely computational modelsfor fixation prediction, less attempts have been made to discover cognitivefactors that guide gaze. Here, we study the effect of a particular type ofscene structural information, known as the vanishing point, and show that humangaze is attracted to the vanishing point regions. We record eye movements of 10observers over 532 images, out of which 319 have vanishing points. We thenconstruct a combined model of traditional saliency and a vanishing pointchannel and show that our model outperforms state of the art saliency modelsusing three scores on our dataset.
arxiv-15000-42 | A Restricted Visual Turing Test for Deep Scene and Event Understanding | http://arxiv.org/pdf/1512.01715v2.pdf | author:Hang Qi, Tianfu Wu, Mun-Wai Lee, Song-Chun Zhu category:cs.CV cs.AI published:2015-12-06 summary:This paper presents a restricted visual Turing test (VTT) for story-linebased deep understanding in long-term and multi-camera captured videos. Given aset of videos of a scene (such as a multi-room office, a garden, and a parkinglot.) and a sequence of story-line based queries, the task is to provideanswers either simply in binary form "true/false" (to a polar query) or in anaccurate natural language description (to a non-polar query). Queries, polar ornon-polar, consist of view-based queries which can be answered from aparticular camera view and scene-centered queries which involves jointinference across different cameras. The story lines are collected to coverspatial, temporal and causal understanding of input videos. The data andqueries distinguish our VTT from recently proposed visual question answering inimages and video captioning. A vision system is proposed to perform joint videoand query parsing which integrates different vision modules, a knowledge baseand a query engine. The system provides unified interfaces for differentmodules so that individual modules can be reconfigured to test a new method. Weprovide a benchmark dataset and a toolkit for ontology guided story-line querygeneration which consists of about 93.5 hours videos captured in four differentlocations and 3,426 queries split into 127 story lines. We also provide abaseline implementation and result analyses.
arxiv-15000-43 | Explaining reviews and ratings with PACO: Poisson Additive Co-Clustering | http://arxiv.org/pdf/1512.01845v1.pdf | author:Chao-Yuan Wu, Alex Beutel, Amr Ahmed, Alexander J. Smola category:cs.LG stat.ML published:2015-12-06 summary:Understanding a user's motivations provides valuable information beyond theability to recommend items. Quite often this can be accomplished by perusingboth ratings and review texts, since it is the latter where the reasoning forspecific preferences is explicitly expressed. Unfortunately matrix factorization approaches to recommendation result inlarge, complex models that are difficult to interpret and give recommendationsthat are hard to clearly explain to users. In contrast, in this paper, weattack this problem through succinct additive co-clustering. We devise a novelBayesian technique for summing co-clusterings of Poisson distributions. Withthis novel technique we propose a new Bayesian model for joint collaborativefiltering of ratings and text reviews through a sum of simple co-clusterings.The simple structure of our model yields easily interpretable recommendations.Even with a simple, succinct structure, our model outperforms competitors interms of predicting ratings with reviews.
arxiv-15000-44 | Classification of Manifolds by Single-Layer Neural Networks | http://arxiv.org/pdf/1512.01834v1.pdf | author:SueYeon Chung, Daniel D. Lee, Haim Sompolinsky category:cs.NE q-bio.NC stat.ML published:2015-12-06 summary:The neuronal representation of objects exhibit enormous variability due tochanges in the object's physical features such as location, size, orientation,and intensity. How the brain copes with the variability across these manifoldsof neuronal states and generates invariant perception of objects remains poorlyunderstood. Here we present a theory of neuronal classification of manifolds,extending Gardner's replica theory of classification of isolated points by asingle layer perceptron. We evaluate how the perceptron capacity depends on thedimensionality, size and shape of the classified manifolds.
arxiv-15000-45 | The Next Best Underwater View | http://arxiv.org/pdf/1512.01789v1.pdf | author:Mark Sheinin, Yoav Y. Schechner category:cs.CV published:2015-12-06 summary:To image in high resolution large and occlusion-prone scenes, a camera mustmove above and around. Degradation of visibility due to geometric occlusionsand distances is exacerbated by scattering, when the scene is in aparticipating medium. Moreover, underwater and in other media, artificiallighting is needed. Overall, data quality depends on the observed surface,medium and the time-varying poses of the camera and light source. This workproposes to optimize camera/light poses as they move, so that the surface isscanned efficiently and the descattered recovery has the highest quality. Thework generalizes the next best view concept of robot vision to scattering mediaand cooperative movable lighting. It also extends descattering to platformsthat move optimally. The optimization criterion is information gain, taken frominformation theory. We exploit the existence of a prior rough 3D model, sinceunderwater such a model is routinely obtained using sonar. We demonstrate thisprinciple in a scaled-down setup.
arxiv-15000-46 | Similarity Learning via Adaptive Regression and Its Application to Image Retrieval | http://arxiv.org/pdf/1512.01728v1.pdf | author:Qi Qian, Inci M. Baytas, Rong Jin, Anil Jain, Shenghuo Zhu category:cs.LG published:2015-12-06 summary:We study the problem of similarity learning and its application to imageretrieval with large-scale data. The similarity between pairs of images can bemeasured by the distances between their high dimensional representations, andthe problem of learning the appropriate similarity is often addressed bydistance metric learning. However, distance metric learning requires thelearned metric to be a PSD matrix, which is computational expensive and notnecessary for retrieval ranking problem. On the other hand, the bilinear modelis shown to be more flexible for large-scale image retrieval task, hence, weadopt it to learn a matrix for estimating pairwise similarities under theregression framework. By adaptively updating the target matrix in regression,we can mimic the hinge loss, which is more appropriate for similarity learningproblem. Although the regression problem can have the closed-form solution, thecomputational cost can be very expensive. The computational challenges comefrom two aspects: the number of images can be very large and image featureshave high dimensionality. We address the first challenge by compressing thedata by a randomized algorithm with the theoretical guarantee. For the highdimensional issue, we address it by taking low rank assumption and applyingalternating method to obtain the partial matrix, which has a global optimalsolution. Empirical studies on real world image datasets (i.e., Caltech andImageNet) demonstrate the effectiveness and efficiency of the proposed method.
arxiv-15000-47 | Large Scale Distributed Semi-Supervised Learning Using Streaming Approximation | http://arxiv.org/pdf/1512.01752v2.pdf | author:Sujith Ravi, Qiming Diao category:cs.LG cs.AI published:2015-12-06 summary:Traditional graph-based semi-supervised learning (SSL) approaches, eventhough widely applied, are not suited for massive data and large labelscenarios since they scale linearly with the number of edges $E$ and distinctlabels $m$. To deal with the large label size problem, recent works proposesketch-based methods to approximate the distribution on labels per node therebyachieving a space reduction from $O(m)$ to $O(\log m)$, under certainconditions. In this paper, we present a novel streaming graph-based SSLapproximation that captures the sparsity of the label distribution and ensuresthe algorithm propagates labels accurately, and further reduces the spacecomplexity per node to $O(1)$. We also provide a distributed version of thealgorithm that scales well to large data sizes. Experiments on real-worlddatasets demonstrate that the new method achieves better performance thanexisting state-of-the-art algorithms with significant reduction in memoryfootprint. We also study different graph construction mechanisms for naturallanguage applications and propose a robust graph augmentation strategy trainedusing state-of-the-art unsupervised deep learning architectures that yieldsfurther significant quality gains.
arxiv-15000-48 | PatchBatch: a Batch Augmented Loss for Optical Flow | http://arxiv.org/pdf/1512.01815v2.pdf | author:David Gadot, Lior Wolf category:cs.CV published:2015-12-06 summary:We propose a new pipeline for optical flow computation, based on DeepLearning techniques. We suggest using a Siamese CNN to independently, and inparallel, compute the descriptors of both images. The learned descriptors arethen compared efficiently using the L2 norm and do not require networkprocessing of patch pairs. The success of the method is based on an innovativeloss function that computes higher moments of the loss distributions for eachtraining batch. Combined with an Approximate Nearest Neighbor patch matchingmethod and a flow interpolation technique, state of the art performance isobtained on the most challenging and competitive optical flow benchmarks.
arxiv-15000-49 | A Benchmark Comparison of State-of-the-Practice Sentiment Analysis Methods | http://arxiv.org/pdf/1512.01818v3.pdf | author:Filipe Nunes Ribeiro, Matheus Araújo, Pollyanna Gonçalves, Fabrício Benevenuto, Marcos André Gonçalves category:cs.CL cs.SI published:2015-12-06 summary:In the last few years thousands of scientific papers have explored sentimentanalysis, several startups that measures opinions on real data have emerged,and a number of innovative products related to this theme have been developed.There are multiple methods for measuring sentiments, including lexical-basedapproaches and supervised machine learning methods. Despite the vast intereston the theme and wide popularity of some methods, it is unclear which method isbetter for identifying the polarity (i.e., positive or negative) of a message.Thus, there is a strong need to conduct a thorough apple-to-apple comparison ofsentiment analysis methods, as they are used in practice, across multipledatasets originated from different data sources. Such a comparison is key forunderstanding the potential limitations, advantages, and disadvantages ofpopular methods. This study aims at filling this gap by presenting a benchmarkcomparison of twenty one popular sentiment analysis methods (which we call thestate-of-the-practice methods). Our evaluation is based on a benchmark oftwenty labeled datasets, covering messages posted on social networks, movie andproduct reviews, as well as opinions and comments in news articles. Our resultshighlight the extent to which the prediction performance of these methodsvaries widely across datasets. Aiming at boosting the development of thisresearch area, we open the methods' codes and datasets used in this paper andwe deploy a benchmark system, which provides an open API for accessing andcomparing sentence-level sentiment analysis methods.
arxiv-15000-50 | PJAIT Systems for the IWSLT 2015 Evaluation Campaign Enhanced by Comparable Corpora | http://arxiv.org/pdf/1512.01639v1.pdf | author:Krzysztof Wołk, Krzysztof Marasek category:cs.CL stat.ML published:2015-12-05 summary:In this paper, we attempt to improve Statistical Machine Translation (SMT)systems on a very diverse set of language pairs (in both directions): Czech -English, Vietnamese - English, French - English and German - English. Toaccomplish this, we performed translation model training, created adaptationsof training settings for each language pair, and obtained comparable corporafor our SMT systems. Innovative tools and data adaptation techniques wereemployed. The TED parallel text corpora for the IWSLT 2015 evaluation campaignwere used to train language models, and to develop, tune, and test the system.In addition, we prepared Wikipedia-based comparable corpora for use with ourSMT system. This data was specified as permissible for the IWSLT 2015evaluation. We explored the use of domain adaptation techniques, symmetrizedword alignment models, the unsupervised transliteration models and the KenLMlanguage modeling tool. To evaluate the effects of different preparations ontranslation results, we conducted experiments and used the BLEU, NIST and TERmetrics. Our results indicate that our approach produced a positive impact onSMT quality.
arxiv-15000-51 | Unsupervised comparable corpora preparation and exploration for bi-lingual translation equivalents | http://arxiv.org/pdf/1512.01641v1.pdf | author:Krzysztof Wołk, Krzysztof Marasek category:cs.CL stat.ML published:2015-12-05 summary:The multilingual nature of the world makes translation a crucial requirementtoday. Parallel dictionaries constructed by humans are a widely-availableresource, but they are limited and do not provide enough coverage for goodquality translation purposes, due to out-of-vocabulary words and neologisms.This motivates the use of statistical translation systems, which areunfortunately dependent on the quantity and quality of training data. Suchsystems have a very limited availability especially for some languages and verynarrow text domains. In this research we present our improvements to currentcomparable corpora mining methodologies by re- implementation of the comparisonalgorithms (using Needleman-Wunch algorithm), introduction of a tuning scriptand computation time improvement by GPU acceleration. Experiments are carriedout on bilingual data extracted from the Wikipedia, on various domains. For theWikipedia itself, additional cross-lingual comparison heuristics wereintroduced. The modifications made a positive impact on the quality andquantity of mined data and on the translation quality.
arxiv-15000-52 | A Deep Structured Model with Radius-Margin Bound for 3D Human Activity Recognition | http://arxiv.org/pdf/1512.01642v1.pdf | author:Liang Lin, Keze Wang, Wangmeng Zuo, Meng Wang, Jiebo Luo, Lei Zhang category:cs.CV published:2015-12-05 summary:Understanding human activity is very challenging even with the recentlydeveloped 3D/depth sensors. To solve this problem, this work investigates anovel deep structured model, which adaptively decomposes an activity instanceinto temporal parts using the convolutional neural networks (CNNs). Our modeladvances the traditional deep learning approaches in two aspects. First, { weincorporate latent temporal structure into the deep model, accounting for largetemporal variations of diverse human activities. In particular, we utilize thelatent variables to decompose the input activity into a number of temporallysegmented sub-activities, and accordingly feed them into the parts (i.e.sub-networks) of the deep architecture}. Second, we incorporate a radius-marginbound as a regularization term into our deep model, which effectively improvesthe generalization performance for classification. For model training, wepropose a principled learning algorithm that iteratively (i) discovers theoptimal latent variables (i.e. the ways of activity decomposition) for alltraining instances, (ii) { updates the classifiers} based on the generatedfeatures, and (iii) updates the parameters of multi-layer neural networks. Inthe experiments, our approach is validated on several complex scenarios forhuman activity recognition and demonstrates superior performances over otherstate-of-the-art approaches.
arxiv-15000-53 | Stochastic Collapsed Variational Inference for Hidden Markov Models | http://arxiv.org/pdf/1512.01665v1.pdf | author:Pengyu Wang, Phil Blunsom category:stat.ML published:2015-12-05 summary:Stochastic variational inference for collapsed models has recently beensuccessfully applied to large scale topic modelling. In this paper, we proposea stochastic collapsed variational inference algorithm for hidden Markovmodels, in a sequential data setting. Given a collapsed hidden Markov Model, webreak its long Markov chain into a set of short subchains. We propose a novelsum-product algorithm to update the posteriors of the subchains, taking intoaccount their boundary transitions due to the sequential dependencies. Ourexperiments on two discrete datasets show that our collapsed algorithm isscalable to very large datasets, memory efficient and significantly moreaccurate than the existing uncollapsed algorithm.
arxiv-15000-54 | Stochastic Collapsed Variational Inference for Sequential Data | http://arxiv.org/pdf/1512.01666v1.pdf | author:Pengyu Wang, Phil Blunsom category:stat.ML published:2015-12-05 summary:Stochastic variational inference for collapsed models has recently beensuccessfully applied to large scale topic modelling. In this paper, we proposea stochastic collapsed variational inference algorithm in the sequential datasetting. Our algorithm is applicable to both finite hidden Markov models andhierarchical Dirichlet process hidden Markov models, and to any datasetsgenerated by emission distributions in the exponential family. Our experimentresults on two discrete datasets show that our inference is both more efficientand more accurate than its uncollapsed version, stochastic variationalinference.
arxiv-15000-55 | Hierarchical Sparse Modeling: A Choice of Two Regularizers | http://arxiv.org/pdf/1512.01631v1.pdf | author:Xiaohan Yan, Jacob Bien category:stat.ME math.ST stat.CO stat.ML stat.TH published:2015-12-05 summary:Demanding sparsity in estimated models has become a routine practice instatistics. In many situations, we wish to demand that the sparsity patternsattained honor certain problem-specific constraints. Hierarchical sparsemodeling (HSM) refers to situations in which these constraints specify that oneset of parameters be set to zero whenever another is set to zero. In recentyears, numerous papers have developed convex regularizers for this form ofsparsity structure arising in areas including interaction modeling, timeseries, and covariance estimation. In this paper, we observe that these methodsfall into two frameworks, which have not been systematically compared in thecontext of HSM. The purpose of this paper is to provide a side-by-sidecomparison of these two frameworks for HSM in terms of their statisticalproperties and computational efficiency. We call attention to a problem withthe more commonly used framework and provide new insights into the other, whichcan greatly improve its computational performance. Finally, we compare the twomethods in the context of covariance estimation, where we introduce a newsparsely-banded estimator, which we show achieves the statistical advantages ofan existing method but is simpler to compute.
arxiv-15000-56 | A Shapley Value Solution to Game Theoretic-based Feature Reduction in False Alarm Detection | http://arxiv.org/pdf/1512.01680v1.pdf | author:Fatemeh Afghah, Abolfazl Razi, Kayvan Najarian category:cs.CV published:2015-12-05 summary:False alarm is one of the main concerns in intensive care units and canresult in care disruption, sleep deprivation, and insensitivity of care-giversto alarms. Several methods have been proposed to suppress the false alarm ratethrough improving the quality of physiological signals by filtering, anddeveloping more accurate sensors. However, significant intrinsic correlationamong the extracted features limits the performance of most currently availabledata mining techniques, as they often discard the predictors with lowindividual impact that may potentially have strong discriminatory power whengrouped with others. We propose a model based on coalition game theory thatconsiders the inter-features dependencies in determining the salient predictorsin respect to false alarm, which results in improved classification accuracy.The superior performance of this method compared to current methods is shown insimulation results using PhysionNet's MIMIC II database.
arxiv-15000-57 | Maximum Entropy Binary Encoding for Face Template Protection | http://arxiv.org/pdf/1512.01691v1.pdf | author:Rohit Kumar Pandey, Yingbo Zhou, Bhargava Urala Kota, Venu Govindaraju category:cs.CV published:2015-12-05 summary:In this paper we present a framework for secure identification using deepneural networks, and apply it to the task of template protection for faceauthentication. We use deep convolutional neural networks (CNNs) to learn amapping from face images to maximum entropy binary (MEB) codes. The mapping isrobust enough to tackle the problem of exact matching, yielding the same codefor new samples of a user as the code assigned during training. These codes arethen hashed using any hash function that follows the random oracle model (likeSHA-512) to generate protected face templates (similar to text based passwordprotection). The algorithm makes no unrealistic assumptions and offers hightemplate security, cancelability, and state-of-the-art matching performance.The efficacy of the approach is shown on CMU-PIE, Extended Yale B, andMulti-PIE face databases. We achieve high (~95%) genuine accept rates (GAR) atzero false accept rate (FAR) with up to 1024 bits of template security.
arxiv-15000-58 | Deep Attention Recurrent Q-Network | http://arxiv.org/pdf/1512.01693v1.pdf | author:Ivan Sorokin, Alexey Seleznev, Mikhail Pavlov, Aleksandr Fedorov, Anastasiia Ignateva category:cs.LG published:2015-12-05 summary:A deep learning approach to reinforcement learning led to a general learnerable to train on visual input to play a variety of arcade games at the humanand superhuman levels. Its creators at the Google DeepMind's team called theapproach: Deep Q-Network (DQN). We present an extension of DQN by "soft" and"hard" attention mechanisms. Tests of the proposed Deep Attention RecurrentQ-Network (DARQN) algorithm on multiple Atari 2600 games show level ofperformance superior to that of DQN. Moreover, built-in attention mechanismsallow a direct online monitoring of the training process by highlighting theregions of the game screen the agent is focusing on when making decisions.
arxiv-15000-59 | Variance Reduction for Distributed Stochastic Gradient Descent | http://arxiv.org/pdf/1512.01708v1.pdf | author:Soham De, Gavin Taylor, Tom Goldstein category:cs.LG cs.DC math.OC stat.ML published:2015-12-05 summary:Variance reduction (VR) methods boost the performance of stochastic gradientdescent (SGD) by enabling the use of larger stepsizes and preserving linearconvergence rates. However, current variance reduced SGD methods require eitherhigh memory usage or require a full pass over the (large) data set at the endof each epoch to calculate the exact gradient of the objective function. Thismakes current VR methods impractical in distributed or parallel settings. Inthis paper, we propose a variance reduction method, called VR-lite, that doesnot require full gradient computations or extra storage. We explore distributedsynchronous and asynchronous variants with both high and low communicationlatency. We find that our distributed algorithms scale linearly with the numberof local workers and remain stable even with low communication frequency. Weempirically compare both the sequential and distributed algorithms tostate-of-the-art stochastic optimization methods, and find that our proposedalgorithms consistently converge faster than other stochastic methods.
arxiv-15000-60 | Generating News Headlines with Recurrent Neural Networks | http://arxiv.org/pdf/1512.01712v1.pdf | author:Konstantin Lopyrev category:cs.CL cs.LG cs.NE published:2015-12-05 summary:We describe an application of an encoder-decoder recurrent neural networkwith LSTM units and attention to generating headlines from the text of newsarticles. We find that the model is quite effective at concisely paraphrasingnews articles. Furthermore, we study how the neural network decides which inputwords to pay attention to, and specifically we identify the function of thedifferent neurons in a simplified attention mechanism. Interestingly, oursimplified attention mechanism performs better that the more complex attentionmechanism on a held out set of articles.
arxiv-15000-61 | A Novel Paradigm for Calculating Ramsey Number via Artificial Bee Colony Algorithm | http://arxiv.org/pdf/1512.01613v2.pdf | author:Wei-Hao Mao, Fei Gao, Yi-Jin Dong, Wen-Ming Li category:cs.AI cs.NE math.CO math.OC published:2015-12-05 summary:The Ramsey number is of vital importance in Ramsey's theorem. This paperproposed a novel methodology for constructing Ramsey graphs about R(3,10),which uses Artificial Bee Colony optimization(ABC) to raise the lower bound ofRamsey number R(3,10). The r(3,10)-graph contains two limitations, that is,neither complete graphs of order 3 nor independent sets of order 10. To resolvethese limitations, a special mathematical model is put in the paradigm toconvert the problems into discrete optimization whose smaller minimizers arecorrespondent to bigger lower bound as approximation of inf R(3,10). Todemonstrate the potential of the proposed method, simulations are done to tominimize the amount of these two types of graphs. For the first time, fourr(3,9,39) graphs with best approximation for inf R(3,10) are reported insimulations to support the current lower bound for R(3,10). The experiments'results show that the proposed paradigm for Ramsey number's calculation drivenby ABC is a successful method with the advantages of high precision androbustness.
arxiv-15000-62 | Risk-Constrained Reinforcement Learning with Percentile Risk Criteria | http://arxiv.org/pdf/1512.01629v1.pdf | author:Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, Marco Pavone category:cs.AI cs.LG math.OC published:2015-12-05 summary:In many sequential decision-making problems one is interested in minimizingan expected cumulative cost while taking into account \emph{risk}, i.e.,increased awareness of events of small probability and high consequences.Accordingly, the objective of this paper is to present efficient reinforcementlearning algorithms for risk-constrained Markov decision processes (MDPs),where risk is represented via a chance constraint or a constraint on theconditional value-at-risk (CVaR) of the cumulative cost. We collectively referto such problems as percentile risk-constrained MDPs. Specifically, we first derive a formula for computing the gradient of theLagrangian function for percentile risk-constrained MDPs. Then, we devisepolicy gradient and actor-critic algorithms that (1) estimate such gradient,(2) update the policy parameters in the descent direction, and (3) update theLagrange multiplier in the ascent direction. For these algorithms we proveconvergence to locally-optimal policies. Finally, we demonstrate theeffectiveness of our algorithms in an optimal stopping problem and an onlinemarketing application.
arxiv-15000-63 | Approximated and User Steerable tSNE for Progressive Visual Analytics | http://arxiv.org/pdf/1512.01655v2.pdf | author:Nicola Pezzotti, Boudewijn P. F. Lelieveldt, Laurens van der Maaten, Thomas Höllt, Elmar Eisemann, Anna Vilanova category:cs.CV cs.LG published:2015-12-05 summary:Progressive Visual Analytics aims at improving the interactivity in existinganalytics techniques by means of visualization as well as interaction withintermediate results. One key method for data analysis is dimensionalityreduction, for example, to produce 2D embeddings that can be visualized andanalyzed efficiently. t-Distributed Stochastic Neighbor Embedding (tSNE) is awell-suited technique for the visualization of several high-dimensional data.tSNE can create meaningful intermediate results but suffers from a slowinitialization that constrains its application in Progressive Visual Analytics.We introduce a controllable tSNE approximation (A-tSNE), which trades off speedand accuracy, to enable interactive data exploration. We offer real-timevisualization techniques, including a density-based solution and a Magic Lensto inspect the degree of approximation. With this feedback, the user can decideon local refinements and steer the approximation level during the analysis. Wedemonstrate our technique with several datasets, in a real-world researchscenario and for the real-time analysis of high-dimensional streams toillustrate its effectiveness for interactive data analysis.
arxiv-15000-64 | Max-Pooling Dropout for Regularization of Convolutional Neural Networks | http://arxiv.org/pdf/1512.01400v1.pdf | author:Haibing Wu, Xiaodong Gu category:cs.LG cs.CV cs.NE published:2015-12-04 summary:Recently, dropout has seen increasing use in deep learning. For deepconvolutional neural networks, dropout is known to work well in fully-connectedlayers. However, its effect in pooling layers is still not clear. This paperdemonstrates that max-pooling dropout is equivalent to randomly pickingactivation based on a multinomial distribution at training time. In light ofthis insight, we advocate employing our proposed probabilistic weightedpooling, instead of commonly used max-pooling, to act as model averaging attest time. Empirical evidence validates the superiority of probabilisticweighted pooling. We also compare max-pooling dropout and stochastic pooling,both of which introduce stochasticity based on multinomial distributions atpooling stage.
arxiv-15000-65 | Model Validation for Vision Systems via Graphics Simulation | http://arxiv.org/pdf/1512.01401v1.pdf | author:V S R Veeravasarapu, Rudra Narayan Hota, Constantin Rothkopf, Ramesh Visvanathan category:cs.CV published:2015-12-04 summary:Rapid advances in computation, combined with latest advances in computergraphics simulations have facilitated the development of vision systems andtraining them in virtual environments. One major stumbling block is incertification of the designs and tuned parameters of these systems to work inreal world. In this paper, we begin to explore the fundamental question: Whichtype of information transfer is more analogous to real world? Inspired from theperformance characterization methodology outlined in the 90's, we note thatinsights derived from simulations can be qualitative or quantitative dependingon the degree of the fidelity of models used in simulations and the nature ofthe questions posed by the experimenter. We adapt the methodology in thecontext of current graphics simulation tools for modeling data generationprocesses and, for systematic performance characterization and trade-offanalysis for vision system design leading to qualitative and quantitativeinsights. In concrete, we examine invariance assumptions used in visionalgorithms for video surveillance settings as a case study and assess thedegree to which those invariance assumptions deviate as a function ofcontextual variables on both graphics simulations and in real data. As computergraphics rendering quality improves, we believe teasing apart the degree towhich model assumptions are valid via systematic graphics simulation can be asignificant aid to assisting more principled ways of approaching vision systemdesign and performance modeling.
arxiv-15000-66 | Neuron's Eye View: Inferring Features of Complex Stimuli from Neural Responses | http://arxiv.org/pdf/1512.01408v1.pdf | author:John M Pearson, Jeffrey M Beck category:stat.ML q-bio.NC published:2015-12-04 summary:Experiments that study neural encoding of stimuli at the level of individualneurons typically choose a small set of features present in the world ---contrast and luminance for vision, pitch and intensity for sound --- andassemble a stimulus set that systematically (and preferably exhaustively)varies along these dimensions. Neuronal responses in the form of firing ratesare then examined for modulation with respect to these features via some formof regression. This approach requires that experimenters know (or guess) inadvance the relevant features coded by a given population of neurons.Unfortunately, for domains as complex as social interaction or naturalmovement, the relevant feature space is poorly understood, and an arbitrary\emph{a priori} choice of feature sets may give rise to confirmation bias.Here, we present a Bayesian model for exploratory data analysis that is capableof automatically identifying the features present in unstructured stimuli basedsolely on neuronal responses. Our approach is unique within the class of latentstate space models of neural activity in that it assumes that firing rates ofneurons are sensitive to multiple discrete time-varying features tied to the\emph{stimulus}, each of which has Markov (or semi-Markov) dynamics. That is,we are modeling stimulus dynamics as driven by neural activity, rather thanintrinsic neural dynamics. We derive a fast variational Bayesian inferencealgorithm and show that it correctly recovers hidden features in syntheticdata, as well as ground-truth stimulus features in a prototypical neuraldataset. To demonstrate the utility of the algorithm, we also apply it to anexploratory analysis of prefrontal cortex recordings performed while monkeyswatched naturalistic videos of primate social activity.
arxiv-15000-67 | What Makes it Difficult to Understand a Scientific Literature? | http://arxiv.org/pdf/1512.01409v1.pdf | author:Mengyun Cao, Jiao Tian, Dezhi Cheng, Jin Liu, Xiaoping Sun category:cs.CL published:2015-12-04 summary:In the artificial intelligence area, one of the ultimate goals is to makecomputers understand human language and offer assistance. In order to achievethis ideal, researchers of computer science have put forward a lot of modelsand algorithms attempting at enabling the machine to analyze and process humannatural language on different levels of semantics. Although recent progress inthis field offers much hope, we still have to ask whether current research canprovide assistance that people really desire in reading and comprehension. Tothis end, we conducted a reading comprehension test on two scientific paperswhich are written in different styles. We use the semantic link models toanalyze the understanding obstacles that people will face in the process ofreading and figure out what makes it difficult for human to understand ascientific literature. Through such analysis, we summarized somecharacteristics and problems which are reflected by people with differentlevels of knowledge on the comprehension of difficult science and technologyliterature, which can be modeled in semantic link network. We believe thatthese characteristics and problems will help us re-examine the existing machinemodels and are helpful in the designing of new one.
arxiv-15000-68 | Computational Imaging for VLBI Image Reconstruction | http://arxiv.org/pdf/1512.01413v1.pdf | author:Katherine L. Bouman, Michael D. Johnson, Daniel Zoran, Vincent L. Fish, Sheperd S. Doeleman, William T. Freeman category:astro-ph.IM astro-ph.GA cs.CV published:2015-12-04 summary:Very long baseline interferometry (VLBI) is a technique for imaging celestialradio emissions by simultaneously observing a source from telescopesdistributed across Earth. The challenges in reconstructing images from fineangular resolution VLBI data are immense. The data is extremely sparse andnoisy, thus requiring statistical image models such as those designed in thecomputer vision community. In this paper we present a novel Bayesian approachfor VLBI image reconstruction. While other methods require careful tuning andparameter selection for different types of images, our method is robust andproduces good results under different settings such as low SNR or extendedemissions. The success of our method is demonstrated on realistic syntheticexperiments as well as publicly available real data. We present this problem ina way that is accessible to members of the computer vision community, andprovide a dataset website (vlbiimaging.csail.mit.edu) to allow for controlledcomparisons across algorithms. This dataset can foster development of newmethods by making VLBI easily approachable to computer vision researchers.
arxiv-15000-69 | ASIST: Automatic Semantically Invariant Scene Transformation | http://arxiv.org/pdf/1512.01515v1.pdf | author:Or Litany, Tal Remez, Daniel Freedman, Lior Shapira, Alex Bronstein, Ran Gal category:cs.CV published:2015-12-04 summary:We present ASIST, a technique for transforming point clouds by replacingobjects with their semantically equivalent counterparts. Transformations ofthis kind have applications in virtual reality, repair of fused scans, androbotics. ASIST is based on a unified formulation of semantic labeling andobject replacement; both result from minimizing a single objective. We presentnumerical tools for the e?cient solution of this optimization problem. Themethod is experimentally assessed on new datasets of both synthetic and realpoint clouds, and is additionally compared to two recent works on objectreplacement on data from the corresponding papers.
arxiv-15000-70 | State of the Art Control of Atari Games Using Shallow Reinforcement Learning | http://arxiv.org/pdf/1512.01563v2.pdf | author:Yitao Liang, Marlos C. Machado, Erik Talvitie, Michael Bowling category:cs.LG published:2015-12-04 summary:The recently introduced Deep Q-Networks (DQN) algorithm has gained attentionas one of the first successful combinations of deep neural networks andreinforcement learning. Its promise was demonstrated in the Arcade LearningEnvironment (ALE), a challenging framework composed of dozens of Atari 2600games used to evaluate general competency in AI. It achieved dramaticallybetter results than earlier approaches, showing that its ability to learn goodrepresentations is quite robust and general. This paper attempts to understandthe principles that underlie DQN's impressive performance and to bettercontextualize its success. We systematically evaluate the importance of keyrepresentational biases encoded by DQN's network by proposing simple linearrepresentations that make use of these concepts. Incorporating thesecharacteristics, we obtain a computationally practical feature set thatachieves competitive performance to DQN in the ALE. Besides offering insightinto the strengths and weaknesses of DQN, we provide a generic representationfor the ALE, significantly reducing the burden of learning a representation foreach game. Moreover, we also provide a simple, reproducible benchmark for thesake of comparison to future work in the ALE.
arxiv-15000-71 | Learning the Semantics of Manipulation Action | http://arxiv.org/pdf/1512.01525v1.pdf | author:Yezhou Yang, Yiannis Aloimonos, Cornelia Fermuller, Eren Erdal Aksoy category:cs.RO cs.CL cs.CV published:2015-12-04 summary:In this paper we present a formal computational framework for modelingmanipulation actions. The introduced formalism leads to semantics ofmanipulation action and has applications to both observing and understandinghuman manipulation actions as well as executing them with a robotic mechanism(e.g. a humanoid robot). It is based on a Combinatory Categorial Grammar. Thegoal of the introduced framework is to: (1) represent manipulation actions withboth syntax and semantic parts, where the semantic part employs$\lambda$-calculus; (2) enable a probabilistic semantic parsing schema to learnthe $\lambda$-calculus representation of manipulation action from an annotatedaction corpus of videos; (3) use (1) and (2) to develop a system that visuallyobserves manipulation actions and understands their meaning while it can reasonbeyond observations using propositional logic and axiom schemata. Theexperiments conducted on a public available large manipulation action datasetvalidate the theoretical framework and our implementation.
arxiv-15000-72 | Predicting psychological attributions from face photographs with a deep neural network | http://arxiv.org/pdf/1512.01289v1.pdf | author:Edward Grant, Stephan Sahm, Mariam Zabihi, Marcel van Gerven category:cs.CV cs.LG cs.NE published:2015-12-04 summary:Judgements about personality based on facial appearance are strong effectorsin social decision making and are known to impact on areas from presidentialelections to jury decisions. Recent work has shown that it is possible topredict perception of memorability, trustworthiness, intelligence and otherattributes in human face images. The most successful of these approachesrequires face images expertly annotated with key facial landmarks. Wedemonstrate a Convolutional Neural Network (CNN) model that is able perform thesame task without the need for landmark features thereby greatly increasingefficiency. The model has high accuracy, surpassing human level performance insome cases. Furthermore, we use a deconvolutional approach to visualizeimportant features for perception of 22 attributes and show that these can bedescribed as a composites of their positive and negative components byseparately visualizing both.
arxiv-15000-73 | Motion trails from time-lapse video | http://arxiv.org/pdf/1512.01533v1.pdf | author:Camille Goudeseune category:cs.CV I.3.3; I.4.6 published:2015-12-04 summary:From an image sequence captured by a stationary camera, backgroundsubtraction can detect moving foreground objects in the scene. Distinguishingforeground from background is further improved by various heuristics. Then eachobject's motion can be emphasized by duplicating its positions as a motiontrail. These trails clarify the objects' spatial relationships. Also, addingmotion trails to a video before previewing it at high speed reduces the risk ofoverlooking transient events.
arxiv-15000-74 | Reuse of Neural Modules for General Video Game Playing | http://arxiv.org/pdf/1512.01537v1.pdf | author:Alexander Braylan, Mark Hollenbeck, Elliot Meyerson, Risto Miikkulainen category:cs.NE cs.AI published:2015-12-04 summary:A general approach to knowledge transfer is introduced in which an agentcontrolled by a neural network adapts how it reuses existing networks as itlearns in a new domain. Networks trained for a new domain can improve theirperformance by routing activation selectively through previously learned neuralstructure, regardless of how or for what it was learned. A neuroevolutionimplementation of this approach is presented with application tohigh-dimensional sequential decision-making domains. This approach is moregeneral than previous approaches to neural transfer for reinforcement learning.It is domain-agnostic and requires no prior assumptions about the nature oftask relatedness or mappings. The method is analyzed in a stochastic version ofthe Arcade Learning Environment, demonstrating that it improves performance insome of the more complex Atari 2600 games, and that the success of transfer canbe predicted based on a high-level characterization of game dynamics.
arxiv-15000-75 | Extracting Biomolecular Interactions Using Semantic Parsing of Biomedical Text | http://arxiv.org/pdf/1512.01587v1.pdf | author:Sahil Garg, Aram Galstyan, Ulf Hermjakob, Daniel Marcu category:cs.CL cs.AI cs.IR cs.IT cs.LG math.IT published:2015-12-04 summary:We advance the state of the art in biomolecular interaction extraction withthree contributions: (i) We show that deep, Abstract Meaning Representations(AMR) significantly improve the accuracy of a biomolecular interactionextraction system when compared to a baseline that relies solely on surface-and syntax-based features; (ii) In contrast with previous approaches that inferrelations on a sentence-by-sentence basis, we expand our framework to enableconsistent predictions over sets of sentences (documents); (iii) We furthermodify and expand a graph kernel learning framework to enable concurrentexploitation of automatically induced AMR (semantic) and dependency structure(syntactic) representations. Our experiments show that our approach yieldsinteraction extraction systems that are more robust in environments where thereis a significant mismatch between training and test conditions.
arxiv-15000-76 | Staple: Complementary Learners for Real-Time Tracking | http://arxiv.org/pdf/1512.01355v2.pdf | author:Luca Bertinetto, Jack Valmadre, Stuart Golodetz, Ondrej Miksik, Philip Torr category:cs.CV published:2015-12-04 summary:Correlation Filter-based trackers have recently achieved excellentperformance, showing great robustness to challenging situations exhibitingmotion blur and illumination changes. However, since the model that they learndepends strongly on the spatial layout of the tracked object, they arenotoriously sensitive to deformation. Models based on colour statistics havecomplementary traits: they cope well with variation in shape, but suffer whenillumination is not consistent throughout a sequence. Moreover, colourdistributions alone can be insufficiently discriminative. In this paper, weshow that a simple tracker combining complementary cues in a ridge regressionframework can operate faster than 80 FPS and outperform not only all entries inthe popular VOT14 competition, but also recent and far more sophisticatedtrackers according to multiple benchmarks.
arxiv-15000-77 | An Online Unsupervised Structural Plasticity Algorithm for Spiking Neural Networks | http://arxiv.org/pdf/1512.01314v1.pdf | author:Subhrajit Roy, Arindam Basu category:cs.NE published:2015-12-04 summary:In this article, we propose a novel Winner-Take-All (WTA) architectureemploying neurons with nonlinear dendrites and an online unsupervisedstructural plasticity rule for training it. Further, to aid hardwareimplementations, our network employs only binary synapses. The proposedlearning rule is inspired by spike time dependent plasticity (STDP) but differsfor each dendrite based on its activation level. It trains the WTA networkthrough formation and elimination of connections between inputs and synapses.To demonstrate the performance of the proposed network and learning rule, weemploy it to solve two, four and six class classification of random Poissonspike time inputs. The results indicate that by proper tuning of the inhibitorytime constant of the WTA, a trade-off between specificity and sensitivity ofthe network can be achieved. We use the inhibitory time constant to set thenumber of subpatterns per pattern we want to detect. We show that while thepercentage of successful trials are 92%, 88% and 82% for two, four and sixclass classification when no pattern subdivisions are made, it increases to100% when each pattern is subdivided into 5 or 10 subpatterns. However, theformer scenario of no pattern subdivision is more jitter resilient than thelater ones.
arxiv-15000-78 | Topic segmentation via community detection in complex networks | http://arxiv.org/pdf/1512.01384v1.pdf | author:Henrique F. de Arruda, Luciano da F. Costa, Diego R. Amancio category:cs.CL cs.SI published:2015-12-04 summary:Many real systems have been modelled in terms of network concepts, andwritten texts are a particular example of information networks. In recentyears, the use of network methods to analyze language has allowed the discoveryof several interesting findings, including the proposition of novel models toexplain the emergence of fundamental universal patterns. While syntacticalnetworks, one of the most prevalent networked models of written texts, displayboth scale-free and small-world properties, such representation fails incapturing other textual features, such as the organization in topics orsubjects. In this context, we propose a novel network representation whose mainpurpose is to capture the semantical relationships of words in a simple way. Todo so, we link all words co-occurring in the same semantic context, which isdefined in a threefold way. We show that the proposed representations favoursthe emergence of communities of semantically related words, and this featuremay be used to identify relevant topics. The proposed methodology to detecttopics was applied to segment selected Wikipedia articles. We have found that,in general, our methods outperform traditional bag-of-words representations,which suggests that a high-level textual representation may be useful to studysemantical features of texts.
arxiv-15000-79 | Toward a Taxonomy and Computational Models of Abnormalities in Images | http://arxiv.org/pdf/1512.01325v1.pdf | author:Babak Saleh, Ahmed Elgammal, Jacob Feldman, Ali Farhadi category:cs.CV cs.AI cs.HC cs.IT cs.LG math.IT published:2015-12-04 summary:The human visual system can spot an abnormal image, and reason about whatmakes it strange. This task has not received enough attention in computervision. In this paper we study various types of atypicalities in images in amore comprehensive way than has been done before. We propose a new dataset ofabnormal images showing a wide range of atypicalities. We design human subjectexperiments to discover a coarse taxonomy of the reasons for abnormality. Ourexperiments reveal three major categories of abnormality: object-centric,scene-centric, and contextual. Based on this taxonomy, we propose acomprehensive computational model that can predict all different types ofabnormality in images and outperform prior arts in abnormality recognition.
arxiv-15000-80 | Sublabel-Accurate Relaxation of Nonconvex Energies | http://arxiv.org/pdf/1512.01383v1.pdf | author:Thomas Möllenhoff, Emanuel Laude, Michael Moeller, Jan Lellmann, Daniel Cremers category:cs.CV published:2015-12-04 summary:We propose a novel spatially continuous framework for convex relaxationsbased on functional lifting. Our method can be interpreted as asublabel-accurate solution to multilabel problems. We show that previouslyproposed functional lifting methods optimize an energy which is linear betweentwo labels and hence require (often infinitely) many labels for a faithfulapproximation. In contrast, the proposed formulation is based on a piecewiseconvex approximation and therefore needs far fewer labels. In comparison torecent MRF-based approaches, our method is formulated in a spatially continuoussetting and shows less grid bias. Moreover, in a local sense, our formulationis the tightest possible convex relaxation. It is easy to implement and allowsan efficient primal-dual optimization on GPUs. We show the effectiveness of ourapproach on several computer vision problems.
arxiv-15000-81 | Q-Networks for Binary Vector Actions | http://arxiv.org/pdf/1512.01332v1.pdf | author:Naoto Yoshida category:cs.NE cs.LG published:2015-12-04 summary:In this paper reinforcement learning with binary vector actions wasinvestigated. We suggest an effective architecture of the neural networks forapproximating an action-value function with binary vector actions. The proposedarchitecture approximates the action-value function by a linear function withrespect to the action vector, but is still non-linear with respect to the stateinput. We show that this approximation method enables the efficient calculationof greedy action selection and softmax action selection. Using thisarchitecture, we suggest an online algorithm based on Q-learning. The empiricalresults in the grid world and the blocker task suggest that our approximationarchitecture would be effective for the RL problems with large discrete actionsets.
arxiv-15000-82 | Creation of a Deep Convolutional Auto-Encoder in Caffe | http://arxiv.org/pdf/1512.01596v3.pdf | author:Volodymyr Turchenko, Artur Luczak category:cs.NE cs.CV cs.LG 68Txx published:2015-12-04 summary:The development of a deep (stacked) convolutional auto-encoder in the Caffedeep learning framework is presented in this paper. We describe simpleprinciples which we used to create this model in Caffe. The proposed model ofconvolutional auto-encoder does not have pooling/unpooling layers yet. Theresults of our experimental research show comparable accuracy of dimensionalityreduction in comparison with a classic auto-encoder on the example of MNISTdataset.
arxiv-15000-83 | Proposition of a Theoretical Model for Missing Data Imputation using Deep Learning and Evolutionary Algorithms | http://arxiv.org/pdf/1512.01362v1.pdf | author:Collins Leke, Tshilidzi Marwala, Satyakama Paul category:cs.NE cs.LG published:2015-12-04 summary:In the last couple of decades, there has been major advancements in thedomain of missing data imputation. The techniques in the domain include amongstothers: Expectation Maximization, Neural Networks with Evolutionary Algorithmsor optimization techniques and K-Nearest Neighbor approaches to solve theproblem. The presence of missing data entries in databases render the tasks ofdecision-making and data analysis nontrivial. As a result this area hasattracted a lot of research interest with the aim being to yield accurate andtime efficient and sensitive missing data imputation techniques especially whentime sensitive applications are concerned like power plants and windingprocesses. In this article, considering arbitrary and monotone missing datapatterns, we hypothesize that the use of deep neural networks built usingautoencoders and denoising autoencoders in conjunction with genetic algorithms,swarm intelligence and maximum likelihood estimator methods as novel dataimputation techniques will lead to better imputed values than existingtechniques. Also considered are the missing at random, missing completely atrandom and missing not at random missing data mechanisms. We also intend to usefuzzy logic in tandem with deep neural networks to perform the missing dataimputation tasks, as well as different building blocks for the deep neuralnetworks like Stacked Restricted Boltzmann Machines and Deep Belief Networks totest our hypothesis. The motivation behind this article is the need for missingdata imputation techniques that lead to better imputed values than existingmethods with higher accuracies and lower errors.
arxiv-15000-84 | Fixed Point Performance Analysis of Recurrent Neural Networks | http://arxiv.org/pdf/1512.01322v2.pdf | author:Sungho Shin, Kyuyeon Hwang, Wonyong Sung category:cs.LG cs.NE published:2015-12-04 summary:Recurrent neural networks have shown excellent performance in manyapplications, however they require increased complexity in hardware or softwarebased implementations. The hardware complexity can be much lowered byminimizing the word-length of weights and signals. This work analyzes thefixed-point performance of recurrent neural networks using a retrain basedquantization method. The quantization sensitivity of each layer in RNNs isstudied, and the overall fixed-point optimization results minimizing thecapacity of weights while not sacrificing the performance are presented. Alanguage model and a phoneme recognition examples are used.
arxiv-15000-85 | What can we learn about CNNs from a large scale controlled object dataset? | http://arxiv.org/pdf/1512.01320v2.pdf | author:Ali Borji, Saeed Izadi, Laurent Itti category:cs.CV published:2015-12-04 summary:Tolerance to image variations (e.g. translation, scale, pose, illumination)is an important desired property of any object recognition system, be it humanor machine. Moving towards increasingly bigger datasets has been trending incomputer vision specially with the emergence of highly popular deep learningmodels. While being very useful for learning invariance to object inter- andintra-class shape variability, these large-scale wild datasets are not veryuseful for learning invariance to other parameters forcing researchers toresort to other tricks for training a model. In this work, we introduce alarge-scale synthetic dataset, which is freely and publicly available, and useit to answer several fundamental questions regarding invariance and selectivityproperties of convolutional neural networks. Our dataset contains two parts: a)objects shot on a turntable: 16 categories, 8 rotation angles, 11 cameras on asemicircular arch, 5 lighting conditions, 3 focus levels, variety ofbackgrounds (23.4 per instance) generating 1320 images per instance (over 20million images in total), and b) scenes: in which a robot arm takes pictures ofobjects on a 1:160 scale scene. We study: 1) invariance and selectivity ofdifferent CNN layers, 2) knowledge transfer from one object category toanother, 3) systematic or random sampling of images to build a train set, 4)domain adaptation from synthetic to natural scenes, and 5) order of knowledgedelivery to CNNs. We also explore how our analyses can lead the field todevelop more efficient CNNs.
arxiv-15000-86 | Neural Generative Question Answering | http://arxiv.org/pdf/1512.01337v4.pdf | author:Jun Yin, Xin Jiang, Zhengdong Lu, Lifeng Shang, Hang Li, Xiaoming Li category:cs.CL published:2015-12-04 summary:This paper presents an end-to-end neural network model, named NeuralGenerative Question Answering (GENQA), that can generate answers to simplefactoid questions, based on the facts in a knowledge-base. More specifically,the model is built on the encoder-decoder framework for sequence-to-sequencelearning, while equipped with the ability to enquire the knowledge-base, and istrained on a corpus of question-answer pairs, with their associated triples inthe knowledge-base. Empirical study shows the proposed model can effectivelydeal with the variations of questions and answers, and generate right andnatural answers by referring to the facts in the knowledge-base. The experimenton question answering demonstrates that the proposed model can outperform anembedding-based QA model as well as a neural dialogue model trained on the samedata.
arxiv-15000-87 | Locally Adaptive Translation for Knowledge Graph Embedding | http://arxiv.org/pdf/1512.01370v1.pdf | author:Yantao Jia, Yuanzhuo Wang, Hailun Lin, Xiaolong Jin, Xueqi Cheng category:cs.AI cs.CL I.2.4; I.2.6 published:2015-12-04 summary:Knowledge graph embedding aims to represent entities and relations in alarge-scale knowledge graph as elements in a continuous vector space. Existingmethods, e.g., TransE and TransH, learn embedding representation by defining aglobal margin-based loss function over the data. However, the optimal lossfunction is determined during experiments whose parameters are examined among aclosed set of candidates. Moreover, embeddings over two knowledge graphs withdifferent entities and relations share the same set of candidate lossfunctions, ignoring the locality of both graphs. This leads to the limitedperformance of embedding related applications. In this paper, we propose alocally adaptive translation method for knowledge graph embedding, calledTransA, to find the optimal loss function by adaptively determining its marginover different knowledge graphs. Experiments on two benchmark data setsdemonstrate the superiority of the proposed method, as compared tothe-state-of-the-art ones.
arxiv-15000-88 | Adjusting for Chance Clustering Comparison Measures | http://arxiv.org/pdf/1512.01286v1.pdf | author:Simone Romano, Nguyen Xuan Vinh, James Bailey, Karin Verspoor category:stat.ML published:2015-12-03 summary:Adjusted for chance measures are widely used to comparepartitions/clusterings of the same data set. In particular, the Adjusted RandIndex (ARI) based on pair-counting, and the Adjusted Mutual Information (AMI)based on Shannon information theory are very popular in the clusteringcommunity. Nonetheless it is an open problem as to what are the bestapplication scenarios for each measure and guidelines in the literature fortheir usage are sparse, with the result that users often resort to using both.Generalized Information Theoretic (IT) measures based on the Tsallis entropyhave been shown to link pair-counting and Shannon IT measures. In this paper,we aim to bridge the gap between adjustment of measures based on pair-countingand measures based on information theory. We solve the key technical challengeof analytically computing the expected value and variance of generalized ITmeasures. This allows us to propose adjustments of generalized IT measures,which reduce to well known adjusted clustering comparison measures as specialcases. Using the theory of generalized IT measures, we are able to propose thefollowing guidelines for using ARI and AMI as external validation indices: ARIshould be used when the reference clustering has large equal sized clusters;AMI should be used when the reference clustering is unbalanced and there existsmall clusters.
arxiv-15000-89 | Predicting the top and bottom ranks of billboard songs using Machine Learning | http://arxiv.org/pdf/1512.01283v1.pdf | author:Vivek Datla, Abhinav Vishnu category:cs.CL cs.LG published:2015-12-03 summary:The music industry is a $130 billion industry. Predicting whether a songcatches the pulse of the audience impacts the industry. In this paper weanalyze language inside the lyrics of the songs using several computationallinguistic algorithms and predict whether a song would make to the top orbottom of the billboard rankings based on the language features. We trained andtested an SVM classifier with a radial kernel function on the linguisticfeatures. Results indicate that we can classify whether a song belongs to topand bottom of the billboard charts with a precision of 0.76.
arxiv-15000-90 | MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems | http://arxiv.org/pdf/1512.01274v1.pdf | author:Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, Zheng Zhang category:cs.DC cs.LG cs.MS cs.NE published:2015-12-03 summary:MXNet is a multi-language machine learning (ML) library to ease thedevelopment of ML algorithms, especially for deep neural networks. Embedded inthe host language, it blends declarative symbolic expression with imperativetensor computation. It offers auto differentiation to derive gradients. MXNetis computation and memory efficient and runs on various heterogeneous systems,ranging from mobile devices to distributed GPU clusters. This paper describes both the API design and the system implementation ofMXNet, and explains how embedding of both symbolic expression and tensoroperation is handled in a unified fashion. Our preliminary experiments revealpromising results on large scale deep neural network applications usingmultiple GPU machines.
arxiv-15000-91 | CrossCat: A Fully Bayesian Nonparametric Method for Analyzing Heterogeneous, High Dimensional Data | http://arxiv.org/pdf/1512.01272v1.pdf | author:Vikash Mansinghka, Patrick Shafto, Eric Jonas, Cap Petschulat, Max Gasner, Joshua B. Tenenbaum category:cs.AI stat.CO stat.ML published:2015-12-03 summary:There is a widespread need for statistical methods that can analyzehigh-dimensional datasets with- out imposing restrictive or opaque modelingassumptions. This paper describes a domain-general data analysis method calledCrossCat. CrossCat infers multiple non-overlapping views of the data, eachconsisting of a subset of the variables, and uses a separate nonparametricmixture to model each view. CrossCat is based on approximately Bayesianinference in a hierarchical, nonparamet- ric model for data tables. This modelconsists of a Dirichlet process mixture over the columns of a data table inwhich each mixture component is itself an independent Dirichlet process mixtureover the rows; the inner mixture components are simple parametric models whoseform depends on the types of data in the table. CrossCat combines strengths ofmixture modeling and Bayesian net- work structure learning. Like mixturemodeling, CrossCat can model a broad class of distributions by positing latentvariables, and produces representations that can be efficiently conditioned andsampled from for prediction. Like Bayesian networks, CrossCat represents thedependencies and independencies between variables, and thus remains accuratewhen there are multiple statistical signals. Inference is done via a scalableGibbs sampling scheme; this paper shows that it works well in practice. Thispaper also includes empirical results on heterogeneous tabular data of up to 10million cells, such as hospital cost and quality measures, voting records,unemployment rates, gene expression measurements, and images of handwrittendigits. CrossCat infers structure that is consistent with accepted findings andcommon-sense knowledge in multiple domains and yields predictive accuracycompetitive with generative, discriminative, and model-free alternatives.
arxiv-15000-92 | Neural Enquirer: Learning to Query Tables with Natural Language | http://arxiv.org/pdf/1512.00965v2.pdf | author:Pengcheng Yin, Zhengdong Lu, Hang Li, Ben Kao category:cs.AI cs.CL cs.LG cs.NE published:2015-12-03 summary:We proposed Neural Enquirer as a neural network architecture to execute anatural language (NL) query on a knowledge-base (KB) for answers. Basically,Neural Enquirer finds the distributed representation of a query and thenexecutes it on knowledge-base tables to obtain the answer as one of the valuesin the tables. Unlike similar efforts in end-to-end training of semanticparsers, Neural Enquirer is fully "neuralized": it not only givesdistributional representation of the query and the knowledge-base, but alsorealizes the execution of compositional queries as a series of differentiableoperations, with intermediate results (consisting of annotations of the tablesat different levels) saved on multiple layers of memory. Neural Enquirer can betrained with gradient descent, with which not only the parameters of thecontrolling components and semantic parsing component, but also the embeddingsof the tables and query words can be learned from scratch. The training can bedone in an end-to-end fashion, but it can take stronger guidance, e.g., thestep-by-step supervision for complicated queries, and benefit from it. NeuralEnquirer is one step towards building neural network systems which seek tounderstand language by executing it on real-world. Our experiments show thatNeural Enquirer can learn to execute fairly complicated NL queries on tableswith rich structures.
arxiv-15000-93 | MERLiN: Mixture Effect Recovery in Linear Networks | http://arxiv.org/pdf/1512.01255v1.pdf | author:Sebastian Weichwald, Moritz Grosse-Wentrup, Arthur Gretton category:stat.ME q-bio.NC stat.AP stat.ML published:2015-12-03 summary:Causal inference concerns the identification of cause-effect relationshipsbetween variables. However, often only a linear combination of variablesconstitutes a meaningful causal variable. We propose to construct causalvariables from non-causal variables such that the resulting statisticalproperties guarantee meaningful cause-effect relationships. Exploiting thisnovel idea, MERLiN is able to recover a causal variable from an observed linearmixture that is an effect of another given variable. We illustrate how to adaptthe algorithm to a particular domain and how to incorporate a priori knowledge.Evaluation on both synthetic and experimental EEG data indicates MERLiN's powerto infer cause-effect relationships.
arxiv-15000-94 | Prototypical Priors: From Improving Classification to Zero-Shot Learning | http://arxiv.org/pdf/1512.01192v1.pdf | author:Saumya Jetley, Bernardino Romera-Paredes, Sadeep Jayasumana, Philip Torr category:cs.CV published:2015-12-03 summary:Recent works on zero-shot learning make use of side information such asvisual attributes or natural language semantics to define the relations betweenoutput visual classes and then use these relationships to draw inference on newunseen classes at test time. In a novel extension to this idea, we propose theuse of visual prototypical concepts as side information. For most real-worldvisual object categories, it may be difficult to establish a unique prototype.However, in cases such as traffic signs, brand logos, flags, and even naturallanguage characters, these prototypical templates are available and can beleveraged for an improved recognition performance. The present work proposes away to incorporate this prototypical information in a deep learning framework.Using prototypes as prior information, the deepnet pipeline learns the inputimage projections into the prototypical embedding space subject to minimizationof the final classification loss. Based on our experiments with two differentdatasets of traffic signs and brand logos, prototypical embeddings incorporatedin a conventional convolutional neural network improve the recognitionperformance. Recognition accuracy on the Belga logo dataset is especiallynoteworthy and establishes a new state-of-the-art. In zero-shot learningscenarios, the same system can be directly deployed to draw inference on unseenclasses by simply adding the prototypical information for these new classes attest time. Thus, unlike earlier approaches, testing on seen and unseen classesis handled using the same pipeline, and the system can be tuned for a trade-offof seen and unseen class performance as per task requirement. Comparison withone of the latest works in the zero-shot learning domain yields top results onthe two datasets mentioned above.
arxiv-15000-95 | Probabilistic Integration: A Role for Statisticians in Numerical Analysis? | http://arxiv.org/pdf/1512.00933v4.pdf | author:François-Xavier Briol, Chris. J. Oates, Mark Girolami, Michael A. Osborne, Dino Sejdinovic category:stat.ML cs.NA math.NA math.ST stat.CO stat.TH published:2015-12-03 summary:A research frontier has emerged in scientific computation, founded on theprinciple that numerical error entails epistemic uncertainty that ought to besubjected to statistical analysis. This viewpoint raises several interestingchallenges, including the design of statistical methods that enable thecoherent propagation of probabilities through a (possibly deterministic)computational pipeline. This paper examines thoroughly the case forprobabilistic numerical methods in statistical computation and a specific casestudy is presented for Markov chain and Quasi Monte Carlo methods. Aprobabilistic integrator is equipped with a full distribution over its output,providing a measure of epistemic uncertainty that is shown to be statisticallyvalid at finite computational levels, as well as in asymptotic regimes. Theapproach is motivated by expensive integration problems, where, as in krigging,one is willing to expend, at worst, cubic computational effort in order to gainuncertainty quantification. There, probabilistic integrators enjoy the "best ofboth worlds", leveraging the sampling efficiency of Monte Carlo methods whilstproviding a principled route to assessment of the impact of numerical error onscientific conclusions. Several substantial applications are provided forillustration and critical evaluation, including examples from statisticalmodelling, computer graphics and uncertainty quantification in oil reservoirmodelling.
arxiv-15000-96 | Deep Reinforcement Learning with Attention for Slate Markov Decision Processes with High-Dimensional States and Actions | http://arxiv.org/pdf/1512.01124v2.pdf | author:Peter Sunehag, Richard Evans, Gabriel Dulac-Arnold, Yori Zwols, Daniel Visentin, Ben Coppin category:cs.AI cs.HC cs.LG published:2015-12-03 summary:Many real-world problems come with action spaces represented as featurevectors. Although high-dimensional control is a largely unsolved problem, therehas recently been progress for modest dimensionalities. Here we report on asuccessful attempt at addressing problems of dimensionality as high as $2000$,of a particular form. Motivated by important applications such asrecommendation systems that do not fit the standard reinforcement learningframeworks, we introduce Slate Markov Decision Processes (slate-MDPs). ASlate-MDP is an MDP with a combinatorial action space consisting of slates(tuples) of primitive actions of which one is executed in an underlying MDP.The agent does not control the choice of this executed action and the actionmight not even be from the slate, e.g., for recommendation systems for whichall recommendations can be ignored. We use deep Q-learning based on featurerepresentations of both the state and action to learn the value of wholeslates. Unlike existing methods, we optimize for both the combinatorial andsequential aspects of our tasks. The new agent's superiority over agents thateither ignore the combinatorial or sequential long-term value aspect isdemonstrated on a range of environments with dynamics from a real-worldrecommendation system. Further, we use deep deterministic policy gradients tolearn a policy that for each position of the slate, guides attention towardsthe part of the action space in which the value is the highest and we onlyevaluate actions in this area. The attention is used within a sequentiallygreedy procedure leveraging submodularity. Finally, we show how introducingrisk-seeking can dramatically improve the agents performance and ability todiscover more far reaching strategies.
arxiv-15000-97 | Building Memory with Concept Learning Capabilities from Large-scale Knowledge Base | http://arxiv.org/pdf/1512.01173v1.pdf | author:Jiaxin Shi, Jun Zhu category:cs.CL cs.AI cs.LG published:2015-12-03 summary:We present a new perspective on neural knowledge base (KB) embeddings, fromwhich we build a framework that can model symbolic knowledge in the KB togetherwith its learning process. We show that this framework well regularizesprevious neural KB embedding model for superior performance in reasoning tasks,while having the capabilities of dealing with unseen entities, that is, tolearn their embeddings from natural language descriptions, which is very likehuman's behavior of learning semantic concepts.
arxiv-15000-98 | Triplet Spike Time Dependent Plasticity: A floating-gate Implementation | http://arxiv.org/pdf/1512.00961v3.pdf | author:Roshan Gopalakrishnan, Arindam Basu category:cs.NE published:2015-12-03 summary:Synapse plays an important role of learning in a neural network; the learningrules which modify the synaptic strength based on the timing difference betweenthe pre- and post-synaptic spike occurrence is termed as Spike Time DependentPlasticity (STDP). The most commonly used rule posits weight change based ontime difference between one pre- and one post spike and is hence termed doubletSTDP (DSTDP). However, D-STDP could not reproduce results of many biologicalexperiments; a triplet STDP (T-STDP) that considers triplets of spikes as thefundamental unit has been proposed recently to explain these observations. Thispaper describes the compact implementation of a synapse using singlefloating-gate (FG) transistor that can store a weight in a nonvolatile mannerand demonstrate the triplet STDP (T-STDP) learning rule by modifying drainvoltages according to triplets of spikes. We describe a mathematical procedureto obtain control voltages for the FG device for T-STDP and also showmeasurement results from a FG synapse fabricated in TSMC 0.35um CMOS process tosupport the theory. Possible VLSI implementation of drain voltage waveformgenerator circuits are also presented with simulation results.
arxiv-15000-99 | Kalman-based Stochastic Gradient Method with Stop Condition and Insensitivity to Conditioning | http://arxiv.org/pdf/1512.01139v1.pdf | author:Vivak Patel category:math.OC stat.CO stat.ML published:2015-12-03 summary:Proximal and stochastic gradient descent (SGD) methods are believed toefficiently minimize large composite objective functions, but such methods havetwo algorithmic challenges: (1) a lack of fast or justified stoppingconditions, and (2) sensitivity to the problem's conditioning. Second order SGDmethods show promise in solving these problems, but they are (3) marred by thecomplexity of their analysis. In this work, we address these three issues onthe limited, but important, linear regression problem by introducing andanalyzing a second order proximal/SGD method based on Kalman Filtering (kSGD).Through our analysis, we develop a fast algorithm with a justified stoppingcondition, prove that kSGD is insensitive to the problem's conditioning, anddevelop a unique approach for analyzing the complex second order dynamics. Ourtheoretical results are supported by numerical experiments on a large publicuse data set from the Center for Medicare and Medicaid. Byproducts of ouranalysis include, primarily, a foundation for extending kSGD to other problemtypes, parallel implementations with convergence guarantees and low memoryapplications, and, secondarily, extensions to Kalman Filtering theory.
arxiv-15000-100 | Target-Dependent Sentiment Classification with Long Short Term Memory | http://arxiv.org/pdf/1512.01100v1.pdf | author:Duyu Tang, Bing Qin, Xiaocheng Feng, Ting Liu category:cs.CL published:2015-12-03 summary:Target-dependent sentiment classification remains a challenge: modeling thesemantic relatedness of a target with its context words in a sentence.Different context words have different influences on determining the sentimentpolarity of a sentence towards the target. Therefore, it is desirable tointegrate the connections between target word and context words when building alearning system. In this paper, we develop two target dependent long short-termmemory (LSTM) models, where target information is automatically taken intoaccount. We evaluate our methods on a benchmark dataset from Twitter. Empiricalresults show that modeling sentence representation with standard LSTM does notperform well. Incorporating target information into LSTM can significantlyboost the classification accuracy. The target-dependent LSTM models achievestate-of-the-art performances without using syntactic parser or externalsentiment lexicons.
arxiv-15000-101 | Occlusion-Aware Human Pose Estimation with Mixtures of Sub-Trees | http://arxiv.org/pdf/1512.01055v1.pdf | author:Ibrahim Radwan, Abhinav Dhall, Roland Goecke category:cs.CV published:2015-12-03 summary:In this paper, we study the problem of learning a model for human poseestimation as mixtures of compositional sub-trees in two layers of prediction.This involves estimating the pose of a sub-tree followed by identifying therelationships between sub-trees that are used to handle occlusions betweendifferent parts. The mixtures of the sub-trees are learnt utilising bothgeometric and appearance distances. The Chow-Liu (CL) algorithm is recursivelyapplied to determine the inter-relations between the nodes and to build thestructure of the sub-trees. These structures are used to learn the latentparameters of the sub-trees and the inference is done using a standard beliefpropagation technique. The proposed method handles occlusions during theinference process by identifying overlapping regions between differentsub-trees and introducing a penalty term for overlapping parts. Experiments areperformed on three different datasets: the Leeds Sports, Image Parse and UIUCPeople datasets. The results show the robustness of the proposed method toocclusions over the state-of-the-art approaches.
arxiv-15000-102 | Approaches for Sentiment Analysis on Twitter: A State-of-Art study | http://arxiv.org/pdf/1512.01043v1.pdf | author:Harsh Thakkar, Dhiren Patel category:cs.SI cs.CL cs.IR published:2015-12-03 summary:Microbloging is an extremely prevalent broadcast medium amidst the Internetfraternity these days. People share their opinions and sentiments about varietyof subjects like products, news, institutions, etc., every day on microblogingwebsites. Sentiment analysis plays a key role in prediction systems, opinionmining systems, etc. Twitter, one of the microbloging platforms allows a limitof 140 characters to its users. This restriction stimulates users to be veryconcise about their opinion and twitter an ocean of sentiments to analyze.Twitter also provides developer friendly streaming API for data retrievalpurpose allowing the analyst to search real time tweets from various users. Inthis paper, we discuss the state-of-art of the works which are focused onTwitter, the online social network platform, for sentiment analysis. We surveyvarious lexical, machine learning and hybrid approaches for sentiment analysison Twitter.
arxiv-15000-103 | Simulations for Validation of Vision Systems | http://arxiv.org/pdf/1512.01030v1.pdf | author:V S R Veeravasarapu, Rudra Narayan Hota, Constantin Rothkopf, Ramesh Visvanathan category:cs.CV published:2015-12-03 summary:As the computer vision matures into a systems science and engineeringdiscipline, there is a trend in leveraging latest advances in computer graphicssimulations for performance evaluation, learning, and inference. However, thereis an open question on the utility of graphics simulations for vision withapparently contradicting views in the literature. In this paper, we place theresults from the recent literature in the context of performancecharacterization methodology outlined in the 90's and note that insightsderived from simulations can be qualitative or quantitative depending on thedegree of fidelity of models used in simulation and the nature of the questionposed by the experimenter. We describe a simulation platform that incorporateslatest graphics advances and use it for systematic performance characterizationand trade-off analysis for vision system design. We verify the utility of theplatform in a case study of validating a generative model inspired visionhypothesis, Rank-Order consistency model, in the contexts of global and localillumination changes, and bad weather, and high-frequency noise. Our approachestablishes the link between alternative viewpoints, involving models withphysics based semantics and signal and perturbation semantics and confirmsinsights in literature on robust change detection.
arxiv-15000-104 | Mean-Field Inference in Gaussian Restricted Boltzmann Machine | http://arxiv.org/pdf/1512.00927v2.pdf | author:Chako Takahashi, Muneki Yasuda category:stat.ML published:2015-12-03 summary:A Gaussian restricted Boltzmann machine (GRBM) is a Boltzmann machine definedon a bipartite graph and is an extension of usual restricted Boltzmannmachines. A GRBM consists of two different layers: a visible layer composed ofcontinuous visible variables and a hidden layer composed of discrete hiddenvariables. In this paper, we derive two different inference algorithms forGRBMs based on the naive mean-field approximation (NMFA). One is an inferencealgorithm for whole variables in a GRBM, and the other is an inferencealgorithm for partial variables in a GBRBM. We compare the two methodsanalytically and numerically and show that the latter method is better.
arxiv-15000-105 | Bayesian Matrix Completion via Adaptive Relaxed Spectral Regularization | http://arxiv.org/pdf/1512.01110v2.pdf | author:Yang Song, Jun Zhu category:cs.NA cs.AI cs.LG published:2015-12-03 summary:Bayesian matrix completion has been studied based on a low-rank matrixfactorization formulation with promising results. However, little work has beendone on Bayesian matrix completion based on the more direct spectralregularization formulation. We fill this gap by presenting a novel Bayesianmatrix completion method based on spectral regularization. In order tocircumvent the difficulties of dealing with the orthonormality constraints ofsingular vectors, we derive a new equivalent form with relaxed constraints,which then leads us to design an adaptive version of spectral regularizationfeasible for Bayesian inference. Our Bayesian method requires no parametertuning and can infer the number of latent factors automatically. Experiments onsynthetic and real datasets demonstrate encouraging results on rank recoveryand collaborative filtering, with notably good results for very sparsematrices.
arxiv-15000-106 | Weighted Schatten $p$-Norm Minimization for Image Denoising and Background Subtraction | http://arxiv.org/pdf/1512.01003v1.pdf | author:Yuan Xie, Shuhang Gu, Yan Liu, Wangmeng Zuo, Wensheng Zhang, Lei Zhang category:cs.CV published:2015-12-03 summary:Low rank matrix approximation (LRMA), which aims to recover the underlyinglow rank matrix from its degraded observation, has a wide range of applicationsin computer vision. The latest LRMA methods resort to using the nuclear normminimization (NNM) as a convex relaxation of the nonconvex rank minimization.However, NNM tends to over-shrink the rank components and treats the differentrank components equally, limiting its flexibility in practical applications. Wepropose a more flexible model, namely the Weighted Schatten $p$-NormMinimization (WSNM), to generalize the NNM to the Schatten $p$-normminimization with weights assigned to different singular values. The proposedWSNM not only gives better approximation to the original low-rank assumption,but also considers the importance of different rank components. We analyze thesolution of WSNM and prove that, under certain weights permutation, WSNM can beequivalently transformed into independent non-convex $l_p$-norm subproblems,whose global optimum can be efficiently solved by generalized iteratedshrinkage algorithm. We apply WSNM to typical low-level vision problems, e.g.,image denoising and background subtraction. Extensive experimental resultsshow, both qualitatively and quantitatively, that the proposed WSNM can moreeffectively remove noise, and model complex and dynamic scenes compared withstate-of-the-art methods.
arxiv-15000-107 | Bag Reference Vector for Multi-instance Learning | http://arxiv.org/pdf/1512.00994v1.pdf | author:Hanqiang Song, Zhuotun Zhu, Xinggang Wang category:stat.ML cs.LG published:2015-12-03 summary:Multi-instance learning (MIL) has a wide range of applications due to itsdistinctive characteristics. Although many state-of-the-art algorithms haveachieved decent performances, a plurality of existing methods solve the problemonly in instance level rather than excavating relations among bags. In thispaper, we propose an efficient algorithm to describe each bag by acorresponding feature vector via comparing it with other bags. In other words,the crucial information of a bag is extracted from the similarity between thatbag and other reference bags. In addition, we apply extensions of Hausdorffdistance to representing the similarity, to a certain extent, overcoming thekey challenge of MIL problem, the ambiguity of instances' labels in positivebags. Experimental results on benchmarks and text categorization tasks showthat the proposed method outperforms the previous state-of-the-art by a largemargin.
arxiv-15000-108 | Fast Low-Rank Matrix Learning with Nonconvex Regularization | http://arxiv.org/pdf/1512.00984v1.pdf | author:Quanming Yao, James T. Kwok, Wenliang Zhong category:cs.NA cs.LG stat.ML published:2015-12-03 summary:Low-rank modeling has a lot of important applications in machine learning,computer vision and social network analysis. While the matrix rank is oftenapproximated by the convex nuclear norm, the use of nonconvex low-rankregularizers has demonstrated better recovery performance. However, theresultant optimization problem is much more challenging. A very recentstate-of-the-art is based on the proximal gradient algorithm. However, itrequires an expensive full SVD in each proximal step. In this paper, we showthat for many commonly-used nonconvex low-rank regularizers, a cutoff can bederived to automatically threshold the singular values obtained from theproximal operator. This allows the use of power method to approximate the SVDefficiently. Besides, the proximal operator can be reduced to that of a muchsmaller matrix projected onto this leading subspace. Convergence, with a rateof O(1/T) where T is the number of iterations, can be guaranteed. Extensiveexperiments are performed on matrix completion and robust principal componentanalysis. The proposed method achieves significant speedup over thestate-of-the-art. Moreover, the matrix solution obtained is more accurate andhas a lower rank than that of the traditional nuclear norm regularizer.
arxiv-15000-109 | A New Statistical Framework for Genetic Pleiotropic Analysis of High Dimensional Phenotype Data | http://arxiv.org/pdf/1512.00947v1.pdf | author:Panpan Wang, Mohammad Rahman, Li Jin, Momiao Xiong category:stat.ML q-bio.GN stat.ME published:2015-12-03 summary:The widely used genetic pleiotropic analysis of multiple phenotypes are oftendesigned for examining the relationship between common variants and a fewphenotypes. They are not suited for both high dimensional phenotypes and highdimensional genotype (next-generation sequencing) data. To overcome theselimitations, we develop sparse structural equation models (SEMs) as a generalframework for a new paradigm of genetic analysis of multiple phenotypes. Toincorporate both common and rare variants into the analysis, we extend thetraditional multivariate SEMs to sparse functional SEMs. To deal with highdimensional phenotype and genotype data, we employ functional data analysis andthe alternative direction methods of multiplier (ADMM) techniques to reducedata dimension and improve computational efficiency. Using large scalesimulations we showed that the proposed methods have higher power to detecttrue causal genetic pleiotropic structure than other existing methods.Simulations also demonstrate that the gene-based pleiotropic analysis hashigher power than the single variant-based pleiotropic analysis. The proposedmethod is applied to exome sequence data from the NHLBI Exome SequencingProject (ESP) with 11 phenotypes, which identifies a network with 137 genesconnected to 11 phenotypes and 341 edges. Among them, 114 genes showedpleiotropic genetic effects and 45 genes were reported to be associated withphenotypes in the analysis or other cardiovascular disease (CVD) relatedphenotypes in the literature.
arxiv-15000-110 | A Literature Survey of various Fingerprint De-noising Techniques to justify the need of a new De-noising model based upon Pixel Component Analysis | http://arxiv.org/pdf/1512.00939v1.pdf | author:Siddharth Choubey, Deepika Banchhor category:cs.CV published:2015-12-03 summary:Image Preprocessing is a vital step in the field of image processing forbiometric pattern recognition. This paper studies and reviews various classicaland modern fingerprint image de-noising models. The various model used forde-noising ranges widely from transform matrix using frequency, histogram modelde-noising, de-noising by introducing Gabor filter and its types to enhancefingerprint images. The output efficiency of various de-noising model proposed earlier iscalculated on the basis of SNR (signal to noise ratio) and MSE (mean squareerror rate). Our simulated experimental results indicates that incorporatingthe de-noising model based on Gabor filter inside domain of wavelet ranges withcomposite method only betters MSE (Mean Square Error). Improved MSE withoutsignificant improvement in SNR improves the fingerprint images only by a littlemargin which is non-optimal in nature. Thus the objective of this researchpaper is to build an optimal de-noising model for fingerprint images so thatits usage in biometric authentication can be more robust in nature.
arxiv-15000-111 | The Indian Spontaneous Expression Database for Emotion Recognition | http://arxiv.org/pdf/1512.00932v1.pdf | author:S L Happy, Priyadarshi Patnaik, Aurobinda Routray, Rajlakshmi Guha category:cs.CV published:2015-12-03 summary:Automatic recognition of spontaneous facial expressions is a major challengein the field of affective computing. Head rotation, face pose, illuminationvariation, occlusion etc. are the attributes that increase the complexity ofrecognition of spontaneous expressions in practical applications. Effectiverecognition of expressions depends significantly on the quality of the databaseused. Most well-known facial expression databases consist of posed expressions.However, currently there is a huge demand for spontaneous expression databasesfor the pragmatic implementation of the facial expression recognitionalgorithms. In this paper, we propose and establish a new facial expressiondatabase containing spontaneous expressions of both male and femaleparticipants of Indian origin. The database consists of 428 segmented videoclips of the spontaneous facial expressions of 50 participants. In ourexperiment, emotions were induced among the participants by using emotionalvideos and simultaneously their self-ratings were collected for eachexperienced emotion. Facial expression clips were annotated carefully by fourtrained decoders, which were further validated by the nature of stimuli usedand self-report of emotions. An extensive analysis was carried out on thedatabase using several machine learning algorithms and the results are providedfor future reference. Such a spontaneous database will help in the developmentand validation of algorithms for recognition of spontaneous expressions.
arxiv-15000-112 | Innovation Pursuit: A New Approach to Subspace Clustering | http://arxiv.org/pdf/1512.00907v1.pdf | author:Mostafa Rahmani, George Atia category:cs.CV cs.IR cs.IT cs.LG math.IT stat.ML published:2015-12-02 summary:In subspace clustering, a group of data points belonging to a union ofsubspaces are assigned membership to their respective subspaces. This paperpresents a new approach dubbed Innovation Pursuit (iPursuit) to the problem ofsubspace clustering using a new geometrical idea whereby each subspace isidentified based on its novelty with respect to the other subspaces. Theproposed approach finds the subspaces consecutively by solving a series ofsimple linear optimization problems, each searching for some direction in thespan of the data that is potentially orthogonal to all subspaces except for theone to be identified in one step of the algorithm. A detailed mathematicalanalysis is provided establishing sufficient conditions for the proposedapproach to correctly cluster the data points. Remarkably, the proposedapproach can provably yield exact clustering even when the subspaces havesignificant intersections under mild conditions on the distribution of the datapoints in the subspaces. Moreover, It is shown that the complexity of iPursuitis almost independent of the dimension of the data. The numerical simulationsdemonstrate that iPursuit can often outperform the state-of-the-art subspaceclustering algorithms, more so for subspaces with significant intersections.
arxiv-15000-113 | Compressive hyperspectral imaging via adaptive sampling and dictionary learning | http://arxiv.org/pdf/1512.00901v1.pdf | author:Mingrui Yang, Frank de Hoog, Yuqi Fan, Wen Hu category:cs.CV published:2015-12-02 summary:In this paper, we propose a new sampling strategy for hyperspectral signalsthat is based on dictionary learning and singular value decomposition (SVD).Specifically, we first learn a sparsifying dictionary from training spectraldata using dictionary learning. We then perform an SVD on the dictionary anduse the first few left singular vectors as the rows of the measurement matrixto obtain the compressive measurements for reconstruction. The proposed methodprovides significant improvement over the conventional compressive sensingapproaches. The reconstruction performance is further improved byreconditioning the sensing matrix using matrix balancing. We also demonstratethat the combination of dictionary learning and SVD is robust by applying themto different datasets.
arxiv-15000-114 | Cleaning Schedule Optimization of Heat Exchanger Networks Using Particle Swarm Optimization | http://arxiv.org/pdf/1512.00883v1.pdf | author:Totok R. Biyanto, Sumitra Wira Suganda, Matraji, Yerry Susatio, Heri Justiono, Sarwono category:cs.NE published:2015-12-02 summary:Oil refinery is one of industries that require huge energy consumption. Thetoday technology advance requires energy saving. Heat integration is a methodused to minimize the energy comsumption though the implementation of HeatExchanger Network (HEN). CPT is one of types of Heat Exchanger Network (HEN)that functions to recover the heat in the flow of product or waste. HENcomprises a number of heat exchangers (HEs) that are serially connected.However, the presence of fouling in the heat exchanger has caused the declineof the performance of both heat exchangers and all heat exchanger networks.Fouling can not be avoided. However, it can be mitigated. In industry, periodicheat exchanger cleaning is the most effective and widely used mitigationtechnique. On the other side, a very frequent cleaning of heat exchanger can bemuch costly in maintenance and lost of production. In this way, an accurateoptimization technique of cleaning schedule interval of heat exchanger is veryessential. Commonly, this technique involves three elements: model to simulatethe heat exchanger network, representative fouling model to describe thefouling behavior and suitable optimization algorithm to solve the problem ofclening schedule interval for heat exchanger network. This paper describe theoptimization of interval cleaning schedule of HEN within the 44-month periodusing PSO (particle swarm optimization). The number of iteration used toachieve the convergent is 100 iterations and the fitness value in PSOcorrelated with the amount of heat recovery, cleaning cost, and additionalpumping cost. The saving after the optimization of cleaning schedule of HEN inthis research achieved at $ 1.236 millions or 23% of maximum potential savings.
arxiv-15000-115 | Optimal whitening and decorrelation | http://arxiv.org/pdf/1512.00809v1.pdf | author:Agnan Kessy, Alex Lewin, Korbinian Strimmer category:stat.ME stat.ML published:2015-12-02 summary:Whitening, or sphering, is a common preprocessing step in statisticalanalysis to transform random variables to orthogonality. However, due torotational freedom there are infinitely many possible whitening procedures.Consequently, there is a diverse range of sphering methods in use, for examplebased on principal component analysis, Cholesky matrix decomposition andMahalanobis transformation, among others. Here we provide an overview of the underlying theory and discuss five naturalwhitening procedures. Subsequently, we demonstrate that investigating thecross-covariance and the cross-correlation matrix between sphered and originalvariables allows to break the rotational invariance of whitening and toidentify optimal transformations. As a result we recommended two particularwhitening approaches: CAT-CAR whitening to produce sphered variables that aremaximally similar to the original variables, and PCA-whitening based on thecorrelation matrix to obtain maximally compressed whitened variables.
arxiv-15000-116 | Actions ~ Transformations | http://arxiv.org/pdf/1512.00795v1.pdf | author:Xiaolong Wang, Ali Farhadi, Abhinav Gupta category:cs.CV published:2015-12-02 summary:What defines an action like "kicking ball"? We argue that the true meaning ofan action lies in the change or transformation an action brings to theenvironment. In this paper, we propose a novel representation for actions bymodeling action as a transformation which changes the state of the environmentbefore the action happens (precondition) to the state after the action(effect). Motivated by the recent advancement of video representation usingdeep learning, we design a Siamese network which models the action as thetransformation on a high-level feature space. We show that our model givesimprovements on standard action recognition datasets including UCF101 andHMDB51. More importantly, our approach is able to generalize beyond learnedaction categories and shows significant performance improvement oncross-category generalization on our new ACT dataset.
arxiv-15000-117 | Microclustering: When the Cluster Sizes Grow Sublinearly with the Size of the Data Set | http://arxiv.org/pdf/1512.00792v1.pdf | author:Jeffrey Miller, Brenda Betancourt, Abbas Zaidi, Hanna Wallach, Rebecca C. Steorts category:stat.ME stat.AP stat.CO stat.ML published:2015-12-02 summary:Most generative models for clustering implicitly assume that the number ofdata points in each cluster grows linearly with the total number of datapoints. Finite mixture models, Dirichlet process mixture models, andPitman--Yor process mixture models make this assumption, as do all otherinfinitely exchangeable clustering models. However, for some tasks, thisassumption is undesirable. For example, when performing entity resolution, thesize of each cluster is often unrelated to the size of the data set.Consequently, each cluster contains a negligible fraction of the total numberof data points. Such tasks therefore require models that yield clusters whosesizes grow sublinearly with the size of the data set. We address thisrequirement by defining the \emph{microclustering property} and introducing anew model that exhibits this property. We compare this model to severalcommonly used clustering models by checking model fit using real and simulateddata sets.
arxiv-15000-118 | The MegaFace Benchmark: 1 Million Faces for Recognition at Scale | http://arxiv.org/pdf/1512.00596v1.pdf | author:Ira Kemelmacher-Shlizerman, Steve Seitz, Daniel Miller, Evan Brossard category:cs.CV published:2015-12-02 summary:Recent face recognition experiments on a major benchmark LFW show stunningperformance--a number of algorithms achieve near to perfect score, surpassinghuman recognition rates. In this paper, we advocate evaluations at the millionscale (LFW includes only 13K photos of 5K people). To this end, we haveassembled the MegaFace dataset and created the first MegaFace challenge. Ourdataset includes One Million photos that capture more than 690K differentindividuals. The challenge evaluates performance of algorithms with increasingnumbers of distractors (going from 10 to 1M) in the gallery set. We presentboth identification and verification performance, evaluate performance withrespect to pose and a person's age, and compare as a function of training datasize (number of photos and people). We report results of state of the art andbaseline algorithms. Our key observations are that testing at the million scalereveals big performance differences (of algorithms that perform similarly wellon smaller scale) and that age invariant recognition as well as pose are stillchallenging for most. The MegaFace dataset, baseline code, and evaluationscripts, are all publicly released for further experimentations at:megaface.cs.washington.edu.
arxiv-15000-119 | Klasifikasi Komponen Argumen Secara Otomatis pada Dokumen Teks berbentuk Esai Argumentatif | http://arxiv.org/pdf/1512.00578v1.pdf | author:Derwin Suhartono category:cs.CL cs.IR published:2015-12-02 summary:By automatically recognize argument component, essay writers can do someinspections to texts that they have written. It will assist essay scoringprocess objectively and precisely because essay grader is able to see how wellthe argument components are constructed. Some reseachers have tried to doargument detection and classification along with its implementation in somedomains. The common approach is by doing feature extraction to the text.Generally, the features are structural, lexical, syntactic, indicator, andcontextual. In this research, we add new feature to the existing features. Itadopts keywords list by Knott and Dale (1993). The experiment result shows theargument classification achieves 72.45% accuracy. Moreover, we still get thesame accuracy without the keyword lists. This concludes that the keyword listsdo not affect significantly to the features. All features are still weak toclassify major claim and claim, so we need other features which are useful todifferentiate those two kind of argument components.
arxiv-15000-120 | Probabilistic Latent Semantic Analysis (PLSA) untuk Klasifikasi Dokumen Teks Berbahasa Indonesia | http://arxiv.org/pdf/1512.00576v1.pdf | author:Derwin Suhartono category:cs.CL cs.IR published:2015-12-02 summary:One task that is included in managing documents is how to find substantialinformation inside. Topic modeling is a technique that has been developed toproduce document representation in form of keywords. The keywords will be usedin the indexing process and document retrieval as needed by users. In thisresearch, we will discuss specifically about Probabilistic Latent SemanticAnalysis (PLSA). It will cover PLSA mechanism which involves ExpectationMaximization (EM) as the training algorithm, how to conduct testing, and obtainthe accuracy result.
arxiv-15000-121 | Learning Semantic Similarity for Very Short Texts | http://arxiv.org/pdf/1512.00765v1.pdf | author:Cedric De Boom, Steven Van Canneyt, Steven Bohez, Thomas Demeester, Bart Dhoedt category:cs.IR cs.CL published:2015-12-02 summary:Levering data on social media, such as Twitter and Facebook, requiresinformation retrieval algorithms to become able to relate very short textfragments to each other. Traditional text similarity methods such as tf-idfcosine-similarity, based on word overlap, mostly fail to produce good resultsin this case, since word overlap is little or non-existent. Recently,distributed word representations, or word embeddings, have been shown tosuccessfully allow words to match on the semantic level. In order to pair shorttext fragments - as a concatenation of separate words - an adequate distributedsentence representation is needed, in existing literature often obtained bynaively combining the individual word representations. We thereforeinvestigated several text representations as a combination of word embeddingsin the context of semantic pair matching. This paper investigates theeffectiveness of several such naive techniques, as well as traditional tf-idfsimilarity, for fragments of different lengths. Our main contribution is afirst step towards a hybrid method that combines the strength of densedistributed representations - as opposed to sparse term matching - with thestrength of tf-idf based methods to automatically reduce the impact of lessinformative terms. Our new approach outperforms the existing techniques in atoy experimental set-up, leading to the conclusion that the combination of wordembeddings and tf-idf information might lead to a better model for semanticcontent within very short text fragments.
arxiv-15000-122 | Active Learning for Delineation of Curvilinear Structures | http://arxiv.org/pdf/1512.00747v1.pdf | author:Agata Mosinska, Raphael Sznitman, Przemysław Głowacki, Pascal Fua category:cs.CV published:2015-12-02 summary:Many recent delineation techniques owe much of their increased effectivenessto path classification algorithms that make it possible to distinguishpromising paths from others. The downside of this development is that theyrequire annotated training data, which is tedious to produce. In this paper, we propose an Active Learning approach that considerablyspeeds up the annotation process. Unlike standard ones, it takes advantage ofthe specificities of the delineation problem. It operates on a graph and canreduce the training set size by up to 80% without compromising thereconstruction quality. We will show that our approach outperforms conventional ones on variousbiomedical and natural image datasets, thus showing that it is broadlyapplicable.
arxiv-15000-123 | Recognizing Semantic Features in Faces using Deep Learning | http://arxiv.org/pdf/1512.00743v1.pdf | author:Amogh Gudi category:cs.LG cs.CV stat.ML published:2015-12-02 summary:The human face constantly conveys information, both consciously andsubconsciously. However, as basic as it is for humans to visually interpretthis information, it is quite a big challenge for machines. Conventionalsemantic facial feature recognition and analysis techniques are already in useand are based on physiological heuristics, but they suffer from lack ofrobustness and high computation time. This thesis aims to explore ways formachines to learn to interpret semantic information available in faces in anautomated manner without requiring manual design of feature detectors, usingthe approach of Deep Learning. This thesis provides a study of the effects ofvarious factors and hyper-parameters of deep neural networks in the process ofdetermining an optimal network configuration for the task of semantic facialfeature recognition. This thesis explores the effectiveness of the system torecognize the various semantic features (like emotions, age, gender, ethnicityetc.) present in faces. Furthermore, the relation between the effect ofhigh-level concepts on low level features is explored through an analysis ofthe similarities in low-level descriptors of different semantic features. Thisthesis also demonstrates a novel idea of using a deep network to generate 3-DActive Appearance Models of faces from real-world 2-D images.
arxiv-15000-124 | Annotating Character Relationships in Literary Texts | http://arxiv.org/pdf/1512.00728v1.pdf | author:Philip Massey, Patrick Xia, David Bamman, Noah A. Smith category:cs.CL published:2015-12-02 summary:We present a dataset of manually annotated relationships between charactersin literary texts, in order to support the training and evaluation of automaticmethods for relation type prediction in this domain (Makazhanov et al., 2014;Kokkinakis, 2013) and the broader computational analysis of literary character(Elson et al., 2010; Bamman et al., 2014; Vala et al., 2015; Flekova andGurevych, 2015). In this work, we solicit annotations from workers on AmazonMechanical Turk for 109 texts ranging from Homer's _Iliad_ to Joyce's _Ulysses_on four dimensions of interest: for a given pair of characters, we collectjudgments as to the coarse-grained category (professional, social, familial),fine-grained category (friend, lover, parent, rival, employer), and affinity(positive, negative, neutral) that describes their primary relationship in atext. We do not assume that this relationship is static; we also collectjudgments as to whether it changes at any point in the course of the text.
arxiv-15000-125 | MMSE Estimation for Poisson Noise Removal in Images | http://arxiv.org/pdf/1512.00717v1.pdf | author:Stanislav Pyatykh, Jürgen Hesser category:cs.CV cs.DS published:2015-12-02 summary:Poisson noise suppression is an important preprocessing step in severalapplications, such as medical imaging, microscopy, and astronomical imaging. Inthis work, we propose a novel patch-wise Poisson noise removal strategy, inwhich the MMSE estimator is utilized in order to produce the denoising resultfor each image patch. Fast and accurate computation of the MMSE estimator iscarried out using k-d tree search followed by search in the K-nearest neighborgraph. Our experiments show that the proposed method is the preferable choicefor low signal-to-noise ratios.
arxiv-15000-126 | Duelist Algorithm: An Algorithm Inspired by How Duelist Improve Their Capabilities in a Duel | http://arxiv.org/pdf/1512.00708v1.pdf | author:Totok Ruki Biyanto, Henokh Yernias Fibrianto, Gunawan Nugroho, Erny Listijorini, Titik Budiati, Hairul Huda category:cs.NE published:2015-12-02 summary:This paper proposes an optimization algorithm based on how human fight andlearn from each duelist. Since this algorithm is based on population, theproposed algorithm starts with an initial set of duelists. The duel is todetermine the winner and loser. The loser learns from the winner, while thewinner try their new skill or technique that may improve their fightingcapabilities. A few duelists with highest fighting capabilities are called aschampion. The champion train a new duelists such as their capabilities. The newduelist will join the tournament as a representative of each champion. Allduelist are re-evaluated, and the duelists with worst fighting capabilities iseliminated to maintain the amount of duelists. Two optimization problem isapplied for the proposed algorithm, together with genetic algorithm, particleswarm optimization and imperialist competitive algorithm. The results show thatthe proposed algorithm is able to find the better global optimum and fasteriteration.
arxiv-15000-127 | Hybrid Approach for Inductive Semi Supervised Learning using Label Propagation and Support Vector Machine | http://arxiv.org/pdf/1512.01568v1.pdf | author:Aruna Govada, Pravin Joshi, Sahil Mittal, Sanjay K Sahay category:cs.LG cs.DC published:2015-12-02 summary:Semi supervised learning methods have gained importance in today's worldbecause of large expenses and time involved in labeling the unlabeled data byhuman experts. The proposed hybrid approach uses SVM and Label Propagation tolabel the unlabeled data. In the process, at each step SVM is trained tominimize the error and thus improve the prediction quality. Experiments areconducted by using SVM and logistic regression(Logreg). Results prove that SVMperforms tremendously better than Logreg. The approach is tested using 12datasets of different sizes ranging from the order of 1000s to the order of10000s. Results show that the proposed approach outperforms Label Propagationby a large margin with F-measure of almost twice on average. The parallelversion of the proposed approach is also designed and implemented, the analysisshows that the training time decreases significantly when parallel version isused.
arxiv-15000-128 | Double Sparse Multi-Frame Image Super Resolution | http://arxiv.org/pdf/1512.00607v1.pdf | author:Toshiyuki Kato, Hideitsu Hino, Noboru Murata category:cs.CV published:2015-12-02 summary:A large number of image super resolution algorithms based on the sparsecoding are proposed, and some algorithms realize the multi-frame superresolution. In multi-frame super resolution based on the sparse coding, bothaccurate image registration and sparse coding are required. Previous study onmulti-frame super resolution based on sparse coding firstly apply blockmatching for image registration, followed by sparse coding to enhance the imageresolution. In this paper, these two problems are solved by optimizing a singleobjective function. The results of numerical experiments support theeffectiveness of the proposed approch.
arxiv-15000-129 | Object-based World Modeling in Semi-Static Environments with Dependent Dirichlet-Process Mixtures | http://arxiv.org/pdf/1512.00573v1.pdf | author:Lawson L. S. Wong, Thanard Kurutach, Leslie Pack Kaelbling, Tomás Lozano-Pérez category:cs.AI cs.LG cs.RO published:2015-12-02 summary:To accomplish tasks in human-centric indoor environments, robots need torepresent and understand the world in terms of objects and their attributes. Werefer to this attribute-based representation as a world model, and consider howto acquire it via noisy perception and maintain it over time, as objects areadded, changed, and removed in the world. Previous work has framed this asmultiple-target tracking problem, where objects are potentially in motion atall times. Although this approach is general, it is computationally expensive.We argue that such generality is not needed in typical world modeling tasks,where objects only change state occasionally. More efficient approaches areenabled by restricting ourselves to such semi-static environments. We consider a previously-proposed clustering-based world modeling approachthat assumed static environments, and extend it to semi-static domains byapplying a dependent Dirichlet-process (DDP) mixture model. We derive a novelMAP inference algorithm under this model, subject to data associationconstraints. We demonstrate our approach improves computational performance insemi-static environments.
arxiv-15000-130 | Centroid Based Binary Tree Structured SVM for Multi Classification | http://arxiv.org/pdf/1512.00659v1.pdf | author:Aruna Govada, Bhavul Gauri, S. K. Sahay category:cs.LG published:2015-12-02 summary:Support Vector Machines (SVMs) were primarily designed for 2-classclassification. But they have been extended for N-class classification alsobased on the requirement of multiclasses in the practical applications.Although N-class classification using SVM has considerable research attention,getting minimum number of classifiers at the time of training and testing isstill a continuing research. We propose a new algorithm CBTS-SVM (Centroidbased Binary Tree Structured SVM) which addresses this issue. In this we builda binary tree of SVM models based on the similarity of the class labels byfinding their distance from the corresponding centroids at the root level. Theexperimental results demonstrates the comparable accuracy for CBTS with OVOwith reasonable gamma and cost values. On the other hand when CBTS is comparedwith OVA, it gives the better accuracy with reduced training time and testingtime. Furthermore CBTS is also scalable as it is able to handle the large datasets.
arxiv-15000-131 | Attribute2Image: Conditional Image Generation from Visual Attributes | http://arxiv.org/pdf/1512.00570v1.pdf | author:Xinchen Yan, Jimei Yang, Kihyuk Sohn, Honglak Lee category:cs.LG cs.AI cs.CV published:2015-12-02 summary:This paper investigates a problem of generating images from visualattributes. Given the prevalent research for image recognition, the conditionalimage generation problem is relatively under-explored due to the challenges oflearning a good generative model and handling rendering uncertainties inimages. To address this, we propose a variety of attribute-conditioned deepvariational auto-encoders that enjoy both effective representation learning andBayesian modeling, from which images can be generated from specified attributesand sampled latent factors. We experiment with natural face images anddemonstrate that the proposed models are capable of generating realistic faceswith diverse appearance. We further evaluate the proposed models by performingattribute-conditioned image progression, transfer and retrieval. In particular,our generation method achieves superior performance in the retrieval experimentagainst traditional nearest-neighbor-based methods both qualitatively andquantitatively.
arxiv-15000-132 | Continuous and Simultaneous Gesture and Posture Recognition for Commanding a Robotic Wheelchair; Towards Spotting the Signal Patterns | http://arxiv.org/pdf/1512.00622v1.pdf | author:Ali Boyali, Naohisa Hashimoto, Manolya Kavakli category:cs.RO cs.CV published:2015-12-02 summary:Spotting signal patterns with varying lengths has been still an open problemin the literature. In this study, we describe a signal pattern recognitionapproach for continuous and simultaneous classification of a tracked hand'sposture and gestures and map them to steering commands for control of a roboticwheelchair. The developed methodology not only affords 100\% recognitionaccuracy on a streaming signal for continuous recognition, but also bringsabout a new perspective for building a training dictionary which eliminateshuman intervention to spot the gesture or postures on a training signal. In thetraining phase we employ a state of art subspace clustering method to find themost representative state samples. The recognition and training frameworkreveal boundaries of the patterns on the streaming signal with a successivedecision tree structure intrinsically. We make use of the Collaborative ansBlock Sparse Representation based classification methods for continuous gestureand posture recognition.
arxiv-15000-133 | Benchmarking sentiment analysis methods for large-scale texts: A case for using continuum-scored words and word shift graphs | http://arxiv.org/pdf/1512.00531v1.pdf | author:Andrew Reagan, Brian Tivnan, Jake Ryland Williams, Christopher M. Danforth, Peter Sheridan Dodds category:cs.CL published:2015-12-02 summary:The emergence and global adoption of social media has rendered possible thereal-time estimation of population-scale sentiment, bearing profoundimplications for our understanding of human behavior. Given the growingassortment of sentiment measuring instruments, comparisons between them areevidently required. Here, we perform detailed tests of 6 dictionary-basedmethods applied to 4 different corpora, and briefly examine a further 8methods. We show that a dictionary-based method will only perform both reliablyand meaningfully if (1) the dictionary covers a sufficiently large enoughportion of a given text's lexicon when weighted by word usage frequency; and(2) words are scored on a continuous scale.
arxiv-15000-134 | Zero-Shot Event Detection by Multimodal Distributional Semantic Embedding of Videos | http://arxiv.org/pdf/1512.00818v2.pdf | author:Mohamed Elhoseiny, Jingen Liu, Hui Cheng, Harpreet Sawhney, Ahmed Elgammal category:cs.CV cs.CL cs.LG published:2015-12-02 summary:We propose a new zero-shot Event Detection method by Multi-modalDistributional Semantic embedding of videos. Our model embeds object and actionconcepts as well as other available modalities from videos into adistributional semantic space. To our knowledge, this is the first Zero-Shotevent detection model that is built on top of distributional semantics andextends it in the following directions: (a) semantic embedding of multimodalinformation in videos (with focus on the visual modalities), (b) automaticallydetermining relevance of concepts/attributes to a free text query, which couldbe useful for other applications, and (c) retrieving videos by free text eventquery (e.g., "changing a vehicle tire") based on their content. We embed videosinto a distributional semantic space and then measure the similarity betweenvideos and the event query in a free text form. We validated our method on thelarge TRECVID MED (Multimedia Event Detection) challenge. Using only the eventtitle as a query, our method outperformed the state-of-the-art that uses bigdescriptions from 12.6% to 13.5% with MAP metric and 0.73 to 0.83 with ROC-AUCmetric. It is also an order of magnitude faster.
arxiv-15000-135 | Rethinking the Inception Architecture for Computer Vision | http://arxiv.org/pdf/1512.00567v3.pdf | author:Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna category:cs.CV published:2015-12-02 summary:Convolutional networks are at the core of most state-of-the-art computervision solutions for a wide variety of tasks. Since 2014 very deepconvolutional networks started to become mainstream, yielding substantial gainsin various benchmarks. Although increased model size and computational costtend to translate to immediate quality gains for most tasks (as long as enoughlabeled data is provided for training), computational efficiency and lowparameter count are still enabling factors for various use cases such as mobilevision and big-data scenarios. Here we explore ways to scale up networks inways that aim at utilizing the added computation as efficiently as possible bysuitably factorized convolutions and aggressive regularization. We benchmarkour methods on the ILSVRC 2012 classification challenge validation setdemonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6%top-5 error for single frame evaluation using a network with a computationalcost of 5 billion multiply-adds per inference and with using less than 25million parameters. With an ensemble of 4 models and multi-crop evaluation, wereport 3.5% top-5 error on the validation set (3.6% error on the test set) and17.3% top-1 error on the validation set.
arxiv-15000-136 | Protein secondary structure prediction using deep convolutional neural fields | http://arxiv.org/pdf/1512.00843v3.pdf | author:Sheng Wang, Jian Peng, Jianzhu Ma, Jinbo Xu category:q-bio.BM cs.LG q-bio.QM published:2015-12-02 summary:Protein secondary structure (SS) prediction is important for studying proteinstructure and function. When only the sequence (profile) information is used asinput feature, currently the best predictors can obtain ~80% Q3 accuracy, whichhas not been improved in the past decade. Here we present DeepCNF (DeepConvolutional Neural Fields) for protein SS prediction. DeepCNF is a DeepLearning extension of Conditional Neural Fields (CNF), which is an integrationof Conditional Random Fields (CRF) and shallow neural networks. DeepCNF canmodel not only complex sequence-structure relationship by a deep hierarchicalarchitecture, but also interdependency between adjacent SS labels, so it ismuch more powerful than CNF. Experimental results show that DeepCNF can obtain~84% Q3 accuracy, ~85% SOV score, and ~72% Q8 accuracy, respectively, on theCASP and CAMEO test proteins, greatly outperforming currently popularpredictors. As a general framework, DeepCNF can be used to predict otherprotein structure properties such as contact number, disorder regions, andsolvent accessibility.
arxiv-15000-137 | Labeling the Features Not the Samples: Efficient Video Classification with Minimal Supervision | http://arxiv.org/pdf/1512.00517v1.pdf | author:Marius Leordeanu, Alexandra Radu, Shumeet Baluja, Rahul Sukthankar category:cs.CV published:2015-12-01 summary:Feature selection is essential for effective visual recognition. We proposean efficient joint classifier learning and feature selection method thatdiscovers sparse, compact representations of input features from a vast sea ofcandidates, with an almost unsupervised formulation. Our method requires onlythe following knowledge, which we call the \emph{feature sign}---whether or nota particular feature has on average stronger values over positive samples thanover negatives. We show how this can be estimated using as few as a singlelabeled training sample per class. Then, using these feature signs, we extendan initial supervised learning problem into an (almost) unsupervised clusteringformulation that can incorporate new data without requiring ground truthlabels. Our method works both as a feature selection mechanism and as a fullycompetitive classifier. It has important properties, low computational cost andexcellent accuracy, especially in difficult cases of very limited trainingdata. We experiment on large-scale recognition in video and show superior speedand performance to established feature selection approaches such as AdaBoost,Lasso, greedy forward-backward selection, and powerful classifiers such as SVM.
arxiv-15000-138 | Loss Functions for Top-k Error: Analysis and Insights | http://arxiv.org/pdf/1512.00486v2.pdf | author:Maksim Lapin, Matthias Hein, Bernt Schiele category:stat.ML cs.CV cs.LG published:2015-12-01 summary:In order to push the performance on realistic computer vision tasks, thenumber of classes in modern benchmark datasets has significantly increased inrecent years. This increase in the number of classes comes along with increasedambiguity between the class labels, raising the question if top-1 error is theright performance measure. In this paper, we provide an extensive comparisonand evaluation of established multiclass methods comparing their top-kperformance both from a practical as well as from a theoretical perspective.Moreover, we introduce novel top-k loss functions as modifications of thesoftmax and the multiclass SVM losses and provide efficient optimizationschemes for them. In the experiments, we compare on various datasets all of theproposed and established methods for top-k error optimization. An interestinginsight of this paper is that the softmax loss yields competitive top-kperformance for all k simultaneously. For a specific top-k error, our new top-klosses lead typically to further improvements while being faster to train thanthe softmax.
arxiv-15000-139 | Efficient Edge Detection on Low-Cost FPGAs | http://arxiv.org/pdf/1512.00504v1.pdf | author:Jamie Schiel, Andrew Bainbridge-Smith category:cs.AR cs.CV published:2015-12-01 summary:Improving the efficiency of edge detection in embedded applications, such asUAV control, is critical for reducing system cost and power dissipation. Fieldprogrammable gate arrays (FPGA) are a good platform for making improvementsbecause of their specialised internal structure. However, current FPGA edgedetectors do not exploit this structure well. A new edge detection architectureis proposed that is better optimised for FPGAs. The basis of the architectureis the Sobel edge kernels that are shown to be the most suitable because oftheir separability and absence of multiplications. Edge intensities arecalculated with a new 4:2 compressor that consists of two custom-designed 3:2compressors. Addition speed is increased by breaking carry propagation chainswith look-ahead logic. Testing of the design showed it gives a 28% increase inspeed and 4.4% reduction in area over previous equivalent designs, whichdemonstrated that it will lower the cost of edge detection systems, dissipateless power and still maintain high-speed control.
arxiv-15000-140 | Fast k-Nearest Neighbour Search via Dynamic Continuous Indexing | http://arxiv.org/pdf/1512.00442v1.pdf | author:Ke Li, Jitendra Malik category:cs.DS cs.AI cs.IR cs.LG stat.ML published:2015-12-01 summary:Existing methods for retrieving k-nearest neighbours suffer from the curse ofdimensionality. We argue this is caused in part by inherent deficiencies ofspace partitioning, which is the underlying strategy used by almost allexisting methods. We devise a new strategy that avoids partitioning the vectorspace and present a novel randomized algorithm that runs in time linear indimensionality and sub-linear in the size of the dataset and takes spaceconstant in dimensionality and linear in the size of the dataset. The proposedalgorithm allows fine-grained control over accuracy and speed on a per-querybasis, automatically adapts to variations in dataset density, supports dynamicupdates to the dataset and is easy-to-implement. We show appealing theoreticalproperties and demonstrate empirically that the proposed algorithm outperformslocality-sensitivity hashing (LSH) in terms of approximation quality and speed.
arxiv-15000-141 | Multilingual Language Processing From Bytes | http://arxiv.org/pdf/1512.00103v2.pdf | author:Dan Gillick, Cliff Brunk, Oriol Vinyals, Amarnag Subramanya category:cs.CL published:2015-12-01 summary:We describe an LSTM-based model which we call Byte-to-Span (BTS) that readstext as bytes and outputs span annotations of the form [start, length, label]where start positions, lengths, and labels are separate entries in ourvocabulary. Because we operate directly on unicode bytes rather thanlanguage-specific words or characters, we can analyze text in many languageswith a single model. Due to the small vocabulary size, these multilingualmodels are very compact, but produce results similar to or better than thestate-of- the-art in Part-of-Speech tagging and Named Entity Recognition thatuse only the provided training datasets (no external data sources). Our modelsare learning "from scratch" in that they do not rely on any elements of thestandard pipeline in Natural Language Processing (including tokenization), andthus can run in standalone fashion on raw text.
arxiv-15000-142 | Taxonomy grounded aggregation of classifiers with different label sets | http://arxiv.org/pdf/1512.00355v1.pdf | author:Amrita Saha, Sathish Indurthi, Shantanu Godbole, Subendhu Rongali, Vikas C. Raykar category:cs.AI cs.LG published:2015-12-01 summary:We describe the problem of aggregating the label predictions of diverseclassifiers using a class taxonomy. Such a taxonomy may not have been availableor referenced when the individual classifiers were designed and trained, yetmapping the output labels into the taxonomy is desirable to integrate theeffort spent in training the constituent classifiers. A hierarchical taxonomyrepresenting some domain knowledge may be different from, but partiallymappable to, the label sets of the individual classifiers. We present aheuristic approach and a principled graphical model to aggregate the labelpredictions by grounding them into the available taxonomy. Our model aggregatesthe labels using the taxonomy structure as constraints to find the most likelyhierarchically consistent class. We experimentally validate our proposed methodon image and text classification tasks.
arxiv-15000-143 | Highly Scalable Tensor Factorization for Prediction of Drug-Protein Interaction Type | http://arxiv.org/pdf/1512.00315v1.pdf | author:Adam Arany, Jaak Simm, Pooya Zakeri, Tom Haber, Jörg K. Wegner, Vladimir Chupakhin, Hugo Ceulemans, Yves Moreau category:stat.ML published:2015-12-01 summary:The understanding of the type of inhibitory interaction plays an importantrole in drug design. Therefore, researchers are interested to know whether adrug has competitive or non-competitive interaction to particular proteintargets. Method: to analyze the interaction types we propose factorization methodMacau which allows us to combine different measurement types into a singletensor together with proteins and compounds. The compounds are characterized byhigh dimensional 2D ECFP fingerprints. The novelty of the proposed method isthat using a specially designed noise injection MCMC sampler it can incorporatehigh dimensional side information, i.e., millions of unique 2D ECFP compoundfeatures, even for large scale datasets of millions of compounds. Without theside information, in this case, the tensor factorization would be practicallyfutile. Results: using public IC50 and Ki data from ChEMBL we trained a model fromwhere we can identify the latent subspace separating the two measurement types(IC50 and Ki). The results suggest the proposed method can detect thecompetitive inhibitory activity between compounds and proteins.
arxiv-15000-144 | Sequential visibility-graph motifs | http://arxiv.org/pdf/1512.00297v2.pdf | author:Jacopo Iacovacci, Lucas Lacasa category:cs.LG nlin.CD published:2015-12-01 summary:Visibility algorithms transform time series into graphs and encode dynamicalinformation in their topology, paving the way for graph-theoretical time seriesanalysis as well as building a bridge between nonlinear dynamics and networkscience. In this work we introduce and study the concept of sequentialvisibility graph motifs, smaller substructures of n consecutive nodes thatappear with characteristic frequencies. We develop a theory to compute in anexact way the motif profiles associated to general classes of deterministic andstochastic dynamics. We find that this simple property is indeed a highlyinformative and computationally efficient feature capable to distinguish amongdifferent dynamics and robust against noise contamination. We finally confirmthat it can be used in practice to perform unsupervised learning, by extractingmotif profiles from experimental heart-rate series and being able, accordingly,to disentangle meditative from other relaxation states. Applications of thisgeneral theory include the automatic classification and description ofphysical, biological, and financial time series.
arxiv-15000-145 | Implicit Sparse Code Hashing | http://arxiv.org/pdf/1512.00130v1.pdf | author:Tsung-Yu Lin, Tsung-Wei Ke, Tyng-Luh Liu category:cs.CV published:2015-12-01 summary:We address the problem of converting large-scale high-dimensional image datainto binary codes so that approximate nearest-neighbor search over them can beefficiently performed. Different from most of the existing unsupervisedapproaches for yielding binary codes, our method is based on adimensionality-reduction criterion that its resulting mapping is designed topreserve the image relationships entailed by the inner products of sparsecodes, rather than those implied by the Euclidean distances in the ambientspace. While the proposed formulation does not require computing any sparsecodes, the underlying computation model still inevitably involves solving anunmanageable eigenproblem when extremely high-dimensional descriptors are used.To overcome the difficulty, we consider the column-sampling technique andpresume a special form of rotation matrix to facilitate subproblemdecomposition. We test our method on several challenging image datasets anddemonstrate its effectiveness by comparing with state-of-the-art binary codingtechniques.
arxiv-15000-146 | MOCICE-BCubed F$_1$: A New Evaluation Measure for Biclustering Algorithms | http://arxiv.org/pdf/1512.00228v2.pdf | author:Henry Rosales-Méndez, Yunior Ramírez-Cruz category:cs.LG cs.IR I.5.3; H.3.3 published:2015-12-01 summary:The validation of biclustering algorithms remains a challenging task, eventhough a number of measures have been proposed for evaluating the quality ofthese algorithms. Although no criterion is universally accepted as the overallbest, a number of meta-evaluation conditions to be satisfied by biclusteringalgorithms have been enunciated. In this work, we present MOCICE-BCubed F$_1$,a new external measure for evaluating biclusterings, in the scenario where goldstandard annotations are available for both the object clusters and theassociated feature subspaces. Our proposal relies on the so-calledmicro-objects transformation and satisfies the most comprehensive set ofmeta-evaluation conditions so far enunciated for biclusterings. Additionally,the proposed measure adequately handles the occurrence of overlapping in boththe object and feature spaces. Moreover, when used for evaluating traditionalclusterings, which are viewed as a particular case of biclustering, theproposed measure also satisfies the most comprehensive set of meta-evaluationconditions so far enunciated for this task.
arxiv-15000-147 | Optimal Estimation and Completion of Matrices with Biclustering Structures | http://arxiv.org/pdf/1512.00150v1.pdf | author:Chao Gao, Yu Lu, Zongming Ma, Harrison H. Zhou category:math.ST stat.ML stat.TH published:2015-12-01 summary:Biclustering structures in data matrices were first formalized in a seminalpaper by John Hartigan (1972) where one seeks to cluster cases and variablessimultaneously. Such structures are also prevalent in block modeling ofnetworks. In this paper, we develop a unified theory for the estimation andcompletion of matrices with biclustering structures, where the data is apartially observed and noise contaminated data matrix with a certainbiclustering structure. In particular, we show that a constrained least squaresestimator achieves minimax rate-optimal performance in several of the mostimportant scenarios. To this end, we derive unified high probability upperbounds for all sub-Gaussian data and also provide matching minimax lower boundsin both Gaussian and binary cases. Due to the close connection of graphon tostochastic block models, an immediate consequence of our general results is aminimax rate-optimal estimator for sparse graphons.
arxiv-15000-148 | Learning Using 1-Local Membership Queries | http://arxiv.org/pdf/1512.00165v1.pdf | author:Galit Bary category:cs.LG cs.AI published:2015-12-01 summary:Classic machine learning algorithms learn from labelled examples. Forexample, to design a machine translation system, a typical training set willconsist of English sentences and their translation. There is a stronger model,in which the algorithm can also query for labels of new examples it creates.E.g, in the translation task, the algorithm can create a new English sentence,and request its translation from the user during training. This combination ofexamples and queries has been widely studied. Yet, despite many theoreticalresults, query algorithms are almost never used. One of the main causes forthis is a report (Baum and Lang, 1992) on very disappointing empiricalperformance of a query algorithm. These poor results were mainly attributed tothe fact that the algorithm queried for labels of examples that are artificial,and impossible to interpret by humans. In this work we study a new model of local membership queries (Awasthi etal., 2012), which tries to resolve the problem of artificial queries. In thismodel, the algorithm is only allowed to query the labels of examples which areclose to examples from the training set. E.g., in translation, the algorithmcan change individual words in a sentence it has already seen, and then ask forthe translation. In this model, the examples queried by the algorithm will beclose to natural examples and hence, hopefully, will not appear as artificialor random. We focus on 1-local queries (i.e., queries of distance 1 from anexample in the training sample). We show that 1-local membership queries arealready stronger than the standard learning model. We also present anexperiment on a well known NLP task of sentiment analysis. In this experiment,the users were asked to provide more information than merely indicating thelabel. We present results that illustrate that this extra information isbeneficial in practice.
arxiv-15000-149 | Augmenting Phrase Table by Employing Lexicons for Pivot-based SMT | http://arxiv.org/pdf/1512.00170v1.pdf | author:Yiming Cui, Conghui Zhu, Xiaoning Zhu, Tiejun Zhao category:cs.CL published:2015-12-01 summary:Pivot language is employed as a way to solve the data sparseness problem inmachine translation, especially when the data for a particular language pairdoes not exist. The combination of source-to-pivot and pivot-to-targettranslation models can induce a new translation model through the pivotlanguage. However, the errors in two models may compound as noise, and still,the combined model may suffer from a serious phrase sparsity problem. In thispaper, we directly employ the word lexical model in IBM models as an additionalresource to augment pivot phrase table. In addition, we also propose a phrasetable pruning method which takes into account both of the source and targetphrasal coverage. Experimental result shows that our pruning methodsignificantly outperforms the conventional one, which only considers sourceside phrasal coverage. Furthermore, by including the entries in the lexiconmodel, the phrase coverage increased, and we achieved improved results inChinese-to-Japanese translation using English as pivot language.
arxiv-15000-150 | Analyzing Classifiers: Fisher Vectors and Deep Neural Networks | http://arxiv.org/pdf/1512.00172v1.pdf | author:Sebastian Bach, Alexander Binder, Grégoire Montavon, Klaus-Robert Müller, Wojciech Samek category:cs.CV published:2015-12-01 summary:Fisher Vector classifiers and Deep Neural Networks (DNNs) are popular andsuccessful algorithms for solving image classification problems. However, bothare generally considered `black box' predictors as the non-lineartransformations involved have so far prevented transparent and interpretablereasoning. Recently, a principled technique, Layer-wise Relevance Propagation(LRP), has been developed in order to better comprehend the inherent structuredreasoning of complex nonlinear classification models such as Bag of Featuremodels or DNNs. In this paper we (1) extend the LRP framework also for FisherVector classifiers and then use it as analysis tool to (2) quantify theimportance of context for classification, (3) qualitatively compare DNNsagainst FV classifiers in terms of important image regions and (4) detectpotential flaws and biases in data. All experiments are performed on the PASCALVOC 2007 data set.
arxiv-15000-151 | Accelerated graph-based nonlinear denoising filters | http://arxiv.org/pdf/1512.00389v2.pdf | author:Andrew Knyazev, Alexander Malyshev category:cs.CV math.NA 65F30 I.4.3; G.1.3 published:2015-12-01 summary:Denoising filters, such as bilateral, guided, and total variation filters,applied to images on general graphs may require repeated application if noiseis not small enough. We formulate two acceleration techniques of the resultediterations: conjugate gradient method and Nesterov's acceleration. Wenumerically show efficiency of the accelerated nonlinear filters for imagedenoising and demonstrate 2-12 times speed-up, i.e., the accelerationtechniques reduce the number of iterations required to reach a given peaksignal-to-noise ratio (PSNR) by the above indicated factor of 2-12.
arxiv-15000-152 | LSTM Neural Reordering Feature for Statistical Machine Translation | http://arxiv.org/pdf/1512.00177v2.pdf | author:Yiming Cui, Shijin Wang, Jianfeng Li, Yuguang Wang category:cs.CL cs.AI cs.NE published:2015-12-01 summary:Artificial neural networks are powerful models, which have been widelyapplied into many aspects of machine translation, such as language modeling andtranslation modeling. Though notable improvements have been made in theseareas, the reordering problem still remains a challenge in statistical machinetranslations. In this paper, we present a novel neural reordering model thatdirectly models word pairs and alignment. By utilizing LSTM recurrent neuralnetworks, much longer context could be learned for reordering prediction.Experimental results on NIST OpenMT12 Arabic-English and Chinese-English1000-best rescoring task show that our LSTM neural reordering feature is robustand achieves significant improvements over various baseline systems.
arxiv-15000-153 | Inferring Interpersonal Relations in Narrative Summaries | http://arxiv.org/pdf/1512.00112v1.pdf | author:Shashank Srivastava, Snigdha Chaturvedi, Tom Mitchell category:cs.CL cs.AI cs.SI published:2015-12-01 summary:Characterizing relationships between people is fundamental for theunderstanding of narratives. In this work, we address the problem of inferringthe polarity of relationships between people in narrative summaries. Weformulate the problem as a joint structured prediction for each narrative, andpresent a model that combines evidence from linguistic and semantic features,as well as features based on the structure of the social community in the text.We also provide a clustering-based approach that can exploit regularities innarrative types. e.g., learn an affinity for love-triangles in romanticstories. On a dataset of movie summaries from Wikipedia, our structured modelsprovide more than a 30% error-reduction over a competitive baseline thatconsiders pairs of characters in isolation.
arxiv-15000-154 | Dynamic Parallel and Distributed Graph Cuts | http://arxiv.org/pdf/1512.00101v1.pdf | author:Miao Yu, Shuhan Shen, Zhanyi Hu category:cs.DS cs.CV published:2015-12-01 summary:Graph-cuts are widely used in computer vision. In order to speed up theoptimization process and improve the scalability for large graphs, Strandmarkand Kahl introduced a splitting method to split a graph into multiple subgraphsfor parallel computation in both shared and distributed memory models. However,this parallel algorithm (parallel BK-algorithm) does not have a polynomialbound on the number of iterations and is found non-convergent in some cases dueto the possible multiple optimal solutions of its sub-problems. To remedy this non-convergence problem, in this work we first introduce amerging method capable of merging any number of those adjacent sub-graphs whichcould hardly reach an agreement on their overlapped region in the parallelBKalgorithm. Based on the pseudo-boolean representations of graphcuts,ourmerging method is shown able to effectively reuse all the computed flows inthese sub-graphs. Through both the splitting and merging, we further propose adynamic parallel and distributed graph-cuts algorithm with guaranteedconvergence to the globally optimal solutions within a predefined number ofiterations. In essence, this work provides a general framework to allow moresophisticated splitting and merging strategies to be employed to further boostperformance. Our dynamic parallel algorithm is validated with extensiveexperimental results.
arxiv-15000-155 | Fast and High Quality Highlight Removal from A Single Image | http://arxiv.org/pdf/1512.00237v1.pdf | author:Dongsheng An, Jinli Suo, Xiangyang Ji, Haoqian Wang, Qionghai Dai category:cs.CV published:2015-12-01 summary:Specular reflection exists widely in photography and causes the recordedcolor deviating from its true value, so fast and high quality highlight removalfrom a single nature image is of great importance. In spite of the progress inthe past decades in highlight removal, achieving wide applicability to thelarge diversity of nature scenes is quite challenging. To handle this problem,we propose an analytic solution to highlight removal based on an L2chromaticity definition and corresponding dichromatic model. Specifically, thispaper derives a normalized dichromatic model for the pixels with identicaldiffuse color: a unit circle equation of projection coefficients in twosubspaces that are orthogonal to and parallel with the illumination,respectively. In the former illumination orthogonal subspace, which isspecular-free, we can conduct robust clustering with an explicit criterion todetermine the cluster number adaptively. In the latter illumination parallelsubspace, a property called pure diffuse pixels distribution rule (PDDR) helpsmap each specular-influenced pixel to its diffuse component. In terms ofefficiency, the proposed approach involves few complex calculation, and thuscan remove highlight from high resolution images fast. Experiments show thatthis method is of superior performance in various challenging cases.
arxiv-15000-156 | Towards Dropout Training for Convolutional Neural Networks | http://arxiv.org/pdf/1512.00242v1.pdf | author:Haibing Wu, Xiaodong Gu category:cs.LG cs.CV cs.NE published:2015-12-01 summary:Recently, dropout has seen increasing use in deep learning. For deepconvolutional neural networks, dropout is known to work well in fully-connectedlayers. However, its effect in convolutional and pooling layers is still notclear. This paper demonstrates that max-pooling dropout is equivalent torandomly picking activation based on a multinomial distribution at trainingtime. In light of this insight, we advocate employing our proposedprobabilistic weighted pooling, instead of commonly used max-pooling, to act asmodel averaging at test time. Empirical evidence validates the superiority ofprobabilistic weighted pooling. We also empirically show that the effect ofconvolutional dropout is not trivial, despite the dramatically reducedpossibility of over-fitting due to the convolutional architecture. Elaboratelydesigning dropout training simultaneously in max-pooling and fully-connectedlayers, we achieve state-of-the-art performance on MNIST, and very competitiveresults on CIFAR-10 and CIFAR-100, relative to other approaches without dataaugmentation. Finally, we compare max-pooling dropout and stochastic pooling,both of which introduce stochasticity based on multinomial distributions atpooling stage.
arxiv-15000-157 | On Optical Flow Models for Variational Motion Estimation | http://arxiv.org/pdf/1512.00298v1.pdf | author:Martin Burger, Hendrik Dirks, Lena Frerking category:math.NA cs.CV math.OC published:2015-12-01 summary:The aim of this paper is to discuss and evaluate total variation basedregularization methods for motion estimation, with particular focus on opticalflow models. In addition to standard $L^2$ and $L^1$ data fidelities we give anoverview of different variants of total variation regularization obtained fromcombination with higher order models and a unified computational optimizationapproach based on primal-dual methods. Moreover, we extend the models byBregman iterations and provide an inverse problems perspective to the analysisof variational optical flow models. A particular focus of the paper is thequantitative evaluation of motion estimation, which is a difficult and oftenunderestimated task. We discuss several approaches for quality measures ofmotion estimation and apply them to compare the previously discussedregularization approaches.
arxiv-15000-158 | Ask, and shall you receive?: Understanding Desire Fulfillment in Natural Language Text | http://arxiv.org/pdf/1511.09460v1.pdf | author:Snigdha Chaturvedi, Dan Goldwasser, Hal Daume III category:cs.AI cs.CL published:2015-11-30 summary:The ability to comprehend wishes or desires and their fulfillment isimportant to Natural Language Understanding. This paper introduces the task ofidentifying if a desire expressed by a subject in a given short piece of textwas fulfilled. We propose various unstructured and structured models thatcapture fulfillment cues such as the subject's emotional state and actions. Ourexperiments with two different datasets demonstrate the importance ofunderstanding the narrative and discourse structure to address this task.
arxiv-15000-159 | Universality laws for randomized dimension reduction, with applications | http://arxiv.org/pdf/1511.09433v1.pdf | author:Samet Oymak, Joel A. Tropp category:math.PR cs.DS cs.IT math.IT math.ST stat.ML stat.TH published:2015-11-30 summary:Dimension reduction is the process of embedding high-dimensional data into alower dimensional space to facilitate its analysis. In the Euclidean setting,one fundamental technique for dimension reduction is to apply a random linearmap to the data. This dimension reduction procedure succeeds when it preservescertain geometric features of the set. The question is how large the embeddingdimension must be to ensure that randomized dimension reduction succeeds withhigh probability. This paper studies a natural family of randomized dimension reduction mapsand a large class of data sets. It proves that there is a phase transition inthe success probability of the dimension reduction map as the embeddingdimension increases. For a given data set, the location of the phase transitionis the same for all maps in this family. Furthermore, each map has the samestability properties, as quantified through the restricted minimum singularvalue. These results can be viewed as new universality laws in high-dimensionalstochastic geometry. Universality laws for randomized dimension reduction have many applicationsin applied mathematics, signal processing, and statistics. They yield designprinciples for numerical linear algebra algorithms, for compressed sensingmeasurement ensembles, and for random linear codes. Furthermore, these resultshave implications for the performance of statistical estimation methods under alarge class of random experimental designs.
arxiv-15000-160 | A General Framework for Constrained Bayesian Optimization using Information-based Search | http://arxiv.org/pdf/1511.09422v1.pdf | author:José Miguel Hernández-Lobato, Michael A. Gelbart, Ryan P. Adams, Matthew W. Hoffman, Zoubin Ghahramani category:stat.ML published:2015-11-30 summary:We present an information-theoretic framework for solving global black-boxoptimization problems that also have black-box constraints. Of particularinterest to us is to efficiently solve problems with decoupled constraints, inwhich subsets of the objective and constraint functions may be evaluatedindependently. For example, when the objective is evaluated on a CPU and theconstraints are evaluated independently on a GPU. These problems require anacquisition function that can be separated into the contributions of theindividual function evaluations. We develop one such acquisition function andcall it Predictive Entropy Search with Constraints (PESC). PESC is anapproximation to the expected information gain criterion and it comparesfavorably to alternative approaches based on improvement in several syntheticand real-world problems. In addition to this, we consider problems with a mixof functions that are fast and slow to evaluate. These problems requirebalancing the amount of time spent in the meta-computation of PESC and in theactual evaluation of the target objective. We take a bounded rationalityapproach and develop a partial update for PESC which trades off accuracyagainst speed. We then propose a method for adaptively switching between thepartial and full updates for PESC. This allows us to interpolate betweenversions of PESC that are efficient in terms of function evaluations and thosethat are efficient in terms of wall-clock time. Overall, we demonstrate thatPESC is an effective algorithm that provides a promising direction towards aunified solution for constrained Bayesian optimization.
arxiv-15000-161 | Modeling Dynamic Relationships Between Characters in Literary Novels | http://arxiv.org/pdf/1511.09376v1.pdf | author:Snigdha Chaturvedi, Shashank Srivastava, Hal Daume III, Chris Dyer category:cs.CL cs.AI published:2015-11-30 summary:Studying characters plays a vital role in computationally representing andinterpreting narratives. Unlike previous work, which has focused on inferringcharacter roles, we focus on the problem of modeling their relationships.Rather than assuming a fixed relationship for a character pair, we hypothesizethat relationships are dynamic and temporally evolve with the progress of thenarrative, and formulate the problem of relationship modeling as a structuredprediction problem. We propose a semi-supervised framework to learnrelationship sequences from fully as well as partially labeled data. We presenta Markovian model capable of accumulating historical beliefs about therelationship and status changes. We use a set of rich linguistic andsemantically motivated features that incorporate world knowledge to investigatethe textual content of narrative. We empirically demonstrate that such aframework outperforms competitive baselines.
arxiv-15000-162 | Behavior Discovery and Alignment of Articulated Object Classes from Unstructured Video | http://arxiv.org/pdf/1511.09319v1.pdf | author:Luca Del Pero, Susanna Ricco, Rahul Sukthankar, Vittorio Ferrari category:cs.CV published:2015-11-30 summary:Internet videos provide a wealth of data that could be used to learn theappearance or expected behaviors of many object classes. However, mostsupervised methods cannot exploit this data directly, as they require a largeamount of time-consuming manual annotations. As a step towards solving thisproblem, we propose an automatic system for organizing the content of acollection of videos of an articulated object class (e.g. tiger, horse). Byexploiting the recurring motion patterns of the class across videos, oursystem: 1) identifies its characteristic behaviors; and 2) recoverspixel-to-pixel alignments across different instances. The behavior discovery stage generates temporal video intervals, eachautomatically trimmed to one instance of the discovered behavior, clustered bytype. It relies on our novel motion representation for articulated motion basedon the displacement of ordered pairs of trajectories (PoTs). The alignmentstage aligns hundreds of instances of the class to a great accuracy despiteconsiderable appearance variations (e.g. an adult tiger and a cub). It uses aflexible Thin Plate Spline deformation model that can vary through time. Wecarefully evaluate each step of our system on a new, fully annotated dataset.On behavior discovery, we outperform the state-of-the-art Improved DTFdescriptor. On spatial alignment, we outperform the popular SIFT Flowalgorithm.
arxiv-15000-163 | On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models | http://arxiv.org/pdf/1511.09249v1.pdf | author:Juergen Schmidhuber category:cs.AI cs.LG cs.NE published:2015-11-30 summary:This paper addresses the general problem of reinforcement learning (RL) inpartially observable environments. In 2013, our large RL recurrent neuralnetworks (RNNs) learned from scratch to drive simulated cars fromhigh-dimensional video input. However, real brains are more powerful in manyways. In particular, they learn a predictive model of their initially unknownenvironment, and somehow use it for abstract (e.g., hierarchical) planning andreasoning. Guided by algorithmic information theory, we describe RNN-based AIs(RNNAIs) designed to do the same. Such an RNNAI can be trained on never-endingsequences of tasks, some of them provided by the user, others invented by theRNNAI itself in a curious, playful fashion, to improve its RNN-based worldmodel. Unlike our previous model-building RNN-based RL machines dating back to1990, the RNNAI learns to actively query its model for abstract reasoning andplanning and decision making, essentially "learning to think." The basic ideasof this report can be applied to many other cases where one RNN-like systemexploits the algorithmic information content of another. They are taken from agrant proposal submitted in Fall 2014, and also explain concepts such as"mirror neurons." Experimental results will be described in separate papers.
arxiv-15000-164 | Fine-Grained Classification via Mixture of Deep Convolutional Neural Networks | http://arxiv.org/pdf/1511.09209v1.pdf | author:ZongYuan Ge, Alex Bewley, Christopher McCool, Ben Upcroft, Peter Corke, Conrad Sanderson category:cs.CV published:2015-11-30 summary:We present a novel deep convolutional neural network (DCNN) system forfine-grained image classification, called a mixture of DCNNs (MixDCNN). Thefine-grained image classification problem is characterised by large intra-classvariations and small inter-class variations. To overcome these problems ourproposed MixDCNN system partitions images into K subsets of similar images andlearns an expert DCNN for each subset. The output from each of the K DCNNs iscombined to form a single classification decision. In contrast to previoustechniques, we provide a formulation to perform joint end-to-end training ofthe K DCNNs simultaneously. Extensive experiments, on three datasets using twonetwork structures (AlexNet and GoogLeNet), show that the proposed MixDCNNsystem consistently outperforms other methods. It provides a relativeimprovement of 12.7% and achieves state-of-the-art results on two datasets.
arxiv-15000-165 | Asynchronous adaptive networks | http://arxiv.org/pdf/1511.09180v1.pdf | author:Ali H. Sayed, Xiaochuan Zhao category:math.OC cs.LG cs.MA published:2015-11-30 summary:In a recent article [1] we surveyed advances related to adaptation, learning,and optimization over synchronous networks. Various distributed strategies werediscussed that enable a collection of networked agents to interact locally inresponse to streaming data and to continually learn and adapt to track driftsin the data and models. Under reasonable technical conditions on the data, theadaptive networks were shown to be mean-square stable in the slow adaptationregime, and their mean-square-error performance and convergence rate werecharacterized in terms of the network topology and data statistical moments[2]. Classical results for single-agent adaptation and learning were recoveredas special cases. Following the works [3]-[5], this chapter complements theexposition from [1] and extends the results to asynchronous networks. Theoperation of this class of networks can be subject to various sources ofuncertainties that influence their dynamic behavior, including randomlychanging topologies, random link failures, random data arrival times, andagents turning on and off randomly. In an asynchronous environment, agents maystop updating their solutions or may stop sending or receiving information in arandom manner and without coordination with other agents. The presentation willreveal that the mean-square-error performance of asynchronous networks remainslargely unaltered compared to synchronous networks. The results justify theremarkable resilience of cooperative networks in the face of random events.
arxiv-15000-166 | Recognizing Temporal Linguistic Expression Pattern of Individual with Suicide Risk on Social Media | http://arxiv.org/pdf/1511.09173v1.pdf | author:Aiqi Zhang, Ang Li, Tingshao Zhu category:cs.SI cs.CL published:2015-11-30 summary:Suicide is a global public health problem. Early detection of individualsuicide risk plays a key role in suicide prevention. In this paper, we proposeto look into individual suicide risk through time series analysis of personallinguistic expression on social media (Weibo). We examined temporal patterns ofthe linguistic expression of individuals on Chinese social media (Weibo). Then,we used such temporal patterns as predictor variables to build classificationmodels for estimating levels of individual suicide risk. Characteristics oftime sequence curves to linguistic features including parentheses, auxiliaryverbs, personal pronouns and body words are reported to affect performance ofsuicide most, and the predicting model has a accuracy higher than 0.60, shownby the results. This paper confirms the efficiency of the social media data indetecting individual suicide risk. Results of this study may be insightful forimproving the performance of suicide prevention programs.
arxiv-15000-167 | Hierarchical Invariant Feature Learning with Marginalization for Person Re-Identification | http://arxiv.org/pdf/1511.09150v1.pdf | author:Rahul Rama Varior, Gang Wang category:cs.CV published:2015-11-30 summary:This paper addresses the problem of matching pedestrians across multiplecamera views, known as person re-identification. Variations in lightingconditions, environment and pose changes across camera views makere-identification a challenging problem. Previous methods address thesechallenges by designing specific features or by learning a distance function.We propose a hierarchical feature learning framework that learns invariantrepresentations from labeled image pairs. A mapping is learned such that theextracted features are invariant for images belonging to same individual acrossviews. To learn robust representations and to achieve better generalization tounseen data, the system has to be trained with a large amount of data.Critically, most of the person re-identification datasets are small. Manuallyaugmenting the dataset by partial corruption of input data introducesadditional computational burden as it requires several training epochs toconverge. We propose a hierarchical network which incorporates amarginalization technique that can reap the benefits of training on largedatasets without explicit augmentation. We compare our approach with severalbaseline algorithms as well as popular linear and non-linear metric learningalgorithms and demonstrate improved performance on challenging publiclyavailable datasets, VIPeR, CUHK01, CAVIAR4REID and iLIDS. Our approach alsoachieves the stateof-the-art results on these datasets.
arxiv-15000-168 | Proximal gradient method for huberized support vector machine | http://arxiv.org/pdf/1511.09159v1.pdf | author:Yangyang Xu, Ioannis Akrotirianakis, Amit Chakraborty category:stat.ML cs.LG cs.NA math.NA published:2015-11-30 summary:The Support Vector Machine (SVM) has been used in a wide variety ofclassification problems. The original SVM uses the hinge loss function, whichis non-differentiable and makes the problem difficult to solve in particularfor regularized SVMs, such as with $\ell_1$-regularization. This paperconsiders the Huberized SVM (HSVM), which uses a differentiable approximationof the hinge loss function. We first explore the use of the Proximal Gradient(PG) method to solving binary-class HSVM (B-HSVM) and then generalize it tomulti-class HSVM (M-HSVM). Under strong convexity assumptions, we show that ouralgorithm converges linearly. In addition, we give a finite convergence resultabout the support of the solution, based on which we further accelerate thealgorithm by a two-stage method. We present extensive numerical experiments onboth synthetic and real datasets which demonstrate the superiority of ourmethods over some state-of-the-art methods for both binary- and multi-classSVMs.
arxiv-15000-169 | Aspect-based Opinion Summarization with Convolutional Neural Networks | http://arxiv.org/pdf/1511.09128v1.pdf | author:Haibing Wu, Yiwei Gu, Shangdi Sun, Xiaodong Gu category:cs.CL cs.IR cs.LG published:2015-11-30 summary:This paper considers Aspect-based Opinion Summarization (AOS) of reviews onparticular products. To enable real applications, an AOS system needs toaddress two core subtasks, aspect extraction and sentiment classification. Mostexisting approaches to aspect extraction, which use linguistic analysis ortopic modeling, are general across different products but not precise enough orsuitable for particular products. Instead we take a less general but moreprecise scheme, directly mapping each review sentence into pre-defined aspects.To tackle aspect mapping and sentiment classification, we propose twoConvolutional Neural Network (CNN) based methods, cascaded CNN and multitaskCNN. Cascaded CNN contains two levels of convolutional networks. Multiple CNNsat level 1 deal with aspect mapping task, and a single CNN at level 2 dealswith sentiment classification. Multitask CNN also contains multiple aspect CNNsand a sentiment CNN, but different networks share the same word embeddings.Experimental results indicate that both cascaded and multitask CNNs outperformSVM-based methods by large margins. Multitask CNN generally performs betterthan cascaded CNN.
arxiv-15000-170 | Optimization theory of Hebbian/anti-Hebbian networks for PCA and whitening | http://arxiv.org/pdf/1511.09468v1.pdf | author:Cengiz Pehlevan, Dmitri B. Chklovskii category:q-bio.NC cs.NE published:2015-11-30 summary:In analyzing information streamed by sensory organs, our brains facechallenges similar to those solved in statistical signal processing. Thissuggests that biologically plausible implementations of online signalprocessing algorithms may model neural computation. Here, we focus on suchworkhorses of signal processing as Principal Component Analysis (PCA) andwhitening which maximize information transmission in the presence of noise. Weadopt the similarity matching framework, recently developed for principalsubspace extraction, but modify the existing objective functions by adding adecorrelating term. From the modified objective functions, we derive online PCAand whitening algorithms which are implementable by neural networks with locallearning rules, i.e. synaptic weight updates that depend on the activity ofonly pre- and postsynaptic neurons. Our theory offers a principled model ofneural computations and makes testable predictions such as the dropout ofunderutilized neurons.
arxiv-15000-171 | Design of Kernels in Convolutional Neural Networks for Image Classification | http://arxiv.org/pdf/1511.09231v2.pdf | author:Zhun Sun, Mete Ozay, Takayuki Okatani category:cs.CV published:2015-11-30 summary:Despite the effectiveness of Convolutional Neural Networks (CNNs) for imageclassification, our understanding of the relationship between shape ofconvolution kernels and learned representations is limited. In this work, weexplore and employ the relationship between shape of kernels which defineReceptive Fields (RFs) in CNNs for learning of feature representations andimage classification. For this purpose, we first propose a featurevisualization method for visualization of pixel-wise classification score mapsof learned features. Motivated by our experimental results, and observationsreported in the literature for modeling of visual systems, we propose a noveldesign of shape of kernels for learning of representations in CNNs. In theexperimental results, we achieved a state-of-the-art classification performancecompared to a base CNN model [28] by reducing the number of parameters andcomputational time of the model using the ILSVRC-2012 dataset [24]. Theproposed models also outperform the state-of-the-art models employed on theCIFAR-10/100 datasets [12] for image classification. Additionally, we analyzedthe robustness of the proposed method to occlusion for classification ofpartially occluded images compared with the state-of-the-art methods. Ourresults indicate the effectiveness of the proposed approach.
arxiv-15000-172 | Incidental Scene Text Understanding: Recent Progresses on ICDAR 2015 Robust Reading Competition Challenge 4 | http://arxiv.org/pdf/1511.09207v2.pdf | author:Cong Yao, Jianan Wu, Xinyu Zhou, Chi Zhang, Shuchang Zhou, Zhimin Cao, Qi Yin category:cs.CV published:2015-11-30 summary:Different from focused texts present in natural images, which are capturedwith user's intention and intervention, incidental texts usually exhibit muchmore diversity, variability and complexity, thus posing significantdifficulties and challenges for scene text detection and recognitionalgorithms. The ICDAR 2015 Robust Reading Competition Challenge 4 was launchedto assess the performance of existing scene text detection and recognitionmethods on incidental texts as well as to stimulate novel ideas and solutions.This report is dedicated to briefly introduce our strategies for thischallenging problem and compare them with prior arts in this field.
arxiv-15000-173 | Low-cost and Faster Tracking Systems Using Core-sets for Pose-Estimation | http://arxiv.org/pdf/1511.09120v2.pdf | author:Soliman Nasser, Ibrahim Jubran, Dan Feldman category:cs.RO cs.CV published:2015-11-30 summary:How can a \$20 toy quadcopter hover using a weak "Internet of Things"mini-board and a web-cam? In the pose-estimation problem we need to rotate aset of $n$ marker (points) and choose one of their n! permutations, so that thesum of squared corresponding distances to another ordered set of $n$ markers isminimized. A popular heuristic for this problem is ICP. We prove that \emph{every} set has a weighted subset (core-set) of constantsize (independent of $n$), such that computing the optimal orientation of thesmall core-set would yield \emph{exactly} the same result as using the full setof $n$ markers. A deterministic algorithm for computing this core-set in $O(n)$time is provided, using the Caratheodory Theorem from computational geometry. We developed a system that enables low-cost and real-time tracking bycomputing this core-set on the cloud in one thread, and uses the last computedcore-set locally in a parallel thread. Our experimental results show how thesecore-sets can boost the tracking time and quality for large and even small setsof both IR and RGB markers on a toy quadcopter. Open source code for the systemand algorithm is provided
arxiv-15000-174 | Non-Adaptive Group Testing on Graphs | http://arxiv.org/pdf/1511.09196v2.pdf | author:Hamid Kameli category:cs.DS cs.LG published:2015-11-30 summary:Grebinski and Kucherov (1998) and Alon et al. (2004-2005) study the problemof learning a hidden graph for some especial cases, such as hamiltonian cycle,cliques, stars, and matchings. This problem is motivated by problems inchemical reactions, molecular biology and genome sequencing. In this paper, we present a generalization of this problem. Precisely, weconsider a graph G and a subgraph H of G and we assume that G contains exactlyone defective subgraph isomorphic to H. The goal is to find the defectivesubgraph by testing whether an induced subgraph contains an edge of thedefective subgraph, with the minimum number of tests. We present an upper boundfor the number of tests to find the defective subgraph by using the symmetricand high probability variation of Lov\'asz Local Lemma.
arxiv-15000-175 | Decoding Hidden Markov Models Faster Than Viterbi Via Online Matrix-Vector (max, +)-Multiplication | http://arxiv.org/pdf/1512.00077v2.pdf | author:Massimo Cairo, Gabriele Farina, Romeo Rizzi category:cs.LG cs.DS cs.IT math.IT published:2015-11-30 summary:In this paper, we present a novel algorithm for the maximum a posterioridecoding (MAPD) of time-homogeneous Hidden Markov Models (HMM), improving theworst-case running time of the classical Viterbi algorithm by a logarithmicfactor. In our approach, we interpret the Viterbi algorithm as a repeatedcomputation of matrix-vector $(\max, +)$-multiplications. On time-homogeneousHMMs, this computation is online: a matrix, known in advance, has to bemultiplied with several vectors revealed one at a time. Our main contributionis an algorithm solving this version of matrix-vector $(\max,+)$-multiplicationin subquadratic time, by performing a polynomial preprocessing of the matrix.Employing this fast multiplication algorithm, we solve the MAPD problem in$O(mn^2/ \log n)$ time for any time-homogeneous HMM of size $n$ and observationsequence of length $m$, with an extra polynomial preprocessing cost negligiblefor $m > n$. To the best of our knowledge, this is the first algorithm for theMAPD problem requiring subquadratic time per observation, under the onlyassumption -- usually verified in practice -- that the transition probabilitymatrix does not change with time.
arxiv-15000-176 | Cost-aware Pre-training for Multiclass Cost-sensitive Deep Learning | http://arxiv.org/pdf/1511.09337v2.pdf | author:Yu-An Chung, Hsuan-Tien Lin, Shao-Wen Yang category:cs.LG cs.NE published:2015-11-30 summary:Deep learning has been one of the most prominent machine learning techniquesnowadays, being the state-of-the-art on a broad range of applications whereautomatic feature extraction is needed. Many such applications also demandvarying costs for different types of mis-classification errors, but it is notclear whether or how such cost information can be incorporated into deeplearning to improve performance. In this work, we propose a novel cost-awarealgorithm that takes into account the cost information into not only thetraining stage but also the pre-training stage of deep learning. The approachallows deep learning to conduct automatic feature extraction with the costinformation effectively. Extensive experimental results demonstrate that theproposed approach outperforms other deep learning models that do not digest thecost information in the pre-training stage.
arxiv-15000-177 | Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video | http://arxiv.org/pdf/1511.09439v2.pdf | author:Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Kosta Derpanis, Kostas Daniilidis category:cs.CV published:2015-11-30 summary:This paper addresses the challenge of 3D full-body human pose estimation froma monocular image sequence. Here, two cases are considered: (i) the imagelocations of the human joints are provided and (ii) the image locations ofjoints are unknown. In the former case, a novel approach is introduced thatintegrates a sparsity-driven 3D geometric prior and temporal smoothness. In thelatter case, the former case is extended by treating the image locations of thejoints as latent variables. A deep fully convolutional network is trained topredict the uncertainty maps of the 2D joint locations. The 3D pose estimatesare realized via an Expectation-Maximization algorithm over the entiresequence, where it is shown that the 2D joint location uncertainties can beconveniently marginalized out during inference. Empirical evaluation on theHuman3.6M dataset shows that the proposed approaches achieve greater 3D poseestimation accuracy over state-of-the-art baselines. Further, the proposedapproach outperforms a publicly available 2D pose estimation baseline on thechallenging PennAction dataset.
arxiv-15000-178 | A Normative Theory of Adaptive Dimensionality Reduction in Neural Networks | http://arxiv.org/pdf/1511.09426v2.pdf | author:Cengiz Pehlevan, Dmitri B. Chklovskii category:q-bio.NC cs.NE published:2015-11-30 summary:To make sense of the world our brains must analyze high-dimensional datasetsstreamed by our sensory organs. Because such analysis begins withdimensionality reduction, modelling early sensory processing requiresbiologically plausible online dimensionality reduction algorithms. Recently, wederived such an algorithm, termed similarity matching, from a MultidimensionalScaling (MDS) objective function. However, in the existing algorithm, thenumber of output dimensions is set a priori by the number of output neurons andcannot be changed. Because the number of informative dimensions in sensoryinputs is variable there is a need for adaptive dimensionality reduction. Here,we derive biologically plausible dimensionality reduction algorithms whichadapt the number of output dimensions to the eigenspectrum of the inputcovariance matrix. We formulate three objective functions which, in the offlinesetting, are optimized by the projections of the input dataset onto itsprincipal subspace scaled by the eigenvalues of the output covariance matrix.In turn, the output eigenvalues are computed as i) soft-thresholded, ii)hard-thresholded, iii) equalized thresholded eigenvalues of the inputcovariance matrix. In the online setting, we derive the three correspondingadaptive algorithms and map them onto the dynamics of neuronal activity innetworks with biologically plausible local learning rules. Remarkably, in thelast two networks, neurons are divided into two classes which we identify withprincipal neurons and interneurons in biological circuits.
arxiv-15000-179 | Alternating direction method of multipliers for regularized multiclass support vector machines | http://arxiv.org/pdf/1511.09153v1.pdf | author:Yangyang Xu, Ioannis Akrotirianakis, Amit Chakraborty category:stat.ML math.OC published:2015-11-30 summary:The support vector machine (SVM) was originally designed for binaryclassifications. A lot of effort has been put to generalize the binary SVM tomulticlass SVM (MSVM) which are more complex problems. Initially, MSVMs weresolved by considering their dual formulations which are quadratic programs andcan be solved by standard second-order methods. However, the duals of MSVMswith regularizers are usually more difficult to formulate and computationallyvery expensive to solve. This paper focuses on several regularized MSVMs andextends the alternating direction method of multiplier (ADMM) to these MSVMs.Using a splitting technique, all considered MSVMs are written as two-blockconvex programs, for which the ADMM has global convergence guarantees.Numerical experiments on synthetic and real data demonstrate the highefficiency and accuracy of our algorithms.
arxiv-15000-180 | Scalable and Accurate Online Feature Selection for Big Data | http://arxiv.org/pdf/1511.09263v2.pdf | author:Kui Yu, Xindong Wu, Wei Ding, Jian Pei category:cs.LG published:2015-11-30 summary:Feature selection is important in many big data applications. There are atleast two critical challenges. Firstly, in many applications, thedimensionality is extremely high, in millions, and keeps growing. Secondly,feature selection has to be highly scalable, preferably in an online mannersuch that each feature can be processed in a sequential scan. In this paper, wedevelop SAOLA, a Scalable and Accurate OnLine Approach for feature selection.With a theoretical analysis on bounds of the pairwise correlations betweenfeatures, SAOLA employs novel online pairwise comparison techniques to addressthe two challenges and maintain a parsimonious model over time in an onlinemanner. Furthermore, to tackle the dimensionality that arrives by groups, weextend our SAOLA algorithm, and then propose a novel group-SAOLA algorithm foronline group feature selection. The group-SAOLA algorithm can online maintain aset of feature groups that is sparse at the level of both groups and individualfeatures simultaneously. An empirical study using a series of benchmark realdata sets shows that our two algorithms, SAOLA and group-SAOLA, are scalable ondata sets of extremely high dimensionality, and have superior performance overthe state-of-the-art feature selection methods.
arxiv-15000-181 | The Multiverse Loss for Robust Transfer Learning | http://arxiv.org/pdf/1511.09033v2.pdf | author:Etai Littwin, Lior Wolf category:cs.CV published:2015-11-29 summary:Deep learning techniques are renowned for supporting effective transferlearning. However, as we demonstrate, the transferred representations supportonly a few modes of separation and much of its dimensionality is unutilized. Inthis work, we suggest to learn, in the source domain, multiple orthogonalclassifiers. We prove that this leads to a reduced rank representation, which,however, supports more discriminative directions. Interestingly, the softmaxprobabilities produced by the multiple classifiers are likely to be identical.Experimental results, on CIFAR-100 and LFW, further demonstrate theeffectiveness of our method.
arxiv-15000-182 | MidRank: Learning to rank based on subsequences | http://arxiv.org/pdf/1511.08951v1.pdf | author:Basura Fernando, Efstratios Gavves, Damien Muselet, Tinne Tuytelaars category:cs.CV cs.LG published:2015-11-29 summary:We present a supervised learning to rank algorithm that effectively ordersimages by exploiting the structure in image sequences. Most often in thesupervised learning to rank literature, ranking is approached either byanalyzing pairs of images or by optimizing a list-wise surrogate loss functionon full sequences. In this work we propose MidRank, which learns frommoderately sized sub-sequences instead. These sub-sequences contain usefulstructural ranking information that leads to better learnability duringtraining and better generalization during testing. By exploiting sub-sequences,the proposed MidRank improves ranking accuracy considerably on an extensivearray of image ranking applications and datasets.
arxiv-15000-183 | Bootstrapping Ternary Relation Extractors | http://arxiv.org/pdf/1511.08952v1.pdf | author:Ndapandula Nakashole category:cs.CL cs.AI 68T50 published:2015-11-29 summary:Binary relation extraction methods have been widely studied in recent years.However, few methods have been developed for higher n-ary relation extraction.One limiting factor is the effort required to generate training data. Forbinary relations, one only has to provide a few dozen pairs of entities perrelation, as training data. For ternary relations (n=3), each training instanceis a triplet of entities, placing a greater cognitive load on people. Forexample, many people know that Google acquired Youtube but not the dollaramount or the date of the acquisition and many people know that Hillary Clintonis married to Bill Clinton by not the location or date of their wedding. Thismakes higher n-nary training data generation a time consuming exercise insearching the Web. We present a resource for training ternary relationextractors. This was generated using a minimally supervised yet effectiveapproach. We present statistics on the size and the quality of the dataset.
arxiv-15000-184 | k-Nearest Neighbour Classification of Datasets with a Family of Distances | http://arxiv.org/pdf/1512.00001v1.pdf | author:Stan Hatko category:stat.ML cs.LG published:2015-11-29 summary:The $k$-nearest neighbour ($k$-NN) classifier is one of the oldest and mostimportant supervised learning algorithms for classifying datasets.Traditionally the Euclidean norm is used as the distance for the $k$-NNclassifier. In this thesis we investigate the use of alternative distances forthe $k$-NN classifier. We start by introducing some background notions in statistical machinelearning. We define the $k$-NN classifier and discuss Stone's theorem and theproof that $k$-NN is universally consistent on the normed space $R^d$. We thenprove that $k$-NN is universally consistent if we take a sequence of randomnorms (that are independent of the sample and the query) from a family of normsthat satisfies a particular boundedness condition. We extend this result byreplacing norms with distances based on uniformly locally Lipschitz functionsthat satisfy certain conditions. We discuss the limitations of Stone's lemmaand Stone's theorem, particularly with respect to quasinorms and adaptivelychoosing a distance for $k$-NN based on the labelled sample. We show theuniversal consistency of a two stage $k$-NN type classifier where we select thedistance adaptively based on a split labelled sample and the query. We concludeby giving some examples of improvements of the accuracy of classifying variousdatasets using the above techniques.
arxiv-15000-185 | Sparseness helps: Sparsity Augmented Collaborative Representation for Classification | http://arxiv.org/pdf/1511.08956v1.pdf | author:Naveed Akhtar, Faisal Shafait, Ajmal Mian category:cs.CV published:2015-11-29 summary:Many classification approaches first represent a test sample using thetraining samples of all the classes. This collaborative representation is thenused to label the test sample. It was a common belief that sparseness of therepresentation is the key to success for this classification scheme. However,more recently, it has been claimed that it is the collaboration and not thesparseness that makes the scheme effective. This claim is attractive as itallows to relinquish the computationally expensive sparsity constraint over therepresentation. In this paper, we first extend the analysis supporting thisclaim and then show that sparseness explicitly contributes to improvedclassification, hence it should not be completely ignored for computationalgains. Inspired by this result, we augment a dense collaborative representationwith a sparse representation and propose an efficient classification methodthat capitalizes on the resulting representation. The augmented representationand the classification method work together meticulously to achieve higheraccuracy and lower computational time compared to state-of-the-artcollaborative representation based classification approaches. Experiments onbenchmark face, object and action databases show the efficacy of our approach.
arxiv-15000-186 | Multiple-Instance Learning: Radon-Nikodym Approach to Distribution Regression Problem | http://arxiv.org/pdf/1511.09058v2.pdf | author:Vladislav Gennadievich Malyshkin category:cs.LG published:2015-11-29 summary:For distribution regression problem, where a bag of $x$--observations ismapped to a single $y$ value, a one--step solution is proposed. The problem ofrandom distribution to random value is transformed to random vector to randomvalue by taking distribution moments of $x$ observations in a bag as randomvector. Then Radon--Nikodym or least squares theory can be applied, what give$y(x)$ estimator. The probability distribution of $y$ is also obtained, whatrequires solving generalized eigenvalues problem, matrix spectrum (notdepending on $x$) give possible $y$ outcomes and depending on $x$ probabilitiesof outcomes can be obtained by projecting the distribution with fixed $x$ value(delta--function) to corresponding eigenvector. A library providing numericallystable polynomial basis for these calculations is available, what make theproposed approach practical.
arxiv-15000-187 | Learning Directed Acyclic Graphs with Penalized Neighbourhood Regression | http://arxiv.org/pdf/1511.08963v2.pdf | author:Bryon Aragam, Arash A. Amini, Qing Zhou category:math.ST cs.LG stat.ML stat.TH published:2015-11-29 summary:We consider the problem of estimating a directed acyclic graph (DAG) for amultivariate normal distribution from high-dimensional data with $p\gg n$. Ourmain results establish nonasymptotic deviation bounds on the estimation error,sparsity bounds, and model selection consistency for a penalized least squaresestimator under concave regularization. The proofs rely on interpreting thegraphical model as a recursive linear structural equation model, which reducesthe estimation problem to a series of tractable neighbourhood regressions andallows us to avoid making any assumptions regarding faithfulness. In doing so,we provide some novel techniques for handling general nonidentifiable andnonconvex problems. These techniques are used to guarantee uniform control overa superexponential number of neighbourhood regression problems by exploitingvarious notions of monotonicity among them. Our results apply to a wide varietyof practical situations that allow for arbitrary nondegenerate covariancestructures as well as many popular regularizers including the MCP, SCAD,$\ell_{0}$ and $\ell_{1}$.
arxiv-15000-188 | Robotic Search & Rescue via Online Multi-task Reinforcement Learning | http://arxiv.org/pdf/1511.08967v1.pdf | author:Lisa Lee category:cs.AI cs.LG cs.RO published:2015-11-29 summary:Reinforcement learning (RL) is a general and well-known method that a robotcan use to learn an optimal control policy to solve a particular task. We wouldlike to build a versatile robot that can learn multiple tasks, but using RL foreach of them would be prohibitively expensive in terms of both time andwear-and-tear on the robot. To remedy this problem, we use the Policy GradientEfficient Lifelong Learning Algorithm (PG-ELLA), an online multi-task RLalgorithm that enables the robot to efficiently learn multiple consecutivetasks by sharing knowledge between these tasks to accelerate learning andimprove performance. We implemented and evaluated three RL methods--Q-learning,policy gradient RL, and PG-ELLA--on a ground robot whose task is to find atarget object in an environment under different surface conditions. In thispaper, we discuss our implementations as well as present an empirical analysisof their learning performance.
arxiv-15000-189 | How do the naive Bayes classifier and the Support Vector Machine compare in their ability to forecast the Stock Exchange of Thailand? | http://arxiv.org/pdf/1511.08987v1.pdf | author:Napas Udomsak category:cs.LG published:2015-11-29 summary:This essay investigates the question of how the naive Bayes classifier andthe support vector machine compare in their ability to forecast the StockExchange of Thailand. The theory behind the SVM and the naive Bayes classifieris explored. The algorithms are trained using data from the month of January2010, extracted from the MarketWatch.com website. Input features are selectedbased on previous studies of the SET100 Index. The Weka 3 software is used tocreate models from the labeled training data. Mean squared error and proportionof correctly classified instances, and a number of other error measurements arethe used to compare the two algorithms. This essay shows that these twoalgorithms are currently not advanced enough to accurately model the stockexchange. Nevertheless, the naive Bayes is better than the support vectormachine at predicting the Stock Exchange of Thailand.
arxiv-15000-190 | On-line Recognition of Handwritten Mathematical Symbols | http://arxiv.org/pdf/1511.09030v1.pdf | author:Martin Thoma category:cs.CV published:2015-11-29 summary:Finding the name of an unknown symbol is often hard, but writing the symbolis easy. This bachelor's thesis presents multiple systems that use the pentrajectory to classify handwritten symbols. Five preprocessing steps, one dataaugmentation algorithm, five features and five variants for multilayerPerceptron training were evaluated using 166898 recordings which were collectedwith two crowdsourcing projects. The evaluation results of these 21 experimentswere used to create an optimized recognizer which has a TOP1 error of less than17.5% and a TOP3 error of 4.0%. This is an improvement of 18.5% for the TOP1error and 29.7% for the TOP3 error.
arxiv-15000-191 | Reinforcement Learning Applied to an Electric Water Heater: From Theory to Practice | http://arxiv.org/pdf/1512.00408v1.pdf | author:Frederik Ruelens, Bert Claessens, Salman Quaiyum, Bart De Schutter, Robert Babuska, Ronnie Belmans category:cs.LG published:2015-11-29 summary:Electric water heaters have the ability to store energy in their water bufferwithout impacting the comfort of the end user. This feature makes them a primecandidate for residential demand response. However, the stochastic andnonlinear dynamics of electric water heaters, makes it challenging to harnesstheir flexibility. Driven by this challenge, this paper formulates theunderlying sequential decision-making problem as a Markov decision process anduses techniques from reinforcement learning. Specifically, we apply anauto-encoder network to find a compact feature representation of the sensormeasurements, which helps to mitigate the curse of dimensionality. A wellknownbatch reinforcement learning technique, fitted Q-iteration, is used to find acontrol policy, given this feature representation. In a simulation-basedexperiment using an electric water heater with 50 temperature sensors, theproposed method was able to achieve good policies much faster than when usingthe full state information. In a lab experiment, we apply fitted Q-iteration toan electric water heater with eight temperature sensors. Further reducing thestate vector did not improve the results of fitted Q-iteration. The results ofthe lab experiment, spanning 40 days, indicate that compared to a thermostatcontroller, the presented approach was able to reduce the total cost of energyconsumption of the electric water heater by 15%.
arxiv-15000-192 | Sparse Coral Classification Using Deep Convolutional Neural Networks | http://arxiv.org/pdf/1511.09067v1.pdf | author:Mohamed Elawady category:cs.CV published:2015-11-29 summary:Autonomous repair of deep-sea coral reefs is a recent proposed idea tosupport the oceans ecosystem in which is vital for commercial fishing, tourismand other species. This idea can be operated through using many smallautonomous underwater vehicles (AUVs) and swarm intelligence techniques tolocate and replace chunks of coral which have been broken off, thus enablingre-growth and maintaining the habitat. The aim of this project is developingmachine vision algorithms to enable an underwater robot to locate a coral reefand a chunk of coral on the seabed and prompt the robot to pick it up. Althoughthere is no literature on this particular problem, related work on fishcounting may give some insight into the problem. The technical challenges areprincipally due to the potential lack of clarity of the water and platformstabilization as well as spurious artifacts (rocks, fish, and crabs). Wepresent an efficient sparse classification for coral species using superviseddeep learning method called Convolutional Neural Networks (CNNs). We computeWeber Local Descriptor (WLD), Phase Congruency (PC), and Zero ComponentAnalysis (ZCA) Whitening to extract shape and texture feature descriptors,which are employed to be supplementary channels (feature-based maps) besidesbasic spatial color channels (spatial-based maps) of coral input image, we alsoexperiment state-of-art preprocessing underwater algorithms for imageenhancement and color normalization and color conversion adjustment. Ourproposed coral classification method is developed under MATLAB platform, andevaluated by two different coral datasets (University of California San Diego'sMoorea Labeled Corals, and Heriot-Watt University's Atlantic Deep Sea).
arxiv-15000-193 | Position paper: a general framework for applying machine learning techniques in operating room | http://arxiv.org/pdf/1511.09099v1.pdf | author:Filippo Maria Bianchi, Enrico De Santis, Hedieh Montazeri, Parisa Naraei, Alireza Sadeghian category:cs.CY cs.LG published:2015-11-29 summary:In this position paper we describe a general framework for applying machinelearning and pattern recognition techniques in healthcare. In particular, weare interested in providing an automated tool for monitoring and incrementingthe level of awareness in the operating room and for identifying human errorswhich occur during the laparoscopy surgical operation. The framework that wepresent is divided in three different layers: each layer implements algorithmswhich have an increasing level of complexity and which perform functionalitywith an higher degree of abstraction. In the first layer, raw data collectedfrom sensors in the operating room during surgical operation, they arepre-processed and aggregated. The results of this initial phase are transferredto a second layer, which implements pattern recognition techniques and extractrelevant features from the data. Finally, in the last layer, expert systems areemployed to take high level decisions, which represent the final output of thesystem.
arxiv-15000-194 | Machine Learning Sentiment Prediction based on Hybrid Document Representation | http://arxiv.org/pdf/1511.09107v1.pdf | author:Panagiotis Stalidis, Maria Giatsoglou, Konstantinos Diamantaras, George Sarigiannidis, Konstantinos Ch. Chatzisavvas category:cs.CL cs.AI stat.ML published:2015-11-29 summary:Automated sentiment analysis and opinion mining is a complex processconcerning the extraction of useful subjective information from text. Theexplosion of user generated content on the Web, especially the fact thatmillions of users, on a daily basis, express their opinions on products andservices to blogs, wikis, social networks, message boards, etc., render thereliable, automated export of sentiments and opinions from unstructured textcrucial for several commercial applications. In this paper, we present a novelhybrid vectorization approach for textual resources that combines a weightedvariant of the popular Word2Vec representation (based on Term Frequency-InverseDocument Frequency) representation and with a Bag- of-Words representation anda vector of lexicon-based sentiment values. The proposed text representationapproach is assessed through the application of several machine learningclassification algorithms on a dataset that is used extensively in literaturefor sentiment detection. The classification accuracy derived through theproposed hybrid vectorization approach is higher than when its individualcomponents are used for text represenation, and comparable withstate-of-the-art sentiment detection methodologies.
arxiv-15000-195 | Semantic Folding Theory And its Application in Semantic Fingerprinting | http://arxiv.org/pdf/1511.08855v2.pdf | author:Francisco De Sousa Webber category:cs.AI cs.CL q-bio.NC published:2015-11-28 summary:Human language is recognized as a very complex domain since decades. Nocomputer system has been able to reach human levels of performance so far. Theonly known computational system capable of proper language processing is thehuman brain. While we gather more and more data about the brain, itsfundamental computational processes still remain obscure. The lack of a soundcomputational brain theory also prevents the fundamental understanding ofNatural Language Processing. As always when science lacks a theoreticalfoundation, statistical modeling is applied to accommodate as many sampledreal-world data as possible. An unsolved fundamental issue is the actualrepresentation of language (data) within the brain, denoted as theRepresentational Problem. Starting with Jeff Hawkins' Hierarchical TemporalMemory (HTM) theory, a consistent computational theory of the human cortex, wehave developed a corresponding theory of language data representation: TheSemantic Folding Theory. The process of encoding words, by using a topographicsemantic space as distributional reference frame into a sparse binaryrepresentational vector is called Semantic Folding and is the central topic ofthis document. Semantic Folding describes a method of converting language fromits symbolic representation (text) into an explicit, semantically groundedrepresentation that can be generically processed by Hawkins' HTM networks. Asit turned out, this change in representation, by itself, can solve many complexNLP problems by applying Boolean operators and a generic similarity functionlike the Euclidian Distance. Many practical problems of statistical NLPsystems, like the high cost of computation, the fundamental incongruity ofprecision and recall , the complex tuning procedures etc., can be elegantlyovercome by applying Semantic Folding.
arxiv-15000-196 | Real-Time Depth Refinement for Specular Objects | http://arxiv.org/pdf/1511.08886v2.pdf | author:Roy Or - El, Rom Hershkovitz, Aaron Wetzler, Guy Rosman, Alfred M. Bruckstein, Ron Kimmel category:cs.CV published:2015-11-28 summary:The introduction of consumer RGB-D scanners set off a major boost in 3Dcomputer vision research. Yet, the precision of existing depth scanners is notaccurate enough to recover fine details of a scanned object. While modernshading based depth refinement methods have been proven to work well withLambertian objects, they break down in the presence of specularities. Wepresent a novel shape from shading framework that addresses this issue andenhances both diffuse and specular objects' depth profiles. We take advantageof the built-in monochromatic IR projector and IR images of the RGB-D scannersand present a lighting model that accounts for the specular regions in theinput image. Using this model, we reconstruct the depth map in real-time. Bothquantitative tests and visual evaluations prove that the proposed methodproduces state of the art depth reconstruction results.
arxiv-15000-197 | Is L2 a Good Loss Function for Neural Networks for Image Processing? | http://arxiv.org/pdf/1511.08861v1.pdf | author:Hang Zhao, Orazio Gallo, Iuri Frosio, Jan Kautz category:cs.CV published:2015-11-28 summary:Neural networks are becoming central in several areas of computer vision andimage processing. Different architectures have been proposed to solve specificproblems. The impact of the loss layer of neural networks, however, has notreceived much attention by the research community: the default and most commonchoice is L2. This can be particularly limiting in the context of imageprocessing, since L2 correlates poorly with perceived image quality. In this paper we bring attention to alternative choices. We study theperformance of several losses, including perceptually-motivated losses, andpropose a novel, differentiable error function. We show that the quality of theresults improves significantly with better loss functions, even for the samenetwork architecture.
arxiv-15000-198 | Designing high-fidelity single-shot three-qubit gates: A machine learning approach | http://arxiv.org/pdf/1511.08862v1.pdf | author:Ehsan Zahedinejad, Joydip Ghosh, Barry C. Sanders category:quant-ph cs.LG published:2015-11-28 summary:Three-qubit quantum gates are crucial for quantum error correction andquantum information processing. We generate policies for quantum controlprocedures to design three types of three-qubit gates, namely Toffoli,Controlled-Not-Not and Fredkin gates. The design procedures are applicable toan architecture of nearest-neighbor-coupled superconducting artificial atoms.The resultant fidelity for each gate is above 99.9%, which is an acceptedthreshold fidelity for fault-tolerant quantum computing. We test our policy inthe presence of decoherence-induced noise as well as show its robustnessagainst random external noise generated by the control electronics. Thethree-qubit gates are designed via our machine learning algorithm calledSubspace-Selective Self-Adaptive Differential Evolution (SuSSADE).
arxiv-15000-199 | Newton-Stein Method: An optimization method for GLMs via Stein's Lemma | http://arxiv.org/pdf/1511.08895v1.pdf | author:Murat A. Erdogdu category:stat.ML math.OC published:2015-11-28 summary:We consider the problem of efficiently computing the maximum likelihoodestimator in Generalized Linear Models (GLMs) when the number of observationsis much larger than the number of coefficients ($n \gg p \gg 1$). In thisregime, optimization algorithms can immensely benefit from approximate secondorder information. We propose an alternative way of constructing the curvatureinformation by formulating it as an estimation problem and applying aStein-type lemma, which allows further improvements through sub-sampling andeigenvalue thresholding. Our algorithm enjoys fast convergence rates,resembling that of second order methods, with modest per-iteration cost. Weprovide its convergence analysis for the general case where the rows of thedesign matrix are samples from a sub-gaussian distribution. We show that theconvergence has two phases, a quadratic phase followed by a linear phase.Finally, we empirically demonstrate that our algorithm achieves the highestperformance compared to various algorithms on several datasets.
arxiv-15000-200 | Applying deep learning to classify pornographic images and videos | http://arxiv.org/pdf/1511.08899v1.pdf | author:Mohamed Moustafa category:cs.CV cs.MM cs.NE published:2015-11-28 summary:It is no secret that pornographic material is now a one-click-away fromeveryone, including children and minors. General social media networks arestriving to isolate adult images and videos from normal ones. Intelligent imageanalysis methods can help to automatically detect and isolate questionableimages in media. Unfortunately, these methods require vast experience to designthe classifier including one or more of the popular computer vision featuredescriptors. We propose to build a classifier based on one of the recentlyflourishing deep learning techniques. Convolutional neural networks containmany layers for both automatic features extraction and classification. Thebenefit is an easier system to build (no need for hand-crafting features andclassifiers). Additionally, our experiments show that it is even more accuratethan the state of the art methods on the most recent benchmark dataset.
arxiv-15000-201 | Sliding-Window Optimization on an Ambiguity-Clearness Graph for Multi-object Tracking | http://arxiv.org/pdf/1511.08913v1.pdf | author:Qi Guo, Le Dan, Dong Yin, Xiangyang Ji category:cs.CV published:2015-11-28 summary:Multi-object tracking remains challenging due to frequent occurrence ofocclusions and outliers. In order to handle this problem, we propose anApproximation-Shrink Scheme for sequential optimization. This scheme isrealized by introducing an Ambiguity-Clearness Graph to avoid conflicts andmaintain sequence independent, as well as a sliding window optimizationframework to constrain the size of state space and guarantee convergence. Basedon this window-wise framework, the states of targets are clustered in aself-organizing manner. Moreover, we show that the traditional online and batchtracking methods can be embraced by the window-wise framework. Experimentsindicate that with only a small window, the optimization performance can bemuch better than online methods and approach to batch methods.
arxiv-15000-202 | A C-LSTM Neural Network for Text Classification | http://arxiv.org/pdf/1511.08630v2.pdf | author:Chunting Zhou, Chonglin Sun, Zhiyuan Liu, Francis C. M. Lau category:cs.CL published:2015-11-27 summary:Neural network models have been demonstrated to be capable of achievingremarkable performance in sentence and document modeling. Convolutional neuralnetwork (CNN) and recurrent neural network (RNN) are two mainstreamarchitectures for such modeling tasks, which adopt totally different ways ofunderstanding natural languages. In this work, we combine the strengths of botharchitectures and propose a novel and unified model called C-LSTM for sentencerepresentation and text classification. C-LSTM utilizes CNN to extract asequence of higher-level phrase representations, and are fed into a longshort-term memory recurrent neural network (LSTM) to obtain the sentencerepresentation. C-LSTM is able to capture both local features of phrases aswell as global and temporal sentence semantics. We evaluate the proposedarchitecture on sentiment classification and question classification tasks. Theexperimental results show that the C-LSTM outperforms both CNN and LSTM and canachieve excellent performance on these tasks.
arxiv-15000-203 | Category Enhanced Word Embedding | http://arxiv.org/pdf/1511.08629v2.pdf | author:Chunting Zhou, Chonglin Sun, Zhiyuan Liu, Francis C. M. Lau category:cs.CL published:2015-11-27 summary:Distributed word representations have been demonstrated to be effective incapturing semantic and syntactic regularities. Unsupervised representationlearning from large unlabeled corpora can learn similar representations forthose words that present similar co-occurrence statistics. Besides localoccurrence statistics, global topical information is also important knowledgethat may help discriminate a word from another. In this paper, we incorporatecategory information of documents in the learning of word representations andto learn the proposed models in a document-wise manner. Our models outperformseveral state-of-the-art models in word analogy and word similarity tasks.Moreover, we evaluate the learned word vectors on sentiment analysis and textclassification tasks, which shows the superiority of our learned word vectors.We also learn high-quality category embeddings that reflect topical meanings.
arxiv-15000-204 | Regularized EM Algorithms: A Unified Framework and Statistical Guarantees | http://arxiv.org/pdf/1511.08551v2.pdf | author:Xinyang Yi, Constantine Caramanis category:cs.LG stat.ML published:2015-11-27 summary:Latent variable models are a fundamental modeling tool in machine learningapplications, but they present significant computational and analyticalchallenges. The popular EM algorithm and its variants, is a much usedalgorithmic tool; yet our rigorous understanding of its performance is highlyincomplete. Recently, work in Balakrishnan et al. (2014) has demonstrated thatfor an important class of problems, EM exhibits linear local convergence. Inthe high-dimensional setting, however, the M-step may not be well defined. Weaddress precisely this setting through a unified treatment usingregularization. While regularization for high-dimensional problems is by nowwell understood, the iterative EM algorithm requires a careful balancing ofmaking progress towards the solution while identifying the right structure(e.g., sparsity or low-rank). In particular, regularizing the M-step using thestate-of-the-art high-dimensional prescriptions (e.g., Wainwright (2014)) isnot guaranteed to provide this balance. Our algorithm and analysis are linkedin a way that reveals the balance between optimization and statistical errors.We specialize our general framework to sparse gaussian mixture models,high-dimensional mixed regression, and regression with missing variables,obtaining statistical guarantees for each of these examples.
arxiv-15000-205 | On the convergence of cycle detection for navigational reinforcement learning | http://arxiv.org/pdf/1511.08724v2.pdf | author:Tom J. Ameloot, Jan Van den Bussche category:cs.LG cs.AI published:2015-11-27 summary:We consider a reinforcement learning framework where agents have to navigatefrom start states to goal states. We prove convergence of a cycle-detectionlearning algorithm on a class of tasks that we call reducible. Reducible taskshave an acyclic solution. We also syntactically characterize the form of thefinal policy. This characterization can be used to precisely detect theconvergence point in a simulation. Our result demonstrates that even simplealgorithms can be successful in learning a large class of nontrivial tasks. Inaddition, our framework is elementary in the sense that we only use basicconcepts to formally prove convergence.
arxiv-15000-206 | Structured learning of metric ensembles with application to person re-identification | http://arxiv.org/pdf/1511.08531v1.pdf | author:Sakrapee Paisitkriangkrai, Lin Wu, Chunhua Shen, Anton van den Hengel category:cs.CV published:2015-11-27 summary:Matching individuals across non-overlapping camera networks, known as personre-identification, is a fundamentally challenging problem due to the largevisual appearance changes caused by variations of viewpoints, lighting, andocclusion. Approaches in literature can be categoried into two streams: Thefirst stream is to develop reliable features against realistic conditions bycombining several visual features in a pre-defined way; the second stream is tolearn a metric from training data to ensure strong inter-class differences andintra-class similarities. However, seeking an optimal combination of visualfeatures which is generic yet adaptive to different benchmarks is a unsovedproblem, and metric learning models easily get over-fitted due to the scarcityof training data in person re-identification. In this paper, we propose twoeffective structured learning based approaches which explore the adaptiveeffects of visual features in recognizing persons in different benchmark datasets. Our framework is built on the basis of multiple low-level visual featureswith an optimal ensemble of their metrics. We formulate two optimizationalgorithms, CMCtriplet and CMCstruct, which directly optimize evaluationmeasures commonly used in person re-identification, also known as theCumulative Matching Characteristic (CMC) curve.
arxiv-15000-207 | Simultaneous Private Learning of Multiple Concepts | http://arxiv.org/pdf/1511.08552v1.pdf | author:Mark Bun, Kobbi Nissim, Uri Stemmer category:cs.DS cs.CR cs.LG published:2015-11-27 summary:We investigate the direct-sum problem in the context of differentiallyprivate PAC learning: What is the sample complexity of solving $k$ learningtasks simultaneously under differential privacy, and how does this cost compareto that of solving $k$ learning tasks without privacy? In our setting, anindividual example consists of a domain element $x$ labeled by $k$ unknownconcepts $(c_1,\ldots,c_k)$. The goal of a multi-learner is to output $k$hypotheses $(h_1,\ldots,h_k)$ that generalize the input examples. Without concern for privacy, the sample complexity needed to simultaneouslylearn $k$ concepts is essentially the same as needed for learning a singleconcept. Under differential privacy, the basic strategy of learning eachhypothesis independently yields sample complexity that grows polynomially with$k$. For some concept classes, we give multi-learners that require fewersamples than the basic strategy. Unfortunately, however, we also give lowerbounds showing that even for very simple concept classes, the sample cost ofprivate multi-learning must grow polynomially in $k$.
arxiv-15000-208 | Efficient Sum of Outer Products Dictionary Learning (SOUP-DIL) - The $\ell_0$ Method | http://arxiv.org/pdf/1511.08842v1.pdf | author:Saiprasad Ravishankar, Raj Rao Nadakuditi, Jeffrey A. Fessler category:cs.LG published:2015-11-27 summary:The sparsity of natural signals and images in a transform domain ordictionary has been extensively exploited in several applications such ascompression, denoising and inverse problems. More recently, data-drivenadaptation of synthesis dictionaries has shown promise in many applicationscompared to fixed or analytical dictionary models. However, dictionary learningproblems are typically non-convex and NP-hard, and the usual alternatingminimization approaches for these problems are often computationally expensive,with the computations dominated by the NP-hard synthesis sparse coding step. Inthis work, we investigate an efficient method for $\ell_{0}$ "norm"-baseddictionary learning by first approximating the training data set with a sum ofsparse rank-one matrices and then using a block coordinate descent approach toestimate the unknowns. The proposed block coordinate descent algorithm involvesefficient closed-form solutions. In particular, the sparse coding step involvesa simple form of thresholding. We provide a convergence analysis for theproposed block coordinate descent approach. Our numerical experiments show thepromising performance and significant speed-ups provided by our method over theclassical K-SVD scheme in sparse signal representation and image denoising.
arxiv-15000-209 | Multiagent Cooperation and Competition with Deep Reinforcement Learning | http://arxiv.org/pdf/1511.08779v1.pdf | author:Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan Aru, Raul Vicente category:cs.AI cs.LG q-bio.NC published:2015-11-27 summary:Multiagent systems appear in most social, economical, and politicalsituations. In the present work we extend the Deep Q-Learning Networkarchitecture proposed by Google DeepMind to multiagent environments andinvestigate how two agents controlled by independent Deep Q-Networks interactin the classic videogame Pong. By manipulating the classical rewarding schemeof Pong we demonstrate how competitive and collaborative behaviors emerge.Competitive agents learn to play and score efficiently. Agents trained undercollaborative rewarding schemes find an optimal strategy to keep the ball inthe game as long as possible. We also describe the progression from competitiveto collaborative behavior. The present work demonstrates that Deep Q-Networkscan become a practical tool for studying the decentralized learning ofmultiagent systems living in highly complex environments.
arxiv-15000-210 | Gradient Estimation with Simultaneous Perturbation and Compressive Sensing | http://arxiv.org/pdf/1511.08768v1.pdf | author:Vivek S. Borkar, Vikranth R. Dwaracherla, Neeraja Sahasrabudhe category:stat.ML published:2015-11-27 summary:This paper aims at achieving a "good" estimator for the gradient of afunction on a high-dimensional space. Often such functions are not sensitive inall coordinates and the gradient of the function is almost sparse. We propose amethod for gradient estimation that combines ideas from Spall's SimultaneousPerturbation Stochastic Approximation with compressive sensing. The aim is toobtain \good" estimator without too many function evaluations. Application toestimating gradient outer product matrix as well as standard optimizationproblems are illustrated via simulations.
arxiv-15000-211 | Informative Data Projections: A Framework and Two Examples | http://arxiv.org/pdf/1511.08762v1.pdf | author:Tijl De Bie, Jefrey Lijffijt, Raul Santos-Rodriguez, Bo Kang category:cs.LG cs.IR math.ST stat.TH published:2015-11-27 summary:Methods for Projection Pursuit aim to facilitate the visual exploration ofhigh-dimensional data by identifying interesting low-dimensional projections. Amajor challenge is the design of a suitable quality metric of projections,commonly referred to as the projection index, to be maximized by the ProjectionPursuit algorithm. In this paper, we introduce a new information-theoreticstrategy for tackling this problem, based on quantifying the amount ofinformation the projection conveys to a user given their prior beliefs aboutthe data. The resulting projection index is a subjective quantity, explicitlydependent on the intended user. As a useful illustration, we developed thisidea for two particular kinds of prior beliefs. The first kind leads to PCA(Principal Component Analysis), shining new light on when PCA is (not)appropriate. The second kind leads to a novel projection index, themaximization of which can be regarded as a robust variant of PCA. We show howthis projection index, though non-convex, can be effectively maximized using amodified power method as well as using a semidefinite programming relaxation.The usefulness of this new projection index is demonstrated in comparativeempirical experiments against PCA and a popular Projection Pursuit method.
arxiv-15000-212 | Algorithms for Differentially Private Multi-Armed Bandits | http://arxiv.org/pdf/1511.08681v1.pdf | author:Aristide Tossou, Christos Dimitrakakis category:stat.ML cs.CR cs.LG published:2015-11-27 summary:We present differentially private algorithms for the stochastic Multi-ArmedBandit (MAB) problem. This is a problem for applications such as adaptiveclinical trials, experiment design, and user-targeted advertising where privateinformation is connected to individual rewards. Our major contribution is toshow that there exist $(\epsilon, \delta)$ differentially private variants ofUpper Confidence Bound algorithms which have optimal regret, $O(\epsilon^{-1} +\log T)$. This is a significant improvement over previous results, which onlyachieve poly-log regret $O(\epsilon^{-2} \log^{2} T)$, because of our use of anovel interval-based mechanism. We also substantially improve the bounds ofprevious family of algorithms which use a continual release mechanism.Experiments clearly validate our theoretical bounds.
arxiv-15000-213 | Shaping Proto-Value Functions via Rewards | http://arxiv.org/pdf/1511.08589v1.pdf | author:Chandrashekar Lakshmi Narayanan, Raj Kumar Maity, Shalabh Bhatnagar category:cs.AI cs.LG published:2015-11-27 summary:In this paper, we combine task-dependent reward shaping and task-independentproto-value functions to obtain reward dependent proto-value functions (RPVFs).In constructing the RPVFs we are making use of the immediate rewards which areavailable during the sampling phase but are not used in the PVF construction.We show via experiments that learning with an RPVF based representation isbetter than learning with just reward shaping or PVFs. In particular, when thestate space is symmetrical and the rewards are asymmetrical, the RPVF capturethe asymmetry better than the PVFs.
arxiv-15000-214 | Iterative Instance Segmentation | http://arxiv.org/pdf/1511.08498v2.pdf | author:Ke Li, Bharath Hariharan, Jitendra Malik category:cs.CV cs.LG published:2015-11-26 summary:Existing methods for pixel-wise labelling tasks generally disregard theunderlying structure of labellings, often leading to predictions that arevisually implausible. While incorporating structure into the model shouldimprove prediction quality, doing so is challenging - manually specifying theform of structural constraints may be impractical and inference often becomesintractable even if structural constraints are given. We sidestep this problemby reducing structured prediction to a sequence of unconstrained predictionproblems and demonstrate that this approach is capable of automaticallydiscovering priors on shape, contiguity of region predictions and smoothness ofregion contours from data without any a priori specification. On the instancesegmentation task, this method outperforms the state-of-the-art, achieving amean AP^r of 63.6% at 50% overlap and 43.3% at 70% overlap.
arxiv-15000-215 | Regularizing RNNs by Stabilizing Activations | http://arxiv.org/pdf/1511.08400v7.pdf | author:David Krueger, Roland Memisevic category:cs.NE cs.CL cs.LG stat.ML published:2015-11-26 summary:We stabilize the activations of Recurrent Neural Networks (RNNs) bypenalizing the squared distance between successive hidden states' norms. This penalty term is an effective regularizer for RNNs including LSTMs andIRNNs, improving performance on character-level language modeling and phonemerecognition, and outperforming weight noise and dropout. We achieve competitive performance (18.6\% PER) on the TIMIT phonemerecognition task for RNNs evaluated without beam search or an RNN transducer. With this penalty term, IRNN can achieve similar performance to LSTM onlanguage modeling, although adding the penalty term to the LSTM results insuperior performance. Our penalty term also prevents the exponential growth of IRNN's activationsoutside of their training horizon, allowing them to generalize to much longersequences.
arxiv-15000-216 | The Automatic Statistician: A Relational Perspective | http://arxiv.org/pdf/1511.08343v2.pdf | author:Yunseong Hwang, Anh Tong, Jaesik Choi category:cs.LG stat.ML published:2015-11-26 summary:Gaussian Processes (GPs) provide a general and analytically tractable way ofmodeling complex time-varying, nonparametric functions. The Automatic BayesianCovariance Discovery (ABCD) system constructs natural-language description oftime-series data by treating unknown time-series data nonparametrically usingGP with a composite covariance kernel function. Unfortunately, learning acomposite covariance kernel with a single time-series data set often results inless informative kernel that may not give qualitative, distinctive descriptionsof data. We address this challenge by proposing two relational kernel learningmethods which can model multiple time-series data sets by finding common,shared causes of changes. We show that the relational kernel learning methodsfind more accurate models for regression problems on several real-world datasets; US stock data, US house price index data and currency exchange rate data.
arxiv-15000-217 | An Introduction to Convolutional Neural Networks | http://arxiv.org/pdf/1511.08458v2.pdf | author:Keiron O'Shea, Ryan Nash category:cs.NE cs.CV cs.LG published:2015-11-26 summary:The field of machine learning has taken a dramatic twist in recent times,with the rise of the Artificial Neural Network (ANN). These biologicallyinspired computational models are able to far exceed the performance ofprevious forms of artificial intelligence in common machine learning tasks. Oneof the most impressive forms of ANN architecture is that of the ConvolutionalNeural Network (CNN). CNNs are primarily used to solve difficult image-drivenpattern recognition tasks and with their precise yet simple architecture,offers a simplified method of getting started with ANNs. This document provides a brief introduction to CNNs, discussing recentlypublished papers and newly formed techniques in developing these brilliantlyfantastic image recognition models. This introduction assumes you are familiarwith the fundamentals of ANNs and machine learning.
arxiv-15000-218 | Incremental Truncated LSTD | http://arxiv.org/pdf/1511.08495v2.pdf | author:Clement Gehring, Yangchen Pan, Martha White category:cs.LG cs.AI published:2015-11-26 summary:Balancing between computational efficiency and sample efficiency is animportant goal in reinforcement learning. Temporal difference (TD) learningalgorithms stochastically update the value function, with a linear timecomplexity in the number of features, whereas least-squares temporal difference(LSTD) algorithms are sample efficient but can be quadratic in the number offeatures. In this work, we develop an efficient incremental low-rankLSTD({\lambda}) algorithm that progresses towards the goal of better balancingcomputation and sample efficiency. The algorithm reduces the computation andstorage complexity to the number of features times the chosen rank parameterwhile summarizing past samples efficiently to nearly obtain the samplecomplexity of LSTD. We derive a simulation bound on the solution given bytruncated low-rank approximation, illustrating a bias- variance trade-offdependent on the choice of rank. We demonstrate that the algorithm effectivelybalances computational complexity and sample efficiency for policy evaluationin a benchmark task and a high-dimensional energy allocation domain.
arxiv-15000-219 | Named Entity Recognition with Bidirectional LSTM-CNNs | http://arxiv.org/pdf/1511.08308v3.pdf | author:Jason P. C. Chiu, Eric Nichols category:cs.CL cs.LG cs.NE 68T50 I.2.7 published:2015-11-26 summary:Named entity recognition is a challenging task that has traditionallyrequired large amounts of knowledge in the form of feature engineering andlexicons to achieve high performance. In this paper, we present a novel neuralnetwork architecture that automatically detects word- and character-levelfeatures using a hybrid bidirectional LSTM and CNN architecture, eliminatingthe need for most feature engineering. We also propose a novel method ofencoding partial lexicon matches in neural networks and compare it to existingapproaches. Extensive evaluation shows that, given only tokenized text andpublicly available word embeddings, our system is competitive on the CoNLL-2003dataset and surpasses the previously reported state of the art on the OntoNotes5.0 dataset by 2.13 F1 points. By using two lexicons from public sources, weestablish new states of the art with an F1 score of 91.62 on CoNLL-2003 and86.28 on OntoNotes, surpassing systems that employ heavy feature engineering,proprietary lexicons, and rich entity linking information.
arxiv-15000-220 | TennisVid2Text: Fine-grained Descriptions for Domain Specific Videos | http://arxiv.org/pdf/1511.08522v1.pdf | author:Mohak Sukhwani, C. V. Jawahar category:cs.CV published:2015-11-26 summary:Automatically describing videos has ever been fascinating. In this work, weattempt to describe videos from a specific domain - broadcast videos of lawntennis matches. Given a video shot from a tennis match, we intend to generate atextual commentary similar to what a human expert would write on a sportswebsite. Unlike many recent works that focus on generating short captions, weare interested in generating semantically richer descriptions. This demands adetailed low-level analysis of the video content, specially the actions andinteractions among subjects. We address this by limiting our domain to the gameof lawn tennis. Rich descriptions are generated by leveraging a large corpus ofhuman created descriptions harvested from Internet. We evaluate our method on anewly created tennis video data set. Extensive analysis demonstrate that ourapproach addresses both semantic correctness as well as readability aspectsinvolved in the task.
arxiv-15000-221 | Distributed Machine Learning via Sufficient Factor Broadcasting | http://arxiv.org/pdf/1511.08486v1.pdf | author:Pengtao Xie, Jin Kyu Kim, Yi Zhou, Qirong Ho, Abhimanu Kumar, Yaoliang Yu, Eric Xing category:cs.LG cs.DC published:2015-11-26 summary:Matrix-parametrized models, including multiclass logistic regression andsparse coding, are used in machine learning (ML) applications ranging fromcomputer vision to computational biology. When these models are applied tolarge-scale ML problems starting at millions of samples and tens of thousandsof classes, their parameter matrix can grow at an unexpected rate, resulting inhigh parameter synchronization costs that greatly slow down distributedlearning. To address this issue, we propose a Sufficient Factor Broadcasting(SFB) computation model for efficient distributed learning of a large family ofmatrix-parameterized models, which share the following property: the parameterupdate computed on each data sample is a rank-1 matrix, i.e., the outer productof two "sufficient factors" (SFs). By broadcasting the SFs among workermachines and reconstructing the update matrices locally at each worker, SFBimproves communication efficiency --- communication costs are linear in theparameter matrix's dimensions, rather than quadratic --- without affectingcomputational correctness. We present a theoretical convergence analysis ofSFB, and empirically corroborate its efficiency on four differentmatrix-parametrized ML models.
arxiv-15000-222 | An analysis of the factors affecting keypoint stability in scale-space | http://arxiv.org/pdf/1511.08478v1.pdf | author:Ives Rey-Otero, Jean-Michel Morel, Mauricio Delbracio category:cs.CV published:2015-11-26 summary:The most popular image matching algorithm SIFT, introduced by D. Lowe adecade ago, has proven to be sufficiently scale invariant to be used innumerous applications. In practice, however, scale invariance may be weakenedby various sources of error inherent to the SIFT implementation affecting thestability and accuracy of keypoint detection. The density of the sampling ofthe Gaussian scale-space and the level of blur in the input image are two ofthese sources. This article presents a numerical analysis of their impact onthe extracted keypoints stability. Such an analysis has both methodological andpractical implications, on how to compare feature detectors and on how toimprove SIFT. We show that even with a significantly oversampled scale-spacenumerical errors prevent from achieving perfect stability. Usual strategies tofilter out unstable detections are shown to be inefficient. We also prove thatthe effect of the error in the assumption on the initial blur is asymmetric andthat the method is strongly degraded in presence of aliasing or without acorrect assumption on the camera blur.
arxiv-15000-223 | A Deep Architecture for Semantic Matching with Multiple Positional Sentence Representations | http://arxiv.org/pdf/1511.08277v1.pdf | author:Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu, Liang Pang, Xueqi Cheng category:cs.AI cs.CL cs.NE published:2015-11-26 summary:Matching natural language sentences is central for many applications such asinformation retrieval and question answering. Existing deep models rely on asingle sentence representation or multiple granularity representations formatching. However, such methods cannot well capture the contextualized localinformation in the matching process. To tackle this problem, we present a newdeep architecture to match two sentences with multiple positional sentencerepresentations. Specifically, each positional sentence representation is asentence representation at this position, generated by a bidirectional longshort term memory (Bi-LSTM). The matching score is finally produced byaggregating interactions between these different positional sentencerepresentations, through $k$-Max pooling and a multi-layer perceptron. Ourmodel has several advantages: (1) By using Bi-LSTM, rich context of the wholesentence is leveraged to capture the contextualized local information in eachpositional sentence representation; (2) By matching with multiple positionalsentence representations, it is flexible to aggregate different importantcontextualized local information in a sentence to support the matching; (3)Experiments on different tasks such as question answering and sentencecompletion demonstrate the superiority of our model.
arxiv-15000-224 | Hierarchical classification of e-commerce related social media | http://arxiv.org/pdf/1511.08299v1.pdf | author:Matthew Long, Aditya Jami, Ashutosh Saxena category:cs.SI cs.CL cs.IR cs.LG published:2015-11-26 summary:In this paper, we attempt to classify tweets into root categories of theAmazon browse node hierarchy using a set of tweets with browse node ID labels,a much larger set of tweets without labels, and a set of Amazon reviews.Examining twitter data presents unique challenges in that the samples are short(under 140 characters) and often contain misspellings or abbreviations that aretrivial for a human to decipher but difficult for a computer to parse. Avariety of query and document expansion techniques are implemented in an effortto improve information retrieval to modest success.
arxiv-15000-225 | Random Forests for Big Data | http://arxiv.org/pdf/1511.08327v1.pdf | author:Robin Genuer, Jean-Michel Poggi, Christine Tuleau-Malot, Nathalie Villa-Vialaneix category:stat.ML cs.LG published:2015-11-26 summary:Big Data is one of the major challenges of statistical science and hasnumerous consequences from algorithmic and theoretical viewpoints. Big Dataalways involve massive data but they also often include data streams and dataheterogeneity. Recently some statistical methods have been adapted to processBig Data, like linear regression models, clustering methods and bootstrappingschemes. Based on decision trees combined with aggregation and bootstrap ideas,random forests were introduced by Breiman in 2001. They are a powerfulnonparametric statistical method allowing to consider in a single and versatileframework regression problems, as well as two-class and multi-classclassification problems. Focusing on classification problems, this paperreviews available proposals about random forests in parallel environments aswell as about online random forests. Then, we formulate various remarks forrandom forests in the Big Data context. Finally, we experiment three variantsinvolving subsampling, Big Data-bootstrap and MapReduce respectively, on twomassive datasets (15 and 120 millions of observations), a simulated one as wellas real world data.
arxiv-15000-226 | A Computational Model for Amodal Completion | http://arxiv.org/pdf/1511.08418v2.pdf | author:Maria Oliver, Gloria Haro, Mariella Dimiccoli, Baptiste Mazin, Coloma Ballester category:cs.CV published:2015-11-26 summary:This paper presents a computational model to recover the most likelyinterpretation of the 3D scene structure from a planar image, where someobjects may occlude others. The estimated scene interpretation is obtained byintegrating some global and local cues and provides both the completedisoccluded objects that form the scene and their ordering according to depth.Our method first computes several distal scenes which are compatible with theproximal planar image. To compute these different hypothesized scenes, wepropose a perceptually inspired object disocclusion method, which works byminimizing the Euler's elastica as well as by incorporating the relatability ofpartially occluded contours and the convexity of the disoccluded objects. Then,to estimate the preferred scene we rely on a Bayesian model and defineprobabilities taking into account the global complexity of the objects in thehypothesized scenes as well as the effort of bringing these objects in theirrelative position in the planar image, which is also measured by an Euler'selastica-based quantity. The model is illustrated with numerical experimentson, both, synthetic and real images showing the ability of our model toreconstruct the occluded objects and the preferred perceptual order among them.We also present results on images of the Berkeley dataset with providedfigure-ground ground-truth labeling.
arxiv-15000-227 | Obtaining A Linear Combination of the Principal Components of a Matrix on Quantum Computers | http://arxiv.org/pdf/1512.02109v3.pdf | author:Anmer Daskin category:quant-ph cs.LG math.ST stat.TH published:2015-11-26 summary:Principal component analysis is a multivariate statistical method frequentlyused in science and engineering to reduce the dimension of a problem or extractthe most significant features from a dataset. In this paper, using a similarnotion to the quantum counting, we show how to apply the amplitudeamplification together with the phase estimation algorithm to an operator inorder to procure the eigenvectors of the operator associated to the eigenvaluesdefined in the range $\left[a, b\right]$, where $a$ and $b$ are real and $0\leq a \leq b \leq 1$. This makes possible to obtain a combination of theeigenvectors associated to the largest eigenvalues and so can be used to doprincipal component analysis on quantum computers.
arxiv-15000-228 | The Mechanism of Additive Composition | http://arxiv.org/pdf/1511.08407v2.pdf | author:Ran Tian, Naoaki Okazaki, Kentaro Inui category:cs.CL cs.LG published:2015-11-26 summary:We prove an upper bound for the bias of additive composition (Foltz et al.,1998; Landauer and Dutnais, 1997; Mitchell and Lapata, 2010), a widely usedmethod for computing meanings of phrases by averaging the vectorrepresentations of their constituent words. The result endorses additivecomposition as a reasonable operation for calculating meanings of phrases,which is the first theoretical analysis on compositional frameworks from amachine learning point of view. The theory also suggests ways to improveadditive compositionality, including: transforming entries of distributionalword vectors by a function that meets a specific condition, constructing anovel type of vector representations to make additive composition sensitive toword order, and utilizing singular value decomposition to train word vectors.
arxiv-15000-229 | On randomization of neural networks as a form of post-learning strategy | http://arxiv.org/pdf/1511.08366v1.pdf | author:K. G. Kapanova, I. Dimov, J. M. Sellier category:cs.NE published:2015-11-26 summary:Today artificial neural networks are applied in various fields - engineering,data analysis, robotics. While they represent a successful tool for a varietyof relevant applications, mathematically speaking they are still far from beingconclusive. In particular, they suffer from being unable to find the bestconfiguration possible during the training process (local minimum problem). Inthis paper, we focus on this issue and suggest a simple, but effective,post-learning strategy to allow the search for improved set of weights at arelatively small extra computational cost. Therefore, we introduce a noveltechnique based on analogy with quantum effects occurring in nature as a way toimprove (and sometimes overcome) this problem. Several numerical experimentsare presented to validate the approach.
arxiv-15000-230 | Gains and Losses are Fundamentally Different in Regret Minimization: The Sparse Case | http://arxiv.org/pdf/1511.08405v1.pdf | author:Joon Kwon, Vianney Perchet category:cs.LG stat.ML published:2015-11-26 summary:We demonstrate that, in the classical non-stochastic regret minimizationproblem with $d$ decisions, gains and losses to be respectively maximized orminimized are fundamentally different. Indeed, by considering the additionalsparsity assumption (at each stage, at most $s$ decisions incur a nonzerooutcome), we derive optimal regret bounds of different orders. Specifically,with gains, we obtain an optimal regret guarantee after $T$ stages of order$\sqrt{T\log s}$, so the classical dependency in the dimension is replaced bythe sparsity size. With losses, we provide matching upper and lower bounds oforder $\sqrt{Ts\log(d)/d}$, which is decreasing in $d$. Eventually, we alsostudy the bandit setting, and obtain an upper bound of order $\sqrt{Ts\log(d/s)}$ when outcomes are losses. This bound is proven to be optimal up to thelogarithmic factor $\sqrt{\log(d/s)}$.
arxiv-15000-231 | OntoSeg: a Novel Approach to Text Segmentation using Ontological Similarity | http://arxiv.org/pdf/1511.08411v1.pdf | author:Mostafa Bayomi, Killian Levacher, M. Rami Ghorab, Séamus Lawless category:cs.CL published:2015-11-26 summary:Text segmentation (TS) aims at dividing long text into coherent segmentswhich reflect the subtopic structure of the text. It is beneficial to manynatural language processing tasks, such as Information Retrieval (IR) anddocument summarisation. Current approaches to text segmentation are similar inthat they all use word-frequency metrics to measure the similarity between tworegions of text, so that a document is segmented based on the lexical cohesionbetween its words. Various NLP tasks are now moving towards the semantic weband ontologies, such as ontology-based IR systems, to capture theconceptualizations associated with user needs and contents. Text segmentationbased on lexical cohesion between words is hence not sufficient anymore forsuch tasks. This paper proposes OntoSeg, a novel approach to text segmentationbased on the ontological similarity between text blocks. The proposed methoduses ontological similarity to explore conceptual relations between textsegments and a Hierarchical Agglomerative Clustering (HAC) algorithm torepresent the text as a tree-like hierarchy that is conceptually structured.The rich structure of the created tree further allows the segmentation of textin a linear fashion at various levels of granularity. The proposed method wasevaluated on a wellknown dataset, and the results show that using ontologicalsimilarity in text segmentation is very promising. Also we enhance the proposedmethod by combining ontological similarity with lexical similarity and theresults show an enhancement of the segmentation quality.
arxiv-15000-232 | TGSum: Build Tweet Guided Multi-Document Summarization Dataset | http://arxiv.org/pdf/1511.08417v1.pdf | author:Ziqiang Cao, Chengyao Chen, Wenjie Li, Sujian Li, Furu Wei, Ming Zhou category:cs.IR cs.CL published:2015-11-26 summary:The development of summarization research has been significantly hampered bythe costly acquisition of reference summaries. This paper proposes an effectiveway to automatically collect large scales of news-related multi-documentsummaries with reference to social media's reactions. We utilize two types ofsocial labels in tweets, i.e., hashtags and hyper-links. Hashtags are used tocluster documents into different topic sets. Also, a tweet with a hyper-linkoften highlights certain key points of the corresponding document. Wesynthesize a linked document cluster to form a reference summary which cancover most key points. To this aim, we adopt the ROUGE metrics to measure thecoverage ratio, and develop an Integer Linear Programming solution to discoverthe sentence set reaching the upper bound of ROUGE. Since we allow summarysentences to be selected from both documents and high-quality tweets, thegenerated reference summaries could be abstractive. Both informativeness andreadability of the collected summaries are verified by manual judgment. Inaddition, we train a Support Vector Regression summarizer on DUC genericmulti-document summarization benchmarks. With the collected data as extratraining resource, the performance of the summarizer improves a lot on all thetest sets. We release this dataset for further research.
arxiv-15000-233 | Towards Automatic Image Editing: Learning to See another You | http://arxiv.org/pdf/1511.08446v1.pdf | author:Amir Ghodrati, Xu Jia, Marco Pedersoli, Tinne Tuytelaars category:cs.CV published:2015-11-26 summary:Learning the distribution of images in order to generate new samples is achallenging task due to the high dimensionality of the data and the highlynon-linear relations that are involved. Nevertheless, some promising resultshave been reported in the literature recently,building on deep networkarchitectures. In this work, we zoom in on a specific type of image generation:given an image and knowing the category of objects it belongs to (e.g. faces),our goal is to generate a similar and plausible image, but with some alteredattributes. This is particularly challenging, as the model needs to learn todisentangle the effect of each attribute and to apply a desired attributechange to a given input image, while keeping the other attributes and overallobject appearance intact. To this end, we learn a convolutional network, wherethe desired attribute information is encoded then merged with the encoded imageat feature map level. We show promising results, both qualitatively as well asquantitatively, in the context of a retrieval experiment, on two face datasets(MultiPie and CAS-PEAL-R1).
arxiv-15000-234 | Exploring Person Context and Local Scene Context for Object Detection | http://arxiv.org/pdf/1511.08177v1.pdf | author:Saurabh Gupta, Bharath Hariharan, Jitendra Malik category:cs.CV published:2015-11-25 summary:In this paper we explore two ways of using context for object detection. Thefirst model focusses on people and the objects they commonly interact with,such as fashion and sports accessories. The second model considers more generalobject detection and uses the spatial relationships between objects and betweenobjects and scenes. Our models are able to capture precise spatialrelationships between the context and the object of interest, and makeeffective use of the appearance of the contextual region. On the newly releasedCOCO dataset, our models provide relative improvements of up to 5% overCNN-based state-of-the-art detectors, with the gains concentrated on hard casessuch as small objects (10% relative improvement).
arxiv-15000-235 | Tracking Motion and Proxemics using Thermal-sensor Array | http://arxiv.org/pdf/1511.08166v1.pdf | author:Chandrayee Basu, Anthony Rowe category:cs.CV published:2015-11-25 summary:Indoor tracking has all-pervasive applications beyond mere surveillance, forexample in education, health monitoring, marketing, energy management and soon. Image and video based tracking systems are intrusive. Thermal array sensorson the other hand can provide coarse-grained tracking while preserving privacyof the subjects. The goal of the project is to facilitate motion detection andgroup proxemics modeling using an 8 x 8 infrared sensor array. Each of the 8 x8 pixels is a temperature reading in Fahrenheit. We refer to each 8 x 8 matrixas a scene. We collected approximately 902 scenes with different configurationsof human groups and different walking directions. We infer direction of motionof a subject across a set of scenes as left-to-right, right-to-left, up-to-downand down-to-up using cross-correlation analysis. We used features fromconnected component analysis of each background subtracted scene and performedSupport Vector Machine classification to estimate number of instances of humansubjects in the scene.
arxiv-15000-236 | Unsupervised Deep Feature Extraction for Remote Sensing Image Classification | http://arxiv.org/pdf/1511.08131v1.pdf | author:Adriana Romero, Carlo Gatta, Gustau Camps-Valls category:cs.CV published:2015-11-25 summary:This paper introduces the use of single layer and deep convolutional networksfor remote sensing data analysis. Direct application to multi- andhyper-spectral imagery of supervised (shallow or deep) convolutional networksis very challenging given the high input data dimensionality and the relativelysmall amount of available labeled data. Therefore, we propose the use of greedylayer-wise unsupervised pre-training coupled with a highly efficient algorithmfor unsupervised learning of sparse features. The algorithm is rooted on sparserepresentations and enforces both population and lifetime sparsity of theextracted features, simultaneously. We successfully illustrate the expressivepower of the extracted representations in several scenarios: classification ofaerial scenes, as well as land-use classification in very high resolution(VHR), or land-cover classification from multi- and hyper-spectral images. Theproposed algorithm clearly outperforms standard Principal Component Analysis(PCA) and its kernel counterpart (kPCA), as well as current state-of-the-artalgorithms of aerial classification, while being extremely computationallyefficient at learning representations of data. Results show that single layerconvolutional networks can extract powerful discriminative features only whenthe receptive field accounts for neighboring pixels, and are preferred when theclassification requires high resolution and detailed results. However, deeparchitectures significantly outperform single layers variants, capturingincreasing levels of abstraction and complexity throughout the featurehierarchy.
arxiv-15000-237 | Learning to detect video events from zero or very few video examples | http://arxiv.org/pdf/1511.08032v1.pdf | author:Christos Tzelepis, Damianos Galanopoulos, Vasileios Mezaris, Ioannis Patras category:cs.LG cs.CV published:2015-11-25 summary:In this work we deal with the problem of high-level event detection in video.Specifically, we study the challenging problems of i) learning to detect videoevents from solely a textual description of the event, without using anypositive video examples, and ii) additionally exploiting very few positivetraining samples together with a small number of ``related'' videos. Forlearning only from an event's textual description, we first identify a generallearning framework and then study the impact of different design choices forvarious stages of this framework. For additionally learning from examplevideos, when true positive training samples are scarce, we employ an extensionof the Support Vector Machine that allows us to exploit ``related'' eventvideos by automatically introducing different weights for subsets of the videosin the overall training set. Experimental evaluations performed on thelarge-scale TRECVID MED 2014 video dataset provide insight on the effectivenessof the proposed methods.
arxiv-15000-238 | A Short Survey on Data Clustering Algorithms | http://arxiv.org/pdf/1511.09123v1.pdf | author:Ka-Chun Wong category:cs.DS cs.CV cs.LG stat.CO stat.ML published:2015-11-25 summary:With rapidly increasing data, clustering algorithms are important tools fordata analytics in modern research. They have been successfully applied to awide range of domains; for instance, bioinformatics, speech recognition, andfinancial analysis. Formally speaking, given a set of data instances, aclustering algorithm is expected to divide the set of data instances into thesubsets which maximize the intra-subset similarity and inter-subsetdissimilarity, where a similarity measure is defined beforehand. In this work,the state-of-the-arts clustering algorithms are reviewed from design concept tomethodology; Different clustering paradigms are discussed. Advanced clusteringalgorithms are also discussed. After that, the existing clustering evaluationmetrics are reviewed. A summary with future insights is provided at the end.
arxiv-15000-239 | Calculate distance to object in the area where car, using video analysis | http://arxiv.org/pdf/1511.07963v1.pdf | author:Elena Legchekova, Oleg Titov category:cs.CV published:2015-11-25 summary:The method of using video cameras installed on the car, to calculate thedistance to the object in its area of movement.
arxiv-15000-240 | MOOCs Meet Measurement Theory: A Topic-Modelling Approach | http://arxiv.org/pdf/1511.07961v1.pdf | author:Jiazhen He, Benjamin I. P. Rubinstein, James Bailey, Rui Zhang, Sandra Milligan, Jeffrey Chan category:cs.LG cs.CY published:2015-11-25 summary:This paper adapts topic models to the psychometric testing of MOOC studentsbased on their online forum postings. Measurement theory from education andpsychology provides statistical models for quantifying a person's attainment ofintangible attributes such as attitudes, abilities or intelligence. Such modelsinfer latent skill levels by relating them to individuals' observed responseson a series of items such as quiz questions. The set of items can be used tomeasure a latent skill if individuals' responses on them conform to a Guttmanscale. Such well-scaled items differentiate between individuals and inferredlevels span the entire range from most basic to the advanced. In practice,education researchers manually devise items (quiz questions) while optimisingwell-scaled conformance. Due to the costly nature and expert requirements ofthis process, psychometric testing has found limited use in everyday teaching.We aim to develop usable measurement models for highly-instrumented MOOCdelivery platforms, by using participation in automatically-extracted onlineforum topics as items. The challenge is to formalise the Guttman scaleeducational constraint and incorporate it into topic models. To favour topicsthat automatically conform to a Guttman scale, we introduce a novelregularisation into non-negative matrix factorisation-based topic modelling. Wedemonstrate the suitability of our approach with both quantitative experimentson three Coursera MOOCs, and with a qualitative survey of topicinterpretability on two MOOCs by domain expert interviews.
arxiv-15000-241 | Exploring Correlation between Labels to improve Multi-Label Classification | http://arxiv.org/pdf/1511.07953v1.pdf | author:Amit Garg, Jonathan Noyola, Romil Verma, Ashutosh Saxena, Aditya Jami category:cs.LG cs.SI published:2015-11-25 summary:This paper attempts multi-label classification by extending the idea ofindependent binary classification models for each output label, and exploringhow the inherent correlation between output labels can be used to improvepredictions. Logistic Regression, Naive Bayes, Random Forest, and SVM modelswere constructed, with SVM giving the best results: an improvement of 12.9\%over binary models was achieved for hold out cross validation by augmentingwith pairwise correlation probabilities of the labels.
arxiv-15000-242 | PASCAL Boundaries: A Class-Agnostic Semantic Boundary Dataset | http://arxiv.org/pdf/1511.07951v1.pdf | author:Vittal Premachandran, Boyan Bonev, Alan L. Yuille category:cs.CV published:2015-11-25 summary:In this paper, we address the boundary detection task motivated by theambiguities in current definition of edge detection. To this end, we generate alarge database consisting of more than 10k images (which is 20x bigger thanexisting edge detection databases) along with ground truth boundaries between459 semantic classes including both foreground objects and different types ofbackground, and call it the PASCAL Boundaries dataset, which will be releasedto the community. In addition, we propose a novel deep network-basedmulti-scale semantic boundary detector and name it Multi-scale Deep SemanticBoundary Detector (M-DSBD). We provide baselines using models that were trainedon edge detection and show that they transfer reasonably to the task ofboundary detection. Finally, we point to various important research problemsthat this dataset can be used for.
arxiv-15000-243 | A Roadmap towards Machine Intelligence | http://arxiv.org/pdf/1511.08130v2.pdf | author:Tomas Mikolov, Armand Joulin, Marco Baroni category:cs.AI cs.CL published:2015-11-25 summary:The development of intelligent machines is one of the biggest unsolvedchallenges in computer science. In this paper, we propose some fundamentalproperties these machines should have, focusing in particular on communicationand learning. We discuss a simple environment that could be used toincrementally teach a machine the basics of natural-language-basedcommunication, as a prerequisite to more complex interaction with human users.We also present some conjectures on the sort of algorithms the machine shouldsupport in order to profitably learn from the environment.
arxiv-15000-244 | Learning Halfspaces and Neural Networks with Random Initialization | http://arxiv.org/pdf/1511.07948v1.pdf | author:Yuchen Zhang, Jason D. Lee, Martin J. Wainwright, Michael I. Jordan category:cs.LG published:2015-11-25 summary:We study non-convex empirical risk minimization for learning halfspaces andneural networks. For loss functions that are $L$-Lipschitz continuous, wepresent algorithms to learn halfspaces and multi-layer neural networks thatachieve arbitrarily small excess risk $\epsilon>0$. The time complexity ispolynomial in the input dimension $d$ and the sample size $n$, but exponentialin the quantity $(L/\epsilon^2)\log(L/\epsilon)$. These algorithms run multiplerounds of random initialization followed by arbitrary optimization steps. Wefurther show that if the data is separable by some neural network with constantmargin $\gamma>0$, then there is a polynomial-time algorithm for learning aneural network that separates the training data with margin $\Omega(\gamma)$.As a consequence, the algorithm achieves arbitrary generalization error$\epsilon>0$ with ${\rm poly}(d,1/\epsilon)$ sample and time complexity. Weestablish the same learnability result when the labels are randomly flippedwith probability $\eta<1/2$.
arxiv-15000-245 | Maximum Likelihood Estimation for Single Linkage Hierarchical Clustering | http://arxiv.org/pdf/1511.07944v1.pdf | author:Dekang Zhu, Dan P. Guralnik, Xuezhi Wang, Xiang Li, Bill Moran category:stat.ML published:2015-11-25 summary:We derive a statistical model for estimation of a dendrogram from singlelinkage hierarchical clustering (SLHC) that takes account of uncertaintythrough noise or corruption in the measurements of separation of data. Ourfocus is on just the estimation of the hierarchy of partitions afforded by thedendrogram, rather than the heights in the latter. The concept of estimatingthis "dendrogram structure'' is introduced, and an approximate maximumlikelihood estimator (MLE) for the dendrogram structure is described. Theseideas are illustrated by a simple Monte Carlo simulation that, at least forsmall data sets, suggests the method outperforms SLHC in the presence of noise.
arxiv-15000-246 | Video Tracking Using Learned Hierarchical Features | http://arxiv.org/pdf/1511.07940v1.pdf | author:Li Wang, Ting Liu, Gang Wang, Kap Luk Chan, Qingxiong Yang category:cs.CV published:2015-11-25 summary:In this paper, we propose an approach to learn hierarchical features forvisual object tracking. First, we offline learn features robust to diversemotion patterns from auxiliary video sequences. The hierarchical features arelearned via a two-layer convolutional neural network. Embedding the temporalslowness constraint in the stacked architecture makes the learned featuresrobust to complicated motion transformations, which is important for visualobject tracking. Then, given a target video sequence, we propose a domainadaptation module to online adapt the pre-learned features according to thespecific target object. The adaptation is conducted in both layers of the deepfeature learning module so as to include appearance information of the specifictarget object. As a result, the learned hierarchical features can be robust toboth complicated motion transformations and appearance changes of targetobjects. We integrate our feature learning algorithm into three trackingmethods. Experimental results demonstrate that significant improvement can beachieved using our learned hierarchical features, especially on video sequenceswith complicated motion transformations.
arxiv-15000-247 | Principal Basis Analysis in Sparse Representation | http://arxiv.org/pdf/1511.07927v1.pdf | author:Hong Sun, Cheng-Wei Sang, Chen-Guang Liu category:cs.CV published:2015-11-25 summary:This article introduces a new signal analysis method, which can beinterpreted as a principal component analysis in sparse decomposition of thesignal. The method, called principal basis analysis, is based on a novelcriterion: reproducibility of component which is an intrinsic characteristic ofregularity in natural signals. We show how to measure reproducibility. Then wepresent the principal basis analysis method, which chooses, in a sparserepresentation of the signal, the components optimizing the reproducibilitydegree to build the so-called principal basis. With this principal basis, weshow that the underlying signal pattern could be effectively extracted fromcorrupted data. As illustration, we apply the principal basis analysis to imagedenoising corrupted by Gaussian and non-Gaussian noises, showing betterperformances than some reference methods at suppressing strong noise and atpreserving signal details.
arxiv-15000-248 | Neural GPUs Learn Algorithms | http://arxiv.org/pdf/1511.08228v3.pdf | author:Łukasz Kaiser, Ilya Sutskever category:cs.LG cs.NE published:2015-11-25 summary:Learning an algorithm from examples is a fundamental problem that has beenwidely studied. Recently it has been addressed using neural networks, inparticular by Neural Turing Machines (NTMs). These are fully differentiablecomputers that use backpropagation to learn their own programming. Despitetheir appeal NTMs have a weakness that is caused by their sequential nature:they are not parallel and are are hard to train due to their large depth whenunfolded. We present a neural network architecture to address this problem: the NeuralGPU. It is based on a type of convolutional gated recurrent unit and, like theNTM, is computationally universal. Unlike the NTM, the Neural GPU is highlyparallel which makes it easier to train and efficient to run. An essential property of algorithms is their ability to handle inputs ofarbitrary size. We show that the Neural GPU can be trained on short instancesof an algorithmic task and successfully generalize to long instances. Weverified it on a number of tasks including long addition and longmultiplication of numbers represented in binary. We train the Neural GPU onnumbers with upto 20 bits and observe no errors whatsoever while testing it,even on much longer numbers. To achieve these results we introduce a technique for training deep recurrentnetworks: parameter sharing relaxation. We also found a small amount of dropoutand gradient noise to have a large positive effect on learning andgeneralization.
arxiv-15000-249 | Learning with Memory Embeddings | http://arxiv.org/pdf/1511.07972v9.pdf | author:Volker Tresp, Cristóbal Esteban, Yinchong Yang, Stephan Baier, Denis Krompaß category:cs.AI cs.CL cs.LG published:2015-11-25 summary:Embedding learning, a.k.a. representation learning, has been shown to be ableto model large-scale semantic knowledge graphs. A key concept is a mapping ofthe knowledge graph to a tensor representation whose entries are predicted bymodels using latent representations of generalized entities. Latent variablemodels are well suited to deal with the high dimensionality and sparsity oftypical knowledge graphs. In recent publications the embedding models wereextended to also consider time evolutions, time patterns and subsymbolicrepresentations. In this paper we map embedding models, which were developedpurely as solutions to technical problems for modelling temporal knowledgegraphs, to various cognitive memory functions, in particular to semantic andconcept memory, episodic memory, sensory memory, short-term memory, and workingmemory. We discuss learning, query answering, the path from sensory input tosemantic decoding, and the relationship between episodic memory and semanticmemory. We introduce a number of hypotheses on human memory that can be derivedfrom the developed mathematical models.
arxiv-15000-250 | Strategic Dialogue Management via Deep Reinforcement Learning | http://arxiv.org/pdf/1511.08099v1.pdf | author:Heriberto Cuayáhuitl, Simon Keizer, Oliver Lemon category:cs.AI cs.LG published:2015-11-25 summary:Artificially intelligent agents equipped with strategic skills that cannegotiate during their interactions with other natural or artificial agents arestill underdeveloped. This paper describes a successful application of DeepReinforcement Learning (DRL) for training intelligent agents with strategicconversational skills, in a situated dialogue setting. Previous studies havemodelled the behaviour of strategic agents using supervised learning andtraditional reinforcement learning techniques, the latter using tabularrepresentations or learning with linear function approximation. In this study,we apply DRL with a high-dimensional state space to the strategic board game ofSettlers of Catan---where players can offer resources in exchange for othersand they can also reply to offers made by other players. Our experimentalresults report that the DRL-based learnt policies significantly outperformedseveral baselines including random, rule-based, and supervised-basedbehaviours. The DRL-based policy has a 53% win rate versus 3 automated players(`bots'), whereas a supervised player trained on a dialogue corpus in thissetting achieved only 27%, versus the same 3 bots. This result supports theclaim that DRL is a promising framework for training dialogue systems, andstrategic agents with negotiation abilities.
arxiv-15000-251 | Recurrent Instance Segmentation | http://arxiv.org/pdf/1511.08250v2.pdf | author:Bernardino Romera-Paredes, Philip H. S. Torr category:cs.CV cs.AI published:2015-11-25 summary:Instance segmentation is the problem of detecting and delineating eachdistinct object of interest appearing in an image. Current instancesegmentation approaches consist of ensembles of modules that are trainedindependently of each other, thus missing learning opportunities. Here wepropose a new instance segmentation paradigm consisting in an end-to-end methodthat learns how to segment instances sequentially. The model is based on arecurrent neural network that sequentially finds objects and theirsegmentations one at a time. This net is provided with a spatial memory thatkeeps track of what pixels have been explained and allows handling occlusion.In order to train the model we designed a new principled loss function thataccurately represents the properties of the instance segmentation problem. Inthe experiments carried out, we found that our method outperforms recentapproaches on multiple person segmentation, and all state of the art approacheson the Plant Phenotyping dataset for leaf counting.
arxiv-15000-252 | Relaxed Majorization-Minimization for Non-smooth and Non-convex Optimization | http://arxiv.org/pdf/1511.08062v1.pdf | author:Chen Xu, Zhouchen Lin, Zhenyu Zhao, Hongbin Zha category:math.OC cs.LG cs.NA published:2015-11-25 summary:We propose a new majorization-minimization (MM) method for non-smooth andnon-convex programs, which is general enough to include the existing MMmethods. Besides the local majorization condition, we only require that thedifference between the directional derivatives of the objective function andits surrogate function vanishes when the number of iterations approachesinfinity, which is a very weak condition. So our method can use a surrogatefunction that directly approximates the non-smooth objective function. Incomparison, all the existing MM methods construct the surrogate function byapproximating the smooth component of the objective function. We apply ourrelaxed MM methods to the robust matrix factorization (RMF) problem withdifferent regularizations, where our locally majorant algorithm showsadvantages over the state-of-the-art approaches for RMF. This is the firstalgorithm for RMF ensuring, without extra assumptions, that any limit point ofthe iterates is a stationary point.
arxiv-15000-253 | Towards Universal Paraphrastic Sentence Embeddings | http://arxiv.org/pdf/1511.08198v3.pdf | author:John Wieting, Mohit Bansal, Kevin Gimpel, Karen Livescu category:cs.CL cs.LG published:2015-11-25 summary:We consider the problem of learning general-purpose, paraphrastic sentenceembeddings based on supervision from the Paraphrase Database (Ganitkevitch etal., 2013). We compare six compositional architectures, evaluating them onannotated textual similarity datasets drawn both from the same distribution asthe training data and from a wide range of other domains. We find that the mostcomplex architectures, such as long short-term memory (LSTM) recurrent neuralnetworks, perform best on the in-domain data. However, in out-of-domainscenarios, simple architectures such as word averaging vastly outperform LSTMs.Our simplest averaging model is even competitive with systems tuned for theparticular tasks while also being extremely efficient and easy to use. In order to better understand how these architectures compare, we conductfurther experiments on three supervised NLP tasks: sentence similarity,entailment, and sentiment classification. We again find that the word averagingmodels perform well for sentence similarity and entailment, outperformingLSTMs. However, on sentiment classification, we find that the LSTM performsvery strongly-even recording new state-of-the-art performance on the StanfordSentiment Treebank. We then demonstrate how to combine our pretrained sentence embeddings withthese supervised tasks, using them both as a prior and as a black box featureextractor. This leads to performance rivaling the state of the art on the SICKsimilarity and entailment tasks. We release all of our resources to theresearch community with the hope that they can serve as the new baseline forfurther work on universal sentence embeddings.
arxiv-15000-254 | Temporal Convolutional Neural Networks for Diagnosis from Lab Tests | http://arxiv.org/pdf/1511.07938v4.pdf | author:Narges Razavian, David Sontag category:cs.LG published:2015-11-25 summary:Early diagnosis of treatable diseases is essential for improving healthcare,and many diseases' onsets are predictable from annual lab tests and theirtemporal trends. We introduce a multi-resolution convolutional neural networkfor early detection of multiple diseases from irregularly measured sparse labvalues. Our novel architecture takes as input both an imputed version of thedata and a binary observation matrix. For imputing the temporal sparseobservations, we develop a flexible, fast to train method for differentiablemultivariate kernel regression. Our experiments on data from 298K individualsover 8 years, 18 common lab measurements, and 171 diseases show that thetemporal signatures learned via convolution are significantly more predictivethan baselines commonly used for early disease diagnosis.
arxiv-15000-255 | Pedestrian Detection Inspired by Appearance Constancy and Shape Symmetry | http://arxiv.org/pdf/1511.08058v1.pdf | author:Jiale Cao, Yanwei Pang, Xuelong Li category:cs.CV published:2015-11-25 summary:The discrimination and simplicity of features are very important foreffective and efficient pedestrian detection. However, most state-of-the-artmethods are unable to achieve good tradeoff between accuracy and efficiency.Inspired by some simple inherent attributes of pedestrians (i.e., appearanceconstancy and shape symmetry), we propose two new types of non-neighboringfeatures (NNF): side-inner difference features (SIDF) and symmetricalsimilarity features (SSF). SIDF can characterize the difference between thebackground and pedestrian and the difference between the pedestrian contour andits inner part. SSF can capture the symmetrical similarity of pedestrian shape.However, it's difficult for neighboring features to have such abovecharacterization abilities. Finally, we propose to combine both non-neighboringand neighboring features for pedestrian detection. It's found thatnon-neighboring features can further decrease the average miss rate by 4.44%.Experimental results on INRIA and Caltech pedestrian datasets demonstrate theeffectiveness and efficiency of the proposed method. Compared to thestate-of-the-art methods without using CNN, our method achieves the bestdetection performance on Caltech, outperforming the second best method (i.e.,Checkboards) by 1.63%.
arxiv-15000-256 | L1-Regularized Least Squares for Support Recovery of High Dimensional Single Index Models with Gaussian Designs | http://arxiv.org/pdf/1511.08102v2.pdf | author:Matey Neykov, Jun S. Liu, Tianxi Cai category:math.ST stat.ML stat.TH published:2015-11-25 summary:It is known that for a certain class of single index models (SIM) $Y =f(\boldsymbol{X}^\intercal\boldsymbol{\beta}_0, \varepsilon)$, support recoveryis impossible when $\boldsymbol{X} \sim \mathcal{N}(0, \mathbb{I}_{p \timesp})$ and a model complexity adjusted sample size is below a critical threshold.Recently, optimal algorithms based on Sliced Inverse Regression (SIR) weresuggested. These algorithms work provably under the assumption that the designmatrix $\boldsymbol{X}$ comes from an i.i.d. Gaussian distribution. In thepresent paper we analyze algorithms based on covariance screening and leastsquares with $L_1$ penalization (i.e. LASSO) and demonstrate that they can alsoenjoy optimal (up to a scalar) rescaled sample size in terms of supportrecovery, albeit under slightly different assumptions on $f$ and $\varepsilon$compared to the SIR based algorithms. Furthermore, we show more generally, thatLASSO succeeds in recovering the signed support of $\boldsymbol{\beta}_0$ if$\boldsymbol{X} \sim \mathcal{N}(0, \boldsymbol{\Sigma})$, and the covariance$\boldsymbol{\Sigma}$ satisfies the irrepresentable condition. Our work extendsexisting results on the support recovery of LASSO for the linear model, to acertain class of SIM.
arxiv-15000-257 | Unifying Decision Trees Split Criteria Using Tsallis Entropy | http://arxiv.org/pdf/1511.08136v4.pdf | author:Yisen Wang, Chaobing Song, Shu-Tao Xia category:stat.ML cs.AI cs.LG published:2015-11-25 summary:The construction of efficient and effective decision trees remains a keytopic in machine learning because of their simplicity and flexibility. A lot ofheuristic algorithms have been proposed to construct near-optimal decisiontrees. ID3, C4.5 and CART are classical decision tree algorithms and the splitcriteria they used are Shannon entropy, Gain Ratio and Gini index respectively.All the split criteria seem to be independent, actually, they can be unified ina Tsallis entropy framework. Tsallis entropy is a generalization of Shannonentropy and provides a new approach to enhance decision trees' performance withan adjustable parameter $q$. In this paper, a Tsallis Entropy Criterion (TEC)algorithm is proposed to unify Shannon entropy, Gain Ratio and Gini index,which generalizes the split criteria of decision trees. More importantly, wereveal the relations between Tsallis entropy with different $q$ and other splitcriteria. Experimental results on UCI data sets indicate that the TEC algorithmachieves statistically significant improvement over the classical algorithms.
arxiv-15000-258 | Higher Order Conditional Random Fields in Deep Neural Networks | http://arxiv.org/pdf/1511.08119v3.pdf | author:Anurag Arnab, Sadeep Jayasumana, Shuai Zheng, Philip Torr category:cs.CV published:2015-11-25 summary:We address the problem of semantic segmentation using deep learning. Mostsegmentation systems include a Conditional Random Field (CRF) to produce astructured output that is consistent with the image's visual features. Recentdeep learning approaches have incorporated CRFs into Convolutional NeuralNetworks (CNNs), with some even training the CRF end-to-end with the rest ofthe network. However, these approaches have not employed higher orderpotentials, which have previously been shown to significantly improvesegmentation performance. In this paper, we demonstrate that two types ofhigher order potential, based on object detections and superpixels, can beincluded in a CRF embedded within a deep network. We design these higher orderpotentials to allow inference with the differentiable mean field algorithm. Asa result, all the parameters of our richer CRF model can be learned end-to-endwith our pixelwise CNN classifier. We achieve state-of-the-art segmentationperformance on the PASCAL VOC benchmark with these trainable higher orderpotentials.
arxiv-15000-259 | Generalized Conjugate Gradient Methods for $\ell_1$ Regularized Convex Quadratic Programming with Finite Convergence | http://arxiv.org/pdf/1511.07837v3.pdf | author:Zhaosong Lu, Xiaojun Chen category:math.OC cs.LG math.NA stat.CO stat.ML published:2015-11-24 summary:The conjugate gradient (CG) method is an efficient iterative method forsolving large-scale strongly convex quadratic programming (QP). In this paperwe propose some generalized CG (GCG) methods for solving the$\ell_1$-regularized (possibly not strongly) convex QP that terminate at anoptimal solution in a finite number of iterations. At each iteration, ourmethods first identify a face of an orthant and then either perform an exactline search along the direction of the negative projected minimum-normsubgradient of the objective function or execute a CG subroutine that conductsa sequence of CG iterations until a CG iterate crosses the boundary of thisface or an approximate minimizer of over this face or a subface is found. Wedetermine which type of step should be taken by comparing the magnitude of somecomponents of the minimum-norm subgradient of the objective function to that ofits rest components. Our analysis on finite convergence of these methods makesuse of an error bound result and some key properties of the aforementionedexact line search and the CG subroutine. We also show that the proposed methodsare capable of finding an approximate solution of the problem by allowing someinexactness on the execution of the CG subroutine. The overall arithmeticoperation cost of our GCG methods for finding an $\epsilon$-optimal solutiondepends on $\epsilon$ in $O(\log(1/\epsilon))$, which is superior to theaccelerated proximal gradient method [2,23] that depends on $\epsilon$ in$O(1/\sqrt{\epsilon})$. In addition, our GCG methods can be extendedstraightforwardly to solve box-constrained convex QP with finite convergence.Numerical results demonstrate that our methods are very favorable for solvingill-conditioned problems.
arxiv-15000-260 | Dynamic Capacity Networks | http://arxiv.org/pdf/1511.07838v6.pdf | author:Amjad Almahairi, Nicolas Ballas, Tim Cooijmans, Yin Zheng, Hugo Larochelle, Aaron Courville category:cs.LG cs.NE published:2015-11-24 summary:We introduce the Dynamic Capacity Network (DCN), a neural network that canadaptively assign its capacity across different portions of the input data.This is achieved by combining modules of two types: low-capacity sub-networksand high-capacity sub-networks. The low-capacity sub-networks are appliedacross most of the input, but also provide a guide to select a few portions ofthe input on which to apply the high-capacity sub-networks. The selection ismade using a novel gradient-based attention mechanism, that efficientlyidentifies input regions for which the DCN's output is most sensitive and towhich we should devote more capacity. We focus our empirical evaluation on theCluttered MNIST and SVHN image datasets. Our findings indicate that DCNs areable to drastically reduce the number of computations, compared to traditionalconvolutional neural networks, while maintaining similar or even betterperformance.
arxiv-15000-261 | rnn : Recurrent Library for Torch | http://arxiv.org/pdf/1511.07889v2.pdf | author:Nicholas Léonard, Sagar Waghmare, Yang Wang, Jin-Hwa Kim category:cs.NE published:2015-11-24 summary:The rnn package provides components for implementing a wide range ofRecurrent Neural Networks. It is built withing the framework of the Torchdistribution for use with the nn package. The components have evolved from 3iterations, each adding to the flexibility and capability of the package. Allcomponent modules inherit either the AbstractRecurrent or AbstractSequencerclasses. Strong unit testing, continued backwards compatibility and access tosupporting material are the principles followed during its development. Thepackage is compared against existing implementations of two published papers.
arxiv-15000-262 | LocNet: Improving Localization Accuracy for Object Detection | http://arxiv.org/pdf/1511.07763v2.pdf | author:Spyros Gidaris, Nikos Komodakis category:cs.CV cs.LG cs.NE published:2015-11-24 summary:We propose a novel object localization methodology with the purpose ofboosting the localization accuracy of state-of-the-art object detectionsystems. Our model, given a search region, aims at returning the bounding boxof an object of interest inside this region. To accomplish its goal, it relieson assigning conditional probabilities to each row and column of this region,where these probabilities provide useful information regarding the location ofthe boundaries of the object inside the search region and allow the accurateinference of the object bounding box under a simple probabilistic framework. For implementing our localization model, we make use of a convolutionalneural network architecture that is properly adapted for this task, calledLocNet. We show experimentally that LocNet achieves a very significantimprovement on the mAP for high IoU thresholds on PASCAL VOC2007 test set andthat it can be very easily coupled with recent state-of-the-art objectdetection systems, helping them to boost their performance. Finally, wedemonstrate that our detection approach can achieve high detection accuracyeven when it is given as input a set of sliding windows, thus proving that itis independent of box proposal methods.
arxiv-15000-263 | Performance Limits of Online Stochastic Sub-Gradient Learning | http://arxiv.org/pdf/1511.07902v2.pdf | author:Bicheng Ying, Ali H. Sayed category:stat.ML cs.LG cs.MA published:2015-11-24 summary:This work examines the performance of stochastic sub-gradient learningstrategies under weaker conditions than usually considered in the literature.The conditions are shown to be automatically satisfied by several importantcases of interest including the construction of Linear-SVM, LASSO, andTotal-Variation denoising formulations. In comparison, these problems do notsatisfy the traditional assumptions automatically and, therefore, conclusionsderived based on these earlier assumptions are not directly applicable to theseproblems. The analysis establishes that stochastic sub-gradient strategies canattain exponential convergence rates, as opposed to sub-linear rates, to thesteady-state. A realizable exponential-weighting procedure is proposed tosmooth the intermediate iterates by the sub-gradient procedure and to guaranteethe established performance bounds in terms of convergence rate and excessiverisk performance. Both single-agent and multi-agent scenarios are studied,where the latter case assumes that a collection of agents are interconnected bya topology and can only interact locally with their neighbors. The theoreticalconclusions are illustrated by several examples and simulations, includingcomparisons with the FISTA procedure.
arxiv-15000-264 | The Limitations of Deep Learning in Adversarial Settings | http://arxiv.org/pdf/1511.07528v1.pdf | author:Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, Ananthram Swami category:cs.CR cs.LG cs.NE stat.ML published:2015-11-24 summary:Deep learning takes advantage of large datasets and computationally efficienttraining algorithms to outperform other approaches at various machine learningtasks. However, imperfections in the training phase of deep neural networksmake them vulnerable to adversarial samples: inputs crafted by adversaries withthe intent of causing deep neural networks to misclassify. In this work, weformalize the space of adversaries against deep neural networks (DNNs) andintroduce a novel class of algorithms to craft adversarial samples based on aprecise understanding of the mapping between inputs and outputs of DNNs. In anapplication to computer vision, we show that our algorithms can reliablyproduce samples correctly classified by human subjects but misclassified inspecific targets by a DNN with a 97% adversarial success rate while onlymodifying on average 4.02% of the input features per sample. We then evaluatethe vulnerability of different sample classes to adversarial perturbations bydefining a hardness measure. Finally, we describe preliminary work outliningdefenses against adversarial samples by defining a predictive measure ofdistance between a benign input and a target classification.
arxiv-15000-265 | Constrained Deep Metric Learning for Person Re-identification | http://arxiv.org/pdf/1511.07545v1.pdf | author:Hailin Shi, Xiangyu Zhu, Shengcai Liao, Zhen Lei, Yang Yang, Stan Z. Li category:cs.CV published:2015-11-24 summary:Person re-identification aims to re-identify the probe image from a given setof images under different camera views. It is challenging due to largevariations of pose, illumination, occlusion and camera view. Since theconvolutional neural networks (CNN) have excellent capability of featureextraction, certain deep learning methods have been recently applied in personre-identification. However, in person re-identification, the deep networksoften suffer from the over-fitting problem. In this paper, we propose a novelCNN-based method to learn a discriminative metric with good robustness to theover-fitting problem in person re-identification. Firstly, a novel deeparchitecture is built where the Mahalanobis metric is learned with a weightconstraint. This weight constraint is used to regularize the learning, so thatthe learned metric has a better generalization ability. Secondly, we find thatthe selection of intra-class sample pairs is crucial for learning but hasreceived little attention. To cope with the large intra-class variations inpedestrian images, we propose a novel training strategy named moderate positivemining to prevent the training process from over-fitting to the extreme samplesin intra-class pairs. Experiments show that our approach significantlyoutperforms state-of-the-art methods on several benchmarks of personre-identification.
arxiv-15000-266 | Convergent Learning: Do different neural networks learn the same representations? | http://arxiv.org/pdf/1511.07543v3.pdf | author:Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, John Hopcroft category:cs.LG cs.NE published:2015-11-24 summary:Recent success in training deep neural networks have prompted activeinvestigation into the features learned on their intermediate layers. Suchresearch is difficult because it requires making sense of non-linearcomputations performed by millions of parameters, but valuable because itincreases our ability to understand current models and create improved versionsof them. In this paper we investigate the extent to which neural networksexhibit what we call convergent learning, which is when the representationslearned by multiple nets converge to a set of features which are eitherindividually similar between networks or where subsets of features span similarlow-dimensional spaces. We propose a specific method of probingrepresentations: training multiple networks and then comparing and contrastingtheir individual, learned representations at the level of neurons or groups ofneurons. We begin research into this question using three techniques toapproximately align different neural networks on a feature level: a bipartitematching approach that makes one-to-one assignments between neurons, a sparseprediction approach that finds one-to-many mappings, and a spectral clusteringapproach that finds many-to-many mappings. This initial investigation reveals afew previously unknown properties of neural networks, and we argue that futureresearch into the question of convergent learning will yield many more. Theinsights described here include (1) that some features are learned reliably inmultiple networks, yet other features are not consistently learned; (2) thatunits learn to span low-dimensional subspaces and, while these subspaces arecommon to multiple networks, the specific basis vectors learned are not; (3)that the representation codes show evidence of being a mix between a local codeand slightly, but not fully, distributed codes across multiple units.
arxiv-15000-267 | Transductive Log Opinion Pool of Gaussian Process Experts | http://arxiv.org/pdf/1511.07551v1.pdf | author:Yanshuai Cao, David J. Fleet category:cs.LG stat.ML published:2015-11-24 summary:We introduce a framework for analyzing transductive combination of Gaussianprocess (GP) experts, where independently trained GP experts are combined in away that depends on test point location, in order to scale GPs to big data. Theframework provides some theoretical justification for the generalized productof GP experts (gPoE-GP) which was previously shown to work well in practice butlacks theoretical basis. Based on the proposed framework, an improvement overgPoE-GP is introduced and empirically validated.
arxiv-15000-268 | DenseCap: Fully Convolutional Localization Networks for Dense Captioning | http://arxiv.org/pdf/1511.07571v1.pdf | author:Justin Johnson, Andrej Karpathy, Li Fei-Fei category:cs.CV cs.LG published:2015-11-24 summary:We introduce the dense captioning task, which requires a computer visionsystem to both localize and describe salient regions in images in naturallanguage. The dense captioning task generalizes object detection when thedescriptions consist of a single word, and Image Captioning when one predictedregion covers the full image. To address the localization and description taskjointly we propose a Fully Convolutional Localization Network (FCLN)architecture that processes an image with a single, efficient forward pass,requires no external regions proposals, and can be trained end-to-end with asingle round of optimization. The architecture is composed of a ConvolutionalNetwork, a novel dense localization layer, and Recurrent Neural Networklanguage model that generates the label sequences. We evaluate our network onthe Visual Genome dataset, which comprises 94,000 images and 4,100,000region-grounded captions. We observe both speed and accuracy improvements overbaselines based on current state of the art approaches in both generation andretrieval settings.
arxiv-15000-269 | Fine-Grain Annotation of Cricket Videos | http://arxiv.org/pdf/1511.07607v1.pdf | author:Rahul Anand Sharma, Pramod Sankar K, CV Jawahar category:cs.MM cs.CL cs.CV published:2015-11-24 summary:The recognition of human activities is one of the key problems in videounderstanding. Action recognition is challenging even for specific categoriesof videos, such as sports, that contain only a small set of actions.Interestingly, sports videos are accompanied by detailed commentaries availableonline, which could be used to perform action annotation in a weakly-supervisedsetting. For the specific case of Cricket videos, we address the challenge oftemporal segmentation and annotation of ctions with semantic descriptions. Oursolution consists of two stages. In the first stage, the video is segmentedinto "scenes", by utilizing the scene category information extracted fromtext-commentary. The second stage consists of classifying video-shots as wellas the phrases in the textual description into various categories. The relevantphrases are then suitably mapped to the video-shots. The novel aspect of thiswork is the fine temporal scale at which semantic information is assigned tothe video. As a result of our approach, we enable retrieval of specific actionsthat last only a few seconds, from several hours of video. This solution yieldsa large number of labeled exemplars, with no manual effort, that could be usedby machine learning algorithms to learn complex actions.
arxiv-15000-270 | Picking a Conveyor Clean by an Autonomously Learning Robot | http://arxiv.org/pdf/1511.07608v1.pdf | author:Janne V. Kujala, Tuomas J. Lukka, Harri Holopainen category:cs.RO cs.CV cs.LG published:2015-11-24 summary:We present a research picking prototype related to our company's industrialwaste sorting application. The goal of the prototype is to be as autonomous aspossible and it both calibrates itself and improves its picking with minimalhuman intervention. The system learns to pick objects better based on afeedback sensor in its gripper and uses machine learning to choosing the bestproposal from a random sample produced by simple hard-coded geometric models.We show experimentally the system improving its picking autonomously bymeasuring the pick success rate as function of time. We also show how thissystem can pick a conveyor belt clean, depositing 70 out of 80 objects in adifficult to manipulate pile of novel objects into the correct chute. Wediscuss potential improvements and next steps in this direction.
arxiv-15000-271 | Mouse Pose Estimation From Depth Images | http://arxiv.org/pdf/1511.07611v1.pdf | author:Ashwin Nanjappa, Li Cheng, Wei Gao, Chi Xu, Adam Claridge-Chang, Zoe Bichler category:cs.CV published:2015-11-24 summary:We focus on the challenging problem of efficient mouse 3D pose estimationbased on static images, and especially single depth images. We introduce anapproach to discriminatively train the split nodes of trees in random forest toimprove their performance on estimation of 3D joint positions of mouse. Ouralgorithm is capable of working with different types of rodents and withdifferent types of depth cameras and imaging setups. In particular, it isdemonstrated in this paper that when a top-mounted depth camera is combinedwith a bottom-mounted color camera, the final system is capable of deliveringfull-body pose estimation including four limbs and the paws. Empiricalexaminations on synthesized and real-world depth images confirm theapplicability of our approach on mouse pose estimation, as well as the closelyrelated task of part-based labeling of mouse.
arxiv-15000-272 | Searching for Objects using Structure in Indoor Scenes | http://arxiv.org/pdf/1511.07710v1.pdf | author:Varun K. Nagaraja, Vlad I. Morariu, Larry S. Davis category:cs.CV cs.AI published:2015-11-24 summary:To identify the location of objects of a particular class, a passive computervision system generally processes all the regions in an image to finally outputfew regions. However, we can use structure in the scene to search for objectswithout processing the entire image. We propose a search technique thatsequentially processes image regions such that the regions that are more likelyto correspond to the query class object are explored earlier. We frame theproblem as a Markov decision process and use an imitation learning algorithm tolearn a search strategy. Since structure in the scene is essential for search,we work with indoor scene images as they contain both unary scene contextinformation and object-object context in the scene. We perform experiments onthe NYU-depth v2 dataset and show that the unary scene context features alonecan achieve a significantly high average precision while processing only20-25\% of the regions for classes like bed and sofa. By consideringobject-object context along with the scene context features, the performance isfurther improved for classes like counter, lamp, pillow and sofa.
arxiv-15000-273 | Shape and Symmetry Induction for 3D Objects | http://arxiv.org/pdf/1511.07845v2.pdf | author:Shubham Tulsiani, Abhishek Kar, Qixing Huang, João Carreira, Jitendra Malik category:cs.CV published:2015-11-24 summary:Actions as simple as grasping an object or navigating around it require arich understanding of that object's 3D shape from a given viewpoint. In thispaper we repurpose powerful learning machinery, originally developed for objectclassification, to discover image cues relevant for recovering the 3D shape ofpotentially unfamiliar objects. We cast the problem as one of local predictionof surface normals and global detection of 3D reflection symmetry planes, whichopen the door for extrapolating occluded surfaces from visible ones. Wedemonstrate that our method is able to recover accurate 3D shape informationfor classes of objects it was not trained on, in both synthetic and realimages.
arxiv-15000-274 | Statistical Properties of the Single Linkage Hierarchical Clustering Estimator | http://arxiv.org/pdf/1511.07715v1.pdf | author:Dekang Zhu, Dan P. Guralnik, Xuezhi Wang, Xiang Li, Bill Moran category:stat.ML published:2015-11-24 summary:Distance-based hierarchical clustering (HC) methods are widely used inunsupervised data analysis but few authors take account of uncertainty in thedistance data. We incorporate a statistical model of the uncertainty throughcorruption or noise in the pairwise distances and investigate the problem ofestimating the HC as unknown parameters from measurements. Specifically, wefocus on single linkage hierarchical clustering (SLHC) and study its geometry.We prove that under fairly reasonable conditions on the probabilitydistribution governing measurements, SLHC is equivalent to maximum partialprofile likelihood estimation (MPPLE) with some of the information contained inthe data ignored. At the same time, we show that direct evaluation of SLHC onmaximum likelihood estimation (MLE) of pairwise distances yields a consistentestimator. Consequently, a full MLE is expected to perform better than SLHC ingetting the correct HC results for the ground truth metric.
arxiv-15000-275 | Context-aware CNNs for person head detection | http://arxiv.org/pdf/1511.07917v1.pdf | author:Tuan-Hung Vu, Anton Osokin, Ivan Laptev category:cs.CV cs.LG published:2015-11-24 summary:Person detection is a key problem for many computer vision tasks. While facedetection has reached maturity, detecting people under a full variation ofcamera view-points, human poses, lighting conditions and occlusions is still adifficult challenge. In this work we focus on detecting human heads in naturalscenes. Starting from the recent local R-CNN object detector, we extend it withtwo types of contextual cues. First, we leverage person-scene relations andpropose a Global CNN model trained to predict positions and scales of headsdirectly from the full image. Second, we explicitly model pairwise relationsamong objects and train a Pairwise CNN model using a structured-outputsurrogate loss. The Local, Global and Pairwise models are combined into a jointCNN framework. To train and test our full model, we introduce a large datasetcomposed of 369,846 human heads annotated in 224,740 movie frames. We evaluateour method and demonstrate improvements of person head detection againstseveral recent baselines in three datasets. We also show improvements of thedetection speed provided by our model.
arxiv-15000-276 | Natural Language Understanding with Distributed Representation | http://arxiv.org/pdf/1511.07916v1.pdf | author:Kyunghyun Cho category:cs.CL stat.ML published:2015-11-24 summary:This is a lecture note for the course DS-GA 3001 <Natural LanguageUnderstanding with Distributed Representation> at the Center for Data Science ,New York University in Fall, 2015. As the name of the course suggests, thislecture note introduces readers to a neural network based approach to naturallanguage understanding/processing. In order to make it as self-contained aspossible, I spend much time on describing basics of machine learning and neuralnetworks, only after which how they are used for natural languages isintroduced. On the language front, I almost solely focus on language modellingand machine translation, two of which I personally find most fascinating andmost fundamental to natural language understanding.
arxiv-15000-277 | Bayesian Identification of Fixations, Saccades, and Smooth Pursuits | http://arxiv.org/pdf/1511.07732v1.pdf | author:Thiago Santini, Wolfgang Fuhl, Thomas Kübler, Enkelejda Kasneci category:cs.CV published:2015-11-24 summary:Smooth pursuit eye movements provide meaningful insights and information onsubject's behavior and health and may, in particular situations, disturb theperformance of typical fixation/saccade classification algorithms. Thus, anautomatic and efficient algorithm to identify these eye movements is paramountfor eye-tracking research involving dynamic stimuli. In this paper, we proposethe Bayesian Decision Theory Identification (I-BDT) algorithm, a novelalgorithm for ternary classification of eye movements that is able to reliablyseparate fixations, saccades, and smooth pursuits in an online fashion, evenfor low-resolution eye trackers. The proposed algorithm is evaluated on fourdatasets with distinct mixtures of eye movements, including fixations,saccades, as well as straight and circular smooth pursuits; data was collectedwith a sample rate of 30 Hz from six subjects, totaling 24 evaluation datasets.The algorithm exhibits high and consistent performance across all datasets andmovements relative to a manual annotation by a domain expert (recall: \mu =91.42%, \sigma = 9.52%; precision: \mu = 95.60%, \sigma = 5.29%; specificity\mu = 95.41%, \sigma = 7.02%) and displays a significant improvement whencompared to I-VDT, an state-of-the-art algorithm (recall: \mu = 87.67%, \sigma= 14.73%; precision: \mu = 89.57%, \sigma = 8.05%; specificity \mu = 92.10%,\sigma = 11.21%). For algorithm implementation and annotated datasets, pleasecontact the first author.
arxiv-15000-278 | Spoken Language Translation for Polish | http://arxiv.org/pdf/1511.07788v1.pdf | author:Krzysztof Marasek, Łukasz Brocki, Danijel Korzinek, Krzysztof Wołk, Ryszard Gubrynowicz category:cs.CL published:2015-11-24 summary:Spoken language translation (SLT) is becoming more important in theincreasingly globalized world, both from a social and economic point of view.It is one of the major challenges for automatic speech recognition (ASR) andmachine translation (MT), driving intense research activities in these areas.While past research in SLT, due to technology limitations, dealt mostly withspeech recorded under controlled conditions, today's major challenge is thetranslation of spoken language as it can be found in real life. Consideredapplication scenarios range from portable translators for tourists, lecturesand presentations translation, to broadcast news and shows with livecaptioning. We would like to present PJIIT's experiences in the SLT gained fromthe Eu-Bridge 7th framework project and the U-Star consortium activities forthe Polish/English language pair. Presented research concentrates on ASRadaptation for Polish (state-of-the-art acoustic models: DBN-BLSTM training,Kaldi: LDA+MLLT+SAT+MMI), language modeling for ASR & MT (text normalization,RNN-based LMs, n-gram model domain interpolation) and statistical translationtechniques (hierarchical models, factored translation models, automatic casingand punctuation, comparable and bilingual corpora preparation). While resultsfor the well-defined domains (phrases for travelers, parliament speeches,medical documentation, movie subtitling) are very encouraging, less defineddomains (presentation, lectures) still form a challenge. Our progress in theIWSLT TED task (MT only) will be presented, as well as current progress in thePolish ASR.
arxiv-15000-279 | Weakly Supervised Object Boundaries | http://arxiv.org/pdf/1511.07803v1.pdf | author:Anna Khoreva, Rodrigo Benenson, Mohamed Omran, Matthias Hein, Bernt Schiele category:cs.CV published:2015-11-24 summary:State-of-the-art learning based boundary detection methods require extensivetraining data. Since labelling object boundaries is one of the most expensivetypes of annotations, there is a need to relax the requirement to carefullyannotate images to make both the training more affordable and to extend theamount of training data. In this paper we propose a technique to generateweakly supervised annotations and show that bounding box annotations alonesuffice to reach high-quality object boundaries without using anyobject-specific boundary annotations. With the proposed weak supervisiontechniques we achieve the top performance on the object boundary detectiontask, outperforming by a large margin the current fully supervisedstate-of-the-art methods.
arxiv-15000-280 | Private Posterior distributions from Variational approximations | http://arxiv.org/pdf/1511.07896v1.pdf | author:Vishesh Karwa, Dan Kifer, Aleksandra B. Slavković category:stat.ML cs.CR cs.LG published:2015-11-24 summary:Privacy preserving mechanisms such as differential privacy inject additionalrandomness in the form of noise in the data, beyond the sampling mechanism.Ignoring this additional noise can lead to inaccurate and invalid inferences.In this paper, we incorporate the privacy mechanism explicitly into thelikelihood function by treating the original data as missing, with an end goalof estimating posterior distributions over model parameters. This leads to aprincipled way of performing valid statistical inference using private data,however, the corresponding likelihoods are intractable. In this paper, wederive fast and accurate variational approximations to tackle such intractablelikelihoods that arise due to privacy. We focus on estimating posteriordistributions of parameters of the naive Bayes log-linear model, where thesufficient statistics of this model are shared using a differentially privateinterface. Using a simulation study, we show that the posterior approximationsoutperform the naive method of ignoring the noise addition mechanism.
arxiv-15000-281 | Super-Linear Gate and Super-Quadratic Wire Lower Bounds for Depth-Two and Depth-Three Threshold Circuits | http://arxiv.org/pdf/1511.07860v1.pdf | author:Daniel M. Kane, Ryan Williams category:cs.CC cs.NE 68Q17 C.1.3; F.1.3 published:2015-11-24 summary:In order to formally understand the power of neural computing, we first needto crack the frontier of threshold circuits with two and three layers, a regimethat has been surprisingly intractable to analyze. We prove the firstsuper-linear gate lower bounds and the first super-quadratic wire lower boundsfor depth-two linear threshold circuits with arbitrary weights, and depth-threemajority circuits computing an explicit function. $\bullet$ We prove that for all $\epsilon\gg \sqrt{\log(n)/n}$, thelinear-time computable Andreev's function cannot be computed on a$(1/2+\epsilon)$-fraction of $n$-bit inputs by depth-two linear thresholdcircuits of $o(\epsilon^3 n^{3/2}/\log^3 n)$ gates, nor can it be computed with$o(\epsilon^{3} n^{5/2}/\log^{7/2} n)$ wires. This establishes an average-case``size hierarchy'' for threshold circuits, as Andreev's function is computableby uniform depth-two circuits of $o(n^3)$ linear threshold gates, and byuniform depth-three circuits of $O(n)$ majority gates. $\bullet$ We present a new function in $P$ based on small-biased sets, whichwe prove cannot be computed by a majority vote of depth-two linear thresholdcircuits with $o(n^{3/2}/\log^3 n)$ gates, nor with $o(n^{5/2}/\log^{7/2}n)$wires. $\bullet$ We give tight average-case (gate and wire) complexity results forcomputing PARITY with depth-two threshold circuits; the answer turns out to bethe same as for depth-two majority circuits. The key is a new random restriction lemma for linear threshold functions. Ourmain analytical tool is the Littlewood-Offord Lemma from additivecombinatorics.
arxiv-15000-282 | DeePM: A Deep Part-Based Model for Object Detection and Semantic Part Localization | http://arxiv.org/pdf/1511.07131v3.pdf | author:Jun Zhu, Xianjie Chen, Alan L. Yuille category:cs.CV published:2015-11-23 summary:In this paper, we propose a deep part-based model (DeePM) for symbioticobject detection and semantic part localization. For this purpose, we annotatesemantic parts for all 20 object categories on the PASCAL VOC 2012 dataset,which provides information on object pose, occlusion, viewpoint andfunctionality. DeePM is a latent graphical model based on the state-of-the-artR-CNN framework, which learns an explicit representation of the object-partconfiguration with flexible type sharing (e.g., a sideview horse head can beshared by a fully-visible sideview horse and a highly truncated sideview horsewith head and neck only). For comparison, we also present an end-to-endObject-Part (OP) R-CNN which learns an implicit feature representation forjointly mapping an image ROI to the object and part bounding boxes. We evaluatethe proposed methods for both the object and part detection performance onPASCAL VOC 2012, and show that DeePM consistently outperforms OP R-CNN indetecting objects and parts. In addition, it obtains superior performance toFast and Faster R-CNNs in object detection.
arxiv-15000-283 | Multi-Scale Context Aggregation by Dilated Convolutions | http://arxiv.org/pdf/1511.07122v3.pdf | author:Fisher Yu, Vladlen Koltun category:cs.CV published:2015-11-23 summary:State-of-the-art models for semantic segmentation are based on adaptations ofconvolutional networks that had originally been designed for imageclassification. However, dense prediction and image classification arestructurally different. In this work, we develop a new convolutional networkmodule that is specifically designed for dense prediction. The presented moduleuses dilated convolutions to systematically aggregate multi-scale contextualinformation without losing resolution. The architecture is based on the factthat dilated convolutions support exponential expansion of the receptive fieldwithout loss of resolution or coverage. We show that the presented contextmodule increases the accuracy of state-of-the-art semantic segmentationsystems. In addition, we examine the adaptation of image classificationnetworks to dense prediction and show that simplifying the adapted network canincrease accuracy.
arxiv-15000-284 | Towards Adapting Deep Visuomotor Representations from Simulated to Real Environments | http://arxiv.org/pdf/1511.07111v3.pdf | author:Eric Tzeng, Coline Devin, Judy Hoffman, Chelsea Finn, Xingchao Peng, Sergey Levine, Kate Saenko, Trevor Darrell category:cs.CV published:2015-11-23 summary:We address the problem of adapting robotic perception from simulated toreal-world environments. For many robotic control tasks, real training imageryis expensive to obtain, but a large number of synthetic images is easy togenerate through simulation. We propose a method that adapts visualrepresentations using a small number of paired synthetic and real views of thesame scene. Our model generalizes prior approaches and combines a standardin-domain loss, a cross-domain adaptation loss, and a contrastive lossexplicitly designed to align pairs of images in feature space. We presume asynthetic dataset comprised of views that are a superset of a small number ofreal views, where the alignment may be either explicit or latent. We evaluateour approach on a manipulation task and show that by exploiting the presence ofsynthetic-real image pairs, our model is able to compensate for domain shiftmore effectively than conventional initialization techniques. Our results serveas an initial step toward pretraining deep visuomotor policies entirely insimulation, significantly reducing physical demands when learning complexpolicies.
arxiv-15000-285 | NetVLAD: CNN architecture for weakly supervised place recognition | http://arxiv.org/pdf/1511.07247v3.pdf | author:Relja Arandjelović, Petr Gronat, Akihiko Torii, Tomas Pajdla, Josef Sivic category:cs.CV cs.LG published:2015-11-23 summary:We tackle the problem of large scale visual place recognition, where the taskis to quickly and accurately recognize the location of a given queryphotograph. We present the following three principal contributions. First, wedevelop a convolutional neural network (CNN) architecture that is trainable inan end-to-end manner directly for the place recognition task. The maincomponent of this architecture, NetVLAD, is a new generalized VLAD layer,inspired by the "Vector of Locally Aggregated Descriptors" image representationcommonly used in image retrieval. The layer is readily pluggable into any CNNarchitecture and amenable to training via backpropagation. Second, we develop atraining procedure, based on a new weakly supervised ranking loss, to learnparameters of the architecture in an end-to-end manner from images depictingthe same places over time downloaded from Google Street View Time Machine.Finally, we show that the proposed architecture significantly outperformsnon-learnt image representations and off-the-shelf CNN descriptors on twochallenging place recognition benchmarks, and improves over currentstate-of-the-art compact image representations on standard image retrievalbenchmarks.
arxiv-15000-286 | Weak Convergence Properties of Constrained Emphatic Temporal-difference Learning with Constant and Slowly Diminishing Stepsize | http://arxiv.org/pdf/1511.07471v2.pdf | author:Huizhen Yu category:cs.LG published:2015-11-23 summary:We consider the emphatic temporal-difference (TD) algorithm, ETD($\lambda$),for learning the value functions of stationary policies in a discounted, finitestate and action Markov decision process. The ETD($\lambda$) algorithm wasrecently proposed by Sutton, Mahmood, and White to solve a long-standingdivergence problem of the standard TD algorithm when it is applied tooff-policy training, where data from an exploratory policy are used to evaluateother policies of interest. The almost sure convergence of ETD($\lambda$) hasbeen proved in our recent work under general off-policy training conditions,but for a narrow range of diminishing stepsize. In this paper we presentconvergence results for constrained versions of ETD($\lambda$) with constantstepsize and with diminishing stepsize from a broad range. Our resultscharacterize the asymptotic behavior of the trajectory of iterates produced bythose algorithms, and are derived by combining key properties of ETD($\lambda$)with powerful convergence theorems from the weak convergence methods instochastic approximation theory. For the case of constant stepsize, in additionto analyzing the behavior of the algorithms in the limit as the stepsizeparameter approaches zero, we also analyze their behavior for a fixed stepsizeand bound the deviations of their averaged iterates from the desired solution.These results are obtained by exploiting the weak Feller property of the Markovchains associated with the algorithms, and by using ergodic theorems for weakFeller Markov chains, in conjunction with the convergence results we get fromthe weak convergence methods. Besides ETD($\lambda$), our analysis also appliesto the off-policy TD($\lambda$) algorithm, when the divergence issue is avoidedby setting $\lambda$ sufficiently large.
arxiv-15000-287 | MazeBase: A Sandbox for Learning from Games | http://arxiv.org/pdf/1511.07401v2.pdf | author:Sainbayar Sukhbaatar, Arthur Szlam, Gabriel Synnaeve, Soumith Chintala, Rob Fergus category:cs.LG cs.AI cs.NE published:2015-11-23 summary:This paper introduces MazeBase: an environment for simple 2D games, designedas a sandbox for machine learning approaches to reasoning and planning. Withinit, we create 10 simple games embodying a range of algorithmic tasks (e.g.if-then statements or set negation). A variety of neural models (fullyconnected, convolutional network, memory network) are deployed viareinforcement learning on these games, with and without a procedurallygenerated curriculum. Despite the tasks' simplicity, the performance of themodels is far from optimal, suggesting directions for future development. Wealso demonstrate the versatility of MazeBase by using it to emulate smallcombat scenarios from StarCraft. Models trained on the MazeBase version can bedirectly applied to StarCraft, where they consistently beat the in-game AI.
arxiv-15000-288 | Ridge Leverage Scores for Low-Rank Approximation | http://arxiv.org/pdf/1511.07263v1.pdf | author:Michael B. Cohen, Cameron Musco, Christopher Musco category:cs.DS cs.LG published:2015-11-23 summary:Often used as importance sampling probabilities, leverage scores have becomeindispensable in randomized algorithms for linear algebra, optimization, graphtheory, and machine learning. A major body of work seeks to adapt these scoresto low-rank approximation problems. However, existing "low-rank leveragescores" can be difficult to compute, often work for just a single application,and are sensitive to matrix perturbations. We show how to avoid these issues by exploiting connections between low-rankapproximation and regularization. Specifically, we employ ridge leveragescores, which are simply standard leverage scores computed with respect to an$\ell_2$ regularized input. Importance sampling by these scores gives the firstunified solution to two of the most important low-rank sampling problems:$(1+\epsilon)$ error column subset selection and $(1+\epsilon)$ errorprojection-cost preservation. Moreover, ridge leverage scores satisfy a key monotonicity property that doesnot hold for any prior low-rank leverage scores. Their resulting robustnessleads to two sought-after results in randomized linear algebra. 1) We give thefirst input-sparsity time low-rank approximation algorithm based on iterativecolumn sampling, resolving an open question posed in [LMP13], [CLM+15], and[AM15]. 2) We give the first single-pass streaming column subset selectionalgorithm whose real-number space complexity has no dependence on streamlength.
arxiv-15000-289 | Noisy Submodular Maximization via Adaptive Sampling with Applications to Crowdsourced Image Collection Summarization | http://arxiv.org/pdf/1511.07211v2.pdf | author:Adish Singla, Sebastian Tschiatschek, Andreas Krause category:cs.AI cs.LG stat.ML published:2015-11-23 summary:We address the problem of maximizing an unknown submodular function that canonly be accessed via noisy evaluations. Our work is motivated by the task ofsummarizing content, e.g., image collections, by leveraging users' feedback inform of clicks or ratings. For summarization tasks with the goal of maximizingcoverage and diversity, submodular set functions are a natural choice. When theunderlying submodular function is unknown, users' feedback can provide noisyevaluations of the function that we seek to maximize. We provide a genericalgorithm -- \submM{} -- for maximizing an unknown submodular function undercardinality constraints. This algorithm makes use of a novel exploration module-- \blbox{} -- that proposes good elements based on adaptively sampling noisyfunction evaluations. \blbox{} is able to accommodate different kinds ofobservation models such as value queries and pairwise comparisons. We providePAC-style guarantees on the quality and sampling cost of the solution obtainedby \submM{}. We demonstrate the effectiveness of our approach in aninteractive, crowdsourced image collection summarization application.
arxiv-15000-290 | Über die Klassifizierung von Knoten in dynamischen Netzwerken mit Inhalt | http://arxiv.org/pdf/1512.04469v1.pdf | author:Martin Thoma category:cs.LG published:2015-11-23 summary:This paper explains the DYCOS-Algorithm as it was introduced in by Aggarwaland Li in 2011. It operates on graphs whichs nodes are partially labeled andautomatically adds missing labels to nodes. To do so, the DYCOS algorithm makesuse of the structure of the graph as well as content which is assigned to thenode. Aggarwal and Li measured in an experimental analysis that DYCOS adds themissing labels to a Graph with 19396 nodes of which 14814 are labeled andanother Graph with 806635 nodes of which 18999 are labeld on one core of anIntel Xeon 2.5 GHz CPU with 32 G RAM within less than a minute. Additionally,extensions of the DYCOS algorithm are proposed. ----- In dieser Arbeit wird der DYCOS-Algorithmus, wie er 2011 von Aggarwal und Livorgestellt wurde, erkl\"art. Er arbeitet auf Graphen, deren Knoten teilweisemit Beschriftungen versehen sind und erg\"anzt automatisch Beschriftungen f\"urKnoten, die bisher noch keine Beschriftung haben. Dieser Vorgang wird"Klassifizierung" genannt. Dazu verwendet er die Struktur des Graphen sowietextuelle Informationen, die den Knoten zugeordnet sind. Die von Aggarwal undLi beschriebene experimentelle Analyse ergab, dass er auch auf dynamischenGraphen mit 19396 bzw. 806635 Knoten, von denen nur 14814 bzw. 18999beschriftet waren, innerhalb von weniger als einer Minute auf einem Kern einerIntel Xeon 2.5 GHz CPU mit 32 G RAM ausgef\"uhrt werden kann. Zus\"atzlich wirddie Ver\"offentlichung von Aggarwal und Li kritisch er\"ortert und und eswerden m\"ogliche Erweiterungen des DYCOS-Algorithmus vorgeschlagen.
arxiv-15000-291 | Recombinator Networks: Learning Coarse-to-Fine Feature Aggregation | http://arxiv.org/pdf/1511.07356v2.pdf | author:Sina Honari, Jason Yosinski, Pascal Vincent, Christopher Pal category:cs.CV published:2015-11-23 summary:Deep neural networks with alternating convolutional, max-pooling anddecimation layers are widely used in state of the art architectures forcomputer vision. Max-pooling purposefully discards precise spatial informationin order to create features that are more robust, and typically organized aslower resolution spatial feature maps. On some tasks, such as whole-imageclassification, max-pooling derived features are well suited; however, fortasks requiring precise localization, such as pixel level prediction andsegmentation, max-pooling destroys exactly the information required to performwell. Precise localization may be preserved by shallow convnets without poolingbut at the expense of robustness. Can we have our max-pooled multi-layered cakeand eat it too? Several papers have proposed summation and concatenation basedmethods for combining upsampled coarse, abstract features with finer featuresto produce robust pixel level predictions. Here we introduce another model ---dubbed Recombinator Networks --- where coarse features inform finer featuresearly in their formation such that finer features can make use of severallayers of computation in deciding how to use coarse features. The model istrained once, end-to-end and performs better than summation-basedarchitectures, reducing the error from the previous state of the art on twofacial keypoint datasets, AFW and AFLW, by 30\% and beating the currentstate-of-the-art on 300W without using extra data. We improve performance evenfurther by adding a denoising prediction model based on a novel convnetformulation.
arxiv-15000-292 | Rendering refraction and reflection of eyeglasses for synthetic eye tracker images | http://arxiv.org/pdf/1511.07299v2.pdf | author:Thomas C. Kübler, Tobias Rittig, Judith Ungewiss, Christina Krauss, Enkelejda Kasneci category:cs.CV published:2015-11-23 summary:While for the evaluation of robustness of eye tracking algorithms the use ofreal-world data is essential, there are many applications where simulated,synthetic eye images are of advantage. They can generate labelled ground-truthdata for appearance based gaze estimation algorithms or enable the developmentof model based gaze estimation techniques by showing the influence on gazeestimation error of different model factors that can then be simplified orextended. We extend the generation of synthetic eye images by a simulation ofrefraction and reflection for eyeglasses. On the one hand this allows for thetesting of pupil and glint detection algorithms under different illuminationand reflection conditions, on the other hand the error of gaze estimationroutines can be estimated in conjunction with different eyeglasses. We show howa polynomial function fitting calibration performs equally well with andwithout eyeglasses, and how a geometrical eye model behaves when exposed toglasses.
arxiv-15000-293 | Black box variational inference for state space models | http://arxiv.org/pdf/1511.07367v1.pdf | author:Evan Archer, Il Memming Park, Lars Buesing, John Cunningham, Liam Paninski category:stat.ML published:2015-11-23 summary:Latent variable time-series models are among the most heavily used tools frommachine learning and applied statistics. These models have the advantage oflearning latent structure both from noisy observations and from the temporalordering in the data, where it is assumed that meaningful correlation structureexists across time. A few highly-structured models, such as the lineardynamical system with linear-Gaussian observations, have closed-form inferenceprocedures (e.g. the Kalman Filter), but this case is an exception to thegeneral rule that exact posterior inference in more complex generative modelsis intractable. Consequently, much work in time-series modeling focuses onapproximate inference procedures for one particular class of models. Here, weextend recent developments in stochastic variational inference to develop a`black-box' approximate inference technique for latent variable models withlatent dynamical structure. We propose a structured Gaussian variationalapproximate posterior that carries the same intuition as the standard Kalmanfilter-smoother but, importantly, permits us to use the same inference approachto approximate the posterior of much more general, nonlinear latent variablegenerative models. We show that our approach recovers accurate estimates in thecase of basic models with closed-form posteriors, and more interestinglyperforms well in comparison to variational approaches that were designed in abespoke fashion for specific non-conjugate models.
arxiv-15000-294 | Face Alignment Across Large Poses: A 3D Solution | http://arxiv.org/pdf/1511.07212v1.pdf | author:Xiangyu Zhu, Zhen Lei, Xiaoming Liu, Hailin Shi, Stan Z. Li category:cs.CV published:2015-11-23 summary:Face alignment, which fits a face model to an image and extracts the semanticmeanings of facial pixels, has been an important topic in CV community.However, most algorithms are designed for faces in small to medium poses (below45 degree), lacking the ability to align faces in large poses up to 90 degree.The challenges are three-fold: Firstly, the commonly used landmark-based facemodel assumes that all the landmarks are visible and is therefore not suitablefor profile views. Secondly, the face appearance varies more dramaticallyacross large poses, ranging from frontal view to profile view. Thirdly,labelling landmarks in large poses is extremely challenging since the invisiblelandmarks have to be guessed. In this paper, we propose a solution to the threeproblems in an new alignment framework, called 3D Dense Face Alignment (3DDFA),in which a dense 3D face model is fitted to the image via convolutional neutralnetwork (CNN). We also propose a method to synthesize large-scale trainingsamples in profile views to solve the third problem of data labelling.Experiments on the challenging AFLW database show that our approach achievessignificant improvements over state-of-the-art methods.
arxiv-15000-295 | Interpretable Two-level Boolean Rule Learning for Classification | http://arxiv.org/pdf/1511.07361v1.pdf | author:Guolong Su, Dennis Wei, Kush R. Varshney, Dmitry M. Malioutov category:cs.LG cs.AI published:2015-11-23 summary:This paper proposes algorithms for learning two-level Boolean rules inConjunctive Normal Form (CNF, i.e. AND-of-ORs) or Disjunctive Normal Form (DNF,i.e. OR-of-ANDs) as a type of human-interpretable classification model, aimingfor a favorable trade-off between the classification accuracy and thesimplicity of the rule. Two formulations are proposed. The first is an integerprogram whose objective function is a combination of the total number of errorsand the total number of features used in the rule. We generalize a previouslyproposed linear programming (LP) relaxation from one-level to two-level rules.The second formulation replaces the 0-1 classification error with the Hammingdistance from the current two-level rule to the closest rule that correctlyclassifies a sample. Based on this second formulation, block coordinate descentand alternating minimization algorithms are developed. Experiments show thatthe two-level rules can yield noticeably better performance than one-levelrules due to their dramatically larger modeling capacity, and the twoalgorithms based on the Hamming distance formulation are generally superior tothe other two-level rule learning methods in our comparison. A proposedapproach to binarize any fractional values in the optimal solutions of LPrelaxations is also shown to be effective.
arxiv-15000-296 | A PAC Approach to Application-Specific Algorithm Selection | http://arxiv.org/pdf/1511.07147v1.pdf | author:Rishi Gupta, Tim Roughgarden category:cs.LG cs.DS I.2.6; F.2.0 published:2015-11-23 summary:The best algorithm for a computational problem generally depends on the"relevant inputs," a concept that depends on the application domain and oftendefies formal articulation. While there is a large literature on empiricalapproaches to selecting the best algorithm for a given application domain,there has been surprisingly little theoretical analysis of the problem. This paper adapts concepts from statistical and online learning theory toreason about application-specific algorithm selection. Our models captureseveral state-of-the-art empirical and theoretical approaches to the problem,ranging from self-improving algorithms to empirical performance models, and ourresults identify conditions under which these approaches are guaranteed toperform well. We present one framework that models algorithm selection as astatistical learning problem, and our work here shows that dimension notionsfrom statistical learning theory, historically used to measure the complexityof classes of binary- and real-valued functions, are relevant in a much broaderalgorithmic context. We also study the online version of the algorithmselection problem, and give possibility and impossibility results for theexistence of no-regret learning algorithms.
arxiv-15000-297 | Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) | http://arxiv.org/pdf/1511.07289v5.pdf | author:Djork-Arné Clevert, Thomas Unterthiner, Sepp Hochreiter category:cs.LG published:2015-11-23 summary:We introduce the "exponential linear unit" (ELU) which speeds up learning indeep neural networks and leads to higher classification accuracies. Likerectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs(PReLUs), ELUs alleviate the vanishing gradient problem via the identity forpositive values. However, ELUs have improved learning characteristics comparedto the units with other activation functions. In contrast to ReLUs, ELUs havenegative values which allows them to push mean unit activations closer to zerolike batch normalization but with lower computational complexity. Mean shiftstoward zero speed up learning by bringing the normal gradient closer to theunit natural gradient because of a reduced bias shift effect. While LReLUs andPReLUs have negative values, too, they do not ensure a noise-robustdeactivation state. ELUs saturate to a negative value with smaller inputs andthereby decrease the forward propagated variation and information. Therefore,ELUs code the degree of presence of particular phenomena in the input, whilethey do not quantitatively model the degree of their absence. In experiments,ELUs lead not only to faster learning, but also to significantly bettergeneralization performance than ReLUs and LReLUs on networks with more than 5layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks withbatch normalization while batch normalization does not improve ELU networks.ELU networks are among the top 10 reported CIFAR-10 results and yield the bestpublished result on CIFAR-100, without resorting to multi-view evaluation ormodel averaging. On ImageNet, ELU networks considerably speed up learningcompared to a ReLU network with the same architecture, obtaining less than 10%classification error for a single crop, single model network.
arxiv-15000-298 | Sparse Linear Models applied to Power Quality Disturbance Classification | http://arxiv.org/pdf/1511.07281v1.pdf | author:Andrés F. López-Lopera, Mauricio A. Álvarez, Ávaro A. Orozco category:stat.AP stat.ML published:2015-11-23 summary:Power quality (PQ) analysis describes the non-pure electric signals that areusually present in electric power systems. The automatic recognition of PQdisturbances can be seen as a pattern recognition problem, in which differenttypes of waveform distortion are differentiated based on their features.Similar to other quasi-stationary signals, PQ disturbances can be decomposedinto time-frequency dependent components by using time-frequency or time-scaletransforms, also known as dictionaries. These dictionaries are used in thefeature extraction step in pattern recognition systems. Short-time Fourier,Wavelets and Stockwell transforms are some of the most common dictionaries usedin the PQ community, aiming to achieve a better signal representation. To thebest of our knowledge, previous works about PQ disturbance classification havebeen restricted to the use of one among several available dictionaries. Takingadvantage of the theory behind sparse linear models (SLM), we introduce asparse method for PQ representation, starting from overcomplete dictionaries.In particular, we apply Group Lasso. We employ different types oftime-frequency (or time-scale) dictionaries to characterize the PQdisturbances, and evaluate their performance under different patternrecognition algorithms. We show that the SLM reduce the PQ classificationcomplexity promoting sparse basis selection, and improving the classificationaccuracy.
arxiv-15000-299 | Learning Simple Algorithms from Examples | http://arxiv.org/pdf/1511.07275v2.pdf | author:Wojciech Zaremba, Tomas Mikolov, Armand Joulin, Rob Fergus category:cs.AI cs.LG published:2015-11-23 summary:We present an approach for learning simple algorithms such as copying,multi-digit addition and single digit multiplication directly from examples.Our framework consists of a set of interfaces, accessed by a controller.Typical interfaces are 1-D tapes or 2-D grids that hold the input and outputdata. For the controller, we explore a range of neural network-based modelswhich vary in their ability to abstract the underlying algorithm from traininginstances and generalize to test examples with many thousands of digits. Thecontroller is trained using $Q$-learning with several enhancements and we showthat the bottleneck is in the capabilities of the controller rather than in thesearch incurred by $Q$-learning.
arxiv-15000-300 | Constrained Structured Regression with Convolutional Neural Networks | http://arxiv.org/pdf/1511.07497v1.pdf | author:Deepak Pathak, Philipp Krähenbühl, Stella X. Yu, Trevor Darrell category:cs.CV cs.LG published:2015-11-23 summary:Convolutional Neural Networks (CNNs) have recently emerged as the dominantmodel in computer vision. If provided with enough training data, they predictalmost any visual quantity. In a discrete setting, such as classification, CNNsare not only able to predict a label but often predict a confidence in the formof a probability distribution over the output space. In continuous regressiontasks, such a probability estimate is often lacking. We present a regressionframework which models the output distribution of neural networks. This outputdistribution allows us to infer the most likely labeling following a set ofphysical or modeling constraints. These constraints capture the intricateinterplay between different input and output variables, and complement theoutput of a CNN. However, they may not hold everywhere. Our setup furtherallows to learn a confidence with which a constraint holds, in the form of adistribution of the constrain satisfaction. We evaluate our approach on theproblem of intrinsic image decomposition, and show that constrained structuredregression significantly increases the state-of-the-art.
