arxiv-15000-1 | Word Representations, Tree Models and Syntactic Functions | http://arxiv.org/pdf/1508.07709v2.pdf | author:Simon Šuster, Gertjan van Noord, Ivan Titov category:cs.CL cs.LG stat.ML published:2015-08-31 summary:Word representations induced from models with discrete latent variables(e.g.\ HMMs) have been shown to be beneficial in many NLP applications. In thiswork, we exploit labeled syntactic dependency trees and formalize the inductionproblem as unsupervised learning of tree-structured hidden Markov models.Syntactic functions are used as additional observed variables in the model,influencing both transition and emission components. Such syntactic informationcan potentially lead to capturing more fine-grain and functional distinctionsbetween words, which, in turn, may be desirable in many NLP applications. Weevaluate the word representations on two tasks -- named entity recognition andsemantic frame identification. We observe improvements from exploitingsyntactic function information in both cases, and the results rivaling those ofstate-of-the-art representation learning methods. Additionally, we revisit therelationship between sequential and unlabeled-tree models and find that theadvantage of the latter is not self-evident.
arxiv-15000-2 | Automatic and Quantitative evaluation of attribute discovery methods | http://arxiv.org/pdf/1602.01940v1.pdf | author:Liangchen Liu, Arnold Wiliem, Shaokang Chen, Brian C. Lovell category:cs.CV published:2016-02-05 summary:Many automatic attribute discovery methods have been developed to extract aset of visual attributes from images for various tasks. However, despite goodperformance in some image classification tasks, it is difficult to evaluatewhether these methods discover meaningful attributes and which one is the bestto find the attributes for image descriptions. An intuitive way to evaluatethis is to manually verify whether consistent identifiable visual conceptsexist to distinguish between positive and negative images of an attribute. Thismanual checking is tedious, labor intensive and expensive and it is very hardto get quantitative comparisons between different methods. In this work, wetackle this problem by proposing an attribute meaningfulness metric, that canperform automatic evaluation on the meaningfulness of attribute sets as well asachieving quantitative comparisons. We apply our proposed metric to recentautomatic attribute discovery methods and popular hashing methods on threeattribute datasets. A user study is also conducted to validate theeffectiveness of the metric. In our evaluation, we gleaned some insights thatcould be beneficial in developing automatic attribute discovery methods togenerate meaningful attributes. To the best of our knowledge, this is the firstwork to quantitatively measure the semantic content of automatically discoveredattributes.
arxiv-15000-3 | Bags of Affine Subspaces for Robust Object Tracking | http://arxiv.org/pdf/1408.2313v3.pdf | author:Sareh Shirazi, Conrad Sanderson, Chris McCool, Mehrtash T. Harandi category:cs.CV cs.MM cs.RO 14M15, 54B05 published:2014-08-11 summary:We propose an adaptive tracking algorithm where the object is modelled as acontinuously updated bag of affine subspaces, with each subspace constructedfrom the object's appearance over several consecutive frames. In contrast tolinear subspaces, affine subspaces explicitly model the origin of subspaces.Furthermore, instead of using a brittle point-to-subspace distance during thesearch for the object in a new frame, we propose to use a subspace-to-subspacedistance by representing candidate image areas also as affine subspaces.Distances between subspaces are then obtained by exploiting the non-Euclideangeometry of Grassmann manifolds. Experiments on challenging videos (containingobject occlusions, deformations, as well as variations in pose andillumination) indicate that the proposed method achieves higher trackingaccuracy than several recent discriminative trackers.
arxiv-15000-4 | Face Attribute Prediction with classification CNN | http://arxiv.org/pdf/1602.01827v2.pdf | author:Yang Zhong, Josephine Sullivan, Haibo Li category:cs.CV published:2016-02-04 summary:Predicting facial attributes from faces in the wild is very challenging dueto pose and lighting variations in the real world. The key to this problem isto build proper feature representations to cope with these unfavorableconditions. Given the success of convolutional neural network (CNN) in imageclassification, the high-level CNN feature as an intuitive and reasonablechoice has been widely utilized for this problem. In this paper, however, weconsider the mid-level CNN features as an alternative to the high-level onesfor attribute prediction. This is based on the observation that face attributesare different: some of them are locally oriented while others are globallydefined. Our investigations reveal that the mid-level deep representationsoutperform the prediction accuracy achieved by the high-level abstractions. Wedemonstrate that the mid-level representations achieve state-of-the-artprediction performance on CelebA and LFWA datasets. Our investigations alsoshow that by utilizing the mid-level representations one can employ a singledeep network to achieve both face recognition and attribute prediction.
arxiv-15000-5 | Fantastic 4 system for NIST 2015 Language Recognition Evaluation | http://arxiv.org/pdf/1602.01929v1.pdf | author:Kong Aik Lee, Ville Hautamäki, Anthony Larcher, Wei Rao, Hanwu Sun, Trung Hieu Nguyen, Guangsen Wang, Aleksandr Sizov, Ivan Kukanov, Amir Poorjam, Trung Ngo Trong, Xiong Xiao, Cheng-Lin Xu, Hai-Hua Xu, Bin Ma, Haizhou Li, Sylvain Meignier category:cs.CL published:2016-02-05 summary:This article describes the systems jointly submitted by Institute forInfocomm (I$^2$R), the Laboratoire d'Informatique de l'Universit\'e du Maine(LIUM), Nanyang Technology University (NTU) and the University of EasternFinland (UEF) for 2015 NIST Language Recognition Evaluation (LRE). Thesubmitted system is a fusion of nine sub-systems based on i-vectors extractedfrom different types of features. Given the i-vectors, several classifiers areadopted for the language detection task including support vector machines(SVM), multi-class logistic regression (MCLR), Probabilistic LinearDiscriminant Analysis (PLDA) and Deep Neural Networks (DNN).
arxiv-15000-6 | On Feature based Delaunay Triangulation for Palmprint Recognition | http://arxiv.org/pdf/1602.01927v1.pdf | author:Zanobya N. Khan, Rashid Jalal Qureshi, Jamil Ahmad category:cs.CV published:2016-02-05 summary:Authentication of individuals via palmprint based biometric system isbecoming very popular due to its reliability as it contains unique and stablefeatures. In this paper, we present a novel approach for palmprint recognitionand its representation. To extract the palm lines, local thresholding techniqueNiblack binarization algorithm is adopted. The endpoints of these lines aredetermined and a connection is created among them using the Delaunaytriangulation thereby generating a distinct topological structure of eachpalmprint. Next, we extract different geometric as well as quantitativefeatures from the triangles of the Delaunay triangulation that assist inidentifying different individuals. To ensure that the proposed approach isinvariant to rotation and scaling, features were made relative to topologicaland geometrical structure of the palmprint. The similarity of the twopalmprints is computed using the weighted sum approach and compared with thek-nearest neighbor. The experimental results obtained reflect the effectivenessof the proposed approach to discriminate between different palmprint images andthus achieved a recognition rate of 90% over large databases.
arxiv-15000-7 | High-dimensional Mixed Graphical Models | http://arxiv.org/pdf/1304.2810v2.pdf | author:Jie Cheng, Tianxi Li, Elizaveta Levina, Ji Zhu category:stat.ML stat.ME published:2013-04-09 summary:While graphical models for continuous data (Gaussian graphical models) anddiscrete data (Ising models) have been extensively studied, there is littlework on graphical models linking both continuous and discrete variables (mixeddata), which are common in many scientific applications. We propose a novelgraphical model for mixed data, which is simple enough to be suitable forhigh-dimensional data, yet flexible enough to represent all possible graphstructures. We develop a computationally efficient regression-based algorithmfor fitting the model by focusing on the conditional log-likelihood of eachvariable given the rest. The parameters have a natural group structure, andsparsity in the fitted graph is attained by incorporating a group lassopenalty, approximated by a weighted $\ell_1$ penalty for computationalefficiency. We demonstrate the effectiveness of our method through an extensivesimulation study and apply it to a music annotation data set (CAL500),obtaining a sparse and interpretable graphical model relating the continuousfeatures of the audio signal to categorical variables such as genre, emotions,and usage associated with particular songs. While we focus on binary discretevariables, we also show that the proposed methodology can be easily extended togeneral discrete variables.
arxiv-15000-8 | Massively Multilingual Word Embeddings | http://arxiv.org/pdf/1602.01925v1.pdf | author:Waleed Ammar, George Mulcaire, Yulia Tsvetkov, Guillaume Lample, Chris Dyer, Noah A. Smith category:cs.CL published:2016-02-05 summary:We introduce new methods for estimating and evaluating embeddings of wordsfrom dozens of languages in a single shared embedding space. Our estimationmethods, multiCluster and multiCCA, use dictionaries and monolingual data; theydo not require parallel data. Our new evaluation method, multiQVEC+, is shownto correlate better than previous ones with two downstream tasks (textcategorization and parsing). On this evaluation and others, our estimationmethods outperform existing ones. We also describe a web portal for evaluationthat will facilitate further research in this area, along with open-sourcereleases of all our methods.
arxiv-15000-9 | Characteristics of Visual Categorization of Long-Concatenated and Object-Directed Human Actions by a Multiple Spatio-Temporal Scales Recurrent Neural Network Model | http://arxiv.org/pdf/1602.01921v1.pdf | author:Haanvid Lee, Minju Jung, Jun Tani category:cs.CV cs.AI cs.LG published:2016-02-05 summary:The current paper proposes a novel dynamic neural network model, multiplespatio-temporal scales recurrent neural network (MSTRNN) used forcategorization of complex human action pattern in video image. The MSTRNN hasbeen developed by newly introducing recurrent connectivity to a prior-proposedmodel, multiple spatio-temporal scales neural network (MSTNN) [1] such that themodel can learn to extract latent spatio-temporal structures more effectivelyby developing adequate recurrent contextual dynamics. The MSTRNN was evaluatedby conducting a set of simulation experiments on learning to categorize humanaction visual patterns. The first experiment on categorizing a set oflong-concatenated human movement patterns showed that MSTRNN outperforms MSTNNin the capability of learning to extract long-ranged correlation in videoimage. The second experiment on categorizing a set of object-directed actionsshowed that the MSTRNN can learn to extract structural relationship betweenactions and directed-objects. Our analysis on the characteristics ofmiscategorization in both cases of object-directed action and pantomime actionsindicated that the model network developed the categorical memories byorganizing relational structure among them. Development of such relationalstructure is considered to be beneficial for gaining generalization incategorization.
arxiv-15000-10 | Fast Multiplier Methods to Optimize Non-exhaustive, Overlapping Clustering | http://arxiv.org/pdf/1602.01910v1.pdf | author:Yangyang Hou, Joyce Jiyoung Whang, David F. Gleich, Inderjit S. Dhillon category:cs.LG published:2016-02-05 summary:Clustering is one of the most fundamental and important tasks in data mining.Traditional clustering algorithms, such as K-means, assign every data point toexactly one cluster. However, in real-world datasets, the clusters may overlapwith each other. Furthermore, often, there are outliers that should not belongto any cluster. We recently proposed the NEO-K-Means (Non-Exhaustive,Overlapping K-Means) objective as a way to address both issues in an integratedfashion. Optimizing this discrete objective is NP-hard, and even though thereis a convex relaxation of the objective, straightforward convex optimizationapproaches are too expensive for large datasets. A practical alternative is touse a low-rank factorization of the solution matrix in the convex formulation.The resulting optimization problem is non-convex, and we can locally optimizethe objective function using an augmented Lagrangian method. In this paper, weconsider two fast multiplier methods to accelerate the convergence of anaugmented Lagrangian scheme: a proximal method of multipliers and analternating direction method of multipliers (ADMM). For the proximal augmentedLagrangian or proximal method of multipliers, we show a convergence result forthe non-convex case with bound-constrained subproblems. These methods are up to13 times faster---with no change in quality---compared with a standardaugmented Lagrangian method on problems with over 10,000 variables and bringruntimes down from over an hour to around 5 minutes.
arxiv-15000-11 | Optimizing affinity-based binary hashing using auxiliary coordinates | http://arxiv.org/pdf/1501.05352v2.pdf | author:Ramin Raziperchikolaei, Miguel Á. Carreira-Perpiñán category:cs.LG cs.CV math.OC stat.ML published:2015-01-21 summary:In supervised binary hashing, one wants to learn a function that maps ahigh-dimensional feature vector to a vector of binary codes, for application tofast image retrieval. This typically results in a difficult optimizationproblem, nonconvex and nonsmooth, because of the discrete variables involved.Much work has simply relaxed the problem during training, solving a continuousoptimization, and truncating the codes a posteriori. This gives reasonableresults but is quite suboptimal. Recent work has tried to optimize theobjective directly over the binary codes and achieved better results, but thehash function was still learned a posteriori, which remains suboptimal. Wepropose a general framework for learning hash functions using affinity-basedloss functions that uses auxiliary coordinates. This closes the loop andoptimizes jointly over the hash functions and the binary codes so that theygradually match each other. The resulting algorithm can be seen as a corrected,iterated version of the procedure of optimizing first over the codes and thenlearning the hash function. Compared to this, our optimization is guaranteed toobtain better hash functions while being not much slower, as demonstratedexperimentally in various supervised datasets. In addition, our frameworkfacilitates the design of optimization algorithms for arbitrary types of lossand hash functions.
arxiv-15000-12 | Generate Image Descriptions based on Deep RNN and Memory Cells for Images Features | http://arxiv.org/pdf/1602.01895v1.pdf | author:Shijian Tang, Song Han category:cs.CV cs.CL cs.LG published:2016-02-05 summary:Generating natural language descriptions for images is a challenging task.The traditional way is to use the convolutional neural network (CNN) to extractimage features, followed by recurrent neural network (RNN) to generatesentences. In this paper, we present a new model that added memory cells togate the feeding of image features to the deep neural network. The intuition isenabling our model to memorize how much information from images should be fedat each stage of the RNN. Experiments on Flickr8K and Flickr30K datasets showedthat our model outperforms other state-of-the-art models with higher BLEUscores.
arxiv-15000-13 | Search Tracker: Human-derived object tracking in-the-wild through large-scale search and retrieval | http://arxiv.org/pdf/1602.01890v1.pdf | author:Archith J. Bency, S. Karthikeyan, Carter De Leo, Santhoshkumar Sunderrajan, B. S. Manjunath category:cs.CV cs.MM published:2016-02-05 summary:Humans use context and scene knowledge to easily localize moving objects inconditions of complex illumination changes, scene clutter and occlusions. Inthis paper, we present a method to leverage human knowledge in the form ofannotated video libraries in a novel search and retrieval based setting totrack objects in unseen video sequences. For every video sequence, a documentthat represents motion information is generated. Documents of the unseen videoare queried against the library at multiple scales to find videos with similarmotion characteristics. This provides us with coarse localization of objects inthe unseen video. We further adapt these retrieved object locations to the newvideo using an efficient warping scheme. The proposed method is validated onin-the-wild video surveillance datasets where we outperform state-of-the-artappearance-based trackers. We also introduce a new challenging dataset withcomplex object appearance changes.
arxiv-15000-14 | Discovering Neuronal Cell Types and Their Gene Expression Profiles Using a Spatial Point Process Mixture Model | http://arxiv.org/pdf/1602.01889v1.pdf | author:Furong Huang, Animashree Anandkumar, Christian Borgs, Jennifer Chayes, Ernest Fraenkel, Michael Hawrylycz, Ed Lein, Alessandro Ingrosso, Srinivas Turaga category:q-bio.NC stat.ML published:2016-02-04 summary:Cataloging the neuronal cell types that comprise circuitry of individualbrain regions is a major goal of modern neuroscience and the BRAIN initiative.Single-cell RNA sequencing can now be used to measure the gene expressionprofiles of individual neurons and to categorize neurons based on their geneexpression profiles. However this modern tool is labor intensive, has a highcost per cell, and most importantly, does not provide information on spatialdistribution of cell types in specific regions of the brain. We propose acomputational method for inferring the cell types and their gene expressionprofiles through computational analysis of brain-wide single-cell resolution insitu hybridization (ISH) imagery contained in the Allen Brain Atlas (ABA). Wemeasure the spatial distribution of neurons labeled in the ISH image for eachgene and model it as a spatial point process mixture, whose mixture weights aregiven by the cell types which express that gene. By fitting a point processmixture model jointly to the ISH images for about two thousand genes, we inferboth the spatial point process distribution for each cell type and their geneexpression profile. We validate our predictions of cell type-specific geneexpression profiles using single cell RNA sequencing data, recently publishedfor the mouse somatosensory cortex.
arxiv-15000-15 | Training Recurrent Neural Networks by Diffusion | http://arxiv.org/pdf/1601.04114v2.pdf | author:Hossein Mobahi category:cs.LG published:2016-01-16 summary:This work presents a new algorithm for training recurrent neural networks(although ideas are applicable to feedforward networks as well). The algorithmis derived from a theory in nonconvex optimization related to the diffusionequation. The contributions made in this work are two fold. First, we show howsome seemingly disconnected mechanisms used in deep learning such as smartinitialization, annealed learning rate, layerwise pretraining, and noiseinjection (as done in dropout and SGD) arise naturally and automatically fromthis framework, without manually crafting them into the algorithms. Second, wepresent some preliminary results on comparing the proposed method against SGD.It turns out that the new algorithm can achieve similar level of generalizationaccuracy of SGD in much fewer number of epochs.
arxiv-15000-16 | Boolean Matrix Factorization and Noisy Completion via Message Passing | http://arxiv.org/pdf/1509.08535v3.pdf | author:Siamak Ravanbakhsh, Barnabas Poczos, Russell Greiner category:math.ST cs.AI cs.DM stat.ML stat.TH published:2015-09-28 summary:Boolean matrix factorization and Boolean matrix completion from noisyobservations are desirable unsupervised data-analysis methods due to theirinterpretability, but hard to perform due to their NP-hardness. We treat theseproblems as maximum a posteriori inference problems in a graphical model andpresent a message passing approach that scales linearly with the number ofobservations and factors. Our empirical study demonstrates that message passingis able to recover low-rank Boolean matrices, in the boundaries oftheoretically possible recovery and compares favorably with state-of-the-art inreal-world applications, such collaborative filtering with large-scale Booleandata.
arxiv-15000-17 | Random Feature Maps via a Layered Random Projection (LaRP) Framework for Object Classification | http://arxiv.org/pdf/1602.01818v1.pdf | author:A. G. Chung, M. J. Shafiee, A. Wong category:cs.CV cs.LG stat.ML published:2016-02-04 summary:The approximation of nonlinear kernels via linear feature maps has recentlygained interest due to their applications in reducing the training and testingtime of kernel-based learning algorithms. Current random projection methodsavoid the curse of dimensionality by embedding the nonlinear feature space intoa low dimensional Euclidean space to create nonlinear kernels. We introduce aLayered Random Projection (LaRP) framework, where we model the linear kernelsand nonlinearity separately for increased training efficiency. The proposedLaRP framework was assessed using the MNIST hand-written digits database andthe COIL-100 object database, and showed notable improvement in objectclassification performance relative to other state-of-the-art random projectionmethods.
arxiv-15000-18 | Modeling User Exposure in Recommendation | http://arxiv.org/pdf/1510.07025v2.pdf | author:Dawen Liang, Laurent Charlin, James McInerney, David M. Blei category:stat.ML cs.IR cs.LG published:2015-10-23 summary:Collaborative filtering analyzes user preferences for items (e.g., books,movies, restaurants, academic papers) by exploiting the similarity patternsacross users. In implicit feedback settings, all the items, including the onesthat a user did not consume, are taken into consideration. But this assumptiondoes not accord with the common sense understanding that users have a limitedscope and awareness of items. For example, a user might not have heard of acertain paper, or might live too far away from a restaurant to experience it.In the language of causal analysis, the assignment mechanism (i.e., the itemsthat a user is exposed to) is a latent variable that may change for varioususer/item combinations. In this paper, we propose a new probabilistic approachthat directly incorporates user exposure to items into collaborative filtering.The exposure is modeled as a latent variable and the model infers its valuefrom data. In doing so, we recover one of the most successful state-of-the-artapproaches as a special case of our model, and provide a plug-in method forconditioning exposure on various forms of exposure covariates (e.g., topics intext, venue locations). We show that our scalable inference algorithmoutperforms existing benchmarks in four different domains both with and withoutexposure covariates.
arxiv-15000-19 | Asynchronous Methods for Deep Reinforcement Learning | http://arxiv.org/pdf/1602.01783v1.pdf | author:Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu category:cs.LG published:2016-02-04 summary:We propose a conceptually simple and lightweight framework for deepreinforcement learning that uses asynchronous gradient descent for optimizationof deep neural network controllers. We present asynchronous variants of fourstandard reinforcement learning algorithms and show that parallelactor-learners have a stabilizing effect on training allowing all four methodsto successfully train neural network controllers. The best performing method,an asynchronous variant of actor-critic, surpasses the current state-of-the-arton the Atari domain while training for half the time on a single multi-core CPUinstead of a GPU. Furthermore, we show that asynchronous actor-critic succeedson a wide variety of continuous motor control problems as well as on a new taskinvolving finding rewards in random 3D mazes using a visual input.
arxiv-15000-20 | Spatial Transformer Networks | http://arxiv.org/pdf/1506.02025v3.pdf | author:Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu category:cs.CV published:2015-06-05 summary:Convolutional Neural Networks define an exceptionally powerful class ofmodels, but are still limited by the lack of ability to be spatially invariantto the input data in a computationally and parameter efficient manner. In thiswork we introduce a new learnable module, the Spatial Transformer, whichexplicitly allows the spatial manipulation of data within the network. Thisdifferentiable module can be inserted into existing convolutionalarchitectures, giving neural networks the ability to actively spatiallytransform feature maps, conditional on the feature map itself, without anyextra training supervision or modification to the optimisation process. We showthat the use of spatial transformers results in models which learn invarianceto translation, scale, rotation and more generic warping, resulting instate-of-the-art performance on several benchmarks, and for a number of classesof transformations.
arxiv-15000-21 | Correntropy Maximization via ADMM - Application to Robust Hyperspectral Unmixing | http://arxiv.org/pdf/1602.01729v1.pdf | author:Fei Zhu, Abderrahim Halimi, Paul Honeine, Badong Chen, Nanning Zheng category:stat.ML cs.CV cs.NE published:2016-02-04 summary:In hyperspectral images, some spectral bands suffer from low signal-to-noiseratio due to noisy acquisition and atmospheric effects, thus requiring robusttechniques for the unmixing problem. This paper presents a robust supervisedspectral unmixing approach for hyperspectral images. The robustness is achievedby writing the unmixing problem as the maximization of the correntropycriterion subject to the most commonly used constraints. Two unmixing problemsare derived: the first problem considers the fully-constrained unmixing, withboth the non-negativity and sum-to-one constraints, while the second one dealswith the non-negativity and the sparsity-promoting of the abundances. Thecorresponding optimization problems are solved efficiently using an alternatingdirection method of multipliers (ADMM) approach. Experiments on synthetic andreal hyperspectral images validate the performance of the proposed algorithmsfor different scenarios, demonstrating that the correntropy-based unmixing isrobust to outlier bands.
arxiv-15000-22 | NeRD: a Neural Response Divergence Approach to Visual Salience Detection | http://arxiv.org/pdf/1602.01728v1.pdf | author:M. J. Shafiee, P. Siva, C. Scharfenberger, P. Fieguth, A. Wong category:cs.CV published:2016-02-04 summary:In this paper, a novel approach to visual salience detection via NeuralResponse Divergence (NeRD) is proposed, where synaptic portions of deep neuralnetworks, previously trained for complex object recognition, are leveraged tocompute low level cues that can be used to compute image regiondistinctiveness. Based on this concept , an efficient visual salience detectionframework is proposed using deep convolutional StochasticNets. Experimentalresults using CSSD and MSRA10k natural image datasets show that the proposedNeRD approach can achieve improved performance when compared tostate-of-the-art image saliency approaches, while the attaining lowcomputational complexity necessary for near-real-time computer visionapplications.
arxiv-15000-23 | The Great Time Series Classification Bake Off: An Experimental Evaluation of Recently Proposed Algorithms. Extended Version | http://arxiv.org/pdf/1602.01711v1.pdf | author:Anthony Bagnall, Aaron Bostrom, James Large, Jason Lines category:cs.LG published:2016-02-04 summary:In the last five years there have been a large number of new time seriesclassification algorithms proposed in the literature. These algorithms havebeen evaluated on subsets of the 47 data sets in the University of California,Riverside time series classification archive. The archive has recently beenexpanded to 85 data sets, over half of which have been donated by researchersat the University of East Anglia. Aspects of previous evaluations have madecomparisons between algorithms difficult. For example, several differentprogramming languages have been used, experiments involved a single train/testsplit and some used normalised data whilst others did not. The relaunch of thearchive provides a timely opportunity to thoroughly evaluate algorithms on alarger number of datasets. We have implemented 18 recently proposed algorithmsin a common Java framework and compared them against two standard benchmarkclassifiers (and each other) by performing 100 resampling experiments on eachof the 85 datasets. We use these results to test several hypotheses relating towhether the algorithms are significantly more accurate than the benchmarks andeach other. Our results indicate that only 9 of these algorithms aresignificantly more accurate than both benchmarks and that one classifier, theCollective of Transformation Ensembles, is significantly more accurate than allof the others. All of our experiments and results are reproducible: we releaseall of our code, results and experimental details and we hope these experimentsform the basis for more rigorous testing of new algorithms in the future.
arxiv-15000-24 | Complex Networks of Words in Fables | http://arxiv.org/pdf/1602.04853v1.pdf | author:Yurij Holovatch, Vasyl Palchykov category:physics.soc-ph cs.CL published:2016-02-04 summary:In this chapter we give an overview of the application of complex networktheory to quantify some properties of language. Our study is based on twofables in Ukrainian, Mykyta the Fox and Abu-Kasym's slippers. It consists oftwo parts: the analysis of frequency-rank distributions of words and theapplication of complex-network theory. The first part shows that the text sizesare sufficiently large to observe statistical properties. This supports theirselection for the analysis of typical properties of the language networks inthe second part of the chapter. In describing language as a complex network,while words are usually associated with nodes, there is more variability in thechoice of links and different representations result in different networks.Here, we examine a number of such representations of the language network andperform a comparative analysis of their characteristics. Our results suggestthat, irrespective of link representation, the Ukrainian language network usedin the selected fables is a strongly correlated, scale-free, small world. Wediscuss how such empirical approaches may help form a useful basis for atheoretical description of language evolution and how they may be used inanalyses of other textual narratives.
arxiv-15000-25 | Minimizing the Maximal Loss: How and Why? | http://arxiv.org/pdf/1602.01690v1.pdf | author:Shai Shalev-Shwartz, Yonatan Wexler category:cs.LG published:2016-02-04 summary:A commonly used learning rule is to approximately minimize the \emph{average}loss over the training set. Other learning algorithms, such as AdaBoost andhard-SVM, aim at minimizing the \emph{maximal} loss over the training set. Theaverage loss is more popular, particularly in deep learning, due to three mainreasons. First, it can be conveniently minimized using online algorithms, thatprocess few examples at each iteration. Second, it is often argued that thereis no sense to minimize the loss on the training set too much, as it will notbe reflected in the generalization loss. Last, the maximal loss is not robustto outliers. In this paper we describe and analyze an algorithm that canconvert any online algorithm to a minimizer of the maximal loss. We prove thatin some situations better accuracy on the training set is crucial to obtaingood performance on unseen examples. Last, we propose robust versions of theapproach that can handle outliers.
arxiv-15000-26 | Unsupervised High-level Feature Learning by Ensemble Projection for Semi-supervised Image Classification and Image Clustering | http://arxiv.org/pdf/1602.00955v2.pdf | author:Dengxin Dai, Luc Van Gool category:cs.CV published:2016-02-02 summary:This paper investigates the problem of image classification with limited orno annotations, but abundant unlabeled data. The setting exists in many taskssuch as semi-supervised image classification, image clustering, and imageretrieval. Unlike previous methods, which develop or learn sophisticatedregularizers for classifiers, our method learns a new image representation byexploiting the distribution patterns of all available data for the task athand. Particularly, a rich set of visual prototypes are sampled from allavailable data, and are taken as surrogate classes to train discriminativeclassifiers; images are projected via the classifiers; the projected values,similarities to the prototypes, are stacked to build the new feature vector.The training set is noisy. Hence, in the spirit of ensemble learning we createa set of such training sets which are all diverse, leading to diverseclassifiers. The method is dubbed Ensemble Projection (EP). EP captures notonly the characteristics of individual images, but also the relationships amongimages. It is conceptually simple and computationally efficient, yet effectiveand flexible. Experiments on eight standard datasets show that: (1) EPoutperforms previous methods for semi-supervised image classification; (2) EPproduces promising results for self-taught image classification, whereunlabeled samples are a random collection of images rather than being from thesame distribution as the labeled ones; and (3) EP improves over the originalfeatures for image clustering. The code of the method is available on theproject page.
arxiv-15000-27 | A semi-automatic computer-aided method for surgical template design | http://arxiv.org/pdf/1602.01644v1.pdf | author:Xiaojun Chen, Lu Xu, Yue Yang, Jan Egger category:cs.GR cs.CG cs.CV published:2016-02-04 summary:This paper presents a generalized integrated framework of semi-automaticsurgical template design. Several algorithms were implemented including themesh segmentation, offset surface generation, collision detection, ruledsurface generation, etc., and a special software named TemDesigner wasdeveloped. With a simple user interface, a customized template can be semi-automatically designed according to the preoperative plan. Firstly, meshsegmentation with signed scalar of vertex is utilized to partition the innersurface from the input surface mesh based on the indicated point loop. Then,the offset surface of the inner surface is obtained through contouring thedistance field of the inner surface, and segmented to generate the outersurface. Ruled surface is employed to connect inner and outer surfaces.Finally, drilling tubes are generated according to the preoperative planthrough collision detection and merging. It has been applied to the templatedesign for various kinds of surgeries, including oral implantology, cervicalpedicle screw insertion, iliosacral screw insertion and osteotomy,demonstrating the efficiency, functionality and generality of our method.
arxiv-15000-28 | A Generalised Quantifier Theory of Natural Language in Categorical Compositional Distributional Semantics with Bialgebras | http://arxiv.org/pdf/1602.01635v1.pdf | author:Jules Hedges, Mehrnoosh Sadrzadeh category:cs.CL cs.AI math.CT I.2.7 published:2016-02-04 summary:Categorical compositional distributional semantics is a model of naturallanguage; it combines the statistical vector space models of words with thecompositional models of grammar. We formalise in this model the generalisedquantifier theory of natural language, due to Barwise and Cooper. Theunderlying setting is a compact closed category with bialgebras. We start froma generative grammar formalisation and develop an abstract categoricalcompositional semantics for it, then instantiate the abstract setting to setsand relations and to finite dimensional vector spaces and linear maps. We provethe equivalence of the relational instantiation to the truth theoreticsemantics of generalized quantifiers. The vector space instantiation formalisesthe statistical usages of words and enables us to, for the first time, reasonabout quantified phrases and sentences compositionally in distributionalsemantics.
arxiv-15000-29 | Self-Transfer Learning for Fully Weakly Supervised Object Localization | http://arxiv.org/pdf/1602.01625v1.pdf | author:Sangheum Hwang, Hyo-Eun Kim category:cs.CV published:2016-02-04 summary:Recent advances of deep learning have achieved remarkable performances invarious challenging computer vision tasks. Especially in object localization,deep convolutional neural networks outperform traditional approaches based onextraction of data/task-driven features instead of hand-crafted features.Although location information of region-of-interests (ROIs) gives good priorfor object localization, it requires heavy annotation efforts from humanresources. Thus a weakly supervised framework for object localization isintroduced. The term "weakly" means that this framework only uses image-levellabeled datasets to train a network. With the help of transfer learning whichadopts weight parameters of a pre-trained network, the weakly supervisedlearning framework for object localization performs well because thepre-trained network already has well-trained class-specific features. However,those approaches cannot be used for some applications which do not havepre-trained networks or well-localized large scale images. Medical imageanalysis is a representative among those applications because it is impossibleto obtain such pre-trained networks. In this work, we present a "fully" weaklysupervised framework for object localization ("semi"-weakly is the counterpartwhich uses pre-trained filters for weakly supervised localization) named asself-transfer learning (STL). It jointly optimizes both classification andlocalization networks simultaneously. By controlling a supervision level of thelocalization network, STL helps the localization network focus on correct ROIswithout any types of priors. We evaluate the proposed STL framework using twomedical image datasets, chest X-rays and mammograms, and achieve signiticantlybetter localization performance compared to previous weakly supervisedapproaches.
arxiv-15000-30 | The Variational Fair Autoencoder | http://arxiv.org/pdf/1511.00830v5.pdf | author:Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, Richard Zemel category:stat.ML cs.LG published:2015-11-03 summary:We investigate the problem of learning representations that are invariant tocertain nuisance or sensitive factors of variation in the data while retainingas much of the remaining information as possible. Our model is based on avariational autoencoding architecture with priors that encourage independencebetween sensitive and latent factors of variation. Any subsequent processing,such as classification, can then be performed on this purged latentrepresentation. To remove any remaining dependencies we incorporate anadditional penalty term based on the "Maximum Mean Discrepancy" (MMD) measure.We discuss how these architectures can be efficiently trained on data and showin experiments that this method is more effective than previous work inremoving unwanted sources of variation while maintaining informative latentrepresentations.
arxiv-15000-31 | Fpga Based Implementation of Deep Neural Networks Using On-chip Memory Only | http://arxiv.org/pdf/1602.01616v1.pdf | author:Jinhwan Park, Wonyong Sung category:cs.AR cs.NE published:2016-02-04 summary:Deep neural networks (DNNs) demand a very large amount of computation andweight storage, and thus efficient implementation using special purposehardware is highly desired. In this work, we have developed an FPGA basedfixed-point DNN system using only on-chip memory not to access external DRAM.The execution time and energy consumption of the developed system is comparedwith a GPU based implementation. Since the capacity of memory in FPGA islimited, only 3-bit weights are used for this implementation, and trainingbased fixed-point weight optimization is employed. The implementation usingXilinx XC7Z045 is tested for the MNIST handwritten digit recognition benchmarkand a phoneme recognition task on TIMIT corpus. The obtained speed is about onequarter of a GPU based implementation and much better than that of a PC basedone. The power consumption is less than 5 Watt at the full speed operationresulting in much higher efficiency compared to GPU based systems.
arxiv-15000-32 | Joint Recognition and Segmentation of Actions via Probabilistic Integration of Spatio-Temporal Fisher Vectors | http://arxiv.org/pdf/1602.01601v1.pdf | author:Johanna Carvajal, Chris McCool, Brian Lovell, Conrad Sanderson category:cs.CV published:2016-02-04 summary:We propose a hierarchical approach to multi-action recognition that performsjoint classification and segmentation. A given video (containing severalconsecutive actions) is processed via a sequence of overlapping temporalwindows. Each frame in a temporal window is represented through selectivelow-level spatio-temporal features which efficiently capture relevant localdynamics. Features from each window are represented as a Fisher vector, whichcaptures first and second order statistics. Instead of directly classifyingeach Fisher vector, it is converted into a vector of class probabilities. Thefinal classification decision for each frame is then obtained by integratingthe class probabilities at the frame level, which exploits the overlapping ofthe temporal windows. Experiments were performed on two datasets: s-KTH (astitched version of the KTH dataset to simulate multi-actions), and thechallenging CMU-MMAC dataset. On s-KTH, the proposed approach achieves anaccuracy of 85.0%, significantly outperforming two recent approaches based onGMMs and HMMs which obtained 78.3% and 71.2%, respectively. On CMU-MMAC, theproposed approach achieves an accuracy of 40.9%, outperforming the GMM and HMMapproaches which obtained 33.7% and 38.4%, respectively. Furthermore, theproposed system is on average 40 times faster than the GMM based approach.
arxiv-15000-33 | Comparative Evaluation of Action Recognition Methods via Riemannian Manifolds, Fisher Vectors and GMMs: Ideal and Challenging Conditions | http://arxiv.org/pdf/1602.01599v1.pdf | author:Johanna Carvajal, Arnold Wiliem, Chris McCool, Brian Lovell, Conrad Sanderson category:cs.CV published:2016-02-04 summary:We present a comparative evaluation of various techniques for actionrecognition while keeping as many variables as possible controlled. We employtwo categories of Riemannian manifolds: symmetric positive definite matricesand linear subspaces. For both categories we use their corresponding nearestneighbour classifiers, kernels, and recent kernelised sparse representations.We compare against traditional action recognition techniques based on Gaussianmixture models and Fisher vectors (FVs). We evaluate these action recognitiontechniques under ideal conditions, as well as their sensitivity in morechallenging conditions (variations in scale and translation). Despite recentadvancements for handling manifolds, manifold based techniques obtain thelowest performance and their kernel representations are more unstable in thepresence of challenging conditions. The FV approach obtains the highestaccuracy under ideal conditions. Moreover, FV best deals with moderate scaleand translation changes.
arxiv-15000-34 | How Important is Weight Symmetry in Backpropagation? | http://arxiv.org/pdf/1510.05067v4.pdf | author:Qianli Liao, Joel Z. Leibo, Tomaso Poggio category:cs.LG published:2015-10-17 summary:Gradient backpropagation (BP) requires symmetric feedforward and feedbackconnections -- the same weights must be used for forward and backward passes.This "weight transport problem" (Grossberg 1987) is thought to be one of themain reasons to doubt BP's biologically plausibility. Using 15 differentclassification datasets, we systematically investigate to what extent BP reallydepends on weight symmetry. In a study that turned out to be surprisinglysimilar in spirit to Lillicrap et al.'s demonstration (Lillicrap et al. 2014)but orthogonal in its results, our experiments indicate that: (1) themagnitudes of feedback weights do not matter to performance (2) the signs offeedback weights do matter -- the more concordant signs between feedforward andtheir corresponding feedback connections, the better (3) with feedback weightshaving random magnitudes and 100% concordant signs, we were able to achieve thesame or even better performance than SGD. (4) somenormalizations/stabilizations are indispensable for such asymmetric BP to work,namely Batch Normalization (BN) (Ioffe and Szegedy 2015) and/or a "BatchManhattan" (BM) update rule.
arxiv-15000-35 | SDCA without Duality, Regularization, and Individual Convexity | http://arxiv.org/pdf/1602.01582v1.pdf | author:Shai Shalev-Shwartz category:cs.LG published:2016-02-04 summary:Stochastic Dual Coordinate Ascent is a popular method for solving regularizedloss minimization for the case of convex losses. We describe variants of SDCAthat do not require explicit regularization and do not rely on duality. Weprove linear convergence rates even if individual loss functions arenon-convex, as long as the expected loss is strongly convex.
arxiv-15000-36 | Long-term Planning by Short-term Prediction | http://arxiv.org/pdf/1602.01580v1.pdf | author:Shai Shalev-Shwartz, Nir Ben-Zrihem, Aviad Cohen, Amnon Shashua category:cs.LG published:2016-02-04 summary:We consider planning problems, that often arise in autonomous drivingapplications, in which an agent should decide on immediate actions so as tooptimize a long term objective. For example, when a car tries to merge in aroundabout it should decide on an immediate acceleration/braking command, whilethe long term effect of the command is the success/failure of the merge. Suchproblems are characterized by continuous state and action spaces, and byinteraction with multiple agents, whose behavior can be adversarial. We arguethat dual versions of the MDP framework (that depend on the value function andthe $Q$ function) are problematic for autonomous driving applications due tothe non Markovian of the natural state space representation, and due to thecontinuous state and action spaces. We propose to tackle the planning task bydecomposing the problem into two phases: First, we apply supervised learningfor predicting the near future based on the present. We require that thepredictor will be differentiable with respect to the representation of thepresent. Second, we model a full trajectory of the agent using a recurrentneural network, where unexplained factors are modeled as (additive) inputnodes. This allows us to solve the long-term planning problem using supervisedlearning techniques and direct optimization over the recurrent neural network.Our approach enables us to learn robust policies by incorporating adversarialelements to the environment.
arxiv-15000-37 | A Factorized Recurrent Neural Network based architecture for medium to large vocabulary Language Modelling | http://arxiv.org/pdf/1602.01576v1.pdf | author:Anantharaman Palacod category:cs.CL cs.AI published:2016-02-04 summary:Statistical language models are central to many applications that usesemantics. Recurrent Neural Networks (RNN) are known to produce state of theart results for language modelling, outperforming their traditional n-gramcounterparts in many cases. To generate a probability distribution across avocabulary, these models require a softmax output layer that linearly increasesin size with the size of the vocabulary. Large vocabularies need acommensurately large softmax layer and training them on typical laptops/PCsrequires significant time and machine resources. In this paper we present a newtechnique for implementing RNN based large vocabulary language models thatsubstantially speeds up computation while optimally using the limited memoryresources. Our technique, while building on the notion of factorizing theoutput layer by having multiple output layers, improves on the earlier work bysubstantially optimizing on the individual output layer size and alsoeliminating the need for a multistep prediction process.
arxiv-15000-38 | BilBOWA: Fast Bilingual Distributed Representations without Word Alignments | http://arxiv.org/pdf/1410.2455v3.pdf | author:Stephan Gouws, Yoshua Bengio, Greg Corrado category:stat.ML cs.CL cs.LG published:2014-10-09 summary:We introduce BilBOWA (Bilingual Bag-of-Words without Alignments), a simpleand computationally-efficient model for learning bilingual distributedrepresentations of words which can scale to large monolingual datasets and doesnot require word-aligned parallel training data. Instead it trains directly onmonolingual data and extracts a bilingual signal from a smaller set of raw-textsentence-aligned data. This is achieved using a novel sampled bag-of-wordscross-lingual objective, which is used to regularize two noise-contrastivelanguage models for efficient cross-lingual feature learning. We show thatbilingual embeddings learned using the proposed model outperformstate-of-the-art methods on a cross-lingual document classification task aswell as a lexical translation task on WMT11 data.
arxiv-15000-39 | A Grassmannian Graph Approach to Affine Invariant Feature Matching | http://arxiv.org/pdf/1601.07648v2.pdf | author:Mark Moyou, John Corring, Adrian Peter, Anand Rangarajan category:cs.CV published:2016-01-28 summary:In this work, we present a novel and practical approach to address one of thelongstanding problems in computer vision: 2D and 3D affine invariant featurematching. Our Grassmannian Graph (GrassGraph) framework employs a two stageprocedure that is capable of robustly recovering correspondences between twounorganized, affinely related feature (point) sets. The first stage maps thefeature sets to an affine invariant Grassmannian representation, where thefeatures are mapped into the same subspace. It turns out that coordinaterepresentations extracted from the Grassmannian differ by an arbitraryorthonormal matrix. In the second stage, by approximating the Laplace-Beltramioperator (LBO) on these coordinates, this extra orthonormal factor isnullified, providing true affine-invariant coordinates which we then utilize torecover correspondences via simple nearest neighbor relations. The resultingGrassGraph algorithm is empirically shown to work well in non-ideal scenarioswith noise, outliers, and occlusions. Our validation benchmarks use anunprecedented 440,000+ experimental trials performed on 2D and 3D datasets,with a variety of parameter settings and competing methods. State-of-the-artperformance in the majority of these extensive evaluations confirm the utilityof our method.
arxiv-15000-40 | An ensemble diversity approach to supervised binary hashing | http://arxiv.org/pdf/1602.01557v1.pdf | author:Miguel Á. Carreira-Perpiñán, Ramin Raziperchikolaei category:cs.LG cs.CV math.OC stat.ML published:2016-02-04 summary:Binary hashing is a well-known approach for fast approximate nearest-neighborsearch in information retrieval. Much work has focused on affinity-basedobjective functions involving the hash functions or binary codes. Theseobjective functions encode neighborhood information between data points and areoften inspired by manifold learning algorithms. They ensure that the hashfunctions differ from each other through constraints or penalty terms thatencourage codes to be orthogonal or dissimilar across bits, but this couplesthe binary variables and complicates the already difficult optimization. Wepropose a much simpler approach: we train each hash function (or bit)independently from each other, but introduce diversity among them usingtechniques from classifier ensembles. Surprisingly, we find that not only isthis faster and trivially parallelizable, but it also improves over the morecomplex, coupled objective function, and achieves state-of-the-art precisionand recall in experiments with image retrieval.
arxiv-15000-41 | Fundamental Limits in Multi-image Alignment | http://arxiv.org/pdf/1602.01541v1.pdf | author:Cecilia Aguerrebere, Mauricio Delbracio, Alberto Bartesaghi, Guillermo Sapiro category:cs.CV published:2016-02-04 summary:The performance of multi-image alignment, bringing different images into onecoordinate system, is critical in many applications with varied signal-to-noiseratio (SNR) conditions. A great amount of effort is being invested intodeveloping methods to solve this problem. Several important questions thusarise, including: Which are the fundamental limits in multi-image alignmentperformance? Does having access to more images improve the alignment?Theoretical bounds provide a fundamental benchmark to compare methods and canhelp establish whether improvements can be made. In this work, we tackle theproblem of finding the performance limits in image registration when multipleshifted and noisy observations are available. We derive and analyze theCram\'er-Rao and Ziv-Zakai lower bounds under different statistical models forthe underlying image. The accuracy of the derived bounds is experimentallyassessed through a comparison to the maximum likelihood estimator. We show theexistence of different behavior zones depending on the difficulty level of theproblem, given by the SNR conditions of the input images. We find thatincreasing the number of images is only useful below a certain SNR threshold,above which the pairwise MLE estimation proves to be optimal. The analysis wepresent here brings further insight into the fundamental limitations of themulti-image alignment problem.
arxiv-15000-42 | Bearing fault diagnosis based on spectrum images of vibration signals | http://arxiv.org/pdf/1511.02503v5.pdf | author:Wei Li, Mingquan Qiu, Zhencai Zhu, Bo Wu, Gongbo Zhou category:cs.CV cs.SD published:2015-11-08 summary:Bearing fault diagnosis has been a challenge in the monitoring activities ofrotating machinery, and it's receiving more and more attention. Theconventional fault diagnosis methods usually extract features from thewaveforms or spectrums of vibration signals in order to realize faultclassification. In this paper, a novel feature in the form of images ispresented, namely the spectrum images of vibration signals. The spectrum imagesare simply obtained by doing fast Fourier transformation. Such images areprocessed with two-dimensional principal component analysis (2DPCA) to reducethe dimensions, and then a minimum distance method is applied to classify thefaults of bearings. The effectiveness of the proposed method is verified withexperimental data.
arxiv-15000-43 | The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems | http://arxiv.org/pdf/1506.08909v3.pdf | author:Ryan Lowe, Nissan Pow, Iulian Serban, Joelle Pineau category:cs.CL cs.AI cs.LG cs.NE published:2015-06-30 summary:This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost1 million multi-turn dialogues, with a total of over 7 million utterances and100 million words. This provides a unique resource for research into buildingdialogue managers based on neural language models that can make use of largeamounts of unlabeled data. The dataset has both the multi-turn property ofconversations in the Dialog State Tracking Challenge datasets, and theunstructured nature of interactions from microblog services such as Twitter. Wealso describe two neural learning architectures suitable for analyzing thisdataset, and provide benchmark performance on the task of selecting the bestnext response.
arxiv-15000-44 | Exploiting Local Structures with the Kronecker Layer in Convolutional Networks | http://arxiv.org/pdf/1512.09194v2.pdf | author:Shuchang Zhou, Jia-Nan Wu, Yuxin Wu, Xinyu Zhou category:cs.CV published:2015-12-31 summary:In this paper, we propose and study a technique to reduce the number ofparameters and computation time in convolutional neural networks. We useKronecker product to exploit the local structures within convolution andfully-connected layers, by replacing the large weight matrices by combinationsof multiple Kronecker products of smaller matrices. Just as the Kroneckerproduct is a generalization of the outer product from vectors to matrices, ourmethod is a generalization of the low rank approximation method for convolutionneural networks. We also introduce combinations of different shapes ofKronecker product to increase modeling capacity. Experiments on SVHN, scenetext recognition and ImageNet dataset demonstrate that we can achieve $3.3\times$ speedup or $3.6 \times$ parameter reduction with less than 1\% drop inaccuracy, showing the effectiveness and efficiency of our method. Moreover, thecomputation efficiency of Kronecker layer makes using larger feature mappossible, which in turn enables us to outperform the previous state-of-the-arton both SVHN(digit recognition) and CASIA-HWDB (handwritten Chinese characterrecognition) datasets.
arxiv-15000-45 | A Hierarchical Spectral Method for Extreme Classification | http://arxiv.org/pdf/1511.03260v4.pdf | author:Paul Mineiro, Nikos Karampatziakis category:stat.ML cs.LG published:2015-11-10 summary:Extreme classification problems are multiclass and multilabel classificationproblems where the number of outputs is so large that straightforwardstrategies are neither statistically nor computationally viable. One strategyfor dealing with the computational burden is via a tree decomposition of theoutput space. While this typically leads to training and inference that scalessublinearly with the number of outputs, it also results in reduced statisticalperformance. In this work, we identify two shortcomings of tree decompositionmethods, and describe two heuristic mitigations. We compose these with aneigenvalue technique for constructing the tree. The end result is acomputationally efficient algorithm that provides good statistical performanceon several extreme data sets.
arxiv-15000-46 | Risk estimation for high-dimensional lasso regression | http://arxiv.org/pdf/1602.01522v1.pdf | author:Darren Homrighausen, Daniel J. McDonald category:stat.ME stat.ML published:2016-02-04 summary:In high-dimensional estimation, analysts are faced with more parameters $p$than available observations $n$, and asymptotic analysis of performance allowsthe ratio $p/n\rightarrow \infty$. This situation makes regularization bothnecessary and desirable in order for estimators to possess theoreticalguarantees. However, the amount of regularization, often determined by one ormore tuning parameters, is integral to achieving good performance. In practice,choosing the tuning parameter is done through resampling methods (e.g.cross-validation), generalized information criteria, or reformulating theoptimization problem (e.g. square-root lasso or scaled sparse regression). Eachof these techniques comes with varying levels of theoretical guarantee for thelow- or high-dimensional regimes. However, there are some notable deficienciesin the literature. The theory, and sometimes practice, of many methods relieson either the knowledge or estimation of the variance parameter, which isdifficult to estimate in high dimensions. In this paper, we provide theoreticalintuition suggesting that some previously proposed approaches based oninformation criteria work poorly in high dimensions. We introduce a suite ofnew risk estimators leveraging the burgeoning literature on high-dimensionalvariance estimation. Finally, we compare our proposal to many existing methodsfor choosing the tuning parameters for lasso regression by providing anextensive simulation to examine their finite sample performance. We find thatour new estimators perform quite well, often better than the existingapproaches across a wide range of simulation conditions and evaluationcriteria.
arxiv-15000-47 | Towards Better Exploiting Convolutional Neural Networks for Remote Sensing Scene Classification | http://arxiv.org/pdf/1602.01517v1.pdf | author:Keiller Nogueira, Otávio A. B. Penatti, Jefersson A. dos Santos category:cs.CV published:2016-02-04 summary:We present an analysis of three possible strategies for exploiting the powerof existing convolutional neural networks (ConvNets) in different scenariosfrom the ones they were trained: full training, fine tuning, and using ConvNetsas feature extractors. In many applications, especially including remotesensing, it is not feasible to fully design and train a new ConvNet, as thisusually requires a considerable amount of labeled data and demands highcomputational costs. Therefore, it is important to understand how to obtain thebest profit from existing ConvNets. We perform experiments with six popularConvNets using three remote sensing datasets. We also compare ConvNets in eachstrategy with existing descriptors and with state-of-the-art baselines. Resultspoint that fine tuning tends to be the best performing strategy. In fact, usingthe features from the fine-tuned ConvNet with linear SVM obtains the bestresults. We also achieved state-of-the-art results for the three datasets used.
arxiv-15000-48 | Unsupervised Regenerative Learning of Hierarchical Features in Spiking Deep Networks for Object Recognition | http://arxiv.org/pdf/1602.01510v1.pdf | author:Priyadarshini Panda, Kaushik Roy category:cs.NE published:2016-02-03 summary:We present a spike-based unsupervised regenerative learning scheme to trainSpiking Deep Networks (SpikeCNN) for object recognition problems usingbiologically realistic leaky integrate-and-fire neurons. The trainingmethodology is based on the Auto-Encoder learning model wherein thehierarchical network is trained layer wise using the encoder-decoder principle.Regenerative learning uses spike-timing information and inherent latencies toupdate the weights and learn representative levels for each convolutional layerin an unsupervised manner. The features learnt from the final layer in thehierarchy are then fed to an output layer. The output layer is trained withsupervision by showing a fraction of the labeled training dataset and performsthe overall classification of the input. Our proposed methodology yields0.92%/29.84% classification error on MNIST/CIFAR10 datasets which is comparablewith state-of-the-art results. The proposed methodology also introducessparsity in the hierarchical feature representations on account of event-basedcoding resulting in computationally efficient learning.
arxiv-15000-49 | A simple method for estimating the fractal dimension from digital images: The compression dimension | http://arxiv.org/pdf/1602.02139v1.pdf | author:P. Chamorro-Posada category:cs.GR cs.CV published:2016-02-03 summary:The fractal structure of real world objects is often analyzed using digitalimages. In this context, the compression fractal dimension is put forward. Itprovides a simple method for the direct estimation of the dimension of fractalsstored as digital image files. The computational scheme can be implementedusing readily available free software. Its simplicity also makes it veryinteresting for introductory elaborations of basic concepts of fractalgeometry, complexity, and information theory. A test of the computationalscheme using limited-quality images of well-defined fractal sets obtained fromthe Internet and free software has been performed.
arxiv-15000-50 | Efficient statistical classification of satellite measurements | http://arxiv.org/pdf/1202.2194v4.pdf | author:Peter Mills category:physics.ao-ph stat.ML published:2012-02-10 summary:Supervised statistical classification is a vital tool for satellite imageprocessing. It is useful not only when a discrete result, such as featureextraction or surface type, is required, but also for continuum retrievals bydividing the quantity of interest into discrete ranges. Because of the highresolution of modern satellite instruments and because of the requirement forreal-time processing, any algorithm has to be fast to be useful. Here wedescribe an algorithm based on kernel estimation called Adaptive GaussianFiltering that incorporates several innovations to produce superior efficiencyas compared to three other popular methods: k-nearest-neighbour (KNN), LearningVector Quantization (LVQ) and Support Vector Machines (SVM). This efficiency isgained with no compromises: accuracy is maintained, while estimates of theconditional probabilities are returned. These are useful not only to gauge theaccuracy of an estimate in the absence of its true value, but also tore-calibrate a retrieved image and as a proxy for a discretized continuumvariable. The algorithm is demonstrated and compared with the other three on apair of synthetic test classes and to map the waterways of the Netherlands.Software may be found at: http://libagf.sourceforge.net.
arxiv-15000-51 | Fast Cross-Validation via Sequential Testing | http://arxiv.org/pdf/1206.2248v6.pdf | author:Tammo Krueger, Danny Panknin, Mikio Braun category:cs.LG stat.ML I.2.6; I.2.8 published:2012-06-11 summary:With the increasing size of today's data sets, finding the right parameterconfiguration in model selection via cross-validation can be an extremelytime-consuming task. In this paper we propose an improved cross-validationprocedure which uses nonparametric testing coupled with sequential analysis todetermine the best parameter set on linearly increasing subsets of the data. Byeliminating underperforming candidates quickly and keeping promising candidatesas long as possible, the method speeds up the computation while preserving thecapability of the full cross-validation. Theoretical considerations underlinethe statistical power of our procedure. The experimental evaluation shows thatour method reduces the computation time by a factor of up to 120 compared to afull cross-validation with a negligible impact on the accuracy.
arxiv-15000-52 | Towards Cultural-Scale Models of Full Text | http://arxiv.org/pdf/1512.05004v2.pdf | author:Jaimie Murdock, Jiaan Zeng, Colin Allen category:cs.DL cs.CL cs.IR published:2015-12-15 summary:This technical report consists of two components: an administrative reportfor the HathiTrust Research Center (HTRC) Advanced Collaborative Support (ACS)program and a research report on the variance of topic models trained overrandom samples of books in the Hathi Trust. Cultural-scale models of full textdocuments are prone to over-interpretation by researchers makingunintentionally strong socio-linguistic claims without recognizing that evenlarge digital libraries are merely samples of all the books ever produced. Inthis study, we test the sensitivity of the topic models to the sampling processby taking random samples of books in the Hathi Trust Digital Library withindifferent Library of Congress Classification (LCC) areas. For eachclassification area, we train several topic models over the entire class withdifferent random seeds, generating a set of spanning models. Then, we traintopic models on random samples of books from the classification area,generating a set of sample models. Finally, we align topics from the samplemodels to the spanning models and measure the alignment distance and topicoverlap. We find that sample models with a large sample size typically have analignment distance that falls in the range of the alignment distance betweenspanning models. Unsurprisingly, as sample size increases, alignment distancedecreases. We also find that the topic overlap increases as sample sizeincreases. However, the decomposition of these measures by sample size differsby field and by number of topics. We speculate that these measures could beused to find classes which have a common "canon" discussed among all books inthe area, as shown by high topic overlap and low alignment distance even insmall sample sizes.
arxiv-15000-53 | Latent-Class Hough Forests for 6 DoF Object Pose Estimation | http://arxiv.org/pdf/1602.01464v1.pdf | author:Rigas Kouskouridas, Alykhan Tejani, Andreas Doumanoglou, Danhang Tang, Tae-Kyun Kim category:cs.CV published:2016-02-03 summary:In this paper we present Latent-Class Hough Forests, a method for objectdetection and 6 DoF pose estimation in heavily cluttered and occludedscenarios. We adapt a state of the art template matching feature into ascale-invariant patch descriptor and integrate it into a regression forestusing a novel template-based split function. We train with positive samplesonly and we treat class distributions at the leaf nodes as latent variables.During testing we infer by iteratively updating these distributions, providingaccurate estimation of background clutter and foreground occlusions and, thus,better detection rate. Furthermore, as a by-product, our Latent-Class HoughForests can provide accurate occlusion aware segmentation masks, even in themulti-instance scenario. In addition to an existing public dataset, whichcontains only single-instance sequences with large amounts of clutter, we havecollected two, more challenging, datasets for multiple-instance detectioncontaining heavy 2D and 3D clutter as well as foreground occlusions. We provideextensive experiments on the various parameters of the framework such as patchsize, number of trees and number of iterations to infer class distributions attest time. We also evaluate the Latent-Class Hough Forests on all datasetswhere we outperform state of the art methods.
arxiv-15000-54 | "Draw My Topics": Find Desired Topics fast from large scale of Corpus | http://arxiv.org/pdf/1602.01428v1.pdf | author:Jason Dou, Ni Sun, Xiaojun Zou category:cs.CL cs.IR published:2016-02-03 summary:We develop the "Draw My Topics" toolkit, which provides a fast way toincorporate social scientists' interest into standard topic modelling. Insteadof using raw corpus with primitive processing as input, an algorithm based onVector Space Model and Conditional Entropy are used to connect socialscientists' willingness and unsupervised topic models' output. Space for users'adjustment on specific corpus of their interest is also accommodated. Wedemonstrate the toolkit's use on the Diachronic People's Daily Corpus inChinese.
arxiv-15000-55 | A General Framework for Fast Image Deconvolution with Incomplete Observations. Applications to Unknown Boundaries, Inpainting, Superresolution, and Demosaicing | http://arxiv.org/pdf/1602.01410v1.pdf | author:Miguel Simões, Luis B. Almeida, José Bioucas-Dias, Jocelyn Chanussot category:cs.CV stat.ML published:2016-02-03 summary:In image deconvolution problems, the diagonalization of the underlyingoperators by means of the FFT usually yields very large speedups. When thereare incomplete observations (e.g., in the case of unknown boundaries), standarddeconvolution techniques normally involve non-diagonalizableoperators---resulting in rather slow methods---or, otherwise, use inexactconvolution models, resulting in the occurrence of artifacts in the enhancedimages. In this paper, we propose a new deconvolution framework for images withincomplete observations that allows us to work with diagonalized convolutionoperators, and therefore is very fast. We iteratively alternate the estimationof the unknown pixels and of the deconvolved image, using, e.g., a FFT-baseddeconvolution method. In principle, any fast deconvolution method can be used.We give an example in which a published method that assumes periodic boundaryconditions is extended, through the use of this framework, to unknown boundaryconditions. Furthermore, we propose an implementation of this framework, basedon the alternating direction method of multipliers (ADMM). We provide a proofof convergence for the resulting algorithm, which can be seen as a "partial"ADMM, in which not all variables are dualized. We report experimentalcomparisons with other primal-dual methods, in which the proposed one performedat the level of the state of the art. Four different kinds of applications weretested in the experiments: deconvolution, deconvolution with inpainting,superresolution, and demosaicing, all with unknown boundaries.
arxiv-15000-56 | A Kronecker-factored approximate Fisher matrix for convolution layers | http://arxiv.org/pdf/1602.01407v1.pdf | author:Roger Grosse, James Martens category:stat.ML cs.LG published:2016-02-03 summary:Second-order optimization methods such as natural gradient descent have thepotential to speed up training of neural networks by correcting for thecurvature of the loss function. Unfortunately, the exact natural gradient isimpractical to compute for large models, and most approximations either requirean expensive iterative procedure or make crude approximations to the curvature.We present Kronecker Factors for Convolution (KFC), a tractable approximationto the Fisher matrix for convolutional networks based on a structuredprobabilistic model for the distribution over backpropagated derivatives.Similarly to the recently proposed Kronecker-Factored Approximate Curvature(K-FAC), each block of the approximate Fisher matrix decomposes as theKronecker product of small matrices, allowing for efficient inversion. KFCcaptures important curvature information while still yielding comparablyefficient updates to stochastic gradient descent (SGD). We show that theupdates are invariant to commonly used reparameterizations, such as centeringof the activations. In our experiments, approximate natural gradient descentwith KFC was able to train convolutional networks several times faster thancarefully tuned SGD. Furthermore, it was able to train the networks in 10-20times fewer iterations than SGD, suggesting its potential applicability in adistributed setting.
arxiv-15000-57 | Incremental Truncated LSTD | http://arxiv.org/pdf/1511.08495v2.pdf | author:Clement Gehring, Yangchen Pan, Martha White category:cs.LG cs.AI published:2015-11-26 summary:Balancing between computational efficiency and sample efficiency is animportant goal in reinforcement learning. Temporal difference (TD) learningalgorithms stochastically update the value function, with a linear timecomplexity in the number of features, whereas least-squares temporal difference(LSTD) algorithms are sample efficient but can be quadratic in the number offeatures. In this work, we develop an efficient incremental low-rankLSTD({\lambda}) algorithm that progresses towards the goal of better balancingcomputation and sample efficiency. The algorithm reduces the computation andstorage complexity to the number of features times the chosen rank parameterwhile summarizing past samples efficiently to nearly obtain the samplecomplexity of LSTD. We derive a simulation bound on the solution given bytruncated low-rank approximation, illustrating a bias- variance trade-offdependent on the choice of rank. We demonstrate that the algorithm effectivelybalances computational complexity and sample efficiency for policy evaluationin a benchmark task and a high-dimensional energy allocation domain.
arxiv-15000-58 | Shearlet-Based Detection of Flame Fronts | http://arxiv.org/pdf/1511.03753v2.pdf | author:Rafael Reisenhofer, Johannes Kiefer, Emily J. King category:cs.CV published:2015-11-12 summary:Identifying and characterizing flame fronts is the most common task in thecomputer-assisted analysis of data obtained from imaging techniques such asplanar laser-induced fluorescence (PLIF), laser Rayleigh scattering (LRS), orparticle imaging velocimetry (PIV). We present a novel edge and ridge (line)detection algorithm based on complex-valued wavelet-like analyzing functions --so-called complex shearlets -- displaying several traits useful for theextraction of flame fronts. In addition to providing a unified approach to thedetection of edges and ridges, our method inherently yields estimates of localtangent orientations and local curvatures. To examine the applicability forhigh-frequency recordings of combustion processes, the algorithm is applied tomock images distorted with varying degrees of noise and real-world PLIF imagesof both OH and CH radicals. Furthermore, we compare the performance of thenewly proposed complex shearlet-based measure to well-established edge andridge detection techniques such as the Canny edge detector, anothershearlet-based edge detector, and the phase congruency measure.
arxiv-15000-59 | Fast spectral algorithms from sum-of-squares proofs: tensor decomposition and planted sparse vectors | http://arxiv.org/pdf/1512.02337v2.pdf | author:Samuel B. Hopkins, Tselil Schramm, Jonathan Shi, David Steurer category:cs.DS cs.CC cs.LG stat.ML published:2015-12-08 summary:We consider two problems that arise in machine learning applications: theproblem of recovering a planted sparse vector in a random linear subspace andthe problem of decomposing a random low-rank overcomplete 3-tensor. For bothproblems, the best known guarantees are based on the sum-of-squares method. Wedevelop new algorithms inspired by analyses of the sum-of-squares method. Ouralgorithms achieve the same or similar guarantees as sum-of-squares for theseproblems but the running time is significantly faster. For the planted sparse vector problem, we give an algorithm with running timenearly linear in the input size that approximately recovers a planted sparsevector with up to constant relative sparsity in a random subspace of $\mathbbR^n$ of dimension up to $\tilde \Omega(\sqrt n)$. These recovery guaranteesmatch the best known ones of Barak, Kelner, and Steurer (STOC 2014) up tologarithmic factors. For tensor decomposition, we give an algorithm with running time close tolinear in the input size (with exponent $\approx 1.086$) that approximatelyrecovers a component of a random 3-tensor over $\mathbb R^n$ of rank up to$\tilde \Omega(n^{4/3})$. The best previous algorithm for this problem due toGe and Ma (RANDOM 2015) works up to rank $\tilde \Omega(n^{3/2})$ but requiresquasipolynomial time.
arxiv-15000-60 | Diversity Networks | http://arxiv.org/pdf/1511.05077v5.pdf | author:Zelda Mariet, Suvrit Sra category:cs.LG cs.NE published:2015-11-16 summary:We introduce Divnet, a flexible technique for learning networks with diverseneurons. Divnet models neuronal diversity by placing a Determinantal PointProcess (DPP) over neurons in a given layer. It uses this DPP to select asubset of diverse neurons and subsequently fuses the redundant neurons into theselected ones. Compared with previous approaches, Divnet offers a moreprincipled, flexible technique for capturing neuronal diversity and thusimplicitly enforcing regularization. This enables effective auto-tuning ofnetwork architecture and leads to smaller network sizes without hurtingperformance. Moreover, through its focus on diversity and neuron fusing, Divnetremains compatible with other procedures that seek to reduce memory footprintsof networks. We present experimental results to corroborate our claims: forpruning neural networks, Divnet is seen to be notably superior to competingapproaches.
arxiv-15000-61 | A Probabilistic Modeling Approach to Hearing Loss Compensation | http://arxiv.org/pdf/1602.01345v1.pdf | author:Thijs van de Laar, Bert de Vries category:stat.ML published:2016-02-03 summary:Hearing Aid (HA) algorithms need to be tuned ("fitted") to match theimpairment of each specific patient. The lack of a fundamental HA fittingtheory is a strong contributing factor to an unsatisfying sound experience forabout 20% of hearing aid patients (Kochkin, 2014). This paper proposes aprobabilistic modeling approach to the design of HA algorithms. The proposedmethod relies on a generative probabilistic model for the hearing loss problemand provides for automated inference of the corresponding (1) signal processingalgorithm, (2) the fitting solution as well as a principled (3) performanceevaluation metric. All three tasks are realized as message passing algorithmsin a factor graph representation of the generative model, which in principleallows for fast implementation on hearing aid or mobile device hardware. Themethods are theoretically worked out and simulated with a custom-built factorgraph toolbox for a specific hearing loss model (Zurek, 2007).
arxiv-15000-62 | Biclustering Readings and Manuscripts via Non-negative Matrix Factorization, with Application to the Text of Jude | http://arxiv.org/pdf/1602.01323v1.pdf | author:Joey McCollum, Stephen Brown category:cs.LG 6U815 I.2.7 published:2016-02-03 summary:The text-critical practice of grouping witnesses into families or texttypesoften faces two obstacles: Contamination in the manuscript tradition, andco-dependence in identifying characteristic readings and manuscripts. Weintroduce non-negative matrix factorization (NMF) as a simple, unsupervised,and efficient way to cluster large numbers of manuscripts and readingssimultaneously while summarizing contamination using an easy-to-interpretmixture model. We apply this method to an extensive collation of the NewTestament epistle of Jude and show that the resulting clusters correspond tohuman-identified textual families from existing research.
arxiv-15000-63 | A continuum among logarithmic, linear, and exponential functions, and its potential to improve generalization in neural networks | http://arxiv.org/pdf/1602.01321v1.pdf | author:Luke B. Godfrey, Michael S. Gashler category:cs.NE published:2016-02-03 summary:We present the soft exponential activation function for artificial neuralnetworks that continuously interpolates between logarithmic, linear, andexponential functions. This activation function is simple, differentiable, andparameterized so that it can be trained as the rest of the network is trained.We hypothesize that soft exponential has the potential to improve neuralnetwork learning, as it can exactly calculate many natural operations thattypical neural networks can only approximate, including addition,multiplication, inner product, distance, polynomials, and sinusoids.
arxiv-15000-64 | Diffusion Maximum Correntropy Criterion Algorithms for Robust Distributed Estimation | http://arxiv.org/pdf/1508.01903v2.pdf | author:Wentao Ma, Badong Chen, Jiandong Duan, Haiquan Zhao category:stat.ML cs.LG published:2015-08-08 summary:Robust diffusion adaptive estimation algorithms based on the maximumcorrentropy criterion (MCC), including adaptation to combination MCC andcombination to adaptation MCC, are developed to deal with the distributedestimation over network in impulsive (long-tailed) noise environments. The costfunctions used in distributed estimation are in general based on the meansquare error (MSE) criterion, which is desirable when the measurement noise isGaussian. In non-Gaussian situations, such as the impulsive-noise case, MCCbased methods may achieve much better performance than the MSE methods as theytake into account higher order statistics of error distribution. The proposedmethods can also outperform the robust diffusion least mean p-power(DLMP) anddiffusion minimum error entropy (DMEE) algorithms. The mean and mean squareconvergence analysis of the new algorithms are also carried out.
arxiv-15000-65 | Multiple Output Regression with Latent Noise | http://arxiv.org/pdf/1410.7365v2.pdf | author:Jussi Gillberg, Pekka Marttinen, Matti Pirinen, Antti J. Kangas, Pasi Soininen, Mehreen Ali, Aki S. Havulinna, Marjo-Riitta Marjo-Riitta Järvelin, Mika Ala-Korpela, Samuel Kaski category:stat.ML published:2014-10-27 summary:In high-dimensional data, structured noise caused by observed and unobservedfactors affecting multiple target variables simultaneously, imposes a seriouschallenge for modeling, by masking the often weak signal. Therefore, (1)explaining away the structured noise in multiple-output regression is ofparamount importance. Additionally, (2) assumptions about the correlationstructure of the regression weights are needed. We note that both can beformulated in a natural way in a latent variable model, in which both theinteresting signal and the noise are mediated through the same latent factors.Under this assumption, the signal model then borrows strength from the noisemodel by encouraging similar effects on correlated targets. We introduce ahyperparameter for the \emph{latent signal-to-noise ratio} which turns out tobe important for modelling weak signals, and an ordered infinite-dimensionalshrinkage prior that resolves the rotational unidentifiability in reduced-rankregression models. Simulations and prediction experiments with metabolite, geneexpression, FMRI measurement, and macroeconomic time series data show that ourmodel equals or exceeds the state-of-the-art performance and, in particular,outperforms the standard approach of assuming independent noise and signalmodels.
arxiv-15000-66 | Using Hadoop for Large Scale Analysis on Twitter: A Technical Report | http://arxiv.org/pdf/1602.01248v1.pdf | author:Nikolaos Nodarakis, Spyros Sioutas, Athanasios Tsakalidis, Giannis Tzimas category:cs.DB cs.CL cs.IR H.2.4 published:2016-02-03 summary:Sentiment analysis (or opinion mining) on Twitter data has attracted muchattention recently. One of the system's key features, is the immediacy incommunication with other users in an easy, user-friendly and fast way.Consequently, people tend to express their feelings freely, which makes Twitteran ideal source for accumulating a vast amount of opinions towards a widediversity of topics. This amount of information offers huge potential and canbe harnessed to receive the sentiment tendency towards these topics. However,since none can invest an infinite amount of time to read through these tweets,an automated decision making approach is necessary. Nevertheless, most existingsolutions are limited in centralized environments only. Thus, they can onlyprocess at most a few thousand tweets. Such a sample, is not representative todefine the sentiment polarity towards a topic due to the massive number oftweets published daily. In this paper, we go one step further and develop anovel method for sentiment learning in the MapReduce framework. Our algorithmexploits the hashtags and emoticons inside a tweet, as sentiment labels, andproceeds to a classification procedure of diverse sentiment types in a paralleland distributed manner. Moreover, we utilize Bloom filters to compact thestorage size of intermediate data and boost the performance of our algorithm.Through an extensive experimental evaluation, we prove that our solution isefficient, robust and scalable and confirm the quality of our sentimentidentification.
arxiv-15000-67 | Comparative evaluation of state-of-the-art algorithms for SSVEP-based BCIs | http://arxiv.org/pdf/1602.00904v2.pdf | author:Vangelis P. Oikonomou, Georgios Liaros, Kostantinos Georgiadis, Elisavet Chatzilari, Katerina Adam, Spiros Nikolopoulos, Ioannis Kompatsiaris category:cs.HC cs.CV stat.ML published:2016-02-02 summary:Brain-computer interfaces (BCIs) have been gaining momentum in makinghuman-computer interaction more natural, especially for people withneuro-muscular disabilities. Among the existing solutions the systems relyingon electroencephalograms (EEG) occupy the most prominent place due to theirnon-invasiveness. However, the process of translating EEG signals into computercommands is far from trivial, since it requires the optimization of manydifferent parameters that need to be tuned jointly. In this report, we focus onthe category of EEG-based BCIs that rely on Steady-State-Visual-EvokedPotentials (SSVEPs) and perform a comparative evaluation of the most promisingalgorithms existing in the literature. More specifically, we define a set ofalgorithms for each of the various different parameters composing a BCI system(i.e. filtering, artifact removal, feature extraction, feature selection andclassification) and study each parameter independently by keeping all otherparameters fixed. The results obtained from this evaluation process areprovided together with a dataset consisting of the 256-channel, EEG signals of11 subjects, as well as a processing toolbox for reproducing the results andsupporting further experimentation. In this way, we manage to make availablefor the community a state-of-the-art baseline for SSVEP-based BCIs that can beused as a basis for introducing novel methods and approaches.
arxiv-15000-68 | How Far are We from Solving Pedestrian Detection? | http://arxiv.org/pdf/1602.01237v1.pdf | author:Shanshan Zhang, Rodrigo Benenson, Mohamed Omran, Jan Hosang, Bernt Schiele category:cs.CV published:2016-02-03 summary:Encouraged by the recent progress in pedestrian detection, we investigate thegap between current state-of-the-art methods and the "perfect single framedetector". We enable our analysis by creating a human baseline for pedestriandetection (over the Caltech dataset), and by manually clustering the recurrenterrors of a top detector. Our results characterize both localization andbackground-versus-foreground errors. To address localization errors we studythe impact of training annotation noise on the detector performance, and showthat we can improve even with a small portion of sanitized training data. Toaddress background/foreground discrimination, we study convnets for pedestriandetection, and discuss which factors affect their performance. Other than ourin-depth analysis, we report top performance on the Caltech dataset, andprovide a new sanitized set of training and test annotations.
arxiv-15000-69 | Consistency of Spectral Hypergraph Partitioning under Planted Partition Model | http://arxiv.org/pdf/1505.01582v2.pdf | author:Debarghya Ghoshdastidar, Ambedkar Dukkipati category:stat.ML published:2015-05-07 summary:Hypergraph partitioning lies at the heart of a number of problems in machinelearning and network sciences. Many algorithms for hypergraph partitioning havebeen proposed that extend standard approaches for graph partitioning to thecase of hypergraphs. However, theoretical aspects of such methods have seldomreceived attention in the literature as compared to the extensive studies onthe guarantees of graph partitioning. For instance, consistency results ofspectral graph partitioning under the stochastic block model are well known. Inthis paper, we present a planted partition model for sparse random non-uniformhypergraphs that generalizes the stochastic block model. We derive an errorbound for a spectral hypergraph partitioning algorithm under this model usingmatrix concentration inequalities. To the best of our knowledge, this is thefirst consistency result related to partitioning non-uniform hypergraphs.
arxiv-15000-70 | Image and Information | http://arxiv.org/pdf/1602.01228v1.pdf | author:Frank Nielsen category:cs.CV published:2016-02-03 summary:A well-known old adage says that {\em "A picture is worth a thousand words!"}(attributed to the Chinese philosopher Confucius ca 500 years BC). But moreprecisely, what do we mean by information in images? And how can it beretrieved effectively by machines? We briefly highlight these puzzlingquestions in this column. But first of all, let us start by defining moreprecisely what is meant by an "Image."
arxiv-15000-71 | Incidental Scene Text Understanding: Recent Progresses on ICDAR 2015 Robust Reading Competition Challenge 4 | http://arxiv.org/pdf/1511.09207v2.pdf | author:Cong Yao, Jianan Wu, Xinyu Zhou, Chi Zhang, Shuchang Zhou, Zhimin Cao, Qi Yin category:cs.CV published:2015-11-30 summary:Different from focused texts present in natural images, which are capturedwith user's intention and intervention, incidental texts usually exhibit muchmore diversity, variability and complexity, thus posing significantdifficulties and challenges for scene text detection and recognitionalgorithms. The ICDAR 2015 Robust Reading Competition Challenge 4 was launchedto assess the performance of existing scene text detection and recognitionmethods on incidental texts as well as to stimulate novel ideas and solutions.This report is dedicated to briefly introduce our strategies for thischallenging problem and compare them with prior arts in this field.
arxiv-15000-72 | Discriminative Sparse Neighbor Approximation for Imbalanced Learning | http://arxiv.org/pdf/1602.01197v1.pdf | author:Chen Huang, Chen Change Loy, Xiaoou Tang category:cs.CV published:2016-02-03 summary:Data imbalance is common in many vision tasks where one or more classes arerare. Without addressing this issue conventional methods tend to be biasedtoward the majority class with poor predictive accuracy for the minority class.These methods further deteriorate on small, imbalanced data that has a largedegree of class overlap. In this study, we propose a novel discriminativesparse neighbor approximation (DSNA) method to ameliorate the effect ofclass-imbalance during prediction. Specifically, given a test sample, we firsttraverse it through a cost-sensitive decision forest to collect a good subsetof training examples in its local neighborhood. Then we generate from thissubset several class-discriminating but overlapping clusters and model each asan affine subspace. From these subspaces, the proposed DSNA iteratively seeksan optimal approximation of the test sample and outputs an unbiased prediction.We show that our method not only effectively mitigates the imbalance issue, butalso allows the prediction to extrapolate to unseen data. The latter capabilityis crucial for achieving accurate prediction on small dataset with limitedsamples. The proposed imbalanced learning method can be applied to bothclassification and regression tasks at a wide range of imbalance levels. Itsignificantly outperforms the state-of-the-art methods that do not possess animbalance handling mechanism, and is found to perform comparably or even betterthan recent deep learning methods by using hand-crafted features only.
arxiv-15000-73 | High-Dimensional Regularized Discriminant Analysis | http://arxiv.org/pdf/1602.01182v1.pdf | author:John A. Ramey, Caleb K. Stein, Phil D. Young, Dean M. Young category:stat.ML published:2016-02-03 summary:Friedman proposed the popular regularized discriminant analysis (RDA)classifier that utilizes a biased covariance-matrix estimator that partiallypools the sample covariance matrices from linear and quadratic discriminantanalysis and shrinks the resulting estimator towards a scaled identity matrix.The RDA classifier's two tuning parameters are typically estimated via acomputationally burdensome cross-validation procedure that uses a grid search.We formulate a new RDA-based classifier for the small-sample, high-dimensionalsetting and then show that the classification decision rule is equivalent to aclassifier in a subspace having a much lower dimension. As a result, theutilization of the dimension-reduction step yields a substantial reduction incomputation during model selection. Also, our parameterization offersinterpretability that was previously lacking with the RDA classifier. Wedemonstrate that our proposed classifier is often superior to several recentlyproposed sparse and regularized classifiers in terms of classification accuracywith three artificial and six real high-dimensional data sets. Finally, weprovide an implementation of our proposed classifier in the sparsediscrim Rpackage, which is available on CRAN.
arxiv-15000-74 | Learning Discriminative Features via Label Consistent Neural Network | http://arxiv.org/pdf/1602.01168v1.pdf | author:Zhuolin Jiang, Yaming Wang, Larry Davis, Walt Andrews, Viktor Rozgic category:cs.CV cs.LG cs.MM cs.NE stat.ML published:2016-02-03 summary:Deep Convolutional Neural Network (CNN) enforces supervised information onlyat the output layer, and hidden layers are trained by back propagating theprediction error from the output layer without explicit supervision. We proposea supervised feature learning approach, Label Consistency Neural Network, whichenforces direct supervision in late hidden layers. We associate each neuron ina hidden layer with a particular class label and encourage it to be activatedfor input signals from the same class. More specifically, we introduce a labelconsistency regularization called "discriminative representation error" lossfor late hidden layers and combine it with classification error loss to buildour overall objective function. This label consistency constraint not onlyalleviates the common problem of gradient vanishing and tends to fasterconvergence, but also makes the features derived from late hidden layersdiscriminative enough for classification even using a simple classifier such as$k$-NN classifier, since input signals from the same class will have verysimilar representations. Experimental results demonstrate that our approach canachieve state-of-the-art performances on several public benchmarks for actionand object category recognition.
arxiv-15000-75 | Single-Solution Hypervolume Maximization and its use for Improving Generalization of Neural Networks | http://arxiv.org/pdf/1602.01164v1.pdf | author:Conrado S. Miranda, Fernando J. Von Zuben category:cs.LG cs.NE stat.ML published:2016-02-03 summary:This paper introduces the hypervolume maximization with a single solution asan alternative to the mean loss minimization. The relationship between the twoproblems is proved through bounds on the cost function when an optimal solutionto one of the problems is evaluated on the other, with a hyperparameter tocontrol the similarity between the two problems. This same hyperparameterallows higher weight to be placed on samples with higher loss when computingthe hypervolume's gradient, whose normalized version can range from the meanloss to the max loss. An experiment on MNIST with a neural network is used tovalidate the theory developed, showing that the hypervolume maximization canbehave similarly to the mean loss minimization and can also provide betterperformance, resulting on a 20% reduction of the classification error on thetest set.
arxiv-15000-76 | GraphPrints: Towards a Graph Analytic Method for Network Anomaly Detection | http://arxiv.org/pdf/1602.01130v1.pdf | author:Christopher R. Harshaw, Robert A. Bridges, Michael D. Iannacone, Joel W. Reed, John R. Goodall category:cs.CR stat.ML published:2016-02-02 summary:This paper introduces a novel graph-analytic approach for detecting anomaliesin network flow data called GraphPrints. Building on foundationalnetwork-mining techniques, our method represents time slices of traffic as agraph, then counts graphlets -- small induced subgraphs that describe localtopology. By performing outlier detection on the sequence of graphlet counts,anomalous intervals of traffic are identified, and furthermore, individual IPsexperiencing abnormal behavior are singled-out. Initial testing of GraphPrintsis performed on real network data with an implanted anomaly. Evaluation showsfalse positive rates bounded by 2.84% at the time-interval level, and 0.05% atthe IP-level with 100% true positive rates at both.
arxiv-15000-77 | Fitting a 3D Morphable Model to Edges: A Comparison Between Hard and Soft Correspondences | http://arxiv.org/pdf/1602.01125v1.pdf | author:Anil Bas, William A. P. Smith, Timo Bolkart, Stefanie Wuhrer category:cs.CV published:2016-02-02 summary:We propose a fully automatic method for fitting a 3D morphable model tosingle face images in arbitrary pose and lighting. Our approach relies ongeometric features (edges and landmarks) and, inspired by the iterated closestpoint algorithm, is based on computing hard correspondences between modelvertices and edge pixels. We demonstrate that this is superior to previous workthat uses soft correspondences to form an edge-derived cost surface that isminimised by nonlinear optimisation.
arxiv-15000-78 | On the Nyström and Column-Sampling Methods for the Approximate Principal Components Analysis of Large Data Sets | http://arxiv.org/pdf/1602.01120v1.pdf | author:Darren Homrighausen, Daniel J. McDonald category:stat.ML stat.CO published:2016-02-02 summary:In this paper we analyze approximate methods for undertaking a principalcomponents analysis (PCA) on large data sets. PCA is a classical dimensionreduction method that involves the projection of the data onto the subspacespanned by the leading eigenvectors of the covariance matrix. This projectioncan be used either for exploratory purposes or as an input for furtheranalysis, e.g. regression. If the data have billions of entries or more, thecomputational and storage requirements for saving and manipulating the designmatrix in fast memory is prohibitive. Recently, the Nystr\"om andcolumn-sampling methods have appeared in the numerical linear algebra communityfor the randomized approximation of the singular value decomposition of largematrices. However, their utility for statistical applications remains unclear.We compare these approximations theoretically by bounding the distance betweenthe induced subspaces and the desired, but computationally infeasible, PCAsubspace. Additionally we show empirically, through simulations and a real dataexample involving a corpus of emails, the trade-off of approximation accuracyand computational complexity.
arxiv-15000-79 | Do Cascades Recur? | http://arxiv.org/pdf/1602.01107v1.pdf | author:Justin Cheng, Lada A Adamic, Jon Kleinberg, Jure Leskovec category:cs.SI physics.soc-ph stat.ML H.2.8 published:2016-02-02 summary:Cascades of information-sharing are a primary mechanism by which contentreaches its audience on social media, and an active line of research hasstudied how such cascades, which form as content is reshared from person toperson, develop and subside. In this paper, we perform a large-scale analysisof cascades on Facebook over significantly longer time scales, and find that amore complex picture emerges, in which many large cascades recur, exhibitingmultiple bursts of popularity with periods of quiescence in between. Wecharacterize recurrence by measuring the time elapsed between bursts, theiroverlap and proximity in the social network, and the diversity in thedemographics of individuals participating in each peak. We discover thatcontent virality, as revealed by its initial popularity, is a main driver ofrecurrence, with the availability of multiple copies of that content helping tospark new bursts. Still, beyond a certain popularity of content, the rate ofrecurrence drops as cascades start exhausting the population of interestedindividuals. We reproduce these observed patterns in a simple model of contentrecurrence simulated on a real social network. Using only characteristics of acascade's initial burst, we demonstrate strong performance in predictingwhether it will recur in the future.
arxiv-15000-80 | Self-taught Object Localization with Deep Networks | http://arxiv.org/pdf/1409.3964v7.pdf | author:Loris Bazzani, Alessandro Bergamo, Dragomir Anguelov, Lorenzo Torresani category:cs.CV published:2014-09-13 summary:This paper introduces self-taught object localization, a novel approach thatleverages deep convolutional networks trained for whole-image recognition tolocalize objects in images without additional human supervision, i.e., withoutusing any ground-truth bounding boxes for training. The key idea is to analyzethe change in the recognition scores when artificially masking out differentregions of the image. The masking out of a region that includes the objecttypically causes a significant drop in recognition score. This idea is embeddedinto an agglomerative clustering technique that generates self-taughtlocalization hypotheses. Our object localization scheme outperforms existingproposal methods in both precision and recall for small number of subwindowproposals (e.g., on ILSVRC-2012 it produces a relative gain of 23.4% over thestate-of-the-art for top-1 hypothesis). Furthermore, our experiments show thatthe annotations automatically-generated by our method can be used to trainobject detectors yielding recognition results remarkably close to thoseobtained by training on manually-annotated bounding boxes.
arxiv-15000-81 | Development of an Ideal Observer that Incorporates Nuisance Parameters and Processes List-Mode Data | http://arxiv.org/pdf/1602.01449v1.pdf | author:Christopher J. MacGahan, Matthew A. Kupinski, Nathan R. Hilton, Erik M. Brubaker, William C. Johnson category:cs.CV published:2016-02-02 summary:Observer models were developed to process data in list-mode format in orderto perform binary discrimination tasks for use in an arms-control-treatycontext. Data used in this study was generated using GEANT4 Monte Carlosimulations for photons using custom models of plutonium inspection objects anda radiation imaging system. Observer model performance was evaluated andpresented using the area under the receiver operating characteristic curve. Theideal observer was studied under both signal-known-exactly conditions and inthe presence of unknowns such as object orientation and absolute count-ratevariability; when these additional sources of randomness were present, theirincorporation into the observer yielded superior performance.
arxiv-15000-82 | Improved Achievability and Converse Bounds for Erdős-Rényi Graph Matching | http://arxiv.org/pdf/1602.01042v1.pdf | author:Daniel Cullina, Negar Kiyavash category:cs.IT cs.LG math.IT published:2016-02-02 summary:We consider the problem of perfectly recovering the vertex correspondencebetween two correlated Erd\H{o}s-R\'enyi (ER) graphs. For a pair of correlatedgraphs on the same vertex set, the correspondence between the vertices can beobscured by randomly permuting the vertex labels of one of the graphs. In somecases, the structural information in the graphs allow this correspondence to berecovered. We investigate the information-theoretic threshold for exactrecovery, i.e. the conditions under which the entire vertex correspondence canbe correctly recovered given unbounded computational resources. Pedarsani and Grossglauser provided an achievability result of this type.Their result establishes the scaling dependence of the threshold on the numberof vertices. We improve on their achievability bound. We also provide aconverse bound, establishing conditions under which exact recovery isimpossible. Together, these establish the scaling dependence of the thresholdon the level of correlation between the two graphs. The converse andachievability bounds differ by a factor of two for sparse, significantlycorrelated graphs.
arxiv-15000-83 | On Deep Multi-View Representation Learning: Objectives and Optimization | http://arxiv.org/pdf/1602.01024v1.pdf | author:Weiran Wang, Raman Arora, Karen Livescu, Jeff Bilmes category:cs.LG published:2016-02-02 summary:We consider learning representations (features) in the setting in which wehave access to multiple unlabeled views of the data for learning while only oneview is available for downstream tasks. Previous work on this problem hasproposed several techniques based on deep neural networks, typically involvingeither autoencoder-like networks with a reconstruction objective or pairedfeedforward networks with a batch-style correlation-based objective. We analyzeseveral techniques based on prior work, as well as new variants, and comparethem empirically on image, speech, and text tasks. We find an advantage forcorrelation-based representation learning, while the best results on most tasksare obtained with our new variant, deep canonically correlated autoencoders(DCCAE). We also explore a stochastic optimization procedure for minibatchcorrelation-based objectives and discuss the time/performance trade-offs forkernel-based and neural network-based implementations.
arxiv-15000-84 | A-expansion for multiple "hedgehog" shapes | http://arxiv.org/pdf/1602.01006v1.pdf | author:Hossam Isack, Yuri Boykov, Olga Veksler category:cs.CV published:2016-02-02 summary:Overlapping colors and cluttered or weak edges are common segmentationproblems requiring additional regularization. For example, star-convexity ispopular for interactive single object segmentation due to simplicity andamenability to exact graph cut optimization. This paper proposes an approach tomultiobject segmentation where objects could be restricted to separate"hedgehog" shapes. We show that a-expansion moves are submodular for ourmulti-shape constraints. Each "hedgehog" shape has its surface normalsconstrained by some vector field, e.g. gradients of a distance transform foruser scribbles. Tight constraint give an extreme case of a shape priorenforcing skeleton consistency with the scribbles. Wider cones of allowednormals gives more relaxed hedgehog shapes. A single click and +/-90 degreesnormal orientation constraints reduce our hedgehog prior to star-convexity. Ifall hedgehogs come from single clicks then our approach defines multi-starprior. Our general method has significantly more applications than standardone-star segmentation. For example, in medical data we can separate multiplenon-star organs with similar appearances and weak or noisy edges.
arxiv-15000-85 | What Can I Do Around Here? Deep Functional Scene Understanding for Cognitive Robots | http://arxiv.org/pdf/1602.00032v2.pdf | author:Chengxi Ye, Yezhou Yang, Cornelia Fermuller, Yiannis Aloimonos category:cs.RO cs.CV published:2016-01-29 summary:For robots that have the capability to interact with the physical environmentthrough their end effectors, understanding the surrounding scenes is not merelya task of image classification or object recognition. To perform actual tasks,it is critical for the robot to have a functional understanding of the visualscene. Here, we address the problem of localizing and recognition of functionalareas from an arbitrary indoor scene, formulated as a two-stage deep learningbased detection pipeline. A new scene functionality testing-bed, which iscomplied from two publicly available indoor scene datasets, is used forevaluation. Our method is evaluated quantitatively on the new dataset,demonstrating the ability to perform efficient recognition of functional areasfrom arbitrary indoor scenes. We also demonstrate that our detection model canbe generalized onto novel indoor scenes by cross validating it with the imagesfrom two different datasets.
arxiv-15000-86 | Head Pose Estimation of Occluded Faces using Regularized Regression | http://arxiv.org/pdf/1602.00997v1.pdf | author:Amit Kumar, Rishabh Bindal, Soumya Indela, Michael Rotkowitz category:cs.CV published:2016-02-02 summary:This paper presents regression methods for estimation of head pose fromoccluded 2-D face images. The process primarily involves reconstructing a facefrom its occluded image, followed by classification. Typical methods forreconstruction assume that the pixel errors of the occluded regions areindependent. However, such an assumption is not true in the case of occlusion,because of its inherent contiguous nature. Hence, we use nuclear norm as ametric that can describe well the structure of the error. We also use LASSORegression based l1 - regularization to improve reconstruction. Next, weimplement Nuclear Norm Regularized Regression (NR), and also our proposedmethod, for reconstruction and subsequent classification. Finally, we comparethe performance of the methods in terms of accuracy of head pose estimation ofoccluded faces.
arxiv-15000-87 | Mental State Recognition via Wearable EEG | http://arxiv.org/pdf/1602.00985v1.pdf | author:Pouya Bashivan, Irina Rish, Steve Heisig category:cs.CV cs.HC published:2016-02-02 summary:The increasing quality and affordability of consumer electroencephalogram(EEG) headsets make them attractive for situations where medical grade devicesare impractical. Predicting and tracking cognitive states is possible for tasksthat were previously not conducive to EEG monitoring. For instance, monitoringoperators for states inappropriate to the task (e.g. drowsy drivers), trackingmental health (e.g. anxiety) and productivity (e.g. tiredness) are amongpossible applications for the technology. Consumer grade EEG headsets areaffordable and relatively easy to use, but they lack the resolution and qualityof signal that can be achieved using medical grade EEG devices. Thus, the keyquestions remain: to what extent are wearable EEG devices capable of mentalstate recognition, and what kind of mental states can be accurately recognizedwith these devices? In this work, we examined responses to two different typesof input: instructional (logical) versus recreational (emotional) videos, usinga range of machine-learning methods. We tried SVMs, sparse logistic regression,and Deep Belief Networks, to discriminate between the states of mind induced bydifferent types of video input, that can be roughly labeled as logical vs.emotional. Our results demonstrate a significant potential of wearable EEGdevices in differentiating cognitive states between situations with largecontextual but subtle apparent differences.
arxiv-15000-88 | Visual descriptors for content-based retrieval of remote sensing images | http://arxiv.org/pdf/1602.00970v1.pdf | author:Paolo Napoletano category:cs.CV published:2016-02-02 summary:In this paper we present an extensive evaluation of visual descriptors forthe content-based retrieval of remote sensing images. The evaluation includesglobal, local, and Convolutional Neural Network (CNNs) features coupled withthree different Content-Based Image Retrieval schemas. We conducted all theexperiments on two publicly available datasets: the 21-class UC Merced LandUse/Land Cover data set and 19-class High-resolution Satellite Scene dataset.Results demonstrate that features extracted from CNNs are the best performingwhatever is the retrieval schema adopted. Local descriptors perform better thanCNN-based descriptors only when dealing with images that contain fine-grainedtextures or objects.
arxiv-15000-89 | Tightening the Sample Complexity of Empirical Risk Minimization via Preconditioned Stability | http://arxiv.org/pdf/1601.04011v2.pdf | author:Alon Gonen, Shai Shalev-Shwartz category:cs.LG published:2016-01-15 summary:We tighten the sample complexity of empirical risk minimization (ERM)associated with a class of generalized linear models that include linear andlogistic regression. In particular, we conclude that ERM attains the optimalsample complexity for linear regression. Our analysis relies on a new notion ofstability, called preconditioned stability, which may be of independentinterest.
arxiv-15000-90 | Marvin: Semantic annotation using multiple knowledge sources | http://arxiv.org/pdf/1602.00515v2.pdf | author:Nikola Milosevic category:cs.AI cs.CL published:2016-02-01 summary:People are producing more written material then anytime in the history. Theincrease is so high that professionals from the various fields are no more ableto cope with this amount of publications. Text mining tools can offer tools tohelp them and one of the tools that can aid information retrieval andinformation extraction is semantic text annotation. In this report we presentMarvin, a text annotator written in Java, which can be used as a command linetool and as a Java library. Marvin is able to annotate text using multiplesources, including WordNet, MetaMap, DBPedia and thesauri represented as SKOS.
arxiv-15000-91 | An analytic comparison of regularization methods for Gaussian Processes | http://arxiv.org/pdf/1602.00853v1.pdf | author:Hossein Mohammadi, Rodolphe Le Riche, Nicolas Durrande, Eric Touboul, Xavier Bay category:math.OC math.ST stat.ML stat.TH published:2016-02-02 summary:Gaussian Processes (GPs) are often used to predict the output of aparameterized deterministic experiment. They have many applications in thefield of Computer Experiments, in particular to perform sensitivity analysis,adaptive design of experiments and global optimization. Nearly all of theapplications of GPs to Computer Experiments require the inversion of acovariance matrix. Because this matrix is often ill-conditioned, regularizationtechniques are required. Today, there is still a need to better regularize GPs.The two most classical regularization methods are i) pseudoinverse (PI) and ii)nugget (or jitter or observation noise). This article provides algebraiccalculations which allow comparing PI and nugget regularizations. It is proventhat pseudoinverse regularization averages the output values and makes thevariance null at redundant points. On the opposite, nugget regularization lacksinterpolation properties but preserves a non-zero variance at every point.However , these two regularization techniques become similar as the nuggetvalue decreases. A distribution-wise GP is introduced which interpolatesGaussian distributions instead of data points and mitigates the drawbacks ofpseudoinverse and nugget regularized GPs. Finally, data-model discrepancy isdiscussed and serves as a guide for choosing a regularization technique.
arxiv-15000-92 | On distances, paths and connections for hyperspectral image segmentation | http://arxiv.org/pdf/1603.08497v1.pdf | author:Guillaume Noyel, Jesus Angulo, Dominique Jeulin category:cs.CV math.NA published:2016-02-02 summary:The present paper introduces the $\eta$ and {\eta} connections in order toadd regional information on $\lambda$-flat zones, which only take into accounta local information. A top-down approach is considered. First $\lambda$-flatzones are built in a way leading to a sub-segmentation. Then a finersegmentation is obtained by computing $\eta$-bounded regions and $\mu$-geodesicballs inside the $\lambda$-flat zones. The proposed algorithms for theconstruction of new partitions are based on queues with an ordered selection ofseeds using the cumulative distance. $\eta$-bounded regions offers a control onthe variations of amplitude in the class from a point, called center, and$\mu$-geodesic balls controls the "size" of the class. These results areapplied to hyperspectral images.
arxiv-15000-93 | Fast unsupervised Bayesian image segmentation with adaptive spatial regularisation | http://arxiv.org/pdf/1502.01400v3.pdf | author:Marcelo Pereyra, Steve McLaughlin category:stat.CO cs.CV published:2015-02-05 summary:This paper presents a new Bayesian estimation technique for hiddenPotts-Markov random fields with unknown regularisation parameters, withapplication to fast unsupervised K-class image segmentation. The technique isderived by first removing the regularisation parameter from the Bayesian modelby marginalisation, followed by a small-variance-asymptotic (SVA) analysis inwhich the spatial regularisation and the integer-constrained terms of the Pottsmodel are decoupled. The evaluation of this SVA Bayesian estimator is thenrelaxed into a problem that can be computed efficiently by iteratively solvinga convex total-variation denoising problem and a least-squares clustering(K-means) problem, both of which can be solved straightforwardly, even inhigh-dimensions, and with parallel computing techniques. This leads to a fastfully unsupervised Bayesian image segmentation methodology in which thestrength of the spatial regularisation is adapted automatically to the observedimage during the inference procedure, and that can be easily applied in large2D and 3D scenarios or in applications requiring low computing times.Experimental results on real images, as well as extensive comparisons withstate-of-the-art algorithms, confirm that the proposed methodology offerextremely fast convergence and produces accurate segmentation results, with theimportant additional advantage of self-adjusting regularisation parameters.
arxiv-15000-94 | Learning a Deep Model for Human Action Recognition from Novel Viewpoints | http://arxiv.org/pdf/1602.00828v1.pdf | author:Hossein Rahmani, Ajmal Mian, Mubarak Shah category:cs.CV published:2016-02-02 summary:Recognizing human actions from unknown and unseen (novel) views is achallenging problem. We propose a Robust Non-Linear Knowledge Transfer Model(R-NKTM) for human action recognition from novel views. The proposed R-NKTM isa deep fully-connected neural network that transfers knowledge of human actionsfrom any unknown view to a shared high-level virtual view by finding anon-linear virtual path that connects the views. The R-NKTM is learned fromdense trajectories of synthetic 3D human models fitted to real motion capturedata and generalizes to real videos of human actions. The strength of ourtechnique is that we learn a single R-NKTM for all actions and all viewpointsfor knowledge transfer of any real human action video without the need forre-training or fine-tuning the model. Thus, R-NKTM can efficiently scale toincorporate new action classes. R-NKTM is learned with dummy labels and doesnot require knowledge of the camera viewpoint at any stage. Experiments onthree benchmark cross-view human action datasets show that our methodoutperforms existing state-of-the-art.
arxiv-15000-95 | The Grail theorem prover: Type theory for syntax and semantics | http://arxiv.org/pdf/1602.00812v1.pdf | author:Richard Moot category:cs.CL published:2016-02-02 summary:As the name suggests, type-logical grammars are a grammar formalism based onlogic and type theory. From the prespective of grammar design, type-logicalgrammars develop the syntactic and semantic aspects of linguistic phenomenahand-in-hand, letting the desired semantics of an expression inform thesyntactic type and vice versa. Prototypical examples of the successfulapplication of type-logical grammars to the syntax-semantics interface includecoordination, quantifier scope and extraction.This chapter describes the Grailtheorem prover, a series of tools for designing and testing grammars in variousmodern type-logical grammars which functions as a tool . All tools described inthis chapter are freely available.
arxiv-15000-96 | Large Scale Business Discovery from Street Level Imagery | http://arxiv.org/pdf/1512.05430v2.pdf | author:Qian Yu, Christian Szegedy, Martin C. Stumpe, Liron Yatziv, Vinay Shet, Julian Ibarz, Sacha Arnoud category:cs.CV published:2015-12-17 summary:Search with local intent is becoming increasingly useful due to thepopularity of the mobile device. The creation and maintenance of accuratelistings of local businesses worldwide is time consuming and expensive. In thispaper, we propose an approach to automatically discover businesses that arevisible on street level imagery. Precise business store front detection enablesaccurate geo-location of businesses, and further provides input for businesscategorization, listing generation, etc. The large variety of businesscategories in different countries makes this a very challenging problem.Moreover, manual annotation is prohibitive due to the scale of this problem. Wepropose the use of a MultiBox based approach that takes input image pixels anddirectly outputs store front bounding boxes. This end-to-end learning approachinstead preempts the need for hand modeling either the proposal generationphase or the post-processing phase, leveraging large labelled trainingdatasets. We demonstrate our approach outperforms the state of the artdetection techniques with a large margin in terms of performance and run-timeefficiency. In the evaluation, we show this approach achieves human accuracy inthe low-recall settings. We also provide an end-to-end evaluation of businessdiscovery in the real world.
arxiv-15000-97 | Speeding Up MCMC by Efficient Data Subsampling | http://arxiv.org/pdf/1404.4178v3.pdf | author:Matias Quiroz, Mattias Villani, Robert Kohn category:stat.ME stat.CO stat.ML published:2014-04-16 summary:We propose a Markov Chain Monte Carlo (MCMC) framework where the likelihoodfunction for $n$ observations is estimated from a random subset of $m$observations. Inspired by the survey sampling literature, we introduce ageneral and highly efficient log-likelihood estimator. The estimatorincorporates information about each observation's contribution to thelog-likelihood function. The computational complexity of the estimator can bemuch smaller than for the full log-likelihood, and we document substantialspeed-ups in the applications. The likelihood estimate is used within aPseudo-marginal framework to sample from a perturbed posterior which we proveto be within $O(m^{-1/2})$ of the true posterior. Moreover, the approximationerror is demonstrated to be negligible even for a small $m$ in ourapplications. We propose a simple way to adaptively choose the sample size $m$during the MCMC to optimize sampling efficiency for a fixed computationalbudget. We also propose a correlated pseudo marginal approach to subsamplingthat dramatically improves performance. The method is illustrated on threeexamples, each one representing a different data structure. In particular, weshow that our method outperforms other subsampling MCMC methods proposed in theliterature.
arxiv-15000-98 | Binary embeddings with structured hashed projections | http://arxiv.org/pdf/1511.05212v2.pdf | author:Anna Choromanska, Krzysztof Choromanski, Mariusz Bojarski, Tony Jebara, Sanjiv Kumar, Yann LeCun category:cs.LG published:2015-11-16 summary:We consider the hashing mechanism for constructing binary embeddings, thatinvolves pseudo-random projections followed by nonlinear (sign function)mappings. The pseudo-random projection is described by a matrix, where not allentries are independent random variables but instead a fixed "budget ofrandomness" is distributed across the matrix. Such matrices can be efficientlystored in sub-quadratic or even linear space, provide reduction in randomnessusage (i.e. number of required random values), and very often lead tocomputational speed ups. We prove several theoretical results showing thatprojections via various structured matrices followed by nonlinear mappingsaccurately preserve the angular distance between input high-dimensionalvectors. To the best of our knowledge, these results are the first that givetheoretical ground for the use of general structured matrices in the nonlinearsetting. In particular, they generalize previous extensions of theJohnson-Lindenstrauss lemma and prove the plausibility of the approach that wasso far only heuristically confirmed for some special structured matrices.Consequently, we show that many structured matrices can be used as an efficientinformation compression mechanism. Our findings build a better understanding ofcertain deep architectures, which contain randomly weighted and untrainedlayers, and yet achieve high performance on different learning tasks. Weempirically verify our theoretical findings and show the dependence of learningvia structured hashed projections on the performance of neural network as wellas nearest neighbor classifier.
arxiv-15000-99 | Simple Online and Realtime Tracking | http://arxiv.org/pdf/1602.00763v1.pdf | author:Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, Ben Upcroft category:cs.CV published:2016-02-02 summary:This paper explores a pragmatic approach to multiple object tracking wherethe main focus is to associate objects efficiently for online and realtimeapplications. To this end, detection quality is identified as a key factorinfluencing tracking performance, where changing the detector can improvetracking by up to 18.9%. Despite only using a rudimentary combination offamiliar techniques such as the Kalman Filter and Hungarian algorithm for thetracking components, this approach achieves an accuracy comparable tostate-of-the-art online trackers. Furthermore, due to the simplicity of ourtracking method, the tracker updates at a rate of 260 Hz which is over 20xfaster than other state-of-the-art trackers.
arxiv-15000-100 | Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects | http://arxiv.org/pdf/1602.00753v1.pdf | author:Hessam Bagherinezhad, Hannaneh Hajishirzi, Yejin Choi, Ali Farhadi category:cs.AI cs.CV published:2016-02-02 summary:Human vision greatly benefits from the information about sizes of objects.The role of size in several visual reasoning tasks has been thoroughly exploredin human perception and cognition. However, the impact of the information aboutsizes of objects is yet to be determined in AI. We postulate that this ismainly attributed to the lack of a comprehensive repository of sizeinformation. In this paper, we introduce a method to automatically infer objectsizes, leveraging visual and textual information from web. By maximizing thejoint likelihood of textual and visual observations, our method learns reliablerelative size estimates, with no explicit human supervision. We introduce therelative size dataset and show that our method outperforms competitive textualand visual baselines in reasoning about size comparisons.
arxiv-15000-101 | Combining ConvNets with Hand-Crafted Features for Action Recognition Based on an HMM-SVM Classifier | http://arxiv.org/pdf/1602.00749v1.pdf | author:Pichao Wang, Zhaoyang Li, Yonghong Hou, Wanqing Li category:cs.CV published:2016-02-01 summary:This paper proposes a new framework for RGB-D-based action recognition thattakes advantages of hand-designed features from skeleton data and deeplylearned features from depth maps, and exploits effectively both the local andglobal temporal information. Specifically, depth and skeleton data are firstlyaugmented for deep learning and making the recognition insensitive to viewvariance. Secondly, depth sequences are segmented using the hand-craftedfeatures based on skeleton joints motion histogram to exploit the localtemporal information. All training se gments are clustered using an InfiniteGaussian Mixture Model (IGMM) through Bayesian estimation and labelled fortraining Convolutional Neural Networks (ConvNets) on the depth maps. Thus, adepth sequence can be reliably encoded into a sequence of segment labels.Finally, the sequence of labels is fed into a joint Hidden Markov Model andSupport Vector Machine (HMM-SVM) classifier to explore the global temporalinformation for final recognition.
arxiv-15000-102 | Learning Data Triage: Linear Decoding Works for Compressive MRI | http://arxiv.org/pdf/1602.00734v1.pdf | author:Yen-Huan Li, Volkan Cevher category:cs.IT cs.LG math.IT stat.ML published:2016-02-01 summary:The standard approach to compressive sampling considers recovering an unknowndeterministic signal with certain known structure, and designing thesub-sampling pattern and recovery algorithm based on the known structure. Thisapproach requires looking for a good representation that reveals the signalstructure, and solving a non-smooth convex minimization problem (e.g., basispursuit). In this paper, another approach is considered: We learn a goodsub-sampling pattern based on available training signals, without knowing thesignal structure in advance, and reconstruct an accordingly sub-sampled signalby computationally much cheaper linear reconstruction. We provide a theoreticalguarantee on the recovery error, and show via experiments on real-world MRIdata the effectiveness of the proposed compressive MRI scheme.
arxiv-15000-103 | Algorithm-Induced Prior for Image Restoration | http://arxiv.org/pdf/1602.00715v1.pdf | author:Stanley H. Chan category:cs.CV published:2016-02-01 summary:This paper studies a type of image priors that are constructed implicitlythrough the alternating direction method of multiplier (ADMM) algorithm, calledthe algorithm-induced prior. Different from classical image priors which aredefined before running the reconstruction algorithm, algorithm-induced priorsare defined by the denoising procedure used to replace one of the two modulesin the ADMM algorithm. Since such prior is not explicitly defined, analyzingthe performance has been difficult in the past. Focusing on the class of symmetric smoothing filters, this paper presents anexplicit expression of the prior induced by the ADMM algorithm. The new prioris reminiscent to the conventional graph Laplacian but with strongerreconstruction performance. It can also be shown that the overallreconstruction has an efficient closed-form implementation if the associatedsymmetric smoothing filter is low rank. The results are validated withexperiments on image inpainting.
arxiv-15000-104 | A Benchmark Comparison of State-of-the-Practice Sentiment Analysis Methods | http://arxiv.org/pdf/1512.01818v3.pdf | author:Filipe Nunes Ribeiro, Matheus Araújo, Pollyanna Gonçalves, Fabrício Benevenuto, Marcos André Gonçalves category:cs.CL cs.SI published:2015-12-06 summary:In the last few years thousands of scientific papers have explored sentimentanalysis, several startups that measures opinions on real data have emerged,and a number of innovative products related to this theme have been developed.There are multiple methods for measuring sentiments, including lexical-basedapproaches and supervised machine learning methods. Despite the vast intereston the theme and wide popularity of some methods, it is unclear which method isbetter for identifying the polarity (i.e., positive or negative) of a message.Thus, there is a strong need to conduct a thorough apple-to-apple comparison ofsentiment analysis methods, as they are used in practice, across multipledatasets originated from different data sources. Such a comparison is key forunderstanding the potential limitations, advantages, and disadvantages ofpopular methods. This study aims at filling this gap by presenting a benchmarkcomparison of twenty one popular sentiment analysis methods (which we call thestate-of-the-practice methods). Our evaluation is based on a benchmark oftwenty labeled datasets, covering messages posted on social networks, movie andproduct reviews, as well as opinions and comments in news articles. Our resultshighlight the extent to which the prediction performance of these methodsvaries widely across datasets. Aiming at boosting the development of thisresearch area, we open the methods' codes and datasets used in this paper andwe deploy a benchmark system, which provides an open API for accessing andcomparing sentence-level sentiment analysis methods.
arxiv-15000-105 | Improving Vertebra Segmentation through Joint Vertebra-Rib Atlases | http://arxiv.org/pdf/1602.00585v1.pdf | author:Yinong Wang, Jianhua Yao, Holger R. Roth, Joseph E. Burns, Ronald M. Summers category:cs.CV published:2016-02-01 summary:Accurate spine segmentation allows for improved identification andquantitative characterization of abnormalities of the vertebra, such asvertebral fractures. However, in existing automated vertebra segmentationmethods on computed tomography (CT) images, leakage into nearby bones such asribs occurs due to the close proximity of these visibly intense structures in a3D CT volume. To reduce this error, we propose the use of joint vertebra-ribatlases to improve the segmentation of vertebrae via multi-atlas joint labelfusion. Segmentation was performed and evaluated on CTs containing 106 thoracicand lumbar vertebrae from 10 pathological and traumatic spine patients on anindividual vertebra level basis. Vertebra atlases produced errors where thesegmentation leaked into the ribs. The use of joint vertebra-rib atlasesproduced a statistically significant increase in the Dice coefficient from 92.5$\pm$ 3.1% to 93.8 $\pm$ 2.1% for the left and right transverse processes and adecrease in the mean and max surface distance from 0.75 $\pm$ 0.60mm and 8.63$\pm$ 4.44mm to 0.30 $\pm$ 0.27mm and 3.65 $\pm$ 2.87mm, respectively.
arxiv-15000-106 | A Deep Learning Based Fast Image Saliency Detection Algorithm | http://arxiv.org/pdf/1602.00577v1.pdf | author:Hengyue Pan, Hui Jiang category:cs.CV published:2016-02-01 summary:In this paper, we propose a fast deep learning method for object saliencydetection using convolutional neural networks. In our approach, we use agradient descent method to iteratively modify the input images based on thepixel-wise gradients to reduce a pre-defined cost function, which is defined tomeasure the class-specific objectness and clamp the class-irrelevant outputs tomaintain image background. The pixel-wise gradients can be efficiently computedusing the back-propagation algorithm. We further apply SLIC superpixels and LABcolor based low level saliency features to smooth and refine the gradients. Ourmethods are quite computationally efficient, much faster than other deeplearning based saliency methods. Experimental results on two benchmark tasks,namely Pascal VOC 2012 and MSRA10k, have shown that our proposed methods cangenerate high-quality salience maps, at least comparable with many slow andcomplicated deep learning methods. Comparing with the pure low-level methods,our approach excels in handling many difficult images, which contain complexbackground, highly-variable salient objects, multiple objects, and/or verysmall salient objects.
arxiv-15000-107 | Multi-object Classification via Crowdsourcing with a Reject Option | http://arxiv.org/pdf/1602.00575v1.pdf | author:Qunwei Li, Aditya Vempaty, Lav R. Varshney, Pramod K. Varshney category:cs.LG published:2016-02-01 summary:We explore the design of an effective crowdsourcing system for an $M$-aryclassification task. Crowd workers complete simple binary microtasks whoseresults are aggregated to give the final decision. We consider the novelscenario where the workers have a reject option so that they are allowed toskip microtasks when they are unable to or choose not to respond to binarymicrotasks. For example, in mismatched speech transcription, crowd workers whodo not know the language may not be able to respond to certain microtasksoutside their categorical perception. We present an aggregation approach usinga weighted majority voting rule, where each worker's response is assigned anoptimized weight to maximize crowd's classification performance. We evaluatesystem performance in both exact and asymptotic forms. Further, we consider thecase where there may be a set of greedy workers in the crowd for whom reward isvery important. Greedy workers may respond to a microtask even when they areunable to perform it reliably. We consider an oblivious and an expurgationstrategy for crowdsourcing with greedy workers, and develop an algorithm toadaptively switch between the two, based on the estimated fraction of greedyworkers in the anonymous crowd. Simulation results show performance improvementof the proposed approaches compared with conventional majority voting.
arxiv-15000-108 | Graph-based Predictable Feature Analysis | http://arxiv.org/pdf/1602.00554v1.pdf | author:Björn Weghenkel, Asja Fischer, Laurenz Wiskott category:cs.LG published:2016-02-01 summary:We propose a new method for the unsupervised extraction of predictablefeatures from high-dimensional time-series, where high predictability isunderstood very generically as low variance in the distribution of the nextdata point given the current one. We show how this objective can be understoodin terms of graph embedding as well as how it corresponds to theinformation-theoretic measure of excess entropy in special cases.Experimentally, we compare the approach to two other algorithms for theextraction of predictable features, namely ForeCA and PFA, and show how it isable to outperform them in certain settings.
arxiv-15000-109 | TED: A Tolerant Edit Distance for Segmentation Evaluation | http://arxiv.org/pdf/1503.02291v3.pdf | author:Jan Funke, Francesc Moreno-Noguer, Albert Cardona, Matthew Cook category:cs.CV published:2015-03-08 summary:In this paper, we present a novel error measure to compare a segmentationagainst ground truth. This measure, which we call Tolerant Edit Distance (TED),is motivated by two observations: (1) Some errors, like small boundary shifts,are tolerable in practice. Which errors are tolerable is application dependentand should be a parameter of the measure. (2) Non-tolerable errors have to becorrected manually. The time needed to do so should be reflected by the errormeasure. Using integer linear programming, the TED finds the minimal weightedsum of split and merge errors exceeding a given tolerance criterion, and thusprovides a time-to-fix estimate. In contrast to commonly used measures likeRand index or variation of information, the TED (1) does not count small, buttolerable, differences, (2) provides intuitive numbers, (3) gives a time-to-fixestimate, and (4) can localize and classify the type of errors. By supportingboth isotropic and anisotropic volumes and having a flexible tolerancecriterion, the TED can be adapted to different requirements. On examplesegmentations for 3D neuron segmentation, we demonstrate that the TED iscapable of counting topological errors, while ignoring small boundary shifts.
arxiv-15000-110 | PAC-Bayesian Online Clustering | http://arxiv.org/pdf/1602.00522v1.pdf | author:Le Li, Benjamin Guedj, Sébastien Loustau category:stat.ML math.ST stat.TH published:2016-02-01 summary:This paper addresses the online clustering problem. When faced with highfrequency streams of data, clustering raises theoretical and algorithmicpitfalls. Working under a sparsity assumption, a new online clusteringalgorithm is introduced. Our procedure relies on the PAC-Bayesian approach,allowing for a dynamic (i.e., time-dependent) estimation of the number ofclusters. Its theoretical merits are supported by sparsity regret bounds, andan RJMCMC-flavored implementation called PACO is proposed along with numericalexperiments to assess its potential.
arxiv-15000-111 | I Know What You Saw Last Minute - Encrypted HTTP Adaptive Video Streaming Title Classification | http://arxiv.org/pdf/1602.00490v1.pdf | author:Ran Dubin, Amit Dvir, Ofir Pele, Ofer Hadar category:cs.MM cs.LG cs.NI published:2016-02-01 summary:Previous research has shown that information can be extracted from encryptedmultimedia streams. This includes video titles classification of non HTTPadaptive streams (non-HAS). This paper presents an algorithm for\emph{encrypted HTTP adaptive video streaming title classification}. Weevaluated our algorithm on a new YouTube popular videos dataset that wascollected from the internet under real-world network conditions. We provide thedataset and the crawler for future research. Our algorithm's classificationaccuracy is 98\%.
arxiv-15000-112 | An Iterative Deep Learning Framework for Unsupervised Discovery of Speech Features and Linguistic Units with Applications on Spoken Term Detection | http://arxiv.org/pdf/1602.00426v1.pdf | author:Cheng-Tao Chung, Cheng-Yu Tsai, Hsiang-Hung Lu, Chia-Hsiang Liu, Hung-yi Lee, Lin-shan Lee category:cs.CL cs.LG published:2016-02-01 summary:In this work we aim to discover high quality speech features and linguisticunits directly from unlabeled speech data in a zero resource scenario. Theresults are evaluated using the metrics and corpora proposed in the ZeroResource Speech Challenge organized at Interspeech 2015. A Multi-layeredAcoustic Tokenizer (MAT) was proposed for automatic discovery of multiple setsof acoustic tokens from the given corpus. Each acoustic token set is specifiedby a set of hyperparameters that describe the model configuration. These setsof acoustic tokens carry different characteristics fof the given corpus and thelanguage behind, thus can be mutually reinforced. The multiple sets of tokenlabels are then used as the targets of a Multi-target Deep Neural Network(MDNN) trained on low-level acoustic features. Bottleneck features extractedfrom the MDNN are then used as the feedback input to the MAT and the MDNNitself in the next iteration. We call this iterative deep learning frameworkthe Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN), whichgenerates both high quality speech features for the Track 1 of the Challengeand acoustic tokens for the Track 2 of the Challenge. In addition, we performedextra experiments on the same corpora on the application of query-by-examplespoken term detection. The experimental results showed the iterative deeplearning framework of MAT-DNN improved the detection performance due to betterunderlying speech features and acoustic tokens.
arxiv-15000-113 | A Survey on Contextual Multi-armed Bandits | http://arxiv.org/pdf/1508.03326v2.pdf | author:Li Zhou category:cs.LG published:2015-08-13 summary:In this survey we cover a few stochastic and adversarial contextual banditalgorithms. We analyze each algorithm's assumption and regret bound.
arxiv-15000-114 | Evolutionary stability implies asymptotic stability under multiplicative weights | http://arxiv.org/pdf/1601.07267v2.pdf | author:Ioannis Avramopoulos category:cs.GT cs.LG math.OC published:2016-01-27 summary:We show that evolutionarily stable states in general (nonlinear) populationgames (which can be viewed as continuous vector fields constrained on apolytope) are asymptotically stable under a multiplicative weights dynamic(under appropriate choices of a parameter called the learning rate or stepsize, which we demonstrate to be crucial to achieve convergence, as otherwiseeven chaotic behavior is possible to manifest). Our result implies thatevolutionary theories based on multiplicative weights are compatible (inprinciple, more general) with those based on the notion of evolutionarystability. However, our result further establishes multiplicative weights as anonlinear programming primitive (on par with standard nonlinear programmingmethods) since various nonlinear optimization problems, such as findingNash/Wardrop equilibria in nonatomic congestion games, which are well-known tobe equipped with a convex potential function, and finding strict local maximaof quadratic programming problems, are special cases of the problem ofcomputing evolutionarily stable states in nonlinear population games.
arxiv-15000-115 | Differentially Private Online Learning for Cloud-Based Video Recommendation with Multimedia Big Data in Social Networks | http://arxiv.org/pdf/1509.00181v7.pdf | author:Pan Zhou, Yingxue Zhou, Dapeng Wu, Hai Jin category:cs.LG published:2015-09-01 summary:With the rapid growth in multimedia services and the enormous offers of videocontents in online social networks, users have difficulty in obtaining theirinterests. Therefore, various personalized recommendation systems have beenproposed. However, they ignore that the accelerated proliferation of socialmedia data has led to the big data era, which has greatly impeded the processof video recommendation. In addition, none of them has considered both theprivacy of users' contexts (e,g., social status, ages and hobbies) and videoservice vendors' repositories, which are extremely sensitive and of significantcommercial value. To handle the problems, we propose a cloud-assisteddifferentially private video recommendation system based on distributed onlinelearning. In our framework, service vendors are modeled as distributedcooperative learners, recommending videos according to user's context, whilesimultaneously adapting the video-selection strategy based on user-clickfeedback to maximize total user clicks (reward). Considering the sparsity andheterogeneity of big social media data, we also propose a novel geometricdifferentially private model, which can greatly reduce the performance(recommendation accuracy) loss. Our simulation shows the proposed algorithmsoutperform other existing methods and keep a delicate balance between computingaccuracy and privacy preserving level.
arxiv-15000-116 | Scene Invariant Crowd Segmentation and Counting Using Scale-Normalized Histogram of Moving Gradients (HoMG) | http://arxiv.org/pdf/1602.00386v1.pdf | author:Parthipan Siva, Mohammad Javad Shafiee, Mike Jamieson, Alexander Wong category:cs.CV published:2016-02-01 summary:The problem of automated crowd segmentation and counting has garneredsignificant interest in the field of video surveillance. This paper proposes anovel scene invariant crowd segmentation and counting algorithm designed withhigh accuracy yet low computational complexity in mind, which is key forwidespread industrial adoption. A novel low-complexity, scale-normalizedfeature called Histogram of Moving Gradients (HoMG) is introduced for highlyeffective spatiotemporal representation of individuals and crowds within avideo. Real-time crowd segmentation is achieved via boosted cascade of weakclassifiers based on sliding-window HoMG features, while linear SVM regressionof crowd-region HoMG features is employed for real-time crowd counting.Experimental results using multi-camera crowd datasets show that the proposedalgorithm significantly outperform state-of-the-art crowd counting algorithms,as well as achieve very promising crowd segmentation results, thusdemonstrating the efficacy of the proposed method for highly-accurate,real-time video-driven crowd analysis.
arxiv-15000-117 | ConfidentCare: A Clinical Decision Support System for Personalized Breast Cancer Screening | http://arxiv.org/pdf/1602.00374v1.pdf | author:Ahmed M. Alaa, Kyeong H. Moon, William Hsu, Mihaela van der Schaar category:cs.LG published:2016-02-01 summary:Breast cancer screening policies attempt to achieve timely diagnosis by theregular screening of apparently healthy women. Various clinical decisions areneeded to manage the screening process; those include: selecting the screeningtests for a woman to take, interpreting the test outcomes, and deciding whetheror not a woman should be referred to a diagnostic test. Such decisions arecurrently guided by clinical practice guidelines (CPGs), which represent aone-size-fits-all approach that are designed to work well on average for apopulation, without guaranteeing that it will work well uniformly over thatpopulation. Since the risks and benefits of screening are functions of eachpatients features, personalized screening policies that are tailored to thefeatures of individuals are needed in order to ensure that the right tests arerecommended to the right woman. In order to address this issue, we presentConfidentCare: a computer-aided clinical decision support system that learns apersonalized screening policy from the electronic health record (EHR) data.ConfidentCare operates by recognizing clusters of similar patients, andlearning the best screening policy to adopt for each cluster. A cluster ofpatients is a set of patients with similar features (e.g. age, breast density,family history, etc.), and the screening policy is a set of guidelines on whatactions to recommend for a woman given her features and screening test scores.ConfidentCare algorithm ensures that the policy adopted for every cluster ofpatients satisfies a predefined accuracy requirement with a high level ofconfidence. We show that our algorithm outperforms the current CPGs in terms ofcost-efficiency and false positive rates.
arxiv-15000-118 | Efficient Character-level Document Classification by Combining Convolution and Recurrent Layers | http://arxiv.org/pdf/1602.00367v1.pdf | author:Yijun Xiao, Kyunghyun Cho category:cs.CL published:2016-02-01 summary:Document classification tasks were primarily tackled at word level. Recentresearch that works with character-level inputs shows several benefits overword-level approaches such as natural incorporation of morphemes and betterhandling of rare words. We propose a neural network architecture that utilizesboth convolution and recurrent layers to efficiently encode character inputs.We validate the proposed model on eight large scale document classificationtasks and compare with character-level convolution-only models. It achievescomparable performances with much less parameters.
arxiv-15000-119 | Semi-supervised K-means++ | http://arxiv.org/pdf/1602.00360v1.pdf | author:Jordan Yoder, Carey E. Priebe category:stat.ML published:2016-02-01 summary:Traditionally, practitioners initialize the {\tt k-means} algorithm withcenters chosen uniformly at random. Randomized initialization with unevenweights ({\tt k-means++}) has recently been used to improve the performanceover this strategy in cost and run-time. We consider the k-means problem withsemi-supervised information, where some of the data are pre-labeled, and weseek to label the rest according to the minimum cost solution. By extending the{\tt k-means++} algorithm and analysis to account for the labels, we derive animproved theoretical bound on expected cost and observe improved performance insimulated and real data examples. This analysis provides theoreticaljustification for a roughly linear semi-supervised clustering algorithm.
arxiv-15000-120 | DeepCare: A Deep Dynamic Memory Model for Predictive Medicine | http://arxiv.org/pdf/1602.00357v1.pdf | author:Trang Pham, Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.LG published:2016-02-01 summary:Personalized predictive medicine necessitates the modeling of patient illnessand care processes, which inherently have long-term temporal dependencies.Healthcare observations, recorded in electronic medical records, are episodicand irregular in time. We introduce DeepCare, an end-to-end deep dynamic neuralnetwork that reads medical records, stores previous illness history, inferscurrent illness states and predicts future medical outcomes. At the data level,DeepCare represents care episodes as vectors in space, models patient healthstate trajectories through explicit memory of historical records. Built on LongShort-Term Memory (LSTM), DeepCare introduces time parameterizations to handleirregular timed events by moderating the forgetting and consolidation of memorycells. DeepCare also incorporates medical interventions that change the courseof illness and shape future medical risk. Moving up to the health state level,historical and present health states are then aggregated through multiscaletemporal pooling, before passing through a neural network that estimates futureoutcomes. We demonstrate the efficacy of DeepCare for disease progressionmodeling, intervention recommendation, and future risk prediction. On twoimportant cohorts with heavy social and economic burden -- diabetes and mentalhealth -- the results show improved modeling and risk prediction accuracy.
arxiv-15000-121 | A Spectral Series Approach to High-Dimensional Nonparametric Regression | http://arxiv.org/pdf/1602.00355v1.pdf | author:Ann B. Lee, Rafael Izbicki category:stat.ME stat.ML published:2016-02-01 summary:A key question in modern statistics is how to make fast and reliableinferences for complex, high-dimensional data. While there has been muchinterest in sparse techniques, current methods do not generalize well to datawith nonlinear structure. In this work, we present an orthogonal seriesestimator for predictors that are complex aggregate objects, such as naturalimages, galaxy spectra, trajectories, and movies. Our series approach tiestogether ideas from kernel machine learning, and Fourier methods. We expand theunknown regression on the data in terms of the eigenfunctions of a kernel-basedoperator, and we take advantage of orthogonality of the basis with respect tothe underlying data distribution, P, to speed up computations and tuning ofparameters. If the kernel is appropriately chosen, then the eigenfunctionsadapt to the intrinsic geometry and dimension of the data. We providetheoretical guarantees for a radial kernel with varying bandwidth, and werelate smoothness of the regression function with respect to P to sparsity inthe eigenbasis. Finally, using simulated and real-world data, we systematicallycompare the performance of the spectral series approach with classical kernelsmoothing, k-nearest neighbors regression, kernel ridge regression, andstate-of-the-art manifold and local regression methods.
arxiv-15000-122 | Adaptive Subgradient Methods for Online AUC Maximization | http://arxiv.org/pdf/1602.00351v1.pdf | author:Yi Ding, Peilin Zhao, Steven C. H. Hoi, Yew-Soon Ong category:cs.LG published:2016-02-01 summary:Learning for maximizing AUC performance is an important research problem inMachine Learning and Artificial Intelligence. Unlike traditional batch learningmethods for maximizing AUC which often suffer from poor scalability, recentyears have witnessed some emerging studies that attempt to maximize AUC bysingle-pass online learning approaches. Despite their encouraging resultsreported, the existing online AUC maximization algorithms often adopt simpleonline gradient descent approaches that fail to exploit the geometricalknowledge of the data observed during the online learning process, and thuscould suffer from relatively larger regret. To address the above limitation, inthis work, we explore a novel algorithm of Adaptive Online AUC Maximization(AdaOAM) which employs an adaptive gradient method that exploits the knowledgeof historical gradients to perform more informative online learning. The newadaptive updating strategy of the AdaOAM is less sensitive to the parametersettings and maintains the same time complexity as previous non-adaptivecounterparts. Additionally, we extend the algorithm to handle high-dimensionalsparse data (SAdaOAM) and address sparsity in the solution by performing lazygradient updating. We analyze the theoretical bounds and evaluate theirempirical performance on various types of data sets. The encouraging empiricalresults obtained clearly highlighted the effectiveness and efficiency of theproposed algorithms.
arxiv-15000-123 | Statistically efficient thinning of a Markov chain sampler | http://arxiv.org/pdf/1510.07727v4.pdf | author:Art B. Owen category:stat.CO cs.LG stat.ML 65C40, 62M05 published:2015-10-27 summary:It is common to subsample Markov chain samples to reduce the storage burdenof the output. It is also well known that discarding $k-1$ out of every $k$observations will not improve statistical efficiency. It is less frequentlyremarked that subsampling a Markov chain allows one to omit some of thecomputation beyond that needed to simply advance the chain. When this reducedcomputation is accounted for, thinning the Markov chain by subsampling it canimprove statistical efficiency. Given an autocorrelation parameter $\rho$ and acost ratio $\theta$, this paper shows how to compute the most efficientsubsampling frequency $k$. The optimal $k$ grows rapidly as $\rho$ increasestowards $1$. The resulting efficiency gain depends primarily on $\theta$, not$\rho$. Taking $k=1$ (no thinning) is optimal when $\rho\le0$. For $\rho>0$ itis optimal if and only if $\theta \le (1-\rho)^2/(2\rho)$. The efficiency gainnever exceeds $1+\theta$. The derivations are exact for an AR(1)autocorrelation which is often a good approximation to the autocorrelations onesees in practice.
arxiv-15000-124 | Marginalization and Conditioning for LWF Chain Graphs | http://arxiv.org/pdf/1405.7129v3.pdf | author:Kayvan Sadeghi category:stat.OT math.ST stat.ML stat.TH published:2014-05-28 summary:In this paper, we deal with the problem of marginalization over andconditioning on two disjoint subsets of the node set of chain graphs (CGs) withthe LWF Markov property. For this purpose, we define the class of chain mixedgraphs (CMGs) with three types of edges and, for this class, provide aseparation criterion under which the class of CMGs is stable undermarginalization and conditioning and contains the class of LWF CGs as itssubclass. We provide a method for generating such graphs after marginalizationand conditioning for a given CMG or a given LWF CG. We then define and studythe class of anterial graphs, which is also stable under marginalization andconditioning and contains LWF CGs, but has a simpler structure than CMGs.
arxiv-15000-125 | Novel Views of Objects from a Single Image | http://arxiv.org/pdf/1602.00328v1.pdf | author:Konstantinos Rematas, Chuong Nguyen, Tobias Ritschel, Mario Fritz, Tinne Tuytelaars category:cs.CV cs.GR published:2016-01-31 summary:Taking an image of an object is at its core a lossy process. The richinformation about the three-dimensional structure of the world is flattened toan image plane and decisions such as viewpoint and camera parameters are finaland not easily revertible. As a consequence, possibilities of changingviewpoint are limited. Given a single image depicting an object, novel-viewsynthesis is the task of generating new images that render the object from adifferent viewpoint than the one given. The main difficulty is to synthesizethe parts that are disoccluded; disocclusion occurs when parts of an object arehidden by the object itself under a specific viewpoint. In this work, we showhow to improve novel-view synthesis by making use of the correlations observedin 3D models and applying them to new image instances. We propose a techniqueto use the structural information extracted from a 3D model that matches theimage object in terms of viewpoint and shape. For the latter part, we proposean efficient 2D-to-3D alignment method that associates precisely the imageappearance with the 3D model geometry with minimal user interaction. Ourtechnique is able to simulate plausible viewpoint changes for a variety ofobject classes within seconds. Additionally, we show that our synthesizedimages can be used as additional training data that improves the performance ofstandard object detectors.
arxiv-15000-126 | Bandits meet Computer Architecture: Designing a Smartly-allocated Cache | http://arxiv.org/pdf/1602.00309v1.pdf | author:Yonatan Glassner, Koby Crammer category:cs.LG published:2016-01-31 summary:In many embedded systems, such as imaging sys- tems, the system has a singledesignated purpose, and same threads are executed repeatedly. Profiling threadbehavior, allows the system to allocate each thread its resources in a way thatimproves overall system performance. We study an online resource al-locationproblem,wherearesourcemanagersimulta- neously allocates resources(exploration), learns the impact on the different consumers (learning) and im-proves allocation towards optimal performance (ex- ploitation). We build on therich framework of multi- armed bandits and present online and offline algo-rithms. Through extensive experiments with both synthetic data and real-worldcache allocation to threads we show the merits and properties of our al-gorithms
arxiv-15000-127 | Bit-Planes: Dense Subpixel Alignment of Binary Descriptors | http://arxiv.org/pdf/1602.00307v1.pdf | author:Hatem Alismail, Brett Browning, Simon Lucey category:cs.CV published:2016-01-31 summary:Binary descriptors have been instrumental in the recent evolution ofcomputationally efficient sparse image alignment algorithms. Increasingly,however, the vision community is interested in dense image alignment methods,which are more suitable for estimating correspondences from high frame ratecameras as they do not rely on exhaustive search. However, classic densealignment approaches are sensitive to illumination change. In this paper, wepropose an easy to implement and low complexity dense binary descriptor, whichwe refer to as bit-planes, that can be seamlessly integrated within amulti-channel Lucas & Kanade framework. This novel approach combines therobustness of binary descriptors with the speed and accuracy of dense alignmentmethods. The approach is demonstrated on a template tracking problem achievingstate-of-the-art robustness and faster than real-time performance on consumerlaptops (400+ fps on a single core Intel i7) and hand-held mobile devices (100+fps on an iPad Air 2).
arxiv-15000-128 | Hough-CNN: Deep Learning for Segmentation of Deep Brain Regions in MRI and Ultrasound | http://arxiv.org/pdf/1601.07014v3.pdf | author:Fausto Milletari, Seyed-Ahmad Ahmadi, Christine Kroll, Annika Plate, Verena Rozanski, Juliana Maiostre, Johannes Levin, Olaf Dietrich, Birgit Ertl-Wagner, Kai Bötzel, Nassir Navab category:cs.CV published:2016-01-26 summary:In this work we propose a novel approach to perform segmentation byleveraging the abstraction capabilities of convolutional neural networks(CNNs). Our method is based on Hough voting, a strategy that allows for fullyautomatic localisation and segmentation of the anatomies of interest. Thisapproach does not only use the CNN classification outcomes, but it alsoimplements voting by exploiting the features produced by the deepest portion ofthe network. We show that this learning-based segmentation method is robust,multi-region, flexible and can be easily adapted to different modalities. Inthe attempt to show the capabilities and the behaviour of CNNs when they areapplied to medical image analysis, we perform a systematic study of theperformances of six different network architectures, conceived according tostate-of-the-art criteria, in various situations. We evaluate the impact ofboth different amount of training data and different data dimensionality (2D,2.5D and 3D) on the final results. We show results on both MRI and transcranialUS volumes depicting respectively 26 regions of the basal ganglia and themidbrain.
arxiv-15000-129 | WASSUP? LOL : Characterizing Out-of-Vocabulary Words in Twitter | http://arxiv.org/pdf/1602.00293v1.pdf | author:Suman Kalyan Maity, Chaitanya Sarda, Anshit Chaudhary, Abhijeet Patil, Shraman Kumar, Akash Mondal, Animesh Mukherjee category:cs.CL cs.SI published:2016-01-31 summary:Language in social media is mostly driven by new words and spellings that areconstantly entering the lexicon thereby polluting it and resulting in highdeviation from the formal written version. The primary entities of suchlanguage are the out-of-vocabulary (OOV) words. In this paper, we study varioussociolinguistic properties of the OOV words and propose a classification modelto categorize them into at least six categories. We achieve 81.26% accuracywith high precision and recall. We observe that the content features are themost discriminative ones followed by lexical and context features.
arxiv-15000-130 | DOLDA - a regularized supervised topic model for high-dimensional multi-class regression | http://arxiv.org/pdf/1602.00260v1.pdf | author:Måns Magnusson, Leif Jonsson, Mattias Villani category:stat.ML published:2016-01-31 summary:We introduce Diagonal Orthant Latent Dirichlet Allocation (DOLDA), asupervised topic model for multi-class classification that can handle both manyclasses as well as many covariates. To handle many classes we use the recentlyproposed Diagonal Orthant (DO) probit model together with an efficienthorseshoe prior for variable selection/shrinkage. An important advantage ofDOLDA is that learned topics are directly connected to individual classeswithout the need for a reference class. We propose a computationally efficientparallel Gibbs sampler for the new model. We study the model properties on anIMDb dataset with roughly 8000 documents, and document preliminary results in abug prediction context where 118 components are predicted using 100 topics frombug reports.
arxiv-15000-131 | Nonlinearities and Adaptation of Color Vision from Sequential Principal Curves Analysis | http://arxiv.org/pdf/1602.00236v1.pdf | author:Valero Laparra, Sandra Jiménez, Gustavo Camps-Valls, Jesús Malo category:stat.ML q-bio.NC published:2016-01-31 summary:Mechanisms of human color vision are characterized by two phenomenologicalaspects: the system is nonlinear and adaptive to changing environments.Conventional attempts to derive these features from statistics use separatearguments for each aspect. The few statistical approaches that do consider bothphenomena simultaneously follow parametric formulations based on empiricalmodels. Therefore, it may be argued that the behavior does not come directlyfrom the color statistics but from the convenient functional form adopted. Inaddition, many times the whole statistical analysis is based on simplifieddatabases that disregard relevant physical effects in the input signal, as forinstance by assuming flat Lambertian surfaces. Here we address the simultaneousstatistical explanation of (i) the nonlinear behavior of achromatic andchromatic mechanisms in a fixed adaptation state, and (ii) the change of suchbehavior. Both phenomena emerge directly from the samples through a singledata-driven method: the Sequential Principal Curves Analysis (SPCA) with localmetric. SPCA is a new manifold learning technique to derive a set of sensorsadapted to the manifold using different optimality criteria. A new database ofcolorimetrically calibrated images of natural objects under these illuminantswas collected. The results obtained by applying SPCA show that thepsychophysical behavior on color discrimination thresholds, discount of theilluminant and corresponding pairs in asymmetric color matching, emergedirectly from realistic data regularities assuming no a priori functional form.These results provide stronger evidence for the hypothesis of a statisticallydriven organization of color sensors. Moreover, the obtained results suggestthat color perception at this low abstraction level may be guided by an errorminimization strategy rather than by the information maximization principle.
arxiv-15000-132 | Iterative Gaussianization: from ICA to Random Rotations | http://arxiv.org/pdf/1602.00229v1.pdf | author:Valero Laparra, Gustavo Camps-Valls, Jesús Malo category:stat.ML published:2016-01-31 summary:Most signal processing problems involve the challenging task ofmultidimensional probability density function (PDF) estimation. In this work,we propose a solution to this problem by using a family of Rotation-basedIterative Gaussianization (RBIG) transforms. The general framework consists ofthe sequential application of a univariate marginal Gaussianization transformfollowed by an orthonormal transform. The proposed procedure looks fordifferentiable transforms to a known PDF so that the unknown PDF can beestimated at any point of the original domain. In particular, we aim at a zeromean unit covariance Gaussian for convenience. RBIG is formally similar toclassical iterative Projection Pursuit (PP) algorithms. However, we show that,unlike in PP methods, the particular class of rotations used has no specialqualitative relevance in this context, since looking for interestingness is nota critical issue for PDF estimation. The key difference is that our approachfocuses on the univariate part (marginal Gaussianization) of the problem ratherthan on the multivariate part (rotation). This difference implies that one mayselect the most convenient rotation suited to each practical application. Thedifferentiability, invertibility and convergence of RBIG are theoretically andexperimentally analyzed. Relation to other methods, such as RadialGaussianization (RG), one-class support vector domain description (SVDD), anddeep neural networks (DNN) is also pointed out. The practical performance ofRBIG is successfully illustrated in a number of multidimensional problems suchas image synthesis, classification, denoising, and multi-informationestimation.
arxiv-15000-133 | Order-aware Convolutional Pooling for Video Based Action Recognition | http://arxiv.org/pdf/1602.00224v1.pdf | author:Peng Wang, Lingqiao Liu, Chunhua Shen, Heng Tao Shen category:cs.CV published:2016-01-31 summary:Most video based action recognition approaches create the video-levelrepresentation by temporally pooling the features extracted at each frame. Thepooling methods that they adopt, however, usually completely or partiallyneglect the dynamic information contained in the temporal domain, which mayundermine the discriminative power of the resulting video representation sincethe video sequence order could unveil the evolution of a specific event oraction. To overcome this drawback and explore the importance of incorporatingthe temporal order information, in this paper we propose a novel temporalpooling approach to aggregate the frame-level features. Inspired by thecapacity of Convolutional Neural Networks (CNN) in making use of the internalstructure of images for information abstraction, we propose to apply thetemporal convolution operation to the frame-level representations to extractthe dynamic information. However, directly implementing this idea on theoriginal high-dimensional feature would inevitably result in parameterexplosion. To tackle this problem, we view the temporal evolution of the feature valueat each feature dimension as a 1D signal and learn a unique convolutionalfilter bank for each of these 1D signals. We conduct experiments on twochallenging video-based action recognition datasets, HMDB51 and UCF101; anddemonstrate that the proposed method is superior to the conventional poolingmethods.
arxiv-15000-134 | Variance-Reduced Second-Order Methods | http://arxiv.org/pdf/1602.00223v1.pdf | author:Luo Luo, Zihao Chen, Zhihua Zhang, Wu-Jun Li category:cs.LG stat.ML published:2016-01-31 summary:In this paper, we discuss the problem of minimizing the sum of two convexfunctions: a smooth function plus a non-smooth function. Further, the smoothpart can be expressed by the average of a large number of smooth componentfunctions, and the non-smooth part is equipped with a simple proximal mapping.We propose a proximal stochastic second-order method, which is efficient andscalable. It incorporates the Hessian in the smooth part of the function andexploits multistage scheme to reduce the variance of the stochastic gradient.We prove that our method can achieve linear rate of convergence.
arxiv-15000-135 | Principal Polynomial Analysis | http://arxiv.org/pdf/1602.00221v1.pdf | author:Valero Laparra, Sandra Jiménez, Devis Tuia, Gustau Camps-Valls, Jesús Malo category:stat.ML published:2016-01-31 summary:This paper presents a new framework for manifold learning based on a sequenceof principal polynomials that capture the possibly nonlinear nature of thedata. The proposed Principal Polynomial Analysis (PPA) generalizes PCA bymodeling the directions of maximal variance by means of curves, instead ofstraight lines. Contrarily to previous approaches, PPA reduces to performingsimple univariate regressions, which makes it computationally feasible androbust. Moreover, PPA shows a number of interesting analytical properties.First, PPA is a volume-preserving map, which in turn guarantees the existenceof the inverse. Second, such an inverse can be obtained in closed form.Invertibility is an important advantage over other learning methods, because itpermits to understand the identified features in the input domain where thedata has physical meaning. Moreover, it allows to evaluate the performance ofdimensionality reduction in sensible (input-domain) units. Volume preservationalso allows an easy computation of information theoretic quantities, such asthe reduction in multi-information after the transform. Third, the analyticalnature of PPA leads to a clear geometrical interpretation of the manifold: itallows the computation of Frenet-Serret frames (local features) and ofgeneralized curvatures at any point of the space. And fourth, the analyticalJacobian allows the computation of the metric induced by the data, thusgeneralizing the Mahalanobis distance. These properties are demonstratedtheoretically and illustrated experimentally. The performance of PPA isevaluated in dimensionality and redundancy reduction, in both synthetic andreal datasets from the UCI repository.
arxiv-15000-136 | Image Denoising with Kernels based on Natural Image Relations | http://arxiv.org/pdf/1602.00217v1.pdf | author:Valero Laparra, Juan Gutiérrez, Gustavo Camps-Valls, Jesús Malo category:cs.CV stat.ML published:2016-01-31 summary:A successful class of image denoising methods is based on Bayesian approachesworking in wavelet representations. However, analytical estimates can beobtained only for particular combinations of analytical models of signal andnoise, thus precluding its straightforward extension to deal with otherarbitrary noise sources. In this paper, we propose an alternative non-explicitway to take into account the relations among natural image wavelet coefficientsfor denoising: we use support vector regression (SVR) in the wavelet domain toenforce these relations in the estimated signal. Since relations among thecoefficients are specific to the signal, the regularization property of SVR isexploited to remove the noise, which does not share this feature. The specificsignal relations are encoded in an anisotropic kernel obtained from mutualinformation measures computed on a representative image database. Trainingconsiders minimizing the Kullback-Leibler divergence (KLD) between theestimated and actual probability functions of signal and noise in order toenforce similarity. Due to its non-parametric nature, the method can eventuallycope with different noise sources without the need of an explicitre-formulation, as it is strictly necessary under parametric Bayesianformalisms. Results under several noise levels and noise sources show that: (1)the proposed method outperforms conventional wavelet methods that assumecoefficient independence, (2) it is similar to state-of-the-art methods that doexplicitly include these relations when the noise source is Gaussian, and (3)it gives better numerical and visual performance when more complex, realisticnoise sources are considered. Therefore, the proposed machine learning approachcan be seen as a more flexible (model-free) alternative to the explicitdescription of wavelet coefficient relations for image denoising.
arxiv-15000-137 | Dimensionality Reduction via Regression in Hyperspectral Imagery | http://arxiv.org/pdf/1602.00214v1.pdf | author:Valero Laparra, Jesus Malo, Gustau Camps-Valls category:stat.ML cs.CV published:2016-01-31 summary:This paper introduces a new unsupervised method for dimensionality reductionvia regression (DRR). The algorithm belongs to the family of invertibletransforms that generalize Principal Component Analysis (PCA) by usingcurvilinear instead of linear features. DRR identifies the nonlinear featuresthrough multivariate regression to ensure the reduction in redundancy betweenhe PCA coefficients, the reduction of the variance of the scores, and thereduction in the reconstruction error. More importantly, unlike other nonlineardimensionality reduction methods, the invertibility, volume-preservation, andstraightforward out-of-sample extension, makes DRR interpretable and easy toapply. The properties of DRR enable learning a more broader class of datamanifolds than the recently proposed Non-linear Principal Components Analysis(NLPCA) and Principal Polynomial Analysis (PPA). We illustrate the performanceof the representation in reducing the dimensionality of remote sensing data. Inparticular, we tackle two common problems: processing very high dimensionalspectral information such as in hyperspectral image sounding data, and dealingwith spatial-spectral image patches of multispectral images. Both settings posecollinearity and ill-determination problems. Evaluation of the expressive powerof the features is assessed in terms of truncation error, estimatingatmospheric variables, and surface land cover classification error. Resultsshow that DRR outperforms linear PCA and recently proposed invertibleextensions based on neural networks (NLPCA) and univariate regressions (PPA).
arxiv-15000-138 | Unsupervised Deep Hashing for Large-scale Visual Search | http://arxiv.org/pdf/1602.00206v1.pdf | author:Zhaoqiang Xia, Xiaoyi Feng, Jinye Peng, Abdenour Hadid category:cs.CV cs.LG published:2016-01-31 summary:Learning based hashing plays a pivotal role in large-scale visual search.However, most existing hashing algorithms tend to learn shallow models that donot seek representative binary codes. In this paper, we propose a novel hashingapproach based on unsupervised deep learning to hierarchically transformfeatures into hash codes. Within the heterogeneous deep hashing framework, theautoencoder layers with specific constraints are considered to model thenonlinear mapping between features and binary codes. Then, a RestrictedBoltzmann Machine (RBM) layer with constraints is utilized to reduce thedimension in the hamming space. Extensive experiments on the problem of visualsearch demonstrate the competitiveness of our proposed approach compared tostate-of-the-art.
arxiv-15000-139 | Greedy Deep Dictionary Learning | http://arxiv.org/pdf/1602.00203v1.pdf | author:Snigdha Tariyal, Angshul Majumdar, Richa Singh, Mayank Vatsa category:cs.LG cs.AI stat.ML published:2016-01-31 summary:In this work we propose a new deep learning tool called deep dictionarylearning. Multi-level dictionaries are learnt in a greedy fashion, one layer ata time. This requires solving a simple (shallow) dictionary learning problem,the solution to this is well known. We apply the proposed technique on somebenchmark deep learning datasets. We compare our results with other deeplearning tools like stacked autoencoder and deep belief network; and state ofthe art supervised dictionary learning tools like discriminative KSVD and labelconsistent KSVD. Our method yields better results than all.
arxiv-15000-140 | Partial Sum Minimization of Singular Values Representation on Grassmann Manifolds | http://arxiv.org/pdf/1601.05613v2.pdf | author:Boyue Wang, Yongli Hu, Junbin Gao, Yanfeng Sun, Baocai Yin category:cs.CV published:2016-01-21 summary:As a significant subspace clustering method, low rank representation (LRR)has attracted great attention in recent years. To further improve theperformance of LRR and extend its applications, there are several issues to beresolved. The nuclear norm in LRR does not sufficiently use the prior knowledgeof the rank which is known in many practical problems. The LRR is designed forvectorial data from linear spaces, thus not suitable for high dimensional datawith intrinsic non-linear manifold structure. This paper proposes an extendedLRR model for manifold-valued Grassmann data which incorporates prior knowledgeby minimizing partial sum of singular values instead of the nuclear norm,namely Partial Sum minimization of Singular Values Representation (GPSSVR). Thenew model not only enforces the global structure of data in low rank, but alsoretains important information by minimizing only smaller singular values. Tofurther maintain the local structures among Grassmann points, we also integratethe Laplacian penalty with GPSSVR. An effective algorithm is proposed to solvethe optimization problem based on the GPSSVR model. The proposed model andalgorithms are assessed on some widely used human action video datasets and areal scenery dataset. The experimental results show that the proposed methodsobviously outperform other state-of-the-art methods.
arxiv-15000-141 | Tracing liquid level and material boundaries in transparent vessels using the graph cut computer vision approach | http://arxiv.org/pdf/1602.00177v1.pdf | author:Sagi Eppel category:cs.CV published:2016-01-31 summary:Detection of boundaries of materials stored in transparent vessels isessential for identifying properties such as liquid level and phase boundaries,which are vital for controlling numerous processes in the industry andchemistry laboratory. This work presents a computer vision method foridentifying the boundary of materials in transparent vessels using thegraph-cut algorithm. The method receives an image of a transparent vesselcontaining a material and the contour of the vessel in the image. The boundaryof the material in the vessel is found by the graph cut method. In general themethod uses the vessel region of the image to create a graph, where pixels arevertices, and the cost of an edge between two pixels is inversely correlatedwith their intensity difference. The bottom 10% of the vessel region in theimage is assumed to correspond to the material phase and defined as the graphand source. The top 10% of the pixels in the vessels are assumed to correspondto the air phase and defined as the graph sink. The minimal cut that splits theresulting graph between the source and sink (hence, material and air) is tracedusing the max-flow/min-cut approach. This cut corresponds to the boundary ofthe material in the image. The method gave high accuracy in boundaryrecognition for a wide range of liquid, solid, granular and powder materials invarious glass vessels from everyday life and the chemistry laboratory, such asbottles, jars, Glasses, Chromotography colums and separatory funnels.
arxiv-15000-142 | Deep Learning For Smile Recognition | http://arxiv.org/pdf/1602.00172v1.pdf | author:Patrick O. Glauner category:cs.CV cs.LG cs.NE published:2016-01-30 summary:Inspired by recent successes of deep learning in computer vision, we proposea novel application of deep convolutional neural networks to facial expressionrecognition, in particular smile recognition. A smile recognition test accuracyof 99.45% is achieved for the Denver Intensity of Spontaneous Facial Action(DISFA) database, significantly outperforming existing approaches based onhand-crafted features with accuracies ranging from 65.55% to 79.67%. Thenovelty of this approach includes a comprehensive model selection of thearchitecture parameters, allowing to find an appropriate architecture for eachexpression such as smile. This is feasible because all experiments were run ona Tesla K40c GPU, allowing a speedup of factor 10 over traditional computationson a CPU.
arxiv-15000-143 | word2vec Parameter Learning Explained | http://arxiv.org/pdf/1411.2738v3.pdf | author:Xin Rong category:cs.CL published:2014-11-11 summary:The word2vec model and application by Mikolov et al. have attracted a greatamount of attention in recent two years. The vector representations of wordslearned by word2vec models have been shown to carry semantic meanings and areuseful in various NLP tasks. As an increasing number of researchers would liketo experiment with word2vec or similar techniques, I notice that there lacks amaterial that comprehensively explains the parameter learning process of wordembedding models in details, thus preventing researchers that are non-expertsin neural networks from understanding the working mechanism of such models. This note provides detailed derivations and explanations of the parameterupdate equations of the word2vec models, including the original continuousbag-of-word (CBOW) and skip-gram (SG) models, as well as advanced optimizationtechniques, including hierarchical softmax and negative sampling. Intuitiveinterpretations of the gradient equations are also provided alongsidemathematical derivations. In the appendix, a review on the basics of neuron networks andbackpropagation is provided. I also created an interactive demo, wevi, tofacilitate the intuitive understanding of the model.
arxiv-15000-144 | A multiple instance learning approach for sequence data with across bag dependencies | http://arxiv.org/pdf/1602.00163v1.pdf | author:Manel Zoghlami, Sabeur Aridhi, Haitham Sghaier, Mondher Maddouri, Engelbert Mephu Nguifo category:cs.LG published:2016-01-30 summary:In Multiple Instance Learning (MIL) problem for sequence data, the learningdata consist of a set of bags where each bag contains a set ofinstances/sequences. In many real world applications such as bioinformatics,web mining, and text mining, comparing a random couple of sequences makes nosense. In fact, each instance of each bag may have structural and/or temporalrelation with other instances in other bags. Thus, the classification taskshould take into account the relation between semantically related instancesacross bags. In this paper, we present two novel MIL approaches for sequencedata classification: (1) ABClass and (2) ABSim. In ABClass, each sequence isrepresented by one vector of attributes. For each sequence of the unknown bag,a discriminative classifier is applied in order to compute a partialclassification result. Then, an aggregation method is applied to these partialresults in order to generate the final result. In ABSim, we use a similaritymeasure between each sequence of the unknown bag and the correspondingsequences in the learning bags. An unknown bag is labeled with the bag thatpresents more similar sequences. We applied both approaches to the problem ofbacterial Ionizing Radiation Resistance (IRR) prediction. We evaluated anddiscussed the proposed approaches on well known Ionizing Radiation ResistanceBacteria (IRRB) and Ionizing Radiation Sensitive Bacteria (IRSB) represented byprimary structure of basal DNA repair proteins. The experimental results showthat both ABClass and ABSim approaches are efficient.
arxiv-15000-145 | A Unified Approach for Learning the Parameters of Sum-Product Networks | http://arxiv.org/pdf/1601.00318v3.pdf | author:Han Zhao, Pascal Poupart category:cs.LG cs.AI published:2016-01-03 summary:We present a unified approach for learning the parameters of Sum-Productnetworks (SPNs). We prove that any complete and decomposable SPN is equivalentto a mixture of trees where each tree corresponds to a product of univariatedistributions. Based on the mixture model perspective, we characterize theobjective function when learning SPNs based on the maximum likelihoodestimation (MLE) principle and show that the optimization problem can beformulated as a signomial program. Both the projected gradient descent (PGD)and the exponentiated gradient (EG) in this setting can be viewed as firstorder approximations of the signomial program after proper transformation ofthe objective function. Based on the signomial program formulation, weconstruct two parameter learning algorithms for SPNs by using sequentialmonomial approximations (SMA) and the concave-convex procedure (CCCP),respectively. The two proposed methods naturally admit multiplicative updates,hence effectively avoiding the projection operation. With the help of theunified framework, we also show that, in the case of SPNs, CCCP leads to thesame algorithm as Expectation Maximization (EM) despite the fact that they aredifferent in general. Extensive experiments on 20 data sets demonstrate theeffectiveness and efficiency of the two proposed approaches for learning SPNs.We also show that the proposed methods can improve the performance of structurelearning and yield state-of-the-art results.
arxiv-15000-146 | DNA-inspired online behavioral modeling and its application to spambot detection | http://arxiv.org/pdf/1602.00110v1.pdf | author:Stefano Cresci, Roberto Di Pietro, Marinella Petrocchi, Angelo Spognardi, Maurizio Tesconi category:cs.SI cs.CR cs.LG H.2.8.d; I.2.4 published:2016-01-30 summary:We propose a strikingly novel, simple, and effective approach to model onlineuser behavior: we extract and analyze digital DNA sequences from user onlineactions and we use Twitter as a benchmark to test our proposal. We obtain anincisive and compact DNA-inspired characterization of user actions. Then, weapply standard DNA analysis techniques to discriminate between genuine andspambot accounts on Twitter. An experimental campaign supports our proposal,showing its effectiveness and viability. To the best of our knowledge, we arethe first ones to identify and adapt DNA-inspired techniques to online userbehavioral modeling. While Twitter spambot detection is a specific use case ona specific social media, our proposed methodology is platform and technologyagnostic, hence paving the way for diverse behavioral characterization tasks.
arxiv-15000-147 | Extracting Keyword for Disambiguating Name Based on the Overlap Principle | http://arxiv.org/pdf/1602.00104v1.pdf | author:Mahyuddin K. M. Nasution category:cs.IR cs.CL F.2.2; I.2.7 published:2016-01-30 summary:Name disambiguation has become one of the main themes in the Semantic Webagenda. The semantic web is an extension of the current Web in whichinformation is not only given well-defined meaning, but also has many purposesthat contain the ambiguous naturally or a lot of thing came with the overlap,mainly deals with the persons name. Therefore, we develop an approach toextract keywords from web snippet with utilizing the overlap principle, aconcept to understand things with ambiguous, whereby features of person aregenerated for dealing with the variety of web, the web is steadily gainingground in the semantic research.
arxiv-15000-148 | Latent common manifold learning with alternating diffusion: analysis and applications | http://arxiv.org/pdf/1602.00078v1.pdf | author:Ronen Talmon, Hau-tieng Wu category:cs.DS math.NA stat.ML published:2016-01-30 summary:The analysis of data sets arising from multiple sensors has drawn significantresearch attention over the years. Traditional methods, including kernel-basedmethods, are typically incapable of capturing nonlinear geometric structures.We introduce a latent common manifold model underlying multiple sensorobservations for the purpose of multimodal data fusion. A method based onalternating diffusion is presented and analyzed; we provide theoreticalanalysis of the method under the latent common manifold model. To exemplify thepower of the proposed framework, experimental results in several applicationsare reported.
arxiv-15000-149 | Spectrum Estimation from Samples | http://arxiv.org/pdf/1602.00061v1.pdf | author:Weihao Kong, Gregory Valiant category:cs.LG stat.ML published:2016-01-30 summary:We consider the problem of approximating the set of eigenvalues of thecovariance matrix of a multivariate distribution (equivalently, the problem ofapproximating the "population spectrum"), given access to samples drawn fromthe distribution. The eigenvalues of the covariance of a distribution containbasic information about the distribution, including the presence or lack ofstructure in the distribution, the effective dimensionality of thedistribution, and the applicability of higher-level machine learning andmultivariate statistical tools. We consider this fundamental recovery problemin the regime where the number of samples is comparable, or even sublinear inthe dimensionality of the distribution in question. First, we propose atheoretically optimal and computationally efficient algorithm for recoveringthe moments of the eigenvalues---the \emph{Schatten} $p$-norms of thepopulation covariance matrix. We then leverage this accurate moment recovery,via an earthmover argument, to show that the vector of eigenvalues can beaccurately recovered. In addition to our theoretical results, we show that ourapproach performs well in practice for a broad range of distributions andsample sizes.
arxiv-15000-150 | Deep convolutional networks for automated detection of posterior-element fractures on spine CT | http://arxiv.org/pdf/1602.00020v1.pdf | author:Holger R. Roth, Yinong Wang, Jianhua Yao, Le Lu, Joseph E. Burns, Ronald M. Summers category:cs.CV published:2016-01-29 summary:Injuries of the spine, and its posterior elements in particular, are a commonoccurrence in trauma patients, with potentially devastating consequences.Computer-aided detection (CADe) could assist in the detection andclassification of spine fractures. Furthermore, CAD could help assess thestability and chronicity of fractures, as well as facilitate research intooptimization of treatment paradigms. In this work, we apply deep convolutional networks (ConvNets) for theautomated detection of posterior element fractures of the spine. First, thevertebra bodies of the spine with its posterior elements are segmented in spineCT using multi-atlas label fusion. Then, edge maps of the posterior elementsare computed. These edge maps serve as candidate regions for predicting a setof probabilities for fractures along the image edges using ConvNets in a 2.5Dfashion (three orthogonal patches in axial, coronal and sagittal planes). Weexplore three different methods for training the ConvNet using 2.5D patchesalong the edge maps of 'positive', i.e. fractured posterior-elements and'negative', i.e. non-fractured elements. An experienced radiologist retrospectively marked the location of 55displaced posterior-element fractures in 18 trauma patients. We randomly splitthe data into training and testing cases. In testing, we achieve anarea-under-the-curve of 0.857. This corresponds to 71% or 81% sensitivities at5 or 10 false-positives per patient, respectively. Analysis of our set oftrauma patients demonstrates the feasibility of detecting posterior-elementfractures in spine CT images using computer vision techniques such as deepconvolutional networks.
arxiv-15000-151 | High Dimensional Data Modeling Techniques for Detection of Chemical Plumes and Anomalies in Hyperspectral Images and Movies | http://arxiv.org/pdf/1509.07497v2.pdf | author:Yi, Wang, Guangliang Chen, Mauro Maggioni category:stat.ML published:2015-09-24 summary:We briefly review recent progress in techniques for modeling and analyzinghyperspectral images and movies, in particular for detecting plumes of bothknown and unknown chemicals. For detecting chemicals of known spectrum, weextend the technique of using a single subspace for modeling the background toa "mixture of subspaces" model to tackle more complicated background.Furthermore, we use partial least squares regression on a resampled trainingset to boost performance. For the detection of unknown chemicals we view theproblem as an anomaly detection problem, and use novel estimators withlow-sampled complexity for intrinsically low-dimensional data inhigh-dimensions that enable us to model the "normal" spectra and detectanomalies. We apply these algorithms to benchmark data sets made available bythe Automated Target Detection program co-funded by NSF, DTRA and NGA, andcompare, when applicable, to current state-of-the-art algorithms, withfavorable results.
arxiv-15000-152 | Probabilistic Bag-Of-Hyperlinks Model for Entity Linking | http://arxiv.org/pdf/1509.02301v3.pdf | author:Octavian-Eugen Ganea, Marina Ganea, Aurelien Lucchi, Carsten Eickhoff, Thomas Hofmann category:cs.CL published:2015-09-08 summary:Many fundamental problems in natural language processing rely on determiningwhat entities appear in a given text. Commonly referenced as entity linking,this step is a fundamental component of many NLP tasks such as textunderstanding, automatic summarization, semantic search or machine translation.Name ambiguity, word polysemy, context dependencies and a heavy-taileddistribution of entities contribute to the complexity of this problem. We here propose a probabilistic approach that makes use of an effectivegraphical model to perform collective entity disambiguation. Input mentions(i.e.,~linkable token spans) are disambiguated jointly across an entiredocument by combining a document-level prior of entity co-occurrences withlocal information captured from mentions and their surrounding context. Themodel is based on simple sufficient statistics extracted from data, thusrelying on few parameters to be learned. Our method does not require extensive feature engineering, nor an expensivetraining procedure. We use loopy belief propagation to perform approximateinference. The low complexity of our model makes this step sufficiently fastfor real-time usage. We demonstrate the accuracy of our approach on a widerange of benchmark datasets, showing that it matches, and in many casesoutperforms, existing state-of-the-art methods.
arxiv-15000-153 | Spectrally Grouped Total Variation Reconstruction for Scatter Imaging Using ADMM | http://arxiv.org/pdf/1601.08201v1.pdf | author:Ikenna Odinaka, Yan Kaganovsky, Joel A. Greenberg, Mehadi Hassan, David G. Politte, Joseph A. O'Sullivan, Lawrence Carin, David J. Brady category:math.NA cs.CV published:2016-01-29 summary:We consider X-ray coherent scatter imaging, where the goal is to reconstructmomentum transfer profiles (spectral distributions) at each spatial locationfrom multiplexed measurements of scatter. Each material is characterized by aunique momentum transfer profile (MTP) which can be used to discriminatebetween different materials. We propose an iterative image reconstructionalgorithm based on a Poisson noise model that can account for photon-limitedmeasurements as well as various second order statistics of the data. To improveimage quality, previous approaches use edge-preserving regularizers to promotepiecewise constancy of the image in the spatial domain while treating eachspectral bin separately. Instead, we propose spectrally grouped regularizationthat promotes piecewise constant images along the spatial directions but alsoensures that the MTPs of neighboring spatial bins are similar, if they containthe same material. We demonstrate that this group regularization results inimprovement of both spectral and spatial image quality. We pursue anoptimization transfer approach where convex decompositions are used to lift theproblem such that all hyper-voxels can be updated in parallel and inclosed-form. The group penalty introduces a challenge since it is not directlyamendable to these decompositions. We use the alternating directions method ofmultipliers (ADMM) to replace the original problem with an equivalent sequenceof sub-problems that are amendable to convex decompositions, leading to ahighly parallel algorithm. We demonstrate the performance on real data.
arxiv-15000-154 | Lipreading with Long Short-Term Memory | http://arxiv.org/pdf/1601.08188v1.pdf | author:Michael Wand, Jan Koutník, Jürgen Schmidhuber category:cs.CV cs.CL published:2016-01-29 summary:Lipreading, i.e. speech recognition from visual-only recordings of aspeaker's face, can be achieved with a processing pipeline based solely onneural networks, yielding significantly better accuracy than conventionalmethods. Feed-forward and recurrent neural network layers (namely LongShort-Term Memory; LSTM) are stacked to form a single structure which istrained by back-propagating error gradients through all the layers. Theperformance of such a stacked network was experimentally evaluated and comparedto a standard Support Vector Machine classifier using conventional computervision features (Eigenlips and Histograms of Oriented Gradients). Theevaluation was performed on data from 19 speakers of the publicly availableGRID corpus. With 51 different words to classify, we report a best wordaccuracy on held-out evaluation speakers of 79.6% using the end-to-end neuralnetwork-based solution (11.6% improvement over the best feature-based solutionevaluated).
arxiv-15000-155 | Joint System and Algorithm Design for Computationally Efficient Fan Beam Coded Aperture X-ray Coherent Scatter Imaging | http://arxiv.org/pdf/1603.06400v1.pdf | author:Ikenna Odinaka, Joseph A. O'Sullivan, David G. Politte, Kenneth P. MacCabe, Yan Kaganovsky, Joel A. Greenberg, Manu Lakshmanan, Kalyani Krishnamurthy, Anuj Kapadia, Lawrence Carin, David J. Brady category:cs.CV stat.ME published:2016-01-29 summary:In x-ray coherent scatter tomography, tomographic measurements of the forwardscatter distribution are used to infer scatter densities within a volume. Aradiopaque 2D pattern placed between the object and the detector array enablesthe disambiguation between different scatter events. The use of a fan beamsource illumination to speed up data acquisition relative to a pencil beampresents computational challenges. To facilitate the use of iterativealgorithms based on a penalized Poisson log-likelihood function, efficientcomputational implementation of the forward and backward models are needed. Ourproposed implementation exploits physical symmetries and structural propertiesof the system and suggests a joint system-algorithm design, where the systemdesign choices are influenced by computational considerations, and in turn leadto reduced reconstruction time. Computational-time speedups of approximately146 and 32 are achieved in the computation of the forward and backward models,respectively. Results validating the forward model and reconstruction algorithmare presented on simulated analytic and Monte Carlo data.
arxiv-15000-156 | Kernels for sequentially ordered data | http://arxiv.org/pdf/1601.08169v1.pdf | author:Franz J Király, Harald Oberhauser category:stat.ML cs.DM cs.LG math.ST stat.ME stat.TH published:2016-01-29 summary:We present a novel framework for kernel learning with sequential data of anykind, such as time series, sequences of graphs, or strings. Our approach isbased on signature features which can be seen as an ordered variant of sample(cross-)moments; it allows to obtain a "sequentialized" version of any statickernel. The sequential kernels are efficiently computable for discretesequences and are shown to approximate a continuous moment form in a samplingsense. A number of known kernels for sequences arise as "sequentializations" ofsuitable static kernels: string kernels may be obtained as a special case, andalignment kernels are closely related up to a modification that resolves theiropen non-definiteness issue. Our experiments indicate that our signature-basedsequential kernel framework may be a promising approach to learning withsequential data, such as time series, that allows to avoid extensive manualpre-processing.
arxiv-15000-157 | Mapping Tractography Across Subjects | http://arxiv.org/pdf/1601.08165v1.pdf | author:Thien Bao Nguyen, Emanuele Olivetti, Paolo Avesani category:stat.ML cs.CV q-bio.NC published:2016-01-29 summary:Diffusion magnetic resonance imaging (dMRI) and tractography provide means tostudy the anatomical structures within the white matter of the brain. Whenstudying tractography data across subjects, it is usually necessary to align,i.e. to register, tractographies together. This registration step is most oftenperformed by applying the transformation resulting from the registration ofother volumetric images (T1, FA). In contrast with registration methods that"transform" tractographies, in this work, we try to find which streamline inone tractography correspond to which streamline in the other tractography,without any transformation. In other words, we try to find a "mapping" betweenthe tractographies. We propose a graph-based solution for the tractographymapping problem and we explain similarities and differences with the relatedwell-known graph matching problem. Specifically, we define a loss functionbased on the pairwise streamline distance and reformulate the mapping problemas combinatorial optimization of that loss function. We show preliminarypromising results where we compare the proposed method, implemented withsimulated annealing, against a standard registration techniques in a task ofsegmentation of the corticospinal tract.
arxiv-15000-158 | Quantum perceptron over a field and neural network architecture selection in a quantum computer | http://arxiv.org/pdf/1602.00709v1.pdf | author:Adenilton J. da Silva, Teresa B. Ludermir, Wilson R. de Oliveira category:quant-ph cs.NE published:2016-01-29 summary:In this work, we propose a quantum neural network named quantum perceptronover a field (QPF). Quantum computers are not yet a reality and the models andalgorithms proposed in this work cannot be simulated in actual (or classical)computers. QPF is a direct generalization of a classical perceptron and solvessome drawbacks found in previous models of quantum perceptrons. We also presenta learning algorithm named Superposition based Architecture Learning algorithm(SAL) that optimizes the neural network weights and architectures. SAL searchesfor the best architecture in a finite set of neural network architectures withlinear time over the number of patterns in the training set. SAL is the firstlearning algorithm to determine neural network architectures in polynomialtime. This speedup is obtained by the use of quantum parallelism and anon-linear quantum operator.
arxiv-15000-159 | Online Sparse Gaussian Process Training with Input Noise | http://arxiv.org/pdf/1601.08068v1.pdf | author:Hildo Bijl, Thomas B. Schön, Jan-Willem van Wingerden, Michel Verhaegen category:stat.ML cs.LG cs.SY published:2016-01-29 summary:Gaussian process regression traditionally has three important downsides. (1)It is computationally intensive, (2) it cannot efficiently implement newlyobtained measurements online, and (3) it cannot deal with stochastic (noisy)input points. In this paper we present an algorithm tackling all these threeissues simultaneously. The resulting Sparse Online Noisy Input GP (SONIG)regression algorithm can incorporate new measurements in constant runtime. Acomparison has shown that it is more accurate than similar existing regressionalgorithms. In addition, the algorithm can be applied to non-linear black-boxsystem modeling, where its performance is competitive with non-linear ARXmodels.
arxiv-15000-160 | On the Geometric Ergodicity of Hamiltonian Monte Carlo | http://arxiv.org/pdf/1601.08057v1.pdf | author:Samuel Livingstone, Michael Betancourt, Simon Byrne, Mark Girolami category:stat.CO stat.ME stat.ML published:2016-01-29 summary:We establish general conditions under under which Markov chains produced bythe Hamiltonian Monte Carlo method will and will not be $\pi$-irreducible andgeometrically ergodic. We consider implementations with both fixed and dynamicintegration times. In the fixed case we find that the conditions for geometricergodicity are essentially a non-vanishing gradient of the log-density whichasymptotically points towards the centre of the space and does not grow fasterthan linearly. In an idealised scenario in which the integration time isallowed to change in different regions of the space, we show that geometricergodicity can be recovered for a much broader class of target distributions,leading to some guidelines for the choice of this free parameter in practice.
arxiv-15000-161 | A Robust UCB Scheme for Active Learning in Regression from Strategic Crowds | http://arxiv.org/pdf/1601.06750v2.pdf | author:Divya Padmanabhan, Satyanath Bhat, Dinesh Garg, Shirish Shevade, Y. Narahari category:cs.LG stat.ML published:2016-01-25 summary:We study the problem of training an accurate linear regression model byprocuring labels from multiple noisy crowd annotators, under a budgetconstraint. We propose a Bayesian model for linear regression in crowdsourcingand use variational inference for parameter estimation. To minimize the numberof labels crowdsourced from the annotators, we adopt an active learningapproach. In this specific context, we prove the equivalence of well-studiedcriteria of active learning like entropy minimization and expected errorreduction. Interestingly, we observe that we can decouple the problems ofidentifying an optimal unlabeled instance and identifying an annotator to labelit. We observe a useful connection between the multi-armed bandit framework andthe annotator selection in active learning. Due to the nature of thedistribution of the rewards on the arms, we use the Robust Upper ConfidenceBound (UCB) scheme with truncated empirical mean estimator to solve theannotator selection problem. This yields provable guarantees on the regret. Wefurther apply our model to the scenario where annotators are strategic anddesign suitable incentives to induce them to put in their best efforts.
arxiv-15000-162 | Efficient Robust Mean Value Calculation of 1D Features | http://arxiv.org/pdf/1601.08003v1.pdf | author:Erik Jonsson, Michael Felsberg category:cs.CV published:2016-01-29 summary:A robust mean value is often a good alternative to the standard mean valuewhen dealing with data containing many outliers. An efficient method forsamples of one-dimensional features and the truncated quadratic error norm ispresented and compared to the method of channel averaging (soft histograms).
arxiv-15000-163 | Towards A Deeper Geometric, Analytic and Algorithmic Understanding of Margins | http://arxiv.org/pdf/1406.5311v2.pdf | author:Aaditya Ramdas, Javier Peña category:math.OC cs.AI cs.LG math.NA stat.ML published:2014-06-20 summary:Given a matrix $A$, a linear feasibility problem (of which linearclassification is a special case) aims to find a solution to a primal problem$w: A^Tw > \textbf{0}$ or a certificate for the dual problem which is aprobability distribution $p: Ap = \textbf{0}$. Inspired by the continuedimportance of "large-margin classifiers" in machine learning, this paperstudies a condition measure of $A$ called its \textit{margin} that determinesthe difficulty of both the above problems. To aid geometrical intuition, wefirst establish new characterizations of the margin in terms of relevant balls,cones and hulls. Our second contribution is analytical, where we presentgeneralizations of Gordan's theorem, and variants of Hoffman's theorems, bothusing margins. We end by proving some new results on a classical iterativescheme, the Perceptron, whose convergence rates famously depends on the margin.Our results are relevant for a deeper understanding of margin-based learningand proving convergence rates of iterative schemes, apart from providing aunifying perspective on this vast topic.
arxiv-15000-164 | Hybrid CNN and Dictionary-Based Models for Scene Recognition and Domain Adaptation | http://arxiv.org/pdf/1601.07977v1.pdf | author:Guo-Sen Xie, Xu-Yao Zhang, Shuicheng Yan, Cheng-Lin Liu category:cs.CV published:2016-01-29 summary:Convolutional neural network (CNN) has achieved state-of-the-art performancein many different visual tasks. Learned from a large-scale training dataset,CNN features are much more discriminative and accurate than the hand-craftedfeatures. Moreover, CNN features are also transferable among different domains.On the other hand, traditional dictionarybased features (such as BoW and SPM)contain much more local discriminative and structural information, which isimplicitly embedded in the images. To further improve the performance, in thispaper, we propose to combine CNN with dictionarybased models for scenerecognition and visual domain adaptation. Specifically, based on the well-tunedCNN models (e.g., AlexNet and VGG Net), two dictionary-based representationsare further constructed, namely mid-level local representation (MLR) andconvolutional Fisher vector representation (CFV). In MLR, an efficienttwo-stage clustering method, i.e., weighted spatial and feature space spectralclustering on the parts of a single image followed by clustering allrepresentative parts of all images, is used to generate a class-mixture or aclassspecific part dictionary. After that, the part dictionary is used tooperate with the multi-scale image inputs for generating midlevelrepresentation. In CFV, a multi-scale and scale-proportional GMM trainingstrategy is utilized to generate Fisher vectors based on the last convolutionallayer of CNN. By integrating the complementary information of MLR, CFV and theCNN features of the fully connected layer, the state-of-the-art performance canbe achieved on scene recognition and domain adaptation problems. An interestedfinding is that our proposed hybrid representation (from VGG net trained onImageNet) is also complementary with GoogLeNet and/or VGG-11 (trained onPlace205) greatly.
arxiv-15000-165 | Selection models of language production support informed text partitioning: an intuitive and practical, bag-of-phrases framework for text analysis | http://arxiv.org/pdf/1601.07969v1.pdf | author:Jake Ryland Williams, James P. Bagrow, Andrew J. Reagan, Sharon E. Alajajian, Christopher M. Danforth, Peter Sheridan Dodds category:cs.CL published:2016-01-29 summary:The task of text segmentation, or 'chunking,' may occur at many levels intext analysis, depending on whether it is most beneficial to break it down byparagraphs of a book, sentences of a paragraph, etc. Here, we focus on afine-grained segmentation task, which we refer to as text partitioning, wherewe apply methodologies to segment sentences or clauses into phrases, or lexicalconstructions of one or more words. In the past, we have explored (uniform)stochastic text partitioning---a process on the gaps between words whereby eachspace assumes one from a binary state of fixed (word binding) or broken (wordseparating) by some probability. In that work, we narrowly explored perhaps themost naive version of this process: random, or, uniform stochasticpartitioning, where all word-word gaps are prescribed a uniformly-set breakageprobability, q. Under this framework, the breakage probability is a tunableparameter, and was set to be pure-uniform: q = 1/2. In this work, we explorephrase frequency distributions under variation of the parameter q, and definenon-uniform, or informed stochastic partitions, where q is a function ofsurrounding information. Using a crude but effective function for q, we go onto apply informed partitions to over 20,000 English texts from the ProjectGutenberg eBooks database. In these analyses, we connect selection models togenerate a notion of fit goodness for the 'bag-of-terms' (words or phrases)representations of texts, and find informed (phrase) partitions to be animprovement over the q = 1 (word) and q = 1/2 (phrase) partitions in mostcases. This, together with the scalability of the methods proposed, suggeststhat the bag-of-phrases model should more often than not be implemented inplace of the bag-of-words model, setting the stage for a paradigm shift infeature selection, which lies at the foundation of text analysis methodology.
arxiv-15000-166 | Face Alignment by Local Deep Descriptor Regression | http://arxiv.org/pdf/1601.07950v1.pdf | author:Amit Kumar, Rajeev Ranjan, Vishal Patel, Rama Chellappa category:cs.CV published:2016-01-29 summary:We present an algorithm for extracting key-point descriptors using deepconvolutional neural networks (CNN). Unlike many existing deep CNNs, our modelcomputes local features around a given point in an image. We also present aface alignment algorithm based on regression using these local descriptors. Theproposed method called Local Deep Descriptor Regression (LDDR) is able tolocalize face landmarks of varying sizes, poses and occlusions with highaccuracy. Deep Descriptors presented in this paper are able to uniquely andefficiently describe every pixel in the image and therefore can potentiallyreplace traditional descriptors such as SIFT and HOG. Extensive evaluations onfive publicly available unconstrained face alignment datasets show that ourdeep descriptor network is able to capture strong local features around a givenlandmark and performs significantly better than many competitive andstate-of-the-art face alignment algorithms.
arxiv-15000-167 | Large-scale Kernel-based Feature Extraction via Budgeted Nonlinear Subspace Tracking | http://arxiv.org/pdf/1601.07947v1.pdf | author:Fateme Sheikholeslami, Dimitris Berberidis, Georgios B. Giannakis category:stat.ML cs.LG published:2016-01-28 summary:Kernel-based methods enjoy powerful generalization capabilities in handling avariety of learning tasks. When such methods are provided with sufficienttraining data, broadly-applicable classes of nonlinear functions can beapproximated with desired accuracy. Nevertheless, inherent to the nonparametricnature of kernel-based estimators are computational and memory requirementsthat become prohibitive with large-scale datasets. In response to thisformidable challenge, the present work puts forward a low-rank, kernel-based,feature extraction approach that is particularly tailored for online operation,where data streams need not be stored in memory. A novel generative model isintroduced to approximate high-dimensional (possibly infinite) features via alow-rank nonlinear subspace, the learning of which leads to a direct kernelfunction approximation. Offline and online solvers are developed for thesubspace learning task, along with affordable versions, in which the number ofstored data vectors is confined to a predefined budget. Analytical resultsprovide performance bounds on how well the kernel matrix as well askernel-based classification and regression tasks can be approximated byleveraging budgeted online subspace learning and feature extraction schemes.Tests on synthetic and real datasets demonstrate and benchmark the efficiencyof the proposed method when linear classification and regression is applied tothe extracted features.
arxiv-15000-168 | Information-Theoretic Lower Bounds for Recovery of Diffusion Network Structures | http://arxiv.org/pdf/1601.07932v1.pdf | author:Keehwan Park, Jean Honorio category:cs.LG cs.IT math.IT stat.ML published:2016-01-28 summary:We study the information-theoretic lower bound of the sample complexity ofthe correct recovery of diffusion network structures. We introduce adiscrete-time diffusion model based on the Independent Cascade model for whichwe obtain a lower bound of order $\Omega(k \log p)$, for directed graphs of $p$nodes, and at most $k$ parents per node. Next, we introduce a continuous-timediffusion model, for which a similar lower bound of order $\Omega(k \log p)$ isobtained. Our results show that the algorithm of Pouget-Abadie et. at. isstatistically optimal for the discrete-time regime. Our work also opens thequestion of whether it is possible to devise optimal algorithms for thecontinuous-time regime.
arxiv-15000-169 | Automating biomedical data science through tree-based pipeline optimization | http://arxiv.org/pdf/1601.07925v1.pdf | author:Randal S. Olson, Ryan J. Urbanowicz, Peter C. Andrews, Nicole A. Lavender, La Creis Kidd, Jason H. Moore category:cs.LG cs.NE published:2016-01-28 summary:Over the past decade, data science and machine learning has grown from amysterious art form to a staple tool across a variety of fields in academia,business, and government. In this paper, we introduce the concept of tree-basedpipeline optimization for automating one of the most tedious parts of machinelearning---pipeline design. We implement a Tree-based Pipeline OptimizationTool (TPOT) and demonstrate its effectiveness on a series of simulated andreal-world genetic data sets. In particular, we show that TPOT can buildmachine learning pipelines that achieve competitive classification accuracy anddiscover novel pipeline operators---such as synthetic featureconstructors---that significantly improve classification accuracy on these datasets. We also highlight the current challenges to pipeline optimization, suchas the tendency to produce pipelines that overfit the data, and suggest futureresearch paths to overcome these challenges. As such, this work represents anearly step toward fully automating machine learning pipeline design.
arxiv-15000-170 | COROLA: A Sequential Solution to Moving Object Detection Using Low-rank Approximation | http://arxiv.org/pdf/1505.03566v2.pdf | author:Moein Shakeri, Hong Zhang category:cs.CV cs.RO published:2015-05-13 summary:Extracting moving objects from a video sequence and estimating the backgroundof each individual image are fundamental issues in many practical applicationssuch as visual surveillance, intelligent vehicle navigation, and trafficmonitoring. Recently, some methods have been proposed to detect moving objectsin a video via low-rank approximation and sparse outliers where the backgroundis modeled with the computed low-rank component of the video and the foregroundobjects are detected as the sparse outliers in the low-rank approximation. Allof these existing methods work in a batch manner, preventing them from beingapplied in real time and long duration tasks. In this paper, we present anonline sequential framework, namely contiguous outliers representation viaonline low-rank approximation (COROLA), to detect moving objects and learn thebackground model at the same time. We also show that our model can detectmoving objects with a moving camera. Our experimental evaluation uses simulateddata and real public datasets and demonstrates the superior performance ofCOROLA in terms of both accuracy and execution time.
arxiv-15000-171 | Parameterized Machine Learning for High-Energy Physics | http://arxiv.org/pdf/1601.07913v1.pdf | author:Pierre Baldi, Kyle Cranmer, Taylor Faucett, Peter Sadowski, Daniel Whiteson category:hep-ex cs.LG hep-ph published:2016-01-28 summary:We investigate a new structure for machine learning classifiers applied toproblems in high-energy physics by expanding the inputs to include not onlymeasured features but also physics parameters. The physics parameters representa smoothly varying learning task, and the resulting parameterized classifiercan smoothly interpolate between them and replace sets of classifiers trainedat individual values. This simplifies the training process and gives improvedperformance at intermediate values, even for complex problems requiring deeplearning. Applications include tools parameterized in terms of theoreticalmodel parameters, such as the mass of a particle, which allow for a singlenetwork to provide improved discrimination across a range of masses. Thisconcept is simple to implement and allows for optimized interpolatable results.
arxiv-15000-172 | Statistical Inference, Learning and Models in Big Data | http://arxiv.org/pdf/1509.02900v2.pdf | author:Beate Franke, Jean-François Plante, Ribana Roscher, Annie Lee, Cathal Smyth, Armin Hatefi, Fuqi Chen, Einat Gil, Alexander Schwing, Alessandro Selvitella, Michael M. Hoffman, Roger Grosse, Dieter Hendricks, Nancy Reid category:stat.ML cs.LG 62-07 published:2015-09-09 summary:The need for new methods to deal with big data is a common theme in mostscientific fields, although its definition tends to vary with the context.Statistical ideas are an essential part of this, and as a partial response, athematic program on statistical inference, learning, and models in big data washeld in 2015 in Canada, under the general direction of the Canadian StatisticalSciences Institute, with major funding from, and most activities located at,the Fields Institute for Research in Mathematical Sciences. This paper gives anoverview of the topics covered, describing challenges and strategies that seemcommon to many different areas of application, and including some examples ofapplications to make these challenges and strategies more concrete.
arxiv-15000-173 | Geo-distinctive Visual Element Matching for Location Estimation of Images | http://arxiv.org/pdf/1601.07884v1.pdf | author:Xinchao Li, Martha A. Larson, Alan Hanjalic category:cs.MM cs.CV published:2016-01-28 summary:We propose an image representation and matching approach that substantiallyimproves visual-based location estimation for images. The main novelty of theapproach, called distinctive visual element matching (DVEM), is its use ofrepresentations that are specific to the query image whose location is beingpredicted. These representations are based on visual element clouds, whichrobustly capture the connection between the query and visual evidence fromcandidate locations. We then maximize the influence of visual elements that aregeo-distinctive because they do not occur in images taken at many otherlocations. We carry out experiments and analysis for both geo-constrained andgeo-unconstrained location estimation cases using two large-scale,publicly-available datasets: the San Francisco Landmark dataset with $1.06$million street-view images and the MediaEval '15 Placing Task dataset with$5.6$ million geo-tagged images from Flickr. We present examples thatillustrate the highly-transparent mechanics of the approach, which are based oncommon sense observations about the visual patterns in image collections. Ourresults show that the proposed method delivers a considerable performanceimprovement compared to the state of the art.
arxiv-15000-174 | Towards the Design of an End-to-End Automated System for Image and Video-based Recognition | http://arxiv.org/pdf/1601.07883v1.pdf | author:Rama Chellappa, Jun-Cheng Chen, Rajeev Ranjan, Swami Sankaranarayanan, Amit Kumar, Vishal M. Patel, Carlos D. Castillo category:cs.CV published:2016-01-28 summary:Over many decades, researchers working in object recognition have longed foran end-to-end automated system that will simply accept 2D or 3D image or videosas inputs and output the labels of objects in the input data. Computer visionmethods that use representations derived based on geometric, radiometric andneural considerations and statistical and structural matchers and artificialneural network-based methods where a multi-layer network learns the mappingfrom inputs to class labels have provided competing approaches for imagerecognition problems. Over the last four years, methods based on DeepConvolutional Neural Networks (DCNNs) have shown impressive performanceimprovements on object detection/recognition challenge problems. This has beenmade possible due to the availability of large annotated data, a betterunderstanding of the non-linear mapping between image and class labels as wellas the affordability of GPUs. In this paper, we present a brief history ofdevelopments in computer vision and artificial neural networks over the lastforty years for the problem of image-based recognition. We then present thedesign details of a deep learning system for end-to-end unconstrained faceverification/recognition. Some open issues regarding DCNNs for objectrecognition problems are then discussed. We caution the readers that the viewsexpressed in this paper are from the authors and authors only!
arxiv-15000-175 | Unsupervised Learning on Neural Network Outputs | http://arxiv.org/pdf/1506.00990v9.pdf | author:Yao Lu category:cs.LG published:2015-06-02 summary:The outputs of a trained neural network contain much richer information thanjust an one-hot classifier. For example, a neural network might give an imageof a dog the probability of one in a million of being a cat but it is stillmuch larger than the probability of being a car. To reveal the hidden structurein them, we apply two unsupervised learning algorithms, PCA and ICA, to theoutputs of a deep Convolutional Neural Network trained on the ImageNet of 1000classes. The PCA/ICA embedding of the object classes reveals their visualsimilarity and the PCA/ICA components can be interpreted as common visualfeatures shared by similar object classes. For an application, we proposed anew zero-shot learning method, in which the visual features learned by PCA/ICAare employed. Our zero-shot learning method achieves the state-of-the-artresults on the ImageNet of over 20000 classes.
arxiv-15000-176 | Conditional Deep Learning for Energy-Efficient and Enhanced Pattern Recognition | http://arxiv.org/pdf/1509.08971v6.pdf | author:Priyadarshini Panda, Abhronil Sengupta, Kaushik Roy category:cs.CV published:2015-09-29 summary:Deep learning neural networks have emerged as one of the most powerfulclassification tools for vision related applications. However, thecomputational and energy requirements associated with such deep nets can bequite high, and hence their energy-efficient implementation is of greatinterest. Although traditionally the entire network is utilized for therecognition of all inputs, we observe that the classification difficulty varieswidely across inputs in real-world datasets; only a small fraction of inputsrequire the full computational effort of a network, while a large majority canbe classified correctly with very low effort. In this paper, we proposeConditional Deep Learning (CDL) where the convolutional layer features are usedto identify the variability in the difficulty of input instances andconditionally activate the deeper layers of the network. We achieve this bycascading a linear network of output neurons for each convolutional layer andmonitoring the output of the linear network to decide whether classificationcan be terminated at the current stage or not. The proposed methodology thusenables the network to dynamically adjust the computational effort dependingupon the difficulty of the input data while maintaining competitiveclassification accuracy. We evaluate our approach on the MNIST dataset. Ourexperiments demonstrate that our proposed CDL yields 1.91x reduction in averagenumber of operations per input, which translates to 1.84x improvement inenergy. In addition, our results show an improvement in classification accuracyfrom 97.5% to 98.9% as compared to the original network.
arxiv-15000-177 | An Overview of Melanoma Detection in Dermoscopy Images Using Image Processing and Machine Learning | http://arxiv.org/pdf/1601.07843v1.pdf | author:Nabin K. Mishra, M. Emre Celebi category:cs.CV stat.ML published:2016-01-28 summary:The incidence of malignant melanoma continues to increase worldwide. Thiscancer can strike at any age; it is one of the leading causes of loss of lifein young persons. Since this cancer is visible on the skin, it is potentiallydetectable at a very early stage when it is curable. New developments haveconverged to make fully automatic early melanoma detection a real possibility.First, the advent of dermoscopy has enabled a dramatic boost in clinicaldiagnostic ability to the point that melanoma can be detected in the clinic atthe very earliest stages. The global adoption of this technology has allowedaccumulation of large collections of dermoscopy images of melanomas and benignlesions validated by histopathology. The development of advanced technologiesin the areas of image processing and machine learning have given us the abilityto allow distinction of malignant melanoma from the many benign mimics thatrequire no biopsy. These new technologies should allow not only earlierdetection of melanoma, but also reduction of the large number of needless andcostly biopsy procedures. Although some of the new systems reported for thesetechnologies have shown promise in preliminary trials, widespreadimplementation must await further technical progress in accuracy andreproducibility. In this paper, we provide an overview of computerizeddetection of melanoma in dermoscopy images. First, we discuss the variousaspects of lesion segmentation. Then, we provide a brief overview of clinicalfeature segmentation. Finally, we discuss the classification stage wheremachine learning algorithms are applied to the attributes generated from thesegmented features to predict the existence of melanoma.
arxiv-15000-178 | Joint Sensing Matrix and Sparsifying Dictionary Optimization for Tensor Compressive Sensing | http://arxiv.org/pdf/1601.07804v1.pdf | author:Xin Ding, Wei Chen, Ian J. Wassell category:cs.LG cs.IT math.IT published:2016-01-28 summary:Tensor Compressive Sensing (TCS) is a multidimensional framework ofCompressive Sensing (CS), and it is advantageous in terms of reducing theamount of storage, easing hardware implementations and preservingmultidimensional structures of signals in comparison to a conventional CSsystem. In a TCS system, instead of using a random sensing matrix and apredefined dictionary, the average-case performance can be further improved byemploying an optimized multidimensional sensing matrix and a learnedmultilinear sparsifying dictionary. In this paper, we propose a jointoptimization approach of the sensing matrix and dictionary for a TCS system.For the sensing matrix design in TCS, an extended separable approach with aclosed form solution and a novel iterative non-separable method are proposedwhen the multilinear dictionary is fixed. In addition, a multidimensionaldictionary learning method that takes advantages of the multidimensionalstructure is derived, and the influence of sensing matrices is taken intoaccount in the learning process. A joint optimization is achieved viaalternately iterating the optimization of the sensing matrix and dictionary.Numerical experiments using both synthetic data and real images demonstrate thesuperiority of the proposed approaches.
arxiv-15000-179 | Is Image Super-resolution Helpful for Other Vision Tasks? | http://arxiv.org/pdf/1509.07009v2.pdf | author:Dengxin Dai, Yujian Wang, Yuhua Chen, Luc Van Gool category:cs.CV published:2015-09-23 summary:Despite the great advances made in the field of image super-resolution (ISR)during the last years, the performance has merely been evaluated perceptually.Thus, it is still unclear whether ISR is helpful for other vision tasks. Inthis paper, we present the first comprehensive study and analysis of theusefulness of ISR for other vision applications. In particular, six ISR methodsare evaluated on four popular vision tasks, namely edge detection, semanticimage segmentation, digit recognition, and scene recognition. We show thatapplying ISR to input images of other vision systems does improve theirperformance when the input images are of low-resolution. We also study thecorrelation between four standard perceptual evaluation criteria (namely PSNR,SSIM, IFC, and NQM) and the usefulness of ISR to the vision tasks. Experimentsshow that they correlate well with each other in general, but perceptualcriteria are still not accurate enough to be used as full proxies for theusefulness. We hope this work will inspire the community to evaluate ISRmethods also in real vision applications, and to adopt ISR as a pre-processingstep of other vision tasks if the resolution of their input images is low.
arxiv-15000-180 | Character-Level Incremental Speech Recognition with Recurrent Neural Networks | http://arxiv.org/pdf/1601.06581v2.pdf | author:Kyuyeon Hwang, Wonyong Sung category:cs.CL cs.LG cs.NE published:2016-01-25 summary:In real-time speech recognition applications, the latency is an importantissue. We have developed a character-level incremental speech recognition (ISR)system that responds quickly even during the speech, where the hypotheses aregradually improved while the speaking proceeds. The algorithm employs aspeech-to-character unidirectional recurrent neural network (RNN), which isend-to-end trained with connectionist temporal classification (CTC), and anRNN-based character-level language model (LM). The output values of theCTC-trained RNN are character-level probabilities, which are processed by beamsearch decoding. The RNN LM augments the decoding by providing long-termdependency information. We propose tree-based online beam search withadditional depth-pruning, which enables the system to process infinitely longinput speech with low latency. This system not only responds quickly on speechbut also can dictate out-of-vocabulary (OOV) words according to pronunciation.The proposed model achieves the word error rate (WER) of 8.90% on the WallStreet Journal (WSJ) Nov'92 20K evaluation set when trained on the WSJ SI-284training set.
arxiv-15000-181 | Distributed Low Rank Approximation of Implicit Functions of a Matrix | http://arxiv.org/pdf/1601.07721v1.pdf | author:David P. Woodruff, Peilin Zhong category:cs.NA cs.LG published:2016-01-28 summary:We study distributed low rank approximation in which the matrix to beapproximated is only implicitly represented across the different servers. Forexample, each of $s$ servers may have an $n \times d$ matrix $A^t$, and we maybe interested in computing a low rank approximation to $A = f(\sum_{t=1}^sA^t)$, where $f$ is a function which is applied entrywise to the matrix$\sum_{t=1}^s A^t$. We show for a wide class of functions $f$ it is possible toefficiently compute a $d \times d$ rank-$k$ projection matrix $P$ for which$\A - AP\_F^2 \leq \A - [A]_k\_F^2 + \varepsilon \A\_F^2$, where $AP$denotes the projection of $A$ onto the row span of $P$, and $[A]_k$ denotes thebest rank-$k$ approximation to $A$ given by the singular value decomposition.The communication cost of our protocols is $d \cdot (sk/\varepsilon)^{O(1)}$,and they succeed with high probability. Our framework allows us to efficientlycompute a low rank approximation to an entry-wise softmax, to a Gaussian kernelexpansion, and to $M$-Estimators applied entrywise (i.e., forms of robust lowrank approximation). We also show that our additive error approximation is bestpossible, in the sense that any protocol achieving relative error for theseproblems requires significantly more communication. Finally, we experimentallyvalidate our algorithms on real datasets.
arxiv-15000-182 | Log-Normal Matrix Completion for Large Scale Link Prediction | http://arxiv.org/pdf/1601.07714v1.pdf | author:Brian Mohtashemi, Thomas Ketseoglou category:cs.SI cs.LG stat.ML published:2016-01-28 summary:The ubiquitous proliferation of online social networks has led to thewidescale emergence of relational graphs expressing unique patterns in linkformation and descriptive user node features. Matrix Factorization andCompletion have become popular methods for Link Prediction due to the low ranknature of mutual node friendship information, and the availability of parallelcomputer architectures for rapid matrix processing. Current Link Predictionliterature has demonstrated vast performance improvement through theutilization of sparsity in addition to the low rank matrix assumption. However,the majority of research has introduced sparsity through the limited L1 orFrobenius norms, instead of considering the more detailed distributions whichled to the graph formation and relationship evolution. In particular, socialnetworks have been found to express either Pareto, or more recently discovered,Log Normal distributions. Employing the convexity-inducing Lovasz Extension, wedemonstrate how incorporating specific degree distribution information can leadto large scale improvements in Matrix Completion based Link prediction. Weintroduce Log-Normal Matrix Completion (LNMC), and solve the complexoptimization problem by employing Alternating Direction Method of Multipliers.Using data from three popular social networks, our experiments yield up to 5%AUC increase over top-performing non-structured sparsity based methods.
arxiv-15000-183 | Matrix Completion from Fewer Entries: Spectral Detectability and Rank Estimation | http://arxiv.org/pdf/1506.03498v3.pdf | author:Alaa Saade, Florent Krzakala, Lenka Zdeborová category:cs.LG stat.ML published:2015-06-10 summary:The completion of low rank matrices from few entries is a task with manypractical applications. We consider here two aspects of this problem:detectability, i.e. the ability to estimate the rank $r$ reliably from thefewest possible random entries, and performance in achieving smallreconstruction error. We propose a spectral algorithm for these two taskscalled MaCBetH (for Matrix Completion with the Bethe Hessian). The rank isestimated as the number of negative eigenvalues of the Bethe Hessian matrix,and the corresponding eigenvectors are used as initial condition for theminimization of the discrepancy between the estimated matrix and the revealedentries. We analyze the performance in a random matrix setting using resultsfrom the statistical mechanics of the Hopfield neural network, and show inparticular that MaCBetH efficiently detects the rank $r$ of a large $n\times m$matrix from $C(r)r\sqrt{nm}$ entries, where $C(r)$ is a constant close to $1$.We also evaluate the corresponding root-mean-square error empirically and showthat MaCBetH compares favorably to other existing approaches.
arxiv-15000-184 | Combining persistent homology and invariance groups for shape comparison | http://arxiv.org/pdf/1312.7219v4.pdf | author:Patrizio Frosini, Grzegorz Jablonski category:math.AT cs.CG cs.CV I.4.7; I.5.1 published:2013-12-27 summary:In many applications concerning the comparison of data expressed by$\mathbb{R}^m$-valued functions defined on a topological space $X$, theinvariance with respect to a given group $G$ of self-homeomorphisms of $X$ isrequired. While persistent homology is quite efficient in the topological andqualitative comparison of this kind of data when the invariance group $G$ isthe group $\mathrm{Homeo}(X)$ of all self-homeomorphisms of $X$, this theory isnot tailored to manage the case in which $G$ is a proper subgroup of$\mathrm{Homeo}(X)$, and its invariance appears too general for several tasks.This paper proposes a way to adapt persistent homology in order to getinvariance just with respect to a given group of self-homeomorphisms of $X$.The main idea consists in a dual approach, based on considering the set of all$G$-invariant non-expanding operators defined on the space of the admissiblefiltering functions on $X$. Some theoretical results concerning this approachare proven and two experiments are presented. An experiment illustrates theapplication of the proposed technique to compare 1D-signals, when theinvariance is expressed by the group of affinities, the group oforientation-preserving affinities, the group of isometries, the group oftranslations and the identity group. Another experiment shows how our techniquecan be used for image comparison.
arxiv-15000-185 | Obtaining A Linear Combination of the Principal Components of a Matrix on Quantum Computers | http://arxiv.org/pdf/1512.02109v3.pdf | author:Anmer Daskin category:quant-ph cs.LG math.ST stat.TH published:2015-11-26 summary:Principal component analysis is a multivariate statistical method frequentlyused in science and engineering to reduce the dimension of a problem or extractthe most significant features from a dataset. In this paper, using a similarnotion to the quantum counting, we show how to apply the amplitudeamplification together with the phase estimation algorithm to an operator inorder to procure the eigenvectors of the operator associated to the eigenvaluesdefined in the range $\left[a, b\right]$, where $a$ and $b$ are real and $0\leq a \leq b \leq 1$. This makes possible to obtain a combination of theeigenvectors associated to the largest eigenvalues and so can be used to doprincipal component analysis on quantum computers.
arxiv-15000-186 | Robustness Analysis of Preconditioned Successive Projection Algorithm for General Form of Separable NMF Problem | http://arxiv.org/pdf/1506.08387v2.pdf | author:Tomohiko Mizutani category:stat.ML math.OC published:2015-06-28 summary:The successive projection algorithm (SPA) has been known to work well forseparable nonnegative matrix factorization (NMF) problems arising inapplications, such as topic extraction from documents and endmember detectionin hyperspectral images. One of the reasons is in that the algorithm is robustto noise. Gillis and Vavasis showed in [SIAM J. Optim., 25(1), pp. 677-698,2015] that a preconditioner can further enhance its noise robustness. The proofrested on the condition that the dimension $d$ and factorization rank $r$ inthe separable NMF problem coincide with each other. However, it may beunrealistic to expect that the condition holds in separable NMF problemsappearing in actual applications; in such problems, $d$ is usually greater than$r$. This paper shows, without the condition $d=r$, that the preconditioned SPAis robust to noise.
arxiv-15000-187 | TEI and LMF crosswalks | http://arxiv.org/pdf/1301.2444v3.pdf | author:Laurent Romary category:cs.CL published:2013-01-11 summary:The present paper explores various arguments in favour of making the TextEncoding Initia-tive (TEI) guidelines an appropriate serialisation for ISOstandard 24613:2008 (LMF, Lexi-cal Mark-up Framework) . It also identifies theissues that would have to be resolved in order to reach an appropriateimplementation of these ideas, in particular in terms of infor-mationalcoverage. We show how the customisation facilities offered by the TEIguidelines can provide an adequate background, not only to cover missingcomponents within the current Dictionary chapter of the TEI guidelines, butalso to allow specific lexical projects to deal with local constraints. Weexpect this proposal to be a basis for a future ISO project in the context ofthe on going revision of LMF.
arxiv-15000-188 | Non-Gaussian Component Analysis with Log-Density Gradient Estimation | http://arxiv.org/pdf/1601.07665v1.pdf | author:Hiroaki Sasaki, Gang Niu, Masashi Sugiyama category:stat.ML published:2016-01-28 summary:Non-Gaussian component analysis (NGCA) is aimed at identifying a linearsubspace such that the projected data follows a non-Gaussian distribution. Inthis paper, we propose a novel NGCA algorithm based on log-density gradientestimation. Unlike existing methods, the proposed NGCA algorithm identifies thelinear subspace by using the eigenvalue decomposition without any iterativeprocedures, and thus is computationally reasonable. Furthermore, throughtheoretical analysis, we prove that the identified subspace converges to thetrue subspace at the optimal parametric rate. Finally, the practicalperformance of the proposed algorithm is demonstrated on both artificial andbenchmark datasets.
arxiv-15000-189 | MAP Support Detection for Greedy Sparse Signal Recovery Algorithms in Compressive Sensing | http://arxiv.org/pdf/1508.00964v2.pdf | author:Namyoon Lee category:cs.IT cs.LG math.IT published:2015-08-05 summary:A reliable support detection is essential for a greedy algorithm toreconstruct a sparse signal accurately from compressed and noisy measurements.This paper proposes a novel support detection method for greedy algorithms,which is referred to as "\textit{maximum a posteriori (MAP) supportdetection}". Unlike existing support detection methods that identify supportindices with the largest correlation value in magnitude per iteration, theproposed method selects them with the largest likelihood ratios computed underthe true and null support hypotheses by simultaneously exploiting thedistributions of sensing matrix, sparse signal, and noise. Leveraging thistechnique, MAP-Matching Pursuit (MAP-MP) is first presented to show theadvantages of exploiting the proposed support detection method, and asufficient condition for perfect signal recovery is derived for the case whenthe sparse signal is binary. Subsequently, a set of iterative greedyalgorithms, called MAP-generalized Orthogonal Matching Pursuit (MAP-gOMP),MAP-Compressive Sampling Matching Pursuit (MAP-CoSaMP), and MAP-SubspacePursuit (MAP-SP) are presented to demonstrate the applicability of the proposedsupport detection method to existing greedy algorithms. From empirical results,it is shown that the proposed greedy algorithms with highly reliable supportdetection can be better, faster, and easier to implement than basis pursuit vialinear programming.
arxiv-15000-190 | Discriminative Training of Deep Fully-connected Continuous CRF with Task-specific Loss | http://arxiv.org/pdf/1601.07649v1.pdf | author:Fayao Liu, Guosheng Lin, Chunhua Shen category:cs.CV published:2016-01-28 summary:Recent works on deep conditional random fields (CRF) have set new records onmany vision tasks involving structured predictions. Here we propose afully-connected deep continuous CRF model for both discrete and continuouslabelling problems. We exemplify the usefulness of the proposed model onmulti-class semantic labelling (discrete) and the robust depth estimation(continuous) problems. In our framework, we model both the unary and the pairwise potentialfunctions as deep convolutional neural networks (CNN), which are jointlylearned in an end-to-end fashion. The proposed method possesses the mainadvantage of continuously-valued CRF, which is a closed-form solution for theMaximum a posteriori (MAP) inference. To better adapt to different tasks, instead of using the commonly employedmaximum likelihood CRF parameter learning protocol, we propose task-specificloss functions for learning the CRF parameters. It enables direct optimization of the quality of the MAP estimates during thecourse of learning. Specifically, we optimize the multi-class classification loss for thesemantic labelling task and the Turkey's biweight loss for the robust depthestimation problem. Experimental results on the semantic labelling and robust depth estimationtasks demonstrate that the proposed method compare favorably against bothbaseline and state-of-the-art methods. In particular, we show that although the proposed deep CRF model iscontinuously valued, with the equipment of task-specific loss, it achievesimpressive results even on discrete labelling tasks.
arxiv-15000-191 | Automatic Generation of Building Models Using 2D Maps and Street View Images | http://arxiv.org/pdf/1601.07630v1.pdf | author:Jiangye Yuan, Anil M. Cheriyadat category:cs.CV published:2016-01-28 summary:We introduce a new approach for generating simple 3D building models bycombining building footprints from 2D maps with street level images. Theapproach works with crowd sourced maps such as the OpenStreetMap and streetlevel images acquired by a calibrated camera mounted on a moving vehicle.Buildings are modeled as boxes extruded from building footprints with theheight information estimated from images. Building footprints are elevated inworld coordinates and projected onto images. Building heights are estimated byscoring projected footprints based on their alignment with visible buildingfeatures in images. However, it is challenging to achieve accurate projectionsdue to camera pose errors inherited from external sensors resulting inincorrect height estimation. We derive a solution to precisely locate camerason maps using correspondence between image features and building footprints. Wetightly couple the camera localization and height estimation steps to producean effective method for 3D building model generation. Experiments using GoogleStreet View images and publicly available map data show the promise of ourmethod.
arxiv-15000-192 | Sparse Generalized Principal Component Analysis for Large-scale Applications beyond Gaussianity | http://arxiv.org/pdf/1512.03883v2.pdf | author:Qiaoya Zhang, Yiyuan She category:stat.CO stat.ML published:2015-12-12 summary:Principal Component Analysis (PCA) is a dimension reduction technique. Itproduces inconsistent estimators when the dimensionality is moderate to high,which is often the problem in modern large-scale applications where algorithmscalability and model interpretability are difficult to achieve, not to mentionthe prevalence of missing values. While existing sparse PCA methods alleviateinconsistency, they are constrained to the Gaussian assumption of classical PCAand fail to address algorithm scalability issues. We generalize sparse PCA tothe broad exponential family distributions under high-dimensional setup, withbuilt-in treatment for missing values. Meanwhile we propose a family ofiterative sparse generalized PCA (SG-PCA) algorithms such that despite thenon-convexity and non-smoothness of the optimization task, the loss functiondecreases in every iteration. In terms of ease and intuitive parameter tuning,our sparsity-inducing regularization is far superior to the popular Lasso.Furthermore, to promote overall scalability, accelerated gradient is integratedfor fast convergence, while a progressive screening technique graduallysqueezes out nuisance dimensions of a large-scale problem for feasibleoptimization. High-dimensional simulation and real data experiments demonstratethe efficiency and efficacy of SG-PCA.
arxiv-15000-193 | Revealing Fundamental Physics from the Daya Bay Neutrino Experiment using Deep Neural Networks | http://arxiv.org/pdf/1601.07621v1.pdf | author:Evan Racah, Seyoon Ko, Peter Sadowski, Wahid Bhimji, Craig Tull, Sang-Yun Oh, Pierre Baldi, Prabhat category:stat.ML cs.LG published:2016-01-28 summary:Experiments in particle physics produce enormous quantities of data that mustbe analyzed and interpreted by teams of physicists. This analysis is oftenexploratory, where scientists are unable to enumerate the possible types ofsignal prior to performing the experiment. Thus, tools for summarizing,clustering, visualizing and classifying high-dimensional data are essential. Inthis work, we show that meaningful physical content can be revealed bytransforming the raw data into a learned high-level representation using deepneural networks, with measurements taken at the Daya Bay Neutrino Experiment asa case study. We further show how convolutional deep neural networks canprovide an effective classification filter with greater than 97% accuracyacross different classes of physics events, significantly better than othermachine learning approaches.
arxiv-15000-194 | Efficient Hill-Climber for Multi-Objective Pseudo-Boolean Optimization | http://arxiv.org/pdf/1601.07596v1.pdf | author:Francisco Chicano, Darrell Whitley, Renato Tinos category:cs.AI cs.NE I.2.8 published:2016-01-27 summary:Local search algorithms and iterated local search algorithms are a basictechnique. Local search can be a stand along search methods, but it can also behybridized with evolutionary algorithms. Recently, it has been shown that it ispossible to identify improving moves in Hamming neighborhoods for k-boundedpseudo-Boolean optimization problems in constant time. This means that localsearch does not need to enumerate neighborhoods to find improving moves. Italso means that evolutionary algorithms do not need to use random mutation as aoperator, except perhaps as a way to escape local optima. In this paper, weshow how improving moves can be identified in constant time for multiobjectiveproblems that are expressed as k-bounded pseudo-Boolean functions. Inparticular, multiobjective forms of NK Landscapes and Mk Landscapes areconsidered.
arxiv-15000-195 | Locally-Supervised Deep Hybrid Model for Scene Recognition | http://arxiv.org/pdf/1601.07576v1.pdf | author:Sheng Guo, Weilin Huang, Yu Qiao category:cs.CV published:2016-01-27 summary:Convolutional neural networks (CNN) have recently achieved remarkablesuccesses in various image classification and understanding tasks. The deepfeatures obtained at the top fully-connected layer of the CNN (FC-features)exhibit rich global semantic information and are extremely effective in imageclassification. On the other hand, the convolutional features in the middlelayers of the CNN also contain meaningful local information, but are not fullyexplored for image representation. In this paper, we propose a novelLocally-Supervised Deep Hybrid Model (LS-DHM) that effectively enhances andexplores the convolutional features for scene recognition. Firstly, we noticethat the convolutional features capture local objects and fine structures ofscene images, which yield important cues for discriminating ambiguous scenes,whereas these features are significantly eliminated in the highly-compressed FCrepresentation. Secondly, we propose a new Local Convolutional Supervision(LCS) layer to enhance the local structure of the image by directly propagatingthe label information to the convolutional layers. Thirdly, we propose anefficient Fisher Convolutional Vector (FCV) that successfully rescues theorderless mid-level semantic information (e.g. objects and textures) of sceneimage. The FCV encodes the large-sized convolutional maps into a fixed-lengthmid-level representation, and is demonstrated to be strongly complementary tothe high-level FC-features. Finally, both the FCV and FC-features arecollaboratively employed in the LSDHM representation, which achievesoutstanding performance in our experiments. It obtains 83.75% and 67.56%accuracies respectively on the heavily benchmarked MIT Indoor67 and SUN397datasets, advancing the stat-of-the-art substantially.
arxiv-15000-196 | Minimax Subsampling for Estimation and Prediction in Low-Dimensional Linear Regression | http://arxiv.org/pdf/1601.02068v2.pdf | author:Yining Wang, Aarti Singh category:stat.ML cs.LG math.ST stat.TH published:2016-01-09 summary:Subsampling strategies are derived to sample a small portion of design (data)points in a low-dimensional linear regression model $y=X\beta+\varepsilon$ withnear-optimal statistical rates. Our results apply to both problems ofestimation of the underlying linear model $\beta$ and predicting thereal-valued response $y$ of a new data point $x$. The derived subsamplingstrategies are minimax optimal under the fixed design setting, up to a small$(1+\epsilon)$ relative factor. We also give interpretable subsamplingprobabilities for the random design setting and demonstrate explicit gaps instatistial rates between optimal and baseline (e.g., uniform) subsamplingmethods.
arxiv-15000-197 | Multi-Target Regression via Input Space Expansion: Treating Targets as Inputs | http://arxiv.org/pdf/1211.6581v5.pdf | author:Eleftherios Spyromitros-Xioufis, Grigorios Tsoumakas, William Groves, Ioannis Vlahavas category:cs.LG published:2012-11-28 summary:In many practical applications of supervised learning the task involves theprediction of multiple target variables from a common set of input variables.When the prediction targets are binary the task is called multi-labelclassification, while when the targets are continuous the task is calledmulti-target regression. In both tasks, target variables often exhibitstatistical dependencies and exploiting them in order to improve predictiveaccuracy is a core challenge. A family of multi-label classification methodsaddress this challenge by building a separate model for each target on anexpanded input space where other targets are treated as additional inputvariables. Despite the success of these methods in the multi-labelclassification domain, their applicability and effectiveness in multi-targetregression has not been studied until now. In this paper, we introduce two newmethods for multi-target regression, called Stacked Single-Target and Ensembleof Regressor Chains, by adapting two popular multi-label classification methodsof this family. Furthermore, we highlight an inherent problem of these methods- a discrepancy of the values of the additional input variables betweentraining and prediction - and develop extensions that use out-of-sampleestimates of the target variables during training in order to tackle thisproblem. The results of an extensive experimental evaluation carried out on alarge and diverse collection of datasets show that, when the discrepancy isappropriately mitigated, the proposed methods attain consistent improvementsover the independent regressions baseline. Moreover, two versions of Ensembleof Regression Chains perform significantly better than four state-of-the-artmethods including regularization-based multi-task learning methods and amulti-objective random forest approach.
arxiv-15000-198 | Artificial neural networks in calibration of nonlinear mechanical models | http://arxiv.org/pdf/1502.01380v2.pdf | author:Tomáš Mareš, Eliška Janouchová, Anna Kučerová category:cs.NE cs.CE published:2015-02-04 summary:Rapid development in numerical modelling of materials and the complexity ofnew models increases quickly together with their computational demands. Despitethe growing performance of modern computers and clusters, calibration of suchmodels from noisy experimental data remains a nontrivial and oftencomputationally exhaustive task. The layered neural networks thus represent arobust and efficient technique to overcome the time-consuming simulations of acalibrated model. The potential of neural networks consists in simpleimplementation and high versatility in approximating nonlinear relationships.Therefore, there were several approaches proposed to accelerate the calibrationof nonlinear models by neural networks. This contribution reviews and comparesthree possible strategies based on approximating (i) model response, (ii)inverse relationship between the model response and its parameters and (iii)error function quantifying how well the model fits the data. The advantages anddrawbacks of particular strategies are demonstrated on the calibration of fourparameters of the affinity hydration model from simulated data as well as fromexperimental measurements. This model is highly nonlinear, but computationallycheap thus allowing its calibration without any approximation and betterquantification of results obtained by the examined calibration strategies. Thepaper can be thus viewed as a guide intended for the engineers to help themselect an appropriate strategy in their particular calibration problems.
arxiv-15000-199 | Osteoporotic and Neoplastic Compression Fracture Classification on Longitudinal CT | http://arxiv.org/pdf/1601.07533v1.pdf | author:Yinong Wang, Jianhua Yao, Joseph E. Burns, Ronald M. Summers category:cs.CV q-bio.TO published:2016-01-27 summary:Classification of vertebral compression fractures (VCF) having osteoporoticor neoplastic origin is fundamental to the planning of treatment. We developeda fracture classification system by acquiring quantitative morphologic and bonedensity determinants of fracture progression through the use of automatedmeasurements from longitudinal studies. A total of 250 CT studies were acquiredfor the task, each having previously identified VCFs with osteoporosis orneoplasm. Thirty-six features or each identified VCF were computed andclassified using a committee of support vector machines. Ten-fold crossvalidation on 695 identified fractured vertebrae showed classificationaccuracies of 0.812, 0.665, and 0.820 for the measured, longitudinal, andcombined feature sets respectively.
arxiv-15000-200 | Learning to Extract Motion from Videos in Convolutional Neural Networks | http://arxiv.org/pdf/1601.07532v1.pdf | author:Damien Teney, Martial Hebert category:cs.CV published:2016-01-27 summary:This paper shows how to extract dense optical flow from videos with aconvolutional neural network (CNN). The proposed model constitutes a potentialbuilding block for deeper architectures to allow using motion without resortingto an external algorithm, \eg for recognition in videos. We derive our networkarchitecture from signal processing principles to provide desired invariancesto image contrast, phase and texture. We constrain weights within the networkto enforce strict rotation invariance and substantially reduce the number ofparameters to learn. We demonstrate end-to-end training on only 8 sequences ofthe Middlebury dataset, orders of magnitude less than competing CNN-basedmotion estimation methods, and obtain comparable performance to classicalmethods on the Middlebury benchmark. Importantly, our method outputs adistributed representation of motion that allows representing multiple,transparent motions, and dynamic textures. Our contributions on network designand rotation invariance offer insights nonspecific to motion estimation.
arxiv-15000-201 | FlatCam: Thin, Bare-Sensor Cameras using Coded Aperture and Computation | http://arxiv.org/pdf/1509.00116v2.pdf | author:M. Salman Asif, Ali Ayremlou, Aswin Sankaranarayanan, Ashok Veeraraghavan, Richard Baraniuk category:cs.CV published:2015-09-01 summary:FlatCam is a thin form-factor lensless camera that consists of a coded maskplaced on top of a bare, conventional sensor array. Unlike a traditional,lens-based camera where an image of the scene is directly recorded on thesensor pixels, each pixel in FlatCam records a linear combination of light frommultiple scene elements. A computational algorithm is then used to demultiplexthe recorded measurements and reconstruct an image of the scene. FlatCam is aninstance of a coded aperture imaging system; however, unlike the vast majorityof related work, we place the coded mask extremely close to the image sensorthat can enable a thin system. We employ a separable mask to ensure that bothcalibration and image reconstruction are scalable in terms of memoryrequirements and computational complexity. We demonstrate the potential of theFlatCam design using two prototypes: one at visible wavelengths and one atinfrared wavelengths.
arxiv-15000-202 | Analysis and approximation of some Shape-from-Shading models for non-Lambertian surfaces | http://arxiv.org/pdf/1502.05197v2.pdf | author:Silvia Tozza, Maurizio Falcone category:math.NA cs.CV cs.NA math.AP published:2015-02-18 summary:The reconstruction of a 3D object or a scene is a classical inverse problemin Computer Vision. In the case of a single image this is called theShape-from-Shading (SfS) problem and it is known to be ill-posed even in asimplified version like the vertical light source case. A huge number of worksdeals with the orthographic SfS problem based on the Lambertian reflectancemodel, the most common and simplest model which leads to an eikonal typeequation when the light source is on the vertical axis. In this paper we wantto study non-Lambertian models since they are more realistic and suitablewhenever one has to deal with different kind of surfaces, rough or specular. Wewill present a unified mathematical formulation of some popular orthographicnon-Lambertian models, considering vertical and oblique light directions aswell as different viewer positions. These models lead to more complexstationary nonlinear partial differential equations of Hamilton-Jacobi typewhich can be regarded as the generalization of the classical eikonal equationcorresponding to the Lambertian case. However, all the equations correspondingto the models considered here (Oren-Nayar and Phong) have a similar structureso we can look for weak solutions to this class in the viscosity solutionframework. Via this unified approach, we are able to develop a semi-Lagrangianapproximation scheme for the Oren-Nayar and the Phong model and to prove ageneral convergence result. Numerical simulations on synthetic and real imageswill illustrate the effectiveness of this approach and the main features of thescheme, also comparing the results with previous results in the literature.
arxiv-15000-203 | Unsupervised Learning in Neuromemristive Systems | http://arxiv.org/pdf/1601.07482v1.pdf | author:Cory Merkel, Dhireesha Kudithipudi category:cs.ET cs.LG stat.ML published:2016-01-27 summary:Neuromemristive systems (NMSs) currently represent the most promisingplatform to achieve energy efficient neuro-inspired computation. However, sincethe research field is less than a decade old, there are still countlessalgorithms and design paradigms to be explored within these systems. Oneparticular domain that remains to be fully investigated within NMSs isunsupervised learning. In this work, we explore the design of an NMS forunsupervised clustering, which is a critical element of several machinelearning algorithms. Using a simple memristor crossbar architecture andlearning rule, we are able to achieve performance which is on par with MATLAB'sk-means clustering.
arxiv-15000-204 | Shape Distributions of Nonlinear Dynamical Systems for Video-based Inference | http://arxiv.org/pdf/1601.07471v1.pdf | author:Vinay Venkataraman, Pavan Turaga category:cs.CV published:2016-01-27 summary:This paper presents a shape-theoretic framework for dynamical analysis ofnonlinear dynamical systems which appear frequently in several video-basedinference tasks. Traditional approaches to dynamical modeling have includedlinear and nonlinear methods with their respective drawbacks. A novel approachwe propose is the use of descriptors of the shape of the dynamical attractor asa feature representation of nature of dynamics. The proposed framework has twomain advantages over traditional approaches: a) representation of the dynamicalsystem is derived directly from the observational data, without any inherentassumptions, and b) the proposed features show stability under differenttime-series lengths where traditional dynamical invariants fail. We illustrateour idea using nonlinear dynamical models such as Lorenz and Rossler systems,where our feature representations (shape distribution) support our hypothesisthat the local shape of the reconstructed phase space can be used as adiscriminative feature. Our experimental analyses on these models also indicatethat the proposed framework show stability for different time-series lengths,which is useful when the available number of samples are small/variable. Thespecific applications of interest in this paper are: 1) activity recognitionusing motion capture and RGBD sensors, 2) activity quality assessment forapplications in stroke rehabilitation, and 3) dynamical scene classification.We provide experimental validation through action and gesture recognitionexperiments on motion capture and Kinect datasets. In all these scenarios, weshow experimental evidence of the favorable properties of the proposedrepresentation.
arxiv-15000-205 | Information-theoretic lower bounds on learning the structure of Bayesian networks | http://arxiv.org/pdf/1601.07460v1.pdf | author:Asish Ghoshal, Jean Honorio category:cs.LG cs.IT math.IT stat.ML published:2016-01-27 summary:In this paper, we study the information theoretic limits of learning thestructure of Bayesian networks from data. We show that for Bayesian networks oncontinuous as well as discrete random variables, there exists aparameterization of the Bayesian network such that, the minimum number ofsamples required to learn the "true" Bayesian network grows as$\mathcal{O}(m)$, where $m$ is the number of variables in the network. Further,for sparse Bayesian networks, where the number of parents of any variable inthe network is restricted to be at most $l$ for $l \ll m$, the minimum numberof samples required grows as $\mathcal{O}(l\log m)$. We discuss conditionsunder which these limits are achieved. For Bayesian networks over continuousvariables, we obtain results for Gaussian regression and Gumbel Bayesiannetworks. While for the discrete variables, we obtain results for Noisy-OR,Conditional Probability Table (CPT) based Bayesian networks and Logisticregression networks. Finally, as a byproduct, we also obtain lower bounds onthe sample complexity of feature selection in logistic regression and show thatthe bounds are sharp.
arxiv-15000-206 | Distributed User Association in Energy Harvesting Small Cell Networks: A Probabilistic Model | http://arxiv.org/pdf/1601.07795v1.pdf | author:Setareh Maghsudi, Ekram Hossain category:cs.IT cs.LG math.IT published:2016-01-27 summary:We consider a distributed downlink user association problem in a small cellnetwork, where small cells obtain the required energy for providing wirelessservices to users through ambient energy harvesting. Since energy harvesting isopportunistic in nature, the amount of harvested energy is a random variable,without any a priori known characteristics. Moreover, since users arrive in thenetwork randomly and require different wireless services, the energyconsumption is a random variable as well. In this paper, we propose aprobabilistic framework to mathematically model and analyze the random behaviorof energy harvesting and energy consumption in dense small cell networks.Furthermore, as acquiring (even statistical) channel and network knowledge isvery costly in a distributed dense network, we develop a bandit-theoreticalformulation for distributed user association when no information is availableat users
arxiv-15000-207 | Hierarchical Vector Autoregression | http://arxiv.org/pdf/1412.5250v2.pdf | author:William B. Nicholson, Jacob Bien, David S. Matteson category:stat.ME stat.CO stat.ML published:2014-12-17 summary:Vector autoregression (VAR) is a fundamental tool for modeling the jointdynamics of multivariate time series. However, as the number of componentseries is increased, the VAR model quickly becomes overparameterized, makingreliable estimation difficult and impeding its adoption as a forecasting toolin high dimensional settings. A number of authors have sought to address thisissue by incorporating regularized approaches, such as the lasso, that imposesparse or low-rank structures on the estimated coefficient parameters of theVAR. More traditional approaches attempt to address overparameterization byselecting a low lag order, based on the assumption that dynamic dependenceamong components is short-range. However, these methods typically assume asingle, universal lag order that applies across all components, unnecessarilyconstraining the dynamic relationship between the components and impedingforecast performance. The lasso-based approaches are more flexible but do notincorporate the notion of lag order selection. We propose a new class of regularized VAR models, called hierarchical vectorautoregression (HVAR), that embed the notion of lag selection into a convexregularizer. The key convex modeling tool is a group lasso with nested groupswhich ensure the sparsity pattern of autoregressive lag coefficients honors theordered structure inherent to VAR. We provide computationally efficientalgorithms for solving HVAR problems that can be parallelized across thecomponents. A simulation study shows the improved performance in forecastingand lag order selection over previous approaches, and a macroeconomicapplication further highlights forecasting improvements as well as theconvenient, interpretable output of a HVAR model.
arxiv-15000-208 | A First Attempt to Cloud-Based User Verification in Distributed System | http://arxiv.org/pdf/1601.07446v1.pdf | author:Marcin Wozniak, Dawid Polap, Grzegorz Borowik, Christian Napoli category:cs.NE cs.AI cs.CR cs.DC published:2016-01-27 summary:In this paper, the idea of client verification in distributed systems ispresented. The proposed solution presents a sample system where clientverification through cloud resources using input signature is discussed. Fordifferent signatures the proposed method has been examined. Research resultsare presented and discussed to show potential advantages.
arxiv-15000-209 | Quantum machine learning with glow for episodic tasks and decision games | http://arxiv.org/pdf/1601.07358v1.pdf | author:Jens Clausen, Hans J. Briegel category:quant-ph cs.AI cs.LG published:2016-01-27 summary:We consider a general class of models, where a reinforcement learning (RL)agent learns from cyclic interactions with an external environment viaclassical signals. Perceptual inputs are encoded as quantum states, which aresubsequently transformed by a quantum channel representing the agent's memory,while the outcomes of measurements performed at the channel's output determinethe agent's actions. The learning takes place via stepwise modifications of thechannel properties. They are described by an update rule that is inspired bythe projective simulation (PS) model and equipped with a glow mechanism thatallows for a backpropagation of policy changes, analogous to the eligibilitytraces in RL and edge glow in PS. In this way, the model combines features ofPS with the ability for generalization, offered by its physical embodiment as aquantum system. We apply the agent to various setups of an invasion game and agrid world, which serve as elementary model tasks allowing a direct comparisonwith a basic classical PS agent.
arxiv-15000-210 | Learning Model-Based Sparsity via Projected Gradient Descent | http://arxiv.org/pdf/1209.1557v4.pdf | author:Sohail Bahmani, Petros T. Boufounos, Bhiksha Raj category:stat.ML cs.LG math.OC 62FXX, 65KXX published:2012-09-07 summary:Several convex formulation methods have been proposed previously forstatistical estimation with structured sparsity as the prior. These methodsoften require a carefully tuned regularization parameter, often a cumbersome orheuristic exercise. Furthermore, the estimate that these methods produce mightnot belong to the desired sparsity model, albeit accurately approximating thetrue parameter. Therefore, greedy-type algorithms could often be more desirablein estimating structured-sparse parameters. So far, these greedy methods havemostly focused on linear statistical models. In this paper we study theprojected gradient descent with non-convex structured-sparse parameter model asthe constraint set. Should the cost function have a Stable Model-RestrictedHessian the algorithm produces an approximation for the desired minimizer. Asan example we elaborate on application of the main results to estimation inGeneralized Linear Model.
arxiv-15000-211 | Analysing domain shift factors between videos and images for object detection | http://arxiv.org/pdf/1501.01186v3.pdf | author:Vicky Kalogeiton, Vittorio Ferrari, Cordelia Schmid category:cs.CV published:2015-01-06 summary:Object detection is one of the most important challenges in computer vision.Object detectors are usually trained on bounding-boxes from still images.Recently, video has been used as an alternative source of data. Yet, for agiven test domain (image or video), the performance of the detector depends onthe domain it was trained on. In this paper, we examine the reasons behind thisperformance gap. We define and evaluate different domain shift factors: spatiallocation accuracy, appearance diversity, image quality and aspect distribution.We examine the impact of these factors by comparing performance before andafter factoring them out. The results show that all four factors affect theperformance of the detectors and their combined effect explains nearly thewhole performance gap.
arxiv-15000-212 | Unsupervised Semantic Parsing of Video Collections | http://arxiv.org/pdf/1506.08438v4.pdf | author:Ozan Sener, Amir Zamir, Silvio Savarese, Ashutosh Saxena category:cs.CV published:2015-06-28 summary:Human communication typically has an underlying structure. This is reflectedin the fact that in many user generated videos, a starting point, ending, andcertain objective steps between these two can be identified. In this paper, wepropose a method for parsing a video into such semantic steps in anunsupervised way. The proposed method is capable of providing a semantic"storyline" of the video composed of its objective steps. We accomplish thisusing both visual and language cues in a joint generative model. The proposedmethod can also provide a textual description for each of the identifiedsemantic steps and video segments. We evaluate this method on a large number ofcomplex YouTube videos and show results of unprecedented quality for thisintricate and impactful problem.
arxiv-15000-213 | Neighborhood Preserved Sparse Representation for Robust Classification on Symmetric Positive Definite Matrices | http://arxiv.org/pdf/1601.07336v1.pdf | author:Ming Yin, Shengli Xie, Yi Guo, Junbin Gao, Yun Zhang category:cs.CV published:2016-01-27 summary:Due to its promising classification performance, sparse representation basedclassification(SRC) algorithm has attracted great attention in the past fewyears. However, the existing SRC type methods apply only to vector data inEuclidean space. As such, there is still no satisfactory approach to conductclassification task for symmetric positive definite (SPD) matrices which isvery useful in computer vision. To address this problem, in this paper, aneighborhood preserved kernel SRC method is proposed on SPD manifolds.Specifically, by embedding the SPD matrices into a Reproducing Kernel HilbertSpace (RKHS), the proposed method can perform classification on SPD manifoldsthrough an appropriate Log-Euclidean kernel. Through exploiting the geodesicdistance between SPD matrices, our method can effectively characterize theintrinsic local Riemannian geometry within data so as to well unravel theunderlying sub-manifold structure. Despite its simplicity, experimental resultson several famous database demonstrate that the proposed method achieves betterclassification results than the state-of-the-art approaches.
arxiv-15000-214 | Performance measures for classification systems with rejection | http://arxiv.org/pdf/1504.02763v2.pdf | author:Filipe Condessa, Jelena Kovacevic, Jose Bioucas-Dias category:cs.CV cs.LG 68-04 published:2015-04-10 summary:Classifiers with rejection are essential in real-world applications wheremisclassifications and their effects are critical. However, if no problemspecific cost function is defined, there are no established measures to assessthe performance of such classifiers. We introduce a set of desired propertiesfor performance measures for classifiers with rejection, based on which wepropose a set of three performance measures for the evaluation of theperformance of classifiers with rejection that satisfy the desired properties.The nonrejected accuracy measures the ability of the classifier to accuratelyclassify nonrejected samples; the classification quality measures the correctdecision making of the classifier with rejector; and the rejection qualitymeasures the ability to concentrate all misclassified samples onto the set ofrejected samples. From the measures, we derive the concept of relativeoptimality that allows us to connect the measures to a family of cost functionsthat take into account the trade-off between rejection and misclassification.We illustrate the use of the proposed performance measures on classifiers withrejection applied to synthetic and real-world data.
arxiv-15000-215 | The Computational Power of Optimization in Online Learning | http://arxiv.org/pdf/1504.02089v4.pdf | author:Elad Hazan, Tomer Koren category:cs.LG cs.GT published:2015-04-08 summary:We consider the fundamental problem of prediction with expert advice wherethe experts are "optimizable": there is a black-box optimization oracle thatcan be used to compute, in constant time, the leading expert in retrospect atany point in time. In this setting, we give a novel online algorithm thatattains vanishing regret with respect to $N$ experts in total$\widetilde{O}(\sqrt{N})$ computation time. We also give a lower bound showingthat this running time cannot be improved (up to log factors) in the oraclemodel, thereby exhibiting a quadratic speedup as compared to the standard,oracle-free setting where the required time for vanishing regret is$\widetilde{\Theta}(N)$. These results demonstrate an exponential gap betweenthe power of optimization in online learning and its power in statisticallearning: in the latter, an optimization oracle---i.e., an efficient empiricalrisk minimizer---allows to learn a finite hypothesis class of size $N$ in time$O(\log{N})$. We also study the implications of our results to learning inrepeated zero-sum games, in a setting where the players have access to oraclesthat compute, in constant time, their best-response to any mixed strategy oftheir opponent. We show that the runtime required for approximating the minimaxvalue of the game in this setting is $\widetilde{\Theta}(\sqrt{N})$, yieldingagain a quadratic improvement upon the oracle-free setting, where$\widetilde{\Theta}(N)$ is known to be tight.
arxiv-15000-216 | Comprehensive Feature-based Robust Video Fingerprinting Using Tensor Model | http://arxiv.org/pdf/1601.07270v1.pdf | author:Xiushan Nie, Yilong Yin, Jiande Sun category:cs.CV published:2016-01-27 summary:Content-based near-duplicate video detection (NDVD) is essential foreffective search and retrieval, and robust video fingerprinting is a goodsolution for NDVD. Most existing video fingerprinting methods use a singlefeature or concatenating different features to generate video fingerprints, andshow a good performance under single-mode modifications such as noise additionand blurring. However, when they suffer combined modifications, the performanceis degraded to a certain extent because such features cannot characterize thevideo content completely. By contrast, the assistance and consensus amongdifferent features can improve the performance of video fingerprinting.Therefore, in the present study, we mine the assistance and consensus amongdifferent features based on tensor model, and present a new comprehensivefeature to fully use them in the proposed video fingerprinting framework. Wealso analyze what the comprehensive feature really is for representing theoriginal video. In this framework, the video is initially set as a high-ordertensor that consists of different features, and the video tensor is decomposedvia the Tucker model with a solution that determines the number of components.Subsequently, the comprehensive feature is generated by the low-order tensorobtained from tensor decomposition. Finally, the video fingerprint is computedusing this feature. A matching strategy used for narrowing the search is alsoproposed based on the core tensor. The robust video fingerprinting framework isresistant not only to single-mode modifications, but also to the combination ofthem.
arxiv-15000-217 | Deep Learning Driven Visual Path Prediction from a Single Image | http://arxiv.org/pdf/1601.07265v1.pdf | author:Siyu Huang, Xi Li, Zhongfei Zhang, Zhouzhou He, Fei Wu, Wei Liu, Jinhui Tang, Yueting Zhuang category:cs.CV published:2016-01-27 summary:Capabilities of inference and prediction are significant components of visualsystems. In this paper, we address an important and challenging task of them:visual path prediction. Its goal is to infer the future path for a visualobject in a static scene. This task is complicated as it needs high-levelsemantic understandings of both the scenes and motion patterns underlying videosequences. In practice, cluttered situations have also raised higher demands onthe effectiveness and robustness of the considered models. Motivated by theseobservations, we propose a deep learning framework which simultaneouslyperforms deep feature learning for visual representation in conjunction withspatio-temporal context modeling. After that, we propose a unified pathplanning scheme to make accurate future path prediction based on the analyticresults of the context models. The highly effective visual representation anddeep context models ensure that our framework makes a deep semanticunderstanding of the scene and motion pattern, consequently improving theperformance of the visual path prediction task. In order to comprehensivelyevaluate the model's performance on the visual path prediction task, weconstruct two large benchmark datasets from the adaptation of video trackingdatasets. The qualitative and quantitative experimental results show that ourapproach outperforms the existing approaches and owns a better generalizationcapability.
arxiv-15000-218 | Fast Integral Image Estimation at 1% measurement rate | http://arxiv.org/pdf/1601.07258v1.pdf | author:Kuldeep Kulkarni, Pavan Turaga category:cs.CV math.OC published:2016-01-27 summary:We propose a framework called ReFInE to directly obtain integral imageestimates from a very small number of spatially multiplexed measurements of thescene without iterative reconstruction of any auxiliary image, and demonstratetheir practical utility in visual object tracking. Specifically, we designmeasurement matrices which are tailored to facilitate extremely fast estimationof the integral image, by using a single-shot linear operation on the measuredvector. Leveraging a prior model for the images, we formulate a nuclear normminimization problem with second order conic constraints to jointly obtain themeasurement matrix and the linear operator. Through qualitative andquantitative experiments, we show that high quality integral image estimatescan be obtained using our framework at very low measurement rates. Further, ona standard dataset of 50 videos, we present object tracking results which arecomparable to the state-of-the-art methods, even at an extremely lowmeasurement rate of 1%.
arxiv-15000-219 | PersonNet: Person Re-identification with Deep Convolutional Neural Networks | http://arxiv.org/pdf/1601.07255v1.pdf | author:Lin Wu, Chunhua Shen, Anton van den Hengel category:cs.CV published:2016-01-27 summary:In this paper, we propose a deep end-to-end neu- ral network tosimultaneously learn high-level features and a corresponding similarity metricfor person re-identification. The network takes a pair of raw RGB images asinput, and outputs a similarity value indicating whether the two input imagesdepict the same person. A layer of computing neighborhood range differencesacross two input images is employed to capture local relationship betweenpatches. This operation is to seek a robust feature from input images. Byincreasing the depth to 10 weight layers and using very small (3$\times$3)convolution filters, our architecture achieves a remarkable improvement on theprior-art configurations. Meanwhile, an adaptive Root- Mean-Square (RMSProp)gradient decent algorithm is integrated into our architecture, which isbeneficial to deep nets. Our method consistently outperforms state-of-the-arton two large datasets (CUHK03 and Market-1501), and a medium-sized data set(CUHK01).
arxiv-15000-220 | Font Identification in Historical Documents Using Active Learning | http://arxiv.org/pdf/1601.07252v1.pdf | author:Anshul Gupta, Ricardo Gutierrez-Osuna, Matthew Christy, Richard Furuta, Laura Mandell category:cs.CV cs.AI cs.DL stat.AP stat.ML I.5; I.2 published:2016-01-27 summary:Identifying the type of font (e.g., Roman, Blackletter) used in historicaldocuments can help optical character recognition (OCR) systems produce moreaccurate text transcriptions. Towards this end, we present an active-learningstrategy that can significantly reduce the number of labeled samples needed totrain a font classifier. Our approach extracts image-based features thatexploit geometric differences between fonts at the word level, and combinesthem into a bag-of-word representation for each page in a document. We evaluatesix sampling strategies based on uncertainty, dissimilarity and diversitycriteria, and test them on a database containing over 3,000 historicaldocuments with Blackletter, Roman and Mixed fonts. Our results show that acombination of uncertainty and diversity achieves the highest predictiveaccuracy (89% of test cases correctly classified) while requiring only a smallfraction of the data (17%) to be labeled. We discuss the implications of thisresult for mass digitization projects of historical documents.
arxiv-15000-221 | GraphConnect: A Regularization Framework for Neural Networks | http://arxiv.org/pdf/1512.06757v2.pdf | author:Jiaji Huang, Qiang Qiu, Robert Calderbank, Guillermo Sapiro category:cs.CV cs.LG cs.NE published:2015-12-21 summary:Deep neural networks have proved very successful in domains where largetraining sets are available, but when the number of training samples is small,their performance suffers from overfitting. Prior methods of reducingoverfitting such as weight decay, Dropout and DropConnect are data-independent.This paper proposes a new method, GraphConnect, that is data-dependent, and ismotivated by the observation that data of interest lie close to a manifold. Thenew method encourages the relationships between the learned decisions toresemble a graph representing the manifold structure. Essentially GraphConnectis designed to learn attributes that are present in data samples in contrast toweight decay, Dropout and DropConnect which are simply designed to make it moredifficult to fit to random error or noise. Empirical Rademacher complexity isused to connect the generalization error of the neural network to spectralproperties of the graph learned from the input data. This framework is used toshow that GraphConnect is superior to weight decay. Experimental results onseveral benchmark datasets validate the theoretical analysis, and show thatwhen the number of training samples is small, GraphConnect is able tosignificantly improve performance over weight decay.
arxiv-15000-222 | On the Sample Complexity of Learning Sparse Graphical Games | http://arxiv.org/pdf/1601.07243v1.pdf | author:Jean Honorio category:cs.GT cs.LG stat.ML published:2016-01-27 summary:We analyze the sample complexity of learning sparse graphical games frompurely behavioral data. That is, we assume that we can only observe theplayers' joint actions and not their payoffs. We analyze the sufficient andnecessary number of samples for the correct recovery of the set ofpure-strategy Nash equilibria (PSNE) of the true game. Our analysis focuses onsparse directed graphs with $n$ nodes and at most $k$ parents per node. We showthat if the number of samples is greater than ${O(k n \log^2{n})}$, thenmaximum likelihood estimation correctly recovers the PSNE with highprobability. We also show that if the number of samples is less than ${O(k n\log^2{n})}$, then any conceivable method fails to recover the PSNE witharbitrary probability.
arxiv-15000-223 | Predicting Drug Interactions and Mutagenicity with Ensemble Classifiers on Subgraphs of Molecules | http://arxiv.org/pdf/1601.07233v1.pdf | author:Andrew Schaumberg, Angela Yu, Tatsuhiro Koshi, Xiaochan Zong, Santoshkalyan Rayadhurgam category:stat.ML cs.LG I.2.1; J.3 published:2016-01-27 summary:In this study, we intend to solve a mutual information problem in interactingmolecules of any type, such as proteins, nucleic acids, and small molecules.Using machine learning techniques, we accurately predict pairwise interactions,which can be of medical and biological importance. Graphs are are useful inthis problem for their generality to all types of molecules, due to theinherent association of atoms through atomic bonds. Subgraphs can representdifferent molecular domains. These domains can be biologically significant asmost molecules only have portions that are of functional significance and caninteract with other domains. Thus, we use subgraphs as features in differentmachine learning algorithms to predict if two drugs interact and predictpotential single molecule effects.
arxiv-15000-224 | A network that learns Strassen multiplication | http://arxiv.org/pdf/1601.07227v1.pdf | author:Veit Elser category:math.NA cs.NE published:2016-01-26 summary:We study neural networks whose only non-linear components are multipliers, totest a new training rule in a context where the precise representation of datais paramount. These networks are challenged to discover the rules of matrixmultiplication, given many examples. By limiting the number of multipliers, thenetwork is forced to discover the Strassen multiplication rules. This is themathematical equivalent of finding low rank decompositions of the $n\times n$matrix multiplication tensor, $M_n$. We train these networks with theconservative learning rule, which makes minimal changes to the weights so as togive the correct output for each input at the time the input-output pair isreceived. Conservative learning needs a few thousand examples to find the rank7 decomposition of $M_2$, and $10^5$ for the rank 23 decomposition of $M_3$(the lowest known). High precision is critical, especially for $M_3$, todiscriminate between true decompositions and "border approximations".
arxiv-15000-225 | Recurrent Neural Network Postfilters for Statistical Parametric Speech Synthesis | http://arxiv.org/pdf/1601.07215v1.pdf | author:Prasanna Kumar Muthukumar, Alan W Black category:cs.CL published:2016-01-26 summary:In the last two years, there have been numerous papers that have looked intousing Deep Neural Networks to replace the acoustic model in traditionalstatistical parametric speech synthesis. However, far less attention has beenpaid to approaches like DNN-based postfiltering where DNNs work in conjunctionwith traditional acoustic models. In this paper, we investigate the use ofRecurrent Neural Networks as a potential postfilter for synthesis. We explorethe possibility of replacing existing postfilters, as well as highlight theease with which arbitrary new features can be added as input to the postfilter.We also tried a novel approach of jointly training the Classification AndRegression Tree and the postfilter, rather than the traditional approach oftraining them independently.
arxiv-15000-226 | Sentence Directed Video Object Codetection | http://arxiv.org/pdf/1506.02059v2.pdf | author:Haonan Yu, Jeffrey Mark Siskind category:cs.CV published:2015-06-05 summary:We tackle the problem of video object codetection by leveraging the weaksemantic constraint implied by sentences that describe the video content.Unlike most existing work that focuses on codetecting large objects which areusually salient both in size and appearance, we can codetect objects that aresmall or medium sized. Our method assumes no human pose or depth informationsuch as is required by the most recent state-of-the-art method. We employ weaksemantic constraint on the codetection process by pairing the video withsentences. Although the semantic information is usually simple and weak, it cangreatly boost the performance of our codetection framework by reducing thesearch space of the hypothesized object detections. Our experiment demonstratesan average IoU score of 0.423 on a new challenging dataset which contains 15object classes and 150 videos with 12,509 frames in total, and an average IoUscore of 0.373 on a subset of an existing dataset, originally intended foractivity recognition, which contains 5 object classes and 75 videos with 8,854frames in total.
arxiv-15000-227 | Stacked Attention Networks for Image Question Answering | http://arxiv.org/pdf/1511.02274v2.pdf | author:Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Smola category:cs.LG cs.CL cs.CV cs.NE published:2015-11-07 summary:This paper presents stacked attention networks (SANs) that learn to answernatural language questions from images. SANs use semantic representation of aquestion as query to search for the regions in an image that are related to theanswer. We argue that image question answering (QA) often requires multiplesteps of reasoning. Thus, we develop a multiple-layer SAN in which we query animage multiple times to infer the answer progressively. Experiments conductedon four image QA data sets demonstrate that the proposed SANs significantlyoutperform previous state-of-the-art approaches. The visualization of theattention layers illustrates the progress that the SAN locates the relevantvisual clues that lead to the answer of the question layer-by-layer.
arxiv-15000-228 | Globally Optimal Cell Tracking using Integer Programming | http://arxiv.org/pdf/1501.05499v2.pdf | author:Engin Türetken, Xinchao Wang, Carlos Becker, Carsten Haubold, Pascal Fua category:cs.CV published:2015-01-22 summary:We propose a novel approach to automatically tracking cell populations intime-lapse images. To account for cell occlusions and overlaps, we introduce arobust method that generates an over-complete set of competing detectionhypotheses. We then perform detection and tracking simultaneously on thesehypotheses by solving to optimality an integer program with only one type offlow variables. This eliminates the need for heuristics to handle misseddetections due to occlusions and complex morphology. We demonstrate theeffectiveness of our approach on a range of challenging sequences consisting ofclumped cells and show that it outperforms state-of-the-art techniques.
arxiv-15000-229 | COCO-Text: Dataset and Benchmark for Text Detection and Recognition in Natural Images | http://arxiv.org/pdf/1601.07140v1.pdf | author:Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, Serge Belongie category:cs.CV published:2016-01-26 summary:This paper describes the COCO-Text dataset. In recent years large-scaledatasets like SUN and Imagenet drove the advancement of scene understanding andobject recognition. The goal of COCO-Text is to advance state-of-the-art intext detection and recognition in natural images. The dataset is based on theMS COCO dataset, which contains images of complex everyday scenes. The imageswere not collected with text in mind and thus contain a broad variety of textinstances. To reflect the diversity of text in natural scenes, we annotate textwith (a) location in terms of a bounding box, (b) fine-grained classificationinto machine printed text and handwritten text, (c) classification into legibleand illegible text, (d) script of the text and (e) transcriptions of legibletext. The dataset contains over 173k text annotations in over 63k images. Weprovide a statistical analysis of the accuracy of our annotations. In addition,we present an analysis of three leading state-of-the-art photo OpticalCharacter Recognition (OCR) approaches on our dataset. While scene textdetection and recognition enjoys strong advances in recent years, we identifysignificant shortcomings motivating future work.
arxiv-15000-230 | A Normative Theory of Adaptive Dimensionality Reduction in Neural Networks | http://arxiv.org/pdf/1511.09426v2.pdf | author:Cengiz Pehlevan, Dmitri B. Chklovskii category:q-bio.NC cs.NE published:2015-11-30 summary:To make sense of the world our brains must analyze high-dimensional datasetsstreamed by our sensory organs. Because such analysis begins withdimensionality reduction, modelling early sensory processing requiresbiologically plausible online dimensionality reduction algorithms. Recently, wederived such an algorithm, termed similarity matching, from a MultidimensionalScaling (MDS) objective function. However, in the existing algorithm, thenumber of output dimensions is set a priori by the number of output neurons andcannot be changed. Because the number of informative dimensions in sensoryinputs is variable there is a need for adaptive dimensionality reduction. Here,we derive biologically plausible dimensionality reduction algorithms whichadapt the number of output dimensions to the eigenspectrum of the inputcovariance matrix. We formulate three objective functions which, in the offlinesetting, are optimized by the projections of the input dataset onto itsprincipal subspace scaled by the eigenvalues of the output covariance matrix.In turn, the output eigenvalues are computed as i) soft-thresholded, ii)hard-thresholded, iii) equalized thresholded eigenvalues of the inputcovariance matrix. In the online setting, we derive the three correspondingadaptive algorithms and map them onto the dynamics of neuronal activity innetworks with biologically plausible local learning rules. Remarkably, in thelast two networks, neurons are divided into two classes which we identify withprincipal neurons and interneurons in biological circuits.
arxiv-15000-231 | LIA-RAG: a system based on graphs and divergence of probabilities applied to Speech-To-Text Summarization | http://arxiv.org/pdf/1601.07124v1.pdf | author:Elvys Linhares Pontes, Juan-Manuel Torres-Moreno, Andréa Carneiro Linhares category:cs.CL cs.IR published:2016-01-26 summary:This paper aims to introduces a new algorithm for automatic speech-to-textsummarization based on statistical divergences of probabilities and graphs. Theinput is a text from speech conversations with noise, and the output a compacttext summary. Our results, on the pilot task CCCS Multiling 2015 French corpusare very encouraging
arxiv-15000-232 | Supersparse Linear Integer Models for Optimized Medical Scoring Systems | http://arxiv.org/pdf/1502.04269v3.pdf | author:Berk Ustun, Cynthia Rudin category:stat.ML cs.DM cs.LG stat.AP stat.ME published:2015-02-15 summary:Scoring systems are linear classification models that only require users toadd, subtract and multiply a few small numbers in order to make a prediction.These models are in widespread use by the medical community, but are difficultto learn from data because they need to be accurate and sparse, have coprimeinteger coefficients, and satisfy multiple operational constraints. We presenta new method for creating data-driven scoring systems called a SupersparseLinear Integer Model (SLIM). SLIM scoring systems are built by solving aninteger program that directly encodes measures of accuracy (the 0-1 loss) andsparsity (the $\ell_0$-seminorm) while restricting coefficients to coprimeintegers. SLIM can seamlessly incorporate a wide range of operationalconstraints related to accuracy and sparsity, and can produce highly tailoredmodels without parameter tuning. We provide bounds on the testing and trainingaccuracy of SLIM scoring systems, and present a new data reduction techniquethat can improve scalability by eliminating a portion of the training databeforehand. Our paper includes results from a collaboration with theMassachusetts General Hospital Sleep Laboratory, where SLIM was used to createa highly tailored scoring system for sleep apnea screening
arxiv-15000-233 | What can we learn about CNNs from a large scale controlled object dataset? | http://arxiv.org/pdf/1512.01320v2.pdf | author:Ali Borji, Saeed Izadi, Laurent Itti category:cs.CV published:2015-12-04 summary:Tolerance to image variations (e.g. translation, scale, pose, illumination)is an important desired property of any object recognition system, be it humanor machine. Moving towards increasingly bigger datasets has been trending incomputer vision specially with the emergence of highly popular deep learningmodels. While being very useful for learning invariance to object inter- andintra-class shape variability, these large-scale wild datasets are not veryuseful for learning invariance to other parameters forcing researchers toresort to other tricks for training a model. In this work, we introduce alarge-scale synthetic dataset, which is freely and publicly available, and useit to answer several fundamental questions regarding invariance and selectivityproperties of convolutional neural networks. Our dataset contains two parts: a)objects shot on a turntable: 16 categories, 8 rotation angles, 11 cameras on asemicircular arch, 5 lighting conditions, 3 focus levels, variety ofbackgrounds (23.4 per instance) generating 1320 images per instance (over 20million images in total), and b) scenes: in which a robot arm takes pictures ofobjects on a 1:160 scale scene. We study: 1) invariance and selectivity ofdifferent CNN layers, 2) knowledge transfer from one object category toanother, 3) systematic or random sampling of images to build a train set, 4)domain adaptation from synthetic to natural scenes, and 5) order of knowledgedelivery to CNNs. We also explore how our analyses can lead the field todevelop more efficient CNNs.
arxiv-15000-234 | Polyhedron Volume-Ratio-based Classification for Image Recognition | http://arxiv.org/pdf/1601.07021v1.pdf | author:Qingxiang Feng, Jeng-Shyang Pan, Jar-Ferr Yang, Yang-Ting Chou category:cs.CV published:2016-01-26 summary:In this paper, a novel method, called polyhedron volume ratio classification(PVRC) is proposed for image recognition
arxiv-15000-235 | Diffusion tensor imaging with deterministic error bounds | http://arxiv.org/pdf/1509.02223v2.pdf | author:Artur Gorokh, Yury Korolev, Tuomo Valkonen category:cs.CV cs.NA published:2015-09-07 summary:Errors in the data and the forward operator of an inverse problem can behandily modelled using partial order in Banach lattices. We present someexisting results of the theory of regularisation in this novel framework, whereerrors are represented as bounds by means of the appropriate partial order. We apply the theory to Diffusion Tensor Imaging, where correct noisemodelling is challenging: it involves the Rician distribution and the nonlinearStejskal-Tanner equation. Linearisation of the latter in the statisticalframework would complicate the noise model even further. We avoid this usingthe error bounds approach, which preserves simple error structure undermonotone transformations.
arxiv-15000-236 | Virtual Rephotography: Novel View Prediction Error for 3D Reconstruction | http://arxiv.org/pdf/1601.06950v1.pdf | author:Michael Waechter, Mate Beljan, Simon Fuhrmann, Nils Moehrle, Johannes Kopf, Michael Goesele category:cs.CV cs.GR I.3.7 published:2016-01-26 summary:The ultimate goal of many image-based modeling systems is to renderphoto-realistic novel views of a scene without visible artifacts. Existingevaluation metrics and benchmarks focus mainly on the geometric accuracy of thereconstructed model, which is, however, a poor predictor of visual accuracy.Furthermore, using only geometric accuracy by itself does not allow evaluatingsystems that either lack a geometric scene representation or utilize coarseproxy geometry. Examples include light field or image-based rendering systems.We propose a unified evaluation approach based on novel view prediction errorthat is able to analyze the visual quality of any method that can render novelviews from input images. One of the key advantages of this approach is that itdoes not require ground truth geometry. This dramatically simplifies thecreation of test datasets and benchmarks. It also allows us to evaluate thequality of an unknown scene during the acquisition and reconstruction process,which is useful for acquisition planning. We evaluate our approach on a rangeof methods including standard geometry-plus-texture pipelines as well asimage-based rendering techniques, compare it to existing geometry-basedbenchmarks, and demonstrate its utility for a range of use cases.
arxiv-15000-237 | DeePM: A Deep Part-Based Model for Object Detection and Semantic Part Localization | http://arxiv.org/pdf/1511.07131v3.pdf | author:Jun Zhu, Xianjie Chen, Alan L. Yuille category:cs.CV published:2015-11-23 summary:In this paper, we propose a deep part-based model (DeePM) for symbioticobject detection and semantic part localization. For this purpose, we annotatesemantic parts for all 20 object categories on the PASCAL VOC 2012 dataset,which provides information on object pose, occlusion, viewpoint andfunctionality. DeePM is a latent graphical model based on the state-of-the-artR-CNN framework, which learns an explicit representation of the object-partconfiguration with flexible type sharing (e.g., a sideview horse head can beshared by a fully-visible sideview horse and a highly truncated sideview horsewith head and neck only). For comparison, we also present an end-to-endObject-Part (OP) R-CNN which learns an implicit feature representation forjointly mapping an image ROI to the object and part bounding boxes. We evaluatethe proposed methods for both the object and part detection performance onPASCAL VOC 2012, and show that DeePM consistently outperforms OP R-CNN indetecting objects and parts. In addition, it obtains superior performance toFast and Faster R-CNNs in object detection.
arxiv-15000-238 | A Novel Memetic Feature Selection Algorithm | http://arxiv.org/pdf/1601.06933v1.pdf | author:Mohadeseh Montazeri, Hamid Reza Naji, Mitra Montazeri, Ahmad Faraahi category:cs.LG published:2016-01-26 summary:Feature selection is a problem of finding efficient features among allfeatures in which the final feature set can improve accuracy and reducecomplexity. In feature selection algorithms search strategies are key aspects.Since feature selection is an NP-Hard problem; therefore heuristic algorithmshave been studied to solve this problem. In this paper, we have proposed amethod based on memetic algorithm to find an efficient feature subset for aclassification problem. It incorporates a filter method in the geneticalgorithm to improve classification performance and accelerates the search inidentifying core feature subsets. Particularly, the method adds or deletes afeature from a candidate feature subset based on the multivariate featureinformation. Empirical study on commonly data sets of the university ofCalifornia, Irvine shows that the proposed method outperforms existing methods.
arxiv-15000-239 | Fisher Motion Descriptor for Multiview Gait Recognition | http://arxiv.org/pdf/1601.06931v1.pdf | author:F. M. Castro, M. J. Marín-Jiménez, N. Guil, R. Muñoz-Salinas category:cs.CV cs.AI published:2016-01-26 summary:The goal of this paper is to identify individuals by analyzing their gait.Instead of using binary silhouettes as input data (as done in many previousworks) we propose and evaluate the use of motion descriptors based on denselysampled short-term trajectories. We take advantage of state-of-the-art peopledetectors to define custom spatial configurations of the descriptors around thetarget person, obtaining a rich representation of the gait motion. The localmotion features (described by the Divergence-Curl-Shear descriptor) extractedon the different spatial areas of the person are combined into a singlehigh-level gait descriptor by using the Fisher Vector encoding. The proposedapproach, coined Pyramidal Fisher Motion, is experimentally validated on`CASIA' dataset (parts B and C), `TUM GAID' dataset, `CMU MoBo' dataset and therecent `AVA Multiview Gait' dataset. The results show that this new approachachieves state-of-the-art results in the problem of gait recognition, allowingto recognize walking people from diverse viewpoints on single and multiplecamera setups, wearing different clothes, carrying bags, walking at diversespeeds and not limited to straight walking paths.
arxiv-15000-240 | Classification and Verification of Online Handwritten Signatures with Time Causal Information Theory Quantifiers | http://arxiv.org/pdf/1601.06925v1.pdf | author:Osvaldo A. Rosso, Raydonal Ospina, Alejandro C. Frery category:cs.IT cs.CV math.IT published:2016-01-26 summary:We present a new approach for online handwritten signature classification andverification based on descriptors stemming from Information Theory. Theproposal uses the Shannon Entropy, the Statistical Complexity, and the FisherInformation evaluated over the Bandt and Pompe symbolization of the horizontaland vertical coordinates of signatures. These six features are easy and fast tocompute, and they are the input to an One-Class Support Vector Machineclassifier. The results produced surpass state-of-the-art techniques thatemploy higher-dimensional feature spaces which often require specializedsoftware and hardware. We assess the consistency of our proposal with respectto the size of the training sample, and we also use it to classify thesignatures into meaningful groups.
arxiv-15000-241 | Functional archetype and archetypoid analysis | http://arxiv.org/pdf/1601.06911v1.pdf | author:Irene Epifanio category:stat.ME stat.AP stat.ML published:2016-01-26 summary:Archetype and archetypoid analysis can be extended to functional data. Eachfunction is represented as a mixture of actual observations (functionalarchetypoids) or functional archetypes, which are a mixture of observations inthe data set. Well-known Canadian temperature data are used to illustrate theanalysis developed. Computational methods are proposed for performing these analyses, based onthe coefficients of a basis. They are compared with other alternatives in asimulation study using a well-known curve discrimination problem, achievingbetter or similar performance. Unlike a previous attempt to compute functionalarchetypes, which was only valid for an orthogonal basis, the proposedmethodology can be used for any basis. It is computationally less demandingthan the simple approach of discretizing the functions. Multivariate functionalarchetype and archetypoid analysis are also introduced and applied in aninteresting problem about the study of human development around the world overthe last 50 years. These tools can contribute to the understanding of afunctional data set, as in the multivariate case.
arxiv-15000-242 | Minimax Structured Normal Means Inference | http://arxiv.org/pdf/1506.07902v2.pdf | author:Akshay Krishnamurthy category:stat.ML cs.IT math.IT published:2015-06-25 summary:We provide a unified treatment of a broad class of noisy structure recoveryproblems, known as structured normal means problems. In this setting, the goalis to identify, from a finite collection of Gaussian distributions withdifferent means, the distribution that produced some observed data. Recent workhas studied several special cases including sparse vectors, biclusters, andgraph-based structures. We establish nearly matching upper and lower bounds onthe minimax probability of error for any structured normal means problem, andwe derive an optimality certificate for the maximum likelihood estimator, whichcan be applied to many instantiations. We also consider an experimental designsetting, where we generalize our minimax bounds and derive an algorithm forcomputing a design strategy with a certain optimality property. We show thatour results give tight minimax bounds for many structure recovery problems andconsider some consequences for interactive sampling.
arxiv-15000-243 | Survey on the attention based RNN model and its applications in computer vision | http://arxiv.org/pdf/1601.06823v1.pdf | author:Feng Wang, David M. J. Tax category:cs.CV cs.LG published:2016-01-25 summary:The recurrent neural networks (RNN) can be used to solve the sequence tosequence problem, where both the input and the output have sequentialstructures. Usually there are some implicit relations between the structures.However, it is hard for the common RNN model to fully explore the relationsbetween the sequences. In this survey, we introduce some attention based RNNmodels which can focus on different parts of the input for each output item, inorder to explore and take advantage of the implicit relations between the inputand the output items. The different attention mechanisms are described indetail. We then introduce some applications in computer vision which apply theattention based RNN models. The superiority of the attention based RNN model isshown by the experimental results. At last some future research directions aregiven.
arxiv-15000-244 | Very Efficient Training of Convolutional Neural Networks using Fast Fourier Transform and Overlap-and-Add | http://arxiv.org/pdf/1601.06815v1.pdf | author:Tyler Highlander, Andres Rodriguez category:cs.NE cs.LG published:2016-01-25 summary:Convolutional neural networks (CNNs) are currently state-of-the-art forvarious classification tasks, but are computationally expensive. Propagatingthrough the convolutional layers is very slow, as each kernel in each layermust sequentially calculate many dot products for a single forward and backwardpropagation which equates to $\mathcal{O}(N^{2}n^{2})$ per kernel per layerwhere the inputs are $N \times N$ arrays and the kernels are $n \times n$arrays. Convolution can be efficiently performed as a Hadamard product in thefrequency domain. The bottleneck is the transformation which has a cost of$\mathcal{O}(N^{2}\log_2 N)$ using the fast Fourier transform (FFT). However,the increase in efficiency is less significant when $N\gg n$ as is the case inCNNs. We mitigate this by using the "overlap-and-add" technique reducing thecomputational complexity to $\mathcal{O}(N^2\log_2 n)$ per kernel. This methodincreases the algorithm's efficiency in both the forward and backwardpropagation, reducing the training and testing time for CNNs. Our empiricalresults show our method reduces computational time by a factor of up to 16.3times the traditional convolution implementation for a 8 $\times$ 8 kernel anda 224 $\times$ 224 image.
arxiv-15000-245 | Emerging Dimension Weights in a Conceptual Spaces Model of Concept Combination | http://arxiv.org/pdf/1601.06763v1.pdf | author:Martha Lewis, Jonathan Lawry category:cs.AI cs.CL cs.MA published:2016-01-25 summary:We investigate the generation of new concepts from combinations of propertiesas an artificial language develops. To do so, we have developed a new frameworkfor conjunctive concept combination. This framework gives a semantic groundingto the weighted sum approach to concept combination seen in the literature. Weimplement the framework in a multi-agent simulation of language evolution andshow that shared combination weights emerge. The expected value and thevariance of these weights across agents may be predicted from the distributionof elements in the conceptual space, as determined by the underlyingenvironment, together with the rate at which agents adopt others' concepts.When this rate is smaller, the agents are able to converge to weights withlower variance. However, the time taken to converge to a steady statedistribution of weights is longer.
arxiv-15000-246 | Fast Robust PCA on Graphs | http://arxiv.org/pdf/1507.08173v2.pdf | author:Nauman Shahid, Nathanael Perraudin, Vassilis Kalofolias, Gilles Puy, Pierre Vandergheynst category:cs.CV published:2015-07-29 summary:Mining useful clusters from high dimensional data has received significantattention of the computer vision and pattern recognition community in therecent years. Linear and non-linear dimensionality reduction has played animportant role to overcome the curse of dimensionality. However, often suchmethods are accompanied with three different problems: high computationalcomplexity (usually associated with the nuclear norm minimization),non-convexity (for matrix factorization methods) and susceptibility to grosscorruptions in the data. In this paper we propose a principal componentanalysis (PCA) based solution that overcomes these three issues andapproximates a low-rank recovery method for high dimensional datasets. Wetarget the low-rank recovery by enforcing two types of graph smoothnessassumptions, one on the data samples and the other on the features by designinga convex optimization problem. The resulting algorithm is fast, efficient andscalable for huge datasets with O(nlog(n)) computational complexity in thenumber of data samples. It is also robust to gross corruptions in the datasetas well as to the model parameters. Clustering experiments on 7 benchmarkdatasets with different types of corruptions and background separationexperiments on 3 video datasets show that our proposed model outperforms 10state-of-the-art dimensionality reduction models. Our theoretical analysisproves that the proposed model is able to recover approximate low-rankrepresentations with a bounded error for clusterable data.
arxiv-15000-247 | The Utility of Hedged Assertions in the Emergence of Shared Categorical Labels | http://arxiv.org/pdf/1601.06755v1.pdf | author:Martha Lewis, Jonathan Lawry category:cs.AI cs.CL cs.MA published:2016-01-25 summary:We investigate the emergence of shared concepts in a community of languageusers using a multi-agent simulation. We extend results showing that negatedassertions are of use in developing shared categories, to include assertionsmodified by linguistic hedges. Results show that using hedged assertionspositively affects the emergence of shared categories in two distinct ways.Firstly, using contraction hedges like `very' gives better convergence overtime. Secondly, using expansion hedges such as `quite' reduces concept overlap.However, both these improvements come at a cost of slower speed of development.
arxiv-15000-248 | Graded Entailment for Compositional Distributional Semantics | http://arxiv.org/pdf/1601.04908v2.pdf | author:Desislava Bankova, Bob Coecke, Martha Lewis, Daniel Marsden category:cs.CL cs.AI cs.LO math.CT quant-ph published:2016-01-19 summary:The categorical compositional distributional model of natural languageprovides a conceptually motivated procedure to compute the meaning ofsentences, given grammatical structure and the meanings of its words. Thisapproach has outperformed other models in mainstream empirical languageprocessing tasks. However, until recently it has lacked the crucial feature oflexical entailment -- as do other distributional models of meaning. In this paper we solve the problem of entailment for categoricalcompositional distributional semantics. Taking advantage of the abstractcategorical framework allows us to vary our choice of model. This enables theintroduction of a notion of entailment, exploiting ideas from the categoricalsemantics of partial knowledge in quantum computation. The new model of language uses density matrices, on which we introduce anovel robust graded order capturing the entailment strength between concepts.This graded measure emerges from a general framework for approximateentailment, induced by any commutative monoid. Quantum logic embeds in ourgraded order. Our main theorem shows that entailment strength lifts compositionally to thesentence level, giving a lower bound on sentence entailment. We describe theessential properties of graded entailment such as continuity, and provide aprocedure for calculating entailment strength.
arxiv-15000-249 | ARock: an Algorithmic Framework for Asynchronous Parallel Coordinate Updates | http://arxiv.org/pdf/1506.02396v4.pdf | author:Zhimin Peng, Yangyang Xu, Ming Yan, Wotao Yin category:math.OC cs.DC stat.ML published:2015-06-08 summary:Finding a fixed point to a nonexpansive operator, i.e., $x^*=Tx^*$, abstractsmany problems in numerical linear algebra, optimization, and other areas ofscientific computing. To solve fixed-point problems, we propose ARock, analgorithmic framework in which multiple agents (machines, processors, or cores)update $x$ in an asynchronous parallel fashion. Asynchrony is crucial toparallel computing since it reduces synchronization wait, relaxes communicationbottleneck, and thus speeds up computing significantly. At each step of ARock,an agent updates a randomly selected coordinate $x_i$ based on possiblyout-of-date information on $x$. The agents share $x$ through either globalmemory or communication. If writing $x_i$ is atomic, the agents can read andwrite $x$ without memory locks. Theoretically, we show that if the nonexpansive operator $T$ has a fixedpoint, then with probability one, ARock generates a sequence that converges toa fixed points of $T$. Our conditions on $T$ and step sizes are weaker thancomparable work. Linear convergence is also obtained. We propose special cases of ARock for linear systems, convex optimization,machine learning, as well as distributed and decentralized consensus problems.Numerical experiments of solving sparse logistic regression problems arepresented.
arxiv-15000-250 | Context-Based Prediction of App Usage | http://arxiv.org/pdf/1512.07851v2.pdf | author:Joseph Keshet, Adam Kariv, Arnon Dagan, Dvir Volk, Joey Simhon category:cs.LG published:2015-12-24 summary:There are around a hundred installed apps on an average smartphone. The highnumber of apps and the limited number of app icons that can be displayed on thedevice's screen requires a new paradigm to address their visibility to theuser. In this paper we propose a new online algorithm for dynamicallypredicting a set of apps that the user is likely to use. The algorithm runs onthe user's device and constantly learns the user's habits at a given time,location, and device state. It is designed to actively help the user tonavigate to the desired app as well as to provide a personalized feeling, andhence is aimed at maximizing the AUC. We show both theoretically andempirically that the algorithm maximizes the AUC, and yields good results on aset of 1,000 devices.
arxiv-15000-251 | A Label Semantics Approach to Linguistic Hedges | http://arxiv.org/pdf/1601.06738v1.pdf | author:Martha Lewis, Jonathan Lawry category:cs.AI cs.CL published:2016-01-25 summary:We introduce a model for the linguistic hedges `very' and `quite' within thelabel semantics framework, and combined with the prototype and conceptualspaces theories of concepts. The proposed model emerges naturally from therepresentational framework we use and as such, has a clear semantic grounding.We give generalisations of these hedge models and show that they can becomposed with themselves and with other functions, going on to examine theirbehaviour in the limit of composition.
arxiv-15000-252 | Concept Generation in Language Evolution | http://arxiv.org/pdf/1601.06732v1.pdf | author:Martha Lewis, Jonathan Lawry category:cs.AI cs.CL cs.MA published:2016-01-25 summary:This thesis investigates the generation of new concepts from combinations ofexisting concepts as a language evolves. We give a method for combiningconcepts, and will be investigating the utility of composite concepts inlanguage evolution and thence the utility of concept generation.
arxiv-15000-253 | Conditional distribution variability measures for causality detection | http://arxiv.org/pdf/1601.06680v1.pdf | author:José A. R. Fonollosa category:stat.ML cs.LG published:2016-01-25 summary:In this paper we derive variability measures for the conditional probabilitydistributions of a pair of random variables, and we study its application inthe inference of causal-effect relationships. We also study the combination ofthe proposed measures with standard statistical measures in the the frameworkof the ChaLearn cause-effect pair challenge. The developed model obtains an AUCscore of 0.82 on the final test database and ranked second in the challenge.
arxiv-15000-254 | Testing for Causality in Continuous Time Bayesian Network Models of High-Frequency Data | http://arxiv.org/pdf/1601.06651v1.pdf | author:Jonas Hallgren, Timo Koski category:stat.ML q-fin.TR published:2016-01-25 summary:Continuous time Bayesian networks are investigated with a special focus ontheir ability to express causality. A framework is presented for doinginference in these networks. The central contributions are a representation ofthe intensity matrices for the networks and the introduction of a causalitymeasure. A new model for high-frequency financial data is presented. It iscalibrated to market data and by the new causality measure it performs betterthan older models.
arxiv-15000-255 | Time-Varying Gaussian Process Bandit Optimization | http://arxiv.org/pdf/1601.06650v1.pdf | author:Ilija Bogunovic, Jonathan Scarlett, Volkan Cevher category:stat.ML cs.LG published:2016-01-25 summary:We consider the sequential Bayesian optimization problem with banditfeedback, adopting a formulation that allows for the reward function to varywith time. We model the reward function using a Gaussian process whoseevolution obeys a simple Markov model. We introduce two natural extensions ofthe classical Gaussian process upper confidence bound (GP-UCB) algorithm. Thefirst, R-GP-UCB, resets GP-UCB at regular intervals. The second, TV-GP-UCB,instead forgets about old data in a smooth fashion. Our main contributioncomprises of novel regret bounds for these algorithms, providing an explicitcharacterization of the trade-off between the time horizon and the rate atwhich the function varies. We illustrate the performance of the algorithms onboth synthetic and real data, and we find the gradual forgetting of TV-GP-UCBto perform favorably compared to the sharp resetting of R-GP-UCB. Moreover,both algorithms significantly outperform classical GP-UCB, since it treatsstale and fresh data equally.
arxiv-15000-256 | Bayesian Estimation of Bipartite Matchings for Record Linkage | http://arxiv.org/pdf/1601.06630v1.pdf | author:Mauricio Sadinle category:stat.ME stat.AP stat.ML published:2016-01-25 summary:The bipartite record linkage task consists of merging two disparate datafilescontaining information on two overlapping sets of entities. This is non-trivialin the absence of unique identifiers and it is important for a wide variety ofapplications given that it needs to be solved whenever we have to combineinformation from different sources. Most statistical techniques currently usedfor record linkage are derived from a seminal paper by Fellegi and Sunter(1969). These techniques usually assume independence in the matching statusesof record pairs to derive estimation procedures and optimal point estimators.We argue that this independence assumption is unreasonable and instead target abipartite matching between the two datafiles as our parameter of interest.Bayesian implementations allow us to quantify uncertainty on the matchingdecisions and derive a variety of point estimators using different lossfunctions. We propose partial Bayes estimates that allow uncertain parts of thebipartite matching to be left unresolved. We evaluate our approach to recordlinkage using a variety of challenging scenarios and show that it outperformsthe traditional methodology. We illustrate the advantages of our methodsmerging two datafiles on casualties from the civil war of El Salvador.
arxiv-15000-257 | Intensity-only optical compressive imaging using a multiply scattering material and a double phase retrieval approach | http://arxiv.org/pdf/1510.01098v2.pdf | author:Boshra Rajaei, Eric W. Tramel, Sylvain Gigan, Florent Krzakala, Laurent Daudet category:cs.CV published:2015-10-05 summary:In this paper, the problem of compressive imaging is addressed using naturalrandomization by means of a multiply scattering medium. To utilize the mediumin this way, its corresponding transmission matrix must be estimated. Tocalibrate the imager, we use a digital micromirror device (DMD) as a simple,cheap, and high-resolution binary intensity modulator. We propose a phaseretrieval algorithm which is well adapted to intensity-only measurements on thecamera, and to the input binary intensity patterns, both to estimate thecomplex transmission matrix as well as image reconstruction. We demonstratepromising experimental results for the proposed algorithm using the MNISTdataset of handwritten digits as example images.
arxiv-15000-258 | A Taxonomy of Deep Convolutional Neural Nets for Computer Vision | http://arxiv.org/pdf/1601.06615v1.pdf | author:Suraj Srinivas, Ravi Kiran Sarvadevabhatla, Konda Reddy Mopuri, Nikita Prabhu, Srinivas S S Kruthiventi, R. Venkatesh Babu category:cs.CV cs.LG cs.MM published:2016-01-25 summary:Traditional architectures for solving computer vision problems and the degreeof success they enjoyed have been heavily reliant on hand-crafted features.However, of late, deep learning techniques have offered a compellingalternative -- that of automatically learning problem-specific features. Withthis new paradigm, every problem in computer vision is now being re-examinedfrom a deep learning perspective. Therefore, it has become important tounderstand what kind of deep networks are suitable for a given problem.Although general surveys of this fast-moving paradigm (i.e. deep-networks)exist, a survey specific to computer vision is missing. We specificallyconsider one form of deep networks widely used in computer vision -convolutional neural networks (CNNs). We start with "AlexNet" as our base CNNand then examine the broad variations proposed over time to suit differentapplications. We hope that our recipe-style survey will serve as a guide,particularly for novice practitioners intending to use deep-learning techniquesfor computer vision.
arxiv-15000-259 | An Unsupervised Method for Detection and Validation of The Optic Disc and The Fovea | http://arxiv.org/pdf/1601.06608v1.pdf | author:Mrinal Haloi, Samarendra Dandapat, Rohit Sinha category:cs.CV 68T45 published:2016-01-25 summary:In this work, we have presented a novel method for detection of retinal imagefeatures, the optic disc and the fovea, from colour fundus photographs ofdilated eyes for Computer-aided Diagnosis(CAD) system. A saliency map basedmethod was used to detect the optic disc followed by an unsupervisedprobabilistic Latent Semantic Analysis for detection validation. The validationconcept is based on distinct vessels structures in the optic disc. By using theclinical information of standard location of the fovea with respect to theoptic disc, the macula region is estimated. Accuracy of 100\% detection isachieved for the optic disc and the macula on MESSIDOR and DIARETDB1 and 98.8\%detection accuracy on STARE dataset.
arxiv-15000-260 | Egocentric Activity Recognition with Multimodal Fisher Vector | http://arxiv.org/pdf/1601.06603v1.pdf | author:Sibo Song, Ngai-Man Cheung, Vijay Chandrasekhar, Bappaditya Mandal, Jie Lin category:cs.MM cs.CV published:2016-01-25 summary:With the increasing availability of wearable devices, research on egocentricactivity recognition has received much attention recently. In this paper, webuild a Multimodal Egocentric Activity dataset which includes egocentric videosand sensor data of 20 fine-grained and diverse activity categories. We presenta novel strategy to extract temporal trajectory-like features from sensor data.We propose to apply the Fisher Kernel framework to fuse video and temporalenhanced sensor features. Experiment results show that with careful design offeature extraction and fusion algorithm, sensor data can enhanceinformation-rich video data. We make publicly available the MultimodalEgocentric Activity dataset to facilitate future research.
arxiv-15000-261 | Is swarm intelligence able to create mazes? | http://arxiv.org/pdf/1601.06580v1.pdf | author:Dawid Polap, Marcin Wozniak, Christian Napoli, Emiliano Tramontana category:cs.NE cs.AI published:2016-01-25 summary:In this paper, the idea of applying Computational Intelligence in the processof creation board games, in particular mazes, is presented. For two differentalgorithms the proposed idea has been examined. The results of the experimentsare shown and discussed to present advantages and disadvantages.
arxiv-15000-262 | A Kernel Independence Test for Geographical Language Variation | http://arxiv.org/pdf/1601.06579v1.pdf | author:Dong Nguyen, Jacob Eisenstein category:cs.CL published:2016-01-25 summary:Quantifying the degree of spatial dependence for linguistic variables is akey task for analyzing dialectal variation. However, existing approaches haveimportant drawbacks. First, they make unjustified assumptions about the natureof spatial variation: some assume that the geographical distribution oflinguistic variables is Gaussian, while others assume that linguistic variationis aligned to pre-defined geopolitical units such as states or counties.Second, they are not applicable to all types of linguistic data: someapproaches apply only to frequencies, others to boolean indicators of whether alinguistic variable is present. We present a new method for measuringgeographical language variation, which solves both of these problems. Ourapproach builds on reproducing kernel Hilbert space (RKHS) representations fornonparametric statistics, and takes the form of a test statistic that iscomputed from pairs of individual geotagged observations without aggregationinto predefined geographical bins. We compare this test with prior work usingsynthetic data as well as a diverse set of real datasets: a corpus of Dutchtweets, a Dutch syntactic atlas, and a dataset of letters to the editor inNorth American newspapers. Our proposed test is shown to support robustinferences across a broad range of scenarios and types of data.
arxiv-15000-263 | Visualizations Relevant to The User By Multi-View Latent Variable Factorization | http://arxiv.org/pdf/1512.07807v2.pdf | author:Seppo Virtanen, Homayun Afrabandpey, Samuel Kaski category:cs.LG cs.IR published:2015-12-24 summary:A main goal of data visualization is to find, from among all the availablealternatives, mappings to the 2D/3D display which are relevant to the user.Assuming user interaction data, or other auxiliary data about the items ortheir relationships, the goal is to identify which aspects in the primary datasupport the user\'s input and, equally importantly, which aspects of theuser\'s potentially noisy input have support in the primary data. For solvingthe problem, we introduce a multi-view embedding in which a latentfactorization identifies which aspects in the two data views (primary data anduser data) are related and which are specific to only one of them. Thefactorization is a generative model in which the display is parameterized as apart of the factorization and the other factors explain away the aspects notexpressible in a two-dimensional display. Functioning of the model isdemonstrated on several data sets.
arxiv-15000-264 | EM Algorithms for Weighted-Data Clustering with Application to Audio-Visual Scene Analysis | http://arxiv.org/pdf/1509.01509v2.pdf | author:Israel D. Gebru, Xavier Alameda-Pineda, Florence Forbes, Radu Horaud category:cs.CV cs.LG stat.ML published:2015-09-04 summary:Data clustering has received a lot of attention and numerous methods,algorithms and software packages are available. Among these techniques,parametric finite-mixture models play a central role due to their interestingmathematical properties and to the existence of maximum-likelihood estimatorsbased on expectation-maximization (EM). In this paper we propose a new mixturemodel that associates a weight with each observed point. We introduce theweighted-data Gaussian mixture and we derive two EM algorithms. The first oneconsiders a fixed weight for each observation. The second one treats eachweight as a random variable following a gamma distribution. We propose a modelselection method based on a minimum message length criterion, provide a weightinitialization strategy, and validate the proposed algorithms by comparing themwith several state of the art parametric and non-parametric clusteringtechniques. We also demonstrate the effectiveness and robustness of theproposed clustering technique in the presence of heterogeneous data, namelyaudio-visual scene analysis.
arxiv-15000-265 | Robust Influence Maximization | http://arxiv.org/pdf/1601.06551v1.pdf | author:Wei Chen, Tian Lin, Zihan Tan, Mingfei Zhao, Xuren Zhou category:cs.SI cs.LG published:2016-01-25 summary:In this paper, we address the important issue of uncertainty in the edgeinfluence probability estimates for the well studied influence maximizationproblem --- the task of finding $k$ seed nodes in a social network to maximizethe influence spread. We propose the problem of robust influence maximization,which maximizes the worse-case ratio between the influence spread of the chosenseed set and the optimal seed set, given the uncertainty of the parameterinput. We design an algorithm that solves this problem with asolution-dependent bound. We further study uniform sampling and adaptivesampling methods to effectively reduce the uncertainty on parameters andimprove the robustness of the influence maximization task. Our empiricalresults show that parameter uncertainty may greatly affect influencemaximization performance and prior studies that learned influence probabilitiescould lead to poor performance in robust influence maximization due torelatively large uncertainty in parameter estimates, and information cascadebased adaptive sampling method may be an effective way to improve therobustness of influence maximization.
arxiv-15000-266 | Non-Adaptive Group Testing on Graphs | http://arxiv.org/pdf/1511.09196v2.pdf | author:Hamid Kameli category:cs.DS cs.LG published:2015-11-30 summary:Grebinski and Kucherov (1998) and Alon et al. (2004-2005) study the problemof learning a hidden graph for some especial cases, such as hamiltonian cycle,cliques, stars, and matchings. This problem is motivated by problems inchemical reactions, molecular biology and genome sequencing. In this paper, we present a generalization of this problem. Precisely, weconsider a graph G and a subgraph H of G and we assume that G contains exactlyone defective subgraph isomorphic to H. The goal is to find the defectivesubgraph by testing whether an induced subgraph contains an edge of thedefective subgraph, with the minimum number of tests. We present an upper boundfor the number of tests to find the defective subgraph by using the symmetricand high probability variation of Lov\'asz Local Lemma.
arxiv-15000-267 | From One Point to A Manifold: Knowledge Graph Embedding For Precise Link Prediction | http://arxiv.org/pdf/1512.04792v3.pdf | author:Han Xiao, Minlie Huang, Xiaoyan Zhu category:cs.AI cs.LG published:2015-12-15 summary:Knowledge graph embedding aims at offering a numerical knowledgerepresentation paradigm by transforming the entities and relations intocontinuous vector space. However, existing methods could not characterize theknowledge graph in a fine degree to make a precise prediction. There are tworeasons: being an ill-posed algebraic system and applying an overstrictgeometric form. As precise prediction is critical, we propose an manifold-basedembedding principle (\textbf{ManifoldE}) which could be treated as a well-posedalgebraic system that expands the position of golden triples from one point incurrent models to a manifold in ours. Extensive experiments show that theproposed models achieve substantial improvements against the state-of-the-artbaselines especially for the precise prediction task, and yet maintain highefficiency.
arxiv-15000-268 | Smooth PARAFAC Decomposition for Tensor Completion | http://arxiv.org/pdf/1505.06611v3.pdf | author:Tatsuya Yokota, Qibin Zhao, Andrzej Cichocki category:cs.CV published:2015-05-25 summary:In recent years, low-rank based tensor completion, which is a higher-orderextension of matrix completion, has received considerable attention. However,the low-rank assumption is not sufficient for the recovery of visual data, suchas color and 3D images, where the ratio of missing data is extremely high. Inthis paper, we consider "smoothness" constraints as well as low-rankapproximations, and propose an efficient algorithm for performing tensorcompletion that is particularly powerful regarding visual data. The proposedmethod admits significant advantages, owing to the integration of smoothPARAFAC decomposition for incomplete tensors and the efficient selection ofmodels in order to minimize the tensor rank. Thus, our proposed method istermed as "smooth PARAFAC tensor completion (SPC)." In order to impose thesmoothness constraints, we employ two strategies, total variation (SPC-TV) andquadratic variation (SPC-QV), and invoke the corresponding algorithms for modellearning. Extensive experimental evaluations on both synthetic and real-worldvisual data illustrate the significant improvements of our method, in terms ofboth prediction performance and efficiency, compared with many state-of-the-arttensor completion methods.
arxiv-15000-269 | Learning Binary Features Online from Motion Dynamics for Incremental Loop-Closure Detection and Place Recognition | http://arxiv.org/pdf/1601.03821v2.pdf | author:Guangcong Zhang, Mason J. Lilly, Patricio A. Vela category:cs.CV published:2016-01-15 summary:This paper proposes a simple yet effective approach to learn visual featuresonline for improving loop-closure detection and place recognition, based onbag-of-words frameworks. The approach learns a codeword in bag-of-words modelfrom a pair of matched features from two consecutive frames, such that thecodeword has temporally-derived perspective invariance to camera motion. Thelearning algorithm is efficient: the binary descriptor is generated from themean image patch, and the mask is learned based on discriminative projection byminimizing the intra-class distances among the learned feature and the twooriginal features. A codeword for bag-of-words models is generated by packagingthe learned descriptor and mask, with a masked Hamming distance defined tomeasure the distance between two codewords. The geometric properties of thelearned codewords are then mathematically justified. In addition, hypothesisconstraints are imposed through temporal consistency in matched codewords,which improves precision. The approach, integrated in an incrementalbag-of-words system, is validated on multiple benchmark data sets and comparedto state-of-the-art methods. Experiments demonstrate improved precision/recalloutperforming state of the art with little loss in runtime.
arxiv-15000-270 | QUDA: A Direct Approach for Sparse Quadratic Discriminant Analysis | http://arxiv.org/pdf/1510.00084v2.pdf | author:Binyan Jiang, Xiangyu Wang, Chenlei Leng category:stat.ME stat.CO stat.ML published:2015-10-01 summary:Quadratic discriminant analysis (QDA) is a standard tool for classificationdue to its simplicity and flexibility. Because the number of its parametersscales quadratically with the number of the variables, QDA is not practical,however, when the dimensionality is relatively large. To address this, wepropose a novel procedure named QUDA for QDA in analyzing high-dimensionaldata. Formulated in a simple and coherent framework, QUDA aims to directlyestimate the key quantities in the Bayes discriminant function includingquadratic interactions and a linear index of the variables for classification.Under appropriate sparsity assumptions, we establish consistency results forestimating the interactions and the linear index, and further demonstrate thatthe misclassification rate of our procedure converges to the optimal Bayesrisk, even when the dimensionality is exponentially high with respect to thesample size. An efficient algorithm based on the alternating direction methodof multipliers (ADMM) is developed for finding interactions, which is muchfaster than its competitor in the literature. The promising performance of QUDAis illustrated via extensive simulation studies and the analysis of twodatasets.
arxiv-15000-271 | Information Limits for Recovering a Hidden Community | http://arxiv.org/pdf/1509.07859v2.pdf | author:Bruce Hajek, Yihong Wu, Jiaming Xu category:stat.ML cs.IT math.IT published:2015-09-25 summary:We study the problem of recovering a hidden community of cardinality $K$ froman $n \times n$ symmetric data matrix $A$, where for distinct indices $i,j$,$A_{ij} \sim P$ if $i, j$ both belong to the community and $A_{ij} \sim Q$otherwise, for two known probability distributions $P$ and $Q$ depending on$n$. If $P={\rm Bern}(p)$ and $Q={\rm Bern}(q)$ with $p>q$, it reduces to theproblem of finding a densely-connected $K$-subgraph planted in a largeErd\"os-R\'enyi graph; if $P=\mathcal{N}(\mu,1)$ and $Q=\mathcal{N}(0,1)$ with$\mu>0$, it corresponds to the problem of locating a $K \times K$ principalsubmatrix of elevated means in a large Gaussian random matrix. We focus on twotypes of asymptotic recovery guarantees as $n \to \infty$: (1) weak recovery:expected number of classification errors is $o(K)$; (2) exact recovery:probability of classifying all indices correctly converges to one. Under mildassumptions on $P$ and $Q$, and allowing the community size to scalesublinearly with $n$, we derive a set of sufficient conditions and a set ofnecessary conditions for recovery, which are asymptotically tight with sharpconstants. The results hold in particular for the Gaussian case, and for thecase of bounded log likelihood ratio, including the Bernoulli case whenever$\frac{p}{q}$ and $\frac{1-p}{1-q}$ are bounded away from zero and infinity. Animportant algorithmic implication is that, whenever exact recovery isinformation theoretically possible, any algorithm that provides weak recoverywhen the community size is concentrated near $K$ can be upgraded to achieveexact recovery in linear additional time by a simple voting procedure.
arxiv-15000-272 | A new correlation clustering method for cancer mutation analysis | http://arxiv.org/pdf/1601.06476v1.pdf | author:Jack P. Hou, Amin Emad, Gregory J. Puleo, Jian Ma, Olgica Milenkovic category:cs.LG q-bio.QM published:2016-01-25 summary:Cancer genomes exhibit a large number of different alterations that affectmany genes in a diverse manner. It is widely believed that these alterationsfollow combinatorial patterns that have a strong connection with the underlyingmolecular interaction networks and functional pathways. A better understandingof the generative mechanisms behind the mutation rules and their influence ongene communities is of great importance for the process of driver mutationsdiscovery and for identification of network modules related to cancerdevelopment and progression. We developed a new method for cancer mutationpattern analysis based on a constrained form of correlation clustering.Correlation clustering is an agnostic learning method that can be used forgeneral community detection problems in which the number of communities ortheir structure is not known beforehand. The resulting algorithm, named $C^3$,leverages mutual exclusivity of mutations, patient coverage, and driver networkconcentration principles; it accepts as its input a user determined combinationof heterogeneous patient data, such as that available from TCGA (includingmutation, copy number, and gene expression information), and creates a largenumber of clusters containing mutually exclusive mutated genes in a particulartype of cancer. The cluster sizes may be required to obey some useful soft sizeconstraints, without impacting the computational complexity of the algorithm.To test $C^3$, we performed a detailed analysis on TCGA breast cancer andglioblastoma data and showed that our algorithm outperforms thestate-of-the-art CoMEt method in terms of discovering mutually exclusive genemodules and identifying driver genes. Our $C^3$ method represents a unique toolfor efficient and reliable identification of mutation patterns and driverpathways in large-scale cancer genomics studies.
arxiv-15000-273 | Galaxy-X: A Novel Approach for Multi-class Classification in an Open Universe | http://arxiv.org/pdf/1511.00725v2.pdf | author:Wajdi Dhifli, Abdoulaye Baniré Diallo category:cs.LG cs.AI cs.DB cs.IR published:2015-11-02 summary:Classification is a fundamental task in machine learning and artificialintelligence. Existing classification methods are designed to classify unknowninstances within a set of previously known classes that are seen in training.Such classification takes the form of prediction within a closed-set. However,a more realistic scenario that fits the ground truth of real world applicationsis to consider the possibility of encountering instances that do not belong toany of the classes that are seen in training, $i.e.$, an open-setclassification. In such situation, existing closed-set classification methodswill assign a training label to these instances resulting in amisclassification. In this paper, we introduce Galaxy-X, a novel multi-classclassification method for open-set problem. For each class of the training set,Galaxy-X creates a minimum bounding hyper-sphere that encompasses thedistribution of the class by enclosing all of its instances. In such manner,our method is able to distinguish instances resembling previously seen classesfrom those that are of unseen classes. To adequately evaluate open-setclassification, we introduce a novel evaluation procedure. Experimental resultson benchmark datasets as well as on synthetic datasets show the efficiency ofour approach in classifying novel instances from known as well as unknownclasses.
arxiv-15000-274 | ProtNN: Fast and Accurate Nearest Neighbor Protein Function Prediction based on Graph Embedding in Structural and Topological Space | http://arxiv.org/pdf/1511.00736v2.pdf | author:Wajdi Dhifli, Abdoulaye Baniré Diallo category:cs.LG cs.SI published:2015-11-02 summary:Studying the function of proteins is important for understanding themolecular mechanisms of life. The number of publicly available proteinstructures has increasingly become extremely large. Still, the determination ofthe function of a protein structure remains a difficult, costly, and timeconsuming task. The difficulties are often due to the essential role of spatialand topological structures in the determination of protein functions in livingcells. In this paper, we propose ProtNN, a novel approach for protein functionprediction. Given an unannotated protein structure and a set of annotatedproteins, ProtNN finds the nearest neighbor annotated structures based onprotein-graph pairwise similarities. Given a query protein, ProtNN finds thenearest neighbor reference proteins based on a graph representation model and apairwise similarity between vector embedding of both query and referenceprotein-graphs in structural and topological spaces. ProtNN assigns to thequery protein the function with the highest number of votes across the set of knearest neighbor reference proteins, where k is a user-defined parameter.Experimental evaluation demonstrates that ProtNN is able to accurately classifyseveral datasets in an extremely fast runtime compared to state-of-the-artapproaches. We further show that ProtNN is able to scale up to a whole PDBdataset in a single-process mode with no parallelization, with a gain ofthousands order of magnitude of runtime compared to state-of-the-artapproaches.
arxiv-15000-275 | On Variance Reduction in Stochastic Gradient Descent and its Asynchronous Variants | http://arxiv.org/pdf/1506.06840v2.pdf | author:Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabás Póczos, Alex Smola category:cs.LG stat.ML published:2015-06-23 summary:We study optimization algorithms based on variance reduction for stochasticgradient descent (SGD). Remarkable recent progress has been made in thisdirection through development of algorithms like SAG, SVRG, SAGA. Thesealgorithms have been shown to outperform SGD, both theoretically andempirically. However, asynchronous versions of these algorithms---a crucialrequirement for modern large-scale applications---have not been studied. Webridge this gap by presenting a unifying framework for many variance reductiontechniques. Subsequently, we propose an asynchronous algorithm grounded in ourframework, and prove its fast convergence. An important consequence of ourgeneral approach is that it yields asynchronous versions of variance reductionalgorithms such as SVRG and SAGA as a byproduct. Our method achieves nearlinear speedup in sparse settings common to machine learning. We demonstratethe empirical performance of our method through a concrete realization ofasynchronous SVRG.
arxiv-15000-276 | Fashion Apparel Detection: The Role of Deep Convolutional Neural Network and Pose-dependent Priors | http://arxiv.org/pdf/1411.5319v2.pdf | author:Kota Hara, Vignesh Jagadeesh, Robinson Piramuthu category:cs.CV published:2014-11-19 summary:In this work, we propose and address a new computer vision task, which wecall fashion item detection, where the aim is to detect various fashion items aperson in the image is wearing or carrying. The types of fashion items weconsider in this work include hat, glasses, bag, pants, shoes and so on. Thedetection of fashion items can be an important first step of various e-commerceapplications for fashion industry. Our method is based on state-of-the-artobject detection method pipeline which combines object proposal methods with aDeep Convolutional Neural Network. Since the locations of fashion items are instrong correlation with the locations of body joints positions, we incorporatecontextual information from body poses in order to improve the detectionperformance. Through the experiments, we demonstrate the effectiveness of theproposed method.
arxiv-15000-277 | Locally Adaptive Dynamic Networks | http://arxiv.org/pdf/1505.05668v2.pdf | author:Daniele Durante, David B. Dunson category:stat.AP stat.ML published:2015-05-21 summary:Our focus is on realistically modeling and forecasting dynamic networks offace-to-face contacts among individuals. Important aspects of such data thatlead to problems with current methods include the tendency to move betweenperiods of slow and rapid changes and the dynamic heterogeneity in theconnectivity behaviors across nodes. Motivated by this application, we developa novel methodology for Locally Adaptive DYnamic (LADY) network inference. Theproposed model relies on a dynamic latent space representation in which eachsubjects' position evolves over time via a stochastic differential equation.Using a state space representation for these stochastic processes andP\'olya-gamma data augmentation, we develop an efficient MCMC algorithm forposterior inference along with tractable online updating and predictionprocedures for forecasting of future networks. We evaluate performance viasimulation studies, and consider an application to face-to-face contacts amongstudents in a primary school.
arxiv-15000-278 | Eye detection in digital images: challenges and solutions | http://arxiv.org/pdf/1601.04871v2.pdf | author:Mitra Montazeri, Mahdieh Montazeri, Saeid Saryazdi category:cs.CV published:2016-01-19 summary:Eye Detection has an important role in the field of biometric identificationand known as one method of person's identification. In recent years, manyefforts have been done which can detect eye automatically and with differentimage conditions. However, each method has its own drawbacks which can controlsome of these conditions. In this paper, different methods of eye detectionwill be categorized and explained. In each category, the advantages anddisadvantages of each method will be presented.
arxiv-15000-279 | Morpho-syntactic Lexicon Generation Using Graph-based Semi-supervised Learning | http://arxiv.org/pdf/1512.05030v3.pdf | author:Manaal Faruqui, Ryan McDonald, Radu Soricut category:cs.CL published:2015-12-16 summary:Morpho-syntactic lexicons provide information about the morphological andsyntactic roles of words in a language. Such lexicons are not available for alllanguages and even when available, their coverage can be limited. We present agraph-based semi-supervised learning method that uses the morphological,syntactic and semantic relations between words to automatically construct widecoverage lexicons from small seed sets. Our method is language-independent, andwe show that we can expand a 1000 word seed lexicon to more than 100 times itssize with high quality for 11 languages. In addition, the automatically createdlexicons provide features that improve performance in two downstream tasks:morphological tagging and dependency parsing.
arxiv-15000-280 | Fast Binary Embedding via Circulant Downsampled Matrix -- A Data-Independent Approach | http://arxiv.org/pdf/1601.06342v1.pdf | author:Sung-Hsien Hsieh, Chun-Shien Lu, Soo-Chang Pei category:cs.IT cs.CV cs.LG math.IT published:2016-01-24 summary:Binary embedding of high-dimensional data aims to produce low-dimensionalbinary codes while preserving discriminative power. State-of-the-art methodsoften suffer from high computation and storage costs. We present a simple andfast embedding scheme by first downsampling N-dimensional data intoM-dimensional data and then multiplying the data with an MxM circulant matrix.Our method requires O(N +M log M) computation and O(N) storage costs. We proveif data have sparsity, our scheme can achieve similarity-preserving well.Experiments further demonstrate that though our method is cost-effective andfast, it still achieves comparable performance in image applications.
arxiv-15000-281 | Persistent Images: A Stable Vector Representation of Persistent Homology | http://arxiv.org/pdf/1507.06217v2.pdf | author:Henry Adams, Sofya Chepushtanova, Tegan Emerson, Eric Hanson, Michael Kirby, Francis Motta, Rachel Neville, Chris Peterson, Patrick Shipman, Lori Ziegelmeier category:cs.CG math.AT stat.ML F.2.2; I.5.2 published:2015-07-22 summary:Many data sets can be viewed as a noisy sampling of an underlying topologicalspace. A suite of tools in topological data analysis allows one to exploit thisstructure for the purpose of knowledge discovery. One such tool is persistenthomology which provides a multiscale description of the homological featureswithin a data set. A useful representation of this homological information is apersistence diagram (PD). The space of PDs can be given a metric structureallowing a given diagram to be used as a statistic for the purpose ofcomparison against other diagrams. We convert a PD to a persistence image (PI)and prove stability with respect to small perturbations in the inputs. The PIis a vector representation allowing the application of vector-based machinelearning tools, such as linear and sparse support vector machines. These toolshelp to identify discriminatory features which can have a topologicalinterpretation. The PIs and PDs derived from randomly sampled topologicalspaces are compared by applying the K-medoids clustering algorithm. To furtherillustrate the PI technique, linear and sparse support vector machines areimplemented on this data set and classification is performed on additional dataarising from a discrete dynamical system called the linked twist map.
arxiv-15000-282 | Very Deep Multilingual Convolutional Neural Networks for LVCSR | http://arxiv.org/pdf/1509.08967v2.pdf | author:Tom Sercu, Christian Puhrsch, Brian Kingsbury, Yann LeCun category:cs.CL cs.NE published:2015-09-29 summary:Convolutional neural networks (CNNs) are a standard component of many currentstate-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR)systems. However, CNNs in LVCSR have not kept pace with recent advances inother domains where deeper neural networks provide superior performance. Inthis paper we propose a number of architectural advances in CNNs for LVCSR.First, we introduce a very deep convolutional network architecture with up to14 weight layers. There are multiple convolutional layers before each poolinglayer, with small 3x3 kernels, inspired by the VGG Imagenet 2014 architecture.Then, we introduce multilingual CNNs with multiple untied layers. Finally, weintroduce multi-scale input features aimed at exploiting more context atnegligible computational cost. We evaluate the improvements first on a Babeltask for low resource speech recognition, obtaining an absolute 5.77% WERimprovement over the baseline PLP DNN by training our CNN on the combined dataof six different languages. We then evaluate the very deep CNNs on the Hub5'00benchmark (using the 262 hours of SWB-1 training data) achieving a word errorrate of 11.8% after cross-entropy training, a 1.4% WER improvement (10.6%relative) over the best published CNN result so far.
arxiv-15000-283 | Solving Dense Image Matching in Real-Time using Discrete-Continuous Optimization | http://arxiv.org/pdf/1601.06274v1.pdf | author:Alexander Shekhovtsov, Christian Reinbacher, Gottfried Graber, Thomas Pock category:cs.CV published:2016-01-23 summary:Dense image matching is a fundamental low-level problem in Computer Vision,which has received tremendous attention from both discrete and continuousoptimization communities. The goal of this paper is to combine the advantagesof discrete and continuous optimization in a coherent framework. We devise amodel based on energy minimization, to be optimized by both discrete andcontinuous algorithms in a consistent way. In the discrete setting, we proposea novel optimization algorithm that can be massively parallelized. In thecontinuous setting we tackle the problem of non-convex regularizers by aformulation based on differences of convex functions. The resulting hybriddiscrete-continuous algorithm can be efficiently accelerated by modern GPUs andwe demonstrate its real-time performance for the applications of dense stereomatching and optical flow.
arxiv-15000-284 | Fixed Point Performance Analysis of Recurrent Neural Networks | http://arxiv.org/pdf/1512.01322v2.pdf | author:Sungho Shin, Kyuyeon Hwang, Wonyong Sung category:cs.LG cs.NE published:2015-12-04 summary:Recurrent neural networks have shown excellent performance in manyapplications, however they require increased complexity in hardware or softwarebased implementations. The hardware complexity can be much lowered byminimizing the word-length of weights and signals. This work analyzes thefixed-point performance of recurrent neural networks using a retrain basedquantization method. The quantization sensitivity of each layer in RNNs isstudied, and the overall fixed-point optimization results minimizing thecapacity of weights while not sacrificing the performance are presented. Alanguage model and a phoneme recognition examples are used.
arxiv-15000-285 | Person Re-Identification by Discriminative Selection in Video Ranking | http://arxiv.org/pdf/1601.06260v1.pdf | author:Taiqing Wang, Shaogang Gong, Xiatian Zhu, Shengjin Wang category:cs.CV published:2016-01-23 summary:Current person re-identification (ReID) methods typically rely onsingle-frame imagery features, whilst ignoring space-time information fromimage sequences often available in the practical surveillance scenarios.Single-frame (single-shot) based visual appearance matching is inherentlylimited for person ReID in public spaces due to the challenging visualambiguity and uncertainty arising from non-overlapping camera views whereviewing condition changes can cause significant people appearance variations.In this work, we present a novel model to automatically select the mostdiscriminative video fragments from noisy/incomplete image sequences of peoplefrom which reliable space-time and appearance features can be computed, whilstsimultaneously learning a video ranking function for person ReID. Using thePRID$2011$, iLIDS-VID, and HDA+ image sequence datasets, we extensivelyconducted comparative evaluations to demonstrate the advantages of the proposedmodel over contemporary gait recognition, holistic image sequence matching andstate-of-the-art single-/multi-shot ReID methods.
arxiv-15000-286 | Minimax Lower Bounds for Linear Independence Testing | http://arxiv.org/pdf/1601.06259v1.pdf | author:Aaditya Ramdas, David Isenberg, Aarti Singh, Larry Wasserman category:stat.ML cs.IT cs.LG math.IT math.ST stat.TH published:2016-01-23 summary:Linear independence testing is a fundamental information-theoretic andstatistical problem that can be posed as follows: given $n$ points$\{(X_i,Y_i)\}^n_{i=1}$ from a $p+q$ dimensional multivariate distributionwhere $X_i \in \mathbb{R}^p$ and $Y_i \in\mathbb{R}^q$, determine whether $a^TX$ and $b^T Y$ are uncorrelated for every $a \in \mathbb{R}^p, b\in\mathbb{R}^q$ or not. We give minimax lower bound for this problem (when $p+q,n\to \infty$, $(p+q)/n \leq \kappa < \infty$, without sparsity assumptions). Insummary, our results imply that $n$ must be at least as large as $\sqrt{pq}/\\Sigma_{XY}\_F^2$ for any procedure (test) to have non-trivial power,where $\Sigma_{XY}$ is the cross-covariance matrix of $X,Y$. We also providesome evidence that the lower bound is tight, by connections to two-sampletesting and regression in specific settings.
arxiv-15000-287 | Using compatible shape descriptor for lexicon reduction of printed Farsi subwords | http://arxiv.org/pdf/1601.06251v1.pdf | author:Homa Davoudi, Ehsanollah Kabir category:cs.CV published:2016-01-23 summary:This Paper presents a method for lexicon reduction of Printed Farsi subwordsbased on their holistic shape features. Because of the large number of Persiansubwords variously shaped from a simple letter to a complex combination ofseveral connected characters, it is not easy to find a fixed shape descriptorsuitable for all subwords. In this paper, we propose to select the descriptoraccording to the input shape characteristics. To do this, a neural network istrained to predict the appropriate descriptor of the input image. This networkis implemented in the proposed lexicon reduction system to decide on thedescriptor used for comparison of the query image with the lexicon entries.Evaluating the proposed method on a dataset of Persian subwords allows one toattest the effectiveness of the proposed idea of dealing differently withvarious query shapes.
arxiv-15000-288 | Automatic recognition of element classes and boundaries in the birdsong with variable sequences | http://arxiv.org/pdf/1601.06248v1.pdf | author:Takuya Koumura, Kazuo Okanoya category:q-bio.NC cs.LG cs.SD published:2016-01-23 summary:Researches on sequential vocalization often require analysis of vocalizationsin long continuous sounds. In such studies as developmental ones or studiesacross generations in which days or months of vocalizations must be analyzed,methods for automatic recognition would be strongly desired. Although methodsfor automatic speech recognition for application purposes have been intensivelystudied, blindly applying them for biological purposes may not be an optimalsolution. This is because, unlike human speech recognition, analysis ofsequential vocalizations often requires accurate extraction of timinginformation. In the present study we propose automated systems suitable forrecognizing birdsong, one of the most intensively investigated sequentialvocalizations, focusing on the three properties of the birdsong. First, a songis a sequence of vocal elements, called notes, which can be grouped intocategories. Second, temporal structure of birdsong is precisely controlled,meaning that temporal information is important in song analysis. Finally, notesare produced according to certain probabilistic rules, which may facilitate theaccurate song recognition. We divided the procedure of song recognition intothree sub-steps: local classification, boundary detection, and globalsequencing, each of which corresponds to each of the three properties ofbirdsong. We compared the performances of several different ways to arrangethese three steps. As results, we demonstrated a hybrid model of a deep neuralnetwork and a hidden Markov model is effective in recognizing birdsong withvariable note sequences. We propose suitable arrangements of methods accordingto whether accurate boundary detection is needed. Also we designed the newmeasure to jointly evaluate the accuracy of note classification and boundarydetection. Our methods should be applicable, with small modification andtuning, to the songs in other species that hold the three properties of thesequential vocalization.
arxiv-15000-289 | Cost-aware Pre-training for Multiclass Cost-sensitive Deep Learning | http://arxiv.org/pdf/1511.09337v2.pdf | author:Yu-An Chung, Hsuan-Tien Lin, Shao-Wen Yang category:cs.LG cs.NE published:2015-11-30 summary:Deep learning has been one of the most prominent machine learning techniquesnowadays, being the state-of-the-art on a broad range of applications whereautomatic feature extraction is needed. Many such applications also demandvarying costs for different types of mis-classification errors, but it is notclear whether or how such cost information can be incorporated into deeplearning to improve performance. In this work, we propose a novel cost-awarealgorithm that takes into account the cost information into not only thetraining stage but also the pre-training stage of deep learning. The approachallows deep learning to conduct automatic feature extraction with the costinformation effectively. Extensive experimental results demonstrate that theproposed approach outperforms other deep learning models that do not digest thecost information in the pre-training stage.
arxiv-15000-290 | Super-resolution reconstruction of hyperspectral images via low rank tensor modeling and total variation regularization | http://arxiv.org/pdf/1601.06243v1.pdf | author:Shiying He, Haiwei Zhou, Yao Wang, Wenfei Cao, Zhi Han category:cs.CV published:2016-01-23 summary:In this paper, we propose a novel approach to hyperspectral imagesuper-resolution by modeling the global spatial-and-spectral correlation andlocal smoothness properties over hyperspectral images. Specifically, we utilizethe tensor nuclear norm and tensor folded-concave penalty functions to describethe global spatial-and-spectral correlation hidden in hyperspectral images, and3D total variation (TV) to characterize the local spatial-and-spectralsmoothness across all hyperspectral bands. Then, we develop an efficientalgorithm for solving the resulting optimization problem by combing the locallinear approximation (LLA) strategy and alternative direction method ofmultipliers (ADMM). Experimental results on one hyperspectral image datasetillustrate the merits of the proposed approach.
arxiv-15000-291 | Unsupervised Network Pretraining via Encoding Human Design | http://arxiv.org/pdf/1502.05689v2.pdf | author:Ming-Yu Liu, Arun Mallya, Oncel C. Tuzel, Xi Chen category:cs.CV published:2015-02-19 summary:Over the years, computer vision researchers have spent an immense amount ofeffort on designing image features for the visual object recognition task. Wepropose to incorporate this valuable experience to guide the task of trainingdeep neural networks. Our idea is to pretrain the network through the task ofreplicating the process of hand-designed feature extraction. By learning toreplicate the process, the neural network integrates previous researchknowledge and learns to model visual objects in a way similar to thehand-designed features. In the succeeding finetuning step, it further learnsobject-specific representations from labeled data and this boosts itsclassification power. We pretrain two convolutional neural networks where onereplicates the process of histogram of oriented gradients feature extraction,and the other replicates the process of region covariance feature extraction.After finetuning, we achieve substantially better performance than the baselinemethods.
arxiv-15000-292 | Universal Collaboration Strategies for Signal Detection: A Sparse Learning Approach | http://arxiv.org/pdf/1601.06201v1.pdf | author:Prashant Khanduri, Bhavya Kailkhura, Jayaraman J. Thiagarajan, Pramod K. Varshney category:cs.LG stat.ML published:2016-01-22 summary:This paper considers the problem of high dimensional signal detection in alarge distributed network. In contrast to conventional distributed detection,the nodes in the network can update their observations by combiningobservations from other one-hop neighboring nodes (spatial collaboration).Under the assumption that only a small subset of nodes are capable ofcommunicating with the Fusion Center (FC), our goal is to design optimalcollaboration strategies which maximize the detection performance at the FC.Note that, if one optimizes the system for the detection of a single knownsignal then the network cannot generalize well to other detection tasks. Hence,we propose to design optimal collaboration strategies which are universal for aclass of equally probable deterministic signals. By establishing theequivalence between the collaboration strategy design problem and Sparse PCA,we seek the answers to the following questions: 1) How much do we gain fromoptimizing the collaboration strategy? 2) What is the effect of dimensionalityreduction for different sparsity constraints? 3) How much do we lose in termsof detection performance by adopting a universal system (cost of universality)?
arxiv-15000-293 | On the Latent Variable Interpretation in Sum-Product Networks | http://arxiv.org/pdf/1601.06180v1.pdf | author:Robert Peharz, Robert Gens, Franz Pernkopf, Pedro Domingos category:cs.AI cs.LG 62 published:2016-01-22 summary:One of the central themes in Sum-Product networks (SPNs) is theinterpretation of sum nodes as marginalized latent variables (LVs). Thisinterpretation yields an increased syntactic or semantic structure, allows theapplication of the EM algorithm and to efficiently perform MPE inference. Inliterature, the LV interpretation was justified by explicitly introducing theindicator variables corresponding to the LVs' states. However, as pointed outin this paper, this approach is in conflict with the completeness condition inSPNs and does not fully specify the probabilistic model. We propose a remedyfor this problem by modifying the original approach for introducing the LVs,which we call SPN augmentation. We discuss conditional independencies inaugmented SPNs, formally establish the probabilistic interpretation of thesum-weights and give an interpretation of augmented SPNs as Bayesian networks.Based on these results, we find a sound derivation of the EM algorithm forSPNs, which was presented mistaken in literature. Furthermore, theViterbi-style algorithm for MPE proposed in literature was never proven to becorrect. We show that this is indeed a correct algorithm, when applied toaugmented SPNs. Our theoretical results are confirmed in experiments onsynthetic data and 103 real-world datasets.
arxiv-15000-294 | Learning Minimum Volume Sets and Anomaly Detectors from KNN Graphs | http://arxiv.org/pdf/1601.06105v1.pdf | author:Jonathan Root, Venkatesh Saligrama, Jing Qian category:stat.ML cs.LG published:2016-01-22 summary:We propose a non-parametric anomaly detection algorithm for high dimensionaldata. We first rank scores derived from nearest neighbor graphs on $n$-pointnominal training data. We then train limited complexity models to imitate thesescores based on the max-margin learning-to-rank framework. A test-point isdeclared as an anomaly at $\alpha$-false alarm level if the predicted score isin the $\alpha$-percentile. The resulting anomaly detector is shown to beasymptotically optimal in that for any false alarm rate $\alpha$, its decisionregion converges to the $\alpha$-percentile minimum volume level set of theunknown underlying density. In addition, we test both the statisticalperformance and computational efficiency of our algorithm on a number ofsynthetic and real-data experiments. Our results demonstrate the superiority ofour algorithm over existing $K$-NN based anomaly detection algorithms, withsignificant computational savings.
arxiv-15000-295 | Unsupervised convolutional neural networks for motion estimation | http://arxiv.org/pdf/1601.06087v1.pdf | author:Aria Ahmadi, Ioannis Patras category:cs.CV published:2016-01-22 summary:Traditional methods for motion estimation estimate the motion field F betweena pair of images as the one that minimizes a predesigned cost function. In thispaper, we propose a direct method and train a Convolutional Neural Network(CNN) that when, at test time, is given a pair of images as input it produces adense motion field F at its output layer. In the absence of large datasets withground truth motion that would allow classical supervised training, we proposeto train the network in an unsupervised manner. The proposed cost function thatis optimized during training, is based on the classical optical flowconstraint. The latter is differentiable with respect to the motion field and,therefore, allows backpropagation of the error to previous layers of thenetwork. Our method is tested on both synthetic and real image sequences andperforms similarly to the state-of-the-art methods.
arxiv-15000-296 | Why Do Urban Legends Go Viral? | http://arxiv.org/pdf/1601.06081v1.pdf | author:Marco Guerini, Carlo Strapparava category:cs.CL cs.CY cs.SI published:2016-01-22 summary:Urban legends are a genre of modern folklore, consisting of stories aboutrare and exceptional events, just plausible enough to be believed, which tendto propagate inexorably across communities. In our view, while urban legendsrepresent a form of "sticky" deceptive text, they are marked by a tensionbetween the credible and incredible. They should be credible like a newsarticle and incredible like a fairy tale to go viral. In particular we willfocus on the idea that urban legends should mimic the details of news (who,where, when) to be credible, while they should be emotional and readable like afairy tale to be catchy and memorable. Using NLP tools we will provide aquantitative analysis of these prototypical characteristics. We also lay outsome machine learning experiments showing that it is possible to recognize anurban legend using just these simple features.
arxiv-15000-297 | Bitwise Neural Networks | http://arxiv.org/pdf/1601.06071v1.pdf | author:Minje Kim, Paris Smaragdis category:cs.LG cs.AI cs.NE published:2016-01-22 summary:Based on the assumption that there exists a neural network that efficientlyrepresents a set of Boolean functions between all binary inputs and outputs, wepropose a process for developing and deploying neural networks whose weightparameters, bias terms, input, and intermediate hidden layer output signals,are all binary-valued, and require only basic bit logic for the feedforwardpass. The proposed Bitwise Neural Network (BNN) is especially suitable forresource-constrained environments, since it replaces either floating orfixed-point arithmetic with significantly more efficient bitwise operations.Hence, the BNN requires for less spatial complexity, less memory bandwidth, andless power consumption in hardware. In order to design such networks, wepropose to add a few training schemes, such as weight compression and noisybackpropagation, which result in a bitwise network that performs almost as wellas its corresponding real-valued network. We test the proposed network on theMNIST dataset, represented using binary features, and show that BNNs result incompetitive performance while offering dramatic computational savings.
arxiv-15000-298 | Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing | http://arxiv.org/pdf/1601.06068v1.pdf | author:Shashi Narayan, Siva Reddy, Shay B. Cohen category:cs.CL published:2016-01-22 summary:One of the limitations of semantic parsing approaches to open-domain questionanswering is the lexicosyntactic gap between natural language questions andknowledge base entries -- there are many ways to ask a question, all with thesame answer. In this paper we propose to bridge this gap by generatingparaphrases of the input question with the goal that at least one of them willbe correctly mapped to a knowledge-base query. We introduce a novel grammarmodel for paraphrase generation that does not require any sentence-alignedparaphrase corpus. Our key idea is to leverage the flexibility and scalabilityof latent-variable probabilistic context-free grammars to sample paraphrases.We do an extrinsic evaluation of our paraphrases by plugging them into asemantic parser for Freebase. Our evaluation experiments on the WebQuestionsbenchmark dataset show that the performance of the semantic parsersignificantly improves over strong baselines.
arxiv-15000-299 | 3-D/2-D Registration of Cardiac Structures by 3-D Contrast Agent Distribution Estimation | http://arxiv.org/pdf/1601.06062v1.pdf | author:Matthias Hoffmann, Christopher Kowalewski, Andreas Maier, Klaus Kurzidim, Norbert Strobel, Joachim Hornegger category:cs.CV published:2016-01-22 summary:For augmented fluoroscopy during cardiac catheter ablation procedures, apreoperatively acquired 3-D model of the left atrium of the patient can beregistered to X-ray images. Therefore the 3D-model is matched with the contrastagent based appearance of the left atrium. Commonly, only small amounts ofcontrast agent (CA) are used to locate the left atrium. This is why we focus onrobust registration methods that work also if the structure of interest is onlypartially contrasted. In particular, we propose two similarity measures forCA-based registration: The first similarity measure, explicit apparent edges,focuses on edges of the patient anatomy made visible by contrast agent and canbe computed quickly on the GPU. The second novel similarity measure computes acontrast agent distribution estimate (CADE) inside the 3-D model and rates itsconsistency with the CA seen in biplane fluoroscopic images. As the CADEcomputation involves a reconstruction of CA in 3-D using the CA within thefluoroscopic images, it is slower. Using a combination of both methods, ourevaluation on 11 well-contrasted clinical datasets yielded an error of7.9+/-6.3 mm over all frames. For 10 datasets with little CA, we obtained anerror of 8.8+/-6.7 mm. Our new methods outperform a registration based on theprojected shadow significantly (p<0.05).
arxiv-15000-300 | Topological descriptors for 3D surface analysis | http://arxiv.org/pdf/1601.06057v1.pdf | author:Matthias Zeppelzauer, Bartosz Zieliński, Mateusz Juda, Markus Seidl category:cs.CV published:2016-01-22 summary:We investigate topological descriptors for 3D surface analysis, i.e. theclassification of surfaces according to their geometric fine structure. On adataset of high-resolution 3D surface reconstructions we compute persistencediagrams for a 2D cubical filtration. In the next step we investigate differenttopological descriptors and measure their ability to discriminate structurallydifferent 3D surface patches. We evaluate their sensitivity to differentparameters and compare the performance of the resulting topological descriptorsto alternative (non-topological) descriptors. We present a comprehensiveevaluation that shows that topological descriptors are (i) robust, (ii) yieldstate-of-the-art performance for the task of 3D surface analysis and (iii)improve classification performance when combined with non-topologicaldescriptors.
