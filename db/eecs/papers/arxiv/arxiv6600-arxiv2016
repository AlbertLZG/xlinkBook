arxiv-6600-1 | Rate-Optimal Detection of Very Short Signal Segments | http://arxiv.org/pdf/1407.2812v1.pdf | author:T. Tony Cai, Ming Yuan category:stat.ML cs.IT math.IT math.ST stat.TH published:2014-07-10 summary:Motivated by a range of applications in engineering and genomics, we considerin this paper detection of very short signal segments in three settings:signals with known shape, arbitrary signals, and smooth signals. Optimal ratesof detection are established for the three cases and rate-optimal detectors areconstructed. The detectors are easily implementable and are based on scanningwith linear and quadratic statistics. Our analysis reveals both similaritiesand differences in the strategy and fundamental difficulty of detection amongthese three settings.
arxiv-6600-2 | Bandits Warm-up Cold Recommender Systems | http://arxiv.org/pdf/1407.2806v1.pdf | author:Jérémie Mary, Romaric Gaudel, Preux Philippe category:cs.LG cs.IR stat.ML published:2014-07-10 summary:We address the cold start problem in recommendation systems assuming nocontextual information is available neither about users, nor items. We considerthe case in which we only have access to a set of ratings of items by users.Most of the existing works consider a batch setting, and use cross-validationto tune parameters. The classical method consists in minimizing the root meansquare error over a training subset of the ratings which provides afactorization of the matrix of ratings, interpreted as a latent representationof items and users. Our contribution in this paper is 5-fold. First, weexplicit the issues raised by this kind of batch setting for users or itemswith very few ratings. Then, we propose an online setting closer to the actualuse of recommender systems; this setting is inspired by the bandit framework.The proposed methodology can be used to turn any recommender system dataset(such as Netflix, MovieLens,...) into a sequential dataset. Then, we explicit astrong and insightful link between contextual bandit algorithms and matrixfactorization; this leads us to a new algorithm that tackles theexploration/exploitation dilemma associated to the cold start problem in astrikingly new perspective. Finally, experimental evidence confirm that ouralgorithm is effective in dealing with the cold start problem on publiclyavailable datasets. Overall, the goal of this paper is to bridge the gapbetween recommender systems based on matrix factorizations and those based oncontextual bandits.
arxiv-6600-3 | What you need to know about the state-of-the-art computational models of object-vision: A tour through the models | http://arxiv.org/pdf/1407.2776v1.pdf | author:Seyed-Mahdi Khaligh-Razavi category:cs.CV cs.AI cs.LG q-bio.NC published:2014-07-10 summary:Models of object vision have been of great interest in computer vision andvisual neuroscience. During the last decades, several models have beendeveloped to extract visual features from images for object recognition tasks.Some of these were inspired by the hierarchical structure of primate visualsystem, and some others were engineered models. The models are varied inseveral aspects: models that are trained by supervision, models trained withoutsupervision, and models (e.g. feature extractors) that are fully hard-wired anddo not need training. Some of the models come with a deep hierarchicalstructure consisting of several layers, and some others are shallow and comewith only one or two layers of processing. More recently, new models have beendeveloped that are not hand-tuned but trained using millions of images, throughwhich they learn how to extract informative task-related features. Here I willsurvey all these different models and provide the reader with an intuitive, aswell as a more detailed, understanding of the underlying computations in eachof the models.
arxiv-6600-4 | A multi-instance learning algorithm based on a stacked ensemble of lazy learners | http://arxiv.org/pdf/1407.2736v1.pdf | author:Ramasubramanian Sundararajan, Hima Patel, Manisha Srivastava category:cs.LG published:2014-07-10 summary:This document describes a novel learning algorithm that classifies "bags" ofinstances rather than individual instances. A bag is labeled positive if itcontains at least one positive instance (which may or may not be specificallyidentified), and negative otherwise. This class of problems is known asmulti-instance learning problems, and is useful in situations where the classlabel at an instance level may be unavailable or imprecise or difficult toobtain, or in situations where the problem is naturally posed as one ofclassifying instance groups. The algorithm described here is an ensemble-basedmethod, wherein the members of the ensemble are lazy learning classifierslearnt using the Citation Nearest Neighbour method. Diversity among theensemble members is achieved by optimizing their parameters using amulti-objective optimization method, with the objectives being to maximizeClass 1 accuracy and minimize false positive rate. The method has been found tobe effective on the Musk1 benchmark dataset.
arxiv-6600-5 | Finito: A Faster, Permutable Incremental Gradient Method for Big Data Problems | http://arxiv.org/pdf/1407.2710v1.pdf | author:Aaron J. Defazio, Tibério S. Caetano, Justin Domke category:cs.LG stat.ML published:2014-07-10 summary:Recent advances in optimization theory have shown that smooth strongly convexfinite sums can be minimized faster than by treating them as a black box"batch" problem. In this work we introduce a new method in this class with atheoretical convergence rate four times faster than existing methods, for sumswith sufficiently many terms. This method is also amendable to a samplingwithout replacement scheme that in practice gives further speed-ups. We giveempirical results showing state of the art performance.
arxiv-6600-6 | Offline handwritten signature identification using adaptive window positioning techniques | http://arxiv.org/pdf/1407.2700v1.pdf | author:Ghazali Sulong, Anwar Yahy Ebrahim, Muhammad Jehanzeb category:cs.CV published:2014-07-10 summary:The paper presents to address this challenge, we have proposed the use ofAdaptive Window Positioning technique which focuses on not just the meaning ofthe handwritten signature but also on the individuality of the writer. Thisinnovative technique divides the handwritten signature into 13 small windows ofsize nxn(13x13).This size should be large enough to contain ample informationabout the style of the author and small enough to ensure a good identificationperformance.The process was tested with a GPDS data set containing 4870signature samples from 90 different writers by comparing the robust features ofthe test signature with that of the user signature using an appropriateclassifier. Experimental results reveal that adaptive window positioningtechnique proved to be the efficient and reliable method for accurate signaturefeature extraction for the identification of offline handwritten signatures.Thecontribution of this technique can be used to detect signatures signed underemotional duress.
arxiv-6600-7 | A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation | http://arxiv.org/pdf/1407.2697v1.pdf | author:Aaron J. Defazio, Tiberio S. Caetano category:cs.LG stat.ML published:2014-07-10 summary:A key problem in statistics and machine learning is the determination ofnetwork structure from data. We consider the case where the structure of thegraph to be reconstructed is known to be scale-free. We show that in such casesit is natural to formulate structured sparsity inducing priors using submodularfunctions, and we use their Lov\'asz extension to obtain a convex relaxation.For tractable classes such as Gaussian graphical models, this leads to a convexoptimization problem that can be efficiently solved. We show that our methodresults in an improvement in the accuracy of reconstructed networks forsynthetic data. We also show how our prior encourages scale-freereconstructions on a bioinfomatics dataset.
arxiv-6600-8 | Quality Estimation Of Machine Translation Outputs Through Stemming | http://arxiv.org/pdf/1407.2694v1.pdf | author:Pooja Gupta, Nisheeth Joshi, Iti Mathur category:cs.CL published:2014-07-10 summary:Machine Translation is the challenging problem for Indian languages. Everyday we can see some machine translators being developed, but getting a highquality automatic translation is still a very distant dream . The correcttranslated sentence for Hindi language is rarely found. In this paper, we areemphasizing on English-Hindi language pair, so in order to preserve the correctMT output we present a ranking system, which employs some machine learningtechniques and morphological features. In ranking no human intervention isrequired. We have also validated our results by comparing it with humanranking.
arxiv-6600-9 | Quantum support vector machine for big data classification | http://arxiv.org/pdf/1307.0471v3.pdf | author:Patrick Rebentrost, Masoud Mohseni, Seth Lloyd category:quant-ph cs.LG published:2013-07-01 summary:Supervised machine learning is the classification of new data based onalready classified training examples. In this work, we show that the supportvector machine, an optimized binary classifier, can be implemented on a quantumcomputer, with complexity logarithmic in the size of the vectors and the numberof training examples. In cases when classical sampling algorithms requirepolynomial time, an exponential speed-up is obtained. At the core of thisquantum big data algorithm is a non-sparse matrix exponentiation technique forefficiently performing a matrix inversion of the training data inner-product(kernel) matrix.
arxiv-6600-10 | Private Learning and Sanitization: Pure vs. Approximate Differential Privacy | http://arxiv.org/pdf/1407.2674v1.pdf | author:Amos Beimel, Kobbi Nissim, Uri Stemmer category:cs.LG cs.CR stat.ML published:2014-07-10 summary:We compare the sample complexity of private learning [Kasiviswanathan et al.2008] and sanitization~[Blum et al. 2008] under pure $\epsilon$-differentialprivacy [Dwork et al. TCC 2006] and approximate$(\epsilon,\delta)$-differential privacy [Dwork et al. Eurocrypt 2006]. We showthat the sample complexity of these tasks under approximate differentialprivacy can be significantly lower than that under pure differential privacy. We define a family of optimization problems, which we call Quasi-ConcavePromise Problems, that generalizes some of our considered tasks. We observethat a quasi-concave promise problem can be privately approximated using asolution to a smaller instance of a quasi-concave promise problem. This allowsus to construct an efficient recursive algorithm solving such problemsprivately. Specifically, we construct private learners for point functions,threshold functions, and axis-aligned rectangles in high dimension. Similarly,we construct sanitizers for point functions and threshold functions. We also examine the sample complexity of label-private learners, a relaxationof private learning where the learner is required to only protect the privacyof the labels in the sample. We show that the VC dimension completelycharacterizes the sample complexity of such learners, that is, the samplecomplexity of learning with label privacy is equal (up to constants) tolearning without privacy.
arxiv-6600-11 | Classifying Fonts and Calligraphy Styles Using Complex Wavelet Transform | http://arxiv.org/pdf/1407.2649v1.pdf | author:Alican Bozkurt, Pinar Duygulu, A. Enis Cetin category:cs.CV published:2014-07-09 summary:Recognizing fonts has become an important task in document analysis, due tothe increasing number of available digital documents in different fonts andemphases. A generic font-recognition system independent of language, script andcontent is desirable for processing various types of documents. At the sametime, categorizing calligraphy styles in handwritten manuscripts is importantfor palaeographic analysis, but has not been studied sufficiently in theliterature. We address the font-recognition problem as analysis andcategorization of textures. We extract features using complex wavelet transformand use support vector machines for classification. Extensive experimentalevaluations on different datasets in four languages and comparisons withstate-of-the-art studies show that our proposed method achieves higherrecognition accuracy while being computationally simpler. Furthermore, on a newdataset generated from Ottoman manuscripts, we show that the proposed methodcan also be used for categorizing Ottoman calligraphy with high accuracy.
arxiv-6600-12 | Learning Probabilistic Programs | http://arxiv.org/pdf/1407.2646v1.pdf | author:Yura N. Perov, Frank D. Wood category:cs.AI cs.LG stat.ML published:2014-07-09 summary:We develop a technique for generalising from data in which models aresamplers represented as program text. We establish encouraging empiricalresults that suggest that Markov chain Monte Carlo probabilistic programminginference techniques coupled with higher-order probabilistic programminglanguages are now sufficiently powerful to enable successful inference of thiskind in nontrivial domains. We also introduce a new notion of probabilisticprogram compilation and show how the same machinery might be used in the futureto compile probabilistic programs for efficient reusable predictive inference.
arxiv-6600-13 | A Statistical Modeling Approach to Computer-Aided Quantification of Dental Biofilm | http://arxiv.org/pdf/1407.2630v1.pdf | author:Awais Mansoor, Valery Patsekin, Dale Scherl, J. Paul Robinson, Bartlomiej Rajwa category:cs.CV published:2014-07-09 summary:Biofilm is a formation of microbial material on tooth substrata. Severalmethods to quantify dental biofilm coverage have recently been reported in theliterature, but at best they provide a semi-automated approach toquantification with significant input from a human grader that comes with thegraders bias of what are foreground, background, biofilm, and tooth.Additionally, human assessment indices limit the resolution of thequantification scale; most commercial scales use five levels of quantificationfor biofilm coverage (0%, 25%, 50%, 75%, and 100%). On the other hand, currentstate-of-the-art techniques in automatic plaque quantification fail to maketheir way into practical applications owing to their inability to incorporatehuman input to handle misclassifications. This paper proposes a new interactivemethod for biofilm quantification in Quantitative light-induced fluorescence(QLF) images of canine teeth that is independent of the perceptual bias of thegrader. The method partitions a QLF image into segments of uniform texture andintensity called superpixels; every superpixel is statistically modeled as arealization of a single 2D Gaussian Markov random field (GMRF) whose parametersare estimated; the superpixel is then assigned to one of three classes(background, biofilm, tooth substratum) based on the training set of data. Thequantification results show a high degree of consistency and precision. At thesame time, the proposed method gives pathologists full control to post-processthe automatic quantification by flipping misclassified superpixels to adifferent state (background, tooth, biofilm) with a single click, providinggreater usability than simply marking the boundaries of biofilm and tooth asdone by current state-of-the-art methods.
arxiv-6600-14 | Approximate resilience, monotonicity, and the complexity of agnostic learning | http://arxiv.org/pdf/1405.5268v2.pdf | author:Dana Dachman-Soled, Vitaly Feldman, Li-Yang Tan, Andrew Wan, Karl Wimmer category:cs.LG cs.CC cs.DM published:2014-05-21 summary:A function $f$ is $d$-resilient if all its Fourier coefficients of degree atmost $d$ are zero, i.e., $f$ is uncorrelated with all low-degree parities. Westudy the notion of $\mathit{approximate}$ $\mathit{resilience}$ of Booleanfunctions, where we say that $f$ is $\alpha$-approximately $d$-resilient if $f$is $\alpha$-close to a $[-1,1]$-valued $d$-resilient function in $\ell_1$distance. We show that approximate resilience essentially characterizes thecomplexity of agnostic learning of a concept class $C$ over the uniformdistribution. Roughly speaking, if all functions in a class $C$ are far frombeing $d$-resilient then $C$ can be learned agnostically in time $n^{O(d)}$ andconversely, if $C$ contains a function close to being $d$-resilient thenagnostic learning of $C$ in the statistical query (SQ) framework of Kearns hascomplexity of at least $n^{\Omega(d)}$. This characterization is based on theduality between $\ell_1$ approximation by degree-$d$ polynomials andapproximate $d$-resilience that we establish. In particular, it implies that$\ell_1$ approximation by low-degree polynomials, known to be sufficient foragnostic learning over product distributions, is in fact necessary. Focusing on monotone Boolean functions, we exhibit the existence ofnear-optimal $\alpha$-approximately$\widetilde{\Omega}(\alpha\sqrt{n})$-resilient monotone functions for all$\alpha>0$. Prior to our work, it was conceivable even that every monotonefunction is $\Omega(1)$-far from any $1$-resilient function. Furthermore, weconstruct simple, explicit monotone functions based on ${\sf Tribes}$ and ${\sfCycleRun}$ that are close to highly resilient functions. Our constructions arebased on a fairly general resilience analysis and amplification. Thesestructural results, together with the characterization, imply nearly optimallower bounds for agnostic learning of monotone juntas.
arxiv-6600-15 | A least-squares method for sparse low rank approximation of multivariate functions | http://arxiv.org/pdf/1305.0030v2.pdf | author:Mathilde Chevreuil, Régis Lebrun, Anthony Nouy, Prashant Rai category:math.NA stat.ML published:2013-04-30 summary:In this paper, we propose a low-rank approximation method based on discreteleast-squares for the approximation of a multivariate function from random,noisy-free observations. Sparsity inducing regularization techniques are usedwithin classical algorithms for low-rank approximation in order to exploit thepossible sparsity of low-rank approximations. Sparse low-rank approximationsare constructed with a robust updated greedy algorithm which includes anoptimal selection of regularization parameters and approximation ranks usingcross validation techniques. Numerical examples demonstrate the capability ofapproximating functions of many variables even when very few functionevaluations are available, thus proving the interest of the proposed algorithmfor the propagation of uncertainties through complex computational models.
arxiv-6600-16 | A Survey of Named Entity Recognition in Assamese and other Indian Languages | http://arxiv.org/pdf/1407.2918v1.pdf | author:Gitimoni Talukdar, Pranjal Protim Borah, Arup Baruah category:cs.CL published:2014-07-09 summary:Named Entity Recognition is always important when dealing with major NaturalLanguage Processing tasks such as information extraction, question-answering,machine translation, document summarization etc so in this paper we put forwarda survey of Named Entities in Indian Languages with particular reference toAssamese. There are various rule-based and machine learning approachesavailable for Named Entity Recognition. At the very first of the paper we givean idea of the available approaches for Named Entity Recognition and then wediscuss about the related research in this field. Assamese like other Indianlanguages is agglutinative and suffers from lack of appropriate resources asNamed Entity Recognition requires large data sets, gazetteer list, dictionaryetc and some useful feature like capitalization as found in English cannot befound in Assamese. Apart from this we also describe some of the issues faced inAssamese while doing Named Entity Recognition.
arxiv-6600-17 | CNN: Single-label to Multi-label | http://arxiv.org/pdf/1406.5726v3.pdf | author:Yunchao Wei, Wei Xia, Junshi Huang, Bingbing Ni, Jian Dong, Yao Zhao, Shuicheng Yan category:cs.CV published:2014-06-22 summary:Convolutional Neural Network (CNN) has demonstrated promising performance insingle-label image classification tasks. However, how CNN best copes withmulti-label images still remains an open problem, mainly due to the complexunderlying object layouts and insufficient multi-label training images. In thiswork, we propose a flexible deep CNN infrastructure, calledHypotheses-CNN-Pooling (HCP), where an arbitrary number of object segmenthypotheses are taken as the inputs, then a shared CNN is connected with eachhypothesis, and finally the CNN output results from different hypotheses areaggregated with max pooling to produce the ultimate multi-label predictions.Some unique characteristics of this flexible deep CNN infrastructure include:1) no ground truth bounding box information is required for training; 2) thewhole HCP infrastructure is robust to possibly noisy and/or redundanthypotheses; 3) no explicit hypothesis label is required; 4) the shared CNN maybe well pre-trained with a large-scale single-label image dataset, e.g.ImageNet; and 5) it may naturally output multi-label prediction results.Experimental results on Pascal VOC2007 and VOC2012 multi-label image datasetswell demonstrate the superiority of the proposed HCP infrastructure over otherstate-of-the-arts. In particular, the mAP reaches 84.2% by HCP only and 90.3%after the fusion with our complementary result in [47] based on hand-craftedfeatures on the VOC2012 dataset, which significantly outperforms thestate-of-the-arts with a large margin of more than 7%.
arxiv-6600-18 | Online Stroke and Akshara Recognition GUI in Assamese Language Using Hidden Markov Model | http://arxiv.org/pdf/1407.2390v1.pdf | author:SRM Prasanna, Rituparna Devi, Deepjoy Das, Subhankar Ghosh, Krishna Naik category:cs.CV published:2014-07-09 summary:The work describes the development of Online Assamese Stroke & AksharaRecognizer based on a set of language rules. In handwriting literature strokesare composed of two coordinate trace in between pen down and pen up labels. TheAssamese aksharas are combination of a number of strokes, the maximum number ofstrokes taken to make a combination being eight. Based on these combinationseight language rule models have been made which are used to test if a set ofstrokes form a valid akshara. A Hidden Markov Model is used to train 181different stroke patterns which generates a model used during stroke leveltesting. Akshara level testing is performed by integrating a GUI (provided byCDAC-Pune) with the Binaries of HTK toolkit classifier, HMM train model and thelanguage rules using a dynamic linked library (dll). We have got a stroke levelperformance of 94.14% and akshara level performance of 84.2%.
arxiv-6600-19 | Representation as a Service | http://arxiv.org/pdf/1404.4108v2.pdf | author:Ouais Alsharif, Philip Bachman, Joelle Pineau category:cs.LG published:2014-02-24 summary:Consider a Machine Learning Service Provider (MLSP) designed to rapidlycreate highly accurate learners for a never-ending stream of new tasks. Thechallenge is to produce task-specific learners that can be trained from fewlabeled samples, even if tasks are not uniquely identified, and the number oftasks and input dimensionality are large. In this paper, we argue that the MLSPshould exploit knowledge from previous tasks to build a good representation ofthe environment it is in, and more precisely, that useful representations forsuch a service are ones that minimize generalization error for a new hypothesistrained on a new task. We formalize this intuition with a novel method thatminimizes an empirical proxy of the intra-task small-sample generalizationerror. We present several empirical results showing state-of-the artperformance on single-task transfer, multitask learning, and the full lifelonglearning problem.
arxiv-6600-20 | PatchLift: Fast and Exact Computation of Patch Distances using Lifting, with Applications to Non-Local Means | http://arxiv.org/pdf/1407.2343v1.pdf | author:Kunal Narayan Chaudhury category:cs.CV published:2014-07-09 summary:In this paper, we propose a fast algorithm called PatchLift for computingdistances between patches extracted from a one-dimensional signal. PatchLift isbased on the observation that the patch distances can be expressed in terms ofsimple moving sums of an image, which is derived from the one-dimensionalsignal via lifting. We apply PatchLift to develop a separable extension of theclassical Non-Local Means (NLM) algorithm which is at least 100 times fasterthan NLM for standard parameter settings. The PSNR obtained using the proposedextension is typically close to (and often larger than) the PSNRs obtainedusing the original NLM. We provide some simulations results to demonstrate theacceleration achieved using separability and PatchLift.
arxiv-6600-21 | The jump set under geometric regularisation. Part 2: Higher-order approaches | http://arxiv.org/pdf/1407.2334v1.pdf | author:Tuomo Valkonen category:math.FA cs.CV published:2014-07-09 summary:In Part 1, we developed a new technique based on Lipschitz pushforwards forproving the jump set containment property $\mathcal{H}^{m-1}(J_u \setminusJ_f)=0$ of solutions $u$ to total variation denoising. We demonstrated that thetechnique also applies to Huber-regularised TV. Now, in this Part 2, we extendthe technique to higher-order regularisers. We are not quite able to prove theproperty for total generalised variation (TGV) based on the symmetrisedgradient for the second-order term. We show that the property holds under threeconditions: First, the solution $u$ is locally bounded. Second, thesecond-order variable is of locally bounded variation, $w \in\mbox{BV}_\mbox{loc}(\Omega; \mathbb{R}^m)$, instead of just boundeddeformation, $w \in \mbox{BD}(\Omega)$. Third, $w$ does not jump on $J_u$parallel to it. The second condition can be achieved for non-symmetric TGV.Both the second and third condition can be achieved if we change the Radon (or$L^1$) norm of the symmetrised gradient $Ew$ into an $L^p$ norm, $p>1$, inwhich case Korn's inequality holds. We also consider the application of thetechnique to infimal convolution TV, and study the limiting behaviour of thesingular part of $D u$, as the second parameter of $\mbox{TGV}^2$ goes to zero.Unsurprisingly, it vanishes, but in numerical discretisations the situationlooks quite different. Finally, our work additionally includes a result onTGV-strict approximation in $\mbox{BV}(\Omega)$.
arxiv-6600-22 | Collaborative Recommendation with Auxiliary Data: A Transfer Learning View | http://arxiv.org/pdf/1407.2919v1.pdf | author:Weike Pan category:cs.IR cs.LG published:2014-07-09 summary:Intelligent recommendation technology has been playing an increasinglyimportant role in various industry applications such as e-commerce productpromotion and Internet advertisement display. Besides users' feedbacks (e.g.,numerical ratings) on items as usually exploited by some typical recommendationalgorithms, there are often some additional data such as users' social circlesand other behaviors. Such auxiliary data are usually related to users'preferences on items behind the numerical ratings. Collaborative recommendationwith auxiliary data (CRAD) aims to leverage such additional information so asto improve the personalization services, which have received much attentionfrom both researchers and practitioners. Transfer learning (TL) is proposed to extract and transfer knowledge fromsome auxiliary data in order to assist the learning task on some target data.In this paper, we consider the CRAD problem from a transfer learning view,especially on how to achieve knowledge transfer from some auxiliary data.First, we give a formal definition of transfer learning for CRAD (TL-CRAD).Second, we extend the existing categorization of TL techniques (i.e., adaptive,collective and integrative knowledge transfer algorithm styles) with threeknowledge transfer strategies (i.e., prediction rule, regularization andconstraint). Third, we propose a novel generic knowledge transfer framework forTL-CRAD. Fourth, we describe some representative works of each specificknowledge transfer strategy of each algorithm style in detail, which areexpected to inspire further works. Finally, we conclude the paper with somesummary discussions and several future directions.
arxiv-6600-23 | Optimal Cooperative Cognitive Relaying and Spectrum Access for an Energy Harvesting Cognitive Radio: Reinforcement Learning Approach | http://arxiv.org/pdf/1403.7735v2.pdf | author:Ahmed El Shafie, Tamer Khattab, Hussien Saad, Amr Mohamed category:cs.NI cs.IT cs.LG math.IT published:2014-03-30 summary:In this paper, we consider a cognitive setting under the context ofcooperative communications, where the cognitive radio (CR) user is assumed tobe a self-organized relay for the network. The CR user and the PU are assumedto be energy harvesters. The CR user cooperatively relays some of theundelivered packets of the primary user (PU). Specifically, the CR user storesa fraction of the undelivered primary packets in a relaying queue (buffer). Itmanages the flow of the undelivered primary packets to its relaying queue usingthe appropriate actions over time slots. Moreover, it has the decision ofchoosing the used queue for channel accessing at idle time slots (slots wherethe PU's queue is empty). It is assumed that one data packet transmissiondissipates one energy packet. The optimal policy changes according to theprimary and CR users arrival rates to the data and energy queues as well as thechannels connectivity. The CR user saves energy for the PU by taking theresponsibility of relaying the undelivered primary packets. It optimallyorganizes its own energy packets to maximize its payoff as time progresses.
arxiv-6600-24 | Inferring latent structures via information inequalities | http://arxiv.org/pdf/1407.2256v1.pdf | author:R. Chaves, L. Luft, T. O. Maciel, D. Gross, D. Janzing, B. Schölkopf category:stat.ML quant-ph published:2014-07-08 summary:One of the goals of probabilistic inference is to decide whether anempirically observed distribution is compatible with a candidate Bayesiannetwork. However, Bayesian networks with hidden variables give rise to highlynon-trivial constraints on the observed distribution. Here, we propose aninformation-theoretic approach, based on the insight that conditions onentropies of Bayesian networks take the form of simple linear inequalities. Wedescribe an algorithm for deriving entropic tests for latent structures. Thewell-known conditional independence tests appear as a special case. While theapproach applies for generic Bayesian networks, we presently adopt the causalview, and show the versatility of the framework by treating several relevantproblems from that domain: detecting common ancestors, quantifying the strengthof causal influence, and inferring the direction of causation from two-variablemarginals.
arxiv-6600-25 | Meteorological time series forecasting with pruned multi-layer perceptron and 2-stage Levenberg-Marquardt method | http://arxiv.org/pdf/1407.2169v1.pdf | author:Cyril Voyant, Wani W. Tamas, Marie Laure Nivet, Gilles Notton, Christophe Paoli, Aurélia Balu, Marc Muselli category:cs.NE cs.SY published:2014-07-08 summary:A Multi-Layer Perceptron (MLP) defines a family of artificial neural networksoften used in TS modeling and forecasting. Because of its "black box" aspect,many researchers refuse to use it. Moreover, the optimization (often based onthe exhaustive approach where "all" configurations are tested) and learningphases of this artificial intelligence tool (often based on theLevenberg-Marquardt algorithm; LMA) are weaknesses of this approach(exhaustively and local minima). These two tasks must be repeated depending onthe knowledge of each new problem studied, making the process, long, laboriousand not systematically robust. In this paper a pruning process is proposed.This method allows, during the training phase, to carry out an inputs selectingmethod activating (or not) inter-nodes connections in order to verify ifforecasting is improved. We propose to use iteratively the popular dampedleast-squares method to activate inputs and neurons. A first pass is applied to10% of the learning sample to determine weights significantly different from 0and delete other. Then a classical batch process based on LMA is used with thenew MLP. The validation is done using 25 measured meteorological TS andcross-comparing the prediction results of the classical LMA and the 2-stageLMA.
arxiv-6600-26 | Assamese-English Bilingual Machine Translation | http://arxiv.org/pdf/1407.2019v1.pdf | author:Kalyanee Kanchan Baruah, Pranjal Das, Abdul Hannan, Shikhar Kr. Sarma category:cs.CL published:2014-07-08 summary:Machine translation is the process of translating text from one language toanother. In this paper, Statistical Machine Translation is done on Assamese andEnglish language by taking their respective parallel corpus. A statisticalphrase based translation toolkit Moses is used here. To develop the languagemodel and to align the words we used two another tools IRSTLM, GIZArespectively. BLEU score is used to check our translation system performance,how good it is. A difference in BLEU scores is obtained while translatingsentences from Assamese to English and vice-versa. Since Indian languages aremorphologically very rich hence translation is relatively harder from Englishto Assamese resulting in a low BLEU score. A statistical transliteration systemis also introduced with our translation system to deal basically with propernouns, OOV (out of vocabulary) words which are not present in our corpus.
arxiv-6600-27 | Log-Euclidean Bag of Words for Human Action Recognition | http://arxiv.org/pdf/1406.2139v2.pdf | author:Masoud Faraki, Maziar Palhang, Conrad Sanderson category:cs.CV I.4.9; I.5.4 published:2014-06-09 summary:Representing videos by densely extracted local space-time features hasrecently become a popular approach for analysing actions. In this paper, wetackle the problem of categorising human actions by devising Bag of Words (BoW)models based on covariance matrices of spatio-temporal features, with thefeatures formed from histograms of optical flow. Since covariance matrices forma special type of Riemannian manifold, the space of Symmetric Positive Definite(SPD) matrices, non-Euclidean geometry should be taken into account whilediscriminating between covariance matrices. To this end, we propose to embedSPD manifolds to Euclidean spaces via a diffeomorphism and extend the BoWapproach to its Riemannian version. The proposed BoW approach takes intoaccount the manifold geometry of SPD matrices during the generation of thecodebook and histograms. Experiments on challenging human action datasets showthat the proposed method obtains notable improvements in discriminationaccuracy, in comparison to several state-of-the-art methods.
arxiv-6600-28 | A Critical Reassessment of Evolutionary Algorithms on the cryptanalysis of the simplified data encryption standard algorithm | http://arxiv.org/pdf/1407.1993v1.pdf | author:Fabien Teytaud, Cyril Fonlupt category:cs.CR cs.NE published:2014-07-08 summary:In this paper we analyze the cryptanalysis of the simplified data encryptionstandard algorithm using meta-heuristics and in particular genetic algorithms.The classic fitness function when using such an algorithm is to compare n-gramstatistics of a the decrypted message with those of the target message. We showthat using such a function is irrelevant in case of Genetic Algorithm, simplybecause there is no correlation between the distance to the real key (theoptimum) and the value of the fitness, in other words, there is no hiddengradient. In order to emphasize this assumption we experimentally show that agenetic algorithm perform worse than a random search on the cryptanalysis ofthe simplified data encryption standard algorithm.
arxiv-6600-29 | Inter-Rater Agreement Study on Readability Assessment in Bengali | http://arxiv.org/pdf/1407.1976v1.pdf | author:Shanta Phani, Shibamouli Lahiri, Arindam Biswas category:cs.CL published:2014-07-08 summary:An inter-rater agreement study is performed for readability assessment inBengali. A 1-7 rating scale was used to indicate different levels ofreadability. We obtained moderate to fair agreement among seven independentannotators on 30 text passages written by four eminent Bengali authors. As a byproduct of our study, we obtained a readability-annotated ground truth datasetin Bengali. .
arxiv-6600-30 | Regression-Based Image Alignment for General Object Categories | http://arxiv.org/pdf/1407.1957v1.pdf | author:Hilton Bristow, Simon Lucey category:cs.CV published:2014-07-08 summary:Gradient-descent methods have exhibited fast and reliable performance forimage alignment in the facial domain, but have largely been ignored by thebroader vision community. They require the image function be smooth and(numerically) differentiable -- properties that hold for pixel-basedrepresentations obeying natural image statistics, but not for more generalclasses of non-linear feature transforms. We show that transforms such as DenseSIFT can be incorporated into a Lucas Kanade alignment framework by predictingdescent directions via regression. This enables robust matching of instancesfrom general object categories whilst maintaining desirable properties of LucasKanade such as the capacity to handle high-dimensional warp parametrizationsand a fast rate of convergence. We present alignment results on a number ofobjects from ImageNet, and an extension of the method to unsupervised jointalignment of objects from a corpus of images.
arxiv-6600-31 | Bregman Alternating Direction Method of Multipliers | http://arxiv.org/pdf/1306.3203v3.pdf | author:Huahua Wang, Arindam Banerjee category:math.OC cs.LG stat.ML published:2013-06-13 summary:The mirror descent algorithm (MDA) generalizes gradient descent by using aBregman divergence to replace squared Euclidean distance. In this paper, wesimilarly generalize the alternating direction method of multipliers (ADMM) toBregman ADMM (BADMM), which allows the choice of different Bregman divergencesto exploit the structure of problems. BADMM provides a unified framework forADMM and its variants, including generalized ADMM, inexact ADMM and Bethe ADMM. We establish the global convergence and the $O(1/T)$ iteration complexity forBADMM. In some cases, BADMM can be faster than ADMM by a factor of$O(n/\log(n))$. In solving the linear program of mass transportation problem,BADMM leads to massive parallelism and can easily run on GPU. BADMM is severaltimes faster than highly optimized commercial software Gurobi.
arxiv-6600-32 | Lexpresso: a Controlled Natural Language | http://arxiv.org/pdf/1407.1933v1.pdf | author:Adam Saulwick category:cs.CL cs.AI published:2014-07-08 summary:This paper presents an overview of `Lexpresso', a Controlled Natural Languagedeveloped at the Defence Science & Technology Organisation as a bidirectionalnatural language interface to a high-level information fusion system. The paperdescribes Lexpresso's main features including lexical coverage, expressivenessand range of linguistic syntactic and semantic structures. It also touches onits tight integration with a formal semantic formalism and tentativelyclassifies it against the PENS system.
arxiv-6600-33 | A higher-order MRF based variational model for multiplicative noise reduction | http://arxiv.org/pdf/1404.5344v3.pdf | author:Yunjin Chen, Wensen Feng, René Ranftl, Hong Qiao, Thomas Pock category:cs.CV published:2014-04-21 summary:The Fields of Experts (FoE) image prior model, a filter-based higher-orderMarkov Random Fields (MRF) model, has been shown to be effective for many imagerestoration problems. Motivated by the successes of FoE-based approaches, inthis letter, we propose a novel variational model for multiplicative noisereduction based on the FoE image prior model. The resulted model corresponds toa non-convex minimization problem, which can be solved by a recently publishednon-convex optimization algorithm. Experimental results based on syntheticspeckle noise and real synthetic aperture radar (SAR) images suggest that theperformance of our proposed method is on par with the best publisheddespeckling algorithm. Besides, our proposed model comes along with anadditional advantage, that the inference is extremely efficient. {Our GPU basedimplementation takes less than 1s to produce state-of-the-art despecklingperformance.}
arxiv-6600-34 | Recommending Learning Algorithms and Their Associated Hyperparameters | http://arxiv.org/pdf/1407.1890v1.pdf | author:Michael R. Smith, Logan Mitchell, Christophe Giraud-Carrier, Tony Martinez category:cs.LG stat.ML published:2014-07-07 summary:The success of machine learning on a given task dependson, among otherthings, which learning algorithm is selected and its associatedhyperparameters. Selecting an appropriate learning algorithm and setting itshyperparameters for a given data set can be a challenging task, especially forusers who are not experts in machine learning. Previous work has examined usingmeta-features to predict which learning algorithm and hyperparameters should beused. However, choosing a set of meta-features that are predictive of algorithmperformance is difficult. Here, we propose to apply collaborative filteringtechniques to learning algorithm and hyperparameter selection, and find thatdoing so avoids determining which meta-features to use and outperformstraditional meta-learning approaches in many cases.
arxiv-6600-35 | Spectral norm of random tensors | http://arxiv.org/pdf/1407.1870v1.pdf | author:Ryota Tomioka, Taiji Suzuki category:math.ST stat.ML stat.TH published:2014-07-07 summary:We show that the spectral norm of a random $n_1\times n_2\times \cdots \timesn_K$ tensor (or higher-order array) scales as$O\left(\sqrt{(\sum_{k=1}^{K}n_k)\log(K)}\right)$ under some sub-Gaussianassumption on the entries. The proof is based on a covering number argument.Since the spectral norm is dual to the tensor nuclear norm (the tightest convexrelaxation of the set of rank one tensors), the bound implies that the convexrelaxation yields sample complexity that is linear in (the sum of) the numberof dimensions, which is much smaller than other recently proposed convexrelaxations of tensor rank that use unfolding.
arxiv-6600-36 | Simultaneous Detection and Segmentation | http://arxiv.org/pdf/1407.1808v1.pdf | author:Bharath Hariharan, Pablo Arbeláez, Ross Girshick, Jitendra Malik category:cs.CV published:2014-07-07 summary:We aim to detect all instances of a category in an image and, for eachinstance, mark the pixels that belong to it. We call this task SimultaneousDetection and Segmentation (SDS). Unlike classical bounding box detection, SDSrequires a segmentation and not just a box. Unlike classical semanticsegmentation, we require individual object instances. We build on recent workthat uses convolutional neural networks to classify category-independent regionproposals (R-CNN [16]), introducing a novel architecture tailored for SDS. Wethen use category-specific, top- down figure-ground predictions to refine ourbottom-up proposals. We show a 7 point boost (16% relative) over our baselineson SDS, a 5 point boost (10% relative) over state-of-the-art on semanticsegmentation, and state-of-the-art performance in object detection. Finally, weprovide diagnostic tools that unpack performance and provide directions forfuture work.
arxiv-6600-37 | DimmWitted: A Study of Main-Memory Statistical Analytics | http://arxiv.org/pdf/1403.7550v3.pdf | author:Ce Zhang, Christopher Ré category:cs.DB cs.LG math.OC stat.ML published:2014-03-28 summary:We perform the first study of the tradeoff space of access methods andreplication to support statistical analytics using first-order methods executedin the main memory of a Non-Uniform Memory Access (NUMA) machine. Statisticalanalytics systems differ from conventional SQL-analytics in the amount andtypes of memory incoherence they can tolerate. Our goal is to understandtradeoffs in accessing the data in row- or column-order and at what granularityone should share the model and data for a statistical task. We study this newtradeoff space, and discover there are tradeoffs between hardware andstatistical efficiency. We argue that our tradeoff study may provide valuableinformation for designers of analytics engines: for each system we consider,our prototype engine can run at least one popular task at least 100x faster. Weconduct our study across five architectures using popular models includingSVMs, logistic regression, Gibbs sampling, and neural networks.
arxiv-6600-38 | Downscaling near-surface atmospheric fields with multi-objective Genetic Programming | http://arxiv.org/pdf/1407.1768v1.pdf | author:Tanja Zerenner, Victor Venema, Petra Friederichs, Clemens Simmer category:physics.ao-ph cs.NE published:2014-07-07 summary:The coupling of models for the different components of theSoil-Vegetation-Atmosphere-System is required to investigate componentinteractions and feedback processes. However, the component models foratmosphere, land-surface and subsurface are usually operated at differentresolutions in space and time owing to the dominant processes. Thecomputationally often more expensive atmospheric models, for instance, aretypically employed at a coarser resolution than land-surface and subsurfacemodels. Thus up- and downscaling procedures are required at the interfacebetween the atmospheric model and the land-surface/subsurface models. We applymulti-objective Genetic Programming (GP) to a training data set ofhigh-resolution atmospheric model runs to learn equations or short programsthat reconstruct the fine-scale fields (e.g., 400 m resolution) of thenear-surface atmospheric state variables from the coarse atmospheric modeloutput (e.g., 2.8 km resolution). Like artificial neural networks, GP canflexibly incorporate multivariate and nonlinear relations, but offers theadvantage that the solutions are human readable and thus can be checked forphysical consistency. Using the Strength Pareto Approach for multi-objectivefitness assignment allows us to consider multiple characteristics of thefine-scale fields during the learning procedure.
arxiv-6600-39 | The Primal-Dual Hybrid Gradient Method for Semiconvex Splittings | http://arxiv.org/pdf/1407.1723v1.pdf | author:Thomas Möllenhoff, Evgeny Strekalovskiy, Michael Moeller, Daniel Cremers category:math.NA cs.CV cs.NA math.OC published:2014-07-07 summary:This paper deals with the analysis of a recent reformulation of theprimal-dual hybrid gradient method [Zhu and Chan 2008, Pock, Cremers, Bischofand Chambolle 2009, Esser, Zhang and Chan 2010, Chambolle and Pock 2011], whichallows to apply it to nonconvex regularizers as first proposed for truncatedquadratic penalization in [Strekalovskiy and Cremers 2014]. Particularly, itinvestigates variational problems for which the energy to be minimized can bewritten as $G(u) + F(Ku)$, where $G$ is convex, $F$ semiconvex, and $K$ is alinear operator. We study the method and prove convergence in the case wherethe nonconvexity of $F$ is compensated by the strong convexity of the $G$. Theconvergence proof yields an interesting requirement for the choice of algorithmparameters, which we show to not only be sufficient, but necessary.Additionally, we show boundedness of the iterates under much weaker conditions.Finally, we demonstrate effectiveness and convergence of the algorithm beyondthe theoretical guarantees in several numerical experiments.
arxiv-6600-40 | WordRep: A Benchmark for Research on Learning Word Representations | http://arxiv.org/pdf/1407.1640v1.pdf | author:Bin Gao, Jiang Bian, Tie-Yan Liu category:cs.CL cs.LG published:2014-07-07 summary:WordRep is a benchmark collection for the research on learning distributedword representations (or word embeddings), released by Microsoft Research. Inthis paper, we describe the details of the WordRep collection and show how touse it in different types of machine learning research related to wordembedding. Specifically, we describe how the evaluation tasks in WordRep areselected, how the data are sampled, and how the evaluation tool is built. Wethen compare several state-of-the-art word representations on WordRep, reporttheir evaluation performance, and make discussions on the results. After that,we discuss new potential research topics that can be supported by WordRep, inaddition to algorithm comparison. We hope that this paper can help people gaindeeper understanding of WordRep, and enable more interesting research onlearning distributed word representations and related topics.
arxiv-6600-41 | Neural Codes for Image Retrieval | http://arxiv.org/pdf/1404.1777v2.pdf | author:Artem Babenko, Anton Slesarev, Alexandr Chigorin, Victor Lempitsky category:cs.CV published:2014-04-07 summary:It has been shown that the activations invoked by an image within the toplayers of a large convolutional neural network provide a high-level descriptorof the visual content of the image. In this paper, we investigate the use ofsuch descriptors (neural codes) within the image retrieval application. In theexperiments with several standard retrieval benchmarks, we establish thatneural codes perform competitively even when the convolutional neural networkhas been trained for an unrelated classification task (e.g.\ Image-Net). Wealso evaluate the improvement in the retrieval performance of neural codes,when the network is retrained on a dataset of images that are similar to imagesencountered at test time. We further evaluate the performance of the compressed neural codes and showthat a simple PCA compression provides very good short codes that givestate-of-the-art accuracy on a number of datasets. In general, neural codesturn out to be much more resilient to such compression in comparison otherstate-of-the-art descriptors. Finally, we show that discriminativedimensionality reduction trained on a dataset of pairs of matched photographsimproves the performance of PCA-compressed neural codes even further. Overall,our quantitative experiments demonstrate the promise of neural codes as visualdescriptors for image retrieval.
arxiv-6600-42 | Les noms propres se traduisent-ils ? Étude d'un corpus multilingue | http://arxiv.org/pdf/1407.1605v1.pdf | author:Émeline Lecuit, Denis Maurel, Dusko Vitas category:cs.CL published:2014-07-07 summary:In this paper, we tackle the problem of the translation of proper names. Weintroduce our hypothesis according to which proper names can be translated moreoften than most people seem to think. Then, we describe the construction of aparallel multilingual corpus used to illustrate our point. We eventuallyevaluate both the advantages and limits of this corpus in our study.
arxiv-6600-43 | A Clustering Approach to Learn Sparsely-Used Overcomplete Dictionaries | http://arxiv.org/pdf/1309.1952v2.pdf | author:Alekh Agarwal, Animashree Anandkumar, Praneeth Netrapalli category:stat.ML cs.LG math.OC published:2013-09-08 summary:We consider the problem of learning overcomplete dictionaries in the contextof sparse coding, where each sample selects a sparse subset of dictionaryelements. Our main result is a strategy to approximately recover the unknowndictionary using an efficient algorithm. Our algorithm is a clustering-styleprocedure, where each cluster is used to estimate a dictionary element. Theresulting solution can often be further cleaned up to obtain a high accuracyestimate, and we provide one simple scenario where $\ell_1$-regularizedregression can be used for such a second stage.
arxiv-6600-44 | Large-Scale Multi-Label Learning with Incomplete Label Assignments | http://arxiv.org/pdf/1407.1538v1.pdf | author:Xiangnan Kong, Zhaoming Wu, Li-Jia Li, Ruofei Zhang, Philip S. Yu, Hang Wu, Wei Fan category:cs.LG published:2014-07-06 summary:Multi-label learning deals with the classification problems where eachinstance can be assigned with multiple labels simultaneously. Conventionalmulti-label learning approaches mainly focus on exploiting label correlations.It is usually assumed, explicitly or implicitly, that the label sets fortraining instances are fully labeled without any missing labels. However, inmany real-world multi-label datasets, the label assignments for traininginstances can be incomplete. Some ground-truth labels can be missed by thelabeler from the label set. This problem is especially typical when the numberinstances is very large, and the labeling cost is very high, which makes italmost impossible to get a fully labeled training set. In this paper, we studythe problem of large-scale multi-label learning with incomplete labelassignments. We propose an approach, called MPU, based upon positive andunlabeled stochastic gradient descent and stacked models. Unlike prior works,our method can effectively and efficiently consider missing labels and labelcorrelations simultaneously, and is very scalable, that has linear timecomplexities over the size of the data. Extensive experiments on two real-worldmulti-label datasets show that our MPU model consistently outperform othercommonly-used baselines.
arxiv-6600-45 | Bandit Online Optimization Over the Permutahedron | http://arxiv.org/pdf/1312.1530v2.pdf | author:Nir Ailon, Kohei Hatano, Eiji Takimoto category:cs.LG published:2013-12-05 summary:The permutahedron is the convex polytope with vertex set consisting of thevectors $(\pi(1),\dots, \pi(n))$ for all permutations (bijections) $\pi$ over$\{1,\dots, n\}$. We study a bandit game in which, at each step $t$, anadversary chooses a hidden weight weight vector $s_t$, a player chooses avertex $\pi_t$ of the permutahedron and suffers an observed loss of$\sum_{i=1}^n \pi(i) s_t(i)$. A previous algorithm CombBand of Cesa-Bianchi et al (2009) guarantees aregret of $O(n\sqrt{T \log n})$ for a time horizon of $T$. Unfortunately,CombBand requires at each step an $n$-by-$n$ matrix permanent approximation towithin improved accuracy as $T$ grows, resulting in a total running time thatis super linear in $T$, making it impractical for large time horizons. We provide an algorithm of regret $O(n^{3/2}\sqrt{T})$ with total timecomplexity $O(n^3T)$. The ideas are a combination of CombBand and a recentalgorithm by Ailon (2013) for online optimization over the permutahedron in thefull information setting. The technical core is a bound on the variance of thePlackett-Luce noisy sorting process's "pseudo loss". The bound is obtained byestablishing positive semi-definiteness of a family of 3-by-3 matricesgenerated from rational functions of exponentials of 3 parameters.
arxiv-6600-46 | Large-scale Supervised Hierarchical Feature Learning for Face Recognition | http://arxiv.org/pdf/1407.1490v1.pdf | author:Jianguo Li, Yurong Chen category:cs.CV published:2014-07-06 summary:This paper proposes a novel face recognition algorithm based on large-scalesupervised hierarchical feature learning. The approach consists of two parts:hierarchical feature learning and large-scale model learning. The hierarchicalfeature learning searches feature in three levels of granularity in asupervised way. First, face images are modeled by receptive field theory, andthe representation is an image with many channels of Gaussian receptive maps.We activate a few most distinguish channels by supervised learning. Second, theface image is further represented by patches of picked channels, and we searchfrom the over-complete patch pool to activate only those most discriminantpatches. Third, the feature descriptor of each patch is further projected tolower dimension subspace with discriminant subspace analysis. Learned feature of activated patches are concatenated to get a full facerepresentation.A linear classifier is learned to separate face pairs from samesubjects and different subjects. As the number of face pairs are extremelylarge, we introduce ADMM (alternative direction method of multipliers) to trainthe linear classifier on a computing cluster. Experiments show that moretraining samples will bring notable accuracy improvement. We conduct experiments on FRGC and LFW. Results show that the proposedapproach outperforms existing algorithms under the same protocol notably.Besides, the proposed approach is small in memory footprint, and low incomputing cost, which makes it suitable for embedded applications.
arxiv-6600-47 | Kinetic Energy Plus Penalty Functions for Sparse Estimation | http://arxiv.org/pdf/1307.5601v3.pdf | author:Zhihua Zhang, Shibo Zhao, Zebang Shen, Shuchang Zhou category:stat.ML published:2013-07-22 summary:In this paper we propose and study a family of sparsity-inducing penaltyfunctions. Since the penalty functions are related to the kinetic energy inspecial relativity, we call them \emph{kinetic energy plus} (KEP) functions. Weconstruct the KEP function by using the concave conjugate of a$\chi^2$-distance function and present several novel insights into the KEPfunction with $q=1$. In particular, we derive a thresholding operator based onthe KEP function, and prove its mathematical properties and asymptoticproperties in sparsity modeling. Moreover, we show that a coordinate descentalgorithm is especially appropriate for the KEP function. Additionally, wediscuss the relationship of KEP with the penalty functions $\ell_{1/2}$ andMCP. The theoretical and empirical analysis validates that the KEP function iseffective and efficient in high-dimensional data modeling.
arxiv-6600-48 | Image Fusion Using LEP Filtering and Bilinear Interpolation | http://arxiv.org/pdf/1407.3986v1.pdf | author:Haritha Raveendran, Deepa Thomas category:cs.CV published:2014-07-05 summary:Image Fusion is the process in which core information from a set of componentimages is merged to form a single image, which is more informative and completethan the component input images in quality and appearance. This paper presentsa fast and effective image fusion method for creating high quality fused imagesby merging component images. In the proposed method, the input image is brokendown to a two-scale image representation with a base layer having large scalevariations in intensity, and a detail layer containing small scale details.Here fusion of the base and detail layers is implemented by means of a LocalEdge preserving filtering based technique. The proposed method is an efficientimage fusion technique in which the noise component is very low and quality ofthe resultant image is high so that it can be used for applications likemedical image processing, requiring very accurate edge preserved images.Performance is tested by calculating PSNR and SSIM of images. The benefit ofthe proposed method is that it removes noise without altering the underlyingstructures of the image. This paper also presents an image zooming techniqueusing bilinear interpolation in which a portion of the input image is croppedand bilinear interpolation is applied. Experimental results showed that thewhen PSNR value is calculated, the noise is found to be very low for theresultant image portion.
arxiv-6600-49 | A New Approach for Super resolution by Using Web Images and FFT Based Image Registration | http://arxiv.org/pdf/1407.3675v1.pdf | author:Archana Vijayan, Vincy Salam category:cs.CV published:2014-07-05 summary:Preserving accuracy is a challenging issue in super resolution images. Inthis paper, we propose a new FFT based image registration algorithm and asparse based super resolution algorithm to improve the accuracy of superresolution image. Given a low resolution image, our approach initially extractsthe local descriptors from the input and then the local descriptors from thewhole correlated images using the SIFT algorithm. Once this is completed, itwill compare the local descriptors on the basis of a threshold value. Theretrieved images could be having different focal length, illumination,inclination and size. To overcome the above differences of the retrievedimages, we propose a new FFT based image registration algorithm. After theregistration stage, we apply a sparse based super resolution on the images forrecreating images with better resolution compared to the input. Based on thePSSNR calculation and SSIM comparison, we can see that the new methodologycreates a better image than the traditional methods.
arxiv-6600-50 | Generalized Higher-Order Tensor Decomposition via Parallel ADMM | http://arxiv.org/pdf/1407.1399v1.pdf | author:Fanhua Shang, Yuanyuan Liu, James Cheng category:cs.NA cs.LG published:2014-07-05 summary:Higher-order tensors are becoming prevalent in many scientific areas such ascomputer vision, social network analysis, data mining and neuroscience.Traditional tensor decomposition approaches face three major challenges: modelselecting, gross corruptions and computational efficiency. To address theseproblems, we first propose a parallel trace norm regularized tensordecomposition method, and formulate it as a convex optimization problem. Thismethod does not require the rank of each mode to be specified beforehand, andcan automatically determine the number of factors in each mode through ouroptimization scheme. By considering the low-rank structure of the observedtensor, we analyze the equivalent relationship of the trace norm between alow-rank tensor and its core tensor. Then, we cast a non-convex tensordecomposition model into a weighted combination of multiple much smaller-scalematrix trace norm minimization. Finally, we develop two parallel alternatingdirection methods of multipliers (ADMM) to solve our problems. Experimentalresults verify that our regularized formulation is effective, and our methodsare robust to noise or outliers.
arxiv-6600-51 | Homophilic Clustering by Locally Asymmetric Geometry | http://arxiv.org/pdf/1407.1352v1.pdf | author:Deli Zhao, Xiaoou Tang category:cs.CV published:2014-07-05 summary:Clustering is indispensable for data analysis in many scientific disciplines.Detecting clusters from heavy noise remains challenging, particularly forhigh-dimensional sparse data. Based on graph-theoretic framework, the presentpaper proposes a novel algorithm to address this issue. The locally asymmetricgeometries of neighborhoods between data points result in a directed similaritygraph to model the structural connectivity of data points. Performingsimilarity propagation on this directed graph simply by its adjacency matrixpowers leads to an interesting discovery, in the sense that if the in-degreesare ordered by the corresponding sorted out-degrees, they will beself-organized to be homophilic layers according to the different distributionsof cluster densities, which is dubbed the Homophilic In-degree figure (the HIfigure). With the HI figure, we can easily single out all cores of clusters,identify the boundary between cluster and noise, and visualize the intrinsicstructures of clusters. Based on the in-degree homophily, we also develop asimple efficient algorithm of linear space complexity to cluster noisy data.Extensive experiments on toy and real-world scientific data validate theeffectiveness of our algorithms.
arxiv-6600-52 | Inverse Graphics with Probabilistic CAD Models | http://arxiv.org/pdf/1407.1339v1.pdf | author:Tejas D. Kulkarni, Vikash K. Mansinghka, Pushmeet Kohli, Joshua B. Tenenbaum category:cs.CV cs.AI stat.ML published:2014-07-04 summary:Recently, multiple formulations of vision problems as probabilisticinversions of generative models based on computer graphics have been proposed.However, applications to 3D perception from natural images have focused onlow-dimensional latent scenes, due to challenges in both modeling andinference. Accounting for the enormous variability in 3D object shape and 2Dappearance via realistic generative models seems intractable, as does invertingeven simple versions of the many-to-many computations that link 3D scenes to 2Dimages. This paper proposes and evaluates an approach that addresses keyaspects of both these challenges. We show that it is possible to solvechallenging, real-world 3D vision problems by approximate inference ingenerative models for images based on rendering the outputs of probabilisticCAD (PCAD) programs. Our PCAD object geometry priors generate deformable 3Dmeshes corresponding to plausible objects and apply affine transformations toplace them in a scene. Image likelihoods are based on similarity in a featurespace based on standard mid-level image representations from the visionliterature. Our inference algorithm integrates single-site and locally blockedMetropolis-Hastings proposals, Hamiltonian Monte Carlo and discriminativedata-driven proposals learned from training data generated from our models. Weapply this approach to 3D human pose estimation and object shape reconstructionfrom single images, achieving quantitative and qualitative performanceimprovements over state-of-the-art baselines.
arxiv-6600-53 | Calibration of Multiple Fish-Eye Cameras Using a Wand | http://arxiv.org/pdf/1407.1267v1.pdf | author:Qiang Fu, Quan Quan, Kai-Yuan Cai category:cs.CV published:2014-07-04 summary:Fish-eye cameras are becoming increasingly popular in computer vision, buttheir use for 3D measurement is limited partly due to the lack of an accurate,efficient and user-friendly calibration procedure. For such a purpose, wepropose a method to calibrate the intrinsic and extrinsic parameters (includingradial distortion parameters) of two/multiple fish-eye cameras simultaneouslyby using a wand under general motions. Thanks to the generic camera model used,the proposed calibration method is also suitable for two/multiple conventionalcameras and mixed cameras (e.g. two conventional cameras and a fish-eyecamera). Simulation and real experiments demonstrate the effectiveness of theproposed method. Moreover, we develop the camera calibration toolbox, which isavailable online.
arxiv-6600-54 | Weakly Supervised Action Labeling in Videos Under Ordering Constraints | http://arxiv.org/pdf/1407.1208v1.pdf | author:Piotr Bojanowski, Rémi Lajugie, Francis Bach, Ivan Laptev, Jean Ponce, Cordelia Schmid, Josef Sivic category:cs.CV cs.LG published:2014-07-04 summary:We are given a set of video clips, each one annotated with an {\em ordered}list of actions, such as "walk" then "sit" then "answer phone" extracted from,for example, the associated text script. We seek to temporally localize theindividual actions in each clip as well as to learn a discriminative classifierfor each action. We formulate the problem as a weakly supervised temporalassignment with ordering constraints. Each video clip is divided into smalltime intervals and each time interval of each video clip is assigned one actionlabel, while respecting the order in which the action labels appear in thegiven annotations. We show that the action label assignment can be determinedtogether with learning a classifier for each action in a discriminative manner.We evaluate the proposed model on a new and challenging dataset of 937 videoclips with a total of 787720 frames containing sequences of 16 differentactions from 69 Hollywood movies.
arxiv-6600-55 | Improving Performance of Self-Organising Maps with Distance Metric Learning Method | http://arxiv.org/pdf/1407.1201v1.pdf | author:Piotr Płoński, Krzysztof Zaremba category:cs.LG cs.NE published:2014-07-04 summary:Self-Organising Maps (SOM) are Artificial Neural Networks used in PatternRecognition tasks. Their major advantage over other architectures is humanreadability of a model. However, they often gain poorer accuracy. Mostly usedmetric in SOM is the Euclidean distance, which is not the best approach to someproblems. In this paper, we study an impact of the metric change on the SOM'sperformance in classification problems. In order to change the metric of theSOM we applied a distance metric learning method, so-called 'Large MarginNearest Neighbour'. It computes the Mahalanobis matrix, which assures smalldistance between nearest neighbour points from the same class and separation ofpoints belonging to different classes by large margin. Results are presented onseveral real data sets, containing for example recognition of written digits,spoken letters or faces.
arxiv-6600-56 | Identifying Higher-order Combinations of Binary Features | http://arxiv.org/pdf/1407.1176v1.pdf | author:Felipe Llinares, Mahito Sugiyama, Karsten M. Borgwardt category:stat.ML cs.LG published:2014-07-04 summary:Finding statistically significant interactions between binary variables iscomputationally and statistically challenging in high-dimensional settings, dueto the combinatorial explosion in the number of hypotheses. Terada et al.recently showed how to elegantly address this multiple testing problem byexcluding non-testable hypotheses. Still, it remains unclear how their approachscales to large datasets. We here proposed strategies to speed up the approach by Terada et al. andevaluate them thoroughly in 11 real-world benchmark datasets. We observe thatone approach, incremental search with early stopping, is orders of magnitudefaster than the current state-of-the-art approach.
arxiv-6600-57 | Recognition of Isolated Words using Zernike and MFCC features for Audio Visual Speech Recognition | http://arxiv.org/pdf/1407.1165v1.pdf | author:Prashant Bordea, Amarsinh Varpeb, Ramesh Manzac, Pravin Yannawara category:cs.CV cs.CL published:2014-07-04 summary:Automatic Speech Recognition (ASR) by machine is an attractive research topicin signal processing domain and has attracted many researchers to contribute inthis area. In recent year, there have been many advances in automatic speechreading system with the inclusion of audio and visual speech features torecognize words under noisy conditions. The objective of audio-visual speechrecognition system is to improve recognition accuracy. In this paper wecomputed visual features using Zernike moments and audio feature using MelFrequency Cepstral Coefficients (MFCC) on vVISWa (Visual Vocabulary ofIndependent Standard Words) dataset which contains collection of isolated setof city names of 10 speakers. The visual features were normalized and dimensionof features set was reduced by Principal Component Analysis (PCA) in order torecognize the isolated word utterance on PCA space.The performance ofrecognition of isolated words based on visual only and audio only featuresresults in 63.88 and 100 respectively.
arxiv-6600-58 | Optimizing Ranking Measures for Compact Binary Code Learning | http://arxiv.org/pdf/1407.1151v1.pdf | author:Guosheng Lin, Chunhua Shen, Jianxin Wu category:cs.LG cs.CV published:2014-07-04 summary:Hashing has proven a valuable tool for large-scale information retrieval.Despite much success, existing hashing methods optimize over simple objectivessuch as the reconstruction error or graph Laplacian related loss functions,instead of the performance evaluation criteria of interest---multivariateperformance measures such as the AUC and NDCG. Here we present a generalframework (termed StructHash) that allows one to directly optimize multivariateperformance measures. The resulting optimization problem can involveexponentially or infinitely many variables and constraints, which is morechallenging than standard structured output learning. To solve the StructHashoptimization problem, we use a combination of column generation andcutting-plane techniques. We demonstrate the generality of StructHash byapplying it to ranking prediction and image retrieval, and show that itoutperforms a few state-of-the-art hashing methods.
arxiv-6600-59 | A framework for improving the performance of verification algorithms with a low false positive rate requirement and limited training data | http://arxiv.org/pdf/1406.7360v2.pdf | author:Ognjen Arandjelovic category:cs.CV published:2014-06-28 summary:In this paper we address the problem of matching patterns in the so-calledverification setting in which a novel, query pattern is verified against asingle training pattern: the decision sought is whether the two match (i.e.belong to the same class) or not. Unlike previous work which has universallyfocused on the development of more discriminative distance functions betweenpatterns, here we consider the equally important and pervasive task ofselecting a distance threshold which fits a particular operational requirement- specifically, the target false positive rate (FPR). First, we argue ontheoretical grounds that a data-driven approach is inherently ill-conditionedwhen the desired FPR is low, because by the very nature of the challenge only asmall portion of training data affects or is affected by the desired threshold.This leads us to propose a general, statistical model-based method instead. Ourapproach is based on the interpretation of an inter-pattern distance asimplicitly defining a pattern embedding which approximately distributespatterns according to an isotropic multi-variate normal distribution in somespace. This interpretation is then used to show that the distribution oftraining inter-pattern distances is the non-central chi2 distribution,differently parameterized for each class. Thus, to make the class-specificthreshold choice we propose a novel analysis-by-synthesis iterative algorithmwhich estimates the three free parameters of the model (for each class) usingtask-specific constraints. The validity of the premises of our work and theeffectiveness of the proposed method are demonstrated by applying the method tothe task of set-based face verification on a large database of pseudo-randomhead motion videos.
arxiv-6600-60 | Expanding the Family of Grassmannian Kernels: An Embedding Perspective | http://arxiv.org/pdf/1407.1123v1.pdf | author:Mehrtash T. Harandi, Mathieu Salzmann, Sadeep Jayasumana, Richard Hartley, Hongdong Li category:cs.CV cs.LG stat.ML published:2014-07-04 summary:Modeling videos and image-sets as linear subspaces has proven beneficial formany visual recognition tasks. However, it also incurs challenges arising fromthe fact that linear subspaces do not obey Euclidean geometry, but lie on aspecial type of Riemannian manifolds known as Grassmannian. To leverage thetechniques developed for Euclidean spaces (e.g, support vector machines) withsubspaces, several recent studies have proposed to embed the Grassmannian intoa Hilbert space by making use of a positive definite kernel. Unfortunately,only two Grassmannian kernels are known, none of which -as we will show- isuniversal, which limits their ability to approximate a target functionarbitrarily well. Here, we introduce several positive definite Grassmanniankernels, including universal ones, and demonstrate their superiority overpreviously-known kernels in various tasks, such as classification, clustering,sparse coding and hashing.
arxiv-6600-61 | Robust Optimization using Machine Learning for Uncertainty Sets | http://arxiv.org/pdf/1407.1097v1.pdf | author:Theja Tulabandhula, Cynthia Rudin category:math.OC cs.LG stat.ML published:2014-07-04 summary:Our goal is to build robust optimization problems for making decisions basedon complex data from the past. In robust optimization (RO) generally, the goalis to create a policy for decision-making that is robust to our uncertaintyabout the future. In particular, we want our policy to best handle the theworst possible situation that could arise, out of an uncertainty set ofpossible situations. Classically, the uncertainty set is simply chosen by theuser, or it might be estimated in overly simplistic ways with strongassumptions; whereas in this work, we learn the uncertainty set from datacollected in the past. The past data are drawn randomly from an (unknown)possibly complicated high-dimensional distribution. We propose a newuncertainty set design and show how tools from statistical learning theory canbe employed to provide probabilistic guarantees on the robustness of thepolicy.
arxiv-6600-62 | Online Submodular Maximization under a Matroid Constraint with Application to Learning Assignments | http://arxiv.org/pdf/1407.1082v1.pdf | author:Daniel Golovin, Andreas Krause, Matthew Streeter category:cs.LG published:2014-07-03 summary:Which ads should we display in sponsored search in order to maximize ourrevenue? How should we dynamically rank information sources to maximize thevalue of the ranking? These applications exhibit strong diminishing returns:Redundancy decreases the marginal utility of each ad or information source. Weshow that these and other problems can be formalized as repeatedly selecting anassignment of items to positions to maximize a sequence of monotone submodularfunctions that arrive one by one. We present an efficient algorithm for thisgeneral problem and analyze it in the no-regret model. Our algorithm possessesstrong theoretical guarantees, such as a performance ratio that converges tothe optimal constant of 1 - 1/e. We empirically evaluate our algorithm on tworeal-world online optimization problems on the web: ad allocation withsubmodular utilities, and dynamically ranking blogs to detect informationcascades. Finally, we present a second algorithm that handles the more generalcase in which the feasible sets are given by a matroid constraint, while stillmaintaining a 1 - 1/e asymptotic performance ratio.
arxiv-6600-63 | A Map of Update Constraints in Inductive Inference | http://arxiv.org/pdf/1404.7527v2.pdf | author:Timo Kötzing, Raphaela Palenta category:cs.LG published:2014-04-29 summary:We investigate how different learning restrictions reduce learning power andhow the different restrictions relate to one another. We give a complete mapfor nine different restrictions both for the cases of complete informationlearning and set-driven learning. This completes the picture for thesewell-studied \emph{delayable} learning restrictions. A further insight isgained by different characterizations of \emph{conservative} learning in termsof variants of \emph{cautious} learning. Our analyses greatly benefit from general theorems we give, for exampleshowing that learners with exclusively delayable restrictions can always beassumed total.
arxiv-6600-64 | Multiple Moving Object Recognitions in video based on Log Gabor-PCA Approach | http://arxiv.org/pdf/1407.0935v1.pdf | author:M. T Gopalakrishna, M. Ravishankar, D. R Rameshbabu category:cs.CV 68T45 I.4.8 published:2014-07-03 summary:Object recognition in the video sequence or images is one of the sub-field ofcomputer vision. Moving object recognition from a video sequence is anappealing topic with applications in various areas such as airport safety,intrusion surveillance, video monitoring, intelligent highway, etc. Movingobject recognition is the most challenging task in intelligent videosurveillance system. In this regard, many techniques have been proposed basedon different methods. Despite of its importance, moving object recognition incomplex environments is still far from being completely solved for lowresolution videos, foggy videos, and also dim video sequences. All in all,these make it necessary to develop exceedingly robust techniques. This paperintroduces multiple moving object recognition in the video sequence based onLoG Gabor-PCA approach and Angle based distance Similarity measures techniquesused to recognize the object as a human, vehicle etc. Number of experiments areconducted for indoor and outdoor video sequences of standard datasets and alsoour own collection of video sequences comprising of partial night vision videosequences. Experimental results show that our proposed approach achieves anexcellent recognition rate. Results obtained are satisfactory and competent.
arxiv-6600-65 | Solving QVIs for Image Restoration with Adaptive Constraint Sets | http://arxiv.org/pdf/1407.0921v1.pdf | author:Frank Lenzen, Jan Lellmann, Florian Becker, Christoph Schnörr category:math.OC cs.CV math.NA published:2014-07-03 summary:We consider a class of quasi-variational inequalities (QVIs) for adaptiveimage restoration, where the adaptivity is described via solution-dependentconstraint sets. In previous work we studied both theoretical and numericalissues. While we were able to show the existence of solutions for a relativelybroad class of problems, we encountered problems concerning uniqueness of thesolution as well as convergence of existing algorithms for solving QVIs. Inparticular, it seemed that with increasing image size the growing conditionnumber of the involved differential operator poses severe problems. In thepresent paper we prove uniqueness for a larger class of problems and inparticular independent of the image size. Moreover, we provide a numericalalgorithm with proved convergence. Experimental results support our theoreticalfindings.
arxiv-6600-66 | Enhanced EZW Technique for Compression of Image by Setting Detail Retaining Pass Number | http://arxiv.org/pdf/1407.3673v1.pdf | author:Isha Tyagi, Ashish Nautiyal, Vishwanath Bijalwan, Meenu Balodhi category:cs.CV published:2014-07-03 summary:For keeping the data secured and maintained, compression is most essentialaspect. For which efficiency is the important part to be researchedcontinuously until the satisfactory result is achieved. the optimized ratio ofdata is necessary for compression and embedded transmission. In this paper themain objective is to improve the execution time evolved in EZW compression.
arxiv-6600-67 | Reducing Offline Evaluation Bias in Recommendation Systems | http://arxiv.org/pdf/1407.0822v1.pdf | author:Arnaud De Myttenaere, Bénédicte Le Grand, Boris Golden, Fabrice Rossi category:cs.IR cs.LG stat.ML published:2014-07-03 summary:Recommendation systems have been integrated into the majority of large onlinesystems. They tailor those systems to individual users by filtering and rankinginformation according to user profiles. This adaptation process influences theway users interact with the system and, as a consequence, increases thedifficulty of evaluating a recommendation algorithm with historical data (viaoffline evaluation). This paper analyses this evaluation bias and proposes asimple item weighting solution that reduces its impact. The efficiency of theproposed solution is evaluated on real world data extracted from Viadeoprofessional social network.
arxiv-6600-68 | $ N^4 $-Fields: Neural Network Nearest Neighbor Fields for Image Transforms | http://arxiv.org/pdf/1406.6558v2.pdf | author:Yaroslav Ganin, Victor Lempitsky category:cs.CV published:2014-06-25 summary:We propose a new architecture for difficult image processing operations, suchas natural edge detection or thin object segmentation. The architecture isbased on a simple combination of convolutional neural networks with the nearestneighbor search. We focus our attention on the situations when the desired imagetransformation is too hard for a neural network to learn explicitly. We showthat in such situations, the use of the nearest neighbor search on top of thenetwork output allows to improve the results considerably and to account forthe underfitting effect during the neural network training. The approach isvalidated on three challenging benchmarks, where the performance of theproposed architecture matches or exceeds the state-of-the-art.
arxiv-6600-69 | Strengthening the Effectiveness of Pedestrian Detection with Spatially Pooled Features | http://arxiv.org/pdf/1407.0786v1.pdf | author:Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel category:cs.CV published:2014-07-03 summary:We propose a simple yet effective approach to the problem of pedestriandetection which outperforms the current state-of-the-art. Our new features arebuilt on the basis of low-level visual features and spatial pooling.Incorporating spatial pooling improves the translational invariance and thusthe robustness of the detection process. We then directly optimise the partialarea under the ROC curve (\pAUC) measure, which concentrates detectionperformance in the range of most practical importance. The combination of thesefactors leads to a pedestrian detector which outperforms all competitors on allof the standard benchmark datasets. We advance state-of-the-art results bylowering the average miss rate from $13\%$ to $11\%$ on the INRIA benchmark,$41\%$ to $37\%$ on the ETH benchmark, $51\%$ to $42\%$ on the TUD-Brusselsbenchmark and $36\%$ to $29\%$ on the Caltech-USA benchmark.
arxiv-6600-70 | Smoothed Functional Algorithms for Stochastic Optimization using q-Gaussian Distributions | http://arxiv.org/pdf/1206.4832v6.pdf | author:Debarghya Ghoshdastidar, Ambedkar Dukkipati, Shalabh Bhatnagar category:cs.IT cs.LG math.IT stat.ME G.1.6; I.6.8 published:2012-06-21 summary:Smoothed functional (SF) schemes for gradient estimation are known to beefficient in stochastic optimization algorithms, specially when the objectiveis to improve the performance of a stochastic system. However, the performanceof these methods depends on several parameters, such as the choice of asuitable smoothing kernel. Different kernels have been studied in literature,which include Gaussian, Cauchy and uniform distributions among others. Thispaper studies a new class of kernels based on the q-Gaussian distribution, thathas gained popularity in statistical physics over the last decade. Though theimportance of this family of distributions is attributed to its ability togeneralize the Gaussian distribution, we observe that this class encompassesalmost all existing smoothing kernels. This motivates us to study SF schemesfor gradient estimation using the q-Gaussian distribution. Using the derivedgradient estimates, we propose two-timescale algorithms for optimization of astochastic objective function in a constrained setting with projected gradientsearch approach. We prove the convergence of our algorithms to the set ofstationary points of an associated ODE. We also demonstrate their performancenumerically through simulations on a queuing model.
arxiv-6600-71 | BiofilmQuant: A Computer-Assisted Tool for Dental Biofilm Quantification | http://arxiv.org/pdf/1407.0765v1.pdf | author:Awais Mansoor, Valery Patsekin, Dale Scherl, J. Paul Robinson, Bartlomiej Rajwa category:cs.CV published:2014-07-03 summary:Dental biofilm is the deposition of microbial material over a toothsubstratum. Several methods have recently been reported in the literature forbiofilm quantification; however, at best they provide a barely automatedsolution requiring significant input needed from the human expert. On thecontrary, state-of-the-art automatic biofilm methods fail to make their wayinto clinical practice because of the lack of effective mechanism toincorporate human input to handle praxis or misclassified regions. Manualdelineation, the current gold standard, is time consuming and subject to expertbias. In this paper, we introduce a new semi-automated software tool,BiofilmQuant, for dental biofilm quantification in quantitative light-inducedfluorescence (QLF) images. The software uses a robust statistical modelingapproach to automatically segment the QLF image into three classes (background,biofilm, and tooth substratum) based on the training data. This initialsegmentation has shown a high degree of consistency and precision on more than200 test QLF dental scans. Further, the proposed software provides theclinicians full control to fix any misclassified areas using a single click. Inaddition, BiofilmQuant also provides a complete solution for the longitudinalquantitative analysis of biofilm of the full set of teeth, providing greaterease of usability.
arxiv-6600-72 | Structured Learning via Logistic Regression | http://arxiv.org/pdf/1407.0754v1.pdf | author:Justin Domke category:cs.LG stat.ML published:2014-07-03 summary:A successful approach to structured learning is to write the learningobjective as a joint function of linear parameters and inference messages, anditerate between updates to each. This paper observes that if the inferenceproblem is "smoothed" through the addition of entropy terms, for fixedmessages, the learning objective reduces to a traditional (non-structured)logistic regression problem with respect to parameters. In these logisticregression problems, each training example has a bias term determined by thecurrent set of messages. Based on this insight, the structured energy functioncan be extended from linear factors to any function class where an "oracle"exists to minimize a logistic loss.
arxiv-6600-73 | Deep Poselets for Human Detection | http://arxiv.org/pdf/1407.0717v1.pdf | author:Lubomir Bourdev, Fei Yang, Rob Fergus category:cs.CV published:2014-07-02 summary:We address the problem of detecting people in natural scenes using a partapproach based on poselets. We propose a bootstrapping method that allows us tocollect millions of weakly labeled examples for each poselet type. We use theseexamples to train a Convolutional Neural Net to discriminate different poselettypes and separate them from the background class. We then use the trained CNNas a way to represent poselet patches with a Pose Discriminative Feature (PDF)vector -- a compact 256-dimensional feature vector that is effective atdiscriminating pose from appearance. We train the poselet model on top of PDFfeatures and combine them with object-level CNNs for detection and bounding boxprediction. The resulting model leads to state-of-the-art performance for humandetection on the PASCAL datasets.
arxiv-6600-74 | Continuous On-line Evolution of Agent Behaviours with Cartesian Genetic Programming | http://arxiv.org/pdf/1407.0698v1.pdf | author:Davide Nunes, Luis Antunes category:cs.NE cs.MA published:2014-07-02 summary:Evolutionary Computation has been successfully used to synthesise controllersfor embodied agents and multi-agent systems in general. Notwithstanding this,continuous on-line adaptation by the means of evolutionary algorithms is stillunder-explored, especially outside the evolutionary robotics domain. In thispaper, we present an on-line evolutionary programming algorithm that searchesin the agent design space for the appropriate behavioural policies to cope withthe underlying environment. We discuss the current problems of continuous agentadaptation, present our on-line evolution testbed for evolutionary simulation.
arxiv-6600-75 | Nonparametric Hierarchical Clustering of Functional Data | http://arxiv.org/pdf/1407.0612v1.pdf | author:Marc Boullé, Romain Guigourès, Fabrice Rossi category:stat.ML cs.LG published:2014-07-02 summary:In this paper, we deal with the problem of curves clustering. We propose anonparametric method which partitions the curves into clusters and discretizesthe dimensions of the curve points into intervals. The cross-product of thesepartitions forms a data-grid which is obtained using a Bayesian model selectionapproach while making no assumptions regarding the curves. Finally, apost-processing technique, aiming at reducing the number of clusters in orderto improve the interpretability of the clustering, is proposed. It consists inoptimally merging the clusters step by step, which corresponds to anagglomerative hierarchical classification whose dissimilarity measure is thevariation of the criterion. Interestingly this measure is none other than thesum of the Kullback-Leibler divergences between clusters distributions beforeand after the merges. The practical interest of the approach for functionaldata exploratory analysis is presented and compared with an alternativeapproach on an artificial and a real world data set.
arxiv-6600-76 | How Many Dissimilarity/Kernel Self Organizing Map Variants Do We Need? | http://arxiv.org/pdf/1407.0611v1.pdf | author:Fabrice Rossi category:stat.ML cs.LG cs.NE published:2014-07-02 summary:In numerous applicative contexts, data are too rich and too complex to berepresented by numerical vectors. A general approach to extend machine learningand data mining techniques to such data is to really on a dissimilarity or on akernel that measures how different or similar two objects are. This approachhas been used to define several variants of the Self Organizing Map (SOM). Thispaper reviews those variants in using a common set of notations in order tooutline differences and similarities between them. It discusses the advantagesand drawbacks of the variants, as well as the actual relevance of thedissimilarity/kernel SOM for practical applications.
arxiv-6600-77 | Higher-Order Quantum-Inspired Genetic Algorithms | http://arxiv.org/pdf/1407.0977v1.pdf | author:Robert Nowotniak, Jacek Kucharski category:cs.NE quant-ph published:2014-07-02 summary:This paper presents a theory and an empirical evaluation of Higher-OrderQuantum-Inspired Genetic Algorithms. Fundamental notions of the theory havebeen introduced, and a novel Order-2 Quantum-Inspired Genetic Algorithm (QIGA2)has been presented. Contrary to all QIGA algorithms which represent quantumgenes as independent qubits, in higher-order QIGAs quantum registers are usedto represent genes strings which allows modelling of genes relations usingquantum phenomena. Performance comparison has been conducted on a benchmark of20 deceptive combinatorial optimization problems. It has been presented thatusing higher quantum orders is beneficial for genetic algorithm efficiency, andthe new QIGA2 algorithm outperforms the old QIGA algorithm which was tuned inhighly compute intensive metaoptimization process.
arxiv-6600-78 | On the Consistency of AUC Pairwise Optimization | http://arxiv.org/pdf/1208.0645v4.pdf | author:Wei Gao, Zhi-Hua Zhou category:cs.LG stat.ML published:2012-08-03 summary:AUC (area under ROC curve) is an important evaluation criterion, which hasbeen popularly used in many learning tasks such as class-imbalance learning,cost-sensitive learning, learning to rank, etc. Many learning approaches try tooptimize AUC, while owing to the non-convexity and discontinuousness of AUC,almost all approaches work with surrogate loss functions. Thus, the consistencyof AUC is crucial; however, it has been almost untouched before. In this paper,we provide a sufficient condition for the asymptotic consistency of learningapproaches based on surrogate loss functions. Based on this result, we provethat exponential loss and logistic loss are consistent with AUC, but hinge lossis inconsistent. Then, we derive the $q$-norm hinge loss and general hinge lossthat are consistent with AUC. We also derive the consistent bounds forexponential loss and logistic loss, and obtain the consistent bounds for manysurrogate loss functions under the non-noise setting. Further, we disclose anequivalence between the exponential surrogate loss of AUC and exponentialsurrogate loss of accuracy, and one straightforward consequence of such findingis that AdaBoost and RankBoost are equivalent.
arxiv-6600-79 | Dropout Rademacher Complexity of Deep Neural Networks | http://arxiv.org/pdf/1402.3811v2.pdf | author:Wei Gao, Zhi-Hua Zhou category:cs.NE stat.ML published:2014-02-16 summary:Great successes of deep neural networks have been witnessed in various realapplications. Many algorithmic and implementation techniques have beendeveloped, however, theoretical understanding of many aspects of deep neuralnetworks is far from clear. A particular interesting issue is the usefulness ofdropout, which was motivated from the intuition of preventing complexco-adaptation of feature detectors. In this paper, we study the Rademachercomplexity of different types of dropout, and our theoretical results disclosethat for shallow neural networks (with one or none hidden layer) dropout isable to reduce the Rademacher complexity in polynomial, whereas for deep neuralnetworks it can amazingly lead to an exponential reduction of the Rademachercomplexity.
arxiv-6600-80 | Systematic Derivation of Behaviour Characterisations in Evolutionary Robotics | http://arxiv.org/pdf/1407.0577v1.pdf | author:Jorge Gomes, Pedro Mariano, Anders Lyhne Christensen category:cs.NE cs.MA cs.RO published:2014-07-02 summary:Evolutionary techniques driven by behavioural diversity, such as noveltysearch, have shown significant potential in evolutionary robotics. Thesetechniques rely on priorly specified behaviour characterisations to estimatethe similarity between individuals. Characterisations are typically defined inan ad hoc manner based on the experimenter's intuition and knowledge about thetask. Alternatively, generic characterisations based on the sensor-effectorvalues of the agents are used. In this paper, we propose a novel approach thatallows for systematic derivation of behaviour characterisations forevolutionary robotics, based on a formal description of the agents and theirenvironment. Systematically derived behaviour characterisations (SDBCs) gobeyond generic characterisations in that they can contain task-specificfeatures related to the internal state of the agents, environmental features,and relations between them. We evaluate SDBCs with novelty search in threesimulated collective robotics tasks. Our results show that SDBCs yield aperformance comparable to the task-specific characterisations, in terms of bothsolution quality and behaviour space exploration.
arxiv-6600-81 | Novelty Search in Competitive Coevolution | http://arxiv.org/pdf/1407.0576v1.pdf | author:Jorge Gomes, Pedro Mariano, Anders Lyhne Christensen category:cs.NE cs.MA published:2014-07-02 summary:One of the main motivations for the use of competitive coevolution systems istheir ability to capitalise on arms races between competing species to evolveincreasingly sophisticated solutions. Such arms races can, however, be hard tosustain, and it has been shown that the competing species often convergeprematurely to certain classes of behaviours. In this paper, we investigate ifand how novelty search, an evolutionary technique driven by behaviouralnovelty, can overcome convergence in coevolution. We propose three methods forapplying novelty search to coevolutionary systems with two species: (i) scoreboth populations according to behavioural novelty; (ii) score one populationaccording to novelty, and the other according to fitness; and (iii) score bothpopulations with a combination of novelty and fitness. We evaluate the methodsin a predator-prey pursuit task. Our results show that novelty-based approachescan evolve a significantly more diverse set of solutions, when compared totraditional fitness-based coevolution.
arxiv-6600-82 | A Theoretical and Experimental Comparison of the EM and SEM Algorithm | http://arxiv.org/pdf/1310.5034v2.pdf | author:Johannes Blömer, Kathrin Bujna, Daniel Kuntze category:cs.LG stat.ML published:2013-10-18 summary:In this paper we provide a new analysis of the SEM algorithm. Unlike previouswork, we focus on the analysis of a single run of the algorithm. First, wediscuss the algorithm for general mixture distributions. Second, we considerGaussian mixture models and show that with high probability the updateequations of the EM algorithm and its stochastic variant are almost the same,given that the input set is sufficiently large. Our experiments confirm thatthis still holds for a large number of successive update steps. In particular,for Gaussian mixture models, we show that the stochastic variant runs nearlytwice as fast.
arxiv-6600-83 | DC approximation approaches for sparse optimization | http://arxiv.org/pdf/1407.0286v2.pdf | author:Hoai An Le Thi, Tao Pham Dinh, Hoai Minh Le, Xuan Thanh Vo category:cs.NA cs.LG stat.ML 90C26, 90C90 published:2014-07-01 summary:Sparse optimization refers to an optimization problem involving the zero-normin objective or constraints. In this paper, nonconvex approximation approachesfor sparse optimization have been studied with a unifying point of view in DC(Difference of Convex functions) programming framework. Considering a common DCapproximation of the zero-norm including all standard sparse inducing penaltyfunctions, we studied the consistency between global minimums (resp. localminimums) of approximate and original problems. We showed that, in severalcases, some global minimizers (resp. local minimizers) of the approximateproblem are also those of the original problem. Using exact penalty techniquesin DC programming, we proved stronger results for some particularapproximations, namely, the approximate problem, with suitable parameters, isequivalent to the original problem. The efficiency of several sparse inducingpenalty functions have been fully analyzed. Four DCA (DC Algorithm) schemeswere developed that cover all standard algorithms in nonconvex sparseapproximation approaches as special versions. They can be viewed as, an $\ell_{1}$-perturbed algorithm / reweighted-$\ell _{1}$ algorithm / reweighted-$\ell_{1}$ algorithm. We offer a unifying nonconvex approximation approach, withsolid theoretical tools as well as efficient algorithms based on DC programmingand DCA, to tackle the zero-norm and sparse optimization. As an application, weimplemented our methods for the feature selection in SVM (Support VectorMachine) problem and performed empirical comparative numerical experiments onthe proposed algorithms with various approximation functions.
arxiv-6600-84 | Estimating complex causal effects from incomplete observational data | http://arxiv.org/pdf/1403.1124v2.pdf | author:Juha Karvanen category:stat.ME cs.LG math.ST stat.ML stat.TH published:2014-03-05 summary:Despite the major advances taken in causal modeling, causality is still anunfamiliar topic for many statisticians. In this paper, it is demonstrated fromthe beginning to the end how causal effects can be estimated from observationaldata assuming that the causal structure is known. To make the problem morechallenging, the causal effects are highly nonlinear and the data are missingat random. The tools used in the estimation include causal models with design,causal calculus, multiple imputation and generalized additive models. The mainmessage is that a trained statistician can estimate causal effects byjudiciously combining existing tools.
arxiv-6600-85 | Classification-based Approximate Policy Iteration: Experiments and Extended Discussions | http://arxiv.org/pdf/1407.0449v1.pdf | author:Amir-massoud Farahmand, Doina Precup, André M. S. Barreto, Mohammad Ghavamzadeh category:cs.LG cs.SY math.OC stat.ML I.2.6; I.2.8 published:2014-07-02 summary:Tackling large approximate dynamic programming or reinforcement learningproblems requires methods that can exploit regularities, or intrinsicstructure, of the problem in hand. Most current methods are geared towardsexploiting the regularities of either the value function or the policy. Weintroduce a general classification-based approximate policy iteration (CAPI)framework, which encompasses a large class of algorithms that can exploitregularities of both the value function and the policy space, depending on whatis advantageous. This framework has two main components: a generic valuefunction estimator and a classifier that learns a policy based on the estimatedvalue function. We establish theoretical guarantees for the sample complexityof CAPI-style algorithms, which allow the policy evaluation step to beperformed by a wide variety of algorithms (including temporal-difference-stylemethods), and can handle nonparametric representations of policies. Our boundson the estimation error of the performance loss are tighter than existingresults. We also illustrate this approach empirically on several problems,including a large HIV control task.
arxiv-6600-86 | Rates of Convergence for Nearest Neighbor Classification | http://arxiv.org/pdf/1407.0067v2.pdf | author:Kamalika Chaudhuri, Sanjoy Dasgupta category:cs.LG math.ST stat.ML stat.TH published:2014-06-30 summary:Nearest neighbor methods are a popular class of nonparametric estimators withseveral desirable properties, such as adaptivity to different distance scalesin different regions of space. Prior work on convergence rates for nearestneighbor classification has not fully reflected these subtle properties. Weanalyze the behavior of these estimators in metric spaces and providefinite-sample, distribution-dependent rates of convergence under minimalassumptions. As a by-product, we are able to establish the universalconsistency of nearest neighbor in a broader range of data spaces than waspreviously known. We illustrate our upper and lower bounds by introducingsmoothness classes that are customized for nearest neighbor classification.
arxiv-6600-87 | A Dynamic Simulation-Optimization Model for Adaptive Management of Urban Water Distribution System Contamination Threats | http://arxiv.org/pdf/1407.0424v1.pdf | author:Amin Rasekh, Kelly Brumbelow category:cs.OH cs.NE published:2014-07-01 summary:Urban water distribution systems hold a critical and strategic position inpreserving public health and industrial growth. Despite the ubiquity of theseurban systems, aging infrastructure, and increased risk of terrorism, decisionsupport models for a timely and adaptive contamination emergency response stillremain at an undeveloped stage. Emergency response is characterized as aprogressive, interactive, and adaptive process that involves parallelactivities of processing streaming information and executing response actions.This study develops a dynamic decision support model that adaptively simulatesthe time-varying emergency environment and tracks changing best healthprotection response measures at every stage of an emergency in real-time.Feedback mechanisms between the contaminated network, emergency managers, andconsumers are incorporated in a dynamic simulation model to capturetime-varying characteristics of an emergency environment. Anevolutionary-computation-based dynamic optimization model is developed toadaptively identify time-dependant optimal health protection measures during anemergency. This dynamic simulation-optimization model treats perceivedcontaminant source attributes as time-varying parameters to account forperceived contamination source updates as more data stream in over time.Performance of the developed dynamic decision support model is analyzed anddemonstrated using a mid-size virtual city that resembles the dynamics andcomplexity of real-world urban systems. This adaptive emergency responseoptimization model is intended to be a major component of an all-inclusivecyberinfrastructure for efficient contamination threat management, which iscurrently under development.
arxiv-6600-88 | copulaedas: An R Package for Estimation of Distribution Algorithms Based on Copulas | http://arxiv.org/pdf/1209.5429v4.pdf | author:Yasser Gonzalez-Fernandez, Marta Soto category:cs.NE cs.MS published:2012-09-24 summary:The use of copula-based models in EDAs (estimation of distributionalgorithms) is currently an active area of research. In this context, thecopulaedas package for R provides a platform where EDAs based on copulas can beimplemented and studied. The package offers complete implementations of variousEDAs based on copulas and vines, a group of well-known optimization problems,and utility functions to study the performance of the algorithms. Newlydeveloped EDAs can be easily integrated into the package by extending an S4class with generic functions for their main components. This paper presentscopulaedas by providing an overview of EDAs based on copulas, a description ofthe implementation of the package, and an illustration of its use throughexamples. The examples include running the EDAs defined in the package,implementing new algorithms, and performing an empirical study to compare thebehavior of different algorithms on benchmark functions and a real-worldproblem.
arxiv-6600-89 | A New Path to Construct Parametric Orientation Field: Sparse FOMFE Model and Compressed Sparse FOMFE Model | http://arxiv.org/pdf/1407.0342v1.pdf | author:Jinwei Xu, Jiankun Hu, Xiuping Jia category:cs.CV cs.CR published:2014-07-01 summary:Orientation field, representing the fingerprint ridge structure direction,plays a crucial role in fingerprint-related image processing tasks. Orientationfield is able to be constructed by either non-parametric or parametric methods.In this paper, the advantages and disadvantages regarding to the existingnon-parametric and parametric approaches are briefly summarized. With thefurther investigation for constructing the orientation field by parametrictechnique, two new models - sparse FOMFE model and compressed sparse FOMFEmodel are introduced, based on the rapidly developing signal sparserepresentation and compressed sensing theories. The experiments on high-qualityfingerprint image dataset (plain and rolled print) and poor-quality fingerprintimage dataset (latent print) demonstrate their feasibilities to construct theorientation field in a sparse or even compressed sparse mode. The comparisonsamong the state-of-art orientation field modeling approaches show that theproposed two models have the potential availability in big data-orientedfingerprint indexing tasks.
arxiv-6600-90 | Supervised learning in Spiking Neural Networks with Limited Precision: SNN/LP | http://arxiv.org/pdf/1407.0265v1.pdf | author:Evangelos Stromatias, John Marsland category:cs.NE published:2014-07-01 summary:A new supervised learning algorithm, SNN/LP, is proposed for Spiking NeuralNetworks. This novel algorithm uses limited precision for both synaptic weightsand synaptic delays; 3 bits in each case. Also a genetic algorithm is used forthe supervised training. The results are comparable or better than previouslypublished work. The results are applicable to the realization of large scalehardware neural networks. One of the trained networks is implemented inprogrammable hardware.
arxiv-6600-91 | Imaging with Kantorovich-Rubinstein discrepancy | http://arxiv.org/pdf/1407.0221v1.pdf | author:Jan Lellmann, Dirk A. Lorenz, Carola Schönlieb, Tuomo Valkonen category:cs.CV math.NA published:2014-07-01 summary:We propose the use of the Kantorovich-Rubinstein norm from optimal transportin imaging problems. In particular, we discuss a variational regularisationmodel endowed with a Kantorovich-Rubinstein discrepancy term and totalvariation regularization in the context of image denoising and cartoon-texturedecomposition. We point out connections of this approach to several otherrecently proposed methods such as total generalized variation and normscapturing oscillating patterns. We also show that the respective optimizationproblem can be turned into a convex-concave saddle point problem with simpleconstraints and hence, can be solved by standard tools. Numerical examplesexhibit interesting features and favourable performance for denoising andcartoon-texture decomposition.
arxiv-6600-92 | Mind the Nuisance: Gaussian Process Classification using Privileged Noise | http://arxiv.org/pdf/1407.0179v1.pdf | author:Daniel Hernández-Lobato, Viktoriia Sharmanska, Kristian Kersting, Christoph H. Lampert, Novi Quadrianto category:stat.ML cs.LG published:2014-07-01 summary:The learning with privileged information setting has recently attracted a lotof attention within the machine learning community, as it allows theintegration of additional knowledge into the training process of a classifier,even when this comes in the form of a data modality that is not available attest time. Here, we show that privileged information can naturally be treatedas noise in the latent function of a Gaussian Process classifier (GPC). Thatis, in contrast to the standard GPC setting, the latent function is not just anuisance but a feature: it becomes a natural measure of confidence about thetraining data by modulating the slope of the GPC sigmoid likelihood function.Extensive experiments on public datasets show that the proposed GPC methodusing privileged noise, called GPC+, improves over a standard GPC withoutprivileged knowledge, and also over the current state-of-the-art SVM-basedmethod, SVM+. Moreover, we show that advanced neural networks and deep learningmethods can be compressed as privileged information.
arxiv-6600-93 | Mathematical Language Processing Project | http://arxiv.org/pdf/1407.0167v1.pdf | author:Robert Pagael, Moritz Schubotz category:cs.DL cs.CL cs.IR published:2014-07-01 summary:In natural language, words and phrases themselves imply the semantics. Incontrast, the meaning of identifiers in mathematical formulae is undefined.Thus scientists must study the context to decode the meaning. The MathematicalLanguage Processing (MLP) project aims to support that process. In this paper,we compare two approaches to discover identifier-definition tuples. At first weuse a simple pattern matching approach. Second, we present the MLP approachthat uses part-of-speech tag based distances as well as sentence positions tocalculate identifier-definition probabilities. The evaluation of ourprototypical system, applied on the Wikipedia text corpus, shows that ourapproach augments the user experience substantially. While hovering theidentifiers in the formula, tool-tips with the most probable definitions occur.Tests with random samples show that the displayed definitions provide a goodmatch with the actual meaning of the identifiers.
arxiv-6600-94 | Multilinear Wavelets: A Statistical Shape Space for Human Faces | http://arxiv.org/pdf/1401.2818v2.pdf | author:Alan Brunton, Timo Bolkart, Stefanie Wuhrer category:cs.CV cs.GR published:2014-01-13 summary:We present a statistical model for $3$D human faces in varying expression,which decomposes the surface of the face using a wavelet transform, and learnsmany localized, decorrelated multilinear models on the resulting coefficients.Using this model we are able to reconstruct faces from noisy and occluded $3$Dface scans, and facial motion sequences. Accurate reconstruction of face shapeis important for applications such as tele-presence and gaming. The localizedand multi-scale nature of our model allows for recovery of fine-scale detailwhile retaining robustness to severe noise and occlusion, and iscomputationally efficient and scalable. We validate these propertiesexperimentally on challenging data in the form of static scans and motionsequences. We show that in comparison to a global multilinear model, our modelbetter preserves fine detail and is computationally faster, while in comparisonto a localized PCA model, our model better handles variation in expression, isfaster, and allows us to fix identity parameters for a given subject.
arxiv-6600-95 | A Unified Framework of Elementary Geometric Transformation Representation | http://arxiv.org/pdf/1307.0998v3.pdf | author:F. Lu, Z. Chen category:cs.CV published:2013-07-03 summary:As an extension of projective homology, stereohomology is proposed via anextension of Desargues theorem and the extended Desargues configuration.Geometric transformations such as reflection, translation, central symmetry,central projection, parallel projection, shearing, central dilation, scaling,and so on are all included in stereohomology and represented asHouseholder-Chen elementary matrices. Hence all these geometric transformationsare called elementary. This makes it possible to represent these elementarygeometric transformations in homogeneous square matrices independent of aparticular choice of coordinate system.
arxiv-6600-96 | Newton-Type Iterative Solver for Multiple View $L2$ Triangulation | http://arxiv.org/pdf/1405.3352v2.pdf | author:F. Lu, Z. Chen category:cs.CV cs.GR published:2014-05-14 summary:In this note, we show that the L2 optimal solutions to most real multipleview L2 triangulation problems can be efficiently obtained by two-stageNewton-like iterative methods, while the difficulty of such problems mainlylies in how to verify the L2 optimality. Such a working two-stage bundleadjustment approach features: first, the algorithm is initialized by symmedianpoint triangulation, a multiple-view generalization of the mid-point method;second, a symbolic-numeric method is employed to compute derivativesaccurately; third, globalizing strategy such as line search or trust region issmoothly applied to the underlying iteration which assures algorithm robustnessin general cases. Numerical comparison with tfml method shows that the local minimizersobtained by the two-stage iterative bundle adjustment approach proposed hereare also the L2 optimal solutions to all the calibrated data sets availableonline by the Oxford visual geometry group. Extensive numerical experimentsindicate the bundle adjustment approach solves more than 99% the realtriangulation problems optimally. An IEEE 754 double precision C++implementation shows that it takes only about 0.205 second tocompute allthe4983 points in the Oxford dinosaur data setvia Gauss-Newton iteration hybridwith a line search strategy on a computer with a 3.4GHz Intel i7 CPU.
arxiv-6600-97 | Block matching algorithm for motion estimation based on Artificial Bee Colony (ABC) | http://arxiv.org/pdf/1407.0061v1.pdf | author:Erik Cuevas, Daniel Zaldivar, Marco Perez, Humberto Sossa, Valentin Osuna category:cs.NE published:2014-06-30 summary:Block matching (BM) motion estimation plays a very important role in videocoding. In a BM approach, image frames in a video sequence are divided intoblocks. For each block in the current frame, the best matching block isidentified inside a region of the previous frame, aiming to minimize the sum ofabsolute differences (SAD). Unfortunately, the SAD evaluation iscomputationally expensive and represents the most consuming operation in the BMprocess. Therefore, BM motion estimation can be approached as an optimizationproblem, where the goal is to find the best matching block within a searchspace. The simplest available BM method is the full search algorithm (FSA)which finds the most accurate motion vector through an exhaustive computationof SAD values for all elements of the search window. Recently, several fast BMalgorithms have been proposed to reduce the number of SAD operations bycalculating only a fixed subset of search locations at the price of pooraccuracy. In this paper, a new algorithm based on Artificial Bee Colony (ABC)optimization is proposed to reduce the number of search locations in the BMprocess. In our algorithm, the computation of search locations is drasticallyreduced by considering a fitness calculation strategy which indicates when itis feasible to calculate or only estimate new search locations. Since theproposed algorithm does not consider any fixed search pattern or any othermovement assumption as most of other BM approaches do, a high probability forfinding the true minimum (accurate motion vector) is expected. Conductedsimulations show that the proposed method achieves the best balance over otherfast BM algorithms, in terms of both estimation accuracy and computationalcost.
arxiv-6600-98 | Infinite Structured Hidden Semi-Markov Models | http://arxiv.org/pdf/1407.0044v1.pdf | author:Jonathan H. Huggins, Frank Wood category:stat.ME stat.AP stat.ML published:2014-06-30 summary:This paper reviews recent advances in Bayesian nonparametric techniques forconstructing and performing inference in infinite hidden Markov models. Wefocus on variants of Bayesian nonparametric hidden Markov models that enhance aposteriori state-persistence in particular. This paper also introduces a newBayesian nonparametric framework for generating left-to-right and otherstructured, explicit-duration infinite hidden Markov models that we call theinfinite structured hidden semi-Markov model.
arxiv-6600-99 | An optimization algorithm for multimodal functions inspired by collective animal behavior | http://arxiv.org/pdf/1406.7811v1.pdf | author:Erik Cuevas, Mauricio Gonzalez category:cs.NE published:2014-06-30 summary:Interest in multimodal function optimization is expanding rapidly since realworld optimization problems often demand locating multiple optima within asearch space. This article presents a new multimodal optimization algorithmnamed as the Collective Animal Behavior (CAB). Animal groups, such as schoolsof fish, flocks of birds, swarms of locusts and herds of wildebeest, exhibit avariety of behaviors including swarming about a food source, milling around acentral location or migrating over large distances in aligned groups. Thesecollective behaviors are often advantageous to groups, allowing them toincrease their harvesting efficiency to follow better migration routes, toimprove their aerodynamic and to avoid predation. In the proposed algorithm,searcher agents are a group of animals which interact to each other based onthe biological laws of collective motion. Experimental results demonstrate thatthe proposed algorithm is capable of finding global and local optima ofbenchmark multimodal optimization problems with a higher efficiency incomparison to other methods reported in the literature.
arxiv-6600-100 | Subjective and Objective Quality Assessment of Image: A Survey | http://arxiv.org/pdf/1406.7799v1.pdf | author:Pedram Mohammadi, Abbas Ebrahimi-Moghadam, Shahram Shirani category:cs.MM cs.CV published:2014-06-30 summary:With the increasing demand for image-based applications, the efficient andreliable evaluation of image quality has increased in importance. Measuring theimage quality is of fundamental importance for numerous image processingapplications, where the goal of image quality assessment (IQA) methods is toautomatically evaluate the quality of images in agreement with human qualityjudgments. Numerous IQA methods have been proposed over the past years tofulfill this goal. In this paper, a survey of the quality assessment methodsfor conventional image signals, as well as the newly emerged ones, whichincludes the high dynamic range (HDR) and 3-D images, is presented. Acomprehensive explanation of the subjective and objective IQA and theirclassification is provided. Six widely used subjective quality datasets, andperformance measures are reviewed. Emphasis is given to the full-referenceimage quality assessment (FR-IQA) methods, and 9 often-used quality measures(including mean squared error (MSE), structural similarity index (SSIM),multi-scale structural similarity index (MS-SSIM), visual information fidelity(VIF), most apparent distortion (MAD), feature similarity measure (FSIM),feature similarity measure for color images (FSIMC), dynamic range independentmeasure (DRIM), and tone-mapped images quality index (TMQI)) are carefullydescribed, and their performance and computation time on four subjectivequality datasets are evaluated. Furthermore, a brief introduction to 3-D IQA isprovided and the issues related to this area of research are reviewed.
arxiv-6600-101 | Theoretical Analysis of Bayesian Optimisation with Unknown Gaussian Process Hyper-Parameters | http://arxiv.org/pdf/1406.7758v1.pdf | author:Ziyu Wang, Nando de Freitas category:stat.ML cs.LG published:2014-06-30 summary:Bayesian optimisation has gained great popularity as a tool for optimisingthe parameters of machine learning algorithms and models. Somewhat ironically,setting up the hyper-parameters of Bayesian optimisation methods is notoriouslyhard. While reasonable practical solutions have been advanced, they can oftenfail to find the best optima. Surprisingly, there is little theoreticalanalysis of this crucial problem in the literature. To address this, we derivea cumulative regret bound for Bayesian optimisation with Gaussian processes andunknown kernel hyper-parameters in the stochastic setting. The bound, whichapplies to the expected improvement acquisition function and sub-Gaussianobservation noise, provides us with guidelines on how to design hyper-parameterestimation methods. A simple simulation demonstrates the importance offollowing these guidelines.
arxiv-6600-102 | Dispersion and Line Formation in Artificial Swarm Intelligence | http://arxiv.org/pdf/1407.0014v1.pdf | author:Donghwa Jeong, Kiju Lee category:cs.NE published:2014-06-30 summary:One of the major motifs in collective or swarm intelligence is that, eventhough individuals follow simple rules, the resulting global behavior can becomplex and intelligent. In artificial swarm systems, such as swarm robots, thegoal is to use systems that are as simple and cheap as possible, deploy many ofthem, and coordinate them to conduct complex tasks that each individual cannotaccomplish. Shape formation in artificial intelligence systems is usuallyrequired for specific task-oriented performance, including 1) forming sensinggrids, 2) exploring and mapping in space, underwater, or hazardousenvironments, and 3) forming a barricade for surveillance or protecting an areaor a person. This paper presents a dynamic model of an artificial swarm systembased on a virtual spring damper model and algorithms for dispersion without aleader and line formation with an interim leader using only the distanceestimation among the neighbors.
arxiv-6600-103 | Relevance Singular Vector Machine for low-rank matrix sensing | http://arxiv.org/pdf/1407.0013v1.pdf | author:Martin Sundin, Saikat Chatterjee, Magnus Jansson, Cristian R. Rojas category:cs.NA cs.LG math.ST stat.TH published:2014-06-30 summary:In this paper we develop a new Bayesian inference method for low rank matrixreconstruction. We call the new method the Relevance Singular Vector Machine(RSVM) where appropriate priors are defined on the singular vectors of theunderlying matrix to promote low rank. To accelerate computations, anumerically efficient approximation is developed. The proposed algorithms areapplied to matrix completion and matrix reconstruction problems and theirperformance is studied numerically.
arxiv-6600-104 | Cross-calibration of Time-of-flight and Colour Cameras | http://arxiv.org/pdf/1401.8092v2.pdf | author:Miles Hansard, Georgios Evangelidis, Quentin Pelorson, Radu Horaud category:cs.CV cs.RO published:2014-01-31 summary:Time-of-flight cameras provide depth information, which is complementary tothe photometric appearance of the scene in ordinary images. It is desirable tomerge the depth and colour information, in order to obtain a coherent scenerepresentation. However, the individual cameras will have different viewpoints,resolutions and fields of view, which means that they must be mutuallycalibrated. This paper presents a geometric framework for this multi-view andmulti-modal calibration problem. It is shown that three-dimensional projectivetransformations can be used to align depth and parallax-based representationsof the scene, with or without Euclidean reconstruction. A new evaluationprocedure is also developed; this allows the reprojection error to bedecomposed into calibration and sensor-dependent components. The completeapproach is demonstrated on a network of three time-of-flight and six colourcameras. The applications of such a system, to a range of automaticscene-interpretation problems, are discussed.
arxiv-6600-105 | Bayesian Analysis for miRNA and mRNA Interactions Using Expression Data | http://arxiv.org/pdf/1210.3456v2.pdf | author:Mingjun Zhong, Rong Liu, Bo Liu category:stat.AP cs.LG q-bio.GN q-bio.MN stat.ML published:2012-10-12 summary:MicroRNAs (miRNAs) are small RNA molecules composed of 19-22 nt, which playimportant regulatory roles in post-transcriptional gene regulation byinhibiting the translation of the mRNA into proteins or otherwise cleaving thetarget mRNA. Inferring miRNA targets provides useful information forunderstanding the roles of miRNA in biological processes that are potentiallyinvolved in complex diseases. Statistical methodologies for point estimation,such as the Least Absolute Shrinkage and Selection Operator (LASSO) algorithm,have been proposed to identify the interactions of miRNA and mRNA based onsequence and expression data. In this paper, we propose using the BayesianLASSO (BLASSO) and the non-negative Bayesian LASSO (nBLASSO) to analyse theinteractions between miRNA and mRNA using expression data. The proposedBayesian methods explore the posterior distributions for those parametersrequired to model the miRNA-mRNA interactions. These approaches can be used toobserve the inferred effects of the miRNAs on the targets by plotting theposterior distributions of those parameters. For comparison purposes, the LeastSquares Regression (LSR), Ridge Regression (RR), LASSO, non-negative LASSO(nLASSO), and the proposed Bayesian approaches were applied to four publicdatasets. We concluded that nLASSO and nBLASSO perform best in terms ofsensitivity and specificity. Compared to the point estimate algorithms, whichonly provide single estimates for those parameters, the Bayesian methods aremore meaningful and provide credible intervals, which take into account theuncertainty of the inferred interactions of the miRNA and mRNA. Furthermore,Bayesian methods naturally provide statistical significance to selectconvincing inferred interactions, while point estimate algorithms require amanually chosen threshold, which is less meaningful, to choose the possibleinteractions.
arxiv-6600-106 | Data Requirement for Phylogenetic Inference from Multiple Loci: A New Distance Method | http://arxiv.org/pdf/1404.7055v2.pdf | author:Gautam Dasarathy, Robert Nowak, Sebastien Roch category:q-bio.PE cs.CE cs.DS math.PR math.ST stat.ML stat.TH published:2014-04-28 summary:We consider the problem of estimating the evolutionary history of a set ofspecies (phylogeny or species tree) from several genes. It is known that theevolutionary history of individual genes (gene trees) might be topologicallydistinct from each other and from the underlying species tree, possiblyconfounding phylogenetic analysis. A further complication in practice is thatone has to estimate gene trees from molecular sequences of finite length. Weprovide the first full data-requirement analysis of a species treereconstruction method that takes into account estimation errors at the genelevel. Under that criterion, we also devise a novel reconstruction algorithmthat provably improves over all previous methods in a regime of interest.
arxiv-6600-107 | Direct Density-Derivative Estimation and Its Application in KL-Divergence Approximation | http://arxiv.org/pdf/1406.7638v1.pdf | author:Hiroaki Sasaki, Yung-Kyun Noh, Masashi Sugiyama category:stat.ML published:2014-06-30 summary:Estimation of density derivatives is a versatile tool in statistical dataanalysis. A naive approach is to first estimate the density and then computeits derivative. However, such a two-step approach does not work well because agood density estimator does not necessarily mean a good density-derivativeestimator. In this paper, we give a direct method to approximate the densityderivative without estimating the density itself. Our proposed estimator allowsanalytic and computationally efficient approximation of multi-dimensionalhigh-order density derivatives, with the ability that all hyper-parameters canbe chosen objectively by cross-validation. We further show that the proposeddensity-derivative estimator is useful in improving the accuracy ofnon-parametric KL-divergence estimation via metric learning. The practicalsuperiority of the proposed method is experimentally demonstrated in changedetection and feature selection.
arxiv-6600-108 | Personalized Medical Treatments Using Novel Reinforcement Learning Algorithms | http://arxiv.org/pdf/1406.3922v2.pdf | author:Yousuf M. Soliman category:cs.LG stat.ML published:2014-06-16 summary:In both the fields of computer science and medicine there is very stronginterest in developing personalized treatment policies for patients who havevariable responses to treatments. In particular, I aim to find an optimalpersonalized treatment policy which is a non-deterministic function of thepatient specific covariate data that maximizes the expected survival time orclinical outcome. I developed an algorithmic framework to solve multistagedecision problem with a varying number of stages that are subject to censoringin which the "rewards" are expected survival times. In specific, I developed anovel Q-learning algorithm that dynamically adjusts for these parameters.Furthermore, I found finite upper bounds on the generalized error of thetreatment paths constructed by this algorithm. I have also shown that when theoptimal Q-function is an element of the approximation space, the anticipatedsurvival times for the treatment regime constructed by the algorithm willconverge to the optimal treatment path. I demonstrated the performance of theproposed algorithmic framework via simulation studies and through the analysisof chronic depression data and a hypothetical clinical trial. The censoredQ-learning algorithm I developed is more effective than the state of the artclinical decision support systems and is able to operate in environments whenmany covariate parameters may be unobtainable or censored.
arxiv-6600-109 | Navigating Robot Swarms Using Collective Intelligence Learned from Golden Shiner Fish | http://arxiv.org/pdf/1407.0008v1.pdf | author:Grace Gao category:cs.NE published:2014-06-30 summary:Navigating networked robot swarms often requires knowing where to go, sensingthe environment, and path-planning based on the destination and barriers in theenvironment. Such a process is computationally intensive. Moreover, as thenetwork scales up, the computational load increases quadratically, or evenexponentially. Unlike these man-made systems, most biological systems scalelinearly in complexity. Furthermore, the scale of a biological swarm can evenenable collective intelligence. One example comes from observations of goldenshiner fish. Golden shiners naturally prefer darkness and school together. Eachindividual golden shiner does not know where the darkness is. Neither does itsense the light gradients in the environment. However, by moving together as aschool, they always end up in the shady area. We apply such collectiveintelligence learned from golden shiner fish to navigating robot swarms. Eachindividual robot's dynamic is based on the gold shiners' movement strategy---arandom walk with its speed modulated by the light intensity and its directionaffected by its neighbors. The theoretical analysis and simulation results showthat our method 1) promises to navigate a robot swarm with little situationalknowledge, 2) simplifies control and decision-making for each individual robot,3) requires minimal or even no information exchange within the swarm, and 4) ishighly distributed, adaptive, and robust.
arxiv-6600-110 | Information Transfer in Swarms with Leaders | http://arxiv.org/pdf/1407.0007v1.pdf | author:Yu Sun, Louis F. Rossi, Chien-Chung Shen, Jennifer Miller, X. Rosalind Wang, Joseph T. Lizier, Mikhail Prokopenko, Upul Senanayake category:cs.NE published:2014-06-30 summary:Swarm dynamics is the study of collections of agents that interact with oneanother without central control. In natural systems, insects, birds, fish andother large mammals function in larger units to increase the overall fitness ofthe individuals. Their behavior is coordinated through local interactions toenhance mate selection, predator detection, migratory route identification andso forth [Andersson and Wallander 2003; Buhl et al. 2006; Nagy et al. 2010;Partridge 1982; Sumpter et al. 2008]. In artificial systems, swarms ofautonomous agents can augment human activities such as search and rescue, andenvironmental monitoring by covering large areas with multiple nodes [Alami etal. 2007; Caruso et al. 2008; Ogren et al. 2004; Paley et al. 2007; Sibley etal. 2002]. In this paper, we explore the interplay between swarm dynamics,covert leadership and theoretical information transfer. A leader is a member ofthe swarm that acts upon information in addition to what is provided by localinteractions. Depending upon the leadership model, leaders can use theirexternal information either all the time or in response to local conditions[Couzin et al. 2005; Sun et al. 2013]. A covert leader is a leader that istreated no differently than others in the swarm, so leaders and followersparticipate equally in whatever interaction model is used [Rossi et al. 2007].In this study, we use theoretical information transfer as a means of analyzingswarm interactions to explore whether or not it is possible to distinguishbetween followers and leaders based on interactions within the swarm. We findthat covert leaders can be distinguished from followers in a swarm because theyreceive less transfer entropy than followers.
arxiv-6600-111 | Human Communication Systems Evolve by Cultural Selection | http://arxiv.org/pdf/1406.7558v1.pdf | author:Nicolas Fay, Monica Tamariz, T Mark Ellison, Dale Barr category:cs.SI cs.CL physics.soc-ph published:2014-06-29 summary:Human communication systems, such as language, evolve culturally; theircomponents undergo reproduction and variation. However, a role for selection incultural evolutionary dynamics is less clear. Often neutral evolution (alsoknown as 'drift') models, are used to explain the evolution of humancommunication systems, and cultural evolution more generally. Under thisaccount, cultural change is unbiased: for instance, vocabulary, baby names andpottery designs have been found to spread through random copying. While drift is the null hypothesis for models of cultural evolution it doesnot always adequately explain empirical results. Alternative models includecultural selection, which assumes variant adoption is biased. Theoreticalmodels of human communication argue that during conversation interlocutors arebiased to adopt the same labels and other aspects of linguistic representation(including prosody and syntax). This basic alignment mechanism has beenextended by computer simulation to account for the emergence of linguisticconventions. When agents are biased to match the linguistic behavior of theirinterlocutor, a single variant can propagate across an entire population ofinteracting computer agents. This behavior-matching account operates at thelevel of the individual. We call it the Conformity-biased model. Under adifferent selection account, called content-biased selection, functionalselection or replicator selection, variant adoption depends upon the intrinsicvalue of the particular variant (e.g., ease of learning or use). This secondalternative account operates at the level of the cultural variant. FollowingBoyd and Richerson we call it the Content-biased model. The present paper teststhe drift model and the two biased selection models' ability to explain thespread of communicative signal variants in an experimental micro-society.
arxiv-6600-112 | Model Consistency of Partly Smooth Regularizers | http://arxiv.org/pdf/1405.1004v3.pdf | author:Samuel Vaiter, Gabriel Peyré, Jalal M. Fadili category:math.OC cs.IT math.IT stat.ML published:2014-05-05 summary:This paper studies least-square regression penalized with partly smoothconvex regularizers. This class of functions is very large and versatileallowing to promote solutions conforming to some notion of low-complexity.Indeed, they force solutions of variational problems to belong to alow-dimensional manifold (the so-called model) which is stable under smallperturbations of the function. This property is crucial to make the underlyinglow-complexity model robust to small noise. We show that a generalized"irrepresentable condition" implies stable model selection under small noiseperturbations in the observations and the design matrix, when theregularization parameter is tuned proportionally to the noise level. Thiscondition is shown to be almost a necessary condition. We then show that thiscondition implies model consistency of the regularized estimator. That is, witha probability tending to one as the number of measurements increases, theregularized estimator belongs to the correct low-dimensional model manifold.This work unifies and generalizes several previous ones, where modelconsistency is known to hold for sparse, group sparse, total variation andlow-rank regularizations.
arxiv-6600-113 | Exploring Task Mappings on Heterogeneous MPSoCs using a Bias-Elitist Genetic Algorithm | http://arxiv.org/pdf/1406.7539v1.pdf | author:Wei Quan, Andy D. Pimentel category:cs.PF cs.NE C.4 published:2014-06-29 summary:Exploration of task mappings plays a crucial role in achieving highperformance in heterogeneous multi-processor system-on-chip (MPSoC) platforms.The problem of optimally mapping a set of tasks onto a set of givenheterogeneous processors for maximal throughput has been known, in general, tobe NP-complete. The problem is further exacerbated when multiple applications(i.e., bigger task sets) and the communication between tasks are alsoconsidered. Previous research has shown that Genetic Algorithms (GA) typicallyare a good choice to solve this problem when the solution space is relativelysmall. However, when the size of the problem space increases, classic geneticalgorithms still suffer from the problem of long evolution times. To addressthis problem, this paper proposes a novel bias-elitist genetic algorithm thatis guided by domain-specific heuristics to speed up the evolution process.Experimental results reveal that our proposed algorithm is able to handle largescale task mapping problems and produces high-quality mapping solutions in onlya short time period.
arxiv-6600-114 | Estimating the distribution of Galaxy Morphologies on a continuous space | http://arxiv.org/pdf/1406.7536v1.pdf | author:Giuseppe Vinci, Peter Freeman, Jeffrey Newman, Larry Wasserman, Christopher Genovese category:astro-ph.GA astro-ph.CO stat.AP stat.CO stat.ML 85-08 published:2014-06-29 summary:The incredible variety of galaxy shapes cannot be summarized by human defineddiscrete classes of shapes without causing a possibly large loss ofinformation. Dictionary learning and sparse coding allow us to reduce the highdimensional space of shapes into a manageable low dimensional continuous vectorspace. Statistical inference can be done in the reduced space via probabilitydistribution estimation and manifold estimation.
arxiv-6600-115 | Fusion Based Holistic Road Scene Understanding | http://arxiv.org/pdf/1406.7525v1.pdf | author:Wenqi Huang, Xiaojin Gong category:cs.CV published:2014-06-29 summary:This paper addresses the problem of holistic road scene understanding basedon the integration of visual and range data. To achieve the grand goal, wepropose an approach that jointly tackles object-level image segmentation andsemantic region labeling within a conditional random field (CRF) framework.Specifically, we first generate semantic object hypotheses by clustering 3Dpoints, learning their prior appearance models, and using a deep learningmethod for reasoning their semantic categories. The learned priors, togetherwith spatial and geometric contexts, are incorporated in CRF. With thisformulation, visual and range data are fused thoroughly, and moreover, thecoupled segmentation and semantic labeling problem can be inferred via GraphCuts. Our approach is validated on the challenging KITTI dataset that containsdiverse complicated road scenarios. Both quantitative and qualitativeevaluations demonstrate its effectiveness.
arxiv-6600-116 | Jabalin: a Comprehensive Computational Model of Modern Standard Arabic Verbal Morphology Based on Traditional Arabic Prosody | http://arxiv.org/pdf/1406.7483v1.pdf | author:Alicia Gonzalez Martinez, Susana Lopez Hervas, Doaa Samy, Carlos G. Arques, Antonio Moreno Sandoval category:cs.CL published:2014-06-29 summary:The computational handling of Modern Standard Arabic is a challenge in thefield of natural language processing due to its highly rich morphology.However, several authors have pointed out that the Arabic morphological systemis in fact extremely regular. The existing Arabic morphological analyzers haveexploited this regularity to variable extent, yet we believe there is stillsome scope for improvement. Taking inspiration in traditional Arabic prosody,we have designed and implemented a compact and simple morphological systemwhich in our opinion takes further advantage of the regularities encountered inthe Arabic morphological system. The output of the system is a large-scalelexicon of inflected forms that has subsequently been used to create an OnlineInterface for a morphological analyzer of Arabic verbs. The Jabalin OnlineInterface is available at http://elvira.lllf.uam.es/jabalin/, hosted at theLLI-UAM lab. The generation system is also available under a GNU GPL 3 license.
arxiv-6600-117 | Missing Entries Matrix Approximation and Completion | http://arxiv.org/pdf/1302.6768v2.pdf | author:Gil Shabat, Yaniv Shmueli, Amir Averbuch category:math.NA cs.LG stat.ML published:2013-02-27 summary:We describe several algorithms for matrix completion and matrix approximationwhen only some of its entries are known. The approximation constraint can beany whose approximated solution is known for the full matrix. For low rankapproximations, similar algorithms appears recently in the literature underdifferent names. In this work, we introduce new theorems for matrixapproximation and show that these algorithms can be extended to handledifferent constraints such as nuclear norm, spectral norm, orthogonalityconstraints and more that are different than low rank approximations. As thealgorithms can be viewed from an optimization point of view, we discuss theirconvergence to global solution for the convex case. We also discuss the optimalstep size and show that it is fixed in each iteration. In addition, the derivedmatrix completion flow is robust and does not require any parameters. Thismatrix completion flow is applicable to different spectral minimizations andcan be applied to physics, mathematics and electrical engineering problems suchas data reconstruction of images and data coming from PDEs such as Helmholtzequation used for electromagnetic waves.
arxiv-6600-118 | Graphical structure of conditional independencies in determinantal point processes | http://arxiv.org/pdf/1406.5577v2.pdf | author:Tvrtko Tadić category:math.PR math.ST stat.ML stat.TH published:2014-06-21 summary:Determinantal point process have recently been used as models in machinelearning and this has raised questions regarding the characterizations ofconditional independence. In this paper we investigate characterizations ofconditional independence. We describe some conditional independencies throughthe conditions on the kernel of a determinantal point process, and show manycan be obtained using the graph induced by a kernel of the $L$-ensemble.
arxiv-6600-119 | Learning Nonlinear Functions Using Regularized Greedy Forest | http://arxiv.org/pdf/1109.0887v7.pdf | author:Rie Johnson, Tong Zhang category:stat.ML published:2011-09-05 summary:We consider the problem of learning a forest of nonlinear decision rules withgeneral loss functions. The standard methods employ boosted decision trees suchas Adaboost for exponential loss and Friedman's gradient boosting for generalloss. In contrast to these traditional boosting algorithms that treat a treelearner as a black box, the method we propose directly learns decision forestsvia fully-corrective regularized greedy search using the underlying foreststructure. Our method achieves higher accuracy and smaller models than gradientboosting (and Adaboost with exponential loss) on many datasets.
arxiv-6600-120 | Contrastive Feature Induction for Efficient Structure Learning of Conditional Random Fields | http://arxiv.org/pdf/1406.7445v1.pdf | author:Ni Lao, Jun Zhu category:cs.LG published:2014-06-28 summary:Structure learning of Conditional Random Fields (CRFs) can be cast into anL1-regularized optimization problem. To avoid optimizing over a fully linkedmodel, gain-based or gradient-based feature selection methods start from anempty model and incrementally add top ranked features to it. However, forhigh-dimensional problems like statistical relational learning, training timeof these incremental methods can be dominated by the cost of evaluating thegain or gradient of a large collection of candidate features. In this study wepropose a fast feature evaluation algorithm called Contrastive FeatureInduction (CFI), which only evaluates a subset of features that involve bothvariables with high signals (deviation from mean) and variables with higherrors (residue). We prove that the gradient of candidate features can berepresented solely as a function of signals and errors, and that CFI is anefficient approximation of gradient-based evaluation methods. Experiments onsynthetic and real data sets show competitive learning speed and accuracy ofCFI on pairwise CRFs, compared to state-of-the-art structure learning methodssuch as full optimization over all features, and Grafting.
arxiv-6600-121 | Learning to Deblur | http://arxiv.org/pdf/1406.7444v1.pdf | author:Christian J. Schuler, Michael Hirsch, Stefan Harmeling, Bernhard Schölkopf category:cs.CV cs.LG published:2014-06-28 summary:We describe a learning-based approach to blind image deconvolution. It uses adeep layered architecture, parts of which are borrowed from recent work onneural network learning, and parts of which incorporate computations that arespecific to image deconvolution. The system is trained end-to-end on a set ofartificially generated training examples, enabling competitive performance inblind deconvolution, both with respect to quality and runtime.
arxiv-6600-122 | Comparison of SVM Optimization Techniques in the Primal | http://arxiv.org/pdf/1406.7429v1.pdf | author:Jonathan Katzman, Diane Duros category:cs.LG published:2014-06-28 summary:This paper examines the efficacy of different optimization techniques in aprimal formulation of a support vector machine (SVM). Three main techniques arecompared. The dataset used to compare all three techniques was the SentimentAnalysis on Movie Reviews dataset, from kaggle.com.
arxiv-6600-123 | Credibility Adjusted Term Frequency: A Supervised Term Weighting Scheme for Sentiment Analysis and Text Classification | http://arxiv.org/pdf/1405.3518v2.pdf | author:Yoon Kim, Owen Zhang category:cs.CL cs.IR published:2014-05-14 summary:We provide a simple but novel supervised weighting scheme for adjusting termfrequency in tf-idf for sentiment analysis and text classification. We compareour method to baseline weighting schemes and find that it outperforms them onmultiple benchmarks. The method is robust and works well on both snippets andlonger documents.
arxiv-6600-124 | Intelligent Emergency Message Broadcasting in VANET Using PSO | http://arxiv.org/pdf/1406.7399v1.pdf | author:Ghassan Samara, Tareq Alhmiedat category:cs.NI cs.NE published:2014-06-28 summary:The new type of Mobile Ad hoc Network which is called Vehicular Ad hocNetworks (VANET) created a fertile environment for research. In this research,a protocol Particle Swarm Optimization Contention Based Broadcast (PCBB) isproposed, for fast andeffective dissemination of emergency messages within ageographical area to distribute the emergency message and achieve the safetysystem, this research will help the VANET system to achieve its safety goals inintelligent and efficient way.
arxiv-6600-125 | Exponentially Increasing the Capacity-to-Computation Ratio for Conditional Computation in Deep Learning | http://arxiv.org/pdf/1406.7362v1.pdf | author:Kyunghyun Cho, Yoshua Bengio category:stat.ML cs.LG cs.NE published:2014-06-28 summary:Many state-of-the-art results obtained with deep networks are achieved withthe largest models that could be trained, and if more computation power wasavailable, we might be able to exploit much larger datasets in order to improvegeneralization ability. Whereas in learning algorithms such as decision treesthe ratio of capacity (e.g., the number of parameters) to computation is veryfavorable (up to exponentially more parameters than computation), the ratio isessentially 1 for deep neural networks. Conditional computation has beenproposed as a way to increase the capacity of a deep neural network withoutincreasing the amount of computation required, by activating some parametersand computation "on-demand", on a per-example basis. In this note, we propose anovel parametrization of weight matrices in neural networks which has thepotential to increase up to exponentially the ratio of the number of parametersto computation. The proposed approach is based on turning on some parameters(weight matrices) when specific bit patterns of hidden unit activations areobtained. In order to better control for the overfitting that might result, wepropose a parametrization that is tree-structured, where each node of the treecorresponds to a prefix of a sequence of sign bits, or gating units, associatedwith hidden units.
arxiv-6600-126 | Optimal Demand Response Using Device Based Reinforcement Learning | http://arxiv.org/pdf/1401.1549v2.pdf | author:Zheng Wen, Daniel O'Neill, Hamid Reza Maei category:cs.LG cs.AI cs.SY published:2014-01-08 summary:Demand response (DR) for residential and small commercial buildings isestimated to account for as much as 65% of the total energy savings potentialof DR, and previous work shows that a fully automated Energy Management System(EMS) is a necessary prerequisite to DR in these areas. In this paper, wepropose a novel EMS formulation for DR problems in these sectors. Specifically,we formulate a fully automated EMS's rescheduling problem as a reinforcementlearning (RL) problem, and argue that this RL problem can be approximatelysolved by decomposing it over device clusters. Compared with existingformulations, our new formulation (1) does not require explicitly modeling theuser's dissatisfaction on job rescheduling, (2) enables the EMS toself-initiate jobs, (3) allows the user to initiate more flexible requests and(4) has a computational complexity linear in the number of devices. We alsodemonstrate the simulation results of applying Q-learning, one of the mostpopular and classical RL algorithms, to a representative example.
arxiv-6600-127 | Stock Market Prediction from WSJ: Text Mining via Sparse Matrix Factorization | http://arxiv.org/pdf/1406.7330v1.pdf | author:Felix Ming Fai Wong, Zhenming Liu, Mung Chiang category:cs.LG q-fin.ST published:2014-06-27 summary:We revisit the problem of predicting directional movements of stock pricesbased on news articles: here our algorithm uses daily articles from The WallStreet Journal to predict the closing stock prices on the same day. We proposea unified latent space model to characterize the "co-movements" between stockprices and news articles. Unlike many existing approaches, our new model isable to simultaneously leverage the correlations: (a) among stock prices, (b)among news articles, and (c) between stock prices and news articles. Thus, ourmodel is able to make daily predictions on more than 500 stocks (most of whichare not even mentioned in any news article) while having low complexity. Wecarry out extensive backtesting on trading strategies based on our algorithm.The result shows that our model has substantially better accuracy rate (55.7%)compared to many widely used algorithms. The return (56%) and Sharpe ratio dueto a trading strategy based on our model are also much higher than baselineindices.
arxiv-6600-128 | On the Use of Different Feature Extraction Methods for Linear and Non Linear kernels | http://arxiv.org/pdf/1406.7314v1.pdf | author:Imen Trabelsi, Dorra Ben Ayed category:cs.CL cs.LG published:2014-06-27 summary:The speech feature extraction has been a key focus in robust speechrecognition research; it significantly affects the recognition performance. Inthis paper, we first study a set of different features extraction methods suchas linear predictive coding (LPC), mel frequency cepstral coefficient (MFCC)and perceptual linear prediction (PLP) with several features normalizationtechniques like rasta filtering and cepstral mean subtraction (CMS). Based onthis, a comparative evaluation of these features is performed on the task oftext independent speaker identification using a combination between gaussianmixture models (GMM) and linear and non-linear kernels based on support vectormachine (SVM).
arxiv-6600-129 | Fourier PCA and Robust Tensor Decomposition | http://arxiv.org/pdf/1306.5825v5.pdf | author:Navin Goyal, Santosh Vempala, Ying Xiao category:cs.LG cs.DS stat.ML published:2013-06-25 summary:Fourier PCA is Principal Component Analysis of a matrix obtained from higherorder derivatives of the logarithm of the Fourier transform of adistribution.We make this method algorithmic by developing a tensordecomposition method for a pair of tensors sharing the same vectors in rank-$1$decompositions. Our main application is the first provably polynomial-timealgorithm for underdetermined ICA, i.e., learning an $n \times m$ matrix $A$from observations $y=Ax$ where $x$ is drawn from an unknown productdistribution with arbitrary non-Gaussian components. The number of componentdistributions $m$ can be arbitrarily higher than the dimension $n$ and thecolumns of $A$ only need to satisfy a natural and efficiently verifiablenondegeneracy condition. As a second application, we give an alternativealgorithm for learning mixtures of spherical Gaussians with linearlyindependent means. These results also hold in the presence of Gaussian noise.
arxiv-6600-130 | A Multi Level Data Fusion Approach for Speaker Identification on Telephone Speech | http://arxiv.org/pdf/1407.0380v1.pdf | author:Imen Trabelsi, Dorra Ben Ayed category:cs.SD cs.LG published:2014-06-27 summary:Several speaker identification systems are giving good performance with cleanspeech but are affected by the degradations introduced by noisy audioconditions. To deal with this problem, we investigate the use of complementaryinformation at different levels for computing a combined match score for theunknown speaker. In this work, we observe the effect of two supervised machinelearning approaches including support vectors machines (SVM) and na\"ive bayes(NB). We define two feature vector sets based on mel frequency cepstralcoefficients (MFCC) and relative spectral perceptual linear predictivecoefficients (RASTA-PLP). Each feature is modeled using the Gaussian MixtureModel (GMM). Several ways of combining these information sources givesignificant improvements in a text-independent speaker identification taskusing a very large telephone degraded NTIMIT database.
arxiv-6600-131 | Neighborhood filters and the decreasing rearrangement | http://arxiv.org/pdf/1311.2191v2.pdf | author:Gonzalo Galiano, Julián Velasco category:cs.CV 68U10 published:2013-11-09 summary:Nonlocal filters are simple and powerful techniques for image denoising. Inthis paper, we give new insights into the analysis of one kind of them, theNeighborhood filter, by using a classical although not very commontransformation: the decreasing rearrangement of a function (the image).Independently of the dimension of the image, we reformulate the Neighborhoodfilter and its iterative variants as an integral operator defined in aone-dimensional space. The simplicity of this formulation allows to perform adetailed analysis of its properties. Among others, we prove that the filterbehaves asymptotically as a shock filter combined with a border diffusive term,responsible for the staircaising effect and the loss of contrast.
arxiv-6600-132 | Inducing Language Networks from Continuous Space Word Representations | http://arxiv.org/pdf/1403.1252v2.pdf | author:Bryan Perozzi, Rami Al-Rfou, Vivek Kulkarni, Steven Skiena category:cs.LG cs.CL cs.SI published:2014-03-06 summary:Recent advancements in unsupervised feature learning have developed powerfullatent representations of words. However, it is still not clear what makes onerepresentation better than another and how we can learn the idealrepresentation. Understanding the structure of latent spaces attained is key toany future advancement in unsupervised learning. In this work, we introduce anew view of continuous space word representations as language networks. Weexplore two techniques to create language networks from learned features byinducing them for two popular word representation methods and examining theproperties of their resulting networks. We find that the induced networksdiffer from other methods of creating language networks, and that they containmeaningful community structure.
arxiv-6600-133 | Polyglot: Distributed Word Representations for Multilingual NLP | http://arxiv.org/pdf/1307.1662v2.pdf | author:Rami Al-Rfou, Bryan Perozzi, Steven Skiena category:cs.CL cs.LG published:2013-07-05 summary:Distributed word representations (word embeddings) have recently contributedto competitive performance in language modeling and several NLP tasks. In thiswork, we train word embeddings for more than 100 languages using theircorresponding Wikipedias. We quantitatively demonstrate the utility of our wordembeddings by using them as the sole features for training a part of speechtagger for a subset of these languages. We find their performance to becompetitive with near state-of-art methods in English, Danish and Swedish.Moreover, we investigate the semantic features captured by these embeddingsthrough the proximity of word groupings. We will release these embeddingspublicly to help researchers in the development and enhancement of multilingualapplications.
arxiv-6600-134 | DeepWalk: Online Learning of Social Representations | http://arxiv.org/pdf/1403.6652v2.pdf | author:Bryan Perozzi, Rami Al-Rfou, Steven Skiena category:cs.SI cs.LG published:2014-03-26 summary:We present DeepWalk, a novel approach for learning latent representations ofvertices in a network. These latent representations encode social relations ina continuous vector space, which is easily exploited by statistical models.DeepWalk generalizes recent advancements in language modeling and unsupervisedfeature learning (or deep learning) from sequences of words to graphs. DeepWalkuses local information obtained from truncated random walks to learn latentrepresentations by treating walks as the equivalent of sentences. Wedemonstrate DeepWalk's latent representations on several multi-label networkclassification tasks for social networks such as BlogCatalog, Flickr, andYouTube. Our results show that DeepWalk outperforms challenging baselines whichare allowed a global view of the network, especially in the presence of missinginformation. DeepWalk's representations can provide $F_1$ scores up to 10%higher than competing methods when labeled data is sparse. In some experiments,DeepWalk's representations are able to outperform all baseline methods whileusing 60% less training data. DeepWalk is also scalable. It is an onlinelearning algorithm which builds useful incremental results, and is triviallyparallelizable. These qualities make it suitable for a broad class of realworld applications such as network classification, and anomaly detection.
arxiv-6600-135 | Performance Limits of Dictionary Learning for Sparse Coding | http://arxiv.org/pdf/1402.4078v2.pdf | author:Alexander Jung, Yonina C. Eldar, Norbert Görtz category:stat.ML published:2014-02-17 summary:We consider the problem of dictionary learning under the assumption that theobserved signals can be represented as sparse linear combinations of thecolumns of a single large dictionary matrix. In particular, we analyze theminimax risk of the dictionary learning problem which governs the mean squarederror (MSE) performance of any learning scheme, regardless of its computationalcomplexity. By following an established information-theoretic method based onFanos inequality, we derive a lower bound on the minimax risk for a givendictionary learning problem. This lower bound yields a characterization of thesample-complexity, i.e., a lower bound on the required number of observationssuch that consistent dictionary learning schemes exist. Our bounds may becompared with the performance of a given learning scheme, allowing tocharacterize how far the method is from optimal performance.
arxiv-6600-136 | Optimal Population Codes for Control and Estimation | http://arxiv.org/pdf/1406.7179v1.pdf | author:Alex Susemihl, Ron Meir, Manfred Opper category:stat.ML cs.IT math.IT q-bio.NC published:2014-06-27 summary:Agents acting in the natural world aim at selecting appropriate actions basedon noisy and partial sensory observations. Many behaviors leading to decisionmak- ing and action selection in a closed loop setting are naturally phrasedwithin a control theoretic framework. Within the framework of optimal ControlTheory, one is usually given a cost function which is minimized by selecting acontrol law based on the observations. While in standard control settings thesensors are assumed fixed, biological systems often gain from the extraflexibility of optimiz- ing the sensors themselves. However, this sensoryadaptation is geared towards control rather than perception, as is oftenassumed. In this work we show that sen- sory adaptation for control differsfrom sensory adaptation for perception, even for simple control setups. Thisimplies, consistently with recent experimental results, that when studyingsensory adaptation, it is essential to account for the task being performed.
arxiv-6600-137 | An improved computer vision method for detecting white blood cells | http://arxiv.org/pdf/1406.6946v2.pdf | author:Erik Cuevas, Margarita Diaz, Miguel Manzanares, Daniel Zaldivar, Marco Perez category:cs.CV published:2014-06-26 summary:The automatic detection of White Blood Cells (WBC) still remains as anunsolved issue in medical imaging. The analysis of WBC images has engagedresearchers from fields of medicine and computer vision alike. Since WBC can beapproximated by an ellipsoid form, an ellipse detector algorithm may besuccessfully applied in order to recognize them. This paper presents analgorithm for the automatic detection of WBC embedded into complicated andcluttered smear images that considers the complete process as a multi-ellipsedetection problem. The approach, based on the Differential Evolution (DE)algorithm, transforms the detection task into an optimization problem whereindividuals emulate candidate ellipses. An objective function evaluates if suchcandidate ellipses are really present in the edge image of the smear. Guided bythe values of such function, the set of encoded candidate ellipses(individuals) are evolved using the DE algorithm so that they can fit into theWBC enclosed within the edge-only map of the image. Experimental results fromwhite blood cell images with a varying range of complexity are included tovalidate the efficiency of the proposed technique in terms of accuracy androbustness.
arxiv-6600-138 | On a new formulation of nonlocal image filters involving the relative rearrangement | http://arxiv.org/pdf/1406.7128v1.pdf | author:Gonzalo Galiano, Julián Velasco category:cs.CV 68U10 published:2014-06-27 summary:Nonlocal filters are simple and powerful techniques for image denoising. Inthis paper we study the reformulation of a broad class of nonlocal filters interms of two functional rearrangements: the decreasing and the relativerearrangements. Independently of the dimension of the image, we reformulate these filters asintegral operators defined in a one-dimensional space corresponding to thelevel sets measures. We prove the equivalency between the original and the rearranged versions ofthe filters and propose a discretization in terms of constant-wiseinterpolators, which we prove to be convergent to the solution of thecontinuous setting. For some particular cases, this new formulation allows us to perform adetailed analysis of the filtering properties. Among others, we prove that thefiltered image is a contrast change of the original image, and that thefiltering procedure behaves asymptotically as a shock filter combined with aborder diffusive term, responsible for the staircaising effect and the loss ofcontrast.
arxiv-6600-139 | Template Matching based Object Detection Using HOG Feature Pyramid | http://arxiv.org/pdf/1406.7120v1.pdf | author:Anish Acharya category:cs.CV published:2014-06-27 summary:This article provides a step by step development of designing a ObjectDetection scheme using the HOG based Feature Pyramid aligned with the conceptof Template Matching.
arxiv-6600-140 | 3D planar patch extraction from stereo using probabilistic region growing | http://arxiv.org/pdf/1406.7112v1.pdf | author:Vasileios Zografos category:cs.CV published:2014-06-27 summary:This article presents a novel 3D planar patch extraction method using aprobabilistic region growing algorithm. Our method works by simultaneouslyinitiating multiple planar patches from seed points, the latter determined byan intensity-based 2D segmentation algorithm in the stereo-pair images. Thepatches are grown incrementally and in parallel as 3D scene points areconsidered for membership, using a probabilistic distance likelihood measure.In addition, we have incorporated prior information based on the noise model inthe 2D images and the scene configuration but also include the intensityinformation resulting from the initial segmentation. This method works wellacross many different data-sets, involving real and synthetic examples of bothregularly and non-regularly sampled data, and is fast enough that may be usedfor robot navigation tasks of path detection and obstacle avoidance.
arxiv-6600-141 | Adaptive texture energy measure method | http://arxiv.org/pdf/1406.7075v1.pdf | author:Omer Faruk Ertugrul category:cs.CV published:2014-06-27 summary:Recent developments in image quality, data storage, and computationalcapacity have heightened the need for texture analysis in image process. Todate various methods have been developed and introduced for assessing texturesin images. One of the most popular texture analysis methods is the TextureEnergy Measure (TEM) and it has been used for detecting edges, levels, waves,spots and ripples by employing predefined TEM masks to images. Despite severalsuccess- ful studies, TEM has a number of serious weaknesses in use. The majordrawback is; the masks are predefined therefore they cannot be adapted toimage. A new method, Adaptive Texture Energy Measure Method (aTEM), was offeredto over- come this disadvantage of TEM by using adaptive masks by adjusting thecontrast, sharpening and orientation angle of the mask. To assess theapplicability of aTEM, it is compared with TEM. The accuracy of theclassification of butterfly, flower seed and Brodatz datasets are 0.08, 0.3292and 0.3343, respectively by TEM and 0.0053, 0.2417 and 0.3153, respectively byaTEM. The results of this study indicate that aTEM is a successful method fortexture analysis.
arxiv-6600-142 | Adaptive Mesh Representation and Restoration of Biomedical Images | http://arxiv.org/pdf/1406.7062v1.pdf | author:Ke Liu, Ming Xu, Zeyun Yu category:cs.CV published:2014-06-27 summary:The triangulation of images has become an active research area in recentyears for its compressive representation and ease of image processing andvisualization. However, little work has been done on how to faithfully recoverimage intensities from a triangulated mesh of an image, a process also known asimage restoration or decoding from meshes. The existing methods such as linearinterpolation, least-square interpolation, or interpolation based on radialbasis functions (RBFs) work to some extent, but often yield blurred features(edges, corners, etc.). The main reason for this problem is due to theisotropically-defined Euclidean distance that is taken into consideration inthese methods, without considering the anisotropicity of feature intensities inan image. Moreover, most existing methods use intensities defined at mesh nodeswhose intensities are often ambiguously defined on or near image edges (orfeature boundaries). In the current paper, a new method of restoring an imagefrom its triangulation representation is proposed, by utilizing anisotropicradial basis functions (ARBFs). This method considers not only the geometrical(Euclidean) distances but also the local feature orientations (anisotropicintensities). Additionally, this method is based on the intensities of meshfaces instead of mesh nodes and thus provides a more robust restoration. Thetwo strategies together guarantee excellent feature-preserving restoration ofan image with arbitrary super-resolutions from its triangulationrepresentation, as demonstrated by various experiments provided in the paper.
arxiv-6600-143 | Architecture of a Web-based Predictive Editor for Controlled Natural Language Processing | http://arxiv.org/pdf/1408.0016v1.pdf | author:Stephen Guy, Rolf Schwitter category:cs.CL cs.AI published:2014-06-27 summary:In this paper, we describe the architecture of a web-based predictive texteditor being developed for the controlled natural language PENG$^{ASP)$. Thiscontrolled language can be used to write non-monotonic specifications that havethe same expressive power as Answer Set Programs. In order to support thewriting process of these specifications, the predictive text editorcommunicates asynchronously with the controlled natural language processor thatgenerates lookahead categories and additional auxiliary information for theauthor of a specification text. The text editor can display multiple sets oflookahead categories simultaneously for different possible sentencecompletions, anaphoric expressions, and supports the addition of new contentwords to the lexicon.
arxiv-6600-144 | Generalized Canonical Correlation Analysis for Classification | http://arxiv.org/pdf/1304.7981v5.pdf | author:Cencheng Shen, Ming Sun, Minh Tang, Carey E. Priebe category:stat.ML published:2013-04-30 summary:For multiple multivariate data sets, we derive conditions under whichGeneralized Canonical Correlation Analysis (GCCA) improves classificationperformance of the projected datasets, compared to standard CanonicalCorrelation Analysis (CCA) using only two data sets. We illustrate ourtheoretical results with simulations and a real data experiment.
arxiv-6600-145 | Deep Learning Multi-View Representation for Face Recognition | http://arxiv.org/pdf/1406.6947v1.pdf | author:Zhenyao Zhu, Ping Luo, Xiaogang Wang, Xiaoou Tang category:cs.CV published:2014-06-26 summary:Various factors, such as identities, views (poses), and illuminations, arecoupled in face images. Disentangling the identity and view representations isa major challenge in face recognition. Existing face recognition systems eitheruse handcrafted features or learn features discriminatively to improverecognition accuracy. This is different from the behavior of human brain.Intriguingly, even without accessing 3D data, human not only can recognize faceidentity, but can also imagine face images of a person under differentviewpoints given a single 2D image, making face perception in the brain robustto view changes. In this sense, human brain has learned and encoded 3D facemodels from 2D images. To take into account this instinct, this paper proposesa novel deep neural net, named multi-view perceptron (MVP), which can untanglethe identity and view features, and infer a full spectrum of multi-view imagesin the meanwhile, given a single 2D face image. The identity features of MVPachieve superior performance on the MultiPIE dataset. MVP is also capable tointerpolate and predict images under viewpoints that are unobserved in thetraining data.
arxiv-6600-146 | Edge Label Inference in Generalized Stochastic Block Models: from Spectral Theory to Impossibility Results | http://arxiv.org/pdf/1406.6897v1.pdf | author:Jiaming Xu, Laurent Massoulié, Marc Lelarge category:math.ST stat.ML stat.TH published:2014-06-26 summary:The classical setting of community detection consists of networks exhibitinga clustered structure. To more accurately model real systems we consider aclass of networks (i) whose edges may carry labels and (ii) which may lack aclustered structure. Specifically we assume that nodes possess latentattributes drawn from a general compact space and edges between two nodes arerandomly generated and labeled according to some unknown distribution as afunction of their latent attributes. Our goal is then to infer the edge labeldistributions from a partially observed network. We propose a computationallyefficient spectral algorithm and show it allows for asymptotically correctinference when the average node degree could be as low as logarithmic in thetotal number of nodes. Conversely, if the average node degree is below aspecific constant threshold, we show that no algorithm can achieve betterinference than guessing without using the observations. As a byproduct of ouranalysis, we show that our model provides a general procedure to constructrandom graph models with a spectrum asymptotic to a pre-specified eigenvaluedistribution such as a power-law distribution.
arxiv-6600-147 | Estimation of Human Body Shape and Posture Under Clothing | http://arxiv.org/pdf/1312.4967v2.pdf | author:Stefanie Wuhrer, Leonid Pishchulin, Alan Brunton, Chang Shu, Jochen Lang category:cs.CV cs.GR published:2013-12-17 summary:Estimating the body shape and posture of a dressed human subject in motionrepresented as a sequence of (possibly incomplete) 3D meshes is important forvirtual change rooms and security. To solve this problem, statistical shapespaces encoding human body shape and posture variations are commonly used toconstrain the search space for the shape estimate. In this work, we propose anovel method that uses a posture-invariant shape space to model body shapevariation combined with a skeleton-based deformation to model posturevariation. Our method can estimate the body shape and posture of both staticscans and motion sequences of dressed human body scans. In case of motionsequences, our method takes advantage of motion cues to solve for a single bodyshape estimate along with a sequence of posture estimates. We apply ourapproach to both static scans and motion sequences and demonstrate that usingour method, higher fitting accuracy is achieved than when using a variant ofthe popular SCAPE model as statistical model.
arxiv-6600-148 | A Fully Automated Latent Fingerprint Matcher with Embedded Self-learning Segmentation Module | http://arxiv.org/pdf/1406.6854v1.pdf | author:Jinwei Xu, Jiankun Hu, Xiuping Jia category:cs.CR cs.CV published:2014-06-26 summary:Latent fingerprint has the practical value to identify the suspects who haveunintentionally left a trace of fingerprint in the crime scenes. However,designing a fully automated latent fingerprint matcher is a very challengingtask as it needs to address many challenging issues including the separation ofoverlapping structured patterns over the partial and poor quality latentfingerprint image, and finding a match against a large background database thatwould have different resolutions. Currently there is no fully automated latentfingerprint matcher available to the public and most literature reports haveutilized a specialized latent fingerprint matcher COTS3 which is not accessibleto the public. This will make it infeasible to assess and compare the relevantresearch work which is vital for this research community. In this study, wetarget to develop a fully automated latent matcher for adaptive detection ofthe region of interest and robust matching of latent prints. Unlike themanually conducted matching procedure, the proposed latent matcher can run likea sealed black box without any manual intervention. This matcher consists ofthe following two modules: (i) the dictionary learning-based region of interest(ROI) segmentation scheme; and (ii) the genetic algorithm-based minutiae setmatching unit. Experimental results on NIST SD27 latent fingerprint databasedemonstrates that the proposed matcher outperforms the currently publicstate-of-art latent fingerprint matcher.
arxiv-6600-149 | FrameNet Resource Grammar Library for GF | http://arxiv.org/pdf/1406.6844v1.pdf | author:Normunds Gruzitis, Peteris Paikens, Guntis Barzdins category:cs.CL published:2014-06-26 summary:In this paper we present an ongoing research investigating the possibilityand potential of integrating frame semantics, particularly FrameNet, in theGrammatical Framework (GF) application grammar development. An importantcomponent of GF is its Resource Grammar Library (RGL) that encapsulates thelow-level linguistic knowledge about morphology and syntax of currently morethan 20 languages facilitating rapid development of multilingual applications.In the ideal case, porting a GF application grammar to a new language wouldonly require introducing the domain lexicon - translation equivalents that areinterlinked via common abstract terms. While it is possible for a highlyrestricted CNL, developing and porting a less restricted CNL requires aboveaverage linguistic knowledge about the particular language, and above averageGF experience. Specifying a lexicon is mostly straightforward in the case ofnouns (incl. multi-word units), however, verbs are the most complex category(in terms of both inflectional paradigms and argument structure), and addingthem to a GF application grammar is not a straightforward task. In this paperwe are focusing on verbs, investigating the possibility of creating amultilingual FrameNet-based GF library. We propose an extension to the currentRGL, allowing GF application developers to define clauses on the semanticlevel, thus leaving the language-specific syntactic mapping to this extension.We demonstrate our approach by reengineering the MOLTO Phrasebook applicationgrammar.
arxiv-6600-150 | Overlapping Community Detection Optimization and Nash Equilibrium | http://arxiv.org/pdf/1406.6832v1.pdf | author:Michel Crampes, Michel Plantié category:cs.SI physics.soc-ph stat.ML published:2014-06-26 summary:Community detection using both graphs and social networks is the focus ofmany algorithms. Recent methods aimed at optimizing the so-called modularityfunction proceed by maximizing relations within communities while minimizinginter-community relations. However, given the NP-completeness of the problem, these algorithms areheuristics that do not guarantee an optimum. In this paper, we introduce a newalgorithm along with a function that takes an approximate solution and modifiesit in order to reach an optimum. This reassignment function is considered a'potential function' and becomes a necessary condition to asserting that thecomputed optimum is indeed a Nash Equilibrium. We also use this function tosimultaneously show partitioning and overlapping communities, two detection andvisualization modes of great value in revealing interesting features of asocial network. Our approach is successfully illustrated through severalexperiments on either real unipartite, multipartite or directed graphs ofmedium and large-sized datasets.
arxiv-6600-151 | Online learning in MDPs with side information | http://arxiv.org/pdf/1406.6812v1.pdf | author:Yasin Abbasi-Yadkori, Gergely Neu category:cs.LG stat.ML published:2014-06-26 summary:We study online learning of finite Markov decision process (MDP) problemswhen a side information vector is available. The problem is motivated byapplications such as clinical trials, recommendation systems, etc. Suchapplications have an episodic structure, where each episode corresponds to apatient/customer. Our objective is to compete with the optimal dynamic policythat can take side information into account. We propose a computationally efficient algorithm and show that its regret isat most $O(\sqrt{T})$, where $T$ is the number of rounds. To best of ourknowledge, this is the first regret bound for this setting.
arxiv-6600-152 | Mass-Univariate Hypothesis Testing on MEEG Data using Cross-Validation | http://arxiv.org/pdf/1406.6720v1.pdf | author:Seyed Mostafa Kia category:stat.ML cs.LG math.ST stat.TH published:2014-06-25 summary:Recent advances in statistical theory, together with advances in thecomputational power of computers, provide alternative methods to domass-univariate hypothesis testing in which a large number of univariate tests,can be properly used to compare MEEG data at a large number of time-frequencypoints and scalp locations. One of the major problematic aspects of this kindof mass-univariate analysis is due to high number of accomplished hypothesistests. Hence procedures that remove or alleviate the increased probability offalse discoveries are crucial for this type of analysis. Here, I propose a newmethod for mass-univariate analysis of MEEG data based on cross-validationscheme. In this method, I suggest a hierarchical classification procedure underk-fold cross-validation to detect which sensors at which time-bin and whichfrequency-bin contributes in discriminating between two different stimuli ortasks. To achieve this goal, a new feature extraction method based on thediscrete cosine transform (DCT) employed to get maximum advantage of all threedata dimensions. Employing cross-validation and hierarchy architecturealongside the DCT feature space makes this method more reliable and at the sametime enough sensitive to detect the narrow effects in brain activities.
arxiv-6600-153 | Systematic N-tuple Networks for Position Evaluation: Exceeding 90% in the Othello League | http://arxiv.org/pdf/1406.1509v3.pdf | author:Wojciech Jaśkowski category:cs.NE cs.AI cs.LG 68T05 published:2014-06-05 summary:N-tuple networks have been successfully used as position evaluation functionsfor board games such as Othello or Connect Four. The effectiveness of suchnetworks depends on their architecture, which is determined by the placement ofconstituent n-tuples, sequences of board locations, providing input to thenetwork. The most popular method of placing n-tuples consists in randomlygenerating a small number of long, snake-shaped board location sequences. Incomparison, we show that learning n-tuple networks is significantly moreeffective if they involve a large number of systematically placed, short,straight n-tuples. Moreover, we demonstrate that in order to obtain the bestperformance and the steepest learning curve for Othello it is enough to usen-tuples of size just 2, yielding a network consisting of only 288 weights. Thebest such network evolved in this study has been evaluated in the onlineOthello League, obtaining the performance of nearly 96% --- more than any otherplayer to date.
arxiv-6600-154 | Learning the ergodic decomposition | http://arxiv.org/pdf/1406.6670v1.pdf | author:Nabil Al-Najjar, Eran Shmaya category:math.ST math.PR stat.ML stat.TH published:2014-06-25 summary:A Bayesian agent learns about the structure of a stationary process from ob-serving past outcomes. We prove that his predictions about the near futurebecome ap- proximately those he would have made if he knew the long runempirical frequencies of the process.
arxiv-6600-155 | Causality Networks | http://arxiv.org/pdf/1406.6651v1.pdf | author:Ishanu Chattopadhyay category:cs.LG cs.IT math.IT q-fin.ST stat.ML 68Q32 published:2014-06-25 summary:While correlation measures are used to discern statistical relationshipsbetween observed variables in almost all branches of data-driven scientificinquiry, what we are really interested in is the existence of causaldependence. Designing an efficient causality test, that may be carried out inthe absence of restrictive pre-suppositions on the underlying dynamicalstructure of the data at hand, is non-trivial. Nevertheless, ability tocomputationally infer statistical prima facie evidence of causal dependence mayyield a far more discriminative tool for data analysis compared to thecalculation of simple correlations. In the present work, we present a newnon-parametric test of Granger causality for quantized or symbolic data streamsgenerated by ergodic stationary sources. In contrast to state-of-art binarytests, our approach makes precise and computes the degree of causal dependencebetween data streams, without making any restrictive assumptions, linearity orotherwise. Additionally, without any a priori imposition of specific dynamicalstructure, we infer explicit generative models of causal cross-dependence,which may be then used for prediction. These explicit models are represented asgeneralized probabilistic automata, referred to crossed automata, and are shownto be sufficient to capture a fairly general class of causal dependence. Theproposed algorithms are computationally efficient in the PAC sense; $i.e.$, wefind good models of cross-dependence with high probability, with polynomialrun-times and sample complexities. The theoretical results are applied toweekly search-frequency data from Google Trends API for a chosen set ofsocially "charged" keywords. The causality network inferred from this datasetreveals, quite expectedly, the causal importance of certain keywords. It isalso illustrated that correlation analysis fails to gather such insight.
arxiv-6600-156 | Active Learning and Best-Response Dynamics | http://arxiv.org/pdf/1406.6633v1.pdf | author:Maria-Florina Balcan, Chris Berlind, Avrim Blum, Emma Cohen, Kaushik Patnaik, Le Song category:cs.LG cs.GT published:2014-06-25 summary:We examine an important setting for engineered systems in which low-powerdistributed sensors are each making highly noisy measurements of some unknowntarget function. A center wants to accurately learn this function by querying asmall number of sensors, which ordinarily would be impossible due to the highnoise rate. The question we address is whether local communication amongsensors, together with natural best-response dynamics in anappropriately-defined game, can denoise the system without destroying the truesignal and allow the center to succeed from only a small number of activequeries. By using techniques from game theory and empirical processes, we provepositive (and negative) results on the denoising power of several naturaldynamics. We then show experimentally that when combined with recent agnosticactive learning algorithms, this process can achieve low error from very fewqueries, performing substantially better than active or passive learningwithout these denoising dynamics as well as passive learning with denoising.
arxiv-6600-157 | When is it Better to Compare than to Score? | http://arxiv.org/pdf/1406.6618v1.pdf | author:Nihar B. Shah, Sivaraman Balakrishnan, Joseph Bradley, Abhay Parekh, Kannan Ramchandran, Martin Wainwright category:stat.ML cs.LG published:2014-06-25 summary:When eliciting judgements from humans for an unknown quantity, one often hasthe choice of making direct-scoring (cardinal) or comparative (ordinal)measurements. In this paper we study the relative merits of either choice,providing empirical and theoretical guidelines for the selection of ameasurement scheme. We provide empirical evidence based on experiments onAmazon Mechanical Turk that in a variety of tasks, (pairwise-comparative)ordinal measurements have lower per sample noise and are typically faster toelicit than cardinal ones. Ordinal measurements however typically provide lessinformation. We then consider the popular Thurstone and Bradley-Terry-Luce(BTL) models for ordinal measurements and characterize the minimax error ratesfor estimating the unknown quantity. We compare these minimax error rates tothose under cardinal measurement models and quantify for what noise levelsordinal measurements are better. Finally, we revisit the data collected fromour experiments and show that fitting these models confirms this prediction:for tasks where the noise in ordinal measurements is sufficiently low, theordinal approach results in smaller errors in the estimation.
arxiv-6600-158 | Optical Flow on Evolving Surfaces with Space and Time Regularisation | http://arxiv.org/pdf/1310.0322v2.pdf | author:Clemens Kirisits, Lukas F. Lang, Otmar Scherzer category:math.OC cs.CV published:2013-10-01 summary:We extend the concept of optical flow with spatiotemporal regularisation to adynamic non-Euclidean setting. Optical flow is traditionally computed from asequence of flat images. The purpose of this paper is to introduce variationalmotion estimation for images that are defined on an evolving surface.Volumetric microscopy images depicting a live zebrafish embryo serve as bothbiological motivation and test data.
arxiv-6600-159 | 3DUNDERWORLD-SLS: An Open-Source Structured-Light Scanning System for Rapid Geometry Acquisition | http://arxiv.org/pdf/1406.6595v1.pdf | author:Kyriakos Herakleous, Charalambos Poullis category:cs.CV published:2014-06-25 summary:Recently, there has been an increase in the demand of virtual 3D objectsrepresenting real-life objects. A plethora of methods and systems have alreadybeen proposed for the acquisition of the geometry of real-life objects rangingfrom those which employ active sensor technology, passive sensor technology ora combination of various techniques. In this paper we present the development of a 3D scanning system which isbased on the principle of structured-light, without having particularrequirements for specialized equipment. We discuss the intrinsic details andinherent difficulties of structured-light scanning techniques and present oursolutions. Finally, we introduce our open-source scanning system"3DUNDERWORLD-SLS" which implements the proposed techniques. We have performedextensive testing with a wide range of models and report the results.Furthermore, we present a comprehensive evaluation of the system and acomparison with a high-end commercial 3D scanner.
arxiv-6600-160 | Support vector machine classification of dimensionally reduced structural MRI images for dementia | http://arxiv.org/pdf/1406.6568v1.pdf | author:V. A. Miller, S. Erlien, J. Piersol category:cs.CV cs.LG physics.med-ph published:2014-06-25 summary:We classify very-mild to moderate dementia in patients (CDR ranging from 0 to2) using a support vector machine classifier acting on dimensionally reducedfeature set derived from MRI brain scans of the 416 subjects available in theOASIS-Brains dataset. We use image segmentation and principal componentanalysis to reduce the dimensionality of the data. Our resulting feature setcontains 11 features for each subject. Performance of the classifiers isevaluated using 10-fold cross-validation. Using linear and (gaussian) kernels,we obtain a training classification accuracy of 86.4% (90.1%), test accuracy of85.0% (85.7%), test precision of 68.7% (68.5%), test recall of 68.0% (74.0%),and test Matthews correlation coefficient of 0.594 (0.616).
arxiv-6600-161 | A hybrid formalism to parse Sign Languages | http://arxiv.org/pdf/1403.4467v2.pdf | author:Rémi Dubot, Christophe Collet category:cs.CL published:2014-03-18 summary:Sign Language (SL) linguistic is dependent on the expensive task ofannotating. Some automation is already available for low-level information (eg.body part tracking) and the lexical level has shown significant progresses. Thesyntactic level lacks annotated corpora as well as complete and consistentmodels. This article presents a solution for the automatic annotation of SLsyntactic elements. It exposes a formalism able to represent bothconstituency-based and dependency-based models. The first enable therepresentation the structures one may want to annotate, the second aims atfulfilling the holes of the first. A parser is presented and used to conducttwo experiments on the solution. One experiment is on a real corpus, the otheris on a synthetic corpus.
arxiv-6600-162 | Multi Circle Detection on Images Using Artificial Bee Colony (ABC) Optimization | http://arxiv.org/pdf/1406.6560v1.pdf | author:Erik Cuevas, Felipe Sencion-Echauri, Daniel Zaldivar, Marco Perez Cisneros category:cs.CV cs.NE published:2014-06-25 summary:Hough transform (HT) has been the most common method for circle detection,exhibiting robustness, but adversely demanding considerable computationaleffort and large memory requirements. Alternative approaches include heuristicmethods that employ iterative optimization procedures for detecting multiplecircles. Since only one circle can be marked at each optimization cycle,multiple executions must be enforced in order to achieve multi detection. Thispaper presents an algorithm for automatic detection of multiple circular shapesthat considers the overall process as a multi-modal optimization problem. Theapproach is based on the artificial bee colony (ABC) algorithm, a swarmoptimization algorithm inspired by the intelligent foraging behavior of honeybees. Unlike the original ABC algorithm, the proposed approach presents theaddition of a memory for discarded solutions. Such memory allows holdingimportant information regarding other local optima which might have emergedduring the optimization process. The detector uses a combination of threenon-collinear edge points as parameters to determine circle candidates. Amatching function (nectar- amount) determines if such circle candidates(bee-food-sources) are actually present in the image. Guided by the values ofsuch matching functions, the set of encoded candidate circles are evolvedthrough the ABC algorithm so that the best candidate (global optimum) can befitted into an actual circle within the edge only image. Then, an analysis ofthe incorporated memory is executed in order to identify potential localoptima, i.e., other circles.
arxiv-6600-163 | A Bimodal Co-Sparse Analysis Model for Image Processing | http://arxiv.org/pdf/1406.6538v1.pdf | author:Martin Kiechle, Tim Habigt, Simon Hawe, Martin Kleinsteuber category:cs.CV published:2014-06-25 summary:The success of many computer vision tasks lies in the ability to exploit theinterdependency between different image modalities such as intensity and depth.Fusing corresponding information can be achieved on several levels, and onepromising approach is the integration at a low level. Moreover, sparse signalmodels have successfully been used in many vision applications. Within thisarea of research, the so called co-sparse analysis model has attractedconsiderably less attention than its well-known counterpart, the sparsesynthesis model, although it has been proven to be very useful in various imageprocessing applications. In this paper, we propose a co-sparse analysis modelthat is able to capture the interdependency of two image modalities. It isbased on the assumption that a pair of analysis operators exists, so that theco-supports of the corresponding bimodal image structures are correlated. Wepropose an algorithm that is able to learn such a coupled pair of operatorsfrom registered and noise-free training data. Furthermore, we explain how thismodel can be applied to solve linear inverse problems in image processing andhow it can be used for image registration tasks. This paper extends the work ofsome of the authors by two major contributions. Firstly, a modification of thelearning process is proposed that a priori guarantees unit norm and zero-meanof the rows of the operator. This accounts for the intuition that contrast inimage modalities carries the most information. Secondly, the model is used in anovel bimodal image registration algorithm which estimates the transformationparameters of unregistered images of different modalities.
arxiv-6600-164 | Weakly-supervised Discovery of Visual Pattern Configurations | http://arxiv.org/pdf/1406.6507v1.pdf | author:Hyun Oh Song, Yong Jae Lee, Stefanie Jegelka, Trevor Darrell category:cs.CV cs.LG published:2014-06-25 summary:The increasing prominence of weakly labeled data nurtures a growing demandfor object detection methods that can cope with minimal supervision. We proposean approach that automatically identifies discriminative configurations ofvisual patterns that are characteristic of a given object class. We formulatethe problem as a constrained submodular optimization problem and demonstratethe benefits of the discovered configurations in remedying mislocalizations andfinding informative positive and negative training examples. Together, theselead to state-of-the-art weakly-supervised detection results on the challengingPASCAL VOC dataset.
arxiv-6600-165 | A Quantitative Neural Coding Model of Sensory Memory | http://arxiv.org/pdf/1406.6453v1.pdf | author:Peilei Liu, Ting Wang category:cs.NE q-bio.NC published:2014-06-25 summary:The coding mechanism of sensory memory on the neuron scale is one of the mostimportant questions in neuroscience. We have put forward a quantitative neuralnetwork model, which is self organized, self similar, and self adaptive, justlike an ecosystem following Darwin theory. According to this model, neuralcoding is a mult to one mapping from objects to neurons. And the whole cerebrumis a real-time statistical Turing Machine, with powerful representing andlearning ability. This model can reconcile some important disputations, suchas: temporal coding versus rate based coding, grandmother cell versuspopulation coding, and decay theory versus interference theory. And it has alsoprovided explanations for some key questions such as memory consolidation,episodic memory, consciousness, and sentiment. Philosophical significance isindicated at last.
arxiv-6600-166 | Incremental Clustering: The Case for Extra Clusters | http://arxiv.org/pdf/1406.6398v1.pdf | author:Margareta Ackerman, Sanjoy Dasgupta category:cs.LG published:2014-06-24 summary:The explosion in the amount of data available for analysis often necessitatesa transition from batch to incremental clustering methods, which process oneelement at a time and typically store only a small subset of the data. In thispaper, we initiate the formal analysis of incremental clustering methodsfocusing on the types of cluster structure that they are able to detect. Wefind that the incremental setting is strictly weaker than the batch model,proving that a fundamental class of cluster structures that can readily bedetected in the batch setting is impossible to identify using any incrementalmethod. Furthermore, we show how the limitations of incremental clustering canbe overcome by allowing additional clusters.
arxiv-6600-167 | Image patch analysis and clustering of sunspots: a dimensionality reduction approach | http://arxiv.org/pdf/1406.6390v1.pdf | author:Kevin R. Moon, Jimmy J. Li, Veronique Delouille, Fraser Watson, Alfred O. Hero III category:cs.CV astro-ph.SR published:2014-06-24 summary:Sunspots, as seen in white light or continuum images, are associated withregions of high magnetic activity on the Sun, visible on magnetogram images.Their complexity is correlated with explosive solar activity and so classifyingthese active regions is useful for predicting future solar activity. Currentclassification of sunspot groups is visually based and suffers from bias.Supervised learning methods can reduce human bias but fail to optimallycapitalize on the information present in sunspot images. This paper uses twoimage modalities (continuum and magnetogram) to characterize the spatial andmodal interactions of sunspot and magnetic active region images and presents anew approach to cluster the images. Specifically, in the framework of imagepatch analysis, we estimate the number of intrinsic parameters required todescribe the spatial and modal dependencies, the correlation between the twomodalities and the corresponding spatial patterns, and examine the phenomena atdifferent scales within the images. To do this, we use linear and nonlinearintrinsic dimension estimators, canonical correlation analysis, andmultiresolution analysis of intrinsic dimension.
arxiv-6600-168 | A multilevel thresholding algorithm using Electromagnetism Optimization | http://arxiv.org/pdf/1406.6336v1.pdf | author:Diego Oliva, Erik Cuevas, Gonzalo Pajares, Daniel Zaldivar, Valentin Osuna category:cs.CV published:2014-06-24 summary:Segmentation is one of the most important tasks in image processing. Itconsist in classify the pixels into two or more groups depending on theirintensity levels and a threshold value. The quality of the segmentation dependson the method applied to select the threshold. The use of the classicalimplementations for multilevel thresholding is computationally expensive sincethey exhaustively search the best values to optimize the objective function.Under such conditions, the use of optimization evolutionary approaches has beenextended. The Electromagnetism Like algorithm (EMO) is an evolutionary methodwhich mimics the attraction repulsion mechanism among charges to evolve themembers of a population. Different to other algorithms, EMO exhibitsinteresting search capabilities whereas maintains a low computational overhead.In this paper, a multilevel thresholding (MT) algorithm based on the EMO isintroduced. The approach combines the good search capabilities of EMO algorithmwith objective functions proposed by the popular MT methods of Otsu and Kapur.The algorithm takes random samples from a feasible search space inside theimage histogram. Such samples build each particle in the EMO context whereasits quality is evaluated considering the objective that is function employed bythe Otsu or Kapur method. Guided by these objective values the set of candidatesolutions are evolved through the EMO operators until an optimal solution isfound. The approach generates a multilevel segmentation algorithm which caneffectively identify the threshold values of a digital image in a reducednumber of iterations. Experimental results show performance evidence of theimplementation of EMO for digital image segmentation.
arxiv-6600-169 | Dense Correspondences Across Scenes and Scales | http://arxiv.org/pdf/1406.6323v1.pdf | author:Moria Tau, Tal Hassner category:cs.CV published:2014-06-24 summary:We seek a practical method for establishing dense correspondences between twoimages with similar content, but possibly different 3D scenes. One of thechallenges in designing such a system is the local scale differences of objectsappearing in the two images. Previous methods often considered only smallsubsets of image pixels; matching only pixels for which stable scales may bereliably estimated. More recently, others have considered densecorrespondences, but with substantial costs associated with generating, storingand matching scale invariant descriptors. Our work here is motivated by theobservation that pixels in the image have contexts -- the pixels around them --which may be exploited in order to estimate local scales reliably andrepeatably. Specifically, we make the following contributions. (i) We show thatscales estimated in sparse interest points may be propagated to neighboringpixels where this information cannot be reliably determined. Doing so allowsscale invariant descriptors to be extracted anywhere in the image, not just indetected interest points. (ii) We present three different means for propagatingthis information: using only the scales at detected interest points, using theunderlying image information to guide the propagation of this informationacross each image, separately, and using both images simultaneously. Finally,(iii), we provide extensive results, both qualitative and quantitative,demonstrating that accurate dense correspondences can be obtained even betweenvery different images, with little computational costs beyond those required byexisting methods.
arxiv-6600-170 | Studying Collective Human Decision Making and Creativity with Evolutionary Computation | http://arxiv.org/pdf/1406.6291v1.pdf | author:Hiroki Sayama, Shelley D. Dionne category:cs.NE cs.MA nlin.AO published:2014-06-24 summary:We report a summary of our interdisciplinary research project "EvolutionaryPerspective on Collective Decision Making" that was conducted through closecollaboration between computational, organizational and social scientists atBinghamton University. We redefined collective human decision making andcreativity as evolution of ecologies of ideas, where populations of ideasevolve via continual applications of evolutionary operators such asreproduction, recombination, mutation, selection, and migration of ideas, eachconducted by participating humans. Based on this evolutionary perspective, wegenerated hypotheses about collective human decision making using agent-basedcomputer simulations. The hypotheses were then tested through severalexperiments with real human subjects. Throughout this project, we utilizedevolutionary computation (EC) in non-traditional ways---(1) as a theoreticalframework for reinterpreting the dynamics of idea generation and selection, (2)as a computational simulation model of collective human decision makingprocesses, and (3) as a research tool for collecting high-resolutionexperimental data of actual collaborative design and decision making from humansubjects. We believe our work demonstrates untapped potential of EC forinterdisciplinary research involving human and social dynamics.
arxiv-6600-171 | Image Completion for View Synthesis Using Markov Random Fields and Efficient Belief Propagation | http://arxiv.org/pdf/1406.6273v1.pdf | author:Julian Habigt, Klaus Diepold category:cs.CV published:2014-06-24 summary:View synthesis is a process for generating novel views from a scene which hasbeen recorded with a 3-D camera setup. It has important applications in 3-Dpost-production and 2-D to 3-D conversion. However, a central problem in thegeneration of novel views lies in the handling of disocclusions. Backgroundcontent, which was occluded in the original view, may become unveiled in thesynthesized view. This leads to missing information in the generated view whichhas to be filled in a visually plausible manner. We present an inpaintingalgorithm for disocclusion filling in synthesized views based on Markov randomfields and efficient belief propagation. We compare the result to twostate-of-the-art algorithms and demonstrate a significant improvement in imagequality.
arxiv-6600-172 | On Soft Power Diagrams | http://arxiv.org/pdf/1307.3949v2.pdf | author:Steffen Borgwardt category:cs.LG math.OC stat.ML published:2013-07-15 summary:Many applications in data analysis begin with a set of points in a Euclideanspace that is partitioned into clusters. Common tasks then are to devise aclassifier deciding which of the clusters a new point is associated to, findingoutliers with respect to the clusters, or identifying the type of clusteringused for the partition. One of the common kinds of clusterings are (balanced) least-squaresassignments with respect to a given set of sites. For these, there is a'separating power diagram' for which each cluster lies in its own cell. In the present paper, we aim for efficient algorithms for outlier detectionand the computation of thresholds that measure how similar a clustering is to aleast-squares assignment for fixed sites. For this purpose, we devise a newmodel for the computation of a 'soft power diagram', which allows a softseparation of the clusters with 'point counting properties'; e.g. we are ableto prescribe how many points we want to classify as outliers. As our results hold for a more general non-convex model of free sites, wedescribe it and our proofs in this more general way. Its locally optimalsolutions satisfy the aforementioned point counting properties. For our targetapplications that use fixed sites, our algorithms are efficiently solvable toglobal optimality by linear programming.
arxiv-6600-173 | Recurrent Models of Visual Attention | http://arxiv.org/pdf/1406.6247v1.pdf | author:Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu category:cs.LG cs.CV stat.ML published:2014-06-24 summary:Applying convolutional neural networks to large images is computationallyexpensive because the amount of computation scales linearly with the number ofimage pixels. We present a novel recurrent neural network model that is capableof extracting information from an image or video by adaptively selecting asequence of regions or locations and only processing the selected regions athigh resolution. Like convolutional neural networks, the proposed model has adegree of translation invariance built-in, but the amount of computation itperforms can be controlled independently of the input image size. While themodel is non-differentiable, it can be trained using reinforcement learningmethods to learn task-specific policies. We evaluate our model on several imageclassification tasks, where it significantly outperforms a convolutional neuralnetwork baseline on cluttered images, and on a dynamic visual control problem,where it learns to track a simple object without an explicit training signalfor doing so.
arxiv-6600-174 | Saccadic Eye Movements and the Generalized Pareto Distribution | http://arxiv.org/pdf/1406.6201v1.pdf | author:Reiner Lenz category:cs.CV I.5.4 published:2014-06-24 summary:We describe a statistical analysis of the eye tracker measurements in adatabase with 15 observers viewing 1003 images under free-viewing conditions.In contrast to the common approach of investigating the properties of thefixation points we analyze the properties of the transition phases betweenfixations. We introduce hyperbolic geometry as a tool to measure the steplength between consecutive eye positions. We show that the step lengths,measured in hyperbolic and euclidean geometry, follow a generalized Paretodistribution. The results based on the hyperbolic distance are more robust thanthose based on euclidean geometry. We show how the structure of the space ofgeneralized Pareto distributions can be used to characterize and identifyindividual observers.
arxiv-6600-175 | Combining predictions from linear models when training and test inputs differ | http://arxiv.org/pdf/1406.6200v1.pdf | author:Thijs van Ommen category:stat.ME cs.LG stat.ML published:2014-06-24 summary:Methods for combining predictions from different models in a supervisedlearning setting must somehow estimate/predict the quality of a model'spredictions at unknown future inputs. Many of these methods (often implicitly)make the assumption that the test inputs are identical to the training inputs,which is seldom reasonable. By failing to take into account that predictionwill generally be harder for test inputs that did not occur in the trainingset, this leads to the selection of too complex models. Based on a novel,unbiased expression for KL divergence, we propose XAIC and its special caseFAIC as versions of AIC intended for prediction that use different degrees ofknowledge of the test inputs. Both methods substantially differ from and mayoutperform all the known versions of AIC even when the training and test inputsare iid, and are especially useful for deterministic inputs and under covariateshift. Our experiments on linear models suggest that if the test and traininginputs differ substantially, then XAIC and FAIC predictively outperform AIC,BIC and several other methods including Bayesian model averaging.
arxiv-6600-176 | Composite Likelihood Estimation for Restricted Boltzmann machines | http://arxiv.org/pdf/1406.6176v1.pdf | author:Muneki Yasuda, Shun Kataoka, Yuji Waizumi, Kazuyuki Tanaka category:cs.LG published:2014-06-24 summary:Learning the parameters of graphical models using the maximum likelihoodestimation is generally hard which requires an approximation. Maximum compositelikelihood estimations are statistical approximations of the maximum likelihoodestimation which are higher-order generalizations of the maximumpseudo-likelihood estimation. In this paper, we propose a composite likelihoodmethod and investigate its property. Furthermore, we apply our compositelikelihood method to restricted Boltzmann machines.
arxiv-6600-177 | A Concise Information-Theoretic Derivation of the Baum-Welch algorithm | http://arxiv.org/pdf/1406.7002v1.pdf | author:Alireza Nejati, Charles Unsworth category:cs.IT cs.LG math.IT published:2014-06-24 summary:We derive the Baum-Welch algorithm for hidden Markov models (HMMs) through aninformation-theoretical approach using cross-entropy instead of the Lagrangemultiplier approach which is universal in machine learning literature. Theproposed approach provides a more concise derivation of the Baum-Welch methodand naturally generalizes to multiple observations.
arxiv-6600-178 | Incorporating Near-Infrared Information into Semantic Image Segmentation | http://arxiv.org/pdf/1406.6147v1.pdf | author:Neda Salamati, Diane Larlus, Gabriela Csurka, Sabine Süsstrunk category:cs.CV published:2014-06-24 summary:Recent progress in computational photography has shown that we can acquirenear-infrared (NIR) information in addition to the normal visible (RGB) band,with only slight modifications to standard digital cameras. Due to theproximity of the NIR band to visible radiation, NIR images share manyproperties with visible images. However, as a result of the material dependentreflection in the NIR part of the spectrum, such images reveal differentcharacteristics of the scene. We investigate how to effectively exploit thesedifferences to improve performance on the semantic image segmentation task.Based on a state-of-the-art segmentation framework and a novel manuallysegmented image database (both indoor and outdoor scenes) that contain4-channel images (RGB+NIR), we study how to best incorporate the specificcharacteristics of the NIR response. We show that adding NIR leads to improvedperformance for classes that correspond to a specific type of material in bothoutdoor and indoor scenes. We also discuss the results with respect to thephysical properties of the NIR response.
arxiv-6600-179 | Fast algorithm for robust subspace recovery | http://arxiv.org/pdf/1406.6145v1.pdf | author:Gilad Lerman, Tyler Maunu category:cs.LG cs.CV stat.AP stat.ML published:2014-06-24 summary:This paper presents a fast algorithm for robust subspace recovery. Thedatasets considered include points drawn around a low-dimensional subspace of ahigher dimensional ambient space, and a possibly large portion of points thatdo not lie nearby this subspace. The proposed algorithm, which we refer to asFast Median Subspace (FMS), is designed to robustly determine the underlyingsubspace of such datasets, while having lower computational complexity thanexisting methods. Numerical experiments on synthetic and real data demonstrateits competitive speed and accuracy.
arxiv-6600-180 | Synthesizing Manipulation Sequences for Under-Specified Tasks using Unrolled Markov Random Fields | http://arxiv.org/pdf/1306.5707v2.pdf | author:Jaeyong Sung, Bart Selman, Ashutosh Saxena category:cs.RO cs.AI cs.LG published:2013-06-24 summary:Many tasks in human environments require performing a sequence of navigationand manipulation steps involving objects. In unstructured human environments,the location and configuration of the objects involved often change inunpredictable ways. This requires a high-level planning strategy that is robustand flexible in an uncertain environment. We propose a novel dynamic planningstrategy, which can be trained from a set of example sequences. High leveltasks are expressed as a sequence of primitive actions or controllers (withappropriate parameters). Our score function, based on Markov Random Field(MRF), captures the relations between environment, controllers, and theirarguments. By expressing the environment using sets of attributes, the approachgeneralizes well to unseen scenarios. We train the parameters of our MRF usinga maximum margin learning method. We provide a detailed empirical validation ofour overall framework demonstrating successful plan strategies for a variety oftasks.
arxiv-6600-181 | Generalized Mixability via Entropic Duality | http://arxiv.org/pdf/1406.6130v1.pdf | author:Mark D. Reid, Rafael M. Frongillo, Robert C. Williamson, Nishant Mehta category:cs.LG published:2014-06-24 summary:Mixability is a property of a loss which characterizes when fast convergenceis possible in the game of prediction with expert advice. We show that a keyproperty of mixability generalizes, and the exp and log operations present inthe usual theory are not as special as one might have thought. In doing this weintroduce a more general notion of $\Phi$-mixability where $\Phi$ is a generalentropy (\ie, any convex function on probabilities). We show how a propertyshared by the convex dual of any such entropy yields a natural algorithm (theminimizer of a regret bound) which, analogous to the classical aggregatingalgorithm, is guaranteed a constant regret when used with $\Phi$-mixablelosses. We characterize precisely which $\Phi$ have $\Phi$-mixable losses andput forward a number of conjectures about the optimality and relationshipsbetween different choices of entropy.
arxiv-6600-182 | Mining Recurrent Concepts in Data Streams using the Discrete Fourier Transform | http://arxiv.org/pdf/1406.6114v1.pdf | author:Sakthithasan Sripirakas, Russel Pears category:cs.LG published:2014-06-24 summary:In this research we address the problem of capturing recurring concepts in adata stream environment. Recurrence capture enables the re-use of previouslylearned classifiers without the need for re-learning while providing for betteraccuracy during the concept recurrence interval. We capture concepts byapplying the Discrete Fourier Transform (DFT) to Decision Tree classifiers toobtain highly compressed versions of the trees at concept drift points in thestream and store such trees in a repository for future use. Our empiricalresults on real world and synthetic data exhibiting varying degrees ofrecurrence show that the Fourier compressed trees are more robust to noise andare able to capture recurring concepts with higher precision than a metalearning approach that chooses to re-use classifiers in their originallyoccurring form.
arxiv-6600-183 | Improved Frame Level Features and SVM Supervectors Approach for the Recogniton of Emotional States from Speech: Application to categorical and dimensional states | http://arxiv.org/pdf/1406.6101v1.pdf | author:Imen Trabelsi, Dorra Ben Ayed, Noureddine Ellouze category:cs.CL cs.LG published:2014-06-23 summary:The purpose of speech emotion recognition system is to classify speakersutterances into different emotional states such as disgust, boredom, sadness,neutral and happiness. Speech features that are commonly used in speech emotionrecognition rely on global utterance level prosodic features. In our work, weevaluate the impact of frame level feature extraction. The speech samples arefrom Berlin emotional database and the features extracted from these utterancesare energy, different variant of mel frequency cepstrum coefficients, velocityand acceleration features.
arxiv-6600-184 | From Black-Scholes to Online Learning: Dynamic Hedging under Adversarial Environments | http://arxiv.org/pdf/1406.6084v1.pdf | author:Henry Lam, Zhenming Liu category:cs.DS cs.LG q-fin.PR F.2; I.2.6 published:2014-06-23 summary:We consider a non-stochastic online learning approach to price financialoptions by modeling the market dynamic as a repeated game between the nature(adversary) and the investor. We demonstrate that such framework yieldsanalogous structure as the Black-Scholes model, the widely popular optionpricing model in stochastic finance, for both European and American optionswith convex payoffs. In the case of non-convex options, we constructapproximate pricing algorithms, and demonstrate that their efficiency can beanalyzed through the introduction of an artificial probability measure, inparallel to the so-called risk-neutral measure in the finance literature, eventhough our framework is completely adversarial. Continuous-time convergenceresults and extensions to incorporate price jumps are also presented.
arxiv-6600-185 | A Novel M-Estimator for Robust PCA | http://arxiv.org/pdf/1112.4863v4.pdf | author:Teng Zhang, Gilad Lerman category:stat.ML math.OC published:2011-12-20 summary:We study the basic problem of robust subspace recovery. That is, we assume adata set that some of its points are sampled around a fixed subspace and therest of them are spread in the whole ambient space, and we aim to recover thefixed underlying subspace. We first estimate "robust inverse sample covariance"by solving a convex minimization procedure; we then recover the subspace by thebottom eigenvectors of this matrix (their number correspond to the number ofeigenvalues close to 0). We guarantee exact subspace recovery under someconditions on the underlying data. Furthermore, we propose a fast iterativealgorithm, which linearly converges to the matrix minimizing the convexproblem. We also quantify the effect of noise and regularization and discussmany other practical and theoretical issues for improving the subspace recoveryin various settings. When replacing the sum of terms in the convex energyfunction (that we minimize) with the sum of squares of terms, we obtain thatthe new minimizer is a scaled version of the inverse sample covariance (whenexists). We thus interpret our minimizer and its subspace (spanned by itsbottom eigenvectors) as robust versions of the empirical inverse covariance andthe PCA subspace respectively. We compare our method with many other algorithmsfor robust PCA on synthetic and real data sets and demonstrate state-of-the-artspeed and accuracy.
arxiv-6600-186 | Stationary Mixing Bandits | http://arxiv.org/pdf/1406.6020v1.pdf | author:Julien Audiffren, Liva Ralaivola category:cs.LG published:2014-06-23 summary:We study the bandit problem where arms are associated with stationaryphi-mixing processes and where rewards are therefore dependent: the questionthat arises from this setting is that of recovering some independence byignoring the value of some rewards. As we shall see, the bandit problem wetackle requires us to address the exploration/exploitation/independencetrade-off. To do so, we provide a UCB strategy together with a general regretanalysis for the case where the size of the independence blocks (the ignoredrewards) is fixed and we go a step beyond by providing an algorithm that isable to compute the size of the independence blocks from the data. Finally, wegive an analysis of our bandit problem in the restless case, i.e., in thesituation where the time counters for all mixing processes simultaneouslyevolve.
arxiv-6600-187 | Reinforcement and Imitation Learning via Interactive No-Regret Learning | http://arxiv.org/pdf/1406.5979v1.pdf | author:Stephane Ross, J. Andrew Bagnell category:cs.LG stat.ML published:2014-06-23 summary:Recent work has demonstrated that problems-- particularly imitation learningand structured prediction-- where a learner's predictions influence theinput-distribution it is tested on can be naturally addressed by an interactiveapproach and analyzed using no-regret online learning. These approaches toimitation learning, however, neither require nor benefit from information aboutthe cost of actions. We extend existing results in two directions: first, wedevelop an interactive imitation learning approach that leverages costinformation; second, we extend the technique to address reinforcement learning.The results provide theoretical support to the commonly observed successes ofonline approximate policy iteration. Our approach suggests a broad new familyof algorithms and provides a unifying view of existing techniques for imitationand reinforcement learning.
arxiv-6600-188 | Committees of deep feedforward networks trained with few data | http://arxiv.org/pdf/1406.5947v1.pdf | author:Bogdan Miclut, Thomas Kaester, Thomas Martinetz, Erhardt Barth category:cs.CV cs.NE published:2014-06-23 summary:Deep convolutional neural networks are known to give good results on imageclassification tasks. In this paper we present a method to improve theclassification result by combining multiple such networks in a committee. Weadopt the STL-10 dataset which has very few training examples and show that ourmethod can achieve results that are better than the state of the art. Thenetworks are trained layer-wise and no backpropagation is used. We also explorethe effects of dataset augmentation by mirroring, rotation, and scaling.
arxiv-6600-189 | Multi-utility Learning: Structured-output Learning with Multiple Annotation-specific Loss Functions | http://arxiv.org/pdf/1406.5910v1.pdf | author:Roman Shapovalov, Dmitry Vetrov, Anton Osokin, Pushmeet Kohli category:cs.CV cs.LG published:2014-06-23 summary:Structured-output learning is a challenging problem; particularly so becauseof the difficulty in obtaining large datasets of fully labelled instances fortraining. In this paper we try to overcome this difficulty by presenting amulti-utility learning framework for structured prediction that can learn fromtraining instances with different forms of supervision. We propose a unifiedtechnique for inferring the loss functions most suitable for quantifying theconsistency of solutions with the given weak annotation. We demonstrate theeffectiveness of our framework on the challenging semantic image segmentationproblem for which a wide variety of annotations can be used. For instance, thepopular training datasets for semantic segmentation are composed of images withhard-to-generate full pixel labellings, as well as images with easy-to-obtainweak annotations, such as bounding boxes around objects, or image-level labelsthat specify which object categories are present in an image. Experimentalevaluation shows that the use of annotation-specific loss functionsdramatically improves segmentation accuracy compared to the baseline systemwhere only one type of weak annotation is used.
arxiv-6600-190 | VideoSET: Video Summary Evaluation through Text | http://arxiv.org/pdf/1406.5824v1.pdf | author:Serena Yeung, Alireza Fathi, Li Fei-Fei category:cs.CV cs.CL cs.IR published:2014-06-23 summary:In this paper we present VideoSET, a method for Video Summary Evaluationthrough Text that can evaluate how well a video summary is able to retain thesemantic information contained in its original video. We observe that semanticsis most easily expressed in words, and develop a text-based approach for theevaluation. Given a video summary, a text representation of the video summaryis first generated, and an NLP-based metric is then used to measure itssemantic distance to ground-truth text summaries written by humans. We showthat our technique has higher agreement with human judgment than pixel-baseddistance metrics. We also release text annotations and ground-truth textsummaries for a number of publicly available video datasets, for use by thecomputer vision community.
arxiv-6600-191 | A Unified Quantitative Model of Vision and Audition | http://arxiv.org/pdf/1406.5807v1.pdf | author:Peilei Liu, Ting Wang category:cs.CV q-bio.NC q-bio.QM I.5.4; I.5.2 published:2014-06-23 summary:We have put forwards a unified quantitative framework of vision and audition,based on existing data and theories. According to this model, the retina is afeedforward network self-adaptive to inputs in a specific period. After fullygrown, cells become specialized detectors based on statistics of stimulushistory. This model has provided explanations for perception mechanisms ofcolour, shape, depth and motion. Moreover, based on this ground we have putforwards a bold conjecture that single ear can detect sound direction. This iscomplementary to existing theories and has provided better explanations forsound localization.
arxiv-6600-192 | Further heuristics for $k$-means: The merge-and-split heuristic and the $(k,l)$-means | http://arxiv.org/pdf/1406.6314v1.pdf | author:Frank Nielsen, Richard Nock category:cs.LG cs.CV cs.IR stat.ML published:2014-06-23 summary:Finding the optimal $k$-means clustering is NP-hard in general and manyheuristics have been designed for minimizing monotonically the $k$-meansobjective. We first show how to extend Lloyd's batched relocation heuristic andHartigan's single-point relocation heuristic to take into account empty-clusterand single-point cluster events, respectively. Those events tend toincreasingly occur when $k$ or $d$ increases, or when performing severalrestarts. First, we show that those special events are a blessing because theyallow to partially re-seed some cluster centers while further minimizing the$k$-means objective function. Second, we describe a novel heuristic,merge-and-split $k$-means, that consists in merging two clusters and splittingthis merged cluster again with two new centers provided it improves the$k$-means objective. This novel heuristic can improve Hartigan's $k$-means whenit has converged to a local minimum. We show empirically that thismerge-and-split $k$-means improves over the Hartigan's heuristic which is the{\em de facto} method of choice. Finally, we propose the $(k,l)$-meansobjective that generalizes the $k$-means objective by associating the datapoints to their $l$ closest cluster centers, and show how to either directlyconvert or iteratively relax the $(k,l)$-means into a $k$-means in order toreach better local minima.
arxiv-6600-193 | Environmental Sensing by Wearable Device for Indoor Activity and Location Estimation | http://arxiv.org/pdf/1406.5765v1.pdf | author:Ming Jin, Han Zou, Kevin Weekly, Ruoxi Jia, Alexandre M. Bayen, Costas J. Spanos category:cs.HC stat.ML published:2014-06-22 summary:We present results from a set of experiments in this pilot study toinvestigate the causal influence of user activity on various environmentalparameters monitored by occupant carried multi-purpose sensors. Hypotheses withrespect to each type of measurements are verified, including temperature,humidity, and light level collected during eight typical activities: sitting inlab / cubicle, indoor walking / running, resting after physical activity,climbing stairs, taking elevators, and outdoor walking. Our main contributionis the development of features for activity and location recognition based onenvironmental measurements, which exploit location- and activity-specificcharacteristics and capture the trends resulted from the underlyingphysiological process. The features are statistically shown to have goodseparability and are also information-rich. Fusing environmental sensingtogether with acceleration is shown to achieve classification accuracy as highas 99.13%. For building applications, this study motivates a sensor fusionparadigm for learning individualized activity, location, and environmentalpreferences for energy management and user comfort.
arxiv-6600-194 | Recovery of Images with Missing Pixels using a Gradient Compressive Sensing Algorithm | http://arxiv.org/pdf/1407.3695v1.pdf | author:Isidora Stanković category:cs.CV published:2014-06-22 summary:This paper investigates the possibility of reconstruction of imagesconsidering that they are sparse in the DCT transformation domain. Twoapproaches are considered. One when the image is pre-processed in the DCTdomain, using 8x8 blocks. The image is made sparse by setting the smallest DCTcoefficients to zero. In the other case the original image is consideredwithout pre-processing, assuming the sparsity as intrinsic property of theanalyzed image. A gradient based algorithm is used to recover a large number ofmissing pixels in the image. The case of a salt-and-paper noise affecting alarge number of pixels is easily reduced to the case of missing pixels andconsidered within the same framework. The reconstruction of images affectedwith salt-and-paper impulsive is compared with the images filtered using amedian filter. The same algorithm can be used considering transformation of thewhole image. Reconstructions of black and white and colour images areconsidered.
arxiv-6600-195 | Divide-and-Conquer Learning by Anchoring a Conical Hull | http://arxiv.org/pdf/1406.5752v1.pdf | author:Tianyi Zhou, Jeff Bilmes, Carlos Guestrin category:stat.ML cs.LG published:2014-06-22 summary:We reduce a broad class of machine learning problems, usually addressed by EMor sampling, to the problem of finding the $k$ extremal rays spanning theconical hull of a data point set. These $k$ "anchors" lead to a global solutionand a more interpretable model that can even outperform EM and sampling ongeneralization error. To find the $k$ anchors, we propose a noveldivide-and-conquer learning scheme "DCA" that distributes the problem to$\mathcal O(k\log k)$ same-type sub-problems on different low-D randomhyperplanes, each can be solved by any solver. For the 2D sub-problem, wepresent a non-iterative solver that only needs to compute an array of cosinevalues and its max/min entries. DCA also provides a faster subroutine for othermethods to check whether a point is covered in a conical hull, which improvesalgorithm design in multiple dimensions and brings significant speedup tolearning. We apply our method to GMM, HMM, LDA, NMF and subspace clustering,then show its competitive performance and scalability over other methods onrich datasets.
arxiv-6600-196 | Convex Optimization Learning of Faithful Euclidean Distance Representations in Nonlinear Dimensionality Reduction | http://arxiv.org/pdf/1406.5736v1.pdf | author:Chao Ding, Hou-Duo Qi category:stat.ML cs.LG math.OC published:2014-06-22 summary:Classical multidimensional scaling only works well when the noisy distancesobserved in a high dimensional space can be faithfully represented by Euclideandistances in a low dimensional space. Advanced models such as Maximum VarianceUnfolding (MVU) and Minimum Volume Embedding (MVE) use Semi-DefiniteProgramming (SDP) to reconstruct such faithful representations. While those SDPmodels are capable of producing high quality configuration numerically, theysuffer two major drawbacks. One is that there exist no theoretically guaranteedbounds on the quality of the configuration. The other is that they are slow incomputation when the data points are beyond moderate size. In this paper, wepropose a convex optimization model of Euclidean distance matrices. Weestablish a non-asymptotic error bound for the random graph model withsub-Gaussian noise, and prove that our model produces a matrix estimator ofhigh accuracy when the order of the uniform sample size is roughly the degreeof freedom of a low-rank matrix up to a logarithmic factor. Our resultspartially explain why MVU and MVE often work well. Moreover, we develop a fastinexact accelerated proximal gradient method. Numerical experiments show thatthe model can produce configurations of high quality on large data points thatthe SDP approach would struggle to cope with.
arxiv-6600-197 | Natural Color Image Enhancement based on Modified Multiscale Retinex Algorithm and Performance Evaluation usingWavelet Energy | http://arxiv.org/pdf/1406.5710v1.pdf | author:M. C Hanumantharaju, M. Ravishankar, D. R Rameshbabu category:cs.CV 68U10 I.4.3 published:2014-06-22 summary:This paper presents a new color image enhancement technique based on modifiedMultiScale Retinex(MSR) algorithm and visual quality of the enhanced images areevaluated using a new metric, namely, wavelet energy. The color imageenhancement is achieved by down sampling the value component of HSV color spaceconverted image into three scales (normal, medium and fine) following thecontrast stretching operation. These down sampled value components are enhancedusing the MSR algorithm. The value component is reconstructed by averaging eachpixels of the lower scale image with that of the upper scale image subsequentto up sampling the lower scale image. This process replaces dark pixel by theaverage pixels of both the lower scale and upper scale, while retaining thebright pixels. The quality of the reconstructed images in the proposed methodis found to be good and far better then the other researchers method. Theperformance of the proposed scheme is evaluated using new wavelet domain basedassessment criterion, referred as wavelet energy. This scheme computes theenergy of both original and enhanced image in wavelet domain. The number ofedge details as well as wavelet energy is less in a poor quality image comparedwith naturally enhanced image. Experimental results presented confirms that theproposed wavelet energy based color image quality assessment techniqueefficiently characterizes both the local and global details of enhanced image.
arxiv-6600-198 | A CNL for Contract-Oriented Diagrams | http://arxiv.org/pdf/1406.5691v1.pdf | author:John J. Camilleri, Gabriele Paganelli, Gerardo Schneider category:cs.CL cs.FL published:2014-06-22 summary:We present a first step towards a framework for defining and manipulatingnormative documents or contracts described as Contract-Oriented (C-O) Diagrams.These diagrams provide a visual representation for such texts, giving thepossibility to express a signatory's obligations, permissions and prohibitions,with or without timing constraints, as well as the penalties resulting from thenon-fulfilment of a contract. This work presents a CNL for verbalising C-ODiagrams, a web-based tool allowing editing in this CNL, and another forvisualising and manipulating the diagrams interactively. We then show how theseproof-of-concept tools can be used by applying them to a small example.
arxiv-6600-199 | Deep Fragment Embeddings for Bidirectional Image Sentence Mapping | http://arxiv.org/pdf/1406.5679v1.pdf | author:Andrej Karpathy, Armand Joulin, Li Fei-Fei category:cs.CV cs.CL cs.LG published:2014-06-22 summary:We introduce a model for bidirectional retrieval of images and sentencesthrough a multi-modal embedding of visual and natural language data. Unlikeprevious models that directly map images or sentences into a common embeddingspace, our model works on a finer level and embeds fragments of images(objects) and fragments of sentences (typed dependency tree relations) into acommon space. In addition to a ranking objective seen in previous work, thisallows us to add a new fragment alignment objective that learns to directlyassociate these fragments across modalities. Extensive experimental evaluationshows that reasoning on both the global level of images and sentences and thefiner level of their respective fragments significantly improves performance onimage-sentence retrieval tasks. Additionally, our model provides interpretablepredictions since the inferred inter-modal fragment alignment is explicit.
arxiv-6600-200 | Constant Factor Approximation for Balanced Cut in the PIE model | http://arxiv.org/pdf/1406.5665v1.pdf | author:Konstantin Makarychev, Yury Makarychev, Aravindan Vijayaraghavan category:cs.DS cs.LG published:2014-06-22 summary:We propose and study a new semi-random semi-adversarial model for BalancedCut, a planted model with permutation-invariant random edges (PIE). Our modelis much more general than planted models considered previously. Consider a setof vertices V partitioned into two clusters $L$ and $R$ of equal size. Let $G$be an arbitrary graph on $V$ with no edges between $L$ and $R$. Let$E_{random}$ be a set of edges sampled from an arbitrary permutation-invariantdistribution (a distribution that is invariant under permutation of vertices in$L$ and in $R$). Then we say that $G + E_{random}$ is a graph withpermutation-invariant random edges. We present an approximation algorithm for the Balanced Cut problem that findsa balanced cut of cost $O(E_{random}) + n \text{polylog}(n)$ in this model.In the regime when $E_{random} = \Omega(n \text{polylog}(n))$, this is aconstant factor approximation with respect to the cost of the planted cut.
arxiv-6600-201 | Interactively Test Driving an Object Detector: Estimating Performance on Unlabeled Data | http://arxiv.org/pdf/1406.5653v1.pdf | author:Rushil Anirudh, Pavan Turaga category:cs.CV published:2014-06-21 summary:In this paper, we study the problem of `test-driving' a detector, i.e.allowing a human user to get a quick sense of how well the detector generalizesto their specific requirement. To this end, we present the first system thatestimates detector performance interactively without extensive ground truthingusing a human in the loop. We approach this as a problem of estimatingproportions and show that it is possible to make accurate inferences on theproportion of classes or groups within a large data collection by observingonly $5-10\%$ of samples from the data. In estimating the false detections (forprecision), the samples are chosen carefully such that the overallcharacteristics of the data collection are preserved. Next, inspired by its usein estimating disease propagation we apply pooled testing approaches toestimate missed detections (for recall) from the dataset. The estimates thusobtained are close to the ones obtained using ground truth, thus reducing theneed for extensive labeling which is expensive and time consuming.
arxiv-6600-202 | Análisis e implementación de algoritmos evolutivos para la optimización de simulaciones en ingeniería civil. (draft) | http://arxiv.org/pdf/1401.5054v3.pdf | author:José Alberto García Gutiérrez, Alejandro Mateo Hernández Díaz category:cs.NE cs.AI published:2014-01-20 summary:This paper studies the applicability of evolutionary algorithms,particularly, the evolution strategies family in order to estimate adegradation parameter in the shear design of reinforced concrete members. Thisproblem represents a great computational task and is highly relevant in theframework of the structural engineering that for the first time is solved usinggenetic algorithms. You are viewing a draft, the authors appreciate corrections, comments andsuggestions to this work.
arxiv-6600-203 | Minimax-optimal Inference from Partial Rankings | http://arxiv.org/pdf/1406.5638v1.pdf | author:Bruce Hajek, Sewoong Oh, Jiaming Xu category:stat.ML math.ST stat.TH published:2014-06-21 summary:This paper studies the problem of inferring a global preference based on thepartial rankings provided by many users over different subsets of itemsaccording to the Plackett-Luce model. A question of particular interest is howto optimally assign items to users for ranking and how many item assignmentsare needed to achieve a target estimation error. For a given assignment ofitems to users, we first derive an oracle lower bound of the estimation errorthat holds even for the more general Thurstone models. Then we show that theCram\'er-Rao lower bound and our upper bounds inversely depend on the spectralgap of the Laplacian of an appropriately defined comparison graph. When thesystem is allowed to choose the item assignment, we propose a random assignmentscheme. Our oracle lower bound and upper bounds imply that it isminimax-optimal up to a logarithmic factor among all assignment schemes and thelower bound can be achieved by the maximum likelihood estimator as well aspopular rank-breaking schemes that decompose partial rankings into pairwisecomparisons. The numerical experiments corroborate our theoretical findings.
arxiv-6600-204 | Thermodynamic-RAM Technology Stack | http://arxiv.org/pdf/1406.5633v1.pdf | author:M. Alexander Nugent, Timothy W. Molter category:cs.NE published:2014-06-21 summary:We introduce a technology stack or specification describing the multiplelevels of abstraction and specialization needed to implement a neuromorphicprocessor based on the theory of AHaH Computing. This specific implementationis called Thermodynamic-RAM (kT-RAM). Bringing us closer to brain-like neuralcomputation, kT-RAM will provide a general-purpose adaptive hardware resourceto existing computing platforms enabling fast and low-power machine learningcapabilities that are currently hampered by the separation of memory andprocessing. The motivation for defining the technology stack is two-fold.First, explaining kT-RAM is much easier if it is broken down into smaller, moremanageable pieces. Secondly, groups interested in realizing kT-RAM can choose alevel to contribute to that matches their interest and expertise. The levels ofthe Thermodynamic-RAM technology stack include the memristor, Knowm-Synapse,AHaH Node, kT-RAM, kT-RAM instruction set, sparse spike encoding, kT-RAMemulator, and SENSE Server.
arxiv-6600-205 | PAC-Bayes Analysis of Multi-view Learning | http://arxiv.org/pdf/1406.5614v1.pdf | author:Shiliang Sun, John Shawe-Taylor category:cs.LG cs.AI stat.ML published:2014-06-21 summary:This paper presents four PAC-Bayes bounds to analyse the generalisationperformance of multi-view classifiers. These bounds adopt data dependentGaussian priors which emphasize the classifiers with high view agreements. Thecentre of the prior for the first two bounds is the origin, while the centre ofthe prior for the last two bounds is given by a data dependent vector. Anotherimportant ingredient to obtain these bounds is two derived logarithmicdeterminant inequalities whose difference lies at whether the dimensionality ofdata is involved. We evaluate the multi-view PAC-Bayes bounds on benchmark datawith preliminary experimental results indicating their usefulness.
arxiv-6600-206 | Venn-Abers predictors | http://arxiv.org/pdf/1211.0025v2.pdf | author:Vladimir Vovk, Ivan Petej category:cs.LG stat.ML 68T05, 68T10 published:2012-10-31 summary:This paper continues study, both theoretical and empirical, of the method ofVenn prediction, concentrating on binary prediction problems. Venn predictorsproduce probability-type predictions for the labels of test objects which areguaranteed to be well calibrated under the standard assumption that theobservations are generated independently from the same distribution. We give asimple formalization and proof of this property. We also introduce Venn-Aberspredictors, a new class of Venn predictors based on the idea of isotonicregression, and report promising empirical results both for Venn-Aberspredictors and for their more computationally efficient simplified version.
arxiv-6600-207 | From conformal to probabilistic prediction | http://arxiv.org/pdf/1406.5600v1.pdf | author:Vladimir Vovk, Ivan Petej, Valentina Fedorova category:cs.LG 68T10 published:2014-06-21 summary:This paper proposes a new method of probabilistic prediction, which is basedon conformal prediction. The method is applied to the standard USPS data setand gives encouraging results.
arxiv-6600-208 | A survey on phrase structure learning methods for text classification | http://arxiv.org/pdf/1406.5598v1.pdf | author:Reshma Prasad, Mary Priya Sebastian category:cs.CL published:2014-06-21 summary:Text classification is a task of automatic classification of text into one ofthe predefined categories. The problem of text classification has been widelystudied in different communities like natural language processing, data miningand information retrieval. Text classification is an important constituent inmany information management tasks like topic identification, spam filtering,email routing, language identification, genre classification, readabilityassessment etc. The performance of text classification improves notably whenphrase patterns are used. The use of phrase patterns helps in capturingnon-local behaviours and thus helps in the improvement of text classificationtask. Phrase structure extraction is the first step to continue with the phrasepattern identification. In this survey, detailed study of phrase structurelearning methods have been carried out. This will enable future work in severalNLP tasks, which uses syntactic information from phrase structure like grammarcheckers, question answering, information extraction, machine translation, textclassification. The paper also provides different levels of classification anddetailed comparison of the phrase structure learning methods.
arxiv-6600-209 | The Sample Complexity of Learning Linear Predictors with the Squared Loss | http://arxiv.org/pdf/1406.5143v2.pdf | author:Ohad Shamir category:cs.LG stat.ML published:2014-06-19 summary:In this short note, we provide tight sample complexity bounds for learninglinear predictors with respect to the squared loss. Our focus is on an agnosticsetting, where no assumptions are made on the data distribution. This contrastswith standard results in the literature, which either make distributionalassumptions, refer to specific parameter settings, or use other performancemeasures.
arxiv-6600-210 | An Open Source Pattern Recognition Toolbox for MATLAB | http://arxiv.org/pdf/1406.5565v1.pdf | author:Kenneth D. Morton Jr., Peter Torrione, Leslie Collins, Sam Keene category:stat.ML cs.CV cs.LG cs.MS published:2014-06-21 summary:Pattern recognition and machine learning are becoming integral parts ofalgorithms in a wide range of applications. Different algorithms and approachesfor machine learning include different tradeoffs between performance andcomputation, so during algorithm development it is often necessary to explore avariety of different approaches to a given task. A toolbox with a unifiedframework across multiple pattern recognition techniques enables algorithmdevelopers the ability to rapidly evaluate different choices prior todeployment. MATLAB is a widely used environment for algorithm development andprototyping, and although several MATLAB toolboxes for pattern recognition arecurrently available these are either incomplete, expensive, or restrictivelylicensed. In this work we describe a MATLAB toolbox for pattern recognition andmachine learning known as the PRT (Pattern Recognition Toolbox), licensed underthe permissive MIT license. The PRT includes many popular techniques for datapreprocessing, supervised learning, clustering, regression and featureselection, as well as a methodology for combining these components using asimple, uniform syntax. The resulting algorithms can be evaluated usingcross-validation and a variety of scoring metrics to ensure robust performancewhen the algorithm is deployed. This paper presents an overview of the PRT aswell as an example of usage on Fisher's Iris dataset.
arxiv-6600-211 | Optimality guarantees for distributed statistical estimation | http://arxiv.org/pdf/1405.0782v2.pdf | author:John C. Duchi, Michael I. Jordan, Martin J. Wainwright, Yuchen Zhang category:cs.IT cs.LG math.IT math.ST stat.TH published:2014-05-05 summary:Large data sets often require performing distributed statistical estimation,with a full data set split across multiple machines and limited communicationbetween machines. To study such scenarios, we define and study some refinementsof the classical minimax risk that apply to distributed settings, comparing tothe performance of estimators with access to the entire data. Lower bounds onthese quantities provide a precise characterization of the minimum amount ofcommunication required to achieve the centralized minimax risk. We study twoclasses of distributed protocols: one in which machines send messagesindependently over channels without feedback, and a second allowing forinteractive communication, in which a central server broadcasts the messagesfrom a given machine to all other machines. We establish lower bounds for avariety of problems, including location estimation in several families andparameter estimation in different types of regression models. Our resultsinclude a novel class of quantitative data-processing inequalities used tocharacterize the effects of limited communication.
arxiv-6600-212 | Caffe: Convolutional Architecture for Fast Feature Embedding | http://arxiv.org/pdf/1408.5093v1.pdf | author:Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, Trevor Darrell category:cs.CV cs.LG cs.NE published:2014-06-20 summary:Caffe provides multimedia scientists and practitioners with a clean andmodifiable framework for state-of-the-art deep learning algorithms and acollection of reference models. The framework is a BSD-licensed C++ librarywith Python and MATLAB bindings for training and deploying general-purposeconvolutional neural networks and other deep models efficiently on commodityarchitectures. Caffe fits industry and internet-scale media needs by CUDA GPUcomputation, processing over 40 million images a day on a single K40 or TitanGPU ($\approx$ 2.5 ms per image). By separating model representation fromactual implementation, Caffe allows experimentation and seamless switchingamong platforms for ease of development and deployment from prototypingmachines to cloud environments. Caffe is maintained and developed by theBerkeley Vision and Learning Center (BVLC) with the help of an active communityof contributors on GitHub. It powers ongoing research projects, large-scaleindustrial applications, and startup prototypes in vision, speech, andmultimedia.
arxiv-6600-213 | Inferring the Why in Images | http://arxiv.org/pdf/1406.5472v1.pdf | author:Hamed Pirsiavash, Carl Vondrick, Antonio Torralba category:cs.CV published:2014-06-20 summary:Humans have the remarkable capability to infer the motivations of otherpeople's actions, likely due to cognitive skills known in psychophysics as thetheory of mind. In this paper, we strive to build a computational model thatpredicts the motivation behind the actions of people from images. To ourknowledge, this challenging problem has not yet been extensively explored incomputer vision. We present a novel learning based framework that useshigh-level visual recognition to infer why people are performing an actions inimages. However, the information in an image alone may not be sufficient toautomatically solve this task. Since humans can rely on their own experiencesto infer motivation, we propose to give computer vision systems access to someof these experiences by using recently developed natural language models tomine knowledge stored in massive amounts of text. While we are still far awayfrom automatically inferring motivation, our results suggest that transferringknowledge from language into vision can help machines understand why a personmight be performing an action in an image.
arxiv-6600-214 | An Inexact Proximal Path-Following Algorithm for Constrained Convex Minimization | http://arxiv.org/pdf/1311.1756v2.pdf | author:Quoc Tran Dinh, Anastasios Kyrillidis, Volkan Cevher category:math.OC stat.ML published:2013-11-07 summary:Many scientific and engineering applications feature nonsmooth convexminimization problems over convex sets. In this paper, we address an importantinstance of this broad class where we assume that the nonsmooth objective isequipped with a tractable proximity operator and that the convex constraint setaffords a self-concordant barrier. We provide a new joint treatment of proximaland self-concordant barrier concepts and illustrate that such problems can beefficiently solved, without the need of lifting the problem dimensions, as indisciplined convex optimization approach. We propose an inexact path-followingalgorithmic framework and theoretically characterize the worst-case analyticalcomplexity of this framework when the proximal subproblems are solvedinexactly. To show the merits of our framework, we apply its instances to bothsynthetic and real-world applications, where it shows advantages over standardinterior point methods. As a by-product, we describe how our framework canobtain points on the Pareto frontier of regularized problems withself-concordant objectives in a tuning free fashion.
arxiv-6600-215 | Classification using log Gaussian Cox processes | http://arxiv.org/pdf/1405.4141v2.pdf | author:Alexander G. de. G Matthews, Zoubin Ghahramani category:stat.ML stat.CO stat.ME published:2014-05-16 summary:McCullagh and Yang (2006) suggest a family of classification algorithms basedon Cox processes. We further investigate the log Gaussian variant which has anumber of appealing properties. Conditioned on the covariates, the distributionover labels is given by a type of conditional Markov random field. In thesupervised case, computation of the predictive probability of a single testpoint scales linearly with the number of training points and the multiclassgeneralization is straightforward. We show new links between the supervisedmethod and classical nonparametric methods. We give a detailed analysis of thepairwise graph representable Markov random field, which we use to extend themodel to semi-supervised learning problems, and propose an inference methodbased on graph min-cuts. We give the first experimental analysis on supervisedand semi-supervised datasets and show good empirical performance.
arxiv-6600-216 | Structured Generative Models of Natural Source Code | http://arxiv.org/pdf/1401.0514v2.pdf | author:Chris J. Maddison, Daniel Tarlow category:cs.PL cs.LG stat.ML published:2014-01-02 summary:We study the problem of building generative models of natural source code(NSC); that is, source code written and understood by humans. Our primarycontribution is to describe a family of generative models for NSC that havethree key properties: First, they incorporate both sequential and hierarchicalstructure. Second, we learn a distributed representation of source codeelements. Finally, they integrate closely with a compiler, which allowsleveraging compiler logic and abstractions when building structure into themodel. We also develop an extension that includes more complex structure,refining how the model generates identifier tokens based on what variables arecurrently in scope. Our models can be learned efficiently, and we showempirically that including appropriate structure greatly improves the models,measured by the probability of generating test programs.
arxiv-6600-217 | Rows vs Columns for Linear Systems of Equations - Randomized Kaczmarz or Coordinate Descent? | http://arxiv.org/pdf/1406.5295v1.pdf | author:Aaditya Ramdas category:math.OC cs.LG cs.NA math.NA stat.ML published:2014-06-20 summary:This paper is about randomized iterative algorithms for solving a linearsystem of equations $X \beta = y$ in different settings. Recent interest in thetopic was reignited when Strohmer and Vershynin (2009) proved the linearconvergence rate of a Randomized Kaczmarz (RK) algorithm that works on the rowsof $X$ (data points). Following that, Leventhal and Lewis (2010) proved thelinear convergence of a Randomized Coordinate Descent (RCD) algorithm thatworks on the columns of $X$ (features). The aim of this paper is to simplifyour understanding of these two algorithms, establish the direct relationshipsbetween them (though RK is often compared to Stochastic Gradient Descent), andexamine the algorithmic commonalities or tradeoffs involved with working onrows or columns. We also discuss Kernel Ridge Regression and present aKaczmarz-style algorithm that works on data points and having the advantage ofsolving the problem without ever storing or forming the Gram matrix, one of therecognized problems encountered when scaling kernelized methods.
arxiv-6600-218 | Enhancing Pure-Pixel Identification Performance via Preconditioning | http://arxiv.org/pdf/1406.5286v1.pdf | author:Nicolas Gillis, Wing-Kin Ma category:stat.ML cs.LG math.NA math.OC published:2014-06-20 summary:In this paper, we analyze different preconditionings designed to enhancerobustness of pure-pixel search algorithms, which are used for blindhyperspectral unmixing and which are equivalent to near-separable nonnegativematrix factorization algorithms. Our analysis focuses on the successiveprojection algorithm (SPA), a simple, efficient and provably robust algorithmin the pure-pixel algorithm class. Recently, a provably robust preconditioningwas proposed by Gillis and Vavasis (arXiv:1310.2273) which requires theresolution of a semidefinite program (SDP) to find a data points-enclosingminimum volume ellipsoid. Since solving the SDP in high precisions can be timeconsuming, we generalize the robustness analysis to approximate solutions ofthe SDP, that is, solutions whose objective function values are somemultiplicative factors away from the optimal value. It is shown that a highaccuracy solution is not crucial for robustness, which paves the way for fasterpreconditionings (e.g., based on first-order optimization methods). This firstcontribution also allows us to provide a robustness analysis for two otherpreconditionings. The first one is pre-whitening, which can be interpreted asan optimal solution of the same SDP with additional constraints. We analyzerobustness of pre-whitening which allows us to characterize situations in whichit performs competitively with the SDP-based preconditioning. The second one isbased on SPA itself and can be interpreted as an optimal solution of arelaxation of the SDP. It is extremely fast while competing with the SDP-basedpreconditioning on several synthetic data sets.
arxiv-6600-219 | Inner Product Similarity Search using Compositional Codes | http://arxiv.org/pdf/1406.4966v2.pdf | author:Chao Du, Jingdong Wang category:cs.CV cs.LG stat.ML published:2014-06-19 summary:This paper addresses the nearest neighbor search problem under inner productsimilarity and introduces a compact code-based approach. The idea is toapproximate a vector using the composition of several elements selected from asource dictionary and to represent this vector by a short code composed of theindices of the selected elements. The inner product between a query vector anda database vector is efficiently estimated from the query vector and the shortcode of the database vector. We show the superior performance of the proposedgroup $M$-selection algorithm that selects $M$ elements from $M$ sourcedictionaries for vector approximation in terms of search accuracy andefficiency for compact codes of the same length via theoretical and empiricalanalysis. Experimental results on large-scale datasets ($1M$ and $1B$ SIFTfeatures, $1M$ linear models and Netflix) demonstrate the superiority of theproposed approach.
arxiv-6600-220 | Lifted Tree-Reweighted Variational Inference | http://arxiv.org/pdf/1406.4200v2.pdf | author:Hung Hai Bui, Tuyen N. Huynh, David Sontag category:cs.AI stat.ML published:2014-06-17 summary:We analyze variational inference for highly symmetric graphical models suchas those arising from first-order probabilistic models. We first show that forthese graphical models, the tree-reweighted variational objective lends itselfto a compact lifted formulation which can be solved much more efficiently thanthe standard TRW formulation for the ground graphical model. Compared toearlier work on lifted belief propagation, our formulation leads to a convexoptimization problem for lifted marginal inference and provides an upper boundon the partition function. We provide two approaches for improving the liftedTRW upper bound. The first is a method for efficiently computing maximumspanning trees in highly symmetric graphs, which can be used to optimize theTRW edge appearance probabilities. The second is a method for tightening therelaxation of the marginal polytope using lifted cycle inequalities and novelexchangeable cluster consistency constraints.
arxiv-6600-221 | R-CNNs for Pose Estimation and Action Detection | http://arxiv.org/pdf/1406.5212v1.pdf | author:Georgia Gkioxari, Bharath Hariharan, Ross Girshick, Jitendra Malik category:cs.CV published:2014-06-19 summary:We present convolutional neural networks for the tasks of keypoint (pose)prediction and action classification of people in unconstrained images. Ourapproach involves training an R-CNN detector with loss functions depending onthe task being tackled. We evaluate our method on the challenging PASCAL VOCdataset and compare it to previous leading approaches. Our method givesstate-of-the-art results for keypoint and action prediction. Additionally, weintroduce a new dataset for action detection, the task of simultaneouslylocalizing people and classifying their actions, and present results using ourapproach.
arxiv-6600-222 | Fast Support Vector Machines Using Parallel Adaptive Shrinking on Distributed Systems | http://arxiv.org/pdf/1406.5161v1.pdf | author:Jeyanthi Narasimhan, Abhinav Vishnu, Lawrence Holder, Adolfy Hoisie category:cs.DC cs.LG published:2014-06-19 summary:Support Vector Machines (SVM), a popular machine learning technique, has beenapplied to a wide range of domains such as science, finance, and socialnetworks for supervised learning. Whether it is identifying high-risk patientsby health-care professionals, or potential high-school students to enroll incollege by school districts, SVMs can play a major role for social good. Thispaper undertakes the challenge of designing a scalable parallel SVM trainingalgorithm for large scale systems, which includes commodity multi-coremachines, tightly connected supercomputers and cloud computing systems.Intuitive techniques for improving the time-space complexity including adaptiveelimination of samples for faster convergence and sparse format representationare proposed. Under sample elimination, several heuristics for {\em earliestpossible} to {\em lazy} elimination of non-contributing samples are proposed.In several cases, where an early sample elimination might result in a falsepositive, low overhead mechanisms for reconstruction of key data structures areproposed. The algorithm and heuristics are implemented and evaluated on variouspublicly available datasets. Empirical evaluation shows up to 26x speedimprovement on some datasets against the sequential baseline, when evaluated onmultiple compute nodes, and an improvement in execution time up to 30-60\% isreadily observed on a number of other datasets against our parallel baseline.
arxiv-6600-223 | MRF-based Background Initialisation for Improved Foreground Detection in Cluttered Surveillance Videos | http://arxiv.org/pdf/1406.5095v1.pdf | author:Vikas Reddy, Conrad Sanderson, Andres Sanin, Brian C. Lovell category:cs.CV published:2014-06-19 summary:Robust foreground object segmentation via background modelling is a difficultproblem in cluttered environments, where obtaining a clear view of thebackground to model is almost impossible. In this paper, we propose a methodcapable of robustly estimating the background and detecting regions of interestin such environments. In particular, we propose to extend the backgroundinitialisation component of a recent patch-based foreground detection algorithmwith an elaborate technique based on Markov Random Fields, where the optimallabelling solution is computed using iterated conditional modes. Rather thanrelying purely on local temporal statistics, the proposed technique takes intoaccount the spatial continuity of the entire background. Experiments withseveral tracking algorithms on the CAVIAR dataset indicate that the proposedmethod leads to considerable improvements in object tracking accuracy, whencompared to methods based on Gaussian mixture models and feature histograms.
arxiv-6600-224 | Robust Outlier Detection Technique in Data Mining: A Univariate Approach | http://arxiv.org/pdf/1406.5074v1.pdf | author:Singh Vijendra, Pathak Shivani category:cs.CV published:2014-06-19 summary:Outliers are the points which are different from or inconsistent with therest of the data. They can be novel, new, abnormal, unusual or noisyinformation. Outliers are sometimes more interesting than the majority of thedata. The main challenges of outlier detection with the increasing complexity,size and variety of datasets, are how to catch similar outliers as a group, andhow to evaluate the outliers. This paper describes an approach which usesUnivariate outlier detection as a pre-processing step to detect the outlier andthen applies K-means algorithm hence to analyse the effects of the outliers onthe cluster analysis of dataset.
arxiv-6600-225 | Inferring causal structure: a quantum advantage | http://arxiv.org/pdf/1406.5036v1.pdf | author:Katja Ried, Megan Agnew, Lydia Vermeyden, Dominik Janzing, Robert W. Spekkens, Kevin J. Resch category:quant-ph cs.LG gr-qc stat.ML published:2014-06-19 summary:The problem of using observed correlations to infer causal relations isrelevant to a wide variety of scientific disciplines. Yet given correlationsbetween just two classical variables, it is impossible to determine whetherthey arose from a causal influence of one on the other or a common causeinfluencing both, unless one can implement a randomized intervention. We hereconsider the problem of causal inference for quantum variables. We introducecausal tomography, which unifies and generalizes conventional quantumtomography schemes to provide a complete solution to the causal inferenceproblem using a quantum analogue of a randomized trial. We furthermore showthat, in contrast to the classical case, observed quantum correlations alonecan sometimes provide a solution. We implement a quantum-optical experimentthat allows us to control the causal relation between two optical modes, andtwo measurement schemes -- one with and one without randomization -- thatextract this relation from the observed correlations. Our results show thatentanglement and coherence, known to be central to quantum informationprocessing, also provide a quantum advantage for causal inference.
arxiv-6600-226 | Why are images smooth? | http://arxiv.org/pdf/1406.5035v1.pdf | author:Uriel Feige category:cs.CV published:2014-06-19 summary:It is a well observed phenomenon that natural images are smooth, in the sensethat nearby pixels tend to have similar values. We describe a mathematicalmodel of images that makes no assumptions on the nature of the environment thatimages depict. It only assumes that images can be taken at different scales(zoom levels). We provide quantitative bounds on the smoothness of a typicalimage in our model, as a function of the number of available scales. Thesebounds can serve as a baseline against which to compare the observed smoothnessof natural images.
arxiv-6600-227 | Fast and Robust Least Squares Estimation in Corrupted Linear Models | http://arxiv.org/pdf/1406.3175v2.pdf | author:Brian McWilliams, Gabriel Krummenacher, Mario Lucic, Joachim M. Buhmann category:stat.ML published:2014-06-12 summary:Subsampling methods have been recently proposed to speed up least squaresestimation in large scale settings. However, these algorithms are typically notrobust to outliers or corruptions in the observed covariates. The concept of influence that was developed for regression diagnostics can beused to detect such corrupted observations as shown in this paper. Thisproperty of influence -- for which we also develop a randomized approximation-- motivates our proposed subsampling algorithm for large scale corruptedlinear regression which limits the influence of data points since highlyinfluential points contribute most to the residual error. Under a general modelof corrupted observations, we show theoretically and empirically on a varietyof simulated and real datasets that our algorithm improves over the currentstate-of-the-art approximation schemes for ordinary least squares.
arxiv-6600-228 | How Many Topics? Stability Analysis for Topic Models | http://arxiv.org/pdf/1404.4606v3.pdf | author:Derek Greene, Derek O'Callaghan, Pádraig Cunningham category:cs.LG cs.CL cs.IR published:2014-04-16 summary:Topic modeling refers to the task of discovering the underlying thematicstructure in a text corpus, where the output is commonly presented as a reportof the top terms appearing in each topic. Despite the diversity of topicmodeling algorithms that have been proposed, a common challenge in successfullyapplying these techniques is the selection of an appropriate number of topicsfor a given corpus. Choosing too few topics will produce results that areoverly broad, while choosing too many will result in the "over-clustering" of acorpus into many small, highly-similar topics. In this paper, we propose aterm-centric stability analysis strategy to address this issue, the idea beingthat a model with an appropriate number of topics will be more robust toperturbations in the data. Using a topic modeling approach based on matrixfactorization, evaluations performed on a range of corpora show that thisstrategy can successfully guide the model selection process.
arxiv-6600-229 | Majority Vote of Diverse Classifiers for Late Fusion | http://arxiv.org/pdf/1404.7796v2.pdf | author:Emilie Morvant, Amaury Habrard, Stéphane Ayache category:stat.ML cs.LG cs.MM published:2014-04-30 summary:In the past few years, a lot of attention has been devoted to multimediaindexing by fusing multimodal informations. Two kinds of fusion schemes aregenerally considered: The early fusion and the late fusion. We focus on lateclassifier fusion, where one combines the scores of each modality at thedecision level. To tackle this problem, we investigate a recent and elegantwell-founded quadratic program named MinCq coming from the machine learningPAC-Bayesian theory. MinCq looks for the weighted combination, over a set ofreal-valued functions seen as voters, leading to the lowest misclassificationrate, while maximizing the voters' diversity. We propose an extension of MinCqtailored to multimedia indexing. Our method is based on an order-preservingpairwise loss adapted to ranking that allows us to improve Mean AveragedPrecision measure while taking into account the diversity of the voters that wewant to fuse. We provide evidence that this method is naturally adapted to latefusion procedures and confirm the good behavior of our approach on thechallenging PASCAL VOC'07 benchmark.
arxiv-6600-230 | On the Application of Generic Summarization Algorithms to Music | http://arxiv.org/pdf/1406.4877v1.pdf | author:Francisco Raposo, Ricardo Ribeiro, David Martins de Matos category:cs.IR cs.LG cs.SD H.5.5 published:2014-06-18 summary:Several generic summarization algorithms were developed in the past andsuccessfully applied in fields such as text and speech summarization. In thispaper, we review and apply these algorithms to music. To evaluate thissummarization's performance, we adopt an extrinsic approach: we compare a FadoGenre Classifier's performance using truncated contiguous clips against thesummaries extracted with those algorithms on 2 different datasets. We show thatMaximal Marginal Relevance (MMR), LexRank and Latent Semantic Analysis (LSA)all improve classification performance in both datasets used for testing.
arxiv-6600-231 | Fast LSTD using stochastic approximation: Finite time analysis and application to traffic control | http://arxiv.org/pdf/1306.2557v4.pdf | author:L. A. Prashanth, Nathaniel Korda, Rémi Munos category:cs.LG stat.ML published:2013-06-11 summary:We propose a stochastic approximation based method with randomisation ofsamples for policy evaluation using the least squares temporal difference(LSTD) algorithm. Our method results in an $O(d)$ improvement in complexity incomparison to regular LSTD, where $d$ is the dimension of the data. We provideconvergence rate results for our proposed method, both in high probability andin expectation. Moreover, we also establish that using our scheme in place ofLSTD does not impact the rate of convergence of the approximate value functionto the true value function. This result coupled with the low complexity of ourmethod makes it attractive for implementation in big data settings, where $d$is large. Further, we also analyse a similar low-complexity alternative forleast squares regression and provide finite-time bounds there. We demonstratethe practicality of our method for LSTD empirically by combining it with theLSPI algorithm in a traffic signal control application.
arxiv-6600-232 | Improved Densification of One Permutation Hashing | http://arxiv.org/pdf/1406.4784v1.pdf | author:Anshumali Shrivastava, Ping Li category:stat.ME cs.DS cs.IR cs.LG published:2014-06-18 summary:The existing work on densification of one permutation hashing reduces thequery processing cost of the $(K,L)$-parameterized Locality Sensitive Hashing(LSH) algorithm with minwise hashing, from $O(dKL)$ to merely $O(d + KL)$,where $d$ is the number of nonzeros of the data vector, $K$ is the number ofhashes in each hash table, and $L$ is the number of hash tables. While that isa substantial improvement, our analysis reveals that the existing densificationscheme is sub-optimal. In particular, there is no enough randomness in thatprocedure, which affects its accuracy on very sparse datasets. In this paper, we provide a new densification procedure which is provablybetter than the existing scheme. This improvement is more significant for verysparse datasets which are common over the web. The improved technique has thesame cost of $O(d + KL)$ for query processing, thereby making it strictlypreferable over the existing procedure. Experimental evaluations on publicdatasets, in the task of hashing based near neighbor search, support ourtheoretical findings.
arxiv-6600-233 | Predictive Modelling of Bone Age through Classification and Regression of Bone Shapes | http://arxiv.org/pdf/1406.4781v1.pdf | author:Anthony Bagnall, Luke Davis category:cs.LG physics.med-ph published:2014-06-18 summary:Bone age assessment is a task performed daily in hospitals worldwide. Thisinvolves a clinician estimating the age of a patient from a radiograph of thenon-dominant hand. Our approach to automated bone age assessment is to modularise the algorithminto the following three stages: segment and verify hand outline; segment andverify bones; use the bone outlines to construct models of age. In this paperwe address the final question: given outlines of bones, can we learn how topredict the bone age of the patient? We examine two alternative approaches.Firstly, we attempt to train classifiers on individual bones to predict thebone stage categories commonly used in bone ageing. Secondly, we constructregression models to directly predict patient age. We demonstrate that models built on summary features of the bone outlineperform better than those built using the one dimensional representation of theoutline, and also do at least as well as other automated systems. We show thatmodels constructed on just three bones are as accurate at predicting age asexpert human assessors using the standard technique. We also demonstrate theutility of the model by quantifying the importance of ethnicity and sex on agedevelopment. Our conclusion is that the feature based system of separating theimage processing from the age modelling is the best approach for automated boneageing, since it offers flexibility and transparency and produces accurateestimates.
arxiv-6600-234 | Deep Learning Face Representation by Joint Identification-Verification | http://arxiv.org/pdf/1406.4773v1.pdf | author:Yi Sun, Xiaogang Wang, Xiaoou Tang category:cs.CV published:2014-06-18 summary:The key challenge of face recognition is to develop effective featurerepresentations for reducing intra-personal variations while enlarginginter-personal differences. In this paper, we show that it can be well solvedwith deep learning and using both face identification and verification signalsas supervision. The Deep IDentification-verification features (DeepID2) arelearned with carefully designed deep convolutional networks. The faceidentification task increases the inter-personal variations by drawing DeepID2extracted from different identities apart, while the face verification taskreduces the intra-personal variations by pulling DeepID2 extracted from thesame identity together, both of which are essential to face recognition. Thelearned DeepID2 features can be well generalized to new identities unseen inthe training data. On the challenging LFW dataset, 99.15% face verificationaccuracy is achieved. Compared with the best deep learning result on LFW, theerror rate has been significantly reduced by 67%.
arxiv-6600-235 | Mass Classification Method in Mammogram Using Fuzzy K-Nearest Neighbour Equality | http://arxiv.org/pdf/1406.4770v1.pdf | author:I. Laurence Aroquiaraj, K. Thangavel category:cs.CV published:2014-06-18 summary:Mass classification of objects is an important area of research andapplication in a variety of fields. In this paper, we present an efficientcomputer aided mass classification method in digitized mammograms using FuzzyK-Nearest Neighbor Equality, which performs benign or malignant classificationon region of interest that contains mass. One of the major mammographiccharacteristics for mass classification is texture. Fuzzy K-Nearest NeighborEquality exploits this important factor to classify the mass into benign ormalignant. The statistical textural features used in characterizing the massesare Haralick and Run length features. The main aim of the method is to increasethe effectiveness and efficiency of the classification process in an objectivemanner to reduce the numbers of false positive of malignancies. In this paperproposes a novel Fuzzy K-Nearest Neighbor Equality algorithm for classifyingthe marked regions into benign and malignant and 94.46 sensitivity,96.81specificity and 96.52 accuracy is achieved that is very much promising compareto the radiologists' accuracy.
arxiv-6600-236 | An Experimental Evaluation of Nearest Neighbour Time Series Classification | http://arxiv.org/pdf/1406.4757v1.pdf | author:Anthony Bagnall, Jason Lines category:cs.LG published:2014-06-18 summary:Data mining research into time series classification (TSC) has focussed onalternative distance measures for nearest neighbour classifiers. It is standardpractice to use 1-NN with Euclidean or dynamic time warping (DTW) distance as astraw man for comparison. As part of a wider investigation into elasticdistance measures for TSC~\cite{lines14elastic}, we perform a series ofexperiments to test whether this standard practice is valid. Specifically, we compare 1-NN classifiers with Euclidean and DTW distance tostandard classifiers, examine whether the performance of 1-NN Euclideanapproaches that of 1-NN DTW as the number of cases increases, assess whetherthere is any benefit of setting $k$ for $k$-NN through cross validation whetherit is worth setting the warping path for DTW through cross validation andfinally is it better to use a window or weighting for DTW. Based on experimentson 77 problems, we conclude that 1-NN with Euclidean distance is fairly easy tobeat but 1-NN with DTW is not, if window size is set through cross validation.
arxiv-6600-237 | Typed Hilbert Epsilon Operators and the Semantics of Determiner Phrases (Invited Lecture) | http://arxiv.org/pdf/1406.4710v1.pdf | author:Christian Retoré category:cs.CL cs.AI cs.LO math.LO published:2014-06-18 summary:The semantics of determiner phrases, be they definite de- scriptions,indefinite descriptions or quantified noun phrases, is often as- sumed to be afully solved question: common nouns are properties, and determiners aregeneralised quantifiers that apply to two predicates: the propertycorresponding to the common noun and the one corresponding to the verb phrase.We first present a criticism of this standard view. Firstly, the semantics ofdeterminers does not follow the syntactical structure of the sentence. Secondlythe standard interpretation of the indefinite article cannot ac- count fornominal sentences. Thirdly, the standard view misses the linguis- tic asymmetrybetween the two properties of a generalised quantifier. In the sequel, wepropose a treatment of determiners and quantifiers as Hilbert terms in a richlytyped system that we initially developed for lexical semantics, using a manysorted logic for semantical representations. We present this semanticalframework called the Montagovian generative lexicon and show how these termsbetter match the syntactical structure and avoid the aforementioned problems ofthe standard approach. Hilbert terms rather differ from choice functions inthat there is one polymorphic operator and not one operator per formula. Theyalso open an intriguing connection between the logic for meaning assembly, thetyped lambda calculus handling compositionality and the many-sorted logic forsemantical representations. Furthermore epsilon terms naturally introducetype-judgements and confirm the claim that type judgment are a form ofpresupposition.
arxiv-6600-238 | Self-Learning Camera: Autonomous Adaptation of Object Detectors to Unlabeled Video Streams | http://arxiv.org/pdf/1406.4296v2.pdf | author:Adrien Gaidon, Gloria Zen, Jose A. Rodriguez-Serrano category:cs.CV cs.LG published:2014-06-17 summary:Learning object detectors requires massive amounts of labeled trainingsamples from the specific data source of interest. This is impractical whendealing with many different sources (e.g., in camera networks), or constantlychanging ones such as mobile cameras (e.g., in robotics or driving assistantsystems). In this paper, we address the problem of self-learning detectors inan autonomous manner, i.e. (i) detectors continuously updating themselves toefficiently adapt to streaming data sources (contrary to transductivealgorithms), (ii) without any labeled data strongly related to the target datastream (contrary to self-paced learning), and (iii) without manual interventionto set and update hyper-parameters. To that end, we propose an unsupervised,on-line, and self-tuning learning algorithm to optimize a multi-task learningconvex objective. Our method uses confident but laconic oracles (high-precisionbut low-recall off-the-shelf generic detectors), and exploits the structure ofthe problem to jointly learn on-line an ensemble of instance-level trackers,from which we derive an adapted category-level object detector. Our approach isvalidated on real-world publicly available video object datasets.
arxiv-6600-239 | The Frobenius anatomy of word meanings II: possessive relative pronouns | http://arxiv.org/pdf/1406.4690v1.pdf | author:Mehrnoosh Sadrzadeh, Stephen Clark, Bob Coecke category:cs.CL math.CT 18Dxx, 18Axx I.2.7; F.4.1 published:2014-06-18 summary:Within the categorical compositional distributional model of meaning, weprovide semantic interpretations for the subject and object roles of thepossessive relative pronoun `whose'. This is done in terms of Frobeniusalgebras over compact closed categories. These algebras and their diagrammaticlanguage expose how meanings of words in relative clauses interact with eachother. We show how our interpretation is related to Montague-style semanticsand provide a truth-theoretic interpretation. We also show how vector spacesprovide a concrete interpretation and provide preliminary corpus-basedexperimental evidence. In a prequel to this paper, we used similar methods anddealt with the case of subject and object relative pronouns.
arxiv-6600-240 | Exact Decoding on Latent Variable Conditional Models is NP-Hard | http://arxiv.org/pdf/1406.4682v1.pdf | author:Xu Sun category:cs.AI cs.CC cs.LG published:2014-06-18 summary:Latent variable conditional models, including the latent conditional randomfields as a special case, are popular models for many natural languageprocessing and vision processing tasks. The computational complexity of theexact decoding/inference in latent conditional random fields is unclear. Inthis paper, we try to clarify the computational complexity of the exactdecoding. We analyze the complexity and demonstrate that it is an NP-hardproblem even on a sequential labeling setting. Furthermore, we propose thelatent-dynamic inference (LDI-Naive) method and its bounded version(LDI-Bounded), which are able to perform exact-inference oralmost-exact-inference by using top-$n$ search and dynamic programming.
arxiv-6600-241 | Notes on hierarchical ensemble methods for DAG-structured taxonomies | http://arxiv.org/pdf/1406.4472v2.pdf | author:Giorgio Valentini category:cs.AI cs.LG stat.ML I.2.6 published:2014-06-17 summary:Several real problems ranging from text classification to computationalbiology are characterized by hierarchical multi-label classification tasks.Most of the methods presented in literature focused on tree-structuredtaxonomies, but only few on taxonomies structured according to a DirectedAcyclic Graph (DAG). In this contribution novel classification ensemblealgorithms for DAG-structured taxonomies are introduced. In particularHierarchical Top-Down (HTD-DAG) and True Path Rule (TPR-DAG) for DAGs arepresented and discussed.
arxiv-6600-242 | RAPID: Rapidly Accelerated Proximal Gradient Algorithms for Convex Minimization | http://arxiv.org/pdf/1406.4445v2.pdf | author:Ziming Zhang, Venkatesh Saligrama category:stat.ML cs.LG math.OC published:2014-06-13 summary:In this paper, we propose a new algorithm to speed-up the convergence ofaccelerated proximal gradient (APG) methods. In order to minimize a convexfunction $f(\mathbf{x})$, our algorithm introduces a simple line search stepafter each proximal gradient step in APG so that a biconvex function$f(\theta\mathbf{x})$ is minimized over scalar variable $\theta>0$ while fixingvariable $\mathbf{x}$. We propose two new ways of constructing the auxiliaryvariables in APG based on the intermediate solutions of the proximal gradientand the line search steps. We prove that at arbitrary iteration step $t(t\geq1)$, our algorithm can achieve a smaller upper-bound for the gap betweenthe current and optimal objective values than those in the traditional APGmethods such as FISTA, making it converge faster in practice. In fact, ouralgorithm can be potentially applied to many important convex optimizationproblems, such as sparse linear regression and kernel SVMs. Our experimentalresults clearly demonstrate that our algorithm converges faster than APG in allof the applications above, even comparable to some sophisticated solvers.
arxiv-6600-243 | A Sober Look at Spectral Learning | http://arxiv.org/pdf/1406.4631v1.pdf | author:Han Zhao, Pascal Poupart category:cs.LG published:2014-06-18 summary:Spectral learning recently generated lots of excitement in machine learning,largely because it is the first known method to produce consistent estimates(under suitable conditions) for several latent variable models. In contrast,maximum likelihood estimates may get trapped in local optima due to thenon-convex nature of the likelihood function of latent variable models. In thispaper, we do an empirical evaluation of spectral learning (SL) and expectationmaximization (EM), which reveals an important gap between the theory and thepractice. First, SL often leads to negative probabilities. Second, EM oftenyields better estimates than spectral learning and it does not seem to getstuck in local optima. We discuss how the rank of the model parameters and theamount of training data can yield negative probabilities. We also question thecommon belief that maximum likelihood estimators are necessarily inconsistent.
arxiv-6600-244 | A Generalized Markov-Chain Modelling Approach to $(1,λ)$-ES Linear Optimization: Technical Report | http://arxiv.org/pdf/1406.4619v1.pdf | author:Alexandre Chotard, Martin Holena category:cs.NA cs.LG cs.NE published:2014-06-18 summary:Several recent publications investigated Markov-chain modelling of linearoptimization by a $(1,\lambda)$-ES, considering both unconstrained and linearlyconstrained optimization, and both constant and varying step size. All of themassume normality of the involved random steps, and while this is consistentwith a black-box scenario, information on the function to be optimized (e.g.separability) may be exploited by the use of another distribution. Theobjective of our contribution is to complement previous studies realized withnormal steps, and to give sufficient conditions on the distribution of therandom steps for the success of a constant step-size $(1,\lambda)$-ES on thesimple problem of a linear function with a linear constraint. The decompositionof a multidimensional distribution into its marginals and the copula combiningthem is applied to the new distributional assumptions, particular attentionbeing paid to distributions with Archimedean copulas.
arxiv-6600-245 | Primitives for Dynamic Big Model Parallelism | http://arxiv.org/pdf/1406.4580v1.pdf | author:Seunghak Lee, Jin Kyu Kim, Xun Zheng, Qirong Ho, Garth A. Gibson, Eric P. Xing category:stat.ML cs.DC cs.LG published:2014-06-18 summary:When training large machine learning models with many variables orparameters, a single machine is often inadequate since the model may be toolarge to fit in memory, while training can take a long time even withstochastic updates. A natural recourse is to turn to distributed clustercomputing, in order to harness additional memory and processors. However,naive, unstructured parallelization of ML algorithms can make inefficient useof distributed memory, while failing to obtain proportional convergencespeedups - or can even result in divergence. We develop a framework ofprimitives for dynamic model-parallelism, STRADS, in order to explorepartitioning and update scheduling of model variables in distributed MLalgorithms - thus improving their memory efficiency while presenting newopportunities to speed up convergence without compromising inferencecorrectness. We demonstrate the efficacy of model-parallel algorithmsimplemented in STRADS versus popular implementations for Topic Modeling, MatrixFactorization and Lasso.
arxiv-6600-246 | DFacTo: Distributed Factorization of Tensors | http://arxiv.org/pdf/1406.4519v1.pdf | author:Joon Hee Choi, S. V. N. Vishwanathan category:stat.ML published:2014-06-17 summary:We present a technique for significantly speeding up Alternating LeastSquares (ALS) and Gradient Descent (GD), two widely used algorithms for tensorfactorization. By exploiting properties of the Khatri-Rao product, we show howto efficiently address a computationally challenging sub-step of bothalgorithms. Our algorithm, DFacTo, only requires two sparse matrix-vectorproducts and is easy to parallelize. DFacTo is not only scalable but also onaverage 4 to 10 times faster than competing algorithms on a variety ofdatasets. For instance, DFacTo only takes 480 seconds on 4 machines to performone iteration of the ALS algorithm and 1,143 seconds to perform one iterationof the GD algorithm on a 6.5 million x 2.5 million x 1.5 million dimensionaltensor with 1.2 billion non-zero entries.
arxiv-6600-247 | Block matching algorithm based on Harmony Search optimization for motion estimation | http://arxiv.org/pdf/1406.4484v1.pdf | author:Erik Cuevas category:cs.CV published:2014-06-17 summary:Motion estimation is one of the major problems in developing video codingapplications. Among all motion estimation approaches, Block-matching (BM)algorithms are the most popular methods due to their effectiveness andsimplicity for both software and hardware implementations. A BM approachassumes that the movement of pixels within a defined region of the currentframe can be modeled as a translation of pixels contained in the previousframe. In this procedure, the motion vector is obtained by minimizing a certainmatching metric that is produced for the current frame over a determined searchwindow from the previous frame. Unfortunately, the evaluation of such matchingmeasurement is computationally expensive and represents the most consumingoperation in the BM process. Therefore, BM motion estimation can be viewed asan optimization problem whose goal is to find the best-matching block within asearch space. The simplest available BM method is the Full Search Algorithm(FSA) which finds the most accurate motion vector through an exhaustivecomputation of all the elements of the search space. Recently, several fast BMalgorithms have been proposed to reduce the search positions by calculatingonly a fixed subset of motion vectors despite lowering its accuracy. On theother hand, the Harmony Search (HS) algorithm is a population-basedoptimization method that is inspired by the music improvisation process inwhich a musician searches for harmony and continues to polish the pitches toobtain a better harmony. In this paper, a new BM algorithm that combines HSwith a fitness approximation model is proposed. The approach uses motionvectors belonging to the search window as potential solutions. A fitnessfunction evaluates the matching quality of each motion vector candidate.
arxiv-6600-248 | Authorship Attribution through Function Word Adjacency Networks | http://arxiv.org/pdf/1406.4469v1.pdf | author:Santiago Segarra, Mark Eisen, Alejandro Ribeiro category:cs.CL cs.LG stat.ML published:2014-06-17 summary:A method for authorship attribution based on function word adjacency networks(WANs) is introduced. Function words are parts of speech that expressgrammatical relationships between other words but do not carry lexical meaningon their own. In the WANs in this paper, nodes are function words and directededges stand in for the likelihood of finding the sink word in the orderedvicinity of the source word. WANs of different authors can be interpreted astransition probabilities of a Markov chain and are therefore compared in termsof their relative entropies. Optimal selection of WAN parameters is studied andattribution accuracy is benchmarked across a diverse pool of authors andvarying text lengths. This analysis shows that, since function words areindependent of content, their use tends to be specific to an author and thatthe relational data captured by function WANs is a good summary of stylometricfingerprints. Attribution accuracy is observed to exceed the one achieved bymethods that rely on word frequencies alone. Further combining WANs withmethods that rely on word frequencies alone, results in larger attributionaccuracy, indicating that both sources of information encode different aspectsof authorial styles.
arxiv-6600-249 | A Kernel Independence Test for Random Processes | http://arxiv.org/pdf/1402.4501v3.pdf | author:Kacper Chwialkowski, Arthur Gretton category:stat.ML 62G10 published:2014-02-18 summary:A new non parametric approach to the problem of testing the independence oftwo random process is developed. The test statistic is the Hilbert SchmidtIndependence Criterion (HSIC), which was used previously in testingindependence for i.i.d pairs of variables. The asymptotic behaviour of HSIC isestablished when computed from samples drawn from random processes. It is shownthat earlier bootstrap procedures which worked in the i.i.d. case will fail forrandom processes, and an alternative consistent estimate of the p-values isproposed. Tests on artificial data and real-world Forex data indicate that thenew test procedure discovers dependence which is missed by linear approaches,while the earlier bootstrap procedure returns an elevated number of falsepositives. The code is available online:https://github.com/kacperChwialkowski/HSIC .
arxiv-6600-250 | Fast Computation of Wasserstein Barycenters | http://arxiv.org/pdf/1310.4375v3.pdf | author:Marco Cuturi, Arnaud Doucet category:stat.ML published:2013-10-16 summary:We present new algorithms to compute the mean of a set of empiricalprobability measures under the optimal transport metric. This mean, known asthe Wasserstein barycenter, is the measure that minimizes the sum of itsWasserstein distances to each element in that set. We propose two originalalgorithms to compute Wasserstein barycenters that build upon the subgradientmethod. A direct implementation of these algorithms is, however, too costlybecause it would require the repeated resolution of large primal and dualoptimal transport problems to compute subgradients. Extending the work ofCuturi (2013), we propose to smooth the Wasserstein distance used in thedefinition of Wasserstein barycenters with an entropic regularizer and recoverin doing so a strictly convex objective whose gradients can be computed for aconsiderably cheaper computational cost using matrix scaling algorithms. We usethese algorithms to visualize a large family of images and to solve aconstrained clustering problem.
arxiv-6600-251 | Single camera pose estimation using Bayesian filtering and Kinect motion priors | http://arxiv.org/pdf/1405.5047v2.pdf | author:Michael Burke, Joan Lasenby category:cs.CV cs.HC published:2014-05-20 summary:Traditional approaches to upper body pose estimation using monocular visionrely on complex body models and a large variety of geometric constraints. Weargue that this is not ideal and somewhat inelegant as it results in largeprocessing burdens, and instead attempt to incorporate these constraintsthrough priors obtained directly from training data. A prior distributioncovering the probability of a human pose occurring is used to incorporatelikely human poses. This distribution is obtained offline, by fitting aGaussian mixture model to a large dataset of recorded human body poses, trackedusing a Kinect sensor. We combine this prior information with a random walktransition model to obtain an upper body model, suitable for use within arecursive Bayesian filtering framework. Our model can be viewed as a mixture ofdiscrete Ornstein-Uhlenbeck processes, in that states behave as random walks,but drift towards a set of typically observed poses. This model is combinedwith measurements of the human head and hand positions, using recursiveBayesian estimation to incorporate temporal information. Measurements areobtained using face detection and a simple skin colour hand detector, trainedusing the detected face. The suggested model is designed with analyticaltractability in mind and we show that the pose tracking can beRao-Blackwellised using the mixture Kalman filter, allowing for computationalefficiency while still incorporating bio-mechanical properties of the upperbody. In addition, the use of the proposed upper body model allows reliablethree-dimensional pose estimates to be obtained indirectly for a number ofjoints that are often difficult to detect using traditional object recognitionstrategies. Comparisons with Kinect sensor results and the state of the art in2D pose estimation highlight the efficacy of the proposed approach.
arxiv-6600-252 | Sparse Estimation with the Swept Approximated Message-Passing Algorithm | http://arxiv.org/pdf/1406.4311v1.pdf | author:Andre Manoel, Florent Krzakala, Eric W. Tramel, Lenka Zdeborová category:cs.IT math.IT stat.ML published:2014-06-17 summary:Approximate Message Passing (AMP) has been shown to be a superior method forinference problems, such as the recovery of signals from sets of noisy,lower-dimensionality measurements, both in terms of reconstruction accuracy andin computational efficiency. However, AMP suffers from serious convergenceissues in contexts that do not exactly match its assumptions. We propose a newapproach to stabilizing AMP in these contexts by applying AMP updates toindividual coefficients rather than in parallel. Our results show that thischange to the AMP iteration can provide theoretically expected, but hithertounobtainable, performance for problems on which the standard AMP iterationdiverges. Additionally, we find that the computational costs of this sweptcoefficient update scheme is not unduly burdensome, allowing it to be appliedefficiently to signals of large dimensionality.
arxiv-6600-253 | Learning States Representations in POMDP | http://arxiv.org/pdf/1312.6042v4.pdf | author:Gabriella Contardo, Ludovic Denoyer, Thierry Artieres, Patrick Gallinari category:cs.LG published:2013-12-20 summary:We propose to deal with sequential processes where only partial observationsare available by learning a latent representation space on which policies maybe accurately learned.
arxiv-6600-254 | An Evolutionary Approach for Optimal Citing and Sizing of Micro-Grid in Radial Distribution Systems | http://arxiv.org/pdf/1406.4237v1.pdf | author:Eswari. J, Dr. S. Jeyadevi category:cs.NE published:2014-06-17 summary:This Paper presents the methodology of penetration of Micro-Grids (MG) in theradial distribution system (RDS). The aim of this paper is to minimize a totalreal power loss that descends the performance of the radial distribution systemby integrating various renewable resources as Distributed Generation (DG). Thecombination of different types of renewable energy resources contributes asustainable MG. These resources are optimally sized and located usingevolutionary approach in various penetration levels. The optimal solutions areexperimented with IEEE 33 radial distribution system using Particle SwarmOptimization (PSO) technique. The results are quite promising and authenticateits potential to solve problem in radial distribution system effectively.
arxiv-6600-255 | Mapping the Economic Crisis: Some Preliminary Investigations | http://arxiv.org/pdf/1406.4211v1.pdf | author:Pierre Bourreau, Thierry Poibeau category:cs.CL published:2014-06-17 summary:In this paper we describe our contribution to the PoliInformatics 2014Challenge on the 2007-2008 financial crisis. We propose a state of the arttechnique to extract information from texts and provide differentrepresentations, giving first a static overview of the domain and then adynamic representation of its main evolutions. We show that this strategyprovides a practical solution to some recent theories in social sciences thatare facing a lack of methods and tools to automatically extract informationfrom natural language texts.
arxiv-6600-256 | Replicating Kernels with a Short Stride Allows Sparse Reconstructions with Fewer Independent Kernels | http://arxiv.org/pdf/1406.4205v1.pdf | author:Peter F. Schultz, Dylan M. Paiton, Wei Lu, Garrett T. Kenyon category:q-bio.QM cs.CV published:2014-06-17 summary:In sparse coding it is common to tile an image into nonoverlapping patches,and then use a dictionary to create a sparse representation of each tileindependently. In this situation, the overcompleteness of the dictionary is thenumber of dictionary elements divided by the patch size. In deconvolutionalneural networks (DCNs), dictionaries learned on nonoverlapping tiles arereplaced by a family of convolution kernels. Hence adjacent points in thefeature maps (V1 layers) have receptive fields in the image that aretranslations of each other. The translational distance is determined by thedimensions of V1 in comparison to the dimensions of the image space. We referto this translational distance as the stride. We implement a type of DCN using a modified Locally Competitive Algorithm(LCA) to investigate the relationship between the number of kernels, thestride, the receptive field size, and the quality of reconstruction. We find,for example, that for 16x16-pixel receptive fields, using eight kernels and astride of 2 leads to sparse reconstructions of comparable quality as using 512kernels and a stride of 16 (the nonoverlapping case). We also find that for agiven stride and number of kernels, the patch size does not significantlyaffect reconstruction quality. Instead, the learned convolution kernels have anatural support radius independent of the patch size.
arxiv-6600-257 | Construction of non-convex polynomial loss functions for training a binary classifier with quantum annealing | http://arxiv.org/pdf/1406.4203v1.pdf | author:Ryan Babbush, Vasil Denchev, Nan Ding, Sergei Isakov, Hartmut Neven category:cs.LG quant-ph published:2014-06-17 summary:Quantum annealing is a heuristic quantum algorithm which exploits quantumresources to minimize an objective function embedded as the energy levels of aprogrammable physical system. To take advantage of a potential quantumadvantage, one needs to be able to map the problem of interest to the nativehardware with reasonably low overhead. Because experimental considerationsconstrain our objective function to take the form of a low degree PUBO(polynomial unconstrained binary optimization), we employ non-convex lossfunctions which are polynomial functions of the margin. We show that these lossfunctions are robust to label noise and provide a clear advantage over convexmethods. These loss functions may also be useful for classical approaches asthey compile to regularized risk expressions which can be evaluated in constanttime with respect to the number of training examples.
arxiv-6600-258 | Matroid Bandits: Fast Combinatorial Optimization with Learning | http://arxiv.org/pdf/1403.5045v3.pdf | author:Branislav Kveton, Zheng Wen, Azin Ashkan, Hoda Eydgahi, Brian Eriksson category:cs.LG cs.AI cs.SY stat.ML published:2014-03-20 summary:A matroid is a notion of independence in combinatorial optimization which isclosely related to computational efficiency. In particular, it is well knownthat the maximum of a constrained modular function can be found greedily if andonly if the constraints are associated with a matroid. In this paper, we bringtogether the ideas of bandits and matroids, and propose a new class ofcombinatorial bandits, matroid bandits. The objective in these problems is tolearn how to maximize a modular function on a matroid. This function isstochastic and initially unknown. We propose a practical algorithm for solvingour problem, Optimistic Matroid Maximization (OMM); and prove two upper bounds,gap-dependent and gap-free, on its regret. Both bounds are sublinear in timeand at most linear in all other quantities of interest. The gap-dependent upperbound is tight and we prove a matching lower bound on a partition matroidbandit. Finally, we evaluate our method on three real-world problems and showthat it is practical.
arxiv-6600-259 | Embedded Controlled Languages | http://arxiv.org/pdf/1406.4057v1.pdf | author:Aarne Ranta category:cs.CL published:2014-06-16 summary:Inspired by embedded programming languages, an embedded CNL (controllednatural language) is a proper fragment of an entire natural language (its hostlanguage), but it has a parser that recognizes the entire host language. Thismakes it possible to process out-of-CNL input and give useful feedback tousers, instead of just reporting syntax errors. This extended abstract explainsthe main concepts of embedded CNL implementation in GF (Grammatical Framework),with examples from machine translation and some other ongoing work.
arxiv-6600-260 | Variational Bayesian inference for linear and logistic regression | http://arxiv.org/pdf/1310.5438v2.pdf | author:Jan Drugowitsch category:stat.ML published:2013-10-21 summary:The article describe the model, derivation, and implementation of variationalBayesian inference for linear and logistic regression, both with and withoutautomatic relevance determination. It has the dual function of acting as atutorial for the derivation of variational Bayesian inference for simplemodels, as well as documenting, and providing brief examples for the MATLABfunctions that implement this inference. These functions are freely availableonline.
arxiv-6600-261 | Sparse coding for multitask and transfer learning | http://arxiv.org/pdf/1209.0738v3.pdf | author:Andreas Maurer, Massimiliano Pontil, Bernardino Romera-Paredes category:cs.LG stat.ML published:2012-09-04 summary:We investigate the use of sparse coding and dictionary learning in thecontext of multitask and transfer learning. The central assumption of ourlearning method is that the tasks parameters are well approximated by sparselinear combinations of the atoms of a dictionary on a high or infinitedimensional space. This assumption, together with the large quantity ofavailable data in the multitask and transfer learning settings, allows aprincipled choice of the dictionary. We provide bounds on the generalizationerror of this approach, for both settings. Numerical experiments on onesynthetic and two real datasets show the advantage of our method over singletask learning, a previous method based on orthogonal and dense representationof the tasks and a related method learning task grouping.
arxiv-6600-262 | Impact of Exponent Parameter Value for the Partition Matrix on the Performance of Fuzzy C Means Algorithm | http://arxiv.org/pdf/1406.4007v1.pdf | author:Dibya Jyoti Bora, Anil Kumar Gupta category:cs.CV published:2014-06-16 summary:Soft Clustering plays a very important rule on clustering real world datawhere a data item contributes to more than one cluster. Fuzzy logic basedalgorithms are always suitable for performing soft clustering tasks. Fuzzy CMeans (FCM) algorithm is a very popular fuzzy logic based algorithm. In case offuzzy logic based algorithm, the parameter like exponent for the partitionmatrix that we have to fix for the clustering task plays a very important ruleon the performance of the algorithm. In this paper, an experimental analysis isdone on FCM algorithm to observe the impact of this parameter on theperformance of the algorithm.
arxiv-6600-263 | Blind Deconvolution with Non-local Sparsity Reweighting | http://arxiv.org/pdf/1311.4029v2.pdf | author:Dilip Krishnan, Joan Bruna, Rob Fergus category:cs.CV published:2013-11-16 summary:Blind deconvolution has made significant progress in the past decade. Mostsuccessful algorithms are classified either as Variational or Maximuma-Posteriori ($MAP$). In spite of the superior theoretical justification ofvariational techniques, carefully constructed $MAP$ algorithms have provenequally effective in practice. In this paper, we show that all successful $MAP$and variational algorithms share a common framework, relying on the followingkey principles: sparsity promotion in the gradient domain, $l_2$ regularizationfor kernel estimation, and the use of convex (often quadratic) cost functions.Our observations lead to a unified understanding of the principles required forsuccessful blind deconvolution. We incorporate these principles into a novelalgorithm that improves significantly upon the state of the art.
arxiv-6600-264 | Towards an Error Correction Memory to Enhance Technical Texts Authoring in LELIE | http://arxiv.org/pdf/1406.3987v1.pdf | author:Juyeon Kang, Patrick Saint Dizier category:cs.CL published:2014-06-16 summary:In this paper, we investigate and experiment the notion of error correctionmemory applied to error correction in technical texts. The main purpose is toinduce relatively generic correction patterns associated with more contextualcorrection recommendations, based on previously memorized and analyzedcorrections. The notion of error correction memory is developed within theframework of the LELIE project and illustrated on the case of fuzzy lexicalitems, which is a major problem in technical texts.
arxiv-6600-265 | Handling non-compositionality in multilingual CNLs | http://arxiv.org/pdf/1406.3976v1.pdf | author:Ramona Enache, Inari Listenmaa, Prasanth Kolachina category:cs.CL published:2014-06-16 summary:In this paper, we describe methods for handling multilingualnon-compositional constructions in the framework of GF. We specifically look atmethods to detect and extract non-compositional phrases from parallel texts andpropose methods to handle such constructions in GF grammars. We expect that themethods to handle non-compositional constructions will enrich CNLs by providingmore flexibility in the design of controlled languages. We look at two specificuse cases of non-compositional constructions: a general-purpose method todetect and extract multilingual multiword expressions and a procedure toidentify nominal compounds in German. We evaluate our procedure for multiwordexpressions by performing a qualitative analysis of the results. For theexperiments on nominal compounds, we incorporate the detected compounds in afull SMT pipeline and evaluate the impact of our method in machine translationprocess.
arxiv-6600-266 | Translation Of Telugu-Marathi and Vice-Versa using Rule Based Machine Translation | http://arxiv.org/pdf/1406.3969v1.pdf | author:Siddhartha Ghosh, Sujata Thamke, Kalyani U. R. S category:cs.CL published:2014-06-16 summary:In todays digital world automated Machine Translation of one language toanother has covered a long way to achieve different kinds of success stories.Whereas Babel Fish supports a good number of foreign languages and only Hindifrom Indian languages, the Google Translator takes care of about 10 Indianlanguages. Though most of the Automated Machine Translation Systems are doingwell but handling Indian languages needs a major care while handling the localproverbs/ idioms. Most of the Machine Translation system follows the directtranslation approach while translating one Indian language to other. Ourresearch at KMIT R&D Lab found that handling the local proverbs/idioms is notgiven enough attention by the earlier research work. This paper focuses on twoof the majorly spoken Indian languages Marathi and Telugu, and translationbetween them. Handling proverbs and idioms of both the languages have beengiven a special care, and the research outcome shows a significant achievementin this direction.
arxiv-6600-267 | A Fusion of Labeled-Grid Shape Descriptors with Weighted Ranking Algorithm for Shapes Recognition | http://arxiv.org/pdf/1406.3949v1.pdf | author:Jamil Ahmad, Zahoor Jan, Zia-ud-Din, Shoaib Muhammad Khan category:cs.CV published:2014-06-16 summary:Retrieving similar images from a large dataset based on the image content hasbeen a very active research area and is a very challenging task. Studies haveshown that retrieving similar images based on their shape is a very effectivemethod. For this purpose a large number of methods exist in literature. Thecombination of more than one feature has also been investigated for thispurpose and has shown promising results. In this paper a fusion based shapesrecognition method has been proposed. A set of local boundary based and regionbased features are derived from the labeled grid based representation of theshape and are combined with a few global shape features to produce a compositeshape descriptor. This composite shape descriptor is then used in a weightedranking algorithm to find similarities among shapes from a large dataset. Theexperimental analysis has shown that the proposed method is powerful enough todiscriminate the geometrically similar shapes from the non-similar ones.
arxiv-6600-268 | Bayesian Optimal Control of Smoothly Parameterized Systems: The Lazy Posterior Sampling Algorithm | http://arxiv.org/pdf/1406.3926v1.pdf | author:Yasin Abbasi-Yadkori, Csaba Szepesvari category:cs.LG stat.ML published:2014-06-16 summary:We study Bayesian optimal control of a general class of smoothlyparameterized Markov decision problems. Since computing the optimal control iscomputationally expensive, we design an algorithm that trades off performancefor computational efficiency. The algorithm is a lazy posterior sampling methodthat maintains a distribution over the unknown parameter. The algorithm changesits policy only when the variance of the distribution is reduced sufficiently.Importantly, we analyze the algorithm and show the precise nature of theperformance vs. computation tradeoff. Finally, we show the effectiveness of themethod on a web server control application.
arxiv-6600-269 | A Bengali HMM Based Speech Synthesis System | http://arxiv.org/pdf/1406.3915v1.pdf | author:Sankar Mukherjee, Shyamal Kumar Das Mandal category:cs.SD cs.CL cs.MM published:2014-06-16 summary:The paper presents the capability of an HMM-based TTS system to produceBengali speech. In this synthesis method, trajectories of speech parameters aregenerated from the trained Hidden Markov Models. A final speech waveform issynthesized from those speech parameters. In our experiments, spectralproperties were represented by Mel Cepstrum Coefficients. Both the training andsynthesis issues are investigated in this paper using annotated Bengali speechdatabase. Experimental evaluation depicts that the developed text-to-speechsystem is capable of producing adequately natural speech in terms ofintelligibility and intonation for Bengali.
arxiv-6600-270 | Human-Machine CRFs for Identifying Bottlenecks in Holistic Scene Understanding | http://arxiv.org/pdf/1406.3906v1.pdf | author:Roozbeh Mottaghi, Sanja Fidler, Alan Yuille, Raquel Urtasun, Devi Parikh category:cs.CV published:2014-06-16 summary:Recent trends in image understanding have pushed for holistic sceneunderstanding models that jointly reason about various tasks such as objectdetection, scene recognition, shape analysis, contextual reasoning, and localappearance based classifiers. In this work, we are interested in understandingthe roles of these different tasks in improved scene understanding, inparticular semantic segmentation, object detection and scene recognition.Towards this goal, we "plug-in" human subjects for each of the variouscomponents in a state-of-the-art conditional random field model. Comparisonsamong various hybrid human-machine CRFs give us indications of how much "headroom" there is to improve scene understanding by focusing research efforts onvarious individual tasks.
arxiv-6600-271 | Freeze-Thaw Bayesian Optimization | http://arxiv.org/pdf/1406.3896v1.pdf | author:Kevin Swersky, Jasper Snoek, Ryan Prescott Adams category:stat.ML cs.LG published:2014-06-16 summary:In this paper we develop a dynamic form of Bayesian optimization for machinelearning models with the goal of rapidly finding good hyperparameter settings.Our method uses the partial information gained during the training of a machinelearning model in order to decide whether to pause training and start a newmodel, or resume the training of a previously-considered model. We specificallytailor our method to machine learning problems by developing a novelpositive-definite covariance kernel to capture a variety of training curves.Furthermore, we develop a Gaussian process prior that scales gracefully withadditional temporal observations. Finally, we provide an information-theoreticframework to automate the decision process. Experiments on several commonmachine learning models show that our approach is extremely effective inpractice.
arxiv-6600-272 | The Laplacian K-modes algorithm for clustering | http://arxiv.org/pdf/1406.3895v1.pdf | author:Weiran Wang, Miguel Á. Carreira-Perpiñán category:cs.LG stat.ME stat.ML published:2014-06-16 summary:In addition to finding meaningful clusters, centroid-based clusteringalgorithms such as K-means or mean-shift should ideally find centroids that arevalid patterns in the input space, representative of data in their cluster.This is challenging with data having a nonconvex or manifold structure, as withimages or text. We introduce a new algorithm, Laplacian K-modes, whichnaturally combines three powerful ideas in clustering: the explicit use ofassignment variables (as in K-means); the estimation of cluster centroids whichare modes of each cluster's density estimate (as in mean-shift); and theregularizing effect of the graph Laplacian, which encourages similarassignments for nearby points (as in spectral clustering). The optimizationalgorithm alternates an assignment step, which is a convex quadratic program,and a mean-shift step, which separates for each cluster centroid. The algorithmfinds meaningful density estimates for each cluster, even with challengingproblems where the clusters have manifold structure, are highly nonconvex or inhigh dimension. It also provides centroids that are valid patterns, trulyrepresentative of their cluster (unlike K-means), and an out-of-sample mappingthat predicts soft assignments for a new point.
arxiv-6600-273 | Tenacious tagging of images via Mellin monomials | http://arxiv.org/pdf/1208.5842v5.pdf | author:Kieran G. Larkin, Peter A. Fletcher, Stephen J. Hardy category:cs.CV math.CA published:2012-08-29 summary:We describe a method for attaching persistent metadata to an image. Themethod can be interpreted as a template-based blind watermarking scheme, robustto common editing operations, namely: cropping, rotation, scaling, stretching,shearing, compression, printing, scanning, noise, and color removal. Robustnessis achieved through the reciprocity of the embedding and detection invariants.The embedded patterns are real onedimensional Mellin monomial patternsdistributed over two-dimensions. The embedded patterns are scale invariant andcan be directly embedded in an image by simple pixel addition. Detectionachieves rotation and general affine invariance by signal projection usingimplicit Radon transformation. Embedded signals contract to one-dimension inthe two-dimensional Fourier polar domain. The real signals are detected bycorrelation with complex Mellin monomial templates. Using a unique template of4 chirp patterns we detect the affine signature with exquisite sensitivity andmoderate security. The practical implementation achieves efficiencies throughfast Fourier transform (FFT) correspondences such as the projection-slicetheorem, the FFT correlation relation, and fast resampling via the chirp-ztransform. The overall method utilizes orthodox spread spectrum patterns forthe payload and performs well in terms of the classicrobustness-capacity-visibility performance triangle. Tags are entirelyimperceptible with a mean SSIM greater than 0.988 in all cases tested.Watermarked images survive almost all Stirmark attacks. The method is ideal forattaching metadata robustly to both digital and analogue images.
arxiv-6600-274 | Learning An Invariant Speech Representation | http://arxiv.org/pdf/1406.3884v1.pdf | author:Georgios Evangelopoulos, Stephen Voinea, Chiyuan Zhang, Lorenzo Rosasco, Tomaso Poggio category:cs.SD cs.LG published:2014-06-16 summary:Recognition of speech, and in particular the ability to generalize and learnfrom small sets of labelled examples like humans do, depends on an appropriaterepresentation of the acoustic input. We formulate the problem of findingrobust speech features for supervised learning with small sample complexity asa problem of learning representations of the signal that are maximallyinvariant to intraclass transformations and deformations. We propose anextension of a theory for unsupervised learning of invariant visualrepresentations to the auditory domain and empirically evaluate its validityfor voiced speech sound classification. Our version of the theory requires thememory-based, unsupervised storage of acoustic templates -- such as specificphones or words -- together with all the transformations of each that normallyoccur. A quasi-invariant representation for a speech segment can be obtained byprojecting it to each template orbit, i.e., the set of transformed signals, andcomputing the associated one-dimensional empirical probability distributions.The computations can be performed by modules of filtering and pooling, andextended to hierarchical architectures. In this paper, we apply a single-layer,multicomponent representation for phonemes and demonstrate improved accuracyand decreased sample complexity for vowel classification compared to standardspectral, cepstral and perceptual features.
arxiv-6600-275 | Human language reveals a universal positivity bias | http://arxiv.org/pdf/1406.3855v1.pdf | author:Peter Sheridan Dodds, Eric M. Clark, Suma Desu, Morgan R. Frank, Andrew J. Reagan, Jake Ryland Williams, Lewis Mitchell, Kameron Decker Harris, Isabel M. Kloumann, James P. Bagrow, Karine Megerdoomian, Matthew T. McMahon, Brian F. Tivnan, Christopher M. Danforth category:physics.soc-ph cs.CL cs.SI published:2014-06-15 summary:Using human evaluation of 100,000 words spread across 24 corpora in 10languages diverse in origin and culture, we present evidence of a deep imprintof human sociality in language, observing that (1) the words of natural humanlanguage possess a universal positivity bias; (2) the estimated emotionalcontent of words is consistent between languages under translation; and (3)this positivity bias is strongly independent of frequency of word usage.Alongside these general regularities, we describe inter-language variations inthe emotional spectrum of languages which allow us to rank corpora. We alsoshow how our word evaluations can be used to construct physical-likeinstruments for both real-time and offline measurement of the emotional contentof large-scale texts.
arxiv-6600-276 | Semi-Separable Hamiltonian Monte Carlo for Inference in Bayesian Hierarchical Models | http://arxiv.org/pdf/1406.3843v1.pdf | author:Yichuan Zhang, Charles Sutton category:stat.CO cs.AI cs.LG published:2014-06-15 summary:Sampling from hierarchical Bayesian models is often difficult for MCMCmethods, because of the strong correlations between the model parameters andthe hyperparameters. Recent Riemannian manifold Hamiltonian Monte Carlo (RMHMC)methods have significant potential advantages in this setting, but arecomputationally expensive. We introduce a new RMHMC method, which we callsemi-separable Hamiltonian Monte Carlo, which uses a specially designed massmatrix that allows the joint Hamiltonian over model parameters andhyperparameters to decompose into two simpler Hamiltonians. This structure isexploited by a new integrator which we call the alternating blockwise leapfrogalgorithm. The resulting method can mix faster than simpler Gibbs samplingwhile being simpler and more efficient than previous instances of RMHMC.
arxiv-6600-277 | Optimal Resource Allocation with Semi-Bandit Feedback | http://arxiv.org/pdf/1406.3840v1.pdf | author:Tor Lattimore, Koby Crammer, Csaba Szepesvári category:cs.LG published:2014-06-15 summary:We study a sequential resource allocation problem involving a fixed number ofrecurring jobs. At each time-step the manager should distribute availableresources among the jobs in order to maximise the expected number of completedjobs. Allocating more resources to a given job increases the probability thatit completes, but with a cut-off. Specifically, we assume a linear model wherethe probability increases linearly until it equals one, after which allocatingadditional resources is wasteful. We assume the difficulty of each job isunknown and present the first algorithm for this problem and prove upper andlower bounds on its regret. Despite its apparent simplicity, the problem has arich structure: we show that an appropriate optimistic algorithm can improveits learning speed dramatically beyond the results one normally expects forsimilar problems as the problem becomes resource-laden.
arxiv-6600-278 | An Incremental Reseeding Strategy for Clustering | http://arxiv.org/pdf/1406.3837v1.pdf | author:Xavier Bresson, Huiyi Hu, Thomas Laurent, Arthur Szlam, James von Brecht category:stat.ML cs.LG published:2014-06-15 summary:In this work we propose a simple and easily parallelizable algorithm formultiway graph partitioning. The algorithm alternates between three basiccomponents: diffusing seed vertices over the graph, thresholding the diffusedseeds, and then randomly reseeding the thresholded clusters. We demonstrateexperimentally that the proper combination of these ingredients leads to analgorithm that achieves state-of-the-art performance in terms of cluster purityon standard benchmarks datasets. Moreover, the algorithm runs an order ofmagnitude faster than the other algorithms that achieve comparable results interms of accuracy. We also describe a coarsen, cluster and refine approachsimilar to GRACLUS and METIS that removes an additional order of magnitude fromthe runtime of our algorithm while still maintaining competitive accuracy.
arxiv-6600-279 | Modelling, Visualising and Summarising Documents with a Single Convolutional Neural Network | http://arxiv.org/pdf/1406.3830v1.pdf | author:Misha Denil, Alban Demiraj, Nal Kalchbrenner, Phil Blunsom, Nando de Freitas category:cs.CL cs.LG stat.ML published:2014-06-15 summary:Capturing the compositional process which maps the meaning of words to thatof documents is a central challenge for researchers in Natural LanguageProcessing and Information Retrieval. We introduce a model that is able torepresent the meaning of documents by embedding them in a low dimensionalvector space, while preserving distinctions of word and sentence order crucialfor capturing nuanced semantics. Our model is based on an extended DynamicConvolution Neural Network, which learns convolution filters at both thesentence and document level, hierarchically learning to capture and compose lowlevel lexical features into high level semantic concepts. We demonstrate theeffectiveness of this model on a range of document modelling tasks, achievingstrong results with no feature engineering and with a more compact model.Inspired by recent advances in visualising deep convolution networks forcomputer vision, we present a novel visualisation technique for our documentnetworks which not only provides insight into their learning process, but alsocan be interpreted to produce a compelling automatic summarisation system fortexts.
arxiv-6600-280 | A Heuristic Method to Generate Better Initial Population for Evolutionary Methods | http://arxiv.org/pdf/1406.4518v1.pdf | author:Erfan Khaji, Amin Satlikh Mohammadi category:cs.NE published:2014-06-15 summary:Initial population plays an important role in heuristic algorithms such as GAas it help to decrease the time those algorithms need to achieve an acceptableresult. Furthermore, it may influence the quality of the final answer given byevolutionary algorithms. In this paper, we shall introduce a heuristic methodto generate a target based initial population which possess two mentionedcharacteristics. The efficiency of the proposed method has been shown bypresenting the results of our tests on the benchmarks.
arxiv-6600-281 | Simultaneous Model Selection and Optimization through Parameter-free Stochastic Learning | http://arxiv.org/pdf/1406.3816v1.pdf | author:Francesco Orabona category:cs.LG stat.ML published:2014-06-15 summary:Stochastic gradient descent algorithms for training linear and kernelpredictors are gaining more and more importance, thanks to their scalability.While various methods have been proposed to speed up their convergence, themodel selection phase is often ignored. In fact, in theoretical works most ofthe time assumptions are made, for example, on the prior knowledge of the normof the optimal solution, while in the practical world validation methods remainthe only viable approach. In this paper, we propose a new kernel-basedstochastic gradient descent algorithm that performs model selection whiletraining, with no parameters to tune, nor any form of cross-validation. Thealgorithm builds on recent advancement in online learning theory forunconstrained settings, to estimate over time the right regularization in adata-dependent way. Optimal rates of convergence are proved under standardsmoothness assumptions on the target function, using the range space of thefractional integral operator associated with the kernel.
arxiv-6600-282 | Neural tuning size is a key factor underlying holistic face processing | http://arxiv.org/pdf/1406.3793v1.pdf | author:Cheston Tan, Tomaso Poggio category:cs.AI cs.CV cs.NE q-bio.NC published:2014-06-15 summary:Faces are a class of visual stimuli with unique significance, for a varietyof reasons. They are ubiquitous throughout the course of a person's life, andface recognition is crucial for daily social interaction. Faces are also unlikeany other stimulus class in terms of certain physical stimulus characteristics.Furthermore, faces have been empirically found to elicit certain characteristicbehavioral phenomena, which are widely held to be evidence of "holistic"processing of faces. However, little is known about the neural mechanismsunderlying such holistic face processing. In other words, for the processing offaces by the primate visual system, the input and output characteristics arerelatively well known, but the internal neural computations are not. The mainaim of this work is to further the fundamental understanding of what causes thevisual processing of faces to be different from that of objects. In thiscomputational modeling work, we show that a single factor - "neural tuningsize" - is able to account for three key phenomena that are characteristic offace processing, namely the Composite Face Effect (CFE), Face Inversion Effect(FIE) and Whole-Part Effect (WPE). Our computational proof-of-principleprovides specific neural tuning properties that correspond to thepoorly-understood notion of holistic face processing, and connects these neuralproperties to psychophysical behavior. Overall, our work provides a unified andparsimonious theoretical account for the disparate empirical data onface-specific processing, deepening the fundamental understanding of faceprocessing.
arxiv-6600-283 | Interval Forecasting of Electricity Demand: A Novel Bivariate EMD-based Support Vector Regression Modeling Framework | http://arxiv.org/pdf/1406.3792v1.pdf | author:Tao Xiong, Yukun Bao, Zhongyi Hu category:cs.LG stat.AP published:2014-06-15 summary:Highly accurate interval forecasting of electricity demand is fundamental tothe success of reducing the risk when making power system planning andoperational decisions by providing a range rather than point estimation. Inthis study, a novel modeling framework integrating bivariate empirical modedecomposition (BEMD) and support vector regression (SVR), extended from thewell-established empirical mode decomposition (EMD) based time series modelingframework in the energy demand forecasting literature, is proposed for intervalforecasting of electricity demand. The novelty of this study arises from theemployment of BEMD, a new extension of classical empirical model decomposition(EMD) destined to handle bivariate time series treated as complex-valued timeseries, as decomposition method instead of classical EMD only capable ofdecomposing one-dimensional single-valued time series. This proposed modelingframework is endowed with BEMD to decompose simultaneously both the lower andupper bounds time series, constructed in forms of complex-valued time series,of electricity demand on a monthly per hour basis, resulting in capturing thepotential interrelationship between lower and upper bounds. The proposedmodeling framework is justified with monthly interval-valued electricity demanddata per hour in Pennsylvania-New Jersey-Maryland Interconnection, indicatingit as a promising method for interval-valued electricity demand forecasting.
arxiv-6600-284 | Detection Bank: An Object Detection Based Video Representation for Multimedia Event Recognition | http://arxiv.org/pdf/1405.7102v2.pdf | author:Tim Althoff, Hyun Oh Song, Trevor Darrell category:cs.MM cs.CV published:2014-05-28 summary:While low-level image features have proven to be effective representationsfor visual recognition tasks such as object recognition and sceneclassification, they are inadequate to capture complex semantic meaningrequired to solve high-level visual tasks such as multimedia event detectionand recognition. Recognition or retrieval of events and activities can beimproved if specific discriminative objects are detected in a video sequence.In this paper, we propose an image representation, called Detection Bank, basedon the detection images from a large number of windowed object detectors wherean image is represented by different statistics derived from these detections.This representation is extended to video by aggregating the key frame levelimage representations through mean and max pooling. We empirically show that itcaptures complementary information to state-of-the-art representations such asSpatial Pyramid Matching and Object Bank. These descriptors combined with ourDetection Bank representation significantly outperforms any of therepresentations alone on TRECVID MED 2011 data.
arxiv-6600-285 | Evaluation of Machine Learning Techniques for Green Energy Prediction | http://arxiv.org/pdf/1406.3726v1.pdf | author:Ankur Sahai category:cs.LG published:2014-06-14 summary:We evaluate the following Machine Learning techniques for Green Energy (Wind,Solar) Prediction: Bayesian Inference, Neural Networks, Support VectorMachines, Clustering techniques (PCA). Our objective is to predict green energyusing weather forecasts, predict deviations from forecast green energy, findcorrelation amongst different weather parameters and green energy availability,recover lost or missing energy (/ weather) data. We use historical weather dataand weather forecasts for the same.
arxiv-6600-286 | Mining of product reviews at aspect level | http://arxiv.org/pdf/1406.3714v1.pdf | author:Richa Sharma, Shweta Nigam, Rekha Jain category:cs.CL cs.IR published:2014-06-14 summary:Todays world is a world of Internet, almost all work can be done with thehelp of it, from simple mobile phone recharge to biggest business deals can bedone with the help of this technology. People spent their most of the times onsurfing on the Web it becomes a new source of entertainment, education,communication, shopping etc. Users not only use these websites but also givetheir feedback and suggestions that will be useful for other users. In this waya large amount of reviews of users are collected on the Web that needs to beexplored, analyse and organized for better decision making. Opinion Mining orSentiment Analysis is a Natural Language Processing and Information Extractiontask that identifies the users views or opinions explained in the form ofpositive, negative or neutral comments and quotes underlying the text. Aspectbased opinion mining is one of the level of Opinion mining that determines theaspect of the given reviews and classify the review for each feature. In thispaper an aspect based opinion mining system is proposed to classify the reviewsas positive, negative and neutral for each feature. Negation is also handled inthe proposed system. Experimental results using reviews of products show theeffectiveness of the system.
arxiv-6600-287 | Dimensionality reduction for time series data | http://arxiv.org/pdf/1406.3711v1.pdf | author:Diego Vidaurre, Iead Rezek, Samuel L. Harrison, Stephen S. Smith, Mark Woolrich category:stat.ML published:2014-06-14 summary:Despite the fact that they do not consider the temporal nature of data,classic dimensionality reduction techniques, such as PCA, are widely applied totime series data. In this paper, we introduce a factor decomposition specificfor time series that builds upon the Bayesian multivariate autoregressive modeland hence evades the assumption that data points are mutually independent. Thekey is to find a low-rank estimation of the autoregressive matrices. As in theprobabilistic version of other factor models, this induces a latentlow-dimensional representation of the original data. We discuss some possiblegeneralisations and alternatives, with the most relevant being a technique forsimultaneous smoothing and dimensionality reduction. To illustrate thepotential applications, we apply the model on a synthetic data set anddifferent types of neuroimaging data (EEG and ECoG).
arxiv-6600-288 | Analyzing Social and Stylometric Features to Identify Spear phishing Emails | http://arxiv.org/pdf/1406.3692v1.pdf | author:Prateek Dewan, Anand Kashyap, Ponnurangam Kumaraguru category:cs.CY cs.LG cs.SI published:2014-06-14 summary:Spear phishing is a complex targeted attack in which, an attacker harvestsinformation about the victim prior to the attack. This information is then usedto create sophisticated, genuine-looking attack vectors, drawing the victim tocompromise confidential information. What makes spear phishing different, andmore powerful than normal phishing, is this contextual information about thevictim. Online social media services can be one such source for gathering vitalinformation about an individual. In this paper, we characterize and examine atrue positive dataset of spear phishing, spam, and normal phishing emails fromSymantec's enterprise email scanning service. We then present a model to detectspear phishing emails sent to employees of 14 international organizations, byusing social features extracted from LinkedIn. Our dataset consists of 4,742targeted attack emails sent to 2,434 victims, and 9,353 non targeted attackemails sent to 5,912 non victims; and publicly available information from theirLinkedIn profiles. We applied various machine learning algorithms to thislabeled data, and achieved an overall maximum accuracy of 97.76% in identifyingspear phishing emails. We used a combination of social features from LinkedInprofiles, and stylometric features extracted from email subjects, bodies, andattachments. However, we achieved a slightly better accuracy of 98.28% withoutthe social features. Our analysis revealed that social features extracted fromLinkedIn do not help in identifying spear phishing emails. To the best of ourknowledge, this is one of the first attempts to make use of a combination ofstylometric features extracted from emails, and social features extracted froman online social network to detect targeted spear phishing emails.
arxiv-6600-289 | Small Sample Learning of Superpixel Classifiers for EM Segmentation- Extended Version | http://arxiv.org/pdf/1406.1774v2.pdf | author:Toufiq Parag, Stephen Plaza, Louis Scheffer category:cs.CV published:2014-06-06 summary:Pixel and superpixel classifiers have become essential tools for EMsegmentation algorithms. Training these classifiers remains a major bottleneckprimarily due to the requirement of completely annotating the dataset which istedious, error-prone and costly. In this paper, we propose an interactivelearning scheme for the superpixel classifier for EM segmentation. Ouralgorithm is "active semi-supervised" because it requests the labels of a smallnumber of examples from user and applies label propagation technique togenerate these queries. Using only a small set ($<20\%$) of all datapoints, theproposed algorithm consistently generates a classifier almost as accurate asthat estimated from a complete groundtruth. We provide segmentation results onmultiple datasets to show the strength of these classifiers.
arxiv-6600-290 | Quaternion Gradient and Hessian | http://arxiv.org/pdf/1406.3587v1.pdf | author:Dongpo Xu, Danilo P. Mandic category:math.NA cs.LG published:2014-06-13 summary:The optimization of real scalar functions of quaternion variables, such asthe mean square error or array output power, underpins many practicalapplications. Solutions often require the calculation of the gradient andHessian, however, real functions of quaternion variables are essentiallynon-analytic. To address this issue, we propose new definitions of quaterniongradient and Hessian, based on the novel generalized HR (GHR) calculus, thusmaking possible efficient derivation of optimization algorithms directly in thequaternion field, rather than transforming the problem to the real domain, asis current practice. In addition, unlike the existing quaternion gradients, theGHR calculus allows for the product and chain rule, and for a one-to-onecorrespondence of the proposed quaternion gradient and Hessian with their realcounterparts. Properties of the quaternion gradient and Hessian relevant tonumerical applications are elaborated, and the results illuminate theusefulness of the GHR calculus in greatly simplifying the derivation of thequaternion least mean squares, and in quaternion least square and Newtonalgorithm. The proposed gradient and Hessian are also shown to enable the samegeneric forms as the corresponding real- and complex-valued algorithms, furtherillustrating the advantages in algorithm design and evaluation.
arxiv-6600-291 | EigenEvent: An Algorithm for Event Detection from Complex Data Streams in Syndromic Surveillance | http://arxiv.org/pdf/1406.3496v1.pdf | author:Hadi Fanaee-T, João Gama category:cs.AI cs.LG stat.AP published:2014-06-13 summary:Syndromic surveillance systems continuously monitor multiple pre-diagnosticdaily streams of indicators from different regions with the aim of earlydetection of disease outbreaks. The main objective of these systems is todetect outbreaks hours or days before the clinical and laboratory confirmation.The type of data that is being generated via these systems is usuallymultivariate and seasonal with spatial and temporal dimensions. The algorithmWhat's Strange About Recent Events (WSARE) is the state-of-the-art method forsuch problems. It exhaustively searches for contrast sets in the multivariatedata and signals an alarm when find statistically significant rules. Thisbottom-up approach presents a much lower detection delay comparing the existingtop-down approaches. However, WSARE is very sensitive to the small-scalechanges and subsequently comes with a relatively high rate of false alarms. Wepropose a new approach called EigenEvent that is neither fully top-down norbottom-up. In this method, we instead of top-down or bottom-up search, trackchanges in data correlation structure via eigenspace techniques. This newmethodology enables us to detect both overall changes (via eigenvalue) anddimension-level changes (via eigenvectors). Experimental results on hundredsets of benchmark data reveals that EigenEvent presents a better overallperformance comparing state-of-the-art, in particular in terms of the falsealarm rate.
arxiv-6600-292 | Heterogeneous Multi-task Learning for Human Pose Estimation with Deep Convolutional Neural Network | http://arxiv.org/pdf/1406.3474v1.pdf | author:Sijin Li, Zhi-Qiang Liu, Antoni B. Chan category:cs.CV cs.LG cs.NE published:2014-06-13 summary:We propose an heterogeneous multi-task learning framework for human poseestimation from monocular image with deep convolutional neural network. Inparticular, we simultaneously learn a pose-joint regressor and a sliding-windowbody-part detector in a deep network architecture. We show that including thebody-part detection task helps to regularize the network, directing it toconverge to a good solution. We report competitive and state-of-art results onseveral data sets. We also empirically show that the learned neurons in themiddle layer of our network are tuned to localized body parts.
arxiv-6600-293 | Are Style Guides Controlled Languages? The Case of Koenig & Bauer AG | http://arxiv.org/pdf/1406.3460v1.pdf | author:Karolina Suchowolec category:cs.CL published:2014-06-13 summary:Controlled natural languages for industrial application are often regarded asa response to the challenges of translation and multilingual communication.This paper presents a quite different approach taken by Koenig & Bauer AG,where the main goal was the improvement of the authoring process for technicaldocumentation. Most importantly, this paper explores the notion of a controlledlanguage and demonstrates how style guides can emerge from non-linguisticconsiderations. Moreover, it shows the transition from loose languagerecommendations into precise and prescriptive rules and investigates whethersuch rules can be regarded as a full-fledged controlled language.
arxiv-6600-294 | Optimality of Graphlet Screening in High Dimensional Variable Selection | http://arxiv.org/pdf/1204.6452v2.pdf | author:Jiashun Jin, Cun-Hui Zhang, Qi Zhang category:math.ST stat.ME stat.ML stat.TH published:2012-04-29 summary:Consider a linear regression model where the design matrix X has n rows and pcolumns. We assume (a) p is much large than n, (b) the coefficient vector betais sparse in the sense that only a small fraction of its coordinates isnonzero, and (c) the Gram matrix G = X'X is sparse in the sense that each rowhas relatively few large coordinates (diagonals of G are normalized to 1). The sparsity in G naturally induces the sparsity of the so-called graph ofstrong dependence (GOSD). We find an interesting interplay between the signalsparsity and the graph sparsity, which ensures that in a broad context, the setof true signals decompose into many different small-size components of GOSD,where different components are disconnected. We propose Graphlet Screening (GS) as a new approach to variable selection,which is a two-stage Screen and Clean method. The key methodological innovationof GS is to use GOSD to guide both the screening and cleaning. Compared tom-variate brute-forth screening that has a computational cost of p^m, the GSonly has a computational cost of p (up to some multi-log(p) factors) inscreening. We measure the performance of any variable selection procedure by the minimaxHamming distance. We show that in a very broad class of situations, GS achievesthe optimal rate of convergence in terms of the Hamming distance. Somewhatsurprisingly, the well-known procedures subset selection and the lasso are ratenon-optimal, even in very simple settings and even when their tuning parametersare ideally set.
arxiv-6600-295 | Fingers' Angle Calculation using Level-Set Method | http://arxiv.org/pdf/1406.3418v1.pdf | author:Ankit Chaudhary, J. L. Raheja, K. Das, S. Raheja category:cs.CV published:2014-06-13 summary:In the current age, use of natural communication in human computerinteraction is a known and well installed thought. Hand gesture recognition andgesture based applications has gained a significant amount of popularityamongst people all over the world. It has a number of applications ranging fromsecurity to entertainment. These applications generally are real timeapplications and need fast, accurate communication with machines. On the otherend, gesture based communications have few limitations also like bent fingerinformation is not provided in vision based techniques. In this paper, a novelmethod for fingertip detection and for angle calculation of both hands bentfingers is discussed. Angle calculation has been done before with sensor basedgloves/devices. This study has been conducted in the context of naturalcomputing for calculating angles without using any wired equipment, colors,marker or any device. The pre-processing and segmentation of the region ofinterest is performed in a HSV color space and a binary format respectively.Fingertips are detected using level-set method and angles were calculated usinggeometrical analysis. This technique requires no training for system to performthe task.
arxiv-6600-296 | Dual coordinate solvers for large-scale structural SVMs | http://arxiv.org/pdf/1312.1743v2.pdf | author:Deva Ramanan category:cs.LG cs.CV published:2013-12-06 summary:This manuscript describes a method for training linear SVMs (including binarySVMs, SVM regression, and structural SVMs) from large, out-of-core trainingdatasets. Current strategies for large-scale learning fall into one of twocamps; batch algorithms which solve the learning problem given a finitedatasets, and online algorithms which can process out-of-core datasets. Theformer typically requires datasets small enough to fit in memory. The latter isoften phrased as a stochastic optimization problem; such algorithms enjoystrong theoretical properties but often require manual tuned annealingschedules, and may converge slowly for problems with large output spaces (e.g.,structural SVMs). We discuss an algorithm for an "intermediate" regime in whichthe data is too large to fit in memory, but the active constraints (supportvectors) are small enough to remain in memory. In this case, one can designrather efficient learning algorithms that are as stable as batch algorithms,but capable of processing out-of-core datasets. We have developed such aMATLAB-based solver and used it to train a collection of recognition systemsfor articulated pose estimation, facial analysis, 3D object recognition, andaction classification, all with publicly-available code. This writeup describesthe solver in detail.
arxiv-6600-297 | Kernel Adaptive Metropolis-Hastings | http://arxiv.org/pdf/1307.5302v3.pdf | author:Dino Sejdinovic, Heiko Strathmann, Maria Lomeli Garcia, Christophe Andrieu, Arthur Gretton category:stat.ML cs.LG published:2013-07-19 summary:A Kernel Adaptive Metropolis-Hastings algorithm is introduced, for thepurpose of sampling from a target distribution with strongly nonlinear support.The algorithm embeds the trajectory of the Markov chain into a reproducingkernel Hilbert space (RKHS), such that the feature space covariance of thesamples informs the choice of proposal. The procedure is computationallyefficient and straightforward to implement, since the RKHS moves can beintegrated out analytically: our proposal distribution in the original space isa normal distribution whose mean and covariance depend on where the currentsample lies in the support of the target distribution, and adapts to its localcovariance structure. Furthermore, the procedure requires neither gradients norany other higher order information about the target, making it particularlyattractive for contexts such as Pseudo-Marginal MCMC. Kernel AdaptiveMetropolis-Hastings outperforms competing fixed and adaptive samplers onmultivariate, highly nonlinear target distributions, arising in both real-worldand synthetic examples. Code may be downloaded athttps://github.com/karlnapf/kameleon-mcmc.
arxiv-6600-298 | Tripartite Graph Clustering for Dynamic Sentiment Analysis on Social Media | http://arxiv.org/pdf/1402.6010v3.pdf | author:Linhong Zhu, Aram Galstyan, James Cheng, Kristina Lerman category:cs.SI cs.CL cs.IR published:2014-02-24 summary:The growing popularity of social media (e.g, Twitter) allows users to easilyshare information with each other and influence others by expressing their ownsentiments on various subjects. In this work, we propose an unsupervised\emph{tri-clustering} framework, which analyzes both user-level and tweet-levelsentiments through co-clustering of a tripartite graph. A compelling feature ofthe proposed framework is that the quality of sentiment clustering of tweets,users, and features can be mutually improved by joint clustering. We furtherinvestigate the evolution of user-level sentiments and latent feature vectorsin an online framework and devise an efficient online algorithm to sequentiallyupdate the clustering of tweets, users and features with newly arrived data.The online framework not only provides better quality of both dynamicuser-level and tweet-level sentiment analysis, but also improves thecomputational and storage efficiency. We verified the effectiveness andefficiency of the proposed approaches on the November 2012 California ballotTwitter data.
arxiv-6600-299 | The Secrets of Salient Object Segmentation | http://arxiv.org/pdf/1406.2807v2.pdf | author:Yin Li, Xiaodi Hou, Christof Koch, James M. Rehg, Alan L. Yuille category:cs.CV published:2014-06-11 summary:In this paper we provide an extensive evaluation of fixation prediction andsalient object segmentation algorithms as well as statistics of major datasets.Our analysis identifies serious design flaws of existing salient objectbenchmarks, called the dataset design bias, by over emphasizing thestereotypical concepts of saliency. The dataset design bias does not onlycreate the discomforting disconnection between fixations and salient objectsegmentation, but also misleads the algorithm designing. Based on our analysis,we propose a new high quality dataset that offers both fixation and salientobject segmentation ground-truth. With fixations and salient object beingpresented simultaneously, we are able to bridge the gap between fixations andsalient objects, and propose a novel method for salient object segmentation.Finally, we report significant benchmark progress on three existing datasets ofsegmenting salient objects
arxiv-6600-300 | Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition | http://arxiv.org/pdf/1406.3284v1.pdf | author:Charles F. Cadieu, Ha Hong, Daniel L. K. Yamins, Nicolas Pinto, Diego Ardila, Ethan A. Solomon, Najib J. Majaj, James J. DiCarlo category:q-bio.NC cs.NE published:2014-06-12 summary:The primate visual system achieves remarkable visual object recognitionperformance even in brief presentations and under changes to object exemplar,geometric transformations, and background variation (a.k.a. core visual objectrecognition). This remarkable performance is mediated by the representationformed in inferior temporal (IT) cortex. In parallel, recent advances inmachine learning have led to ever higher performing models of objectrecognition using artificial deep neural networks (DNNs). It remains unclear,however, whether the representational performance of DNNs rivals that of thebrain. To accurately produce such a comparison, a major difficulty has been aunifying metric that accounts for experimental limitations such as the amountof noise, the number of neural recording sites, and the number trials, andcomputational limitations such as the complexity of the decoding classifier andthe number of classifier training examples. In this work we perform a directcomparison that corrects for these experimental limitations and computationalconsiderations. As part of our methodology, we propose an extension of "kernelanalysis" that measures the generalization accuracy as a function ofrepresentational complexity. Our evaluations show that, unlike previousbio-inspired models, the latest DNNs rival the representational performance ofIT cortex on this visual object recognition task. Furthermore, we show thatmodels that perform well on measures of representational performance alsoperform well on measures of representational similarity to IT and on measuresof predicting individual IT multi-unit responses. Whether these DNNs rely oncomputational mechanisms similar to the primate visual system is yet to bedetermined, but, unlike all previous bio-inspired models, that possibilitycannot be ruled out merely on representational performance grounds.
