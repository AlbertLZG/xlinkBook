arxiv-1404-6538 | On Quadratization of Pseudo-Boolean Functions |  http://arxiv.org/abs/1404.6538  | author:Endre Boros, Aritanan Gruber category:math.OC cs.CV math.CO published:2014-04-25 summary:We survey current term-wise techniques for quadratizing high-degreepseudo-Boolean functions and introduce a new one, which allows multiple splitsof terms. We also introduce the first aggregative approach, which splits acollection of terms based on their common parts.
arxiv-1404-7048 | Multiscale Event Detection in Social Media |  http://arxiv.org/abs/1404.7048  | author:Xiaowen Dong, Dimitrios Mavroeidis, Francesco Calabrese, Pascal Frossard category:cs.SI cs.LG physics.soc-ph stat.ML published:2014-04-25 summary:Event detection has been one of the most important research topics in socialmedia analysis. Most of the traditional approaches detect events based on fixedtemporal and spatial resolutions, while in reality events of different scalesusually occur simultaneously, namely, they span different intervals in time andspace. In this paper, we propose a novel approach towards multiscale eventdetection using social media data, which takes into account different temporaland spatial scales of events in the data. Specifically, we explore theproperties of the wavelet transform, which is a well-developed multiscaletransform in signal processing, to enable automatic handling of the interactionbetween temporal and spatial scales. We then propose a novel algorithm tocompute a data similarity graph at appropriate scales and detect events ofdifferent scales simultaneously by a single graph-based clustering process.Furthermore, we present spatiotemporal statistical analysis of the noisyinformation present in the data stream, which allows us to define a novelterm-filtering procedure for the proposed event detection algorithm and helpsus study its behavior using simulated noisy data. Experimental results on bothsynthetically generated data and real world data collected from Twitterdemonstrate the meaningfulness and effectiveness of the proposed approach. Ourframework further extends to numerous application domains that involvemultiscale and multiresolution data analysis.
arxiv-1404-6580 | Multitask Learning for Sequence Labeling Tasks |  http://arxiv.org/abs/1404.6580  | author:Arvind Agarwal, Saurabh Kataria category:cs.LG published:2014-04-25 summary:In this paper, we present a learning method for sequence labeling tasks inwhich each example sequence has multiple label sequences. Our method learnsmultiple models, one model for each label sequence. Each model computes thejoint probability of all label sequences given the example sequence. Althougheach model considers all label sequences, its primary focus is only one labelsequence, and therefore, each model becomes a task-specific model, for the taskbelonging to that primary label. Such multiple models are learned {\itsimultaneously} by facilitating the learning transfer among models through {\itexplicit parameter sharing}. We experiment the proposed method on twoapplications and show that our method significantly outperforms thestate-of-the-art method.
arxiv-1405-2294 | Nonparametric Detection of Anomalous Data via Kernel Mean Embedding |  http://arxiv.org/abs/1405.2294  | author:Shaofeng Zou, Yingbin Liang, H. Vincent Poor, Xinghua Shi category:cs.LG stat.ML published:2014-04-25 summary:An anomaly detection problem is investigated, in which there are totally nsequences with s anomalous sequences to be detected. Each normal sequencecontains m independent and identically distributed (i.i.d.) samples drawn froma distribution p, whereas each anomalous sequence contains m i.i.d. samplesdrawn from a distribution q that is distinct from p. The distributions p and qare assumed to be unknown a priori. Two scenarios, respectively with andwithout a reference sequence generated by p, are studied. Distribution-freetests are constructed using maximum mean discrepancy (MMD) as the metric, whichis based on mean embeddings of distributions into a reproducing kernel Hilbertspace (RKHS). For both scenarios, it is shown that as the number n of sequencesgoes to infinity, if the value of s is known, then the number m of samples ineach sequence should be at the order O(log n) or larger in order for thedeveloped tests to consistently detect s anomalous sequences. If the value of sis unknown, then m should be at the order strictly larger than O(log n).Computational complexity of all developed tests is shown to be polynomial.Numerical results demonstrate that our tests outperform (or perform as well as)the tests based on other competitive traditional statistical approaches andkernel-based approaches under various cases. Consistency of the proposed testis also demonstrated on a real data set.
arxiv-1404-6535 | Quadratization of Symmetric Pseudo-Boolean Functions |  http://arxiv.org/abs/1404.6535  | author:Martin Anthony, Endre Boros, Yves Crama, Aritanan Gruber category:math.OC cs.CC cs.CV math.CO published:2014-04-25 summary:A pseudo-Boolean function is a real-valued function$f(x)=f(x_1,x_2,\ldots,x_n)$ of $n$ binary variables; that is, a mapping from$\{0,1\}^n$ to $\mathbb{R}$. For a pseudo-Boolean function $f(x)$ on$\{0,1\}^n$, we say that $g(x,y)$ is a quadratization of $f$ if $g(x,y)$ is aquadratic polynomial depending on $x$ and on $m$ auxiliary binary variables$y_1,y_2,\ldots,y_m$ such that $f(x)= \min \{g(x,y) : y \in \{0,1\}^m \}$ forall $x \in \{0,1\}^n$. By means of quadratizations, minimization of $f$ isreduced to minimization (over its extended set of variables) of the quadraticfunction $g(x,y)$. This is of some practical interest because minimization ofquadratic functions has been thoroughly studied for the last few decades, andmuch progress has been made in solving such problems exactly or heuristically.A related paper \cite{ABCG} initiated a systematic study of the minimum numberof auxiliary $y$-variables required in a quadratization of an arbitraryfunction $f$ (a natural question, since the complexity of minimizing thequadratic function $g(x,y)$ depends, among other factors, on the number ofbinary variables). In this paper, we determine more precisely the number ofauxiliary variables required by quadratizations of symmetric pseudo-Booleanfunctions $f(x)$, those functions whose value depends only on the Hammingweight of the input $x$ (the number of variables equal to $1$).
arxiv-1404-6413 | Indoor Activity Detection and Recognition for Sport Games Analysis |  http://arxiv.org/abs/1404.6413  | author:Georg Waltner, Thomas Mauthner, Horst Bischof category:cs.CV published:2014-04-25 summary:Activity recognition in sport is an attractive field for computer visionresearch. Game, player and team analysis are of great interest and researchtopics within this field emerge with the goal of automated analysis. The veryspecific underlying rules of sports can be used as prior knowledge for therecognition task and present a constrained environment for evaluation. Thispaper describes recognition of single player activities in sport with specialemphasis on volleyball. Starting from a per-frame player-centered activityrecognition, we incorporate geometry and contextual information via an activitycontext descriptor that collects information about all player's activities overa certain timespan relative to the investigated player. The benefit of thiscontext information on single player activity recognition is evaluated on ournew real-life dataset presenting a total amount of almost 36k annotated framescontaining 7 activity classes within 6 videos of professional volleyball games.Our incorporation of the contextual information improves the averageplayer-centered classification performance of 77.56% by up to 18.35% onspecific classes, proving that spatio-temporal context is an important clue foractivity recognition.
arxiv-1405-4894 | Optimization of OFDM radar waveforms using genetic algorithms |  http://arxiv.org/abs/1405.4894  | author:Gabriel Lellouch, Amit Kumar Mishra category:cs.NE published:2014-04-25 summary:In this paper, we present our investigations on the use of single objectiveand multiobjective genetic algorithms based optimisation algorithms to improvethe design of OFDM pulses for radar. We discuss these optimization proceduresin the scope of a waveform design intended for two different radar processingsolutions. Lastly, we show how the encoding solution is suited to permit theoptimizations of waveform for OFDM radar related challenges such as enhanceddetection.
arxiv-1404-6473 | Quantifying Uncertainty in Random Forests via Confidence Intervals and Hypothesis Tests |  http://arxiv.org/abs/1404.6473  | author:Lucas Mentch, Giles Hooker category:stat.ML stat.AP stat.CO stat.ME published:2014-04-25 summary:This work develops formal statistical inference procedures for machinelearning ensemble methods. Ensemble methods based on bootstrapping, such asbagging and random forests, have improved the predictive accuracy of individualtrees, but fail to provide a framework in which distributional results can beeasily determined. Instead of aggregating full bootstrap samples, we considerpredicting by averaging over trees built on subsamples of the training set anddemonstrate that the resulting estimator takes the form of a U-statistic. Assuch, predictions for individual feature vectors are asymptotically normal,allowing for confidence intervals to accompany predictions. In practice, asubset of subsamples is used for computational speed; here our estimators takethe form of incomplete U-statistics and equivalent results are derived. Wefurther demonstrate that this setup provides a framework for testing thesignificance of features. Moreover, the internal estimation method we developallows us to estimate the variance parameters and perform these inferenceprocedures at no additional computational cost. Simulations and illustrationson a real dataset are provided.
arxiv-1404-6334 | Input anticipating critical reservoirs show power law forgetting of unexpected input events |  http://arxiv.org/abs/1404.6334  | author:Norbert Michael Mayer category:cs.NE published:2014-04-25 summary:Usually, reservoir computing shows an exponential memory decay. This paperinvestigates under which circumstances echo state networks can show a power lawforgetting. That means traces of earlier events can be found in the reservoirfor very long time spans. Such a setting requires critical connectivity exactlyat the limit of what is permissible according the echo state condition.However, for general matrices the limit cannot be determined exactly fromtheory. In addition, the behavior of the network is strongly influenced by theinput flow. Results are presented that use certain types of restrictedrecurrent connectivity and anticipation learning with regard to the input,where indeed power law forgetting can be achieved.
arxiv-1404-6312 | Reconstructing Native Language Typology from Foreign Language Usage |  http://arxiv.org/abs/1404.6312  | author:Yevgeni Berzak, Roi Reichart, Boris Katz category:cs.CL published:2014-04-25 summary:Linguists and psychologists have long been studying cross-linguistictransfer, the influence of native language properties on linguistic performancein a foreign language. In this work we provide empirical evidence for thisprocess in the form of a strong correlation between language similaritiesderived from structural features in English as Second Language (ESL) texts andequivalent similarities obtained from the typological features of the nativelanguages. We leverage this finding to recover native language typologicalsimilarity structure directly from ESL text, and perform prediction oftypological features in an unsupervised fashion with respect to the targetlanguages. Our method achieves 72.2% accuracy on the typology prediction task,a result that is highly competitive with equivalent methods that rely ontypological resources.
arxiv-1404-6351 | Improving weather radar by fusion and classification |  http://arxiv.org/abs/1404.6351  | author:Harald Ganster, Martina Uray, Sylwia Steginska, Gerardus Croonen, Rudolf Kaltenböck, Karin Hennermann category:cs.CV published:2014-04-25 summary:In air traffic management (ATM) all necessary operations (tactical planing,sector configuration, required staffing, runway configuration, routing ofapproaching aircrafts) rely on accurate measurements and predictions of thecurrent weather situation. An essential basis of information is delivered byweather radar images (WXR), which, unfortunately, exhibit a vast amount ofdisturbances. Thus, the improvement of these datasets is the key factor formore accurate predictions of weather phenomena and weather conditions. Imageprocessing methods based on texture analysis and geometric operators allow toidentify regions including artefacts as well as zones of missing information.Correction of these zones is implemented by exploiting multi-spectral satellitedata (Meteosat Second Generation). Results prove that the proposed system forartefact detection and data correction significantly improves the quality ofWXR data and, thus, enables more reliable weather now- and forecast leading toincreased ATM safety.
arxiv-1404-6369 | Applying machine learning to the problem of choosing a heuristic to select the variable ordering for cylindrical algebraic decomposition |  http://arxiv.org/abs/1404.6369  | author:Zongyan Huang, Matthew England, David Wilson, James H. Davenport, Lawrence C. Paulson, James Bridge category:cs.SC cs.LG I.2.6 published:2014-04-25 summary:Cylindrical algebraic decomposition(CAD) is a key tool in computationalalgebraic geometry, particularly for quantifier elimination over real-closedfields. When using CAD, there is often a choice for the ordering placed on thevariables. This can be important, with some problems infeasible with onevariable ordering but easy with another. Machine learning is the process offitting a computer model to a complex function based on properties learned frommeasured data. In this paper we use machine learning (specifically a supportvector machine) to select between heuristics for choosing a variable ordering,outperforming each of the separate heuristics.
arxiv-1404-6031 | Maximum Margin Vector Correlation Filter |  http://arxiv.org/abs/1404.6031  | author:Vishnu Naresh Boddeti, B. V. K. Vijaya Kumar category:cs.CV published:2014-04-24 summary:Correlation Filters (CFs) are a class of classifiers which are designed foraccurate pattern localization. Traditionally CFs have been used with scalarfeatures only, which limits their ability to be used with vector featurerepresentations like Gabor filter banks, SIFT, HOG, etc. In this paper wepresent a new CF named Maximum Margin Vector Correlation Filter (MMVCF) whichextends the traditional CF designs to vector features. MMVCF further combinesthe generalization capability of large margin based classifiers like SupportVector Machines (SVMs) and the localization properties of CFs for betterrobustness to outliers. We demonstrate the efficacy of MMVCF for objectdetection and landmark localization on a variety of databases and demonstratethat MMVCF consistently shows improved pattern localization capability incomparison to SVMs.
arxiv-1404-6163 | Overlapping Trace Norms in Multi-View Learning |  http://arxiv.org/abs/1404.6163  | author:Behrouz Behmardi, Cedric Archambeau, Guillaume Bouchard category:cs.LG published:2014-04-24 summary:Multi-view learning leverages correlations between different sources of datato make predictions in one view based on observations in another view. Apopular approach is to assume that, both, the correlations between the viewsand the view-specific covariances have a low-rank structure, leading tointer-battery factor analysis, a model closely related to canonical correlationanalysis. We propose a convex relaxation of this model using structured normregularization. Further, we extend the convex formulation to a robust versionby adding an l1-penalized matrix to our estimator, similarly to convex robustPCA. We develop and compare scalable algorithms for several convex multi-viewmodels. We show experimentally that the view-specific correlations areimproving data imputation performances, as well as labeling accuracy inreal-world multi-label prediction tasks.
arxiv-1404-6039 | The fshape framework for the variability analysis of functional shapes |  http://arxiv.org/abs/1404.6039  | author:Benjamin Charlier, Nicolas Charon, Alain Trouvé category:cs.CG cs.CV math.DG published:2014-04-24 summary:This article introduces a full mathematical and numerical framework fortreating functional shapes (or fshapes) following the landmarks of shape spacesand shape analysis. Functional shapes can be described as signal functionssupported on varying geometrical supports. Analysing variability of fshapes'ensembles require the modelling and quantification of joint variations ingeometry and signal, which have been treated separately in previous approaches.Instead, building on the ideas of shape spaces for purely geometrical objects,we propose the extended concept of fshape bundles and define Riemannian metricsfor fshape metamorphoses to model geometrico-functional transformations withinthese bundles. We also generalize previous works on data attachment terms basedon the notion of varifolds and demonstrate the utility of these distances.Based on these, we propose variational formulations of the atlas estimationproblem on populations of fshapes and prove existence of solutions for thedifferent models. The second part of the article examines the numericalimplementation of the models by detailing discrete expressions for the metricsand gradients and proposing an optimization scheme for the atlas estimationproblem. We present a few results of the methodology on a synthetic dataset aswell as on a population of retinal membranes with thickness maps.
arxiv-1404-6075 | Unsupervised Text Extraction from G-Maps |  http://arxiv.org/abs/1404.6075  | author:Chandranath Adak category:cs.CV cs.AI published:2014-04-24 summary:This paper represents an text extraction method from Google maps, GISmaps/images. Due to an unsupervised approach there is no requirement of anyprior knowledge or training set about the textual and non-textual parts. FuzzyCMeans clustering technique is used for image segmentation and Prewitt methodis used to detect the edges. Connected component analysis and griddingtechnique enhance the correctness of the results. The proposed method reaches98.5% accuracy level on the basis of experimental data sets.
arxiv-1404-6071 | Rough Clustering Based Unsupervised Image Change Detection |  http://arxiv.org/abs/1404.6071  | author:Chandranath Adak category:cs.CV cs.AI published:2014-04-24 summary:This paper introduces an unsupervised technique to detect the changed regionof multitemporal images on a same reference plane with the help of roughclustering. The proposed technique is a soft-computing approach, based on theconcept of rough set with rough clustering and Pawlak's accuracy. It is lessnoisy and avoids pre-deterministic knowledge about the distribution of thechanged and unchanged regions. To show the effectiveness, the proposedtechnique is compared with some other approaches.
arxiv-1404-6074 | Classifying pairs with trees for supervised biological network inference |  http://arxiv.org/abs/1404.6074  | author:Marie Schrynemackers, Louis Wehenkel, M. Madan Babu, Pierre Geurts category:cs.LG stat.ML published:2014-04-24 summary:Networks are ubiquitous in biology and computational approaches have beenlargely investigated for their inference. In particular, supervised machinelearning methods can be used to complete a partially known network byintegrating various measurements. Two main supervised frameworks have beenproposed: the local approach, which trains a separate model for each networknode, and the global approach, which trains a single model over pairs of nodes.Here, we systematically investigate, theoretically and empirically, theexploitation of tree-based ensemble methods in the context of these twoapproaches for biological network inference. We first formalize the problem ofnetwork inference as classification of pairs, unifying in the processhomogeneous and bipartite graphs and discussing two main sampling schemes. Wethen present the global and the local approaches, extending the later for theprediction of interactions between two unseen network nodes, and discuss theirspecializations to tree-based ensemble methods, highlighting theirinterpretability and drawing links with clustering techniques. Extensivecomputational experiments are carried out with these methods on variousbiological networks that clearly highlight that these methods are competitivewith existing methods.
arxiv-1404-6055 | A General Homogeneous Matrix Formulation to 3D Rotation Geometric Transformations |  http://arxiv.org/abs/1404.6055  | author:Feng Lu, Ziqiang Chen category:cs.CV published:2014-04-24 summary:We present algebraic projective geometry definitions of 3D rotations so as tobridge a small gap between the applications and the definitions of 3D rotationsin homogeneous matrix form. A general homogeneous matrix formulation to 3Drotation geometric transformations is proposed which suits for the cases whenthe rotation axis is unnecessarily through the coordinate system origin giventheir rotation axes and rotation angles.
arxiv-1404-6289 | Solution Path Clustering with Adaptive Concave Penalty |  http://arxiv.org/abs/1404.6289  | author:Yuliya Marchetti, Qing Zhou category:stat.ME stat.ML published:2014-04-24 summary:Fast accumulation of large amounts of complex data has created a need formore sophisticated statistical methodologies to discover interesting patternsand better extract information from these data. The large scale of the dataoften results in challenging high-dimensional estimation problems where only aminority of the data shows specific grouping patterns. To address theseemerging challenges, we develop a new clustering methodology that introducesthe idea of a regularization path into unsupervised learning. A regularizationpath for a clustering problem is created by varying the degree of sparsityconstraint that is imposed on the differences between objects via the minimaxconcave penalty with adaptive tuning parameters. Instead of providing a singlesolution represented by a cluster assignment for each object, the methodproduces a short sequence of solutions that determines not only the clusterassignment but also a corresponding number of clusters for each solution. Theoptimization of the penalized loss function is carried out through an MMalgorithm with block coordinate descent. The advantages of this clusteringalgorithm compared to other existing methods are as follows: it does notrequire the input of the number of clusters; it is capable of simultaneouslyseparating irrelevant or noisy observations that show no grouping pattern,which can greatly improve data interpretation; it is a general methodology thatcan be applied to many clustering problems. We test this method on varioussimulated datasets and on gene expression data, where it shows better orcompetitive performance compared against several clustering methods.
arxiv-1405-5488 | On Learning Where To Look |  http://arxiv.org/abs/1405.5488  | author:Marc'Aurelio Ranzato category:cs.CV cs.LG published:2014-04-24 summary:Current automatic vision systems face two major challenges: scalability andextreme variability of appearance. First, the computational time required toprocess an image typically scales linearly with the number of pixels in theimage, therefore limiting the resolution of input images to thumbnail size.Second, variability in appearance and pose of the objects constitute a majorhurdle for robust recognition and detection. In this work, we propose a modelthat makes baby steps towards addressing these challenges. We describe alearning based method that recognizes objects through a series of glimpses.This system performs an amount of computation that scales with the complexityof the input rather than its number of pixels. Moreover, the proposed method ispotentially more robust to changes in appearance since its parameters arelearned in a data driven manner. Preliminary experiments on a handwrittendataset of digits demonstrate the computational advantages of this approach.
arxiv-1404-6283 | Automated adaptive inference of coarse-grained dynamical models in systems biology |  http://arxiv.org/abs/1404.6283  | author:Bryan C. Daniels, Ilya Nemenman category:q-bio.QM stat.ML published:2014-04-24 summary:Cellular regulatory dynamics is driven by large and intricate networks ofinteractions at the molecular scale, whose sheer size obfuscates understanding.In light of limited experimental data, many parameters of such dynamics areunknown, and thus models built on the detailed, mechanistic viewpoint overfitand are not predictive. At the other extreme, simple ad hoc models of complexprocesses often miss defining features of the underlying systems. Here wepropose an approach that instead constructs phenomenological, coarse-grainedmodels of network dynamics that automatically adapt their complexity to theamount of available data. Such adaptive models lead to accurate predictionseven when microscopic details of the studied systems are unknown due toinsufficient data. The approach is computationally tractable, even for arelatively large number of dynamical variables, allowing its softwarerealization, named Sir Isaac, to make successful predictions even whenimportant dynamic variables are unobserved. For example, it matches the knownphase space structure for simulated planetary motion data, avoids overfittingin a complex biological signaling system, and produces accurate predictions fora yeast glycolysis model with only tens of data points and over half of theinteracting species unobserved.
arxiv-1405-6164 | Generating Natural Language Descriptions from OWL Ontologies: the NaturalOWL System |  http://arxiv.org/abs/1405.6164  | author:Ion Androutsopoulos, Gerasimos Lampouras, Dimitrios Galanis category:cs.CL cs.AI published:2014-04-24 summary:We present NaturalOWL, a natural language generation system that producestexts describing individuals or classes of OWL ontologies. Unlike simpler OWLverbalizers, which typically express a single axiom at a time in controlled,often not entirely fluent natural language primarily for the benefit of domainexperts, we aim to generate fluent and coherent multi-sentence texts forend-users. With a system like NaturalOWL, one can publish information in OWL onthe Web, along with automatically produced corresponding texts in multiplelanguages, making the information accessible not only to computer programs anddomain experts, but also end-users. We discuss the processing stages ofNaturalOWL, the optional domain-dependent linguistic resources that the systemcan use at each stage, and why they are useful. We also present trials showingthat when the domain-dependent llinguistic resources are available, NaturalOWLproduces significantly better texts compared to a simpler verbalizer, and thatthe resources can be created with relatively light effort.
arxiv-1404-6272 | Scalable Similarity Learning using Large Margin Neighborhood Embedding |  http://arxiv.org/abs/1404.6272  | author:Zhaowen Wang, Jianchao Yang, Zhe Lin, Jonathan Brandt, Shiyu Chang, Thomas Huang category:cs.CV cs.LG published:2014-04-24 summary:Classifying large-scale image data into object categories is an importantproblem that has received increasing research attention. Given the huge amountof data, non-parametric approaches such as nearest neighbor classifiers haveshown promising results, especially when they are underpinned by a learneddistance or similarity measurement. Although metric learning has been wellstudied in the past decades, most existing algorithms are impractical to handlelarge-scale data sets. In this paper, we present an image similarity learningmethod that can scale well in both the number of images and the dimensionalityof image descriptors. To this end, similarity comparison is restricted to eachsample's local neighbors and a discriminative similarity measure is inducedfrom large margin neighborhood embedding. We also exploit the ensemble ofprojections so that high-dimensional features can be processed in a set oflower-dimensional subspaces in parallel without much performance compromise.The similarity function is learned online using a stochastic gradient descentalgorithm in which the triplet sampling strategy is customized for quickconvergence of classification performance. The effectiveness of our proposedmodel is validated on several data sets with scales varying from tens ofthousands to one million images. Recognition accuracies competitive with thestate-of-the-art performance are achieved with much higher efficiency andscalability.
arxiv-1404-6216 | CoRE Kernels |  http://arxiv.org/abs/1404.6216  | author:Ping Li category:stat.ML cs.DS cs.LG stat.ME published:2014-04-24 summary:The term "CoRE kernel" stands for correlation-resemblance kernel. In manyapplications (e.g., vision), the data are often high-dimensional, sparse, andnon-binary. We propose two types of (nonlinear) CoRE kernels for non-binarysparse data and demonstrate the effectiveness of the new kernels through aclassification experiment. CoRE kernels are simple with no tuning parameters.However, training nonlinear kernel SVM can be (very) costly in time and memoryand may not be suitable for truly large-scale industrial applications (e.g.search). In order to make the proposed CoRE kernels more practical, we developbasic probabilistic hashing algorithms which transform nonlinear kernels intolinear kernels.
arxiv-1404-6955 | Probabilistic graphs using coupled random variables |  http://arxiv.org/abs/1404.6955  | author:Kenric P. Nelson, Madalina Barbu, Brian J. Scannell category:cs.LG cs.IT cs.NE math.IT published:2014-04-23 summary:Neural network design has utilized flexible nonlinear processes which canmimic biological systems, but has suffered from a lack of traceability in theresulting network. Graphical probabilistic models ground network design inprobabilistic reasoning, but the restrictions reduce the expressive capabilityof each node making network designs complex. The ability to model coupledrandom variables using the calculus of nonextensive statistical mechanicsprovides a neural node design incorporating nonlinear coupling between inputstates while maintaining the rigor of probabilistic reasoning. A generalizationof Bayes rule using the coupled product enables a single node to modelcorrelation between hundreds of random variables. A coupled Markov random fieldis designed for the inferencing and classification of UCI's MLR 'MultipleFeatures Data Set' such that thousands of linear correlation parameters can bereplaced with a single coupling parameter with just a (3%, 4%) percentreduction in (classification, inference) performance.
arxiv-1404-5765 | Find my mug: Efficient object search with a mobile robot using semantic segmentation |  http://arxiv.org/abs/1404.5765  | author:Daniel Wolf, Markus Bajones, Johann Prankl, Markus Vincze category:cs.CV cs.RO published:2014-04-23 summary:In this paper, we propose an efficient semantic segmentation framework forindoor scenes, tailored to the application on a mobile robot. Semanticsegmentation can help robots to gain a reasonable understanding of theirenvironment, but to reach this goal, the algorithms not only need to beaccurate, but also fast and robust. Therefore, we developed an optimized 3Dpoint cloud processing framework based on a Randomized Decision Forest,achieving competitive results at sufficiently high frame rates. We evaluate thecapabilities of our method on the popular NYU depth dataset and our own dataand demonstrate its feasibility by deploying it on a mobile service robot, forwhich we could optimize an object search procedure using our results.
arxiv-1404-5767 | Codynamic Fitness Landscapes of Coevolutionary Minimal Substrates |  http://arxiv.org/abs/1404.5767  | author:Hendrik Richter category:cs.NE published:2014-04-23 summary:Coevolutionary minimal substrates are simple and abstract models that allowstudying the relationships and codynamics between objective and subjectivefitness. Using these models an approach is presented for defining and analyzingfitness landscapes of coevolutionary problems. We devise similarity measures ofcodynamic fitness landscapes and experimentally study minimal substrates oftest--based and compositional problems for both cooperative and competitiveinteraction.
arxiv-1404-5903 | Most Correlated Arms Identification |  http://arxiv.org/abs/1404.5903  | author:Che-Yu Liu, Sébastien Bubeck category:stat.ML cs.LG published:2014-04-23 summary:We study the problem of finding the most mutually correlated arms among manyarms. We show that adaptive arms sampling strategies can have significantadvantages over the non-adaptive uniform sampling strategy. Our proposedalgorithms rely on a novel correlation estimator. The use of this accurateestimator allows us to get improved results for a wide range of probleminstances.
arxiv-1404-5997 | One weird trick for parallelizing convolutional neural networks |  http://arxiv.org/abs/1404.5997  | author:Alex Krizhevsky category:cs.NE cs.DC cs.LG published:2014-04-23 summary:I present a new way to parallelize the training of convolutional neuralnetworks across multiple GPUs. The method scales significantly better than allalternatives when applied to modern convolutional neural networks.
arxiv-1404-6491 | An Account of Opinion Implicatures |  http://arxiv.org/abs/1404.6491  | author:Janyce Wiebe, Lingjia Deng category:cs.CL cs.IR published:2014-04-23 summary:While previous sentiment analysis research has concentrated on theinterpretation of explicitly stated opinions and attitudes, this work initiatesthe computational study of a type of opinion implicature (i.e.,opinion-oriented inference) in text. This paper described a rule-basedframework for representing and analyzing opinion implicatures which we hopewill contribute to deeper automatic interpretation of subjective language. Inthe course of understanding implicatures, the system recognizes implicitsentiments (and beliefs) toward various events and entities in the sentence,often attributed to different sources (holders) and of mixed polarities; thus,it produces a richer interpretation than is typical in opinion analysis.
arxiv-1404-6000 | Robust and computationally feasible community detection in the presence of arbitrary outlier nodes |  http://arxiv.org/abs/1404.6000  | author:T. Tony Cai, Xiaodong Li category:math.ST cs.IT math.IT math.OC stat.ML stat.TH published:2014-04-23 summary:Community detection, which aims to cluster $N$ nodes in a given graph into$r$ distinct groups based on the observed undirected edges, is an importantproblem in network data analysis. In this paper, the popular stochastic blockmodel (SBM) is extended to the generalized stochastic block model (GSBM) thatallows for adversarial outlier nodes, which are connected with the other nodesin the graph in an arbitrary way. Under this model, we introduce a procedureusing convex optimization followed by $k$-means algorithm with $k=r$. Boththeoretical and numerical properties of the method are analyzed. A theoreticalguarantee is given for the procedure to accurately detect the communities withsmall misclassification rate under the setting where the number of clusters cangrow with $N$. This theoretical result admits to the best-known result in theliterature of computationally feasible community detection in SBM withoutoutliers. Numerical results show that our method is both computationally fastand robust to different kinds of outliers, while some popular computationallyfast community detection algorithms, such as spectral clustering applied toadjacency matrices or graph Laplacians, may fail to retrieve the major clustersdue to a small portion of outliers. We apply a slight modification of ourmethod to a political blogs data set, showing that our method is competent inpractice and comparable to existing computationally feasible methods in theliterature. To the best of the authors' knowledge, our result is the first inthe literature in terms of clustering communities with fast growing numbersunder the GSBM where a portion of arbitrary outlier nodes exist.
arxiv-1404-5772 | Sequential Click Prediction for Sponsored Search with Recurrent Neural Networks |  http://arxiv.org/abs/1404.5772  | author:Yuyu Zhang, Hanjun Dai, Chang Xu, Jun Feng, Taifeng Wang, Jiang Bian, Bin Wang, Tie-Yan Liu category:cs.IR cs.LG cs.NE published:2014-04-23 summary:Click prediction is one of the fundamental problems in sponsored search. Mostof existing studies took advantage of machine learning approaches to predict adclick for each event of ad view independently. However, as observed in thereal-world sponsored search system, user's behaviors on ads yield highdependency on how the user behaved along with the past time, especially interms of what queries she submitted, what ads she clicked or ignored, and howlong she spent on the landing pages of clicked ads, etc. Inspired by theseobservations, we introduce a novel framework based on Recurrent Neural Networks(RNN). Compared to traditional methods, this framework directly models thedependency on user's sequential behaviors into the click prediction processthrough the recurrent structure in RNN. Large scale evaluations on theclick-through logs from a commercial search engine demonstrate that ourapproach can significantly improve the click prediction accuracy, compared tosequence-independent approaches.
arxiv-1404-5793 | Bayesian Reconstruction of Missing Observations |  http://arxiv.org/abs/1404.5793  | author:Shun Kataoka, Muneki Yasuda, Kazuyuki Tanaka category:stat.ML published:2014-04-23 summary:We focus on an interpolation method referred to Bayesian reconstruction inthis paper. Whereas in standard interpolation methods missing data areinterpolated deterministically, in Bayesian reconstruction, missing data areinterpolated probabilistically using a Bayesian treatment. In this paper, weaddress the framework of Bayesian reconstruction and its application to thetraffic data reconstruction problem in the field of traffic engineering. In thelatter part of this paper, we describe the evaluation of the statisticalperformance of our Bayesian traffic reconstruction model using a statisticalmechanical approach and clarify its statistical behavior.
arxiv-1404-5692 | Forward - Backward Greedy Algorithms for Atomic Norm Regularization |  http://arxiv.org/abs/1404.5692  | author:Nikhil Rao, Parikshit Shah, Stephen Wright category:cs.DS cs.LG math.OC stat.ML published:2014-04-23 summary:In many signal processing applications, the aim is to reconstruct a signalthat has a simple representation with respect to a certain basis or frame.Fundamental elements of the basis known as "atoms" allow us to define "atomicnorms" that can be used to formulate convex regularizations for thereconstruction problem. Efficient algorithms are available to solve theseformulations in certain special cases, but an approach that works well forgeneral atomic norms, both in terms of speed and reconstruction accuracy,remains to be found. This paper describes an optimization algorithm calledCoGEnT that produces solutions with succinct atomic representations forreconstruction problems, generally formulated with atomic-norm constraints.CoGEnT combines a greedy selection scheme based on the conditional gradientapproach with a backward (or "truncation") step that exploits the quadraticnature of the objective to reduce the basis size. We establish convergenceproperties and validate the algorithm via extensive numerical experiments on asuite of signal processing applications. Our algorithm and analysis also allowfor inexact forward steps and for occasional enhancements of the currentrepresentation to be performed. CoGEnT can outperform the basic conditionalgradient method, and indeed many methods that are tailored to specificapplications, when the enhancement and truncation steps are definedappropriately. We also introduce several novel applications that are enabled bythe atomic-norm framework, including tensor completion, moment problems insignal processing, and graph deconvolution.
arxiv-1404-5367 | Lexicon Infused Phrase Embeddings for Named Entity Resolution |  http://arxiv.org/abs/1404.5367  | author:Alexandre Passos, Vineet Kumar, Andrew McCallum category:cs.CL published:2014-04-22 summary:Most state-of-the-art approaches for named-entity recognition (NER) use semisupervised information in the form of word clusters and lexicons. Recentlyneural network-based language models have been explored, as they as a byproductgenerate highly informative vector representations for words, known as wordembeddings. In this paper we present two contributions: a new form of learningword embeddings that can leverage information from relevant lexicons to improvethe representations, and the first system to use neural word embeddings toachieve state-of-the-art results on named-entity recognition in both CoNLL andOntonotes NER. Our system achieves an F1 score of 90.90 on the test set forCoNLL 2003---significantly better than any previous system trained on publicdata, and matching a system employing massive private industrial query-logdata.
arxiv-1404-5357 | Morphological Analysis of the Bishnupriya Manipuri Language using Finite State Transducers |  http://arxiv.org/abs/1404.5357  | author:Nayan Jyoti Kalita, Navanath Saharia, Smriti Kumar Sinha category:cs.CL published:2014-04-22 summary:In this work we present a morphological analysis of Bishnupriya Manipurilanguage, an Indo-Aryan language spoken in the north eastern India. As of now,there is no computational work available for the language. Finite statemorphology is one of the successful approaches applied in a wide variety oflanguages over the year. Therefore we adapted the finite state approach toanalyse morphology of the Bishnupriya Manipuri language.
arxiv-1404-5475 | Combining pattern-based CRFs and weighted context-free grammars |  http://arxiv.org/abs/1404.5475  | author:Rustem Takhanov, Vladimir Kolmogorov category:cs.FL cs.DS cs.LG I.2.7 published:2014-04-22 summary:We consider two models for the sequence labeling (tagging) problem. The firstone is a {\em Pattern-Based Conditional Random Field }(\PB), in which theenergy of a string (chain labeling) $x=x_1\ldots x_n\in D^n$ is a sum of termsover intervals $[i,j]$ where each term is non-zero only if the substring$x_i\ldots x_j$ equals a prespecified word $w\in \Lambda$. The second model isa {\em Weighted Context-Free Grammar }(\WCFG) frequently used for naturallanguage processing. \PB and \WCFG encode local and non-local interactionsrespectively, and thus can be viewed as complementary. We propose a {\em Grammatical Pattern-Based CRF model }(\GPB) that combinesthe two in a natural way. We argue that it has certain advantages over existingapproaches such as the {\em Hybrid model} of Bened{\'i} and Sanchez thatcombines {\em $\mbox{$N$-grams}$} and \WCFGs. The focus of this paper is toanalyze the complexity of inference tasks in a \GPB such as computing MAP. Wepresent a polynomial-time algorithm for general \GPBs and a faster version fora special case that we call {\em Interaction Grammars}.
arxiv-1404-5351 | Fast Approximate Matching of Cell-Phone Videos for Robust Background Subtraction |  http://arxiv.org/abs/1404.5351  | author:Raffay Hamid, Atish Das Sarma, Dennis DeCoste, Neel Sundaresan category:cs.CV published:2014-04-22 summary:We identify a novel instance of the background subtraction problem thatfocuses on extracting near-field foreground objects captured using handheldcameras. Given two user-generated videos of a scene, one with and the otherwithout the foreground object(s), our goal is to efficiently generate an outputvideo with only the foreground object(s) present in it. We cast this challengeas a spatio-temporal frame matching problem, and propose an efficient solutionfor it that exploits the temporal smoothness of the video sequences. We presenttheoretical analyses for the error bounds of our approach, and validate ourfindings using a detailed set of simulation experiments. Finally, we presentthe results of our approach tested on multiple real videos captured usinghandheld cameras, and compare them to several alternate foreground extractionapproaches.
arxiv-1404-5421 | Concurrent bandits and cognitive radio networks |  http://arxiv.org/abs/1404.5421  | author:Orly Avner, Shie Mannor category:cs.LG cs.MA published:2014-04-22 summary:We consider the problem of multiple users targeting the arms of a singlemulti-armed stochastic bandit. The motivation for this problem comes fromcognitive radio networks, where selfish users need to coexist without any sidecommunication between them, implicit cooperation or common control. Even thenumber of users may be unknown and can vary as users join or leave the network.We propose an algorithm that combines an $\epsilon$-greedy learning rule with acollision avoidance mechanism. We analyze its regret with respect to thesystem-wide optimum and show that sub-linear regret can be obtained in thissetting. Experiments show dramatic improvement compared to other algorithms forthis setting.
arxiv-1404-5372 | Linking Geographic Vocabularies through WordNet |  http://arxiv.org/abs/1404.5372  | author:Andrea Ballatore, Michela Bertolotto, David C. Wilson category:cs.IR cs.CL published:2014-04-22 summary:The linked open data (LOD) paradigm has emerged as a promising approach tostructuring and sharing geospatial information. One of the major obstacles tothis vision lies in the difficulties found in the automatic integration betweenheterogeneous vocabularies and ontologies that provides the semantic backboneof the growing constellation of open geo-knowledge bases. In this article, weshow how to utilize WordNet as a semantic hub to increase the integration ofLOD. With this purpose in mind, we devise Voc2WordNet, an unsupervised mappingtechnique between a given vocabulary and WordNet, combining intensional andextensional aspects of the geographic terms. Voc2WordNet is evaluated against asample of human-generated alignments with the OpenStreetMap (OSM) SemanticNetwork, a crowdsourced geospatial resource, and the GeoNames ontology, thevocabulary of a large digital gazetteer. These empirical results indicate thatthe approach can obtain high precision and recall.
arxiv-1404-5899 | A Comparison of Clustering and Missing Data Methods for Health Sciences |  http://arxiv.org/abs/1404.5899  | author:Ran Zhao, Deanna Needell, Christopher Johansen, Jerry L. Grenard category:math.NA cs.LG published:2014-04-22 summary:In this paper, we compare and analyze clustering methods with missing data inhealth behavior research. In particular, we propose and analyze the use ofcompressive sensing's matrix completion along with spectral clustering tocluster health related data. The empirical tests and real data results showthat these methods can outperform standard methods like LPA and FIML, in termsof lower misclassification rates in clustering and better matrix completionperformance in missing data problems. According to our examination, a possibleexplanation of these improvements is that spectral clustering takes advantageof high data dimension and compressive sensing methods utilize thenear-to-low-rank property of health data.
arxiv-1404-5417 | Attractor Metadynamics in Adapting Neural Networks |  http://arxiv.org/abs/1404.5417  | author:Claudius Gros, Mathias Linkerhand, Valentin Walther category:q-bio.NC cs.NE published:2014-04-22 summary:Slow adaption processes, like synaptic and intrinsic plasticity, abound inthe brain and shape the landscape for the neural dynamics occurring onsubstantially faster timescales. At any given time the network is characterizedby a set of internal parameters, which are adapting continuously, albeitslowly. This set of parameters defines the number and the location of therespective adiabatic attractors. The slow evolution of network parameters henceinduces an evolving attractor landscape, a process which we term attractormetadynamics. We study the nature of the metadynamics of the attractorlandscape for several continuous-time autonomous model networks. We find bothfirst- and second-order changes in the location of adiabatic attractors andargue that the study of the continuously evolving attractor landscapeconstitutes a powerful tool for understanding the overall development of theneural dynamics.
arxiv-1404-5588 | Large Margin Image Set Representation and Classification |  http://arxiv.org/abs/1404.5588  | author:Jim Jing-Yan Wang, Majed Alzahrani, Xin Gao category:cs.CV published:2014-04-22 summary:In this paper, we propose a novel image set representation and classificationmethod by maximizing the margin of image sets. The margin of an image set isdefined as the difference of the distance to its nearest image set fromdifferent classes and the distance to its nearest image set of the same class.By modeling the image sets by using both their image samples and their affinehull models, and maximizing the margins of the images sets, the image setrepresentation parameter learning problem is formulated as an minimizationproblem, which is further optimized by an expectation -maximization (EM)strategy with accelerated proximal gradient (APG) optimization in an iterativealgorithm. To classify a given test image set, we assign it to the class whichcould provide the largest margin. Experiments on two applications ofvideo-sequence-based face recognition demonstrate that the proposed methodsignificantly outperforms state-of-the-art image set classification methods interms of both effectiveness and efficiency.
arxiv-1404-5585 | A Structural Query System for Han Characters |  http://arxiv.org/abs/1404.5585  | author:Matthew Skala category:cs.CL cs.DB H.3.1 published:2014-04-22 summary:The IDSgrep structural query system for Han character dictionaries ispresented. This system includes a data model and syntax for describing thespatial structure of Han characters using Extended Ideographic DescriptionSequences (EIDSes) based on the Unicode IDS syntax; a language for queryingEIDS databases, designed to suit the needs of font developers and foreignlanguage learners; a bit vector index inspired by Bloom filters for fasterquery operations; a freely available implementation; and format translationfrom popular third-party IDS and XML character databases. Experimental resultsare included, with a comparison to other software used for similarapplications.
arxiv-1404-5521 | Together we stand, Together we fall, Together we win: Dynamic Team Formation in Massive Open Online Courses |  http://arxiv.org/abs/1404.5521  | author:Tanmay Sinha category:cs.SI cs.CY cs.LG cs.MA published:2014-04-22 summary:Massive Open Online Courses (MOOCs) offer a new scalable paradigm fore-learning by providing students with global exposure and opportunities forconnecting and interacting with millions of people all around the world. Veryoften, students work as teams to effectively accomplish course related tasks.However, due to lack of face to face interaction, it becomes difficult for MOOCstudents to collaborate. Additionally, the instructor also faces challenges inmanually organizing students into teams because students flock to these MOOCsin huge numbers. Thus, the proposed research is aimed at developing a robustmethodology for dynamic team formation in MOOCs, the theoretical framework forwhich is grounded at the confluence of organizational team theory, socialnetwork analysis and machine learning. A prerequisite for such an undertakingis that we understand the fact that, each and every informal tie establishedamong students offers the opportunities to influence and be influenced.Therefore, we aim to extract value from the inherent connectedness of studentsin the MOOC. These connections carry with them radical implications for the waystudents understand each other in the networked learning community. Ourapproach will enable course instructors to automatically group students inteams that have fairly balanced social connections with their peers, welldefined in terms of appropriately selected qualitative and quantitative networkmetrics.
arxiv-1404-5443 | Approximate Inference for Nonstationary Heteroscedastic Gaussian process Regression |  http://arxiv.org/abs/1404.5443  | author:Ville Tolvanen, Pasi Jylänki, Aki Vehtari category:stat.ML published:2014-04-22 summary:This paper presents a novel approach for approximate integration over theuncertainty of noise and signal variances in Gaussian process (GP) regression.Our efficient and straightforward approach can also be applied to integrationover input dependent noise variance (heteroscedasticity) and input dependentsignal variance (nonstationarity) by setting independent GP priors for thenoise and signal variances. We use expectation propagation (EP) for inferenceand compare results to Markov chain Monte Carlo in two simulated data sets andthree empirical examples. The results show that EP produces comparable resultswith less computational burden.
arxiv-1404-5122 | Spatiotemporal Sparse Bayesian Learning with Applications to Compressed Sensing of Multichannel Physiological Signals |  http://arxiv.org/abs/1404.5122  | author:Zhilin Zhang, Tzyy-Ping Jung, Scott Makeig, Zhouyue Pi, Bhaskar D. Rao category:cs.IT cs.LG math.IT stat.ML published:2014-04-21 summary:Energy consumption is an important issue in continuous wirelesstelemonitoring of physiological signals. Compressed sensing (CS) is a promisingframework to address it, due to its energy-efficient data compressionprocedure. However, most CS algorithms have difficulty in data recovery due tonon-sparsity characteristic of many physiological signals. Block sparseBayesian learning (BSBL) is an effective approach to recover such signals withsatisfactory recovery quality. However, it is time-consuming in recoveringmultichannel signals, since its computational load almost linearly increaseswith the number of channels. This work proposes a spatiotemporal sparse Bayesian learning algorithm torecover multichannel signals simultaneously. It not only exploits temporalcorrelation within each channel signal, but also exploits inter-channelcorrelation among different channel signals. Furthermore, its computationalload is not significantly affected by the number of channels. The proposedalgorithm was applied to brain computer interface (BCI) and EEG-based driver'sdrowsiness estimation. Results showed that the algorithm had both betterrecovery performance and much higher speed than BSBL. Particularly, theproposed algorithm ensured that the BCI classification and the drowsinessestimation had little degradation even when data were compressed by 80%, makingit very suitable for continuous wireless telemonitoring of multichannelsignals.
arxiv-1404-5236 | Sum-of-squares proofs and the quest toward optimal algorithms |  http://arxiv.org/abs/1404.5236  | author:Boaz Barak, David Steurer category:cs.DS cs.CC cs.LG math.OC published:2014-04-21 summary:In order to obtain the best-known guarantees, algorithms are traditionallytailored to the particular problem we want to solve. Two recent developments,the Unique Games Conjecture (UGC) and the Sum-of-Squares (SOS) method,surprisingly suggest that this tailoring is not necessary and that a singleefficient algorithm could achieve best possible guarantees for a wide range ofdifferent problems. The Unique Games Conjecture (UGC) is a tantalizing conjecture incomputational complexity, which, if true, will shed light on the complexity ofa great many problems. In particular this conjecture predicts that a singleconcrete algorithm provides optimal guarantees among all efficient algorithmsfor a large class of computational problems. The Sum-of-Squares (SOS) method is a general approach for solving systems ofpolynomial constraints. This approach is studied in several scientificdisciplines, including real algebraic geometry, proof complexity, controltheory, and mathematical programming, and has found applications in fields asdiverse as quantum information theory, formal verification, game theory andmany others. We survey some connections that were recently uncovered between the UniqueGames Conjecture and the Sum-of-Squares method. In particular, we discuss newtools to rigorously bound the running time of the SOS method for obtainingapproximate solutions to hard optimization problems, and how these tools givethe potential for the sum-of-squares method to provide new guarantees for manyproblems of interest, and possibly to even refute the UGC.
arxiv-1404-5214 | Graph Kernels via Functional Embedding |  http://arxiv.org/abs/1404.5214  | author:Anshumali Shrivastava, Ping Li category:cs.LG cs.AI stat.ML published:2014-04-21 summary:We propose a representation of graph as a functional object derived from thepower iteration of the underlying adjacency matrix. The proposed functionalrepresentation is a graph invariant, i.e., the functional remains unchangedunder any reordering of the vertices. This property eliminates the difficultyof handling exponentially many isomorphic forms. Bhattacharyya kernelconstructed between these functionals significantly outperforms thestate-of-the-art graph kernels on 3 out of the 4 standard benchmark graphclassification datasets, demonstrating the superiority of our approach. Theproposed methodology is simple and runs in time linear in the number of edges,which makes our kernel more efficient and scalable compared to many widelyadopted graph kernels with running time cubic in the number of vertices.
arxiv-1404-5165 | GP-Localize: Persistent Mobile Robot Localization using Online Sparse Gaussian Process Observation Model |  http://arxiv.org/abs/1404.5165  | author:Nuo Xu, Kian Hsiang Low, Jie Chen, Keng Kiat Lim, Etkin Baris Ozgul category:cs.RO cs.LG stat.ML published:2014-04-21 summary:Central to robot exploration and mapping is the task of persistentlocalization in environmental fields characterized by spatially correlatedmeasurements. This paper presents a Gaussian process localization (GP-Localize)algorithm that, in contrast to existing works, can exploit the spatiallycorrelated field measurements taken during a robot's exploration (instead ofrelying on prior training data) for efficiently and scalably learning the GPobservation model online through our proposed novel online sparse GP. As aresult, GP-Localize is capable of achieving constant time and memory (i.e.,independent of the size of the data) per filtering step, which demonstrates thepractical feasibility of using GPs for persistent robot localization andautonomy. Empirical evaluation via simulated experiments with real-worlddatasets and a real robot experiment shows that GP-Localize outperformsexisting GP localization algorithms.
arxiv-1404-5278 | The Frobenius anatomy of word meanings I: subject and object relative pronouns |  http://arxiv.org/abs/1404.5278  | author:Mehrnoosh Sadrzadeh, Stephen Clark, Bob Coecke category:cs.CL published:2014-04-21 summary:This paper develops a compositional vector-based semantics of subject andobject relative pronouns within a categorical framework. Frobenius algebras areused to formalise the operations required to model the semantics of relativepronouns, including passing information between the relative clause and themodified noun phrase, as well as copying, combining, and discarding parts ofthe relative clause. We develop two instantiations of the abstract semantics,one based on a truth-theoretic approach and one based on corpus statistics.
arxiv-1404-5520 | A Computationally Efficient Limited Memory CMA-ES for Large Scale Optimization |  http://arxiv.org/abs/1404.5520  | author:Ilya Loshchilov category:cs.NE published:2014-04-21 summary:We propose a computationally efficient limited memory Covariance MatrixAdaptation Evolution Strategy for large scale optimization, which we call theLM-CMA-ES. The LM-CMA-ES is a stochastic, derivative-free algorithm fornumerical optimization of non-linear, non-convex optimization problems incontinuous domain. Inspired by the limited memory BFGS method of Liu andNocedal (1989), the LM-CMA-ES samples candidate solutions according to acovariance matrix reproduced from $m$ direction vectors selected during theoptimization process. The decomposition of the covariance matrix into Choleskyfactors allows to reduce the time and memory complexity of the sampling to$O(mn)$, where $n$ is the number of decision variables. When $n$ is large(e.g., $n$ > 1000), even relatively small values of $m$ (e.g., $m=20,30$) aresufficient to efficiently solve fully non-separable problems and to reduce theoverall run-time.
arxiv-1404-5144 | Influence of the learning method in the performance of feedforward neural networks when the activity of neurons is modified |  http://arxiv.org/abs/1404.5144  | author:M. Konomi, G. M. Sacha category:cs.NE published:2014-04-21 summary:A method that allows us to give a different treatment to any neuron insidefeedforward neural networks is presented. The algorithm has been implementedwith two very different learning methods: a standard Back-propagation (BP)procedure and an evolutionary algorithm. First, we have demonstrated that theEA training method converges faster and gives more accurate results than BP.Then we have made a full analysis of the effects of turning off differentcombinations of neurons after the training phase. We demonstrate that EA ismuch more robust than BP for all the cases under study. Even in the case whentwo hidden neurons are lost, EA training is still able to give good averageresults. This difference implies that we must be very careful when pruning orredundancy effects are being studied since the network performance when losingneurons strongly depends on the training method. Moreover, the influence of theindividual inputs will also depend on the training algorithm. Since EA keeps agood classification performance when units are lost, this method could be agood way to simulate biological learning systems since they must be robustagainst deficient neuron performance. Although biological systems are much morecomplex than the simulations shown in this article, we propose that a smarttraining strategy such as the one shown here could be considered as a firstprotection against the losing of a certain number of neurons.
arxiv-1404-5344 | A higher-order MRF based variational model for multiplicative noise reduction |  http://arxiv.org/abs/1404.5344  | author:Yunjin Chen, Wensen Feng, René Ranftl, Hong Qiao, Thomas Pock category:cs.CV published:2014-04-21 summary:The Fields of Experts (FoE) image prior model, a filter-based higher-orderMarkov Random Fields (MRF) model, has been shown to be effective for many imagerestoration problems. Motivated by the successes of FoE-based approaches, inthis letter, we propose a novel variational model for multiplicative noisereduction based on the FoE image prior model. The resulted model corresponds toa non-convex minimization problem, which can be solved by a recently publishednon-convex optimization algorithm. Experimental results based on syntheticspeckle noise and real synthetic aperture radar (SAR) images suggest that theperformance of our proposed method is on par with the best publisheddespeckling algorithm. Besides, our proposed model comes along with anadditional advantage, that the inference is extremely efficient. {Our GPU basedimplementation takes less than 1s to produce state-of-the-art despecklingperformance.}
arxiv-1404-5009 | Efficient Semidefinite Branch-and-Cut for MAP-MRF Inference |  http://arxiv.org/abs/1404.5009  | author:Peng Wang, Chunhua Shen, Anton van den Hengel, Philip Torr category:cs.CV cs.LG cs.NA published:2014-04-20 summary:We propose a Branch-and-Cut (B&C) method for solving general MAP-MRFinference problems. The core of our method is a very efficient boundingprocedure, which combines scalable semidefinite programming (SDP) and acutting-plane method for seeking violated constraints. In order to furtherspeed up the computation, several strategies have been exploited, includingmodel reduction, warm start and removal of inactive constraints. We analyze the performance of the proposed method under different settings,and demonstrate that our method either outperforms or performs on par withstate-of-the-art approaches. Especially when the connectivities are dense orwhen the relative magnitudes of the unary costs are low, we achieve the bestreported results. Experiments show that the proposed algorithm achieves betterapproximation than the state-of-the-art methods within a variety of timebudgets on challenging non-submodular MAP-MRF inference problems.
arxiv-1404-5065 | Multi-Target Regression via Random Linear Target Combinations |  http://arxiv.org/abs/1404.5065  | author:Grigorios Tsoumakas, Eleftherios Spyromitros-Xioufis, Aikaterini Vrekou, Ioannis Vlahavas category:cs.LG published:2014-04-20 summary:Multi-target regression is concerned with the simultaneous prediction ofmultiple continuous target variables based on the same set of input variables.It arises in several interesting industrial and environmental applicationdomains, such as ecological modelling and energy forecasting. This paperpresents an ensemble method for multi-target regression that constructs newtarget variables via random linear combinations of existing targets. We discussthe connection of our approach with multi-label classification algorithms, inparticular RA$k$EL, which originally inspired this work, and a family of recentmulti-label classification algorithms that involve output coding. Experimentalresults on 12 multi-target datasets show that it performs significantly betterthan a strong baseline that learns a single model for each target usinggradient boosting and compares favourably to multi-objective random forestapproach, which is a state-of-the-art approach. The experiments further showthat our approach improves more when stronger unconditional dependencies existamong the targets.
arxiv-1404-5028 | Clustering via Mode Seeking by Direct Estimation of the Gradient of a Log-Density |  http://arxiv.org/abs/1404.5028  | author:Hiroaki Sasaki, Aapo Hyvärinen, Masashi Sugiyama category:stat.ML published:2014-04-20 summary:Mean shift clustering finds the modes of the data probability density byidentifying the zero points of the density gradient. Since it does not requireto fix the number of clusters in advance, the mean shift has been a popularclustering algorithm in various application fields. A typical implementation ofthe mean shift is to first estimate the density by kernel density estimationand then compute its gradient. However, since good density estimation does notnecessarily imply accurate estimation of the density gradient, such an indirecttwo-step approach is not reliable. In this paper, we propose a method todirectly estimate the gradient of the log-density without going through densityestimation. The proposed method gives the global solution analytically and thusis computationally efficient. We then develop a mean-shift-like fixed-pointalgorithm to find the modes of the density for clustering. As in the meanshift, one does not need to set the number of clusters in advance. Weempirically show that the proposed clustering method works much better than themean shift especially for high-dimensional data. Experimental results furtherindicate that the proposed method outperforms existing clustering methods.
arxiv-1404-4923 | Unified Structured Learning for Simultaneous Human Pose Estimation and Garment Attribute Classification |  http://arxiv.org/abs/1404.4923  | author:Jie Shen, Guangcan Liu, Jia Chen, Yuqiang Fang, Jianbin Xie, Yong Yu, Shuicheng Yan category:cs.CV published:2014-04-19 summary:In this paper, we utilize structured learning to simultaneously address twointertwined problems: human pose estimation (HPE) and garment attributeclassification (GAC), which are valuable for a variety of computer vision andmultimedia applications. Unlike previous works that usually handle the twoproblems separately, our approach aims to produce a jointly optimal estimationfor both HPE and GAC via a unified inference procedure. To this end, we adopt apreprocessing step to detect potential human parts from each image (i.e., a setof "candidates") that allows us to have a manageable input space. In this way,the simultaneous inference of HPE and GAC is converted to a structured learningproblem, where the inputs are the collections of candidate ensembles, theoutputs are the joint labels of human parts and garment attributes, and thejoint feature representation involves various cues such as pose-specificfeatures, garment-specific features, and cross-task features that encodecorrelations between human parts and garment attributes. Furthermore, weexplore the "strong edge" evidence around the potential human parts so as toderive more powerful representations for oriented human parts. Such evidencescan be seamlessly integrated into our structured learning model as a kind ofenergy function, and the learning process could be performed by standardstructured Support Vector Machines (SVM) algorithm. However, the jointstructure of the two problems is a cyclic graph, which hinders efficientinference. To resolve this issue, we compute instead approximate optima byusing an iterative procedure, where in each iteration the variables of oneproblem are fixed. In this way, satisfactory solutions can be efficientlycomputed by dynamic programming. Experimental results on two benchmark datasetsshow the state-of-the-art performance of our approach.
arxiv-1404-4997 | Tight bounds for learning a mixture of two gaussians |  http://arxiv.org/abs/1404.4997  | author:Moritz Hardt, Eric Price category:cs.LG cs.DS stat.ML published:2014-04-19 summary:We consider the problem of identifying the parameters of an unknown mixtureof two arbitrary $d$-dimensional gaussians from a sequence of independentrandom samples. Our main results are upper and lower bounds giving acomputationally efficient moment-based estimator with an optimal convergencerate, thus resolving a problem introduced by Pearson (1894). Denoting by$\sigma^2$ the variance of the unknown mixture, we prove that$\Theta(\sigma^{12})$ samples are necessary and sufficient to estimate eachparameter up to constant additive error when $d=1.$ Our upper bound extends toarbitrary dimension $d>1$ up to a (provably necessary) logarithmic loss in $d$using a novel---yet simple---dimensionality reduction technique. We furtheridentify several interesting special cases where the sample complexity isnotably smaller than our optimal worst-case bound. For instance, if the meansof the two components are separated by $\Omega(\sigma)$ the sample complexityreduces to $O(\sigma^2)$ and this is again optimal. Our results also apply to learning each component of the mixture up to smallerror in total variation distance, where our algorithm gives strongimprovements in sample complexity over previous work. We also extend our lowerbound to mixtures of $k$ Gaussians, showing that $\Omega(\sigma^{6k-2})$samples are necessary to estimate each parameter up to constant additive error.
arxiv-1404-4960 | Agent Behavior Prediction and Its Generalization Analysis |  http://arxiv.org/abs/1404.4960  | author:Fei Tian, Haifang Li, Wei Chen, Tao Qin, Enhong Chen, Tie-Yan Liu category:cs.LG published:2014-04-19 summary:Machine learning algorithms have been applied to predict agent behaviors inreal-world dynamic systems, such as advertiser behaviors in sponsored searchand worker behaviors in crowdsourcing. The behavior data in these systems aregenerated by live agents: once the systems change due to the adoption of theprediction models learnt from the behavior data, agents will observe andrespond to these changes by changing their own behaviors accordingly. As aresult, the behavior data will evolve and will not be identically andindependently distributed, posing great challenges to the theoretical analysison the machine learning algorithms for behavior prediction. To tackle thischallenge, in this paper, we propose to use Markov Chain in Random Environments(MCRE) to describe the behavior data, and perform generalization analysis ofthe machine learning algorithms on its basis. Since the one-step transitionprobability matrix of MCRE depends on both previous states and the randomenvironment, conventional techniques for generalization analysis cannot bedirectly applied. To address this issue, we propose a novel technique thattransforms the original MCRE into a higher-dimensional time-homogeneous Markovchain. The new Markov chain involves more variables but is more regular, andthus easier to deal with. We prove the convergence of the new Markov chain whentime approaches infinity. Then we prove a generalization bound for the machinelearning algorithms on the behavior data generated by the new Markov chain,which depends on both the Markovian parameters and the covering number of thefunction class compounded by the loss function for behavior prediction and thebehavior prediction model. To the best of our knowledge, this is the first workthat performs the generalization analysis on data generated by complexprocesses in real-world dynamic systems.
arxiv-1404-4935 | Opinion Mining In Hindi Language: A Survey |  http://arxiv.org/abs/1404.4935  | author:Richa Sharma, Shweta Nigam, Rekha Jain category:cs.IR cs.CL published:2014-04-19 summary:Opinions are very important in the life of human beings. These Opinionshelped the humans to carry out the decisions. As the impact of the Web isincreasing day by day, Web documents can be seen as a new source of opinion forhuman beings. Web contains a huge amount of information generated by the usersthrough blogs, forum entries, and social networking websites and so on Toanalyze this large amount of information it is required to develop a methodthat automatically classifies the information available on the Web. This domainis called Sentiment Analysis and Opinion Mining. Opinion Mining or SentimentAnalysis is a natural language processing task that mine information fromvarious text forms such as reviews, news, and blogs and classify them on thebasis of their polarity as positive, negative or neutral. But, from the lastfew years, enormous increase has been seen in Hindi language on the Web.Research in opinion mining mostly carried out in English language but it isvery important to perform the opinion mining in Hindi language also as largeamount of information in Hindi is also available on the Web. This paper givesan overview of the work that has been done Hindi language.
arxiv-1404-4942 | Geometric Abstraction from Noisy Image-Based 3D Reconstructions |  http://arxiv.org/abs/1404.4942  | author:Thomas Holzmann, Christof Hoppe, Stefan Kluckner, Horst Bischof category:cs.CV published:2014-04-19 summary:Creating geometric abstracted models from image-based scene reconstructionsis difficult due to noise and irregularities in the reconstructed model. Inthis paper, we present a geometric modeling method for noisy reconstructionsdominated by planar horizontal and orthogonal vertical structures. We partitionthe scene into horizontal slices and create an inside/outside labelingrepresented by a floor plan for each slice by solving an energy minimizationproblem. Consecutively, we create an irregular discretization of the volumeaccording to the individual floor plans and again label each cell asinside/outside by minimizing an energy function. By adjusting the smoothnessparameter, we introduce different levels of detail. In our experiments, we showresults with varying regularization levels using synthetically generated andreal-world data.
arxiv-1404-4714 | Radical-Enhanced Chinese Character Embedding |  http://arxiv.org/abs/1404.4714  | author:Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhenzhou Ji, Xiaolong Wang category:cs.CL published:2014-04-18 summary:We present a method to leverage radical for learning Chinese characterembedding. Radical is a semantic and phonetic component of Chinese character.It plays an important role as characters with the same radical usually havesimilar semantic meaning and grammatical usage. However, existing Chineseprocessing algorithms typically regard word or character as the basic unit butignore the crucial radical information. In this paper, we fill this gap byleveraging radical for learning continuous representation of Chinese character.We develop a dedicated neural architecture to effectively learn characterembedding and apply it on Chinese character similarity judgement and Chineseword segmentation. Experiment results show that our radical-enhanced methodoutperforms existing embedding learning algorithms on both tasks.
arxiv-1404-4702 | Nearly Tight Bounds on $\ell_1$ Approximation of Self-Bounding Functions |  http://arxiv.org/abs/1404.4702  | author:Vitaly Feldman, Pravesh Kothari, Jan Vondrák category:cs.LG cs.DS published:2014-04-18 summary:We study the complexity of learning and approximation of self-boundingfunctions over the uniform distribution on the Boolean hypercube ${0,1}^n$.Informally, a function $f:{0,1}^n \rightarrow \mathbb{R}$ is self-bounding iffor every $x \in {0,1}^n$, $f(x)$ upper bounds the sum of all the $n$ marginaldecreases in the value of the function at $x$. Self-bounding functions includesuch well-known classes of functions as submodular and fractionally-subadditive(XOS) functions. They were introduced by Boucheron et al in the context ofconcentration of measure inequalities. Our main result is a nearly tight$\ell_1$-approximation of self-bounding functions by low-degree juntas.Specifically, all self-bounding functions can be $\epsilon$-approximated in$\ell_1$ by a polynomial of degree $\tilde{O}(1/\epsilon)$ over$2^{\tilde{O}(1/\epsilon)}$ variables. Both the degree and junta-size areoptimal up to logarithmic terms. Previously, the best known bound was$O(1/\epsilon^{2})$ on the degree and $2^{O(1/\epsilon^2)}$ on the number ofvariables (Feldman and Vondr \'{a}k 2013). These results lead to improved andin several cases almost tight bounds for PAC and agnostic learning ofsubmodular, XOS and self-bounding functions. In particular, assuming hardnessof learning juntas, we show that PAC and agnostic learning of self-boundingfunctions have complexity of $n^{\tilde{\Theta}(1/\epsilon)}$.
arxiv-1404-4812 | Beyond Bell's Theorem II: Scenarios with arbitrary causal structure |  http://arxiv.org/abs/1404.4812  | author:Tobias Fritz category:quant-ph stat.ML published:2014-04-18 summary:It has recently been found that Bell scenarios are only a small subclass ofinteresting setups for studying the non-classical features of quantum theorywithin spacetime. We find that it is possible to talk about classicalcorrelations, quantum correlations and other kinds of correlations on anydirected acyclic graph, and this captures various extensions of Bell scenarioswhich have been considered in the literature. From a conceptual point of view,the main feature of our approach is its high level of unification: while thenotions of source, choice of setting and measurement play all seeminglydifferent roles in a Bell scenario, our formalism shows that they are allinstances of the same concept of "event". Our work can also be understood as a contribution to the subject of causalinference with latent variables. Among other things, we introduce hiddenBayesian networks as a generalization of hidden Markov models.
arxiv-1404-4893 | CTBNCToolkit: Continuous Time Bayesian Network Classifier Toolkit |  http://arxiv.org/abs/1404.4893  | author:Daniele Codecasa, Fabio Stella category:cs.AI cs.LG cs.MS published:2014-04-18 summary:Continuous time Bayesian network classifiers are designed for temporalclassification of multivariate streaming data when time duration of eventsmatters and the class does not change over time. This paper introduces theCTBNCToolkit: an open source Java toolkit which provides a stand-aloneapplication for temporal classification and a library for continuous timeBayesian network classifiers. CTBNCToolkit implements the inference algorithm,the parameter learning algorithm, and the structural learning algorithm forcontinuous time Bayesian network classifiers. The structural learning algorithmis based on scoring functions: the marginal log-likelihood score and theconditional log-likelihood score are provided. CTBNCToolkit provides also animplementation of the expectation maximization algorithm for clusteringpurpose. The paper introduces continuous time Bayesian network classifiers. Howto use the CTBNToolkit from the command line is described in a specificsection. Tutorial examples are included to facilitate users to understand howthe toolkit must be used. A section dedicate to the Java library is proposed tohelp further code extensions.
arxiv-1404-5511 | Coactive Learning for Locally Optimal Problem Solving |  http://arxiv.org/abs/1404.5511  | author:Robby Goetschalckx, Alan Fern, Prasad Tadepalli category:cs.LG published:2014-04-18 summary:Coactive learning is an online problem solving setting where the solutionsprovided by a solver are interactively improved by a domain expert, which inturn drives learning. In this paper we extend the study of coactive learning toproblems where obtaining a globally optimal or near-optimal solution may beintractable or where an expert can only be expected to make small, localimprovements to a candidate solution. The goal of learning in this new settingis to minimize the cost as measured by the expert effort over time. We firstestablish theoretical bounds on the average cost of the existing coactivePerceptron algorithm. In addition, we consider new online algorithms that usecost-sensitive and Passive-Aggressive (PA) updates, showing similar or improvedtheoretical bounds. We provide an empirical evaluation of the learners invarious domains, which show that the Perceptron based algorithms are quiteeffective and that unlike the case for online classification, the PA algorithmsdo not yield significant performance gains.
arxiv-1404-4880 | Bias Correction and Modified Profile Likelihood under the Wishart Complex Distribution |  http://arxiv.org/abs/1404.4880  | author:Abraão D. C. Nascimento, Alejandro C. Frery, Renato J. Cintra category:cs.CV stat.ME published:2014-04-18 summary:This paper proposes improved methods for the maximum likelihood (ML)estimation of the equivalent number of looks $L$. This parameter has ameaningful interpretation in the context of polarimetric synthetic apertureradar (PolSAR) images. Due to the presence of coherent illumination in theirprocessing, PolSAR systems generate images which present a granular noisecalled speckle. As a potential solution for reducing such interference, theparameter $L$ controls the signal-noise ratio. Thus, the proposal of efficientestimation methodologies for $L$ has been sought. To that end, we considerfirstly that a PolSAR image is well described by the scaled complex Wishartdistribution. In recent years, Anfinsen et al. derived and analyzed estimationmethods based on the ML and on trace statistical moments for obtaining theparameter $L$ of the unscaled version of such probability law. This papergeneralizes that approach. We present the second-order bias expression proposedby Cox and Snell for the ML estimator of this parameter. Moreover, the formulaof the profile likelihood modified by Barndorff-Nielsen in terms of $L$ isdiscussed. Such derivations yield two new ML estimators for the parameter $L$,which are compared to the estimators proposed by Anfinsen et al. Theperformance of these estimators is assessed by means of Monte Carloexperiments, adopting three statistical measures as comparison criterion: themean square error, the bias, and the coefficient of variation. Equivalently tothe simulation study, an application to actual PolSAR data concludes that theproposed estimators outperform all the others in homogeneous scenarios.
arxiv-1404-4805 | iPiano: Inertial Proximal Algorithm for Non-Convex Optimization |  http://arxiv.org/abs/1404.4805  | author:Peter Ochs, Yunjin Chen, Thomas Brox, Thomas Pock category:cs.CV math.OC published:2014-04-18 summary:In this paper we study an algorithm for solving a minimization problemcomposed of a differentiable (possibly non-convex) and a convex (possiblynon-differentiable) function. The algorithm iPiano combines forward-backwardsplitting with an inertial force. It can be seen as a non-smooth split versionof the Heavy-ball method from Polyak. A rigorous analysis of the algorithm forthe proposed class of problems yields global convergence of the function valuesand the arguments. This makes the algorithm robust for usage on non-convexproblems. The convergence result is obtained based on the \KL inequality. Thisis a very weak restriction, which was used to prove convergence for severalother gradient methods. First, an abstract convergence theorem for a genericalgorithm is proved, and, then iPiano is shown to satisfy the requirements ofthis theorem. Furthermore, a convergence rate is established for the generalproblem class. We demonstrate iPiano on computer vision problems: imagedenoising with learned priors and diffusion based image compression.
arxiv-1404-4780 | Robust Face Recognition via Adaptive Sparse Representation |  http://arxiv.org/abs/1404.4780  | author:Jing Wang, Canyi Lu, Meng Wang, Peipei Li, Shuicheng Yan, Xuegang Hu category:cs.CV published:2014-04-18 summary:Sparse Representation (or coding) based Classification (SRC) has gained greatsuccess in face recognition in recent years. However, SRC emphasizes thesparsity too much and overlooks the correlation information which has beendemonstrated to be critical in real-world face recognition problems. Besides,some work considers the correlation but overlooks the discriminative ability ofsparsity. Different from these existing techniques, in this paper, we propose aframework called Adaptive Sparse Representation based Classification (ASRC) inwhich sparsity and correlation are jointly considered. Specifically, when thesamples are of low correlation, ASRC selects the most discriminative samplesfor representation, like SRC; when the training samples are highly correlated,ASRC selects most of the correlated and discriminative samples forrepresentation, rather than choosing some related samples randomly. In general,the representation model is adaptive to the correlation structure, whichbenefits from both $\ell_1$-norm and $\ell_2$-norm. Extensive experiments conducted on publicly available data sets verify theeffectiveness and robustness of the proposed algorithm by comparing it withstate-of-the-art methods.
arxiv-1404-4888 | Supervised detection of anomalous light-curves in massive astronomical catalogs |  http://arxiv.org/abs/1404.4888  | author:Isadora Nun, Karim Pichara, Pavlos Protopapas, Dae-Won Kim category:cs.CE astro-ph.IM cs.LG published:2014-04-18 summary:The development of synoptic sky surveys has led to a massive amount of datafor which resources needed for analysis are beyond human capabilities. Toprocess this information and to extract all possible knowledge, machinelearning techniques become necessary. Here we present a new method toautomatically discover unknown variable objects in large astronomical catalogs.With the aim of taking full advantage of all the information we have aboutknown objects, our method is based on a supervised algorithm. In particular, wetrain a random forest classifier using known variability classes of objects andobtain votes for each of the objects in the training set. We then model thisvoting distribution with a Bayesian network and obtain the joint votingdistribution among the training objects. Consequently, an unknown object isconsidered as an outlier insofar it has a low joint probability. Our method issuitable for exploring massive datasets given that the training process isperformed offline. We tested our algorithm on 20 millions light-curves from theMACHO catalog and generated a list of anomalous candidates. We divided thecandidates into two main classes of outliers: artifacts and intrinsic outliers.Artifacts were principally due to air mass variation, seasonal variation, badcalibration or instrumental errors and were consequently removed from ouroutlier list and added to the training set. After retraining, we selected about4000 objects, which we passed to a post analysis stage by perfoming across-match with all publicly available catalogs. Within these candidates weidentified certain known but rare objects such as eclipsing Cepheids, bluevariables, cataclysmic variables and X-ray sources. For some outliers therewere no additional information. Among them we identified three unknownvariability types and few individual outliers that will be followed up for adeeper analysis.
arxiv-1404-4774 | Online Group Feature Selection |  http://arxiv.org/abs/1404.4774  | author:Wang Jing, Zhao Zhong-Qiu, Hu Xuegang, Cheung Yiu-ming, Wang Meng, Wu Xindong category:cs.CV published:2014-04-18 summary:Online feature selection with dynamic features has become an active researcharea in recent years. However, in some real-world applications such as imageanalysis and email spam filtering, features may arrive by groups. Existingonline feature selection methods evaluate features individually, while existinggroup feature selection methods cannot handle online processing. Motivated bythis, we formulate the online group feature selection problem, and propose anovel selection approach for this problem. Our proposed approach consists oftwo stages: online intra-group selection and online inter-group selection. Inthe intra-group selection, we use spectral analysis to select discriminativefeatures in each group when it arrives. In the inter-group selection, we useLasso to select a globally optimal subset of features. This 2-stage procedurecontinues until there are no more features to come or some predefined stoppingconditions are met. Extensive experiments conducted on benchmark and real-worlddata sets demonstrate that our proposed approach outperforms otherstate-of-the-art online feature selection methods.
arxiv-1404-4740 | Challenges in Persian Electronic Text Analysis |  http://arxiv.org/abs/1404.4740  | author:Behrang QasemiZadeh, Saeed Rahimi, Mehdi Safaee Ghalati category:cs.CL 68T50 I.2.7 published:2014-04-18 summary:Farsi, also known as Persian, is the official language of Iran and Tajikistanand one of the two main languages spoken in Afghanistan. Farsi enjoys a unifiedArabic script as its writing system. In this paper we briefly introduce thewriting standards of Farsi and highlight problems one would face when analyzingFarsi electronic texts, especially during development of Farsi corporaregarding to transcription and encoding of Farsi e-texts. The pointes mentionedmay sounds easy but they are crucial when developing and processing writtencorpora of Farsi.
arxiv-1404-4797 | Parallel Graph Partitioning for Complex Networks |  http://arxiv.org/abs/1404.4797  | author:Henning Meyerhenke, Peter Sanders, Christian Schulz category:cs.DC cs.DS cs.NE cs.SI physics.soc-ph published:2014-04-18 summary:Processing large complex networks like social networks or web graphs hasrecently attracted considerable interest. In order to do this in parallel, weneed to partition them into pieces of about equal size. Unfortunately, previousparallel graph partitioners originally developed for more regular mesh-likenetworks do not work well for these networks. This paper addresses this problemby parallelizing and adapting the label propagation technique originallydeveloped for graph clustering. By introducing size constraints, labelpropagation becomes applicable for both the coarsening and the refinement phaseof multilevel graph partitioning. We obtain very high quality by applying ahighly parallel evolutionary algorithm to the coarsened graph. The resultingsystem is both more scalable and achieves higher quality than state-of-the-artsystems like ParMetis or PT-Scotch. For large complex networks the performancedifferences are very big. For example, our algorithm can partition a web graphwith 3.3 billion edges in less than sixteen seconds using 512 cores of a highperformance cluster while producing a high quality partition -- none of thecompeting systems can handle this graph on our system.
arxiv-1404-4572 | The First Parallel Multilingual Corpus of Persian: Toward a Persian BLARK |  http://arxiv.org/abs/1404.4572  | author:Behrang Qasemizadeh, Saeed Rahimi, Behrooz Mahmoodi Bakhtiari category:cs.CL 68T50 I.2.7 published:2014-04-17 summary:In this article, we have introduced the first parallel corpus of Persian withmore than 10 other European languages. This article describes primary stepstoward preparing a Basic Language Resources Kit (BLARK) for Persian. Up to now,we have proposed morphosyntactic specification of Persian based onEAGLE/MULTEXT guidelines and specific resources of MULTEXT-East. The articleintroduces Persian Language, with emphasis on its orthography andmorphosyntactic features, then a new Part-of-Speech categorization andorthography for Persian in digital environments is proposed. Finally, thecorpus and related statistic will be analyzed.
arxiv-1404-4641 | Multilingual Models for Compositional Distributed Semantics |  http://arxiv.org/abs/1404.4641  | author:Karl Moritz Hermann, Phil Blunsom category:cs.CL published:2014-04-17 summary:We present a novel technique for learning semantic representations, whichextends the distributional hypothesis to multilingual data and joint-spaceembeddings. Our models leverage parallel data and learn to strongly align theembeddings of semantically equivalent sentences, while maintaining sufficientdistance between those of dissimilar sentences. The models do not rely on wordalignments or any syntactic information and are successfully applied to anumber of diverse languages. We extend our approach to learn semanticrepresentations at the document level, too. We evaluate these models on twocross-lingual document classification tasks, outperforming the prior state ofthe art. Through qualitative analysis and the study of pivoting effects wedemonstrate that our representations are semantically plausible and can capturesemantic relationships across languages without parallel data.
arxiv-1404-4412 | Efficient Nonnegative Tucker Decompositions: Algorithms and Uniqueness |  http://arxiv.org/abs/1404.4412  | author:Guoxu Zhou, Andrzej Cichocki, Qibin Zhao, Shengli Xie category:cs.LG cs.CV stat.ML published:2014-04-17 summary:Nonnegative Tucker decomposition (NTD) is a powerful tool for the extractionof nonnegative parts-based and physically meaningful latent components fromhigh-dimensional tensor data while preserving the natural multilinear structureof data. However, as the data tensor often has multiple modes and islarge-scale, existing NTD algorithms suffer from a very high computationalcomplexity in terms of both storage and computation time, which has been onemajor obstacle for practical applications of NTD. To overcome thesedisadvantages, we show how low (multilinear) rank approximation (LRA) oftensors is able to significantly simplify the computation of the gradients ofthe cost function, upon which a family of efficient first-order NTD algorithmsare developed. Besides dramatically reducing the storage complexity and runningtime, the new algorithms are quite flexible and robust to noise because anywell-established LRA approaches can be applied. We also show how nonnegativityincorporating sparsity substantially improves the uniqueness property andpartially alleviates the curse of dimensionality of the Tucker decompositions.Simulation results on synthetic and real-world data justify the validity andhigh efficiency of the proposed NTD algorithms.
arxiv-1404-4646 | Advancing Matrix Completion by Modeling Extra Structures beyond Low-Rankness |  http://arxiv.org/abs/1404.4646  | author:Guangcan Liu, Ping Li category:stat.ME cs.IT cs.LG math.IT math.ST stat.TH published:2014-04-17 summary:A well-known method for completing low-rank matrices based on convexoptimization has been established by Cand{\`e}s and Recht. Althoughtheoretically complete, the method may not entirely solve the low-rank matrixcompletion problem. This is because the method captures only the low-ranknessproperty which gives merely a rough constraint that the data points locate onsome low-dimensional subspace, but generally ignores the extra structures whichspecify in more detail how the data points locate on the subspace. Whenever thegeometric distribution of the data points is not uniform, the coherenceparameters of data might be large and, accordingly, the method might fail evenif the latent matrix we want to recover is fairly low-rank. To better handlenon-uniform data, in this paper we propose a method termed Low-Rank FactorDecomposition (LRFD), which imposes an additional restriction that the datapoints must be represented as linear combinations of the bases in a dictionaryconstructed or learnt in advance. We show that LRFD can well handle non-uniformdata, provided that the dictionary is configured properly: We mathematicallyprove that if the dictionary itself is low-rank then LRFD is immune to thecoherence parameters which might be large on non-uniform data. This provides anelementary principle for learning the dictionary in LRFD and, naturally, leadsto a practical algorithm for advancing matrix completion. Extensive experimentson randomly generated matrices and motion datasets show encouraging results.
arxiv-1404-4467 | Cube-Cut: Vertebral Body Segmentation in MRI-Data through Cubic-Shaped Divergences |  http://arxiv.org/abs/1404.4467  | author:Robert Schwarzenberg, Bernd Freisleben, Christopher Nimsky, Jan Egger category:cs.CV published:2014-04-17 summary:In this article, we present a graph-based method using a cubic template forvolumetric segmentation of vertebrae in magnetic resonance imaging (MRI)acquisitions. The user can define the degree of deviation from a regular cubevia a smoothness value Delta. The Cube-Cut algorithm generates a directed graphwith two terminal nodes (s-t-network), where the nodes of the graph correspondto a cubic-shaped subset of the image's voxels. The weightings of the graph'sterminal edges, which connect every node with a virtual source s or a virtualsink t, represent the affinity of a voxel to the vertebra (source) and to thebackground (sink). Furthermore, a set of infinite weighted and non-terminaledges implements the smoothness term. After graph construction, a minimals-t-cut is calculated within polynomial computation time, which splits thenodes into two disjoint units. Subsequently, the segmentation result isdetermined out of the source-set. A quantitative evaluation of a C++implementation of the algorithm resulted in an average Dice SimilarityCoefficient (DSC) of 81.33% and a running time of less than a minute.
arxiv-1404-4644 | A New Space for Comparing Graphs |  http://arxiv.org/abs/1404.4644  | author:Anshumali Shrivastava, Ping Li category:stat.ME cs.IR cs.LG stat.ML published:2014-04-17 summary:Finding a new mathematical representations for graph, which allows directcomparison between different graph structures, is an open-ended researchdirection. Having such a representation is the first prerequisite for a varietyof machine learning algorithms like classification, clustering, etc., overgraph datasets. In this paper, we propose a symmetric positive semidefinitematrix with the $(i,j)$-{th} entry equal to the covariance between normalizedvectors $A^ie$ and $A^je$ ($e$ being vector of all ones) as a representationfor graph with adjacency matrix $A$. We show that the proposed matrixrepresentation encodes the spectrum of the underlying adjacency matrix and italso contains information about the counts of small sub-structures present inthe graph such as triangles and small paths. In addition, we show that thismatrix is a \emph{"graph invariant"}. All these properties make the proposedmatrix a suitable object for representing graphs. The representation, being a covariance matrix in a fixed dimensional metricspace, gives a mathematical embedding for graphs. This naturally leads to ameasure of similarity on graph objects. We define similarity between two givengraphs as a Bhattacharya similarity measure between their correspondingcovariance matrix representations. As shown in our experimental study on thetask of social network classification, such a similarity measure outperformsother widely used state-of-the-art methodologies. Our proposed method is alsocomputationally efficient. The computation of both the matrix representationand the similarity value can be performed in operations linear in the number ofedges. This makes our method scalable in practice. We believe our theoretical and empirical results provide evidence forstudying truncated power iterations, of the adjacency matrix, to characterizesocial networks.
arxiv-1404-4655 | Hierarchical Quasi-Clustering Methods for Asymmetric Networks |  http://arxiv.org/abs/1404.4655  | author:Gunnar Carlsson, Facundo Mémoli, Alejandro Ribeiro, Santiago Segarra category:cs.LG stat.ML published:2014-04-17 summary:This paper introduces hierarchical quasi-clustering methods, a generalizationof hierarchical clustering for asymmetric networks where the output structurepreserves the asymmetry of the input data. We show that this output structureis equivalent to a finite quasi-ultrametric space and study admissibility withrespect to two desirable properties. We prove that a modified version of singlelinkage is the only admissible quasi-clustering method. Moreover, we showstability of the proposed method and we establish invariance propertiesfulfilled by it. Algorithms are further developed and the value ofquasi-clustering analysis is illustrated with a study of internal migrationwithin United States.
arxiv-1404-4661 | Learning Fine-grained Image Similarity with Deep Ranking |  http://arxiv.org/abs/1404.4661  | author:Jiang Wang, Yang song, Thomas Leung, Chuck Rosenberg, Jinbin Wang, James Philbin, Bo Chen, Ying Wu category:cs.CV published:2014-04-17 summary:Learning fine-grained image similarity is a challenging task. It needs tocapture between-class and within-class image differences. This paper proposes adeep ranking model that employs deep learning techniques to learn similaritymetric directly from images.It has higher learning capability than models basedon hand-crafted features. A novel multiscale network structure has beendeveloped to describe the images effectively. An efficient triplet samplingalgorithm is proposed to learn the model with distributed asynchronizedstochastic gradient. Extensive experiments show that the proposed algorithmoutperforms models based on hand-crafted visual features and deepclassification models.
arxiv-1404-4408 | Geometric Inference for General High-Dimensional Linear Inverse Problems |  http://arxiv.org/abs/1404.4408  | author:T. Tony Cai, Tengyuan Liang, Alexander Rakhlin category:math.ST stat.ML stat.TH published:2014-04-17 summary:This paper presents a unified geometric framework for the statisticalanalysis of a general ill-posed linear inverse model which includes as specialcases noisy compressed sensing, sign vector recovery, trace regression,orthogonal matrix estimation, and noisy matrix completion. We proposecomputationally feasible convex programs for statistical inference includingestimation, confidence intervals and hypothesis testing. A theoreticalframework is developed to characterize the local estimation rate of convergenceand to provide statistical inference guarantees. Our results are built based onthe local conic geometry and duality. The difficulty of statistical inferenceis captured by the geometric characterization of the local tangent cone throughthe Gaussian width and Sudakov minoration estimate.
arxiv-1404-4667 | Subspace Learning and Imputation for Streaming Big Data Matrices and Tensors |  http://arxiv.org/abs/1404.4667  | author:Morteza Mardani, Gonzalo Mateos, Georgios B. Giannakis category:stat.ML cs.IT cs.LG math.IT published:2014-04-17 summary:Extracting latent low-dimensional structure from high-dimensional data is ofparamount importance in timely inference tasks encountered with `Big Data'analytics. However, increasingly noisy, heterogeneous, and incomplete datasetsas well as the need for {\em real-time} processing of streaming data pose majorchallenges to this end. In this context, the present paper permeates benefitsfrom rank minimization to scalable imputation of missing data, via trackinglow-dimensional subspaces and unraveling latent (possibly multi-way) structurefrom \emph{incomplete streaming} data. For low-rank matrix data, a subspaceestimator is proposed based on an exponentially-weighted least-squarescriterion regularized with the nuclear norm. After recasting the non-separablenuclear norm into a form amenable to online optimization, real-time algorithmswith complementary strengths are developed and their convergence is establishedunder simplifying technical assumptions. In a stationary setting, theasymptotic estimates obtained offer the well-documented performance guaranteesof the {\em batch} nuclear-norm regularized estimator. Under the same unifyingframework, a novel online (adaptive) algorithm is developed to obtain multi-waydecompositions of \emph{low-rank tensors} with missing entries, and performimputation as a byproduct. Simulated tests with both synthetic as well as realInternet and cardiac magnetic resonance imagery (MRI) data confirm the efficacyof the proposed algorithms, and their superior performance relative tostate-of-the-art alternatives.
arxiv-1404-4171 | Dropout Training for Support Vector Machines |  http://arxiv.org/abs/1404.4171  | author:Ning Chen, Jun Zhu, Jianfei Chen, Bo Zhang category:cs.LG published:2014-04-16 summary:Dropout and other feature noising schemes have shown promising results incontrolling over-fitting by artificially corrupting the training data. Thoughextensive theoretical and empirical studies have been performed for generalizedlinear models, little work has been done for support vector machines (SVMs),one of the most successful approaches for supervised learning. This paperpresents dropout training for linear SVMs. To deal with the intractableexpectation of the non-smooth hinge loss under corrupting distributions, wedevelop an iteratively re-weighted least square (IRLS) algorithm by exploringdata augmentation techniques. Our algorithm iteratively minimizes theexpectation of a re-weighted least square problem, where the re-weights haveclosed-form solutions. The similar ideas are applied to develop a new IRLSalgorithm for the expected logistic loss under corrupting distributions. Ouralgorithms offer insights on the connection and difference between the hingeloss and logistic loss in dropout training. Empirical results on several realdatasets demonstrate the effectiveness of dropout training on significantlyboosting the classification accuracy of linear SVMs.
arxiv-1404-4175 | MEG Decoding Across Subjects |  http://arxiv.org/abs/1404.4175  | author:Emanuele Olivetti, Seyed Mostafa Kia, Paolo Avesani category:stat.ML cs.LG q-bio.NC published:2014-04-16 summary:Brain decoding is a data analysis paradigm for neuroimaging experiments thatis based on predicting the stimulus presented to the subject from theconcurrent brain activity. In order to make inference at the group level, astraightforward but sometimes unsuccessful approach is to train a classifier onthe trials of a group of subjects and then to test it on unseen trials from newsubjects. The extreme difficulty is related to the structural and functionalvariability across the subjects. We call this approach "decoding acrosssubjects". In this work, we address the problem of decoding across subjects formagnetoencephalographic (MEG) experiments and we provide the followingcontributions: first, we formally describe the problem and show that it belongsto a machine learning sub-field called transductive transfer learning (TTL).Second, we propose to use a simple TTL technique that accounts for thedifferences between train data and test data. Third, we propose the use ofensemble learning, and specifically of stacked generalization, to address thevariability across subjects within train data, with the aim of producing morestable classifiers. On a face vs. scramble task MEG dataset of 16 subjects, wecompare the standard approach of not modelling the differences across subjects,to the proposed one of combining TTL and ensemble learning. We show that theproposed approach is consistently more accurate than the standard one.
arxiv-1404-4606 | How Many Topics? Stability Analysis for Topic Models |  http://arxiv.org/abs/1404.4606  | author:Derek Greene, Derek O'Callaghan, Pádraig Cunningham category:cs.LG cs.CL cs.IR published:2014-04-16 summary:Topic modeling refers to the task of discovering the underlying thematicstructure in a text corpus, where the output is commonly presented as a reportof the top terms appearing in each topic. Despite the diversity of topicmodeling algorithms that have been proposed, a common challenge in successfullyapplying these techniques is the selection of an appropriate number of topicsfor a given corpus. Choosing too few topics will produce results that areoverly broad, while choosing too many will result in the "over-clustering" of acorpus into many small, highly-similar topics. In this paper, we propose aterm-centric stability analysis strategy to address this issue, the idea beingthat a model with an appropriate number of topics will be more robust toperturbations in the data. Using a topic modeling approach based on matrixfactorization, evaluations performed on a range of corpora show that thisstrategy can successfully guide the model selection process.
arxiv-1405-1965 | Automatic Annotation of Axoplasmic Reticula in Pursuit of Connectomes using High-Resolution Neural EM Data |  http://arxiv.org/abs/1405.1965  | author:Ayushi Sinha, William Gray Roncal, Narayanan Kasthuri, Jeff W. Lichtman, Randal Burns, Michael Kazhdan category:cs.CV published:2014-04-16 summary:Accurately estimating the wiring diagram of a brain, known as a connectome,at an ultrastructure level is an open research problem. Specifically, preciselytracking neural processes is difficult, especially across many image slices.Here, we propose a novel method to automatically identify and annotate smallsubcellular structures present in axons, known as axoplasmic reticula, througha 3D volume of high-resolution neural electron microscopy data. Our methodproduces high precision annotations, which can help improve automaticsegmentation by using our results as seeds for segmentation, and as cues to aidsegment merging.
arxiv-1404-4114 | Structured Stochastic Variational Inference |  http://arxiv.org/abs/1404.4114  | author:Matthew D. Hoffman, David M. Blei category:cs.LG published:2014-04-16 summary:Stochastic variational inference makes it possible to approximate posteriordistributions induced by large datasets quickly using stochastic optimization.The algorithm relies on the use of fully factorized variational distributions.However, this "mean-field" independence approximation limits the fidelity ofthe posterior approximation, and introduces local optima. We show how to relaxthe mean-field approximation to allow arbitrary dependencies between globalparameters and local hidden variables, producing better parameter estimates byreducing bias, sensitivity to local optima, and sensitivity to hyperparameters.
arxiv-1404-4351 | Stable Graphical Models |  http://arxiv.org/abs/1404.4351  | author:Navodit Misra, Ercan E. Kuruoglu category:cs.LG stat.ML published:2014-04-16 summary:Stable random variables are motivated by the central limit theorem fordensities with (potentially) unbounded variance and can be thought of asnatural generalizations of the Gaussian distribution to skewed and heavy-tailedphenomenon. In this paper, we introduce stable graphical (SG) models, a classof multivariate stable densities that can also be represented as Bayesiannetworks whose edges encode linear dependencies between random variables. Onemajor hurdle to the extensive use of stable distributions is the lack of aclosed-form analytical expression for their densities. This makes penalizedmaximum-likelihood based learning computationally demanding. We establishtheoretically that the Bayesian information criterion (BIC) can asymptoticallybe reduced to the computationally more tractable minimum dispersion criterion(MDC) and develop StabLe, a structure learning algorithm based on MDC. We usesimulated datasets for five benchmark network topologies to empiricallydemonstrate how StabLe improves upon ordinary least squares (OLS) regression.We also apply StabLe to microarray gene expression data for lymphoblastoidcells from 727 individuals belonging to eight global population groups. Weestablish that StabLe improves test set performance relative to OLS viaten-fold cross-validation. Finally, we develop SGEX, a method for quantifyingdifferential expression of genes between different population groups.
arxiv-1404-4326 | Open Question Answering with Weakly Supervised Embedding Models |  http://arxiv.org/abs/1404.4326  | author:Antoine Bordes, Jason Weston, Nicolas Usunier category:cs.CL cs.LG published:2014-04-16 summary:Building computers able to answer questions on any subject is a long standinggoal of artificial intelligence. Promising progress has recently been achievedby methods that learn to map questions to logical forms or database queries.Such approaches can be effective but at the cost of either large amounts ofhuman-labeled data or by defining lexicons and grammars tailored bypractitioners. In this paper, we instead take the radical approach of learningto map questions to vectorial feature representations. By mapping answers intothe same space one can query any knowledge base independent of its schema,without requiring any grammar or lexicon. Our method is trained with a newoptimization procedure combining stochastic gradient descent followed by afine-tuning step using the weak supervision provided by blending automaticallyand collaboratively generated resources. We empirically demonstrate that ourmodel can capture meaningful signals from its noisy supervision leading tomajor improvements over paralex, the only existing method able to be trained onsimilar weakly labeled data.
arxiv-1404-4314 | An Empirical Comparison of Parsing Methods for Stanford Dependencies |  http://arxiv.org/abs/1404.4314  | author:Lingpeng Kong, Noah A. Smith category:cs.CL published:2014-04-16 summary:Stanford typed dependencies are a widely desired representation of naturallanguage sentences, but parsing is one of the major computational bottlenecksin text analysis systems. In light of the evolving definition of the Stanforddependencies and developments in statistical dependency parsing algorithms,this paper revisits the question of Cer et al. (2010): what is the tradeoffbetween accuracy and speed in obtaining Stanford dependencies in particular? Wealso explore the effects of input representations on this tradeoff:part-of-speech tags, the novel use of an alternative dependency representationas input, and distributional representaions of words. We find that directdependency parsing is a more viable solution than it was found to be in thepast. An accompanying software release can be found at:http://www.ark.cs.cmu.edu/TBSD
arxiv-1404-4800 | Automatic Annotation of Axoplasmic Reticula in Pursuit of Connectomes |  http://arxiv.org/abs/1404.4800  | author:Ayushi Sinha, William Gray Roncal, Narayanan Kasthuri, Ming Chuang, Priya Manavalan, Dean M. Kleissas, Joshua T. Vogelstein, R. Jacob Vogelstein, Randal Burns, Jeff W. Lichtman, Michael Kazhdan category:cs.CV published:2014-04-16 summary:In this paper, we present a new pipeline which automatically identifies andannotates axoplasmic reticula, which are small subcellular structures presentonly in axons. We run our algorithm on the Kasthuri11 dataset, which was colorcorrected using gradient-domain techniques to adjust contrast. We use abilateral filter to smooth out the noise in this data while preserving edges,which highlights axoplasmic reticula. These axoplasmic reticula are thenannotated using a morphological region growing algorithm. Additionally, weperform Laplacian sharpening on the bilaterally filtered data to enhance edges,and repeat the morphological region growing algorithm to annotate moreaxoplasmic reticula. We track our annotations through the slices to improveprecision, and to create long objects to aid in segment merging. This methodannotates axoplasmic reticula with high precision. Our algorithm can easily beadapted to annotate axoplasmic reticula in different sets of brain data bychanging a few thresholds. The contribution of this work is the introduction ofa straightforward and robust pipeline which annotates axoplasmic reticula withhigh precision, contributing towards advancements in automatic featureannotations in neural EM data.
arxiv-1404-4316 | Generic Object Detection With Dense Neural Patterns and Regionlets |  http://arxiv.org/abs/1404.4316  | author:Will Y. Zou, Xiaoyu Wang, Miao Sun, Yuanqing Lin category:cs.CV published:2014-04-16 summary:This paper addresses the challenge of establishing a bridge between deepconvolutional neural networks and conventional object detection frameworks foraccurate and efficient generic object detection. We introduce Dense NeuralPatterns, short for DNPs, which are dense local features derived fromdiscriminatively trained deep convolutional neural networks. DNPs can be easilyplugged into conventional detection frameworks in the same way as other denselocal features(like HOG or LBP). The effectiveness of the proposed approach isdemonstrated with the Regionlets object detection framework. It achieved 46.1%mean average precision on the PASCAL VOC 2007 dataset, and 44.1% on the PASCALVOC 2010 dataset, which dramatically improves the original Regionlets approachwithout DNPs.
arxiv-1404-4178 | Speeding Up MCMC by Efficient Data Subsampling |  http://arxiv.org/abs/1404.4178  | author:Matias Quiroz, Mattias Villani, Robert Kohn category:stat.ME stat.CO stat.ML published:2014-04-16 summary:We propose a Markov Chain Monte Carlo (MCMC) framework where the likelihoodfunction for $n$ observations is estimated from a random subset of $m$observations. Inspired by the survey sampling literature, we introduce ageneral and highly efficient log-likelihood estimator. The estimatorincorporates information about each observation's contribution to thelog-likelihood function. The computational complexity of the estimator can bemuch smaller than for the full log-likelihood, and we document substantialspeed-ups in the applications. The likelihood estimate is used within aPseudo-marginal framework to sample from a perturbed posterior which we proveto be within $O(m^{-1/2})$ of the true posterior. Moreover, the approximationerror is demonstrated to be negligible even for a small $m$ in ourapplications. We propose a simple way to adaptively choose the sample size $m$during the MCMC to optimize sampling efficiency for a fixed computationalbudget. We also propose a correlated pseudo marginal approach to subsamplingthat dramatically improves performance. The method is illustrated on threeexamples, each one representing a different data structure. In particular, weshow that our method outperforms other subsampling MCMC methods proposed in theliterature.
arxiv-1404-4088 | Ensemble Classifiers and Their Applications: A Review |  http://arxiv.org/abs/1404.4088  | author:Akhlaqur Rahman, Sumaira Tasnim category:cs.LG published:2014-04-15 summary:Ensemble classifier refers to a group of individual classifiers that arecooperatively trained on data set in a supervised classification problem. Inthis paper we present a review of commonly used ensemble classifiers in theliterature. Some ensemble classifiers are also developed targeting specificapplications. We also present some application driven ensemble classifiers inthis paper.
arxiv-1404-4067 | An effective AHP-based metaheuristic approach to solve supplier selection problem |  http://arxiv.org/abs/1404.4067  | author:Tamal Ghosh, Tanmoy Chakraborty, Pranab K Dan category:cs.NE published:2014-04-15 summary:The supplier selection problem is based on electing the best supplier from agroup of pre-specified candidates, is identified as a Multi Criteria DecisionMaking (MCDM), is proportionately significant in terms of qualitative andquantitative attributes. It is a fundamental issue to achieve a trade-offbetween such quantifiable and unquantifiable attributes with an aim toaccomplish the best solution to the abovementioned problem. This articleportrays a metaheuristic based optimization model to solve this NP-Completeproblem. Initially the Analytic Hierarchy Process (AHP) is implemented togenerate an initial feasible solution of the problem. Thereafter a SimulatedAnnealing (SA) algorithm is exploited to improve the quality of the obtainedsolution. The Taguchi robust design method is exploited to solve the criticalissues on the subject of the parameter selection of the SA technique. In orderto verify the proposed methodology the numerical results are demonstrated basedon tangible industry data.
arxiv-1404-3840 | Surpassing Human-Level Face Verification Performance on LFW with GaussianFace |  http://arxiv.org/abs/1404.3840  | author:Chaochao Lu, Xiaoou Tang category:cs.CV cs.LG stat.ML published:2014-04-15 summary:Face verification remains a challenging problem in very complex conditionswith large variations such as pose, illumination, expression, and occlusions.This problem is exacerbated when we rely unrealistically on a single trainingdata source, which is often insufficient to cover the intrinsically complexface variations. This paper proposes a principled multi-task learning approachbased on Discriminative Gaussian Process Latent Variable Model, namedGaussianFace, to enrich the diversity of training data. In comparison toexisting methods, our model exploits additional data from multiplesource-domains to improve the generalization performance of face verificationin an unknown target-domain. Importantly, our model can adapt automatically tocomplex data distributions, and therefore can well capture complex facevariations inherent in multiple sources. Extensive experiments demonstrate theeffectiveness of the proposed model in learning from diverse data sources andgeneralize to unseen domain. Specifically, the accuracy of our algorithmachieves an impressive accuracy rate of 98.52% on the well-known andchallenging Labeled Faces in the Wild (LFW) benchmark. For the first time, thehuman-level performance in face verification (97.53%) on LFW is surpassed.
arxiv-1404-4105 | Sparse Compositional Metric Learning |  http://arxiv.org/abs/1404.4105  | author:Yuan Shi, Aurélien Bellet, Fei Sha category:cs.LG stat.ML published:2014-04-15 summary:We propose a new approach for metric learning by framing it as learning asparse combination of locally discriminative metrics that are inexpensive togenerate from the training data. This flexible framework allows us to naturallyderive formulations for global, multi-task and local metric learning. Theresulting algorithms have several advantages over existing methods in theliterature: a much smaller number of parameters to be estimated and aprincipled way to generalize learned metrics to new testing data points. Toanalyze the approach theoretically, we derive a generalization bound thatjustifies the sparse combination. Empirically, we evaluate our algorithms onseveral datasets against state-of-the-art metric learning methods. The resultsare consistent with our theoretical findings and demonstrate the superiority ofour approach in terms of classification performance and scalability.
arxiv-1404-4038 | Discovering and Exploiting Entailment Relationships in Multi-Label Learning |  http://arxiv.org/abs/1404.4038  | author:Christina Papagiannopoulou, Grigorios Tsoumakas, Ioannis Tsamardinos category:cs.LG published:2014-04-15 summary:This work presents a sound probabilistic method for enforcing adherence ofthe marginal probabilities of a multi-label model to automatically discovereddeterministic relationships among labels. In particular we focus on discoveringtwo kinds of relationships among the labels. The first one concerns pairwisepositive entailement: pairs of labels, where the presence of one implies thepresence of the other in all instances of a dataset. The second concernsexclusion: sets of labels that do not coexist in the same instances of thedataset. These relationships are represented with a Bayesian network. Marginalprobabilities are entered as soft evidence in the network and adjusted throughprobabilistic inference. Our approach offers robust improvements in meanaverage precision compared to the standard binary relavance approach across all12 datasets involved in our experiments. The discovery process helpsinteresting implicit knowledge to emerge, which could be useful in itself.
arxiv-1404-3989 | Bayesian Neural Networks for Genetic Association Studies of Complex Disease |  http://arxiv.org/abs/1404.3989  | author:Andrew L. Beam, Alison Motsinger-Reif, Jon Doyle category:q-bio.GN stat.AP stat.ML published:2014-04-15 summary:Discovering causal genetic variants from large genetic association studiesposes many difficult challenges. Assessing which genetic markers are involvedin determining trait status is a computationally demanding task, especially inthe presence of gene-gene interactions. A non-parametric Bayesian approach inthe form of a Bayesian neural network is proposed for use in analyzing geneticassociation studies. Demonstrations on synthetic and real data reveal they areable to efficiently and accurately determine which variants are involved indetermining case-control status. Using graphics processing units (GPUs) thetime needed to build these models is decreased by several orders of magnitude.In comparison with commonly used approaches for detecting genetic interactions,Bayesian neural networks perform very well across a broad spectrum of possiblegenetic relationships while having the computational efficiency needed tohandle large datasets.
arxiv-1404-4104 | Sparse Bilinear Logistic Regression |  http://arxiv.org/abs/1404.4104  | author:Jianing V. Shi, Yangyang Xu, Richard G. Baraniuk category:math.OC cs.CV cs.LG published:2014-04-15 summary:In this paper, we introduce the concept of sparse bilinear logisticregression for decision problems involving explanatory variables that aretwo-dimensional matrices. Such problems are common in computer vision,brain-computer interfaces, style/content factorization, and parallel factoranalysis. The underlying optimization problem is bi-convex; we study itssolution and develop an efficient algorithm based on block coordinate descent.We provide a theoretical guarantee for global convergence and estimate theasymptotical convergence rate using the Kurdyka-{\L}ojasiewicz inequality. Arange of experiments with simulated and real data demonstrate that sparsebilinear logistic regression outperforms current techniques in severalimportant applications.
arxiv-1404-3992 | Assessing the Quality of MT Systems for Hindi to English Translation |  http://arxiv.org/abs/1404.3992  | author:Aditi Kalyani, Hemant Kumud, Shashi Pal Singh, Ajai Kumar category:cs.CL published:2014-04-15 summary:Evaluation plays a vital role in checking the quality of MT output. It isdone either manually or automatically. Manual evaluation is very time consumingand subjective, hence use of automatic metrics is done most of the times. Thispaper evaluates the translation quality of different MT Engines forHindi-English (Hindi data is provided as input and English is obtained asoutput) using various automatic metrics like BLEU, METEOR etc. Further thecomparison automatic evaluation results with Human ranking have also beengiven.
arxiv-1404-4032 | Recovery of Coherent Data via Low-Rank Dictionary Pursuit |  http://arxiv.org/abs/1404.4032  | author:Guangcan Liu, Ping Li category:stat.ME cs.IT cs.LG math.IT math.ST stat.TH published:2014-04-15 summary:The recently established RPCA method provides us a convenient way to restorelow-rank matrices from grossly corrupted observations. While elegant in theoryand powerful in reality, RPCA may be not an ultimate solution to the low-rankmatrix recovery problem. Indeed, its performance may not be perfect even whendata are strictly low-rank. This is because conventional RPCA ignores theclustering structures of the data which are ubiquitous in modern applications.As the number of cluster grows, the coherence of data keeps increasing, andaccordingly, the recovery performance of RPCA degrades. We show that thechallenges raised by coherent data (i.e., the data with high coherence) couldbe alleviated by Low-Rank Representation (LRR), provided that the dictionary inLRR is configured appropriately. More precisely, we mathematically prove thatif the dictionary itself is low-rank then LRR is immune to the coherenceparameter which increases with the underlying cluster number. This provides anelementary principle for dealing with coherent data. Subsequently, we devise apractical algorithm to obtain proper dictionaries in unsupervised environments.Our extensive experiments on randomly generated matrices verify our claims.
arxiv-1404-3862 | Optimizing the CVaR via Sampling |  http://arxiv.org/abs/1404.3862  | author:Aviv Tamar, Yonatan Glassner, Shie Mannor category:stat.ML cs.AI cs.LG published:2014-04-15 summary:Conditional Value at Risk (CVaR) is a prominent risk measure that is beingused extensively in various domains. We develop a new formula for the gradientof the CVaR in the form of a conditional expectation. Based on this formula, wepropose a novel sampling-based estimator for the CVaR gradient, in the spiritof the likelihood-ratio method. We analyze the bias of the estimator, and provethe convergence of a corresponding stochastic gradient descent algorithm to alocal CVaR optimum. Our method allows to consider CVaR optimization in newdomains. As an example, we consider a reinforcement learning application, andlearn a risk-sensitive controller for the game of Tetris.
arxiv-1404-4095 | Multi-borders classification |  http://arxiv.org/abs/1404.4095  | author:Peter Mills category:stat.ML cs.LG published:2014-04-15 summary:The number of possible methods of generalizing binary classification tomulti-class classification increases exponentially with the number of classlabels. Often, the best method of doing so will be highly problem dependent.Here we present classification software in which the partitioning ofmulti-class classification problems into binary classification problems isspecified using a recursive control language.
arxiv-1404-3991 | Spiralet Sparse Representation |  http://arxiv.org/abs/1404.3991  | author:Reza Farrahi Moghaddam, Mohamed Cheriet category:cs.CV published:2014-04-15 summary:This is the first report on Working Paper WP-RFM-14-01. The potential andcapability of sparse representations is well-known. However, their(multivariate variable) vectorial form, which is completely fine in many fieldsand disciplines, results in removal and filtering of important "spatial"relations that are implicitly carried by two-dimensional [or multi-dimensional]objects, such as images. In this paper, a new approach, called spiralet sparserepresentation, is proposed in order to develop an augmented representation andtherefore a modified sparse representation and theory, which is capable topreserve the data associated to the spatial relations.
arxiv-1405-1966 | Texture Based Image Segmentation of Chili Pepper X-Ray Images Using Gabor Filter |  http://arxiv.org/abs/1405.1966  | author:M. Rajalakshmi, Dr. P. Subashini category:cs.CV cs.LG published:2014-04-15 summary:Texture segmentation is the process of partitioning an image into regionswith different textures containing a similar group of pixels. Detecting thediscontinuity of the filter's output and their statistical properties help insegmenting and classifying a given image with different texture regions. Inthis proposed paper, chili x-ray image texture segmentation is performed byusing Gabor filter. The texture segmented result obtained from Gabor filter fedinto three texture filters, namely Entropy, Standard Deviation and Rangefilter. After performing texture analysis, features can be extracted by usingStatistical methods. In this paper Gray Level Co-occurrence Matrices and Firstorder statistics are used as feature extraction methods. Features extractedfrom statistical methods are given to Support Vector Machine (SVM) classifier.Using this methodology, it is found that texture segmentation is followed bythe Gray Level Co-occurrence Matrix feature extraction method gives a higheraccuracy rate of 84% when compared with First order feature extraction method. Key Words: Texture segmentation, Texture filter, Gabor filter, Featureextraction methods, SVM classifier.
arxiv-1404-3959 | Is it morally acceptable for a system to lie to persuade me? |  http://arxiv.org/abs/1404.3959  | author:Marco Guerini, Fabio Pianesi, Oliviero Stock category:cs.CY cs.CL published:2014-04-15 summary:Given the fast rise of increasingly autonomous artificial agents and robots,a key acceptability criterion will be the possible moral implications of theiractions. In particular, intelligent persuasive systems (systems designed toinfluence humans via communication) constitute a highly sensitive topic becauseof their intrinsically social nature. Still, ethical studies in this area arerare and tend to focus on the output of the required action. Instead, this workfocuses on the persuasive acts themselves (e.g. "is it morally acceptable thata machine lies or appeals to the emotions of a person to persuade her, even iffor a good end?"). Exploiting a behavioral approach, based on human assessmentof moral dilemmas -- i.e. without any prior assumption of underlying ethicaltheories -- this paper reports on a set of experiments. These experimentsaddress the type of persuader (human or machine), the strategies adopted(purely argumentative, appeal to positive emotions, appeal to negativeemotions, lie) and the circumstances. Findings display no differences due tothe agent, mild acceptability for persuasion and reveal that truth-conditionalreasoning (i.e. argument validity) is a significant dimension affectingsubjects' judgment. Some implications for the design of intelligent persuasivesystems are discussed.
arxiv-1404-3933 | Scalable Matting: A Sub-linear Approach |  http://arxiv.org/abs/1404.3933  | author:Philip G. Lee, Ying Wu category:cs.CV published:2014-04-15 summary:Natural image matting, which separates foreground from background, is a veryimportant intermediate step in recent computer vision algorithms. However, itis severely underconstrained and difficult to solve. State-of-the-artapproaches include matting by graph Laplacian, which significantly improves theunderconstrained nature by reducing the solution space. However, matting bygraph Laplacian is still very difficult to solve and gets much harder as theimage size grows: current iterative methods slow down as $\mathcal{O}\left(n^2\right)$ in the resolution $n$. This creates uncomfortable practical limits onthe resolution of images that we can matte. Current literature mitigates theproblem, but they all remain super-linear in complexity. We expose propertiesof the problem that remain heretofore unexploited, demonstrating that anoptimization technique originally intended to solve PDEs can be adapted to takeadvantage of this knowledge to solve the matting problem, not heuristically,but exactly and with sub-linear complexity. This makes ours the most efficientmatting solver currently known by a very wide margin and allows matting finallyto be practical and scalable in the future as consumer photos exceed manydozens of megapixels, and also relieves matting from being a bottleneck forvision algorithms that depend on it.
arxiv-1404-3759 | Meta-evaluation of comparability metrics using parallel corpora |  http://arxiv.org/abs/1404.3759  | author:Bogdan Babych, Anthony Hartley category:cs.CL published:2014-04-14 summary:Metrics for measuring the comparability of corpora or texts need to bedeveloped and evaluated systematically. Applications based on a corpus, such astraining Statistical MT systems in specialised narrow domains, require findinga reasonable balance between the size of the corpus and its consistency, withcontrolled and benchmarked levels of comparability for any newly addedsections. In this article we propose a method that can meta-evaluatecomparability metrics by calculating monolingual comparability scoresseparately on the 'source' and 'target' sides of parallel corpora. The range ofscores on the source side is then correlated (using Pearson's r coefficient)with the range of 'target' scores; the higher the correlation - the morereliable is the metric. The intuition is that a good metric should yield thesame distance between different domains in different languages. Our methodgives consistent results for the same metrics on different data sets, whichindicates that it is reliable and can be used for metric comparison or foroptimising settings of parametrised metrics.
arxiv-1404-3606 | PCANet: A Simple Deep Learning Baseline for Image Classification? |  http://arxiv.org/abs/1404.3606  | author:Tsung-Han Chan, Kui Jia, Shenghua Gao, Jiwen Lu, Zinan Zeng, Yi Ma category:cs.CV cs.LG cs.NE published:2014-04-14 summary:In this work, we propose a very simple deep learning network for imageclassification which comprises only the very basic data processing components:cascaded principal component analysis (PCA), binary hashing, and block-wisehistograms. In the proposed architecture, PCA is employed to learn multistagefilter banks. It is followed by simple binary hashing and block histograms forindexing and pooling. This architecture is thus named as a PCA network (PCANet)and can be designed and learned extremely easily and efficiently. Forcomparison and better understanding, we also introduce and study two simplevariations to the PCANet, namely the RandNet and LDANet. They share the sametopology of PCANet but their cascaded filters are either selected randomly orlearned from LDA. We have tested these basic networks extensively on manybenchmark visual datasets for different tasks, such as LFW for faceverification, MultiPIE, Extended Yale B, AR, FERET datasets for facerecognition, as well as MNIST for hand-written digits recognition.Surprisingly, for all tasks, such a seemingly naive PCANet model is on par withthe state of the art features, either prefixed, highly hand-crafted orcarefully learned (by DNNs). Even more surprisingly, it sets new records formany classification tasks in Extended Yale B, AR, FERET datasets, and MNISTvariations. Additional experiments on other public datasets also demonstratethe potential of the PCANet serving as a simple but highly competitive baselinefor texture classification and object recognition.
arxiv-1404-3656 | Methods for Ordinal Peer Grading |  http://arxiv.org/abs/1404.3656  | author:Karthik Raman, Thorsten Joachims category:cs.LG cs.IR H.4 published:2014-04-14 summary:MOOCs have the potential to revolutionize higher education with their wideoutreach and accessibility, but they require instructors to come up withscalable alternates to traditional student evaluation. Peer grading -- havingstudents assess each other -- is a promising approach to tackling the problemof evaluation at scale, since the number of "graders" naturally scales with thenumber of students. However, students are not trained in grading, which meansthat one cannot expect the same level of grading skills as in traditionalsettings. Drawing on broad evidence that ordinal feedback is easier to provideand more reliable than cardinal feedback, it is therefore desirable to allowpeer graders to make ordinal statements (e.g. "project X is better than projectY") and not require them to make cardinal statements (e.g. "project X is aB-"). Thus, in this paper we study the problem of automatically inferringstudent grades from ordinal peer feedback, as opposed to existing methods thatrequire cardinal peer feedback. We formulate the ordinal peer grading problemas a type of rank aggregation problem, and explore several probabilistic modelsunder which to estimate student grades and grader reliability. We study theapplicability of these methods using peer grading data collected from a realclass -- with instructor and TA grades as a baseline -- and demonstrate theefficacy of ordinal feedback techniques in comparison to existing cardinal peergrading methods. Finally, we compare these peer-grading techniques totraditional evaluation techniques.
arxiv-1404-3520 | A Theoretical Assessment of Solution Quality in Evolutionary Algorithms for the Knapsack Problem |  http://arxiv.org/abs/1404.3520  | author:Jun He, Boris Mitavskiy, Yuren Zhou category:cs.NE published:2014-04-14 summary:Evolutionary algorithms are well suited for solving the knapsack problem.Some empirical studies claim that evolutionary algorithms can produce goodsolutions to the 0-1 knapsack problem. Nonetheless, few rigorous investigationsaddress the quality of solutions that evolutionary algorithms may produce forthe knapsack problem. The current paper focuses on a theoretical investigationof three types of (N+1) evolutionary algorithms that exploit bitwise mutation,truncation selection, plus different repair methods for the 0-1 knapsackproblem. It assesses the solution quality in terms of the approximation ratio.Our work indicates that the solution produced by pure strategy and mixedstrategy evolutionary algorithms is arbitrarily bad. Nevertheless, theevolutionary algorithm using helper objectives may produce 1/2-approximationsolutions to the 0-1 knapsack problem.
arxiv-1404-3591 | Hybrid Conditional Gradient - Smoothing Algorithms with Applications to Sparse and Low Rank Regularization |  http://arxiv.org/abs/1404.3591  | author:Andreas Argyriou, Marco Signoretto, Johan Suykens category:math.OC cs.LG stat.ML published:2014-04-14 summary:We study a hybrid conditional gradient - smoothing algorithm (HCGS) forsolving composite convex optimization problems which contain several terms overa bounded set. Examples of these include regularization problems with severalnorms as penalties and a norm constraint. HCGS extends conditional gradientmethods to cases with multiple nonsmooth terms, in which standard conditionalgradient methods may be difficult to apply. The HCGS algorithm borrowstechniques from smoothing proximal methods and requires first-ordercomputations (subgradients and proximity operations). Unlike proximal methods,HCGS benefits from the advantages of conditional gradient methods, which renderit more efficient on certain large scale optimization problems. We demonstratethese advantages with simulations on two matrix optimization problems:regularization of matrices with combined $\ell_1$ and trace norm penalties; anda convex relaxation of sparse PCA.
arxiv-1404-3581 | Random forests with random projections of the output space for high dimensional multi-label classification |  http://arxiv.org/abs/1404.3581  | author:Arnaud Joly, Pierre Geurts, Louis Wehenkel category:stat.ML cs.LG published:2014-04-14 summary:We adapt the idea of random projections applied to the output space, so as toenhance tree-based ensemble methods in the context of multi-labelclassification. We show how learning time complexity can be reduced withoutaffecting computational complexity and accuracy of predictions. We also showthat random output space projections may be used in order to reach differentbias-variance tradeoffs, over a broad panel of benchmark problems, and thatthis may lead to improved accuracy while reducing significantly thecomputational burden of the learning stage.
arxiv-1404-3543 | Recover Canonical-View Faces in the Wild with Deep Neural Networks |  http://arxiv.org/abs/1404.3543  | author:Zhenyao Zhu, Ping Luo, Xiaogang Wang, Xiaoou Tang category:cs.CV published:2014-04-14 summary:Face images in the wild undergo large intra-personal variations, such asposes, illuminations, occlusions, and low resolutions, which cause greatchallenges to face-related applications. This paper addresses this challenge byproposing a new deep learning framework that can recover the canonical view offace images. It dramatically reduces the intra-person variances, whilemaintaining the inter-person discriminativeness. Unlike the existing facereconstruction methods that were either evaluated in controlled 2D environmentor employed 3D information, our approach directly learns the transformationfrom the face images with a complex set of variations to their canonical views.At the training stage, to avoid the costly process of labeling canonical-viewimages from the training set by hand, we have devised a new measurement toautomatically select or synthesize a canonical-view image for each identity. Asan application, this face recovery approach is used for face verification.Facial features are learned from the recovered canonical-view face images byusing a facial component-based convolutional neural network. Our approachachieves the state-of-the-art performance on the LFW dataset.
arxiv-1404-3538 | Proceedings of The 38th Annual Workshop of the Austrian Association for Pattern Recognition (ÖAGM), 2014 |  http://arxiv.org/abs/1404.3538  | author:Vladimir Kolmogorov, Christoph Lampert, Emilie Morvant, Rustem Takhanov category:cs.CV published:2014-04-14 summary:The 38th Annual Workshop of the Austrian Association for Pattern Recognition(\"OAGM) will be held at IST Austria, on May 22-23, 2014. The workshop providesa platform for researchers and industry to discuss traditional and new areas ofcomputer vision. This year the main topic is: Pattern Recognition:interdisciplinary challenges and opportunities.
arxiv-1404-3596 | Face Detection with a 3D Model |  http://arxiv.org/abs/1404.3596  | author:Adrian Barbu, Nathan Lay, Gary Gramajo category:cs.CV published:2014-04-14 summary:This paper presents a part-based face detection approach where the spatialrelationship between the face parts is represented by a hidden 3D model withsix parameters. The computational complexity of the search in the sixdimensional pose space is addressed by proposing meaningful 3D pose candidatesby image-based regression from detected face keypoint locations. The 3D posecandidates are evaluated using a parameter sensitive classifier based ondifference features relative to the 3D pose. A compatible subset of candidatesis then obtained by non-maximal suppression. Experiments on two standard facedetection datasets show that the proposed 3D model based approach obtainsresults comparable to or better than state of the art.
arxiv-1404-3439 | Anytime Hierarchical Clustering |  http://arxiv.org/abs/1404.3439  | author:Omur Arslan, Daniel E. Koditschek category:stat.ML cs.IR cs.LG H.3.3; I.5.3 published:2014-04-13 summary:We propose a new anytime hierarchical clustering method that iterativelytransforms an arbitrary initial hierarchy on the configuration of measurementsalong a sequence of trees we prove for a fixed data set must terminate in achain of nested partitions that satisfies a natural homogeneity requirement.Each recursive step re-edits the tree so as to improve a local measure ofcluster homogeneity that is compatible with a number of commonly used (e.g.,single, average, complete) linkage functions. As an alternative to the standardbatch algorithms, we present numerical evidence to suggest that appropriateadaptations of this method can yield decentralized, scalable algorithmssuitable for distributed/parallel computation of clustering hierarchies andonline tracking of clustering trees applicable to large, dynamically changingdatabases and anomaly detection.
arxiv-1404-3925 | Complexity of Grammar Induction for Quantum Types |  http://arxiv.org/abs/1404.3925  | author:Antonin Delpeuch category:cs.CL math.CT published:2014-04-13 summary:Most categorical models of meaning use a functor from the syntactic categoryto the semantic category. When semantic information is available, the problemof grammar induction can therefore be defined as finding preimages of thesemantic types under this forgetful functor, lifting the information flow fromthe semantic level to a valid reduction at the syntactic level. We study thecomplexity of grammar induction, and show that for a variety of type systems,including pivotal and compact closed categories, the grammar induction problemis NP-complete. Our approach could be extended to linguistic type systems suchas autonomous or bi-closed categories.
arxiv-1404-3366 | Learning Deep Convolutional Features for MRI Based Alzheimer's Disease Classification |  http://arxiv.org/abs/1404.3366  | author:Fayao Liu, Chunhua Shen category:cs.CV published:2014-04-13 summary:Effective and accurate diagnosis of Alzheimer's disease (AD) or mildcognitive impairment (MCI) can be critical for early treatment and thus hasattracted more and more attention nowadays. Since first introduced, machinelearning methods have been gaining increasing popularity for AD relatedresearch. Among the various identified biomarkers, magnetic resonance imaging(MRI) are widely used for the prediction of AD or MCI. However, before amachine learning algorithm can be applied, image features need to be extractedto represent the MRI images. While good representations can be pivotal to theclassification performance, almost all the previous studies typically rely onhuman labelling to find the regions of interest (ROI) which may be correlatedto AD, such as hippocampus, amygdala, precuneus, etc. This procedure requiresdomain knowledge and is costly and tedious. Instead of relying on extraction of ROI features, it is more promising toremove manual ROI labelling from the pipeline and directly work on the raw MRIimages. In other words, we can let the machine learning methods to figure outthese informative and discriminative image structures for AD classification. Inthis work, we propose to learn deep convolutional image features usingunsupervised and supervised learning. Deep learning has emerged as a powerfultool in the machine learning community and has been successfully applied tovarious tasks. We thus propose to exploit deep features of MRI images based ona pre-trained large convolutional neural network (CNN) for AD and MCIclassification, which spares the effort of manual ROI annotation process.
arxiv-1404-3377 | A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing |  http://arxiv.org/abs/1404.3377  | author:Rene Pickhardt, Thomas Gottron, Martin Körner, Paul Georg Wagner, Till Speicher, Steffen Staab category:cs.CL published:2014-04-13 summary:We introduce a novel approach for building language models based on asystematic, recursive exploration of skip n-gram models which are interpolatedusing modified Kneser-Ney smoothing. Our approach generalizes language modelsas it contains the classical interpolation with lower order models as a specialcase. In this paper we motivate, formalize and present our approach. In anextensive empirical experiment over English text corpora we demonstrate thatour generalized language models lead to a substantial reduction of perplexitybetween 3.1% and 12.7% in comparison to traditional language models usingmodified Kneser-Ney smoothing. Furthermore, we investigate the behaviour overthree other languages and a domain specific corpus where we observed consistentimprovements. Finally, we also show that the strength of our approach lies inits ability to cope in particular with sparse training data. Using a very smalltraining data set of only 736 KB text we yield improvements of even 25.7%reduction of perplexity.
arxiv-1404-3415 | Generalized version of the support vector machine for binary classification problems: supporting hyperplane machine |  http://arxiv.org/abs/1404.3415  | author:E. G. Abramov, A. B. Komissarov, D. A. Kornyakov category:cs.LG stat.ML F.1.1; I.5.1 published:2014-04-13 summary:In this paper there is proposed a generalized version of the SVM for binaryclassification problems in the case of using an arbitrary transformation x ->y. An approach similar to the classic SVM method is used. The problem is widelyexplained. Various formulations of primal and dual problems are proposed. Forone of the most important cases the formulae are derived in detail. A simplecomputational example is demonstrated. The algorithm and its implementation ispresented in Octave language.
arxiv-1404-3418 | Active Learning for Undirected Graphical Model Selection |  http://arxiv.org/abs/1404.3418  | author:Divyanshu Vats, Robert D. Nowak, Richard G. Baraniuk category:stat.ML cs.IT math.IT math.ST stat.TH published:2014-04-13 summary:This paper studies graphical model selection, i.e., the problem of estimatinga graph of statistical relationships among a collection of random variables.Conventional graphical model selection algorithms are passive, i.e., theyrequire all the measurements to have been collected before processing begins.We propose an active learning algorithm that uses junction tree representationsto adapt future measurements based on the information gathered from priormeasurements. We prove that, under certain conditions, our active learningalgorithm requires fewer scalar measurements than any passive algorithm toreliably estimate a graph. A range of numerical results validate our theory anddemonstrates the benefits of active learning.
arxiv-1404-3368 | Near-optimal sample compression for nearest neighbors |  http://arxiv.org/abs/1404.3368  | author:Lee-Ad Gottlieb, Aryeh Kontorovich, Pinhas Nisnevitch category:cs.LG cs.CC published:2014-04-13 summary:We present the first sample compression algorithm for nearest neighbors withnon-trivial performance guarantees. We complement these guarantees bydemonstrating almost matching hardness lower bounds, which show that our boundis nearly optimal. Our result yields new insight into margin-based nearestneighbor classification in metric spaces and allows us to significantly sharpenand simplify existing bounds. Some encouraging empirical results are alsopresented.
arxiv-1404-3378 | Complexity theoretic limitations on learning DNF's |  http://arxiv.org/abs/1404.3378  | author:Amit Daniely, Shai Shalev-Shwatz category:cs.LG cs.CC published:2014-04-13 summary:Using the recently developed framework of [Daniely et al, 2014], we show thatunder a natural assumption on the complexity of refuting random K-SAT formulas,learning DNF formulas is hard. Furthermore, the same assumption implies thehardness of learning intersections of $\omega(\log(n))$ halfspaces,agnostically learning conjunctions, as well as virtually all (distributionfree) learning problems that were previously shown hard (under complexityassumptions).
arxiv-1404-3291 | Cost-Effective HITs for Relative Similarity Comparisons |  http://arxiv.org/abs/1404.3291  | author:Michael J. Wilber, Iljung S. Kwak, Serge J. Belongie category:cs.CV cs.LG published:2014-04-12 summary:Similarity comparisons of the form "Is object a more similar to b than to c?"are useful for computer vision and machine learning applications.Unfortunately, an embedding of $n$ points is specified by $n^3$ triplets,making collecting every triplet an expensive task. In noticing this difficulty,other researchers have investigated more intelligent triplet samplingtechniques, but they do not study their effectiveness or their potentialdrawbacks. Although it is important to reduce the number of collected triplets,it is also important to understand how best to display a triplet collectiontask to a user. In this work we explore an alternative display for collectingtriplets and analyze the monetary cost and speed of the display. We proposebest practices for creating cost effective human intelligence tasks forcollecting triplets. We show that rather than changing the sampling algorithm,simple changes to the crowdsourcing UI can lead to much higher qualityembeddings. We also provide a dataset as well as the labels collected fromcrowd workers.
arxiv-1404-3312 | Shrinkage Optimized Directed Information using Pictorial Structures for Action Recognition |  http://arxiv.org/abs/1404.3312  | author:Xu Chen, Alfred Hero, Silvio Savarese category:cs.CV published:2014-04-12 summary:In this paper, we propose a novel action recognition framework. The methoduses pictorial structures and shrinkage optimized directed informationassessment (SODA) coupled with Markov Random Fields called SODA+MRF to modelthe directional temporal dependency and bidirectional spatial dependency. As avariant of mutual information, directional information captures the directionalinformation flow and temporal structure of video sequences across frames.Meanwhile, within each frame, Markov random fields are utilized to model thespatial relations among different parts of a human body and the body parts ofdifferent people. The proposed SODA+MRF model is robust to view pointtransformations and detect complex interactions accurately. We compare theproposed method against several baseline methods to highlight the effectivenessof the SODA+MRF model. We demonstrate that our algorithm has superior actionrecognition performance on the UCF action recognition dataset, the Olympicsports dataset and the collective activity dataset over severalstate-of-the-art methods.
arxiv-1404-3290 | Motion-Compensated Coding and Frame-Rate Up-Conversion: Models and Analysis |  http://arxiv.org/abs/1404.3290  | author:Yehuda Dar, Alfred M. Bruckstein category:cs.MM cs.CV published:2014-04-12 summary:Block-based motion estimation (ME) and compensation (MC) techniques arewidely used in modern video processing algorithms and compression systems. Thegreat variety of video applications and devices results in numerous compressionspecifications. Specifically, there is a diversity of frame-rates andbit-rates. In this paper, we study the effect of frame-rate and compressionbit-rate on block-based ME and MC as commonly utilized in inter-frame codingand frame-rate up conversion (FRUC). This joint examination yields acomprehensive foundation for comparing MC procedures in coding and FRUC. First,the video signal is modeled as a noisy translational motion of an image. Then,we theoretically model the motion-compensated prediction of an available andabsent frames as in coding and FRUC applications, respectively. The theoreticMC-prediction error is further analyzed and its autocorrelation function iscalculated for coding and FRUC applications. We show a linear relation betweenthe variance of the MC-prediction error and temporal-distance. While theaffecting distance in MC-coding is between the predicted and reference frames,MC-FRUC is affected by the distance between the available frames used for theinterpolation. Moreover, the dependency in temporal-distance implies an inverseeffect of the frame-rate. FRUC performance analysis considers the predictionerror variance, since it equals to the mean-squared-error of the interpolation.However, MC-coding analysis requires the entire autocorrelation function of theerror; hence, analytic simplicity is beneficial. Therefore, we propose twoconstructions of a separable autocorrelation function for prediction error inMC-coding. We conclude by comparing our estimations with experimental results.
arxiv-1404-3331 | Priors for Random Count Matrices Derived from a Family of Negative Binomial Processes |  http://arxiv.org/abs/1404.3331  | author:Mingyuan Zhou, Oscar Hernan Madrid Padilla, James G. Scott category:stat.ME stat.ML published:2014-04-12 summary:We define a family of probability distributions for random count matriceswith a potentially unbounded number of rows and columns. The threedistributions we consider are derived from the gamma-Poisson, gamma-negativebinomial, and beta-negative binomial processes. Because the models lead toclosed-form Gibbs sampling update equations, they are natural candidates fornonparametric Bayesian priors over count matrices. A key aspect of our analysisis the recognition that, although the random count matrices within the familyare defined by a row-wise construction, their columns can be shown to be i.i.d.This fact is used to derive explicit formulas for drawing all the columns atonce. Moreover, by analyzing these matrices' combinatorial structure, wedescribe how to sequentially construct a column-i.i.d. random count matrix onerow at a time, and derive the predictive distribution of a new row count vectorwith previously unseen features. We describe the similarities and differencesbetween the three priors, and argue that the greater flexibility of the gamma-and beta- negative binomial processes, especially their ability to modelover-dispersed, heavy-tailed count data, makes these well suited to a widevariety of real-world applications. As an example of our framework, weconstruct a naive-Bayes text classifier to categorize a count vector to one ofseveral existing random count matrices of different categories. The classifiersupports an unbounded number of features, and unlike most existing methods, itdoes not require a predefined finite vocabulary to be shared by all thecategories, and needs neither feature selection nor parameter tuning. Both thegamma- and beta- negative binomial processes are shown to significantlyoutperform the gamma-Poisson process for document categorization, withcomparable performance to other state-of-the-art supervised text classificationalgorithms.
arxiv-1404-3174 | Model Based Clustering of High-Dimensional Binary Data |  http://arxiv.org/abs/1404.3174  | author:Yang Tang, Ryan P. Browne, Paul D. McNicholas category:stat.ME stat.CO stat.ML published:2014-04-11 summary:We propose a mixture of latent trait models with common slope parameters(MCLT) for model-based clustering of high-dimensional binary data, a data typefor which few established methods exist. Recent work on clustering of binarydata, based on a $d$-dimensional Gaussian latent variable, is extended byincorporating common factor analyzers. Accordingly, our approach facilitates alow-dimensional visual representation of the clusters. We extend the modelfurther by the incorporation of random block effects. The dependencies in eachblock are taken into account through block-specific parameters that areconsidered to be random variables. A variational approximation to thelikelihood is exploited to derive a fast algorithm for determining the modelparameters. Our approach is demonstrated on real and simulated data.
arxiv-1404-3012 | Bayesian image segmentations by Potts prior and loopy belief propagation |  http://arxiv.org/abs/1404.3012  | author:Kazuyuki Tanaka, Shun Kataoka, Muneki Yasuda, Yuji Waizumi, Chiou-Ting Hsu category:cs.CV cs.LG stat.ML published:2014-04-11 summary:This paper presents a Bayesian image segmentation model based on Potts priorand loopy belief propagation. The proposed Bayesian model involves severalterms, including the pairwise interactions of Potts models, and the averagevectors and covariant matrices of Gauss distributions in color image modeling.These terms are often referred to as hyperparameters in statistical machinelearning theory. In order to determine these hyperparameters, we propose a newscheme for hyperparameter estimation based on conditional maximization ofentropy in the Potts prior. The algorithm is given based on loopy beliefpropagation. In addition, we compare our conditional maximum entropy frameworkwith the conventional maximum likelihood framework, and also clarify how thefirst order phase transitions in LBP's for Potts models influence ourhyperparameter estimation procedures.
arxiv-1404-3190 | Pareto-Path Multi-Task Multiple Kernel Learning |  http://arxiv.org/abs/1404.3190  | author:Cong Li, Michael Georgiopoulos, Georgios C. Anagnostopoulos category:cs.LG published:2014-04-11 summary:A traditional and intuitively appealing Multi-Task Multiple Kernel Learning(MT-MKL) method is to optimize the sum (thus, the average) of objectivefunctions with (partially) shared kernel function, which allows informationsharing amongst tasks. We point out that the obtained solution corresponds to asingle point on the Pareto Front (PF) of a Multi-Objective Optimization (MOO)problem, which considers the concurrent optimization of all task objectivesinvolved in the Multi-Task Learning (MTL) problem. Motivated by this lastobservation and arguing that the former approach is heuristic, we propose anovel Support Vector Machine (SVM) MT-MKL framework, that considers animplicitly-defined set of conic combinations of task objectives. We show thatsolving our framework produces solutions along a path on the aforementioned PFand that it subsumes the optimization of the average of objective functions asa special case. Using algorithms we derived, we demonstrate through a series ofexperimental results that the framework is capable of achieving betterclassification performance, when compared to other similar MTL approaches.
arxiv-1404-3184 | Decreasing Weighted Sorted $\ell_1$ Regularization |  http://arxiv.org/abs/1404.3184  | author:Xiangrong Zeng, Mário A. T. Figueiredo category:cs.CV cs.IT cs.LG math.IT published:2014-04-11 summary:We consider a new family of regularizers, termed {\it weighted sorted$\ell_1$ norms} (WSL1), which generalizes the recently introduced {\itoctagonal shrinkage and clustering algorithm for regression} (OSCAR) and alsocontains the $\ell_1$ and $\ell_{\infty}$ norms as particular instances. Wefocus on a special case of the WSL1, the {\sl decreasing WSL1} (DWSL1), wherethe elements of the argument vector are sorted in non-increasing order and theweights are also non-increasing. In this paper, after showing that the DWSL1 isindeed a norm, we derive two key tools for its use as a regularizer: the dualnorm and the Moreau proximity operator.
arxiv-1404-3026 | On the Ground Validation of Online Diagnosis with Twitter and Medical Records |  http://arxiv.org/abs/1404.3026  | author:Todd Bodnar, Victoria C Barclay, Nilam Ram, Conrad S Tucker, Marcel Salathé category:cs.SI cs.CL cs.LG I.2.1 published:2014-04-11 summary:Social media has been considered as a data source for tracking disease.However, most analyses are based on models that prioritize strong correlationwith population-level disease rates over determining whether or not specificindividual users are actually sick. Taking a different approach, we develop anovel system for social-media based disease detection at the individual levelusing a sample of professionally diagnosed individuals. Specifically, wedevelop a system for making an accurate influenza diagnosis based on anindividual's publicly available Twitter data. We find that about half (17/35 =48.57%) of the users in our sample that were sick explicitly discuss theirdisease on Twitter. By developing a meta classifier that combines textanalysis, anomaly detection, and social network analysis, we are able todiagnose an individual with greater than 99% accuracy even if she does notdiscuss her health.
arxiv-1404-3233 | Pagination: It's what you say, not how long it takes to say it |  http://arxiv.org/abs/1404.3233  | author:Joshua Hailpern, Niranjan Damera Venkata, Marina Danilevsky category:cs.CL cs.IR I.7.2; I.7.4 published:2014-04-11 summary:Pagination - the process of determining where to break an article acrosspages in a multi-article layout is a common layout challenge for mostcommercially printed newspapers and magazines. To date, no one has created analgorithm that determines a minimal pagination break point based on the contentof the article. Existing approaches for automatic multi-article layout focusexclusively on maximizing content (number of articles) and optimizing aestheticpresentation (e.g., spacing between articles). However, disregarding thesemantic information within the article can lead to overly aggressive cutting,thereby eliminating key content and potentially confusing the reader, orsetting too generous of a break point, thereby leaving in superfluous contentand making automatic layout more difficult. This is one of the remainingchallenges on the path from manual layouts to fully automated processes thatstill ensure article content quality. In this work, we present a new approachto calculating a document minimal break point for the task of pagination. Ourapproach uses a statistical language model to predict minimal break pointsbased on the semantic content of an article. We then compare 4 novel candidateapproaches, and 4 baselines (currently in use by layout algorithms). Resultsfrom this experiment show that one of our approaches strongly outperforms thebaselines and alternatives. Results from a second study suggest that humans arenot able to agree on a single "best" break point. Therefore, this work showsthat a semantic-based lower bound break point prediction is necessary for idealautomated document synthesis within a real-world context.
arxiv-1404-3023 | Markov Chain Analysis of Evolution Strategies on a Linear Constraint Optimization Problem |  http://arxiv.org/abs/1404.3023  | author:Alexandre Chotard, Anne Auger, Nikolaus Hansen category:cs.NE math.OC published:2014-04-11 summary:This paper analyses a $(1,\lambda)$-Evolution Strategy, a randomisedcomparison-based adaptive search algorithm, on a simple constraint optimisationproblem. The algorithm uses resampling to handle the constraint and optimizes alinear function with a linear constraint. Two cases are investigated: first thecase where the step-size is constant, and second the case where the step-sizeis adapted using path length control. We exhibit for each case a Markov chainwhose stability analysis would allow us to deduce the divergence of thealgorithm depending on its internal parameters. We show divergence at aconstant rate when the step-size is constant. We sketch that with step-sizeadaptation geometric divergence takes place. Our results complement previousstudies where stability was assumed.
arxiv-1404-2999 | A Reverse Hierarchy Model for Predicting Eye Fixations |  http://arxiv.org/abs/1404.2999  | author:Tianlin Shi, Liang Ming, Xiaolin Hu category:cs.CV published:2014-04-11 summary:A number of psychological and physiological evidences suggest that earlyvisual attention works in a coarse-to-fine way, which lays a basis for thereverse hierarchy theory (RHT). This theory states that attention propagatesfrom the top level of the visual hierarchy that processes gist and abstractinformation of input, to the bottom level that processes local details.Inspired by the theory, we develop a computational model for saliency detectionin images. First, the original image is downsampled to different scales toconstitute a pyramid. Then, saliency on each layer is obtained by imagesuper-resolution reconstruction from the layer above, which is defined asunpredictability from this coarse-to-fine reconstruction. Finally, saliency oneach layer of the pyramid is fused into stochastic fixations through aprobabilistic model, where attention initiates from the top layer andpropagates downward through the pyramid. Extensive experiments on two standardeye-tracking datasets show that the proposed method can achieve competitiveresults with state-of-the-art models.
arxiv-1404-2997 | Automatic Detection of Reuses and Citations in Literary Texts |  http://arxiv.org/abs/1404.2997  | author:Jean-Gabriel Ganascia, Pierre Glaudes, Andrea Del Lungo category:cs.CL cs.DL published:2014-04-11 summary:For more than forty years now, modern theories of literature (Compagnon,1979) insist on the role of paraphrases, rewritings, citations, reciprocalborrowings and mutual contributions of any kinds. The notions ofintertextuality, transtextuality, hypertextuality/hypotextuality, wereintroduced in the seventies and eighties to approach these phenomena. Thecareful analysis of these references is of particular interest in evaluatingthe distance that the creator voluntarily introduces with his/her masters.Phoebus is collaborative project that makes computer scientists from theUniversity Pierre and Marie Curie (LIP6-UPMC) collaborate with the literaryteams of Paris-Sorbonne University with the aim to develop efficient tools forliterary studies that take advantage of modern computer science techniques. Inthis context, we have developed a piece of software that automatically detectsand explores networks of textual reuses in classical literature. This paperdescribes the principles on which is based this program, the significantresults that have already been obtained and the perspectives for the nearfuture.
arxiv-1404-2986 | A Tutorial on Independent Component Analysis |  http://arxiv.org/abs/1404.2986  | author:Jonathon Shlens category:cs.LG stat.ML published:2014-04-11 summary:Independent component analysis (ICA) has become a standard data analysistechnique applied to an array of problems in signal processing and machinelearning. This tutorial provides an introduction to ICA based on linear algebraformulating an intuition for ICA from first principles. The goal of thistutorial is to provide a solid foundation on this advanced topic so that onemight learn the motivation behind ICA, learn why and when to apply thistechnique and in the process gain an introduction to this exciting field ofactive research.
arxiv-1404-3219 | Estimating nonlinear regression errors without doing regression |  http://arxiv.org/abs/1404.3219  | author:Hong Pi, Carsten Peterson category:stat.ML nlin.CD q-fin.ST published:2014-04-11 summary:A method for estimating nonlinear regression errors and their distributionswithout performing regression is presented. Assuming continuity of the modelingfunction the variance is given in terms of conditional probabilities extractedfrom the data. For N data points the computational demand is N2. Comparing thepredicted residual errors with those derived from a linear model assumptionprovides a signal for nonlinearity. The method is successfully illustrated withdata generated by the Ikeda and Lorenz maps augmented with noise. As aby-product the embedding dimensions of these maps are also extracted.
arxiv-1404-3203 | Compressive classification and the rare eclipse problem |  http://arxiv.org/abs/1404.3203  | author:Afonso S. Bandeira, Dustin G. Mixon, Benjamin Recht category:cs.LG cs.IT math.IT math.ST stat.TH published:2014-04-11 summary:This paper addresses the fundamental question of when convex sets remaindisjoint after random projection. We provide an analysis using ideas fromhigh-dimensional convex geometry. For ellipsoids, we provide a bound in termsof the distance between these ellipsoids and simple functions of theirpolynomial coefficients. As an application, this theorem provides bounds forcompressive classification of convex sets. Rather than assuming that the datato be classified is sparse, our results show that the data can be acquired viavery few measurements yet will remain linearly separable. We demonstrate thefeasibility of this approach in the context of hyperspectral imaging.
arxiv-1404-3610 | Targeting HIV-related Medication Side Effects and Sentiment Using Twitter Data |  http://arxiv.org/abs/1404.3610  | author:Cosme Adrover, Todd Bodnar, Marcel Salathe category:cs.SI cs.CL cs.IR published:2014-04-11 summary:We present a descriptive analysis of Twitter data. Our study focuses onextracting the main side effects associated with HIV treatments. The crux ofour work was the identification of personal tweets referring to HIV. Wesummarize our results in an infographic aimed at the general public. Inaddition, we present a measure of user sentiment based on hand-rated tweets.
arxiv-1404-2728 | Real-time Decolorization using Dominant Colors |  http://arxiv.org/abs/1404.2728  | author:Wei Hu, Wei Li, Fan Zhang, Qian Du category:cs.GR cs.CV published:2014-04-10 summary:Decolorization is the process to convert a color image or video to itsgrayscale version, and it has received great attention in recent years. Anideal decolorization algorithm should preserve the original color contrast asmuch as possible. Meanwhile, it should provide the final decolorized result asfast as possible. However, most of the current methods are suffering fromeither unsatisfied color information preservation or high computational cost,limiting their application value. In this paper, a simple but effectivetechnique is proposed for real-time decolorization. Based on the typicalrgb2gray() color conversion model, which produces a grayscale image by linearlycombining R, G, and B channels, we propose a dominant color hypothesis and acorresponding distance measurement metric to evaluate the quality of grayscaleconversion. The local optimum scheme provides several "good" candidates in aconfidence interval, from which the "best" result can be extracted.Experimental results demonstrate that remarkable simplicity of the proposedmethod facilitates the process of high resolution images and videos inreal-time using a common CPU.
arxiv-1404-2655 | Open problem: Tightness of maximum likelihood semidefinite relaxations |  http://arxiv.org/abs/1404.2655  | author:Afonso S. Bandeira, Yuehaw Khoo, Amit Singer category:math.OC cs.LG stat.ML published:2014-04-10 summary:We have observed an interesting, yet unexplained, phenomenon: Semidefiniteprogramming (SDP) based relaxations of maximum likelihood estimators (MLE) tendto be tight in recovery problems with noisy data, even when MLE cannot exactlyrecover the ground truth. Several results establish tightness of SDP basedrelaxations in the regime where exact recovery from MLE is possible. However,to the best of our knowledge, their tightness is not understood beyond thisregime. As an illustrative example, we focus on the generalized Procrustesproblem.
arxiv-1404-2948 | Gradient-based Laplacian Feature Selection |  http://arxiv.org/abs/1404.2948  | author:Bo Wang, Anna Goldenberg category:cs.LG published:2014-04-10 summary:Analysis of high dimensional noisy data is of essence across a variety ofresearch fields. Feature selection techniques are designed to find the relevantfeature subset that can facilitate classification or pattern detection.Traditional (supervised) feature selection methods utilize label information toguide the identification of relevant feature subsets. In this paper, however,we consider the unsupervised feature selection problem. Without the labelinformation, it is particularly difficult to identify a small set of relevantfeatures due to the noisy nature of real-world data which corrupts theintrinsic structure of the data. Our Gradient-based Laplacian Feature Selection(GLFS) selects important features by minimizing the variance of the Laplacianregularized least squares regression model. With $\ell_1$ relaxation, GLFS canfind a sparse subset of features that is relevant to the Laplacian manifolds.Extensive experiments on simulated, three real-world object recognition and twocomputational biology datasets, have illustrated the power and superiorperformance of our approach over multiple state-of-the-art unsupervised featureselection methods. Additionally, we show that GLFS selects a sparser set ofmore relevant features in a supervised setting outperforming the popularelastic net methodology.
arxiv-1404-2772 | A New Clustering Approach for Anomaly Intrusion Detection |  http://arxiv.org/abs/1404.2772  | author:Ravi Ranjan, G. Sahoo category:cs.DC cs.CR cs.LG published:2014-04-10 summary:Recent advances in technology have made our work easier compare to earliertimes. Computer network is growing day by day but while discussing about thesecurity of computers and networks it has always been a major concerns fororganizations varying from smaller to larger enterprises. It is true thatorganizations are aware of the possible threats and attacks so they alwaysprepare for the safer side but due to some loopholes attackers are able to makeattacks. Intrusion detection is one of the major fields of research andresearchers are trying to find new algorithms for detecting intrusions.Clustering techniques of data mining is an interested area of research fordetecting possible intrusions and attacks. This paper presents a new clusteringapproach for anomaly intrusion detection by using the approach of K-medoidsmethod of clustering and its certain modifications. The proposed algorithm isable to achieve high detection rate and overcomes the disadvantages of K-meansalgorithm.
arxiv-1404-2878 | Overview of Stemming Algorithms for Indian and Non-Indian Languages |  http://arxiv.org/abs/1404.2878  | author:Dalwadi Bijal, Suthar Sanket category:cs.CL published:2014-04-10 summary:Stemming is a pre-processing step in Text Mining applications as well as avery common requirement of Natural Language processing functions. Stemming isthe process for reducing inflected words to their stem. The main purpose ofstemming is to reduce different grammatical forms / word forms of a word likeits noun, adjective, verb, adverb etc. to its root form. Stemming is widelyuses in Information Retrieval system and reduces the size of index files. Wecan say that the goal of stemming is to reduce inflectional forms and sometimesderivationally related forms of a word to a common base form. In this paper wehave discussed different stemming algorithm for non-Indian and Indian language,methods of stemming, accuracy and errors.
arxiv-1405-6166 | Real Time Speckle Image De-Noising |  http://arxiv.org/abs/1405.6166  | author:D. Sachin Kumar, P. R. Seshadri, N. Vaishnav, Dr. Saraswathi Janaki category:cs.CV published:2014-04-10 summary:The paper presents real time speckle de-noising based on activity computationalgorithm and wavelet transform. Speckles arise in an image when laser light isreflected from an illuminated surface. The process involves detection ofspeckles in an image by obtaining a number of frames of the same object underdifferent illumination or angle and comparing the frames for the granularcomputation and de-noising the same on presence of greater activity index. Theproject can be implemented in FPGA (Field Programmable Gate Array) technology.The results can be shown that the used activity computation algorithm andwavelet transform has better accuracy in the process of speckle detection andde-noising.
arxiv-1404-2644 | A Distributed Frank-Wolfe Algorithm for Communication-Efficient Sparse Learning |  http://arxiv.org/abs/1404.2644  | author:Aurélien Bellet, Yingyu Liang, Alireza Bagheri Garakani, Maria-Florina Balcan, Fei Sha category:cs.DC cs.LG stat.ML published:2014-04-09 summary:Learning sparse combinations is a frequent theme in machine learning. In thispaper, we study its associated optimization problem in the distributed settingwhere the elements to be combined are not centrally located but spread over anetwork. We address the key challenges of balancing communication costs andoptimization errors. To this end, we propose a distributed Frank-Wolfe (dFW)algorithm. We obtain theoretical guarantees on the optimization error$\epsilon$ and communication cost that do not depend on the total number ofcombining elements. We further show that the communication cost of dFW isoptimal by deriving a lower-bound on the communication cost required toconstruct an $\epsilon$-approximate solution. We validate our theoreticalanalysis with empirical studies on synthetic and real-world data, whichdemonstrate that dFW outperforms both baselines and competing methods. We alsostudy the performance of dFW when the conditions of our analysis are relaxed,and show that dFW is fairly robust.
arxiv-1404-2353 | Power System Parameters Forecasting Using Hilbert-Huang Transform and Machine Learning |  http://arxiv.org/abs/1404.2353  | author:Victor Kurbatsky, Nikita Tomin, Vadim Spiryaev, Paul Leahy, Denis Sidorov, Alexei Zhukov category:cs.LG stat.ML 62M10, 91B84 published:2014-04-09 summary:A novel hybrid data-driven approach is developed for forecasting power systemparameters with the goal of increasing the efficiency of short-term forecastingstudies for non-stationary time-series. The proposed approach is based on modedecomposition and a feature analysis of initial retrospective data using theHilbert-Huang transform and machine learning algorithms. The random forests andgradient boosting trees learning techniques were examined. The decision treetechniques were used to rank the importance of variables employed in theforecasting models. The Mean Decrease Gini index is employed as an impurityfunction. The resulting hybrid forecasting models employ the radial basisfunction neural network and support vector regression. Apart from introductionand references the paper is organized as follows. The section 2 presents thebackground and the review of several approaches for short-term forecasting ofpower system parameters. In the third section a hybrid machine learning-basedalgorithm using Hilbert-Huang transform is developed for short-term forecastingof power system parameters. Fourth section describes the decision tree learningalgorithms used for the issue of variables importance. Finally in section sixthe experimental results in the following electric power problems arepresented: active power flow forecasting, electricity price forecasting and forthe wind speed and direction forecasting.
arxiv-1404-2268 | A Compact Linear Programming Relaxation for Binary Sub-modular MRF |  http://arxiv.org/abs/1404.2268  | author:Junyan Wang, Sai-Kit Yeung category:cs.CV published:2014-04-09 summary:We propose a novel compact linear programming (LP) relaxation for binarysub-modular MRF in the context of object segmentation. Our model is obtained bylinearizing an $l_1^+$-norm derived from the quadratic programming (QP) form ofthe MRF energy. The resultant LP model contains significantly fewer variablesand constraints compared to the conventional LP relaxation of the MRF energy.In addition, unlike QP which can produce ambiguous labels, our model can beviewed as a quasi-total-variation minimization problem, and it can thereforepreserve the discontinuities in the labels. We further establish a relaxationbound between our LP model and the conventional LP model. In the experiments,we demonstrate our method for the task of interactive object segmentation. OurLP model outperforms QP when converting the continuous labels to binary labelsusing different threshold values on the entire Oxford interactive segmentationdataset. The computational complexity of our LP is of the same order as that ofthe QP, and it is significantly lower than the conventional LP relaxation.
arxiv-1404-2571 | RANCOR: Non-Linear Image Registration with Total Variation Regularization |  http://arxiv.org/abs/1404.2571  | author:Martin Rajchl, John S. H. Baxter, Wu Qiu, Ali R. Khan, Aaron Fenster, Terry M. Peters, Jing Yuan category:cs.CV published:2014-04-09 summary:Optimization techniques have been widely used in deformable registration,allowing for the incorporation of similarity metrics with regularizationmechanisms. These regularization mechanisms are designed to mitigate theeffects of trivial solutions to ill-posed registration problems and tootherwise ensure the resulting deformation fields are well-behaved. This paperintroduces a novel deformable registration algorithm, RANCOR, which usesiterative convexification to address deformable registration problems undertotal-variation regularization. Initial comparative results against fourstate-of-the-art registration algorithms are presented using the Internet BrainSegmentation Repository (IBSR) database.
arxiv-1404-2553 | Noisy Optimization: Convergence with a Fixed Number of Resamplings |  http://arxiv.org/abs/1404.2553  | author:Marie-Liesse Cauwet category:math.OC stat.ML published:2014-04-09 summary:It is known that evolution strategies in continuous domains might notconverge in the presence of noise. It is also known that, under mildassumptions, and using an increasing number of resamplings, one can mitigatethe effect of additive noise and recover convergence. We show new sufficientconditions for the convergence of an evolutionary algorithm with constantnumber of resamplings; in particular, we get fast rates (log-linearconvergence) provided that the variance decreases around the optimum slightlyfaster than in the so-called multiplicative noise model. Keywords: Noisyoptimization, evolutionary algorithm, theory.
arxiv-1404-2083 | Efficiency of conformalized ridge regression |  http://arxiv.org/abs/1404.2083  | author:Evgeny Burnaev, Vladimir Vovk category:cs.LG stat.ML published:2014-04-08 summary:Conformal prediction is a method of producing prediction sets that can beapplied on top of a wide range of prediction algorithms. The method has aguaranteed coverage probability under the standard IID assumption regardless ofwhether the assumptions (often considerably more restrictive) of the underlyingalgorithm are satisfied. However, for the method to be really useful it isdesirable that in the case where the assumptions of the underlying algorithmare satisfied, the conformal predictor loses little in efficiency as comparedwith the underlying algorithm (whereas being a conformal predictor, it has thestronger guarantee of validity). In this paper we explore the degree to whichthis additional requirement of efficiency is satisfied in the case of Bayesianridge regression; we find that asymptotically conformal prediction sets differlittle from ridge regression prediction intervals when the standard Bayesianassumptions are satisfied.
arxiv-1404-2005 | Automatic Tracker Selection w.r.t Object Detection Performance |  http://arxiv.org/abs/1404.2005  | author:Duc Phu Chau, François Bremond, Monique Thonnat, Slawomir Bak category:cs.CV published:2014-04-08 summary:The tracking algorithm performance depends on video content. This paperpresents a new multi-object tracking approach which is able to cope with videocontent variations. First the object detection is improved using Kanade-Lucas-Tomasi (KLT) feature tracking. Second, for each mobile object, anappropriate tracker is selected among a KLT-based tracker and a discriminativeappearance-based tracker. This selection is supported by an online trackingevaluation. The approach has been experimented on three public video datasets.The experimental results show a better performance of the proposed approachcompared to recent state of the art trackers.
arxiv-1404-2229 | Towards the Safety of Human-in-the-Loop Robotics: Challenges and Opportunities for Safety Assurance of Robotic Co-Workers |  http://arxiv.org/abs/1404.2229  | author:Kerstin Eder, Chris Harper, Ute Leonards category:cs.RO cs.LG I.2.9 published:2014-04-08 summary:The success of the human-robot co-worker team in a flexible manufacturingenvironment where robots learn from demonstration heavily relies on the correctand safe operation of the robot. How this can be achieved is a challenge thatrequires addressing both technical as well as human-centric research questions.In this paper we discuss the state of the art in safety assurance, existing aswell as emerging standards in this area, and the need for new approaches tosafety assurance in the context of learning machines. We then focus on roboticlearning from demonstration, the challenges these techniques pose to safetyassurance and indicate opportunities to integrate safety considerations intoalgorithms "by design". Finally, from a human-centric perspective, we stipulatethat, to achieve high levels of safety and ultimately trust, the roboticco-worker must meet the innate expectations of the humans it works with. It isour aim to stimulate a discussion focused on the safety aspects ofhuman-in-the-loop robotics, and to foster multidisciplinary collaboration toaddress the research challenges identified.
arxiv-1404-2078 | Optimistic Risk Perception in the Temporal Difference error Explains the Relation between Risk-taking, Gambling, Sensation-seeking and Low Fear |  http://arxiv.org/abs/1404.2078  | author:Joost Broekens, Tim Baarslag category:cs.LG q-bio.NC published:2014-04-08 summary:Understanding the affective, cognitive and behavioural processes involved inrisk taking is essential for treatment and for setting environmental conditionsto limit damage. Using Temporal Difference Reinforcement Learning (TDRL) wecomputationally investigated the effect of optimism in risk perception in avariety of goal-oriented tasks. Optimism in risk perception was studied byvarying the calculation of the Temporal Difference error, i.e., delta, in threeways: realistic (stochastically correct), optimistic (assuming action control),and overly optimistic (assuming outcome control). We show that for the gamblingtask individuals with 'healthy' perception of control, i.e., action optimism,do not develop gambling behaviour while individuals with 'unhealthy' perceptionof control, i.e., outcome optimism, do. We show that high intensity ofsensations and low levels of fear co-occur due to optimistic risk perception.We found that overly optimistic risk perception (outcome optimism) results inrisk taking and in persistent gambling behaviour in addition to high intensityof sensations. We discuss how our results replicate risk-taking relatedphenomena.
arxiv-1404-2071 | Extracting a bilingual semantic grammar from FrameNet-annotated corpora |  http://arxiv.org/abs/1404.2071  | author:Dana Dannélls, Normunds Grūzītis category:cs.CL published:2014-04-08 summary:We present the creation of an English-Swedish FrameNet-based grammar inGrammatical Framework. The aim of this research is to make existing framenetscomputationally accessible for multilingual natural language applications via acommon semantic grammar API, and to facilitate the porting of such grammar toother languages. In this paper, we describe the abstract syntax of the semanticgrammar while focusing on its automatic extraction possibilities. We haveextracted a shared abstract syntax from ~58,500 annotated sentences in BerkeleyFrameNet (BFN) and ~3,500 annotated sentences in Swedish FrameNet (SweFN). Theabstract syntax defines 769 frame-specific valence patterns that cover 77.8%examples in BFN and 74.9% in SweFN belonging to the shared set of 471 frames.As a side result, we provide a unified method for comparing semantic andsyntactic valence patterns across framenets.
arxiv-1404-1999 | Notes on Generalized Linear Models of Neurons |  http://arxiv.org/abs/1404.1999  | author:Jonathon Shlens category:cs.NE cs.LG q-bio.NC published:2014-04-08 summary:Experimental neuroscience increasingly requires tractable models foranalyzing and predicting the behavior of neurons and networks. The generalizedlinear model (GLM) is an increasingly popular statistical framework foranalyzing neural data that is flexible, exhibits rich dynamic behavior and iscomputationally tractable (Paninski, 2004; Pillow et al., 2008; Truccolo etal., 2005). What follows is a brief summary of the primary equations governingthe application of GLM's to spike trains with a few sentences linking this workto the larger statistical literature. Latter sections include extensions of abasic GLM to model spatio-temporal receptive fields as well as network activityin an arbitrary numbers of neurons.
arxiv-1404-2014 | Entropy Computation of Document Images in Run-Length Compressed Domain |  http://arxiv.org/abs/1404.2014  | author:P. Nagabhushan, Mohammed Javed, B. B. Chaudhuri category:cs.CV published:2014-04-08 summary:Compression of documents, images, audios and videos have been traditionallypracticed to increase the efficiency of data storage and transfer. However, inorder to process or carry out any analytical computations, decompression hasbecome an unavoidable pre-requisite. In this research work, we have attemptedto compute the entropy, which is an important document analytic directly fromthe compressed documents. We use Conventional Entropy Quantifier (CEQ) andSpatial Entropy Quantifiers (SEQ) for entropy computations [1]. The entropiesobtained are useful in applications like establishing equivalence, wordspotting and document retrieval. Experiments have been performed with all thedata sets of [1], at character, word and line levels taking compresseddocuments in run-length compressed domain. The algorithms developed arecomputational and space efficient, and results obtained match 100% with theresults reported in [1].
arxiv-1404-2007 | A Permutation Approach for Selecting the Penalty Parameter in Penalized Model Selection |  http://arxiv.org/abs/1404.2007  | author:Jeremy Sabourin, William Valdar, Andrew Nobel category:stat.ML published:2014-04-08 summary:We describe a simple, efficient, permutation based procedure for selectingthe penalty parameter in the LASSO. The procedure, which is intended forapplications where variable selection is the primary focus, can be applied in avariety of structural settings, including generalized linear models. We brieflydiscuss connections between permutation selection and existing theory for theLASSO. In addition, we present a simulation study and an analysis of three realdata sets in which permutation selection is compared with cross-validation(CV), the Bayesian information criterion (BIC), and a selection method based onrecently developed testing procedures for the LASSO.
arxiv-1405-6168 | Human Face as human single identity |  http://arxiv.org/abs/1405.6168  | author:Spits Warnars category:cs.CV published:2014-04-08 summary:Human face as a physical human recognition can be used as a unique identityfor computer to recognize human by transforming human face with face algorithmas simple text number which can be primary key for human. Human face as singleidentity for human will be done by making a huge and large world centre humanface database, where the human face around the world will be recorded from timeto time and from generation to generation. Architecture database will bedivided become human face image database which will save human face images andhuman face output code which will save human face output code as atransformation human face image with face algorithm. As an improvement theslightly and simple human face output code database will make human facesearching process become more fast. Transaction with human face as atransaction without card can make human no need their card for the transactionand office automation and banking system as an example for implementationarchitecture. As an addition suspect human face database can be extended forfighting crime and terrorism by doing surveillance and searching suspect humanface around the world.
arxiv-1404-2086 | Cascades of Regression Tree Fields for Image Restoration |  http://arxiv.org/abs/1404.2086  | author:Uwe Schmidt, Jeremy Jancsary, Sebastian Nowozin, Stefan Roth, Carsten Rother category:cs.CV published:2014-04-08 summary:Conditional random fields (CRFs) are popular discriminative models forcomputer vision and have been successfully applied in the domain of imagerestoration, especially to image denoising. For image deblurring, however,discriminative approaches have been mostly lacking. We posit two reasons forthis: First, the blur kernel is often only known at test time, requiring anydiscriminative approach to cope with considerable variability. Second, giventhis variability it is quite difficult to construct suitable features fordiscriminative prediction. To address these challenges we first show aconnection between common half-quadratic inference for generative image priorsand Gaussian CRFs. Based on this analysis, we then propose a cascade model forimage restoration that consists of a Gaussian CRF at each stage. Each stage ofour cascade is semi-parametric, i.e. it depends on the instance-specificparameters of the restoration problem, such as the blur kernel. We train ourmodel by loss minimization with synthetically generated training data. Ourexperiments show that when applied to non-blind image deblurring, the proposedapproach is efficient and yields state-of-the-art restoration quality on imagescorrupted with synthetic and real blur. Moreover, we demonstrate itssuitability for image denoising, where we achieve competitive results forgrayscale and color images.
arxiv-1404-1982 | Aspect-Based Opinion Extraction from Customer reviews |  http://arxiv.org/abs/1404.1982  | author:Amani K Samha, Yuefeng Li, Jinglan Zhang category:cs.CL cs.IR published:2014-04-08 summary:Text is the main method of communicating information in the digital age.Messages, blogs, news articles, reviews, and opinionated information abound onthe Internet. People commonly purchase products online and post their opinionsabout purchased items. This feedback is displayed publicly to assist otherswith their purchasing decisions, creating the need for a mechanism with whichto extract and summarize useful information for enhancing the decision-makingprocess. Our contribution is to improve the accuracy of extraction by combiningdifferent techniques from three major areas, named Data Mining, NaturalLanguage Processing techniques and Ontologies. The proposed frameworksequentially mines products aspects and users opinions, groups representativeaspects by similarity, and generates an output summary. This paper focuses onthe task of extracting product aspects and users opinions by extracting allpossible aspects and opinions from reviews using natural language, ontology,and frequent (tag) sets. The proposed framework, when compared with an existingbaseline model, yielded promising results.
arxiv-1405-6223 | Coupled Item-based Matrix Factorization |  http://arxiv.org/abs/1405.6223  | author:Fangfang Li, Guandong Xu, Longbing Cao category:cs.LG cs.IR published:2014-04-08 summary:The essence of the challenges cold start and sparsity in Recommender Systems(RS) is that the extant techniques, such as Collaborative Filtering (CF) andMatrix Factorization (MF), mainly rely on the user-item rating matrix, whichsometimes is not informative enough for predicting recommendations. To solvethese challenges, the objective item attributes are incorporated ascomplementary information. However, most of the existing methods for inferringthe relationships between items assume that the attributes are "independentlyand identically distributed (iid)", which does not always hold in reality. Infact, the attributes are more or less coupled with each other by some implicitrelationships. Therefore, in this pa-per we propose an attribute-based coupledsimilarity measure to capture the implicit relationships between items. We thenintegrate the implicit item coupling into MF to form the Coupled Item-basedMatrix Factorization (CIMF) model. Experimental results on two open data setsdemonstrate that CIMF outperforms the benchmark methods.
arxiv-1404-2885 | A Networks and Machine Learning Approach to Determine the Best College Coaches of the 20th-21st Centuries |  http://arxiv.org/abs/1404.2885  | author:Tian-Shun Jiang, Zachary Polizzi, Christopher Yuan category:stat.AP cs.LG cs.SI published:2014-04-08 summary:Our objective is to find the five best college sports coaches of past centuryfor three different sports. We decided to look at men's basketball, football,and baseball. We wanted to use an approach that could definitively determineteam skill from the games played, and then use a machine-learning algorithm tocalculate the correct coach skills for each team in a given year. We created anetworks-based model to calculate team skill from historical game data. Adigraph was created for each year in each sport. Nodes represented teams, andedges represented a game played between two teams. The arrowhead pointedtowards the losing team. We calculated the team skill of each graph using aright-hand eigenvector centrality measure. This way, teams that beat good teamswill be ranked higher than teams that beat mediocre teams. The eigenvectorcentrality rankings for most years were well correlated with tournamentperformance and poll-based rankings. We assumed that the relationship betweencoach skill $C_s$, player skill $P_s$, and team skill $T_s$ was $C_s \cdot P_s= T_s$. We then created a function to describe the probability that a givenscore difference would occur based on player skill and coach skill. Wemultiplied the probabilities of all edges in the network together to find theprobability that the correct network would occur with any given player skilland coach skill matrix. We was able to determine player skill as a function ofteam skill and coach skill, eliminating the need to optimize two unknownmatrices. The top five coaches in each year were noted, and the top coach ofall time was calculated by dividing the number of times that coach ranked inthe yearly top five by the years said coach had been active.
arxiv-1404-2189 | Data mining for censored time-to-event data: A Bayesian network model for predicting cardiovascular risk from electronic health record data |  http://arxiv.org/abs/1404.2189  | author:Sunayan Bandyopadhyay, Julian Wolfson, David M. Vock, Gabriela Vazquez-Benitez, Gediminas Adomavicius, Mohamed Elidrisi, Paul E. Johnson, Patrick J. O'Connor category:stat.ML stat.AP published:2014-04-08 summary:Models for predicting the risk of cardiovascular events based on individualpatient characteristics are important tools for managing patient care. Mostcurrent and commonly used risk prediction models have been built from carefullyselected epidemiological cohorts. However, the homogeneity and limited size ofsuch cohorts restricts the predictive power and generalizability of these riskmodels to other populations. Electronic health data (EHD) from large healthcare systems provide access to data on large, heterogeneous, andcontemporaneous patient populations. The unique features and challenges of EHD,including missing risk factor information, non-linear relationships betweenrisk factors and cardiovascular event outcomes, and differing effects fromdifferent patient subgroups, demand novel machine learning approaches to riskmodel development. In this paper, we present a machine learning approach basedon Bayesian networks trained on EHD to predict the probability of having acardiovascular event within five years. In such data, event status may beunknown for some individuals as the event time is right-censored due todisenrollment and incomplete follow-up. Since many traditional data miningmethods are not well-suited for such data, we describe how to modify bothmodelling and assessment techniques to account for censored observation times.We show that our approach can lead to better predictive performance than theCox proportional hazards model (i.e., a regression-based approach commonly usedfor censored, time-to-event data) or a Bayesian network with {\em{ad hoc}}approaches to right-censoring. Our techniques are motivated by and illustratedon data from a large U.S. Midwestern health care system.
arxiv-1404-2188 | A Convolutional Neural Network for Modelling Sentences |  http://arxiv.org/abs/1404.2188  | author:Nal Kalchbrenner, Edward Grefenstette, Phil Blunsom category:cs.CL published:2014-04-08 summary:The ability to accurately represent sentences is central to languageunderstanding. We describe a convolutional architecture dubbed the DynamicConvolutional Neural Network (DCNN) that we adopt for the semantic modelling ofsentences. The network uses Dynamic k-Max Pooling, a global pooling operationover linear sequences. The network handles input sentences of varying lengthand induces a feature graph over the sentence that is capable of explicitlycapturing short and long-range relations. The network does not rely on a parsetree and is easily applicable to any language. We test the DCNN in fourexperiments: small scale binary and multi-class sentiment prediction, six-wayquestion classification and Twitter sentiment prediction by distantsupervision. The network achieves excellent performance in the first threetasks and a greater than 25% error reduction in the last task with respect tothe strongest baseline.
arxiv-1404-2124 | A Naive Bayes machine learning approach to risk prediction using censored, time-to-event data |  http://arxiv.org/abs/1404.2124  | author:Julian Wolfson, Sunayan Bandyopadhyay, Mohamed Elidrisi, Gabriela Vazquez-Benitez, Donald Musgrove, Gediminas Adomavicius, Paul Johnson, Patrick O'Connor category:stat.ML published:2014-04-08 summary:Predicting an individual's risk of experiencing a future clinical outcome isa statistical task with important consequences for both practicing cliniciansand public health experts. Modern observational databases such as electronichealth records (EHRs) provide an alternative to the longitudinal cohort studiestraditionally used to construct risk models, bringing with them bothopportunities and challenges. Large sample sizes and detailed covariatehistories enable the use of sophisticated machine learning techniques touncover complex associations and interactions, but observational databases areoften ``messy,'' with high levels of missing data and incomplete patientfollow-up. In this paper, we propose an adaptation of the well-known NaiveBayes (NB) machine learning approach for classification to time-to-eventoutcomes subject to censoring. We compare the predictive performance of ourmethod to the Cox proportional hazards model which is commonly used for riskprediction in healthcare populations, and illustrate its application toprediction of cardiovascular risk using an EHR dataset from a large Midwestintegrated healthcare system.
arxiv-1404-1777 | Neural Codes for Image Retrieval |  http://arxiv.org/abs/1404.1777  | author:Artem Babenko, Anton Slesarev, Alexandr Chigorin, Victor Lempitsky category:cs.CV published:2014-04-07 summary:It has been shown that the activations invoked by an image within the toplayers of a large convolutional neural network provide a high-level descriptorof the visual content of the image. In this paper, we investigate the use ofsuch descriptors (neural codes) within the image retrieval application. In theexperiments with several standard retrieval benchmarks, we establish thatneural codes perform competitively even when the convolutional neural networkhas been trained for an unrelated classification task (e.g.\ Image-Net). Wealso evaluate the improvement in the retrieval performance of neural codes,when the network is retrained on a dataset of images that are similar to imagesencountered at test time. We further evaluate the performance of the compressed neural codes and showthat a simple PCA compression provides very good short codes that givestate-of-the-art accuracy on a number of datasets. In general, neural codesturn out to be much more resilient to such compression in comparison otherstate-of-the-art descriptors. Finally, we show that discriminativedimensionality reduction trained on a dataset of pairs of matched photographsimproves the performance of PCA-compressed neural codes even further. Overall,our quantitative experiments demonstrate the promise of neural codes as visualdescriptors for image retrieval.
arxiv-1404-1664 | Icon Based Information Retrieval and Disease Identification in Agriculture |  http://arxiv.org/abs/1404.1664  | author:Namita Mittal, Basant Agarwal, Ajay Gupta, Hemant Madhur category:cs.HC cs.CV cs.CY cs.IR published:2014-04-07 summary:Recent developments in the ICT industry in past few decades has enabled thequick and easy access to the information available on the internet. But,digital literacy is the pre-requisite for its use. The main purpose of thispaper is to provide an interface for digitally illiterate users, especiallyfarmers to efficiently and effectively retrieve information through Internet.In addition, to enable the farmers to identify the disease in their crop, itscause and symptoms using digital image processing and pattern recognitioninstantly without waiting for an expert to visit the farms and identify thedisease.
arxiv-1404-1935 | Tyler's Covariance Matrix Estimator in Elliptical Models with Convex Structure |  http://arxiv.org/abs/1404.1935  | author:Ilya Soloveychik, Ami Wiesel category:stat.ML published:2014-04-07 summary:We address structured covariance estimation in elliptical distributions byassuming that the covariance is a priori known to belong to a given convex set,e.g., the set of Toeplitz or banded matrices. We consider the General Method ofMoments (GMM) optimization applied to robust Tyler's scatter M-estimatorsubject to these convex constraints. Unfortunately, GMM turns out to benon-convex due to the objective. Instead, we propose a new COCA estimator - aconvex relaxation which can be efficiently solved. We prove that the relaxationis tight in the unconstrained case for a finite number of samples, and in theconstrained case asymptotically. We then illustrate the advantages of COCA insynthetic simulations with structured compound Gaussian distributions. In theseexamples, COCA outperforms competing methods such as Tyler's estimator and itsprojection onto the structure set.
arxiv-1404-1682 | Pseudo-Zernike Based Multi-Pass Automatic Target Recognition From Multi-Channel SAR |  http://arxiv.org/abs/1404.1682  | author:Carmine Clemente, Luca Pallotta, Ian Proudler, Antonio De Maio, John J. Soraghan, Alfonso Farina category:cs.CV published:2014-04-07 summary:The capability to exploit multiple sources of information is of fundamentalimportance in a battlefield scenario. Information obtained from differentsources, and separated in space and time, provide the opportunity to exploitdiversities in order to mitigate uncertainty. For the specific challenge ofAutomatic Target Recognition (ATR) from radar platforms, both channel (e.g.polarization) and spatial diversity can provide useful information for such aspecific and critical task. In this paper the use of pseudo-Zernike momentsapplied to multi-channel multi-pass data is presented exploiting diversitiesand invariant properties leading to high confidence ATR, small computationalcomplexity and data transfer requirements. The effectiveness of the proposedapproach, in different configurations and data source availability isdemonstrated using real data.
arxiv-1404-1831 | Improving Bilayer Product Quantization for Billion-Scale Approximate Nearest Neighbors in High Dimensions |  http://arxiv.org/abs/1404.1831  | author:Artem Babenko, Victor Lempitsky category:cs.CV H.3.3 published:2014-04-07 summary:The top-performing systems for billion-scale high-dimensional approximatenearest neighbor (ANN) search are all based on two-layer architectures thatinclude an indexing structure and a compressed datapoints layer. An indexingstructure is crucial as it allows to avoid exhaustive search, while the lossydata compression is needed to fit the dataset into RAM. Several of the mostsuccessful systems use product quantization (PQ) for both the indexing and thedataset compression layers. These systems are however limited in the way theyexploit the interaction of product quantization processes that happen atdifferent stages of these systems. Here we introduce and evaluate two approximate nearest neighbor searchsystems that both exploit the synergy of product quantization processes in amore efficient way. The first system, called Fast Bilayer Product Quantization(FBPQ), speeds up the runtime of the baseline system (Multi-D-ADC) by severaltimes, while achieving the same accuracy. The second system, HierarchicalBilayer Product Quantization (HBPQ) provides a significantly better recall forthe same runtime at a cost of small memory footprint increase. For the BIGANNdataset of billion SIFT descriptors, the 10% increase in Recall@1 and the 17%increase in Recall@10 is observed.
arxiv-1404-1847 | Evaluation and Ranking of Machine Translated Output in Hindi Language using Precision and Recall Oriented Metrics |  http://arxiv.org/abs/1404.1847  | author:Aditi Kalyani, Hemant Kumud, Shashi Pal Singh, Ajai Kumar, Hemant Darbari category:cs.CL published:2014-04-07 summary:Evaluation plays a crucial role in development of Machine translationsystems. In order to judge the quality of an existing MT system i.e. if thetranslated output is of human translation quality or not, various automaticmetrics exist. We here present the implementation results of different metricswhen used on Hindi language along with their comparisons, illustrating howeffective are these metrics on languages like Hindi (free word order language).
arxiv-1404-1869 | DenseNet: Implementing Efficient ConvNet Descriptor Pyramids |  http://arxiv.org/abs/1404.1869  | author:Forrest Iandola, Matt Moskewicz, Sergey Karayev, Ross Girshick, Trevor Darrell, Kurt Keutzer category:cs.CV published:2014-04-07 summary:Convolutional Neural Networks (CNNs) can provide accurate objectclassification. They can be extended to perform object detection by iteratingover dense or selected proposed object regions. However, the runtime of suchdetectors scales as the total number and/or area of regions to examine perimage, and training such detectors may be prohibitively slow. However, for someCNN classifier topologies, it is possible to share significant work amongoverlapping regions to be classified. This paper presents DenseNet, an opensource system that computes dense, multiscale features from the convolutionallayers of a CNN based object classifier. Future work will involve trainingefficient object detectors with DenseNet feature descriptors.
arxiv-1404-1872 | Intégration des données d'un lexique syntaxique dans un analyseur syntaxique probabiliste |  http://arxiv.org/abs/1404.1872  | author:Anthony Sigogne, Matthieu Constant, Eric Laporte category:cs.CL published:2014-04-07 summary:This article reports the evaluation of the integration of data from asyntactic-semantic lexicon, the Lexicon-Grammar of French, into a syntacticparser. We show that by changing the set of labels for verbs and predicationalnouns, we can improve the performance on French of a non-lexicalizedprobabilistic parser.
arxiv-1404-1890 | Polish and English wordnets -- statistical analysis of interconnected networks |  http://arxiv.org/abs/1404.1890  | author:Maksymilian Bujok, Piotr Fronczak, Agata Fronczak category:cs.CL physics.soc-ph published:2014-04-07 summary:Wordnets are semantic networks containing nouns, verbs, adjectives, andadverbs organized according to linguistic principles, by means of semanticrelations. In this work, we adopt a complex network perspective to perform acomparative analysis of the English and Polish wordnets. We determine theirsimilarities and show that the networks exhibit some of the typicalcharacteristics observed in other real-world networks. We analyse interlingualrelations between both wordnets and deliberate over the problem of mapping thePolish lexicon onto the English one.
arxiv-1404-1559 | Sparse Coding: A Deep Learning using Unlabeled Data for High - Level Representation |  http://arxiv.org/abs/1404.1559  | author:R. Vidya, Dr. G. M. Nasira, R. P. Jaia Priyankka category:cs.LG cs.NE published:2014-04-06 summary:Sparse coding algorithm is an learning algorithm mainly for unsupervisedfeature for finding succinct, a little above high - level Representation ofinputs, and it has successfully given a way for Deep learning. Our objective isto use High - Level Representation data in form of unlabeled category to helpunsupervised learning task. when compared with labeled data, unlabeled data iseasier to acquire because, unlike labeled data it does not follow someparticular class labels. This really makes the Deep learning wider andapplicable to practical problems and learning. The main problem with sparsecoding is it uses Quadratic loss function and Gaussian noise mode. So, itsperforms is very poor when binary or integer value or other Non- Gaussian typedata is applied. Thus first we propose an algorithm for solving the L1 -regularized convex optimization algorithm for the problem to allow High - LevelRepresentation of unlabeled data. Through this we derive a optimal solution fordescribing an approach to Deep learning algorithm by using sparse code.
arxiv-1404-1592 | The Power of Online Learning in Stochastic Network Optimization |  http://arxiv.org/abs/1404.1592  | author:Longbo Huang, Xin Liu, Xiaohong Hao category:math.OC cs.LG cs.SY published:2014-04-06 summary:In this paper, we investigate the power of online learning in stochasticnetwork optimization with unknown system statistics {\it a priori}. We areinterested in understanding how information and learning can be efficientlyincorporated into system control techniques, and what are the fundamentalbenefits of doing so. We propose two \emph{Online Learning-Aided Control}techniques, $\mathtt{OLAC}$ and $\mathtt{OLAC2}$, that explicitly utilize thepast system information in current system control via a learning procedurecalled \emph{dual learning}. We prove strong performance guarantees of theproposed algorithms: $\mathtt{OLAC}$ and $\mathtt{OLAC2}$ achieve thenear-optimal $[O(\epsilon), O([\log(1/\epsilon)]^2)]$ utility-delay tradeoffand $\mathtt{OLAC2}$ possesses an $O(\epsilon^{-2/3})$ convergence time.$\mathtt{OLAC}$ and $\mathtt{OLAC2}$ are probably the first algorithms thatsimultaneously possess explicit near-optimal delay guarantee and sub-linearconvergence time. Simulation results also confirm the superior performance ofthe proposed algorithms in practice. To the best of our knowledge, our attemptis the first to explicitly incorporate online learning into stochastic networkoptimization and to demonstrate its power in both theory and practice.
arxiv-1404-1614 | A Denoising Autoencoder that Guides Stochastic Search |  http://arxiv.org/abs/1404.1614  | author:Alexander W. Churchill, Siddharth Sigtia, Chrisantha Fernando category:cs.NE cs.LG published:2014-04-06 summary:An algorithm is described that adaptively learns a non-linear mutationdistribution. It works by training a denoising autoencoder (DA) online at eachgeneration of a genetic algorithm to reconstruct a slowly decaying memory ofthe best genotypes so far. A compressed hidden layer forces the autoencoder tolearn hidden features in the training set that can be used to accelerate searchon novel problems with similar structure. Its output neurons define aprobability distribution that we sample from to produce offspring solutions.The algorithm outperforms a canonical genetic algorithm on severalcombinatorial optimisation problems, e.g. multidimensional 0/1 knapsackproblem, MAXSAT, HIFF, and on parameter optimisation problems, e.g. Rastriginand Rosenbrock functions.
arxiv-1404-1561 | Fast Supervised Hashing with Decision Trees for High-Dimensional Data |  http://arxiv.org/abs/1404.1561  | author:Guosheng Lin, Chunhua Shen, Qinfeng Shi, Anton van den Hengel, David Suter category:cs.CV cs.LG published:2014-04-06 summary:Supervised hashing aims to map the original features to compact binary codesthat are able to preserve label based similarity in the Hamming space.Non-linear hash functions have demonstrated the advantage over linear ones dueto their powerful generalization capability. In the literature, kernelfunctions are typically used to achieve non-linearity in hashing, which achieveencouraging retrieval performance at the price of slow evaluation and trainingtime. Here we propose to use boosted decision trees for achieving non-linearityin hashing, which are fast to train and evaluate, hence more suitable forhashing with high dimensional data. In our approach, we first proposesub-modular formulations for the hashing binary code inference problem and anefficient GraphCut based block search method for solving large-scale inference.Then we learn hash functions by training boosted decision trees to fit thebinary codes. Experiments demonstrate that our proposed method significantlyoutperforms most state-of-the-art methods in retrieval precision and trainingtime. Especially for high-dimensional data, our method is orders of magnitudefaster than many methods in terms of training time.
arxiv-1404-1530 | Provable Deterministic Leverage Score Sampling |  http://arxiv.org/abs/1404.1530  | author:Dimitris Papailiopoulos, Anastasios Kyrillidis, Christos Boutsidis category:cs.DS cs.IT cs.NA math.IT math.ST stat.ML stat.TH published:2014-04-06 summary:We explain theoretically a curious empirical phenomenon: "Approximating amatrix by deterministically selecting a subset of its columns with thecorresponding largest leverage scores results in a good low-rank matrixsurrogate". To obtain provable guarantees, previous work requires randomizedsampling of the columns with probabilities proportional to their leveragescores. In this work, we provide a novel theoretical analysis of deterministicleverage score sampling. We show that such deterministic sampling can beprovably as accurate as its randomized counterparts, if the leverage scoresfollow a moderately steep power-law decay. We support this power-law assumptionby providing empirical evidence that such decay laws are abundant in real-worlddata sets. We then demonstrate empirically the performance of deterministicleverage score sampling, which many times matches or outperforms thestate-of-the-art techniques.
arxiv-1404-1521 | Exploring the power of GPU's for training Polyglot language models |  http://arxiv.org/abs/1404.1521  | author:Vivek Kulkarni, Rami Al-Rfou', Bryan Perozzi, Steven Skiena category:cs.LG cs.CL published:2014-04-05 summary:One of the major research trends currently is the evolution of heterogeneousparallel computing. GP-GPU computing is being widely used and severalapplications have been designed to exploit the massive parallelism thatGP-GPU's have to offer. While GPU's have always been widely used in areas ofcomputer vision for image processing, little has been done to investigatewhether the massive parallelism provided by GP-GPU's can be utilizedeffectively for Natural Language Processing(NLP) tasks. In this work, weinvestigate and explore the power of GP-GPU's in the task of learning languagemodels. More specifically, we investigate the performance of training Polyglotlanguage models using deep belief neural networks. We evaluate the performanceof training the model on the GPU and present optimizations that boost theperformance on the GPU.One of the key optimizations, we propose increases theperformance of a function involved in calculating and updating the gradient byapproximately 50 times on the GPU for sufficiently large batch sizes. We showthat with the above optimizations, the GP-GPU's performance on the taskincreases by factor of approximately 3-4. The optimizations we made are genericTheano optimizations and hence potentially boost the performance of othermodels which rely on these operations.We also show that these optimizationsresult in the GPU's performance at this task being now comparable to that onthe CPU. We conclude by presenting a thorough evaluation of the applicabilityof GP-GPU's for this task and highlight the factors limiting the performance oftraining a Polyglot model on the GPU.
arxiv-1404-1425 | Density Estimation via Adaptive Partition and Discrepancy Control |  http://arxiv.org/abs/1404.1425  | author:Kun Yang, Wing Hung Wong category:stat.ML published:2014-04-05 summary:Given iid samples from some unknown continuous density on hyper-rectangle$[0, 1]^d$, we attempt to learn a piecewise constant function that approximatesthis underlying density nonparametrically. Our density estimate is defined on abinary split of $[0, 1]^d$ and built up sequentially according to discrepancycriteria; the key ingredient is to control the discrepancy adaptively in eachsub-rectangle to achieve overall bound. We prove that the estimate, even thoughsimple as it appears, preserves most of the estimation power. By exploiting itsstructure, it can be directly applied to some important pattern recognitiontasks such as mode seeking and density landscape exploration, we demonstrateits applicability through simulations and examples.
arxiv-1404-1492 | Ensemble Committees for Stock Return Classification and Prediction |  http://arxiv.org/abs/1404.1492  | author:James Brofos category:stat.ML cs.LG published:2014-04-05 summary:This paper considers a portfolio trading strategy formulated by algorithms inthe field of machine learning. The profitability of the strategy is measured bythe algorithm's capability to consistently and accurately identify stockindices with positive or negative returns, and to generate a preferredportfolio allocation on the basis of a learned model. Stocks are characterizedby time series data sets consisting of technical variables that reflect marketconditions in a previous time interval, which are utilized produce binaryclassification decisions in subsequent intervals. The learned model isconstructed as a committee of random forest classifiers, a non-linear supportvector machine classifier, a relevance vector machine classifier, and aconstituent ensemble of k-nearest neighbors classifiers. The Global IndustryClassification Standard (GICS) is used to explore the ensemble model's efficacywithin the context of various fields of investment including Energy, Materials,Financials, and Information Technology. Data from 2006 to 2012, inclusive, areconsidered, which are chosen for providing a range of market circumstances forevaluating the model. The model is observed to achieve an accuracy ofapproximately 70% when predicting stock price returns three months in advance.
arxiv-1404-1504 | A Compression Technique for Analyzing Disagreement-Based Active Learning |  http://arxiv.org/abs/1404.1504  | author:Yair Wiener, Steve Hanneke, Ran El-Yaniv category:cs.LG stat.ML published:2014-04-05 summary:We introduce a new and improved characterization of the label complexity ofdisagreement-based active learning, in which the leading quantity is theversion space compression set size. This quantity is defined as the size of thesmallest subset of the training data that induces the same version space. Weshow various applications of the new characterization, including a tightanalysis of CAL and refined label complexity bounds for linear separators undermixtures of Gaussians and axis-aligned rectangles under product densities. Theversion space compression set size, as well as the new characterization of thelabel complexity, can be naturally extended to agnostic learning problems, forwhich we show new speedup results for two well known active learningalgorithms.
arxiv-1404-1514 | Text Based Approach For Indexing And Retrieval Of Image And Video: A Review |  http://arxiv.org/abs/1404.1514  | author:Avinash N Bhute, B. B. Meshram category:cs.IR cs.CV cs.DL cs.MM published:2014-04-05 summary:Text data present in multimedia contain useful information for automaticannotation, indexing. Extracted information used for recognition of the overlayor scene text from a given video or image. The Extracted text can be used forretrieving the videos and images. In this paper, firstly, we are discussed thedifferent techniques for text extraction from images and videos. Secondly, weare reviewed the techniques for indexing and retrieval of image and videos byusing extracted text.
arxiv-1404-1151 | Recognition of Handwritten MODI Numerals using Hu and Zernike features |  http://arxiv.org/abs/1404.1151  | author:Sadanand A. Kulkarni, Prashant L. Borde, Ramesh R. Manza, Pravin L. Yannawar category:cs.CV published:2014-04-04 summary:Handwritten automatic character recognition has attracted many researchersall over the world to contribute automatic character recognition domain. Shapeidentification and feature extraction is very important part of any characterrecognition system and success of method is highly dependent on selection offeatures. However feature extraction is the most important step in defining theshape of the character as precisely and as uniquely as possible. This is indeedthe most important step and complex task as well and achieved success by usinginvariance property, irrespective of position and orientation. Zernike momentsdescribes shape, identify rotation invariant due to its Orthogonality property.MODI is an ancient script of India had cursive and complex representation ofcharacters. The work described in this paper presents efficiency of Zernikemoments over Hus moment for automatic recognition of handwritten MODI numerals.
arxiv-1404-1333 | Understanding Machine-learned Density Functionals |  http://arxiv.org/abs/1404.1333  | author:Li Li, John C. Snyder, Isabelle M. Pelaschier, Jessica Huang, Uma-Naresh Niranjan, Paul Duncan, Matthias Rupp, Klaus-Robert Müller, Kieron Burke category:cs.LG stat.ML published:2014-04-04 summary:Kernel ridge regression is used to approximate the kinetic energy ofnon-interacting fermions in a one-dimensional box as a functional of theirdensity. The properties of different kernels and methods of cross-validationare explored, and highly accurate energies are achieved. Accurate {\emconstrained optimal densities} are found via a modified Euler-Lagrangeconstrained minimization of the total energy. A projected gradient descentalgorithm is derived using local principal component analysis. Additionally, asparse grid representation of the density can be used without degrading theperformance of the methods. The implications for machine-learned densityfunctional approximations are discussed.
arxiv-1404-1361 | Learning the Conditional Independence Structure of Stationary Time Series: A Multitask Learning Approach |  http://arxiv.org/abs/1404.1361  | author:Alexander Jung category:stat.ML published:2014-04-04 summary:We propose a method for inferring the conditional independence graph (CIG) ofa high-dimensional Gaussian vector time series (discrete-time process) from afinite-length observation. By contrast to existing approaches, we do not relyon a parametric process model (such as, e.g., an autoregressive model) for theobserved random process. Instead, we only require certain smoothness properties(in the Fourier domain) of the process. The proposed inference scheme workseven for sample sizes much smaller than the number of scalar process componentsif the true underlying CIG is sufficiently sparse. A theoretical performanceanalysis provides conditions which guarantee that the probability of theproposed inference method to deliver a wrong CIG is below a prescribed value.These conditions imply lower bounds on the sample size such that the new methodis consistent asymptotically. Some numerical experiments validate ourtheoretical performance analysis and demonstrate superior performance of ourscheme compared to an existing (parametric) approach in case of model mismatch.
arxiv-1404-1140 | Scalable Planning and Learning for Multiagent POMDPs: Extended Version |  http://arxiv.org/abs/1404.1140  | author:Christopher Amato, Frans A. Oliehoek category:cs.AI cs.LG published:2014-04-04 summary:Online, sample-based planning algorithms for POMDPs have shown great promisein scaling to problems with large state spaces, but they become intractable forlarge action and observation spaces. This is particularly problematic inmultiagent POMDPs where the action and observation space grows exponentiallywith the number of agents. To combat this intractability, we propose a novelscalable approach based on sample-based planning and factored value functionsthat exploits structure present in many multiagent settings. This approachapplies not only in the planning case, but also in the Bayesian reinforcementlearning setting. Experimental results show that we are able to provide highquality solutions to large multiagent planning and learning problems.
arxiv-1404-1356 | Optimal learning with Bernstein Online Aggregation |  http://arxiv.org/abs/1404.1356  | author:Olivier Wintenberger category:stat.ML cs.LG math.ST stat.TH published:2014-04-04 summary:We introduce a new recursive aggregation procedure called Bernstein OnlineAggregation (BOA). The exponential weights include an accuracy term and asecond order term that is a proxy of the quadratic variation as in Hazan andKale (2010). This second term stabilizes the procedure that is optimal indifferent senses. We first obtain optimal regret bounds in the deterministiccontext. Then, an adaptive version is the first exponential weights algorithmthat exhibits a second order bound with excess losses that appears first inGaillard et al. (2014). The second order bounds in the deterministic contextare extended to a general stochastic context using the cumulative predictiverisk. Such conversion provides the main result of the paper, an inequality of anovel type comparing the procedure with any deterministic aggregation procedurefor an integrated criteria. Then we obtain an observable estimate of the excessof risk of the BOA procedure. To assert the optimality, we consider finally theiid case for strongly convex and Lipschitz continuous losses and we prove thatthe optimal rate of aggregation of Tsybakov (2003) is achieved. The batchversion of the BOA procedure is then the first adaptive explicit algorithm thatsatisfies an optimal oracle inequality with high probability.
arxiv-1404-1238 | Exact Estimation of Multiple Directed Acyclic Graphs |  http://arxiv.org/abs/1404.1238  | author:Chris J. Oates, Jim Q. Smith, Sach Mukherjee, James Cussens category:stat.ML published:2014-04-04 summary:This paper considers the problem of estimating the structure of multiplerelated directed acyclic graph (DAG) models. Building on recent developments inexact estimation of DAGs using integer linear programming (ILP), we present anILP approach for joint estimation over multiple DAGs, that does not requirethat the vertices in each DAG share a common ordering. Furthermore, we allowalso for (potentially unknown) dependency structure between the DAGs. Resultsare presented on both simulated data and fMRI data obtained from multiplesubjects.
arxiv-1404-1129 | An Efficient Two-Stage Sparse Representation Method |  http://arxiv.org/abs/1404.1129  | author:Chengyu Peng, Hong Cheng, Manchor Ko category:cs.CV G.1.6; I.4.10 published:2014-04-04 summary:There are a large number of methods for solving under-determined linearinverse problem. Many of them have very high time complexity for largedatasets. We propose a new method called Two-Stage Sparse Representation (TSSR)to tackle this problem. We decompose the representing space of signals into twoparts, the measurement dictionary and the sparsifying basis. The dictionary isdesigned to approximate a sub-Gaussian distribution to exploit itsconcentration property. We apply sparse coding to the signals on the dictionaryin the first stage, and obtain the training and testing coefficientsrespectively. Then we design the basis to approach an identity matrix in thesecond stage, to acquire the Restricted Isometry Property (RIP) anduniversality property. The testing coefficients are encoded over the basis andthe final representing coefficients are obtained. We verify that the projectionof testing coefficients onto the basis is a good approximation of the signalonto the representing space. Since the projection is conducted on a muchsparser space, the runtime is greatly reduced. For concrete realization, weprovide an instance for the proposed TSSR. Experiments on four biometricsdatabases show that TSSR is effective and efficient, comparing with severalclassical methods for solving linear inverse problem.
arxiv-1404-1377 | Orthogonal Rank-One Matrix Pursuit for Low Rank Matrix Completion |  http://arxiv.org/abs/1404.1377  | author:Zheng Wang, Ming-Jun Lai, Zhaosong Lu, Wei Fan, Hasan Davulcu, Jieping Ye category:cs.LG math.NA stat.ML published:2014-04-04 summary:In this paper, we propose an efficient and scalable low rank matrixcompletion algorithm. The key idea is to extend orthogonal matching pursuitmethod from the vector case to the matrix case. We further propose an economicversion of our algorithm by introducing a novel weight updating rule to reducethe time and storage complexity. Both versions are computationally inexpensivefor each matrix pursuit iteration, and find satisfactory results in a fewiterations. Another advantage of our proposed algorithm is that it has only onetunable parameter, which is the rank. It is easy to understand and to use bythe user. This becomes especially important in large-scale learning problems.In addition, we rigorously show that both versions achieve a linear convergencerate, which is significantly better than the previous known results. We alsoempirically compare the proposed algorithms with several state-of-the-artmatrix completion algorithms on many real-world datasets, including thelarge-scale recommendation dataset Netflix as well as the MovieLens datasets.Numerical results show that our proposed algorithm is more efficient thancompeting algorithms while achieving similar or better prediction performance.
arxiv-1404-1144 | AIS-MACA- Z: MACA based Clonal Classifier for Splicing Site, Protein Coding and Promoter Region Identification in Eukaryotes |  http://arxiv.org/abs/1404.1144  | author:Pokkuluri Kiran Sree, Inampudi Ramesh Babu, SSSN Usha Devi N category:cs.CE cs.LG published:2014-04-04 summary:Bioinformatics incorporates information regarding biological data storage,accessing mechanisms and presentation of characteristics within this data. Mostof the problems in bioinformatics and be addressed efficiently by computertechniques. This paper aims at building a classifier based on MultipleAttractor Cellular Automata (MACA) which uses fuzzy logic with version Z topredict splicing site, protein coding and promoter region identification ineukaryotes. It is strengthened with an artificial immune system technique(AIS), Clonal algorithm for choosing rules of best fitness. The proposedclassifier can handle DNA sequences of lengths 54,108,162,252,354. Thisclassifier gives the exact boundaries of both protein and promoter regions withan average accuracy of 90.6%. This classifier can predict the splicing sitewith 97% accuracy. This classifier was tested with 1, 97,000 data componentswhich were taken from Fickett & Toung , EPDnew, and other sequences from arenowned medical university.
arxiv-1404-1371 | Multiple Testing for Neuroimaging via Hidden Markov Random Field |  http://arxiv.org/abs/1404.1371  | author:Hai Shu, Bin Nan, Robert Koeppe, the Alzheimer's Dise category:stat.AP stat.ML published:2014-04-04 summary:One of the important objectives that the Alzheimer's Disease NeuroimagingInitiative (ADNI) tries to achieve is to understand how the human brain changesover the course of disease progression. We consider voxel-level analysis forthe 18F-Fluorodeoxyglucose positron emission tomography (FDG-PET) imaging studyin ADNI for such a purpose. Traditional voxel-level multiple testing proceduresin neuroimaging, which are mostly p-value based, often ignore the spatialcorrelations among neighboring voxels and thus suffer from substantial loss ofpower. We extend the local-significance-index based procedure, which aims tominimize the false nondiscovery rate subject to a constraint on the falsediscovery rate, to three-dimensional neuroimaging data using a hidden Markovrandom field model. A generalized expectation-maximization algorithm isproposed for estimating the model parameters. Extensive simulations show thatthe proposed approach is more powerful than conventional false discovery rateprocedures. We apply the method to the comparison between mild cognitiveimpairment, a disease status with increased risk of developing Alzheimer's oranother dementia, and normal controls in the ADNI's FDG-PET imaging study.
arxiv-1404-0979 | Kernel-Based Adaptive Online Reconstruction of Coverage Maps With Side Information |  http://arxiv.org/abs/1404.0979  | author:Martin Kasparick, Renato L. G. Cavalcante, Stefan Valentin, Slawomir Stanczak, Masahiro Yukawa category:cs.NI cs.LG stat.ML published:2014-04-03 summary:In this paper, we address the problem of reconstructing coverage maps frompath-loss measurements in cellular networks. We propose and evaluate twokernel-based adaptive online algorithms as an alternative to typical offlinemethods. The proposed algorithms are application-tailored extensions ofpowerful iterative methods such as the adaptive projected subgradient methodand a state-of-the-art adaptive multikernel method. Assuming that the movingtrajectories of users are available, it is shown how side information can beincorporated in the algorithms to improve their convergence performance and thequality of the estimation. The complexity is significantly reduced by imposingsparsity-awareness in the sense that the algorithms exploit the compressibilityof the measurement data to reduce the amount of data which is saved andprocessed. Finally, we present extensive simulations based on realistic data toshow that our algorithms provide fast, robust estimates of coverage maps inreal-world scenarios. Envisioned applications include path-loss predictionalong trajectories of mobile users as a building block for anticipatorybuffering or traffic offloading.
arxiv-1404-0789 | The Least Wrong Model Is Not in the Data |  http://arxiv.org/abs/1404.0789  | author:Oscar Stiffelman category:cs.LG published:2014-04-03 summary:The true process that generated data cannot be determined when multipleexplanations are possible. Prediction requires a model of the probability thata process, chosen randomly from the set of candidate explanations, generatessome future observation. The best model includes all of the informationcontained in the minimal description of the data that is not contained in thedata. It is closely related to the Halting Problem and is logarithmic in thesize of the data. Prediction is difficult because the ideal model is notcomputable, and the best computable model is not "findable." However, the errorfrom any approximation can be bounded by the size of the description using themodel.
arxiv-1404-0751 | Subspace Learning from Extremely Compressed Measurements |  http://arxiv.org/abs/1404.0751  | author:Akshay Krishnamurthy, Martin Azizyan, Aarti Singh category:stat.ML cs.LG published:2014-04-03 summary:We consider learning the principal subspace of a large set of vectors from anextremely small number of compressive measurements of each vector. Ourtheoretical results show that even a constant number of measurements per columnsuffices to approximate the principal subspace to arbitrary precision, providedthat the number of vectors is large. This result is achieved by a simplealgorithm that computes the eigenvectors of an estimate of the covariancematrix. The main insight is to exploit an averaging effect that arises fromapplying a different random projection to each vector. We provide a number ofsimulations confirming our theoretical results.
arxiv-1404-0752 | An Efficient Search Strategy for Aggregation and Discretization of Attributes of Bayesian Networks Using Minimum Description Length |  http://arxiv.org/abs/1404.0752  | author:Jem Corcoran, Daniel Tran, Nicholas Levine category:stat.ML published:2014-04-03 summary:Bayesian networks are convenient graphical expressions for high dimensionalprobability distributions representing complex relationships between a largenumber of random variables. They have been employed extensively in areas suchas bioinformatics, artificial intelligence, diagnosis, and risk management. Therecovery of the structure of a network from data is of prime importance for thepurposes of modeling, analysis, and prediction. Most recovery algorithms in theliterature assume either discrete of continuous but Gaussian data. For generalcontinuous data, discretization is usually employed but often destroys the verystructure one is out to recover. Friedman and Goldszmidt suggest an approachbased on the minimum description length principle that chooses a discretizationwhich preserves the information in the original data set, however it is onewhich is difficult, if not impossible, to implement for even moderately sizednetworks. In this paper we provide an extremely efficient search strategy whichallows one to use the Friedman and Goldszmidt discretization in practice.
arxiv-1404-1116 | Resolving Multi-path Interference in Time-of-Flight Imaging via Modulation Frequency Diversity and Sparse Regularization |  http://arxiv.org/abs/1404.1116  | author:Ayush Bhandari, Achuta Kadambi, Refael Whyte, Christopher Barsi, Micha Feigin, Adrian Dorrington, Ramesh Raskar category:cs.CV cs.IT math.IT physics.optics published:2014-04-03 summary:Time-of-flight (ToF) cameras calculate depth maps by reconstructing phaseshifts of amplitude-modulated signals. For broad illumination or transparentobjects, reflections from multiple scene points can illuminate a given pixel,giving rise to an erroneous depth map. We report here a sparsity regularizedsolution that separates K-interfering components using multiple modulationfrequency measurements. The method maps ToF imaging to the general framework ofspectral estimation theory and has applications in improving depth profiles andexploiting multiple scattering.
arxiv-1404-1100 | A Tutorial on Principal Component Analysis |  http://arxiv.org/abs/1404.1100  | author:Jonathon Shlens category:cs.LG stat.ML published:2014-04-03 summary:Principal component analysis (PCA) is a mainstay of modern data analysis - ablack box that is widely used but (sometimes) poorly understood. The goal ofthis paper is to dispel the magic behind this black box. This manuscriptfocuses on building a solid intuition for how and why principal componentanalysis works. This manuscript crystallizes this knowledge by deriving fromsimple intuitions, the mathematics behind PCA. This tutorial does not shy awayfrom explaining the ideas informally, nor does it shy away from themathematics. The hope is that by addressing both aspects, readers of all levelswill be able to gain a better understanding of PCA as well as the when, the howand the why of applying this technique.
arxiv-1404-1066 | Parallel Support Vector Machines in Practice |  http://arxiv.org/abs/1404.1066  | author:Stephen Tyree, Jacob R. Gardner, Kilian Q. Weinberger, Kunal Agrawal, John Tran category:cs.LG published:2014-04-03 summary:In this paper, we evaluate the performance of various parallel optimizationmethods for Kernel Support Vector Machines on multicore CPUs and GPUs. Inparticular, we provide the first comparison of algorithms with explicit andimplicit parallelization. Most existing parallel implementations for multi-coreor GPU architectures are based on explicit parallelization of SequentialMinimal Optimization (SMO)---the programmers identified parallelizablecomponents and hand-parallelized them, specifically tuned for a particulararchitecture. We compare these approaches with each other and with implicitlyparallelized algorithms---where the algorithm is expressed such that most ofthe work is done within few iterations with large dense linear algebraoperations. These can be computed with highly-optimized libraries, that arecarefully parallelized for a large variety of parallel platforms. We highlightthe advantages and disadvantages of both approaches and compare them on variousbenchmark data sets. We find an approximate implicitly parallel algorithm whichis surprisingly efficient, permits a much simpler implementation, and leads tounprecedented speedups in SVM training.
arxiv-1404-0933 | Bayes and Naive Bayes Classifier |  http://arxiv.org/abs/1404.0933  | author:Vikramkumar, Vijaykumar B, Trilochan category:cs.LG published:2014-04-03 summary:The Bayesian Classification represents a supervised learning method as wellas a statistical method for classification. Assumes an underlying probabilisticmodel and it allows us to capture uncertainty about the model in a principledway by determining probabilities of the outcomes. This Classification is namedafter Thomas Bayes (1702-1761), who proposed the Bayes Theorem. Bayesianclassification provides practical learning algorithms and prior knowledge andobserved data can be combined. Bayesian Classification provides a usefulperspective for understanding and evaluating many learning algorithms. Itcalculates explicit probabilities for hypothesis and it is robust to noise ininput data. In statistical classification the Bayes classifier minimises theprobability of misclassification. That was a visual intuition for a simple caseof the Bayes classifier, also called: 1)Idiot Bayes 2)Naive Bayes 3)SimpleBayes
arxiv-1404-0868 | A Novel Genetic Algorithm using Helper Objectives for the 0-1 Knapsack Problem |  http://arxiv.org/abs/1404.0868  | author:Jun He, Feidun He, Hongbin Dong category:cs.NE published:2014-04-03 summary:The 0-1 knapsack problem is a well-known combinatorial optimisation problem.Approximation algorithms have been designed for solving it and they returnprovably good solutions within polynomial time. On the other hand, geneticalgorithms are well suited for solving the knapsack problem and they findreasonably good solutions quickly. A naturally arising question is whethergenetic algorithms are able to find solutions as good as approximationalgorithms do. This paper presents a novel multi-objective optimisation geneticalgorithm for solving the 0-1 knapsack problem. Experiment results show thatthe new algorithm outperforms its rivals, the greedy algorithm, mixed strategygenetic algorithm, and greedy algorithm + mixed strategy genetic algorithm.
arxiv-1404-0850 | Application of Ontologies in Identifying Requirements Patterns in Use Cases |  http://arxiv.org/abs/1404.0850  | author:Rui Couto, António Nestor Ribeiro, José Creissac Campos category:cs.SE cs.CL cs.IR published:2014-04-03 summary:Use case specifications have successfully been used for requirementsdescription. They allow joining, in the same modeling space, the expectationsof the stakeholders as well as the needs of the software engineer and analystinvolved in the process. While use cases are not meant to describe a system'simplementation, by formalizing their description we are able to extractimplementation relevant information from them. More specifically, we areinterested in identifying requirements patterns (common requirements withtypical implementation solutions) in support for a requirements based softwaredevelopment approach. In the paper we propose the transformation of Use Casedescriptions expressed in a Controlled Natural Language into an ontologyexpressed in the Web Ontology Language (OWL). OWL's query engines can then beused to identify requirements patterns expressed as queries over the ontology.We describe a tool that we have developed to support the approach and providean example of usage.
arxiv-1404-0774 | GPU Accelerated Fractal Image Compression for Medical Imaging in Parallel Computing Platform |  http://arxiv.org/abs/1404.0774  | author:Md. Enamul Haque, Abdullah Al Kaisan, Mahmudur R Saniat, Aminur Rahman category:cs.DC cs.CV published:2014-04-03 summary:In this paper, we implemented both sequential and parallel version of fractalimage compression algorithms using CUDA (Compute Unified Device Architecture)programming model for parallelizing the program in Graphics Processing Unit formedical images, as they are highly similar within the image itself. There areseveral improvement in the implementation of the algorithm as well. Fractalimage compression is based on the self similarity of an image, meaning an imagehaving similarity in majority of the regions. We take this opportunity toimplement the compression algorithm and monitor the effect of it using bothparallel and sequential implementation. Fractal compression has the property ofhigh compression rate and the dimensionless scheme. Compression scheme forfractal image is of two kind, one is encoding and another is decoding. Encodingis very much computational expensive. On the other hand decoding is lesscomputational. The application of fractal compression to medical images wouldallow obtaining much higher compression ratios. While the fractal magnificationan inseparable feature of the fractal compression would be very useful inpresenting the reconstructed image in a highly readable form. However, like allirreversible methods, the fractal compression is connected with the problem ofinformation loss, which is especially troublesome in the medical imaging. Avery time consuming encoding pro- cess, which can last even several hours, isanother bothersome drawback of the fractal compression.
arxiv-1404-0736 | Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation |  http://arxiv.org/abs/1404.0736  | author:Emily Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, Rob Fergus category:cs.CV cs.LG published:2014-04-02 summary:We present techniques for speeding up the test-time evaluation of largeconvolutional networks, designed for object recognition tasks. These modelsdeliver impressive accuracy but each image evaluation requires millions offloating point operations, making their deployment on smartphones andInternet-scale clusters problematic. The computation is dominated by theconvolution operations in the lower layers of the model. We exploit the linearstructure present within the convolutional filters to derive approximationsthat significantly reduce the required computation. Using largestate-of-the-art models, we demonstrate we demonstrate speedups ofconvolutional layers on both CPU and GPU by a factor of 2x, while keeping theaccuracy within 1% of the original model.
arxiv-1404-0541 | Don't Fall for Tuning Parameters: Tuning-Free Variable Selection in High Dimensions With the TREX |  http://arxiv.org/abs/1404.0541  | author:Johannes Lederer, Christian Müller category:stat.ME stat.ML published:2014-04-02 summary:Lasso is a seminal contribution to high-dimensional statistics, but it hingeson a tuning parameter that is difficult to calibrate in practice. A partialremedy for this problem is Square-Root Lasso, because it inherently calibratesto the noise variance. However, Square-Root Lasso still requires thecalibration of a tuning parameter to all other aspects of the model. In thisstudy, we introduce TREX, an alternative to Lasso with an inherent calibrationto all aspects of the model. This adaptation to the entire model renders TREXan estimator that does not require any calibration of tuning parameters. Weshow that TREX can outperform cross-validated Lasso in terms of variableselection and computational efficiency. We also introduce a bootstrappedversion of TREX that can further improve variable selection. We illustrate thepromising performance of TREX both on synthetic data and on a recenthigh-dimensional biological data set that considers riboflavin production in B.subtilis.
arxiv-1404-0600 | MBIS: Multivariate Bayesian Image Segmentation Tool |  http://arxiv.org/abs/1404.0600  | author:Oscar Esteban, Gert Wollny, Subrahmanyam Gorthi, Maria-J. Ledesma-Carbayo, Jean-Philippe Thiran, Andres Santos, Meritxell Bach-Cuadra category:cs.CV 62P10, 62F15 published:2014-04-02 summary:We present MBIS (Multivariate Bayesian Image Segmentation tool), a clusteringtool based on the mixture of multivariate normal distributions model. MBISsupports multi-channel bias field correction based on a B-spline model. Asecond methodological novelty is the inclusion of graph-cuts optimization forthe stationary anisotropic hidden Markov random field model. Along with MBIS,we release an evaluation framework that contains three different experiments onmulti-site data. We first validate the accuracy of segmentation and theestimated bias field for each channel. MBIS outperforms a widely usedsegmentation tool in a cross-comparison evaluation. The second experimentdemonstrates the robustness of results on atlas-free segmentation of two imagesets from scan-rescan protocols on 21 healthy subjects. Multivariatesegmentation is more replicable than the monospectral counterpart onT1-weighted images. Finally, we provide a third experiment to illustrate howMBIS can be used in a large-scale study of tissue volume change with increasingage in 584 healthy subjects. This last result is meaningful as multivariatesegmentation performs robustly without the need for prior knowledge
arxiv-1404-0708 | Computational Optimization, Modelling and Simulation: Recent Trends and Challenges |  http://arxiv.org/abs/1404.0708  | author:Xin-She Yang, Slawomir Koziel, Leifur Leifsson category:cs.NE math.OC 90C26 published:2014-04-02 summary:Modelling, simulation and optimization form an integrated part of moderndesign practice in engineering and industry. Tremendous progress has beenobserved for all three components over the last few decades. However, manychallenging issues remain unresolved, and the current trends tend to usenature-inspired algorithms and surrogate-based techniques for modelling andoptimization. This 4th workshop on Computational Optimization, Modelling andSimulation (COMS 2013) at ICCS 2013 will further summarize the latestdevelopments of optimization and modelling and their applications in science,engineering and industry. In this review paper, we will analyse the recenttrends in modelling and optimization, and their associated challenges. We willdiscuss important topics for further research, including parameter-tuning,large-scale problems, and the gaps between theory and applications.
arxiv-1404-0695 | Multi-objective Flower Algorithm for Optimization |  http://arxiv.org/abs/1404.0695  | author:Xin-She Yang, M. Karamanoglu, Xingshi He category:cs.NE math.OC 90C26 published:2014-04-02 summary:Flower pollination algorithm is a new nature-inspired algorithm, based on thecharacteristics of flowering plants. In this paper, we extend this floweralgorithm to solve multi-objective optimization problems in engineering. Byusing the weighted sum method with random weights, we show that the proposedmulti-objective flower algorithm can accurately find the Pareto fronts for aset of test functions. We then solve a bi-objective disc brake design problem,which indeed converges quickly.
arxiv-1404-0627 | Extraction of Projection Profile, Run-Histogram and Entropy Features Straight from Run-Length Compressed Text-Documents |  http://arxiv.org/abs/1404.0627  | author:Mohammed Javed, P. Nagabhushan, B. B. Chaudhuri category:cs.CV published:2014-04-02 summary:Document Image Analysis, like any Digital Image Analysis requiresidentification and extraction of proper features, which are generally extractedfrom uncompressed images, though in reality images are made available incompressed form for the reasons such as transmission and storage efficiency.However, this implies that the compressed image should be decompressed, whichindents additional computing resources. This limitation induces the motivationto research in extracting features directly from the compressed image. In thisresearch, we propose to extract essential features such as projection profile,run-histogram and entropy for text document analysis directly from run-lengthcompressed text-documents. The experimentation illustrates that features areextracted directly from the compressed image without going through the stage ofdecompression, because of which the computing time is reduced. The featurevalues so extracted are exactly identical to those extracted from uncompressedimages.
arxiv-1404-0554 | From ADP to the Brain: Foundations, Roadmap, Challenges and Research Priorities |  http://arxiv.org/abs/1404.0554  | author:Paul J Werbos category:cs.NE published:2014-04-02 summary:This paper defines and discusses Mouse Level Computational Intelligence(MLCI) as a grand challenge for the coming century. It provides a specificroadmap to reach that target, citing relevant work and review papers anddiscussing the relation to funding priorities in two NSF funding activities:the ongoing Energy, Power and Adaptive Systems program (EPAS) and the recentinitiative in Cognitive Optimization and Prediction (COPN). It elaborates onthe first step, vector intelligence, a challenge in the development ofuniversal learning systems, which itself will require considerable new researchto attain. This in turn is a crucial prerequisite to true functionalunderstanding of how mammal brains achieve such general learning capabilities.
arxiv-1404-0533 | A Comparative Study of Modern Inference Techniques for Structured Discrete Energy Minimization Problems |  http://arxiv.org/abs/1404.0533  | author:Jörg H. Kappes, Bjoern Andres, Fred A. Hamprecht, Christoph Schnörr, Sebastian Nowozin, Dhruv Batra, Sungwoong Kim, Bernhard X. Kausler, Thorben Kröger, Jan Lellmann, Nikos Komodakis, Bogdan Savchynskyy, Carsten Rother category:cs.CV published:2014-04-02 summary:Szeliski et al. published an influential study in 2006 on energy minimizationmethods for Markov Random Fields (MRF). This study provided valuable insightsin choosing the best optimization technique for certain classes of problems.While these insights remain generally useful today, the phenomenal success ofrandom field models means that the kinds of inference problems that have to besolved changed significantly. Specifically, the models today often includehigher order interactions, flexible connectivity structures, largela\-bel-spaces of different cardinalities, or learned energy tables. To reflectthese changes, we provide a modernized and enlarged study. We present anempirical comparison of 32 state-of-the-art optimization techniques on a corpusof 2,453 energy minimization instances from diverse applications in computervision. To ensure reproducibility, we evaluate all methods in the OpenGM 2framework and report extensive results regarding runtime and solution quality.Key insights from our study agree with the results of Szeliski et al. for thetypes of models they studied. However, on new and challenging types of modelsour findings disagree and suggest that polyhedral methods and integerprogramming solvers are competitive in terms of runtime and solution qualityover a large range of model types.
arxiv-1404-2903 | Thoughts on a Recursive Classifier Graph: a Multiclass Network for Deep Object Recognition |  http://arxiv.org/abs/1404.2903  | author:Marius Leordeanu, Rahul Sukthankar category:cs.CV cs.LG cs.NE published:2014-04-02 summary:We propose a general multi-class visual recognition model, termed theClassifier Graph, which aims to generalize and integrate ideas from many oftoday's successful hierarchical recognition approaches. Our graph-based modelhas the advantage of enabling rich interactions between classes from differentlevels of interpretation and abstraction. The proposed multi-class system isefficiently learned using step by step updates. The structure consists ofsimple logistic linear layers with inputs from features that are automaticallyselected from a large pool. Each newly learned classifier becomes a potentialnew feature. Thus, our feature pool can consist both of initial manuallydesigned features as well as learned classifiers from previous steps (graphnodes), each copied many times at different scales and locations. In thismanner we can learn and grow both a deep, complex graph of classifiers and arich pool of features at different levels of abstraction and interpretation.Our proposed graph of classifiers becomes a multi-class system with a recursivestructure, suitable for deep detection and recognition of several classessimultaneously.
arxiv-1404-0453 | Cellular Automata and Its Applications in Bioinformatics: A Review |  http://arxiv.org/abs/1404.0453  | author:Pokkuluri Kiran Sree, Inampudi Ramesh Babu, SSSN Usha Devi N category:cs.CE cs.LG published:2014-04-02 summary:This paper aims at providing a survey on the problems that can be easilyaddressed by cellular automata in bioinformatics. Some of the authors haveproposed algorithms for addressing some problems in bioinformatics but theapplication of cellular automata in bioinformatics is a virgin field inresearch. None of the researchers has tried to relate the major problems inbioinformatics and find a common solution. Extensive literature surveys wereconducted. We have considered some papers in various journals and conferencesfor conduct of our research. This paper provides intuition towards relatingvarious problems in bioinformatics logically and tries to attain a common framework for addressing the same.
arxiv-1404-0437 | Theory and Application of Shapelets to the Analysis of Surface Self-assembly Imaging |  http://arxiv.org/abs/1404.0437  | author:Robert Suderman, Daniel Lizotte, Nasser Mohieddin Abukhdeir category:cs.CV published:2014-04-02 summary:A method for quantitative analysis of local pattern strength and defects insurface self-assembly imaging is presented and applied to images of stripe andhexagonal ordered domains. The presented method uses "shapelet" functions whichwere originally developed for quantitative analysis of images of galaxies($\propto 10^{20}\mathrm{m}$). In this work, they are used instead to quantifythe presence of translational order in surface self-assembled films ($\propto10^{-9}\mathrm{m}$) through reformulation into "steerable" filters. Theresulting method is both computationally efficient (with respect to the numberof filter evaluations), robust to variation in pattern feature shape, and,unlike previous approaches, is applicable to a wide variety of pattern types.An application of the method is presented which uses a nearest-neighbouranalysis to distinguish between uniform (defect-free) and non-uniform(strained, defect-containing) regions within imaged self-assembled domains,both with striped and hexagonal patterns.
arxiv-1404-0427 | Learning Two-input Linear and Nonlinear Analog Functions with a Simple Chemical System |  http://arxiv.org/abs/1404.0427  | author:Peter Banda, Christof Teuscher category:q-bio.MN cs.LG published:2014-04-02 summary:The current biochemical information processing systems behave in apredetermined manner because all features are defined during the design phase.To make such unconventional computing systems reusable and programmable forbiomedical applications, adaptation, learning, and self-modification based onexternal stimuli would be highly desirable. However, so far, it has been toochallenging to implement these in wet chemistries. In this paper we extend thechemical perceptron, a model previously proposed by the authors, to function asan analog instead of a binary system. The new analog asymmetric signalperceptron learns through feedback and supports Michaelis-Menten kinetics. Theresults show that our perceptron is able to learn linear and nonlinear(quadratic) functions of two inputs. To the best of our knowledge, it is thefirst simulated chemical system capable of doing so. The small number ofspecies and reactions and their simplicity allows for a mapping to an actualwet implementation using DNA-strand displacement or deoxyribozymes. Our resultsare an important step toward actual biochemical systems that can learn andadapt.
arxiv-1404-0431 | Learning Latent Block Structure in Weighted Networks |  http://arxiv.org/abs/1404.0431  | author:Christopher Aicher, Abigail Z. Jacobs, Aaron Clauset category:stat.ML cs.SI physics.soc-ph published:2014-04-02 summary:Community detection is an important task in network analysis, in which we aimto learn a network partition that groups together vertices with similarcommunity-level connectivity patterns. By finding such groups of vertices withsimilar structural roles, we extract a compact representation of the network'slarge-scale structure, which can facilitate its scientific interpretation andthe prediction of unknown or future interactions. Popular approaches, includingthe stochastic block model, assume edges are unweighted, which limits theirutility by throwing away potentially useful information. We introduce the`weighted stochastic block model' (WSBM), which generalizes the stochasticblock model to networks with edge weights drawn from any exponential familydistribution. This model learns from both the presence and weight of edges,allowing it to discover structure that would otherwise be hidden when weightsare discarded or thresholded. We describe a Bayesian variational algorithm forefficiently approximating this model's posterior distribution over latent blockstructures. We then evaluate the WSBM's performance on both edge-existence andedge-weight prediction tasks for a set of real-world weighted networks. In allcases, the WSBM performs as well or better than the best alternatives on thesetasks.
arxiv-1404-0466 | piCholesky: Polynomial Interpolation of Multiple Cholesky Factors for Efficient Approximate Cross-Validation |  http://arxiv.org/abs/1404.0466  | author:Da Kuang, Alex Gittens, Raffay Hamid category:cs.LG cs.NA published:2014-04-02 summary:The dominant cost in solving least-square problems using Newton's method isoften that of factorizing the Hessian matrix over multiple values of theregularization parameter ($\lambda$). We propose an efficient way tointerpolate the Cholesky factors of the Hessian matrix computed over a smallset of $\lambda$ values. This approximation enables us to optimally minimizethe hold-out error while incurring only a fraction of the cost compared toexact cross-validation. We provide a formal error bound for our approximationscheme and present solutions to a set of key implementation challenges thatallow our approach to maximally exploit the compute power of modernarchitectures. We present a thorough empirical analysis over multiple datasetsto show the effectiveness of our approach.
arxiv-1404-0298 | A Kernel-Based Nonparametric Test for Anomaly Detection over Line Networks |  http://arxiv.org/abs/1404.0298  | author:Shaofeng Zou, Yingbin Liang, H. Vincent Poor category:cs.IT math.IT stat.ML published:2014-04-01 summary:The nonparametric problem of detecting existence of an anomalous intervalover a one dimensional line network is studied. Nodes corresponding to ananomalous interval (if exists) receive samples generated by a distribution q,which is different from the distribution p that generates samples for othernodes. If anomalous interval does not exist, then all nodes receive samplesgenerated by p. It is assumed that the distributions p and q are arbitrary, andare unknown. In order to detect whether an anomalous interval exists, a test isbuilt based on mean embeddings of distributions into a reproducing kernelHilbert space (RKHS) and the metric of maximummean discrepancy (MMD). It isshown that as the network size n goes to infinity, if the minimum length ofcandidate anomalous intervals is larger than a threshold which has the orderO(log n), the proposed test is asymptotically successful, i.e., the probabilityof detection error approaches zero asymptotically. An efficient algorithm toperform the test with substantial computational complexity reduction isproposed, and is shown to be asymptotically successful if the condition on theminimum length of candidate anomalous interval is satisfied. Numerical resultsare provided, which are consistent with the theoretical results.
arxiv-1404-0106 | Traffic Monitoring Using M2M Communication |  http://arxiv.org/abs/1404.0106  | author:Shiu Kumar, Eun Sik Ham, Seong Ro Lee category:cs.CV published:2014-04-01 summary:This paper presents an intelligent traffic monitoring system using wirelessvision sensor network that captures and processes the real-time video image toobtain the traffic flow rate and vehicle speeds along different urban roadways.This system will display the traffic states on the front roadways that canguide the drivers to select the right way and avoid potential trafficcongestions. On the other hand, it will also monitor the vehicle speeds andstore the vehicle details, for those breaking the roadway speed limits, in itsdatabase. The real-time traffic data is processed by the Personal Computer (PC)at the sub roadway station and the traffic flow rate data is transmitted to themain roadway station Arduino 3G via email, where the data is extracted andtraffic flow rate displayed.
arxiv-1404-0099 | Venture: a higher-order probabilistic programming platform with programmable inference |  http://arxiv.org/abs/1404.0099  | author:Vikash Mansinghka, Daniel Selsam, Yura Perov category:cs.AI cs.PL stat.CO stat.ML published:2014-04-01 summary:We describe Venture, an interactive virtual machine for probabilisticprogramming that aims to be sufficiently expressive, extensible, and efficientfor general-purpose use. Like Church, probabilistic models and inferenceproblems in Venture are specified via a Turing-complete, higher-orderprobabilistic language descended from Lisp. Unlike Church, Venture alsoprovides a compositional language for custom inference strategies built out ofscalable exact and approximate techniques. We also describe four key aspects ofVenture's implementation that build on ideas from probabilistic graphicalmodels. First, we describe the stochastic procedure interface (SPI) thatspecifies and encapsulates primitive random variables. The SPI supports customcontrol flow, higher-order probabilistic procedures, partially exchangeablesequences and ``likelihood-free'' stochastic simulators. It also supportsexternal models that do inference over latent variables hidden from Venture.Second, we describe probabilistic execution traces (PETs), which representexecution histories of Venture programs. PETs capture conditional dependencies,existential dependencies and exchangeable coupling. Third, we describepartitions of execution histories called scaffolds that factor global inferenceproblems into coherent sub-problems. Finally, we describe a family ofstochastic regeneration algorithms for efficiently modifying PET fragmentscontained within scaffolds. Stochastic regeneration linear runtime scaling incases where many previous approaches scaled quadratically. We show how to usestochastic regeneration and the SPI to implement general-purpose inferencestrategies such as Metropolis-Hastings, Gibbs sampling, and blocked proposalsbased on particle Markov chain Monte Carlo and mean-field variational inferencetechniques.
arxiv-1404-0086 | Using HMM in Strategic Games |  http://arxiv.org/abs/1404.0086  | author:Mario Benevides, Isaque Lima, Rafael Nader, Pedro Rougemont category:cs.GT cs.IR cs.LG I.2.8 published:2014-04-01 summary:In this paper we describe an approach to resolve strategic games in whichplayers can assume different types along the game. Our goal is to infer whichtype the opponent is adopting at each moment so that we can increase theplayer's odds. To achieve that we use Markov games combined with hidden Markovmodel. We discuss a hypothetical example of a tennis game whose solution can beapplied to any game with similar characteristics.
arxiv-1404-0200 | Household Electricity Demand Forecasting -- Benchmarking State-of-the-Art Methods |  http://arxiv.org/abs/1404.0200  | author:Andreas Veit, Christoph Goebel, Rohit Tidke, Christoph Doblander, Hans-Arno Jacobsen category:cs.LG stat.AP I.2.6 published:2014-04-01 summary:The increasing use of renewable energy sources with variable output, such assolar photovoltaic and wind power generation, calls for Smart Grids thateffectively manage flexible loads and energy storage. The ability to forecastconsumption at different locations in distribution systems will be a keycapability of Smart Grids. The goal of this paper is to benchmarkstate-of-the-art methods for forecasting electricity demand on the householdlevel across different granularities and time scales in an explorative way,thereby revealing potential shortcomings and find promising directions forfuture research in this area. We apply a number of forecasting methodsincluding ARIMA, neural networks, and exponential smoothening using severalstrategies for training data selection, in particular day type and slidingwindow based strategies. We consider forecasting horizons ranging between 15minutes and 24 hours. Our evaluation is based on two data sets containing thepower usage of individual appliances at second time granularity collected overthe course of several months. The results indicate that forecasting accuracyvaries significantly depending on the choice of forecasting methods/strategyand the parameter configuration. Measured by the Mean Absolute Percentage Error(MAPE), the considered state-of-the-art forecasting methods rarely beatcorresponding persistence forecasts. Overall, we observed MAPEs in the rangebetween 5 and >100%. The average MAPE for the first data set was ~30%, while itwas ~85% for the other data set. These results show big room for improvement.Based on the identified trends and experiences from our experiments, wecontribute a detailed discussion of promising future research.
arxiv-1404-0400 | A Deep Representation for Invariance And Music Classification |  http://arxiv.org/abs/1404.0400  | author:Chiyuan Zhang, Georgios Evangelopoulos, Stephen Voinea, Lorenzo Rosasco, Tomaso Poggio category:cs.SD cs.LG stat.ML published:2014-04-01 summary:Representations in the auditory cortex might be based on mechanisms similarto the visual ventral stream; modules for building invariance totransformations and multiple layers for compositionality and selectivity. Inthis paper we propose the use of such computational modules for extractinginvariant and discriminative audio representations. Building on a theory ofinvariance in hierarchical architectures, we propose a novel, mid-levelrepresentation for acoustical signals, using the empirical distributions ofprojections on a set of templates and their transformations. Under theassumption that, by construction, this dictionary of templates is composed fromsimilar classes, and samples the orbit of variance-inducing signaltransformations (such as shift and scale), the resulting signature istheoretically guaranteed to be unique, invariant to transformations and stableto deformations. Modules of projection and pooling can then constitute layersof deep networks, for learning composite representations. We present the maintheoretical and computational aspects of a framework for unsupervised learningof invariant audio representations, empirically evaluated on music genreclassification.
arxiv-1404-0334 | Active Deformable Part Models |  http://arxiv.org/abs/1404.0334  | author:Menglong Zhu, Nikolay Atanasov, George J. Pappas, Kostas Daniilidis category:cs.CV cs.LG published:2014-04-01 summary:This paper presents an active approach for part-based object detection, whichoptimizes the order of part filter evaluations and the time at which to stopand make a prediction. Statistics, describing the part responses, are learnedfrom training data and are used to formalize the part scheduling problem as anoffline optimization. Dynamic programming is applied to obtain a policy, whichbalances the number of part evaluations with the classification accuracy.During inference, the policy is used as a look-up table to choose the partorder and the stopping time based on the observed filter responses. The methodis faster than cascade detection with deformable part models (which does notoptimize the part order) with negligible loss in accuracy when evaluated on thePASCAL VOC 2007 and 2010 datasets.
arxiv-1404-0138 | Efficient Algorithms and Error Analysis for the Modified Nystrom Method |  http://arxiv.org/abs/1404.0138  | author:Shusen Wang, Zhihua Zhang category:cs.LG published:2014-04-01 summary:Many kernel methods suffer from high time and space complexities and are thusprohibitive in big-data applications. To tackle the computational challenge,the Nystr\"om method has been extensively used to reduce time and spacecomplexities by sacrificing some accuracy. The Nystr\"om method speedupscomputation by constructing an approximation of the kernel matrix using only afew columns of the matrix. Recently, a variant of the Nystr\"om method calledthe modified Nystr\"om method has demonstrated significant improvement over thestandard Nystr\"om method in approximation accuracy, both theoretically andempirically. In this paper, we propose two algorithms that make the modified Nystr\"ommethod practical. First, we devise a simple column selection algorithm with aprovable error bound. Our algorithm is more efficient and easier to implementthan and nearly as accurate as the state-of-the-art algorithm. Second, with theselected columns at hand, we propose an algorithm that computes theapproximation in lower time complexity than the approach in the previous work.Furthermore, we prove that the modified Nystr\"om method is exact under certainconditions, and we establish a lower error bound for the modified Nystr\"ommethod.
arxiv-1404-0336 | A Continuous Max-Flow Approach to General Hierarchical Multi-Labeling Problems |  http://arxiv.org/abs/1404.0336  | author:John S. H. Baxter, Martin Rajchl, Jing Yuan, Terry M. Peters category:cs.CV published:2014-04-01 summary:Multi-region segmentation algorithms often have the onus of incorporatingcomplex anatomical knowledge representing spatial or geometric relationshipsbetween objects, and general-purpose methods of addressing this knowledge in anoptimization-based manner have thus been lacking. This paper presentsGeneralized Hierarchical Max-Flow (GHMF) segmentation, which captures simpleanatomical part-whole relationships in the form of an unconstrained hierarchy.Regularization can then be applied to both parts and wholes independently,allowing for spatial grouping and clustering of labels in a globally optimalconvex optimization framework. For the purposes of ready integration into avariety of segmentation tasks, the hierarchies can be presented in run-time,allowing for the segmentation problem to be readily specified and alternativesexplored without undue programming effort or recompilation.
arxiv-1404-0329 | Toward computational cumulative biology by combining models of biological datasets |  http://arxiv.org/abs/1404.0329  | author:Ali Faisal, Jaakko Peltonen, Elisabeth Georgii, Johan Rung, Samuel Kaski category:q-bio.QM q-bio.GN stat.ML published:2014-04-01 summary:A main challenge of data-driven sciences is how to make maximal use of theprogressively expanding databases of experimental datasets in order to keepresearch cumulative. We introduce the idea of a modeling-based datasetretrieval engine designed for relating a researcher's experimental dataset toearlier work in the field. The search is (i) data-driven to enable newfindings, going beyond the state of the art of keyword searches in annotations,(ii) modeling-driven, to both include biological knowledge and insights learnedfrom data, and (iii) scalable, as it is accomplished without building oneunified grand model of all data. Assuming each dataset has been modeledbeforehand, by the researchers or by database managers, we apply a rapidlycomputable and optimizable combination model to decompose a new dataset intocontributions from earlier relevant models. By using the data-drivendecomposition we identify a network of interrelated datasets from a largeannotated human gene expression atlas. While tissue type and disease were majordriving forces for determining relevant datasets, the found relationships werericher and the model-based search was more accurate than keyword search; itmoreover recovered biologically meaningful relationships that are notstraightforwardly visible from annotations, for instance, between cells indifferent developmental stages such as thymocytes and T-cells. Data-drivenlinks and citations matched to a large extent; the data-driven links evenuncovered corrections to the publication data, as two of the most linkeddatasets were not highly cited and turned out to have wrong publication entriesin the database.
arxiv-1403-8084 | Privacy Tradeoffs in Predictive Analytics |  http://arxiv.org/abs/1403.8084  | author:Stratis Ioannidis, Andrea Montanari, Udi Weinsberg, Smriti Bhagat, Nadia Fawaz, Nina Taft category:cs.CR cs.LG published:2014-03-31 summary:Online services routinely mine user data to predict user preferences, makerecommendations, and place targeted ads. Recent research has demonstrated thatseveral private user attributes (such as political affiliation, sexualorientation, and gender) can be inferred from such data. Can aprivacy-conscious user benefit from personalization while simultaneouslyprotecting her private attributes? We study this question in the context of arating prediction service based on matrix factorization. We construct aprotocol of interactions between the service and users that has remarkableoptimality properties: it is privacy-preserving, in that no inference algorithmcan succeed in inferring a user's private attribute with a probability betterthan random guessing; it has maximal accuracy, in that no otherprivacy-preserving protocol improves rating prediction; and, finally, itinvolves a minimal disclosure, as the prediction accuracy strictly decreaseswhen the service reveals less information. We extensively evaluate our protocolusing several rating datasets, demonstrating that it successfully blocks theinference of gender, age and political affiliation, while incurring less than5% decrease in the accuracy of rating prediction.
arxiv-1403-8098 | Hyperspectral image superresolution: An edge-preserving convex formulation |  http://arxiv.org/abs/1403.8098  | author:Miguel Simões, José Bioucas-Dias, Luis B. Almeida, Jocelyn Chanussot category:cs.CV stat.ML published:2014-03-31 summary:Hyperspectral remote sensing images (HSIs) are characterized by having a lowspatial resolution and a high spectral resolution, whereas multispectral images(MSIs) are characterized by low spectral and high spatial resolutions. Thesecomplementary characteristics have stimulated active research in the inferenceof images with high spatial and spectral resolutions from HSI-MSI pairs. In this paper, we formulate this data fusion problem as the minimization of aconvex objective function containing two data-fitting terms and anedge-preserving regularizer. The data-fitting terms are quadratic and accountfor blur, different spatial resolutions, and additive noise; the regularizer, aform of vector Total Variation, promotes aligned discontinuities across thereconstructed hyperspectral bands. The optimization described above is rather hard, owing to itsnon-diagonalizable linear operators, to the non-quadratic and non-smooth natureof the regularizer, and to the very large size of the image to be inferred. Wetackle these difficulties by tailoring the Split Augmented Lagrangian ShrinkageAlgorithm (SALSA)---an instance of the Alternating Direction Method ofMultipliers (ADMM)---to this optimization problem. By using a convenientvariable splitting and by exploiting the fact that HSIs generally "live" in alow-dimensional subspace, we obtain an effective algorithm that yieldsstate-of-the-art results, as illustrated by experiments.
arxiv-1403-8144 | Coding for Random Projections and Approximate Near Neighbor Search |  http://arxiv.org/abs/1403.8144  | author:Ping Li, Michael Mitzenmacher, Anshumali Shrivastava category:cs.LG cs.DB cs.DS stat.CO published:2014-03-31 summary:This technical note compares two coding (quantization) schemes for randomprojections in the context of sub-linear time approximate near neighbor search.The first scheme is based on uniform quantization while the second schemeutilizes a uniform quantization plus a uniformly random offset (which has beenpopular in practice). The prior work compared the two schemes in the context ofsimilarity estimation and training linear classifiers, with the conclusion thatthe step of random offset is not necessary and may hurt the performance(depending on the similarity level). The task of near neighbor search isrelated to similarity estimation with importance distinctions and requires ownstudy. In this paper, we demonstrate that in the context of near neighborsearch, the step of random offset is not needed either and may hurt theperformance (sometimes significantly so, depending on the similarity and otherparameters).
arxiv-1403-8003 | Probabilistic Intra-Retinal Layer Segmentation in 3-D OCT Images Using Global Shape Regularization |  http://arxiv.org/abs/1403.8003  | author:Fabian Rathke, Stefan Schmidt, Christoph Schnörr category:cs.CV published:2014-03-31 summary:With the introduction of spectral-domain optical coherence tomography (OCT),resulting in a significant increase in acquisition speed, the fast and accuratesegmentation of 3-D OCT scans has become evermore important. This paperpresents a novel probabilistic approach, that models the appearance of retinallayers as well as the global shape variations of layer boundaries. Given an OCTscan, the full posterior distribution over segmentations is approximatelyinferred using a variational method enabling efficient probabilistic inferencein terms of computationally tractable model components: Segmenting a full 3-Dvolume takes around a minute. Accurate segmentations demonstrate the benefit ofusing global shape regularization: We segmented 35 fovea-centered 3-D volumeswith an average unsigned error of 2.46 $\pm$ 0.22 {\mu}m as well as 80 normaland 66 glaucomatous 2-D circular scans with errors of 2.92 $\pm$ 0.53 {\mu}mand 4.09 $\pm$ 0.98 {\mu}m respectively. Furthermore, we utilized the inferredposterior distribution to rate the quality of the segmentation, point outpotentially erroneous regions and discriminate normal from pathological scans.No pre- or postprocessing was required and we used the same set of parametersfor all data sets, underlining the robustness and out-of-the-box nature of ourapproach.
arxiv-1403-8067 | Robust Subspace Recovery via Bi-Sparsity Pursuit |  http://arxiv.org/abs/1403.8067  | author:Xiao Bian, Hamid Krim category:cs.CV published:2014-03-31 summary:Successful applications of sparse models in computer vision and machinelearning imply that in many real-world applications, high dimensional data isdistributed in a union of low dimensional subspaces. Nevertheless, theunderlying structure may be affected by sparse errors and/or outliers. In thispaper, we propose a bi-sparse model as a framework to analyze this problem andprovide a novel algorithm to recover the union of subspaces in presence ofsparse corruptions. We further show the effectiveness of our method byexperiments on both synthetic data and real-world vision data.
arxiv-1403-7890 | Sparse K-Means with $\ell_{\infty}/\ell_0$ Penalty for High-Dimensional Data Clustering |  http://arxiv.org/abs/1403.7890  | author:Xiangyu Chang, Yu Wang, Rongjian Li, Zongben Xu category:stat.ML cs.LG stat.ME published:2014-03-31 summary:Sparse clustering, which aims to find a proper partition of an extremelyhigh-dimensional data set with redundant noise features, has been attractedmore and more interests in recent years. The existing studies commonly solvethe problem in a framework of maximizing the weighted feature contributionssubject to a $\ell_2/\ell_1$ penalty. Nevertheless, this framework has twoserious drawbacks: One is that the solution of the framework unavoidablyinvolves a considerable portion of redundant noise features in many situations,and the other is that the framework neither offers intuitive explanations onwhy this framework can select relevant features nor leads to any theoreticalguarantee for feature selection consistency. In this article, we attempt to overcome those drawbacks through developing anew sparse clustering framework which uses a $\ell_{\infty}/\ell_0$ penalty.First, we introduce new concepts on optimal partitions and noise features forthe high-dimensional data clustering problems, based on which the previouslyknown framework can be intuitively explained in principle. Then, we apply thesuggested $\ell_{\infty}/\ell_0$ framework to formulate a new sparse k-meansmodel with the $\ell_{\infty}/\ell_0$ penalty ($\ell_0$-k-means for short). Wepropose an efficient iterative algorithm for solving the $\ell_0$-k-means. Todeeply understand the behavior of $\ell_0$-k-means, we prove that the solutionyielded by the $\ell_0$-k-means algorithm has feature selection consistencywhenever the data matrix is generated from a high-dimensional Gaussian mixturemodel. Finally, we provide experiments with both synthetic data and the AllenDeveloping Mouse Brain Atlas data to support that the proposed $\ell_0$-k-meansexhibits better noise feature detection capacity over the previously knownsparse k-means with the $\ell_2/\ell_1$ penalty ($\ell_1$-k-means for short).
arxiv-1403-7876 | Correlation Filters with Limited Boundaries |  http://arxiv.org/abs/1403.7876  | author:Hamed Kiani Galoogahi, Terence Sim, Simon Lucey category:cs.CV published:2014-03-31 summary:Correlation filters take advantage of specific properties in the Fourierdomain allowing them to be estimated efficiently: O(NDlogD) in the frequencydomain, versus O(D^3 + ND^2) spatially where D is signal length, and N is thenumber of signals. Recent extensions to correlation filters, such as MOSSE,have reignited interest of their use in the vision community due to theirrobustness and attractive computational properties. In this paper wedemonstrate, however, that this computational efficiency comes at a cost.Specifically, we demonstrate that only 1/D proportion of shifted examples areunaffected by boundary effects which has a dramatic effect ondetection/tracking performance. In this paper, we propose a novel approach tocorrelation filter estimation that: (i) takes advantage of inherentcomputational redundancies in the frequency domain, and (ii) dramaticallyreduces boundary effects. Impressive object tracking and detection results arepresented in terms of both accuracy and computational efficiency.
arxiv-1403-7877 | ROML: A Robust Feature Correspondence Approach for Matching Objects in A Set of Images |  http://arxiv.org/abs/1403.7877  | author:Kui Jia, Tsung-Han Chan, Zinan Zeng, Shenghua Gao, Gang Wang, Tianzhu Zhang, Yi Ma category:cs.CV published:2014-03-31 summary:Feature-based object matching is a fundamental problem for many applicationsin computer vision, such as object recognition, 3D reconstruction, tracking,and motion segmentation. In this work, we consider simultaneously matchingobject instances in a set of images, where both inlier and outlier features areextracted. The task is to identify the inlier features and establish theirconsistent correspondences across the image set. This is a challengingcombinatorial problem, and the problem complexity grows exponentially with theimage number. To this end, we propose a novel framework, termed ROML, toaddress this problem. ROML optimizes simultaneously a partial permutationmatrix (PPM) for each image, and feature correspondences are established by theobtained PPMs. Two of our key contributions are summarized as follows. (1) Weformulate the problem as rank and sparsity minimization for PPM optimization,and treat simultaneous optimization of multiple PPMs as a regularized consensusproblem in the context of distributed optimization. (2) We use the ADMM methodto solve the thus formulated ROML problem, in which a subproblem associatedwith a single PPM optimization appears to be a difficult integer quadraticprogram (IQP). We prove that under wildly applicable conditions, this IQP isequivalent to a linear sum assignment problem (LSAP), which can be efficientlysolved to an exact solution. Extensive experiments on rigid/non-rigid objectmatching, matching instances of a common object category, and common objectlocalization show the efficacy of our proposed method.
arxiv-1403-7752 | Auto-encoders: reconstruction versus compression |  http://arxiv.org/abs/1403.7752  | author:Yann Ollivier category:cs.NE cs.IT cs.LG math.IT published:2014-03-30 summary:We discuss the similarities and differences between training an auto-encoderto minimize the reconstruction error, and training the same auto-encoder tocompress the data via a generative model. Minimizing a codelength for the datausing an auto-encoder is equivalent to minimizing the reconstruction error plussome correcting terms which have an interpretation as either a denoising orcontractive property of the decoding function. These terms are related but notidentical to those used in denoising or contractive auto-encoders [Vincent etal. 2010, Rifai et al. 2011]. In particular, the codelength viewpoint fullydetermines an optimal noise level for the denoising criterion.
arxiv-1403-7806 | Unbiased Black-Box Complexities of Jump Functions |  http://arxiv.org/abs/1403.7806  | author:Benjamin Doerr, Carola Doerr, Timo Kötzing category:cs.NE F.2.2 published:2014-03-30 summary:We analyze the unbiased black-box complexity of jump functions with small,medium, and large sizes of the fitness plateau surrounding the optimalsolution. Among other results, we show that when the jump size is $(1/2 -\varepsilon)n$, that is, only a small constant fraction of the fitness valuesis visible, then the unbiased black-box complexities for arities $3$ and higherare of the same order as those for the simple \textsc{OneMax} function. Evenfor the extreme jump function, in which all but the two fitness values $n/2$and $n$ are blanked out, polynomial-time mutation-based (i.e., unary unbiased)black-box optimization algorithms exist. This is quite surprising given thatfor the extreme jump function almost the whole search space (all but a$\Theta(n^{-1/2})$ fraction) is a plateau of constant fitness. To prove these results, we introduce new tools for the analysis of unbiasedblack-box complexities, for example, selecting the new parent individual not bycomparing the fitnesses of the competing search points, but also by taking intoaccount the (empirical) expected fitnesses of their offspring.
arxiv-1403-7737 | Sharpened Error Bounds for Random Sampling Based $\ell_2$ Regression |  http://arxiv.org/abs/1403.7737  | author:Shusen Wang category:cs.LG cs.NA stat.ML published:2014-03-30 summary:Given a data matrix $X \in R^{n\times d}$ and a response vector $y \inR^{n}$, suppose $n>d$, it costs $O(n d^2)$ time and $O(n d)$ space to solve theleast squares regression (LSR) problem. When $n$ and $d$ are both large,exactly solving the LSR problem is very expensive. When $n \gg d$, one feasibleapproach to speeding up LSR is to randomly embed $y$ and all columns of $X$into a smaller subspace $R^c$; the induced LSR problem has the same number ofcolumns but much fewer number of rows, and it can be solved in $O(c d^2)$ timeand $O(c d)$ space. We discuss in this paper two random sampling based methods for solving LSRmore efficiently. Previous work showed that the leverage scores based samplingbased LSR achieves $1+\epsilon$ accuracy when $c \geq O(d \epsilon^{-2} \logd)$. In this paper we sharpen this error bound, showing that $c = O(d \log d +d \epsilon^{-1})$ is enough for achieving $1+\epsilon$ accuracy. We also showthat when $c \geq O(\mu d \epsilon^{-2} \log d)$, the uniform sampling basedLSR attains a $2+\epsilon$ bound with positive probability.
arxiv-1404-0649 | A probabilistic estimation and prediction technique for dynamic continuous social science models: The evolution of the attitude of the Basque Country population towards ETA as a case study |  http://arxiv.org/abs/1404.0649  | author:Juan-Carlos Cortés, Francisco-J. Santonja, Ana-C. Tarazona, Rafael-J. Villanueva, Javier Villanueva-Oller category:cs.LG published:2014-03-30 summary:In this paper, we present a computational technique to deal with uncertaintyin dynamic continuous models in Social Sciences. Considering data from surveys,the method consists of determining the probability distribution of the surveyoutput and this allows to sample data and fit the model to the sampled datausing a goodness-of-fit criterion based on the chi-square-test. Taking thefitted parameters non-rejected by the chi-square-test, substituting them intothe model and computing their outputs, we build 95% confidence intervals ineach time instant capturing uncertainty of the survey data (probabilisticestimation). Using the same set of obtained model parameters, we also provide aprediction over the next few years with 95% confidence intervals (probabilisticprediction). This technique is applied to a dynamic social model describing theevolution of the attitude of the Basque Country population towards therevolutionary organization ETA.
arxiv-1403-7795 | Bio-Inspired Computation: Success and Challenges of IJBIC |  http://arxiv.org/abs/1403.7795  | author:Xin-She Yang, Zhihua Cui category:math.OC cs.NE 78M32 published:2014-03-30 summary:It is now five years since the launch of the International Journal ofBio-Inspired Computation (IJBIC). At the same time, significant new progresshas been made in the area of bio-inspired computation. This review papersummarizes the success and achievements of IJBIC in the past five years, andalso highlights the challenges and key issues for further research.
arxiv-1403-7793 | True Global Optimality of the Pressure Vessel Design Problem: A Benchmark for Bio-Inspired Optimisation Algorithms |  http://arxiv.org/abs/1403.7793  | author:Xin-She Yang, Christian Huyck, Mehmet Karamanoglu, Nawaz Khan category:math.OC cs.NE nlin.AO published:2014-03-30 summary:The pressure vessel design problem is a well-known design benchmark forvalidating bio-inspired optimization algorithms. However, its global optimalityis not clear and there has been no mathematical proof put forward. In thispaper, a detailed mathematical analysis of this problem is provided that provesthat 6059.714335048436 is the global minimum. The Lagrange multiplier method isalso used as an alternative proof and this method is extended to find theglobal optimum of a cantilever beam design problem.
arxiv-1403-7792 | Swarm Intelligence Based Algorithms: A Critical Analysis |  http://arxiv.org/abs/1403.7792  | author:Xin-She Yang category:math.OC cs.NE nlin.AO 78M32 published:2014-03-30 summary:Many optimization algorithms have been developed by drawing inspiration fromswarm intelligence (SI). These SI-based algorithms can have some advantagesover traditional algorithms. In this paper, we carry out a critical analysis ofthese SI-based algorithms by analyzing their ways to mimic evolutionaryoperators. We also analyze the ways of achieving exploration and exploitationin algorithms by using mutation, crossover and selection. In addition, we alsolook at algorithms using dynamic systems, self-organization and Markov chainframework. Finally, we provide some discussions and topics for furtherresearch.
arxiv-1403-7783 | Extraction of Line Word Character Segments Directly from Run Length Compressed Printed Text Documents |  http://arxiv.org/abs/1403.7783  | author:Mohammed Javed, P. Nagabhushan, B. B. Chaudhuri category:cs.CV published:2014-03-30 summary:Segmentation of a text-document into lines, words and characters, which isconsidered to be the crucial pre-processing stage in Optical CharacterRecognition (OCR) is traditionally carried out on uncompressed documents,although most of the documents in real life are available in compressed form,for the reasons such as transmission and storage efficiency. However, thisimplies that the compressed image should be decompressed, which indentsadditional computing resources. This limitation has motivated us to take upresearch in document image analysis using compressed documents. In this paper,we think in a new way to carry out segmentation at line, word and characterlevel in run-length compressed printed-text-documents. We extract thehorizontal projection profile curve from the compressed file and using thelocal minima points perform line segmentation. However, tracing verticalinformation which leads to tracking words-characters in a run-length compressedfile is not very straight forward. Therefore, we propose a novel technique forcarrying out simultaneous word and character segmentation by popping out columnruns from each row in an intelligent sequence. The proposed algorithms havebeen validated with 1101 text-lines, 1409 words and 7582 characters from adata-set of 35 noise and skew free compressed documents of Bengali, Kannada andEnglish Scripts.
arxiv-1403-7683 | Approximate Matrix Multiplication with Application to Linear Embeddings |  http://arxiv.org/abs/1403.7683  | author:Anastasios Kyrillidis, Michail Vlachos, Anastasios Zouzias category:math.ST cs.IT math.IT stat.ML stat.TH published:2014-03-30 summary:In this paper, we study the problem of approximately computing the product oftwo real matrices. In particular, we analyze a dimensionality-reduction-basedapproximation algorithm due to Sarlos [1], introducing the notion of nuclearrank as the ratio of the nuclear norm over the spectral norm. The presentedbound has improved dependence with respect to the approximation error (ascompared to previous approaches), whereas the subspace -- on which we projectthe input matrices -- has dimensions proportional to the maximum of theirnuclear rank and it is independent of the input dimensions. In addition, weprovide an application of this result to linear low-dimensional embeddings.Namely, we show that any Euclidean point-set with bounded nuclear rank isamenable to projection onto number of dimensions that is independent of theinput dimensionality, while achieving additive error guarantees.
arxiv-1403-7735 | Optimal Cooperative Cognitive Relaying and Spectrum Access for an Energy Harvesting Cognitive Radio: Reinforcement Learning Approach |  http://arxiv.org/abs/1403.7735  | author:Ahmed El Shafie, Tamer Khattab, Hussien Saad, Amr Mohamed category:cs.NI cs.IT cs.LG math.IT published:2014-03-30 summary:In this paper, we consider a cognitive setting under the context ofcooperative communications, where the cognitive radio (CR) user is assumed tobe a self-organized relay for the network. The CR user and the PU are assumedto be energy harvesters. The CR user cooperatively relays some of theundelivered packets of the primary user (PU). Specifically, the CR user storesa fraction of the undelivered primary packets in a relaying queue (buffer). Itmanages the flow of the undelivered primary packets to its relaying queue usingthe appropriate actions over time slots. Moreover, it has the decision ofchoosing the used queue for channel accessing at idle time slots (slots wherethe PU's queue is empty). It is assumed that one data packet transmissiondissipates one energy packet. The optimal policy changes according to theprimary and CR users arrival rates to the data and energy queues as well as thechannels connectivity. The CR user saves energy for the PU by taking theresponsibility of relaying the undelivered primary packets. It optimallyorganizes its own energy packets to maximize its payoff as time progresses.
arxiv-1403-7726 | Relevant Feature Selection Model Using Data Mining for Intrusion Detection System |  http://arxiv.org/abs/1403.7726  | author:Ayman I. Madbouly, Amr M. Gody, Tamer M. Barakat category:cs.CR cs.LG published:2014-03-30 summary:Network intrusions have become a significant threat in recent years as aresult of the increased demand of computer networks for critical systems.Intrusion detection system (IDS) has been widely deployed as a defense measurefor computer networks. Features extracted from network traffic can be used assign to detect anomalies. However with the huge amount of network traffic,collected data contains irrelevant and redundant features that affect thedetection rate of the IDS, consumes high amount of system resources, andslowdown the training and testing process of the IDS. In this paper, a newfeature selection model is proposed; this model can effectively select the mostrelevant features for intrusion detection. Our goal is to build a lightweightintrusion detection system by using a reduced features set. Deleting irrelevantand redundant features helps to build a faster training and testing process, tohave less resource consumption as well as to maintain high detection rates. Theeffectiveness and the feasibility of our feature selection model were verifiedby several experiments on KDD intrusion detection dataset. The experimentalresults strongly showed that our model is not only able to yield high detectionrates but also to speed up the detection process.
arxiv-1403-7746 | Multi-label Ferns for Efficient Recognition of Musical Instruments in Recordings |  http://arxiv.org/abs/1403.7746  | author:Miron B. Kursa, Alicja A. Wieczorkowska category:cs.LG cs.SD published:2014-03-30 summary:In this paper we introduce multi-label ferns, and apply this technique forautomatic classification of musical instruments in audio recordings. We comparethe performance of our proposed method to a set of binary random ferns, usingjazz recordings as input data. Our main result is obtaining much fasterclassification and higher F-score. We also achieve substantial reduction of themodel size.
arxiv-1403-7591 | Building A Large Concept Bank for Representing Events in Video |  http://arxiv.org/abs/1403.7591  | author:Yin Cui, Dong Liu, Jiawei Chen, Shih-Fu Chang category:cs.MM cs.CV cs.IR H.3.1 published:2014-03-29 summary:Concept-based video representation has proven to be effective in complexevent detection. However, existing methods either manually design concepts ordirectly adopt concept libraries not specifically designed for events. In thispaper, we propose to build Concept Bank, the largest concept library consistingof 4,876 concepts specifically designed to cover 631 real-world events. Toconstruct the Concept Bank, we first gather a comprehensive event collectionfrom WikiHow, a collaborative writing project that aims to build the world'slargest manual for any possible How-To event. For each event, we then searchFlickr and discover relevant concepts from the tags of the returned images. Wetrain a Multiple Kernel Linear SVM for each discovered concept as a conceptdetector in Concept Bank. We organize the concepts into a five-layer treestructure, in which the higher-level nodes correspond to the event categorieswhile the leaf nodes are the event-specific concepts discovered for each event.Based on such tree ontology, we develop a semantic matching method to selectrelevant concepts for each textual event query, and then apply thecorresponding concept detectors to generate concept-based videorepresentations. We use TRECVID Multimedia Event Detection 2013 and ColumbiaConsumer Video open source event definitions and videos as our test sets andshow very promising results on two video event detection tasks: event modelingover concept space and zero-shot event retrieval. To the best of our knowledge,this is the largest concept library covering the largest number of real-worldevents.
arxiv-1403-7588 | Scalable Robust Matrix Recovery: Frank-Wolfe Meets Proximal Methods |  http://arxiv.org/abs/1403.7588  | author:Cun Mu, Yuqian Zhang, John Wright, Donald Goldfarb category:math.OC cs.CV cs.NA stat.ML published:2014-03-29 summary:Recovering matrices from compressive and grossly corrupted observations is afundamental problem in robust statistics, with rich applications in computervision and machine learning. In theory, under certain conditions, this problemcan be solved in polynomial time via a natural convex relaxation, known asCompressive Principal Component Pursuit (CPCP). However, all existing provablealgorithms for CPCP suffer from superlinear per-iteration cost, which severelylimits their applicability to large scale problems. In this paper, we proposeprovable, scalable and efficient methods to solve CPCP with (essentially)linear per-iteration cost. Our method combines classical ideas from Frank-Wolfeand proximal methods. In each iteration, we mainly exploit Frank-Wolfe toupdate the low-rank component with rank-one SVD and exploit the proximal stepfor the sparse term. Convergence results and implementation details are alsodiscussed. We demonstrate the scalability of the proposed approach withpromising numerical experiments on visual data.
arxiv-1403-7365 | Expectation-Maximization Technique and Spatial-Adaptation Applied to Pel-Recursive Motion Estimation |  http://arxiv.org/abs/1403.7365  | author:Vania Vieira Estrela, Marcos Henrique da Silva Bassani category:cs.CV published:2014-03-28 summary:Pel-recursive motion estimation isa well-established approach. However, inthe presence of noise, it becomes an ill-posed problem that requiresregularization. In this paper, motion vectors are estimated in an iterativefashion by means of the Expectation-Maximization (EM) algorithm and a Gaussiandata model. Our proposed algorithm also utilizes the local image properties ofthe scene to improve the motion vector estimates following a spatially adaptiveapproach. Numerical experiments are presented that demonstrate the merits ofour method.
arxiv-1403-7311 | Performance Evaluation of Raster Based Shape Vectors in Object Recognition |  http://arxiv.org/abs/1403.7311  | author:Akbar Khan, Pratap Reddy L category:cs.CV published:2014-03-28 summary:Object recognition is still an impediment in the field of computer vision andmultimedia retrieval.Defining an object model is a critical task. Shapeinformation of an object play a critical role in the process of objectrecognition. Extraction of boundary information of an object from themultimedia data and classifying this information with associated objects is theprimary step towards object recognition. Rasters play an important role whilecomputing object boundary. The trade-off lies with the dimensionality of theobject versus computational cost while achieving maximum efficiency. In thistreatise an attempt is made to evaluate the performance of circular and spiralraster models in terms of average retrieval efficiency and computational cost.
arxiv-1403-7308 | Data generator based on RBF network |  http://arxiv.org/abs/1403.7308  | author:Marko Robnik-Šikonja category:stat.ML cs.AI cs.LG published:2014-03-28 summary:There are plenty of problems where the data available is scarce andexpensive. We propose a generator of semi-artificial data with similarproperties to the original data which enables development and testing ofdifferent data mining algorithms and optimization of their parameters. Thegenerated data allow a large scale experimentation and simulations withoutdanger of overfitting. The proposed generator is based on RBF networks whichlearn sets of Gaussian kernels. Learned Gaussian kernels can be used in agenerative mode to generate the data from the same distributions. To assesquality of the generated data we developed several workflows and used them toevaluate the statistical properties of the generated data, structuralsimilarity, and predictive similarity using supervised and unsupervisedlearning techniques. To determine usability of the proposed generator weconducted a large scale evaluation using 51 UCI data sets. The results show aconsiderable similarity between the original and generated data and indicatethat the method can be useful in several development and simulation scenarios.
arxiv-1403-7267 | Systematic Ensemble Learning for Regression |  http://arxiv.org/abs/1403.7267  | author:Roberto Aldave, Jean-Pierre Dussault category:stat.ML published:2014-03-28 summary:The motivation of this work is to improve the performance of standardstacking approaches or ensembles, which are composed of simple, heterogeneousbase models, through the integration of the generation and selection stages forregression problems. We propose two extensions to the standard stackingapproach. In the first extension we combine a set of standard stackingapproaches into an ensemble of ensembles using a two-step ensemble learning inthe regression setting. The second extension consists of two parts. In theinitial part a diversity mechanism is injected into the original training dataset, systematically generating different training subsets or partitions, andcorresponding ensembles of ensembles. In the final part after measuring thequality of the different partitions or ensembles, a max-min rule-basedselection algorithm is used to select the most appropriate ensemble/partitionon which to make the final prediction. We show, based on experiments over abroad range of data sets, that the second extension performs better than thebest of the standard stacking approaches, and is as good as the oracle ofdatabases, which has the best base model selected by cross-validation for eachdata set. In addition to that, the second extension performs better than twostate-of-the-art ensemble methods for regression, and it is as good as a thirdstate-of-the-art ensemble method.
arxiv-1403-7429 | Distributed Reconstruction of Nonlinear Networks: An ADMM Approach |  http://arxiv.org/abs/1403.7429  | author:Wei Pan, Aivar Sootla, Guy-Bart Stan category:math.OC cs.DC cs.LG cs.SY published:2014-03-28 summary:In this paper, we present a distributed algorithm for the reconstruction oflarge-scale nonlinear networks. In particular, we focus on the identificationfrom time-series data of the nonlinear functional forms and associatedparameters of large-scale nonlinear networks. Recently, a nonlinear networkreconstruction problem was formulated as a nonconvex optimisation problem basedon the combination of a marginal likelihood maximisation procedure withsparsity inducing priors. Using a convex-concave procedure (CCCP), an iterativereweighted lasso algorithm was derived to solve the initial nonconvexoptimisation problem. By exploiting the structure of the objective function ofthis reweighted lasso algorithm, a distributed algorithm can be designed. Tothis end, we apply the alternating direction method of multipliers (ADMM) todecompose the original problem into several subproblems. To illustrate theeffectiveness of the proposed methods, we use our approach to identify anetwork of interconnected Kuramoto oscillators with different network sizes(500~100,000 nodes).
arxiv-1403-7455 | Hybrid Approach to English-Hindi Name Entity Transliteration |  http://arxiv.org/abs/1403.7455  | author:Shruti Mathur, Varun Prakash Saxena category:cs.CL published:2014-03-28 summary:Machine translation (MT) research in Indian languages is still in itsinfancy. Not much work has been done in proper transliteration of name entitiesin this domain. In this paper we address this issue. We have used English-Hindilanguage pair for our experiments and have used a hybrid approach. At first wehave processed English words using a rule based approach which extractsindividual phonemes from the words and then we have applied statisticalapproach which converts the English into its equivalent Hindi phoneme and inturn the corresponding Hindi word. Through this approach we have attained83.40% accuracy.
arxiv-1403-7543 | A sparse Kaczmarz solver and a linearized Bregman method for online compressed sensing |  http://arxiv.org/abs/1403.7543  | author:Dirk A. Lorenz, Stephan Wenger, Frank Schöpfer, Marcus Magnor category:math.OC cs.CV cs.IT math.IT math.NA published:2014-03-28 summary:An algorithmic framework to compute sparse or minimal-TV solutions of linearsystems is proposed. The framework includes both the Kaczmarz method and thelinearized Bregman method as special cases and also several new methods such asa sparse Kaczmarz solver. The algorithmic framework has a variety ofapplications and is especially useful for problems in which the linearmeasurements are slow and expensive to obtain. We present examples for onlinecompressed sensing, TV tomographic reconstruction and radio interferometry.
arxiv-1403-7335 | Emotion Analysis Platform on Chinese Microblog |  http://arxiv.org/abs/1403.7335  | author:Duyu Tang, Bing Qin, Ting Liu, Qiuhui Shi category:cs.CL cs.CY cs.IR published:2014-03-28 summary:Weibo, as the largest social media service in China, has billions of messagesgenerated every day. The huge number of messages contain rich sentimentalinformation. In order to analyze the emotional changes in accordance with timeand space, this paper presents an Emotion Analysis Platform (EAP), whichexplores the emotional distribution of each province, so that can monitor theglobal pulse of each province in China. The massive data of Weibo and thereal-time requirements make the building of EAP challenging. In order to solvethe above problems, emoticons, emotion lexicon and emotion-shifting rules areadopted in EAP to analyze the emotion of each tweet. In order to verify theeffectiveness of the platform, case study on the Sichuan earthquake is done,and the analysis result of the platform accords with the fact. In order toanalyze from quantity, we manually annotate a test set and conduct experimenton it. The experimental results show that the macro-Precision of EAP reaches80% and the EAP works effectively.
arxiv-1403-7321 | Learning detectors quickly using structured covariance matrices |  http://arxiv.org/abs/1403.7321  | author:Jack Valmadre, Sridha Sridharan, Simon Lucey category:cs.CV published:2014-03-28 summary:Computer vision is increasingly becoming interested in the rapid estimationof object detectors. Canonical hard negative mining strategies are slow as theyrequire multiple passes of the large negative training set. Recent work hasdemonstrated that if the distribution of negative examples is assumed to bestationary, then Linear Discriminant Analysis (LDA) can learn comparabledetectors without ever revisiting the negative set. Even with this insight,however, the time to learn a single object detector can still be on the orderof tens of seconds on a modern desktop computer. This paper proposes toleverage the resulting structured covariance matrix to obtain detectors withidentical performance in orders of magnitude less time and memory. We elucidatean important connection to the correlation filter literature, demonstratingthat these can also be trained without ever revisiting the negative set.
arxiv-1403-7265 | Accelerating MCMC via Parallel Predictive Prefetching |  http://arxiv.org/abs/1403.7265  | author:Elaine Angelino, Eddie Kohler, Amos Waterland, Margo Seltzer, Ryan P. Adams category:stat.ML stat.CO published:2014-03-28 summary:We present a general framework for accelerating a large class of widely usedMarkov chain Monte Carlo (MCMC) algorithms. Our approach exploits fast,iterative approximations to the target density to speculatively evaluate manypotential future steps of the chain in parallel. The approach can acceleratecomputation of the target distribution of a Bayesian inference problem, withoutcompromising exactness, by exploiting subsets of data. It takes advantage ofwhatever parallel resources are available, but produces results exactlyequivalent to standard serial execution. In the initial burn-in phase of chainevaluation, it achieves speedup over serial evaluation that is close to linearin the number of available cores.
arxiv-1403-7471 | Approximate Decentralized Bayesian Inference |  http://arxiv.org/abs/1403.7471  | author:Trevor Campbell, Jonathan P. How category:cs.LG published:2014-03-28 summary:This paper presents an approximate method for performing Bayesian inferencein models with conditional independence over a decentralized network oflearning agents. The method first employs variational inference on eachindividual learning agent to generate a local approximate posterior, the agentstransmit their local posteriors to other agents in the network, and finallyeach agent combines its set of received local posteriors. The key insight inthis work is that, for many Bayesian models, approximate inference schemesdestroy symmetry and dependencies in the model that are crucial to the correctapplication of Bayes' rule when combining the local posteriors. The proposedmethod addresses this issue by including an additional optimization step in thecombination procedure that accounts for these broken dependencies. Experimentson synthetic and real data demonstrate that the decentralized method providesadvantages in computational performance and predictive test likelihood overprevious batch and distributed methods.
arxiv-1403-7550 | DimmWitted: A Study of Main-Memory Statistical Analytics |  http://arxiv.org/abs/1403.7550  | author:Ce Zhang, Christopher Ré category:cs.DB cs.LG math.OC stat.ML published:2014-03-28 summary:We perform the first study of the tradeoff space of access methods andreplication to support statistical analytics using first-order methods executedin the main memory of a Non-Uniform Memory Access (NUMA) machine. Statisticalanalytics systems differ from conventional SQL-analytics in the amount andtypes of memory incoherence they can tolerate. Our goal is to understandtradeoffs in accessing the data in row- or column-order and at what granularityone should share the model and data for a statistical task. We study this newtradeoff space, and discover there are tradeoffs between hardware andstatistical efficiency. We argue that our tradeoff study may provide valuableinformation for designers of analytics engines: for each system we consider,our prototype engine can run at least one popular task at least 100x faster. Weconduct our study across five architectures using popular models includingSVMs, logistic regression, Gibbs sampling, and neural networks.
arxiv-1403-7304 | Characteristic Kernels and Infinitely Divisible Distributions |  http://arxiv.org/abs/1403.7304  | author:Yu Nishiyama, Kenji Fukumizu category:stat.ML published:2014-03-28 summary:We connect shift-invariant characteristic kernels to infinitely divisibledistributions on $\mathbb{R}^{d}$. Characteristic kernels play an importantrole in machine learning applications with their kernel means to distinguishany two probability measures. The contribution of this paper is two-fold.First, we show, using the L\'evy-Khintchine formula, that any shift-invariantkernel given by a bounded, continuous and symmetric probability densityfunction (pdf) of an infinitely divisible distribution on $\mathbb{R}^d$ ischaracteristic. We also present some closure property of such characteristickernels under addition, pointwise product, and convolution. Second, indeveloping various kernel mean algorithms, it is fundamental to compute thefollowing values: (i) kernel mean values $m_P(x)$, $x \in \mathcal{X}$, and(ii) kernel mean RKHS inner products ${\left\langle m_P, m_Q \right\rangle_{\mathcal{H}}}$, for probability measures $P, Q$. If $P, Q$, and kernel $k$are Gaussians, then computation (i) and (ii) results in Gaussian pdfs that istractable. We generalize this Gaussian combination to more general cases in theclass of infinitely divisible distributions. We then introduce a {\itconjugate} kernel and {\it convolution trick}, so that the above (i) and (ii)have the same pdf form, expecting tractable computation at least in some cases.As specific instances, we explore $\alpha$-stable distributions and a richclass of generalized hyperbolic distributions, where the Laplace, Cauchy andStudent-t distributions are included.
arxiv-1403-6901 | Automatic Segmentation of Broadcast News Audio using Self Similarity Matrix |  http://arxiv.org/abs/1403.6901  | author:Sapna Soni, Ahmed Imran, Sunil Kumar Kopparapu category:cs.SD cs.LG cs.MM published:2014-03-27 summary:Generally audio news broadcast on radio is com- posed of music, commercials,news from correspondents and recorded statements in addition to the actual newsread by the newsreader. When news transcripts are available, automaticsegmentation of audio news broadcast to time align the audio with the texttranscription to build frugal speech corpora is essential. We address theproblem of identifying segmentation in the audio news broadcast correspondingto the news read by the newsreader so that they can be mapped to the texttranscripts. The existing techniques produce sub-optimal solutions when used toextract newsreader read segments. In this paper, we propose a new techniquewhich is able to identify the acoustic change points reliably using an acousticSelf Similarity Matrix (SSM). We describe the two pass technique in detail andverify its performance on real audio news broadcast of All India Radio fordifferent languages.
arxiv-1403-7178 | Offshore Wind Farm Layout Optimization Using Adapted Genetic Algorithm: A different perspective |  http://arxiv.org/abs/1403.7178  | author:Feng Liu, Zhifang Wang category:cs.NE published:2014-03-27 summary:In this paper we study the problem of optimal layout of an offshore wind farmto minimize the wake effect impacts. Considering the specific requirements ofconcerned offshore wind farm, we propose an adaptive genetic algorithm (AGA)which introduces location swaps to replace random crossovers in conventionalGAs. That way the total number of turbines in the resulting layout will beeffectively kept to the initially specified value. We experiment the proposedAGA method on three cases with free wind speed of 12 m/s, 20 m/s, and a typicaloffshore wind distribution setting respectively. Numerical results verify theeffectiveness of our proposed algorithm which achieves a much fasterconvergence compared to conventional GA algorithms.
arxiv-1403-7057 | Closed-Form Training of Conditional Random Fields for Large Scale Image Segmentation |  http://arxiv.org/abs/1403.7057  | author:Alexander Kolesnikov, Matthieu Guillaumin, Vittorio Ferrari, Christoph H. Lampert category:cs.LG cs.CV published:2014-03-27 summary:We present LS-CRF, a new method for very efficient large-scale training ofConditional Random Fields (CRFs). It is inspired by existing closed-formexpressions for the maximum likelihood parameters of a generative graphicalmodel with tree topology. LS-CRF training requires only solving a set ofindependent regression problems, for which closed-form expression as well asefficient iterative solvers are available. This makes it orders of magnitudefaster than conventional maximum likelihood learning for CRFs that requirerepeated runs of probabilistic inference. At the same time, the models learnedby our method still allow for joint inference at test time. We apply LS-CRF tothe task of semantic image segmentation, showing that it is highly efficient,even for loopy models where probabilistic inference is problematic. It allowsthe training of image segmentation models from significantly larger trainingsets than had been used previously. We demonstrate this on two new datasetsthat form a second contribution of this paper. They consist of over 180,000images with figure-ground segmentation annotations. Our large-scale experimentsshow that the possibilities of CRF-based image segmentation are far fromexhausted, indicating, for example, that semi-supervised learning and the useof non-linear predictors are promising directions for achieving highersegmentation accuracy in the future.
arxiv-1403-6958 | Compressive Pattern Matching on Multispectral Data |  http://arxiv.org/abs/1403.6958  | author:S. Rousseau, D. Helbert, P. Carré, J. Blanc-Talon category:cs.CV published:2014-03-27 summary:We introduce a new constrained minimization problem that performs templateand pattern detection on a multispectral image in a compressive sensingcontext. We use an original minimization problem from Guo and Osher that uses$L_1$ minimization techniques to perform template detection in a multispectralimage. We first adapt this minimization problem to work with compressivesensing data. Then we extend it to perform pattern detection using a formaltransform called the spectralization along a pattern. That extension brings outthe problem of measurement reconstruction. We introduce shifted measurementsthat allow us to reconstruct all the measurement with a small overhead and wegive an optimality constraint for simple patterns. We present numerical resultsshowing the performances of the original minimization problem and thecompressed ones with different measurement rates and applied on remotely senseddata.
arxiv-1403-6950 | Pyramidal Fisher Motion for Multiview Gait Recognition |  http://arxiv.org/abs/1403.6950  | author:F. M. Castro, M. J. Marin-Jimenez, R. Medina-Carnicer category:cs.CV published:2014-03-27 summary:The goal of this paper is to identify individuals by analyzing their gait.Instead of using binary silhouettes as input data (as done in many previousworks) we propose and evaluate the use of motion descriptors based on denselysampled short-term trajectories. We take advantage of state-of-the-art peopledetectors to define custom spatial configurations of the descriptors around thetarget person. Thus, obtaining a pyramidal representation of the gait motion.The local motion features (described by the Divergence-Curl-Shear descriptor)extracted on the different spatial areas of the person are combined into asingle high-level gait descriptor by using the Fisher Vector encoding. Theproposed approach, coined Pyramidal Fisher Motion, is experimentally validatedon the recent `AVA Multiview Gait' dataset. The results show that this newapproach achieves promising results in the problem of gait recognition.
arxiv-1403-6794 | KPCA Spatio-temporal trajectory point cloud classifier for recognizing human actions in a CBVR system |  http://arxiv.org/abs/1403.6794  | author:Iván Gómez-Conde, David N. Olivieri category:cs.IR cs.CV published:2014-03-26 summary:We describe a content based video retrieval (CBVR) software system foridentifying specific locations of a human action within a full length film, andretrieving similar video shots from a query. For this, we introduce the conceptof a trajectory point cloud for classifying unique actions, encoded in aspatio-temporal covariant eigenspace, where each point is characterized by itsspatial location, local Frenet-Serret vector basis, time averaged curvature andtorsion and the mean osculating hyperplane. Since each action can bedistinguished by their unique trajectories within this space, the trajectorypoint cloud is used to define an adaptive distance metric for classifyingqueries against stored actions. Depending upon the distance to othertrajectories, the distance metric uses either large scale structure of thetrajectory point cloud, such as the mean distance between cloud centroids orthe difference in hyperplane orientation, or small structure such as the timeaveraged curvature and torsion, to classify individual points in a fuzzy-KNN.Our system can function in real-time and has an accuracy greater than 93% formultiple action recognition within video repositories. We demonstrate the useof our CBVR system in two situations: by locating specific frame positions oftrained actions in two full featured films, and video shot retrieval from adatabase with a web search application.
arxiv-1403-6706 | Beyond L2-Loss Functions for Learning Sparse Models |  http://arxiv.org/abs/1403.6706  | author:Karthikeyan Natesan Ramamurthy, Aleksandr Y. Aravkin, Jayaraman J. Thiagarajan category:stat.ML cs.CV cs.LG math.OC I.2.6; G.1.6 published:2014-03-26 summary:Incorporating sparsity priors in learning tasks can give rise to simple, andinterpretable models for complex high dimensional data. Sparse models havefound widespread use in structure discovery, recovering data from corruptions,and a variety of large scale unsupervised and supervised learning problems.Assuming the availability of sufficient data, these methods infer dictionariesfor sparse representations by optimizing for high-fidelity reconstruction. Inmost scenarios, the reconstruction quality is measured using the squaredEuclidean distance, and efficient algorithms have been developed for both batchand online learning cases. However, new application domains motivate lookingbeyond conventional loss functions. For example, robust loss functions such as$\ell_1$ and Huber are useful in learning outlier-resilient models, and thequantile loss is beneficial in discovering structures that are therepresentative of a particular quantile. These new applications motivate ourwork in generalizing sparse learning to a broad class of convex loss functions.In particular, we consider the class of piecewise linear quadratic (PLQ) costfunctions that includes Huber, as well as $\ell_1$, quantile, Vapnik, hingeloss, and smoothed variants of these penalties. We propose an algorithm tolearn dictionaries and obtain sparse codes when the data reconstructionfidelity is measured using any smooth PLQ cost function. We provide convergenceguarantees for the proposed algorithm, and demonstrate the convergence behaviorusing empirical experiments. Furthermore, we present three case studies thatrequire the use of PLQ cost functions: (i) robust image modeling, (ii) tagrefinement for image annotation and retrieval and (iii) computing empiricalconfidence limits for subspace clustering.
arxiv-1403-6636 | Sign Language Lexical Recognition With Propositional Dynamic Logic |  http://arxiv.org/abs/1403.6636  | author:Arturo Curiel, Christophe Collet category:cs.CL I.2.7 published:2014-03-26 summary:This paper explores the use of Propositional Dynamic Logic (PDL) as asuitable formal framework for describing Sign Language (SL), the language ofdeaf people, in the context of natural language processing. SLs are visual,complete, standalone languages which are just as expressive as oral languages.Signs in SL usually correspond to sequences of highly specific body posturesinterleaved with movements, which make reference to real world objects,characters or situations. Here we propose a formal representation of SL signs,that will help us with the analysis of automatically-collected hand trackingdata from French Sign Language (FSL) video corpora. We further show how such arepresentation could help us with the design of computer aided SL verificationtools, which in turn would bring us closer to the development of an automaticrecognition system for these languages.
arxiv-1403-6614 | QCMC: Quasi-conformal Parameterizations for Multiply-connected domains |  http://arxiv.org/abs/1403.6614  | author:Kin Tat Ho, Lok Ming Lui category:cs.CG cs.CV math.DG published:2014-03-26 summary:This paper presents a method to compute the {\it quasi-conformalparameterization} (QCMC) for a multiply-connected 2D domain or surface. QCMCcomputes a quasi-conformal map from a multiply-connected domain $S$ onto apunctured disk $D_S$ associated with a given Beltrami differential. TheBeltrami differential, which measures the conformality distortion, is acomplex-valued function $\mu:S\to\mathbb{C}$ with supremum norm strictly lessthan 1. Every Beltrami differential gives a conformal structure of $S$. Hence,the conformal module of $D_S$, which are the radii and centers of the innercircles, can be fully determined by $\mu$, up to a M\"obius transformation. Inthis paper, we propose an iterative algorithm to simultaneously search for theconformal module and the optimal quasi-conformal parameterization. The key ideais to minimize the Beltrami energy subject to the boundary constraints. Theoptimal solution is our desired quasi-conformal parameterization onto apunctured disk. The parameterization of the multiply-connected domainsimplifies numerical computations and has important applications in variousfields, such as in computer graphics and vision. Experiments have been carriedout on synthetic data together with real multiply-connected Riemann surfaces.Results show that our proposed method can efficiently compute quasi-conformalparameterizations of multiply-connected domains and outperforms otherstate-of-the-art algorithms. Applications of the proposed parameterizationtechnique have also been explored.
arxiv-1403-7100 | A study on cost behaviors of binary classification measures in class-imbalanced problems |  http://arxiv.org/abs/1403.7100  | author:Bao-Gang Hu, Wei-Ming Dong category:cs.LG published:2014-03-26 summary:This work investigates into cost behaviors of binary classification measuresin a background of class-imbalanced problems. Twelve performance measures arestudied, such as F measure, G-means in terms of accuracy rates, and of recalland precision, balance error rate (BER), Matthews correlation coefficient(MCC), Kappa coefficient, etc. A new perspective is presented for thosemeasures by revealing their cost functions with respect to the class imbalanceratio. Basically, they are described by four types of cost functions. Thefunctions provides a theoretical understanding why some measures are suitablefor dealing with class-imbalanced problems. Based on their cost functions, weare able to conclude that G-means of accuracy rates and BER are suitablemeasures because they show "proper" cost behaviors in terms of "amisclassification from a small class will cause a greater cost than that from alarge class". On the contrary, F1 measure, G-means of recall and precision, MCCand Kappa coefficient measures do not produce such behaviors so that they areunsuitable to serve our goal in dealing with the problems properly.
arxiv-1403-6888 | Fast Localization of Facial Landmark Points |  http://arxiv.org/abs/1403.6888  | author:Nenad Markuš, Miroslav Frljak, Igor S. Pandžić, Jörgen Ahlberg, Robert Forchheimer category:cs.CV published:2014-03-26 summary:Localization of salient facial landmark points, such as eye corners or thetip of the nose, is still considered a challenging computer vision problemdespite recent efforts. This is especially evident in unconstrainedenvironments, i.e., in the presence of background clutter and large head posevariations. Most methods that achieve state-of-the-art accuracy are slow, and,thus, have limited applications. We describe a method that can accuratelyestimate the positions of relevant facial landmarks in real-time even onhardware with limited processing power, such as mobile devices. This isachieved with a sequence of estimators based on ensembles of regression trees.The trees use simple pixel intensity comparisons in their internal nodes andthis makes them able to process image regions very fast. We test the developedsystem on several publicly available datasets and analyse its processing speedon various devices. Experimental results show that our method has practicalvalue.
arxiv-1403-6566 | Image Retargeting by Content-Aware Synthesis |  http://arxiv.org/abs/1403.6566  | author:Weiming Dong, Fuzhang Wu, Yan Kong, Xing Mei, Tong-Yee Lee, Xiaopeng Zhang category:cs.GR cs.CV published:2014-03-26 summary:Real-world images usually contain vivid contents and rich textural details,which will complicate the manipulation on them. In this paper, we design a newframework based on content-aware synthesis to enhance content-aware imageretargeting. By detecting the textural regions in an image, the textural imagecontent can be synthesized rather than simply distorted or cropped. This methodenables the manipulation of textural & non-textural regions with differentstrategy since they have different natures. We propose to retarget the texturalregions by content-aware synthesis and non-textural regions by fastmulti-operators. To achieve practical retargeting applications for generalimages, we develop an automatic and fast texture detection method that candetect multiple disjoint textural regions. We adjust the saliency of the imageaccording to the features of the textural regions. To validate the proposedmethod, comparisons with state-of-the-art image targeting techniques and a userstudy were conducted. Convincing visual results are shown to demonstrate theeffectiveness of the proposed method.
arxiv-1403-6774 | Optimized imaging using non-rigid registration |  http://arxiv.org/abs/1403.6774  | author:Benjamin Berkels, Peter Binev, Douglas A. Blom, Wolfgang Dahmen, Robert C. Sharpley, Thomas Vogt category:cs.CV published:2014-03-26 summary:The extraordinary improvements of modern imaging devices offer access to datawith unprecedented information content. However, widely used image processingmethodologies fall far short of exploiting the full breadth of informationoffered by numerous types of scanning probe, optical, and electronmicroscopies. In many applications, it is necessary to keep measurementintensities below a desired threshold. We propose a methodology for extractingan increased level of information by processing a series of data setssuffering, in particular, from high degree of spatial uncertainty caused bycomplex multiscale motion during the acquisition process. An important role isplayed by a nonrigid pixel-wise registration method that can cope with lowsignal-to-noise ratios. This is accompanied by formulating objective qualitymeasures which replace human intervention and visual inspection in theprocessing chain. Scanning transmission electron microscopy of siliceouszeolite material exhibits the above-mentioned obstructions and therefore servesas orientation and a test of our procedures.
arxiv-1403-6822 | Comparison of Multi-agent and Single-agent Inverse Learning on a Simulated Soccer Example |  http://arxiv.org/abs/1403.6822  | author:Xiaomin Lin, Peter A. Beling, Randy Cogill category:cs.LG cs.GT published:2014-03-26 summary:We compare the performance of Inverse Reinforcement Learning (IRL) with therelative new model of Multi-agent Inverse Reinforcement Learning (MIRL). Beforecomparing the methods, we extend a published Bayesian IRL approach that is onlyapplicable to the case where the reward is only state dependent to a generalone capable of tackling the case where the reward depends on both state andaction. Comparison between IRL and MIRL is made in the context of an abstractsoccer game, using both a game model in which the reward depends only on stateand one in which it depends on both state and action. Results suggest that theIRL approach performs much worse than the MIRL approach. We speculate that theunderperformance of IRL is because it fails to capture equilibrium informationin the manner possible in MIRL.
arxiv-1403-6863 | Online Learning of k-CNF Boolean Functions |  http://arxiv.org/abs/1403.6863  | author:Joel Veness, Marcus Hutter category:cs.LG published:2014-03-26 summary:This paper revisits the problem of learning a k-CNF Boolean function fromexamples in the context of online learning under the logarithmic loss. In doingso, we give a Bayesian interpretation to one of Valiant's celebrated PAClearning algorithms, which we then build upon to derive two efficient, online,probabilistic, supervised learning algorithms for predicting the output of anunknown k-CNF Boolean function. We analyze the loss of our methods, and showthat the cumulative log-loss can be upper bounded, ignoring logarithmicfactors, by a polynomial function of the size of each example.
arxiv-1403-6652 | DeepWalk: Online Learning of Social Representations |  http://arxiv.org/abs/1403.6652  | author:Bryan Perozzi, Rami Al-Rfou, Steven Skiena category:cs.SI cs.LG published:2014-03-26 summary:We present DeepWalk, a novel approach for learning latent representations ofvertices in a network. These latent representations encode social relations ina continuous vector space, which is easily exploited by statistical models.DeepWalk generalizes recent advancements in language modeling and unsupervisedfeature learning (or deep learning) from sequences of words to graphs. DeepWalkuses local information obtained from truncated random walks to learn latentrepresentations by treating walks as the equivalent of sentences. Wedemonstrate DeepWalk's latent representations on several multi-label networkclassification tasks for social networks such as BlogCatalog, Flickr, andYouTube. Our results show that DeepWalk outperforms challenging baselines whichare allowed a global view of the network, especially in the presence of missinginformation. DeepWalk's representations can provide $F_1$ scores up to 10%higher than competing methods when labeled data is sparse. In some experiments,DeepWalk's representations are able to outperform all baseline methods whileusing 60% less training data. DeepWalk is also scalable. It is an onlinelearning algorithm which builds useful incremental results, and is triviallyparallelizable. These qualities make it suitable for a broad class of realworld applications such as network classification, and anomaly detection.
arxiv-1403-6600 | How Crossover Speeds Up Building-Block Assembly in Genetic Algorithms |  http://arxiv.org/abs/1403.6600  | author:Dirk Sudholt category:cs.NE cs.DS published:2014-03-26 summary:We re-investigate a fundamental question: how effective is crossover inGenetic Algorithms in combining building blocks of good solutions? Althoughthis has been discussed controversially for decades, we are still lacking arigorous and intuitive answer. We provide such answers for royal road functionsand OneMax, where every bit is a building block. For the latter we show thatusing crossover makes every ($\mu$+$\lambda$) Genetic Algorithm at least twiceas fast as the fastest evolutionary algorithm using only standard bit mutation,up to small-order terms and for moderate $\mu$ and $\lambda$. Crossover isbeneficial because it effectively turns fitness-neutral mutations intoimprovements by combining the right building blocks at a later stage. Comparedto mutation-based evolutionary algorithms, this makes multi-bit mutations moreuseful. Introducing crossover changes the optimal mutation rate on OneMax from$1/n$ to $(1+\sqrt{5})/2 \cdot 1/n \approx 1.618/n$. This holds both foruniform crossover and $k$-point crossover. Experiments and statistical testsconfirm that our findings apply to a broad class of building-block functions.
arxiv-1403-7084 | Constrained speaker linking |  http://arxiv.org/abs/1403.7084  | author:David A. van Leeuwen, Niko Brümmer category:stat.ML cs.SD published:2014-03-26 summary:In this paper we study speaker linking (a.k.a.\ partitioning) givenconstraints of the distribution of speaker identities over speech recordings.Specifically, we show that the intractable partitioning problem becomestractable when the constraints pre-partition the data in smaller cliques withnon-overlapping speakers. The surprisingly common case where speakers intelephone conversations are known, but the assignment of channels to identitiesis unspecified, is treated in a Bayesian way. We show that for the Dutch CGNdatabase, where this channel assignment task is at hand, a lightweight speakerrecognition system can quite effectively solve the channel assignment problem,with 93% of the cliques solved. We further show that the posterior distributionover channel assignment configurations is well calibrated.
arxiv-1403-6348 | Updating Formulas and Algorithms for Computing Entropy and Gini Index from Time-Changing Data Streams |  http://arxiv.org/abs/1403.6348  | author:Blaz Sovdat category:cs.AI cs.LG I.2.6 published:2014-03-25 summary:Despite growing interest in data stream mining the most successfulincremental learners, such as VFDT, still use periodic recomputation to updateattribute information gains and Gini indices. This note provides simpleincremental formulas and algorithms for computing entropy and Gini index fromtime-changing data streams.
arxiv-1403-6397 | Evaluating topic coherence measures |  http://arxiv.org/abs/1403.6397  | author:Frank Rosner, Alexander Hinneburg, Michael Röder, Martin Nettling, Andreas Both category:cs.LG cs.CL cs.IR published:2014-03-25 summary:Topic models extract representative word sets - called topics - from wordcounts in documents without requiring any semantic annotations. Topics are notguaranteed to be well interpretable, therefore, coherence measures have beenproposed to distinguish between good and bad topics. Studies of topic coherenceso far are limited to measures that score pairs of individual words. For thefirst time, we include coherence measures from scientific philosophy that scorepairs of more complex word subsets and apply them to topic scoring.
arxiv-1403-6318 | Stabilizing dual-energy X-ray computed tomography reconstructions using patch-based regularization |  http://arxiv.org/abs/1403.6318  | author:Brian H. Tracey, Eric L. Miller category:cs.CV physics.med-ph published:2014-03-25 summary:Recent years have seen growing interest in exploiting dual- and multi-energymeasurements in computed tomography (CT) in order to characterize materialproperties as well as object shape. Material characterization is performed bydecomposing the scene into constitutive basis functions, such as Comptonscatter and photoelectric absorption functions. While well motivatedphysically, the joint recovery of the spatial distribution of photoelectric andCompton properties is severely complicated by the fact that the data areseveral orders of magnitude more sensitive to Compton scatter coefficients thanto photoelectric absorption, so small errors in Compton estimates can createlarge artifacts in the photoelectric estimate. To address these issues, wepropose a model-based iterative approach which uses patch-based regularizationterms to stabilize inversion of photoelectric coefficients, and solve theresulting problem though use of computationally attractive AlternatingDirection Method of Multipliers (ADMM) solution techniques. Using simulationsand experimental data acquired on a commercial scanner, we demonstrate that theproposed processing can lead to more stable material property estimates whichshould aid materials characterization in future dual- and multi-energy CTsystems.
arxiv-1403-6275 | A Tiered Move-making Algorithm for General Non-submodular Pairwise Energies |  http://arxiv.org/abs/1403.6275  | author:Vibhav Vineet, Jonathan Warrell, Philip H. S. Torr category:cs.CV published:2014-03-25 summary:A large number of problems in computer vision can be modelled as energyminimization problems in a Markov Random Field (MRF) or Conditional RandomField (CRF) framework. Graph-cuts based $\alpha$-expansion is a standardmove-making method to minimize the energy functions with sub-modular pairwiseterms. However, certain problems require more complex pairwise terms where the$\alpha$-expansion method is generally not applicable. In this paper, we propose an iterative {\em tiered move making algorithm}which is able to handle general pairwise terms. Each move to the nextconfiguration is based on the current labeling and an optimal tiered move,where each tiered move requires one application of the dynamic programmingbased tiered labeling method introduced in Felzenszwalb et. al.\cite{tiered_cvpr_felzenszwalbV10}. The algorithm converges to a local minimumfor any general pairwise potential, and we give a theoretical analysis of theproperties of the algorithm, characterizing the situations in which we canexpect good performance. We first evaluate our method on an object-classsegmentation problem using the Pascal VOC-11 segmentation dataset where welearn general pairwise terms. Further we evaluate the algorithm on many otherbenchmark labeling problems such as stereo, image segmentation, image stitchingand image denoising. Our method consistently gets better accuracy and energyvalues than alpha-expansion, loopy belief propagation (LBP), quadraticpseudo-boolean optimization (QPBO), and is competitive with TRWS.
arxiv-1403-6260 | Capturing and Recognizing Objects Appearance Employing Eigenspace |  http://arxiv.org/abs/1403.6260  | author:M. Ashrafuzzaman, M. M . Rahman, M. M. A. Hashem category:cs.CV published:2014-03-25 summary:This paper presents a method of capturing objects appearances from itsenvironment and it also describes how to recognize unknown appearances creatingan eigenspace. This representation and recognition can be done automaticallytaking objects various appearances by using robotic vision from a definedenvironment. This technique also allows extracting objects from some sort ofcomplicated scenes. In this case, some of object appearances are taken withdefined occlusions and eigenspaces are created by accepting both ofnon-occluded and occluded appearances together. Eigenspace is constructedsuccessfully every times when a new object appears, and various appearancesaccumulated gradually. A sequence of appearances is generated from itsaccumulated shapes, which is used for recognition of the unknown objectsappearances. Various objects environments are shown in the experiment tocapture objects appearances and experimental results show effectiveness of theproposed approach.
arxiv-1403-6248 | Classroom Video Assessment and Retrieval via Multiple Instance Learning |  http://arxiv.org/abs/1403.6248  | author:Qifeng Qiao, Peter A. Beling category:cs.IR cs.CY cs.LG published:2014-03-25 summary:We propose a multiple instance learning approach to content-based retrievalof classroom video for the purpose of supporting human assessing the learningenvironment. The key element of our approach is a mapping between the semanticconcepts of the assessment system and features of the video that can bemeasured using techniques from the fields of computer vision and speechanalysis. We report on a formative experiment in content-based video retrievalinvolving trained experts in the Classroom Assessment Scoring System, a widelyused framework for assessment and improvement of learning environments. Theresults of this experiment suggest that our approach has potential applicationto productivity enhancement in assessment and to broader retrieval tasks.
arxiv-1403-6355 | Continuum limit of total variation on point clouds |  http://arxiv.org/abs/1403.6355  | author:Nicolás García Trillos, Dejan Slepčev category:math.ST math.AP stat.ML stat.TH published:2014-03-25 summary:We consider point clouds obtained as random samples of a measure on aEuclidean domain. A graph representing the point cloud is obtained by assigningweights to edges based on the distance between the points they connect. Ourgoal is to develop mathematical tools needed to study the consistency, as thenumber of available data points increases, of graph-based machine learningalgorithms for tasks such as clustering. In particular, we study when is thecut capacity, and more generally total variation, on these graphs a goodapproximation of the perimeter (total variation) in the continuum setting. Weaddress this question in the setting of $\Gamma$-convergence. We obtain almostoptimal conditions on the scaling, as number of points increases, of the sizeof the neighborhood over which the points are connected by an edge for the$\Gamma$-convergence to hold. Taking the limit is enabled by a transportationbased metric which allows to suitably compare functionals defined on differentpoint clouds.
arxiv-1403-6499 | Optimal Schatten-q and Ky-Fan-k Norm Rate of Low Rank Matrix Estimation |  http://arxiv.org/abs/1403.6499  | author:Dong Xia category:stat.ML published:2014-03-25 summary:In this paper, we consider low rank matrix estimation using eithermatrix-version Dantzig Selector $\hat{A}_{\lambda}^d$ or matrix-version LASSOestimator $\hat{A}_{\lambda}^L$. We consider sub-Gaussian measurements, $i.e.$,the measurements $X_1,\ldots,X_n\in\mathbb{R}^{m\times m}$ have $i.i.d.$sub-Gaussian entries. Suppose $\textrm{rank}(A_0)=r$. We proved that, when$n\geq Cm[r^2\vee r\log(m)\log(n)]$ for some $C>0$, both $\hat{A}_{\lambda}^d$and $\hat{A}_{\lambda}^L$ can obtain optimal upper bounds(except somelogarithmic terms) for estimation accuracy under spectral norm. By applyingmetric entropy of Grassmann manifolds, we construct (near) matching minimaxlower bound for estimation accuracy under spectral norm. We also give upperbounds and matching minimax lower bound(except some logarithmic terms) forestimation accuracy under Schatten-q norm for every $1\leq q\leq\infty$. As adirect corollary, we show both upper bounds and minimax lower bounds ofestimation accuracy under Ky-Fan-k norms for every $1\leq k\leq m$.
arxiv-1403-6508 | Multi-agent Inverse Reinforcement Learning for Zero-sum Games |  http://arxiv.org/abs/1403.6508  | author:Xiaomin Lin, Peter A. Beling, Randy Cogill category:cs.GT cs.AI cs.LG published:2014-03-25 summary:In this paper we introduce a Bayesian framework for solving a class ofproblems termed Multi-agent Inverse Reinforcement Learning (MIRL). Compared tothe well-known Inverse Reinforcement Learning (IRL) problem, MIRL is formalizedin the context of a stochastic game rather than a Markov decision process(MDP). Games bring two primary challenges: First, the concept of optimality,central to MDPs, loses its meaning and must be replaced with a more generalsolution concept, such as the Nash equilibrium. Second, the non-uniqueness ofequilibria means that in MIRL, in addition to multiple reasonable solutions fora given inversion model, there may be multiple inversion models that are allequally sensible approaches to solving the problem. We establish a theoreticalfoundation for competitive two-agent MIRL problems and propose a Bayesianoptimization algorithm to solve the problem. We focus on the case of two-personzero-sum stochastic games, developing a generative model for the likelihood ofunknown rewards of agents given observed game play assuming that the two agentsfollow a minimax bipolicy. As a numerical illustration, we apply our method inthe context of an abstract soccer game. For the soccer game, we investigaterelationships between the extent of prior information and the quality oflearned rewards. Results suggest that covariance structure is more importantthan mean value in reward priors.
arxiv-1403-6274 | Arguments for Nested Patterns in Neural Ensembles |  http://arxiv.org/abs/1403.6274  | author:Kieran Greer category:cs.NE q-bio.NC published:2014-03-25 summary:This paper describes a relatively simple way of allowing a brain model toself-organise its concept patterns through nested structures. Time is a keyelement and a simulator would be able to show how patterns may form and thenfire in sequence, as part of a search or thought process. It uses a very simpleequation to show how the inhibitors in particular, can switch off certainareas, to allow other areas to become the prominent ones and thereby define thecurrent brain state. This allows for a small amount of control over whatappears to be a chaotic structure inside of the brain. It is attractive becauseit is still mostly mechanical and therefore can be added as an automaticprocess, or the modelling of that. The paper also describes how the nestedpattern structure can be used as a basic counting mechanism.
arxiv-1403-6392 | Implementation of an Automatic Sign Language Lexical Annotation Framework based on Propositional Dynamic Logic |  http://arxiv.org/abs/1403.6392  | author:Arturo Curiel, Christophe Collet category:cs.CL I.2.7 published:2014-03-25 summary:In this paper, we present the implementation of an automatic Sign Language(SL) sign annotation framework based on a formal logic, the PropositionalDynamic Logic (PDL). Our system relies heavily on the use of a specific variantof PDL, the Propositional Dynamic Logic for Sign Language (PDLSL), which letsus describe SL signs as formulae and corpora videos as labeled transitionsystems (LTSs). Here, we intend to show how a generic annotation system can beconstructed upon these underlying theoretical principles, regardless of thetracking technologies available or the input format of corpora. With this inmind, we generated a development framework that adapts the system to specificuse cases. Furthermore, we present some results obtained by our applicationwhen adapted to one distinct case, 2D corpora analysis with pre-processedtracking information. We also present some insights on how such a technologycan be used to analyze 3D real-time data, captured with a depth device.
