arxiv-3900-1 | On statistics, computation and scalability | http://arxiv.org/pdf/1309.7804v1.pdf | author:Michael I. Jordan category:stat.ML cs.LG math.ST stat.TH published:2013-09-30 summary:How should statistical procedures be designed so as to be scalablecomputationally to the massive datasets that are increasingly the norm? Whencoupled with the requirement that an answer to an inferential question bedelivered within a certain time budget, this question has significantrepercussions for the field of statistics. With the goal of identifying"time-data tradeoffs," we investigate some of the statistical consequences ofcomputational perspectives on scability, in particular divide-and-conquermethodology and hierarchies of convex relaxations.
arxiv-3900-2 | Random walk kernels and learning curves for Gaussian process regression on random graphs | http://arxiv.org/pdf/1211.1328v2.pdf | author:Matthew Urry, Peter Sollich category:stat.ML cs.LG published:2012-11-06 summary:We consider learning on graphs, guided by kernels that encode similaritybetween vertices. Our focus is on random walk kernels, the analogues of squaredexponential kernels in Euclidean spaces. We show that on large, locallytreelike, graphs these have some counter-intuitive properties, specifically inthe limit of large kernel lengthscales. We consider using these kernels ascovariance matrices of e.g.\ Gaussian processes (GPs). In this situation onetypically scales the prior globally to normalise the average of the priorvariance across vertices. We demonstrate that, in contrast to the Euclideancase, this generically leads to significant variation in the prior varianceacross vertices, which is undesirable from the probabilistic modelling point ofview. We suggest the random walk kernel should be normalised locally, so thateach vertex has the same prior variance, and analyse the consequences of thisby studying learning curves for Gaussian process regression. Numericalcalculations as well as novel theoretical predictions for the learning curvesusing belief propagation make it clear that one obtains distinctly differentprobabilistic models depending on the choice of normalisation. Our method forpredicting the learning curves using belief propagation is significantly moreaccurate than previous approximations and should become exact in the limit oflarge random graphs.
arxiv-3900-3 | Generic Image Classification Approaches Excel on Face Recognition | http://arxiv.org/pdf/1309.5594v2.pdf | author:Fumin Shen, Chunhua Shen category:cs.CV published:2013-09-22 summary:The main finding of this work is that the standard image classificationpipeline, which consists of dictionary learning, feature encoding, spatialpyramid pooling and linear classification, outperforms all state-of-the-artface recognition methods on the tested benchmark datasets (we have tested onAR, Extended Yale B, the challenging FERET, and LFW-a datasets). Thissurprising and prominent result suggests that those advances in generic imageclassification can be directly applied to improve face recognition systems. Inother words, face recognition may not need to be viewed as a separate objectclassification problem. While recently a large body of residual based face recognition methods focuson developing complex dictionary learning algorithms, in this work we show thata dictionary of randomly extracted patches (even from non-face images) canachieve very promising results using the image classification pipeline. Thatmeans, the choice of dictionary learning methods may not be important. Instead,we find that learning multiple dictionaries using different low-level imagefeatures often improve the final classification accuracy. Our proposed facerecognition approach offers the best reported results on the widely-used facerecognition benchmark datasets. In particular, on the challenging FERET andLFW-a datasets, we improve the best reported accuracies in the literature byabout 20% and 30% respectively.
arxiv-3900-4 | Signed Networks, Triadic Interactions and the Evolution of Cooperation | http://arxiv.org/pdf/1309.7698v1.pdf | author:Simone Righi, Károly Takács category:cs.SI cs.GT cs.NE physics.soc-ph published:2013-09-30 summary:We outline a model to study the evolution of cooperation in a population ofagents playing the prisoner's dilemma in signed networks. We highlight that ifonly dyadic interactions are taken into account, cooperation never evolves.However, when triadic considerations are introduced, a window of opportunityfor emergence of cooperation as a stable behaviour emerges.
arxiv-3900-5 | Semi-structured data extraction and modelling: the WIA Project | http://arxiv.org/pdf/1309.7697v1.pdf | author:Gianluca Colombo, Ettore Colombo, Andrea Bonomi, Alessandro Mosca, Simone Bassis category:cs.SE cs.CY cs.NE H3; I.2; H.1.2 published:2013-09-30 summary:Over the last decades, the amount of data of all kinds availableelectronically has increased dramatically. Data are accessible through a rangeof interfaces including Web browsers, database query languages,application-specific interfaces, built on top of a number of different dataexchange formats. All these data span from un-structured to highly structureddata. Very often, some of them have structure even if the structure isimplicit, and not as rigid or regular as that found in standard databasesystems. Spreadsheet documents are prototypical in this respect. Spreadsheetsare the lightweight technology able to supply companies with easy to buildbusiness management and business intelligence applications, and business peoplelargely adopt spreadsheets as smart vehicles for data files generation andsharing. Actually, the more spreadsheets grow in complexity (e.g., their use inproduct development plans and quoting), the more their arrangement,maintenance, and analysis appear as a knowledge-driven activity. Thealgorithmic approach to the problem of automatic data structure extraction fromspreadsheet documents (i.e., grid-structured and free topological-related data)emerges from the WIA project: Worksheets Intelligent Analyser. TheWIA-algorithm shows how to provide a description of spreadsheet contents interms of higher level of abstractions or conceptualisations. In particular, theWIA-algorithm target is about the extraction of i) the calculus work-flowimplemented in the spreadsheets formulas and ii) the logical role played by thedata which take part into the calculus. The aim of the resultingconceptualisations is to provide spreadsheets with abstract representationsuseful for further model refinements and optimizations through evolutionaryalgorithms computations.
arxiv-3900-6 | A Hybrid Monte Carlo Ant Colony Optimization Approach for Protein Structure Prediction in the HP Model | http://arxiv.org/pdf/1309.7690v1.pdf | author:Andrea G. Citrolo, Giancarlo Mauri category:cs.NE cs.CE published:2013-09-30 summary:The hydrophobic-polar (HP) model has been widely studied in the field ofprotein structure prediction (PSP) both for theoretical purposes and as abenchmark for new optimization strategies. In this work we introduce a newheuristics based on Ant Colony Optimization (ACO) and Markov Chain Monte Carlo(MCMC) that we called Hybrid Monte Carlo Ant Colony Optimization (HMCACO). Wedescribe this method and compare results obtained on well known HP instances inthe 3 dimensional cubic lattice to those obtained with standard ACO andSimulated Annealing (SA). All methods were implemented using an unconstrainedneighborhood and a modified objective function to prevent the creation ofoverlapping walks. Results show that our methods perform better than the otherheuristics in all benchmark instances.
arxiv-3900-7 | An upper bound on prototype set size for condensed nearest neighbor | http://arxiv.org/pdf/1309.7676v1.pdf | author:Eric Christiansen category:cs.LG stat.ML published:2013-09-29 summary:The condensed nearest neighbor (CNN) algorithm is a heuristic for reducingthe number of prototypical points stored by a nearest neighbor classifier,while keeping the classification rule given by the reduced prototypical setconsistent with the full set. I present an upper bound on the number ofprototypical points accumulated by CNN. The bound originates in a bound on thenumber of times the decision rule is updated during training in the multiclassperceptron algorithm, and thus is independent of training set size.
arxiv-3900-8 | Error AMP Chain Graphs | http://arxiv.org/pdf/1306.6843v2.pdf | author:Jose M. Peña category:stat.ML cs.AI published:2013-06-28 summary:Any regular Gaussian probability distribution that can be represented by anAMP chain graph (CG) can be expressed as a system of linear equations withcorrelated errors whose structure depends on the CG. However, the CG representsthe errors implicitly, as no nodes in the CG correspond to the errors. Wepropose in this paper to add some deterministic nodes to the CG in order torepresent the errors explicitly. We call the result an EAMP CG. We will showthat, as desired, every AMP CG is Markov equivalent to its corresponding EAMPCG under marginalization of the error nodes. We will also show that every EAMPCG under marginalization of the error nodes is Markov equivalent to some LWF CGunder marginalization of the error nodes, and that the latter is Markovequivalent to some directed and acyclic graph (DAG) under marginalization ofthe error nodes and conditioning on some selection nodes. This is importantbecause it implies that the independence model represented by an AMP CG can beaccounted for by some data generating process that is partially observed andhas selection bias. Finally, we will show that EAMP CGs are closed undermarginalization. This is a desirable feature because it guarantees parsimoniousmodels under marginalization.
arxiv-3900-9 | Correcting Multi-focus Images via Simple Standard Deviation for Image Fusion | http://arxiv.org/pdf/1309.7615v1.pdf | author:Firas A. Jassim category:cs.CV published:2013-09-29 summary:Image fusion is one of the recent trends in image registration which is anessential field of image processing. The basic principle of this paper is tofuse multi-focus images using simple statistical standard deviation. Firstly,the simple standard deviation for the k-by-k window inside each of themulti-focus images was computed. The contribution in this paper came from theidea that the focused part inside an image had high details rather than theunfocused part. Hence, the dispersion between pixels inside the focused part ishigher than the dispersion inside the unfocused part. Secondly, a simplecomparison between the standard deviation for each k-by-k window in themulti-focus images could be computed. The highest standard deviation betweenall the computed standard deviations for the multi-focus images could betreated as the optimal that is to be placed in the fused image. Theexperimental visual results show that the proposed method produces verysatisfactory results in spite of its simplicity.
arxiv-3900-10 | Fast Marginalized Block Sparse Bayesian Learning Algorithm | http://arxiv.org/pdf/1211.4909v7.pdf | author:Benyuan Liu, Zhilin Zhang, Hongqi Fan, Qiang Fu category:cs.IT cs.LG math.IT stat.ML published:2012-11-21 summary:The performance of sparse signal recovery from noise corrupted,underdetermined measurements can be improved if both sparsity and correlationstructure of signals are exploited. One typical correlation structure is theintra-block correlation in block sparse signals. To exploit this structure, aframework, called block sparse Bayesian learning (BSBL), has been proposedrecently. Algorithms derived from this framework showed superior performancebut they are not very fast, which limits their applications. This work derivesan efficient algorithm from this framework, using a marginalized likelihoodmaximization method. Compared to existing BSBL algorithms, it has closerecovery performance but is much faster. Therefore, it is more suitable forlarge scale datasets and applications requiring real-time implementation.
arxiv-3900-11 | Context-aware recommendations from implicit data via scalable tensor factorization | http://arxiv.org/pdf/1309.7611v1.pdf | author:Balázs Hidasi, Domonkos Tikk category:cs.LG cs.IR published:2013-09-29 summary:Albeit the implicit feedback based recommendation problem - when only theuser history is available but there are no ratings - is the most typicalsetting in real-world applications, it is much less researched than theexplicit feedback case. State-of-the-art algorithms that are efficient on theexplicit case cannot be automatically transformed to the implicit case ifscalability should be maintained. There are few implicit feedback benchmarkdata sets, therefore new ideas are usually experimented on explicit benchmarks.In this paper, we propose a generic context-aware implicit feedback recommenderalgorithm, coined iTALS. iTALS applies a fast, ALS-based tensor factorizationlearning method that scales linearly with the number of non-zero elements inthe tensor. We also present two approximate and faster variants of iTALS usingcoordinate descent and conjugate gradient methods at learning. The method alsoallows us to incorporate various contextual information into the model whilemaintaining its computational efficiency. We present two context-aware variantsof iTALS incorporating seasonality and item purchase sequentiality into themodel to distinguish user behavior at different time intervals, and producttypes with different repetitiveness. Experiments run on six data sets showsthat iTALS clearly outperforms context-unaware models and context awarebaselines, while it is on par with factorization machines (beats 7 times out of12 cases) both in terms of recall and MAP.
arxiv-3900-12 | Identificación y Registro Catastral de Cuerpos de Agua mediante Técnicas de Procesamiento Digital de Imagenes | http://arxiv.org/pdf/1309.7609v1.pdf | author:Kevin Rojas Laura, Christhian Cardenas Alvarez category:cs.CV published:2013-09-29 summary:The effects of global climate change on Peruvian glaciers have brought aboutseveral processes of deglaciation during the last few years. The immediateeffect is the change of size of lakes and rivers. Public institutions thatmonitor water resources currently have only recent studies which make up lessthan 10% of the total. The effects of climate change and the lack of updatedinformation intensify social-economic problems related to water resources inPeru. The objective of this research is to develop a software application toautomate the Cadastral Registry of Water Bodies in Peru, using techniques ofdigital image processing, which would provide tools for detection, record,temporal analysis and visualization of water bodies. The images used are fromthe satellite Landsat5, which undergo a pre-processing of calibration andcorrection of the satellite. Detection results are archived into a file thatcontains location vectors and images of the segmentated bodies of water.
arxiv-3900-13 | On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations | http://arxiv.org/pdf/1309.7598v1.pdf | author:Tamir Hazan, Subhransu Maji, Tommi Jaakkola category:cs.LG published:2013-09-29 summary:In this paper we describe how MAP inference can be used to sample efficientlyfrom Gibbs distributions. Specifically, we provide means for drawing eitherapproximate or unbiased samples from Gibbs' distributions by introducing lowdimensional perturbations and solving the corresponding MAP assignments. Ourapproach also leads to new ways to derive lower bounds on partition functions.We demonstrate empirically that our method excels in the typical "high signal -high coupling" regime. The setting results in ragged energy landscapes that arechallenging for alternative approaches to sampling and/or lower bounds.
arxiv-3900-14 | Meme and Variations: A Computer Model of Cultural Evolution | http://arxiv.org/pdf/1309.7524v1.pdf | author:Liane Gabora category:cs.MA cs.NE published:2013-09-29 summary:Holland's (1975) genetic algorithm is a minimal computer model of naturalselection that made it possible to investigate the effect of manipulatingspecific parameters on the evolutionary process. If culture is, like biology, aform of evolution, it should be possible to similarly abstract the underlyingskeleton of the process and develop a minimal model of it. Meme and Variations,or MAV, is a computational model, inspired by the genetic algorithm, of howideas evolve in a society of interacting individuals (Gabora 1995). The name isa pun on the classical music form 'theme and variations', because it is basedon the premise that novel ideas are variations of old ones; they result fromtweaking or combining existing ideas in new ways (Holland et al. 1981). MAVexplores the impact of biological phenomena such as over-dominance andepistasis as well as cognitive and social phenomena such as the ability tolearn generalizations or imitate others on the fitness and diversity ofcultural transmissible actions.
arxiv-3900-15 | An Application of Backpropagation Artificial Neural Network Method for Measuring The Severity of Osteoarthritis | http://arxiv.org/pdf/1309.7522v1.pdf | author:Dian Pratiwi, Diaz D. Santika, Bens Pardamean category:cs.NE cs.CE cs.CV published:2013-09-29 summary:The examination of Osteoarthritis disease through X-ray by rheumatology canbe classified into four grade of severity. This paper discusses about theapplication of artificial neural network backpropagation method for measuringthe severity of the disease, where the observed X-ray range from wrist tofingers. The main procedures of system in this paper is divided into three,which are image processing, feature extraction, and artificial neural networkprocess. First, an X-ray image digital (200x150 pixels and greyscale) will bethresholded, then extracted features based on probabilistic values of the colorintensity of seven bit quantization result, and statistical textures. Thatfeature values then will be normalizing to interval [0.1, 0.9], and then theresult would be processing on backpropagation artificial neural network systemas input to determine the severity of disease from an X-ray had input beforeit. From testing with learning rate 0.3, momentum 0.4, hidden units five piecesand about 132 feature vectors, this system had had a level of accuracy of 100%for learning data, 80% for learning and non-learning data, and 66.6% fornon-learning data
arxiv-3900-16 | CSIFT Based Locality-constrained Linear Coding for Image Classification | http://arxiv.org/pdf/1309.7484v1.pdf | author:Chen Junzhou, Li Qing, Peng Qiang, Kin Hong Wong category:cs.CV published:2013-09-28 summary:In the past decade, SIFT descriptor has been witnessed as one of the mostrobust local invariant feature descriptors and widely used in various visiontasks. Most traditional image classification systems depend on theluminance-based SIFT descriptors, which only analyze the gray level variationsof the images. Misclassification may happen since their color contents areignored. In this article, we concentrate on improving the performance ofexisting image classification algorithms by adding color information. Toachieve this purpose, different kinds of colored SIFT descriptors areintroduced and implemented. Locality-constrained Linear Coding (LLC), astate-of-the-art sparse coding technology, is employed to construct the imageclassification system for the evaluation. The real experiments are carried outon several benchmarks. With the enhancements of color SIFT, the proposed imageclassification system obtains approximate 3% improvement of classificationaccuracy on the Caltech-101 dataset and approximate 4% improvement ofclassification accuracy on the Caltech-256 dataset.
arxiv-3900-17 | Optimal Hybrid Channel Allocation:Based On Machine Learning Algorithms | http://arxiv.org/pdf/1309.7439v1.pdf | author:K Viswanadh, Dr. G Rama Murthy category:cs.NI cs.LG published:2013-09-28 summary:Recent advances in cellular communication systems resulted in a huge increasein spectrum demand. To meet the requirements of the ever-growing need forspectrum, efficient utilization of the existing resources is of utmostimportance. Channel Allocation, has thus become an inevitable research topic inwireless communications. In this paper, we propose an optimal channelallocation scheme, Optimal Hybrid Channel Allocation (OHCA) for an effectiveallocation of channels. We improvise upon the existing Fixed Channel Allocation(FCA) technique by imparting intelligence to the existing system by employingthe multilayer perceptron technique.
arxiv-3900-18 | Face Verification Using Boosted Cross-Image Features | http://arxiv.org/pdf/1309.7434v1.pdf | author:Dong Zhang, Omar Oreifej, Mubarak Shah category:cs.CV published:2013-09-28 summary:This paper proposes a new approach for face verification, where a pair ofimages needs to be classified as belonging to the same person or not. Thisproblem is relatively new and not well-explored in the literature. Currentmethods mostly adopt techniques borrowed from face recognition, and processeach of the images in the pair independently, which is counter intuitive. Incontrast, we propose to extract cross-image features, i.e. features across thepair of images, which, as we demonstrate, is more discriminative to thesimilarity and the dissimilarity of faces. Our features are derived from thepopular Haar-like features, however, extended to handle the face verificationproblem instead of face detection. We collect a large bank of cross-imagefeatures using filters of different sizes, locations, and orientations.Consequently, we use AdaBoost to select and weight the most discriminativefeatures. We carried out extensive experiments on the proposed ideas usingthree standard face verification datasets, and obtained promising resultsoutperforming state-of-the-art.
arxiv-3900-19 | Solving OSCAR regularization problems by proximal splitting algorithms | http://arxiv.org/pdf/1309.6301v2.pdf | author:Xiangrong Zeng, Mário A. T. Figueiredo category:cs.CV cs.LG stat.ML published:2013-09-24 summary:The OSCAR (octagonal selection and clustering algorithm for regression)regularizer consists of a L_1 norm plus a pair-wise L_inf norm (responsible forits grouping behavior) and was proposed to encourage group sparsity inscenarios where the groups are a priori unknown. The OSCAR regularizer has anon-trivial proximity operator, which limits its applicability. We reformulatethis regularizer as a weighted sorted L_1 norm, and propose its groupingproximity operator (GPO) and approximate proximity operator (APO), thus makingstate-of-the-art proximal splitting algorithms (PSAs) available to solveinverse problems with OSCAR regularization. The GPO is in fact the APO followedby additional grouping and averaging operations, which are costly in time andstorage, explaining the reason why algorithms with APO are much faster thanthat with GPO. The convergences of PSAs with GPO are guaranteed since GPO is anexact proximity operator. Although convergence of PSAs with APO is may not beguaranteed, we have experimentally found that APO behaves similarly to GPO whenthe regularization parameter of the pair-wise L_inf norm is set to anappropriately small value. Experiments on recovery of group-sparse signals(with unknown groups) show that PSAs with APO are very fast and accurate.
arxiv-3900-20 | Development and Transcription of Assamese Speech Corpus | http://arxiv.org/pdf/1309.7312v1.pdf | author:Himangshu Sarma, Navanath Saharia, Utpal Sharma, Smriti Kumar Sinha, Mancha Jyoti Malakar category:cs.CL published:2013-09-27 summary:A balanced speech corpus is the basic need for any speech processing task. Inthis report we describe our effort on development of Assamese speech corpus. Wemainly focused on some issues and challenges faced during development of thecorpus. Being a less computationally aware language, this is the first effortto develop speech corpus for Assamese. As corpus development is an ongoingprocess, in this paper we report only the initial task.
arxiv-3900-21 | Bayesian Inference in Sparse Gaussian Graphical Models | http://arxiv.org/pdf/1309.7311v1.pdf | author:Peter Orchard, Felix Agakov, Amos Storkey category:stat.ML cs.LG published:2013-09-27 summary:One of the fundamental tasks of science is to find explainable relationshipsbetween observed phenomena. One approach to this task that has receivedattention in recent years is based on probabilistic graphical modelling withsparsity constraints on model structures. In this paper, we describe two newapproaches to Bayesian inference of sparse structures of Gaussian graphicalmodels (GGMs). One is based on a simple modification of the cutting-edge blockGibbs sampler for sparse GGMs, which results in significant computational gainsin high dimensions. The other method is based on a specific construction of theHamiltonian Monte Carlo sampler, which results in further significantimprovements. We compare our fully Bayesian approaches with the popularregularisation-based graphical LASSO, and demonstrate significant advantages ofthe Bayesian treatment under the same computing costs. We apply the methods toa broad range of simulated data sets, and a real-life financial data set.
arxiv-3900-22 | Order-independent constraint-based causal structure learning | http://arxiv.org/pdf/1211.3295v2.pdf | author:Diego Colombo, Marloes H. Maathuis category:stat.ML cs.LG published:2012-11-14 summary:We consider constraint-based methods for causal structure learning, such asthe PC-, FCI-, RFCI- and CCD- algorithms (Spirtes et al. (2000, 1993),Richardson (1996), Colombo et al. (2012), Claassen et al. (2013)). The firststep of all these algorithms consists of the PC-algorithm. This algorithm isknown to be order-dependent, in the sense that the output can depend on theorder in which the variables are given. This order-dependence is a minor issuein low-dimensional settings. We show, however, that it can be very pronouncedin high-dimensional settings, where it can lead to highly variable results. Wepropose several modifications of the PC-algorithm (and hence also of the otheralgorithms) that remove part or all of this order-dependence. All proposedmodifications are consistent in high-dimensional settings under the sameconditions as their original counterparts. We compare the PC-, FCI-, andRFCI-algorithms and their modifications in simulation studies and on a yeastgene expression data set. We show that our modifications yield similarperformance in low-dimensional settings and improved performance inhigh-dimensional settings. All software is implemented in the R-package pcalg.
arxiv-3900-23 | Evaluating the Usefulness of Sentiment Information for Focused Crawlers | http://arxiv.org/pdf/1309.7270v1.pdf | author:Tianjun Fu, Ahmed Abbasi, Daniel Zeng, Hsinchun Chen category:cs.IR cs.CL published:2013-09-27 summary:Despite the prevalence of sentiment-related content on the Web, there hasbeen limited work on focused crawlers capable of effectively collecting suchcontent. In this study, we evaluated the efficacy of using sentiment-relatedinformation for enhanced focused crawling of opinion-rich web content regardinga particular topic. We also assessed the impact of using sentiment-labeled webgraphs to further improve collection accuracy. Experimental results on a largetest bed encompassing over half a million web pages revealed that focusedcrawlers utilizing sentiment information as well as sentiment-labeled webgraphs are capable of gathering more holistic collections of opinion-relatedcontent regarding a particular topic. The results have important implicationsfor business and marketing intelligence gathering efforts in the Web 2.0 era.
arxiv-3900-24 | Evaluating Link-Based Techniques for Detecting Fake Pharmacy Websites | http://arxiv.org/pdf/1309.7266v1.pdf | author:Ahmed Abbasi, Siddharth Kaza, F. Mariam Zahedi category:cs.CY cs.LG published:2013-09-27 summary:Fake online pharmacies have become increasingly pervasive, constituting over90% of online pharmacy websites. There is a need for fake website detectiontechniques capable of identifying fake online pharmacy websites with a highdegree of accuracy. In this study, we compared several well-known link-baseddetection techniques on a large-scale test bed with the hyperlink graphencompassing over 80 million links between 15.5 million web pages, including1.2 million known legitimate and fake pharmacy pages. We found that the QoC andQoL class propagation algorithms achieved an accuracy of over 90% on ourdataset. The results revealed that algorithms that incorporate dual classpropagation as well as inlink and outlink information, on page-level orsite-level graphs, are better suited for detecting fake pharmacy websites. Inaddition, site-level analysis yielded significantly better results thanpage-level analysis for most algorithms evaluated.
arxiv-3900-25 | A Statistical Learning Based System for Fake Website Detection | http://arxiv.org/pdf/1309.7958v1.pdf | author:Ahmed Abbasi, Zhu Zhang, Hsinchun Chen category:cs.CY cs.LG published:2013-09-27 summary:Existing fake website detection systems are unable to effectively detect fakewebsites. In this study, we advocate the development of fake website detectionsystems that employ classification methods grounded in statistical learningtheory (SLT). Experimental results reveal that a prototype system developedusing SLT-based methods outperforms seven existing fake website detectionsystems on a test bed encompassing 900 real and fake websites.
arxiv-3900-26 | Detecting Fake Escrow Websites using Rich Fraud Cues and Kernel Based Methods | http://arxiv.org/pdf/1309.7261v1.pdf | author:Ahmed Abbasi, Hsinchun Chen category:cs.CY cs.LG published:2013-09-27 summary:The ability to automatically detect fraudulent escrow websites is importantin order to alleviate online auction fraud. Despite research on related topics,fake escrow website categorization has received little attention. In this studywe evaluated the effectiveness of various features and techniques for detectingfake escrow websites. Our analysis included a rich set of features extractedfrom web page text, image, and link information. We also proposed a compositekernel tailored to represent the properties of fake websites, including contentduplication and structural attributes. Experiments were conducted to assess theproposed features, techniques, and kernels on a test bed encompassing nearly90,000 web pages derived from 410 legitimate and fake escrow sites. Thecombination of an extended feature set and the composite kernel attained over98% accuracy when differentiating fake sites from real ones, using the supportvector machines algorithm. The results suggest that automated web-basedinformation systems for detecting fake escrow sites could be feasible and maybe utilized as authentication mechanisms.
arxiv-3900-27 | Gaussian Processes for Nonlinear Signal Processing | http://arxiv.org/pdf/1303.2823v2.pdf | author:Fernando Pérez-Cruz, Steven Van Vaerenbergh, Juan José Murillo-Fuentes, Miguel Lázaro-Gredilla, Ignacio Santamaria category:cs.LG cs.IT math.IT stat.ML published:2013-03-12 summary:Gaussian processes (GPs) are versatile tools that have been successfullyemployed to solve nonlinear estimation problems in machine learning, but thatare rarely used in signal processing. In this tutorial, we present GPs forregression as a natural nonlinear extension to optimal Wiener filtering. Afterestablishing their basic formulation, we discuss several important aspects andextensions, including recursive and adaptive algorithms for dealing withnon-stationarity, low-complexity solutions, non-Gaussian noise models andclassification scenarios. Furthermore, we provide a selection of relevantapplications to wireless digital communications.
arxiv-3900-28 | An Efficient Index for Visual Search in Appearance-based SLAM | http://arxiv.org/pdf/1309.7170v1.pdf | author:Kiana Hajebi, Hong Zhang category:cs.CV cs.RO published:2013-09-27 summary:Vector-quantization can be a computationally expensive step in visualbag-of-words (BoW) search when the vocabulary is large. A BoW-based appearanceSLAM needs to tackle this problem for an efficient real-time operation. Wepropose an effective method to speed up the vector-quantization process inBoW-based visual SLAM. We employ a graph-based nearest neighbor search (GNNS)algorithm to this aim, and experimentally show that it can outperform thestate-of-the-art. The graph-based search structure used in GNNS can efficientlybe integrated into the BoW model and the SLAM framework. The graph-based index,which is a k-NN graph, is built over the vocabulary words and can be extractedfrom the BoW's vocabulary construction procedure, by adding one iteration tothe k-means clustering, which adds small extra cost. Moreover, exploiting thefact that images acquired for appearance-based SLAM are sequential, GNNS searchcan be initiated judiciously which helps increase the speedup of thequantization process considerably.
arxiv-3900-29 | Diffusion map for clustering fMRI spatial maps extracted by independent component analysis | http://arxiv.org/pdf/1306.1350v4.pdf | author:Tuomo Sipola, Fengyu Cong, Tapani Ristaniemi, Vinoo Alluri, Petri Toiviainen, Elvira Brattico, Asoke K. Nandi category:cs.CE cs.LG stat.ML published:2013-06-06 summary:Functional magnetic resonance imaging (fMRI) produces data about activityinside the brain, from which spatial maps can be extracted by independentcomponent analysis (ICA). In datasets, there are n spatial maps that contain pvoxels. The number of voxels is very high compared to the number of analyzedspatial maps. Clustering of the spatial maps is usually based on correlationmatrices. This usually works well, although such a similarity matrix inherentlycan explain only a certain amount of the total variance contained in thehigh-dimensional data where n is relatively small but p is large. Forhigh-dimensional space, it is reasonable to perform dimensionality reductionbefore clustering. In this research, we used the recently developed diffusionmap for dimensionality reduction in conjunction with spectral clustering. Thisresearch revealed that the diffusion map based clustering worked as well as themore traditional methods, and produced more compact clusters when needed.
arxiv-3900-30 | Proceedings Wivace 2013 - Italian Workshop on Artificial Life and Evolutionary Computation | http://arxiv.org/pdf/1309.7122v1.pdf | author:Alex Graudenzi, Giulio Caravagna, Giancarlo Mauri, Marco Antoniotti category:cs.CE cs.NE published:2013-09-27 summary:The Wivace 2013 Electronic Proceedings in Theoretical Computer Science(EPTCS) contain some selected long and short articles accepted for thepresentation at Wivace 2013 - Italian Workshop on Artificial Life andEvolutionary Computation, which was held at the University of Milan-Bicocca,Milan, on the 1st and 2nd of July, 2013.
arxiv-3900-31 | Market Index and Stock Price Direction Prediction using Machine Learning Techniques: An empirical study on the KOSPI and HSI | http://arxiv.org/pdf/1309.7119v1.pdf | author:Yanshan Wang, In-Chan Choi category:cs.CE cs.LG q-fin.ST published:2013-09-27 summary:The prediction of a stock market direction may serve as an earlyrecommendation system for short-term investors and as an early financialdistress warning system for long-term shareholders. In this paper, we proposean empirical study on the Korean and Hong Kong stock market with an integratedmachine learning framework that employs Principal Component Analysis (PCA) andSupport Vector Machine (SVM). We try to predict the upward or downwarddirection of stock market index and stock price. In the proposed framework,PCA, as a feature selection method, identifies principal components in thestock market movement and SVM, as a classifier for future stock marketmovement, processes them along with other economic factors in training andforecasting. We present the results of an extensive empirical study of theproposed method on the Korean composite stock price index (KOSPI) and Hangsengindex (HSI), as well as the individual constituents included in the indices. Inour experiment, ten years data (from January 1st, 2002 to January 1st, 2012)are collected and schemed by rolling windows to predict one-day-aheaddirections. The experimental results show notably high hit ratios in predictingthe movements of the individual constituents in the KOSPI and HSI. The resultsalso varify the \textit{co-movement} effect between the Korean (Hong Kong)stock market and the American stock market.
arxiv-3900-32 | Estimating Undirected Graphs Under Weak Assumptions | http://arxiv.org/pdf/1309.6933v1.pdf | author:Larry Wasserman, Mladen Kolar, Alessandro Rinaldo category:math.ST cs.LG stat.ML stat.TH 62H12 published:2013-09-26 summary:We consider the problem of providing nonparametric confidence guarantees forundirected graphs under weak assumptions. In particular, we do not assumesparsity, incoherence or Normality. We allow the dimension $D$ to increase withthe sample size $n$. First, we prove lower bounds that show that if we wantaccurate inferences with low assumptions then there are limitations on thedimension as a function of sample size. When the dimension increases slowlywith sample size, we show that methods based on Normal approximations and onthe bootstrap lead to valid inferences and we provide Berry-Esseen bounds onthe accuracy of the Normal approximation. When the dimension is large relativeto sample size, accurate inferences for graphs under low assumptions are notpossible. Instead we propose to estimate something less demanding than theentire partial correlation graph. In particular, we consider: cluster graphs,restricted partial correlation graphs and correlation graphs.
arxiv-3900-33 | On the Feature Discovery for App Usage Prediction in Smartphones | http://arxiv.org/pdf/1309.7982v1.pdf | author:Zhung-Xun Liao, Shou-Chung Li, Wen-Chih Peng, Philip S Yu category:cs.LG published:2013-09-26 summary:With the increasing number of mobile Apps developed, they are now closelyintegrated into daily life. In this paper, we develop a framework to predictmobile Apps that are most likely to be used regarding the current device statusof a smartphone. Such an Apps usage prediction framework is a crucialprerequisite for fast App launching, intelligent user experience, and powermanagement of smartphones. By analyzing real App usage log data, we discovertwo kinds of features: The Explicit Feature (EF) from sensing readings ofbuilt-in sensors, and the Implicit Feature (IF) from App usage relations. TheIF feature is derived by constructing the proposed App Usage Graph (abbreviatedas AUG) that models App usage transitions. In light of AUG, we are able todiscover usage relations among Apps. Since users may have different usagebehaviors on their smartphones, we further propose one personalized featureselection algorithm. We explore minimum description length (MDL) from thetraining data and select those features which need less length to describe thetraining data. The personalized feature selection can successfully reduce thelog size and the prediction time. Finally, we adopt the kNN classificationmodel to predict Apps usage. Note that through the features selected by theproposed personalized feature selection algorithm, we only need to keep thesefeatures, which in turn reduces the prediction time and avoids the curse ofdimensionality when using the kNN classifier. We conduct a comprehensiveexperimental study based on a real mobile App usage dataset. The resultsdemonstrate the effectiveness of the proposed framework and show the predictivecapability for App usage prediction.
arxiv-3900-34 | Bennett-type Generalization Bounds: Large-deviation Case and Faster Rate of Convergence | http://arxiv.org/pdf/1309.6876v1.pdf | author:Chao Zhang category:stat.ML cs.LG published:2013-09-26 summary:In this paper, we present the Bennett-type generalization bounds of thelearning process for i.i.d. samples, and then show that the generalizationbounds have a faster rate of convergence than the traditional results. Inparticular, we first develop two types of Bennett-type deviation inequality forthe i.i.d. learning process: one provides the generalization bounds based onthe uniform entropy number; the other leads to the bounds based on theRademacher complexity. We then adopt a new method to obtain the alternativeexpressions of the Bennett-type generalization bounds, which imply that thebounds have a faster rate o(N^{-1/2}) of convergence than the traditionalresults O(N^{-1/2}). Additionally, we find that the rate of the bounds willbecome faster in the large-deviation case, which refers to a situation wherethe empirical risk is far away from (at least not close to) the expected risk.Finally, we analyze the asymptotical convergence of the learning process andcompare our analysis with the existing results.
arxiv-3900-35 | Active Learning with Expert Advice | http://arxiv.org/pdf/1309.6875v1.pdf | author:Peilin Zhao, Steven Hoi, Jinfeng Zhuang category:cs.LG stat.ML published:2013-09-26 summary:Conventional learning with expert advice methods assumes a learner is alwaysreceiving the outcome (e.g., class labels) of every incoming training instanceat the end of each trial. In real applications, acquiring the outcome fromoracle can be costly or time consuming. In this paper, we address a new problemof active learning with expert advice, where the outcome of an instance isdisclosed only when it is requested by the online learner. Our goal is to learnan accurate prediction model by asking the oracle the number of questions assmall as possible. To address this challenge, we propose a framework of activeforecasters for online active learning with expert advice, which attempts toextend two regular forecasters, i.e., Exponentially Weighted Average Forecasterand Greedy Forecaster, to tackle the task of active learning with expertadvice. We prove that the proposed algorithms satisfy the Hannan consistencyunder some proper assumptions, and validate the efficacy of our technique by anextensive set of experiments.
arxiv-3900-36 | Integrating Document Clustering and Topic Modeling | http://arxiv.org/pdf/1309.6874v1.pdf | author:Pengtao Xie, Eric P. Xing category:cs.LG cs.CL cs.IR stat.ML published:2013-09-26 summary:Document clustering and topic modeling are two closely related tasks whichcan mutually benefit each other. Topic modeling can project documents into atopic space which facilitates effective document clustering. Cluster labelsdiscovered by document clustering can be incorporated into topic models toextract local topics specific to each cluster and global topics shared by allclusters. In this paper, we propose a multi-grain clustering topic model(MGCTM) which integrates document clustering and topic modeling into a unifiedframework and jointly performs the two tasks to achieve the overall bestperformance. Our model tightly couples two components: a mixture component usedfor discovering latent groups in document collection and a topic modelcomponent used for mining multi-grain topics including local topics specific toeach cluster and global topics shared across clusters.We employ variationalinference to approximate the posterior of hidden variables and learn modelparameters. Experiments on two datasets demonstrate the effectiveness of ourmodel.
arxiv-3900-37 | Finite-Time Analysis of Kernelised Contextual Bandits | http://arxiv.org/pdf/1309.6869v1.pdf | author:Michal Valko, Nathaniel Korda, Remi Munos, Ilias Flaounas, Nelo Cristianini category:cs.LG stat.ML published:2013-09-26 summary:We tackle the problem of online reward maximisation over a large finite setof actions described by their contexts. We focus on the case when the number ofactions is too big to sample all of them even once. However we assume that wehave access to the similarities between actions' contexts and that the expectedreward is an arbitrary linear function of the contexts' images in the relatedreproducing kernel Hilbert space (RKHS). We propose KernelUCB, a kernelised UCBalgorithm, and give a cumulative regret bound through a frequentist analysis.For contextual bandits, the related algorithm GP-UCB turns out to be a specialcase of our algorithm, and our finite-time analysis improves the regret boundof GP-UCB for the agnostic case, both in the terms of the kernel-dependentquantity and the RKHS norm of the reward function. Moreover, for the linearkernel, our regret bound matches the lower bound for contextual linear bandits.
arxiv-3900-38 | Approximate Kalman Filter Q-Learning for Continuous State-Space MDPs | http://arxiv.org/pdf/1309.6868v1.pdf | author:Charles Tripp, Ross D. Shachter category:cs.LG stat.ML published:2013-09-26 summary:We seek to learn an effective policy for a Markov Decision Process (MDP) withcontinuous states via Q-Learning. Given a set of basis functions over stateaction pairs we search for a corresponding set of linear weights that minimizesthe mean Bellman residual. Our algorithm uses a Kalman filter model to estimatethose weights and we have developed a simpler approximate Kalman filter modelthat outperforms the current state of the art projected TD-Learning methods onseveral standard benchmark problems.
arxiv-3900-39 | Speedy Model Selection (SMS) for Copula Models | http://arxiv.org/pdf/1309.6867v1.pdf | author:Yaniv Tenzer, Gal Elidan category:cs.LG stat.ME published:2013-09-26 summary:We tackle the challenge of efficiently learning the structure of expressivemultivariate real-valued densities of copula graphical models. We start bytheoretically substantiating the conjecture that for many copula families themagnitude of Spearman's rank correlation coefficient is monotone in theexpected contribution of an edge in network, namely the negative copulaentropy. We then build on this theory and suggest a novel Bayesian approachthat makes use of a prior over values of Spearman's rho for learningcopula-based models that involve a mix of copula families. We demonstrate thegeneralization effectiveness of our highly efficient approach on sizable andvaried real-life datasets.
arxiv-3900-40 | Modeling Documents with Deep Boltzmann Machines | http://arxiv.org/pdf/1309.6865v1.pdf | author:Nitish Srivastava, Ruslan R Salakhutdinov, Geoffrey E. Hinton category:cs.LG cs.IR stat.ML published:2013-09-26 summary:We introduce a Deep Boltzmann Machine model suitable for modeling andextracting latent semantic representations from a large unstructured collectionof documents. We overcome the apparent difficulty of training a DBM withjudicious parameter tying. This parameter tying enables an efficientpretraining algorithm and a state initialization scheme that aids inference.The model can be trained just as efficiently as a standard Restricted BoltzmannMachine. Our experiments show that the model assigns better log probability tounseen data than the Replicated Softmax model. Features extracted from ourmodel outperform LDA, Replicated Softmax, and DocNADE models on documentretrieval and document classification tasks.
arxiv-3900-41 | Sparse Nested Markov models with Log-linear Parameters | http://arxiv.org/pdf/1309.6863v1.pdf | author:Ilya Shpitser, Robin J. Evans, Thomas S. Richardson, James M. Robins category:cs.LG cs.AI stat.ML published:2013-09-26 summary:Hidden variables are ubiquitous in practical data analysis, and thereforemodeling marginal densities and doing inference with the resulting models is animportant problem in statistics, machine learning, and causal inference.Recently, a new type of graphical model, called the nested Markov model, wasdeveloped which captures equality constraints found in marginals of directedacyclic graph (DAG) models. Some of these constraints, such as the so called`Verma constraint', strictly generalize conditional independence. To makemodeling and inference with nested Markov models practical, it is necessary tolimit the number of parameters in the model, while still correctly capturingthe constraints in the marginal of a DAG model. Placing such limits is similarin spirit to sparsity methods for undirected graphical models, and regressionmodels. In this paper, we give a log-linear parameterization which allowssparse modeling with nested Markov models. We illustrate the advantages of thisparameterization with a simulation study.
arxiv-3900-42 | Determinantal Clustering Processes - A Nonparametric Bayesian Approach to Kernel Based Semi-Supervised Clustering | http://arxiv.org/pdf/1309.6862v1.pdf | author:Amar Shah, Zoubin Ghahramani category:cs.LG stat.ML published:2013-09-26 summary:Semi-supervised clustering is the task of clustering data points intoclusters where only a fraction of the points are labelled. The true number ofclusters in the data is often unknown and most models require this parameter asan input. Dirichlet process mixture models are appealing as they can infer thenumber of clusters from the data. However, these models do not deal with highdimensional data well and can encounter difficulties in inference. We present anovel nonparameteric Bayesian kernel based method to cluster data pointswithout the need to prespecify the number of clusters or to model complicateddensities from which data points are assumed to be generated from. The keyinsight is to use determinants of submatrices of a kernel matrix as a measureof how close together a set of points are. We explore some theoreticalproperties of the model and derive a natural Gibbs based algorithm with MCMChyperparameter learning. The model is implemented on a variety of synthetic andreal world data sets.
arxiv-3900-43 | Identifying Finite Mixtures of Nonparametric Product Distributions and Causal Inference of Confounders | http://arxiv.org/pdf/1309.6860v1.pdf | author:Eleni Sgouritsa, Dominik Janzing, Jonas Peters, Bernhard Schoelkopf category:cs.LG cs.AI stat.ML published:2013-09-26 summary:We propose a kernel method to identify finite mixtures of nonparametricproduct distributions. It is based on a Hilbert space embedding of the jointdistribution. The rank of the constructed tensor is equal to the number ofmixture components. We present an algorithm to recover the components bypartitioning the data points into clusters such that the variables are jointlyconditionally independent given the cluster. This method can be used toidentify finite confounders.
arxiv-3900-44 | The Supervised IBP: Neighbourhood Preserving Infinite Latent Feature Models | http://arxiv.org/pdf/1309.6858v1.pdf | author:Novi Quadrianto, Viktoriia Sharmanska, David A. Knowles, Zoubin Ghahramani category:cs.LG stat.ML published:2013-09-26 summary:We propose a probabilistic model to infer supervised latent variables in theHamming space from observed data. Our model allows simultaneous inference ofthe number of binary latent variables, and their values. The latent variablespreserve neighbourhood structure of the data in a sense that objects in thesame semantic concept have similar latent values, and objects in differentconcepts have dissimilar latent values. We formulate the supervised infinitelatent variable problem based on an intuitive principle of pulling objectstogether if they are of the same type, and pushing them apart if they are not.We then combine this principle with a flexible Indian Buffet Process prior onthe latent variables. We show that the inferred supervised latent variables canbe directly used to perform a nearest neighbour search for the purpose ofretrieval. We introduce a new application of dynamically extending hash codes,and show how to effectively couple the structure of the hash codes withcontinuously growing structure of the neighbourhood preserving infinite latentfeature space.
arxiv-3900-45 | Stochastic Rank Aggregation | http://arxiv.org/pdf/1309.6852v1.pdf | author:Shuzi Niu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng category:cs.LG cs.IR stat.ML published:2013-09-26 summary:This paper addresses the problem of rank aggregation, which aims to find aconsensus ranking among multiple ranking inputs. Traditional rank aggregationmethods are deterministic, and can be categorized into explicit and implicitmethods depending on whether rank information is explicitly or implicitlyutilized. Surprisingly, experimental results on real data sets show thatexplicit rank aggregation methods would not work as well as implicit methods,although rank information is critical for the task. Our analysis indicates thatthe major reason might be the unreliable rank information from incompleteranking inputs. To solve this problem, we propose to incorporate uncertaintyinto rank aggregation and tackle the problem in both unsupervised andsupervised scenario. We call this novel framework {stochastic rank aggregation}(St.Agg for short). Specifically, we introduce a prior distribution on ranks,and transform the ranking functions or objectives in traditional explicitmethods to their expectations over this distribution. Our experiments onbenchmark data sets show that the proposed St.Agg outperforms the baselines inboth unsupervised and supervised scenarios.
arxiv-3900-46 | Treedy: A Heuristic for Counting and Sampling Subsets | http://arxiv.org/pdf/1309.6851v1.pdf | author:Teppo Niinimaki, Mikko Koivisto category:cs.DS cs.AI cs.LG published:2013-09-26 summary:Consider a collection of weighted subsets of a ground set N. Given a querysubset Q of N, how fast can one (1) find the weighted sum over all subsets ofQ, and (2) sample a subset of Q proportionally to the weights? We present atree-based greedy heuristic, Treedy, that for a given positive tolerance danswers such counting and sampling queries to within a guaranteed relativeerror d and total variation distance d, respectively. Experimental results onartificial instances and in application to Bayesian structure discovery inBayesian networks show that approximations yield dramatic savings in runningtime compared to exact computation, and that Treedy typically outperforms apreviously proposed sorting-based heuristic.
arxiv-3900-47 | Structured Convex Optimization under Submodular Constraints | http://arxiv.org/pdf/1309.6850v1.pdf | author:Kiyohito Nagano, Yoshinobu Kawahara category:cs.LG cs.DS stat.ML published:2013-09-26 summary:A number of discrete and continuous optimization problems in machine learningare related to convex minimization problems under submodular constraints. Inthis paper, we deal with a submodular function with a directed graph structure,and we show that a wide range of convex optimization problems under submodularconstraints can be solved much more efficiently than general submodularoptimization methods by a reduction to a maximum flow problem. Furthermore, wegive some applications, including sparse optimization methods, in which theproposed methods are effective. Additionally, we evaluate the performance ofthe proposed method through computational experiments.
arxiv-3900-48 | Cyclic Causal Discovery from Continuous Equilibrium Data | http://arxiv.org/pdf/1309.6849v1.pdf | author:Joris Mooij, Tom Heskes category:cs.LG cs.AI stat.ML published:2013-09-26 summary:We propose a method for learning cyclic causal models from a combination ofobservational and interventional equilibrium data. Novel aspects of theproposed method are its ability to work with continuous data (without assuminglinearity) and to deal with feedback loops. Within the context of biochemicalreactions, we also propose a novel way of modeling interventions that modifythe activity of compounds instead of their abundance. For computationalreasons, we approximate the nonlinear causal mechanisms by (coupled) locallinearizations, one for each experimental condition. We apply the method toreconstruct a cellular signaling network from the flow cytometry data measuredby Sachs et al. (2005). We show that our method finds evidence in the data forfeedback loops and that it gives a more accurate quantitative description ofthe data at comparable model complexity.
arxiv-3900-49 | Learning Max-Margin Tree Predictors | http://arxiv.org/pdf/1309.6847v1.pdf | author:Ofer Meshi, Elad Eban, Gal Elidan, Amir Globerson category:cs.LG stat.ML published:2013-09-26 summary:Structured prediction is a powerful framework for coping with jointprediction of interacting outputs. A central difficulty in using this frameworkis that often the correct label dependence structure is unknown. At the sametime, we would like to avoid an overly complex structure that will lead tointractable prediction. In this work we address the challenge of learning treestructured predictive models that achieve high accuracy while at the same timefacilitate efficient (linear time) inference. We start by proving that thistask is in general NP-hard, and then suggest an approximate alternative.Briefly, our CRANK approach relies on a novel Circuit-RANK regularizer thatpenalizes non-tree structures and that can be optimized using a CCCP procedure.We demonstrate the effectiveness of our approach on several domains and showthat, despite the relative simplicity of the structure, prediction accuracy iscompetitive with a fully connected model that is computationally costly atprediction time.
arxiv-3900-50 | Constrained Bayesian Inference for Low Rank Multitask Learning | http://arxiv.org/pdf/1309.6840v1.pdf | author:Oluwasanmi Koyejo, Joydeep Ghosh category:cs.LG stat.ML published:2013-09-26 summary:We present a novel approach for constrained Bayesian inference. Unlikecurrent methods, our approach does not require convexity of the constraint set.We reduce the constrained variational inference to a parametric optimizationover the feasible set of densities and propose a general recipe for suchproblems. We apply the proposed constrained Bayesian inference approach tomultitask learning subject to rank constraints on the weight matrix. Further,constrained parameter estimation is applied to recover the sparse conditionalindependence structure encoded by prior precision matrices. Our approach ismotivated by reverse inference for high dimensional functional neuroimaging, adomain where the high dimensionality and small number of examples requires theuse of constraints to ensure meaningful and effective models. For thisapplication, we propose a model that jointly learns a weight matrix and theprior inverse covariance structure between different tasks. We presentexperimental validation showing that the proposed approach outperforms strongbaseline models in terms of predictive performance and structure recovery.
arxiv-3900-51 | Inverse Covariance Estimation for High-Dimensional Data in Linear Time and Space: Spectral Methods for Riccati and Sparse Models | http://arxiv.org/pdf/1309.6838v1.pdf | author:Jean Honorio, Tommi S. Jaakkola category:cs.LG stat.ML published:2013-09-26 summary:We propose maximum likelihood estimation for learning Gaussian graphicalmodels with a Gaussian (ell_2^2) prior on the parameters. This is in contrastto the commonly used Laplace (ell_1) prior for encouraging sparseness. We showthat our optimization problem leads to a Riccati matrix equation, which has aclosed form solution. We propose an efficient algorithm that performs asingular value decomposition of the training data. Our algorithm isO(NT^2)-time and O(NT)-space for N variables and T samples. Our method istailored to high-dimensional problems (N gg T), in which sparseness promotingmethods become intractable. Furthermore, instead of obtaining a single solutionfor a specific regularization parameter, our algorithm finds the whole solutionpath. We show that the method has logarithmic sample complexity under thespiked covariance model. We also propose sparsification of the dense solutionwith provable performance guarantees. We provide techniques for using ourlearnt models, such as removing unimportant variables, computing likelihoodsand conditional distributions. Finally, we show promising results in severalgene expressions datasets.
arxiv-3900-52 | Gaussian Processes for Big Data | http://arxiv.org/pdf/1309.6835v1.pdf | author:James Hensman, Nicolo Fusi, Neil D. Lawrence category:cs.LG stat.ML published:2013-09-26 summary:We introduce stochastic variational inference for Gaussian process models.This enables the application of Gaussian process (GP) models to data setscontaining millions of data points. We show how GPs can be vari- ationallydecomposed to depend on a set of globally relevant inducing variables whichfactorize the model in the necessary manner to perform variational inference.Our ap- proach is readily extended to models with non-Gaussian likelihoods andlatent variable models based around Gaussian processes. We demonstrate theapproach on a simple toy problem and two real world data sets.
arxiv-3900-53 | Unsupervised Learning of Noisy-Or Bayesian Networks | http://arxiv.org/pdf/1309.6834v1.pdf | author:Yonatan Halpern, David Sontag category:cs.LG stat.ML published:2013-09-26 summary:This paper considers the problem of learning the parameters in Bayesiannetworks of discrete variables with known structure and hidden variables.Previous approaches in these settings typically use expectation maximization;when the network has high treewidth, the required expectations might beapproximated using Monte Carlo or variational methods. We show how to avoidinference altogether during learning by giving a polynomial-time algorithmbased on the method-of-moments, building upon recent work on learningdiscrete-valued mixture models. In particular, we show how to learn theparameters for a family of bipartite noisy-or Bayesian networks. In ourexperimental results, we demonstrate an application of our algorithm tolearning QMR-DT, a large Bayesian network used for medical diagnosis. We showthat it is possible to fully learn the parameters of QMR-DT even when only thefindings are observed in the training data (ground truth diseases unknown).
arxiv-3900-54 | Multiple Instance Learning by Discriminative Training of Markov Networks | http://arxiv.org/pdf/1309.6833v1.pdf | author:Hossein Hajimirsadeghi, Jinling Li, Greg Mori, Mohammad Zaki, Tarek Sayed category:cs.LG stat.ML published:2013-09-26 summary:We introduce a graphical framework for multiple instance learning (MIL) basedon Markov networks. This framework can be used to model the traditional MILdefinition as well as more general MIL definitions. Different levels ofambiguity -- the portion of positive instances in a bag -- can be explored inweakly supervised data. To train these models, we propose a discriminativemax-margin learning algorithm leveraging efficient inference forcardinality-based cliques. The efficacy of the proposed framework is evaluatedon a variety of data sets. Experimental results verify that encoding orlearning the degree of ambiguity can improve classification performance.
arxiv-3900-55 | Batch-iFDD for Representation Expansion in Large MDPs | http://arxiv.org/pdf/1309.6831v1.pdf | author:Alborz Geramifard, Thomas J. Walsh, Nicholas Roy, Jonathan How category:cs.LG stat.ML published:2013-09-26 summary:Matching pursuit (MP) methods are a promising class of feature constructionalgorithms for value function approximation. Yet existing MP methods requirecreating a pool of potential features, mandating expert knowledge orenumeration of a large feature pool, both of which hinder scalability. Thispaper introduces batch incremental feature dependency discovery (Batch-iFDD) asan MP method that inherits a provable convergence property. Additionally,Batch-iFDD does not require a large pool of features, leading to lowercomputational complexity. Empirical policy evaluation results across threedomains with up to one million states highlight the scalability of Batch-iFDDover the previous state of the art MP algorithm.
arxiv-3900-56 | Building Bridges: Viewing Active Learning from the Multi-Armed Bandit Lens | http://arxiv.org/pdf/1309.6830v1.pdf | author:Ravi Ganti, Alexander G. Gray category:cs.LG stat.ML published:2013-09-26 summary:In this paper we propose a multi-armed bandit inspired, pool based activelearning algorithm for the problem of binary classification. By carefullyconstructing an analogy between active learning and multi-armed bandits, weutilize ideas such as lower confidence bounds, and self-concordantregularization from the multi-armed bandit literature to design our proposedalgorithm. Our algorithm is a sequential algorithm, which in each round assignsa sampling distribution on the pool, samples one point from this distribution,and queries the oracle for the label of this sampled point. The design of thissampling distribution is also inspired by the analogy between active learningand multi-armed bandits. We show how to derive lower confidence bounds requiredby our algorithm. Experimental comparisons to previously proposed activelearning algorithms show superior performance on some standard UCI datasets.
arxiv-3900-57 | Bethe-ADMM for Tree Decomposition based Parallel MAP Inference | http://arxiv.org/pdf/1309.6829v1.pdf | author:Qiang Fu, Huahua Wang, Arindam Banerjee category:cs.AI cs.LG stat.ML published:2013-09-26 summary:We consider the problem of maximum a posteriori (MAP) inference in discretegraphical models. We present a parallel MAP inference algorithm calledBethe-ADMM based on two ideas: tree-decomposition of the graph and thealternating direction method of multipliers (ADMM). However, unlike thestandard ADMM, we use an inexact ADMM augmented with a Bethe-divergence basedproximal function, which makes each subproblem in ADMM easy to solve inparallel using the sum-product algorithm. We rigorously prove globalconvergence of Bethe-ADMM. The proposed algorithm is extensively evaluated onboth synthetic and real datasets to illustrate its effectiveness. Further, theparallel Bethe-ADMM is shown to scale almost linearly with increasing number ofcores.
arxiv-3900-58 | Convex Relaxations of Bregman Divergence Clustering | http://arxiv.org/pdf/1309.6823v1.pdf | author:Hao Cheng, Xinhua Zhang, Dale Schuurmans category:cs.LG stat.ML published:2013-09-26 summary:Although many convex relaxations of clustering have been proposed in the pastdecade, current formulations remain restricted to spherical Gaussian ordiscriminative models and are susceptible to imbalanced clusters. To addressthese shortcomings, we propose a new class of convex relaxations that can beflexibly applied to more general forms of Bregman divergence clustering. Bybasing these new formulations on normalized equivalence relations we retainadditional control on relaxation quality, which allows improvement inclustering quality. We furthermore develop optimization methods that improvescalability by exploiting recent implicit matrix norm methods. In practice, wefind that the new formulations are able to efficiently produce tighterclusterings that improve the accuracy of state of the art methods.
arxiv-3900-59 | Sample Complexity of Multi-task Reinforcement Learning | http://arxiv.org/pdf/1309.6821v1.pdf | author:Emma Brunskill, Lihong Li category:cs.LG stat.ML published:2013-09-26 summary:Transferring knowledge across a sequence of reinforcement-learning tasks ischallenging, and has a number of important applications. Though there isencouraging empirical evidence that transfer can improve performance insubsequent reinforcement-learning tasks, there has been very little theoreticalanalysis. In this paper, we introduce a new multi-task algorithm for a sequenceof reinforcement-learning tasks when each task is sampled independently from(an unknown) distribution over a finite set of Markov decision processes whoseparameters are initially unknown. For this setting, we prove under certainassumptions that the per-task sample complexity of exploration is reducedsignificantly due to transfer compared to standard single-task algorithms. Ourmulti-task algorithm also has the desired characteristic that it is guaranteednot to exhibit negative transfer: in the worst case its per-task samplecomplexity is comparable to the corresponding single-task algorithm.
arxiv-3900-60 | SparsityBoost: A New Scoring Function for Learning Bayesian Network Structure | http://arxiv.org/pdf/1309.6820v1.pdf | author:Eliot Brenner, David Sontag category:cs.LG cs.AI stat.ML published:2013-09-26 summary:We give a new consistent scoring function for structure learning of Bayesiannetworks. In contrast to traditional approaches to scorebased structurelearning, such as BDeu or MDL, the complexity penalty that we propose isdata-dependent and is given by the probability that a conditional independencetest correctly shows that an edge cannot exist. What really distinguishes thisnew scoring function from earlier work is that it has the property of becomingcomputationally easier to maximize as the amount of data increases. We prove apolynomial sample complexity result, showing that maximizing this score isguaranteed to correctly learn a structure with no false edges and adistribution close to the generating distribution, whenever there exists aBayesian network which is a perfect map for the data generating distribution.Although the new score can be used with any search algorithm, we give empiricalresults showing that it is particularly effective when used together with alinear programming relaxation approach to Bayesian network structure learning.
arxiv-3900-61 | Hilbert Space Embeddings of Predictive State Representations | http://arxiv.org/pdf/1309.6819v1.pdf | author:Byron Boots, Geoffrey Gordon, Arthur Gretton category:cs.LG stat.ML published:2013-09-26 summary:Predictive State Representations (PSRs) are an expressive class of models forcontrolled stochastic processes. PSRs represent state as a set of predictionsof future observable events. Because PSRs are defined entirely in terms ofobservable data, statistically consistent estimates of PSR parameters can belearned efficiently by manipulating moments of observed training data. Mostlearning algorithms for PSRs have assumed that actions and observations arefinite with low cardinality. In this paper, we generalize PSRs to infinite setsof observations and actions, using the recent concept of Hilbert spaceembeddings of distributions. The essence is to represent the state as anonparametric conditional embedding operator in a Reproducing Kernel HilbertSpace (RKHS) and leverage recent work in kernel methods to estimate, predict,and update the representation. We show that these Hilbert space embeddings ofPSRs are able to gracefully handle continuous actions and observations, andthat our learned models outperform competing system identification algorithmson several prediction benchmarks.
arxiv-3900-62 | Boosting in the presence of label noise | http://arxiv.org/pdf/1309.6818v1.pdf | author:Jakramate Bootkrajang, Ata Kaban category:cs.LG stat.ML published:2013-09-26 summary:Boosting is known to be sensitive to label noise. We studied two approachesto improve AdaBoost's robustness against labelling errors. One is to employ alabel-noise robust classifier as a base learner, while the other is to modifythe AdaBoost algorithm to be more robust. Empirical evaluation shows that acommittee of robust classifiers, although converges faster than non label-noiseaware AdaBoost, is still susceptible to label noise. However, pairing it withthe new robust Boosting algorithm we propose here results in a more resilientalgorithm under mislabelling.
arxiv-3900-63 | High-dimensional Joint Sparsity Random Effects Model for Multi-task Learning | http://arxiv.org/pdf/1309.6814v1.pdf | author:Krishnakumar Balasubramanian, Kai Yu, Tong Zhang category:cs.LG stat.ML published:2013-09-26 summary:Joint sparsity regularization in multi-task learning has attracted muchattention in recent years. The traditional convex formulation employs the groupLasso relaxation to achieve joint sparsity across tasks. Although this approachleads to a simple convex formulation, it suffers from several issues due to thelooseness of the relaxation. To remedy this problem, we view jointly sparsemulti-task learning as a specialized random effects model, and derive a convexrelaxation approach that involves two steps. The first step learns thecovariance matrix of the coefficients using a convex formulation which we referto as sparse covariance coding; the second step solves a ridge regressionproblem with a sparse quadratic regularizer based on the covariance matrixobtained in the first step. It is shown that this approach produces anasymptotically optimal quadratic regularizer in the multitask learning settingwhen the number of tasks approaches infinity. Experimental results demonstratethat the convex formulation obtained via the proposed model significantlyoutperforms group Lasso (and related multi-stage formulations
arxiv-3900-64 | Hinge-loss Markov Random Fields: Convex Inference for Structured Prediction | http://arxiv.org/pdf/1309.6813v1.pdf | author:Stephen Bach, Bert Huang, Ben London, Lise Getoor category:cs.LG stat.ML published:2013-09-26 summary:Graphical models for structured domains are powerful tools, but thecomputational complexities of combinatorial prediction spaces can forcerestrictions on models, or require approximate inference in order to betractable. Instead of working in a combinatorial space, we use hinge-lossMarkov random fields (HL-MRFs), an expressive class of graphical models withlog-concave density functions over continuous variables, which can representconfidences in discrete predictions. This paper demonstrates that HL-MRFs aregeneral tools for fast and accurate structured prediction. We introduce thefirst inference algorithm that is both scalable and applicable to the fullclass of HL-MRFs, and show how to train HL-MRFs with several learningalgorithms. Our experiments show that HL-MRFs match or surpass the predictiveperformance of state-of-the-art methods, including discrete models, in fourapplication domains.
arxiv-3900-65 | The Bregman Variational Dual-Tree Framework | http://arxiv.org/pdf/1309.6812v1.pdf | author:Saeed Amizadeh, Bo Thiesson, Milos Hauskrecht category:cs.LG stat.ML published:2013-09-26 summary:Graph-based methods provide a powerful tool set for many non-parametricframeworks in Machine Learning. In general, the memory and computationalcomplexity of these methods is quadratic in the number of examples in the datawhich makes them quickly infeasible for moderate to large scale datasets. Asignificant effort to find more efficient solutions to the problem has beenmade in the literature. One of the state-of-the-art methods that has beenrecently introduced is the Variational Dual-Tree (VDT) framework. Despite someof its unique features, VDT is currently restricted only to Euclidean spaceswhere the Euclidean distance quantifies the similarity. In this paper, weextend the VDT framework beyond the Euclidean distance to more general Bregmandivergences that include the Euclidean distance as a special case. Byexploiting the properties of the general Bregman divergence, we show how thenew framework can maintain all the pivotal features of the VDT framework andyet significantly improve its performance in non-Euclidean domains. We applythe proposed framework to different text categorization problems anddemonstrate its benefits over the original VDT.
arxiv-3900-66 | Generative Multiple-Instance Learning Models For Quantitative Electromyography | http://arxiv.org/pdf/1309.6811v1.pdf | author:Tameem Adel, Benn Smith, Ruth Urner, Daniel Stashuk, Daniel J. Lizotte category:cs.LG stat.ML published:2013-09-26 summary:We present a comprehensive study of the use of generative modeling approachesfor Multiple-Instance Learning (MIL) problems. In MIL a learner receivestraining instances grouped together into bags with labels for the bags only(which might not be correct for the comprised instances). Our work wasmotivated by the task of facilitating the diagnosis of neuromuscular disordersusing sets of motor unit potential trains (MUPTs) detected within a musclewhich can be cast as a MIL problem. Our approach leads to a state-of-the-artsolution to the problem of muscle classification. By introducing and analyzinggenerative models for MIL in a general framework and examining a variety ofmodel structures and components, our work also serves as a methodological guideto modelling MIL tasks. We evaluate our proposed methods both on MUPT datasetsand on the MUSK1 dataset, one of the most widely used benchmarks for MIL.
arxiv-3900-67 | Adopting level set theory based algorithms to segment human ear | http://arxiv.org/pdf/1309.7276v1.pdf | author:Bijeesh T. V, Nimmi I. P category:cs.CV published:2013-09-26 summary:Human identification has always been a topic that interested researchersaround the world. Biometric methods are found to be more effective and mucheasier for the users than the traditional identification methods like keys,smart cards and passwords. Unlike with the traditional methods, with biometricmethods the data acquisition is most of the times passive, which means theusers do not take active part in data acquisition. Data acquisition can beperformed using cameras, scanners or sensors. Human physiological biometricssuch as face, eye and ear are good candidates for uniquely identifying anindividual. However, human ear scores over face and eye because of certainadvantages it has over face. The most challenging phase in human identificationbased on ear biometric is the segmentation of the ear image from the capturedimage which may contain many unwanted details. In this work, PDE based imageprocessing techniques are used to segment out the ear image. Level Set Theorybased image processing is employed to obtain the contour of the ear image. Afew Level set algorithms are compared for their efficiency in segmenting testear images.
arxiv-3900-68 | Domain-Specific Sentiment Word Extraction by Seed Expansion and Pattern Generation | http://arxiv.org/pdf/1309.6722v1.pdf | author:Tang Duyu, Qin Bing, Zhou LanJun, Wong KamFai, Zhao Yanyan, Liu Ting category:cs.CL published:2013-09-26 summary:This paper focuses on the automatic extraction of domain-specific sentimentword (DSSW), which is a fundamental subtask of sentiment analysis. Mostprevious work utilizes manual patterns for this task. However, the performanceof those methods highly relies on the labelled patterns or selected seeds. Inorder to overcome the above problem, this paper presents an automatic frameworkto detect large-scale domain-specific patterns for DSSW extraction. To thisend, sentiment seeds are extracted from massive dataset of user comments.Subsequently, these sentiment seeds are expanded by synonyms using abootstrapping mechanism. Simultaneously, a synonymy graph is built and thegraph propagation algorithm is applied on the built synonymy graph. Afterwards,syntactic and sequential relations between target words and high-rankedsentiment words are extracted automatically to construct large-scale patterns,which are further used to extracte DSSWs. The experimental results in threedomains reveal the effectiveness of our method.
arxiv-3900-69 | Characterness: An Indicator of Text in the Wild | http://arxiv.org/pdf/1309.6691v1.pdf | author:Yao Li, Wenjing Jia, Chunhua Shen, Anton van den Hengel category:cs.CV published:2013-09-25 summary:Text in an image provides vital information for interpreting its contents,and text in a scene can aide with a variety of tasks from navigation, toobstacle avoidance, and odometry. Despite its value, however, identifyinggeneral text in images remains a challenging research problem. Motivated by theneed to consider the widely varying forms of natural text, we propose abottom-up approach to the problem which reflects the `characterness' of animage region. In this sense our approach mirrors the move from saliencydetection methods to measures of `objectness'. In order to measure thecharacterness we develop three novel cues that are tailored for characterdetection, and a Bayesian method for their integration. Because text is made upof sets of characters, we then design a Markov random field (MRF) model so asto exploit the inherent dependencies between characters. We experimentally demonstrate the effectiveness of our characterness cues aswell as the advantage of Bayesian multi-cue integration. The proposed textdetector outperforms state-of-the-art methods on a few benchmark scene textdetection datasets. We also show that our measurement of `characterness' issuperior than state-of-the-art saliency detection models when applied to thesame task.
arxiv-3900-70 | The Classification Accuracy of Multiple-Metric Learning Algorithm on Multi-Sensor Fusion | http://arxiv.org/pdf/1309.3006v2.pdf | author:Firouz Abdullah Al-Wassai, N. V. Kalyankar category:cs.CV published:2013-09-11 summary:This paper focuses on two main issues; first one is the impact of SimilaritySearch to learning the training sample in metric space, and searching based onsupervised learning classi-fication. In particular, four metrics spacesearching are based on spatial information that are introduced as thefollowing; Cheby-shev Distance (CD); Bray Curtis Distance (BCD); ManhattanDistance (MD) and Euclidean Distance(ED) classifiers. The second issueinvestigates the performance of combination of mul-ti-sensor images on thesupervised learning classification accura-cy. QuickBird multispectral data (MS)and panchromatic data (PAN) have been used in this study to demonstrate theenhance-ment and accuracy assessment of fused image over the original images.The supervised classification results of fusion image generated better than theMS did. QuickBird and the best results with ED classifier than the other did.
arxiv-3900-71 | An Inter-lingual Reference Approach For Multi-Lingual Ontology Matching | http://arxiv.org/pdf/1309.6650v1.pdf | author:Haytham Al-Feel, Ralph Schafermeier, Adrian Paschke category:cs.CL cs.DL published:2013-09-25 summary:Ontologies are considered as the backbone of the Semantic Web. With therising success of the Semantic Web, the number of participating communitiesfrom different countries is constantly increasing. The growing number ofontologies available in different natural languages leads to aninteroperability problem. In this paper, we discuss several approaches forontology matching; examine similarities and differences, identify weaknesses,and compare the existing automated approaches with the manual approaches forintegrating multilingual ontologies. In addition to that, we propose a newarchitecture for a multilingual ontology matching service. As a case study weused an example of two multilingual enterprise ontologies - the universityontology of Freie Universitaet Berlin and the ontology for Fayoum University inEgypt.
arxiv-3900-72 | Learning Stable Multilevel Dictionaries for Sparse Representations | http://arxiv.org/pdf/1303.0448v2.pdf | author:Jayaraman J. Thiagarajan, Karthikeyan Natesan Ramamurthy, Andreas Spanias category:cs.CV stat.ML published:2013-03-03 summary:Sparse representations using learned dictionaries are being increasingly usedwith success in several data processing and machine learning applications. Theavailability of abundant training data necessitates the development ofefficient, robust and provably good dictionary learning algorithms. Algorithmicstability and generalization are desirable characteristics for dictionarylearning algorithms that aim to build global dictionaries which can efficientlymodel any test data similar to the training samples. In this paper, we proposean algorithm to learn dictionaries for sparse representations from large scaledata, and prove that the proposed learning algorithm is stable andgeneralizable asymptotically. The algorithm employs a 1-D subspace clusteringprocedure, the K-hyperline clustering, in order to learn a hierarchicaldictionary with multiple levels. We also propose an information-theoreticscheme to estimate the number of atoms needed in each level of learning anddevelop an ensemble approach to learn robust dictionaries. Using the proposeddictionaries, the sparse code for novel test data can be computed using alow-complexity pursuit procedure. We demonstrate the stability andgeneralization characteristics of the proposed algorithm using simulations. Wealso evaluate the utility of the multilevel dictionaries in compressed recoveryand subspace learning applications.
arxiv-3900-73 | Should I Stay or Should I Go: Coordinating Biological Needs with Continuously-updated Assessments of the Environment | http://arxiv.org/pdf/1309.6584v1.pdf | author:Liane Gabora category:cs.NE cs.LG q-bio.NC published:2013-09-25 summary:This paper presents Wanderer, a model of how autonomous adaptive systemscoordinate internal biological needs with moment-by-moment assessments of theprobabilities of events in the external world. The extent to which Wanderermoves about or explores its environment reflects the relative activations oftwo competing motivational sub-systems: one represents the need to acquireenergy and it excites exploration, and the other represents the need to avoidpredators and it inhibits exploration. The environment contains food,predators, and neutral stimuli. Wanderer responds to these events in a way thatis adaptive in the short turn, and reassesses the probabilities of these eventsso that it can modify its long term behaviour appropriately. When food appears,Wanderer be-comes satiated and exploration temporarily decreases. When apredator appears, Wanderer both decreases exploration in the short term, andbecomes more "cautious" about exploring in the future. Wanderer also formsassociations between neutral features and salient ones (food and predators)when they are present at the same time, and uses these associations to guideits behaviour.
arxiv-3900-74 | Online Douglas-Rachford splitting method | http://arxiv.org/pdf/1308.4757v5.pdf | author:Ziqiang Shi category:cs.NA cs.LG stat.ML published:2013-08-22 summary:Online learning has emerged as powerful tool in large scale optimization. Inthis work, we generalize the Douglas-Rachford splitting method for minimizingcomposite functions to online settings.
arxiv-3900-75 | Multiple-object tracking in cluttered and crowded public spaces | http://arxiv.org/pdf/1309.6391v1.pdf | author:Rhys Martin, Ognjen Arandjelović category:cs.CV published:2013-09-25 summary:This paper addresses the problem of tracking moving objects of variableappearance in challenging scenes rich with features and texture. Reliabletracking is of pivotal importance in surveillance applications. It is madeparticularly difficult by the nature of objects encountered in such scenes:these too change in appearance and scale, and are often articulated (e.g.humans). We propose a method which uses fast motion detection and segmentationas a constraint for both building appearance models and their robustpropagation (matching) in time. The appearance model is based on sets of localappearances automatically clustered using spatio-kinetic similarity, and isupdated with each new appearance seen. This integration of all seen appearancesof a tracked object makes it extremely resilient to errors caused by occlusionand the lack of permanence of due to low data quality, appearance change orbackground clutter. These theoretical strengths of our algorithm areempirically demonstrated on two hour long video footage of a busy citymarketplace.
arxiv-3900-76 | Contextually learnt detection of unusual motion-based behaviour in crowded public spaces | http://arxiv.org/pdf/1309.6390v1.pdf | author:Ognjen Arandjelović category:cs.CV published:2013-09-25 summary:In this paper we are interested in analyzing behaviour in crowded publicplaces at the level of holistic motion. Our aim is to learn, without userinput, strong scene priors or labelled data, the scope of "normal behaviour"for a particular scene and thus alert to novelty in unseen footage. The firstcontribution is a low-level motion model based on what we term trackletprimitives, which are scene-specific elementary motions. We propose aclustering-based algorithm for tracklet estimation from local approximations totracks of appearance features. This is followed by two methods for motionnovelty inference from tracklet primitives: (a) we describe an approach basedon a non-hierarchial ensemble of Markov chains as a means of capturingbehavioural characteristics at different scales, and (b) a more flexiblealternative which exhibits a higher generalizing power by accounting forconstraints introduced by intentionality and goal-oriented planning of humanmotion in a particular scene. Evaluated on a 2h long video of a busy citymarketplace, both algorithms are shown to be successful at inferring unusualbehaviour, the latter model achieving better performance for novelties at alarger spatial scale.
arxiv-3900-77 | Diffeomorphic Metric Mapping and Probabilistic Atlas Generation of Hybrid Diffusion Imaging based on BFOR Signal Basis | http://arxiv.org/pdf/1309.6379v1.pdf | author:Jia Du, A. Pasha Hosseinbor, Moo K. Chung, Barbara B. Bendlin, Gaurav Suryawanshi, Andrew L. Alexander, Anqi Qiu category:cs.CV published:2013-09-25 summary:We propose a large deformation diffeomorphic metric mapping algorithm toalign multiple b-value diffusion weighted imaging (mDWI) data, specificallyacquired via hybrid diffusion imaging (HYDI), denoted as LDDMM-HYDI. We thenpropose a Bayesian model for estimating the white matter atlas from HYDIs. Weadopt the work given in Hosseinbor et al. (2012) and represent the q-spacediffusion signal with the Bessel Fourier orientation reconstruction (BFOR)signal basis. The BFOR framework provides the representation of mDWI in theq-space and thus reduces memory requirement. In addition, since the BFOR signalbasis is orthonormal, the L2 norm that quantifies the differences in theq-space signals of any two mDWI datasets can be easily computed as the sum ofthe squared differences in the BFOR expansion coefficients. In this work, weshow that the reorientation of the $q$-space signal due to spatialtransformation can be easily defined on the BFOR signal basis. We incorporatethe BFOR signal basis into the LDDMM framework and derive the gradient descentalgorithm for LDDMM-HYDI with explicit orientation optimization. Additionally,we extend the previous Bayesian atlas estimation framework for scalar-valuedimages to HYDIs and derive the expectation-maximization algorithm for solvingthe HYDI atlas estimation problem. Using real HYDI datasets, we show theBayesian model generates the white matter atlas with anatomical details.Moreover, we show that it is important to consider the variation of mDWIreorientation due to a small change in diffeomorphic transformation in theLDDMM-HYDI optimization and to incorporate the full information of HYDI foraligning mDWI.
arxiv-3900-78 | Using Nuances of Emotion to Identify Personality | http://arxiv.org/pdf/1309.6352v1.pdf | author:Saif M. Mohammad, Svetlana Kiritchenko category:cs.CL published:2013-09-24 summary:Past work on personality detection has shown that frequency of lexicalcategories such as first person pronouns, past tense verbs, and sentiment wordshave significant correlations with personality traits. In this paper, for thefirst time, we show that fine affect (emotion) categories such as that ofexcitement, guilt, yearning, and admiration are significant indicators ofpersonality. Additionally, we perform experiments to show that the gainsprovided by the fine affect categories are not obtained by using coarse affectcategories alone or with specificity features alone. We employ these featuresin five SVM classifiers for detecting five personality traits through essays.We find that the use of fine emotion features leads to statisticallysignificant improvement over a competitive baseline, whereas the use of coarseaffect and specificity features does not.
arxiv-3900-79 | Tracking Sentiment in Mail: How Genders Differ on Emotional Axes | http://arxiv.org/pdf/1309.6347v1.pdf | author:Saif M. Mohammad, Tony, Yang category:cs.CL published:2013-09-24 summary:With the widespread use of email, we now have access to unprecedented amountsof text that we ourselves have written. In this paper, we show how sentimentanalysis can be used in tandem with effective visualizations to quantify andtrack emotions in many types of mail. We create a large word--emotionassociation lexicon by crowdsourcing, and use it to compare emotions in loveletters, hate mail, and suicide notes. We show that there are markeddifferences across genders in how they use emotion words in work-place email.For example, women use many words from the joy--sadness axis, whereas menprefer terms from the fear--trust axis. Finally, we show visualizations thatcan help people track emotions in their emails.
arxiv-3900-80 | Sentiment Analysis in the News | http://arxiv.org/pdf/1309.6202v1.pdf | author:Alexandra Balahur, Ralf Steinberger, Mijail Kabadjov, Vanni Zavarella, Erik van der Goot, Matina Halkia, Bruno Pouliquen, Jenya Belyaeva category:cs.CL published:2013-09-24 summary:Recent years have brought a significant growth in the volume of research insentiment analysis, mostly on highly subjective text types (movie or productreviews). The main difference these texts have with news articles is that theirtarget is clearly defined and unique across the text. Following differentannotation efforts and the analysis of the issues encountered, we realised thatnews opinion mining is different from that of other text types. We identifiedthree subtasks that need to be addressed: definition of the target; separationof the good and bad news content from the good and bad sentiment expressed onthe target; and analysis of clearly marked opinion that is expressedexplicitly, not needing interpretation or the use of world knowledge.Furthermore, we distinguish three different possible views on newspaperarticles - author, reader and text, which have to be addressed differently atthe time of analysing sentiment. Given these definitions, we present work onmining opinions about entities in English language news, in which (a) we testthe relative suitability of various sentiment dictionaries and (b) we attemptto separate positive or negative opinion from good or bad news. In theexperiments described here, we tested whether or not subject domain-definingvocabulary should be ignored. Results showed that this idea is more appropriatein the context of news opinion mining and that the approaches taking this intoconsideration produce a better performance.
arxiv-3900-81 | Acronym recognition and processing in 22 languages | http://arxiv.org/pdf/1309.6185v1.pdf | author:Maud Ehrmann, Leonida della Rocca, Ralf Steinberger, Hristo Tanev category:cs.CL published:2013-09-24 summary:We are presenting work on recognising acronyms of the form Long-Form(Short-Form) such as "International Monetary Fund (IMF)" in millions of newsarticles in twenty-two languages, as part of our more general effort torecognise entities and their variants in news text and to use them for theautomatic analysis of the news, including the linking of related news acrosslanguages. We show how the acronym recognition patterns, initially developedfor medical terms, needed to be adapted to the more general news domain and wepresent evaluation results. We describe our effort to automatically merge thenumerous long-form variants referring to the same short-form, while keepingnon-related long-forms separate. Finally, we provide extensive statistics onthe frequency and the distribution of short-form/long-form pairs acrosslanguages.
arxiv-3900-82 | JRC-Names: A freely available, highly multilingual named entity resource | http://arxiv.org/pdf/1309.6162v1.pdf | author:Ralf Steinberger, Bruno Pouliquen, Mijail Kabadjov, Erik van der Goot category:cs.CL published:2013-09-24 summary:This paper describes a new, freely available, highly multilingual namedentity resource for person and organisation names that has been compiled overseven years of large-scale multilingual news analysis combined with Wikipediamining, resulting in 205,000 per-son and organisation names plus about the samenumber of spelling variants written in over 20 different scripts and in manymore languages. This resource, produced as part of the Europe Media Monitoractivity (EMM, http://emm.newsbrief.eu/overview.html), can be used for a numberof purposes. These include improving name search in databases or on theinternet, seeding machine learning systems to learn named entity recognitionrules, improve machine translation results, and more. We describe here how thisresource was created; we give statistics on its current size; we address theissue of morphological inflection; and we give details regarding itsfunctionality. Updates to this resource will be made available daily.
arxiv-3900-83 | Random Forests on Distance Matrices for Imaging Genetics Studies | http://arxiv.org/pdf/1309.6158v1.pdf | author:Aaron Sim, Dimosthenis Tsagkrasoulis, Giovanni Montana category:stat.ML stat.AP published:2013-09-24 summary:We propose a non-parametric regression methodology, Random Forests onDistance Matrices (RFDM), for detecting genetic variants associated toquantitative phenotypes representing the human brain's structure or function,and obtained using neuroimaging techniques. RFDM, which is an extension ofdecision forests, requires a distance matrix as response that encodes allpair-wise phenotypic distances in the random sample. We discuss ways to learnsuch distances directly from the data using manifold learning techniques, andhow to define such distances when the phenotypes are non-vectorial objects suchas brain connectivity networks. We also describe an extension of RFDM to detectespistatic effects while keeping the computational complexity low. Extensivesimulation results and an application to an imaging genetics study ofAlzheimer's Disease are presented and discussed.
arxiv-3900-84 | Non-negative Matrix Factorization with Linear Constraints for Single-Channel Speech Enhancement | http://arxiv.org/pdf/1309.6047v1.pdf | author:Nikolay Lyubimov, Mikhail Kotov category:cs.SD cs.CL published:2013-09-24 summary:This paper investigates a non-negative matrix factorization (NMF)-basedapproach to the semi-supervised single-channel speech enhancement problem whereonly non-stationary additive noise signals are given. The proposed methodrelies on sinusoidal model of speech production which is integrated inside NMFframework using linear constraints on dictionary atoms. This method is furtherdeveloped to regularize harmonic amplitudes. Simple multiplicative algorithmsare presented. The experimental evaluation was made on TIMIT corpus mixed withvarious types of noise. It has been shown that the proposed method outperformssome of the state-of-the-art noise suppression techniques in terms ofsignal-to-noise ratio.
arxiv-3900-85 | A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion | http://arxiv.org/pdf/1309.6013v1.pdf | author:T. Tony Cai, Wen-Xin Zhou category:stat.ML math.ST stat.TH published:2013-09-24 summary:We consider in this paper the problem of noisy 1-bit matrix completion undera general non-uniform sampling distribution using the max-norm as a convexrelaxation for the rank. A max-norm constrained maximum likelihood estimate isintroduced and studied. The rate of convergence for the estimate is obtained.Information-theoretical methods are used to establish a minimax lower boundunder the general sampling model. The minimax upper and lower bounds togetheryield the optimal rate of convergence for the Frobenius norm loss.Computational algorithms and numerical performance are also discussed.
arxiv-3900-86 | Asymptotic Analysis of LASSOs Solution Path with Implications for Approximate Message Passing | http://arxiv.org/pdf/1309.5979v1.pdf | author:Ali Mousavi, Arian Maleki, Richard G. Baraniuk category:math.ST cs.IT math.IT stat.ML stat.TH published:2013-09-23 summary:This paper concerns the performance of the LASSO (also knows as basis pursuitdenoising) for recovering sparse signals from undersampled, randomized, noisymeasurements. We consider the recovery of the signal $x_o \in \mathbb{R}^N$from $n$ random and noisy linear observations $y= Ax_o + w$, where $A$ is themeasurement matrix and $w$ is the noise. The LASSO estimate is given by thesolution to the optimization problem $x_o$ with $\hat{x}_{\lambda} = \arg\min_x \frac{1}{2} \y-Ax\_2^2 + \lambda \x\_1$. Despite major progress inthe theoretical analysis of the LASSO solution, little is known about itsbehavior as a function of the regularization parameter $\lambda$. In this paperwe study two questions in the asymptotic setting (i.e., where $N \rightarrow\infty$, $n \rightarrow \infty$ while the ratio $n/N$ converges to a fixednumber in $(0,1)$): (i) How does the size of the active set$\\hat{x}_\lambda\_0/N$ behave as a function of $\lambda$, and (ii) How doesthe mean square error $\\hat{x}_{\lambda} - x_o\_2^2/N$ behave as a functionof $\lambda$? We then employ these results in a new, reliable algorithm forsolving LASSO based on approximate message passing (AMP).
arxiv-3900-87 | Efficient Sampling from Time-Varying Log-Concave Distributions | http://arxiv.org/pdf/1309.5977v1.pdf | author:Hariharan Narayanan, Alexander Rakhlin category:stat.ML stat.CO published:2013-09-23 summary:We propose a computationally efficient random walk on a convex body whichrapidly mixes and closely tracks a time-varying log-concave distribution. Wedevelop general theoretical guarantees on the required number of steps; thisnumber can be calculated on the fly according to the distance from and theshape of the next distribution. We then illustrate the technique on severalexamples. Within the context of exponential families, the proposed methodproduces samples from a posterior distribution which is updated as data arrivein a streaming fashion. The sampling technique can be used to tracktime-varying truncated distributions, as well as to obtain samples from achanging mixture model, fitted in a streaming fashion to data. In the settingof linear optimization, the proposed method has oracle complexity with bestknown dependence on the dimension for certain geometries. In the context ofonline learning and repeated games, the algorithm is an efficient method forimplementing no-regret mixture forecasting strategies. Remarkably, in some ofthese examples, only one step of the random walk is needed to track the nextdistribution.
arxiv-3900-88 | Data Mining using Unguided Symbolic Regression on a Blast Furnace Dataset | http://arxiv.org/pdf/1309.5931v1.pdf | author:Michael Kommenda, Gabriel Kronberger, Christoph Feilmayr, Michael Affenzeller category:cs.NE published:2013-09-23 summary:In this paper a data mining approach for variable selection and knowledgeextraction from datasets is presented. The approach is based on unguidedsymbolic regression (every variable present in the dataset is treated as thetarget variable in multiple regression runs) and a novel variable relevancemetric for genetic programming. The relevance of each input variable iscalculated and a model approximating the target variable is created. Thegenetic programming configurations with different target variables are executedmultiple times to reduce stochastic effects and the aggregated results aredisplayed as a variable interaction network. This interaction networkhighlights important system components and implicit relations between thevariables. The whole approach is tested on a blast furnace dataset, because ofthe complexity of the blast furnace and the many interrelations between thevariables. Finally the achieved results are discussed with respect to existingknowledge about the blast furnace process.
arxiv-3900-89 | From Once Upon a Time to Happily Ever After: Tracking Emotions in Novels and Fairy Tales | http://arxiv.org/pdf/1309.5909v1.pdf | author:Saif Mohammad category:cs.CL published:2013-09-23 summary:Today we have access to unprecedented amounts of literary texts. However,search still relies heavily on key words. In this paper, we show how sentimentanalysis can be used in tandem with effective visualizations to quantify andtrack emotions in both individual books and across very large collections. Weintroduce the concept of emotion word density, and using the Brothers Grimmfairy tales as example, we show how collections of text can be organized forbetter search. Using the Google Books Corpus we show how to determine anentity's emotion associations from co-occurring words. Finally, we compareemotion words in fairy tales and novels, to show that fairy tales have a muchwider range of emotion word densities than novels.
arxiv-3900-90 | Fenchel Duals for Drifting Adversaries | http://arxiv.org/pdf/1309.5904v1.pdf | author:Suman K Bera, Anamitra R Choudhury, Syamantak Das, Sambuddha Roy, Jayram S. Thatchachar category:cs.LG published:2013-09-23 summary:We describe a primal-dual framework for the design and analysis of onlineconvex optimization algorithms for {\em drifting regret}. Existing literatureshows (nearly) optimal drifting regret bounds only for the $\ell_2$ and the$\ell_1$-norms. Our work provides a connection between these algorithms and theOnline Mirror Descent ($\omd$) updates; one key insight that results from ourwork is that in order for these algorithms to succeed, it suffices to have thegradient of the regularizer to be bounded (in an appropriate norm). Forsituations (like for the $\ell_1$ norm) where the vanilla regularizer does nothave this property, we have to {\em shift} the regularizer to ensure this.Thus, this helps explain the various updates presented in \cite{bansal10,buchbinder12}. We also consider the online variant of the problem with1-lookahead, and with movement costs in the $\ell_2$-norm. Our primal dualapproach yields nearly optimal competitive ratios for this problem.
arxiv-3900-91 | On the Success Rate of Crossover Operators for Genetic Programming with Offspring Selection | http://arxiv.org/pdf/1309.5896v1.pdf | author:Gabriel Kronberger, Stephan Winkler, Michael Affenzeller, Andreas Beham, Stefan Wagner category:cs.NE published:2013-09-23 summary:Genetic programming is a powerful heuristic search technique that is used fora number of real world applications to solve among others regression,classification, and time-series forecasting problems. A lot of progress towardsa theoretic description of genetic programming in form of schema theorems hasbeen made, but the internal dynamics and success factors of genetic programmingare still not fully understood. In particular, the effects of differentcrossover operators in combination with offspring selection are largelyunknown. This contribution sheds light on the ability of well-known GP crossoveroperators to create better offspring when applied to benchmark problems. Weconclude that standard (sub-tree swapping) crossover is a good default choicein combination with offspring selection, and that GP with offspring selectionand random selection of crossover operators can improve the performance of thealgorithm in terms of best solution quality when no solution size constraintsare applied.
arxiv-3900-92 | Smooth minimization of nonsmooth functions with parallel coordinate descent methods | http://arxiv.org/pdf/1309.5885v1.pdf | author:Olivier Fercoq, Peter Richtárik category:cs.DC math.OC stat.ML published:2013-09-23 summary:We study the performance of a family of randomized parallel coordinatedescent methods for minimizing the sum of a nonsmooth and separable convexfunctions. The problem class includes as a special case L1-regularized L1regression and the minimization of the exponential loss ("AdaBoost problem").We assume the input data defining the loss function is contained in a sparse$m\times n$ matrix $A$ with at most $\omega$ nonzeros in each row. Our methodsneed $O(n \beta/\tau)$ iterations to find an approximate solution with highprobability, where $\tau$ is the number of processors and $\beta = 1 +(\omega-1)(\tau-1)/(n-1)$ for the fastest variant. The notation hidesdependence on quantities such as the required accuracy and confidence levelsand the distance of the starting iterate from an optimal point. Since$\beta/\tau$ is a decreasing function of $\tau$, the method needs feweriterations when more processors are used. Certain variants of our algorithmsperform on average only $O(\nnz(A)/n)$ arithmetic operations during a singleiteration per processor and, because $\beta$ decreases when $\omega$ does,fewer iterations are needed for sparser problems.
arxiv-3900-93 | Scalable Spectral Algorithms for Community Detection in Directed Networks | http://arxiv.org/pdf/1211.6807v2.pdf | author:Sungmin Kim, Tao Shi category:cs.SI physics.soc-ph stat.ML published:2012-11-29 summary:Community detection has been one of the central problems in network studiesand directed network is particularly challenging due to asymmetry among itslinks. In this paper, we found that incorporating the direction of linksreveals new perspectives on communities regarding to two different roles,source and terminal, that a node plays in each community. Intriguingly, suchcommunities appear to be connected with unique spectral property of the graphLaplacian of the adjacency matrix and we exploit this connection by usingregularized SVD methods. We propose harvesting algorithms, coupled withregularized SVDs, that are linearly scalable for efficient identification ofcommunities in huge directed networks. The proposed algorithm shows greatperformance and scalability on benchmark networks in simulations andsuccessfully recovers communities in real network applications.
arxiv-3900-94 | Macro-Economic Time Series Modeling and Interaction Networks | http://arxiv.org/pdf/1212.2044v2.pdf | author:Gabriel Kronberger, Stefan Fink, Michael Kommenda, Michael Affenzeller category:cs.NE stat.AP published:2012-12-10 summary:Macro-economic models describe the dynamics of economic quantities. Theestimations and forecasts produced by such models play a substantial role forfinancial and political decisions. In this contribution we describe an approachbased on genetic programming and symbolic regression to identify variableinteractions in large datasets. In the proposed approach multiple symbolicregression runs are executed for each variable of the dataset to findpotentially interesting models. The result is a variable interaction networkthat describes which variables are most relevant for the approximation of eachvariable of the dataset. This approach is applied to a macro-economic datasetwith monthly observations of important economic indicators in order to identifypotentially interesting dependencies of these indicators. The resultinginteraction network of macro-economic indicators is briefly discussed and twoof the identified models are presented in detail. The two models approximatethe help wanted index and the CPI inflation in the US.
arxiv-3900-95 | Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet | http://arxiv.org/pdf/1309.5843v1.pdf | author:Marco Guerini, Lorenzo Gatti, Marco Turchi category:cs.CL published:2013-09-23 summary:Assigning a positive or negative score to a word out of context (i.e. aword's prior polarity) is a challenging task for sentiment analysis. In theliterature, various approaches based on SentiWordNet have been proposed. Inthis paper, we compare the most often used techniques together with newlyproposed ones and incorporate all of them in a learning framework to seewhether blending them can further improve the estimation of prior polarityscores. Using two different versions of SentiWordNet and testing regression andclassification models across tasks and datasets, our learning approachconsistently outperforms the single metrics, providing a new state-of-the-artapproach in computing words' prior polarity for sentiment analysis. We concludeour investigation showing interesting biases in calculated prior polarityscores when word Part of Speech and annotator gender are considered.
arxiv-3900-96 | A Kernel Classification Framework for Metric Learning | http://arxiv.org/pdf/1309.5823v1.pdf | author:Faqiang Wang, Wangmeng Zuo, Lei Zhang, Deyu Meng, David Zhang category:cs.LG I.5.1 published:2013-09-23 summary:Learning a distance metric from the given training samples plays a crucialrole in many machine learning tasks, and various models and optimizationalgorithms have been proposed in the past decade. In this paper, we generalizeseveral state-of-the-art metric learning methods, such as large margin nearestneighbor (LMNN) and information theoretic metric learning (ITML), into a kernelclassification framework. First, doublets and triplets are constructed from thetraining samples, and a family of degree-2 polynomial kernel functions areproposed for pairs of doublets or triplets. Then, a kernel classificationframework is established, which can not only generalize many popular metriclearning methods such as LMNN and ITML, but also suggest new metric learningmethods, which can be efficiently implemented, interestingly, by using thestandard support vector machine (SVM) solvers. Two novel metric learningmethods, namely doublet-SVM and triplet-SVM, are then developed under theproposed framework. Experimental results show that doublet-SVM and triplet-SVMachieve competitive classification accuracies with state-of-the-art metriclearning methods such as ITML and LMNN but with significantly less trainingtime.
arxiv-3900-97 | Feature Learning with Gaussian Restricted Boltzmann Machine for Robust Speech Recognition | http://arxiv.org/pdf/1309.6176v1.pdf | author:Xin Zheng, Zhiyong Wu, Helen Meng, Weifeng Li, Lianhong Cai category:cs.CL cs.LG cs.SD published:2013-09-23 summary:In this paper, we first present a new variant of Gaussian restrictedBoltzmann machine (GRBM) called multivariate Gaussian restricted Boltzmannmachine (MGRBM), with its definition and learning algorithm. Then we proposeusing a learned GRBM or MGRBM to extract better features for robust speechrecognition. Our experiments on Aurora2 show that both GRBM-extracted andMGRBM-extracted feature performs much better than Mel-frequency cepstralcoefficient (MFCC) with either HMM-GMM or hybrid HMM-deep neural network (DNN)acoustic model, and MGRBM-extracted feature is slightly better.
arxiv-3900-98 | Spike Synchronization Dynamics of Small-World Networks | http://arxiv.org/pdf/1309.5660v1.pdf | author:Derek Harter category:cs.NE nlin.AO q-bio.NC published:2013-09-22 summary:In this research report, we examine the effects of small-world networkorganization on spike synchronization dynamics in networks of Izhikevichspiking units. We interpolate network organizations from regular ring lattices,through the small-world region, to random networks, and measure global spikesynchronization dynamics. We examine how average path length and clusteringeffect the dynamics of global and neighborhood clique spike organization andpropagation. We show that the emergence of global synchronization undergoes aphase transition in the small-world region, between the clustering and pathlength phase transitions that are known to exist. We add additional realisticconstraints on the dynamics by introducing propagation delays of spikingsignals proportional to wiring length. The addition of delays interferes withthe ability of random networks to sustain global synchronization, in relationto the breakdown of clustering in the networks. The addition of delays furtherenhances the finding that small-world organization is beneficial for balancingneighborhood synchronized waves of organization with global synchronizationdynamics.
arxiv-3900-99 | A Hybrid Algorithm for Matching Arabic Names | http://arxiv.org/pdf/1309.5657v1.pdf | author:T. El-Shishtawy category:cs.CL published:2013-09-22 summary:In this paper, a new hybrid algorithm which combines both of token-based andcharacter-based approaches is presented. The basic Levenshtein approach hasbeen extended to token-based distance metric. The distance metric is enhancedto set the proper granularity level behavior of the algorithm. It smoothly mapsa threshold of misspellings differences at the character level, and theimportance of token level errors in terms of token's position and frequency.Using a large Arabic dataset, the experimental results show that the proposedalgorithm overcomes successfully many types of errors such as: typographicalerrors, omission or insertion of middle name components, omission ofnon-significant popular name components, and different writing styles charactervariations. When compared the results with other classical algorithms, usingthe same dataset, the proposed algorithm was found to increase the minimumsuccess level of best tested algorithms, while achieving higher upper limits .
arxiv-3900-100 | LDC Arabic Treebanks and Associated Corpora: Data Divisions Manual | http://arxiv.org/pdf/1309.5652v1.pdf | author:Mona Diab, Nizar Habash, Owen Rambow, Ryan Roth category:cs.CL published:2013-09-22 summary:The Linguistic Data Consortium (LDC) has developed hundreds of data corporafor natural language processing (NLP) research. Among these are a number ofannotated treebank corpora for Arabic. Typically, these corpora consist of asingle collection of annotated documents. NLP research, however, usuallyrequires multiple data sets for the purposes of training models, developingtechniques, and final evaluation. Therefore it becomes necessary to divide thecorpora used into the required data sets (divisions). This document details aset of rules that have been defined to enable consistent divisions for old andnew Arabic treebanks (ATB) and related corpora.
arxiv-3900-101 | Stochastic Bound Majorization | http://arxiv.org/pdf/1309.5605v1.pdf | author:Anna Choromanska, Tony Jebara category:cs.LG published:2013-09-22 summary:Recently a majorization method for optimizing partition functions oflog-linear models was proposed alongside a novel quadratic variationalupper-bound. In the batch setting, it outperformed state-of-the-art first- andsecond-order optimization methods on various learning tasks. We propose astochastic version of this bound majorization method as well as a low-rankmodification for high-dimensional data-sets. The resulting stochasticsecond-order method outperforms stochastic gradient descent (across variationsand various tunings) both in terms of the number of iterations and computationtime till convergence while finding a better quality parameter setting. Theproposed method bridges first- and second-order stochastic optimization methodsby maintaining a computational complexity that is linear in the data dimensionand while exploiting second order information about the pseudo-global curvatureof the objective function (as opposed to the local curvature in the Hessian).
arxiv-3900-102 | Stochastic First- and Zeroth-order Methods for Nonconvex Stochastic Programming | http://arxiv.org/pdf/1309.5549v1.pdf | author:Saeed Ghadimi, Guanghui Lan category:math.OC cs.CC stat.ML published:2013-09-22 summary:In this paper, we introduce a new stochastic approximation (SA) typealgorithm, namely the randomized stochastic gradient (RSG) method, for solvingan important class of nonlinear (possibly nonconvex) stochastic programming(SP) problems. We establish the complexity of this method for computing anapproximate stationary point of a nonlinear programming problem. We also showthat this method possesses a nearly optimal rate of convergence if the problemis convex. We discuss a variant of the algorithm which consists of applying apost-optimization phase to evaluate a short list of solutions generated byseveral independent runs of the RSG method, and show that such modificationallows to improve significantly the large-deviation properties of thealgorithm. These methods are then specialized for solving a class ofsimulation-based optimization problems in which only stochastic zeroth-orderinformation is available.
arxiv-3900-103 | Latent Fisher Discriminant Analysis | http://arxiv.org/pdf/1309.5427v1.pdf | author:Gang Chen category:cs.LG cs.CV stat.ML I.2.10 published:2013-09-21 summary:Linear Discriminant Analysis (LDA) is a well-known method for dimensionalityreduction and classification. Previous studies have also extended thebinary-class case into multi-classes. However, many applications, such asobject detection and keyframe extraction cannot provide consistentinstance-label pairs, while LDA requires labels on instance level for training.Thus it cannot be directly applied for semi-supervised classification problem.In this paper, we overcome this limitation and propose a latent variable Fisherdiscriminant analysis model. We relax the instance-level labeling intobag-level, is a kind of semi-supervised (video-level labels of event type arerequired for semantic frame extraction) and incorporates a data-driven priorover the latent variables. Hence, our method combines the latent variableinference and dimension reduction in an unified bayesian framework. We test ourmethod on MUSK and Corel data sets and yield competitive results compared tothe baseline approach. We also demonstrate its capacity on the challengingTRECVID MED11 dataset for semantic keyframe extraction and conduct ahuman-factors ranking-based experimental evaluation, which clearly demonstratesour proposed method consistently extracts more semantically meaningfulkeyframes than challenging baselines.
arxiv-3900-104 | Scan-based Compressed Terahertz Imaging and Real-Time Reconstruction via the Complex-valued Fast Block Sparse Bayesian Learning Algorithm | http://arxiv.org/pdf/1309.6195v1.pdf | author:Benyuan Liu, Hongqi Fan, Zaiqi Lu, Qiang Fu category:cs.CV published:2013-09-20 summary:Compressed Sensing based Terahertz imaging (CS-THz) is a computationalimaging technique. It uses only one THz receiver to accumulate the randommodulated image measurements where the original THz image is reconstruct fromthese measurements using compressed sensing solvers. The advantage of theCS-THz is its reduced acquisition time compared with the raster scan mode.However, when it applied to large-scale two-dimensional (2D) imaging, theincreased dimension resulted in both high computational complexity andexcessive memory usage. In this paper, we introduced a novel CS-based THzimaging system that progressively compressed the THz image column by column.Therefore, the CS-THz system could be simplified with a much smaller sizedmodulator and reduced dimension. In order to utilize the block structure andthe correlation of adjacent columns of the THz image, a complex-valued blocksparse Bayesian learning algorithm was proposed. We conducted systematicevaluation of state-of-the-art CS algorithms under the scan based CS-THzarchitecture. The compression ratios and the choices of the sensing matriceswere analyzed in detail using both synthetic and real-life THz images.Simulation results showed that both the scan based architecture and theproposed recovery algorithm were superior and efficient for large scale CS-THzapplications.
arxiv-3900-105 | Nonmyopic View Planning for Active Object Detection | http://arxiv.org/pdf/1309.5401v1.pdf | author:Nikolay Atanasov, Bharath Sankaran, Jerome Le Ny, George J. Pappas, Kostas Daniilidis category:cs.RO cs.CV cs.SY published:2013-09-20 summary:One of the central problems in computer vision is the detection ofsemantically important objects and the estimation of their pose. Most of thework in object detection has been based on single image processing and itsperformance is limited by occlusions and ambiguity in appearance and geometry.This paper proposes an active approach to object detection by controlling thepoint of view of a mobile depth camera. When an initial static detection phaseidentifies an object of interest, several hypotheses are made about its classand orientation. The sensor then plans a sequence of views, which balances theamount of energy used to move with the chance of identifying the correcthypothesis. We formulate an active hypothesis testing problem, which includessensor mobility, and solve it using a point-based approximate POMDP algorithm.The validity of our approach is verified through simulation and real-worldexperiments with the PR2 robot. The results suggest that our approachoutperforms the widely-used greedy view point selection and provides asignificant improvement over static object detection.
arxiv-3900-106 | Colourful Language: Measuring Word-Colour Associations | http://arxiv.org/pdf/1309.5942v1.pdf | author:Saif Mohammad category:cs.CL published:2013-09-20 summary:Since many real-world concepts are associated with colour, for example dangerwith red, linguistic information is often complimented with the use ofappropriate colours in information visualization and product marketing. Yet,there is no comprehensive resource that captures concept-colour associations.We present a method to create a large word-colour association lexicon bycrowdsourcing. We focus especially on abstract concepts and emotions to showthat even though they cannot be physically visualized, they too tend to havestrong colour associations. Finally, we show how word-colour associationsmanifest themselves in language, and quantify usefulness of co-occurrence andpolarity cues in automatically detecting colour associations.
arxiv-3900-107 | Even the Abstract have Colour: Consensus in Word-Colour Associations | http://arxiv.org/pdf/1309.5391v1.pdf | author:Saif M. Mohammad category:cs.CL published:2013-09-20 summary:Colour is a key component in the successful dissemination of information.Since many real-world concepts are associated with colour, for example dangerwith red, linguistic information is often complemented with the use ofappropriate colours in information visualization and product marketing. Yet,there is no comprehensive resource that captures concept-colour associations.We present a method to create a large word-colour association lexicon bycrowdsourcing. A word-choice question was used to obtain sense-levelannotations and to ensure data quality. We focus especially on abstractconcepts and emotions to show that even they tend to have strong colourassociations. Thus, using the right colours can not only improve semanticcoherence, but also inspire the desired emotional response.
arxiv-3900-108 | Recovering Non-negative and Combined Sparse Representations | http://arxiv.org/pdf/1303.4694v2.pdf | author:Karthikeyan Natesan Ramamurthy, Jayaraman J. Thiagarajan, Andreas Spanias category:math.NA cs.LG stat.ML published:2013-03-12 summary:The non-negative solution to an underdetermined linear system can be uniquelyrecovered sometimes, even without imposing any additional sparsity constraints.In this paper, we derive conditions under which a unique non-negative solutionfor such a system can exist, based on the theory of polytopes. Furthermore, wedevelop the paradigm of combined sparse representations, where only a part ofthe coefficient vector is constrained to be non-negative, and the rest isunconstrained (general). We analyze the recovery of the unique, sparsestsolution, for combined representations, under three different cases ofcoefficient support knowledge: (a) the non-zero supports of non-negative andgeneral coefficients are known, (b) the non-zero support of generalcoefficients alone is known, and (c) both the non-zero supports are unknown.For case (c), we propose the combined orthogonal matching pursuit algorithm forcoefficient recovery and derive the deterministic sparsity threshold underwhich recovery of the unique, sparsest coefficient vector is possible. Wequantify the order complexity of the algorithms, and examine their performancein exact and approximate recovery of coefficients under various conditions ofnoise. Furthermore, we also obtain their empirical phase transitioncharacteristics. We show that the basis pursuit algorithm, with partialnon-negative constraints, and the proposed greedy algorithm perform better inrecovering the unique sparse representation when compared to theirunconstrained counterparts. Finally, we demonstrate the utility of the proposedmethods in recovering images corrupted by saturation noise.
arxiv-3900-109 | Recognizing Speech in a Novel Accent: The Motor Theory of Speech Perception Reframed | http://arxiv.org/pdf/1309.5319v1.pdf | author:Clément Moulin-Frier, M. A. Arbib category:cs.CL cs.LG q-bio.NC published:2013-09-20 summary:The motor theory of speech perception holds that we perceive the speech ofanother in terms of a motor representation of that speech. However, when wehave learned to recognize a foreign accent, it seems plausible that recognitionof a word rarely involves reconstruction of the speech gestures of the speakerrather than the listener. To better assess the motor theory and thisobservation, we proceed in three stages. Part 1 places the motor theory ofspeech perception in a larger framework based on our earlier models of theadaptive formation of mirror neurons for grasping, and for viewing extensionsof that mirror system as part of a larger system for neuro-linguisticprocessing, augmented by the present consideration of recognizing speech in anovel accent. Part 2 then offers a novel computational model of how a listenercomes to understand the speech of someone speaking the listener's nativelanguage with a foreign accent. The core tenet of the model is that thelistener uses hypotheses about the word the speaker is currently uttering toupdate probabilities linking the sound produced by the speaker to phonemes inthe native language repertoire of the listener. This, on average, improves therecognition of later words. This model is neutral regarding the nature of therepresentations it uses (motor vs. auditory). It serve as a reference point forthe discussion in Part 3, which proposes a dual-stream neuro-linguisticarchitecture to revisits claims for and against the motor theory of speechperception and the relevance of mirror neurons, and extracts some implicationsfor the reframing of the motor theory.
arxiv-3900-110 | An introduction to the Europe Media Monitor family of applications | http://arxiv.org/pdf/1309.5290v1.pdf | author:Ralf Steinberger, Bruno Pouliquen, Erik van der Goot category:cs.CL published:2013-09-20 summary:Most large organizations have dedicated departments that monitor the media tokeep up-to-date with relevant developments and to keep an eye on how they arerepresented in the news. Part of this media monitoring work can be automated.In the European Union with its 23 official languages, it is particularlyimportant to cover media reports in many languages in order to capture thecomplementary news content published in the different countries. It is alsoimportant to be able to access the news content across languages and to mergethe extracted information. We present here the four publicly accessible systemsof the Europe Media Monitor (EMM) family of applications, which cover between19 and 50 languages (see http://press.jrc.it/overview.html). We give anoverview of their functionality and discuss some of the implications of thefact that they cover quite so many languages. We discuss design issuesnecessary to be able to achieve this high multilinguality, as well as thebenefits of this multilinguality.
arxiv-3900-111 | Scalable Anomaly Detection in Large Homogenous Populations | http://arxiv.org/pdf/1309.5803v1.pdf | author:Henrik Ohlsson, Tianshi Chen, Sina Khoshfetrat Pakazad, Lennart Ljung, S. Shankar Sastry category:cs.LG cs.DC cs.SY math.OC published:2013-09-20 summary:Anomaly detection in large populations is a challenging but highly relevantproblem. The problem is essentially a multi-hypothesis problem, with ahypothesis for every division of the systems into normal and anomal systems.The number of hypothesis grows rapidly with the number of systems andapproximate solutions become a necessity for any problems of practicalinterests. In the current paper we take an optimization approach to thismulti-hypothesis problem. We first observe that the problem is equivalent to anon-convex combinatorial optimization problem. We then relax the problem to aconvex problem that can be solved distributively on the systems and that stayscomputationally tractable as the number of systems increase. An interestingproperty of the proposed method is that it can under certain conditions beshown to give exactly the same result as the combinatorial multi-hypothesisproblem and the relaxation is hence tight.
arxiv-3900-112 | DGT-TM: A freely Available Translation Memory in 22 Languages | http://arxiv.org/pdf/1309.5226v1.pdf | author:Ralf Steinberger, Andreas Eisele, Szymon Klocek, Spyridon Pilos, Patrick Schlüter category:cs.CL I.2.7 published:2013-09-20 summary:The European Commission's (EC) Directorate General for Translation, togetherwith the EC's Joint Research Centre, is making available a large translationmemory (TM; i.e. sentences and their professionally produced translations)covering twenty-two official European Union (EU) languages and their 231language pairs. Such a resource is typically used by translation professionalsin combination with TM software to improve speed and consistency of theirtranslations. However, this resource has also many uses for translation studiesand for language technology applications, including Statistical MachineTranslation (SMT), terminology extraction, Named Entity Recognition (NER),multilingual classification and clustering, and many more. In this referencepaper for DGT-TM, we introduce this new resource, provide statistics regardingits size, and explain how it was produced and how to use it.
arxiv-3900-113 | JRC EuroVoc Indexer JEX - A freely available multi-label categorisation tool | http://arxiv.org/pdf/1309.5223v1.pdf | author:Ralf Steinberger, Mohamed Ebrahim, Marco Turchi category:cs.CL H.3.1; H.3.6 published:2013-09-20 summary:EuroVoc (2012) is a highly multilingual thesaurus consisting of over 6,700hierarchically organised subject domains used by European Institutions and manyauthorities in Member States of the European Union (EU) for the classificationand retrieval of official documents. JEX is JRC-developed multi-labelclassification software that learns from manually labelled data toautomatically assign EuroVoc descriptors to new documents in a profile-basedcategory-ranking task. The JEX release consists of trained classifiers for 22official EU languages, of parallel training data in the same languages, of aninterface that allows viewing and amending the assignment results, and of amodule that allows users to re-train the tool on their own documentcollections. JEX allows advanced users to change the document representation soas to possibly improve the categorisation result through linguisticpre-processing. JEX can be used as a tool for interactive EuroVoc descriptorassignment to increase speed and consistency of the human categorisationprocess, or it can be used fully automatically. The output of JEX is alanguage-independent EuroVoc feature vector lending itself also as input tovarious other Language Technology tasks, including cross-lingual clustering andclassification, cross-lingual plagiarism detection, sentence selection andranking, and more.
arxiv-3900-114 | Maxout Networks | http://arxiv.org/pdf/1302.4389v4.pdf | author:Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, Yoshua Bengio category:stat.ML cs.LG published:2013-02-18 summary:We consider the problem of designing models to leverage a recently introducedapproximate model averaging technique called dropout. We define a simple newmodel called maxout (so named because its output is the max of a set of inputs,and because it is a natural companion to dropout) designed to both facilitateoptimization by dropout and improve the accuracy of dropout's fast approximatemodel averaging technique. We empirically verify that the model successfullyaccomplishes both of these tasks. We use maxout and dropout to demonstratestate of the art classification performance on four benchmark datasets: MNIST,CIFAR-10, CIFAR-100, and SVHN.
arxiv-3900-115 | mTim: Rapid and accurate transcript reconstruction from RNA-Seq data | http://arxiv.org/pdf/1309.5211v1.pdf | author:Georg Zeller, Nico Goernitz, Andre Kahles, Jonas Behr, Pramod Mudrakarta, Soeren Sonnenburg, Gunnar Raetsch category:q-bio.GN stat.ML published:2013-09-20 summary:Recent advances in high-throughput cDNA sequencing (RNA-Seq) technology haverevolutionized transcriptome studies. A major motivation for RNA-Seq is to mapthe structure of expressed transcripts at nucleotide resolution. With accuratecomputational tools for transcript reconstruction, this technology may alsobecome useful for genome (re-)annotation, which has mostly relied on de novogene finding where gene structures are primarily inferred from the genomesequence. We developed a machine-learning method, called mTim (margin-basedtranscript inference method) for transcript reconstruction from RNA-Seq readalignments that is based on discriminatively trained hidden Markov supportvector machines. In addition to features derived from read alignments, itutilizes characteristic genomic sequences, e.g. around splice sites, to improvetranscript predictions. mTim inferred transcripts that were highly accurate andrelatively robust to alignment errors in comparison to those from Cufflinks, awidely used transcript assembly method.
arxiv-3900-116 | Saying What You're Looking For: Linguistics Meets Video Search | http://arxiv.org/pdf/1309.5174v1.pdf | author:Andrei Barbu, N. Siddharth, Jeffrey Mark Siskind category:cs.CV cs.CL cs.IR published:2013-09-20 summary:We present an approach to searching large video corpora for video clips whichdepict a natural-language query in the form of a sentence. This approach usescompositional semantics to encode subtle meaning that is lost in other systems,such as the difference between two sentences which have identical words butentirely different meaning: "The person rode the horse} vs. \emph{The horserode the person". Given a video-sentence pair and a natural-language parser,along with a grammar that describes the space of sentential queries, we producea score which indicates how well the video depicts the sentence. We producesuch a score for each video clip in a corpus and return a ranked list of clips.Furthermore, this approach addresses two fundamental problems simultaneously:detecting and tracking objects, and recognizing whether those tracks depict thequery. Because both tracking and object detection are unreliable, this usesknowledge about the intended sentential query to focus the tracker on therelevant participants and ensures that the resulting tracks are described bythe sentential query. While earlier work was limited to single-word querieswhich correspond to either verbs or nouns, we show how one can search forcomplex queries which contain multiple phrases, such as prepositional phrases,and modifiers, such as adverbs. We demonstrate this approach by searching for141 queries involving people and horses interacting with each other in 10full-length Hollywood movies.
arxiv-3900-117 | An ant colony optimization algorithm for job shop scheduling problem | http://arxiv.org/pdf/1309.5110v1.pdf | author:Edson Flórez, Wilfredo Gómez, Lola Bautista category:cs.AI cs.NE published:2013-09-19 summary:The nature has inspired several metaheuristics, outstanding among these isAnt Colony Optimization (ACO), which have proved to be very effective andefficient in problems of high complexity (NP-hard) in combinatorialoptimization. This paper describes the implementation of an ACO model algorithmknown as Elitist Ant System (EAS), applied to a combinatorial optimizationproblem called Job Shop Scheduling Problem (JSSP). We propose a method thatseeks to reduce delays designating the operation immediately available, butconsidering the operations that lack little to be available and have a greateramount of pheromone. The performance of the algorithm was evaluated forproblems of JSSP reference, comparing the quality of the solutions obtainedregarding the best known solution of the most effective methods. The solutionswere of good quality and obtained with a remarkable efficiency by having tomake a very low number of objective function evaluations.
arxiv-3900-118 | A Comparative Analysis of Ensemble Classifiers: Case Studies in Genomics | http://arxiv.org/pdf/1309.5047v1.pdf | author:Sean Whalen, Gaurav Pandey category:cs.LG q-bio.GN stat.ML published:2013-09-19 summary:The combination of multiple classifiers using ensemble methods isincreasingly important for making progress in a variety of difficult predictionproblems. We present a comparative analysis of several ensemble methods throughtwo case studies in genomics, namely the prediction of genetic interactions andprotein functions, to demonstrate their efficacy on real-world datasets anddraw useful conclusions about their behavior. These methods include simpleaggregation, meta-learning, cluster-based meta-learning, and ensemble selectionusing heterogeneous classifiers trained on resampled data to improve thediversity of their predictions. We present a detailed analysis of these methodsacross 4 genomics datasets and find the best of these methods offerstatistically significant improvements over the state of the art in theirrespective domains. In addition, we establish a novel connection betweenensemble selection and meta-learning, demonstrating how both of these disparatemethods establish a balance between ensemble diversity and performance.
arxiv-3900-119 | Constructing Runge-Kutta Methods with the Use of Artificial Neural Networks | http://arxiv.org/pdf/1106.1194v2.pdf | author:Angelos A. Anastassi category:cs.NE math.NA 68T05, 65L06 published:2011-06-06 summary:A methodology that can generate the optimal coefficients of a numericalmethod with the use of an artificial neural network is presented in this work.The network can be designed to produce a finite difference algorithm thatsolves a specific system of ordinary differential equations numerically. Thecase we are examining here concerns an explicit two-stage Runge-Kutta methodfor the numerical solution of the two-body problem. Following theimplementation of the network, the latter is trained to obtain the optimalvalues for the coefficients of the Runge-Kutta method. The comparison of thenew method to others that are well known in the literature proves itsefficiency and demonstrates the capability of the network to provide efficientalgorithms for specific problems.
arxiv-3900-120 | Blind Deconvolution via Maximum Kurtosis Adaptive Filtering | http://arxiv.org/pdf/1309.5004v1.pdf | author:Deborah Pereg, Doron Benzvi category:cs.CV published:2013-09-19 summary:In this paper, we present an algorithm for identifying a parametricallydescribed destructive unknown system based on a non-gaussianity measure. It isknown that under certain conditions the output of a linear system is moregaussian than the input. Hence, an inverse filter is searched, such that itsoutput is minimally gaussian. We use the kurtosis as a measure of thenon-gaussianity of the signal. A maximum of the kurtosis as a function of thedeconvolving filter coefficients is searched. The search is done iterativelyusing the gradient ascent algorithm, and the coefficients at the maximum pointcorrespond to the inverse filter coefficients. This filter may be applied tothe distorted signal to obtain the original undistorted signal. While a similarapproach has been used before, it was always directed at a particular kind of asignal, commonly of impulsive characteristics. In this paper a successfulattempt has been made to apply the algorithm to a wider range of signals, suchas to process distorted audio signals and destructed images. This innovativeimplementation required the revelation of a way to preprocess the distortedsignal at hand. The experimental results show very good performance in terms ofrecovering audio signals and blurred images, both for an FIR and IIR distortingfilters.
arxiv-3900-121 | HOL(y)Hammer: Online ATP Service for HOL Light | http://arxiv.org/pdf/1309.4962v1.pdf | author:Cezary Kaliszyk, Josef Urban category:cs.AI cs.DL cs.LG cs.LO cs.MS published:2013-09-19 summary:HOL(y)Hammer is an online AI/ATP service for formal (computer-understandable)mathematics encoded in the HOL Light system. The service allows its users toupload and automatically process an arbitrary formal development (project)based on HOL Light, and to attack arbitrary conjectures that use the conceptsdefined in some of the uploaded projects. For that, the service uses severalautomated reasoning systems combined with several premise selection methodstrained on all the project proofs. The projects that are readily available onthe server for such query answering include the recent versions of theFlyspeck, Multivariate Analysis and Complex Analysis libraries. The serviceruns on a 48-CPU server, currently employing in parallel for each task 7 AI/ATPcombinations and 4 decision procedures that contribute to its overallperformance. The system is also available for local installation by interestedusers, who can customize it for their own proof development. An Emacs interfaceallowing parallel asynchronous queries to the service is also provided. Theoverall structure of the service is outlined, problems that arise and theirsolutions are discussed, and an initial account of using the system is given.
arxiv-3900-122 | Exploration and Exploitation in Visuomotor Prediction of Autonomous Agents | http://arxiv.org/pdf/1309.7959v1.pdf | author:Laurens Bliek category:cs.LG cs.CV math.DS published:2013-09-19 summary:This paper discusses various techniques to let an agent learn how to predictthe effects of its own actions on its sensor data autonomously, and theirusefulness to apply them to visual sensors. An Extreme Learning Machine is usedfor visuomotor prediction, while various autonomous control techniques that canaid the prediction process by balancing exploration and exploitation arediscussed and tested in a simple system: a camera moving over a 2D greyscaleimage.
arxiv-3900-123 | Optimal detection of sparse principal components in high dimension | http://arxiv.org/pdf/1202.5070v3.pdf | author:Quentin Berthet, Philippe Rigollet category:math.ST stat.ML stat.TH published:2012-02-23 summary:We perform a finite sample analysis of the detection levels for sparseprincipal components of a high-dimensional covariance matrix. Our minimaxoptimal test is based on a sparse eigenvalue statistic. Alas, computing thistest is known to be NP-complete in general, and we describe a computationallyefficient alternative test using convex relaxations. Our relaxation is alsoproved to detect sparse principal components at near optimal detection levels,and it performs well on simulated datasets. Moreover, using polynomial timereductions from theoretical computer science, we bring significant evidencethat our results cannot be improved, thus revealing an inherent trade offbetween statistical and computational performance.
arxiv-3900-124 | Predictive PAC Learning and Process Decompositions | http://arxiv.org/pdf/1309.4859v1.pdf | author:Cosma Rohilla Shalizi, Aryeh Kontorovich category:stat.ML published:2013-09-19 summary:We informally call a stochastic process learnable if it admits ageneralization error approaching zero in probability for any concept class withfinite VC-dimension (IID processes are the simplest example). A mixture oflearnable processes need not be learnable itself, and certainly itsgeneralization error need not decay at the same rate. In this paper, we arguethat it is natural in predictive PAC to condition not on the past observationsbut on the mixture component of the sample path. This definition not onlymatches what a realistic learner might demand, but also allows us to sidestepseveral otherwise grave problems in learning from dependent data. Inparticular, we give a novel PAC generalization bound for mixtures of learnableprocesses with a generalization error that is not worse than that of eachmixture component. We also provide a characterization of mixtures of absolutelyregular ($\beta$-mixing) processes, of independent probability-theoreticinterest.
arxiv-3900-125 | Network Anomaly Detection: A Survey and Comparative Analysis of Stochastic and Deterministic Methods | http://arxiv.org/pdf/1309.4844v1.pdf | author:Jing Wang, Daniel Rossell, Christos G. Cassandras, Ioannis Ch. Paschalidis category:stat.ML cs.LG cs.NI published:2013-09-19 summary:We present five methods to the problem of network anomaly detection. Thesemethods cover most of the common techniques in the anomaly detection field,including Statistical Hypothesis Tests (SHT), Support Vector Machines (SVM) andclustering analysis. We evaluate all methods in a simulated network thatconsists of nominal data, three flow-level anomalies and one packet-levelattack. Through analyzing the results, we point out the advantages anddisadvantages of each method and conclude that combining the results of theindividual methods can yield improved anomaly detection results.
arxiv-3900-126 | Learning joint intensity-depth sparse representations | http://arxiv.org/pdf/1201.0566v2.pdf | author:Ivana Tosic, Sarah Drewes category:cs.CV published:2012-01-03 summary:This paper presents a method for learning overcomplete dictionaries composedof two modalities that describe a 3D scene: image intensity and scene depth. Wepropose a novel Joint Basis Pursuit (JBP) algorithm that finds related sparsefeatures in two modalities using conic programming and integrate it into atwo-step dictionary learning algorithm. JBP differs from related convexalgorithms because it finds joint sparsity models with different atoms anddifferent coefficient values for intensity and depth. This is crucial forrecovering generative models where the same sparse underlying causes (3Dfeatures) give rise to different signals (intensity and depth). We give atheoretical bound for the sparse coefficient recovery error obtained by JBP,and show experimentally that JBP is far superior to the state of the art GroupLasso algorithm. When applied to the Middlebury depth-intensity database, ourlearning algorithm converges to a set of related features, such as pairs ofdepth and intensity edges or image textures and depth slants. Finally, we showthat the learned dictionary and JBP achieve the state of the art depthinpainting performance on time-of-flight 3D data.
arxiv-3900-127 | Mapping Mutable Genres in Structurally Complex Volumes | http://arxiv.org/pdf/1309.3323v2.pdf | author:Ted Underwood, Michael L. Black, Loretta Auvil, Boris Capitanu category:cs.CL cs.DL published:2013-09-12 summary:To mine large digital libraries in humanistically meaningful ways, scholarsneed to divide them by genre. This is a task that classification algorithms arewell suited to assist, but they need adjustment to address the specificchallenges of this domain. Digital libraries pose two problems of scale notusually found in the article datasets used to test these algorithms. 1) Becauselibraries span several centuries, the genres being identified may changegradually across the time axis. 2) Because volumes are much longer thanarticles, they tend to be internally heterogeneous, and the classification taskneeds to begin with segmentation. We describe a multi-layered solution thattrains hidden Markov models to segment volumes, and uses ensembles ofoverlapping classifiers to address historical change. We test this approach ona collection of 469,200 volumes drawn from HathiTrust Digital Library. Todemonstrate the humanistic value of these methods, we extract 32,209 volumes offiction from the digital library, and trace the changing proportions of first-and third-person narration in the corpus. We note that narrative points of viewseem to have strong associations with particular themes and genres.
arxiv-3900-128 | Temporal-Difference Learning to Assist Human Decision Making during the Control of an Artificial Limb | http://arxiv.org/pdf/1309.4714v1.pdf | author:Ann L. Edwards, Alexandra Kearney, Michael Rory Dawson, Richard S. Sutton, Patrick M. Pilarski category:cs.AI cs.LG cs.RO published:2013-09-18 summary:In this work we explore the use of reinforcement learning (RL) to help withhuman decision making, combining state-of-the-art RL algorithms with anapplication to prosthetics. Managing human-machine interaction is a problem ofconsiderable scope, and the simplification of human-robot interfaces isespecially important in the domains of biomedical technology and rehabilitationmedicine. For example, amputees who control artificial limbs are often requiredto quickly switch between a number of control actions or modes of operation inorder to operate their devices. We suggest that by learning to anticipate(predict) a user's behaviour, artificial limbs could take on an active role ina human's control decisions so as to reduce the burden on their users.Recently, we showed that RL in the form of general value functions (GVFs) couldbe used to accurately detect a user's control intent prior to their explicitcontrol choices. In the present work, we explore the use of temporal-differencelearning and GVFs to predict when users will switch their control influencebetween the different motor functions of a robot arm. Experiments wereperformed using a multi-function robot arm that was controlled by musclesignals from a user's body (similar to conventional artificial limb control).Our approach was able to acquire and maintain forecasts about a user'sswitching decisions in real time. It also provides an intuitive and reward-freeway for users to correct or reinforce the decisions made by the machinelearning system. We expect that when a system is certain enough about itspredictions, it can begin to take over switching decisions from the user tostreamline control and potentially decrease the time and effort needed tocomplete tasks. This preliminary study therefore suggests a way to naturallyintegrate human- and machine-based decision making systems.
arxiv-3900-129 | Distribution-Dependent Sample Complexity of Large Margin Learning | http://arxiv.org/pdf/1204.1276v4.pdf | author:Sivan Sabato, Nathan Srebro, Naftali Tishby category:stat.ML cs.LG published:2012-04-05 summary:We obtain a tight distribution-specific characterization of the samplecomplexity of large-margin classification with L2 regularization: We introducethe margin-adapted dimension, which is a simple function of the second orderstatistics of the data distribution, and show distribution-specific upper andlower bounds on the sample complexity, both governed by the margin-adapteddimension of the data distribution. The upper bounds are universal, and thelower bounds hold for the rich family of sub-Gaussian distributions withindependent features. We conclude that this new quantity tightly characterizesthe true sample complexity of large-margin classification. To prove the lowerbound, we develop several new tools of independent interest. These include newconnections between shattering and hardness of learning, new properties ofshattering with linear classifiers, and a new lower bound on the smallesteigenvalue of a random Gram matrix generated by sub-Gaussian variables. Ourresults can be used to quantitatively compare large margin learning to otherlearning rules, and to improve the effectiveness of methods that use samplecomplexity bounds, such as active learning.
arxiv-3900-130 | Text segmentation with character-level text embeddings | http://arxiv.org/pdf/1309.4628v1.pdf | author:Grzegorz Chrupała category:cs.CL published:2013-09-18 summary:Learning word representations has recently seen much success in computationallinguistics. However, assuming sequences of word tokens as input to linguisticanalysis is often unjustified. For many languages word segmentation is anon-trivial task and naturally occurring text is sometimes a mixture of naturallanguage strings and other character data. We propose to learn textrepresentations directly from raw character sequences by training a Simplerecurrent Network to predict the next character in text. The network uses itshidden layer to evolve abstract representations of the character sequences itsees. To demonstrate the usefulness of the learned text embeddings, we use themas features in a supervised character level text segmentation and labelingtask: recognizing spans of text containing programming language code. By usingthe embeddings as features we are able to substantially improve over a baselinewhich uses only surface character n-grams.
arxiv-3900-131 | A novel approach to nose-tip and eye corners detection using H-K Curvature Analysis in case of 3D images | http://arxiv.org/pdf/1309.4582v1.pdf | author:Parama Bagchi, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu category:cs.CV published:2013-09-18 summary:In this paper we present a novel method that combines a HK curvature-basedapproach for three-dimensional (3D) face detection in different poses (X-axis,Y-axis and Z-axis). Salient face features, such as the eyes and nose, aredetected through an analysis of the curvature of the entire facial surface. Allthe experiments have been performed on the FRAV3D Database. After applying theproposed algorithm to the 3D facial surface we have obtained considerably goodresults i.e. on 752 3D face images our method detected the eye corners for 543face images, thus giving a 72.20% of eye corners detection and 743 face imagesfor nose-tip detection thus giving a 98.80% of good nose tip localization
arxiv-3900-132 | Detection of pose orientation across single and multiple axes in case of 3D face images | http://arxiv.org/pdf/1309.4577v1.pdf | author:Parama Bagchi, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu category:cs.CV published:2013-09-18 summary:In this paper, we propose a new approach that takes as input a 3D face imageacross X, Y and Z axes as well as both Y and X axes and gives output as itspose i.e. it tells whether the face is oriented with respect the X, Y or Z axesor is it oriented across multiple axes with angles of rotation up to 42 degree.All the experiments have been performed on the FRAV3D, GAVADB and Bosphorusdatabase which has two figures of each individual across multiple axes. Afterapplying the proposed algorithm to the 3D facial surface from FRAV3D on 848 3Dfaces, 566 3D faces were correctly recognized for pose thus giving 67% ofcorrect identification rate. We had experimented on 420 images from the GAVADBdatabase, and only 336 images were detected for correct pose identificationrate i.e. 80% and from Bosphorus database on 560 images only 448 images weredetected for correct pose identification i.e. 80%.abstract goes here.
arxiv-3900-133 | A novel approach for nose tip detection using smoothing by weighted median filtering applied to 3D face images in variant poses | http://arxiv.org/pdf/1309.4573v1.pdf | author:Parama Bagchi, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu category:cs.CV published:2013-09-18 summary:This paper is based on an application of smoothing of 3D face images followedby feature detection i.e. detecting the nose tip. The present method uses aweighted mesh median filtering technique for smoothing. In this presentsmoothing technique we have built the neighborhood surrounding a particularpoint in 3D face and replaced that with the weighted value of the surroundingpoints in 3D face image. After applying the smoothing technique to the 3D faceimages our experimental results show that we have obtained considerableimprovement as compared to the algorithm without smoothing. We have used herethe maximum intensity algorithm for detecting the nose-tip and this methodcorrectly detects the nose-tip in case of any pose i.e. along X, Y, and Z axes.The present technique gave us worked successfully on 535 out of 542 3D faceimages as compared to the method without smoothing which worked only on 521 3Dface images out of 542 face images. Thus we have obtained a 98.70% performancerate over 96.12% performance rate of the algorithm without smoothing. All theexperiments have been performed on the FRAV3D database.
arxiv-3900-134 | Bayesian rules and stochastic models for high accuracy prediction of solar radiation | http://arxiv.org/pdf/1309.4999v1.pdf | author:Cyril Voyant, C. Darras, Marc Muselli, Christophe Paoli, Marie Laure Nivet, Philippe Poggi category:cs.LG stat.AP published:2013-09-18 summary:It is essential to find solar predictive methods to massively insertrenewable energies on the electrical distribution grid. The goal of this studyis to find the best methodology allowing predicting with high accuracy thehourly global radiation. The knowledge of this quantity is essential for thegrid manager or the private PV producer in order to anticipate fluctuationsrelated to clouds occurrences and to stabilize the injected PV power. In thispaper, we test both methodologies: single and hybrid predictors. In the firstclass, we include the multi-layer perceptron (MLP), auto-regressive and movingaverage (ARMA), and persistence models. In the second class, we mix thesepredictors with Bayesian rules to obtain ad-hoc models selections, and Bayesianaverages of outputs related to single models. If MLP and ARMA are equivalent(nRMSE close to 40.5% for the both), this hybridization allows a nRMSE gainupper than 14 percentage points compared to the persistence estimation(nRMSE=37% versus 51%).
arxiv-3900-135 | Learning Mixtures of Arbitrary Distributions over Large Discrete Domains | http://arxiv.org/pdf/1212.1527v3.pdf | author:Yuval Rabani, Leonard Schulman, Chaitanya Swamy category:cs.LG cs.DS published:2012-12-07 summary:We give an algorithm for learning a mixture of {\em unstructured}distributions. This problem arises in various unsupervised learning scenarios,for example in learning {\em topic models} from a corpus of documents spanningseveral topics. We show how to learn the constituents of a mixture of $k$arbitrary distributions over a large discrete domain $[n]=\{1,2,\dots,n\}$ andthe mixture weights, using $O(n\polylog n)$ samples. (In the topic-modellearning setting, the mixture constituents correspond to the topicdistributions.) This task is information-theoretically impossible for $k>1$under the usual sampling process from a mixture distribution. However, thereare situations (such as the above-mentioned topic model case) in which eachsample point consists of several observations from the same mixtureconstituent. This number of observations, which we call the {\em "samplingaperture"}, is a crucial parameter of the problem. We obtain the {\em first}bounds for this mixture-learning problem {\em without imposing any assumptionson the mixture constituents.} We show that efficient learning is possibleexactly at the information-theoretically least-possible aperture of $2k-1$.Thus, we achieve near-optimal dependence on $n$ and optimal aperture. While thesample-size required by our algorithm depends exponentially on $k$, we provethat such a dependence is {\em unavoidable} when one considers generalmixtures. A sequence of tools contribute to the algorithm, such asconcentration results for random matrices, dimension reduction, momentestimations, and sensitivity analysis.
arxiv-3900-136 | Robust Face Recognition via Block Sparse Bayesian Learning | http://arxiv.org/pdf/1301.6847v2.pdf | author:Taiyong Li, Zhilin Zhang category:cs.CV published:2013-01-29 summary:Face recognition (FR) is an important task in pattern recognition andcomputer vision. Sparse representation (SR) has been demonstrated to be apowerful framework for FR. In general, an SR algorithm treats each face in atraining dataset as a basis function, and tries to find a sparse representationof a test face under these basis functions. The sparse representationcoefficients then provide a recognition hint. Early SR algorithms are based ona basic sparse model. Recently, it has been found that algorithms based on ablock sparse model can achieve better recognition rates. Based on this model,in this study we use block sparse Bayesian learning (BSBL) to find a sparserepresentation of a test face for recognition. BSBL is a recently proposedframework, which has many advantages over existing block-sparse-model basedalgorithms. Experimental results on the Extended Yale B, the AR and the CMU PIEface databases show that using BSBL can achieve better recognition rates andhigher robustness than state-of-the-art algorithms in most cases.
arxiv-3900-137 | Robust image reconstruction from multi-view measurements | http://arxiv.org/pdf/1212.3268v3.pdf | author:Gilles Puy, Pierre Vandergheynst category:cs.CV published:2012-12-13 summary:We propose a novel method to accurately reconstruct a set of imagesrepresenting a single scene from few linear multi-view measurements. Eachobserved image is modeled as the sum of a background image and a foregroundone. The background image is common to all observed images but undergoesgeometric transformations, as the scene is observed from different viewpoints.In this paper, we assume that these geometric transformations are representedby a few parameters, e.g., translations, rotations, affine transformations,etc.. The foreground images differ from one observed image to another, and areused to model possible occlusions of the scene. The proposed reconstructionalgorithm estimates jointly the images and the transformation parameters fromthe available multi-view measurements. The ideal solution of this multi-viewimaging problem minimizes a non-convex functional, and the reconstructiontechnique is an alternating descent method built to minimize this functional.The convergence of the proposed algorithm is studied, and conditions underwhich the sequence of estimated images and parameters converges to a criticalpoint of the non-convex functional are provided. Finally, the efficiency of thealgorithm is demonstrated using numerical simulations for applications such ascompressed sensing or super-resolution.
arxiv-3900-138 | GRED: Graph-Regularized 3D Shape Reconstruction from Highly Anisotropic and Noisy Images | http://arxiv.org/pdf/1309.4426v1.pdf | author:Christian Widmer, Philipp Drewe, Xinghua Lou, Shefali Umrania, Stephanie Heinrich, Gunnar Rätsch category:cs.CV published:2013-09-17 summary:Analysis of microscopy images can provide insight into many biologicalprocesses. One particularly challenging problem is cell nuclear segmentation inhighly anisotropic and noisy 3D image data. Manually localizing and segmentingeach and every cell nuclei is very time consuming, which remains a bottleneckin large scale biological experiments. In this work we present a tool forautomated segmentation of cell nuclei from 3D fluorescent microscopic data. Ourtool is based on state-of-the-art image processing and machine learningtechniques and supports a friendly graphical user interface (GUI). We show thatour tool is as accurate as manual annotation but greatly reduces the time forthe registration.
arxiv-3900-139 | Photon counting compressive depth mapping | http://arxiv.org/pdf/1309.4385v1.pdf | author:Gregory A. Howland, Daniel J. Lum, Matthew R. Ware, John C. Howell category:physics.optics cs.CV published:2013-09-17 summary:We demonstrate a compressed sensing, photon counting lidar system based onthe single-pixel camera. Our technique recovers both depth and intensity mapsfrom a single under-sampled set of incoherent, linear projections of a scene ofinterest at ultra-low light levels around 0.5 picowatts. Only two-dimensionalreconstructions are required to image a three-dimensional scene. We demonstrateintensity imaging and depth mapping at 256 x 256 pixel transverse resolutionwith acquisition times as short as 3 seconds. We also show novelty filtering,reconstructing only the difference between two instances of a scene. Finally,we acquire 32 x 32 pixel real-time video for three-dimensional object trackingat 14 frames-per-second.
arxiv-3900-140 | Calculation of Entailed Rank Constraints in Partially Non-Linear and Cyclic Models | http://arxiv.org/pdf/1309.7004v1.pdf | author:Peter L. Spirtes category:cs.AI stat.ML published:2013-09-17 summary:The Trek Separation Theorem (Sullivant et al. 2010) states necessary andsufficient conditions for a linear directed acyclic graphical model to entailfor all possible values of its linear coefficients that the rank of varioussub-matrices of the covariance matrix is less than or equal to n, for any givenn. In this paper, I extend the Trek Separation Theorem in two ways: I provethat the same necessary and sufficient conditions apply even when thegenerating model is partially non-linear and contains some cycles. Thisjustifies application of constraint-based causal search algorithms such as theBuildPureClusters algorithm (Silva et al. 2006) for discovering the causalstructure of latent variable models to data generated by a wider class ofcausal models that may contain non-linear and cyclic relations among the latentvariables.
arxiv-3900-141 | Integrated Pre-Processing for Bayesian Nonlinear System Identification with Gaussian Processes | http://arxiv.org/pdf/1303.2912v3.pdf | author:Roger Frigola, Carl Edward Rasmussen category:cs.AI cs.RO cs.SY stat.ML published:2013-03-12 summary:We introduce GP-FNARX: a new model for nonlinear system identification basedon a nonlinear autoregressive exogenous model (NARX) with filtered regressors(F) where the nonlinear regression problem is tackled using sparse Gaussianprocesses (GP). We integrate data pre-processing with system identificationinto a fully automated procedure that goes from raw data to an identifiedmodel. Both pre-processing parameters and GP hyper-parameters are tuned bymaximizing the marginal likelihood of the probabilistic model. We obtain aBayesian model of the system's dynamics which is able to report its uncertaintyin regions where the data is scarce. The automated approach, the modeling ofuncertainty and its relatively low computational cost make of GP-FNARX a goodcandidate for applications in robotics and adaptive control.
arxiv-3900-142 | A notion of continuity in discrete spaces and applications | http://arxiv.org/pdf/1210.2352v2.pdf | author:Valerio Capraro category:math.MG cs.CV math.CO math.GN published:2012-10-08 summary:We propose a notion of continuous path for locally finite metric spaces,taking inspiration from the recent development of A-theory for locally finiteconnected graphs. We use this notion of continuity to derive an analogue in Z^2of the Jordan curve theorem and to extend to a quite large class of locallyfinite metric spaces (containing all finite metric spaces) an inequality forthe \ell^p-distortion of a metric space that has been recently proved byPierre-Nicolas Jolissaint and Alain Valette for finite connected graphs.
arxiv-3900-143 | Exploiting Similarities among Languages for Machine Translation | http://arxiv.org/pdf/1309.4168v1.pdf | author:Tomas Mikolov, Quoc V. Le, Ilya Sutskever category:cs.CL published:2013-09-17 summary:Dictionaries and phrase tables are the basis of modern statistical machinetranslation systems. This paper develops a method that can automate the processof generating and extending dictionaries and phrase tables. Our method cantranslate missing word and phrase entries by learning language structures basedon large monolingual data and mapping between languages from small bilingualdata. It uses distributed representation of words and learns a linear mappingbetween vector spaces of languages. Despite its simplicity, our method issurprisingly effective: we can achieve almost 90% precision@5 for translationof words between English and Spanish. This method makes little assumption aboutthe languages, so it can be used to extend and refine dictionaries andtranslation tables for any language pairs.
arxiv-3900-144 | A Non-Local Means Filter for Removing the Poisson Noise | http://arxiv.org/pdf/1309.4151v1.pdf | author:Qiyu Jin, Ion Grama, Quansheng Liu category:stat.AP cs.CV published:2013-09-17 summary:A new image denoising algorithm to deal with the Poisson noise model isgiven, which is based on the idea of Non-Local Mean. By using the "Oracle"concept, we establish a theorem to show that the Non-Local Means Filter caneffectively deal with Poisson noise with some modification. Under thetheoretical result, we construct our new algorithm called Non-Local MeansPoisson Filter and demonstrate in theory that the filter converges at the usualoptimal rate. The filter is as simple as the classic Non-Local Means and thesimulation results show that our filter is very competitive.
arxiv-3900-145 | On Convergent Finite Difference Schemes for Variational - PDE Based Image Processing | http://arxiv.org/pdf/1310.7443v1.pdf | author:V. B. S. Prasath, Juan C. Moreno category:cs.CV math.NA I.4.3 published:2013-09-16 summary:We study an adaptive anisotropic Huber functional based image restorationscheme. By using a combination of L2-L1 regularization functions, an adaptiveHuber functional based energy minimization model provides denoising with edgepreservation in noisy digital images. We study a convergent finite differencescheme based on continuous piecewise linear functions and use a variablesplitting scheme, namely the Split Bregman, to obtain the discrete minimizer.Experimental results are given in image denoising and comparison with additiveoperator splitting, dual fixed point, and projected gradient schemes illustratethat the best convergence rates are obtained for our algorithm.
arxiv-3900-146 | Regularized Spectral Clustering under the Degree-Corrected Stochastic Blockmodel | http://arxiv.org/pdf/1309.4111v1.pdf | author:Tai Qin, Karl Rohe category:stat.ML cs.LG math.ST stat.TH published:2013-09-16 summary:Spectral clustering is a fast and popular algorithm for finding clusters innetworks. Recently, Chaudhuri et al. (2012) and Amini et al.(2012) proposedinspired variations on the algorithm that artificially inflate the node degreesfor improved statistical performance. The current paper extends the previousstatistical estimation results to the more canonical spectral clusteringalgorithm in a way that removes any assumption on the minimum degree andprovides guidance on the choice of the tuning parameter. Moreover, our resultsshow how the "star shape" in the eigenvectors--a common feature of empiricalnetworks--can be explained by the Degree-Corrected Stochastic Blockmodel andthe Extended Planted Partition model, two statistical models that allow forhighly heterogeneous degrees. Throughout, the paper characterizes and justifiesseveral of the variations of the spectral clustering algorithm in terms ofthese models.
arxiv-3900-147 | Learning a Loopy Model For Semantic Segmentation Exactly | http://arxiv.org/pdf/1309.4061v1.pdf | author:Andreas Christian Mueller, Sven Behnke category:cs.LG cs.CV published:2013-09-16 summary:Learning structured models using maximum margin techniques has become anindispensable tool for com- puter vision researchers, as many computer visionapplications can be cast naturally as an image labeling problem. Pixel-based orsuperpixel-based conditional random fields are particularly popular examples.Typ- ically, neighborhood graphs, which contain a large number of cycles, areused. As exact inference in loopy graphs is NP-hard in general, learning thesemodels without approximations is usually deemed infeasible. In this work weshow that, despite the theoretical hardness, it is possible to learn loopymodels exactly in practical applications. To this end, we analyze the use ofmultiple approximate inference techniques together with cutting plane trainingof structural SVMs. We show that our proposed method yields exact solutionswith an optimality guarantees in a computer vision application, for littleadditional computational cost. We also propose a dynamic caching scheme toaccelerate training further, yielding runtimes that are comparable withapproximate methods. We hope that this insight can lead to a reconsideration ofthe tractability of loopy models in computer vision.
arxiv-3900-148 | Why SOV might be initially preferred and then lost or recovered? A theoretical framework | http://arxiv.org/pdf/1309.4058v1.pdf | author:Ramon Ferrer-i-Cancho category:cs.CL nlin.AO physics.soc-ph q-bio.NC published:2013-09-16 summary:Little is known about why SOV order is initially preferred and then discardedor recovered. Here we present a framework for understanding these and manyrelated word order phenomena: the diversity of dominant orders, the existenceof free words orders, the need of alternative word orders and word orderreversions and cycles in evolution. Under that framework, word order isregarded as a multiconstraint satisfaction problem in which at least twoconstraints are in conflict: online memory minimization and maximumpredictability.
arxiv-3900-149 | Domain and Function: A Dual-Space Model of Semantic Relations and Compositions | http://arxiv.org/pdf/1309.4035v1.pdf | author:Peter D. Turney category:cs.CL cs.AI cs.LG published:2013-09-16 summary:Given appropriate representations of the semantic relations between carpenterand wood and between mason and stone (for example, vectors in a vector spacemodel), a suitable algorithm should be able to recognize that these relationsare highly similar (carpenter is to wood as mason is to stone; the relationsare analogous). Likewise, with representations of dog, house, and kennel, analgorithm should be able to recognize that the semantic composition of dog andhouse, dog house, is highly similar to kennel (dog house and kennel aresynonymous). It seems that these two tasks, recognizing relations andcompositions, are closely connected. However, up to now, the best models forrelations are significantly different from the best models for compositions. Inthis paper, we introduce a dual-space model that unifies these two tasks. Thismodel matches the performance of the best previous models for relations andcompositions. The dual-space model consists of a space for measuring domainsimilarity and a space for measuring function similarity. Carpenter and woodshare the same domain, the domain of carpentry. Mason and stone share the samedomain, the domain of masonry. Carpenter and mason share the same function, thefunction of artisans. Wood and stone share the same function, the function ofmaterials. In the composition dog house, kennel has some domain overlap withboth dog and house (the domains of pets and buildings). The function of kennelis similar to the function of house (the function of shelters). By combiningdomain and function similarities in various ways, we can model relations,compositions, and other aspects of semantics.
arxiv-3900-150 | The Cyborg Astrobiologist: Matching of Prior Textures by Image Compression for Geological Mapping and Novelty Detection | http://arxiv.org/pdf/1309.4024v1.pdf | author:P. C. McGuire, A. Bonnici, K. R. Bruner, C. Gross, J. Ormö, R. A. Smosna, S. Walter, L. Wendt category:cs.CV astro-ph.EP astro-ph.IM cs.LG published:2013-09-16 summary:(abridged) We describe an image-comparison technique of Heidemann and Ritterthat uses image compression, and is capable of: (i) detecting novel textures ina series of images, as well as of: (ii) alerting the user to the similarity ofa new image to a previously-observed texture. This image-comparison techniquehas been implemented and tested using our Astrobiology Phone-cam system, whichemploys Bluetooth communication to send images to a local laptop server in thefield for the image-compression analysis. We tested the system in a field sitedisplaying a heterogeneous suite of sandstones, limestones, mudstones andcoalbeds. Some of the rocks are partly covered with lichen. The image-matchingprocedure of this system performed very well with data obtained through ourfield test, grouping all images of yellow lichens together and grouping allimages of a coal bed together, and giving a 91% accuracy for similaritydetection. Such similarity detection could be employed to make maps ofdifferent geological units. The novelty-detection performance of our system wasalso rather good (a 64% accuracy). Such novelty detection may become valuablein searching for new geological units, which could be of astrobiologicalinterest. The image-comparison technique is an unsupervised technique that isnot capable of directly classifying an image as containing a particulargeological feature; labeling of such geological features is done post facto byhuman geologists associated with this study, for the purpose of analyzing thesystem's performance. By providing more advanced capabilities for similaritydetection and novelty detection, this image-compression technique could beuseful in giving more scientific autonomy to robotic planetary rovers, and inassisting human astronauts in their geological exploration and assessment.
arxiv-3900-151 | Performance Investigation of Feature Selection Methods | http://arxiv.org/pdf/1309.3949v1.pdf | author:Anuj sharma, Shubhamoy Dey category:cs.IR cs.CL cs.LG published:2013-09-16 summary:Sentiment analysis or opinion mining has become an open research domain afterproliferation of Internet and Web 2.0 social media. People express theirattitudes and opinions on social media including blogs, discussion forums,tweets, etc. and, sentiment analysis concerns about detecting and extractingsentiment or opinion from online text. Sentiment based text classification isdifferent from topical text classification since it involves discriminationbased on expressed opinion on a topic. Feature selection is significant forsentiment analysis as the opinionated text may have high dimensions, which canadversely affect the performance of sentiment analysis classifier. This paperexplores applicability of feature selection methods for sentiment analysis andinvestigates their performance for classification in term of recall, precisionand accuracy. Five feature selection methods (Document Frequency, InformationGain, Gain Ratio, Chi Squared, and Relief-F) and three popular sentimentfeature lexicons (HM, GI and Opinion Lexicon) are investigated on movie reviewscorpus with a size of 2000 documents. The experimental results show thatInformation Gain gave consistent results and Gain Ratio performs overall bestfor sentimental feature selection while sentiment lexicons gave poorperformance. Furthermore, we found that performance of the classifier dependson appropriate number of representative feature selected from text.
arxiv-3900-152 | Using Self-Organizing Maps for Sentiment Analysis | http://arxiv.org/pdf/1309.3946v1.pdf | author:Anuj Sharma, Shubhamoy Dey category:cs.IR cs.CL cs.NE published:2013-09-16 summary:Web 2.0 services have enabled people to express their opinions, experienceand feelings in the form of user-generated content. Sentiment analysis oropinion mining involves identifying, classifying and aggregating opinions asper their positive or negative polarity. This paper investigates the efficacyof different implementations of Self-Organizing Maps (SOM) for sentiment basedvisualization and classification of online reviews. Specifically, this paperimplements the SOM algorithm for both supervised and unsupervised learning fromtext documents. The unsupervised SOM algorithm is implemented for sentimentbased visualization and classification tasks. For supervised sentimentanalysis, a competitive learning algorithm known as Learning VectorQuantization is used. Both algorithms are also compared with their respectivemulti-pass implementations where a quick rough ordering pass is followed by afine tuning pass. The experimental results on the online movie review data setshow that SOMs are well suited for sentiment based classification and sentimentpolarity visualization.
arxiv-3900-153 | A Neural Network based Approach for Predicting Customer Churn in Cellular Network Services | http://arxiv.org/pdf/1309.3945v1.pdf | author:Anuj Sharma, Dr. Prabin Kumar Panigrahi category:cs.NE cs.CE published:2013-09-16 summary:Marketing literature states that it is more costly to engage a new customerthan to retain an existing loyal customer. Churn prediction models aredeveloped by academics and practitioners to effectively manage and controlcustomer churn in order to retain existing customers. As churn management is animportant activity for companies to retain loyal customers, the ability tocorrectly predict customer churn is necessary. As the cellular network servicesmarket becoming more competitive, customer churn management has become acrucial task for mobile communication operators. This paper proposes a neuralnetwork based approach to predict customer churn in subscription of cellularwireless services. The results of experiments indicate that neural networkbased approach can predict customer churn.
arxiv-3900-154 | Hidden Structure and Function in the Lexicon | http://arxiv.org/pdf/1308.2428v2.pdf | author:Olivier Picard, Mélanie Lord, Alexandre Blondin-Massé, Odile Marcotte, Marcos Lopes, Stevan Harnad category:cs.CL published:2013-08-11 summary:How many words are needed to define all the words in a dictionary?Graph-theoretic analysis reveals that about 10% of a dictionary is a uniqueKernel of words that define one another and all the rest, but this is not thesmallest such subset. The Kernel consists of one huge strongly connectedcomponent (SCC), about half its size, the Core, surrounded by many small SCCs,the Satellites. Core words can define one another but not the rest of thedictionary. The Kernel also contains many overlapping Minimal Grounding Sets(MGSs), each about the same size as the Core, each part-Core, part-Satellite.MGS words can define all the rest of the dictionary. They are learned earlier,more concrete and more frequent than the rest of the dictionary. Satellitewords, not correlated with age or frequency, are less concrete (more abstract)words that are also needed for full lexical power.
arxiv-3900-155 | A Metric-learning based framework for Support Vector Machines and Multiple Kernel Learning | http://arxiv.org/pdf/1309.3877v1.pdf | author:Huyen Do, Alexandros Kalousis category:cs.LG published:2013-09-16 summary:Most metric learning algorithms, as well as Fisher's Discriminant Analysis(FDA), optimize some cost function of different measures of within-andbetween-class distances. On the other hand, Support Vector Machines(SVMs) andseveral Multiple Kernel Learning (MKL) algorithms are based on the SVM largemargin theory. Recently, SVMs have been analyzed from SVM and metric learning,and to develop new algorithms that build on the strengths of each. Inspired bythe metric learning interpretation of SVM, we develop here a newmetric-learning based SVM framework in which we incorporate metric learningconcepts within SVM. We extend the optimization problem of SVM to include somemeasure of the within-class distance and along the way we develop a newwithin-class distance measure which is appropriate for SVM. In addition, weadopt the same approach for MKL and show that it can be also formulated as aMahalanobis metric learning problem. Our end result is a number of SVM/MKLalgorithms that incorporate metric learning concepts. We experiment with themon a set of benchmark datasets and observe important predictive performanceimprovements.
arxiv-3900-156 | Evaluation the efficiency of artificial bee colony and the firefly algorithm in solving the continuous optimization problem | http://arxiv.org/pdf/1310.7961v1.pdf | author:Seyyed Reza Khaze, Isa maleki, Sohrab Hojjatkhah, Ali Bagherinia category:cs.NE cs.AI published:2013-09-16 summary:Now the Meta-Heuristic algorithms have been used vastly in solving theproblem of continuous optimization. In this paper the Artificial Bee Colony(ABC) algorithm and the Firefly Algorithm (FA) are valuated. And for presentingthe efficiency of the algorithms and also for more analysis of them, thecontinuous optimization problems which are of the type of the problems of vastlimit of answer and the close optimized points are tested. So, in this paperthe efficiency of the ABC algorithm and FA are presented for solving thecontinuous optimization problems and also the said algorithms are studied fromthe accuracy in reaching the optimized solution and the resulting time and thereliability of the optimized answer points of view.
arxiv-3900-157 | SEEDS: Superpixels Extracted via Energy-Driven Sampling | http://arxiv.org/pdf/1309.3848v1.pdf | author:Michael Van den Bergh, Xavier Boix, Gemma Roig, Luc Van Gool category:cs.CV published:2013-09-16 summary:Superpixel algorithms aim to over-segment the image by grouping pixels thatbelong to the same object. Many state-of-the-art superpixel algorithms rely onminimizing objective functions to enforce color ho- mogeneity. The optimizationis accomplished by sophis- ticated methods that progressively build thesuperpix- els, typically by adding cuts or growing superpixels. As a result,they are computationally too expensive for real-time applications. We introducea new approach based on a simple hill-climbing optimization. Starting from aninitial superpixel partitioning, it continuously refines the superpixels bymodifying the boundaries. We define a robust and fast to evaluate energyfunction, based on enforcing color similarity between the bound- aries and thesuperpixel color histogram. In a series of experiments, we show that we achievean excellent com- promise between accuracy and efficiency. We are able toachieve a performance comparable to the state-of- the-art, but in real-time ona single Intel i7 CPU at 2.8GHz.
arxiv-3900-158 | Estimation of intrinsic volumes from digital grey-scale images | http://arxiv.org/pdf/1309.3842v1.pdf | author:Anne Marie Svane category:math.ST cs.CV stat.TH 62H35 published:2013-09-16 summary:Local algorithms are common tools for estimating intrinsic volumes fromblack-and-white digital images. However, these algorithms are typically biasedin the design based setting, even when the resolution tends to infinity.Moreover, images recorded in practice are most often blurred grey-scale imagesrather than black-and-white. In this paper, an extended definition of localalgorithms, applying directly to grey-scale images without thresholding, issuggested. We investigate the asymptotics of these new algorithms when theresolution tends to infinity and apply this to construct estimators for surfacearea and integrated mean curvature that are asymptotically unbiased in certainnatural settings.
arxiv-3900-159 | An iterative algorithm for computed tomography image reconstruction from limited-angle projections | http://arxiv.org/pdf/1310.7448v1.pdf | author:Yuli Sun, Jinxu Tao, Conggui Liu category:cs.CV G.1.1 published:2013-09-16 summary:In application of tomography imaging, limited-angle problem is a quitepractical and important issue. In this paper, an iterativereprojection-reconstruction (IRR) algorithm using a modified Papoulis-Gerchberg(PG) iterative scheme is developed for reconstruction from limited-angleprojections which contain noise. The proposed algorithm has two iterativeupdate processes, one is the extrapolation of unknown data, and the other isthe modification of the known noisy observation data. And the algorithmintroduces scaling factors to control the two processes, respectively. Theconvergence of the algorithm is guaranteed, and the method of choosing thescaling factors is given with energy constraints. The simulation resultdemonstrates our conclusions and indicates that the algorithm proposed in thispaper can obviously improve the reconstruction quality.
arxiv-3900-160 | Fast and accurate sentiment classification using an enhanced Naive Bayes model | http://arxiv.org/pdf/1305.6143v2.pdf | author:Vivek Narayanan, Ishan Arora, Arjun Bhatia category:cs.CL cs.IR cs.LG I.2.7 published:2013-05-27 summary:We have explored different methods of improving the accuracy of a Naive Bayesclassifier for sentiment analysis. We observed that a combination of methodslike negation handling, word n-grams and feature selection by mutualinformation results in a significant improvement in accuracy. This implies thata highly accurate and fast sentiment classifier can be built using a simpleNaive Bayes model that has linear training and testing time complexities. Weachieved an accuracy of 88.80% on the popular IMDB movie reviews dataset.
arxiv-3900-161 | Multiplicative Approximations, Optimal Hypervolume Distributions, and the Choice of the Reference Point | http://arxiv.org/pdf/1309.3816v1.pdf | author:Tobias Friedrich, Frank Neumann, Christian Thyssen category:cs.NE published:2013-09-16 summary:Many optimization problems arising in applications have to consider severalobjective functions at the same time. Evolutionary algorithms seem to be a verynatural choice for dealing with multi-objective problems as the population ofsuch an algorithm can be used to represent the trade-offs with respect to thegiven objective functions. In this paper, we contribute to the theoreticalunderstanding of evolutionary algorithms for multi-objective problems. Weconsider indicator-based algorithms whose goal is to maximize the hypervolumefor a given problem by distributing {\mu} points on the Pareto front. To gainnew theoretical insights into the behavior of hypervolume-based algorithms wecompare their optimization goal to the goal of achieving an optimalmultiplicative approximation ratio. Our studies are carried out for differentPareto front shapes of bi-objective problems. For the class of linear frontsand a class of convex fronts, we prove that maximizing the hypervolume givesthe best possible approximation ratio when assuming that the extreme pointshave to be included in both distributions of the points on the Pareto front.Furthermore, we investigate the choice of the reference point on theapproximation behavior of hypervolume-based approaches and examine Paretofronts of different shapes by numerical calculations.
arxiv-3900-162 | Visual-Semantic Scene Understanding by Sharing Labels in a Context Network | http://arxiv.org/pdf/1309.3809v1.pdf | author:Ishani Chakraborty, Ahmed Elgammal category:cs.CV cs.LG stat.ML published:2013-09-16 summary:We consider the problem of naming objects in complex, natural scenescontaining widely varying object appearance and subtly different names.Informed by cognitive research, we propose an approach based on sharing contextbased object hypotheses between visual and lexical spaces. To this end, wepresent the Visual Semantic Integration Model (VSIM) that represents objectlabels as entities shared between semantic and visual contexts and infers a newimage by updating labels through context switching. At the core of VSIM is asemantic Pachinko Allocation Model and a visual nearest neighbor LatentDirichlet Allocation Model. For inference, we derive an iterative DataAugmentation algorithm that pools the label probabilities and maximizes thejoint label posterior of an image. Our model surpasses the performance ofstate-of-art methods in several visual tasks on the challenging SUN09 dataset.
arxiv-3900-163 | Local Support Vector Machines:Formulation and Analysis | http://arxiv.org/pdf/1309.3699v1.pdf | author:Ravi Ganti, Alexander Gray category:stat.ML published:2013-09-14 summary:We provide a formulation for Local Support Vector Machines (LSVMs) thatgeneralizes previous formulations, and brings out the explicit connections tolocal polynomial learning used in nonparametric estimation literature. Weinvestigate the simplest type of LSVMs called Local Linear Support VectorMachines (LLSVMs). For the first time we establish conditions under whichLLSVMs make Bayes consistent predictions at each test point $x_0$. We alsoestablish rates at which the local risk of LLSVMs converges to the minimumvalue of expected local risk at each point $x_0$. Using stability arguments weestablish generalization error bounds for LLSVMs.
arxiv-3900-164 | Group Learning and Opinion Diffusion in a Broadcast Network | http://arxiv.org/pdf/1309.3697v1.pdf | author:Yang Liu, Mingyan Liu category:cs.LG published:2013-09-14 summary:We analyze the following group learning problem in the context of opiniondiffusion: Consider a network with $M$ users, each facing $N$ options. In adiscrete time setting, at each time step, each user chooses $K$ out of the $N$options, and receive randomly generated rewards, whose statistics depend on theoptions chosen as well as the user itself, and are unknown to the users. Eachuser aims to maximize their expected total rewards over a certain time horizonthrough an online learning process, i.e., a sequence of exploration (samplingthe return of each option) and exploitation (selecting empirically goodoptions) steps. Within this context we consider two group learning scenarios, (1) users withuniform preferences and (2) users with diverse preferences, and examine how auser should construct its learning process to best extract information fromother's decisions and experiences so as to maximize its own reward. Performanceis measured in {\em weak regret}, the difference between the user's totalreward and the reward from a user-specific best single-action policy (i.e.,always selecting the set of options generating the highest mean rewards forthis user). Within each scenario we also consider two cases: (i) when usersexchange full information, meaning they share the actual rewards they obtainedfrom their choices, and (ii) when users exchange limited information, e.g.,only their choices but not rewards obtained from these choices.
arxiv-3900-165 | Optimized projections for compressed sensing via rank-constrained nearest correlation matrix | http://arxiv.org/pdf/1309.3676v1.pdf | author:Nicolae Cleju category:cs.IT cs.LG math.IT stat.ML published:2013-09-14 summary:Optimizing the acquisition matrix is useful for compressed sensing of signalsthat are sparse in overcomplete dictionaries, because the acquisition matrixcan be adapted to the particular correlations of the dictionary atoms. In thispaper a novel formulation of the optimization problem is proposed, in the formof a rank-constrained nearest correlation matrix problem. Furthermore,improvements for three existing optimization algorithms are introduced, whichare shown to be particular instances of the proposed formulation. Simulationresults show notable improvements and superior robustness in sparse signalrecovery.
arxiv-3900-166 | Mixed Membership Models for Time Series | http://arxiv.org/pdf/1309.3533v1.pdf | author:Emily B. Fox, Michael I. Jordan category:stat.ME cs.LG stat.ML published:2013-09-13 summary:In this article we discuss some of the consequences of the mixed membershipperspective on time series analysis. In its most abstract form, a mixedmembership model aims to associate an individual entity with some set ofattributes based on a collection of observed data. Although much of theliterature on mixed membership models considers the setting in whichexchangeable collections of data are associated with each member of a set ofentities, it is equally natural to consider problems in which an entire timeseries is viewed as an entity and the goal is to characterize the time seriesin terms of a set of underlying dynamic attributes or "dynamic regimes".Indeed, this perspective is already present in the classical hidden Markovmodel, where the dynamic regimes are referred to as "states", and thecollection of states realized in a sample path of the underlying process can beviewed as a mixed membership characterization of the observed time series. Ourgoal here is to review some of the richer modeling possibilities for timeseries that are provided by recent developments in the mixed membershipframework.
arxiv-3900-167 | A two-layer Conditional Random Field for the classification of partially occluded objects | http://arxiv.org/pdf/1307.3043v2.pdf | author:Sergey Kosov, Pushmeet Kohli, Franz Rottensteiner, Christian Heipke category:cs.CV published:2013-07-11 summary:Conditional Random Fields (CRF) are among the most popular techniques forimage labelling because of their flexibility in modelling dependencies betweenthe labels and the image features. This paper proposes a novel CRF-frameworkfor image labeling problems which is capable to classify partially occludedobjects. Our approach is evaluated on aerial near-vertical images as well as onurban street-view images and compared with another methods.
arxiv-3900-168 | A method for nose-tip based 3D face registration using maximum intensity algorithm | http://arxiv.org/pdf/1309.3425v1.pdf | author:Parama Bagchi, Debotosh Bhattacharjee, Mita Nasipuri, Dipak kr. Basu category:cs.CV published:2013-09-13 summary:In this paper we present a novel technique of registering 3D images acrosspose. In this context, we have taken into account the images which are alignedacross X, Y and Z axes. We have first determined the angle across which theimage is rotated with respect to X, Y and Z axes and then translation isperformed on the images. After testing the proposed method on 472 images fromthe FRAV3D database, the method correctly registers 358 images thus giving aperformance rate of 75.84%.
arxiv-3900-169 | A Novel Approach in detecting pose orientation of a 3D face required for face | http://arxiv.org/pdf/1309.3418v1.pdf | author:Parama Bagchi, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu category:cs.CV published:2013-09-13 summary:In this paper we present a novel approach that takes as input a 3D image andgives as output its pose i.e. it tells whether the face is oriented withrespect the X, Y or Z axes with angles of rotation up to 40 degree. All theexperiments have been performed on the FRAV3D Database. After applying theproposed algorithm to the 3D facial surface we have obtained i.e. on 848 3Dface images our method detected the pose correctly for 566 face images,thusgiving an approximately 67 % of correct pose detection.
arxiv-3900-170 | Efficient Orthogonal Tensor Decomposition, with an Application to Latent Variable Model Learning | http://arxiv.org/pdf/1309.3233v1.pdf | author:Franz J. Király category:stat.ML cs.LG math.ST stat.TH published:2013-09-12 summary:Decomposing tensors into orthogonal factors is a well-known task instatistics, machine learning, and signal processing. We study orthogonal outerproduct decompositions where the factors in the summands in the decompositionare required to be orthogonal across summands, by relating this orthogonaldecomposition to the singular value decompositions of the flattenings. We showthat it is a non-trivial assumption for a tensor to have such an orthogonaldecomposition, and we show that it is unique (up to natural symmetries) in caseit exists, in which case we also demonstrate how it can be efficiently andreliably obtained by a sequence of singular value decompositions. Wedemonstrate how the factoring algorithm can be applied for parameteridentification in latent variable and mixture models.
arxiv-3900-171 | Modeling Based on Elman Wavelet Neural Network for Class-D Power Amplifiers | http://arxiv.org/pdf/1309.3214v1.pdf | author:Li Wang, Jie Shao, Yaqin Zhong, Weisong Zhao, Reza Malekian category:cs.NE published:2013-09-12 summary:In Class-D Power Amplifiers (CDPAs), the power supply noise can intermodulatewith the input signal, manifesting into power-supply induced intermodulationdistortion (PS-IMD) and due to the memory effects of the system, there existasymmetries in the PS-IMDs. In this paper, a new behavioral modeling based onthe Elman Wavelet Neural Network (EWNN) is proposed to study the nonlineardistortion of the CDPAs. In EWNN model, the Morlet wavelet functions areemployed as the activation function and there is a normalized operation in thehidden layer, the modification of the scale factor and translation factor inthe wavelet functions are ignored to avoid the fluctuations of the errorcurves. When there are 30 neurons in the hidden layer, to achieve the samesquare sum error (SSE) $\epsilon_{min}=10^{-3}$, EWNN needs 31 iteration steps,while the basic Elman neural network (BENN) model needs 86 steps. TheVolterra-Laguerre model has 605 parameters to be estimated but still can'tachieve the same magnitude accuracy of EWNN. Simulation results show that theproposed approach of EWNN model has fewer parameters and higher accuracy thanthe Volterra-Laguerre model and its convergence rate is much faster than theBENN model.
arxiv-3900-172 | A new framework for optimal classifier design | http://arxiv.org/pdf/1305.1396v2.pdf | author:Matías Di Martino, Guzman Hernández, Marcelo Fiori, Alicia Fernández category:cs.CV cs.LG stat.ML published:2013-05-07 summary:The use of alternative measures to evaluate classifier performance is gainingattention, specially for imbalanced problems. However, the use of thesemeasures in the classifier design process is still unsolved. In this work wepropose a classifier designed specifically to optimize one of these alternativemeasures, namely, the so-called F-measure. Nevertheless, the technique isgeneral, and it can be used to optimize other evaluation measures. An algorithmto train the novel classifier is proposed, and the numerical scheme is testedwith several databases, showing the optimality and robustness of the presentedclassifier.
arxiv-3900-173 | Multi-view in Lensless Compressive Imaging | http://arxiv.org/pdf/1306.3946v2.pdf | author:Hong Jiang, Gang Huang, Paul Wilford category:cs.IT cs.CV math.IT published:2013-06-17 summary:Multi-view images are acquired by a lensless compressive imagingarchitecture, which consists of an aperture assembly and multiple sensors. Theaperture assembly consists of a two dimensional array of aperture elementswhose transmittance can be individually controlled to implement a compressivesensing matrix. For each transmittance pattern of the aperture assembly, eachof the sensors takes a measurement. The measurement vectors from the multiplesensors represent multi-view images of the same scene. We present theoreticalframework for multi-view reconstruction and experimental results for enhancingquality of image using multi-view.
arxiv-3900-174 | Convex relaxations of structured matrix factorizations | http://arxiv.org/pdf/1309.3117v1.pdf | author:Francis Bach category:cs.LG math.OC published:2013-09-12 summary:We consider the factorization of a rectangular matrix $X $ into a positivelinear combination of rank-one factors of the form $u v^\top$, where $u$ and$v$ belongs to certain sets $\mathcal{U}$ and $\mathcal{V}$, that may encodespecific structures regarding the factors, such as positivity or sparsity. Inthis paper, we show that computing the optimal decomposition is equivalent tocomputing a certain gauge function of $X$ and we provide a detailed analysis ofthese gauge functions and their polars. Since these gauge functions aretypically hard to compute, we present semi-definite relaxations and severalalgorithms that may recover approximate decompositions with approximationguarantees. We illustrate our results with simulations on findingdecompositions with elements in $\{0,1\}$. As side contributions, we present adetailed analysis of variational quadratic representations of norms as well asa new iterative basis pursuit algorithm that can deal with inexact first-orderoracles.
arxiv-3900-175 | Temporal Autoencoding Improves Generative Models of Time Series | http://arxiv.org/pdf/1309.3103v1.pdf | author:Chris Häusler, Alex Susemihl, Martin P Nawrot, Manfred Opper category:stat.ML cs.LG published:2013-09-12 summary:Restricted Boltzmann Machines (RBMs) are generative models which can learnuseful representations from samples of a dataset in an unsupervised fashion.They have been widely employed as an unsupervised pre-training method inmachine learning. RBMs have been modified to model time series in two mainways: The Temporal RBM stacks a number of RBMs laterally and introducestemporal dependencies between the hidden layer units; The Conditional RBM, onthe other hand, considers past samples of the dataset as a conditional bias andlearns a representation which takes these into account. Here we propose a newtraining method for both the TRBM and the CRBM, which enforces the dynamicstructure of temporal datasets. We do so by treating the temporal models asdenoising autoencoders, considering past frames of the dataset as corruptedversions of the present frame and minimizing the reconstruction error of thepresent data by the model. We call this approach Temporal Autoencoding. Thisleads to a significant improvement in the performance of both models in afilling-in-frames task across a number of datasets. The error reduction formotion capture data is 56\% for the CRBM and 80\% for the TRBM. Taking theposterior mean prediction instead of single samples further improves themodel's estimates, decreasing the error by as much as 91\% for the CRBM onmotion capture data. We also trained the model to perform forecasting on alarge number of datasets and have found TA pretraining to consistently improvethe performance of the forecasts. Furthermore, by looking at the predictionerror across time, we can see that this improvement reflects a betterrepresentation of the dynamics of the data as opposed to a bias towardsreconstructing the observed data on a short time scale.
arxiv-3900-176 | Distributed Adaptive Networks: A Graphical Evolutionary Game-Theoretic View | http://arxiv.org/pdf/1212.1245v2.pdf | author:Chunxiao Jiang, Yan Chen, K. J. Ray Liu category:cs.GT cs.LG published:2012-12-06 summary:Distributed adaptive filtering has been considered as an effective approachfor data processing and estimation over distributed networks. Most existingdistributed adaptive filtering algorithms focus on designing differentinformation diffusion rules, regardless of the nature evolutionarycharacteristic of a distributed network. In this paper, we study the adaptivenetwork from the game theoretic perspective and formulate the distributedadaptive filtering problem as a graphical evolutionary game. With the proposedformulation, the nodes in the network are regarded as players and the localcombiner of estimation information from different neighbors is regarded asdifferent strategies selection. We show that this graphical evolutionary gameframework is very general and can unify the existing adaptive networkalgorithms. Based on this framework, as examples, we further propose twoerror-aware adaptive filtering algorithms. Moreover, we use graphicalevolutionary game theory to analyze the information diffusion process over theadaptive networks and evolutionarily stable strategy of the system. Finally,simulation results are shown to verify the effectiveness of our analysis andproposed methods.
arxiv-3900-177 | Sparse and Functional Principal Components Analysis | http://arxiv.org/pdf/1309.2895v1.pdf | author:Genevera I. Allen category:stat.ML published:2013-09-11 summary:Regularized principal components analysis, especially Sparse PCA andFunctional PCA, has become widely used for dimension reduction inhigh-dimensional settings. Many examples of massive data, however, may benefitfrom estimating both sparse AND functional factors. These include neuroimagingdata where there are discrete brain regions of activation (sparsity) but theseregions tend to be smooth spatially (functional). Here, we introduce anoptimization framework that can encourage both sparsity and smoothness of therow and/or column PCA factors. This framework generalizes many of the existingapproaches to Sparse PCA, Functional PCA and two-way Sparse PCA and FunctionalPCA, as these are all special cases of our method. In particular, our methodpermits flexible combinations of sparsity and smoothness that lead toimprovements in feature selection and signal recovery as well as moreinterpretable PCA factors. We demonstrate our method on simulated data and aneuroimaging example on EEG data. This work provides a unified framework forregularized PCA that can form the foundation for a cohesive approach toregularization in high-dimensional multivariate analysis.
arxiv-3900-178 | Concentration in unbounded metric spaces and algorithmic stability | http://arxiv.org/pdf/1309.1007v2.pdf | author:Aryeh Kontorovich category:math.PR cs.LG math.FA 60D99 published:2013-09-04 summary:We prove an extension of McDiarmid's inequality for metric spaces withunbounded diameter. To this end, we introduce the notion of the {\emsubgaussian diameter}, which is a distribution-dependent refinement of themetric diameter. Our technique provides an alternative approach to that ofKutin and Niyogi's method of weakly difference-bounded functions, and yieldsnontrivial, dimension-free results in some interesting cases where the formerdoes not. As an application, we give apparently the first generalization boundin the algorithmic stability setting that holds for unbounded loss functions.We furthermore extend our concentration inequality to strongly mixingprocesses.
arxiv-3900-179 | General Purpose Textual Sentiment Analysis and Emotion Detection Tools | http://arxiv.org/pdf/1309.2853v1.pdf | author:Alexandre Denis, Samuel Cruz-Lara, Nadia Bellalem category:cs.CL published:2013-09-11 summary:Textual sentiment analysis and emotion detection consists in retrieving thesentiment or emotion carried by a text or document. This task can be useful inmany domains: opinion mining, prediction, feedbacks, etc. However, building ageneral purpose tool for doing sentiment analysis and emotion detection raisesa number of issues, theoretical issues like the dependence to the domain or tothe language but also pratical issues like the emotion representation forinteroperability. In this paper we present our sentiment/emotion analysistools, the way we propose to circumvent the di culties and the applicationsthey are used for.
arxiv-3900-180 | High-dimensional cluster analysis with the Masked EM Algorithm | http://arxiv.org/pdf/1309.2848v1.pdf | author:Shabnam N. Kadir, Dan F. M. Goodman, Kenneth D. Harris category:q-bio.QM cs.LG q-bio.NC stat.AP published:2013-09-11 summary:Cluster analysis faces two problems in high dimensions: first, the `curse ofdimensionality' that can lead to overfitting and poor generalizationperformance; and second, the sheer time taken for conventional algorithms toprocess large amounts of high-dimensional data. In many applications, only asmall subset of features provide information about the cluster membership ofany one data point, however this informative feature subset may not be the samefor all data points. Here we introduce a `Masked EM' algorithm for fittingmixture of Gaussians models in such cases. We show that the algorithm performsclose to optimally on simulated Gaussian data, and in an application of `spikesorting' of high channel-count neuronal recordings.
arxiv-3900-181 | Efficient Transductive Online Learning via Randomized Rounding | http://arxiv.org/pdf/1106.2429v4.pdf | author:Nicolò Cesa-Bianchi, Ohad Shamir category:cs.LG stat.ML published:2011-06-13 summary:Most traditional online learning algorithms are based on variants of mirrordescent or follow-the-leader. In this paper, we present an online algorithmbased on a completely different approach, tailored for transductive settings,which combines "random playout" and randomized rounding of loss subgradients.As an application of our approach, we present the first computationallyefficient online algorithm for collaborative filtering with trace-normconstrained matrices. As a second application, we solve an open questionlinking batch learning and transductive online learning
arxiv-3900-182 | Enhancements of Multi-class Support Vector Machine Construction from Binary Learners using Generalization Performance | http://arxiv.org/pdf/1309.2765v1.pdf | author:Patoomsiri Songsiri, Thimaporn Phetkaew, Boonserm Kijsirikul category:cs.LG stat.ML published:2013-09-11 summary:We propose several novel methods for enhancing the multi-class SVMs byapplying the generalization performance of binary classifiers as the core idea.This concept will be applied on the existing algorithms, i.e., the DecisionDirected Acyclic Graph (DDAG), the Adaptive Directed Acyclic Graphs (ADAG), andMax Wins. Although in the previous approaches there have been many attempts touse some information such as the margin size and the number of support vectorsas performance estimators for binary SVMs, they may not accurately reflect theactual performance of the binary SVMs. We show that the generalization abilityevaluated via a cross-validation mechanism is more suitable to directly extractthe actual performance of binary SVMs. Our methods are built around thisperformance measure, and each of them is crafted to overcome the weakness ofthe previous algorithm. The proposed methods include the Reordering AdaptiveDirected Acyclic Graph (RADAG), Strong Elimination of the classifiers (SE),Weak Elimination of the classifiers (WE), and Voting based Candidate Filtering(VCF). Experimental results demonstrate that our methods give significantlyhigher accuracy than all of the traditional ones. Especially, WE providessignificantly superior results compared to Max Wins which is recognized as thestate of the art algorithm in terms of both accuracy and classification speedwith two times faster in average.
arxiv-3900-183 | Robust Periocular Recognition By Fusing Sparse Representations of Color and Geometry Information | http://arxiv.org/pdf/1309.2752v1.pdf | author:Juan C. Moreno, V. B. S. Prasath, Gil Santos, Hugo Proenca category:cs.CV published:2013-09-11 summary:In this paper, we propose a re-weighted elastic net (REN) model for biometricrecognition. The new model is applied to data separated into geometric andcolor spatial components. The geometric information is extracted using a fastcartoon - texture decomposition model based on a dual formulation of the totalvariation norm allowing us to carry information about the overall geometry ofimages. Color components are defined using linear and nonlinear color spaces,namely the red-green-blue (RGB), chromaticity-brightness (CB) andhue-saturation-value (HSV). Next, according to a Bayesian fusion-scheme, sparserepresentations for classification purposes are obtained. The scheme isnumerically solved using a gradient projection (GP) algorithm. In the empiricalvalidation of the proposed model, we have chosen the periocular region, whichis an emerging trait known for its robustness against low quality data. Ourresults were obtained in the publicly available UBIRIS.v2 data set and showconsistent improvements in recognition effectiveness when compared to relatedstate-of-the-art techniques.
arxiv-3900-184 | Maximizing submodular functions using probabilistic graphical models | http://arxiv.org/pdf/1309.2593v1.pdf | author:K. S. Sesh Kumar, Francis Bach category:cs.LG math.OC published:2013-09-10 summary:We consider the problem of maximizing submodular functions; while thisproblem is known to be NP-hard, several numerically efficient local searchtechniques with approximation guarantees are available. In this paper, wepropose a novel convex relaxation which is based on the relationship betweensubmodular functions, entropies and probabilistic graphical models. In agraphical model, the entropy of the joint distribution decomposes as a sum ofmarginal entropies of subsets of variables; moreover, for any distribution, theentropy of the closest distribution factorizing in the graphical model providesan bound on the entropy. For directed graphical models, this last propertyturns out to be a direct consequence of the submodularity of the entropyfunction, and allows the generalization of graphical-model-based upper boundsto any submodular functions. These upper bounds may then be jointly maximizedwith respect to a set, while minimized with respect to the graph, leading to aconvex variational inference scheme for maximizing submodular functions, basedon outer approximations of the marginal polytope and maximum likelihood boundedtreewidth structures. By considering graphs of increasing treewidths, we maythen explore the trade-off between computational complexity and tightness ofthe relaxation. We also present extensions to constrained problems andmaximizing the difference of submodular functions, which include all possibleset functions.
arxiv-3900-185 | Preparing Korean Data for the Shared Task on Parsing Morphologically Rich Languages | http://arxiv.org/pdf/1309.1649v2.pdf | author:Jinho D. Choi category:cs.CL published:2013-09-06 summary:This document gives a brief description of Korean data prepared for the SPMRL2013 shared task. A total of 27,363 sentences with 350,090 tokens are used forthe shared task. All constituent trees are collected from the KAIST Treebankand transformed to the Penn Treebank style. All dependency trees are convertedfrom the transformed constituent trees using heuristics and labeling rules de-signed specifically for the KAIST Treebank. In addition to the gold-standardmorphological analysis provided by the KAIST Treebank, two sets of automaticmorphological analysis are provided for the shared task, one is generated bythe HanNanum morphological analyzer, and the other is generated by the Sejongmorphological analyzer.
arxiv-3900-186 | A multi-stream hmm approach to offline handwritten arabic word recognition | http://arxiv.org/pdf/1309.2506v1.pdf | author:Ahlam Maqqor, Akram Halli, Khaled Satori category:cs.CV published:2013-09-10 summary:In This paper we presented new approach for cursive Arabic text recognitionsystem. The objective is to propose methodology analytical offline recognitionof handwritten Arabic for rapid implementation. The first part in the writingrecognition system is the preprocessing phase is the preprocessing phase toprepare the data was introduces and extracts a set of simple statisticalfeatures by two methods : from a window which is sliding long that text linethe right to left and the approach VH2D (consists in projecting every characteron the abscissa, on the ordinate and the diagonals 45{\deg} and 135{\deg}) . Itthen injects the resulting feature vectors to Hidden Markov Model (HMM) andcombined the two HMM by multi-stream approach.
arxiv-3900-187 | Compressed Sensing for Block-Sparse Smooth Signals | http://arxiv.org/pdf/1309.2505v1.pdf | author:Shahzad Gishkori, Geert Leus category:stat.ML cs.IT math.IT math.ST stat.TH published:2013-09-10 summary:We present reconstruction algorithms for smooth signals with block sparsityfrom their compressed measurements. We tackle the issue of varying group sizevia group-sparse least absolute shrinkage selection operator (LASSO) as well asvia latent group LASSO regularizations. We achieve smoothness in the signal viafusion. We develop low-complexity solvers for our proposed formulations throughthe alternating direction method of multipliers.
arxiv-3900-188 | Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization | http://arxiv.org/pdf/1306.4650v2.pdf | author:Julien Mairal category:stat.ML cs.LG math.OC published:2013-06-19 summary:Majorization-minimization algorithms consist of iteratively minimizing amajorizing surrogate of an objective function. Because of its simplicity andits wide applicability, this principle has been very popular in statistics andin signal processing. In this paper, we intend to make this principle scalable.We introduce a stochastic majorization-minimization scheme which is able todeal with large-scale or possibly infinite data sets. When applied to convexoptimization problems under suitable assumptions, we show that it achieves anexpected convergence rate of $O(1/\sqrt{n})$ after $n$ iterations, and of$O(1/n)$ for strongly convex functions. Equally important, our scheme almostsurely converges to stationary points for a large class of non-convex problems.We develop several efficient algorithms based on our framework. First, wepropose a new stochastic proximal gradient method, which experimentally matchesstate-of-the-art solvers for large-scale $\ell_1$-logistic regression. Second,we develop an online DC programming algorithm for non-convex sparse estimation.Finally, we demonstrate the effectiveness of our approach for solvinglarge-scale structured matrix factorization problems.
arxiv-3900-189 | Implementation of nlization framework for verbs, pronouns and determiners with eugene | http://arxiv.org/pdf/1309.2471v1.pdf | author:Harinder Singh, Parteek Kumar category:cs.CL published:2013-09-10 summary:UNL system is designed and implemented by a nonprofit organization, UNDLFoundation at Geneva in 1999. UNL applications are application softwares thatallow end users to accomplish natural language tasks, such as translating,summarizing, retrieving or extracting information, etc. Two major web basedapplication softwares are Interactive ANalyzer (IAN), which is a naturallanguage analysis system. It represents natural language sentences as semanticnetworks in the UNL format. Other application software is dEep-to-sUrfaceGENErator (EUGENE), which is an open-source interactive NLizer. It generatesnatural language sentences out of semantic networks represented in the UNLformat. In this paper, NLization framework with EUGENE is focused, while usingUNL system for accomplishing the task of machine translation. In wholeNLization process, EUGENE takes a UNL input and delivers an output in naturallanguage without any human intervention. It is language-independent and has tobe parametrized to the natural language input through a dictionary and agrammar, provided as separate interpretable files. In this paper, it isexplained that how UNL input is syntactically and semantically analyzed withthe UNL-NL T-Grammar for NLization of UNL sentences involving verbs, pronounsand determiners for Punjabi natural language.
arxiv-3900-190 | The Linearized Bregman Method via Split Feasibility Problems: Analysis and Generalizations | http://arxiv.org/pdf/1309.2094v2.pdf | author:Dirk A. Lorenz, Frank Schöpfer, Stephan Wenger category:math.OC cs.CV cs.NA math.NA published:2013-09-09 summary:The linearized Bregman method is a method to calculate sparse solutions tosystems of linear equations. We formulate this problem as a split feasibilityproblem, propose an algorithmic framework based on Bregman projections andprove a general convergence result for this framework. Convergence of thelinearized Bregman method will be obtained as a special case. Our approach alsoallows for several generalizations such as other objective functions,incremental iterations, incorporation of non-gaussian noise models or boxconstraints.
arxiv-3900-191 | Exponentially Fast Parameter Estimation in Networks Using Distributed Dual Averaging | http://arxiv.org/pdf/1309.2350v1.pdf | author:Shahin Shahrampour, Ali Jadbabaie category:cs.LG cs.SI math.OC stat.ML published:2013-09-10 summary:In this paper we present an optimization-based view of distributed parameterestimation and observational social learning in networks. Agents receive asequence of random, independent and identically distributed (i.i.d.) signals,each of which individually may not be informative about the underlying truestate, but the signals together are globally informative enough to make thetrue state identifiable. Using an optimization-based characterization ofBayesian learning as proximal stochastic gradient descent (withKullback-Leibler divergence from a prior as a proximal function), we show howto efficiently use a distributed, online variant of Nesterov's dual averagingmethod to solve the estimation with purely local information. When the truestate is globally identifiable, and the network is connected, we prove thatagents eventually learn the true parameter using a randomized gossip scheme. Wedemonstrate that with high probability the convergence is exponentially fastwith a rate dependent on the KL divergence of observations under the true statefrom observations under the second likeliest state. Furthermore, our work alsohighlights the possibility of learning under continuous adaptation of networkwhich is a consequence of employing constant, unit stepsize for the algorithm.
arxiv-3900-192 | Spectral Clustering with Imbalanced Data | http://arxiv.org/pdf/1309.2303v1.pdf | author:Jing Qian, Venkatesh Saligrama category:stat.ML published:2013-09-09 summary:Spectral clustering is sensitive to how graphs are constructed from dataparticularly when proximal and imbalanced clusters are present. We show thatRatio-Cut (RCut) or normalized cut (NCut) objectives are not tailored toimbalanced data since they tend to emphasize cut sizes over cut values. Wepropose a graph partitioning problem that seeks minimum cut partitions underminimum size constraints on partitions to deal with imbalanced data. Ourapproach parameterizes a family of graphs, by adaptively modulating nodedegrees on a fixed node set, to yield a set of parameter dependent cutsreflecting varying levels of imbalance. The solution to our problem is thenobtained by optimizing over these parameters. We present rigorous limit cutanalysis results to justify our approach. We demonstrate the superiority of ourmethod through unsupervised and semi-supervised experiments on synthetic andreal data sets.
arxiv-3900-193 | Contour Manifolds and Optimal Transport | http://arxiv.org/pdf/1309.2240v1.pdf | author:Bernhard Schmitzer, Christoph Schnörr category:math.DG cs.CV published:2013-09-09 summary:Describing shapes by suitable measures in object segmentation, as proposed in[24], allows to combine the advantages of the representations as parametrizedcontours and indicator functions. The pseudo-Riemannian structure of optimaltransport can be used to model shapes in ways similar as with contours, whilethe Kantorovich functional enables the application of convex optimizationmethods for global optimality of the segmentation functional. In this paper we provide a mathematical study of the shape measurerepresentation and its relation to the contour description. In particular weshow that the pseudo-Riemannian structure of optimal transport, when restrictedto the set of shape measures, yields a manifold which is diffeomorphic to themanifold of closed contours. A discussion of the metric induced by optimaltransport and the corresponding geodesic equation is given.
arxiv-3900-194 | Application of Artificial Neural Networks in Estimating Participation in Elections | http://arxiv.org/pdf/1309.2183v1.pdf | author:Seyyed Reza Khaze, Mohammad Masdari, Sohrab Hojjatkhah category:cs.NE cs.CY published:2013-09-09 summary:It is approved that artificial neural networks can be considerable effectivein anticipating and analyzing flows in which traditional methods and staticsare not able to solve. in this article, by using two-layer feedforward networkwith tan-sigmoid transmission function in input and output layers, we cananticipate participation rate of public in kohgiloye and boyerahmad province infuture presidential election of islamic republic of iran with 91% accuracy. theassessment standards of participation such as confusion matrix and roc diagramshave been approved our claims.
arxiv-3900-195 | Task-Driven Dictionary Learning | http://arxiv.org/pdf/1009.5358v2.pdf | author:Julien Mairal, Francis Bach, Jean Ponce category:stat.ML published:2010-09-27 summary:Modeling data with linear combinations of a few elements from a learneddictionary has been the focus of much recent research in machine learning,neuroscience and signal processing. For signals such as natural images thatadmit such sparse representations, it is now well established that these modelsare well suited to restoration tasks. In this context, learning the dictionaryamounts to solving a large-scale matrix factorization problem, which can bedone efficiently with classical optimization tools. The same approach has alsobeen used for learning features from data for other purposes, e.g., imageclassification, but tuning the dictionary in a supervised way for these taskshas proven to be more difficult. In this paper, we present a generalformulation for supervised dictionary learning adapted to a wide variety oftasks, and present an efficient algorithm for solving the correspondingoptimization problem. Experiments on handwritten digit classification, digitalart identification, nonlinear inverse image problems, and compressed sensingdemonstrate that our approach is effective in large-scale settings, and is wellsuited to supervised and semi-supervised classification, as well as regressiontasks for data that admit sparse representations.
arxiv-3900-196 | Real-Time and Continuous Hand Gesture Spotting: an Approach Based on Artificial Neural Networks | http://arxiv.org/pdf/1309.2084v1.pdf | author:Pedro Neto, Dário Pereira, Norberto Pires, Paulo Moreira category:cs.RO cs.CV published:2013-09-09 summary:New and more natural human-robot interfaces are of crucial interest to theevolution of robotics. This paper addresses continuous and real-time handgesture spotting, i.e., gesture segmentation plus gesture recognition. Gesturepatterns are recognized by using artificial neural networks (ANNs) specificallyadapted to the process of controlling an industrial robot. Since in continuousgesture recognition the communicative gestures appear intermittently with thenoncommunicative, we are proposing a new architecture with two ANNs in seriesto recognize both kinds of gesture. A data glove is used as interfacetechnology. Experimental results demonstrated that the proposed solutionpresents high recognition rates (over 99% for a library of ten gestures andover 96% for a library of thirty gestures), low training and learning time anda good capacity to generalize from particular situations.
arxiv-3900-197 | Structure Learning of Probabilistic Logic Programs by Searching the Clause Space | http://arxiv.org/pdf/1309.2080v1.pdf | author:Elena Bellodi, Fabrizio Riguzzi category:cs.LG cs.AI published:2013-09-09 summary:Learning probabilistic logic programming languages is receiving an increasingattention and systems are available for learning the parameters (PRISM,LeProbLog, LFI-ProbLog and EMBLEM) or both the structure and the parameters(SEM-CP-logic and SLIPCASE) of these languages. In this paper we present thealgorithm SLIPCOVER for "Structure LearnIng of Probabilistic logic programs bysearChing OVER the clause space". It performs a beam search in the space ofprobabilistic clauses and a greedy search in the space of theories, using thelog likelihood of the data as the guiding heuristics. To estimate the loglikelihood SLIPCOVER performs Expectation Maximization with EMBLEM. Thealgorithm has been tested on five real world datasets and compared withSLIPCASE, SEM-CP-logic, Aleph and two algorithms for learning Markov LogicNetworks (Learning using Structural Motifs (LSM) and ALEPH++ExactL1). SLIPCOVERachieves higher areas under the precision-recall and ROC curves in most cases.
arxiv-3900-198 | Single image super resolution in spatial and wavelet domain | http://arxiv.org/pdf/1309.2057v1.pdf | author:Sapan Naik, Nikunj Patel category:cs.CV published:2013-09-09 summary:Recently single image super resolution is very important research area togenerate high resolution image from given low resolution image. Algorithms ofsingle image resolution are mainly based on wavelet domain and spatial domain.Filters support to model the regularity of natural images is exploited inwavelet domain while edges of images get sharp during up sampling in spatialdomain. Here single image super resolution algorithm is presented which basedon both spatial and wavelet domain and take the advantage of both. Algorithm isiterative and use back projection to minimize reconstruction error. Waveletbased denoising method is also introduced to remove noise.
arxiv-3900-199 | Variational Bayes Approximations for Clustering via Mixtures of Normal Inverse Gaussian Distributions | http://arxiv.org/pdf/1309.1901v1.pdf | author:Sanjeena Subedi, Paul D. McNicholas category:stat.ME stat.CO stat.ML published:2013-09-07 summary:Parameter estimation for model-based clustering using a finite mixture ofnormal inverse Gaussian (NIG) distributions is achieved through variationalBayes approximations. Univariate NIG mixtures and multivariate NIG mixtures areconsidered. The use of variational Bayes approximations here is a substantialdeparture from the traditional EM approach and alleviates some of theassociated computational complexities and uncertainties. Our variationalalgorithm is applied to simulated and real data. The paper concludes withdiscussion and suggestions for future work.
arxiv-3900-200 | Finding a most biased coin with fewest flips | http://arxiv.org/pdf/1202.3639v3.pdf | author:Karthekeyan Chandrasekaran, Richard Karp category:cs.DS cs.LG published:2012-02-16 summary:We study the problem of learning a most biased coin among a set of coins bytossing the coins adaptively. The goal is to minimize the number of tossesuntil we identify a coin i* whose posterior probability of being most biased isat least 1-delta for a given delta. Under a particular probabilistic model, wegive an optimal algorithm, i.e., an algorithm that minimizes the expectednumber of future tosses. The problem is closely related to finding the best armin the multi-armed bandit problem using adaptive strategies. Our algorithmemploys an optimal adaptive strategy -- a strategy that performs the bestpossible action at each step after observing the outcomes of all previous cointosses. Consequently, our algorithm is also optimal for any starting history ofoutcomes. To our knowledge, this is the first algorithm that employs an optimaladaptive strategy under a Bayesian setting for this problem. Our proof ofoptimality employs tools from the field of Markov games.
arxiv-3900-201 | Efficient Monte Carlo Methods for Multi-Dimensional Learning with Classifier Chains | http://arxiv.org/pdf/1211.2190v4.pdf | author:Jesse Read, Luca Martino, David Luengo category:cs.LG stat.CO stat.ML published:2012-11-09 summary:Multi-dimensional classification (MDC) is the supervised learning problemwhere an instance is associated with multiple classes, rather than with asingle class, as in traditional classification problems. Since these classesare often strongly correlated, modeling the dependencies between them allowsMDC methods to improve their performance - at the expense of an increasedcomputational cost. In this paper we focus on the classifier chains (CC)approach for modeling dependencies, one of the most popular and highest-performing methods for multi-label classification (MLC), a particular case ofMDC which involves only binary classes (i.e., labels). The original CCalgorithm makes a greedy approximation, and is fast but tends to propagateerrors along the chain. Here we present novel Monte Carlo schemes, both forfinding a good chain sequence and performing efficient inference. Ouralgorithms remain tractable for high-dimensional data sets and obtain the bestpredictive performance across several real data sets.
arxiv-3900-202 | A General Two-Step Approach to Learning-Based Hashing | http://arxiv.org/pdf/1309.1853v1.pdf | author:Guosheng Lin, Chunhua Shen, David Suter, Anton van den Hengel category:cs.LG cs.CV published:2013-09-07 summary:Most existing approaches to hashing apply a single form of hash function, andan optimization process which is typically deeply coupled to this specificform. This tight coupling restricts the flexibility of the method to respond tothe data, and can result in complex optimization problems that are difficult tosolve. Here we propose a flexible yet simple framework that is able toaccommodate different types of loss functions and hash functions. Thisframework allows a number of existing approaches to hashing to be placed incontext, and simplifies the development of new problem-specific hashingmethods. Our framework decomposes hashing learning problem into two steps: hashbit learning and hash function learning based on the learned bits. The firststep can typically be formulated as binary quadratic problems, and the secondstep can be accomplished by training standard binary classifiers. Both problemshave been extensively studied in the literature. Our extensive experimentsdemonstrate that the proposed framework is effective, flexible and outperformsthe state-of-the-art.
arxiv-3900-203 | Radar shadow detection in SAR images using DEM and projections | http://arxiv.org/pdf/1309.1830v1.pdf | author:V. B. S. Prasath, O. Haddad category:cs.CV 68U10 I.4.8 published:2013-09-07 summary:Synthetic aperture radar (SAR) images are widely used in target recognitiontasks nowadays. In this letter, we propose an automatic approach for radarshadow detection and extraction from SAR images utilizing geometric projectionsalong with the digital elevation model (DEM) which corresponds to the givengeo-referenced SAR image. First, the DEM is rotated into the radar geometry sothat each row would match that of a radar line of sight. Next, we extract theshadow regions by processing row by row until the image is covered fully. Wetest the proposed shadow detection approach on different DEMs and a simulated1D signals and 2D hills and volleys modeled by various variance based Gaussianfunctions. Experimental results indicate the proposed algorithm produces goodresults in detecting shadows in SAR images with high resolution.
arxiv-3900-204 | Efficient Estimation of Word Representations in Vector Space | http://arxiv.org/pdf/1301.3781v3.pdf | author:Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean category:cs.CL published:2013-01-16 summary:We propose two novel model architectures for computing continuous vectorrepresentations of words from very large data sets. The quality of theserepresentations is measured in a word similarity task, and the results arecompared to the previously best performing techniques based on different typesof neural networks. We observe large improvements in accuracy at much lowercomputational cost, i.e. it takes less than a day to learn high quality wordvectors from a 1.6 billion words data set. Furthermore, we show that thesevectors provide state-of-the-art performance on our test set for measuringsyntactic and semantic word similarities.
arxiv-3900-205 | Convergence of Nearest Neighbor Pattern Classification with Selective Sampling | http://arxiv.org/pdf/1309.1761v1.pdf | author:Shaun N. Joseph, Seif Omar Abu Bakr, Gabriel Lugo category:cs.LG stat.ML 60G25 F.2.2; G.3 published:2013-09-06 summary:In the panoply of pattern classification techniques, few enjoy the intuitiveappeal and simplicity of the nearest neighbor rule: given a set of samples insome metric domain space whose value under some function is known, we estimatethe function anywhere in the domain by giving the value of the nearest sampleper the metric. More generally, one may use the modal value of the m nearestsamples, where m is a fixed positive integer (although m=1 is known to beadmissible in the sense that no larger value is asymptotically superior interms of prediction error). The nearest neighbor rule is nonparametric andextremely general, requiring in principle only that the domain be a metricspace. The classic paper on the technique, proving convergence underindependent, identically-distributed (iid) sampling, is due to Cover and Hart(1967). Because taking samples is costly, there has been much research inrecent years on selective sampling, in which each sample is selected from apool of candidates ranked by a heuristic; the heuristic tries to guess whichcandidate would be the most "informative" sample. Lindenbaum et al. (2004)apply selective sampling to the nearest neighbor rule, but their approachsacrifices the austere generality of Cover and Hart; furthermore, theirheuristic algorithm is complex and computationally expensive. Here we reportrecent results that enable selective sampling in the original Cover-Hartsetting. Our results pose three selection heuristics and prove that theirnearest neighbor rule predictions converge to the true pattern. Two of thealgorithms are computationally cheap, with complexity growing linearly in thenumber of samples. We believe that these results constitute an importantadvance in the art.
arxiv-3900-206 | A Comparism of the Performance of Supervised and Unsupervised Machine Learning Techniques in evolving Awale/Mancala/Ayo Game Player | http://arxiv.org/pdf/1309.1543v1.pdf | author:O. A. Randle, O. O. Ogunduyile, T. Zuva, N. A. Fashola category:cs.LG cs.GT published:2013-09-06 summary:Awale games have become widely recognized across the world, for theirinnovative strategies and techniques which were used in evolving the agents(player) and have produced interesting results under various conditions. Thispaper will compare the results of the two major machine learning techniques byreviewing their performance when using minimax, endgame database, a combinationof both techniques or other techniques, and will determine which are the besttechniques.
arxiv-3900-207 | Projection onto the probability simplex: An efficient algorithm with a simple proof, and an application | http://arxiv.org/pdf/1309.1541v1.pdf | author:Weiran Wang, Miguel Á. Carreira-Perpiñán category:cs.LG math.OC stat.ML published:2013-09-06 summary:We provide an elementary proof of a simple, efficient algorithm for computingthe Euclidean projection of a point onto the probability simplex. We also showan application in Laplacian K-modes clustering.
arxiv-3900-208 | Guided Self-Organization of Input-Driven Recurrent Neural Networks | http://arxiv.org/pdf/1309.1524v1.pdf | author:Oliver Obst, Joschka Boedecker category:cs.NE cs.AI nlin.AO published:2013-09-06 summary:We review attempts that have been made towards understanding thecomputational properties and mechanisms of input-driven dynamical systems likeRNNs, and reservoir computing networks in particular. We provide details onmethods that have been developed to give quantitative answers to the questionsabove. Following this, we show how self-organization may be used to improvereservoirs for better performance, in some cases guided by the measurespresented before. We also present a possible way to quantify task performanceusing an information-theoretic approach, and finally discuss promising futuredirections aimed at a better understanding of how these systems perform theircomputations and how to best guide self-organized processes for theiroptimization.
arxiv-3900-209 | Nested Nonnegative Cone Analysis | http://arxiv.org/pdf/1308.4206v2.pdf | author:Lingsong Zhang, J. S. Marron, Shu Lu category:stat.ME cs.LG published:2013-08-20 summary:Motivated by the analysis of nonnegative data objects, a novel NestedNonnegative Cone Analysis (NNCA) approach is proposed to overcome somedrawbacks of existing methods. The application of traditional PCA/SVD method tononnegative data often cause the approximation matrix leave the nonnegativecone, which leads to non-interpretable and sometimes nonsensical results. Thenonnegative matrix factorization (NMF) approach overcomes this issue, howeverthe NMF approximation matrices suffer several drawbacks: 1) the factorizationmay not be unique, 2) the resulting approximation matrix at a specific rank maynot be unique, and 3) the subspaces spanned by the approximation matrices atdifferent ranks may not be nested. These drawbacks will cause troubles indetermining the number of components and in multi-scale (in ranks)interpretability. The NNCA approach proposed in this paper naturally generatesa nested structure, and is shown to be unique at each rank. Simulations areused in this paper to illustrate the drawbacks of the traditional methods, andthe usefulness of the NNCA method.
arxiv-3900-210 | Nano-scale reservoir computing | http://arxiv.org/pdf/1309.1521v1.pdf | author:Oliver Obst, Adrian Trinchi, Simon G. Hardin, Matthew Chadwick, Ivan Cole, Tim H. Muster, Nigel Hoschke, Diet Ostry, Don Price, Khoa N. Pham, Tim Wark category:cs.ET cs.NE nlin.AO published:2013-09-06 summary:This work describes preliminary steps towards nano-scale reservoir computingusing quantum dots. Our research has focused on the development of anaccumulator-based sensing system that reacts to changes in the environment, aswell as the development of a software simulation. The investigated systemsgenerate nonlinear responses to inputs that make them suitable for a physicalimplementation of a neural network. This development will enableminiaturisation of the neurons to the molecular level, leading to a range ofapplications including monitoring of changes in materials or structures. Thesystem is based around the optical properties of quantum dots. The paper willreport on experimental work on systems using Cadmium Selenide (CdSe) quantumdots and on the various methods to render the systems sensitive to pH, redoxpotential or specific ion concentration. Once the quantum dot-based systems arerendered sensitive to these triggers they can provide a distributed array thatcan monitor and transmit information on changes within the material.
arxiv-3900-211 | A Small Universal Petri Net | http://arxiv.org/pdf/1309.1274v1.pdf | author:Dmitry A. Zaitsev category:cs.FL cs.CC cs.DC cs.NE published:2013-09-05 summary:A universal deterministic inhibitor Petri net with 14 places, 29 transitionsand 138 arcs was constructed via simulation of Neary and Woods' weaklyuniversal Turing machine with 2 states and 4 symbols; the total time complexityis exponential in the running time of their weak machine. To simulate the blankwords of the weakly universal Turing machine, a couple of dedicated transitionsinsert their codes when reaching edges of the working zone. To complete a chainof a given Petri net encoding to be executed by the universal Petri net, atranslation of a bi-tag system into a Turing machine was constructed. Theconstructed Petri net is universal in the standard sense; a weaker form ofuniversality for Petri nets was not introduced in this work.
arxiv-3900-212 | Information fusion in multi-task Gaussian processes | http://arxiv.org/pdf/1210.1928v3.pdf | author:Shrihari Vasudevan, Arman Melkumyan, Steven Scheding category:stat.ML cs.AI cs.LG published:2012-10-06 summary:This paper evaluates heterogeneous information fusion using multi-taskGaussian processes in the context of geological resource modeling.Specifically, it empirically demonstrates that information integration acrossheterogeneous information sources leads to superior estimates of all thequantities being modeled, compared to modeling them individually. Multi-taskGaussian processes provide a powerful approach for simultaneous modeling ofmultiple quantities of interest while taking correlations between thesequantities into consideration. Experiments are performed on large scale realsensor data.
arxiv-3900-213 | Some Options for L1-Subspace Signal Processing | http://arxiv.org/pdf/1309.1194v1.pdf | author:Panos P. Markopoulos, George N. Karystinos, Dimitris A. Pados category:stat.ML published:2013-09-04 summary:We describe ways to define and calculate $L_1$-norm signal subspaces whichare less sensitive to outlying data than $L_2$-calculated subspaces. We focuson the computation of the $L_1$ maximum-projection principal component of adata matrix containing N signal samples of dimension D and conclude that thegeneral problem is formally NP-hard in asymptotically large N, D. We prove,however, that the case of engineering interest of fixed dimension D andasymptotically large sample support N is not and we present an optimalalgorithm of complexity $O(N^D)$. We generalize to multiple$L_1$-max-projection components and present an explicit optimal $L_1$ subspacecalculation algorithm in the form of matrix nuclear-norm evaluations. Weconclude with illustrations of $L_1$-subspace signal processing in the fieldsof data dimensionality reduction and direction-of-arrival estimation.
arxiv-3900-214 | Analysing Quality of English-Hindi Machine Translation Engine Outputs Using Bayesian Classification | http://arxiv.org/pdf/1309.1129v1.pdf | author:Rashmi Gupta, Nisheeth Joshi, Iti Mathur category:cs.CL published:2013-09-04 summary:This paper considers the problem for estimating the quality of machinetranslation outputs which are independent of human intervention and aregenerally addressed using machine learning techniques.There are variousmeasures through which a machine learns translations quality. AutomaticEvaluation metrics produce good co-relation at corpus level but cannot producethe same results at the same segment or sentence level. In this paper 16features are extracted from the input sentences and their translations and aquality score is obtained based on Bayesian inference produced from trainingdata.
arxiv-3900-215 | Learning to answer questions | http://arxiv.org/pdf/1309.1125v1.pdf | author:Ana Cristina Mendes, Luísa Coheur, Sérgio Curto category:cs.CL published:2013-09-04 summary:We present an open-domain Question-Answering system that learns to answerquestions based on successful past interactions. We follow a pattern-basedapproach to Answer-Extraction, where (lexico-syntactic) patterns that relate aquestion to its answer are automatically learned and used to answer futurequestions. Results show that our approach contributes to the system's bestperformance when it is conjugated with typical Answer-Extraction strategies.Moreover, it allows the system to learn with the answered questions and torectify wrong or unsolved past questions.
arxiv-3900-216 | Boosting in Location Space | http://arxiv.org/pdf/1309.1080v1.pdf | author:Damian Eads, David Helmbold, Ed Rosten category:cs.CV published:2013-09-04 summary:The goal of object detection is to find objects in an image. An objectdetector accepts an image and produces a list of locations as $(x,y)$ pairs.Here we introduce a new concept: {\bf location-based boosting}. Location-basedboosting differs from previous boosting algorithms because it optimizes a newspatial loss function to combine object detectors, each of which may havemarginal performance, into a single, more accurate object detector. Astructured representation of object locations as a list of $(x,y)$ pairs is amore natural domain for object detection than the spatially unstructuredrepresentation produced by classifiers. Furthermore, this formulation allows usto take advantage of the intuition that large areas of the background areuninteresting and it is not worth expending computational effort on them. Thisresults in a more scalable algorithm because it does not need to take measuresto prevent the background data from swamping the foreground data such assubsampling or applying an ad-hoc weighting to the pixels. We first present thetheory of location-based boosting, and then motivate it with empirical resultson a challenging data set.
arxiv-3900-217 | Thermal Human face recognition based on Haar wavelet transform and series matching technique | http://arxiv.org/pdf/1309.1156v1.pdf | author:Ayan Seal, Suranjan Ganguly, Debotosh Bhattacharjee, Mita Nasipuri, Dipak kr. Basu category:cs.CV published:2013-09-04 summary:Thermal infrared (IR) images represent the heat patterns emitted from hotobject and they do not consider the energies reflected from an object. Objectsliving or non-living emit different amounts of IR energy according to theirbody temperature and characteristics. Humans are homoeothermic and hencecapable of maintaining constant temperature under different surroundingtemperature. Face recognition from thermal (IR) images should focus on changesof temperature on facial blood vessels. These temperature changes can beregarded as texture features of images and wavelet transform is a very goodtool to analyze multi-scale and multi-directional texture. Wavelet transform isalso used for image dimensionality reduction, by removing redundancies andpreserving original features of the image. The sizes of the facial images arenormally large. So, the wavelet transform is used before image similarity ismeasured. Therefore this paper describes an efficient approach of human facerecognition based on wavelet transform from thermal IR images. The systemconsists of three steps. At the very first step, human thermal IR face image ispreprocessed and the face region is only cropped from the entire image.Secondly, Haar wavelet is used to extract low frequency band from the croppedface region. Lastly, the image classification between the training images andthe test images is done, which is based on low-frequency components. Theproposed approach is tested on a number of human thermal infrared face imagescreated at our own laboratory and Terravic Facial IR Database. Experimentalresults indicated that the thermal infra red face images can be recognized bythe proposed system effectively. The maximum success of 95% recognition hasbeen achieved.
arxiv-3900-218 | Minutiae Based Thermal Human Face Recognition using Label Connected Component Algorithm | http://arxiv.org/pdf/1309.1155v1.pdf | author:Ayan Seal, Suranjan Ganguly, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu category:cs.CV published:2013-09-04 summary:In this paper, a thermal infra red face recognition system for humanidentification and verification using blood perfusion data and back propagationfeed forward neural network is proposed. The system consists of three steps. Atthe very first step face region is cropped from the colour 24-bit input images.Secondly face features are extracted from the croped region, which will betaken as the input of the back propagation feed forward neural network in thethird step and classification and recognition is carried out. The proposedapproaches are tested on a number of human thermal infra red face imagescreated at our own laboratory. Experimental results reveal the higher degreeperformance
arxiv-3900-219 | Advances in the Logical Representation of Lexical Semantics | http://arxiv.org/pdf/1309.1014v1.pdf | author:Bruno Mery, Christian Retoré category:cs.CL published:2013-09-04 summary:The integration of lexical semantics and pragmatics in the analysis of themeaning of natural lan- guage has prompted changes to the global frameworkderived from Montague. In those works, the original lexicon, in which wordswere assigned an atomic type of a single-sorted logic, has been re- placed by aset of many-facetted lexical items that can compose their meaning with salientcontextual properties using a rich typing system as a guide. Having related ourproposal for such an expanded framework \LambdaTYn, we present some recentadvances in the logical formalisms associated, including constraints on lexicaltransformations and polymorphic quantifiers, and ongoing discussions andresearch on the granularity of the type system and the limits of transitivity.
arxiv-3900-220 | A Comparative Study of Human thermal face recognition based on Haar wavelet transform (HWT) and Local Binary Pattern (LBP) | http://arxiv.org/pdf/1309.1009v1.pdf | author:Ayan Seal, Suranjan Ganguly, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu category:cs.CV published:2013-09-04 summary:Thermal infra-red (IR) images focus on changes of temperature distribution onfacial muscles and blood vessels. These temperature changes can be regarded astexture features of images. A comparative study of face recognition methodsworking in thermal spectrum is carried out in this paper. In these study twolocal-matching methods based on Haar wavelet transform and Local Binary Pattern(LBP) are analyzed. Wavelet transform is a good tool to analyze multi-scale,multi-direction changes of texture. Local binary patterns (LBP) are a type offeature used for classification in computer vision. Firstly, human thermal IRface image is preprocessed and cropped the face region only from the entireimage. Secondly, two different approaches are used to extract the features fromthe cropped face region. In the first approach, the training images and thetest images are processed with Haar wavelet transform and the LL band and theaverage of LH/HL/HH bands sub-images are created for each face image. Then atotal confidence matrix is formed for each face image by taking a weighted sumof the corresponding pixel values of the LL band and average band. For LBPfeature extraction, each of the face images in training and test datasets isdivided into 161 numbers of sub images, each of size 8X8 pixels. For each suchsub images, LBP features are extracted which are concatenated in row wisemanner. PCA is performed separately on the individual feature set fordimensionality reeducation. Finally two different classifiers are used toclassify face images. One such classifier multi-layer feed forward neuralnetwork and another classifier is minimum distance classifier. The Experimentshave been performed on the database created at our own laboratory and TerravicFacial IR Database.
arxiv-3900-221 | Automated Thermal Face recognition based on Minutiae Extraction | http://arxiv.org/pdf/1309.1000v1.pdf | author:Ayan Seal, Suranjan Ganguly, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kr. Basu category:cs.CV published:2013-09-04 summary:In this paper an efficient approach for human face recognition based on theuse of minutiae points in thermal face image is proposed. The thermogram ofhuman face is captured by thermal infra-red camera. Image processing methodsare used to pre-process the captured thermogram, from which differentphysiological features based on blood perfusion data are extracted. Bloodperfusion data are related to distribution of blood vessels under the faceskin. In the present work, three different methods have been used to get theblood perfusion image, namely bit-plane slicing and medial axis transform,morphological erosion and medial axis transform, sobel edge operators.Distribution of blood vessels is unique for each person and a set of extractedminutiae points from a blood perfusion data of a human face should be uniquefor that face. Two different methods are discussed for extracting minutiaepoints from blood perfusion data. For extraction of features entire face imageis partitioned into equal size blocks and the total number of minutiae pointsfrom each block is computed to construct final feature vector. Therefore, thesize of the feature vectors is found to be same as total number of blocksconsidered. A five layer feed-forward back propagation neural network is usedas the classification tool. A number of experiments were conducted to evaluatethe performance of the proposed face recognition methodologies with varyingblock size on the database created at our own laboratory. It has been foundthat the first method supercedes the other two producing an accuracy of 97.62%with block size 16X16 for bit-plane 4.
arxiv-3900-222 | Minutiae Based Thermal Face Recognition using Blood Perfusion Data | http://arxiv.org/pdf/1309.0999v1.pdf | author:Ayan Seal, Mita Nasipuri, Debotosh Bhattacharjee, Dipak Kumar Basu category:cs.CV published:2013-09-04 summary:This paper describes an efficient approach for human face recognition basedon blood perfusion data from infra-red face images. Blood perfusion data arecharacterized by the regional blood flow in human tissue and therefore do notdepend entirely on surrounding temperature. These data bear a great potentialfor deriving discriminating facial thermogram for better classification andrecognition of face images in comparison to optical image data. Blood perfusiondata are related to distribution of blood vessels under the face skin. Adistribution of blood vessels are unique for each person and as a set ofextracted minutiae points from a blood perfusion data of a human face should beunique for that face. There may be several such minutiae point sets for asingle face but all of these correspond to that particular face only. Entireface image is partitioned into equal blocks and the total number of minutiaepoints from each block is computed to construct final vector. Therefore, thesize of the feature vectors is found to be same as total number of blocksconsidered. For classification, a five layer feed-forward backpropagationneural network has been used. A number of experiments were conducted toevaluate the performance of the proposed face recognition system with varyingblock sizes. Experiments have been performed on the database created at our ownlaboratory. The maximum success of 91.47% recognition has been achieved withblock size 8X8.
arxiv-3900-223 | Efficient binary tomographic reconstruction | http://arxiv.org/pdf/1309.0985v1.pdf | author:Stephane Roux, Hugo Leclerc, François Hild category:cs.CV published:2013-09-04 summary:Tomographic reconstruction of a binary image from few projections isconsidered. A novel {\em heuristic} algorithm is proposed, the central elementof which is a nonlinear transformation $\psi(p)=\log(p/(1-p))$ of theprobability $p$ that a pixel of the sought image be 1-valued. It consists ofbackprojections based on $\psi(p)$ and iterative corrections. Application ofthis algorithm to a series of artificial test cases leads to exact binaryreconstructions, (i.e recovery of the binary image for each single pixel) fromthe knowledge of projection data over a few directions. Images up to $10^6$pixels are reconstructed in a few seconds. A series of test cases is performedfor comparison with previous methods, showing a better efficiency and reducedcomputation times.
arxiv-3900-224 | On the Robustness of Temporal Properties for Stochastic Models | http://arxiv.org/pdf/1309.0866v1.pdf | author:Ezio Bartocci, Luca Bortolussi, Laura Nenzi, Guido Sanguinetti category:cs.LO cs.AI cs.LG cs.SY published:2013-09-03 summary:Stochastic models such as Continuous-Time Markov Chains (CTMC) and StochasticHybrid Automata (SHA) are powerful formalisms to model and to reason about thedynamics of biological systems, due to their ability to capture thestochasticity inherent in biological processes. A classical question in formalmodelling with clear relevance to biological modelling is the model checkingproblem. i.e. calculate the probability that a behaviour, expressed forinstance in terms of a certain temporal logic formula, may occur in a givenstochastic process. However, one may not only be interested in the notion ofsatisfiability, but also in the capacity of a system to mantain a particularemergent behaviour unaffected by the perturbations, caused e.g. from extrinsicnoise, or by possible small changes in the model parameters. To address thisissue, researchers from the verification community have recently proposedseveral notions of robustness for temporal logic providing suitable definitionsof distance between a trajectory of a (deterministic) dynamical system and theboundaries of the set of trajectories satisfying the property of interest. Thecontributions of this paper are twofold. First, we extend the notion ofrobustness to stochastic systems, showing that this naturally leads to adistribution of robustness scores. By discussing two examples, we show how toapproximate the distribution of the robustness score and its key indicators:the average robustness and the conditional average robustness. Secondly, weshow how to combine these indicators with the satisfaction probability toaddress the system design problem, where the goal is to optimize some controlparameters of a stochastic model in order to best maximize robustness of thedesired specifications.
arxiv-3900-225 | Nonlinear Time Series Modeling by LPTime,Nonparametric Empirical Learning | http://arxiv.org/pdf/1308.0642v2.pdf | author:Subhadeep Mukhopadhyay, Emanuel Parzen category:math.ST stat.AP stat.ME stat.ML stat.TH published:2013-08-03 summary:We describe a new comprehensive approach to nonlinear time series analysisand modeling based on recently developed theory on unified algorithms of datascience via LP modeling. We introduce novel data-specific mid-distributionbased Legendre Polynomial(LP) like nonlinear transformations of the originaltime series Y(t) that enables us to adapt all the existing stationary linearGaussian time series modeling strategy and made it applicable for non-Gaussianand nonlinear processes in a robust fashion. The emphasis of the present paperis on empirical time series modeling via the algorithm LPTime. We describe eachstage of the model building process, associated theoretical concepts andillustrate with daily S&P 500 return data between Jan/2/1963 - Dec/31/2009. Ourproposed LPTime algorithm systematically discovers all the `stylized facts' ofthe financial time series automatically all at once, which were previouslynoted by many researchers one at a time.
arxiv-3900-226 | BayesOpt: A Library for Bayesian optimization with Robotics Applications | http://arxiv.org/pdf/1309.0671v1.pdf | author:Ruben Martinez-Cantin category:cs.RO cs.AI cs.LG cs.MS published:2013-09-03 summary:The purpose of this paper is twofold. On one side, we present a generalframework for Bayesian optimization and we compare it with some related fieldsin active learning and Bayesian numerical analysis. On the other hand, Bayesianoptimization and related problems (bandits, sequential experimental design) arehighly dependent on the surrogate model that is selected. However, there is noclear standard in the literature. Thus, we present a fast and flexible toolboxthat allows to test and combine different models and criteria with littleeffort. It includes most of the state-of-the-art contributions, algorithms andmodels. Its speed also removes part of the stigma that Bayesian optimizationmethods are only good for "expensive functions". The software is free and itcan be used in many operating systems and computer languages.
arxiv-3900-227 | Learning to Rank for Blind Image Quality Assessment | http://arxiv.org/pdf/1309.0213v2.pdf | author:Fei Gao, Dacheng Tao, Xinbo Gao, Xuelong Li category:cs.CV published:2013-09-01 summary:Blind image quality assessment (BIQA) aims to predict perceptual imagequality scores without access to reference images. State-of-the-art BIQAmethods typically require subjects to score a large number of images to train arobust model. However, the acquisition of image quality scores has severallimitations: 1) scores are not precise, because subjects are usually uncertainabout which score most precisely represents the perceptual quality of a givenimage; 2) subjective judgements of quality may be biased by image content; 3)the quality scales between different distortion categories are inconsistent;and 4) it is challenging to obtain a large scale database, or to extendexisting databases, because of the inconvenience of collecting sufficientimages, training the subjects, conducting subjective experiments, andrealigning human quality evaluations. To combat these limitations, this paperexplores and exploits preference image pairs such as "the quality of image Iais better than that of image Ib" for training a robust BIQA model. Thepreference label, representing the relative quality of two images, is generallyprecise and consistent, and is not sensitive to image content, distortion type,or subject identity; such PIPs can be generated at very low cost. The proposedBIQA method is one of learning to rank. We first formulate the problem oflearning the mapping from the image features to the preference label as one ofclassification. In particular, we investigate the utilization of a multiplekernel learning algorithm based on group lasso (MKLGL) to provide a solution. Asimple but effective strategy to estimate perceptual image quality scores isthen presented. Experiments show that the proposed BIQA method is highlyeffective and achieves comparable performance to state-of-the-art BIQAalgorithms. Moreover, the proposed method can be easily extended to newdistortion categories.
arxiv-3900-228 | A Multi-View Embedding Space for Modeling Internet Images, Tags, and their Semantics | http://arxiv.org/pdf/1212.4522v2.pdf | author:Yunchao Gong, Qifa Ke, Michael Isard, Svetlana Lazebnik category:cs.CV cs.IR cs.LG cs.MM published:2012-12-18 summary:This paper investigates the problem of modeling Internet images andassociated text or tags for tasks such as image-to-image search, tag-to-imagesearch, and image-to-tag search (image annotation). We start with canonicalcorrelation analysis (CCA), a popular and successful approach for mappingvisual and textual features to the same latent space, and incorporate a thirdview capturing high-level image semantics, represented either by a singlecategory or multiple non-mutually-exclusive concepts. We present two ways totrain the three-view embedding: supervised, with the third view coming fromground-truth labels or search keywords; and unsupervised, with semantic themesautomatically obtained by clustering the tags. To ensure high accuracy forretrieval tasks while keeping the learning process scalable, we combinemultiple strong visual features and use explicit nonlinear kernel mappings toefficiently approximate kernel CCA. To perform retrieval, we use a speciallydesigned similarity function in the embedded space, which substantiallyoutperforms the Euclidean distance. The resulting system produces compellingqualitative results and outperforms a number of two-view baselines on retrievaltasks on three large-scale Internet image datasets.
arxiv-3900-229 | Scalable Probabilistic Entity-Topic Modeling | http://arxiv.org/pdf/1309.0337v1.pdf | author:Neil Houlsby, Massimiliano Ciaramita category:stat.ML cs.IR cs.LG published:2013-09-02 summary:We present an LDA approach to entity disambiguation. Each topic is associatedwith a Wikipedia article and topics generate either content words or entitymentions. Training such models is challenging because of the topic andvocabulary size, both in the millions. We tackle these problems using a noveldistributed inference and representation framework based on a parallel Gibbssampler guided by the Wikipedia link graph, and pipelines of MapReduce allowingfast and memory-frugal processing of large datasets. We report state-of-the-artperformance on a public dataset.
arxiv-3900-230 | Parallel Gaussian Process Optimization with Upper Confidence Bound and Pure Exploration | http://arxiv.org/pdf/1304.5350v3.pdf | author:Emile Contal, David Buffoni, Alexandre Robicquet, Nicolas Vayatis category:cs.LG stat.ML published:2013-04-19 summary:In this paper, we consider the challenge of maximizing an unknown function ffor which evaluations are noisy and are acquired with high cost. An iterativeprocedure uses the previous measures to actively select the next estimation off which is predicted to be the most useful. We focus on the case where thefunction can be evaluated in parallel with batches of fixed size and analyzethe benefit compared to the purely sequential procedure in terms of cumulativeregret. We introduce the Gaussian Process Upper Confidence Bound and PureExploration algorithm (GP-UCB-PE) which combines the UCB strategy and PureExploration in the same batch of evaluations along the parallel iterations. Weprove theoretical upper bounds on the regret with batches of size K for thisprocedure which show the improvement of the order of sqrt{K} for fixediteration cost over purely sequential versions. Moreover, the multiplicativeconstants involved have the property of being dimension-free. We also confirmempirically the efficiency of GP-UCB-PE on real and synthetic problems comparedto state-of-the-art competitors.
arxiv-3900-231 | A Study on Unsupervised Dictionary Learning and Feature Encoding for Action Classification | http://arxiv.org/pdf/1309.0309v1.pdf | author:Xiaojiang Peng, Qiang Peng, Yu Qiao, Junzhou Chen, Mehtab Afzal category:cs.CV published:2013-09-02 summary:Many efforts have been devoted to develop alternative methods to traditionalvector quantization in image domain such as sparse coding and soft-assignment.These approaches can be split into a dictionary learning phase and a featureencoding phase which are often closely connected. In this paper, we investigatethe effects of these phases by separating them for video-based actionclassification. We compare several dictionary learning methods and featureencoding schemes through extensive experiments on KTH and HMDB51 datasets.Experimental results indicate that sparse coding performs consistently betterthan the other encoding methods in large complex dataset (i.e., HMDB51), and itis robust to different dictionaries. For small simple dataset (i.e., KTH) withless variation, however, all the encoding strategies perform competitively. Inaddition, we note that the strength of sophisticated encoding approaches comesnot from their corresponding dictionaries but the encoding mechanisms, and wecan just use randomly selected exemplars as dictionaries for video-based actionclassification.
arxiv-3900-232 | Unmixing Incoherent Structures of Big Data by Randomized or Greedy Decomposition | http://arxiv.org/pdf/1309.0302v1.pdf | author:Tianyi Zhou, Dacheng Tao category:stat.ML cs.DS cs.LG published:2013-09-02 summary:Learning big data by matrix decomposition always suffers from expensivecomputation, mixing of complicated structures and noise. In this paper, westudy more adaptive models and efficient algorithms that decompose a datamatrix as the sum of semantic components with incoherent structures. We firstlyintroduce "GO decomposition (GoDec)", an alternating projection methodestimating the low-rank part $L$ and the sparse part $S$ from data matrix$X=L+S+G$ corrupted by noise $G$. Two acceleration strategies are proposed toobtain scalable unmixing algorithm on big data: 1) Bilateral random projection(BRP) is developed to speed up the update of $L$ in GoDec by a closed-formbuilt from left and right random projections of $X-S$ in lower dimensions; 2)Greedy bilateral (GreB) paradigm updates the left and right factors of $L$ in amutually adaptive and greedy incremental manner, and achieve significantimprovement in both time and sample complexities. Then we proposes threenontrivial variants of GoDec that generalizes GoDec to more general data typeand whose fast algorithms can be derived from the two strategies......
arxiv-3900-233 | Parallel machine scheduling with step deteriorating jobs and setup times by a hybrid discrete cuckoo search algorithm | http://arxiv.org/pdf/1309.1453v1.pdf | author:Peng Guo, Wenming Cheng, Yi Wang category:math.OC cs.DS cs.NE published:2013-09-02 summary:This article considers the parallel machine scheduling problem withstep-deteriorating jobs and sequence-dependent setup times. The objective is tominimize the total tardiness by determining the allocation and sequence of jobson identical parallel machines. In this problem, the processing time of eachjob is a step function dependent upon its starting time. An individual extendedtime is penalized when the starting time of a job is later than a specificdeterioration date. The possibility of deterioration of a job makes theparallel machine scheduling problem more challenging than ordinary ones. Amixed integer programming model for the optimal solution is derived. Due to itsNP-hard nature, a hybrid discrete cuckoo search algorithm is proposed to solvethis problem. In order to generate a good initial swarm, a modified heuristicnamed the MBHG is incorporated into the initialization of population. Severaldiscrete operators are proposed in the random walk of L\'{e}vy Flights and thecrossover search. Moreover, a local search procedure based on variableneighborhood descent is integrated into the algorithm as a hybrid strategy inorder to improve the quality of elite solutions. Computational experiments areexecuted on two sets of randomly generated test instances. The results showthat the proposed hybrid algorithm can yield better solutions in comparisonwith the commercial solver CPLEX with one hour time limit, discrete cuckoosearch algorithm and the existing variable neighborhood search algorithm.
arxiv-3900-234 | Demodulation of Sparse PPM Signals with Low Samples Using Trained RIP Matrix | http://arxiv.org/pdf/1309.5854v1.pdf | author:Seyed Hossein Hosseini, Mahrokh G. Shayesteh, Mehdi Chehel Amirani category:cs.OH cs.IT cs.LG math.IT published:2013-09-01 summary:Compressed sensing (CS) theory considers the restricted isometry property(RIP) as a sufficient condition for measurement matrix which guarantees therecovery of any sparse signal from its compressed measurements. The RIPcondition also preserves enough information for classification of sparsesymbols, even with fewer measurements. In this work, we utilize RIP bound asthe cost function for training a simple neural network in order to exploit thenear optimal measurements or equivalently near optimal features forclassification of a known set of sparse symbols. As an example, we considerdemodulation of pulse position modulation (PPM) signals. The results indicatethat the proposed method has much better performance than the randommeasurements and requires less samples than the optimum matched filterdemodulator, at the expense of some performance loss. Further, the proposedapproach does not need equalizer for multipath channels in contrast to theconventional receiver.
arxiv-3900-235 | Multi-Column Deep Neural Networks for Offline Handwritten Chinese Character Classification | http://arxiv.org/pdf/1309.0261v1.pdf | author:Dan Cireşan, Jürgen Schmidhuber category:cs.CV published:2013-09-01 summary:Our Multi-Column Deep Neural Networks achieve best known recognition rates onChinese characters from the ICDAR 2011 and 2013 offline handwritingcompetitions, approaching human performance.
arxiv-3900-236 | Ensemble approaches for improving community detection methods | http://arxiv.org/pdf/1309.0242v1.pdf | author:Johan Dahlin, Pontus Svenson category:physics.soc-ph cs.LG cs.SI stat.ML published:2013-09-01 summary:Statistical estimates can often be improved by fusion of data from severaldifferent sources. One example is so-called ensemble methods which have beensuccessfully applied in areas such as machine learning for classification andclustering. In this paper, we present an ensemble method to improve communitydetection by aggregating the information found in an ensemble of communitystructures. This ensemble can found by re-sampling methods, multiple runs of astochastic community detection method, or by several different communitydetection algorithms applied to the same network. The proposed method isevaluated using random networks with community structures and compared with twocommonly used community detection methods. The proposed method when applied ona stochastic community detection algorithm performs well with low computationalcomplexity, thus offering both a new approach to community detection and anadditional community detection method.
arxiv-3900-237 | API design for machine learning software: experiences from the scikit-learn project | http://arxiv.org/pdf/1309.0238v1.pdf | author:Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake Vanderplas, Arnaud Joly, Brian Holt, Gaël Varoquaux category:cs.LG cs.MS published:2013-09-01 summary:Scikit-learn is an increasingly popular machine learning li- brary. Writtenin Python, it is designed to be simple and efficient, accessible tonon-experts, and reusable in various contexts. In this paper, we present anddiscuss our design choices for the application programming interface (API) ofthe project. In particular, we describe the simple and elegant interface sharedby all learning and processing units in the library and then discuss itsadvantages in terms of composition and reusability. The paper also comments onimplementation details specific to the Python ecosystem and analyzes obstaclesfaced by users and developers of the library.
arxiv-3900-238 | Non-Asymptotic Convergence Analysis of Inexact Gradient Methods for Machine Learning Without Strong Convexity | http://arxiv.org/pdf/1309.0113v1.pdf | author:Anthony Man-Cho So category:math.OC cs.LG published:2013-08-31 summary:Many recent applications in machine learning and data fitting call for thealgorithmic solution of structured smooth convex optimization problems.Although the gradient descent method is a natural choice for this task, itrequires exact gradient computations and hence can be inefficient when theproblem size is large or the gradient is difficult to evaluate. Therefore,there has been much interest in inexact gradient methods (IGMs), in which anefficiently computable approximate gradient is used to perform the update ineach iteration. Currently, non-asymptotic linear convergence results for IGMsare typically established under the assumption that the objective function isstrongly convex, which is not satisfied in many applications of interest; whilelinear convergence results that do not require the strong convexity assumptionare usually asymptotic in nature. In this paper, we combine the best of thesetwo types of results and establish---under the standard assumption that thegradient approximation errors decrease linearly to zero---the non-asymptoticlinear convergence of IGMs when applied to a class of structured convexoptimization problems. Such a class covers settings where the objectivefunction is not necessarily strongly convex and includes the least squares andlogistic regression problems. We believe that our techniques will find furtherapplications in the non-asymptotic convergence analysis of other first-ordermethods.
arxiv-3900-239 | Derivation of Upper Bounds on Optimization Time of Population-Based Evolutionary Algorithm on a Function with Fitness Plateaus Using Elitism Levels Traverse Mechanism | http://arxiv.org/pdf/1204.2321v6.pdf | author:Aram Ter-Sarkisov, Stephen Marsland category:cs.NE cs.AI published:2012-04-11 summary:In this article a tool for the analysis of population-based EAs is used toderive asymptotic upper bounds on the optimization time of the algorithmsolving Royal Roads problem, a test function with plateaus of fitness. Inaddition to this, limiting distribution of a certain subset of the populationis approximated.
arxiv-3900-240 | Mixtures of Common Skew-t Factor Analyzers | http://arxiv.org/pdf/1307.5558v3.pdf | author:Paula M. Murray, Paul D. McNicholas, Ryan P. Browne category:stat.ME stat.AP stat.CO stat.ML published:2013-07-21 summary:A mixture of common skew-t factor analyzers model is introduced formodel-based clustering of high-dimensional data. By assuming common componentfactor loadings, this model allows clustering to be performed in the presenceof a large number of mixture components or when the number of dimensions is toolarge to be well-modelled by the mixtures of factor analyzers model or avariant thereof. Furthermore, assuming that the component densities follow askew-t distribution allows robust clustering of skewed data. The alternatingexpectation-conditional maximization algorithm is employed for parameterestimation. We demonstrate excellent clustering performance when our model isapplied to real and simulated data.This paper marks the first time that skewedcommon factors have been used.
arxiv-3900-241 | Inconsistency of Pitman-Yor process mixtures for the number of components | http://arxiv.org/pdf/1309.0024v1.pdf | author:Jeffrey W. Miller, Matthew T. Harrison category:math.ST stat.ML stat.TH published:2013-08-30 summary:In many applications, a finite mixture is a natural model, but it can bedifficult to choose an appropriate number of components. To circumvent thischoice, investigators are increasingly turning to Dirichlet process mixtures(DPMs), and Pitman-Yor process mixtures (PYMs), more generally. While thesemodels may be well-suited for Bayesian density estimation, many investigatorsare using them for inferences about the number of components, by consideringthe posterior on the number of components represented in the observed data. Weshow that this posterior is not consistent --- that is, on data from a finitemixture, it does not concentrate at the true number of components. This resultapplies to a large class of nonparametric mixtures, including DPMs and PYMs,over a wide variety of families of component distributions, includingessentially all discrete families, as well as continuous exponential familiessatisfying mild regularity conditions (such as multivariate Gaussians).
arxiv-3900-242 | Concentration Inequalities for Bounded Random Vectors | http://arxiv.org/pdf/1309.0003v1.pdf | author:Xinjia Chen category:math.PR cs.LG math.ST stat.TH published:2013-08-30 summary:We derive simple concentration inequalities for bounded random vectors, whichgeneralize Hoeffding's inequalities for bounded scalar random variables. Asapplications, we apply the general results to multinomial and Dirichletdistributions to obtain multivariate concentration inequalities.
arxiv-3900-243 | Blending Learning and Inference in Structured Prediction | http://arxiv.org/pdf/1210.2346v2.pdf | author:Tamir Hazan, Alexander Schwing, David McAllester, Raquel Urtasun category:cs.LG published:2012-10-08 summary:In this paper we derive an efficient algorithm to learn the parameters ofstructured predictors in general graphical models. This algorithm blends thelearning and inference tasks, which results in a significant speedup overtraditional approaches, such as conditional random fields and structuredsupport vector machines. For this purpose we utilize the structures of thepredictors to describe a low dimensional structured prediction task whichencourages local consistencies within the different structures while learningthe parameters of the model. Convexity of the learning task provides the meansto enforce the consistencies between the different parts. Theinference-learning blending algorithm that we propose is guaranteed to convergeto the optimum of the low dimensional primal and dual programs. Unlike many ofthe existing approaches, the inference-learning blending allows us to learnefficiently high-order graphical models, over regions of any size, and verylarge number of parameters. We demonstrate the effectiveness of our approach,while presenting state-of-the-art results in stereo estimation, semanticsegmentation, shape reconstruction, and indoor scene understanding.
arxiv-3900-244 | Separable Approximations and Decomposition Methods for the Augmented Lagrangian | http://arxiv.org/pdf/1308.6774v1.pdf | author:Rachael Tappenden, Peter Richtarik, Burak Buke category:math.OC cs.DC cs.NA stat.ML published:2013-08-30 summary:In this paper we study decomposition methods based on separableapproximations for minimizing the augmented Lagrangian. In particular, we studyand compare the Diagonal Quadratic Approximation Method (DQAM) of Mulvey andRuszczy\'{n}ski and the Parallel Coordinate Descent Method (PCDM) ofRicht\'arik and Tak\'a\v{c}. We show that the two methods are equivalent forfeasibility problems up to the selection of a single step-size parameter.Furthermore, we prove an improved complexity bound for PCDM under strongconvexity, and show that this bound is at least $8(L'/\bar{L})(\omega-1)^2$times better than the best known bound for DQAM, where $\omega$ is the degreeof partial separability and $L'$ and $\bar{L}$ are the maximum and average ofthe block Lipschitz constants of the gradient of the quadratic penaltyappearing in the augmented Lagrangian.
arxiv-3900-245 | Discriminative Parameter Estimation for Random Walks Segmentation | http://arxiv.org/pdf/1308.6721v1.pdf | author:Pierre-Yves Baudin, Danny Goodman, Puneet Kumar, Noura Azzabou, Pierre G. Carlier, Nikos Paragios, M. Pawan Kumar category:cs.CV cs.LG published:2013-08-30 summary:The Random Walks (RW) algorithm is one of the most e - cient and easy-to-useprobabilistic segmentation methods. By combining contrast terms with priorterms, it provides accurate segmentations of medical images in a fullyautomated manner. However, one of the main drawbacks of using the RW algorithmis that its parameters have to be hand-tuned. we propose a novel discriminativelearning framework that estimates the parameters using a training dataset. Themain challenge we face is that the training samples are not fully supervised.Speci cally, they provide a hard segmentation of the images, instead of aproba- bilistic segmentation. We overcome this challenge by treating the opti-mal probabilistic segmentation that is compatible with the given hardsegmentation as a latent variable. This allows us to employ the latent supportvector machine formulation for parameter estimation. We show that our approachsigni cantly outperforms the baseline methods on a challenging datasetconsisting of real clinical 3D MRI volumes of skeletal muscles.
arxiv-3900-246 | Image Set based Collaborative Representation for Face Recognition | http://arxiv.org/pdf/1308.6687v1.pdf | author:Pengfei Zhu, Wangmeng Zuo, Lei Zhang, Simon C. K. Shiu, David Zhang category:cs.CV published:2013-08-30 summary:With the rapid development of digital imaging and communication technologies,image set based face recognition (ISFR) is becoming increasingly important. Onekey issue of ISFR is how to effectively and efficiently represent the queryface image set by using the gallery face image sets. The set-to-set distancebased methods ignore the relationship between gallery sets, while representingthe query set images individually over the gallery sets ignores the correlationbetween query set images. In this paper, we propose a novel image set basedcollaborative representation and classification method for ISFR. By modelingthe query set as a convex or regularized hull, we represent this hullcollaboratively over all the gallery sets. With the resolved representationcoefficients, the distance between the query set and each gallery set can thenbe calculated for classification. The proposed model naturally and effectivelyextends the image based collaborative representation to an image set based one,and our extensive experiments on benchmark ISFR databases show the superiorityof the proposed method to state-of-the-art ISFR methods under different setsizes in terms of both recognition rate and efficiency.
arxiv-3900-247 | Guaranteed Classification via Regularized Similarity Learning | http://arxiv.org/pdf/1306.3108v2.pdf | author:Zheng-Chu Guo, Yiming Ying category:cs.LG published:2013-06-13 summary:Learning an appropriate (dis)similarity function from the available data is acentral problem in machine learning, since the success of many machine learningalgorithms critically depends on the choice of a similarity function to compareexamples. Despite many approaches for similarity metric learning have beenproposed, there is little theoretical study on the links between similaritymet- ric learning and the classification performance of the result classifier.In this paper, we propose a regularized similarity learning formulationassociated with general matrix-norms, and establish their generalizationbounds. We show that the generalization error of the resulting linear separatorcan be bounded by the derived generalization bound of similarity learning. Thisshows that a good gen- eralization of the learnt similarity function guaranteesa good classification of the resulting linear classifier. Our results extendand improve those obtained by Bellet at al. [3]. Due to the techniquesdependent on the notion of uniform stability [6], the bound obtained thereholds true only for the Frobenius matrix- norm regularization. Our techniquesusing the Rademacher complexity [5] and its related Khinchin-type inequalityenable us to establish bounds for regularized similarity learning formulationsassociated with general matrix-norms including sparse L 1 -norm and mixed(2,1)-norm.
arxiv-3900-248 | The KL-UCB Algorithm for Bounded Stochastic Bandits and Beyond | http://arxiv.org/pdf/1102.2490v5.pdf | author:Aurélien Garivier, Olivier Cappé category:math.ST cs.LG cs.SY math.OC stat.TH 93E35 published:2011-02-12 summary:This paper presents a finite-time analysis of the KL-UCB algorithm, anonline, horizon-free index policy for stochastic bandit problems. We prove twodistinct results: first, for arbitrary bounded rewards, the KL-UCB algorithmsatisfies a uniformly better regret bound than UCB or UCB2; second, in thespecial case of Bernoulli rewards, it reaches the lower bound of Lai andRobbins. Furthermore, we show that simple adaptations of the KL-UCB algorithmare also optimal for specific classes of (possibly unbounded) rewards,including those generated from exponential families of distributions. Alarge-scale numerical study comparing KL-UCB with its main competitors (UCB,UCB2, UCB-Tuned, UCB-V, DMED) shows that KL-UCB is remarkably efficient andstable, including for short time horizons. KL-UCB is also the only method thatalways performs better than the basic UCB policy. Our regret bounds rely ondeviations results of independent interest which are stated and proved in theAppendix. As a by-product, we also obtain an improved regret bound for thestandard UCB algorithm.
arxiv-3900-249 | A New Algorithm of Speckle Filtering using Stochastic Distances | http://arxiv.org/pdf/1308.6487v1.pdf | author:Leonardo Torres, Tamer Cavalcante, Alejandro C. Frery category:cs.IT cs.CV cs.GR math.IT stat.AP stat.ML published:2013-08-29 summary:This paper presents a new approach for filter design based on stochasticdistances and tests between distributions. A window is defined around eachpixel, overlapping samples are compared and only those which pass agoodness-of-fit test are used to compute the filtered value. The technique isapplied to intensity SAR data with homogeneous regions using the Gamma model.The proposal is compared with the Lee's filter using a protocol based on MonteCarlo. Among the criteria used to quantify the quality of filters, we employthe equivalent number of looks, line and edge preservation. Moreover, we alsoassessed the filters by the Universal Image Quality Index and the Pearson'scorrelation on edges regions.
arxiv-3900-250 | Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows | http://arxiv.org/pdf/1204.4539v3.pdf | author:Julien Mairal, Bin Yu category:stat.ML cs.LG math.OC published:2012-04-20 summary:We consider supervised learning problems where the features are embedded in agraph, such as gene expressions in a gene network. In this context, it is ofmuch interest to automatically select a subgraph with few connected components;by exploiting prior knowledge, one can indeed improve the predictionperformance or obtain results that are easier to interpret. Regularization orpenalty functions for selecting features in graphs have recently been proposed,but they raise new algorithmic challenges. For example, they typically requiresolving a combinatorially hard selection problem among all connected subgraphs.In this paper, we propose computationally feasible strategies to select asparse and well-connected subset of features sitting on a directed acyclicgraph (DAG). We introduce structured sparsity penalties over paths on a DAGcalled "path coding" penalties. Unlike existing regularization functions thatmodel long-range interactions between features in a graph, path codingpenalties are tractable. The penalties and their proximal operators involvepath selection problems, which we efficiently solve by leveraging network flowoptimization. We experimentally show on synthetic, image, and genomic data thatour approach is scalable and leads to more connected subgraphs than otherregularization functions for graphs.
arxiv-3900-251 | A Synergistic Approach for Recovering Occlusion-Free Textured 3D Maps of Urban Facades from Heterogeneous Cartographic Data | http://arxiv.org/pdf/1308.6401v1.pdf | author:Karim Hammoudi, Fadi Dornaika, Bahman Soheilian, Bruno Vallet, John McDonald, Nicolas Paparoditis category:cs.CV I.4; I.5 published:2013-08-29 summary:In this paper we present a practical approach for generating anocclusion-free textured 3D map of urban facades by the synergistic use ofterrestrial images, 3D point clouds and area-based information. Particularly indense urban environments, the high presence of urban objects in front of thefacades causes significant difficulties for several stages in computationalbuilding modeling. Major challenges lie on the one hand in extracting complete3D facade quadrilateral delimitations and on the other hand in generatingocclusion-free facade textures. For these reasons, we describe astraightforward approach for completing and recovering facade geometry andtextures by exploiting the data complementarity of terrestrial multi-sourceimagery and area-based information.
arxiv-3900-252 | Says who? Automatic Text-Based Content Analysis of Television News | http://arxiv.org/pdf/1307.4879v2.pdf | author:Carlos Castillo, Gianmarco De Francisci Morales, Marcelo Mendoza, Nasir Khan category:cs.CL cs.IR published:2013-07-18 summary:We perform an automatic analysis of television news programs, based on theclosed captions that accompany them. Specifically, we collect all the newsbroadcasted in over 140 television channels in the US during a period of sixmonths. We start by segmenting, processing, and annotating the closed captionsautomatically. Next, we focus on the analysis of their linguistic style and onmentions of people using NLP methods. We present a series of key insights aboutnews providers, people in the news, and we discuss the biases that can beuncovered by automatic means. These insights are contrasted by looking at thedata from multiple points of view, including qualitative assessment.
arxiv-3900-253 | GNCGCP - Graduated NonConvexity and Graduated Concavity Procedure | http://arxiv.org/pdf/1308.6388v1.pdf | author:Zhi-Yong Liu, Hong Qiao category:cs.CV published:2013-08-29 summary:In this paper we propose the Graduated NonConvexity and Graduated ConcavityProcedure (GNCGCP) as a general optimization framework to approximately solvethe combinatorial optimization problems on the set of partial permutationmatrices. GNCGCP comprises two sub-procedures, graduated nonconvexity (GNC)which realizes a convex relaxation and graduated concavity (GC) which realizesa concave relaxation. It is proved that GNCGCP realizes exactly a type ofconvex-concave relaxation procedure (CCRP), but with a much simpler formulationwithout needing convex or concave relaxation in an explicit way. Actually,GNCGCP involves only the gradient of the objective function and is thereforevery easy to use in practical applications. Two typical NP-hard problems,(sub)graph matching and quadratic assignment problem (QAP), are employed todemonstrate its simplicity and state-of-the-art performance.
arxiv-3900-254 | Collecting Coupons with Random Initial Stake | http://arxiv.org/pdf/1308.6384v1.pdf | author:Benjamin Doerr, Carola Doerr category:cs.DM cs.DS cs.NE F.2.2 published:2013-08-29 summary:Motivated by a problem in the theory of randomized search heuristics, we givea very precise analysis for the coupon collector problem where the collectorstarts with a random set of coupons (chosen uniformly from all sets). We show that the expected number of rounds until we have a coupon of eachtype is $nH_{n/2} - 1/2 \pm o(1)$, where $H_{n/2}$ denotes the $(n/2)$thharmonic number when $n$ is even, and $H_{n/2}:= (1/2) H_{\lfloor n/2 \rfloor}+ (1/2) H_{\lceil n/2 \rceil}$ when $n$ is odd. Consequently, the couponcollector with random initial stake is by half a round faster than the onestarting with exactly $n/2$ coupons (apart from additive $o(1)$ terms). This result implies that classic simple heuristic called \emph{randomizedlocal search} needs an expected number of $nH_{n/2} - 1/2 \pm o(1)$ iterationsto find the optimum of any monotonic function defined on bit-strings of length$n$.
arxiv-3900-255 | Ranking relations using analogies in biological and information networks | http://arxiv.org/pdf/0912.5193v3.pdf | author:Ricardo Silva, Katherine Heller, Zoubin Ghahramani, Edoardo M. Airoldi category:stat.ME cs.LG physics.soc-ph q-bio.QM stat.AP published:2009-12-28 summary:Analogical reasoning depends fundamentally on the ability to learn andgeneralize about relations between objects. We develop an approach torelational learning which, given a set of pairs of objects$\mathbf{S}=\{A^{(1)}:B^{(1)},A^{(2)}:B^{(2)},\ldots,A^{(N)}:B ^{(N)}\}$,measures how well other pairs A:B fit in with the set $\mathbf{S}$. Our workaddresses the following question: is the relation between objects A and Banalogous to those relations found in $\mathbf{S}$? Such questions areparticularly relevant in information retrieval, where an investigator mightwant to search for analogous pairs of objects that match the query set ofinterest. There are many ways in which objects can be related, making the taskof measuring analogies very challenging. Our approach combines a similaritymeasure on function spaces with Bayesian analysis to produce a ranking. Itrequires data containing features of the objects of interest and a link matrixspecifying which relationships exist; no further attributes of suchrelationships are necessary. We illustrate the potential of our method on textanalysis and information networks. An application on discovering functionalinteractions between pairs of proteins is discussed in detail, where we showthat our approach can work in practice even if a small set of protein pairs isprovided.
arxiv-3900-256 | A proposition of a robust system for historical document images indexation | http://arxiv.org/pdf/1308.6319v1.pdf | author:Nizar Zaghden, Remy Mullot, Mohamed Adel Alimi category:cs.CV published:2013-08-28 summary:Characterizing noisy or ancient documents is a challenging problem up to now.Many techniques have been done in order to effectuate feature extraction andimage indexation for such documents. Global approaches are in general lessrobust and exact than local approaches. That's why, we propose in this paper, ahybrid system based on global approach(fractal dimension), and a local onebased on SIFT descriptor. The Scale Invariant Feature Transform seems to dowell with our application since it's rotation invariant and relatively robustto changing illumination.In the first step the calculation of fractal dimensionis applied to images in order to eliminate images which have distant featuresthan image request characteristics. Next, the SIFT is applied to show whichimages match well the request. However the average matching time using thehybrid approach is better than "fractal dimension" and "SIFT descriptor" ifthey are used alone.
arxiv-3900-257 | Categorizing ancient documents | http://arxiv.org/pdf/1308.6311v1.pdf | author:Nizar Zaghden, Remy Mullot, Mohamed Adel Alimi category:cs.CV published:2013-08-28 summary:The analysis of historical documents is still a topical issue given theimportance of information that can be extracted and also the importance givenby the institutions to preserve their heritage. The main idea in order tocharacterize the content of the images of ancient documents after attempting toclean the image is segmented blocks texts from the same image and tries to findsimilar blocks in either the same image or the entire image database. Mostapproaches of offline handwriting recognition proceed by segmenting words intosmaller pieces (usually characters) which are recognized separately.Recognition of a word then requires the recognition of all characters (OCR)that compose it. Our work focuses mainly on the characterization of classes inimages of old documents. We use Som toolbox for finding classes in documents.We applied also fractal dimensions and points of interest to categorize andmatch ancient documents.
arxiv-3900-258 | Text recognition in both ancient and cartographic documents | http://arxiv.org/pdf/1308.6309v1.pdf | author:Nizar Zaghden, Badreddine Khelifi, Adel M. Alimi, Remy Mullot category:cs.CV published:2013-08-28 summary:This paper deals with the recognition and matching of text in bothcartographic maps and ancient documents. The purpose of this work is to findsimilar text regions based on statistical and global features. A phase ofnormalization is done first, in object to well categorize the same quantity ofinformation. A phase of wordspotting is done next by combining local and globalfeatures. We make different experiments by combining the different techniquesof extracting features in order to obtain better results in recognition phase.We applied fontspotting on both ancient documents and cartographic ones. Wealso applied the wordspotting in which we adopted a new technique which triesto compare the images of character and not the entire images words. We presentthe precision and recall values obtained with three methods for the new methodof wordspotting applied on characters only.
arxiv-3900-259 | Computing Lexical Contrast | http://arxiv.org/pdf/1308.6300v1.pdf | author:Saif M. Mohammad, Bonnie J. Dorr, Graeme Hirst, Peter D. Turney category:cs.CL published:2013-08-28 summary:Knowing the degree of semantic contrast between words has widespreadapplication in natural language processing, including machine translation,information retrieval, and dialogue systems. Manually-created lexicons focus onopposites, such as {\rm hot} and {\rm cold}. Opposites are of many kinds suchas antipodals, complementaries, and gradable. However, existing lexicons oftendo not classify opposites into the different kinds. They also do not explicitlylist word pairs that are not opposites but yet have some degree of contrast inmeaning, such as {\rm warm} and {\rm cold} or {\rm tropical} and {\rmfreezing}. We propose an automatic method to identify contrasting word pairsthat is based on the hypothesis that if a pair of words, $A$ and $B$, arecontrasting, then there is a pair of opposites, $C$ and $D$, such that $A$ and$C$ are strongly related and $B$ and $D$ are strongly related. (For example,there exists the pair of opposites {\rm hot} and {\rm cold} such that {\rmtropical} is related to {\rm hot,} and {\rm freezing} is related to {\rmcold}.) We will call this the contrast hypothesis. We begin with a largecrowdsourcing experiment to determine the amount of human agreement on theconcept of oppositeness and its different kinds. In the process, we flesh outkey features of different kinds of opposites. We then present an automatic andempirical measure of lexical contrast that relies on the contrast hypothesis,corpus statistics, and the structure of a {\it Roget}-like thesaurus. We showthat the proposed measure of lexical contrast obtains high precision and largecoverage, outperforming existing methods.
arxiv-3900-260 | Crowdsourcing a Word-Emotion Association Lexicon | http://arxiv.org/pdf/1308.6297v1.pdf | author:Saif M. Mohammad, Peter D. Turney category:cs.CL published:2013-08-28 summary:Even though considerable attention has been given to the polarity of words(positive and negative) and the creation of large polarity lexicons, researchin emotion analysis has had to rely on limited and small emotion lexicons. Inthis paper we show how the combined strength and wisdom of the crowds can beused to generate a large, high-quality, word-emotion and word-polarityassociation lexicon quickly and inexpensively. We enumerate the challenges inemotion annotation in a crowdsourcing scenario and propose solutions to addressthem. Most notably, in addition to questions about emotions associated withterms, we show how the inclusion of a word choice question can discouragemalicious data entry, help identify instances where the annotator may not befamiliar with the target term (allowing us to reject such annotations), andhelp obtain annotations at sense level (rather than at word level). Weconducted experiments on how to formulate the emotion-annotation questions, andshow that asking if a term is associated with an emotion leads to markedlyhigher inter-annotator agreement than that obtained by asking if a term evokesan emotion.
arxiv-3900-261 | NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of Tweets | http://arxiv.org/pdf/1308.6242v1.pdf | author:Saif M. Mohammad, Svetlana Kiritchenko, Xiaodan Zhu category:cs.CL published:2013-08-28 summary:In this paper, we describe how we created two state-of-the-art SVMclassifiers, one to detect the sentiment of messages such as tweets and SMS(message-level task) and one to detect the sentiment of a term within asubmissions stood first in both tasks on tweets, obtaining an F-score of 69.02in the message-level task and 88.93 in the term-level task. We implemented avariety of surface-form, semantic, and sentiment features. with sentiment-wordhashtags, and one from tweets with emoticons. In the message-level task, thelexicon-based features provided a gain of 5 F-score points over all others.Both of our systems can be replicated us available resources.
arxiv-3900-262 | Bayesian Conditional Gaussian Network Classifiers with Applications to Mass Spectra Classification | http://arxiv.org/pdf/1308.6181v1.pdf | author:Victor Bellon, Jesus Cerquides, Ivo Grosse category:cs.LG stat.ML published:2013-08-28 summary:Classifiers based on probabilistic graphical models are very effective. Incontinuous domains, maximum likelihood is usually used to assess thepredictions of those classifiers. When data is scarce, this can easily lead tooverfitting. In any probabilistic setting, Bayesian averaging (BA) providestheoretically optimal predictions and is known to be robust to overfitting. Inthis work we introduce Bayesian Conditional Gaussian Network Classifiers, whichefficiently perform exact Bayesian averaging over the parameters. We evaluatethe proposed classifiers against the maximum likelihood alternatives proposedso far over standard UCI datasets, concluding that performing BA improves thequality of the assessed probabilities (conditional log likelihood) whilstmaintaining the error rate. Overfitting is more likely to occur in domains where the number of data itemsis small and the number of variables is large. These two conditions are met inthe realm of bioinformatics, where the early diagnosis of cancer from massspectra is a relevant task. We provide an application of our classificationframework to that problem, comparing it with the standard maximum likelihoodalternative, where the improvement of quality in the assessed probabilities isconfirmed.
arxiv-3900-263 | Identifiability of Gaussian structural equation models with equal error variances | http://arxiv.org/pdf/1205.2536v3.pdf | author:Jonas Peters, Peter Bühlmann category:stat.ML math.ST stat.TH published:2012-05-11 summary:We consider structural equation models in which variables can be written as afunction of their parents and noise terms, which are assumed to be jointlyindependent. Corresponding to each structural equation model, there is adirected acyclic graph describing the relationships between the variables. InGaussian structural equation models with linear functions, the graph can beidentified from the joint distribution only up to Markov equivalence classes,assuming faithfulness. In this work, we prove full identifiability if all noisevariables have the same variances: the directed acyclic graph can be recoveredfrom the joint Gaussian distribution. Our result has direct implications forcausal inference: if the data follow a Gaussian structural equation model withequal error variances and assuming that all variables are observed, the causalstructure can be inferred from observational data only. We propose astatistical method and an algorithm that exploit our theoretical findings.
arxiv-3900-264 | Preventing Disclosure of Sensitive Knowledge by Hiding Inference | http://arxiv.org/pdf/1308.6744v1.pdf | author:A. S. Syed Navaz, M. Ravi, T. Prabhu category:cs.CR cs.DB cs.LG published:2013-08-28 summary:Data Mining is a way of extracting data or uncovering hidden patterns ofinformation from databases. So, there is a need to prevent the inference rulesfrom being disclosed such that the more secure data sets cannot be identifiedfrom non sensitive attributes. This can be done through removing or addingcertain item sets in the transactions Sanitization. The purpose is to hide theInference rules, so that the user may not be able to discover any valuableinformation from other non sensitive data and any organisation can release allsamples of their data without the fear of Knowledge Discovery In Databaseswhich can be achieved by investigating frequently occurring item sets, rulesthat can be mined from them with the objective of hiding them. Another way isto release only limited samples in the new database so that there is noinformation loss and it also satisfies the legitimate needs of the users. Themajor problem is uncovering hidden patterns, which causes a threat to thedatabase security. Sensitive data are inferred from non-sensitive data based onthe semantics of the application the user has, commonly known as the inferenceproblem. Two fundamental approaches to protect sensitive rules from disclosureare that, preventing rules from being generated by hiding the frequent sets ofdata items and reducing the importance of the rules by setting their confidencebelow a user-specified threshold.
arxiv-3900-265 | Brain MRI Segmentation with Fast and Globally Convex Multiphase Active Contours | http://arxiv.org/pdf/1308.6056v1.pdf | author:Juan C. Moreno, V. B. S. Prasath, Hugo Proenca, K. Palaniappan category:cs.CV 68U10 I.4.6 published:2013-08-28 summary:Multiphase active contour based models are useful in identifying multipleregions with different characteristics such as the mean values of regions. Thisis relevant in brain magnetic resonance images (MRIs), allowing thedifferentiation of white matter against gray matter. We consider a well definedglobally convex formulation of Vese and Chan multiphase active contour modelfor segmenting brain MRI images. A well-established theory and an efficientdual minimization scheme are thoroughly described which guarantees optimalsolutions and provides stable segmentations. Moreover, under the dualminimization implementation our model perfectly describes disjoint regions byavoiding local minima solutions. Experimental results indicate that theproposed approach provides better accuracy than other related multiphase activecontour algorithms even under severe noise, intensity inhomogeneities, andpartial volume effects.
arxiv-3900-266 | On the Doubt about Margin Explanation of Boosting | http://arxiv.org/pdf/1009.3613v5.pdf | author:Wei Gao, Zhi-Hua Zhou category:cs.LG published:2010-09-19 summary:Margin theory provides one of the most popular explanations to the success of\texttt{AdaBoost}, where the central point lies in the recognition that\textit{margin} is the key for characterizing the performance of\texttt{AdaBoost}. This theory has been very influential, e.g., it has beenused to argue that \texttt{AdaBoost} usually does not overfit since it tends toenlarge the margin even after the training error reaches zero. Previously the\textit{minimum margin bound} was established for \texttt{AdaBoost}, however,\cite{Breiman1999} pointed out that maximizing the minimum margin does notnecessarily lead to a better generalization. Later, \cite{Reyzin:Schapire2006}emphasized that the margin distribution rather than minimum margin is crucialto the performance of \texttt{AdaBoost}. In this paper, we first present the\textit{$k$th margin bound} and further study on its relationship to previouswork such as the minimum margin bound and Emargin bound. Then, we improve theprevious empirical Bernstein bounds\citep{Maurer:Pontil2009,Audibert:Munos:Szepesvari2009}, and based on suchfindings, we defend the margin-based explanation against Breiman's doubts byproving a new generalization error bound that considers exactly the samefactors as \cite{Schapire:Freund:Bartlett:Lee1998} but is sharper than\cite{Breiman1999}'s minimum margin bound. By incorporating factors such asaverage margin and variance, we present a generalization error bound that isheavily related to the whole margin distribution. We also provide margindistribution bounds for generalization error of voting classifiers in finiteVC-dimension space.
arxiv-3900-267 | Improving Sparse Associative Memories by Escaping from Bogus Fixed Points | http://arxiv.org/pdf/1308.6003v1.pdf | author:Zhe Yao, Vincent Gripon, Michael Rabbat category:cs.NE cs.IT math.IT published:2013-08-27 summary:The Gripon-Berrou neural network (GBNN) is a recently invented recurrentneural network embracing a LDPC-like sparse encoding setup which makes itextremely resilient to noise and errors. A natural use of GBNN is as anassociative memory. There are two activation rules for the neuron dynamics,namely sum-of-sum and sum-of-max. The latter outperforms the former in terms ofretrieval rate by a huge margin. In prior discussions and experiments, it isbelieved that although sum-of-sum may lead the network to oscillate, sum-of-maxalways converges to an ensemble of neuron cliques corresponding to previouslystored patterns. However, this is not entirely correct. In fact, sum-of-maxoften converges to bogus fixed points where the ensemble only comprises a smallsubset of the converged state. By taking advantage of this overlooked fact, wecan greatly improve the retrieval rate. We discuss this particular issue andpropose a number of heuristics to push sum-of-max beyond these bogus fixedpoints. To tackle the problem directly and completely, a novel post-processingalgorithm is also developed and customized to the structure of GBNN.Experimental results show that the new algorithm achieves a huge performanceboost in terms of both retrieval rate and run-time, compared to the standardsum-of-max and all the other heuristics.
arxiv-3900-268 | Hierarchized block wise image approximation by greedy pursuit strategies | http://arxiv.org/pdf/1308.5876v1.pdf | author:Laura Rebollo-Neira, Ryszard Maciol, Shabnam Bibi category:cs.CV 68U10, 94A08 G.1.2 published:2013-08-27 summary:An approach for effective implementation of greedy selection methodologies,to approximate an image partitioned into blocks, is proposed. The method isspecially designed for approximating partitions on a transformed image. Itevolves by selecting, at each iteration step, i) the elements for approximatingeach of the blocks partitioning the image and ii) the hierarchized sequence inwhich the blocks are approximated to reach the required global condition onsparsity.
arxiv-3900-269 | Backhaul-Aware Interference Management in the Uplink of Wireless Small Cell Networks | http://arxiv.org/pdf/1308.5835v1.pdf | author:Sumudu Samarakoon, Mehdi Bennis, Walid Saad, Matti Latva-aho category:cs.NI cs.GT cs.LG published:2013-08-27 summary:The design of distributed mechanisms for interference management is one ofthe key challenges in emerging wireless small cell networks whose backhaul iscapacity limited and heterogeneous (wired, wireless and a mix thereof). In thispaper, a novel, backhaul-aware approach to interference management in wirelesssmall cell networks is proposed. The proposed approach enables macrocell userequipments (MUEs) to optimize their uplink performance, by exploiting thepresence of neighboring small cell base stations. The problem is formulated asa noncooperative game among the MUEs that seek to optimize their delay-ratetradeoff, given the conditions of both the radio access network and the --possibly heterogeneous -- backhaul. To solve this game, a novel, distributedlearning algorithm is proposed using which the MUEs autonomously choose theiroptimal uplink transmission strategies, given a limited amount of availableinformation. The convergence of the proposed algorithm is shown and itsproperties are studied. Simulation results show that, under various types ofbackhauls, the proposed approach yields significant performance gains, in termsof both average throughput and delay for the MUEs, when compared to existingbenchmark algorithms.
arxiv-3900-270 | Multi-Objective Particle Swarm Optimization for Facility Location Problem in Wireless Mesh Networks | http://arxiv.org/pdf/1308.5807v1.pdf | author:Tarik Mountassir Bou, Abdelkrim Haqiq, Samir Bennani category:cs.NI cs.NE published:2013-08-27 summary:Wireless mesh networks have seen a real progress due of their implementationat a low cost. They present one of Next Generation Networks technologies andcan serve as home, companies and universities networks. In this paper, wepropose and discuss a new multi-objective model for nodes deploymentoptimization in Multi-Radio Multi-Channel Wireless Mesh Networks. We exploitthe trade-off between network cost and the overall network performance. Thisoptimization problem is solved simultaneously by using a meta-heuristic methodthat returns a non-dominated set of near optimal solutions. A comparative studywas driven to evaluate the efficiency of the proposed model.
arxiv-3900-271 | The Generalized Mean Information Coefficient | http://arxiv.org/pdf/1308.5712v1.pdf | author:Alexander Luedtke, Linh Tran category:stat.ML published:2013-08-26 summary:Reshef & Reshef recently published a paper in which they present a methodcalled the Maximal Information Coefficient (MIC) that can detect all forms ofstatistical dependence between pairs of variables as sample size goes toinfinity. While this method has been praised by some, it has also beencriticized for its lack of power in finite samples. We seek to modify MIC sothat it has higher power in detecting associations for limited sample sizes.Here we present the Generalized Mean Information Coefficient (GMIC), ageneralization of MIC which incorporates a tuning parameter that can be used tomodify the complexity of the association favored by the measure. We define GMICand prove it maintains several key asymptotic properties of MIC. Its increasedpower over MIC is demonstrated using a simulation of eight different functionalrelationships at sixty different noise levels. The results are compared to thePearson correlation, distance correlation, and MIC. Simulation results suggestthat while generally GMIC has slightly lower power than the distancecorrelation measure, it achieves higher power than MIC for many forms ofunderlying association. For some functional relationships, GMIC surpasses allother statistics calculated. Preliminary results suggest choosing a moderatevalue of the tuning parameter for GMIC will yield a test that is robust acrossunderlying relationships. GMIC is a promising new method that mitigates thepower issues suffered by MIC, at the possible expense of equitability.Nonetheless, distance correlation was in our simulations more powerful for manyforms of underlying relationships. At a minimum, this work motivates furtherconsideration of maximal information-based nonparametric exploration (MINE)methods as statistical tests of independence.
arxiv-3900-272 | Detection of copy-move forgery in digital images based on DCT | http://arxiv.org/pdf/1308.5661v1.pdf | author:Nathalie Diane Wandji, Sun Xingming, Moise Fah Kue category:cs.CV cs.CR published:2013-08-26 summary:With rapid advances in digital information processing systems, and morespecifically in digital image processing software, there is a widespreaddevelopment of advanced tools and techniques for digital image forgery. One ofthe techniques most commonly used is the Copy-move forgery which proceeds bycopying a part of an image and pasting it into the same image, in order tomaliciously hide an object or a region. In this paper, we propose a method todetect this specific kind of counterfeit. Firstly, the color image is convertedfrom RGB color space to YCbCr color space and then the R, G, B and Y-componentare splitted into fixed-size overlapping blocks and, features are extractedfrom the R, G and B-components image blocks on one hand and on the other, fromthe DCT representation of the R, G, B and Ycomponent image block. The featurevectors obtained are then lexicographically sorted to make similar image blocksneighbors and duplicated image blocks are identified using Euclidean distanceas similarity criterion. Experimental results showed that the proposed methodcan detect the duplicated regions when there is more than one copy move forgedarea in the image and even in case of slight rotations, JPEG compression,shift, scale, blur and noise addition.
arxiv-3900-273 | A Comparison of Algorithms for Learning Hidden Variables in Normal Graphs | http://arxiv.org/pdf/1308.5576v1.pdf | author:Francesco A. N. Palmieri category:stat.ML cs.IT cs.SY math.IT published:2013-08-26 summary:A Bayesian factor graph reduced to normal form consists in theinterconnection of diverter units (or equal constraint units) andSingle-Input/Single-Output (SISO) blocks. In this framework localizedadaptation rules are explicitly derived from a constrained maximum likelihood(ML) formulation and from a minimum KL-divergence criterion using KKTconditions. The learning algorithms are compared with two other updatingequations based on a Viterbi-like and on a variational approximationrespectively. The performance of the various algorithm is verified on syntheticdata sets for various architectures. The objective of this paper is to providethe programmer with explicit algorithms for rapid deployment of Bayesian graphsin the applications.
arxiv-3900-274 | Sparse and Non-Negative BSS for Noisy Data | http://arxiv.org/pdf/1308.5546v1.pdf | author:Jérémy Rapin, Jérôme Bobin, Anthony Larue, Jean-Luc Starck category:stat.ML cs.LG 94A12 I.5.4 published:2013-08-26 summary:Non-negative blind source separation (BSS) has raised interest in variousfields of research, as testified by the wide literature on the topic ofnon-negative matrix factorization (NMF). In this context, it is fundamentalthat the sources to be estimated present some diversity in order to beefficiently retrieved. Sparsity is known to enhance such contrast between thesources while producing very robust approaches, especially to noise. In thispaper we introduce a new algorithm in order to tackle the blind separation ofnon-negative sparse sources from noisy measurements. We first show thatsparsity and non-negativity constraints have to be carefully applied on thesought-after solution. In fact, improperly constrained solutions are unlikelyto be stable and are therefore sub-optimal. The proposed algorithm, named nGMCA(non-negative Generalized Morphological Component Analysis), makes use ofproximal calculus techniques to provide properly constrained solutions. Theperformance of nGMCA compared to other state-of-the-art algorithms isdemonstrated by numerical experiments encompassing a wide variety of settings,with negligible parameter tuning. In particular, nGMCA is shown to providerobustness to noise and performs well on synthetic mixtures of real NMRspectra.
arxiv-3900-275 | Linear models and linear mixed effects models in R with linguistic applications | http://arxiv.org/pdf/1308.5499v1.pdf | author:Bodo Winter category:cs.CL published:2013-08-26 summary:This text is a conceptual introduction to mixed effects modeling withlinguistic applications, using the R programming environment. The reader isintroduced to linear modeling and assumptions, as well as to mixedeffects/multilevel modeling, including a discussion of random intercepts,random slopes and likelihood ratio tests. The example used throughout the textfocuses on the phonetic analysis of voice pitch data.
arxiv-3900-276 | Fast learning rate of multiple kernel learning: Trade-off between sparsity and smoothness | http://arxiv.org/pdf/1203.0565v2.pdf | author:Taiji Suzuki, Masashi Sugiyama category:stat.ML math.ST stat.TH published:2012-03-02 summary:We investigate the learning rate of multiple kernel learning (MKL) with$\ell_1$ and elastic-net regularizations. The elastic-net regularization is acomposition of an $\ell_1$-regularizer for inducing the sparsity and an$\ell_2$-regularizer for controlling the smoothness. We focus on a sparsesetting where the total number of kernels is large, but the number of nonzerocomponents of the ground truth is relatively small, and show sharperconvergence rates than the learning rates have ever shown for both $\ell_1$ andelastic-net regularizations. Our analysis reveals some relations between thechoice of a regularization function and the performance. If the ground truth issmooth, we show a faster convergence rate for the elastic-net regularizationwith less conditions than $\ell_1$-regularization; otherwise, a fasterconvergence rate for the $\ell_1$-regularization is shown.
arxiv-3900-277 | Stability of Phase Retrievable Frames | http://arxiv.org/pdf/1308.5465v1.pdf | author:Radu Balan category:math.FA cs.CV stat.ML published:2013-08-25 summary:In this paper we study the property of phase retrievability by redundantsysems of vectors under perturbations of the frame set. Specifically we showthat if a set $\fc$ of $m$ vectors in the complex Hilbert space of dimension nallows for vector reconstruction from magnitudes of its coefficients, thenthere is a perturbation bound $\rho$ so that any frame set within $\rho$ from$\fc$ has the same property. In particular this proves the recent constructionin \cite{BH13} is stable under perturbations. By the same token we reduce thecritical cardinality conjectured in \cite{BCMN13a} to proving a stabilityresult for non phase-retrievable frames.
arxiv-3900-278 | A Max-Product EM Algorithm for Reconstructing Markov-tree Sparse Signals from Compressive Samples | http://arxiv.org/pdf/1209.1064v4.pdf | author:Zhao Song, Aleksandar Dogandzic category:stat.ML cs.IT math.IT published:2012-09-05 summary:We propose a Bayesian expectation-maximization (EM) algorithm forreconstructing Markov-tree sparse signals via belief propagation. Themeasurements follow an underdetermined linear model where theregression-coefficient vector is the sum of an unknown approximately sparsesignal and a zero-mean white Gaussian noise with an unknown variance. Thesignal is composed of large- and small-magnitude components identified bybinary state variables whose probabilistic dependence structure is described bya Markov tree. Gaussian priors are assigned to the signal coefficients giventheir state variables and the Jeffreys' noninformative prior is assigned to thenoise variance. Our signal reconstruction scheme is based on an EM iterationthat aims at maximizing the posterior distribution of the signal and its statevariables given the noise variance. We construct the missing data for the EMiteration so that the complete-data posterior distribution corresponds to ahidden Markov tree (HMT) probabilistic graphical model that contains no loopsand implement its maximization (M) step via a max-product algorithm. This EMalgorithm estimates the vector of state variables as well as solves iterativelya linear system of equations to obtain the corresponding signal estimate. Weselect the noise variance so that the corresponding estimated signal and statevariables obtained upon convergence of the EM iteration have the largestmarginal posterior distribution. We compare the proposed and existingstate-of-the-art reconstruction methods via signal and image reconstructionexperiments.
arxiv-3900-279 | On the Achievability of Cramér-Rao Bound In Noisy Compressed Sensing | http://arxiv.org/pdf/1006.2513v3.pdf | author:Rad Niazadeh, Masoud Babaie-Zadeh, Christian Jutten category:cs.IT cs.LG math.IT published:2010-06-13 summary:Recently, it has been proved in Babadi et al. that in noisy compressedsensing, a joint typical estimator can asymptotically achieve the Cramer-Raolower bound of the problem.To prove this result, this paper used a lemma,whichis provided in Akcakaya et al,that comprises the main building block of theproof. This lemma is based on the assumption of Gaussianity of the measurementmatrix and its randomness in the domain of noise. In this correspondence, wegeneralize the results obtained in Babadi et al by dropping the Gaussianityassumption on the measurement matrix. In fact, by considering the measurementmatrix as a deterministic matrix in our analysis, we find a theorem similar tothe main theorem of Babadi et al for a family of randomly generated (butdeterministic in the noise domain) measurement matrices that satisfy ageneralized condition known as The Concentration of Measures Inequality. Bythis, we finally show that under our generalized assumptions, the Cramer-Raobound of the estimation is achievable by using the typical estimator introducedin Babadi et al.
arxiv-3900-280 | A Literature Review: Stemming Algorithms for Indian Languages | http://arxiv.org/pdf/1308.5423v1.pdf | author:M. Thangarasu, R. Manavalan category:cs.CL published:2013-08-25 summary:Stemming is the process of extracting root word from the given inflectionword. It also plays significant role in numerous application of NaturalLanguage Processing (NLP). The stemming problem has addressed in many contextsand by researchers in many disciplines. This expository paper presents surveyof some of the latest developments on stemming algorithms in data mining andalso presents with some of the solutions for various Indian language stemmingalgorithms along with the results.
arxiv-3900-281 | Decentralized Online Big Data Classification - a Bandit Framework | http://arxiv.org/pdf/1308.4565v2.pdf | author:Cem Tekin, Mihaela van der Schaar category:cs.LG cs.MA published:2013-08-21 summary:Distributed, online data mining systems have emerged as a result ofapplications requiring analysis of large amounts of correlated andhigh-dimensional data produced by multiple distributed data sources. We proposea distributed online data classification framework where data is gathered bydistributed data sources and processed by a heterogeneous set of distributedlearners which learn online, at run-time, how to classify the different datastreams either by using their locally available classification functions or byhelping each other by classifying each other's data. Importantly, since thedata is gathered at different locations, sending the data to another learner toprocess incurs additional costs such as delays, and hence this will be onlybeneficial if the benefits obtained from a better classification will exceedthe costs. We assume that the classification functions available to eachprocessing element are fixed, but their prediction accuracy for various typesof incoming data are unknown and can change dynamically over time, and thusthey need to be learned online. We model the problem of joint classification bythe distributed and heterogeneous learners from multiple data sources as adistributed contextual bandit problem where each data is characterized by aspecific context. We develop distributed online learning algorithms for whichwe can prove that they have sublinear regret. Compared to prior work indistributed online data mining, our work is the first to provide analyticregret results characterizing the performance of the proposed algorithms.
arxiv-3900-282 | A stochastic hybrid model of a biological filter | http://arxiv.org/pdf/1308.5338v1.pdf | author:Andrea Ocone, Guido Sanguinetti category:cs.LG cs.CE q-bio.MN published:2013-08-24 summary:We present a hybrid model of a biological filter, a genetic circuit whichremoves fast fluctuations in the cell's internal representation of the extracellular environment. The model takes the classic feed-forward loop (FFL) motifand represents it as a network of continuous protein concentrations and binary,unobserved gene promoter states. We address the problem of statisticalinference and parameter learning for this class of models from partial,discrete time observations. We show that the hybrid representation leads to anefficient algorithm for approximate statistical inference in this circuit, andshow its effectiveness on a simulated data set.
arxiv-3900-283 | Monitoring with uncertainty | http://arxiv.org/pdf/1308.5329v1.pdf | author:Ezio Bartocci, Radu Grosu category:cs.LO cs.LG cs.SY published:2013-08-24 summary:We discuss the problem of runtime verification of an instrumented programthat misses to emit and to monitor some events. These gaps can occur when amonitoring overhead control mechanism is introduced to disable the monitor ofan application with real-time constraints. We show how to use statisticalmodels to learn the application behavior and to "fill in" the introduced gaps.Finally, we present and discuss some techniques developed in the last threeyears to estimate the probability that a property of interest is violated inthe presence of an incomplete trace.
arxiv-3900-284 | Edge-detection applied to moving sand dunes on Mars | http://arxiv.org/pdf/1308.5315v1.pdf | author:Amelia Carolina Sparavigna category:cs.CV published:2013-08-24 summary:Here we discuss the application of an edge detection filter, the Sobel filterof GIMP, to the recently discovered motion of some sand dunes on Mars. Thefilter allows a good comparison of an image HiRISE of 2007 and an image of 1999recorded by the Mars Global Surveyor of the dunes in the Nili Patera caldera,measuring therefore the motion of the dunes on a longer period of time thanthat previously investigated.
arxiv-3900-285 | Algorithms for Approximate Minimization of the Difference Between Submodular Functions, with Applications | http://arxiv.org/pdf/1207.0560v4.pdf | author:Rishabh Iyer, Jeff Bilmes category:cs.DS cs.LG published:2012-07-03 summary:We extend the work of Narasimhan and Bilmes [30] for minimizing set functionsrepresentable as a difference between submodular functions. Similar to [30],our new algorithms are guaranteed to monotonically reduce the objectivefunction at every step. We empirically and theoretically show that theper-iteration cost of our algorithms is much less than [30], and our algorithmscan be used to efficiently minimize a difference between submodular functionsunder various combinatorial constraints, a problem not previously addressed. Weprovide computational bounds and a hardness result on the mul- tiplicativeinapproximability of minimizing the difference between submodular functions. Weshow, however, that it is possible to give worst-case additive bounds byproviding a polynomial time computable lower-bound on the minima. Finally weshow how a number of machine learning problems can be modeled as minimizing thedifference between submodular functions. We experimentally show the validity ofour algorithms by testing them on the problem of feature selection withsubmodular cost features.
arxiv-3900-286 | Ensemble of Distributed Learners for Online Classification of Dynamic Data Streams | http://arxiv.org/pdf/1308.5281v1.pdf | author:Luca Canzian, Yu Zhang, Mihaela van der Schaar category:cs.LG published:2013-08-24 summary:We present an efficient distributed online learning scheme to classify datacaptured from distributed, heterogeneous, and dynamic data sources. Our schemeconsists of multiple distributed local learners, that analyze different streamsof data that are correlated to a common event that needs to be classified. Eachlearner uses a local classifier to make a local prediction. The localpredictions are then collected by each learner and combined using a weightedmajority rule to output the final prediction. We propose a novel onlineensemble learning algorithm to update the aggregation rule in order to adapt tothe underlying data dynamics. We rigorously determine a bound for the worstcase misclassification probability of our algorithm which depends on themisclassification probabilities of the best static aggregation rule, and of thebest local classifier. Importantly, the worst case misclassificationprobability of our algorithm tends asymptotically to 0 if the misclassificationprobability of the best static aggregation rule or the misclassificationprobability of the best local classifier tend to 0. Then we extend ouralgorithm to address challenges specific to the distributed implementation andwe prove new bounds that apply to these settings. Finally, we test our schemeby performing an evaluation study on several data sets. When applied to datasets widely used by the literature dealing with dynamic data streams andconcept drift, our scheme exhibits performance gains ranging from 34% to 71%with respect to state of the art solutions.
arxiv-3900-287 | The Lovasz-Bregman Divergence and connections to rank aggregation, clustering, and web ranking | http://arxiv.org/pdf/1308.5275v1.pdf | author:Rishabh Iyer, Jeff Bilmes category:cs.LG cs.IR stat.ML published:2013-08-24 summary:We extend the recently introduced theory of Lovasz-Bregman (LB) divergences(Iyer & Bilmes, 2012) in several ways. We show that they represent a distortionbetween a 'score' and an 'ordering', thus providing a new view of rankaggregation and order based clustering with interesting connections to webranking. We show how the LB divergences have a number of properties akin tomany permutation based metrics, and in fact have as special cases forms verysimilar to the Kendall-$\tau$ metric. We also show how the LB divergencessubsume a number of commonly used ranking measures in information retrieval,like the NDCG and AUC. Unlike the traditional permutation based metrics,however, the LB divergence naturally captures a notion of "confidence" in theorderings, thus providing a new representation to applications involvingaggregating scores as opposed to just orderings. We show how a number ofrecently used web ranking models are forms of Lovasz-Bregman rank aggregationand also observe that a natural form of Mallow's model using the LB divergencehas been used as conditional ranking models for the 'Learning to Rank' problem.
arxiv-3900-288 | Understanding the Predictive Power of Computational Mechanics and Echo State Networks in Social Media | http://arxiv.org/pdf/1306.6111v2.pdf | author:David Darmon, Jared Sylvester, Michelle Girvan, William Rand category:cs.SI cs.LG physics.soc-ph stat.AP stat.ML published:2013-06-26 summary:There is a large amount of interest in understanding users of social media inorder to predict their behavior in this space. Despite this interest, userpredictability in social media is not well-understood. To examine thisquestion, we consider a network of fifteen thousand users on Twitter over aseven week period. We apply two contrasting modeling paradigms: computationalmechanics and echo state networks. Both methods attempt to model the behaviorof users on the basis of their past behavior. We demonstrate that the behaviorof users on Twitter can be well-modeled as processes with self-feedback. Wefind that the two modeling approaches perform very similarly for most users,but that they differ in performance on a small subset of the users. Byexploring the properties of these performance-differentiated users, wehighlight the challenges faced in applying predictive models to dynamic socialdata.
arxiv-3900-289 | Geodesic-based Salient Object Detection | http://arxiv.org/pdf/1302.6557v2.pdf | author:Richard M Jiang category:cs.CV cs.AI published:2013-02-26 summary:Saliency detection has been an intuitive way to provide useful cues forobject detection and segmentation, as desired for many vision and graphicsapplications. In this paper, we provided a robust method for salient objectdetection and segmentation. Other than using various pixel-level contrastdefinitions, we exploited global image structures and proposed a new geodesicmethod dedicated for salient object detection. In the proposed approach, a newgeodesic scheme, namely geodesic tunneling is proposed to tackle with texturesand local chaotic structures. With our new geodesic approach, a geodesicsaliency map is estimated in correspondence to spatial structures in an image.Experimental evaluation on a salient object benchmark dataset validated thatour algorithm consistently outperformed a number of the state-of-art saliencymethods, yielding higher precision and better recall rates. With the robustsaliency estimation, we also present an unsupervised hierarchical salientobject cut scheme simply using adaptive saliency thresholding, which attainedthe highest score in our F-measure test. We also applied our geodesic cutscheme to a number of image editing tasks as demonstrated in additionalexperiments.
arxiv-3900-290 | Manopt, a Matlab toolbox for optimization on manifolds | http://arxiv.org/pdf/1308.5200v1.pdf | author:Nicolas Boumal, Bamdev Mishra, P. -A. Absil, Rodolphe Sepulchre category:cs.MS cs.LG math.OC stat.ML published:2013-08-23 summary:Optimization on manifolds is a rapidly developing branch of nonlinearoptimization. Its focus is on problems where the smooth geometry of the searchspace can be leveraged to design efficient numerical algorithms. In particular,optimization on manifolds is well-suited to deal with rank and orthogonalityconstraints. Such structured constraints appear pervasively in machine learningapplications, including low-rank matrix completion, sensor networklocalization, camera network registration, independent component analysis,metric learning, dimensionality reduction and so on. The Manopt toolbox,available at www.manopt.org, is a user-friendly, documented piece of softwarededicated to simplify experimenting with state of the art Riemannianoptimization algorithms. We aim particularly at reaching practitioners outsideour field.
arxiv-3900-291 | Spectral redemption: clustering sparse networks | http://arxiv.org/pdf/1306.5550v2.pdf | author:Florent Krzakala, Cristopher Moore, Elchanan Mossel, Joe Neeman, Allan Sly, Lenka Zdeborová, Pan Zhang category:cs.SI physics.soc-ph stat.ML published:2013-06-24 summary:Spectral algorithms are classic approaches to clustering and communitydetection in networks. However, for sparse networks the standard versions ofthese algorithms are suboptimal, in some cases completely failing to detectcommunities even when other algorithms such as belief propagation can do so.Here we introduce a new class of spectral algorithms based on anon-backtracking walk on the directed edges of the graph. The spectrum of thisoperator is much better-behaved than that of the adjacency matrix or othercommonly used matrices, maintaining a strong separation between the bulkeigenvalues and the eigenvalues relevant to community structure even in thesparse case. We show that our algorithm is optimal for graphs generated by thestochastic block model, detecting communities all the way down to thetheoretical limit. We also show the spectrum of the non-backtracking operatorfor some real-world networks, illustrating its advantages over traditionalspectral clustering.
arxiv-3900-292 | Group descent algorithms for nonconvex penalized linear and logistic regression models with grouped predictors | http://arxiv.org/pdf/1209.2160v2.pdf | author:Patrick Breheny, Jian Huang category:stat.CO stat.ML published:2012-09-10 summary:Penalized regression is an attractive framework for variable selectionproblems. Often, variables possess a grouping structure, and the relevantselection problem is that of selecting groups, not individual variables. Thegroup lasso has been proposed as a way of extending the ideas of the lasso tothe problem of group selection. Nonconvex penalties such as SCAD and MCP havebeen proposed and shown to have several advantages over the lasso; thesepenalties may also be extended to the group selection problem, giving rise togroup SCAD and group MCP methods. Here, we describe algorithms for fittingthese models stably and efficiently. In addition, we present simulation resultsand real data examples comparing and contrasting the statistical properties ofthese methods.
arxiv-3900-293 | Artificial Immune Systems (INTROS 2) | http://arxiv.org/pdf/1308.5138v1.pdf | author:Uwe Aickelin, Dipankar Dasgupta, Feng Gu category:cs.NE cs.ET published:2013-08-23 summary:The biological immune system is a robust, complex, adaptive system thatdefends the body from foreign pathogens. It is able to categorize all cells (ormolecules) within the body as self or non-self substances. It does this withthe help of a distributed task force that has the intelligence to take actionfrom a local and also a global perspective using its network of chemicalmessengers for communication. There are two major branches of the immunesystem. The innate immune system is an unchanging mechanism that detects anddestroys certain invading organisms, whilst the adaptive immune system respondsto previously unknown foreign cells and builds a response to them that canremain in the body over a long period of time. This remarkable informationprocessing biological system has caught the attention of computer science inrecent years. A novel computational intelligence technique, inspired by immunology, hasemerged, called Artificial Immune Systems. Several concepts from the immunesystem have been extracted and applied for solution to real world science andengineering problems. In this tutorial, we briefly describe the immune systemmetaphors that are relevant to existing Artificial Immune Systems methods. Wewill then show illustrative real-world problems suitable for Artificial ImmuneSystems and give a step-by-step algorithm walkthrough for one such problem. Acomparison of the Artificial Immune Systems to other well-known algorithms,areas for future work, tips & tricks and a list of resources will round thistutorial off. It should be noted that as Artificial Immune Systems is still ayoung and evolving field, there is not yet a fixed algorithm template and henceactual implementations might differ somewhat from time to time and from thoseexamples given here.
arxiv-3900-294 | Coupled Neural Associative Memories | http://arxiv.org/pdf/1301.1555v5.pdf | author:Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi category:cs.NE cs.IT cs.LG math.IT published:2013-01-08 summary:We propose a novel architecture to design a neural associative memory that iscapable of learning a large number of patterns and recalling them later inpresence of noise. It is based on dividing the neurons into local clusters andparallel plains, very similar to the architecture of the visual cortex ofmacaque brain. The common features of our proposed architecture with those ofspatially-coupled codes enable us to show that the performance of such networksin eliminating noise is drastically better than the previous approaches whilemaintaining the ability of learning an exponentially large number of patterns.Previous work either failed in providing good performance during the recallphase or in offering large pattern retrieval (storage) capacities. We alsopresent computational experiments that lend additional support to thetheoretical analysis.
arxiv-3900-295 | Performance Measurement Under Increasing Environmental Uncertainty In The Context of Interval Type-2 Fuzzy Logic Based Robotic Sailing | http://arxiv.org/pdf/1308.5133v1.pdf | author:Naisan Benatar, Uwe Aickelin, Jonathan M. Garibald category:cs.RO cs.NE cs.SY published:2013-08-23 summary:Performance measurement of robotic controllers based on fuzzy logic,operating under uncertainty, is a subject area which has been somewhat ignoredin the current literature. In this paper standard measures such as RMSE areshown to be inappropriate for use under conditions where the environmentaluncertainty changes significantly between experiments. An overview of currentmethods which have been applied by other authors is presented, followed by adesign of a more sophisticated method of comparison. This method is thenapplied to a robotic control problem to observe its outcome compared with asingle measure. Results show that the technique described provides a morerobust method of performance comparison than less complex methods allowingbetter comparisons to be drawn.
arxiv-3900-296 | Complexity of evolutionary equilibria in static fitness landscapes | http://arxiv.org/pdf/1308.5094v1.pdf | author:Artem Kaznatcheev category:q-bio.PE cs.NE F.2.2; J.3 published:2013-08-23 summary:A fitness landscape is a genetic space -- with two genotypes adjacent if theydiffer in a single locus -- and a fitness function. Evolutionary dynamicsproduce a flow on this landscape from lower fitness to higher; reachingequilibrium only if a local fitness peak is found. I use computationalcomplexity to question the common assumption that evolution on static fitnesslandscapes can quickly reach a local fitness peak. I do this by showing thatthe popular NK model of rugged fitness landscapes is PLS-complete for K >= 2;the reduction from Weighted 2SAT is a bijection on adaptive walks, so there areNK fitness landscapes where every adaptive path from some vertices is ofexponential length. Alternatively -- under the standard complexity theoreticassumption that there are problems in PLS not solvable in polynomial time --this means that there are no evolutionary dynamics (known, or to be discovered,and not necessarily following adaptive paths) that can converge to a localfitness peak on all NK landscapes with K = 2. Applying results from theanalysis of simplex algorithms, I show that there exist single-peakedlandscapes with no reciprocal sign epistasis where the expected length of anadaptive path following strong selection weak mutation dynamics is$e^{O(n^{1/3})}$ even though an adaptive path to the optimum of length lessthan n is available from every vertex. The technical results are written to beaccessible to mathematical biologists without a computer science background,and the biological literature is summarized for the convenience ofnon-biologists with the aim to open a constructive dialogue between the twodisciplines.
arxiv-3900-297 | Suspicious Object Recognition Method in Video Stream Based on Visual Attention | http://arxiv.org/pdf/1308.5063v1.pdf | author:Panqu Wang, Yan Zhang category:cs.CV published:2013-08-23 summary:We propose a state of the art method for intelligent object recognition andvideo surveillance based on human visual attention. Bottom up and top downattention are applied respectively in the process of acquiring interestedobject(saliency map) and object recognition. The revision of 4 channel PFTmethod is proposed for bottom up attention and enhances the speed and accuracy.Inhibit of return (IOR) is applied in judging the sequence of saliency objectpop out. Euclidean distance of color distribution, object center coordinatesand speed are considered in judging whether the target is match and suspicious.The extensive tests on videos and images show that our method in video analysishas high accuracy and fast speed compared with traditional method. The methodcan be applied into many fields such as video surveillance and security.
arxiv-3900-298 | Likelihood Adaptively Modified Penalties | http://arxiv.org/pdf/1308.5036v1.pdf | author:Yang Feng, Tengfei Li, Zhiliang Ying category:stat.ME math.ST stat.ML stat.TH published:2013-08-23 summary:A new family of penalty functions, adaptive to likelihood, is introduced formodel selection in general regression models. It arises naturally throughassuming certain types of prior distribution on the regression parameters. Tostudy stability properties of the penalized maximum likelihood estimator, twotypes of asymptotic stability are defined. Theoretical properties, includingthe parameter estimation consistency, model selection consistency, andasymptotic stability, are established under suitable regularity conditions. Anefficient coordinate-descent algorithm is proposed. Simulation results and realdata analysis show that the proposed method has competitive performance incomparison with existing ones.
arxiv-3900-299 | A hybrid evolutionary algorithm with importance sampling for multi-dimensional optimization | http://arxiv.org/pdf/1308.5033v1.pdf | author:Guanghui Huang, Zhifeng Pan category:cs.NE published:2013-08-23 summary:A hybrid evolutionary algorithm with importance sampling method is proposedfor multi-dimensional optimization problems in this paper. In order to make useof the information provided in the search process, a set of visited solutionsis selected to give scores for intervals in each dimension, and they areupdated as algorithm proceeds. Those intervals with higher scores are regardedas good intervals, which are used to estimate the joint distribution of optimalsolutions through an interaction between the pool of good genetics, which arethe individuals with smaller fitness values. And the sampling probabilities forgood genetics are determined through an interaction between those estimatedgood intervals. It is a cross validation mechanism which determines thesampling probabilities for good intervals and genetics, and the resultedprobabilities are used to design crossover, mutation and other stochasticoperators with importance sampling method. As the selection of genetics andintervals is not directly dependent on the values of fitness, the resultedoffsprings may avoid the trap of local optima. And a purely random EA is alsocombined into the proposed algorithm to maintain the diversity of population.30 benchmark test functions are used to evaluate the performance of theproposed algorithm, and it is found that the proposed hybrid algorithm is anefficient algorithm for multi-dimensional optimization problems considered inthis paper.
arxiv-3900-300 | How Did Humans Become So Creative? A Computational Approach | http://arxiv.org/pdf/1308.5032v1.pdf | author:Liane Gabora, Steve DiPaola category:cs.NE cs.AI cs.MA q-bio.NC published:2013-08-23 summary:This paper summarizes efforts to computationally model two transitions in theevolution of human creativity: its origins about two million years ago, and the'big bang' of creativity about 50,000 years ago. Using a computational model ofcultural evolution in which neural network based agents evolve ideas foractions through invention and imitation, we tested the hypothesis that humancreativity began with onset of the capacity for recursive recall. We comparedruns in which agents were limited to single-step actions to runs in which theyused recursive recall to chain simple actions into complex ones. Chainingresulted in higher diversity, open-ended novelty, no ceiling on the meanfitness of actions, and greater ability to make use of learning. Using acomputational model of portrait painting, we tested the hypothesis that theexplosion of creativity in the Middle/Upper Paleolithic was due to onset ofcon-textual focus: the capacity to shift between associative and analyticthought. This resulted in faster convergence on portraits that resembled thesitter, employed painterly techniques, and were rated as preferable. Weconclude that recursive recall and contextual focus provide a computationallyplausible explanation of how humans evolved the means to transform this planet.
