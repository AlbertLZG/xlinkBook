arxiv-3900-1 | An Efficient Model Selection for Gaussian Mixture Model in a Bayesian Framework | http://arxiv.org/abs/1307.0995 | author:Ji Won Yoon category:cs.LG stat.ML published:2013-07-03 summary:In order to cluster or partition data, we often useExpectation-and-Maximization (EM) or Variational approximation with a GaussianMixture Model (GMM), which is a parametric probability density functionrepresented as a weighted sum of $\hat{K}$ Gaussian component densities.However, model selection to find underlying $\hat{K}$ is one of the keyconcerns in GMM clustering, since we can obtain the desired clusters only when$\hat{K}$ is known. In this paper, we propose a new model selection algorithmto explore $\hat{K}$ in a Bayesian framework. The proposed algorithm builds thedensity of the model order which any information criterions such as AIC and BICbasically fail to reconstruct. In addition, this algorithm reconstructs thedensity quickly as compared to the time-consuming Monte Carlo simulation.
arxiv-3900-2 | A Comparison of Non-stationary, Type-2 and Dual Surface Fuzzy Control | http://arxiv.org/abs/1307.1070 | author:Naisan Benatar, Uwe Aickelin, Jonathan M. Garibaldi category:cs.AI cs.NE published:2013-07-03 summary:Type-1 fuzzy logic has frequently been used in control systems. However thismethod is sometimes shown to be too restrictive and unable to adapt in thepresence of uncertainty. In this paper we compare type-1 fuzzy control withseveral other fuzzy approaches under a range of uncertain conditions. Intervaltype-2 and non-stationary fuzzy controllers are compared, along with 'dualsurface' type-2 control, named due to utilising both the lower and upper valuesproduced from standard interval type-2 systems. We tune a type-1 controller,then derive the membership functions and footprints of uncertainty from thetype-1 system and evaluate them using a simulated autonomous sailing problemwith varying amounts of environmental uncertainty. We show that while thesemore sophisticated controllers can produce better performance than the type-1controller, this is not guaranteed and that selection of Footprint ofUncertainty (FOU) size has a large effect on this relative performance.
arxiv-3900-3 | Investigating the Detection of Adverse Drug Events in a UK General Practice Electronic Health-Care Database | http://arxiv.org/abs/1307.1078 | author:Jenna Reps, Jan Feyereisl, Jonathan M. Garibaldi, Uwe Aickelin, Jack E. Gibson, Richard B. Hubbard category:cs.CE cs.LG published:2013-07-03 summary:Data-mining techniques have frequently been developed for Spontaneousreporting databases. These techniques aim to find adverse drug eventsaccurately and efficiently. Spontaneous reporting databases are prone tomissing information, under reporting and incorrect entries. This often resultsin a detection lag or prevents the detection of some adverse drug events. Theselimitations do not occur in electronic health-care databases. In this paper,existing methods developed for spontaneous reporting databases are implementedon both a spontaneous reporting database and a general practice electronichealth-care database and compared. The results suggests that the application ofexisting methods to the general practice database may help find signals thathave gone undetected when using the spontaneous reporting system database. Inaddition the general practice database provides far more supplementaryinformation, that if incorporated in analysis could provide a wealth ofinformation for identifying adverse events more accurately.
arxiv-3900-4 | Application of a clustering framework to UK domestic electricity data | http://arxiv.org/abs/1307.1079 | author:Ian Dent, Uwe Aickelin, Tom Rodden category:cs.CE cs.LG published:2013-07-03 summary:This paper takes an approach to clustering domestic electricity load profilesthat has been successfully used with data from Portugal and applies it to UKdata. Clustering techniques are applied and it is found that the preferredtechnique in the Portuguese work (a two stage process combining Self OrganisedMaps and Kmeans) is not appropriate for the UK data. The work shows that up tonine clusters of households can be identified with the differences in usageprofiles being visually striking. This demonstrates the appropriateness ofbreaking the electricity usage patterns down to more detail than the two loadprofiles currently published by the electricity industry. The paper detailsinitial results using data collected in Milton Keynes around 1990. Further workis described and will concentrate on building accurate and meaningful clustersof similar electricity users in order to better direct demand side managementinitiatives to the most relevant target customers.
arxiv-3900-5 | Distributed Online Big Data Classification Using Context Information | http://arxiv.org/abs/1307.0781 | author:Cem Tekin, Mihaela van der Schaar category:cs.LG stat.ML published:2013-07-02 summary:Distributed, online data mining systems have emerged as a result ofapplications requiring analysis of large amounts of correlated andhigh-dimensional data produced by multiple distributed data sources. We proposea distributed online data classification framework where data is gathered bydistributed data sources and processed by a heterogeneous set of distributedlearners which learn online, at run-time, how to classify the different datastreams either by using their locally available classification functions or byhelping each other by classifying each other's data. Importantly, since thedata is gathered at different locations, sending the data to another learner toprocess incurs additional costs such as delays, and hence this will be onlybeneficial if the benefits obtained from a better classification will exceedthe costs. We model the problem of joint classification by the distributed andheterogeneous learners from multiple data sources as a distributed contextualbandit problem where each data is characterized by a specific context. Wedevelop a distributed online learning algorithm for which we can provesublinear regret. Compared to prior work in distributed online data mining, ourwork is the first to provide analytic regret results characterizing theperformance of the proposed algorithm.
arxiv-3900-6 | Regularized Spherical Polar Fourier Diffusion MRI with Optimal Dictionary Learning | http://arxiv.org/abs/1307.0776 | author:Jian Cheng, Tianzi Jiang, Rachid Deriche, Dinggang Shen, Pew-Thian Yap category:cs.CV published:2013-07-02 summary:Compressed Sensing (CS) takes advantage of signal sparsity or compressibilityand allows superb signal reconstruction from relatively few measurements. Basedon CS theory, a suitable dictionary for sparse representation of the signal isrequired. In diffusion MRI (dMRI), CS methods were proposed to reconstructdiffusion-weighted signal and the Ensemble Average Propagator (EAP), and thereare two kinds of Dictionary Learning (DL) methods: 1) Discrete RepresentationDL (DR-DL), and 2) Continuous Representation DL (CR-DL). DR-DL is susceptibleto numerical inaccuracy owing to interpolation and regridding errors in adiscretized q-space. In this paper, we propose a novel CR-DL approach, calledDictionary Learning - Spherical Polar Fourier Imaging (DL-SPFI) for effectivecompressed-sensing reconstruction of the q-space diffusion-weighted signal andthe EAP. In DL-SPFI, an dictionary that sparsifies the signal is learned fromthe space of continuous Gaussian diffusion signals. The learned dictionary isthen adaptively applied to different voxels using a weighted LASSO frameworkfor robust signal reconstruction. The adaptive dictionary is proved to beoptimal. Compared with the start-of-the-art CR-DL and DR-DL methods proposed byMerlet et al. and Bilgic et al., espectively, our work offers the followingadvantages. First, the learned dictionary is proved to be optimal for Gaussiandiffusion signals. Second, to our knowledge, this is the first work to learn avoxel-adaptive dictionary. The importance of the adaptive dictionary in EAPreconstruction will be demonstrated theoretically and empirically. Third,optimization in DL-SPFI is only performed in a small subspace resided by theSPF coefficients, as opposed to the q-space approach utilized by Merlet et al.The experiment results demonstrate the advantages of DL-SPFI over the originalSPF basis and Bilgic et al.'s method.
arxiv-3900-7 | A Statistical Learning Theory Framework for Supervised Pattern Discovery | http://arxiv.org/abs/1307.0802 | author:Jonathan H. Huggins, Cynthia Rudin category:stat.ML cs.AI published:2013-07-02 summary:This paper formalizes a latent variable inference problem we call {\emsupervised pattern discovery}, the goal of which is to find sets ofobservations that belong to a single ``pattern.'' We discuss two versions ofthe problem and prove uniform risk bounds for both. In the first version,collections of patterns can be generated in an arbitrary manner and the dataconsist of multiple labeled collections. In the second version, the patternsare assumed to be generated independently by identically distributed processes.These processes are allowed to take an arbitrary form, so observations within apattern are not in general independent of each other. The bounds for the secondversion of the problem are stated in terms of a new complexity measure, thequasi-Rademacher complexity.
arxiv-3900-8 | Comparing various regression methods on ensemble strategies in differential evolution | http://arxiv.org/abs/1307.0841 | author:Iztok Fister Jr., Iztok Fister, Janez Brest category:cs.NE published:2013-07-02 summary:Differential evolution possesses a multitude of various strategies forgenerating new trial solutions. Unfortunately, the best strategy is not knownin advance. Moreover, this strategy usually depends on the problem to besolved. This paper suggests using various regression methods (like randomforest, extremely randomized trees, gradient boosting, decision trees, and ageneralized linear model) on ensemble strategies in differential evolutionalgorithm by predicting the best differential evolution strategy during therun. Comparing the preliminary results of this algorithm by optimizing a suiteof five well-known functions from literature, it was shown that using therandom forest regression method substantially outperformed the results of theother regression methods.
arxiv-3900-9 | Semi-supervised Ranking Pursuit | http://arxiv.org/abs/1307.0846 | author:Evgeni Tsivtsivadze, Tom Heskes category:stat.ML cs.IR cs.LG published:2013-07-02 summary:We propose a novel sparse preference learning/ranking algorithm. Ouralgorithm approximates the true utility function by a weighted sum of basisfunctions using the squared loss on pairs of data points, and is ageneralization of the kernel matching pursuit method. It can operate both in asupervised and a semi-supervised setting and allows efficient search formultiple, near-optimal solutions. Furthermore, we describe the extension of thealgorithm suitable for combined ranking and regression tasks. In ourexperiments we demonstrate that the proposed algorithm outperforms severalstate-of-the-art learning methods when taking into account unlabeled data andperforms comparably in a supervised learning scenario, while providing sparsersolutions.
arxiv-3900-10 | Submodularity of a Set Label Disagreement Function | http://arxiv.org/abs/1307.1303 | author:Toufiq Parag category:cs.CV published:2013-07-02 summary:A set label disagreement function is defined over the number of variablesthat deviates from the dominant label. The dominant label is the value assumedby the largest number of variables within a set of binary variables. Thesubmodularity of a certain family of set label disagreement function isdiscussed in this manuscript. Such disagreement function could be utilized as acost function in combinatorial optimization approaches for problems definedover hypergraphs.
arxiv-3900-11 | Discovering the Markov network structure | http://arxiv.org/abs/1307.0643 | author:Edith Kovács, Tamás Szántai category:cs.IT cs.LG math.IT published:2013-07-02 summary:In this paper a new proof is given for the supermodularity of informationcontent. Using the decomposability of the information content an algorithm isgiven for discovering the Markov network graph structure endowed by thepairwise Markov property of a given probability distribution. A discreteprobability distribution is given for which the equivalence ofHammersley-Clifford theorem is fulfilled although some of the possible vectorrealizations are taken on with zero probability. Our algorithm for discoveringthe pairwise Markov network is illustrated on this example, too.
arxiv-3900-12 | Improving Pointwise Mutual Information (PMI) by Incorporating Significant Co-occurrence | http://arxiv.org/abs/1307.0596 | author:Om P. Damani category:cs.CL published:2013-07-02 summary:We design a new co-occurrence based word association measure by incorporatingthe concept of significant cooccurrence in the popular word association measurePointwise Mutual Information (PMI). By extensive experiments with a largenumber of publicly available datasets we show that the newly introduced measureperforms better than other co-occurrence based measures and despite beingresource-light, compares well with the best known resource-heavy distributionalsimilarity and knowledge based word association measures. We investigate thesource of this performance improvement and find that of the two types ofsignificant co-occurrence - corpus-level and document-level, the concept ofcorpus level significance combined with the use of document counts in place ofword counts is responsible for all the performance gains observed. The conceptof document level significance is not helpful for PMI adaptation.
arxiv-3900-13 | The Orchive : Data mining a massive bioacoustic archive | http://arxiv.org/abs/1307.0589 | author:Steven Ness, Helena Symonds, Paul Spong, George Tzanetakis category:cs.LG cs.DB cs.SD published:2013-07-02 summary:The Orchive is a large collection of over 20,000 hours of audio recordingsfrom the OrcaLab research facility located off the northern tip of VancouverIsland. It contains recorded orca vocalizations from the 1980 to the presenttime and is one of the largest resources of bioacoustic data in the world. Wehave developed a web-based interface that allows researchers to listen to theserecordings, view waveform and spectral representations of the audio, labelclips with annotations, and view the results of machine learning classifiersbased on automatic audio features extraction. In this paper we describe suchclassifiers that discriminate between background noise, orca calls, and thevoice notes that are present in most of the tapes. Furthermore we showclassification results for individual calls based on a previously existing orcacall catalog. We have also experimentally investigated the scalability ofclassifiers over the entire Orchive.
arxiv-3900-14 | A non-parametric conditional factor regression model for high-dimensional input and response | http://arxiv.org/abs/1307.0578 | author:Ava Bargi, Richard Yi Da Xu, Massimo Piccardi category:stat.ML cs.LG published:2013-07-02 summary:In this paper, we propose a non-parametric conditional factor regression(NCFR)model for domains with high-dimensional input and response. NCFR enhanceslinear regression in two ways: a) introducing low-dimensional latent factorsleading to dimensionality reduction and b) integrating an Indian Buffet Processas a prior for the latent factors to derive unlimited sparse dimensions.Experimental results comparing NCRF to several alternatives give evidence toremarkable prediction performance.
arxiv-3900-15 | Novel Factorization Strategies for Higher Order Tensors: Implications for Compression and Recovery of Multi-linear Data | http://arxiv.org/abs/1307.0805 | author:Zemin Zhang, Gregory Ely, Shuchin Aeron, Ning Hao, Misha Kilmer category:cs.IT cs.CV math.IT published:2013-07-02 summary:In this paper we propose novel methods for compression and recovery ofmultilinear data under limited sampling. We exploit the recently proposedtensor- Singular Value Decomposition (t-SVD)[1], which is a group theoreticframework for tensor decomposition. In contrast to popular existing tensordecomposition techniques such as higher-order SVD (HOSVD), t-SVD has optimalityproperties similar to the truncated SVD for matrices. Based on t-SVD, we firstconstruct novel tensor-rank like measures to characterize informational andstructural complexity of multilinear data. Following that we outline acomplexity penalized algorithm for tensor completion from missing entries. Asan application, 3-D and 4-D (color) video data compression and recovery areconsidered. We show that videos with linear camera motion can be representedmore efficiently using t-SVD compared to traditional approaches based onvectorizing or flattening of the tensors. Application of the proposed tensorcompletion algorithm for video recovery from missing entries is shown to yielda superior performance over existing methods. In conclusion we point outseveral research directions and implications to online prediction ofmultilinear data.
arxiv-3900-16 | Multi-Task Policy Search | http://arxiv.org/abs/1307.0813 | author:Marc Peter Deisenroth, Peter Englert, Jan Peters, Dieter Fox category:stat.ML cs.AI cs.LG cs.RO published:2013-07-02 summary:Learning policies that generalize across multiple tasks is an important andchallenging research topic in reinforcement learning and robotics. Trainingindividual policies for every single potential task is often impractical,especially for continuous task variations, requiring more principled approachesto share and transfer knowledge among similar tasks. We present a novelapproach for learning a nonlinear feedback policy that generalizes acrossmultiple tasks. The key idea is to define a parametrized policy as a functionof both the state and the task, which allows learning a single policy thatgeneralizes across multiple known and unknown tasks. Applications of our novelapproach to reinforcement and imitation learning in real-robot experiments areshown.
arxiv-3900-17 | Data Fusion by Matrix Factorization | http://arxiv.org/abs/1307.0803 | author:Marinka Žitnik, Blaž Zupan category:cs.LG cs.AI cs.DB stat.ML published:2013-07-02 summary:For most problems in science and engineering we can obtain data sets thatdescribe the observed system from various perspectives and record the behaviorof its individual components. Heterogeneous data sets can be collectively minedby data fusion. Fusion can focus on a specific target relation and exploitdirectly associated data together with contextual data and data about system'sconstraints. In the paper we describe a data fusion approach with penalizedmatrix tri-factorization (DFMF) that simultaneously factorizes data matrices toreveal hidden associations. The approach can directly consider any data thatcan be expressed in a matrix, including those from feature-basedrepresentations, ontologies, associations and networks. We demonstrate theutility of DFMF for gene function prediction task with eleven different datasources and for prediction of pharmacologic actions by fusing six data sources.Our data fusion algorithm compares favorably to alternative data integrationapproaches and achieves higher accuracy than can be obtained from any singledata source alone.
arxiv-3900-18 | Challenges in Representation Learning: A report on three machine learning contests | http://arxiv.org/abs/1307.0414 | author:Ian J. Goodfellow, Dumitru Erhan, Pierre Luc Carrier, Aaron Courville, Mehdi Mirza, Ben Hamner, Will Cukierski, Yichuan Tang, David Thaler, Dong-Hyun Lee, Yingbo Zhou, Chetan Ramaiah, Fangxiang Feng, Ruifan Li, Xiaojie Wang, Dimitris Athanasakis, John Shawe-Taylor, Maxim Milakov, John Park, Radu Ionescu, Marius Popescu, Cristian Grozea, James Bergstra, Jingjing Xie, Lukasz Romaszko, Bing Xu, Zhang Chuang, Yoshua Bengio category:stat.ML cs.LG published:2013-07-01 summary:The ICML 2013 Workshop on Challenges in Representation Learning focused onthree challenges: the black box learning challenge, the facial expressionrecognition challenge, and the multimodal learning challenge. We describe thedatasets created for these challenges and summarize the results of thecompetitions. We provide suggestions for organizers of future challenges andsome comments on what kind of knowledge can be gained from machine learningcompetitions.
arxiv-3900-19 | Gaussian Process Conditional Copulas with Applications to Financial Time Series | http://arxiv.org/abs/1307.0373 | author:José Miguel Hernández-Lobato, James Robert Lloyd, Daniel Hernández-Lobato category:stat.ML published:2013-07-01 summary:The estimation of dependencies between multiple variables is a centralproblem in the analysis of financial time series. A common approach is toexpress these dependencies in terms of a copula function. Typically the copulafunction is assumed to be constant but this may be inaccurate when there arecovariates that could have a large influence on the dependence structure of thedata. To account for this, a Bayesian framework for the estimation ofconditional copulas is proposed. In this framework the parameters of a copulaare non-linearly related to some arbitrary conditioning variables. We evaluatethe ability of our method to predict time-varying dependencies on severalequities and currencies and observe consistent performance gains compared tostatic copula models and other time-varying copula methods.
arxiv-3900-20 | Dimensionality Detection and Integration of Multiple Data Sources via the GP-LVM | http://arxiv.org/abs/1307.0323 | author:James Barrett, Anthony C. C. Coolen category:stat.ML published:2013-07-01 summary:The Gaussian Process Latent Variable Model (GP-LVM) is a non-linearprobabilistic method of embedding a high dimensional dataset in terms lowdimensional `latent' variables. In this paper we illustrate that maximum aposteriori (MAP) estimation of the latent variables and hyperparameters can beused for model selection and hence we can determine the optimal number orlatent variables and the most appropriate model. This is an alternative to thevariational approaches developed recently and may be useful when we want to usea non-Gaussian prior or kernel functions that don't have automatic relevancedetermination (ARD) parameters. Using a second order expansion of the latentvariable posterior we can marginalise the latent variables and obtain anestimate for the hyperparameter posterior. Secondly, we use the GP-LVM tointegrate multiple data sources by simultaneously embedding them in terms ofcommon latent variables. We present results from synthetic data to illustratethe successful detection and retrieval of low dimensional structure from highdimensional data. We demonstrate that the integration of multiple data sourcesleads to more robust performance. Finally, we show that when the data are usedfor binary classification tasks we can attain a significant gain in predictionaccuracy when the low dimensional representation is used.
arxiv-3900-21 | Algorithms of the LDA model [REPORT] | http://arxiv.org/abs/1307.0317 | author:Jaka Špeh, Andrej Muhič, Jan Rupnik category:cs.LG cs.IR stat.ML published:2013-07-01 summary:We review three algorithms for Latent Dirichlet Allocation (LDA). Two of themare variational inference algorithms: Variational Bayesian inference and OnlineVariational Bayesian inference and one is Markov Chain Monte Carlo (MCMC)algorithm -- Collapsed Gibbs sampling. We compare their time complexity andperformance. We find that online variational Bayesian inference is the fastestalgorithm and still returns reasonably good results.
arxiv-3900-22 | Multilevel Threshold Based Gray Scale Image Segmentation using Cuckoo Search | http://arxiv.org/abs/1307.0277 | author:Sourav Samantaa, Nilanjan Dey, Poulami Das, Suvojit Acharjee, Sheli Sinha Chaudhuri category:cs.CV published:2013-07-01 summary:Image Segmentation is a technique of partitioning the original image intosome distinct classes. Many possible solutions may be available for segmentingan image into a certain number of classes, each one having different quality ofsegmentation. In our proposed method, multilevel thresholding technique hasbeen used for image segmentation. A new approach of Cuckoo Search (CS) is usedfor selection of optimal threshold value. In other words, the algorithm is usedto achieve the best solution from the initial random threshold values orsolutions and to evaluate the quality of a solution correlation function isused. Finally, MSE and PSNR are measured to understand the segmentationquality.
arxiv-3900-23 | WebSets: Extracting Sets of Entities from the Web Using Unsupervised Information Extraction | http://arxiv.org/abs/1307.0261 | author:Bhavana Dalvi, William W. Cohen, Jamie Callan category:cs.LG cs.CL cs.IR published:2013-07-01 summary:We describe a open-domain information extraction method for extractingconcept-instance pairs from an HTML corpus. Most earlier approaches to thisproblem rely on combining clusters of distributionally similar terms andconcept-instance pairs obtained with Hearst patterns. In contrast, our methodrelies on a novel approach for clustering terms found in HTML tables, and thenassigning concept names to these clusters using Hearst patterns. The method canbe efficiently applied to a large corpus, and experimental results on severaldatasets show that our method can accurately extract large numbers ofconcept-instance pairs.
arxiv-3900-24 | Exploratory Learning | http://arxiv.org/abs/1307.0253 | author:Bhavana Dalvi, William W. Cohen, Jamie Callan category:cs.LG published:2013-07-01 summary:In multiclass semi-supervised learning (SSL), it is sometimes the case thatthe number of classes present in the data is not known, and hence no labeledexamples are provided for some classes. In this paper we present variants ofwell-known semi-supervised multiclass learning methods that are robust when thedata contains an unknown number of classes. In particular, we present an"exploratory" extension of expectation-maximization (EM) that exploresdifferent numbers of classes while learning. "Exploratory" SSL greatly improvesperformance on three datasets in terms of F1 on the classes with seed examplesi.e., the classes which are expected to be in the data. Our Exploratory EMalgorithm also outperforms a SSL method based non-parametric Bayesianclustering.
arxiv-3900-25 | Semi-supervised clustering methods | http://arxiv.org/abs/1307.0252 | author:Eric Bair category:stat.ME cs.LG stat.ML published:2013-07-01 summary:Cluster analysis methods seek to partition a data set into homogeneoussubgroups. It is useful in a wide variety of applications, including documentprocessing and modern genetics. Conventional clustering methods areunsupervised, meaning that there is no outcome variable nor is anything knownabout the relationship between the observations in the data set. In manysituations, however, information about the clusters is available in addition tothe values of the features. For example, the cluster labels of someobservations may be known, or certain observations may be known to belong tothe same cluster. In other cases, one may wish to identify clusters that areassociated with a particular outcome variable. This review describes severalclustering algorithms (known as "semi-supervised clustering" methods) that canbe applied in these situations. The majority of these methods are modificationsof the popular k-means clustering method, and several of them will be describedin detail. A brief description of some other semi-supervised clusteringalgorithms is also provided.
arxiv-3900-26 | Online discrete optimization in social networks in the presence of Knightian uncertainty | http://arxiv.org/abs/1307.0473 | author:Maxim Raginsky, Angelia Nedić category:math.OC cs.DC cs.LG published:2013-07-01 summary:We study a model of collective real-time decision-making (or learning) in asocial network operating in an uncertain environment, for which no a prioriprobabilistic model is available. Instead, the environment's impact on theagents in the network is seen through a sequence of cost functions, revealed tothe agents in a causal manner only after all the relevant actions are taken.There are two kinds of costs: individual costs incurred by each agent andlocal-interaction costs incurred by each agent and its neighbors in the socialnetwork. Moreover, agents have inertia: each agent has a default mixed strategythat stays fixed regardless of the state of the environment, and must expendeffort to deviate from this strategy in order to respond to cost signals comingfrom the environment. We construct a decentralized strategy, wherein each agentselects its action based only on the costs directly affecting it and on thedecisions made by its neighbors in the network. In this setting, we quantifysocial learning in terms of regret, which is given by the difference betweenthe realized network performance over a given time horizon and the bestperformance that could have been achieved in hindsight by a fictitiouscentralized entity with full knowledge of the environment's evolution. We showthat our strategy achieves the regret that scales polylogarithmically with thetime horizon and polynomially with the number of agents and the maximum numberof neighbors of any agent in the social network.
arxiv-3900-27 | Quantum support vector machine for big data classification | http://arxiv.org/abs/1307.0471 | author:Patrick Rebentrost, Masoud Mohseni, Seth Lloyd category:quant-ph cs.LG published:2013-07-01 summary:Supervised machine learning is the classification of new data based onalready classified training examples. In this work, we show that the supportvector machine, an optimized binary classifier, can be implemented on a quantumcomputer, with complexity logarithmic in the size of the vectors and the numberof training examples. In cases when classical sampling algorithms requirepolynomial time, an exponential speed-up is obtained. At the core of thisquantum big data algorithm is a non-sparse matrix exponentiation technique forefficiently performing a matrix inversion of the training data inner-product(kernel) matrix.
arxiv-3900-28 | Short Term Memory Capacity in Networks via the Restricted Isometry Property | http://arxiv.org/abs/1307.7970 | author:Adam S. Charles, Han Lun Yap, Christopher J. Rozell category:cs.IT cs.NE math.IT published:2013-07-01 summary:Cortical networks are hypothesized to rely on transient network activity tosupport short term memory (STM). In this paper we study the capacity ofrandomly connected recurrent linear networks for performing STM when the inputsignals are approximately sparse in some basis. We leverage results fromcompressed sensing to provide rigorous non asymptotic recovery guarantees,quantifying the impact of the input sparsity level, the input sparsity basis,and the network characteristics on the system capacity. Our analysisdemonstrates that network memory capacities can scale superlinearly with thenumber of nodes, and in some situations can achieve STM capacities that aremuch larger than the network size. We provide perfect recovery guarantees forfinite sequences and recovery bounds for infinite sequences. The latteranalysis predicts that network STM systems may have an optimal recovery lengththat balances errors due to omission and recall mistakes. Furthermore, we showthat the conditions yielding optimal STM capacity can be embodied in severalnetwork topologies, including networks with sparse or dense connectivities.
arxiv-3900-29 | A Direct Estimation of High Dimensional Stationary Vector Autoregressions | http://arxiv.org/abs/1307.0293 | author:Fang Han, Huanran Lu, Han Liu category:stat.ML published:2013-07-01 summary:The vector autoregressive (VAR) model is a powerful tool in modeling complextime series and has been exploited in many fields. However, fitting highdimensional VAR model poses some unique challenges: On one hand, thedimensionality, caused by modeling a large number of time series and higherorder autoregressive processes, is usually much higher than the time serieslength; On the other hand, the temporal dependence structure in the VAR modelgives rise to extra theoretical challenges. In high dimensions, one popularapproach is to assume the transition matrix is sparse and fit the VAR modelusing the "least squares" method with a lasso-type penalty. In this manuscript,we propose an alternative way in estimating the VAR model. The main idea is,via exploiting the temporal dependence structure, to formulate the estimatingproblem into a linear program. There is instant advantage for the proposedapproach over the lasso-type estimators: The estimation equation can bedecomposed into multiple sub-equations and accordingly can be efficientlysolved in a parallel fashion. In addition, our method brings new theoreticalinsights into the VAR model analysis. So far the theoretical results developedin high dimensions (e.g., Song and Bickel (2011) and Kock and Callot (2012))mainly pose assumptions on the design matrix of the formulated regressionproblems. Such conditions are indirect about the transition matrices and nottransparent. In contrast, our results show that the operator norm of thetransition matrices plays an important role in estimation accuracy. We provideexplicit rates of convergence for both estimation and prediction. In addition,we provide thorough experiments on both synthetic and real-world equity data toshow that there are empirical advantages of our method over the lasso-typeestimators in both parameter estimation and forecasting.
arxiv-3900-30 | An Empirical Study into Annotator Agreement, Ground Truth Estimation, and Algorithm Evaluation | http://arxiv.org/abs/1307.0426 | author:Thomas A. Lampert, André Stumpf, Pierre Gançarski category:cs.CV cs.AI I.4.6; I.5.4 published:2013-07-01 summary:Although agreement between annotators has been studied in the past from astatistical viewpoint, little work has attempted to quantify the extent towhich this phenomenon affects the evaluation of computer vision (CV) objectdetection algorithms. Many researchers utilise ground truth (GT) in experimentsand more often than not this GT is derived from one annotator's opinion. Howdoes the difference in opinion affect an algorithm's evaluation? Four examplesof typical CV problems are chosen, and a methodology is applied to each toquantify the inter-annotator variance and to offer insight into the mechanismsbehind agreement and the use of GT. It is found that when detecting linearobjects annotator agreement is very low. The agreement in object position,linear or otherwise, can be partially explained through basic image properties.Automatic object detectors are compared to annotator agreement and it is foundthat a clear relationship exists. Several methods for calculating GTs from anumber of annotations are applied and the resulting differences in theperformance of the object detectors are quantified. It is found that the rankof a detector is highly dependent upon the method used to form the GT. It isalso found that although the STAPLE and LSML GT estimation methods appear torepresent the mean of the performance measured using the individualannotations, when there are few annotations, or there is a large variance inthem, these estimates tend to degrade. Furthermore, one of the most commonlyadopted annotation combination methods--consensus voting--accentuates moreobvious features, which results in an overestimation of the algorithm'sperformance. Finally, it is concluded that in some datasets it may not bepossible to state with any confidence that one algorithm outperforms anotherwhen evaluating upon one GT and a method for calculating confidence bounds isdiscussed.
arxiv-3900-31 | Learning directed acyclic graphs based on sparsest permutations | http://arxiv.org/abs/1307.0366 | author:Garvesh Raskutti, Caroline Uhler category:math.ST cs.LG stat.TH published:2013-07-01 summary:We consider the problem of learning a Bayesian network or directed acyclicgraph (DAG) model from observational data. A number of constraint-based,score-based and hybrid algorithms have been developed for this purpose. Forconstraint-based methods, statistical consistency guarantees typically rely onthe faithfulness assumption, which has been show to be restrictive especiallyfor graphs with cycles in the skeleton. However, there is only limited work onconsistency guarantees for score-based and hybrid algorithms and it has beenunclear whether consistency guarantees can be proven under weaker conditionsthan the faithfulness assumption. In this paper, we propose the sparsest permutation (SP) algorithm. Thisalgorithm is based on finding the causal ordering of the variables that yieldsthe sparsest DAG. We prove that this new score-based method is consistent understrictly weaker conditions than the faithfulness assumption. We alsodemonstrate through simulations on small DAGs that the SP algorithm comparesfavorably to the constraint-based PC and SGS algorithms as well as thescore-based Greedy Equivalence Search and hybrid Max-Min Hill-Climbing method.In the Gaussian setting, we prove that our algorithm boils down to finding thepermutation of the variables with sparsest Cholesky decomposition for theinverse covariance matrix. Using this connection, we show that in the oraclesetting, where the true covariance matrix is known, the SP algorithm is in factequivalent to $\ell_0$-penalized maximum likelihood estimation.
arxiv-3900-32 | Sparse Principal Component Analysis for High Dimensional Vector Autoregressive Models | http://arxiv.org/abs/1307.0164 | author:Zhaoran Wang, Fang Han, Han Liu category:stat.ML published:2013-06-30 summary:We study sparse principal component analysis for high dimensional vectorautoregressive time series under a doubly asymptotic framework, which allowsthe dimension $d$ to scale with the series length $T$. We treat the transitionmatrix of time series as a nuisance parameter and directly apply sparseprincipal component analysis on multivariate time series as if the data areindependent. We provide explicit non-asymptotic rates of convergence forleading eigenvector estimation and extend this result to principal subspaceestimation. Our analysis illustrates that the spectral norm of the transitionmatrix plays an essential role in determining the final rates. We alsocharacterize sufficient conditions under which sparse principal componentanalysis attains the optimal parametric rate. Our theoretical results arebacked up by thorough numerical studies.
arxiv-3900-33 | Approximate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs | http://arxiv.org/abs/1307.0060 | author:Vikash K. Mansinghka, Tejas D. Kulkarni, Yura N. Perov, Joshua B. Tenenbaum category:cs.AI cs.CV stat.ML published:2013-06-29 summary:The idea of computer vision as the Bayesian inverse problem to computergraphics has a long history and an appealing elegance, but it has proveddifficult to directly implement. Instead, most vision tasks are approached viacomplex bottom-up processing pipelines. Here we show that it is possible towrite short, simple probabilistic graphics programs that define flexiblegenerative models and to automatically invert them to interpret real-worldimages. Generative probabilistic graphics programs consist of a stochasticscene generator, a renderer based on graphics software, a stochastic likelihoodmodel linking the renderer's output and the data, and latent variables thatadjust the fidelity of the renderer and the tolerance of the likelihood model.Representations and algorithms from computer graphics, originally designed toproduce high-quality images, are instead used as the deterministic backbone forhighly approximate and stochastic generative models. This formulation combinesprobabilistic programming, computer graphics, and approximate Bayesiancomputation, and depends only on general-purpose, automatic inferencetechniques. We describe two applications: reading sequences of degraded andadversarially obscured alphanumeric characters, and inferring 3D road modelsfrom vehicle-mounted camera images. Each of the probabilistic graphics programswe present relies on under 20 lines of probabilistic code, and supportsaccurate, approximately Bayesian inferences about ambiguous real-world images.
arxiv-3900-34 | Semantics and pragmatics in actual software applications and in web search engines: exploring innovations | http://arxiv.org/abs/1307.0087 | author:Fabrizio M. A. Lolli category:cs.IR cs.CL cs.HC published:2013-06-29 summary:While new ways to use the Semantic Web are developed every week, which allowthe user to find information on web more accurately - for example in searchengines - some sophisticated pragmatic tools are becoming more important - forexample in web interfaces known as Social Intelligence, or in the most famousSiri by Apple. The work aims to analyze whether and where we can identify theboundary between semantics and pragmatics in the software used by analyzedsystems. examining how the linguistic disciplines are fundamental in theirprogress. Is it possible to assume that the tools of social intelligence have apragmatic approach to the questions of the user, or it is just a use of a veryrich vocabulary, with the use of semantic tools?
arxiv-3900-35 | Concentration and Confidence for Discrete Bayesian Sequence Predictors | http://arxiv.org/abs/1307.0127 | author:Tor Lattimore, Marcus Hutter, Peter Sunehag category:cs.LG stat.ML published:2013-06-29 summary:Bayesian sequence prediction is a simple technique for predicting futuresymbols sampled from an unknown measure on infinite sequences over a countablealphabet. While strong bounds on the expected cumulative error are known, thereare only limited results on the distribution of this error. We prove tighthigh-probability bounds on the cumulative error, which is measured in terms ofthe Kullback-Leibler (KL) divergence. We also consider the problem ofconstructing upper confidence bounds on the KL and Hellinger errors similar tothose constructed from Hoeffding-like bounds in the i.i.d. case. The newresults are applied to show that Bayesian sequence prediction can be used inthe Knows What It Knows (KWIK) framework with bounds that match thestate-of-the-art.
arxiv-3900-36 | Hyperspectral Data Unmixing Using GNMF Method and Sparseness Constraint | http://arxiv.org/abs/1307.0129 | author:Roozbeh Rajabi, Hassan Ghassemian category:cs.CV published:2013-06-29 summary:Hyperspectral images contain mixed pixels due to low spatial resolution ofhyperspectral sensors. Mixed pixels are pixels containing more than onedistinct material called endmembers. The presence percentages of endmembers inmixed pixels are called abundance fractions. Spectral unmixing problem refersto decomposing these pixels into a set of endmembers and abundance fractions.Due to nonnegativity constraint on abundance fractions, nonnegative matrixfactorization methods (NMF) have been widely used for solving spectral unmixingproblem. In this paper we have used graph regularized (GNMF) method withsparseness constraint to unmix hyperspectral data. This method applied onsimulated data using AVIRIS Indian Pines dataset and USGS library and resultsare quantified based on AAD and SAD measures. Results in comparison with othermethods show that the proposed method can unmix data more effectively.
arxiv-3900-37 | A Novel Active Contour Model for Texture Segmentation | http://arxiv.org/abs/1306.6726 | author:Aditya Tatu, Sumukh Bansal category:cs.CV published:2013-06-28 summary:Texture is intuitively defined as a repeated arrangement of a basic patternor object in an image. There is no mathematical definition of a texture though.The human visual system is able to identify and segment different textures in agiven image. Automating this task for a computer is far from trivial. There arethree major components of any texture segmentation algorithm: (a) The featuresused to represent a texture, (b) the metric induced on this representationspace and (c) the clustering algorithm that runs over these features in orderto segment a given image into different textures. In this paper, we propose anactive contour based novel unsupervised algorithm for texture segmentation. Weuse intensity covariance matrices of regions as the defining feature oftextures and find regions that have the most inter-region dissimilar covariancematrices using active contours. Since covariance matrices are symmetricpositive definite, we use geodesic distance defined on the manifold ofsymmetric positive definite matrices PD(n) as a measure of dissimlarity betweensuch matrices. We demonstrate performance of our algorithm on both artificialand real texture images.
arxiv-3900-38 | Error AMP Chain Graphs | http://arxiv.org/abs/1306.6843 | author:Jose M. Peña category:stat.ML cs.AI published:2013-06-28 summary:Any regular Gaussian probability distribution that can be represented by anAMP chain graph (CG) can be expressed as a system of linear equations withcorrelated errors whose structure depends on the CG. However, the CG representsthe errors implicitly, as no nodes in the CG correspond to the errors. Wepropose in this paper to add some deterministic nodes to the CG in order torepresent the errors explicitly. We call the result an EAMP CG. We will showthat, as desired, every AMP CG is Markov equivalent to its corresponding EAMPCG under marginalization of the error nodes. We will also show that every EAMPCG under marginalization of the error nodes is Markov equivalent to some LWF CGunder marginalization of the error nodes, and that the latter is Markovequivalent to some directed and acyclic graph (DAG) under marginalization ofthe error nodes and conditioning on some selection nodes. This is importantbecause it implies that the independence model represented by an AMP CG can beaccounted for by some data generating process that is partially observed andhas selection bias. Finally, we will show that EAMP CGs are closed undermarginalization. This is a desirable feature because it guarantees parsimoniousmodels under marginalization.
arxiv-3900-39 | New Mathematical and Algorithmic Schemes for Pattern Classification with Application to the Identification of Writers of Important Ancient Documents | http://arxiv.org/abs/1306.6842 | author:Dimitris Arabadjis, Fotios Giannopoulos, Constantin Papaodysseus, Solomon Zannos, Panayiotis Rousopoulos, Michail Panagopoulos, Christopher Blackwell category:cs.CV published:2013-06-28 summary:In this paper, a novel approach is introduced for classifying curves intoproper families, according to their similarity. First, a mathematical quantitywe call plane curvature is introduced and a number of propositions are statedand proved. Proper similarity measures of two curves are introduced and asubsequent statistical analysis is applied. First, the efficiency of the curvefitting process has been tested on 2 shapes datasets of reference. Next, themethodology has been applied to the very important problem of classifying 23Byzantine codices and 46 Ancient inscriptions to their writers, thus achievingcorrect dating of their content. The inscriptions have been attributed to tenindividual hands and the Byzantine codices to four writers.
arxiv-3900-40 | Evaluation Measures for Hierarchical Classification: a unified view and novel approaches | http://arxiv.org/abs/1306.6802 | author:Aris Kosmopoulos, Ioannis Partalas, Eric Gaussier, Georgios Paliouras, Ion Androutsopoulos category:cs.AI cs.LG published:2013-06-28 summary:Hierarchical classification addresses the problem of classifying items into ahierarchy of classes. An important issue in hierarchical classification is theevaluation of different classification algorithms, which is complicated by thehierarchical relations among the classes. Several evaluation measures have beenproposed for hierarchical classification using the hierarchy in different ways.This paper studies the problem of evaluation in hierarchical classification byanalyzing and abstracting the key components of the existing performancemeasures. It also proposes two alternative generic views of hierarchicalevaluation and introduces two corresponding novel measures. The proposedmeasures, along with the state-of-the art ones, are empirically tested on threelarge datasets from the domain of text classification. The empirical resultsillustrate the undesirable behavior of existing approaches and how the proposedmethods overcome most of these methods across a range of cases.
arxiv-3900-41 | Simple one-pass algorithm for penalized linear regression with cross-validation on MapReduce | http://arxiv.org/abs/1307.0048 | author:Kun Yang category:stat.ML cs.DC cs.LG published:2013-06-28 summary:In this paper, we propose a one-pass algorithm on MapReduce for penalizedlinear regression \[f_\lambda(\alpha, \beta) = \Y - \alpha\mathbf{1} - X\beta\_2^2 +p_{\lambda}(\beta)\] where $\alpha$ is the intercept which can be omitteddepending on application; $\beta$ is the coefficients and $p_{\lambda}$ is thepenalized function with penalizing parameter $\lambda$. $f_\lambda(\alpha,\beta)$ includes interesting classes such as Lasso, Ridge regression andElastic-net. Compared to latest iterative distributed algorithms requiringmultiple MapReduce jobs, our algorithm achieves huge performance improvement;moreover, our algorithm is exact compared to the approximate algorithms such asparallel stochastic gradient decent. Moreover, what our algorithm distinguisheswith others is that it trains the model with cross validation to choose optimal$\lambda$ instead of user specified one. Key words: penalized linear regression, lasso, elastic-net, ridge, MapReduce
arxiv-3900-42 | Digital Image Tamper Detection Techniques - A Comprehensive Study | http://arxiv.org/abs/1306.6737 | author:Minati Mishra, Flt. Lt. Dr. M. C. Adhikary category:cs.CR cs.CV published:2013-06-28 summary:Photographs are considered to be the most powerful and trustworthy media ofexpression. For a long time, those were accepted as proves of evidences invaried fields such as journalism, forensic investigations, militaryintelligence, scientific research and publications, crime detection and legalproceedings, investigation of insurance claims, medical imaging etc. Today,digital images have completely replaced the conventional photographs from everysphere of life but unfortunately, they seldom enjoy the credibility of theirconventional counterparts, thanks to the rapid advancements in the field ofdigital image processing. The increasing availability of low cost and sometimesfree of cost image editing software such as Photoshop, Corel Paint Shop,Photoscape, PhotoPlus, GIMP and Pixelmator have made the tampering of digitalimages even more easier and a common practice. Now it has become quiteimpossible to say whether a photograph is a genuine camera output or amanipulated version of it just by looking at it. As a result, photographs havealmost lost their reliability and place as proves of evidences in all fields.This is why digital image tamper detection has emerged as an important researcharea to establish the authenticity of digital photographs by separating thetampered lots from the original ones. This paper gives a brief history of imagetampering and a state-of-the-art review of the tamper detection techniques.
arxiv-3900-43 | Increasing Compression Ratio in PNG Images by k-Modulus Method for Image Transformation | http://arxiv.org/abs/1307.0036 | author:Firas A. Jassim category:cs.CV cs.MM published:2013-06-28 summary:Image compression is an important filed in image processing. The sciencewelcomes any tinny contribution that may increase the compression ratio bywhichever insignificant percentage. Therefore, the essential contribution inthis paper is to increase the compression ratio for the well known PortableNetwork Graphics (PNG) image file format. The contribution starts withconverting the original PNG image into k-Modulus Method (k-MM). Practically,taking k equals to ten, and then the pixels in the constructed image will beintegers divisible by ten. Since PNG uses Lempel-Ziv compression algorithm,then the ability to reduce file size will increase according to the repetitionin pixels in each k-by-k window according to the transformation done by k-MM.Experimental results show that the proposed technique (k-PNG) produces highcompression ratio with smaller file size in comparison to the original PNGfile.
arxiv-3900-44 | A Survey on Metric Learning for Feature Vectors and Structured Data | http://arxiv.org/abs/1306.6709 | author:Aurélien Bellet, Amaury Habrard, Marc Sebban category:cs.LG stat.ML published:2013-06-28 summary:The need for appropriate ways to measure the distance or similarity betweendata is ubiquitous in machine learning, pattern recognition and data mining,but handcrafting such good metrics for specific problems is generallydifficult. This has led to the emergence of metric learning, which aims atautomatically learning a metric from data and has attracted a lot of interestin machine learning and related fields for the past ten years. This surveypaper proposes a systematic review of the metric learning literature,highlighting the pros and cons of each approach. We pay particular attention toMahalanobis distance metric learning, a well-studied and successful framework,but additionally present a wide range of methods that have recently emerged aspowerful alternatives, including nonlinear metric learning, similarity learningand local metric learning. Recent trends and extensions, such assemi-supervised metric learning, metric learning for histogram data and thederivation of generalization guarantees, are also covered. Finally, this surveyaddresses metric learning for structured data, in particular edit distancelearning, and attempts to give an overview of the remaining challenges inmetric learning for the years to come.
arxiv-3900-45 | Memory Limited, Streaming PCA | http://arxiv.org/abs/1307.0032 | author:Ioannis Mitliagkas, Constantine Caramanis, Prateek Jain category:stat.ML cs.IT cs.LG math.IT published:2013-06-28 summary:We consider streaming, one-pass principal component analysis (PCA), in thehigh-dimensional regime, with limited memory. Here, $p$-dimensional samples arepresented sequentially, and the goal is to produce the $k$-dimensional subspacethat best approximates these points. Standard algorithms require $O(p^2)$memory; meanwhile no algorithm can do better than $O(kp)$ memory, since this iswhat the output itself requires. Memory (or storage) complexity is mostmeaningful when understood in the context of computational and samplecomplexity. Sample complexity for high-dimensional PCA is typically studied inthe setting of the {\em spiked covariance model}, where $p$-dimensional pointsare generated from a population covariance equal to the identity (white noise)plus a low-dimensional perturbation (the spike) which is the signal to berecovered. It is now well-understood that the spike can be recovered when thenumber of samples, $n$, scales proportionally with the dimension, $p$. Yet, allalgorithms that provably achieve this, have memory complexity $O(p^2)$.Meanwhile, algorithms with memory-complexity $O(kp)$ do not have provablebounds on sample complexity comparable to $p$. We present an algorithm thatachieves both: it uses $O(kp)$ memory (meaning storage of any kind) and is ableto compute the $k$-dimensional spike with $O(p \log p)$ sample-complexity --the first algorithm of its kind. While our theoretical analysis focuses on thespiked covariance model, our simulations show that our algorithm is successfulon much more general models for the data.
arxiv-3900-46 | Arabizi Detection and Conversion to Arabic | http://arxiv.org/abs/1306.6755 | author:Kareem Darwish category:cs.CL cs.IR I.2.7 published:2013-06-28 summary:Arabizi is Arabic text that is written using Latin characters. Arabizi isused to present both Modern Standard Arabic (MSA) or Arabic dialects. It iscommonly used in informal settings such as social networking sites and is oftenwith mixed with English. In this paper we address the problems of: identifyingArabizi in text and converting it to Arabic characters. We used word andsequence-level features to identify Arabizi that is mixed with English. Weachieved an identification accuracy of 98.5%. As for conversion, we usedtransliteration mining with language modeling to generate equivalent Arabictext. We achieved 88.7% conversion accuracy, with roughly a third of errorsbeing spelling and morphological variants of the forms in ground truth.
arxiv-3900-47 | Traffic data reconstruction based on Markov random field modeling | http://arxiv.org/abs/1306.6482 | author:Shun Kataoka, Muneki Yasuda, Cyril Furtlehner, Kazuyuki Tanaka category:stat.ML cs.LG published:2013-06-27 summary:We consider the traffic data reconstruction problem. Suppose we have thetraffic data of an entire city that are incomplete because some road data areunobserved. The problem is to reconstruct the unobserved parts of the data. Inthis paper, we propose a new method to reconstruct incomplete traffic datacollected from various traffic sensors. Our approach is based on Markov randomfield modeling of road traffic. The reconstruction is achieved by usingmean-field method and a machine learning method. We numerically verify theperformance of our method using realistic simulated traffic data for the realroad network of Sendai, Japan.
arxiv-3900-48 | Supersparse Linear Integer Models for Interpretable Classification | http://arxiv.org/abs/1306.6677 | author:Berk Ustun, Stefano Tracà, Cynthia Rudin category:stat.ML stat.AP published:2013-06-27 summary:Scoring systems are classification models that only require users to add,subtract and multiply a few meaningful numbers to make a prediction. Thesemodels are often used because they are practical and interpretable. In thispaper, we introduce an off-the-shelf tool to create scoring systems that bothaccurate and interpretable, known as a Supersparse Linear Integer Model (SLIM).SLIM is a discrete optimization problem that minimizes the 0-1 loss toencourage a high level of accuracy, regularizes the L0-norm to encourage a highlevel of sparsity, and constrains coefficients to a set of interpretablevalues. We illustrate the practical and interpretable nature of SLIM scoringsystems through applications in medicine and criminology, and show that theyare are accurate and sparse in comparison to state-of-the-art classificationmodels using numerical experiments.
arxiv-3900-49 | Optimal Feature Selection in High-Dimensional Discriminant Analysis | http://arxiv.org/abs/1306.6557 | author:Mladen Kolar, Han Liu category:stat.ML math.ST stat.TH published:2013-06-27 summary:We consider the high-dimensional discriminant analysis problem. For thisproblem, different methods have been proposed and justified by establishingexact convergence rates for the classification risk, as well as the l2convergence results to the discriminative rule. However, sharp theoreticalanalysis for the variable selection performance of these procedures have notbeen established, even though model interpretation is of fundamental importancein scientific data analysis. This paper bridges the gap by providing sharpsufficient conditions for consistent variable selection using the sparsediscriminant analysis (Mai et al., 2012). Through careful analysis, weestablish rates of convergence that are significantly faster than the bestknown results and admit an optimal scaling of the sample size n, dimensionalityp, and sparsity level s in the high-dimensional setting. Sufficient conditionsare complemented by the necessary information theoretic limits on the variableselection problem in the context of high-dimensional discriminant analysis.Exploiting a numerical equivalence result, our method also establish theoptimal results for the ROAD estimator (Fan et al., 2012) and the sparseoptimal scaling estimator (Clemmensen et al., 2011). Furthermore, we analyze anexhaustive search procedure, whose performance serves as a benchmark, and showthat it is variable selection consistent under weaker conditions. Extensivesimulations demonstrating the sharpness of the bounds are also provided.
arxiv-3900-50 | Competency Tracking for English as a Second or Foreign Language Learners | http://arxiv.org/abs/1306.6130 | author:Robert Bishop Jr category:cs.CL published:2013-06-26 summary:My system utilizes the outcomes feature found in Moodle and other learningcontent management systems (LCMSs) to keep track of where students are in termsof what language competencies they have mastered and the competencies they needto get where they want to go. These competencies are based on the CommonEuropean Framework for (English) Language Learning. This data can be availablefor everyone involved with a given student's progress (e.g. educators, parents,supervisors and the students themselves). A given student's record of pastaccomplishments can also be meshed with those of his classmates. Not only are astudent's competencies easily seen and tracked, educators can view competenciesof a group of students that were achieved prior to enrollment in the class.This should make curriculum decision making easier and more efficient foreducators.
arxiv-3900-51 | Understanding the Predictive Power of Computational Mechanics and Echo State Networks in Social Media | http://arxiv.org/abs/1306.6111 | author:David Darmon, Jared Sylvester, Michelle Girvan, William Rand category:cs.SI cs.LG physics.soc-ph stat.AP stat.ML published:2013-06-26 summary:There is a large amount of interest in understanding users of social media inorder to predict their behavior in this space. Despite this interest, userpredictability in social media is not well-understood. To examine thisquestion, we consider a network of fifteen thousand users on Twitter over aseven week period. We apply two contrasting modeling paradigms: computationalmechanics and echo state networks. Both methods attempt to model the behaviorof users on the basis of their past behavior. We demonstrate that the behaviorof users on Twitter can be well-modeled as processes with self-feedback. Wefind that the two modeling approaches perform very similarly for most users,but that they differ in performance on a small subset of the users. Byexploring the properties of these performance-differentiated users, wehighlight the challenges faced in applying predictive models to dynamic socialdata.
arxiv-3900-52 | Scaling Up Robust MDPs by Reinforcement Learning | http://arxiv.org/abs/1306.6189 | author:Aviv Tamar, Huan Xu, Shie Mannor category:cs.LG stat.ML published:2013-06-26 summary:We consider large-scale Markov decision processes (MDPs) with parameteruncertainty, under the robust MDP paradigm. Previous studies showed that robustMDPs, based on a minimax approach to handle uncertainty, can be solved usingdynamic programming for small to medium sized problems. However, due to the"curse of dimensionality", MDPs that model real-life problems are typicallyprohibitively large for such approaches. In this work we employ a reinforcementlearning approach to tackle this planning problem: we develop a robustapproximate dynamic programming method based on a projected fixed pointequation to approximately solve large scale robust MDPs. We show that theproposed method provably succeeds under certain technical conditions, anddemonstrate its effectiveness through simulation of an option pricing problem.To the best of our knowledge, this is the first attempt to scale up the robustMDPs paradigm.
arxiv-3900-53 | Compressive Coded Aperture Keyed Exposure Imaging with Optical Flow Reconstruction | http://arxiv.org/abs/1306.6281 | author:Zachary T. Harmany, Roummel F. Marcia, Rebecca M. Willett category:cs.IT cs.CV math.IT stat.AP published:2013-06-26 summary:This paper describes a coded aperture and keyed exposure approach tocompressive video measurement which admits a small physical platform, highphoton efficiency, high temporal resolution, and fast reconstructionalgorithms. The proposed projections satisfy the Restricted Isometry Property(RIP), and hence compressed sensing theory provides theoretical guarantees onthe video reconstruction quality. Moreover, the projections can be easilyimplemented using existing optical elements such as spatial light modulators(SLMs). We extend these coded mask designs to novel dual-scale masks (DSMs)which enable the recovery of a coarse-resolution estimate of the scene withnegligible computational cost. We develop fast numerical algorithms whichutilize both temporal correlations and optical flow in the video sequence aswell as the innovative structure of the projections. Our numerical experimentsdemonstrate the efficacy of the proposed approach on short-wave infrared data.
arxiv-3900-54 | Solving Relational MDPs with Exogenous Events and Additive Rewards | http://arxiv.org/abs/1306.6302 | author:S. Joshi, R. Khardon, P. Tadepalli, A. Raghavan, A. Fern category:cs.AI cs.LG published:2013-06-26 summary:We formalize a simple but natural subclass of service domains for relationalplanning problems with object-centered, independent exogenous events andadditive rewards capturing, for example, problems in inventory control.Focusing on this subclass, we present a new symbolic planning algorithm whichis the first algorithm that has explicit performance guarantees for relationalMDPs with exogenous events. In particular, under some technical conditions, ourplanning algorithm provides a monotonic lower bound on the optimal valuefunction. To support this algorithm we present novel evaluation and reductiontechniques for generalized first order decision diagrams, a knowledgerepresentation for real-valued functions over relational world states. Ourplanning algorithm uses a set of focus states, which serves as a training set,to simplify and approximate the symbolic solution, and can thus be seen toperform learning for planning. A preliminary experimental evaluationdemonstrates the validity of our approach.
arxiv-3900-55 | Active Contour Models for Manifold Valued Image Segmentation | http://arxiv.org/abs/1306.6269 | author:Sumukh Bansal, Aditya Tatu category:cs.CV published:2013-06-26 summary:Image segmentation is the process of partitioning a image into differentregions or groups based on some characteristics like color, texture, motion orshape etc. Active contours is a popular variational method for objectsegmentation in images, in which the user initializes a contour which evolvesin order to optimize an objective function designed such that the desiredobject boundary is the optimal solution. Recently, imaging modalities thatproduce Manifold valued images have come up, for example, DT-MRI images, vectorfields. The traditional active contour model does not work on such images. Inthis paper, we generalize the active contour model to work on Manifold valuedimages. As expected, our algorithm detects regions with similar Manifold valuesin the image. Our algorithm also produces expected results on usual gray-scaleimages, since these are nothing but trivial examples of Manifold valued images.As another application of our general active contour model, we perform texturesegmentation on gray-scale images by first creating an appropriate Manifoldvalued image. We demonstrate segmentation results for manifold valued imagesand texture images.
arxiv-3900-56 | Near-Optimal Adaptive Compressed Sensing | http://arxiv.org/abs/1306.6239 | author:Matthew L. Malloy, Robert D. Nowak category:cs.IT math.IT stat.ML published:2013-06-26 summary:This paper proposes a simple adaptive sensing and group testing algorithm forsparse signal recovery. The algorithm, termed Compressive Adaptive Sense andSearch (CASS), is shown to be near-optimal in that it succeeds at the lowestpossible signal-to-noise-ratio (SNR) levels, improving on previous work inadaptive compressed sensing. Like traditional compressed sensing based onrandom non-adaptive design matrices, the CASS algorithm requires only k log nmeasurements to recover a k-sparse signal of dimension n. However, CASSsucceeds at SNR levels that are a factor log n less than required by standardcompressed sensing. From the point of view of constructing and implementing thesensing operation as well as computing the reconstruction, the proposedalgorithm is substantially less computationally intensive than standardcompressed sensing. CASS is also demonstrated to perform considerably better inpractice through simulation. To the best of our knowledge, this is the firstdemonstration of an adaptive compressed sensing algorithm with near-optimaltheoretical guarantees and excellent practical performance. This paper alsoshows that methods like compressed sensing, group testing, and pooling have anadvantage beyond simply reducing the number of measurements or tests --adaptive versions of such methods can also improve detection and estimationperformance when compared to non-adaptive direct (uncompressed) sensing.
arxiv-3900-57 | Persian Heritage Image Binarization Competition (PHIBC 2012) | http://arxiv.org/abs/1306.6263 | author:Seyed Morteza Ayatollahi, Hossein Ziaei Nafchi category:cs.CV published:2013-06-26 summary:The first competition on the binarization of historical Persian documents andmanuscripts (PHIBC 2012) has been organized in conjunction with the firstIranian conference on pattern recognition and image analysis (PRIA 2013). Themain objective of PHIBC 2012 is to evaluate performance of the binarizationmethodologies, when applied on the Persian heritage images. This paper providesa report on the methodology and performance of the three submitted algorithmsbased on evaluation measures has been used.
arxiv-3900-58 | OptShrink: An algorithm for improved low-rank signal matrix denoising by optimal, data-driven singular value shrinkage | http://arxiv.org/abs/1306.6042 | author:Raj Rao Nadakuditi category:math.ST cs.IT math.IT stat.ML stat.TH published:2013-06-25 summary:The truncated singular value decomposition (SVD) of the measurement matrix isthe optimal solution to the_representation_ problem of how to best approximatea noisy measurement matrix using a low-rank matrix. Here, we consider the(unobservable)_denoising_ problem of how to best approximate a low-rank signalmatrix buried in noise by optimal (re)weighting of the singular vectors of themeasurement matrix. We exploit recent results from random matrix theory toexactly characterize the large matrix limit of the optimal weightingcoefficients and show that they can be computed directly from data for a largeclass of noise models that includes the i.i.d. Gaussian noise case. Our analysis brings into sharp focus the shrinkage-and-thresholding form ofthe optimal weights, the non-convex nature of the associated shrinkage function(on the singular values) and explains why matrix regularization via singularvalue thresholding with convex penalty functions (such as the nuclear norm)will always be suboptimal. We validate our theoretical predictions withnumerical simulations, develop an implementable algorithm (OptShrink) thatrealizes the predicted performance gains and show how our methods can be usedto improve estimation in the setting where the measured matrix has missingentries.
arxiv-3900-59 | Design of an Agent for Answering Back in Smart Phones | http://arxiv.org/abs/1306.5884 | author:Sandeep Venkatesh, Meera V Patil, Nanditha Swamy category:cs.AI cs.HC cs.LG published:2013-06-25 summary:The objective of the paper is to design an agent which provides efficientresponse to the caller when a call goes unanswered in smartphones. The agentprovides responses through text messages, email etc stating the most likelyreason as to why the callee is unable to answer a call. Responses are composedtaking into consideration the importance of the present call and the situationthe callee is in at the moment like driving, sleeping, at work etc. The agentmakes decisons in the compostion of response messages based on the patterns ithas come across in the learning environment. Initially the user helps the agentto compose response messages. The agent associates this message to the perceptit recieves with respect to the environment the callee is in. The user maythereafter either choose to make to response system automatic or choose torecieve suggestions from the agent for responses messages and confirm what isto be sent to the caller.
arxiv-3900-60 | Constrained Optimization for a Subset of the Gaussian Parsimonious Clustering Models | http://arxiv.org/abs/1306.5824 | author:Ryan P. Browne, Sanjeena Subedi, Paul McNicholas category:stat.CO math.ST stat.ML stat.TH published:2013-06-25 summary:The expectation-maximization (EM) algorithm is an iterative method forfinding maximum likelihood estimates when data are incomplete or are treated asbeing incomplete. The EM algorithm and its variants are commonly used forparameter estimation in applications of mixture models for clustering andclassification. This despite the fact that even the Gaussian mixture modellikelihood surface contains many local maxima and is singularity riddled.Previous work has focused on circumventing this problem by constraining thesmallest eigenvalue of the component covariance matrices. In this paper, weconsider constraining the smallest eigenvalue, the largest eigenvalue, and boththe smallest and largest within the family setting. Specifically, a subset ofthe GPCM family is considered for model-based clustering, where we use are-parameterized version of the famous eigenvalue decomposition of thecomponent covariance matrices. Our approach is illustrated using variousexperiments with simulated and real data.
arxiv-3900-61 | Supersparse Linear Integer Models for Predictive Scoring Systems | http://arxiv.org/abs/1306.5860 | author:Berk Ustun, Stefano Traca, Cynthia Rudin category:stat.ML published:2013-06-25 summary:We introduce Supersparse Linear Integer Models (SLIM) as a tool to createscoring systems for binary classification. We derive theoretical bounds on thetrue risk of SLIM scoring systems, and present experimental results to showthat SLIM scoring systems are accurate, sparse, and interpretableclassification models.
arxiv-3900-62 | Fourier PCA and Robust Tensor Decomposition | http://arxiv.org/abs/1306.5825 | author:Navin Goyal, Santosh Vempala, Ying Xiao category:cs.LG cs.DS stat.ML published:2013-06-25 summary:Fourier PCA is Principal Component Analysis of a matrix obtained from higherorder derivatives of the logarithm of the Fourier transform of adistribution.We make this method algorithmic by developing a tensordecomposition method for a pair of tensors sharing the same vectors in rank-$1$decompositions. Our main application is the first provably polynomial-timealgorithm for underdetermined ICA, i.e., learning an $n \times m$ matrix $A$from observations $y=Ax$ where $x$ is drawn from an unknown productdistribution with arbitrary non-Gaussian components. The number of componentdistributions $m$ can be arbitrarily higher than the dimension $n$ and thecolumns of $A$ only need to satisfy a natural and efficiently verifiablenondegeneracy condition. As a second application, we give an alternativealgorithm for learning mixtures of spherical Gaussians with linearlyindependent means. These results also hold in the presence of Gaussian noise.
arxiv-3900-63 | DNA Reservoir Computing: A Novel Molecular Computing Approach | http://arxiv.org/abs/1306.5998 | author:Alireza Goudarzi, Matthew R. Lakin, Darko Stefanovic category:cs.NE cs.ET nlin.AO nlin.CD physics.bio-ph published:2013-06-25 summary:We propose a novel molecular computing approach based on reservoir computing.In reservoir computing, a dynamical core, called a reservoir, is perturbed withan external input signal while a readout layer maps the reservoir dynamics to atarget output. Computation takes place as a transformation from the input spaceto a high-dimensional spatiotemporal feature space created by the transientdynamics of the reservoir. The readout layer then combines these features toproduce the target output. We show that coupled deoxyribozyme oscillators canact as the reservoir. We show that despite using only three coupledoscillators, a molecular reservoir computer could achieve 90% accuracy on abenchmark temporal problem.
arxiv-3900-64 | On Parametric Modelling and Inference for Complex-Valued Time Series | http://arxiv.org/abs/1306.5993 | author:Adam M. Sykulski, Sofia C. Olhede, Jonathan M. Lilly, Jeffrey J. Early category:stat.ME stat.AP stat.CO stat.ML published:2013-06-25 summary:This paper introduces new parametric models for the autocovariance functionsand spectra of complex-valued time series, together with novel modifications offrequency-domain parameter inference methods appropriate for such time series.In particular, we introduce a version of the frequency-domain Whittlelikelihood for complex-valued processes. This represents a nontrivial extensionof the Whittle likelihood for bivariate real-valued processes, ascomplex-valued models can capture structure that is only evident by separatingnegative and positive frequency behaviour. Flexible inference methods for suchparametric models are proposed, and the properties of such methods are derived.The methods are applied to oceanographic and seismic time series, as examplesof naturally occurring sampled complex-valued time processes. We demonstratehow to reduce estimation bias of the Whittle likelihood caused by leakage andaliasing, improving parameter estimation of both real-valued andbivariate/complex-valued processes. We also provide techniques for testing theseries propriety or isotropy, as well as procedures for model choice andsemi-parametric inference.
arxiv-3900-65 | Learning, Generalization, and Functional Entropy in Random Automata Networks | http://arxiv.org/abs/1306.6041 | author:Alireza Goudarzi, Christof Teuscher, Natali Gulbahce, Thimo Rohlf category:cs.NE nlin.AO nlin.CD physics.bio-ph published:2013-06-25 summary:It has been shown \citep{broeck90:physicalreview,patarnello87:europhys} thatfeedforward Boolean networks can learn to perform specific simple tasks andgeneralize well if only a subset of the learning examples is provided forlearning. Here, we extend this body of work and show experimentally that randomBoolean networks (RBNs), where both the interconnections and the Booleantransfer functions are chosen at random initially, can be evolved by using astate-topology evolution to solve simple tasks. We measure the learning andgeneralization performance, investigate the influence of the average nodeconnectivity $K$, the system size $N$, and introduce a new measure that allowsto better describe the network's learning and generalization behavior. We showthat the connectivity of the maximum entropy networks scales as a power-law ofthe system size $N$. Our results show that networks with higher averageconnectivity $K$ (supercritical) achieve higher memorization and partialgeneralization. However, near critical connectivity, the networks show a higherperfect generalization on the even-odd task.
arxiv-3900-66 | A Computational Approach to Politeness with Application to Social Factors | http://arxiv.org/abs/1306.6078 | author:Cristian Danescu-Niculescu-Mizil, Moritz Sudhof, Dan Jurafsky, Jure Leskovec, Christopher Potts category:cs.CL cs.SI physics.soc-ph I.2.7 published:2013-06-25 summary:We propose a computational framework for identifying linguistic aspects ofpoliteness. Our starting point is a new corpus of requests annotated forpoliteness, which we use to evaluate aspects of politeness theory and touncover new interactions between politeness markers and context. These findingsguide our construction of a classifier with domain-independent lexical andsyntactic features operationalizing key components of politeness theory, suchas indirection, deference, impersonalization and modality. Our classifierachieves close to human performance and is effective across domains. We use ourframework to study the relationship between politeness and social power,showing that polite Wikipedia editors are more likely to achieve high statusthrough elections, but, once elevated, they become less polite. We see asimilar negative correlation between politeness and power on Stack Exchange,where users at the top of the reputation scale are less polite than those atthe bottom. Finally, we apply our classifier to a preliminary analysis ofpoliteness variation by gender and community.
arxiv-3900-67 | A maximal-information color to gray conversion method for document images: Toward an optimal grayscale representation for document image binarization | http://arxiv.org/abs/1306.6058 | author:Reza Farrahi Moghaddam, Shaohua Chen, Rachid Hedjam, Mohamed Cheriet category:cs.CV published:2013-06-25 summary:A novel method to convert color/multi-spectral images to gray-level images isintroduced to increase the performance of document binarization methods. Themethod uses the distribution of the pixel data of the input document image in acolor space to find a transformation, called the dual transform, which balancesthe amount of information on all color channels. Furthermore, in order toreduce the intensity variations on the gray output, a color reductionpreprocessing step is applied. Then, a channel is selected as the gray valuerepresentation of the document image based on the homogeneity criterion on thetext regions. In this way, the proposed method can provide aluminance-independent contrast enhancement. The performance of the method isevaluated against various images from two databases, the ICDAR'03 RobustReading, the KAIST and the DIBCO'09 datasets, subjectively and objectively withpromising results. The ground truth images for the images from the ICDAR'03Robust Reading dataset have been created manually by the authors.
arxiv-3900-68 | A Randomized Nonmonotone Block Proximal Gradient Method for a Class of Structured Nonlinear Programming | http://arxiv.org/abs/1306.5918 | author:Zhaosong Lu, Lin Xiao category:math.OC cs.LG cs.NA math.NA stat.ML published:2013-06-25 summary:We propose a randomized nonmonotone block proximal gradient (RNBPG) methodfor minimizing the sum of a smooth (possibly nonconvex) function and ablock-separable (possibly nonconvex nonsmooth) function. At each iteration,this method randomly picks a block according to any prescribed probabilitydistribution and solves typically several associated proximal subproblems thatusually have a closed-form solution, until a certain progress on objectivevalue is achieved. In contrast to the usual randomized block coordinate descentmethod [23,20], our method has a nonmonotone flavor and uses variable stepsizesthat can partially utilize the local curvature information of the smoothcomponent of objective function. We show that any accumulation point of thesolution sequence of the method is a stationary point of the problem {\italmost surely} and the method is capable of finding an approximate stationarypoint with high probability. We also establish a sublinear rate of convergencefor the method in terms of the minimal expected squared norm of certainproximal gradients over the iterations. When the problem under consideration isconvex, we show that the expected objective values generated by RNBPG convergeto the optimal value of the problem. Under some assumptions, we furtherestablish a sublinear and linear rate of convergence on the expected objectivevalues generated by a monotone version of RNBPG. Finally, we conduct somepreliminary experiments to test the performance of RNBPG on the$\ell_1$-regularized least-squares problem and a dual SVM problem in machinelearning. The computational results demonstrate that our method substantiallyoutperforms the randomized block coordinate {\it descent} method with fixed orvariable stepsizes.
arxiv-3900-69 | Correlated random features for fast semi-supervised learning | http://arxiv.org/abs/1306.5554 | author:Brian McWilliams, David Balduzzi, Joachim M. Buhmann category:stat.ML cs.LG published:2013-06-24 summary:This paper presents Correlated Nystrom Views (XNV), a fast semi-supervisedalgorithm for regression and classification. The algorithm draws on two mainideas. First, it generates two views consisting of computationally inexpensiverandom features. Second, XNV applies multiview regression using CanonicalCorrelation Analysis (CCA) on unlabeled data to bias the regression towardsuseful features. It has been shown that, if the views contains accurateestimators, CCA regression can substantially reduce variance with a minimalincrease in bias. Random views are justified by recent theoretical andempirical work showing that regression with random features closelyapproximates kernel regression, implying that random views can be expected tocontain accurate estimators. We show that XNV consistently outperforms astate-of-the-art algorithm for semi-supervised learning: substantiallyimproving predictive performance and reducing the variability of performance ona wide variety of real-world datasets, whilst also reducing runtime by ordersof magnitude.
arxiv-3900-70 | Using Genetic Programming to Model Software | http://arxiv.org/abs/1306.5667 | author:W. B. Langdon, M. Harman category:cs.NE cs.AI published:2013-06-24 summary:We study a generic program to investigate the scope for automaticallycustomising it for a vital current task, which was not considered when it wasfirst written. In detail, we show genetic programming (GP) can evolve models ofaspects of BLAST's output when it is used to map Solexa Next-Gen DNA sequencesto the human genome.
arxiv-3900-71 | Deep Learning by Scattering | http://arxiv.org/abs/1306.5532 | author:Stéphane Mallat, Irène Waldspurger category:cs.LG stat.ML published:2013-06-24 summary:We introduce general scattering transforms as mathematical models of deepneural networks with l2 pooling. Scattering networks iteratively apply complexvalued unitary operators, and the pooling is performed by a complex modulus. Anexpected scattering defines a contractive representation of a high-dimensionalprobability distribution, which preserves its mean-square norm. We show thatunsupervised learning can be casted as an optimization of the space contractionto preserve the volume occupied by unlabeled examples, at each layer of thenetwork. Supervised learning and classification are performed with an averagedscattering, which provides scattering estimations for multiple classes.
arxiv-3900-72 | Spectral redemption: clustering sparse networks | http://arxiv.org/abs/1306.5550 | author:Florent Krzakala, Cristopher Moore, Elchanan Mossel, Joe Neeman, Allan Sly, Lenka Zdeborová, Pan Zhang category:cs.SI physics.soc-ph stat.ML published:2013-06-24 summary:Spectral algorithms are classic approaches to clustering and communitydetection in networks. However, for sparse networks the standard versions ofthese algorithms are suboptimal, in some cases completely failing to detectcommunities even when other algorithms such as belief propagation can do so.Here we introduce a new class of spectral algorithms based on anon-backtracking walk on the directed edges of the graph. The spectrum of thisoperator is much better-behaved than that of the adjacency matrix or othercommonly used matrices, maintaining a strong separation between the bulkeigenvalues and the eigenvalues relevant to community structure even in thesparse case. We show that our algorithm is optimal for graphs generated by thestochastic block model, detecting communities all the way down to thetheoretical limit. We also show the spectrum of the non-backtracking operatorfor some real-world networks, illustrating its advantages over traditionalspectral clustering.
arxiv-3900-73 | A State-Space Approach for Optimal Traffic Monitoring via Network Flow Sampling | http://arxiv.org/abs/1306.5793 | author:Michael Kallitsis, Stilian Stoev, George Michailidis category:cs.SY cs.NI stat.AP stat.ML published:2013-06-24 summary:The robustness and integrity of IP networks require efficient tools fortraffic monitoring and analysis, which scale well with traffic volume andnetwork size. We address the problem of optimal large-scale flow monitoring ofcomputer networks under resource constraints. We propose a stochasticoptimization framework where traffic measurements are done by exploiting thespatial (across network links) and temporal relationship of traffic flows.Specifically, given the network topology, the state-space characterization ofnetwork flows and sampling constraints at each monitoring station, we seek anoptimal packet sampling strategy that yields the best traffic volume estimationfor all flows of the network. The optimal sampling design is the result of aconcave minimization problem; then, Kalman filtering is employed to yield asequence of traffic estimates for each network flow. We evaluate our algorithmusing real-world Internet2 data.
arxiv-3900-74 | Synthesizing Manipulation Sequences for Under-Specified Tasks using Unrolled Markov Random Fields | http://arxiv.org/abs/1306.5707 | author:Jaeyong Sung, Bart Selman, Ashutosh Saxena category:cs.RO cs.AI cs.LG published:2013-06-24 summary:Many tasks in human environments require performing a sequence of navigationand manipulation steps involving objects. In unstructured human environments,the location and configuration of the objects involved often change inunpredictable ways. This requires a high-level planning strategy that is robustand flexible in an uncertain environment. We propose a novel dynamic planningstrategy, which can be trained from a set of example sequences. High leveltasks are expressed as a sequence of primitive actions or controllers (withappropriate parameters). Our score function, based on Markov Random Field(MRF), captures the relations between environment, controllers, and theirarguments. By expressing the environment using sets of attributes, the approachgeneralizes well to unseen scenarios. We train the parameters of our MRF usinga maximum margin learning method. We provide a detailed empirical validation ofour overall framework demonstrating successful plan strategies for a variety oftasks.
arxiv-3900-75 | Modeling The Stable Operating Envelope For Partially Stable Combustion Engines Using Class Imbalance Learning | http://arxiv.org/abs/1306.5702 | author:Vijay Manikandan Janakiraman, XuanLong Nguyen, Jeff Sterniak, Dennis Assanis category:cs.NE published:2013-06-24 summary:Advanced combustion technologies such as homogeneous charge compressionignition (HCCI) engines have a narrow stable operating region defined bycomplex control strategies such as exhaust gas recirculation (EGR) and variablevalve timing among others. For such systems, it is important to identify theoperating envelope or the boundary of stable operation for diagnostics andcontrol purposes. Obtaining a good model of the operating envelope usingphysics becomes intractable owing to engine transient effects. In this paper, amachine learning based approach is employed to identify the stable operatingboundary of HCCI combustion directly from experimental data. Owing to imbalancein class proportions in the data, two approaches are considered. A re-sampling(under-sampling, over-sampling) based approach is used to develop models usingexisting algorithms while a cost-sensitive approach is used to modify thelearning algorithm without modifying the data set. Support vector machines andrecently developed extreme learning machines are used for model development andresults compared against linear classification methods show that cost-sensitiveversions of ELM and SVM algorithms are well suited to model the HCCI operatingenvelope. The prediction results indicate that the models have the potential tobe used for predicting HCCI instability based on sensor measurement history.
arxiv-3900-76 | Model Reframing by Feature Context Change | http://arxiv.org/abs/1306.5487 | author:Celestine-Periale Ma category:cs.LG published:2013-06-23 summary:The feature space (including both input and output variables) characterises adata mining problem. In predictive (supervised) problems, the quality andavailability of features determines the predictability of the dependentvariable, and the performance of data mining models in terms ofmisclassification or regression error. Good features, however, are usuallydifficult to obtain. It is usual that many instances come with missing values,either because the actual value for a given attribute was not available orbecause it was too expensive. This is usually interpreted as a utility orcost-sensitive learning dilemma, in this case between misclassification (orregression error) costs and attribute tests costs. Both misclassification cost(MC) and test cost (TC) can be integrated into a single measure, known as jointcost (JC). We introduce methods and plots (such as the so-called JROC plots)that can work with any of-the-shelf predictive technique, including ensembles,such that we re-frame the model to use the appropriate subset of attributes(the feature configuration) during deployment time. In other words, models aretrained with the available attributes (once and for all) and then deployed bysetting missing values on the attributes that are deemed ineffective forreducing the joint cost. As the number of feature configuration combinationsgrows exponentially with the number of features we introduce quadratic methodsthat are able to approximate the optimal configuration and model choices, asshown by the experimental results.
arxiv-3900-77 | Exploiting Data Parallelism in the yConvex Hypergraph Algorithm for Image Representation using GPGPUs | http://arxiv.org/abs/1307.2560 | author:Saurabh Jha, Tejaswi Agarwal, B. Rajesh Kanna category:cs.DC cs.CV I.3 published:2013-06-23 summary:To define and identify a region-of-interest (ROI) in a digital image, theshape descriptor of the ROI has to be described in terms of its boundarycharacteristics. To address the generic issues of contour tracking, the yConvexHypergraph (yCHG) model was proposed by Kanna et al [1]. In this work, wepropose a parallel approach to implement the yCHG model by exploiting massivelyparallel cores of NVIDIA's Compute Unified Device Architecture (CUDA). Weperform our experiments on the MODIS satellite image database by NASA, andbased on our analysis we observe that the performance of the serialimplementation is better on smaller images, but once the threshold is achievedin terms of image resolution, the parallel implementation outperforms itssequential counterpart by 2 to 10 times (2x-10x). We also conclude that anincrease in the number of hyperedges in the ROI of a given size does not impactthe performance of the overall algorithm.
arxiv-3900-78 | A Statistical Perspective on Algorithmic Leveraging | http://arxiv.org/abs/1306.5362 | author:Ping Ma, Michael W. Mahoney, Bin Yu category:stat.ME cs.LG stat.ML published:2013-06-23 summary:One popular method for dealing with large-scale data sets is sampling. Forexample, by using the empirical statistical leverage scores as an importancesampling distribution, the method of algorithmic leveraging samples andrescales rows/columns of data matrices to reduce the data size beforeperforming computations on the subproblem. This method has been successful inimproving computational efficiency of algorithms for matrix problems such asleast-squares approximation, least absolute deviations approximation, andlow-rank matrix approximation. Existing work has focused on algorithmic issuessuch as worst-case running times and numerical issues associated with providinghigh-quality implementations, but none of it addresses statistical aspects ofthis method. In this paper, we provide a simple yet effective framework to evaluate thestatistical properties of algorithmic leveraging in the context of estimatingparameters in a linear regression model with a fixed number of predictors. Weshow that from the statistical perspective of bias and variance, neitherleverage-based sampling nor uniform sampling dominates the other. This resultis particularly striking, given the well-known result that, from thealgorithmic perspective of worst-case analysis, leverage-based samplingprovides uniformly superior worst-case algorithmic results, when compared withuniform sampling. Based on these theoretical results, we propose and analyzetwo new leveraging algorithms. A detailed empirical evaluation of existingleverage-based methods as well as these two new methods is carried out on bothsynthetic and real data sets. The empirical results indicate that our theory isa good predictor of practical performance of existing and new leverage-basedalgorithms and that the new algorithms achieve improved performance.
arxiv-3900-79 | Characterizing Ambiguity in Light Source Invariant Shape from Shading | http://arxiv.org/abs/1306.5480 | author:Benjamin Kunsberg, Steven W. Zucker category:cs.CV q-bio.NC published:2013-06-23 summary:Shape from shading is a classical inverse problem in computer vision. Thisshape reconstruction problem is inherently ill-defined; it depends on theassumed light source direction. We introduce a novel mathematical formulationfor calculating local surface shape based on covariant derivatives of theshading flow field, rather than the customary integral minimization or P.D.Eapproaches. On smooth surfaces, we show second derivatives of brightness areindependent of the light sources and can be directly related to surfaceproperties. We use these measurements to define the matching local family ofsurfaces that can result from any given shading patch, changing the emphasis tocharacterizing ambiguity in the problem. We give an example of how these localsurface ambiguities collapse along certain image contours and how this can beused for the reconstruction problem.
arxiv-3900-80 | A Variational Approximations-DIC Rubric for Parameter Estimation and Mixture Model Selection Within a Family Setting | http://arxiv.org/abs/1306.5368 | author:Sanjeena Subedi, Paul McNicholas category:stat.ME stat.CO stat.ML published:2013-06-23 summary:Mixture model-based clustering has become an increasingly popular dataanalysis technique since its introduction fifty years ago, and is now commonlyutilized within the family setting. Families of mixture models arise when thecomponent parameters, usually the component covariance matrices, are decomposedand a number of constraints are imposed. Within the family setting, we need tochoose the member of the family, i.e., the appropriate covariance structure, inaddition to the number of mixture components. To date, the Bayesian informationcriterion (BIC) has proven most effective for model selection, and theexpectation-maximization (EM) algorithm is usually used for parameterestimation. To date, this EM-BIC rubric has monopolized the literature onfamilies of mixture models. We deviate from this rubric, using variationalBayes approximations for parameter estimation and the deviance informationcriterion for model selection. The variational Bayes approach alleviates someof the computational complexities associated with the EM algorithm byconstructing a tight lower bound on the complex marginal likelihood andmaximizing this lower bound by minimizing the associated Kullback-Leiblerdivergence. We use this approach on the most famous family of Gaussian mixturemodels within the literature, and real and simulated data are used to compareour approach to the EM-BIC rubric.
arxiv-3900-81 | P-HGRMS: A Parallel Hypergraph Based Root Mean Square Algorithm for Image Denoising | http://arxiv.org/abs/1306.5390 | author:Tejaswi Agarwal, Saurabh Jha, B. Rajesh Kanna category:cs.DC cs.CV I.3 published:2013-06-23 summary:This paper presents a parallel Salt and Pepper (SP) noise removal algorithmin a grey level digital image based on the Hypergraph Based Root Mean Square(HGRMS) approach. HGRMS is generic algorithm for identifying noisy pixels inany digital image using a two level hierarchical serial approach. However, forSP noise removal, we reduce this algorithm to a parallel model by introducing acardinality matrix and an iteration factor, k, which helps us reduce thedependencies in the existing approach. We also observe that the performance ofthe serial implementation is better on smaller images, but once the thresholdis achieved in terms of image resolution, its computational complexityincreases drastically. We test P-HGRMS using standard images from the BerkeleySegmentation dataset on NVIDIAs Compute Unified Device Architecture (CUDA) fornoise identification and attenuation. We also compare the noise removalefficiency of the proposed algorithm using Peak Signal to Noise Ratio (PSNR) tothe existing approach. P-HGRMS maintains the noise removal efficiency andoutperforms its sequential counterpart by 6 to 18 times (6x - 18x) incomputational efficiency.
arxiv-3900-82 | Song-based Classification techniques for Endangered Bird Conservation | http://arxiv.org/abs/1306.5349 | author:Erick Stattner, Wilfried Segretier, Martine Collard, Philippe Hunel, Nicolas Vidot category:cs.LG published:2013-06-22 summary:The work presented in this paper is part of a global framework which longterm goal is to design a wireless sensor network able to support theobservation of a population of endangered birds. We present the first stage forwhich we have conducted a knowledge discovery approach on a sample ofacoustical data. We use MFCC features extracted from bird songs and we exploittwo knowledge discovery techniques. One that relies on clustering-basedapproaches, that highlights the homogeneity in the songs of the species. Theother, based on predictive modeling, that demonstrates the good performances ofvarious machine learning techniques for the identification process. Theknowledge elicited provides promising results to consider a widespread studyand to elicit guidelines for designing a first version of the automaticapproach for data collection based on acoustic sensors.
arxiv-3900-83 | Online dictionary learning for kernel LMS. Analysis and forward-backward splitting algorithm | http://arxiv.org/abs/1306.5310 | author:Wei Gao, Jie Chen, Cédric Richard, Jianguo Huang category:stat.ML published:2013-06-22 summary:Adaptive filtering algorithms operating in reproducing kernel Hilbert spaceshave demonstrated superiority over their linear counterpart for nonlinearsystem identification. Unfortunately, an undesirable characteristic of thesemethods is that the order of the filters grows linearly with the number ofinput data. This dramatically increases the computational burden and memoryrequirement. A variety of strategies based on dictionary learning have beenproposed to overcome this severe drawback. Few, if any, of these works analyzethe problem of updating the dictionary in a time-varying environment. In thispaper, we present an analytical study of the convergence behavior of theGaussian least-mean-square algorithm in the case where the statistics of thedictionary elements only partially match the statistics of the input data. Thisallows us to emphasize the need for updating the dictionary in an online way,by discarding the obsolete elements and adding appropriate ones. We introduce akernel least-mean-square algorithm with L1-norm regularization to automaticallyperform this task. The stability in the mean of this method is analyzed, andits performance is tested with experiments.
arxiv-3900-84 | New Approach of Estimating PSNR-B For De-blocked Images | http://arxiv.org/abs/1306.5293 | author:S. Aruna Mastani, K. Shilpa category:cs.CV published:2013-06-22 summary:Measurement of image quality is very crucial to many image processingapplications. Quality metrics are used to measure the quality of improvement inthe images after they are processed and compared with the original images.Compression is one of the applications where it is required to monitor thequality of decompressed or decoded image. JPEG compression is the lossycompression which is most prevalent technique for image codecs. But it suffersfrom blocking artifacts. Various deblocking filters are used to reduce blockingartifacts. The efficiency of deblocking filters which improves visual signalsdegraded by blocking artifacts from compression will also be studied. Objectivequality metrics like PSNR, SSIM, and PSNRB for analyzing the quality ofdeblocked images will be studied. We introduce a new approach of PSNR-B foranalyzing quality of deblocked images. Simulation results show that newapproach of PSNR-B called modified PSNR-B. it gives even better resultscompared to existing well known blockiness specific indices
arxiv-3900-85 | Cognitive Interpretation of Everyday Activities: Toward Perceptual Narrative Based Visuo-Spatial Scene Interpretation | http://arxiv.org/abs/1306.5308 | author:Mehul Bhatt, Jakob Suchan, Carl Schultz category:cs.AI cs.CV cs.HC cs.RO published:2013-06-22 summary:We position a narrative-centred computational model for high-level knowledgerepresentation and reasoning in the context of a range of assistivetechnologies concerned with "visuo-spatial perception and cognition" tasks. Ourproposed narrative model encompasses aspects such as \emph{space, events,actions, change, and interaction} from the viewpoint of commonsense reasoningand learning in large-scale cognitive systems. The broad focus of this paper ison the domain of "human-activity interpretation" in smart environments, ambientintelligence etc. In the backdrop of a "smart meeting cinematography" domain,we position the proposed narrative model, preliminary work on perceptualnarrativisation, and the immediate outlook on constructing general-purposeopen-source tools for perceptual narrativisation. ACM Classification: I.2 Artificial Intelligence: I.2.0 General -- CognitiveSimulation, I.2.4 Knowledge Representation Formalisms and Methods, I.2.10Vision and Scene Understanding: Architecture and control structures, Motion,Perceptual reasoning, Shape, Video analysis General keywords: cognitive systems; human-computer interaction; spatialcognition and computation; commonsense reasoning; spatial and temporalreasoning; assistive technologies
arxiv-3900-86 | Fine-Grained Visual Classification of Aircraft | http://arxiv.org/abs/1306.5151 | author:Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, Andrea Vedaldi category:cs.CV published:2013-06-21 summary:This paper introduces FGVC-Aircraft, a new dataset containing 10,000 imagesof aircraft spanning 100 aircraft models, organised in a three-level hierarchy.At the finer level, differences between models are often subtle but alwaysvisually measurable, making visual recognition challenging but possible. Abenchmark is obtained by defining corresponding classification tasks andevaluation protocols, and baseline results are presented. The construction ofthis dataset was made possible by the work of aircraft enthusiasts, a strategythat can extend to the study of number of other object classes. Compared to thedomains usually considered in fine-grained visual classification (FGVC), forexample animals, aircraft are rigid and hence less deformable. They, however,present other interesting modes of variation, including purpose, size,designation, structure, historical style, and branding.
arxiv-3900-87 | Computer Aided ECG Analysis - State of the Art and Upcoming Challenges | http://arxiv.org/abs/1306.5096 | author:Marko Velic, Ivan Padavic, Sinisa Car category:cs.CV published:2013-06-21 summary:In this paper we present current achievements in computer aided ECG analysisand their applicability in real world medical diagnosis process. Most of thecurrent work is covering problems of removing noise, detecting heartbeats andrhythm-based analysis. There are some advancements in particular ECG segmentsdetection and beat classifications but with limited evaluations and withoutclinical approvals. This paper presents state of the art advancements in thoseareas till present day. Besides this short computer science and signalprocessing literature review, paper covers future challenges regarding the ECGsignal morphology analysis deriving from the medical literature review. Paperis concluded with identified gaps in current advancements and testing, upcomingchallenges for future research and a bullseye test is suggested for morphologyanalysis evaluation.
arxiv-3900-88 | Discriminative Training: Learning to Describe Video with Sentences, from Video Described with Sentences | http://arxiv.org/abs/1306.5263 | author:Haonan Yu, Jeffrey Mark Siskind category:cs.CV cs.CL published:2013-06-21 summary:We present a method for learning word meanings from complex and realisticvideo clips by discriminatively training (DT) positive sentential labelsagainst negative ones, and then use the trained word models to generatesentential descriptions for new video. This new work is inspired by recent workwhich adopts a maximum likelihood (ML) framework to address the same problemusing only positive sentential labels. The new method, like the ML-based one,is able to automatically determine which words in the sentence correspond towhich concepts in the video (i.e., ground words to meanings) in a weaklysupervised fashion. While both DT and ML yield comparable results withsufficient training data, DT outperforms ML significantly with smaller trainingsets because it can exploit negative training labels to better constrain thelearning problem.
arxiv-3900-89 | Clinical Relationships Extraction Techniques from Patient Narratives | http://arxiv.org/abs/1306.5170 | author:Wafaa Tawfik Abdel-moneim, Mohamed Hashem Abdel-Aziz, Mohamed Monier Hassan category:cs.IR cs.CL published:2013-06-21 summary:The Clinical E-Science Framework (CLEF) project was used to extract importantinformation from medical texts by building a system for the purpose of clinicalresearch, evidence-based healthcare and genotype-meets-phenotype informatics.The system is divided into two parts, one part concerns with the identificationof relationships between clinically important entities in the text. The fullparses and domain-specific grammars had been used to apply many approaches toextract the relationship. In the second part of the system, statistical machinelearning (ML) approaches are applied to extract relationship. A corpus ofoncology narratives that hand annotated with clinical relationships can be usedto train and test a system that has been designed and implemented by supervisedmachine learning (ML) approaches. Many features can be extracted from thesetexts that are used to build a model by the classifier. Multiple supervisedmachine learning algorithms can be applied for relationship extraction. Effectsof adding the features, changing the size of the corpus, and changing the typeof the algorithm on relationship extraction are examined. Keywords: Textmining; information extraction; NLP; entities; and relations.
arxiv-3900-90 | 3-SAT Problem A New Memetic-PSO Algorithm | http://arxiv.org/abs/1306.5070 | author:Nasser Lotfi, Jamshid Tamouk, Mina Farmanbar category:cs.AI cs.NE published:2013-06-21 summary:3-SAT problem is of great importance to many technical and scientificapplications. This paper presents a new hybrid evolutionary algorithm forsolving this satisfiability problem. 3-SAT problem has the huge search spaceand hence it is known as a NP-hard problem. So, deterministic approaches arenot applicable in this context. Thereof, application of evolutionary processingapproaches and especially PSO will be very effective for solving these kinds ofproblems. In this paper, we introduce a new evolutionary optimization techniquebased on PSO, Memetic algorithm and local search approaches. When someheuristics are mixed, their advantages are collected as well and we can reachto the better outcomes. Finally, we test our proposed algorithm over somebenchmarks used by some another available algorithms. Obtained results showthat our new method leads to the suitable results by the appropriate time.Thereby, it achieves a better result in compared with the existent approachessuch as pure genetic algorithm and some verified types
arxiv-3900-91 | Class Proportion Estimation with Application to Multiclass Anomaly Rejection | http://arxiv.org/abs/1306.5056 | author:Tyler Sanderson, Clayton Scott category:stat.ML cs.LG published:2013-06-21 summary:This work addresses two classification problems that fall under the headingof domain adaptation, wherein the distributions of training and testingexamples differ. The first problem studied is that of class proportionestimation, which is the problem of estimating the class proportions in anunlabeled testing data set given labeled examples of each class. Compared toprevious work on this problem, our approach has the novel feature that it doesnot require labeled training data from one of the classes. This property allowsus to address the second domain adaptation problem, namely, multiclass anomalyrejection. Here, the goal is to design a classifier that has the option ofassigning a "reject" label, indicating that the instance did not arise from aclass present in the training data. We establish consistent learning strategiesfor both of these domain adaptation problems, which to our knowledge are thefirst of their kind. We also implement the class proportion estimationtechnique and demonstrate its performance on several benchmark data sets.
arxiv-3900-92 | Global registration of multiple point clouds using semidefinite programming | http://arxiv.org/abs/1306.5226 | author:Kunal N. Chaudhury, Yuehaw Khoo, Amit Singer category:cs.CV cs.NA math.NA math.OC published:2013-06-21 summary:Consider $N$ points in $\mathbb{R}^d$ and $M$ local coordinate systems thatare related through unknown rigid transforms. For each point we are given(possibly noisy) measurements of its local coordinates in some of thecoordinate systems. Alternatively, for each coordinate system, we observe thecoordinates of a subset of the points. The problem of estimating the globalcoordinates of the $N$ points (up to a rigid transform) from such measurementscomes up in distributed approaches to molecular conformation and sensor networklocalization, and also in computer vision and graphics. The least-squares formulation of this problem, though non-convex, has a wellknown closed-form solution when $M=2$ (based on the singular valuedecomposition). However, no closed form solution is known for $M\geq 3$. In this paper, we demonstrate how the least-squares formulation can berelaxed into a convex program, namely a semidefinite program (SDP). By settingup connections between the uniqueness of this SDP and results from rigiditytheory, we prove conditions for exact and stable recovery for the SDPrelaxation. In particular, we prove that the SDP relaxation can guaranteerecovery under more adversarial conditions compared to earlier proposedspectral relaxations, and derive error bounds for the registration errorincurred by the SDP relaxation. We also present results of numerical experiments on simulated data to confirmthe theoretical findings. We empirically demonstrate that (a) unlike thespectral relaxation, the relaxation gap is mostly zero for the semidefiniteprogram (i.e., we are able to solve the original non-convex least-squaresproblem) up to a certain noise threshold, and (b) the semidefinite programperforms significantly better than spectral and manifold-optimization methods,particularly at large noise levels.
arxiv-3900-93 | Supervised Topical Key Phrase Extraction of News Stories using Crowdsourcing, Light Filtering and Co-reference Normalization | http://arxiv.org/abs/1306.4886 | author:Luis Marujo, Anatole Gershman, Jaime Carbonell, Robert Frederking, João P. Neto category:cs.CL cs.IR published:2013-06-20 summary:Fast and effective automated indexing is critical for search and personalizedservices. Key phrases that consist of one or more words and represent the mainconcepts of the document are often used for the purpose of indexing. In thispaper, we investigate the use of additional semantic features andpre-processing steps to improve automatic key phrase extraction. These featuresinclude the use of signal words and freebase categories. Some of these featureslead to significant improvements in the accuracy of the results. We alsoexperimented with 2 forms of document pre-processing that we call lightfiltering and co-reference normalization. Light filtering removes sentencesfrom the document, which are judged peripheral to its main content.Co-reference normalization unifies several written forms of the same namedentity into a unique form. We also needed a "Gold Standard" - a set of labeleddocuments for training and evaluation. While the subjective nature of keyphrase selection precludes a true "Gold Standard", we used Amazon's MechanicalTurk service to obtain a useful approximation. Our data indicates that thebiggest improvements in performance were due to shallow semantic features, newscategories, and rhetorical signals (nDCG 78.47% vs. 68.93%). The inclusion ofdeeper semantic features such as Freebase sub-categories was not beneficial byitself, but in combination with pre-processing, did cause slight improvementsin the nDCG scores.
arxiv-3900-94 | Recognition of Named-Event Passages in News Articles | http://arxiv.org/abs/1306.4908 | author:Luis Marujo, Wang Ling, Anatole Gershman, Jaime Carbonell, João P. Neto, David Matos category:cs.CL cs.IR published:2013-06-20 summary:We extend the concept of Named Entities to Named Events - commonly occurringevents such as battles and earthquakes. We propose a method for findingspecific passages in news articles that contain information about such eventsand report our preliminary evaluation results. Collecting "Gold Standard" datapresents many problems, both practical and conceptual. We present a method forobtaining such data using the Amazon Mechanical Turk service.
arxiv-3900-95 | Determining Points on Handwritten Mathematical Symbols | http://arxiv.org/abs/1306.4966 | author:Rui Hu, Stephen M. Watt category:cs.CV cs.CY published:2013-06-20 summary:In a variety of applications, such as handwritten mathematics and diagramlabelling, it is common to have symbols of many different sizes in use and forthe writing not to follow simple baselines. In order to understand the scaleand relative positioning of individual characters, it is necessary to identifythe location of certain expected features. These are typically identified byparticular points in the symbols, for example, the baseline of a lower case "p"would be identified by the lowest part of the bowl, ignoring the descender. Weinvestigate how to find these special points automatically so they may be usedin a number of problems, such as improving two-dimensional mathematicalrecognition and in handwriting neatening, while preserving the original style.
arxiv-3900-96 | Evolving Boolean Regulatory Networks with Epigenetic Control | http://arxiv.org/abs/1306.4793 | author:Larry Bull category:cs.NE q-bio.MN published:2013-06-20 summary:The significant role of epigenetic mechanisms within natural systems hasbecome increasingly clear. This paper uses a recently presented abstract,tunable Boolean genetic regulatory network model to explore aspects ofepigenetics. It is shown how dynamically controlling transcription via a DNAmethylation-inspired mechanism can be selected for by simulated evolution undervarious single and multiple cell scenarios. Further, it is shown that theeffects of such control can be inherited without detriment to fitness.
arxiv-3900-97 | Optimal computational and statistical rates of convergence for sparse nonconvex learning problems | http://arxiv.org/abs/1306.4960 | author:Zhaoran Wang, Han Liu, Tong Zhang category:stat.ML published:2013-06-20 summary:We provide theoretical analysis of the statistical and computationalproperties of penalized $M$-estimators that can be formulated as the solutionto a possibly nonconvex optimization problem. Many important estimators fall inthis category, including least squares regression with nonconvexregularization, generalized linear models with nonconvex regularization andsparse elliptical random design regression. For these problems, it isintractable to calculate the global solution due to the nonconvex formulation.In this paper, we propose an approximate regularization path-following methodfor solving a variety of learning problems with nonconvex objective functions.Under a unified analytic framework, we simultaneously provide explicitstatistical and computational rates of convergence for any local solutionattained by the algorithm. Computationally, our algorithm attains a globalgeometric rate of convergence for calculating the full regularization path,which is optimal among all first-order algorithms. Unlike most existing methodsthat only attain geometric rates of convergence for one single regularizationparameter, our algorithm calculates the full regularization path with the sameiteration complexity. In particular, we provide a refined iteration complexitybound to sharply characterize the performance of each stage along theregularization path. Statistically, we provide sharp sample complexity analysisfor all the approximate local solutions along the regularization path. Inparticular, our analysis improves upon existing results by providing a morerefined sample complexity bound as well as an exact support recovery result forthe final estimator. These results show that the final estimator attains anoracle statistical property due to the usage of nonconvex penalty.
arxiv-3900-98 | Key Phrase Extraction of Lightly Filtered Broadcast News | http://arxiv.org/abs/1306.4890 | author:Luis Marujo, Ricardo Ribeiro, David Martins de Matos, João P. Neto, Anatole Gershman, Jaime Carbonell category:cs.CL cs.IR published:2013-06-20 summary:This paper explores the impact of light filtering on automatic key phraseextraction (AKE) applied to Broadcast News (BN). Key phrases are words andexpressions that best characterize the content of a document. Key phrases areoften used to index the document or as features in further processing. Thismakes improvements in AKE accuracy particularly important. We hypothesized thatfiltering out marginally relevant sentences from a document would improve AKEaccuracy. Our experiments confirmed this hypothesis. Elimination of as littleas 10% of the document sentences lead to a 2% improvement in AKE precision andrecall. AKE is built over MAUI toolkit that follows a supervised learningapproach. We trained and tested our AKE method on a gold standard made of 8 BNprograms containing 110 manually annotated news stories. The experiments wereconducted within a Multimedia Monitoring Solution (MMS) system for TV and radionews/programs, running daily, and monitoring 12 TV and 4 radio channels.
arxiv-3900-99 | Analysing Word Importance for Image Annotation | http://arxiv.org/abs/1306.4758 | author:Payal Gulati, A. K. Sharma category:cs.IR cs.CV published:2013-06-20 summary:Image annotation provides several keywords automatically for a given imagebased on various tags to describe its contents which is useful in Imageretrieval. Various researchers are working on text based and content basedimage annotations [7,9]. It is seen, in traditional Image annotationapproaches, annotation words are treated equally without considering theimportance of each word in real world. In context of this, in this work, imagesare annotated with keywords based on their frequency count and wordcorrelation. Moreover this work proposes an approach to compute importancescore of candidate keywords, having same frequency count.
arxiv-3900-100 | Galerkin Methods for Complementarity Problems and Variational Inequalities | http://arxiv.org/abs/1306.4753 | author:Geoffrey J. Gordon category:cs.LG cs.AI math.OC published:2013-06-20 summary:Complementarity problems and variational inequalities arise in a wide varietyof areas, including machine learning, planning, game theory, and physicalsimulation. In all of these areas, to handle large-scale problem instances, weneed fast approximate solution methods. One promising idea is Galerkinapproximation, in which we search for the best answer within the span of agiven set of basis functions. Bertsekas proposed one possible Galerkin methodfor variational inequalities. However, this method can exhibit two problems inpractice: its approximation error is worse than might be expected based on theability of the basis to represent the desired solution, and each iterationrequires a projection step that is not always easy to implement efficiently.So, in this paper, we present a new Galerkin method with improved behavior: ournew error bounds depend directly on the distance from the true solution to thesubspace spanned by our basis, and the only projections we require are onto thefeasible region or onto the span of our basis.
arxiv-3900-101 | Failure of Calibration is Typical | http://arxiv.org/abs/1306.4943 | author:Gordon Belot category:math.ST stat.ML stat.TH published:2013-06-20 summary:Schervish (1985b) showed that every forecasting system is noncalibrated foruncountably many data sequences that it might see. This result is strengthenedhere: from a topological point of view, failure of calibration is typical andcalibration rare. Meanwhile, Bayesian forecasters are certain that they arecalibrated---this invites worries about the connection between Bayesianism andrationality.
arxiv-3900-102 | From-Below Approximations in Boolean Matrix Factorization: Geometry and New Algorithm | http://arxiv.org/abs/1306.4905 | author:Radim Belohlavek, Martin Trnecka category:cs.NA cs.LG published:2013-06-20 summary:We present new results on Boolean matrix factorization and a new algorithmbased on these results. The results emphasize the significance offactorizations that provide from-below approximations of the input matrix.While the previously proposed algorithms do not consider the possibly differentsignificance of different matrix entries, our results help measure suchsignificance and suggest where to focus when computing factors. An experimentalevaluation of the new algorithm on both synthetic and real data demonstratesits good performance in terms of good coverage by the first k factors as wellas a small number of factors needed for exact decomposition and indicates thatthe algorithm outperforms the available ones in these terms. We also proposefuture research topics.
arxiv-3900-103 | Felzenszwalb-Baum-Welch: Event Detection by Changing Appearance | http://arxiv.org/abs/1306.4746 | author:Daniel Paul Barrett, Jeffrey Mark Siskind category:cs.CV published:2013-06-20 summary:We propose a method which can detect events in videos by modeling the changein appearance of the event participants over time. This method makes itpossible to detect events which are characterized not by motion, but by thechanging state of the people or objects involved. This is accomplished by usingobject detectors as output models for the states of a hidden Markov model(HMM). The method allows an HMM to model the sequence of poses of the eventparticipants over time, and is effective for poses of humans and inanimateobjects. The ability to use existing object-detection methods as part of anevent model makes it possible to leverage ongoing work in the object-detectioncommunity. A novel training method uses an EM loop to simultaneously learn thetemporal structure and object models automatically, without the need to specifyeither the individual poses to be modeled or the frames in which they occur.The E-step estimates the latent assignment of video frames to HMM states, whilethe M-step estimates both the HMM transition probabilities and state outputmodels, including the object detectors, which are trained on the weightedsubset of frames assigned to their state. A new dataset was gathered becauselittle work has been done on events characterized by changing object pose, andsuitable datasets are not available. Our method produced results superior tothat of comparison systems on this dataset.
arxiv-3900-104 | Computer simulation based parameter selection for resistance exercise | http://arxiv.org/abs/1306.4724 | author:Ognjen Arandjelovic category:cs.CV cs.HC published:2013-06-20 summary:In contrast to most scientific disciplines, sports science research has beencharacterized by comparatively little effort investment in the development ofrelevant phenomenological models. Scarcer yet is the application of said modelsin practice. We present a framework which allows resistance trainingpractitioners to employ a recently proposed neuromuscular model in actualtraining program design. The first novelty concerns the monitoring aspect ofcoaching. A method for extracting training performance characteristics fromloosely constrained video sequences, effortlessly and with minimal human input,using computer vision is described. The extracted data is subsequently used tofit the underlying neuromuscular model. This is achieved by solving an inversedynamics problem corresponding to a particular exercise. Lastly, a computersimulation of hypothetical training bouts, using athlete-specific capabilityparameters, is used to predict the effected adaptation and changes inperformance. The software described here allows the practitioner to manipulatehypothetical training parameters and immediately see their effect on predictedadaptation for a specific athlete. Thus, this work presents a holistic view ofthe monitoring-assessment-adjustment loop.
arxiv-3900-105 | Machine Teaching for Bayesian Learners in the Exponential Family | http://arxiv.org/abs/1306.4947 | author:Xiaojin Zhu category:cs.LG published:2013-06-20 summary:What if there is a teacher who knows the learning goal and wants to designgood training data for a machine learner? We propose an optimal teachingframework aimed at learners who employ Bayesian models. Our framework isexpressed as an optimization problem over teaching examples that balance thefuture loss of the learner and the effort of the teacher. This optimizationproblem is in general hard. In the case where the learner employs conjugateexponential family models, we present an approximate algorithm for finding theoptimal teaching set. Our algorithm optimizes the aggregate sufficientstatistics, then unpacks them into actual teaching examples. We give severalexamples to illustrate our framework.
arxiv-3900-106 | Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization | http://arxiv.org/abs/1306.4650 | author:Julien Mairal category:stat.ML cs.LG math.OC published:2013-06-19 summary:Majorization-minimization algorithms consist of iteratively minimizing amajorizing surrogate of an objective function. Because of its simplicity andits wide applicability, this principle has been very popular in statistics andin signal processing. In this paper, we intend to make this principle scalable.We introduce a stochastic majorization-minimization scheme which is able todeal with large-scale or possibly infinite data sets. When applied to convexoptimization problems under suitable assumptions, we show that it achieves anexpected convergence rate of $O(1/\sqrt{n})$ after $n$ iterations, and of$O(1/n)$ for strongly convex functions. Equally important, our scheme almostsurely converges to stationary points for a large class of non-convex problems.We develop several efficient algorithms based on our framework. First, wepropose a new stochastic proximal gradient method, which experimentally matchesstate-of-the-art solvers for large-scale $\ell_1$-logistic regression. Second,we develop an online DC programming algorithm for non-convex sparse estimation.Finally, we demonstrate the effectiveness of our approach for solvinglarge-scale structured matrix factorization problems.
arxiv-3900-107 | Finite Element Based Tracking of Deforming Surfaces | http://arxiv.org/abs/1306.4478 | author:Stefanie Wuhrer, Jochen Lang, Motahareh Tekieh, Chang Shu category:cs.CV cs.GR published:2013-06-19 summary:We present an approach to robustly track the geometry of an object thatdeforms over time from a set of input point clouds captured from a singleviewpoint. The deformations we consider are caused by applying forces to knownlocations on the object's surface. Our method combines the use of priorinformation on the geometry of the object modeled by a smooth template and theuse of a linear finite element method to predict the deformation. This allowsthe accurate reconstruction of both the observed and the unobserved sides ofthe object. We present tracking results for noisy low-quality point cloudsacquired by either a stereo camera or a depth camera, and simulations withpoint clouds corrupted by different error terms. We show that our method isalso applicable to large non-linear deformations.
arxiv-3900-108 | Joint estimation of sparse multivariate regression and conditional graphical models | http://arxiv.org/abs/1306.4410 | author:Junhui Wang category:stat.ML cs.LG published:2013-06-19 summary:Multivariate regression model is a natural generalization of the classicalunivari- ate regression model for ?tting multiple responses. In this paper, wepropose a high- dimensional multivariate conditional regression model forconstructing sparse estimates of the multivariate regression coe?cient matrixthat accounts for the dependency struc- ture among the multiple responses. Theproposed method decomposes the multivariate regression problem into a series ofpenalized conditional log-likelihood of each response conditioned on thecovariates and other responses. It allows simultaneous estimation of the sparseregression coe?cient matrix and the sparse inverse covariance matrix. Theasymptotic selection consistency and normality are established for thediverging dimension of the covariates and number of responses. The e?ectivenessof the pro- posed method is also demonstrated in a variety of simulatedexamples as well as an application to the Glioblastoma multiforme cancer data.
arxiv-3900-109 | English Character Recognition using Artificial Neural Network | http://arxiv.org/abs/1306.4621 | author:Tirtharaj Dash, Tanistha Nayak category:cs.NE published:2013-06-19 summary:This work focuses on development of a Offline Hand Written English CharacterRecognition algorithm based on Artificial Neural Network (ANN). The ANNimplemented in this work has single output neuron which shows whether thetested character belongs to a particular cluster or not. The implementation iscarried out completely in 'C' language. Ten sets of English alphabets(small-26, capital-26) were used to train the ANN and 5 sets of Englishalphabets were used to test the network. The characters were collected fromdifferent persons over duration of about 25 days. The algorithm was tested with5 capital letters and 5 small letter sets. However, the result showed that thealgorithm recognized English alphabet patterns with maximum accuracy of 92.59%and False Rejection Rate (FRR) of 0%.
arxiv-3900-110 | Time Efficient Approach To Offline Hand Written Character Recognition Using Associative Memory Net | http://arxiv.org/abs/1306.4592 | author:Tirtharaj Dash category:cs.NE cs.CV published:2013-06-19 summary:In this paper, an efficient Offline Hand Written Character Recognitionalgorithm is proposed based on Associative Memory Net (AMN). The AMN used inthis work is basically auto associative. The implementation is carried outcompletely in 'C' language. To make the system perform to its best with minimalcomputation time, a Parallel algorithm is also developed using an API packageOpenMP. Characters are mainly English alphabets (Small (26), Capital (26))collected from system (52) and from different persons (52). The characterscollected from system are used to train the AMN and characters collected fromdifferent persons are used for testing the recognition ability of the net. Thedetailed analysis showed that the network recognizes the hand writtencharacters with recognition rate of 72.20% in average case. However, in bestcase, it recognizes the collected hand written characters with 88.5%. Thedeveloped network consumes 3.57 sec (average) in Serial implementation and 1.16sec (average) in Parallel implementation using OpenMP.
arxiv-3900-111 | Multiarmed Bandits With Limited Expert Advice | http://arxiv.org/abs/1306.4653 | author:Satyen Kale category:cs.LG published:2013-06-19 summary:We solve the COLT 2013 open problem of \citet{SCB} on minimizing regret inthe setting of advice-efficient multiarmed bandits with expert advice. We givean algorithm for the setting of K arms and N experts out of which we areallowed to query and use only M experts' advices in each round, which has aregret bound of \tilde{O}\bigP{\sqrt{\frac{\min\{K, M\} N}{M} T}} after Trounds. We also prove that any algorithm for this problem must have expectedregret at least \tilde{\Omega}\bigP{\sqrt{\frac{\min\{K, M\} N}{M}T}}, thusshowing that our upper bound is nearly tight.
arxiv-3900-112 | Solution to Quadratic Equation Using Genetic Algorithm | http://arxiv.org/abs/1306.4622 | author:Tanistha Nayak, Tirtharaj Dash category:cs.NE published:2013-06-19 summary:Solving Quadratic equation is one of the intrinsic interests as it is thesimplest nonlinear equations. A novel approach for solving Quadratic Equationbased on Genetic Algorithms (GAs) is presented. Genetic Algorithms (GAs) are atechnique to solve problems which need optimization. Generation of trialsolutions have been formed by this method. Many examples have been worked out,and in most cases we find out the exact solution. We have discussed the effectof different parameters on the performance of the developed algorithm. Theresults are concluded after rigorous testing on different equations.
arxiv-3900-113 | Non-Correlated Character Recognition using Artificial Neural Network | http://arxiv.org/abs/1306.4629 | author:Tirtharaj Dash, Tanistha Nayak category:cs.NE cs.CV published:2013-06-19 summary:This paper investigates a method of Handwritten English Character Recognitionusing Artificial Neural Network (ANN). This work has been done in offlineEnvironment for non correlated characters, which do not possess any linearrelationships among them. We test that whether the particular tested characterbelongs to a cluster or not. The implementation is carried out in Matlabenvironment and successfully tested. Fifty-two sets of English alphabets areused to train the ANN and test the network. The algorithms are tested with 26capital letters and 26 small letters. The testing result showed that theproposed ANN based algorithm showed a maximum recognition rate of 85%.
arxiv-3900-114 | Hacking Smart Machines with Smarter Ones: How to Extract Meaningful Data from Machine Learning Classifiers | http://arxiv.org/abs/1306.4447 | author:Giuseppe Ateniese, Giovanni Felici, Luigi V. Mancini, Angelo Spognardi, Antonio Villani, Domenico Vitali category:cs.CR cs.LG stat.ML published:2013-06-19 summary:Machine Learning (ML) algorithms are used to train computers to perform avariety of complex tasks and improve with experience. Computers learn how torecognize patterns, make unintended decisions, or react to a dynamicenvironment. Certain trained machines may be more effective than others becausethey are based on more suitable ML algorithms or because they were trainedthrough superior training sets. Although ML algorithms are known and publiclyreleased, training sets may not be reasonably ascertainable and, indeed, may beguarded as trade secrets. While much research has been performed about theprivacy of the elements of training sets, in this paper we focus our attentionon ML classifiers and on the statistical information that can be unconsciouslyor maliciously revealed from them. We show that it is possible to inferunexpected but useful information from ML classifiers. In particular, we builda novel meta-classifier and train it to hack other classifiers, obtainingmeaningful information about their training sets. This kind of informationleakage can be exploited, for example, by a vendor to build more effectiveclassifiers or to simply acquire trade secrets from a competitor's apparatus,potentially violating its intellectual property rights.
arxiv-3900-115 | An Overview of the Research on Texture Based Plant Leaf Classification | http://arxiv.org/abs/1306.4345 | author:Vishakha Metre, Jayshree Ghorpade category:cs.CV published:2013-06-18 summary:Plant classification has a broad application prospective in agriculture andmedicine, and is especially significant to the biology diversity research. Asplants are vitally important for environmental protection, it is more importantto identify and classify them accurately. Plant leaf classification is atechnique where leaf is classified based on its different morphologicalfeatures. The goal of this paper is to provide an overview of different aspectsof texture based plant leaf classification and related things. At last we willbe concluding about the efficient method i.e. the method that gives betterperformance compared to the other methods.
arxiv-3900-116 | On the Fundamental Limits of Recovering Tree Sparse Vectors from Noisy Linear Measurements | http://arxiv.org/abs/1306.4391 | author:Akshay Soni, Jarvis Haupt category:cs.IT math.IT math.ST stat.ML stat.TH published:2013-06-18 summary:Recent breakthrough results in compressive sensing (CS) have established thatmany high dimensional signals can be accurately recovered from a relativelysmall number of non-adaptive linear observations, provided that the signalspossess a sparse representation in some basis. Subsequent efforts have shownthat the performance of CS can be improved by exploiting additional structurein the locations of the nonzero signal coefficients during inference, or byutilizing some form of data-dependent adaptive measurement focusing during thesensing process. To our knowledge, our own previous work was the first toestablish the potential benefits that can be achieved when fusing the notionsof adaptive sensing and structured sparsity -- that work examined the task ofsupport recovery from noisy linear measurements, and established that anadaptive sensing strategy specifically tailored to signals that are tree-sparsecan significantly outperform adaptive and non-adaptive sensing strategies thatare agnostic to the underlying structure. In this work we establish fundamentalperformance limits for the task of support recovery of tree-sparse signals fromnoisy measurements, in settings where measurements may be obtained eithernon-adaptively (using a randomized Gaussian measurement strategy motivated byinitial CS investigations) or by any adaptive sensing strategy. Our mainresults here imply that the adaptive tree sensing procedure analyzed in ourprevious work is nearly optimal, in the sense that no other sensing andestimation strategy can perform fundamentally better for identifying thesupport of tree-sparse signals.
arxiv-3900-117 | A Novel Block-DCT and PCA Based Image Perceptual Hashing Algorithm | http://arxiv.org/abs/1306.4079 | author:Zeng Jie category:cs.CV published:2013-06-18 summary:Image perceptual hashing finds applications in content indexing, large-scaleimage database management, certification and authentication and digitalwatermarking. We propose a Block-DCT and PCA based image perceptual hash inthis article and explore the algorithm in the application of tamper detection.The main idea of the algorithm is to integrate color histogram and DCTcoefficients of image blocks as perceptual feature, then to compress perceptualfeatures as inter-feature with PCA, and to threshold to create a robust hash.The robustness and discrimination properties of the proposed algorithm areevaluated in detail. Our algorithms first construct a secondary image, derivedfrom input image by pseudo-randomly extracting features that approximatelycapture semi-global geometric characteristics. From the secondary image (whichdoes not perceptually resemble the input), we further extract the finalfeatures which can be used as a hash value (and can be further suitablyquantized). In this paper, we use spectral matrix invariants as embodied bySingular Value Decomposition. Surprisingly, formation of the secondary imageturns out be quite important since it not only introduces further robustness,but also enhances the security properties. Indeed, our experiments reveal thatour hashing algorithms extract most of the geometric information from theimages and hence are robust to severe perturbations (e.g. up to %50 cropping byarea with 20 degree rotations) on images while avoiding misclassification.Experimental results show that the proposed image perceptual hash algorithm caneffectively address the tamper detection problem with advantageous robustnessand discrimination.
arxiv-3900-118 | Group Symmetry and non-Gaussian Covariance Estimation | http://arxiv.org/abs/1306.4103 | author:Ilya Soloveychik, Ami Wiesel category:stat.ML published:2013-06-18 summary:We consider robust covariance estimation with group symmetry constraints.Non-Gaussian covariance estimation, e.g., Tyler scatter estimator andMultivariate Generalized Gaussian distribution methods, usually involvenon-convex minimization problems. Recently, it was shown that the underlyingprinciple behind their success is an extended form of convexity over thegeodesics in the manifold of positive definite matrices. A modern approach toimprove estimation accuracy is to exploit prior knowledge via additionalconstraints, e.g., restricting the attention to specific classes of covarianceswhich adhere to prior symmetry structures. In this paper, we prove that suchgroup symmetry constraints are also geodesically convex and can therefore beincorporated into various non-Gaussian covariance estimators. Practicalexamples of such sets include: circulant, persymmetric and complex/quaternionproper structures. We provide a simple numerical technique for finding maximumlikelihood estimates under such constraints, and demonstrate their performanceadvantage using synthetic experiments.
arxiv-3900-119 | Parallel Coordinate Descent Newton Method for Efficient $\ell_1$-Regularized Minimization | http://arxiv.org/abs/1306.4080 | author:Yatao Bian, Xiong Li, Yuncai Liu, Ming-Hsuan Yang category:cs.LG cs.NA published:2013-06-18 summary:The recent years have witnessed advances in parallel algorithms for largescale optimization problems. Notwithstanding demonstrated success, existingalgorithms that parallelize over features are usually limited by divergenceissues under high parallelism or require data preprocessing to alleviate theseproblems. In this work, we propose a Parallel Coordinate Descent Newtonalgorithm using multidimensional approximate Newton steps (PCDN), where theoff-diagonal elements of the Hessian are set to zero to enable parallelization.It randomly partitions the feature set into $b$ bundles/subsets with size of$P$, and sequentially processes each bundle by first computing the descentdirections for each feature in parallel and then conducting $P$-dimensionalline search to obtain the step size. We show that: (1) PCDN is guaranteed toconverge globally despite increasing parallelism; (2) PCDN converges to thespecified accuracy $\epsilon$ within the limited iteration number of$T_\epsilon$, and $T_\epsilon$ decreases with increasing parallelism (bundlesize $P$). Using the implementation technique of maintaining intermediatequantities, we minimize the data transfer and synchronization cost of the$P$-dimensional line search. For concreteness, the proposed PCDN algorithm isapplied to $\ell_1$-regularized logistic regression and $\ell_2$-loss SVM.Experimental evaluations on six benchmark datasets show that the proposed PCDNalgorithm exploits parallelism well and outperforms the state-of-the-artmethods in speed without losing accuracy.
arxiv-3900-120 | Dialogue System: A Brief Review | http://arxiv.org/abs/1306.4134 | author:Suket Arora, Kamaljeet Batra, Sarabjit Singh category:cs.CL published:2013-06-18 summary:A Dialogue System is a system which interacts with human in natural language.At present many universities are developing the dialogue system in theirregional language. This paper will discuss about dialogue system, itscomponents, challenges and its evaluation. This paper helps the researchers forgetting info regarding dialogues system.
arxiv-3900-121 | Bioclimating Modelling: A Machine Learning Perspective | http://arxiv.org/abs/1306.4152 | author:Maumita Bhattacharya category:cs.LG stat.ML 68T05 published:2013-06-18 summary:Many machine learning (ML) approaches are widely used to generate bioclimaticmodels for prediction of geographic range of organism as a function of climate.Applications such as prediction of range shift in organism, range of invasivespecies influenced by climate change are important parameters in understandingthe impact of climate change. However, success of machine learning-basedapproaches depends on a number of factors. While it can be safely said that noparticular ML technique can be effective in all applications and success of atechnique is predominantly dependent on the application or the type of theproblem, it is useful to understand their behaviour to ensure informed choiceof techniques. This paper presents a comprehensive review of machinelearning-based bioclimatic model generation and analyses the factorsinfluencing success of such models. Considering the wide use of statisticaltechniques, in our discussion we also include conventional statisticaltechniques used in bioclimatic modelling.
arxiv-3900-122 | Punjabi Language Interface to Database: a brief review | http://arxiv.org/abs/1306.4139 | author:Preeti Verma, Suket Arora, Kamaljit Batra category:cs.CL cs.HC published:2013-06-18 summary:Unlike most user-computer interfaces, a natural language interface allowsusers to communicate fluently with a computer system with very littlepreparation. Databases are often hard to use in cooperating with the usersbecause of their rigid interface. A good NLIDB allows a user to enter commandsand ask questions in native language and then after interpreting respond to theuser in native language. For a large number of applications requiringinteraction between humans and the computer systems, it would be convenient toprovide the end-user friendly interface. Punjabi language interface to databasewould proof fruitful to native people of Punjab, as it provides ease to them touse various e-governance applications like Punjab Sewa, Suwidha, Online PublicUtility Forms, Online Grievance Cell, Land Records Management System,legacymatters, e-District, agriculture, etc. Punjabi is the mother tongue of morethan 110 million people all around the world. According to availableinformation, Punjabi ranks 10th from top out of a total of 6,900 languagesrecognized internationally by the United Nations. This paper covers a briefoverview of the Natural language interface to database, its differentcomponents, its advantages, disadvantages, approaches and techniques used. Thepaper ends with the work done on Punjabi language interface to database andfuture enhancements that can be done.
arxiv-3900-123 | Two-View Matching with View Synthesis Revisited | http://arxiv.org/abs/1306.3855 | author:Dmytro Mishkin, Michal Perdoch, Jiri Matas category:cs.CV published:2013-06-17 summary:Wide-baseline matching focussing on problems with extreme viewpoint change isconsidered. We introduce the use of view synthesis with affine-covariantdetectors to solve such problems and show that matching with the Hessian-Affineor MSER detectors outperforms the state-of-the-art ASIFT. To minimise the loss of speed caused by view synthesis, we propose theMatching On Demand with view Synthesis algorithm (MODS) that uses progressivelymore synthesized images and more (time-consuming) detectors until reliableestimation of geometry is possible. We show experimentally that the MODSalgorithm solves problems beyond the state-of-the-art and yet is comparable inspeed to standard wide-baseline matchers on simpler problems. Minor contributions include an improved method for tentative correspondenceselection, applicable both with and without view synthesis and a view synthesissetup greatly improving MSER robustness to blur and scale change that increaseits running time by 10% only.
arxiv-3900-124 | Classifying and Visualizing Motion Capture Sequences using Deep Neural Networks | http://arxiv.org/abs/1306.3874 | author:Kyunghyun Cho, Xi Chen category:cs.CV published:2013-06-17 summary:The gesture recognition using motion capture data and depth sensors hasrecently drawn more attention in vision recognition. Currently most systemsonly classify dataset with a couple of dozens different actions. Moreover,feature extraction from the data is often computational complex. In this paper,we propose a novel system to recognize the actions from skeleton data withsimple, but effective, features using deep neural networks. Features areextracted for each frame based on the relative positions of joints (PO),temporal differences (TD), and normalized trajectories of motion (NT). Giventhese features a hybrid multi-layer perceptron is trained, which simultaneouslyclassifies and reconstructs input data. We use deep autoencoder to visualizelearnt features, and the experiments show that deep neural networks can capturemore discriminative information than, for instance, principal componentanalysis can. We test our system on a public database with 65 classes and morethan 2,000 motion sequences. We obtain an accuracy above 95% which is, to ourknowledge, the state of the art result for such a large dataset.
arxiv-3900-125 | Multi-view in Lensless Compressive Imaging | http://arxiv.org/abs/1306.3946 | author:Hong Jiang, Gang Huang, Paul Wilford category:cs.IT cs.CV math.IT published:2013-06-17 summary:Multi-view images are acquired by a lensless compressive imagingarchitecture, which consists of an aperture assembly and multiple sensors. Theaperture assembly consists of a two dimensional array of aperture elementswhose transmittance can be individually controlled to implement a compressivesensing matrix. For each transmittance pattern of the aperture assembly, eachof the sensors takes a measurement. The measurement vectors from the multiplesensors represent multi-view images of the same scene. We present theoreticalframework for multi-view reconstruction and experimental results for enhancingquality of image using multi-view.
arxiv-3900-126 | Online Alternating Direction Method (longer version) | http://arxiv.org/abs/1306.3721 | author:Huahua Wang, Arindam Banerjee category:cs.LG math.OC published:2013-06-17 summary:Online optimization has emerged as powerful tool in large scale optimization.In this pa- per, we introduce efficient online optimization algorithms based onthe alternating direction method (ADM), which can solve online convexoptimization under linear constraints where the objective could be non-smooth.We introduce new proof techniques for ADM in the batch setting, which yields aO(1/T) convergence rate for ADM and forms the basis for regret anal- ysis inthe online setting. We consider two scenarios in the online setting, based onwhether an additional Bregman divergence is needed or not. In both settings, weestablish regret bounds for both the objective function as well as constraintsviolation for general and strongly convex functions. We also consider inexactADM updates where certain terms are linearized to yield efficient updates andshow the stochastic convergence rates. In addition, we briefly discuss thatonline ADM can be used as projection- free online learning algorithm in somescenarios. Preliminary results are presented to illustrate the performance ofthe proposed algorithms.
arxiv-3900-127 | Discriminating word senses with tourist walks in complex networks | http://arxiv.org/abs/1306.3920 | author:Thiago C. Silva, Diego R. Amancio category:cs.CL cs.SI physics.soc-ph published:2013-06-17 summary:Patterns of topological arrangement are widely used for both animal and humanbrains in the learning process. Nevertheless, automatic learning techniquesfrequently overlook these patterns. In this paper, we apply a learningtechnique based on the structural organization of the data in the attributespace to the problem of discriminating the senses of 10 polysemous words. Usingtwo types of characterization of meanings, namely semantical and topologicalapproaches, we have observed significative accuracy rates in identifying thesuitable meanings in both techniques. Most importantly, we have found that thecharacterization based on the deterministic tourist walk improves thedisambiguation process when one compares with the discrimination achieved withtraditional complex networks measurements such as assortativity and clusteringcoefficient. To our knowledge, this is the first time that such deterministicwalk has been applied to such a kind of problem. Therefore, our findingsuggests that the tourist walk characterization may be useful in other relatedapplications.
arxiv-3900-128 | On Finding the Largest Mean Among Many | http://arxiv.org/abs/1306.3917 | author:Kevin Jamieson, Matthew Malloy, Robert Nowak, Sebastien Bubeck category:stat.ML cs.LG published:2013-06-17 summary:Sampling from distributions to find the one with the largest mean arises in abroad range of applications, and it can be mathematically modeled as amulti-armed bandit problem in which each distribution is associated with anarm. This paper studies the sample complexity of identifying the best arm(largest mean) in a multi-armed bandit problem. Motivated by large-scaleapplications, we are especially interested in identifying situations where thetotal number of samples that are necessary and sufficient to find the best armscale linearly with the number of arms. We present a single-parametermulti-armed bandit model that spans the range from linear to superlinear samplecomplexity. We also give a new algorithm for best arm identification, calledPRISM, with linear sample complexity for a wide range of mean distributions.The algorithm, like most exploration procedures for multi-armed bandits, isadaptive in the sense that the next arms to sample are selected based onprevious samples. We compare the sample complexity of adaptive procedures withsimpler non-adaptive procedures using new lower bounds. For many probleminstances, the increased sample complexity required by non-adaptive proceduresis a polynomial factor of the number of arms.
arxiv-3900-129 | Stability of Multi-Task Kernel Regression Algorithms | http://arxiv.org/abs/1306.3905 | author:Julien Audiffren, Hachem Kadri category:cs.LG stat.ML published:2013-06-17 summary:We study the stability properties of nonlinear multi-task regression inreproducing Hilbert spaces with operator-valued kernels. Such kernels, a.k.a.multi-task kernels, are appropriate for learning prob- lems with nonscalaroutputs like multi-task learning and structured out- put prediction. We showthat multi-task kernel regression algorithms are uniformly stable in thegeneral case of infinite-dimensional output spaces. We then derive under mildassumption on the kernel generaliza- tion bounds of such algorithms, and weshow their consistency even with non Hilbert-Schmidt operator-valued kernels .We demonstrate how to apply the results to various multi-task kernel regressionmethods such as vector-valued SVR and functional ridge regression.
arxiv-3900-130 | Bayesian methods for low-rank matrix estimation: short survey and theoretical study | http://arxiv.org/abs/1306.3862 | author:Pierre Alquier category:stat.ML published:2013-06-17 summary:The problem of low-rank matrix estimation recently received a lot ofattention due to challenging applications. A lot of work has been done onrank-penalized methods and convex relaxation, both on the theoretical andapplied sides. However, only a few papers considered Bayesian estimation. Inthis paper, we review the different type of priors considered on matrices tofavour low-rank. We also prove that the obtained Bayesian estimators, undersuitable assumptions, enjoys the same optimality properties as the ones basedon penalization.
arxiv-3900-131 | Cluster coloring of the Self-Organizing Map: An information visualization perspective | http://arxiv.org/abs/1306.3860 | author:Peter Sarlin, Samuel Rönnqvist category:cs.LG cs.HC published:2013-06-17 summary:This paper takes an information visualization perspective to visualrepresentations in the general SOM paradigm. This involves viewing SOM-basedvisualizations through the eyes of Bertin's and Tufte's theories on datagraphics. The regular grid shape of the Self-Organizing Map (SOM), while beinga virtue for linking visualizations to it, restricts representation of clusterstructures. From the viewpoint of information visualization, this paperprovides a general, yet simple, solution to projection-based coloring of theSOM that reveals structures. First, the proposed color space is easy toconstruct and customize to the purpose of use, while aiming at beingperceptually correct and informative through two separable dimensions. Second,the coloring method is not dependent on any specific method of projection, butis rather modular to fit any objective function suitable for the task at hand.The cluster coloring is illustrated on two datasets: the iris data, and welfareand poverty indicators.
arxiv-3900-132 | Non-Uniform Blind Deblurring with a Spatially-Adaptive Sparse Prior | http://arxiv.org/abs/1306.3828 | author:Haichao Zhang, David Wipf category:cs.CV published:2013-06-17 summary:Typical blur from camera shake often deviates from the standard uniformconvolutional script, in part because of problematic rotations which creategreater blurring away from some unknown center point. Consequently, successfulblind deconvolution requires the estimation of a spatially-varying ornon-uniform blur operator. Using ideas from Bayesian inference and convexanalysis, this paper derives a non-uniform blind deblurring algorithm withseveral desirable, yet previously-unexplored attributes. The underlyingobjective function includes a spatially adaptive penalty which couples thelatent sharp image, non-uniform blur operator, and noise level together. Thiscoupling allows the penalty to automatically adjust its shape based on theestimated degree of local blur and image structure such that regions with largeblur or few prominent edges are discounted. Remaining regions with modest blurand revealing edges therefore dominate the overall estimation process withoutexplicitly incorporating structure-selection heuristics. The algorithm can beimplemented using a majorization-minimization strategy that is virtuallyparameter free. Detailed theoretical analysis and empirical validation on realimages serve to validate the proposed method.
arxiv-3900-133 | Discrete perceptrons | http://arxiv.org/abs/1306.4375 | author:Mihailo Stojnic category:math.PR math-ph math.MP stat.ML published:2013-06-17 summary:Perceptrons have been known for a long time as a promising tool within theneural networks theory. The analytical treatment for a special class ofperceptrons started in seminal work of Gardner \cite{Gar88}. Techniquesinitially employed to characterize perceptrons relied on a statisticalmechanics approach. Many of such predictions obtained in \cite{Gar88} (and in afollow-up \cite{GarDer88}) were later on established rigorously as mathematicalfacts (see, e.g.\cite{SchTir02,SchTir03,TalBook,StojnicGardGen13,StojnicGardSphNeg13,StojnicGardSphErr13}).These typically related to spherical perceptrons. A lot of work has been donerelated to various other types of perceptrons. Among the most challenging onesare what we will refer to as the discrete perceptrons. An introductorystatistical mechanics treatment of such perceptrons was given in\cite{GutSte90}. Relying on results of \cite{Gar88}, \cite{GutSte90}characterized many of the features of several types of discrete perceptrons. Wein this paper, consider a similar subclass of discrete perceptrons and providea mathematically rigorous set of results related to their performance. As itwill turn out, many of the statistical mechanics predictions obtained fordiscrete predictions will in fact appear as mathematically provable bounds.This will in a way emulate a similar type of behavior we observed in\cite{StojnicGardGen13,StojnicGardSphNeg13,StojnicGardSphErr13} when studyingspherical perceptrons.
arxiv-3900-134 | Spherical perceptron as a storage memory with limited errors | http://arxiv.org/abs/1306.3809 | author:Mihailo Stojnic category:math.PR math-ph math.MP stat.ML published:2013-06-17 summary:It has been known for a long time that the classical spherical perceptronscan be used as storage memories. Seminal work of Gardner, \cite{Gar88}, startedan analytical study of perceptrons storage abilities. Many of the Gardner'spredictions obtained through statistical mechanics tools have been rigorouslyjustified. Among the most important ones are of course the storage capacities.The first rigorous confirmations were obtained in \cite{SchTir02,SchTir03} forthe storage capacity of the so-called positive spherical perceptron. These werelater reestablished in \cite{TalBook} and a bit more recently in\cite{StojnicGardGen13}. In this paper we consider a variant of the sphericalperceptron that operates as a storage memory but allows for a certain fractionof errors. In Gardner's original work the statistical mechanics predictions inthis directions were presented sa well. Here, through a mathematically rigorousanalysis, we confirm that the Gardner's predictions in this direction are infact provable upper bounds on the true values of the storage capacity.Moreover, we then present a mechanism that can be used to lower these bounds.Numerical results that we present indicate that the Garnder's storage capacitypredictions may, in a fairly wide range of parameters, be not that far awayfrom the true values.
arxiv-3900-135 | On-line PCA with Optimal Regrets | http://arxiv.org/abs/1306.3895 | author:Jiazhong Nie, Wojciech Kotlowski, Manfred K. Warmuth category:cs.LG published:2013-06-17 summary:We carefully investigate the on-line version of PCA, where in each trial alearning algorithm plays a k-dimensional subspace, and suffers the compressionloss on the next instance when projected into the chosen subspace. In thissetting, we analyze two popular on-line algorithms, Gradient Descent (GD) andExponentiated Gradient (EG). We show that both algorithms are essentiallyoptimal in the worst-case. This comes as a surprise, since EG is known toperform sub-optimally when the instances are sparse. This different behavior ofEG for PCA is mainly related to the non-negativity of the loss in this case,which makes the PCA setting qualitatively different from other settings studiedin the literature. Furthermore, we show that when considering regret bounds asfunction of a loss budget, EG remains optimal and strictly outperforms GD.Next, we study the extension of the PCA setting, in which the Nature is allowedto play with dense instances, which are positive matrices with bounded largesteigenvalue. Again we can show that EG is optimal and strictly better than GD inthis setting.
arxiv-3900-136 | Spectral Experts for Estimating Mixtures of Linear Regressions | http://arxiv.org/abs/1306.3729 | author:Arun Tejasvi Chaganty, Percy Liang category:cs.LG stat.ML published:2013-06-17 summary:Discriminative latent-variable models are typically learned using EM orgradient-based optimization, which suffer from local optima. In this paper, wedevelop a new computationally efficient and provably consistent estimator for amixture of linear regressions, a simple instance of a discriminativelatent-variable model. Our approach relies on a low-rank linear regression torecover a symmetric tensor, which can be factorized into the parameters using atensor power method. We prove rates of convergence for our estimator andprovide an empirical evaluation illustrating its strengths relative to localoptimization (EM).
arxiv-3900-137 | On Russian Roulette Estimates for Bayesian Inference with Doubly-Intractable Likelihoods | http://arxiv.org/abs/1306.4032 | author:Anne-Marie Lyne, Mark Girolami, Yves Atchadé, Heiko Strathmann, Daniel Simpson category:stat.ME stat.CO stat.ML published:2013-06-17 summary:A large number of statistical models are "doubly-intractable": the likelihoodnormalising term, which is a function of the model parameters, is intractable,as well as the marginal likelihood (model evidence). This means that standardinference techniques to sample from the posterior, such as Markov chain MonteCarlo (MCMC), cannot be used. Examples include, but are not confined to,massive Gaussian Markov random fields, autologistic models and Exponentialrandom graph models. A number of approximate schemes based on MCMC techniques,Approximate Bayesian computation (ABC) or analytic approximations to theposterior have been suggested, and these are reviewed here. Exact MCMC schemes,which can be applied to a subset of doubly-intractable distributions, have alsobeen developed and are described in this paper. As yet, no general methodexists which can be applied to all classes of models with doubly-intractableposteriors. In addition, taking inspiration from the Physics literature, westudy an alternative method based on representing the intractable likelihood asan infinite series. Unbiased estimates of the likelihood can then be obtainedby finite time stochastic truncation of the series via Russian Roulettesampling, although the estimates are not necessarily positive. Results from theQuantum Chromodynamics literature are exploited to allow the use of possiblynegative estimates in a pseudo-marginal MCMC scheme such that expectations withrespect to the posterior distribution are preserved. The methodology isreviewed on well-known examples such as the parameters in Ising models, theposterior for Fisher-Bingham distributions on the $d$-Sphere and a large-scaleGaussian Markov Random Field model describing the Ozone Column data. This leadsto a critical assessment of the strengths and weaknesses of the methodologywith pointers to ongoing research.
arxiv-3900-138 | An open diachronic corpus of historical Spanish: annotation criteria and automatic modernisation of spelling | http://arxiv.org/abs/1306.3692 | author:Felipe Sánchez-Martínez, Isabel Martínez-Sempere, Xavier Ivars-Ribes, Rafael C. Carrasco category:cs.CL cs.DL published:2013-06-16 summary:The IMPACT-es diachronic corpus of historical Spanish compiles over onehundred books --containing approximately 8 million words-- in addition to acomplementary lexicon which links more than 10 thousand lemmas withattestations of the different variants found in the documents. This textualcorpus and the accompanying lexicon have been released under an open license(Creative Commons by-nc-sa) in order to permit their intensive exploitation inlinguistic research. Approximately 7% of the words in the corpus (a selectionaimed at enhancing the coverage of the most frequent word forms) have beenannotated with their lemma, part of speech, and modern equivalent. This paperdescribes the annotation criteria followed and the standards, based on the TextEncoding Initiative recommendations, used to the represent the texts in digitalform. As an illustration of the possible synergies between diachronic textualresources and linguistic research, we describe the application of statisticalmachine translation techniques to infer probabilistic context-sensitive rulesfor the automatic modernisation of spelling. The automatic modernisation withthis type of statistical methods leads to very low character error rates whenthe output is compared with the supervised modern version of the text.
arxiv-3900-139 | Do semidefinite relaxations solve sparse PCA up to the information limit? | http://arxiv.org/abs/1306.3690 | author:Robert Krauthgamer, Boaz Nadler, Dan Vilenchik category:math.ST stat.ML stat.TH published:2013-06-16 summary:Estimating the leading principal components of data, assuming they aresparse, is a central task in modern high-dimensional statistics. Manyalgorithms were developed for this sparse PCA problem, from simple diagonalthresholding to sophisticated semidefinite programming (SDP) methods. A keytheoretical question is under what conditions can such algorithms recover thesparse principal components? We study this question for a single-spike modelwith an $\ell_0$-sparse eigenvector, in the asymptotic regime as dimension $p$and sample size $n$ both tend to infinity. Amini and Wainwright [Ann. Statist.37 (2009) 2877-2921] proved that for sparsity levels $k\geq\Omega(n/\log p)$,no algorithm, efficient or not, can reliably recover the sparse eigenvector. Incontrast, for $k\leq O(\sqrt{n/\log p})$, diagonal thresholding is consistent.It was further conjectured that an SDP approach may close this gap betweencomputational and information limits. We prove that when$k\geq\Omega(\sqrt{n})$, the proposed SDP approach, at least in its standardusage, cannot recover the sparse spike. In fact, we conjecture that in thesingle-spike model, no computationally-efficient algorithm can recover a spikeof $\ell_0$-sparsity $k\geq\Omega(\sqrt{n})$. Finally, we present empiricalresults suggesting that up to sparsity levels $k=O(\sqrt{n})$, recovery ispossible by a simple covariance thresholding algorithm.
arxiv-3900-140 | Local case-control sampling: Efficient subsampling in imbalanced data sets | http://arxiv.org/abs/1306.3706 | author:William Fithian, Trevor Hastie category:stat.CO stat.ML published:2013-06-16 summary:For classification problems with significant class imbalance, subsampling canreduce computational costs at the price of inflated variance in estimatingmodel parameters. We propose a method for subsampling efficiently for logisticregression by adjusting the class balance locally in feature space via anaccept-reject scheme. Our method generalizes standard case-control sampling,using a pilot estimate to preferentially select examples whose responses areconditionally rare given their features. The biased subsampling is corrected bya post-hoc analytic adjustment to the parameters. The method is simple andrequires one parallelizable scan over the full data set. Standard case-controlsampling is inconsistent under model misspecification for the populationrisk-minimizing coefficients $\theta^*$. By contrast, our estimator isconsistent for $\theta^*$ provided that the pilot estimate is. Moreover, undercorrect specification and with a consistent, independent pilot estimate, ourestimator has exactly twice the asymptotic variance of the full-sample MLE -even if the selected subsample comprises a miniscule fraction of the full dataset, as happens when the original data are severely imbalanced. The factor oftwo improves to $1+\frac{1}{c}$ if we multiply the baseline acceptanceprobabilities by $c>1$ (and weight points with acceptance probability greaterthan 1), taking roughly $\frac{1+c}{2}$ times as many data points into thesubsample. Experiments on simulated and real data show that our method cansubstantially outperform standard case-control subsampling.
arxiv-3900-141 | Bayesian test of significance for conditional independence: The multinomial model | http://arxiv.org/abs/1306.3627 | author:Pablo de Morais Andrade, Julio Michael Stern, Carlos Alberto de Bragança Pereira category:stat.CO stat.ML 47N30 G.3 published:2013-06-16 summary:Conditional independence tests (CI tests) have received special attentionlately in Machine Learning and Computational Intelligence related literature asan important indicator of the relationship among the variables used by theirmodels. In the field of Probabilistic Graphical Models (PGM)--which includesBayesian Networks (BN) models--CI tests are especially important for the taskof learning the PGM structure from data. In this paper, we propose the FullBayesian Significance Test (FBST) for tests of conditional independence fordiscrete datasets. FBST is a powerful Bayesian test for precise hypothesis, asan alternative to frequentist's significance tests (characterized by thecalculation of the \emph{p-value}).
arxiv-3900-142 | Outlying Property Detection with Numerical Attributes | http://arxiv.org/abs/1306.3558 | author:Fabrizio Angiulli, Fabio Fassetti, Luigi Palopoli, Giuseppe Manco category:cs.LG cs.DB stat.ML published:2013-06-15 summary:The outlying property detection problem is the problem of discovering theproperties distinguishing a given object, known in advance to be an outlier ina database, from the other database objects. In this paper, we analyze theproblem within a context where numerical attributes are taken into account,which represents a relevant case left open in the literature. We introduce ameasure to quantify the degree the outlierness of an object, which isassociated with the relative likelihood of the value, compared to the to therelative likelihood of other objects in the database. As a major contribution,we present an efficient algorithm to compute the outlierness relative tosignificant subsets of the data. The latter subsets are characterized in a"rule-based" fashion, and hence the basis for the underlying explanation of theoutlierness.
arxiv-3900-143 | iCub World: Friendly Robots Help Building Good Vision Data-Sets | http://arxiv.org/abs/1306.3560 | author:Sean Ryan Fanello, Carlo Ciliberto, Matteo Santoro, Lorenzo Natale, Giorgio Metta, Lorenzo Rosasco, Francesca Odone category:cs.CV published:2013-06-15 summary:In this paper we present and start analyzing the iCub World data-set, anobject recognition data-set, we acquired using a Human-Robot Interaction (HRI)scheme and the iCub humanoid robot platform. Our set up allows for rapidacquisition and annotation of data with corresponding ground truth. While moreconstrained in its scopes -- the iCub world is essentially a robotics researchlab -- we demonstrate how the proposed data-set poses challenges to currentrecognition systems. The iCubWorld data-set is publicly available. The data-setcan be downloaded from: http://www.iit.it/en/projects/data-sets.html.
arxiv-3900-144 | Early stopping and non-parametric regression: An optimal data-dependent stopping rule | http://arxiv.org/abs/1306.3574 | author:Garvesh Raskutti, Martin J. Wainwright, Bin Yu category:stat.ML published:2013-06-15 summary:The strategy of early stopping is a regularization technique based onchoosing a stopping time for an iterative algorithm. Focusing on non-parametricregression in a reproducing kernel Hilbert space, we analyze the early stoppingstrategy for a form of gradient-descent applied to the least-squares lossfunction. We propose a data-dependent stopping rule that does not involvehold-out or cross-validation data, and we prove upper bounds on the squarederror of the resulting function estimate, measured in either the $L^2(P)$ and$L^2(P_n)$ norm. These upper bounds lead to minimax-optimal rates for variouskernel classes, including Sobolev smoothness classes and other forms ofreproducing kernel Hilbert spaces. We show through simulation that our stoppingrule compares favorably to two other stopping rules, one based on hold-out dataand the other based on Stein's unbiased risk estimate. We also establish atight connection between our early stopping strategy and the solution path of akernel ridge regression estimator.
arxiv-3900-145 | Recurrent Convolutional Neural Networks for Discourse Compositionality | http://arxiv.org/abs/1306.3584 | author:Nal Kalchbrenner, Phil Blunsom category:cs.CL published:2013-06-15 summary:The compositionality of meaning extends beyond the single sentence. Just aswords combine to form the meaning of sentences, so do sentences combine to formthe meaning of paragraphs, dialogues and general discourse. We introduce both asentence model and a discourse model corresponding to the two levels ofcompositionality. The sentence model adopts convolution as the centraloperation for composing semantic vectors and is based on a novel hierarchicalconvolutional neural network. The discourse model extends the sentence modeland is based on a recurrent neural network that is conditioned in a novel wayboth on the current sentence and on the current speaker. The discourse model isable to capture both the sequentiality of sentences and the interaction betweendifferent speakers. Without feature engineering or pretraining and with simplegreedy decoding, the discourse model coupled to the sentence model obtainsstate of the art performance on a dialogue act classification experiment.
arxiv-3900-146 | Generalized Beta Divergence | http://arxiv.org/abs/1306.3530 | author:Y. Kenan Yilmaz category:stat.ML published:2013-06-14 summary:This paper generalizes beta divergence beyond its classical form associatedwith power variance functions of Tweedie models. Generalized form isrepresented by a compact definite integral as a function of variance functionof the exponential dispersion model. This compact integral form simplifiesderivations of many properties such as scaling, translation and expectation ofthe beta divergence. Further, we show that beta divergence and (half of) thestatistical deviance are equivalent measures.
arxiv-3900-147 | Symmetries in LDDMM with higher order momentum distributions | http://arxiv.org/abs/1306.3309 | author:Henry Jacobs category:math.DS cs.CV published:2013-06-14 summary:In some implementations of the Large Deformation Diffeomorphic Metric Mappingformulation for image registration we consider the motion of particles whichlocally translate image data. We then lift the motion of the particles toobtain a motion on the entire image. However, it is certainly possible toconsider particles which do more than translate, and this is what will bedescribed in this paper. As the unreduced Lagrangian associated to EPDiffpossesses $\Diff(M)$ symmetry, it must also exhibit $G \subset \Diff(M)$symmetry, for any Lie subgroup. In this paper we will describe a tower of Liegroups $G^{(0)} \subseteq G^{(1)} \subseteq G^{(2)} \subseteq...$ whichcorrespond to preserving $k$-th order jet-data. The reduced configurationspaces $Q^{(k)} := \Diff(M) / G^{(k)}$ will be finite-dimensional (inparticular, $Q^{(0)}$ is the configuration manifold for $N$ particles in $M$).We will observe that $G^{(k)}$ is a normal subgroup of $G^{(0)}$ and so thequotient $G^{(0)} / G^{(k)}$ is itself a (finite dimensional) Lie group whichacts on $Q^{(k)}$. This makes $Q^{(k)}$ a principle bundle over $Q^{(0)}$ andthe reduced geodesic equations on $Q^{(k)}$ will possess $G^{(0)} /G^{(k)}$-symmetry. Noether's theorem implies the existence of conserved momentafor the reduced system on $T^{\ast}Q^{(k)}$.
arxiv-3900-148 | Sparse Recovery of Streaming Signals Using L1-Homotopy | http://arxiv.org/abs/1306.3331 | author:M. Salman Asif, Justin Romberg category:cs.IT math.IT math.OC stat.ML published:2013-06-14 summary:Most of the existing methods for sparse signal recovery assume a staticsystem: the unknown signal is a finite-length vector for which a fixed set oflinear measurements and a sparse representation basis are available and anL1-norm minimization program is solved for the reconstruction. However, thesame representation and reconstruction framework is not readily applicable in astreaming system: the unknown signal changes over time, and it is measured andreconstructed sequentially over small time intervals. In this paper, we discuss two such streaming systems and a homotopy-basedalgorithm for quickly solving the associated L1-norm minimization programs: 1)Recovery of a smooth, time-varying signal for which, instead of using blocktransforms, we use lapped orthogonal transforms for sparse representation. 2)Recovery of a sparse, time-varying signal that follows a linear dynamic model.For both the systems, we iteratively process measurements over a slidinginterval and estimate sparse coefficients by solving a weighted L1-normminimization program. Instead of solving a new L1 program from scratch at everyiteration, we use an available signal estimate as a starting point in ahomotopy formulation. Starting with a warm-start vector, our homotopy algorithmupdates the solution in a small number of computationally inexpensive steps asthe system changes. The homotopy algorithm presented in this paper is highlyversatile as it can update the solution for the L1 problem in a number ofdynamical settings. We demonstrate with numerical experiments that our proposedstreaming recovery framework outperforms the methods that represent andreconstruct a signal as independent, disjoint blocks, in terms of quality ofreconstruction, and that our proposed homotopy-based updating schemeoutperforms current state-of-the-art solvers in terms of the computation timeand complexity.
arxiv-3900-149 | Constrained fractional set programs and their application in local clustering and community detection | http://arxiv.org/abs/1306.3409 | author:Thomas Bühler, Syama Sundar Rangapuram, Simon Setzer, Matthias Hein category:stat.ML cs.LG math.OC published:2013-06-14 summary:The (constrained) minimization of a ratio of set functions is a problemfrequently occurring in clustering and community detection. As theseoptimization problems are typically NP-hard, one uses convex or spectralrelaxations in practice. While these relaxations can be solved globallyoptimally, they are often too loose and thus lead to results far away from theoptimum. In this paper we show that every constrained minimization problem of aratio of non-negative set functions allows a tight relaxation into anunconstrained continuous optimization problem. This result leads to a flexibleframework for solving constrained problems in network analysis. While aglobally optimal solution for the resulting non-convex problem cannot beguaranteed, we outperform the loose convex or spectral relaxations by a largemargin on constrained local clustering problems.
arxiv-3900-150 | Approximation Algorithms for Bayesian Multi-Armed Bandit Problems | http://arxiv.org/abs/1306.3525 | author:Sudipto Guha, Kamesh Munagala category:cs.DS cs.LG published:2013-06-14 summary:In this paper, we consider several finite-horizon Bayesian multi-armed banditproblems with side constraints which are computationally intractable (NP-Hard)and for which no optimal (or near optimal) algorithms are known to exist withsub-exponential running time. All of these problems violate the standardexchange property, which assumes that the reward from the play of an arm is notcontingent upon when the arm is played. Not only are index policies suboptimalin these contexts, there has been little analysis of such policies in theseproblem settings. We show that if we consider near-optimal policies, in thesense of approximation algorithms, then there exists (near) index policies.Conceptually, if we can find policies that satisfy an approximate version ofthe exchange property, namely, that the reward from the play of an arm dependson when the arm is played to within a constant factor, then we have an avenuetowards solving these problems. However such an approximate version of theidling bandit property does not hold on a per-play basis and are shown to holdin a global sense. Clearly, such a property is not necessarily true ofarbitrary single arm policies and finding such single arm policies isnontrivial. We show that by restricting the state spaces of arms we can findsingle arm policies and that these single arm policies can be combined intoglobal (near) index policies where the approximate version of the exchangeproperty is true in expectation. The number of different bandit problems thatcan be addressed by this technique already demonstrate its wide applicability.
arxiv-3900-151 | Relaxed Sparse Eigenvalue Conditions for Sparse Estimation via Non-convex Regularized Regression | http://arxiv.org/abs/1306.3343 | author:Zheng Pan, Changshui Zhang category:cs.LG cs.NA stat.ML published:2013-06-14 summary:Non-convex regularizers usually improve the performance of sparse estimationin practice. To prove this fact, we study the conditions of sparse estimationsfor the sharp concave regularizers which are a general family of non-convexregularizers including many existing regularizers. For the global solutions ofthe regularized regression, our sparse eigenvalue based conditions are weakerthan that of L1-regularization for parameter estimation and sparsenessestimation. For the approximate global and approximate stationary (AGAS)solutions, almost the same conditions are also enough. We show that the desiredAGAS solutions can be obtained by coordinate descent (CD) based methods.Finally, we perform some experiments to show the performance of CD methods ongiving AGAS solutions and the degree of weakness of the estimation conditionsrequired by the sharp concave regularizers.
arxiv-3900-152 | Unsupervised deconvolution of dynamic imaging reveals intratumor vascular heterogeneity | http://arxiv.org/abs/1306.3392 | author:Li Chen, Peter L. Choyke, Niya Wang, Robert Clarke, Zaver M. Bhujwalla, Elizabeth M. C. Hillman, Yue Wang category:q-bio.QM stat.ML published:2013-06-14 summary:Intratumor heterogeneity is often manifested by vascular compartments withdistinct pharmacokinetics that cannot be resolved directly by in vivo dynamicimaging. We developed tissue-specific compartment modeling (TSCM), anunsupervised computational method of deconvolving dynamic imaging series fromheterogeneous tumors that can improve vascular phenotyping in many biologicalcontexts. Applying TSCM to dynamic contrast-enhanced MRI of breast cancersrevealed characteristic intratumor vascular heterogeneity and therapeuticresponses that were otherwise undetectable.
arxiv-3900-153 | Feature Learning by Multidimensional Scaling and its Applications in Object Recognition | http://arxiv.org/abs/1306.3294 | author:Quan Wang, Kim L. Boyer category:cs.CV published:2013-06-14 summary:We present the MDS feature learning framework, in which multidimensionalscaling (MDS) is applied on high-level pairwise image distances to learnfixed-length vector representations of images. The aspects of the images thatare captured by the learned features, which we call MDS features, completelydepend on what kind of image distance measurement is employed. With properlyselected semantics-sensitive image distances, the MDS features provide richsemantic information about the images that is not captured by other featureextraction techniques. In our work, we introduce the iteratedLevenberg-Marquardt algorithm for solving MDS, and study the MDS featurelearning with IMage Euclidean Distance (IMED) and Spatial Pyramid Matching(SPM) distance. We present experiments on both synthetic data and real images--- the publicly accessible UIUC car image dataset. The MDS features based onSPM distance achieve exceptional performance for the car recognition task.
arxiv-3900-154 | Hyperparameter Optimization and Boosting for Classifying Facial Expressions: How good can a "Null" Model be? | http://arxiv.org/abs/1306.3476 | author:James Bergstra, David D. Cox category:cs.CV cs.LG stat.ML published:2013-06-14 summary:One of the goals of the ICML workshop on representation and learning is toestablish benchmark scores for a new data set of labeled facial expressions.This paper presents the performance of a "Null" model consisting ofconvolutions with random weights, PCA, pooling, normalization, and a linearreadout. Our approach focused on hyperparameter optimization rather than novelmodel components. On the Facial Expression Recognition Challenge held by theKaggle website, our hyperparameter optimization approach achieved a score of60% accuracy on the test data. This paper also introduces a new ensembleconstruction variant that combines hyperparameter optimization with theconstruction of ensembles. This algorithm constructed an ensemble of fourmodels that scored 65.5% accuracy. These scores rank 12th and 5th respectivelyamong the 56 challenge participants. It is worth noting that our approach wasdeveloped prior to the release of the data set, and applied withoutmodification; our strong competition performance suggests that the TPEhyperparameter optimization algorithm and domain expertise encoded in our Nullmodel can generalize to new image classification data sets.
arxiv-3900-155 | Classifying Single-Trial EEG during Motor Imagery with a Small Training Set | http://arxiv.org/abs/1306.3474 | author:Yijun Wang category:cs.LG cs.HC stat.ML published:2013-06-14 summary:Before the operation of a motor imagery based brain-computer interface (BCI)adopting machine learning techniques, a cumbersome training procedure isunavoidable. The development of a practical BCI posed the challenge ofclassifying single-trial EEG with a small training set. In this letter, weaddressed this problem by employing a series of signal processing and machinelearning approaches to alleviate overfitting and obtained test accuracy similarto training accuracy on the datasets from BCI Competition III and our ownexperiments.
arxiv-3900-156 | Matching objects across the textured-smooth continuum | http://arxiv.org/abs/1306.3297 | author:Ognjen Arandjelovic category:cs.CV published:2013-06-14 summary:The problem of 3D object recognition is of immense practical importance, withthe last decade witnessing a number of breakthroughs in the state of the art.Most of the previous work has focused on the matching of textured objects usinglocal appearance descriptors extracted around salient image points. Therecently proposed bag of boundaries method was the first to address directlythe problem of matching smooth objects using boundary features. However, noprevious work has attempted to achieve a holistic treatment of the problem byjointly using textural and shape features which is what we describe herein. Dueto the complementarity of the two modalities, we fuse the correspondingmatching scores and learn their relative weighting in a data specific manner byoptimizing discriminative performance on synthetically distorted data. For thetextural description of an object we adopt a representation in the form of ahistogram of SIFT based visual words. Similarly the apparent shape of an objectis represented by a histogram of discretized features capturing local shape. Ona large public database of a diverse set of objects, the proposed method isshown to outperform significantly both purely textural and purely shape basedapproaches for matching across viewpoint variation.
arxiv-3900-157 | Live-wire 3D medical images segmentation | http://arxiv.org/abs/1306.3415 | author:Ognjen Arandjelovic category:cs.CV published:2013-06-14 summary:This report describes the design, implementation, evaluation and originalenhancements to the Live-Wire method for 2D and 3D image segmentation.Live-Wire 2D employs a semi-automatic paradigm; the user is asked to select afew boundary points of the object to segment, to steer the process in the rightdirection, while the result is displayed in real time. In our implementationsegmentation is extended to three dimensions by performing this process on aslice-by-slice basis. User's time and involvement is further reduced byallowing him to specify object contours in planes orthogonal to the slices. Ifthese planes are chosen strategically, Live-Wire 3D can perform 2D segmentationin the plane of each slice automatically. This report also proposes twoimprovements to the original method, path heating and a new graph edge featurefunction based on variance of path properties along the boundary. We show thatthese improvements lead up to a 33% reduction in interaction with the user, andimproved delineation in presence of strong interfering edges.
arxiv-3900-158 | Guaranteed Classification via Regularized Similarity Learning | http://arxiv.org/abs/1306.3108 | author:Zheng-Chu Guo, Yiming Ying category:cs.LG published:2013-06-13 summary:Learning an appropriate (dis)similarity function from the available data is acentral problem in machine learning, since the success of many machine learningalgorithms critically depends on the choice of a similarity function to compareexamples. Despite many approaches for similarity metric learning have beenproposed, there is little theoretical study on the links between similaritymet- ric learning and the classification performance of the result classifier.In this paper, we propose a regularized similarity learning formulationassociated with general matrix-norms, and establish their generalizationbounds. We show that the generalization error of the resulting linear separatorcan be bounded by the derived generalization bound of similarity learning. Thisshows that a good gen- eralization of the learnt similarity function guaranteesa good classification of the resulting linear classifier. Our results extendand improve those obtained by Bellet at al. [3]. Due to the techniquesdependent on the notion of uniform stability [6], the bound obtained thereholds true only for the Frobenius matrix- norm regularization. Our techniquesusing the Rademacher complexity [5] and its related Khinchin-type inequalityenable us to establish bounds for regularized similarity learning formulationsassociated with general matrix-norms including sparse L 1 -norm and mixed(2,1)-norm.
arxiv-3900-159 | Bregman Alternating Direction Method of Multipliers | http://arxiv.org/abs/1306.3203 | author:Huahua Wang, Arindam Banerjee category:math.OC cs.LG stat.ML published:2013-06-13 summary:The mirror descent algorithm (MDA) generalizes gradient descent by using aBregman divergence to replace squared Euclidean distance. In this paper, wesimilarly generalize the alternating direction method of multipliers (ADMM) toBregman ADMM (BADMM), which allows the choice of different Bregman divergencesto exploit the structure of problems. BADMM provides a unified framework forADMM and its variants, including generalized ADMM, inexact ADMM and Bethe ADMM. We establish the global convergence and the $O(1/T)$ iteration complexity forBADMM. In some cases, BADMM can be faster than ADMM by a factor of$O(n/\log(n))$. In solving the linear program of mass transportation problem,BADMM leads to massive parallelism and can easily run on GPU. BADMM is severaltimes faster than highly optimized commercial software Gurobi.
arxiv-3900-160 | Learning Using Privileged Information: SVM+ and Weighted SVM | http://arxiv.org/abs/1306.3161 | author:Maksim Lapin, Matthias Hein, Bernt Schiele category:stat.ML cs.LG published:2013-06-13 summary:Prior knowledge can be used to improve predictive performance of learningalgorithms or reduce the amount of data required for training. The same goal ispursued within the learning using privileged information paradigm which wasrecently introduced by Vapnik et al. and is aimed at utilizing additionalinformation available only at training time -- a framework implemented by SVM+.We relate the privileged information to importance weighting and show that theprior knowledge expressible with privileged features can also be encoded byweights associated with every training example. We show that a weighted SVM canalways replicate an SVM+ solution, while the converse is not true and weconstruct a counterexample highlighting the limitations of SVM+. Finally, wetouch on the problem of choosing weights for weighted SVMs when privilegedfeatures are not available.
arxiv-3900-161 | A Convergence Theorem for the Graph Shift-type Algorithms | http://arxiv.org/abs/1306.3002 | author:Xuhui Fan, Longbing Cao category:stat.ML cs.LG published:2013-06-13 summary:Graph Shift (GS) algorithms are recently focused as a promising approach fordiscovering dense subgraphs in noisy data. However, there are no theoreticalfoundations for proving the convergence of the GS Algorithm. In this paper, wepropose a generic theoretical framework consisting of three key GS components:simplex of generated sequence set, monotonic and continuous objective functionand closed mapping. We prove that GS algorithms with such components can betransformed to fit the Zangwill's convergence theorem, and the sequence setgenerated by the GS procedures always terminates at a local maximum, or atworst, contains a subsequence which converges to a local maximum of thesimilarity measure function. The framework is verified by expanding it to otherGS-type algorithms and experimental results.
arxiv-3900-162 | Sparse Inverse Covariance Matrix Estimation Using Quadratic Approximation | http://arxiv.org/abs/1306.3212 | author:Cho-Jui Hsieh, Matyas A. Sustik, Inderjit S. Dhillon, Pradeep Ravikumar category:cs.LG stat.ML published:2013-06-13 summary:The L1-regularized Gaussian maximum likelihood estimator (MLE) has been shownto have strong statistical guarantees in recovering a sparse inverse covariancematrix, or alternatively the underlying graph structure of a Gaussian MarkovRandom Field, from very limited samples. We propose a novel algorithm forsolving the resulting optimization problem which is a regularizedlog-determinant program. In contrast to recent state-of-the-art methods thatlargely use first order gradient information, our algorithm is based onNewton's method and employs a quadratic approximation, but with somemodifications that leverage the structure of the sparse Gaussian MLE problem.We show that our method is superlinearly convergent, and present experimentalresults using synthetic and real-world application data that demonstrate theconsiderable improvements in performance of our method when compared to otherstate-of-the-art methods.
arxiv-3900-163 | Non-parametric Power-law Data Clustering | http://arxiv.org/abs/1306.3003 | author:Xuhui Fan, Yiling Zeng, Longbing Cao category:cs.LG cs.CV stat.ML published:2013-06-13 summary:It has always been a great challenge for clustering algorithms toautomatically determine the cluster numbers according to the distribution ofdatasets. Several approaches have been proposed to address this issue,including the recent promising work which incorporate Bayesian Nonparametricsinto the $k$-means clustering procedure. This approach shows simplicity inimplementation and solidity in theory, while it also provides a feasible way toinference in large scale datasets. However, several problems remains unsolvedin this pioneering work, including the power-law data applicability, mechanismto merge centers to avoid the over-fitting problem, clustering order problem,e.t.c.. To address these issues, the Pitman-Yor Process based k-means (namely\emph{pyp-means}) is proposed in this paper. Taking advantage of the Pitman-YorProcess, \emph{pyp-means} treats clusters differently by dynamically andadaptively changing the threshold to guarantee the generation of power-lawclustering results. Also, one center agglomeration procedure is integrated intothe implementation to be able to merge small but close clusters and thenadaptively determine the cluster number. With more discussion on the clusteringorder, the convergence proof, complexity analysis and extension to spectralclustering, our approach is compared with traditional clustering algorithm andvariational inference methods. The advantages and properties of pyp-means arevalidated by experiments on both synthetic datasets and real world datasets.
arxiv-3900-164 | Segmentation et Interprétation de Nuages de Points pour la Modélisation d'Environnements Urbains | http://arxiv.org/abs/1306.3084 | author:Jorge Hernandez, Beatriz Marcotegui category:cs.CV published:2013-06-13 summary:Dans cet article, nous pr\'esentons une m\'ethode pour la d\'etection et laclassification d'artefacts au niveau du sol, comme phase de filtragepr\'ealable \`a la mod\'elisation d'environnements urbains. La m\'ethode ded\'etection est r\'ealis\'ee sur l'image profondeur, une projection de nuage depoints sur un plan image o\`u la valeur du pixel correspond \`a la distance dupoint au plan. En faisant l'hypoth\`ese que les artefacts sont situ\'es au sol,ils sont d\'etect\'es par une transformation de chapeau haut de forme parremplissage de trous sur l'image de profondeur. Les composantes connexes ainsiobtenues, sont ensuite caract\'eris\'ees et une analyse des variables estutilis\'ee pour la s\'election des caract\'eristiques les plus discriminantes.Les composantes connexes sont donc classifi\'ees en quatre cat\'egories(lampadaires, pi\'etons, voitures et "Reste") \`a l'aide d'un algorithmed'apprentissage supervis\'e. La m\'ethode a \'et\'e test\'ee sur des nuages depoints de la ville de Paris, en montrant de bons r\'esultats de d\'etection etde classification dans l'ensemble de donn\'ees.---In this article, we present amethod for detection and classification of artifacts at the street level, inorder to filter cloud point, facilitating the urban modeling process. Ourapproach exploits 3D information by using range image, a projection of 3Dpoints onto an image plane where the pixel intensity is a function of themeasured distance between 3D points and the plane. By assuming that theartifacts are on the ground, they are detected using a Top-Hat of the holefilling algorithm of range images. Then, several features are extracted fromthe detected connected components and a stepwise forward variable/modelselection by using the Wilk's Lambda criterion is performed. Afterward, CCs areclassified in four categories (lampposts, pedestrians, cars and others) byusing a supervised machine learning method. The proposed method was tested oncloud points of Paris, and have shown satisfactory results on the wholedataset.
arxiv-3900-165 | Confidence Intervals and Hypothesis Testing for High-Dimensional Regression | http://arxiv.org/abs/1306.3171 | author:Adel Javanmard, Andrea Montanari category:stat.ME cs.IT cs.LG math.IT published:2013-06-13 summary:Fitting high-dimensional statistical models often requires the use ofnon-linear parameter estimation procedures. As a consequence, it is generallyimpossible to obtain an exact characterization of the probability distributionof the parameter estimates. This in turn implies that it is extremelychallenging to quantify the \emph{uncertainty} associated with a certainparameter estimate. Concretely, no commonly accepted procedure exists forcomputing classical measures of uncertainty and statistical significance asconfidence intervals or $p$-values for these models. We consider here high-dimensional linear regression problem, and propose anefficient algorithm for constructing confidence intervals and $p$-values. Theresulting confidence intervals have nearly optimal size. When testing for thenull hypothesis that a certain parameter is vanishing, our method has nearlyoptimal power. Our approach is based on constructing a `de-biased' version of regularizedM-estimators. The new construction improves over recent work in the field inthat it does not assume a special structure on the design matrix. We test ourmethod on synthetic data and a high-throughput genomic data set aboutriboflavin production rate.
arxiv-3900-166 | Physeter catodon localization by sparse coding | http://arxiv.org/abs/1306.3058 | author:Sébastien Paris, Yann Doh, Hervé Glotin, Xanadu Halkias, Joseph Razik category:cs.LG cs.CE stat.ML published:2013-06-13 summary:This paper presents a spermwhale' localization architecture using jointly abag-of-features (BoF) approach and machine learning framework. BoF methods areknown, especially in computer vision, to produce from a collection of localfeatures a global representation invariant to principal signal transformations.Our idea is to regress supervisely from these local features two roughestimates of the distance and azimuth thanks to some datasets where bothacoustic events and ground-truth position are now available. Furthermore, theseestimates can feed a particle filter system in order to obtain a precisespermwhale' position even in mono-hydrophone configuration. Anti-collisionsystem and whale watching are considered applications of this work.
arxiv-3900-167 | The Ripple Pond: Enabling Spiking Networks to See | http://arxiv.org/abs/1306.3036 | author:Saeed Afshar, Gregory Cohen, Runchun Wang, Andre van Schaik, Jonathan Tapson, Torsten Lehmann, Tara Julia Hamilton category:cs.NE q-bio.NC published:2013-06-13 summary:In this paper we present the biologically inspired Ripple Pond Network (RPN),a simply connected spiking neural network that, operating together withrecently proposed PolyChronous Networks (PCN), enables rapid, unsupervised,scale and rotation invariant object recognition using efficient spatio-temporalspike coding. The RPN has been developed as a hardware solution linkingpreviously implemented neuromorphic vision and memory structures capable ofdelivering end-to-end high-speed, low-power and low-resolution recognition formobile and autonomous applications where slow, highly sophisticated and powerhungry signal processing solutions are ineffective. Key aspects in the proposedapproach include utilising the spatial properties of physically embedded neuralnetworks and propagating waves of activity therein for information processing,using dimensional collapse of imagery information into amenable temporalpatterns and the use of asynchronous frames for information binding.
arxiv-3900-168 | Learning to encode motion using spatio-temporal synchrony | http://arxiv.org/abs/1306.3162 | author:Kishore Reddy Konda, Roland Memisevic, Vincent Michalski category:cs.CV cs.LG stat.ML published:2013-06-13 summary:We consider the task of learning to extract motion from videos. To this end,we show that the detection of spatial transformations can be viewed as thedetection of synchrony between the image sequence and a sequence of featuresundergoing the motion we wish to detect. We show that learning about synchronyis possible using very fast, local learning rules, by introducingmultiplicative "gating" interactions between hidden units across frames. Thismakes it possible to achieve competitive performance in a wide variety ofmotion estimation tasks, using a small fraction of the time required to learnfeatures, and to outperform hand-crafted spatio-temporal features by a largemargin. We also show how learning about synchrony can be viewed as performinggreedy parameter estimation in the well-known motion energy model.
arxiv-3900-169 | A Face-like Structure Detection on Planet and Satellite Surfaces using Image Processing | http://arxiv.org/abs/1306.3032 | author:Kazutaka Kurihara, Masakazu Takasu, Kazuhiro Sasao, Hal Seki, Takayuki Narabu, Mitsuo Yamamoto, Satoshi Iida, Hiroyuki Yamamoto category:cs.CV published:2013-06-13 summary:This paper demonstrates that face-like structures are everywhere, and can bede-tected automatically even with computers. Huge amount of satellite images ofthe Earth, the Moon, the Mars are explored and many interesting face-likestructure are detected. Throughout this fact, we believe that science andtechnologies can alert people not to easily become an occultist.
arxiv-3900-170 | Second Order Swarm Intelligence | http://arxiv.org/abs/1306.3018 | author:Vitorino Ramos, David M. S. Rodrigues, Jorge Louçã category:cs.NE 68T05 published:2013-06-13 summary:An artificial Ant Colony System (ACS) algorithm to solve general-purposecombinatorial Optimization Problems (COP) that extends previous AC models [21]by the inclusion of a negative pheromone, is here described. Several TravellingSalesman Problem (TSP) were used as benchmark. We show that by using twodifferent sets of pheromones, a second-order co-evolved compromise betweenpositive and negative feedbacks achieves better results than single positivefeedback systems. The algorithm was tested against known NP-completecombinatorial Optimization Problems, running on symmetrical TSP's. We show thatthe new algorithm compares favourably against these benchmarks, accordingly torecent biological findings by Robinson [26,27], and Gruter [28] where "Noentry" signals and negative feedback allows a colony to quickly reallocate themajority of its foragers to superior food patches. This is the first time anextended ACS algorithm is implemented with these successful characteristics.
arxiv-3900-171 | Dynamic Infinite Mixed-Membership Stochastic Blockmodel | http://arxiv.org/abs/1306.2999 | author:Xuhui Fan, Longbing Cao, Richard Yi Da Xu category:cs.SI cs.LG stat.ML published:2013-06-13 summary:Directional and pairwise measurements are often used to modelinter-relationships in a social network setting. The Mixed-MembershipStochastic Blockmodel (MMSB) was a seminal work in this area, and many of itscapabilities were extended since then. In this paper, we propose the\emph{Dynamic Infinite Mixed-Membership stochastic blockModel (DIM3)}, ageneralised framework that extends the existing work to a potentially infinitenumber of communities and mixture memberships for each of the network's nodes.This model is in a dynamic setting, where additional model parameters areintroduced to reflect the degree of persistence between one's memberships atconsecutive times. Accordingly, two effective posterior sampling strategies andtheir results are presented using both synthetic and real data.
arxiv-3900-172 | Optimization of Clustering for Clustering-based Image Denoising | http://arxiv.org/abs/1306.2967 | author:Mohsen Joneidi, Mostafa Sadeghi category:cs.CV published:2013-06-12 summary:In this paper, the problem of de-noising of an image contaminated withadditive white Gaussian noise (AWGN) is studied. This subject has beencontinued to be an open problem in signal processing for more than 50 years. Inthe present paper, we suggest a method based on global clustering of imageconstructing blocks. Noting that the type of clustering plays an important rolein clustering-based de-noising methods, we address two questions about theclustering. First, which parts of data should be considered for clustering?Second, what data clustering method is suitable for de-noising? Clustering isexploited to learn an over complete dictionary. By obtaining sparsedecomposition of the noisy image blocks in terms of the dictionary atoms, thede-noised version is achieved. Experimental results show that our dictionarylearning framework outperforms traditional dictionary learning methods such asK-SVD.
arxiv-3900-173 | Copula Mixed-Membership Stochastic Blockmodel for Intra-Subgroup Correlations | http://arxiv.org/abs/1306.2733 | author:Xuhui Fan, Longbing Cao, Richard Yi Da Xu category:cs.LG stat.ML published:2013-06-12 summary:The \emph{Mixed-Membership Stochastic Blockmodel (MMSB)} is a popularframework for modeling social network relationships. It can fully exploit eachindividual node's participation (or membership) in a social structure. Despiteits powerful representations, this model makes an assumption that thedistributions of relational membership indicators between two nodes areindependent. Under many social network settings, however, it is possible thatcertain known subgroups of people may have high or low correlations in terms oftheir membership categories towards each other, and such prior informationshould be incorporated into the model. To this end, we introduce a \emph{CopulaMixed-Membership Stochastic Blockmodel (cMMSB)} where an individual Copulafunction is employed to jointly model the membership pairs of those nodeswithin the subgroup of interest. The model enables the use of various Copulafunctions to suit the scenario, while maintaining the membership's marginaldistribution, as needed, for modeling membership indicators with other nodesoutside of the subgroup of interest. We describe the proposed model and itsinference algorithm in detail for both the finite and infinite cases. In theexperiment section, we compare our algorithms with other popular models interms of link prediction, using both synthetic and real world data.
arxiv-3900-174 | Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC | http://arxiv.org/abs/1306.2861 | author:Roger Frigola, Fredrik Lindsten, Thomas B. Schön, Carl E. Rasmussen category:stat.ML cs.LG cs.SY published:2013-06-12 summary:State-space models are successfully used in many areas of science,engineering and economics to model time series and dynamical systems. Wepresent a fully Bayesian approach to inference \emph{and learning} (i.e. stateestimation and system identification) in nonlinear nonparametric state-spacemodels. We place a Gaussian process prior over the state transition dynamics,resulting in a flexible model able to capture complex dynamical phenomena. Toenable efficient inference, we marginalize over the transition dynamicsfunction and infer directly the joint smoothing distribution using speciallytailored Particle Markov Chain Monte Carlo samplers. Once a sample from thesmoothing distribution is computed, the state transition predictivedistribution can be formulated analytically. Our approach preserves the fullnonparametric expressivity of the model and can make use of sparse Gaussianprocesses to greatly reduce computational complexity.
arxiv-3900-175 | Reinforcement learning with restrictions on the action set | http://arxiv.org/abs/1306.2918 | author:Mario Bravo, Mathieu Faure category:cs.GT cs.LG math.PR published:2013-06-12 summary:Consider a 2-player normal-form game repeated over time. We introduce anadaptive learning procedure, where the players only observe their own realizedpayoff at each stage. We assume that agents do not know their own payofffunction, and have no information on the other player. Furthermore, we assumethat they have restrictions on their own action set such that, at each stage,their choice is limited to a subset of their action set. We prove that theempirical distributions of play converge to the set of Nash equilibria forzero-sum and potential games, and games where one player has two actions.
arxiv-3900-176 | Robust Support Vector Machines for Speaker Verification Task | http://arxiv.org/abs/1306.2906 | author:Kawthar Yasmine Zergat, Abderrahmane Amrouche category:cs.LG cs.SD stat.ML published:2013-06-12 summary:An important step in speaker verification is extracting features that bestcharacterize the speaker voice. This paper investigates a front-end processingthat aims at improving the performance of speaker verification based on theSVMs classifier, in text independent mode. This approach combines featuresbased on conventional Mel-cepstral Coefficients (MFCCs) and Line SpectralFrequencies (LSFs) to constitute robust multivariate feature vectors. To reducethe high dimensionality required for training these feature vectors, we use adimension reduction method called principal component analysis (PCA). In orderto evaluate the robustness of these systems, different noisy environments havebeen used. The obtained results using TIMIT database showed that, using theparadigm that combines these spectral cues leads to a significant improvementin verification accuracy, especially with PCA reduction for low signal-to-noiseratio noisy environment.
arxiv-3900-177 | Random Drift Particle Swarm Optimization | http://arxiv.org/abs/1306.2863 | author:Jun Sun, Xiaojun Wu, Vasile Palade, Wei Fang, Yuhui Shi category:cs.AI cs.NE math.OC 68T20 published:2013-06-12 summary:The random drift particle swarm optimization (RDPSO) algorithm, inspired bythe free electron model in metal conductors placed in an external electricfield, is presented, systematically analyzed and empirically studied in thispaper. The free electron model considers that electrons have both a thermal anda drift motion in a conductor that is placed in an external electric field. Themotivation of the RDPSO algorithm is described first, and the velocity equationof the particle is designed by simulating the thermal motion as well as thedrift motion of the electrons, both of which lead the electrons to a locationwith minimum potential energy in the external electric field. Then, acomprehensive analysis of the algorithm is made, in order to provide a deepinsight into how the RDPSO algorithm works. It involves a theoretical analysisand the simulation of the stochastic dynamical behavior of a single particle inthe RDPSO algorithm. The search behavior of the algorithm itself is alsoinvestigated in detail, by analyzing the interaction between the particles.Some variants of the RDPSO algorithm are proposed by incorporating differentrandom velocity components with different neighborhood topologies. Finally,empirical studies on the RDPSO algorithm are performed by using a set ofbenchmark functions from the CEC2005 benchmark suite. Based on the theoreticalanalysis of the particle's behavior, two methods of controlling the algorithmicparameters are employed, followed by an experimental analysis on how to selectthe parameter values, in order to obtain a good overall performance of theRDPSO algorithm and its variants in real-world applications. A furtherperformance comparison between the RDPSO algorithms and other variants of PSOis made to prove the efficiency of the RDPSO algorithms.
arxiv-3900-178 | The Quantum Challenge in Concept Theory and Natural Language Processing | http://arxiv.org/abs/1306.2838 | author:Diederik Aerts, Jan Broekaert, Sandro Sozzo, Tomas Veloz category:cs.CL cs.IR quant-ph published:2013-06-12 summary:The mathematical formalism of quantum theory has been successfully used inhuman cognition to model decision processes and to deliver representations ofhuman knowledge. As such, quantum cognition inspired tools have improvedtechnologies for Natural Language Processing and Information Retrieval. In thispaper, we overview the quantum cognition approach developed in our Brusselsteam during the last two decades, specifically our identification of quantumstructures in human concepts and language, and the modeling of data frompsychological and corpus-text-based experiments. We discuss ourquantum-theoretic framework for concepts and their conjunctions/disjunctions ina Fock-Hilbert space structure, adequately modeling a large amount of datacollected on concept combinations. Inspired by this modeling, we put forwardelements for a quantum contextual and meaning-based approach to informationtechnologies in which 'entities of meaning' are inversely reconstructed fromtexts, which are considered as traces of these entities' states.
arxiv-3900-179 | Flexible sampling of discrete data correlations without the marginal distributions | http://arxiv.org/abs/1306.2685 | author:Alfredo Kalaitzis, Ricardo Silva category:stat.ML cs.LG stat.CO published:2013-06-12 summary:Learning the joint dependence of discrete variables is a fundamental problemin machine learning, with many applications including prediction, clusteringand dimensionality reduction. More recently, the framework of copula modelinghas gained popularity due to its modular parametrization of jointdistributions. Among other properties, copulas provide a recipe for combiningflexible models for univariate marginal distributions with parametric familiessuitable for potentially high dimensional dependence structures. Moreradically, the extended rank likelihood approach of Hoff (2007) bypasseslearning marginal models completely when such information is ancillary to thelearning task at hand as in, e.g., standard dimensionality reduction problemsor copula parameter estimation. The main idea is to represent data by theirobservable rank statistics, ignoring any other information from the marginals.Inference is typically done in a Bayesian framework with Gaussian copulas, andit is complicated by the fact this implies sampling within a space where thenumber of constraints increases quadratically with the number of data points.The result is slow mixing when using off-the-shelf Gibbs sampling. We presentan efficient algorithm based on recent advances on constrained HamiltonianMarkov chain Monte Carlo that is simple to implement and does not requirepaying for a quadratic cost in sample size.
arxiv-3900-180 | Completing Any Low-rank Matrix, Provably | http://arxiv.org/abs/1306.2979 | author:Yudong Chen, Srinadh Bhojanapalli, Sujay Sanghavi, Rachel Ward category:stat.ML cs.IT cs.LG math.IT published:2013-06-12 summary:Matrix completion, i.e., the exact and provable recovery of a low-rank matrixfrom a small subset of its elements, is currently only known to be possible ifthe matrix satisfies a restrictive structural constraint---known as {\emincoherence}---on its row and column spaces. In these cases, the subset ofelements is sampled uniformly at random. In this paper, we show that {\em any} rank-$ r $ $ n$-by-$ n $ matrix can beexactly recovered from as few as $O(nr \log^2 n)$ randomly chosen elements,provided this random choice is made according to a {\em specific biaseddistribution}: the probability of any element being sampled should beproportional to the sum of the leverage scores of the corresponding row, andcolumn. Perhaps equally important, we show that this specific form of samplingis nearly necessary, in a natural precise sense; this implies that otherperhaps more intuitive sampling schemes fail. We further establish three ways to use the above result for the setting whenleverage scores are not known \textit{a priori}: (a) a sampling strategy forthe case when only one of the row or column spaces are incoherent, (b) atwo-phase sampling procedure for general matrices that first samples toestimate leverage scores followed by sampling for exact recovery, and (c) ananalysis showing the advantages of weighted nuclear/trace-norm minimizationover the vanilla un-weighted formulation for the case of non-uniform sampling.
arxiv-3900-181 | Recurrent Convolutional Neural Networks for Scene Parsing | http://arxiv.org/abs/1306.2795 | author:Pedro H. O. Pinheiro, Ronan Collobert category:cs.CV published:2013-06-12 summary:Scene parsing is a technique that consist on giving a label to all pixels inan image according to the class they belong to. To ensure a good visualcoherence and a high class accuracy, it is essential for a scene parser tocapture image long range dependencies. In a feed-forward architecture, this canbe simply achieved by considering a sufficiently large input context patch,around each pixel to be labeled. We propose an approach consisting of arecurrent convolutional neural network which allows us to consider a largeinput context, while limiting the capacity of the model. Contrary to moststandard approaches, our method does not rely on any segmentation methods, norany task-specific features. The system is trained in an end-to-end manner overraw pixels, and models complex spatial dependencies with low inference cost. Asthe context size increases with the built-in recurrence, the system identifiesand corrects its own errors. Our approach yields state-of-the-art performanceon both the Stanford Background Dataset and the SIFT Flow Dataset, whileremaining very fast at test time.
arxiv-3900-182 | Understanding Dropout: Training Multi-Layer Perceptrons with Auxiliary Independent Stochastic Neurons | http://arxiv.org/abs/1306.2801 | author:Kyunghyun Cho category:cs.NE cs.LG stat.ML published:2013-06-12 summary:In this paper, a simple, general method of adding auxiliary stochasticneurons to a multi-layer perceptron is proposed. It is shown that the proposedmethod is a generalization of recently successful methods of dropout (Hinton etal., 2012), explicit noise injection (Vincent et al., 2010; Bishop, 1995) andsemantic hashing (Salakhutdinov & Hinton, 2009). Under the proposed framework,an extension of dropout which allows using separate dropping probabilities fordifferent hidden neurons, or layers, is found to be available. The use ofdifferent dropping probabilities for hidden layers separately is empiricallyinvestigated.
arxiv-3900-183 | Sparse Representation-based Image Quality Assessment | http://arxiv.org/abs/1306.2727 | author:Tanaya Guha, Ehsan Nezhadarya, Rabab K Ward category:cs.CV cs.MM published:2013-06-12 summary:A successful approach to image quality assessment involves comparing thestructural information between a distorted and its reference image. However,extracting structural information that is perceptually important to our visualsystem is a challenging task. This paper addresses this issue by employing asparse representation-based approach and proposes a new metric called the\emph{sparse representation-based quality} (SPARQ) \emph{index}. The proposedmethod learns the inherent structures of the reference image as a set of basisvectors, such that any structure in the image can be represented by a linearcombination of only a few of those basis vectors. This sparse strategy isemployed because it is known to generate basis vectors that are qualitativelysimilar to the receptive field of the simple cells present in the mammalianprimary visual cortex. The visual quality of the distorted image is estimatedby comparing the structures of the reference and the distorted images in termsof the learnt basis vectors resembling cortical cells. Our approach isevaluated on six publicly available subject-rated image quality assessmentdatasets. The proposed SPARQ index consistently exhibits high correlation withthe subjective ratings on all datasets and performs better or at par with thestate-of-the-art.
arxiv-3900-184 | Horizontal and Vertical Ensemble with Deep Representation for Classification | http://arxiv.org/abs/1306.2759 | author:Jingjing Xie, Bing Xu, Zhang Chuang category:cs.LG stat.ML published:2013-06-12 summary:Representation learning, especially which by using deep learning, has beenwidely applied in classification. However, how to use limited size of labeleddata to achieve good classification performance with deep neural network, andhow can the learned features further improve classification remain indefinite.In this paper, we propose Horizontal Voting Vertical Voting and HorizontalStacked Ensemble methods to improve the classification performance of deepneural networks. In the ICML 2013 Black Box Challenge, via using these methodsindependently, Bing Xu achieved 3rd in public leaderboard, and 7th in privateleaderboard; Jingjing Xie achieved 4th in public leaderboard, and 5th inprivate leaderboard.
arxiv-3900-185 | R3MC: A Riemannian three-factor algorithm for low-rank matrix completion | http://arxiv.org/abs/1306.2672 | author:B. Mishra, R. Sepulchre category:math.OC cs.LG published:2013-06-11 summary:We exploit the versatile framework of Riemannian optimization on quotientmanifolds to develop R3MC, a nonlinear conjugate-gradient method for low-rankmatrix completion. The underlying search space of fixed-rank matrices isendowed with a novel Riemannian metric that is tailored to the least-squarescost. Numerical comparisons suggest that R3MC robustly outperformsstate-of-the-art algorithms across different problem instances, especiallythose that combine scarcely sampled and ill-conditioned data.
arxiv-3900-186 | DISCOMAX: A Proximity-Preserving Distance Correlation Maximization Algorithm | http://arxiv.org/abs/1306.2533 | author:Praneeth Vepakomma, Ahmed Elgammal category:cs.LG stat.ML published:2013-06-11 summary:In a regression setting we propose algorithms that reduce the dimensionalityof the features while simultaneously maximizing a statistical measure ofdependence known as distance correlation between the low-dimensional featuresand a response variable. This helps in solving the prediction problem with alow-dimensional set of features. Our setting is different from subset-selectionalgorithms where the problem is to choose the best subset of features forregression. Instead, we attempt to generate a new set of low-dimensionalfeatures as in a feature-learning setting. We attempt to keep our proposedapproach as model-free and our algorithm does not assume the application of anyspecific regression model in conjunction with the low-dimensional features thatit learns. The algorithm is iterative and is fomulated as a combination of themajorization-minimization and concave-convex optimization procedures. We alsopresent spectral radius based convergence results for the proposed iterations.
arxiv-3900-187 | Precisely Verifying the Null Space Conditions in Compressed Sensing: A Sandwiching Algorithm | http://arxiv.org/abs/1306.2665 | author:Myung Cho, Weiyu Xu category:cs.IT cs.LG cs.SY math.IT math.OC stat.ML published:2013-06-11 summary:In this paper, we propose new efficient algorithms to verify the null spacecondition in compressed sensing (CS). Given an $(n-m) \times n$ ($m>0$) CSmatrix $A$ and a positive $k$, we are interested in computing $\displaystyle\alpha_k = \max_{\{z: Az=0,z\neq 0\}}\max_{\{K: K\leq k\}}$ ${\z_K\_{1}}{\z\_{1}}$, where $K$ represents subsets of $\{1,2,...,n\}$, and $K$is the cardinality of $K$. In particular, we are interested in finding themaximum $k$ such that $\alpha_k < {1}{2}$. However, computing $\alpha_k$ isknown to be extremely challenging. In this paper, we first propose a series ofnew polynomial-time algorithms to compute upper bounds on $\alpha_k$. Based onthese new polynomial-time algorithms, we further design a new sandwichingalgorithm, to compute the \emph{exact} $\alpha_k$ with greatly reducedcomplexity. When needed, this new sandwiching algorithm also achieves a smoothtradeoff between computational complexity and result accuracy. Empiricalresults show the performance improvements of our algorithm over existing knownmethods; and our algorithm outputs precise values of $\alpha_k$, with muchlower complexity than exhaustive search.
arxiv-3900-188 | Efficient Classification for Metric Data | http://arxiv.org/abs/1306.2547 | author:Lee-Ad Gottlieb, Aryeh Kontorovich, Robert Krauthgamer category:cs.LG cs.DS stat.ML published:2013-06-11 summary:Recent advances in large-margin classification of data residing in generalmetric spaces (rather than Hilbert spaces) enable classification under variousnatural metrics, such as string edit and earthmover distance. A generalframework developed for this purpose by von Luxburg and Bousquet [JMLR, 2004]left open the questions of computational efficiency and of providing directbounds on generalization error. We design a new algorithm for classification in general metric spaces, whoseruntime and accuracy depend on the doubling dimension of the data points, andcan thus achieve superior classification performance in many common scenarios.The algorithmic core of our approach is an approximate (rather than exact)solution to the classical problems of Lipschitz extension and of NearestNeighbor Search. The algorithm's generalization performance is guaranteed viathe fat-shattering dimension of Lipschitz classifiers, and we presentexperimental evidence of its superiority to some common kernel methods. As aby-product, we offer a new perspective on the nearest neighbor classifier,which yields significantly sharper risk asymptotics than the classic analysisof Cover and Hart [IEEE Trans. Info. Theory, 1967].
arxiv-3900-189 | Using Arabic Wordnet for semantic indexation in information retrieval system | http://arxiv.org/abs/1306.2499 | author:Mohammed Alaeddine Abderrahim, Mohammed El Amine Abderrahim, Mohammed Amine Chikh category:cs.IR cs.CL published:2013-06-11 summary:In the context of arabic Information Retrieval Systems (IRS) guided by arabicontology and to enable those systems to better respond to user requirements,this paper aims to representing documents and queries by the best conceptsextracted from Arabic Wordnet. Identified concepts belonging to Arabic WordNetsynsets are extracted from documents and queries, and those having a singlesense are expanded. The expanded query is then used by the IRS to retrieve therelevant documents searched. Our experiments are based primarily on a mediumsize corpus of arabic text. The results obtained shown us that there are aglobal improvement in the performance of the arabic IRS.
arxiv-3900-190 | The association problem in wireless networks: a Policy Gradient Reinforcement Learning approach | http://arxiv.org/abs/1306.2554 | author:Richard Combes, Ilham El Bouloumi, Stephane Senecal, Zwi Altman category:cs.NI cs.IT cs.LG math.IT published:2013-06-11 summary:The purpose of this paper is to develop a self-optimized associationalgorithm based on PGRL (Policy Gradient Reinforcement Learning), which is bothscalable, stable and robust. The term robust means that performance degradationin the learning phase should be forbidden or limited to predefined thresholds.The algorithm is model-free (as opposed to Value Iteration) and robust (asopposed to Q-Learning). The association problem is modeled as a Markov DecisionProcess (MDP). The policy space is parameterized. The parameterized family ofpolicies is then used as expert knowledge for the PGRL. The PGRL convergestowards a local optimum and the average cost decreases monotonically during thelearning process. The properties of the solution make it a good candidate forpractical implementation. Furthermore, the robustness property allows to usethe PGRL algorithm in an "always-on" learning mode.
arxiv-3900-191 | Fast LSTD using stochastic approximation: Finite time analysis and application to traffic control | http://arxiv.org/abs/1306.2557 | author:L. A. Prashanth, Nathaniel Korda, Rémi Munos category:cs.LG stat.ML published:2013-06-11 summary:We propose a stochastic approximation based method with randomisation ofsamples for policy evaluation using the least squares temporal difference(LSTD) algorithm. Our method results in an $O(d)$ improvement in complexity incomparison to regular LSTD, where $d$ is the dimension of the data. We provideconvergence rate results for our proposed method, both in high probability andin expectation. Moreover, we also establish that using our scheme in place ofLSTD does not impact the rate of convergence of the approximate value functionto the true value function. This result coupled with the low complexity of ourmethod makes it attractive for implementation in big data settings, where $d$is large. Further, we also analyse a similar low-complexity alternative forleast squares regression and provide finite-time bounds there. We demonstratethe practicality of our method for LSTD empirically by combining it with theLSPI algorithm in a traffic signal control application.
arxiv-3900-192 | A 10-dimensional Phonetic-prosodic Space and its Stochastic Structure (A framework for probabilistic modeling of spoken languages and their phonology) | http://arxiv.org/abs/1306.2593 | author:Elaine Tsiang category:cs.SD cs.CL I.2.7 published:2013-06-11 summary:We formulate a phonetic-prosodic space based on attributes as perceptualobservables, rather than articulatory specifications. We propose an alphabet asmarkers in the phonetic subspace, aiming for a resolution sufficient to supportrecognition of all spoken languages. The prosodic subspace is made up ofdirectly measurable physical variables. With the proposed alphabet, traditionaldiphthongs naturally generalize to a broader class of language-neutralphonotactic constraints, indicating a correlation structure similar to that ofthe traditional sonority-based syllable. We define a stochastic structure onthe phone strings based on this diphthongal constraint, and show how a specificspoken language can be defined as a specific set of probability distributionsof this stochastic structure. Furthermore, phonological variations within aspoken language can be modeled as varying probability distributions restrictedto the phonetic subspace, conditioned on different values in the prosodicsubspace.
arxiv-3900-193 | Hand Gesture Recognition Based on Karhunen-Loeve Transform | http://arxiv.org/abs/1306.2599 | author:Joyeeta Singha, Karen Das category:cs.CV published:2013-06-11 summary:In this paper, we have proposed a system based on K-L Transform to recognizedifferent hand gestures. The system consists of five steps: skin filtering,palm cropping, edge detection, feature extraction, and classification. Firstlythe hand is detected using skin filtering and palm cropping was performed toextract out only the palm portion of the hand. The extracted image was thenprocessed using the Canny Edge Detection technique to extract the outlineimages of palm. After palm extraction, the features of hand were extractedusing K-L Transform technique and finally the input gesture was recognizedusing proper classifier. In our system, we have tested for 10 different handgestures, and recognizing rate obtained was 96%. Hence we propose an easyapproach to recognize different hand gestures.
arxiv-3900-194 | Stopping Criterion for the Mean Shift Iterative Algorithm | http://arxiv.org/abs/1306.2624 | author:Yasel Garcés Suárez, Esley Torres, Osvaldo Pereira, Claudia Pérez, Roberto Rogríguez category:cs.CV math.RA published:2013-06-11 summary:Image segmentation is a critical step in computer vision tasks constitutingan essential issue for pattern recognition and visual interpretation. In thispaper, we propose a new stopping criterion for the mean shift iterativealgorithm by using images defined in Zn ring, with the goal of reaching abetter segmentation. We carried out also a study on the weak and strong ofequivalence classes between two images. An analysis on the convergence withthis new stopping criterion is carried out too.
arxiv-3900-195 | Large Margin Low Rank Tensor Analysis | http://arxiv.org/abs/1306.2663 | author:Guoqiang Zhong, Mohamed Cheriet category:cs.LG cs.NA 57-04 published:2013-06-11 summary:Other than vector representations, the direct objects of human cognition aregenerally high-order tensors, such as 2D images and 3D textures. From thisfact, two interesting questions naturally arise: How does the human brainrepresent these tensor perceptions in a "manifold" way, and how can they berecognized on the "manifold"? In this paper, we present a supervised model tolearn the intrinsic structure of the tensors embedded in a high dimensionalEuclidean space. With the fixed point continuation procedures, our modelautomatically and jointly discovers the optimal dimensionality and therepresentations of the low dimensional embeddings. This makes it an effectivesimulation of the cognitive process of human brain. Furthermore, thegeneralization of our model based on similarity between the learned lowdimensional embeddings can be viewed as counterpart of recognition of humanbrain. Experiments on applications for object recognition and face recognitiondemonstrate the superiority of our proposed model over state-of-the-artapproaches.
arxiv-3900-196 | A Kernel Test for Three-Variable Interactions | http://arxiv.org/abs/1306.2281 | author:Dino Sejdinovic, Arthur Gretton, Wicher Bergsma category:stat.ME stat.ML published:2013-06-10 summary:We introduce kernel nonparametric tests for Lancaster three-variableinteraction and for total independence, using embeddings of signed measuresinto a reproducing kernel Hilbert space. The resulting test statistics arestraightforward to compute, and are used in powerful interaction tests, whichare consistent against all alternatives for a large family of reproducingkernels. We show the Lancaster test to be sensitive to cases where twoindependent causes individually have weak influence on a third dependentvariable, but their combined effect has a strong influence. This makes theLancaster test especially suited to finding structure in directed graphicalmodels, where it outperforms competing nonparametric tests in detecting suchV-structures.
arxiv-3900-197 | Using the quaternion's representation of individuals in swarm intelligence and evolutionary computation | http://arxiv.org/abs/1306.2257 | author:Iztok Fister, Iztok Fister Jr category:cs.NE published:2013-06-10 summary:This paper introduces a novel idea for representation of individuals usingquaternions in swarm intelligence and evolutionary algorithms. Quaternions area number system, which extends complex numbers. They are successfully appliedto problems of theoretical physics and to those areas needing fast rotationcalculations. We propose the application of quaternions in optimization, moreprecisely, we have been using quaternions for representation of individuals inBat algorithm. The preliminary results of our experiments when optimizing atest-suite consisting of ten standard functions showed that this new algorithmsignificantly improved the results of the original Bat algorithm. Moreover, theobtained results are comparable with other swarm intelligence and evolutionaryalgorithms, like the artificial bees colony, and differential evolution. Webelieve that this representation could also be successfully applied to otherswarm intelligence and evolutionary algorithms.
arxiv-3900-198 | Detection of Outer Rotations on 3D-Vector Fields with Iterative Geometric Correlation and its Efficiency | http://arxiv.org/abs/1307.2457 | author:Roxana Bujack, Gerik Scheuermann, Eckhard Hitzer category:cs.CV cs.GR published:2013-06-10 summary:Correlation is a common technique for the detection of shifts. Itsgeneralization to the multidimensional geometric correlation in Cliffordalgebras has been proven a useful tool for color image processing, because itadditionally contains information about a rotational misalignment. But so farthe exact correction of a three-dimensional outer rotation could only beachieved in certain special cases. In this paper we prove that applying thegeometric correlation iteratively has the potential to detect the outerrotational misalignment for arbitrary three-dimensional vector fields. Wefurther present the explicit iterative algorithm, analyze its efficiencydetecting the rotational misalignment in the color space of a color image. Theexperiments suggest a method for the acceleration of the algorithm, which ispractically tested with great success.
arxiv-3900-199 | Adaptive Noisy Clustering | http://arxiv.org/abs/1306.2194 | author:Michael Chichignoud, Sébastien Loustau category:math.ST stat.ML stat.TH published:2013-06-10 summary:The problem of adaptive noisy clustering is investigated. Given a set ofnoisy observations $Z_i=X_i+\epsilon_i$, $i=1,...,n$, the goal is to designclusters associated with the law of $X_i$'s, with unknown density $f$ withrespect to the Lebesgue measure. Since we observe a corrupted sample, a directapproach as the popular {\it $k$-means} is not suitable in this case. In thispaper, we propose a noisy $k$-means minimization, which is based on the$k$-means loss function and a deconvolution estimator of the density $f$. Inparticular, this approach suffers from the dependence on a bandwidth involvedin the deconvolution kernel. Fast rates of convergence for the excess risk areproposed for a particular choice of the bandwidth, which depends on thesmoothness of the density $f$. Then, we turn out into the main issue of the paper: the data-driven choice ofthe bandwidth. We state an adaptive upper bound for a new selection rule,called ERC (Empirical Risk Comparison). This selection rule is based on theLepski's principle, where empirical risks associated with different bandwidthsare compared. Finally, we illustrate that this adaptive rule can be used inmany statistical problems of $M$-estimation where the empirical risk depends ona nuisance parameter.
arxiv-3900-200 | Image segmentation by optimal and hierarchical piecewise constant approximations | http://arxiv.org/abs/1306.2159 | author:M. Kharinov category:cs.CV published:2013-06-10 summary:Piecewise constant image approximations of sequential number of segments orclusters of disconnected pixels are treated. The method of majorizing ofoptimal approximation sequence by hierarchical sequence of image approximationsis proposed. A generalization for multidimensional case of color andmultispectral images is foreseen.
arxiv-3900-201 | "Not not bad" is not "bad": A distributional account of negation | http://arxiv.org/abs/1306.2158 | author:Karl Moritz Hermann, Edward Grefenstette, Phil Blunsom category:cs.CL 68T50 I.2.7 published:2013-06-10 summary:With the increasing empirical success of distributional models ofcompositional semantics, it is timely to consider the types of textual logicthat such models are capable of capturing. In this paper, we addressshortcomings in the ability of current models to capture logical operationssuch as negation. As a solution we propose a tripartite formulation for acontinuous vector space representation of semantics and subsequently use thisrepresentation to develop a formal compositional notion of negation within suchmodels.
arxiv-3900-202 | Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n) | http://arxiv.org/abs/1306.2119 | author:Francis Bach, Eric Moulines category:cs.LG math.OC stat.ML published:2013-06-10 summary:We consider the stochastic approximation problem where a convex function hasto be minimized, given only the knowledge of unbiased estimates of itsgradients at certain points, a framework which includes machine learningmethods based on the minimization of the empirical risk. We focus on problemswithout strong convexity, for which all previously known algorithms achieve aconvergence rate for function values of O(1/n^{1/2}). We consider and analyzetwo algorithms that achieve a rate of O(1/n) for classical supervised learningproblems. For least-squares regression, we show that averaged stochasticgradient descent with constant step-size achieves the desired rate. Forlogistic regression, this is achieved by a simple novel stochastic gradientalgorithm that (a) constructs successive local quadratic approximations of theloss functions, while (b) preserving the same running time complexity asstochastic gradient descent. For these algorithms, we provide a non-asymptoticanalysis of the generalization error (in expectation, and also in highprobability for least-squares), and run extensive experiments on standardmachine learning benchmarks showing that they often outperform existingapproaches.
arxiv-3900-203 | A Novel Approach for Single Gene Selection Using Clustering and Dimensionality Reduction | http://arxiv.org/abs/1306.2118 | author:E. N. Sathishkumar, K. Thangavel, T. Chandrasekhar category:cs.CE cs.LG published:2013-06-10 summary:We extend the standard rough set-based approach to deal with huge amounts ofnumeric attributes versus small amount of available objects. Here, a novelapproach of clustering along with dimensionality reduction; Hybrid Fuzzy CMeans-Quick Reduct (FCMQR) algorithm is proposed for single gene selection.Gene selection is a process to select genes which are more informative. It isone of the important steps in knowledge discovery. The problem is that allgenes are not important in gene expression data. Some of the genes may beredundant, and others may be irrelevant and noisy. In this study, the entiredataset is divided in proper grouping of similar genes by applying Fuzzy CMeans (FCM) algorithm. A high class discriminated genes has been selected basedon their degree of dependence by applying Quick Reduct algorithm based on RoughSet Theory to all the resultant clusters. Average Correlation Value (ACV) iscalculated for the high class discriminated genes. The clusters which have theACV value a s 1 is determined as significant clusters, whose classificationaccuracy will be equal or high when comparing to the accuracy of the entiredataset. The proposed algorithm is evaluated using WEKA classifiers andcompared. Finally, experimental results related to the leukemia cancer dataconfirm that our approach is quite promising, though it surely requires furtherresearch.
arxiv-3900-204 | Discriminative k-means clustering | http://arxiv.org/abs/1306.2102 | author:Ognjen Arandjelovic category:cs.CV published:2013-06-10 summary:The k-means algorithm is a partitional clustering method. Over 60 years old,it has been successfully used for a variety of problems. The popularity ofk-means is in large part a consequence of its simplicity and efficiency. Inthis paper we are inspired by these appealing properties of k-means in thedevelopment of a clustering algorithm which accepts the notion of "positively"and "negatively" labelled data. The goal is to discover the cluster structureof both positive and negative data in a manner which allows for thediscrimination between the two sets. The usefulness of this idea isdemonstrated practically on the problem of face recognition, where the task oflearning the scope of a person's appearance should be done in a manner whichallows this face to be differentiated from others.
arxiv-3900-205 | Discriminative extended canonical correlation analysis for pattern set matching | http://arxiv.org/abs/1306.2100 | author:Ognjen Arandjelovic category:cs.CV published:2013-06-10 summary:In this paper we address the problem of matching sets of vectors embedded inthe same input space. We propose an approach which is motivated by canonicalcorrelation analysis (CCA), a statistical technique which has proven successfulin a wide variety of pattern recognition problems. Like CCA when applied to thematching of sets, our extended canonical correlation analysis (E-CCA) aims toextract the most similar modes of variability within two sets. Our first majorcontribution is the formulation of a principled framework for robust inferenceof such modes from data in the presence of uncertainty associated with noiseand sampling randomness. E-CCA retains the efficiency and closed formcomputability of CCA, but unlike it, does not possess free parameters whichcannot be inferred directly from data (inherent data dimensionality, and thenumber of canonical correlations used for set similarity computation). Oursecond major contribution is to show that in contrast to CCA, E-CCA is readilyadapted to match sets in a discriminative learning scheme which we calldiscriminative extended canonical correlation analysis (DE-CCA). Theoreticalcontributions of this paper are followed by an empirical evaluation of itspremises on the task of face recognition from sets of rasterized appearanceimages. The results demonstrate that our approach, E-CCA, already outperformsboth CCA and its quasi-discriminative counterpart constrained CCA (C-CCA), forall values of their free parameters. An even greater improvement is achievedwith the discriminative variant, DE-CCA.
arxiv-3900-206 | Predicting Risk-of-Readmission for Congestive Heart Failure Patients: A Multi-Layer Approach | http://arxiv.org/abs/1306.2094 | author:Kiyana Zolfaghar, Nele Verbiest, Jayshree Agarwal, Naren Meadem, Si-Chi Chin, Senjuti Basu Roy, Ankur Teredesai, David Hazel, Paul Amoroso, Lester Reed category:cs.LG stat.AP published:2013-06-10 summary:Mitigating risk-of-readmission of Congestive Heart Failure (CHF) patientswithin 30 days of discharge is important because such readmissions are not onlyexpensive but also critical indicator of provider care and quality oftreatment. Accurately predicting the risk-of-readmission may allow hospitals toidentify high-risk patients and eventually improve quality of care byidentifying factors that contribute to such readmissions in many scenarios. Inthis paper, we investigate the problem of predicting risk-of-readmission as asupervised learning problem, using a multi-layer classification approach.Earlier contributions inadequately attempted to assess a risk value for 30 dayreadmission by building a direct predictive model as opposed to our approach.We first split the problem into various stages, (a) at risk in general (b) riskwithin 60 days (c) risk within 30 days, and then build suitable classifiers foreach stage, thereby increasing the ability to accurately predict the risk usingmultiple layers of decision. The advantage of our approach is that we can usedifferent classification models for the subtasks that are more suited for therespective problems. Moreover, each of the subtasks can be solved usingdifferent features and training data leading to a highly confident diagnosis orrisk compared to a one-shot single layer approach. An experimental evaluationon actual hospital patient record data from Multicare Health Systems shows thatour model is significantly better at predicting risk-of-readmission of CHFpatients within 30 days after discharge compared to prior attempts.
arxiv-3900-207 | Logistic Tensor Factorization for Multi-Relational Data | http://arxiv.org/abs/1306.2084 | author:Maximilian Nickel, Volker Tresp category:stat.ML cs.LG published:2013-06-10 summary:Tensor factorizations have become increasingly popular approaches for variouslearning tasks on structured data. In this work, we extend the RESCAL tensorfactorization, which has shown state-of-the-art results for multi-relationallearning, to account for the binary nature of adjacency tensors. We study theimprovements that can be gained via this approach on various benchmark datasetsand show that the logistic extension can improve the prediction resultssignificantly.
arxiv-3900-208 | 3D model retrieval using global and local radial distances | http://arxiv.org/abs/1306.2081 | author:Bo Li, Henry Johan category:cs.GR cs.CV cs.IR published:2013-06-10 summary:3D model retrieval techniques can be classified as histogram-based,view-based and graph-based approaches. We propose a hybrid shape descriptorwhich combines the global and local radial distance features by utilizing thehistogram-based and view-based approaches respectively. We define anarea-weighted global radial distance with respect to the center of the boundingsphere of the model and encode its distribution into a 2D histogram as theglobal radial distance shape descriptor. We then uniformly divide the boundingcube of a 3D model into a set of small cubes and define their centers as localcenters. Then, we compute the local radial distance of a point based on thenearest local center. By sparsely sampling a set of views and encoding thelocal radial distance feature on the rendered views by color coding, we extractthe local radial distance shape descriptor. Based on these two shapedescriptors, we develop a hybrid radial distance shape descriptor for 3D modelretrieval. Experiment results show that our hybrid shape descriptor outperformsseveral typical histogram-based and view-based approaches.
arxiv-3900-209 | Asymptotically Optimal Sequential Estimation of the Mean Based on Inclusion Principle | http://arxiv.org/abs/1306.2290 | author:Xinjia Chen category:math.ST cs.LG math.PR stat.TH published:2013-06-10 summary:A large class of problems in sciences and engineering can be formulated asthe general problem of constructing random intervals with pre-specifiedcoverage probabilities for the mean. Wee propose a general approach forstatistical inference of mean values based on accumulated observational data.We show that the construction of such random intervals can be accomplished bycomparing the endpoints of random intervals with confidence sequences for themean. Asymptotic results are obtained for such sequential methods.
arxiv-3900-210 | Markov random fields factorization with context-specific independences | http://arxiv.org/abs/1306.2295 | author:Alejandro Edera, Facundo Bromberg, Federico Schlüter category:cs.AI cs.LG published:2013-06-10 summary:Markov random fields provide a compact representation of joint probabilitydistributions by representing its independence properties in an undirectedgraph. The well-known Hammersley-Clifford theorem uses these conditionalindependences to factorize a Gibbs distribution into a set of factors. However,an important issue of using a graph to represent independences is that itcannot encode some types of independence relations, such as thecontext-specific independences (CSIs). They are a particular case ofconditional independences that is true only for a certain assignment of itsconditioning set; in contrast to conditional independences that must hold forall its assignments. This work presents a method for factorizing a Markovrandom field according to CSIs present in a distribution, and formallyguarantees that this factorization is correct. This is presented in our maincontribution, the context-specific Hammersley-Clifford theorem, ageneralization to CSIs of the Hammersley-Clifford theorem that applies forconditional independences.
arxiv-3900-211 | A framework for (under)specifying dependency syntax without overloading annotators | http://arxiv.org/abs/1306.2091 | author:Nathan Schneider, Brendan O'Connor, Naomi Saphra, David Bamman, Manaal Faruqui, Noah A. Smith, Chris Dyer, Jason Baldridge category:cs.CL published:2013-06-10 summary:We introduce a framework for lightweight dependency syntax annotation. Ourformalism builds upon the typical representation for unlabeled dependencies,permitting a simple notation and annotation workflow. Moreover, the formalismencourages annotators to underspecify parts of the syntax if doing so wouldstreamline the annotation process. We demonstrate the efficacy of thisannotation on three languages and develop algorithms to evaluate and compareunderspecified annotations.
arxiv-3900-212 | Generative Model Selection Using a Scalable and Size-Independent Complex Network Classifier | http://arxiv.org/abs/1306.2298 | author:Sadegh Motallebi, Sadegh Aliakbary, Jafar Habibi category:cs.SI cs.LG physics.soc-ph stat.ML published:2013-06-10 summary:Real networks exhibit nontrivial topological features such as heavy-taileddegree distribution, high clustering, and small-worldness. Researchers havedeveloped several generative models for synthesizing artificial networks thatare structurally similar to real networks. An important research problem is toidentify the generative model that best fits to a target network. In thispaper, we investigate this problem and our goal is to select the model that isable to generate graphs similar to a given network instance. By the means ofgenerating synthetic networks with seven outstanding generative models, we haveutilized machine learning methods to develop a decision tree for modelselection. Our proposed method, which is named "Generative Model Selection forComplex Networks" (GMSCN), outperforms existing methods with respect toaccuracy, scalability and size-independence.
arxiv-3900-213 | Auditing: Active Learning with Outcome-Dependent Query Costs | http://arxiv.org/abs/1306.2347 | author:Sivan Sabato, Anand D. Sarwate, Nathan Srebro category:cs.LG published:2013-06-10 summary:We propose a learning setting in which unlabeled data is free, and the costof a label depends on its value, which is not known in advance. We study binaryclassification in an extreme case, where the algorithm only pays for negativelabels. Our motivation are applications such as fraud detection, in whichinvestigating an honest transaction should be avoided if possible. We term thesetting auditing, and consider the auditing complexity of an algorithm: thenumber of negative labels the algorithm requires in order to learn a hypothesiswith low relative error. We design auditing algorithms for simple hypothesisclasses (thresholds and rectangles), and show that with these algorithms, theauditing complexity can be significantly lower than the active labelcomplexity. We also discuss a general competitive approach for auditing andpossible modifications to the framework.
arxiv-3900-214 | Comparing Edge Detection Methods based on Stochastic Entropies and Distances for PolSAR Imagery | http://arxiv.org/abs/1306.2003 | author:Abraão D. C. Nascimento, Michelle M. Horta, Alejandro C. Frery, Renato J. Cintra category:math.ST cs.CV stat.TH published:2013-06-09 summary:Polarimetric synthetic aperture radar (PolSAR) has achieved a prominentposition as a remote imaging method. However, PolSAR images are contaminated byspeckle noise due to the coherent illumination employed during the dataacquisition. This noise provides a granular aspect to the image, making itsprocessing and analysis (such as in edge detection) hard tasks. This paperdiscusses seven methods for edge detection in multilook PolSAR images. In allmethods, the basic idea consists in detecting transition points in the finestpossible strip of data which spans two regions. The edge is contoured using thetransitions points and a B-spline curve. Four stochastic distances, twodifferences of entropies, and the maximum likelihood criterion were used underthe scaled complex Wishart distribution; the first six stem from the h-phiclass of measures. The performance of the discussed detection methods wasquantified and analyzed by the computational time and probability of correctedge detection, with respect to the number of looks, the backscatter matrix asa whole, the SPAN, the covariance an the spatial resolution. The detectionprocedures were applied to three real PolSAR images. Results provide evidencethat the methods based on the Bhattacharyya distance and the difference ofShannon entropies outperform the other techniques.
arxiv-3900-215 | Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation | http://arxiv.org/abs/1306.2035 | author:Martin Azizyan, Aarti Singh, Larry Wasserman category:stat.ML cs.LG math.ST stat.TH published:2013-06-09 summary:While several papers have investigated computationally and statisticallyefficient methods for learning Gaussian mixtures, precise minimax bounds fortheir statistical performance as well as fundamental limits in high-dimensionalsettings are not well-understood. In this paper, we provide precise informationtheoretic bounds on the clustering accuracy and sample complexity of learning amixture of two isotropic Gaussians in high dimensions under small meanseparation. If there is a sparse subset of relevant dimensions that determinethe mean separation, then the sample complexity only depends on the number ofrelevant dimensions and mean separation, and can be achieved by a simplecomputationally efficient procedure. Our results provide the first step of atheoretical basis for recent methods that combine feature selection andclustering.
arxiv-3900-216 | Speckle Reduction with Adaptive Stack Filters | http://arxiv.org/abs/1306.1894 | author:María Elena Buemi, Alejandro C. Frery, Heitor S. Ramos category:cs.CV published:2013-06-08 summary:Stack filters are a special case of non-linear filters. They have a goodperformance for filtering images with different types of noise while preservingedges and details. A stack filter decomposes an input image into stacks ofbinary images according to a set of thresholds. Each binary image is thenfiltered by a Boolean function, which characterizes the filter. Adaptive stackfilters can be computed by training using a prototype (ideal) image and itscorrupted version, leading to optimized filters with respect to a lossfunction. In this work we propose the use of training with selected samples forthe estimation of the optimal Boolean function. We study the performance ofadaptive stack filters when they are applied to speckled imagery, in particularto Synthetic Aperture Radar (SAR) images. This is done by evaluating thequality of the filtered images through the use of suitable image qualityindexes and by measuring the classification accuracy of the resulting images.We used SAR images as input, since they are affected by speckle noise thatmakes classification a difficult task.
arxiv-3900-217 | Learning About Meetings | http://arxiv.org/abs/1306.1927 | author:Been Kim, Cynthia Rudin category:stat.AP cs.CL published:2013-06-08 summary:Most people participate in meetings almost every day, multiple times a day.The study of meetings is important, but also challenging, as it requires anunderstanding of social signals and complex interpersonal dynamics. Our aimthis work is to use a data-driven approach to the science of meetings. Weprovide tentative evidence that: i) it is possible to automatically detect whenduring the meeting a key decision is taking place, from analyzing only thelocal dialogue acts, ii) there are common patterns in the way social dialogueacts are interspersed throughout a meeting, iii) at the time key decisions aremade, the amount of time left in the meeting can be predicted from the amountof time that has passed, iv) it is often possible to predict whether a proposalduring a meeting will be accepted or rejected based entirely on the language(the set of persuasive words) used by the speaker.
arxiv-3900-218 | Emotional Expression Classification using Time-Series Kernels | http://arxiv.org/abs/1306.1913 | author:Andras Lorincz, Laszlo Jeni, Zoltan Szabo, Jeffrey Cohn, Takeo Kanade category:cs.CV cs.LG stat.ML published:2013-06-08 summary:Estimation of facial expressions, as spatio-temporal processes, can takeadvantage of kernel methods if one considers facial landmark positions andtheir motion in 3D space. We applied support vector classification with kernelsderived from dynamic time-warping similarity measures. We achieved over 99%accuracy - measured by area under ROC curve - using only the 'motion pattern'of the PCA compressed representation of the marker point vector, the so-calledshape parameters. Beyond the classification of full motion patterns, severalexpressions were recognized with over 90% accuracy in as few as 5-6 frames fromtheir onset, about 200 milliseconds.
arxiv-3900-219 | Clifford Fourier-Mellin transform with two real square roots of -1 in Cl(p,q), p+q=2 | http://arxiv.org/abs/1306.1679 | author:Eckhard Hitzer category:math.RA cs.CV published:2013-06-07 summary:We describe a non-commutative generalization of the complex Fourier-Mellintransform to Clifford algebra valued signal functions over the domain$\R^{p,q}$ taking values in Cl(p,q), p+q=2. Keywords: algebra, Fourier transforms; Logic, set theory, and algebra,Fourier analysis, Integral transforms
arxiv-3900-220 | Algebraic foundations of split hypercomplex nonlinear adaptive filtering | http://arxiv.org/abs/1306.1676 | author:Eckhard Hitzer category:cs.CV math.RA 60G35, 15A66 published:2013-06-07 summary:A split hypercomplex learning algorithm for the training of nonlinear finiteimpulse response adaptive filters for the processing of hypercomplex signals ofany dimension is proposed. The derivation strictly takes into account the lawsof hypercomplex algebra and hypercomplex calculus, some of which have beenneglected in existing learning approaches (e.g. for quaternions). Already inthe case of quaternions we can predict improvements in performance ofhypercomplex processes. The convergence of the proposed algorithms isrigorously analyzed. Keywords: Quaternionic adaptive filtering, Hypercomplex adaptive filtering,Nonlinear adaptive filtering, Hypercomplex Multilayer Perceptron, Cliffordgeometric algebra
arxiv-3900-221 | Fast greedy algorithm for subspace clustering from corrupted and incomplete data | http://arxiv.org/abs/1306.1716 | author:Alexander Petukhov, Inna Kozlov category:cs.LG cs.DS math.NA stat.ML published:2013-06-07 summary:We describe the Fast Greedy Sparse Subspace Clustering (FGSSC) algorithmproviding an efficient method for clustering data belonging to a fewlow-dimensional linear or affine subspaces. The main difference of ouralgorithm from predecessors is its ability to work with noisy data having ahigh rate of erasures (missed entries with the known coordinates) and errors(corrupted entries with unknown coordinates). We discuss here how to implementthe fast version of the greedy algorithm with the maximum efficiency whosegreedy strategy is incorporated into iterations of the basic algorithm. We provide numerical evidences that, in the subspace clustering capability,the fast greedy algorithm outperforms not only the existing state-of-the artSSC algorithm taken by the authors as a basic algorithm but also the recentGSSC algorithm. At the same time, its computational cost is only slightlyhigher than the cost of SSC. The numerical evidence of the algorithm significant advantage is presentedfor a few synthetic models as well as for the Extended Yale B dataset of facialimages. In particular, the face recognition misclassification rate turned outto be 6-20 times lower than for the SSC algorithm. We provide also thenumerical evidence that the FGSSC algorithm is able to perform clustering ofcorrupted data efficiently even when the sum of subspace dimensionssignificantly exceeds the dimension of the ambient space.
arxiv-3900-222 | Quaternionic Fourier-Mellin Transform | http://arxiv.org/abs/1306.1669 | author:Eckhard Hitzer category:math.RA cs.CV published:2013-06-07 summary:In this contribution we generalize the classical Fourier Mellin transform [S.Dorrode and F. Ghorbel, Robust and efficient Fourier-Mellin transformapproximations for gray-level image reconstruction and complete invariantdescription, Computer Vision and Image Understanding, 83(1) (2001), 57-78, DOI10.1006/cviu.2001.0922.], which transforms functions $f$ representing, e.g., agray level image defined over a compact set of $\mathbb{R}^2$. The quaternionicFourier Mellin transform (QFMT) applies to functions $f: \mathbb{R}^2\rightarrow \mathbb{H}$, for which $f$ is summable over $\mathbb{R}_+^*\times \mathbb{S}^1$ under the measure $d\theta \frac{dr}{r}$. $\mathbb{R}_+^*$is the multiplicative group of positive and non-zero real numbers. Weinvestigate the properties of the QFMT similar to the investigation of thequaternionic Fourier Transform (QFT) in [E. Hitzer, Quaternion FourierTransform on Quaternion Fields and Generalizations, Advances in AppliedClifford Algebras, 17(3) (2007), 497-517.; E. Hitzer, Directional UncertaintyPrinciple for Quaternion Fourier Transforms, Advances in Applied CliffordAlgebras, 20(2) (2010), 271-284, online since 08 July 2009.].
arxiv-3900-223 | Non-constant bounded holomorphic functions of hyperbolic numbers - Candidates for hyperbolic activation functions | http://arxiv.org/abs/1306.1653 | author:Eckhard Hitzer category:cs.NE cs.CV math.RA published:2013-06-07 summary:The Liouville theorem states that bounded holomorphic complex functions arenecessarily constant. Holomorphic functions fulfill the socalled Cauchy-Riemann(CR) conditions. The CR conditions mean that a complex $z$-derivative isindependent of the direction. Holomorphic functions are ideal for activationfunctions of complex neural networks, but the Liouville theorem makes themuseless. Yet recently the use of hyperbolic numbers, lead to the constructionof hyperbolic number neural networks. We will describe the Cauchy-Riemannconditions for hyperbolic numbers and show that there exists a new interestingtype of bounded holomorphic functions of hyperbolic numbers, which are notconstant. We give examples of such functions. They therefore substantiallyexpand the available candidates for holomorphic activation functions forhyperbolic number neural networks. Keywords: Hyperbolic numbers, Liouville theorem, Cauchy-Riemann conditions,bounded holomorphic functions
arxiv-3900-224 | OPS-QFTs: A new type of quaternion Fourier transforms based on the orthogonal planes split with one or two general pure quaternions | http://arxiv.org/abs/1306.1650 | author:Eckhard Hitzer category:math.RA cs.CV 15A66, 42A38 published:2013-06-07 summary:We explain the orthogonal planes split (OPS) of quaternions based on thearbitrary choice of one or two linearly independent pure unit quaternions$f,g$. Next we systematically generalize the quaternionic Fourier transform(QFT) applied to quaternion fields to conform with the OPS determined by $f,g$,or by only one pure unit quaternion $f$, comment on their geometric meaning,and establish inverse transformations. Keywords: Clifford geometric algebra, quaternion geometry, quaternion Fouriertransform, inverse Fourier transform, orthogonal planes split
arxiv-3900-225 | The DeLiVerMATH project - Text analysis in mathematics | http://arxiv.org/abs/1306.6944 | author:Ulf Schöneberg, Wolfram Sperber category:cs.CL cs.DL cs.IR published:2013-06-07 summary:A high-quality content analysis is essential for retrieval functionalitiesbut the manual extraction of key phrases and classification is expensive.Natural language processing provides a framework to automatize the process.Here, a machine-based approach for the content analysis of mathematical textsis described. A prototype for key phrase extraction and classification ofmathematical texts is presented.
arxiv-3900-226 | Statistical Denoising for single molecule fluorescence microscopic images | http://arxiv.org/abs/1306.1619 | author:Ji Won Yoon category:cs.CV published:2013-06-07 summary:Single molecule fluorescence microscopy is a powerful technique foruncovering detailed information about biological systems, both in vitro and invivo. In such experiments, the inherently low signal to noise ratios mean thataccurate algorithms to separate true signal and background noise are essentialto generate meaningful results. To this end, we have developed a new and robustmethod to reduce noise in single molecule fluorescence images by using aGaussian Markov Random Field (GMRF) prior in a Bayesian framework. Twodifferent strategies are proposed to build the prior - an intrinsic GMRF, witha stationary relationship between pixels and a heterogeneous intrinsic GMRF,with a differently weighted relationship between pixels classified as moleculesand background. Testing with synthetic and real experimental fluorescenceimages demonstrates that the heterogeneous intrinsic GMRF is superior to otherconventional de-noising approaches.
arxiv-3900-227 | Vesselness features and the inverse compositional AAM for robust face recognition using thermal IR | http://arxiv.org/abs/1306.1609 | author:Reza Shoja Ghiass, Ognjen Arandjelovic, Hakim Bendada, Xavier Maldague category:cs.CV published:2013-06-07 summary:Over the course of the last decade, infrared (IR) and particularly thermal IRimaging based face recognition has emerged as a promising complement toconventional, visible spectrum based approaches which continue to struggle whenapplied in the real world. While inherently insensitive to visible spectrumillumination changes, IR images introduce specific challenges of their own,most notably sensitivity to factors which affect facial heat emission patterns,e.g. emotional state, ambient temperature, and alcohol intake. In addition,facial expression and pose changes are more difficult to correct in IR imagesbecause they are less rich in high frequency detail which is an important cuefor fitting any deformable model. We describe a novel method which addressesthese challenges. To normalize for pose and facial expression changes wegenerate a synthetic frontal image of a face in a canonical, neutral facialexpression from an image of the face in an arbitrary pose and facialexpression. This is achieved by piecewise affine warping which follows activeappearance model (AAM) fitting. This is the first publication which exploresthe use of an AAM on thermal IR images; we propose a pre-processing step whichenhances detail in thermal images, making AAM convergence faster and moreaccurate. To overcome the problem of thermal IR image sensitivity to thepattern of facial temperature emissions we describe a representation based onreliable anatomical features. In contrast to previous approaches, ourrepresentation is not binary; rather, our method accounts for the reliabilityof the extracted features. This makes the proposed representation much morerobust both to pose and scale changes. The effectiveness of the proposedapproach is demonstrated on the largest public database of thermal IR images offaces on which it achieved 100% identification, significantly outperformingprevious methods.
arxiv-3900-228 | Accomplishable Tasks in Knowledge Representation | http://arxiv.org/abs/1306.2268 | author:Keehang Kwon, Mi-Young Park category:cs.AI cs.CL published:2013-06-07 summary:Knowledge Representation (KR) is traditionally based on the logic of facts,expressed in boolean logic. However, facts about an agent can also be seen as aset of accomplished tasks by the agent. This paper proposes a new approach toKR: the notion of task logical KR based on Computability Logic. This notionallows the user to represent both accomplished tasks and accomplishable tasksby the agent. This notion allows us to build sophisticated KRs about manyinteresting agents, which have not been supported by previous logicallanguages.
arxiv-3900-229 | Illumination-invariant face recognition from a single image across extreme pose using a dual dimension AAM ensemble in the thermal infrared spectrum | http://arxiv.org/abs/1306.1822 | author:Reza Shoja Ghiass, Ognjen Arandjelovic, Hakim Bendada, Xavier Maldague category:cs.CV published:2013-06-07 summary:Over the course of the last decade, infrared (IR) and particularly thermal IRimaging based face recognition has emerged as a promising complement toconventional, visible spectrum based approaches which continue to struggle whenapplied in practice. While inherently insensitive to visible spectrumillumination changes, IR data introduces specific challenges of its own, mostnotably sensitivity to factors which affect facial heat emission patterns, e.g.emotional state, ambient temperature, and alcohol intake. In addition, facialexpression and pose changes are more difficult to correct in IR images becausethey are less rich in high frequency detail which is an important cue forfitting any deformable model. In this paper we describe a novel method whichaddresses these major challenges. Specifically, when comparing two thermal IRimages of faces, we mutually normalize their poses and facial expressions byusing an active appearance model (AAM) to generate synthetic images of the twofaces with a neutral facial expression and in the same view (the average of thetwo input views). This is achieved by piecewise affine warping which followsAAM fitting. A major contribution of our work is the use of an AAM ensemble inwhich each AAM is specialized to a particular range of poses and a particularregion of the thermal IR face space. Combined with the contributions from ourprevious work which addressed the problem of reliable AAM fitting in thethermal IR spectrum, and the development of a person-specific representationrobust to transient changes in the pattern of facial temperature emissions, theproposed ensemble framework accurately matches faces across the full range ofyaw from frontal to profile, even in the presence of scale variation (e.g. dueto the varying distance of a subject from the camera).
arxiv-3900-230 | A Factor Graph Approach to Joint OFDM Channel Estimation and Decoding in Impulsive Noise Environments | http://arxiv.org/abs/1306.1851 | author:Marcel Nassar, Philip Schniter, Brian L. Evans category:cs.IT math.IT stat.ML published:2013-06-07 summary:We propose a novel receiver for orthogonal frequency division multiplexing(OFDM) transmissions in impulsive noise environments. Impulsive noise arises inmany modern wireless and wireline communication systems, such as Wi-Fi andpowerline communications, due to uncoordinated interference that is muchstronger than thermal noise. We first show that the bit-error-rate optimalreceiver jointly estimates the propagation channel coefficients, the noiseimpulses, the finite-alphabet symbols, and the unknown bits. We then propose anear-optimal yet computationally tractable approach to this joint estimationproblem using loopy belief propagation. In particular, we merge the recentlyproposed "generalized approximate message passing" (GAMP) algorithm with theforward-backward algorithm and soft-input soft-output decoding using a "turbo"approach. Numerical results indicate that the proposed receiver drasticallyoutperforms existing receivers under impulsive noise and comes within 1 dB ofthe matched-filter bound. Meanwhile, with N tones, the proposedfactor-graph-based receiver has only O(N log N) complexity, and it can beparallelized.
arxiv-3900-231 | Infrared face recognition: a literature review | http://arxiv.org/abs/1306.1603 | author:Reza Shoja Ghiass, Ognjen Arandjelovic, Hakim Bendada, Xavier Maldague category:cs.CV published:2013-06-07 summary:Automatic face recognition (AFR) is an area with immense practical potentialwhich includes a wide range of commercial and law enforcement applications, andit continues to be one of the most active research areas of computer vision.Even after over three decades of intense research, the state-of-the-art in AFRcontinues to improve, benefiting from advances in a range of different fieldsincluding image processing, pattern recognition, computer graphics andphysiology. However, systems based on visible spectrum images continue to facechallenges in the presence of illumination, pose and expression changes, aswell as facial disguises, all of which can significantly decrease theiraccuracy. Amongst various approaches which have been proposed in an attempt toovercome these limitations, the use of infrared (IR) imaging has emerged as aparticularly promising research direction. This paper presents a comprehensiveand timely review of the literature on this subject.
arxiv-3900-232 | Orbital-free Bond Breaking via Machine Learning | http://arxiv.org/abs/1306.1812 | author:John C. Snyder, Matthias Rupp, Katja Hansen, Leo Blooston, Klaus-Robert Müller, Kieron Burke category:stat.ML published:2013-06-07 summary:Machine learning is used to approximate the kinetic energy of one dimensionaldiatomics as a functional of the electron density. The functional canaccurately dissociate a diatomic, and can be systematically improved withtraining. Highly accurate self-consistent densities and molecular forces arefound, indicating the possibility for ab-initio molecular dynamics simulations.
arxiv-3900-233 | Spectral Convergence of the connection Laplacian from random samples | http://arxiv.org/abs/1306.1587 | author:Amit Singer, Hau-tieng Wu category:math.NA math.ST stat.ME stat.ML stat.TH published:2013-06-07 summary:Spectral methods that are based on eigenvectors and eigenvalues of discretegraph Laplacians, such as Diffusion Maps and Laplacian Eigenmaps are often usedfor manifold learning and non-linear dimensionality reduction. It waspreviously shown by Belkin and Niyogi \cite{belkin_niyogi:2007} that theeigenvectors and eigenvalues of the graph Laplacian converge to theeigenfunctions and eigenvalues of the Laplace-Beltrami operator of the manifoldin the limit of infinitely many data points sampled independently from theuniform distribution over the manifold. Recently, we introduced VectorDiffusion Maps and showed that the connection Laplacian of the tangent bundleof the manifold can be approximated from random samples. In this paper, wepresent a unified framework for approximating other connection Laplacians overthe manifold by considering its principle bundle structure. We prove that theeigenvectors and eigenvalues of these Laplacians converge in the limit ofinfinitely many independent random samples. We generalize the spectralconvergence results to the case where the data points are sampled from anon-uniform distribution, and for manifolds with and without boundary.
arxiv-3900-234 | Loss-Proportional Subsampling for Subsequent ERM | http://arxiv.org/abs/1306.1840 | author:Paul Mineiro, Nikos Karampatziakis category:cs.LG stat.ML published:2013-06-07 summary:We propose a sampling scheme suitable for reducing a data set prior toselecting a hypothesis with minimum empirical risk. The sampling only considersa subset of the ultimate (unknown) hypothesis set, but can nonethelessguarantee that the final excess risk will compare favorably with utilizing theentire original data set. We demonstrate the practical benefits of our approachon a large dataset which we subsample and subsequently fit with boosted trees.
arxiv-3900-235 | Diffusion map for clustering fMRI spatial maps extracted by independent component analysis | http://arxiv.org/abs/1306.1350 | author:Tuomo Sipola, Fengyu Cong, Tapani Ristaniemi, Vinoo Alluri, Petri Toiviainen, Elvira Brattico, Asoke K. Nandi category:cs.CE cs.LG stat.ML published:2013-06-06 summary:Functional magnetic resonance imaging (fMRI) produces data about activityinside the brain, from which spatial maps can be extracted by independentcomponent analysis (ICA). In datasets, there are n spatial maps that contain pvoxels. The number of voxels is very high compared to the number of analyzedspatial maps. Clustering of the spatial maps is usually based on correlationmatrices. This usually works well, although such a similarity matrix inherentlycan explain only a certain amount of the total variance contained in thehigh-dimensional data where n is relatively small but p is large. Forhigh-dimensional space, it is reasonable to perform dimensionality reductionbefore clustering. In this research, we used the recently developed diffusionmap for dimensionality reduction in conjunction with spectral clustering. Thisresearch revealed that the diffusion map based clustering worked as well as themore traditional methods, and produced more compact clusters when needed.
arxiv-3900-236 | Recognition of Indian Sign Language in Live Video | http://arxiv.org/abs/1306.1301 | author:Joyeeta Singha, Karen Das category:cs.CV published:2013-06-06 summary:Sign Language Recognition has emerged as one of the important area ofresearch in Computer Vision. The difficulty faced by the researchers is thatthe instances of signs vary with both motion and appearance. Thus, in thispaper a novel approach for recognizing various alphabets of Indian SignLanguage is proposed where continuous video sequences of the signs have beenconsidered. The proposed system comprises of three stages: Preprocessing stage,Feature Extraction and Classification. Preprocessing stage includes skinfiltering, histogram matching. Eigen values and Eigen Vectors were consideredfor feature extraction stage and finally Eigen value weighted Euclideandistance is used to recognize the sign. It deals with bare hands, thus allowingthe user to interact with the system in natural way. We have considered 24different alphabets in the video sequences and attained a success rate of96.25%.
arxiv-3900-237 | Policy Search: Any Local Optimum Enjoys a Global Performance Guarantee | http://arxiv.org/abs/1306.1520 | author:Bruno Scherrer, Matthieu Geist category:cs.LG cs.AI cs.RO math.OC published:2013-06-06 summary:Local Policy Search is a popular reinforcement learning approach for handlinglarge state spaces. Formally, it searches locally in a paramet erized policyspace in order to maximize the associated value function averaged over somepredefined distribution. It is probably commonly b elieved that the best onecan hope in general from such an approach is to get a local optimum of thiscriterion. In this article, we show th e following surprising result:\emph{any} (approximate) \emph{local optimum} enjoys a \emph{global performanceguarantee}. We compare this g uarantee with the one that is satisfied by DirectPolicy Iteration, an approximate dynamic programming algorithm that does someform of Poli cy Search: if the approximation error of Local Policy Search maygenerally be bigger (because local search requires to consider a space of stochastic policies), we argue that the concentrability coefficient that appearsin the performance bound is much nicer. Finally, we discuss several practicaland theoretical consequences of our analysis.
arxiv-3900-238 | Verdict Accuracy of Quick Reduct Algorithm using Clustering and Classification Techniques for Gene Expression Data | http://arxiv.org/abs/1306.1323 | author:T. Chandrasekhar, K. Thangavel, E. N. Sathishkumar category:cs.LG cs.CE stat.ML published:2013-06-06 summary:In most gene expression data, the number of training samples is very smallcompared to the large number of genes involved in the experiments. However,among the large amount of genes, only a small fraction is effective forperforming a certain task. Furthermore, a small subset of genes is desirable indeveloping gene expression based diagnostic tools for delivering reliable andunderstandable results. With the gene selection results, the cost of biologicalexperiment and decision can be greatly reduced by analyzing only the markergenes. An important application of gene expression data in functional genomicsis to classify samples according to their gene expression profiles. Featureselection (FS) is a process which attempts to select more informative features.It is one of the important steps in knowledge discovery. Conventionalsupervised FS methods evaluate various feature subsets using an evaluationfunction or metric to select only those features which are related to thedecision classes of the data under consideration. This paper studies a featureselection method based on rough set theory. Further K-Means, Fuzzy C-Means(FCM) algorithm have implemented for the reduced feature set withoutconsidering class labels. Then the obtained results are compared with theoriginal class labels. Back Propagation Network (BPN) has also been used forclassification. Then the performance of K-Means, FCM, and BPN are analyzedthrough the confusion matrix. It is found that the BPN is performing wellcomparatively.
arxiv-3900-239 | Highly Scalable, Parallel and Distributed AdaBoost Algorithm using Light Weight Threads and Web Services on a Network of Multi-Core Machines | http://arxiv.org/abs/1306.1467 | author:Munther Abualkibash, Ahmed ElSayed, Ausif Mahmood category:cs.DC cs.LG published:2013-06-06 summary:AdaBoost is an important algorithm in machine learning and is being widelyused in object detection. AdaBoost works by iteratively selecting the bestamongst weak classifiers, and then combines several weak classifiers to obtaina strong classifier. Even though AdaBoost has proven to be very effective, itslearning execution time can be quite large depending upon the application e.g.,in face detection, the learning time can be several days. Due to its increasinguse in computer vision applications, the learning time needs to be drasticallyreduced so that an adaptive near real time object detection system can beincorporated. In this paper, we develop a hybrid parallel and distributedAdaBoost algorithm that exploits the multiple cores in a CPU via light weightthreads, and also uses multiple machines via a web service softwarearchitecture to achieve high scalability. We present a novel hierarchical webservices based distributed architecture and achieve nearly linear speedup up tothe number of processors available to us. In comparison with the previouslypublished work, which used a single level master-slave parallel and distributedimplementation [1] and only achieved a speedup of 2.66 on four nodes, weachieve a speedup of 95.1 on 31 workstations each having a quad-core processor,resulting in a learning time of only 4.8 seconds per feature.
arxiv-3900-240 | A Fuzzy Based Approach to Text Mining and Document Clustering | http://arxiv.org/abs/1306.4633 | author:Sumit Goswami, Mayank Singh Shishodia category:cs.LG cs.IR published:2013-06-06 summary:Fuzzy logic deals with degrees of truth. In this paper, we have shown how toapply fuzzy logic in text mining in order to perform document clustering. Wetook an example of document clustering where the documents had to be clusteredinto two categories. The method involved cleaning up the text and stemming ofwords. Then, we chose m number of features which differ significantly in theirword frequencies (WF), normalized by document length, between documentsbelonging to these two clusters. The documents to be clustered were representedas a collection of m normalized WF values. Fuzzy c-means (FCM) algorithm wasused to cluster these documents into two clusters. After the FCM executionfinished, the documents in the two clusters were analysed for the values oftheir respective m features. It was known that documents belonging to adocument type, say X, tend to have higher WF values for some particularfeatures. If the documents belonging to a cluster had higher WF values forthose same features, then that cluster was said to represent X. By fuzzy logic,we not only get the cluster name, but also the degree to which a documentbelongs to a cluster.
arxiv-3900-241 | Performance analysis of unsupervised feature selection methods | http://arxiv.org/abs/1306.1326 | author:A. Nisthana Parveen, H. Hannah Inbarani, E. N. Sathishkumar category:cs.LG published:2013-06-06 summary:Feature selection (FS) is a process which attempts to select more informativefeatures. In some cases, too many redundant or irrelevant features mayoverpower main features for classification. Feature selection can remedy thisproblem and therefore improve the prediction accuracy and reduce thecomputational overhead of classification algorithms. The main aim of featureselection is to determine a minimal feature subset from a problem domain whileretaining a suitably high accuracy in representing the original features. Inthis paper, Principal Component Analysis (PCA), Rough PCA, Unsupervised QuickReduct (USQR) algorithm and Empirical Distribution Ranking (EDR) approaches areapplied to discover discriminative features that will be the most adequate onesfor classification. Efficiency of the approaches is evaluated using standardclassification metrics.
arxiv-3900-242 | Table of Content detection using Machine Learning | http://arxiv.org/abs/1306.4631 | author:Rachana Parikh, Avani R. Vasant category:cs.LG cs.DL cs.IR published:2013-06-06 summary:Table of content (TOC) detection has drawn attention now a day because itplays an important role in digitization of multipage document. Generally bookdocument is multipage document. So it becomes necessary to detect Table ofContent page for easy navigation of multipage document and also to makeinformation retrieval faster for desirable data from the multipage document.All the Table of content pages follow the different layout, different way ofpresenting the contents of the document like chapter, section, subsection etc.This paper introduces a new method to detect Table of content using machinelearning technique with different features. With the main aim to detect Tableof Content pages is to structure the document according to their contents.
arxiv-3900-243 | Tight Lower Bound on the Probability of a Binomial Exceeding its Expectation | http://arxiv.org/abs/1306.1433 | author:Spencer Greenberg, Mehryar Mohri category:cs.LG stat.ML published:2013-06-06 summary:We give the proof of a tight lower bound on the probability that a binomialrandom variable exceeds its expected value. The inequality plays an importantrole in a variety of contexts, including the analysis of relative deviationbounds in learning theory and generalization bounds for unbounded lossfunctions.
arxiv-3900-244 | The User Feedback on SentiWordNet | http://arxiv.org/abs/1306.1343 | author:Andrea Esuli category:cs.CL cs.IR I.2.7 published:2013-06-06 summary:With the release of SentiWordNet 3.0 the related Web interface has beenrestyled and improved in order to allow users to submit feedback on theSentiWordNet entries, in the form of the suggestion of alternative triplets ofvalues for an entry. This paper reports on the release of the user feedbackcollected so far and on the plans for the future.
arxiv-3900-245 | Geometric operations implemented by conformal geometric algebra neural nodes | http://arxiv.org/abs/1306.1358 | author:Eckhard Hitzer category:cs.CV cs.NE math.RA published:2013-06-06 summary:Geometric algebra is an optimal frame work for calculating with vectors. Thegeometric algebra of a space includes elements that represent all the itssubspaces (lines, planes, volumes, ...). Conformal geometric algebra expandsthis approach to elementary representations of arbitrary points, point pairs,lines, circles, planes and spheres. Apart from including curved objects,conformal geometric algebra has an elegant unified quaternion likerepresentation for all proper and improper Euclidean transformations, includingreflections at spheres, general screw transformations and scaling. Expandingthe concepts of real and complex neurons we arrive at the new powerful conceptof conformal geometric algebra neurons. These neurons can easily take the abovementioned geometric objects or sets of these objects as inputs and apply a widerange of geometric transformations via the geometric algebra valued weights.
arxiv-3900-246 | K-Algorithm A Modified Technique for Noise Removal in Handwritten Documents | http://arxiv.org/abs/1306.1462 | author:Kanika Bansal, Rajiv Kumar category:cs.CV published:2013-06-06 summary:OCR has been an active research area since last few decades. OCR performs therecognition of the text in the scanned document image and converts it intoeditable form. The OCR process can have several stages like pre-processing,segmentation, recognition and post processing. The pre-processing stage is acrucial stage for the success of OCR, which mainly deals with noise removal. Inthe present paper, a modified technique for noise removal named as K-Algorithmhas been proposed, which has two stages as filtering and binarization. Theproposed technique shows improvised results in comparison to median filteringtechnique.
arxiv-3900-247 | PyHST2: an hybrid distributed code for high speed tomographic reconstruction with iterative reconstruction and a priori knowledge capabilities | http://arxiv.org/abs/1306.1392 | author:Alessandro Mirone, Emmanuelle Gouillart, Emmanuel Brun, Paul Tafforeau, Jerome Kieffer category:math.NA cs.CV published:2013-06-06 summary:We present the PyHST2 code which is in service at ESRF for phase-contrast andabsorption tomography. This code has been engineered to sustain the high dataflow typical of the third generation synchrotron facilities (10 terabytes perexperiment) by adopting a distributed and pipelined architecture. The codeimplements, beside a default filtered backprojection reconstruction, iterativereconstruction techniques with a-priori knowledge. These latter are used toimprove the reconstruction quality or in order to reduce the required datavolume and reach a given quality goal. The implemented a-priori knowledgetechniques are based on the total variation penalisation and a new recentlyfound convex functional which is based on overlapping patches. We give details of the different methods and their implementations while thecode is distributed under free license. We provide methods for estimating, in the absence of ground-truth data, theoptimal parameters values for a-priori techniques.
arxiv-3900-248 | Multiclass Semi-Supervised Learning on Graphs using Ginzburg-Landau Functional Minimization | http://arxiv.org/abs/1306.1298 | author:Cristina Garcia-Cardona, Arjuna Flenner, Allon G. Percus category:stat.ML cs.LG math.ST stat.TH I.5.3 published:2013-06-06 summary:We present a graph-based variational algorithm for classification ofhigh-dimensional data, generalizing the binary diffuse interface model to thecase of multiple classes. Motivated by total variation techniques, the methodinvolves minimizing an energy functional made up of three terms. The first twoterms promote a stepwise continuous classification function with sharptransitions between classes, while preserving symmetry among the class labels.The third term is a data fidelity term, allowing us to incorporate priorinformation into the model in a semi-supervised framework. The performance ofthe algorithm on synthetic data, as well as on the COIL and MNIST benchmarkdatasets, is competitive with state-of-the-art graph-based multiclasssegmentation methods.
arxiv-3900-249 | Structural Intervention Distance (SID) for Evaluating Causal Graphs | http://arxiv.org/abs/1306.1043 | author:Jonas Peters, Peter Bühlmann category:stat.ML published:2013-06-05 summary:Causal inference relies on the structure of a graph, often a directed acyclicgraph (DAG). Different graphs may result in different causal inferencestatements and different intervention distributions. To quantify suchdifferences, we propose a (pre-) distance between DAGs, the structuralintervention distance (SID). The SID is based on a graphical criterion only andquantifies the closeness between two DAGs in terms of their correspondingcausal inference statements. It is therefore well-suited for evaluating graphsthat are used for computing interventions. Instead of DAGs it is also possibleto compare CPDAGs, completed partially directed acyclic graphs that representMarkov equivalence classes. Since it differs significantly from the popularStructural Hamming Distance (SHD), the SID constitutes a valuable additionalmeasure. We discuss properties of this distance and provide an efficientimplementation with software code available on the first author's homepage (anR package is under construction).
arxiv-3900-250 | Deep Generative Stochastic Networks Trainable by Backprop | http://arxiv.org/abs/1306.1091 | author:Yoshua Bengio, Éric Thibodeau-Laufer, Guillaume Alain, Jason Yosinski category:cs.LG published:2013-06-05 summary:We introduce a novel training principle for probabilistic models that is analternative to maximum likelihood. The proposed Generative Stochastic Networks(GSN) framework is based on learning the transition operator of a Markov chainwhose stationary distribution estimates the data distribution. The transitiondistribution of the Markov chain is conditional on the previous state,generally involving a small move, so this conditional distribution has fewerdominant modes, being unimodal in the limit of small moves. Thus, it is easierto learn because it is easier to approximate its partition function, more likelearning to perform supervised function approximation, with gradients that canbe obtained by backprop. We provide theorems that generalize recent work on theprobabilistic interpretation of denoising autoencoders and obtain along the wayan interesting justification for dependency networks and generalizedpseudolikelihood, along with a definition of an appropriate joint distributionand sampling mechanism even when the conditionals are not consistent. GSNs canbe used with missing inputs and can be used to sample subsets of variablesgiven the rest. We validate these theoretical results with experiments on twoimage datasets using an architecture that mimics the Deep Boltzmann MachineGibbs sampler but allows training to proceed with simple backprop, without theneed for layerwise pretraining.
arxiv-3900-251 | Inferring Robot Task Plans from Human Team Meetings: A Generative Modeling Approach with Logic-Based Prior | http://arxiv.org/abs/1306.0963 | author:Been Kim, Caleb M. Chacha, Julie Shah category:cs.AI cs.CL cs.RO stat.ML published:2013-06-05 summary:We aim to reduce the burden of programming and deploying autonomous systemsto work in concert with people in time-critical domains, such as military fieldoperations and disaster response. Deployment plans for these operations arefrequently negotiated on-the-fly by teams of human planners. A human operatorthen translates the agreed upon plan into machine instructions for the robots.We present an algorithm that reduces this translation burden by inferring thefinal plan from a processed form of the human team's planning conversation. Ourapproach combines probabilistic generative modeling with logical planvalidation used to compute a highly structured prior over possible plans. Thishybrid approach enables us to overcome the challenge of performing inferenceover the large solution space with only a small amount of noisy data from theteam planning session. We validate the algorithm through human subjectexperimentation and show we are able to infer a human team's final plan with83% accuracy on average. We also describe a robot demonstration in which twopeople plan and execute a first-response collaborative task with a PR2 robot.To the best of our knowledge, this is the first work that integrates a logicalplanning technique within a generative model to perform plan inference.
arxiv-3900-252 | Distributed Bayesian inference for consistent labeling of tracked objects in non-overlapping camera networks | http://arxiv.org/abs/1306.0974 | author:Jiuqing Wan, Li Liu category:cs.CV published:2013-06-05 summary:One of the fundamental requirements for visual surveillance usingnon-overlapping camera networks is the correct labeling of tracked objects oneach camera in a consistent way,in the sense that the captured tracklets, orobservations in this paper, of the same object at different cameras should beassigned with the same label. In this paper, we formulate this task as aBayesian inference problem and propose a distributed inference framework inwhich the posterior distribution of labeling variable corresponding to eachobservation, conditioned on all history appearance and spatio-temporal evidencemade in the whole networks, is calculated based solely on local informationprocessing on each camera and mutual information exchanging between neighboringcameras. In our framework, the number of objects presenting in the monitoredregion, i.e. the sampling space of labeling variables, does not need to bespecified beforehand. Instead, it can be determined automatically on the fly.In addition, we make no assumption about the appearance distribution of asingle object, but use similarity scores between appearance pairs, given byadvanced object re-identification algorithm, as appearance likelihood forinference. This feature makes our method very flexible and competitive whenobserving condition undergoes large changes across camera views. To cope withthe problem of missing detection, which is critical for distributed inference,we consider an enlarged neighborhood of each camera during inference and use amixture model to describe the higher order spatio-temporal constraints. Therobustness of the algorithm against missing detection is improved at the costof slightly increased computation and communication burden at each camera node.Finally, we demonstrate the effectiveness of our method through experiments onan indoor Office Building dataset and an outdoor Campus Garden dataset.
arxiv-3900-253 | Sparse Representation of a Polytope and Recovery of Sparse Signals and Low-rank Matrices | http://arxiv.org/abs/1306.1154 | author:T. Tony Cai, Anru Zhang category:cs.IT math.IT math.ST stat.ML stat.TH published:2013-06-05 summary:This paper considers compressed sensing and affine rank minimization in bothnoiseless and noisy cases and establishes sharp restricted isometry conditionsfor sparse signal and low-rank matrix recovery. The analysis relies on a keytechnical tool which represents points in a polytope by convex combinations ofsparse vectors. The technique is elementary while leads to sharp results. It is shown that for any given constant $t\ge {4/3}$, in compressed sensing$\delta_{tk}^A < \sqrt{(t-1)/t}$ guarantees the exact recovery of all $k$sparse signals in the noiseless case through the constrained $\ell_1$minimization, and similarly in affine rank minimization$\delta_{tr}^\mathcal{M}< \sqrt{(t-1)/t}$ ensures the exact reconstruction ofall matrices with rank at most $r$ in the noiseless case via the constrainednuclear norm minimization. Moreover, for any $\epsilon>0$,$\delta_{tk}^A<\sqrt{\frac{t-1}{t}}+\epsilon$ is not sufficient to guaranteethe exact recovery of all $k$-sparse signals for large $k$. Similar result alsoholds for matrix recovery. In addition, the conditions $\delta_{tk}^A <\sqrt{(t-1)/t}$ and $\delta_{tr}^\mathcal{M}< \sqrt{(t-1)/t}$ are also shown tobe sufficient respectively for stable recovery of approximately sparse signalsand low-rank matrices in the noisy case.
arxiv-3900-254 | Quaternion Fourier Transform on Quaternion Fields and Generalizations | http://arxiv.org/abs/1306.1023 | author:Eckhard Hitzer category:math.RA cs.CV math-ph math.MP published:2013-06-05 summary:We treat the quaternionic Fourier transform (QFT) applied to quaternionfields and investigate QFT properties useful for applications. Different formsof the QFT lead us to different Plancherel theorems. We relate the QFTcomputation for quaternion fields to the QFT of real signals. We research thegeneral linear ($GL$) transformation behavior of the QFT with matrices,Clifford geometric algebra and with examples. We finally arrive at wide-rangingnon-commutative multivector FT generalizations of the QFT. Examples given arenew volume-time and spacetime algebra Fourier transformations.
arxiv-3900-255 | ROTUNDE - A Smart Meeting Cinematography Initiative: Tools, Datasets, and Benchmarks for Cognitive Interpretation and Control | http://arxiv.org/abs/1306.1034 | author:Mehul Bhatt, Jakob Suchan, Christian Freksa category:cs.AI cs.CV cs.HC published:2013-06-05 summary:We construe smart meeting cinematography with a focus on professionalsituations such as meetings and seminars, possibly conducted in a distributedmanner across socio-spatially separated groups. The basic objective in smartmeeting cinematography is to interpret professional interactions involvingpeople, and automatically produce dynamic recordings of discussions, debates,presentations etc in the presence of multiple communication modalities. Typicalmodalities include gestures (e.g., raising one's hand for a question,applause), voice and interruption, electronic apparatus (e.g., pressing abutton), movement (e.g., standing-up, moving around) etc. ROTUNDE, an instanceof smart meeting cinematography concept, aims to: (a) developfunctionality-driven benchmarks with respect to the interpretation and controlcapabilities of human-cinematographers, real-time video editors, surveillancepersonnel, and typical human performance in everyday situations; (b) Developgeneral tools for the commonsense cognitive interpretation of dynamic scenesfrom the viewpoint of visuo-spatial cognition centred perceptualnarrativisation. Particular emphasis is placed on declarative representationsand interfacing mechanisms that seamlessly integrate within large-scalecognitive (interaction) systems and companion technologies consisting ofdiverse AI sub-components. For instance, the envisaged tools would providegeneral capabilities for high-level commonsense reasoning about space, events,actions, change, and interaction.
arxiv-3900-256 | Differential Privacy in a Bayesian setting through posterior sampling | http://arxiv.org/abs/1306.1066 | author:Christos Dimitrakakis, Blaine Nelson, and Zuhe Zhang, Aikaterini Mitrokotsa, Benjamin Rubinstein category:stat.ML cs.LG published:2013-06-05 summary:We examine the robustness and privacy properties of Bayesian inference, underassumptions on the prior. With no modifications to the Bayesian framework, weshow that a simple posterior sampling algorithm results in uniform utility andprivacy guarantees. In more detail, we generalise the concept of differentialprivacy to arbitrary dataset distances, outcome spaces and distributionfamilies. We then prove bounds on the robustness of the posterior, introduce aposterior sampling mechanism, show that it is differentially private andprovide finite sample bounds for distinguishability-based privacy under astrong adversarial model. Finally, we give examples satisfying our assumptions.
arxiv-3900-257 | Multiclass Total Variation Clustering | http://arxiv.org/abs/1306.1185 | author:Xavier Bresson, Thomas Laurent, David Uminsky, James H. von Brecht category:stat.ML cs.LG math.OC published:2013-06-05 summary:Ideas from the image processing literature have recently motivated a new setof clustering algorithms that rely on the concept of total variation. Whilethese algorithms perform well for bi-partitioning tasks, their recursiveextensions yield unimpressive results for multiclass clustering tasks. Thispaper presents a general framework for multiclass total variation clusteringthat does not rely on recursion. The results greatly outperform previous totalvariation algorithms and compare well with state-of-the-art NMF approaches.
arxiv-3900-258 | Fast Dual Variational Inference for Non-Conjugate LGMs | http://arxiv.org/abs/1306.1052 | author:Mohammad Emtiyaz Khan, Aleksandr Y. Aravkin, Michael P. Friedlander, Matthias Seeger category:stat.ML math.OC stat.CO published:2013-06-05 summary:Latent Gaussian models (LGMs) are widely used in statistics and machinelearning. Bayesian inference in non-conjugate LGMs is difficult due tointractable integrals involving the Gaussian prior and non-conjugatelikelihoods. Algorithms based on variational Gaussian (VG) approximations arewidely employed since they strike a favorable balance between accuracy,generality, speed, and ease of use. However, the structure of the optimizationproblems associated with these approximations remains poorly understood, andstandard solvers take too long to converge. We derive a novel dual variationalinference approach that exploits the convexity property of the VGapproximations. We obtain an algorithm that solves a convex optimizationproblem, reduces the number of variational parameters, and converges muchfaster than previous methods. Using real-world data, we demonstrate theseadvantages on a variety of LGMs, including Gaussian process classification, andlatent Gaussian Markov random fields.
arxiv-3900-259 | Discriminative Parameter Estimation for Random Walks Segmentation: Technical Report | http://arxiv.org/abs/1306.1083 | author:Pierre-Yves Baudin, Danny Goodman, Puneet Kumar, Noura Azzabou, Pierre G. Carlier, Nikos Paragios, M. Pawan Kumar category:cs.CV cs.LG published:2013-06-05 summary:The Random Walks (RW) algorithm is one of the most e - cient and easy-to-useprobabilistic segmentation methods. By combining contrast terms with priorterms, it provides accurate segmentations of medical images in a fullyautomated manner. However, one of the main drawbacks of using the RW algorithmis that its parameters have to be hand-tuned. we propose a novel discriminativelearning framework that estimates the parameters using a training dataset. Themain challenge we face is that the training samples are not fully supervised.Speci cally, they provide a hard segmentation of the images, instead of aproba-bilistic segmentation. We overcome this challenge by treating the optimalprobabilistic segmentation that is compatible with the given hard segmentationas a latent variable. This allows us to employ the latent support vectormachine formulation for parameter estimation. We show that our approach signicantly outperforms the baseline methods on a challenging dataset consisting ofreal clinical 3D MRI volumes of skeletal muscles.
arxiv-3900-260 | Online Learning under Delayed Feedback | http://arxiv.org/abs/1306.0686 | author:Pooria Joulani, András György, Csaba Szepesvári category:cs.LG cs.AI stat.ML published:2013-06-04 summary:Online learning with delayed feedback has received increasing attentionrecently due to its several applications in distributed, web-based learningproblems. In this paper we provide a systematic study of the topic, and analyzethe effect of delay on the regret of online learning algorithms. Somewhatsurprisingly, it turns out that delay increases the regret in a multiplicativeway in adversarial problems, and in an additive way in stochastic problems. Wegive meta-algorithms that transform, in a black-box fashion, algorithmsdeveloped for the non-delayed case into ones that can handle the presence ofdelays in the feedback loop. Modifications of the well-known UCB algorithm arealso developed for the bandit problem with delayed feedback, with the advantageover the meta-algorithms that they can be implemented with lower complexity.
arxiv-3900-261 | $\propto$SVM for learning with label proportions | http://arxiv.org/abs/1306.0886 | author:Felix X. Yu, Dong Liu, Sanjiv Kumar, Tony Jebara, Shih-Fu Chang category:cs.LG stat.ML published:2013-06-04 summary:We study the problem of learning with label proportions in which the trainingdata is provided in groups and only the proportion of each class in each groupis known. We propose a new method called proportion-SVM, or $\propto$SVM, whichexplicitly models the latent unknown instance labels together with the knowngroup label proportions in a large-margin framework. Unlike the existing works,our approach avoids making restrictive assumptions about the data. The$\propto$SVM model leads to a non-convex integer programming problem. In orderto solve it efficiently, we propose two algorithms: one based on simplealternating optimization and the other based on a convex relaxation. Extensiveexperiments on standard datasets show that $\propto$SVM outperforms thestate-of-the-art, especially for larger group sizes.
arxiv-3900-262 | Kernel Mean Estimation and Stein's Effect | http://arxiv.org/abs/1306.0842 | author:Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Arthur Gretton, Bernhard Schölkopf category:stat.ML cs.LG math.ST stat.TH published:2013-06-04 summary:A mean function in reproducing kernel Hilbert space, or a kernel mean, is animportant part of many applications ranging from kernel principal componentanalysis to Hilbert-space embedding of distributions. Given finite samples, anempirical average is the standard estimate for the true kernel mean. We showthat this estimator can be improved via a well-known phenomenon in statisticscalled Stein's phenomenon. After consideration, our theoretical analysisreveals the existence of a wide class of estimators that are better than thestandard. Focusing on a subset of this class, we propose efficient shrinkageestimators for the kernel mean. Empirical evaluations on several benchmarkapplications clearly demonstrate that the proposed estimators outperform thestandard kernel mean estimator.
arxiv-3900-263 | Urban ozone concentration forecasting with artificial neural network in Corsica | http://arxiv.org/abs/1306.0897 | author:Wani W. Tamas, Gilles Notton, Christophe Paoli, Cyril Voyant, Marie Laure Nivet, Aurélia Balu category:cs.NE published:2013-06-04 summary:Atmospheric pollutants concentration forecasting is an important issue in airquality monitoring. Qualitair Corse, the organization responsible formonitoring air quality in Corsica (France) region, needs to develop ashort-term prediction model to lead its mission of information towards thepublic. Various deterministic models exist for meso-scale or local forecasting,but need powerful large variable sets, a good knowledge of atmosphericprocesses, and can be inaccurate because of local climatical or geographicalparticularities, as observed in Corsica, a mountainous island located in aMediterranean Sea. As a result, we focus in this study on statistical models,and particularly Artificial Neural Networks (ANN) that have shown good resultsin the prediction of ozone concentration at horizon h+1 with data measuredlocally. The purpose of this study is to build a predictor to realizepredictions of ozone and PM10 at horizon d+1 in Corsica in order to be able toanticipate pollution peak formation and to take appropriated preventionmeasures. Specific meteorological conditions are known to lead to particularpollution event in Corsica (e.g. Saharan dust event). Therefore, several ANNmodels will be used, for meteorological conditions clustering and foroperational forecasting.
arxiv-3900-264 | Finding Numerical Solutions of Diophantine Equations using Ant Colony Optimization | http://arxiv.org/abs/1306.0896 | author:Siby Abraham, Sugata Sanyal, Mukund Sanglikar category:cs.NE cs.ET published:2013-06-04 summary:The paper attempts to find numerical solutions of Diophantine equations, achallenging problem as there are no general methods to find solutions of suchequations. It uses the metaphor of foraging habits of real ants. The ant colonyoptimization based procedure starts with randomly assigned locations to a fixednumber of artificial ants. Depending upon the quality of these positions, antsdeposit pheromone at the nodes. A successor node is selected from thetopological neighborhood of each of the nodes based on this stochasticpheromone deposit. If an ant bumps into an already encountered node, thepheromone is updated correspondingly. A suitably defined pheromone evaporationstrategy guarantees that premature convergence does not take place. Theexperimental results, which compares with those of other machine intelligencetechniques, validate the effectiveness of the proposed method.
arxiv-3900-265 | A Gang of Bandits | http://arxiv.org/abs/1306.0811 | author:Nicolò Cesa-Bianchi, Claudio Gentile, Giovanni Zappella category:cs.LG cs.SI stat.ML published:2013-06-04 summary:Multi-armed bandit problems are receiving a great deal of attention becausethey adequately formalize the exploration-exploitation trade-offs arising inseveral industrially relevant applications, such as online advertisement and,more generally, recommendation systems. In many cases, however, theseapplications have a strong social component, whose integration in the banditalgorithm could lead to a dramatic performance increase. For instance, we maywant to serve content to a group of users by taking advantage of an underlyingnetwork of social relationships among them. In this paper, we introduce novelalgorithmic approaches to the solution of such networked bandit problems. Morespecifically, we design and analyze a global strategy which allocates a banditalgorithm to each network node (user) and allows it to "share" signals(contexts and payoffs) with the neghboring nodes. We then derive two morescalable variants of this strategy based on different ways of clustering thegraph nodes. We experimentally compare the algorithm and its variants tostate-of-the-art methods for contextual bandits that do not use the relationalinformation. Our experiments, carried out on synthetic and real-world datasets,show a marked increase in prediction performance obtained by exploiting thenetwork structure.
arxiv-3900-266 | Sinkhorn Distances: Lightspeed Computation of Optimal Transportation Distances | http://arxiv.org/abs/1306.0895 | author:Marco Cuturi category:stat.ML published:2013-06-04 summary:Optimal transportation distances are a fundamental family of parameterizeddistances for histograms. Despite their appealing theoretical properties,excellent performance in retrieval tasks and intuitive formulation, theircomputation involves the resolution of a linear program whose cost isprohibitive whenever the histograms' dimension exceeds a few hundreds. Wepropose in this work a new family of optimal transportation distances that lookat transportation problems from a maximum-entropy perspective. We smooth theclassical optimal transportation problem with an entropic regularization term,and show that the resulting optimum is also a distance which can be computedthrough Sinkhorn-Knopp's matrix scaling algorithm at a speed that is severalorders of magnitude faster than that of transportation solvers. We also reportimproved performance over classical optimal transportation distances on theMNIST benchmark problem.
arxiv-3900-267 | Fast Gradient-Based Inference with Continuous Latent Variable Models in Auxiliary Form | http://arxiv.org/abs/1306.0733 | author:Diederik P Kingma category:cs.LG stat.ML published:2013-06-04 summary:We propose a technique for increasing the efficiency of gradient-basedinference and learning in Bayesian networks with multiple layers of continuouslatent vari- ables. We show that, in many cases, it is possible to express suchmodels in an auxiliary form, where continuous latent variables areconditionally deterministic given their parents and a set of independentauxiliary variables. Variables of mod- els in this auxiliary form have muchlarger Markov blankets, leading to significant speedups in gradient-basedinference, e.g. rapid mixing Hybrid Monte Carlo and efficient gradient-basedoptimization. The relative efficiency is confirmed in ex- periments.
arxiv-3900-268 | Provable Inductive Matrix Completion | http://arxiv.org/abs/1306.0626 | author:Prateek Jain, Inderjit S. Dhillon category:cs.LG cs.IT math.IT stat.ML published:2013-06-04 summary:Consider a movie recommendation system where apart from the ratingsinformation, side information such as user's age or movie's genre is alsoavailable. Unlike standard matrix completion, in this setting one should beable to predict inductively on new users/movies. In this paper, we study theproblem of inductive matrix completion in the exact recovery setting. That is,we assume that the ratings matrix is generated by applying feature vectors to alow-rank matrix and the goal is to recover back the underlying matrix.Furthermore, we generalize the problem to that of low-rank matrix estimationusing rank-1 measurements. We study this generic problem and provide conditionsthat the set of measurements should satisfy so that the alternatingminimization method (which otherwise is a non-convex method with no convergenceguarantees) is able to recover back the {\em exact} underlying low-rank matrix. In addition to inductive matrix completion, we show that two other low-rankestimation problems can be studied in our framework: a) general low-rank matrixsensing using rank-1 measurements, and b) multi-label regression with missinglabels. For both the problems, we provide novel and interesting bounds on thenumber of measurements required by alternating minimization to provablyconverges to the {\em exact} low-rank matrix. In particular, our analysis forthe general low rank matrix sensing problem significantly improves the requiredstorage and computational cost than that required by the RIP-based matrixsensing methods \cite{RechtFP2007}. Finally, we provide empirical validation ofour approach and demonstrate that alternating minimization is able to recoverthe true matrix for the above mentioned problems using a small number ofmeasurements.
arxiv-3900-269 | Particle approximations of the score and observed information matrix for parameter estimation in state space models with linear computational cost | http://arxiv.org/abs/1306.0735 | author:Christopher Nemeth, Paul Fearnhead, Lyudmila Mihaylova category:stat.CO stat.ML published:2013-06-04 summary:Poyiadjis et al. (2011) show how particle methods can be used to estimateboth the score and the observed information matrix for state space models.These methods either suffer from a computational cost that is quadratic in thenumber of particles, or produce estimates whose variance increasesquadratically with the amount of data. This paper introduces an alternativeapproach for estimating these terms at a computational cost that is linear inthe number of particles. The method is derived using a combination of kerneldensity estimation, to avoid the particle degeneracy that causes thequadratically increasing variance, and Rao-Blackwellisation. Crucially, we showthe method is robust to the choice of bandwidth within the kernel densityestimation, as it has good asymptotic properties regardless of this choice. Ourestimates of the score and observed information matrix can be used within bothonline and batch procedures for estimating parameters for state space models.Empirical results show improved parameter estimates compared to existingmethods at a significantly reduced computational cost. Supplementary materialsincluding code are available.
arxiv-3900-270 | (More) Efficient Reinforcement Learning via Posterior Sampling | http://arxiv.org/abs/1306.0940 | author:Ian Osband, Daniel Russo, Benjamin Van Roy category:stat.ML cs.LG published:2013-06-04 summary:Most provably-efficient learning algorithms introduce optimism aboutpoorly-understood states and actions to encourage exploration. We study analternative approach for efficient exploration, posterior sampling forreinforcement learning (PSRL). This algorithm proceeds in repeated episodes ofknown duration. At the start of each episode, PSRL updates a prior distributionover Markov decision processes and takes one sample from this posterior. PSRLthen follows the policy that is optimal for this sample during the episode. Thealgorithm is conceptually simple, computationally efficient and allows an agentto encode prior knowledge in a natural way. We establish an $\tilde{O}(\tau S\sqrt{AT})$ bound on the expected regret, where $T$ is time, $\tau$ is theepisode length and $S$ and $A$ are the cardinalities of the state and actionspaces. This bound is one of the first for an algorithm not based on optimism,and close to the state of the art for any reinforcement learning algorithm. Weshow through simulation that PSRL significantly outperforms existing algorithmswith similar regret bounds.
arxiv-3900-271 | Distributed k-Means and k-Median Clustering on General Topologies | http://arxiv.org/abs/1306.0604 | author:Maria Florina Balcan, Steven Ehrlich, Yingyu Liang category:cs.LG cs.DC stat.ML published:2013-06-03 summary:This paper provides new algorithms for distributed clustering for two popularcenter-based objectives, k-median and k-means. These algorithms have provableguarantees and improve communication complexity over existing approaches.Following a classic approach in clustering by \cite{har2004coresets}, we reducethe problem of finding a clustering with low cost to the problem of finding acoreset of small size. We provide a distributed method for constructing aglobal coreset which improves over the previous methods by reducing thecommunication complexity, and which works over general communicationtopologies. Experimental results on large scale data sets show that thisapproach outperforms other coreset-based distributed clustering algorithms.
arxiv-3900-272 | Constructive Setting of the Density Ratio Estimation Problem and its Rigorous Solution | http://arxiv.org/abs/1306.0407 | author:Vladimir Vapnik, Igor Braga, Rauf Izmailov category:stat.ML published:2013-06-03 summary:We introduce a general constructive setting of the density ratio estimationproblem as a solution of a (multidimensional) integral equation. In thisequation, not only its right hand side is known approximately, but also theintegral operator is defined approximately. We show that this ill-posed problemhas a rigorous solution and obtain the solution in a closed form. The keyelement of this solution is the novel V-matrix, which captures the geometry ofthe observed samples. We compare our method with three well-known previouslyproposed ones. Our experimental results demonstrate the good potential of thenew approach.
arxiv-3900-273 | Prediction with Missing Data via Bayesian Additive Regression Trees | http://arxiv.org/abs/1306.0618 | author:Adam Kapelner, Justin Bleich category:stat.ML cs.LG published:2013-06-03 summary:We present a method for incorporating missing data in non-parametricstatistical learning without the need for imputation. We focus on a tree-basedmethod, Bayesian Additive Regression Trees (BART), enhanced with "MissingnessIncorporated in Attributes," an approach recently proposed incorporatingmissingness into decision trees (Twala, 2008). This procedure takes advantageof the partitioning mechanisms found in tree-based models. Simulations ongenerated models and real data indicate that our proposed method can forecastwell on complicated missing-at-random and not-missing-at-random models as wellas models where missingness itself influences the response. Our procedure hashigher predictive performance and is more stable than competitors in manycases. We also illustrate BART's abilities to incorporate missingness intouncertainty intervals and to detect the influence of missingness on the modelfit.
arxiv-3900-274 | Probabilistic Solutions to Differential Equations and their Application to Riemannian Statistics | http://arxiv.org/abs/1306.0308 | author:Philipp Hennig, Søren Hauberg category:stat.ML cs.LG math.NA published:2013-06-03 summary:We study a probabilistic numerical method for the solution of both boundaryand initial value problems that returns a joint Gaussian process posterior overthe solution. Such methods have concrete value in the statistics on Riemannianmanifolds, where non-analytic ordinary differential equations are involved invirtually all computations. The probabilistic formulation permits marginalisingthe uncertainty of the numerical solution such that statistics are lesssensitive to inaccuracies. This leads to new Riemannian algorithms for meanvalue computations and principal geodesic analysis. Marginalisation also meansresults can be less precise than point estimates, enabling a noticeablespeed-up over the state of the art. Our approach is an argument for a widerpoint that uncertainty caused by numerical calculations should be trackedthroughout the pipeline of machine learning algorithms.
arxiv-3900-275 | Iterative Grassmannian Optimization for Robust Image Alignment | http://arxiv.org/abs/1306.0404 | author:Jun He, Dejiao Zhang, Laura Balzano, Tao Tao category:cs.CV math.OC stat.ML published:2013-06-03 summary:Robust high-dimensional data processing has witnessed an exciting developmentin recent years, as theoretical results have shown that it is possible usingconvex programming to optimize data fit to a low-rank component plus a sparseoutlier component. This problem is also known as Robust PCA, and it has foundapplication in many areas of computer vision. In image and video processing andface recognition, the opportunity to process massive image databases isemerging as people upload photo and video data online in unprecedented volumes.However, data quality and consistency is not controlled in any way, and themassiveness of the data poses a serious computational challenge. In this paperwe present t-GRASTA, or "Transformed GRASTA (Grassmannian Robust AdaptiveSubspace Tracking Algorithm)". t-GRASTA iteratively performs incrementalgradient descent constrained to the Grassmann manifold of subspaces in order tosimultaneously estimate a decomposition of a collection of images into alow-rank subspace, a sparse part of occlusions and foreground objects, and atransformation such as rotation or translation of the image. We show thatt-GRASTA is 4 $\times$ faster than state-of-the-art algorithms, has half thememory requirement, and can achieve alignment for face images as well asjittered camera surveillance images.
arxiv-3900-276 | Learning from networked examples in a k-partite graph | http://arxiv.org/abs/1306.0393 | author:Yuyi Wang, Jan Ramon, Zheng-Chu Guo category:cs.LG stat.ML published:2013-06-03 summary:Many machine learning algorithms are based on the assumption that trainingexamples are drawn independently. However, this assumption does not holdanymore when learning from a networked sample where two or more trainingexamples may share common features. We propose an efficient weighting methodfor learning from networked examples and show the sample error bound which isbetter than previous work.
arxiv-3900-277 | On the Performance Bounds of some Policy Search Dynamic Programming Algorithms | http://arxiv.org/abs/1306.0539 | author:Bruno Scherrer category:cs.AI cs.LG published:2013-06-03 summary:We consider the infinite-horizon discounted optimal control problemformalized by Markov Decision Processes. We focus on Policy Search algorithms,that compute an approximately optimal policy by following the standard PolicyIteration (PI) scheme via an -approximate greedy operator (Kakade and Langford,2002; Lazaric et al., 2010). We describe existing and a few new performancebounds for Direct Policy Iteration (DPI) (Lagoudakis and Parr, 2003; Fern etal., 2006; Lazaric et al., 2010) and Conservative Policy Iteration (CPI)(Kakade and Langford, 2002). By paying a particular attention to theconcentrability constants involved in such guarantees, we notably argue thatthe guarantee of CPI is much better than that of DPI, but this comes at thecost of a relative--exponential in $\frac{1}{\epsilon}$-- increase of timecomplexity. We then describe an algorithm, Non-Stationary Direct PolicyIteration (NSDPI), that can either be seen as 1) a variation of Policy Searchby Dynamic Programming by Bagnell et al. (2003) to the infinite horizonsituation or 2) a simplified version of the Non-Stationary PI with growingperiod of Scherrer and Lesner (2012). We provide an analysis of this algorithm,that shows in particular that it enjoys the best of both worlds: itsperformance guarantee is similar to that of CPI, but within a time complexitysimilar to that of DPI.
arxiv-3900-278 | Riemannian metrics for neural networks II: recurrent networks and learning symbolic data sequences | http://arxiv.org/abs/1306.0514 | author:Yann Ollivier category:cs.NE cs.LG 68T05, 68T10 published:2013-06-03 summary:Recurrent neural networks are powerful models for sequential data, able torepresent complex dependencies in the sequence that simpler models such ashidden Markov models cannot handle. Yet they are notoriously hard to train.Here we introduce a training procedure using a gradient ascent in a Riemannianmetric: this produces an algorithm independent from design choices such as theencoding of parameters and unit activities. This metric gradient ascent isdesigned to have an algorithmic cost close to backpropagation through time forsparsely connected networks. We use this procedure on gated leaky neuralnetworks (GLNNs), a variant of recurrent neural networks with an architectureinspired by finite automata and an evolution equation inspired bycontinuous-time networks. GLNNs trained with a Riemannian gradient aredemonstrated to effectively capture a variety of structures in syntheticproblems: basic block nesting as in context-free grammars (an important featureof natural languages, but difficult to learn), intersections of multipleindependent Markov-type relations, or long-distance relationships such as thedistant-XOR problem. This method does not require adjusting the networkstructure or initial parameters: the network used is a sparse random graph andthe initialization is identical for all problems considered.
arxiv-3900-279 | KERT: Automatic Extraction and Ranking of Topical Keyphrases from Content-Representative Document Titles | http://arxiv.org/abs/1306.0271 | author:Marina Danilevsky, Chi Wang, Nihit Desai, Jingyi Guo, Jiawei Han category:cs.LG cs.IR published:2013-06-03 summary:We introduce KERT (Keyphrase Extraction and Ranking by Topic), a frameworkfor topical keyphrase generation and ranking. By shifting from theunigram-centric traditional methods of unsupervised keyphrase extraction to aphrase-centric approach, we are able to directly compare and rank phrases ofdifferent lengths. We construct a topical keyphrase ranking function whichimplements the four criteria that represent high quality topical keyphrases(coverage, purity, phraseness, and completeness). The effectiveness of ourapproach is demonstrated on two collections of content-representative titles inthe domains of Computer Science and Physics.
arxiv-3900-280 | Predicting Parameters in Deep Learning | http://arxiv.org/abs/1306.0543 | author:Misha Denil, Babak Shakibi, Laurent Dinh, Marc'Aurelio Ranzato, Nando de Freitas category:cs.LG cs.NE stat.ML published:2013-06-03 summary:We demonstrate that there is significant redundancy in the parameterizationof several deep learning models. Given only a few weight values for eachfeature it is possible to accurately predict the remaining values. Moreover, weshow that not only can the parameter values be predicted, but many of them neednot be learned at all. We train several different architectures by learningonly a small number of weights and predicting the rest. In the best case we areable to predict more than 95% of the weights of a network without any drop inaccuracy.
arxiv-3900-281 | Evolutionary Approach for the Containers Bin-Packing Problem | http://arxiv.org/abs/1306.0442 | author:R. Kammarti, I. Ayachi, M. Ksouri, P. Borne category:cs.NE published:2013-06-03 summary:This paper deals with the resolution of combinatorial optimization problems,particularly those concerning the maritime transport scheduling. We areinterested in the management platforms in a river port and more specifically incontainer organisation operations with a view to minimizing the number ofcontainer rehandlings. Subsequently, we rmeet customers delivery deadlines andwe reduce ship stoppage time In this paper, we propose a genetic algorithm tosolve this problem and we present some experiments and results.
arxiv-3900-282 | Convergence Analysis and Parallel Computing Implementation for the Multiagent Coordination Optimization Algorithm | http://arxiv.org/abs/1306.0225v10.pdf | author:Qing Hui, Haopeng Zhang category:math.OC cs.NE math.DS published:2013-06-02 summary:In this report, a novel variation of Particle Swarm Optimization (PSO)algorithm, called Multiagent Coordination Optimization (MCO), is implemented ina parallel computing way for practical use by introducing MATLAB built-infunction "parfor" into MCO. Then we rigorously analyze the global convergenceof MCO by means of semistability theory. Besides sharing global optimalsolutions with the PSO algorithm, the MCO algorithm integrates cooperativeswarm behavior of multiple agents into the update formula by sharing velocityand position information between neighbors to improve its performance.Numerical evaluation of the parallel MCO algorithm is provided in the report byrunning the proposed algorithm on supercomputers in the High PerformanceComputing Center at Texas Tech University. In particular, the optimal value andconsuming time are compared with PSO and serial MCO by solving severalbenchmark functions in the literature, respectively. Based on the simulationresults, the performance of the parallel MCO is not only superb compared withPSO for solving many nonlinear, noncovex optimization problems, but also is ofhigh efficiency by saving the computational time.
arxiv-3900-283 | Phase Retrieval using Alternating Minimization | http://arxiv.org/abs/1306.0160 | author:Praneeth Netrapalli, Prateek Jain, Sujay Sanghavi category:stat.ML cs.IT cs.LG math.IT published:2013-06-02 summary:Phase retrieval problems involve solving linear equations, but with missingsign (or phase, for complex numbers) information. More than four decades afterit was first proposed, the seminal error reduction algorithm of (Gerchberg andSaxton 1972) and (Fienup 1982) is still the popular choice for solving manyvariants of this problem. The algorithm is based on alternating minimization;i.e. it alternates between estimating the missing phase information, and thecandidate solution. Despite its wide usage in practice, no global convergenceguarantees for this algorithm are known. In this paper, we show that a(resampling) variant of this approach converges geometrically to the solutionof one such problem -- finding a vector $\mathbf{x}$ from$\mathbf{y},\mathbf{A}$, where $\mathbf{y} =\left\mathbf{A}^{\top}\mathbf{x}\right$ and $\mathbf{z}$ denotes a vectorof element-wise magnitudes of $\mathbf{z}$ -- under the assumption that$\mathbf{A}$ is Gaussian. Empirically, we demonstrate that alternating minimization performs similar torecently proposed convex techniques for this problem (which are based on"lifting" to a convex matrix problem) in sample complexity and robustness tonoise. However, it is much more efficient and can scale to large problems.Analytically, for a resampling version of alternating minimization, we showgeometric convergence to the solution, and sample complexity that is off by logfactors from obvious lower bounds. We also establish close to optimal scalingfor the case when the unknown vector is sparse. Our work represents the firsttheoretical guarantee for alternating minimization (albeit with resampling) forany variant of phase retrieval problems in the non-convex setting.
arxiv-3900-284 | Gaussian Process-Based Decentralized Data Fusion and Active Sensing for Mobility-on-Demand System | http://arxiv.org/abs/1306.1491 | author:Jie Chen, Kian Hsiang Low, Colin Keng-Yan Tan category:cs.RO cs.DC cs.LG cs.MA published:2013-06-02 summary:Mobility-on-demand (MoD) systems have recently emerged as a promisingparadigm of one-way vehicle sharing for sustainable personal urban mobility indensely populated cities. In this paper, we enhance the capability of a MoDsystem by deploying robotic shared vehicles that can autonomously cruise thestreets to be hailed by users. A key challenge to managing the MoD systemeffectively is that of real-time, fine-grained mobility demand sensing andprediction. This paper presents a novel decentralized data fusion and activesensing algorithm for real-time, fine-grained mobility demand sensing andprediction with a fleet of autonomous robotic vehicles in a MoD system. OurGaussian process (GP)-based decentralized data fusion algorithm can achieve afine balance between predictive power and time efficiency. We theoreticallyguarantee its predictive performance to be equivalent to that of asophisticated centralized sparse approximation for the GP model: Thecomputation of such a sparse approximate GP model can thus be distributed amongthe MoD vehicles, hence achieving efficient and scalable demand prediction.Though our decentralized active sensing strategy is devised to gather the mostinformative demand data for demand prediction, it can achieve a dual effect offleet rebalancing to service the mobility demands. Empirical evaluation onreal-world mobility demand data shows that our proposed algorithm can achieve abetter balance between predictive accuracy and time efficiency thanstate-of-the-art algorithms.
arxiv-3900-285 | Deep Learning using Linear Support Vector Machines | http://arxiv.org/abs/1306.0239 | author:Yichuan Tang category:cs.LG stat.ML published:2013-06-02 summary:Recently, fully-connected and convolutional neural networks have been trainedto achieve state-of-the-art performance on a wide variety of tasks such asspeech recognition, image classification, natural language processing, andbioinformatics. For classification tasks, most of these "deep learning" modelsemploy the softmax activation function for prediction and minimizecross-entropy loss. In this paper, we demonstrate a small but consistentadvantage of replacing the softmax layer with a linear support vector machine.Learning minimizes a margin-based loss instead of the cross-entropy loss. Whilethere have been various combinations of neural nets and SVMs in prior art, ourresults using L2-SVMs show that by simply replacing softmax with linear SVMsgives significant gains on popular deep learning datasets MNIST, CIFAR-10, andthe ICML 2013 Representation Learning Workshop's face expression recognitionchallenge.
arxiv-3900-286 | Using a bag of Words for Automatic Medical Image Annotation with a Latent Semantic | http://arxiv.org/abs/1306.0178 | author:Riadh Bouslimi, Abir Messaoudi, Jalel Akaichi category:cs.IR cs.CV published:2013-06-02 summary:We present in this paper a new approach for the automatic annotation ofmedical images, using the approach of "bag-of-words" to represent the visualcontent of the medical image combined with text descriptors based approachtf.idf and reduced by latent semantic to extract the co-occurrence betweenterms and visual terms. A medical report is composed of a text describing amedical image. First, we are interested to index the text and extract allrelevant terms using a thesaurus containing MeSH medical concepts. In a secondphase, the medical image is indexed while recovering areas of interest whichare invariant to change in scale, light and tilt. To annotate a new medicalimage, we use the approach of "bagof-words" to recover the feature vector.Indeed, we use the vector space model to retrieve similar medical image fromthe database training. The calculation of the relevance value of an image tothe query image is based on the cosine function. We conclude with an experimentcarried out on five types of radiological imaging to evaluate the performanceof our system of medical annotation. The results showed that our approach worksbetter with more images from the radiology of the skull.
arxiv-3900-287 | Declarative Modeling and Bayesian Inference of Dark Matter Halos | http://arxiv.org/abs/1306.0202 | author:Gabriel Kronberger category:stat.ML astro-ph.IM published:2013-06-02 summary:Probabilistic programming allows specification of probabilistic models in adeclarative manner. Recently, several new software systems and languages forprobabilistic programming have been developed on the basis of newly developedand improved methods for approximate inference in probabilistic models. In thiscontribution a probabilistic model for an idealized dark matter localizationproblem is described. We first derive the probabilistic model for the inferenceof dark matter locations and masses, and then show how this model can beimplemented using BUGS and Infer.NET, two software systems for probabilisticprogramming. Finally, the different capabilities of both systems are discussed.The presented dark matter model includes mainly non-conjugate factors, thus, itis difficult to implement this model with Infer.NET.
arxiv-3900-288 | RNADE: The real-valued neural autoregressive density-estimator | http://arxiv.org/abs/1306.0186 | author:Benigno Uria, Iain Murray, Hugo Larochelle category:stat.ML cs.LG published:2013-06-02 summary:We introduce RNADE, a new model for joint density estimation of real-valuedvectors. Our model calculates the density of a datapoint as the product ofone-dimensional conditionals modeled using mixture density networks with sharedparameters. RNADE learns a distributed representation of the data, while havinga tractable expression for the calculation of densities. A tractable likelihoodallows direct comparison with other methods and training by standardgradient-based optimizers. We compare the performance of RNADE on severaldatasets of heterogeneous and perceptual data, finding it outperforms mixturemodels in all but one case.
arxiv-3900-289 | Guided Random Forest in the RRF Package | http://arxiv.org/abs/1306.0237 | author:Houtao Deng category:cs.LG published:2013-06-02 summary:Random Forest (RF) is a powerful supervised learner and has been popularlyused in many applications such as bioinformatics. In this work we propose the guided random forest (GRF) for feature selection.Similar to a feature selection method called guided regularized random forest(GRRF), GRF is built using the importance scores from an ordinary RF. However,the trees in GRRF are built sequentially, are highly correlated and do notallow for parallel computing, while the trees in GRF are built independentlyand can be implemented in parallel. Experiments on 10 high-dimensional genedata sets show that, with a fixed parameter value (without tuning theparameter), RF applied to features selected by GRF outperforms RF applied toall features on 9 data sets and 7 of them have significant differences at the0.05 level. Therefore, both accuracy and interpretability are significantlyimproved. GRF selects more features than GRRF, however, leads to betterclassification accuracy. Note in this work the guided random forest is guidedby the importance scores from an ordinary random forest, however, it can alsobe guided by other methods such as human insights (by specifying $\lambda_i$).GRF can be used in "RRF" v1.4 (and later versions), a package that alsoincludes the regularized random forest methods.
arxiv-3900-290 | Image Inpainting by Kriging Interpolation Technique | http://arxiv.org/abs/1306.0139 | author:Firas A. Jassim category:cs.CV published:2013-06-01 summary:Image inpainting is the art of predicting damaged regions of an image. Themanual way of image inpainting is a time consuming. Therefore, there must be anautomatic digital method for image inpainting that recovers the image from thedamaged regions. In this paper, a novel statistical image inpainting algorithmbased on Kriging interpolation technique was proposed. Kriging techniqueautomatically fills the damaged region in an image using the informationavailable from its surrounding regions in such away that it uses the spatialcorrelation structure of points inside the k-by-k block. Kriging has theability to face the challenge of keeping the structure and texture informationas the size of damaged region heighten. Experimental results showed that,Kriging has a high PSNR value when recovering a variety of test images fromscratches and text as damaged regions.
arxiv-3900-291 | Understanding ACT-R - an Outsider's Perspective | http://arxiv.org/abs/1306.0125 | author:Jacob Whitehill category:cs.LG published:2013-06-01 summary:The ACT-R theory of cognition developed by John Anderson and colleaguesendeavors to explain how humans recall chunks of information and how they solveproblems. ACT-R also serves as a theoretical basis for "cognitive tutors",i.e., automatic tutoring systems that help students learn mathematics, computerprogramming, and other subjects. The official ACT-R definition is distributedacross a large body of literature spanning many articles and monographs, andhence it is difficult for an "outsider" to learn the most important aspects ofthe theory. This paper aims to provide a tutorial to the core components of theACT-R theory.
arxiv-3900-292 | Harmony search algorithm for the container storage problem | http://arxiv.org/abs/1306.0090 | author:I. Ayachi, R. Kammarti, M. Ksouri, P. Borne, Lagis Ecole Centrale de Lille, Lacs Ecole Nationale category:cs.NE published:2013-06-01 summary:Recently a new metaheuristic called harmony search was developed. It mimicsthe behaviors of musicians improvising to find the better state harmony. Inthis paper, this algorithm is described and applied to solve the containerstorage problem in the harbor. The objective of this problem is to determine avalid containers arrangement, which meets customers delivery deadlines, reducesthe number of container rehandlings and minimizes the ship idle time. In thispaper, an adaptation of the harmony search algorithm to the container storageproblem is detailed and some experimental results are presented and discussed.The proposed approach was compared to a genetic algorithm previously applied tothe same problem and recorded a good results.
arxiv-3900-293 | An Analysis of the Connections Between Layers of Deep Neural Networks | http://arxiv.org/abs/1306.0152 | author:Eugenio Culurciello, Jonghoon Jin, Aysegul Dundar, Jordan Bates category:cs.CV published:2013-06-01 summary:We present an analysis of different techniques for selecting the connectionbe- tween layers of deep neural networks. Traditional deep neural networks useran- dom connection tables between layers to keep the number of connectionssmall and tune to different image features. This kind of connection performsadequately in supervised deep networks because their values are refined duringthe training. On the other hand, in unsupervised learning, one cannot rely onback-propagation techniques to learn the connections between layers. In thiswork, we tested four different techniques for connecting the first layer of thenetwork to the second layer on the CIFAR and SVHN datasets and showed that theaccuracy can be im- proved up to 3% depending on the technique used. We alsoshowed that learning the connections based on the co-occurrences of thefeatures does not confer an advantage over a random connection table in smallnetworks. This work is helpful to improve the efficiency of connections betweenthe layers of unsupervised deep neural networks.
arxiv-3900-294 | Dynamic Ad Allocation: Bandits with Budgets | http://arxiv.org/abs/1306.0155 | author:Aleksandrs Slivkins category:cs.LG cs.DS published:2013-06-01 summary:We consider an application of multi-armed bandits to internet advertising(specifically, to dynamic ad allocation in the pay-per-click model, withuncertainty on the click probabilities). We focus on an important practicalissue that advertisers are constrained in how much money they can spend ontheir ad campaigns. This issue has not been considered in the prior work onbandit-based approaches for ad allocation, to the best of our knowledge. We define a simple, stylized model where an algorithm picks one ad to displayin each round, and each ad has a \emph{budget}: the maximal amount of moneythat can be spent on this ad. This model admits a natural variant of UCB1, awell-known algorithm for multi-armed bandits with stochastic rewards. We derivestrong provable guarantees for this algorithm.
arxiv-3900-295 | Joint Modeling and Registration of Cell Populations in Cohorts of High-Dimensional Flow Cytometric Data | http://arxiv.org/abs/1305.7344 | author:Saumyadipta Pyne, Kui Wang, Jonathan Irish, Pablo Tamayo, Marc-Danie Nazaire, Tarn Duong, Sharon Lee, Shu-Kay Ng, David Hafler, Ronald Levy, Garry Nolan, Jill Mesirov, Geoffrey J. McLachlan category:stat.ML published:2013-05-31 summary:In systems biomedicine, an experimenter encounters different potentialsources of variation in data such as individual samples, multiple experimentalconditions, and multi-variable network-level responses. In multiparametriccytometry, which is often used for analyzing patient samples, such issues arecritical. While computational methods can identify cell populations inindividual samples, without the ability to automatically match them acrosssamples, it is difficult to compare and characterize the populations in typicalexperiments, such as those responding to various stimulations or distinctive ofparticular patients or time-points, especially when there are many samples.Joint Clustering and Matching (JCM) is a multi-level framework for simultaneousmodeling and registration of populations across a cohort. JCM models everypopulation with a robust multivariate probability distribution. Simultaneously,JCM fits a random-effects model to construct an overall batch template -- usedfor registering populations across samples, and classifying new samples. Bytackling systems-level variation, JCM supports practical biomedicalapplications involving large cohorts.
arxiv-3900-296 | Real-world Transfer of Evolved Artificial Immune System Behaviours between Small and Large Scale Robotic Platforms | http://arxiv.org/abs/1305.7432 | author:Amanda Whitbrook, Uwe Aickelin, Jonathan M. Garibaldi category:cs.NE cs.RO published:2013-05-31 summary:In mobile robotics, a solid test for adaptation is the ability of a controlsystem to function not only in a diverse number of physical environments, butalso on a number of different robotic platforms. This paper demonstrates that aset of behaviours evolved in simulation on a miniature robot (epuck) can betransferred to a much larger-scale platform (Pioneer), both in simulation andin the real world. The chosen architecture uses artificial evolution of epuckbehaviours to obtain a genetic sequence, which is then employed to seed anidiotypic, artificial immune system (AIS) on the Pioneers. Despite numeroushardware and software differences between the platforms, navigation andtarget-finding experiments show that the evolved behaviours transfer very wellto the larger robot when the idiotypic AIS technique is used. In contrast,transferability is poor when reinforcement learning alone is used, whichvalidates the adaptability of the chosen architecture.
arxiv-3900-297 | On model selection consistency of regularized M-estimators | http://arxiv.org/abs/1305.7477 | author:Jason D. Lee, Yuekai Sun, Jonathan E. Taylor category:math.ST cs.LG math.OC stat.ME stat.ML stat.TH published:2013-05-31 summary:Regularized M-estimators are used in diverse areas of science and engineeringto fit high-dimensional models with some low-dimensional structure. Usually thelow-dimensional structure is encoded by the presence of the (unknown)parameters in some low-dimensional model subspace. In such settings, it isdesirable for estimates of the model parameters to be \emph{model selectionconsistent}: the estimates also fall in the model subspace. We develop ageneral framework for establishing consistency and model selection consistencyof regularized M-estimators and show how it applies to some special cases ofinterest in statistical learning. Our analysis identifies two key properties ofregularized M-estimators, referred to as geometric decomposability andirrepresentability, that ensure the estimators are consistent and modelselection consistent.
arxiv-3900-298 | A central limit theorem for scaled eigenvectors of random dot product graphs | http://arxiv.org/abs/1305.7388 | author:Avanti Athreya, Vince Lyzinski, David J. Marchette, Carey E. Priebe, Daniel L. Sussman, Minh Tang category:math.ST stat.ML stat.TH published:2013-05-31 summary:We prove a central limit theorem for the components of the largesteigenvectors of the adjacency matrix of a finite-dimensional random dot productgraph whose true latent positions are unknown. In particular, we follow themethodology outlined in \citet{sussman2012universally} to construct consistentestimates for the latent positions, and we show that the appropriately scaleddifferences between the estimated and true latent positions converge to amixture of Gaussian random variables. As a corollary, we obtain a central limittheorem for the first eigenvector of the adjacency matrix of an Erd\"os-Renyirandom graph.
arxiv-3900-299 | Robust Hyperspectral Unmixing with Correntropy based Metric | http://arxiv.org/abs/1305.7311 | author:Ying Wang, Chunhong Pan, Shiming Xiang, Feiyun Zhu category:cs.CV published:2013-05-31 summary:Hyperspectral unmixing is one of the crucial steps for many hyperspectralapplications. The problem of hyperspectral unmixing has proven to be adifficult task in unsupervised work settings where the endmembers andabundances are both unknown. What is more, this task becomes more challengingin the case that the spectral bands are degraded with noise. This paperpresents a robust model for unsupervised hyperspectral unmixing. Specifically,our model is developed with the correntropy based metric where the non-negativeconstraints on both endmembers and abundances are imposed to keep physicalsignificance. In addition, a sparsity prior is explicitly formulated toconstrain the distribution of the abundances of each endmember. To solve ourmodel, a half-quadratic optimization technique is developed to convert theoriginal complex optimization problem into an iteratively re-weighted NMF withsparsity constraints. As a result, the optimization of our model can adaptivelyassign small weights to noisy bands and give more emphasis on noise-free bands.In addition, with sparsity constraints, our model can naturally generate sparseabundances. Experiments on synthetic and real data demonstrate theeffectiveness of our model in comparison to the related state-of-the-artunmixing models.
arxiv-3900-300 | Expectation-maximization for logistic regression | http://arxiv.org/abs/1306.0040 | author:James G. Scott, Liang Sun category:stat.CO math.ST stat.ML stat.TH published:2013-05-31 summary:We present a family of expectation-maximization (EM) algorithms for binaryand negative-binomial logistic regression, drawing a sharp connection with thevariational-Bayes algorithm of Jaakkola and Jordan (2000). Indeed, our resultsallow a version of this variational-Bayes approach to be re-interpreted as atrue EM algorithm. We study several interesting features of the algorithm, andof this previously unrecognized connection with variational Bayes. We alsogeneralize the approach to sparsity-promoting priors, and to an online methodwhose convergence properties are easily established. This latter methodcompares favorably with stochastic-gradient descent in situations with markedcollinearity.
