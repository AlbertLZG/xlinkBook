arxiv-5700-1 | Submodularization for Quadratic Pseudo-Boolean Optimization | http://arxiv.org/pdf/1311.1856v2.pdf | author:Lena Gorelick, Yuri Boykov, Olga Veksler, Ismail Ben Ayed, Andrew Delong category:cs.CV published:2013-11-08 summary:Many computer vision problems require optimization of binary non-submodularenergies. We propose a general optimization framework based on local submodularapproximations (LSA). Unlike standard LP relaxation methods that linearize thewhole energy globally, our approach iteratively approximates the energieslocally. On the other hand, unlike standard local optimization methods (e.g.gradient descent or projection techniques) we use non-linear submodularapproximations and optimize them without leaving the domain of integersolutions. We discuss two specific LSA algorithms based on "trust region" and"auxiliary function" principles, LSA-TR and LSA-AUX. These methods obtainstate-of-the-art results on a wide range of applications outperforming manystandard techniques such as LBP, QPBO, and TRWS. While our paper is focused onpairwise energies, our ideas extend to higher-order problems. The code isavailable online (http://vision.csd.uwo.ca/code/).
arxiv-5700-2 | Relative Comparison Kernel Learning with Auxiliary Kernels | http://arxiv.org/pdf/1309.0489v3.pdf | author:Eric Heim, Hamed Valizadegan, Milos Hauskrecht category:cs.LG published:2013-09-02 summary:In this work we consider the problem of learning a positive semidefinitekernel matrix from relative comparisons of the form: "object A is more similarto object B than it is to C", where comparisons are given by humans. Existingsolutions to this problem assume many comparisons are provided to learn a highquality kernel. However, this can be considered unrealistic for many real-worldtasks since relative assessments require human input, which is often costly ordifficult to obtain. Because of this, only a limited number of thesecomparisons may be provided. In this work, we explore methods for aiding theprocess of learning a kernel with the help of auxiliary kernels built from moreeasily extractable information regarding the relationships among objects. Wepropose a new kernel learning approach in which the target kernel is defined asa conic combination of auxiliary kernels and a kernel whose elements arelearned directly. We formulate a convex optimization to solve for this targetkernel that adds only minor overhead to methods that use no auxiliaryinformation. Empirical results show that in the presence of few trainingrelative comparisons, our method can learn kernels that generalize to moreout-of-sample comparisons than methods that do not utilize auxiliaryinformation, as well as similar methods that learn metrics over objects.
arxiv-5700-3 | An effective AHP-based metaheuristic approach to solve supplier selection problem | http://arxiv.org/pdf/1404.4067v1.pdf | author:Tamal Ghosh, Tanmoy Chakraborty, Pranab K Dan category:cs.NE published:2014-04-15 summary:The supplier selection problem is based on electing the best supplier from agroup of pre-specified candidates, is identified as a Multi Criteria DecisionMaking (MCDM), is proportionately significant in terms of qualitative andquantitative attributes. It is a fundamental issue to achieve a trade-offbetween such quantifiable and unquantifiable attributes with an aim toaccomplish the best solution to the abovementioned problem. This articleportrays a metaheuristic based optimization model to solve this NP-Completeproblem. Initially the Analytic Hierarchy Process (AHP) is implemented togenerate an initial feasible solution of the problem. Thereafter a SimulatedAnnealing (SA) algorithm is exploited to improve the quality of the obtainedsolution. The Taguchi robust design method is exploited to solve the criticalissues on the subject of the parameter selection of the SA technique. In orderto verify the proposed methodology the numerical results are demonstrated basedon tangible industry data.
arxiv-5700-4 | Hybrid Conditional Gradient - Smoothing Algorithms with Applications to Sparse and Low Rank Regularization | http://arxiv.org/pdf/1404.3591v2.pdf | author:Andreas Argyriou, Marco Signoretto, Johan Suykens category:math.OC cs.LG stat.ML published:2014-04-14 summary:We study a hybrid conditional gradient - smoothing algorithm (HCGS) forsolving composite convex optimization problems which contain several terms overa bounded set. Examples of these include regularization problems with severalnorms as penalties and a norm constraint. HCGS extends conditional gradientmethods to cases with multiple nonsmooth terms, in which standard conditionalgradient methods may be difficult to apply. The HCGS algorithm borrowstechniques from smoothing proximal methods and requires first-ordercomputations (subgradients and proximity operations). Unlike proximal methods,HCGS benefits from the advantages of conditional gradient methods, which renderit more efficient on certain large scale optimization problems. We demonstratethese advantages with simulations on two matrix optimization problems:regularization of matrices with combined $\ell_1$ and trace norm penalties; anda convex relaxation of sparse PCA.
arxiv-5700-5 | Generalized version of the support vector machine for binary classification problems: supporting hyperplane machine | http://arxiv.org/pdf/1404.3415v2.pdf | author:E. G. Abramov, A. B. Komissarov, D. A. Kornyakov category:cs.LG stat.ML F.1.1; I.5.1 published:2014-04-13 summary:In this paper there is proposed a generalized version of the SVM for binaryclassification problems in the case of using an arbitrary transformation x ->y. An approach similar to the classic SVM method is used. The problem is widelyexplained. Various formulations of primal and dual problems are proposed. Forone of the most important cases the formulae are derived in detail. A simplecomputational example is demonstrated. The algorithm and its implementation ispresented in Octave language.
arxiv-5700-6 | Assessing the Quality of MT Systems for Hindi to English Translation | http://arxiv.org/pdf/1404.3992v1.pdf | author:Aditi Kalyani, Hemant Kumud, Shashi Pal Singh, Ajai Kumar category:cs.CL published:2014-04-15 summary:Evaluation plays a vital role in checking the quality of MT output. It isdone either manually or automatically. Manual evaluation is very time consumingand subjective, hence use of automatic metrics is done most of the times. Thispaper evaluates the translation quality of different MT Engines forHindi-English (Hindi data is provided as input and English is obtained asoutput) using various automatic metrics like BLEU, METEOR etc. Further thecomparison automatic evaluation results with Human ranking have also beengiven.
arxiv-5700-7 | Spiralet Sparse Representation | http://arxiv.org/pdf/1404.3991v1.pdf | author:Reza Farrahi Moghaddam, Mohamed Cheriet category:cs.CV published:2014-04-15 summary:This is the first report on Working Paper WP-RFM-14-01. The potential andcapability of sparse representations is well-known. However, their(multivariate variable) vectorial form, which is completely fine in many fieldsand disciplines, results in removal and filtering of important "spatial"relations that are implicitly carried by two-dimensional [or multi-dimensional]objects, such as images. In this paper, a new approach, called spiralet sparserepresentation, is proposed in order to develop an augmented representation andtherefore a modified sparse representation and theory, which is capable topreserve the data associated to the spatial relations.
arxiv-5700-8 | Texture Based Image Segmentation of Chili Pepper X-Ray Images Using Gabor Filter | http://arxiv.org/pdf/1405.1966v1.pdf | author:M. Rajalakshmi, Dr. P. Subashini category:cs.CV cs.LG published:2014-04-15 summary:Texture segmentation is the process of partitioning an image into regionswith different textures containing a similar group of pixels. Detecting thediscontinuity of the filter's output and their statistical properties help insegmenting and classifying a given image with different texture regions. Inthis proposed paper, chili x-ray image texture segmentation is performed byusing Gabor filter. The texture segmented result obtained from Gabor filter fedinto three texture filters, namely Entropy, Standard Deviation and Rangefilter. After performing texture analysis, features can be extracted by usingStatistical methods. In this paper Gray Level Co-occurrence Matrices and Firstorder statistics are used as feature extraction methods. Features extractedfrom statistical methods are given to Support Vector Machine (SVM) classifier.Using this methodology, it is found that texture segmentation is followed bythe Gray Level Co-occurrence Matrix feature extraction method gives a higheraccuracy rate of 84% when compared with First order feature extraction method. Key Words: Texture segmentation, Texture filter, Gabor filter, Featureextraction methods, SVM classifier.
arxiv-5700-9 | Is it morally acceptable for a system to lie to persuade me? | http://arxiv.org/pdf/1404.3959v1.pdf | author:Marco Guerini, Fabio Pianesi, Oliviero Stock category:cs.CY cs.CL published:2014-04-15 summary:Given the fast rise of increasingly autonomous artificial agents and robots,a key acceptability criterion will be the possible moral implications of theiractions. In particular, intelligent persuasive systems (systems designed toinfluence humans via communication) constitute a highly sensitive topic becauseof their intrinsically social nature. Still, ethical studies in this area arerare and tend to focus on the output of the required action. Instead, this workfocuses on the persuasive acts themselves (e.g. "is it morally acceptable thata machine lies or appeals to the emotions of a person to persuade her, even iffor a good end?"). Exploiting a behavioral approach, based on human assessmentof moral dilemmas -- i.e. without any prior assumption of underlying ethicaltheories -- this paper reports on a set of experiments. These experimentsaddress the type of persuader (human or machine), the strategies adopted(purely argumentative, appeal to positive emotions, appeal to negativeemotions, lie) and the circumstances. Findings display no differences due tothe agent, mild acceptability for persuasion and reveal that truth-conditionalreasoning (i.e. argument validity) is a significant dimension affectingsubjects' judgment. Some implications for the design of intelligent persuasivesystems are discussed.
arxiv-5700-10 | Scalable Matting: A Sub-linear Approach | http://arxiv.org/pdf/1404.3933v1.pdf | author:Philip G. Lee, Ying Wu category:cs.CV published:2014-04-15 summary:Natural image matting, which separates foreground from background, is a veryimportant intermediate step in recent computer vision algorithms. However, itis severely underconstrained and difficult to solve. State-of-the-artapproaches include matting by graph Laplacian, which significantly improves theunderconstrained nature by reducing the solution space. However, matting bygraph Laplacian is still very difficult to solve and gets much harder as theimage size grows: current iterative methods slow down as $\mathcal{O}\left(n^2\right)$ in the resolution $n$. This creates uncomfortable practical limits onthe resolution of images that we can matte. Current literature mitigates theproblem, but they all remain super-linear in complexity. We expose propertiesof the problem that remain heretofore unexploited, demonstrating that anoptimization technique originally intended to solve PDEs can be adapted to takeadvantage of this knowledge to solve the matting problem, not heuristically,but exactly and with sub-linear complexity. This makes ours the most efficientmatting solver currently known by a very wide margin and allows matting finallyto be practical and scalable in the future as consumer photos exceed manydozens of megapixels, and also relieves matting from being a bottleneck forvision algorithms that depend on it.
arxiv-5700-11 | Exploring the power of GPU's for training Polyglot language models | http://arxiv.org/pdf/1404.1521v3.pdf | author:Vivek Kulkarni, Rami Al-Rfou', Bryan Perozzi, Steven Skiena category:cs.LG cs.CL published:2014-04-05 summary:One of the major research trends currently is the evolution of heterogeneousparallel computing. GP-GPU computing is being widely used and severalapplications have been designed to exploit the massive parallelism thatGP-GPU's have to offer. While GPU's have always been widely used in areas ofcomputer vision for image processing, little has been done to investigatewhether the massive parallelism provided by GP-GPU's can be utilizedeffectively for Natural Language Processing(NLP) tasks. In this work, weinvestigate and explore the power of GP-GPU's in the task of learning languagemodels. More specifically, we investigate the performance of training Polyglotlanguage models using deep belief neural networks. We evaluate the performanceof training the model on the GPU and present optimizations that boost theperformance on the GPU.One of the key optimizations, we propose increases theperformance of a function involved in calculating and updating the gradient byapproximately 50 times on the GPU for sufficiently large batch sizes. We showthat with the above optimizations, the GP-GPU's performance on the taskincreases by factor of approximately 3-4. The optimizations we made are genericTheano optimizations and hence potentially boost the performance of othermodels which rely on these operations.We also show that these optimizationsresult in the GPU's performance at this task being now comparable to that onthe CPU. We conclude by presenting a thorough evaluation of the applicabilityof GP-GPU's for this task and highlight the factors limiting the performance oftraining a Polyglot model on the GPU.
arxiv-5700-12 | Combining Drift Analysis and Generalized Schema Theory to Design Efficient Hybrid and/or Mixed Strategy EAs | http://arxiv.org/pdf/1305.2490v2.pdf | author:Boris Mitavskiy, Jun He category:cs.NE published:2013-05-11 summary:Hybrid and mixed strategy EAs have become rather popular for tackling variouscomplex and NP-hard optimization problems. While empirical evidence suggeststhat such algorithms are successful in practice, rather little theoreticalsupport for their success is available, not mentioning a solid mathematicalfoundation that would provide guidance towards an efficient design of this typeof EAs. In the current paper we develop a rigorous mathematical framework thatsuggests such designs based on generalized schema theory, fitness levels anddrift analysis. An example-application for tackling one of the classicalNP-hard problems, the "single-machine scheduling problem" is presented.
arxiv-5700-13 | Meta-evaluation of comparability metrics using parallel corpora | http://arxiv.org/pdf/1404.3759v1.pdf | author:Bogdan Babych, Anthony Hartley category:cs.CL published:2014-04-14 summary:Metrics for measuring the comparability of corpora or texts need to bedeveloped and evaluated systematically. Applications based on a corpus, such astraining Statistical MT systems in specialised narrow domains, require findinga reasonable balance between the size of the corpus and its consistency, withcontrolled and benchmarked levels of comparability for any newly addedsections. In this article we propose a method that can meta-evaluatecomparability metrics by calculating monolingual comparability scoresseparately on the 'source' and 'target' sides of parallel corpora. The range ofscores on the source side is then correlated (using Pearson's r coefficient)with the range of 'target' scores; the higher the correlation - the morereliable is the metric. The intuition is that a good metric should yield thesame distance between different domains in different languages. Our methodgives consistent results for the same metrics on different data sets, whichindicates that it is reliable and can be used for metric comparison or foroptimising settings of parametrised metrics.
arxiv-5700-14 | Pure Strategy or Mixed Strategy? | http://arxiv.org/pdf/1112.1517v4.pdf | author:Jun He, Feidun He, Hongbin Dong category:cs.NE published:2011-12-07 summary:Mixed strategy EAs aim to integrate several mutation operators into a singlealgorithm. However few theoretical analysis has been made to answer thequestion whether and when the performance of mixed strategy EAs is better thanthat of pure strategy EAs. In theory, the performance of EAs can be measured byasymptotic convergence rate and asymptotic hitting time. In this paper, it isproven that given a mixed strategy (1+1) EAs consisting of several mutationoperators, its performance (asymptotic convergence rate and asymptotic hittingtime)is not worse than that of the worst pure strategy (1+1) EA using onemutation operator; if these mutation operators are mutually complementary, thenit is possible to design a mixed strategy (1+1) EA whose performance is betterthan that of any pure strategy (1+1) EA using one mutation operator.
arxiv-5700-15 | Deep Convolutional Ranking for Multilabel Image Annotation | http://arxiv.org/pdf/1312.4894v2.pdf | author:Yunchao Gong, Yangqing Jia, Thomas Leung, Alexander Toshev, Sergey Ioffe category:cs.CV published:2013-12-17 summary:Multilabel image annotation is one of the most important challenges incomputer vision with many real-world applications. While existing work usuallyuse conventional visual features for multilabel annotation, features based onDeep Neural Networks have shown potential to significantly boost performance.In this work, we propose to leverage the advantage of such features and analyzekey components that lead to better performances. Specifically, we show that asignificant performance gain could be obtained by combining convolutionalarchitectures with approximate top-$k$ ranking objectives, as thye naturallyfit the multilabel tagging problem. Our experiments on the NUS-WIDE datasetoutperforms the conventional visual features by about 10%, obtaining the bestreported performance in the literature.
arxiv-5700-16 | Methods for Ordinal Peer Grading | http://arxiv.org/pdf/1404.3656v1.pdf | author:Karthik Raman, Thorsten Joachims category:cs.LG cs.IR H.4 published:2014-04-14 summary:MOOCs have the potential to revolutionize higher education with their wideoutreach and accessibility, but they require instructors to come up withscalable alternates to traditional student evaluation. Peer grading -- havingstudents assess each other -- is a promising approach to tackling the problemof evaluation at scale, since the number of "graders" naturally scales with thenumber of students. However, students are not trained in grading, which meansthat one cannot expect the same level of grading skills as in traditionalsettings. Drawing on broad evidence that ordinal feedback is easier to provideand more reliable than cardinal feedback, it is therefore desirable to allowpeer graders to make ordinal statements (e.g. "project X is better than projectY") and not require them to make cardinal statements (e.g. "project X is aB-"). Thus, in this paper we study the problem of automatically inferringstudent grades from ordinal peer feedback, as opposed to existing methods thatrequire cardinal peer feedback. We formulate the ordinal peer grading problemas a type of rank aggregation problem, and explore several probabilistic modelsunder which to estimate student grades and grader reliability. We study theapplicability of these methods using peer grading data collected from a realclass -- with instructor and TA grades as a baseline -- and demonstrate theefficacy of ordinal feedback techniques in comparison to existing cardinal peergrading methods. Finally, we compare these peer-grading techniques totraditional evaluation techniques.
arxiv-5700-17 | Composite Self-Concordant Minimization | http://arxiv.org/pdf/1308.2867v2.pdf | author:Quoc Tran-Dinh, Anastasios Kyrillidis, Volkan Cevher category:stat.ML cs.LG math.OC published:2013-08-13 summary:We propose a variable metric framework for minimizing the sum of aself-concordant function and a possibly non-smooth convex function, endowedwith an easily computable proximal operator. We theoretically establish theconvergence of our framework without relying on the usual Lipschitz gradientassumption on the smooth part. An important highlight of our work is a new setof analytic step-size selection and correction procedures based on thestructure of the problem. We describe concrete algorithmic instances of ourframework for several interesting applications and demonstrate them numericallyon both synthetic and real data.
arxiv-5700-18 | A Theoretical Assessment of Solution Quality in Evolutionary Algorithms for the Knapsack Problem | http://arxiv.org/pdf/1404.3520v1.pdf | author:Jun He, Boris Mitavskiy, Yuren Zhou category:cs.NE published:2014-04-14 summary:Evolutionary algorithms are well suited for solving the knapsack problem.Some empirical studies claim that evolutionary algorithms can produce goodsolutions to the 0-1 knapsack problem. Nonetheless, few rigorous investigationsaddress the quality of solutions that evolutionary algorithms may produce forthe knapsack problem. The current paper focuses on a theoretical investigationof three types of (N+1) evolutionary algorithms that exploit bitwise mutation,truncation selection, plus different repair methods for the 0-1 knapsackproblem. It assesses the solution quality in terms of the approximation ratio.Our work indicates that the solution produced by pure strategy and mixedstrategy evolutionary algorithms is arbitrarily bad. Nevertheless, theevolutionary algorithm using helper objectives may produce 1/2-approximationsolutions to the 0-1 knapsack problem.
arxiv-5700-19 | A Study on Stroke Rehabilitation through Task-Oriented Control of a Haptic Device via Near-Infrared Spectroscopy-Based BCI | http://arxiv.org/pdf/1308.4017v3.pdf | author:Berdakh Abibullaev, Jinung An, Seung-Hyun Lee, Jeon-Il Moon category:stat.ML cs.HC q-bio.NC published:2013-08-19 summary:This paper presents a study in task-oriented approach to strokerehabilitation by controlling a haptic device via near-infraredspectroscopy-based brain-computer interface (BCI). The task is to command thehaptic device to move in opposing directions of leftward and rightwardmovement. Our study consists of data acquisition, signal preprocessing, andclassification. In data acquisition, we conduct experiments based on twodifferent mental tasks: one on pure motor imagery, and another on combinedmotor imagery and action observation. The experiments were conducted in bothoffline and online modes. In the signal preprocessing, we use localizationmethod to eliminate channels that are irrelevant to the mental task, as well asperform feature extraction for subsequent classification. We propose multiplesupport vector machine classifiers with a majority-voting scheme for improvedclassification results. And lastly, we present test results to demonstrate theefficacy of our proposed approach to possible stroke rehabilitation practice.
arxiv-5700-20 | Fast nonparametric clustering of structured time-series | http://arxiv.org/pdf/1401.1605v2.pdf | author:James Hensman, Magnus Rattray, Neil D. Lawrence category:cs.LG cs.CV stat.ML published:2014-01-08 summary:In this publication, we combine two Bayesian non-parametric models: theGaussian Process (GP) and the Dirichlet Process (DP). Our innovation in the GPmodel is to introduce a variation on the GP prior which enables us to modelstructured time-series data, i.e. data containing groups where we wish to modelinter- and intra-group variability. Our innovation in the DP model is animplementation of a new fast collapsed variational inference procedure whichenables us to optimize our variationala pproximation significantly faster thanstandard VB approaches. In a biological time series application we show how ourmodel better captures salient features of the data, leading to betterconsistency with existing biological classifications, while the associatedinference algorithm provides a twofold speed-up over EM-based variationalinference.
arxiv-5700-21 | Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks | http://arxiv.org/pdf/1312.6082v4.pdf | author:Ian J. Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, Vinay Shet category:cs.CV published:2013-12-20 summary:Recognizing arbitrary multi-character text in unconstrained naturalphotographs is a hard problem. In this paper, we address an equally hardsub-problem in this domain viz. recognizing arbitrary multi-digit numbers fromStreet View imagery. Traditional approaches to solve this problem typicallyseparate out the localization, segmentation, and recognition steps. In thispaper we propose a unified approach that integrates these three steps via theuse of a deep convolutional neural network that operates directly on the imagepixels. We employ the DistBelief implementation of deep neural networks inorder to train large, distributed neural networks on high quality images. Wefind that the performance of this approach increases with the depth of theconvolutional network, with the best performance occurring in the deepestarchitecture we trained, with eleven hidden layers. We evaluate this approachon the publicly available SVHN dataset and achieve over $96\%$ accuracy inrecognizing complete street numbers. We show that on a per-digit recognitiontask, we improve upon the state-of-the-art, achieving $97.84\%$ accuracy. Wealso evaluate this approach on an even more challenging dataset generated fromStreet View imagery containing several tens of millions of street numberannotations and achieve over $90\%$ accuracy. To further explore theapplicability of the proposed system to broader text recognition tasks, weapply it to synthetic distorted text from reCAPTCHA. reCAPTCHA is one of themost secure reverse turing tests that uses distorted text to distinguish humansfrom bots. We report a $99.8\%$ accuracy on the hardest category of reCAPTCHA.Our evaluations on both tasks indicate that at specific operating thresholds,the performance of the proposed system is comparable to, and in some casesexceeds, that of human operators.
arxiv-5700-22 | Anytime Hierarchical Clustering | http://arxiv.org/pdf/1404.3439v1.pdf | author:Omur Arslan, Daniel E. Koditschek category:stat.ML cs.IR cs.LG H.3.3; I.5.3 published:2014-04-13 summary:We propose a new anytime hierarchical clustering method that iterativelytransforms an arbitrary initial hierarchy on the configuration of measurementsalong a sequence of trees we prove for a fixed data set must terminate in achain of nested partitions that satisfies a natural homogeneity requirement.Each recursive step re-edits the tree so as to improve a local measure ofcluster homogeneity that is compatible with a number of commonly used (e.g.,single, average, complete) linkage functions. As an alternative to the standardbatch algorithms, we present numerical evidence to suggest that appropriateadaptations of this method can yield decentralized, scalable algorithmssuitable for distributed/parallel computation of clustering hierarchies andonline tracking of clustering trees applicable to large, dynamically changingdatabases and anomaly detection.
arxiv-5700-23 | Active Learning for Undirected Graphical Model Selection | http://arxiv.org/pdf/1404.3418v1.pdf | author:Divyanshu Vats, Robert D. Nowak, Richard G. Baraniuk category:stat.ML cs.IT math.IT math.ST stat.TH published:2014-04-13 summary:This paper studies graphical model selection, i.e., the problem of estimatinga graph of statistical relationships among a collection of random variables.Conventional graphical model selection algorithms are passive, i.e., theyrequire all the measurements to have been collected before processing begins.We propose an active learning algorithm that uses junction tree representationsto adapt future measurements based on the information gathered from priormeasurements. We prove that, under certain conditions, our active learningalgorithm requires fewer scalar measurements than any passive algorithm toreliably estimate a graph. A range of numerical results validate our theory anddemonstrates the benefits of active learning.
arxiv-5700-24 | Second-order Shape Optimization for Geometric Inverse Problems in Vision | http://arxiv.org/pdf/1311.2626v5.pdf | author:J. Balzer, S. Soatto category:cs.CV published:2013-11-11 summary:We develop a method for optimization in shape spaces, i.e., sets of surfacesmodulo re-parametrization. Unlike previously proposed gradient flows, weachieve superlinear convergence rates through a subtle approximation of theshape Hessian, which is generally hard to compute and suffers from a series ofdegeneracies. Our analysis highlights the role of mean curvature motion incomparison with first-order schemes: instead of surface area, our approachpenalizes deformation, either by its Dirichlet energy or total variation.Latter regularizer sparks the development of an alternating direction method ofmultipliers on triangular meshes. Therein, a conjugate-gradients solver enablesus to bypass formation of the Gaussian normal equations appearing in the courseof the overall optimization. We combine all of the aforementioned ideas in aversatile geometric variation-regularized Levenberg-Marquardt-type methodapplicable to a variety of shape functionals, depending on intrinsic propertiesof the surface such as normal field and curvature as well as its embedding intospace. Promising experimental results are reported.
arxiv-5700-25 | A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing | http://arxiv.org/pdf/1404.3377v1.pdf | author:Rene Pickhardt, Thomas Gottron, Martin Körner, Paul Georg Wagner, Till Speicher, Steffen Staab category:cs.CL published:2014-04-13 summary:We introduce a novel approach for building language models based on asystematic, recursive exploration of skip n-gram models which are interpolatedusing modified Kneser-Ney smoothing. Our approach generalizes language modelsas it contains the classical interpolation with lower order models as a specialcase. In this paper we motivate, formalize and present our approach. In anextensive empirical experiment over English text corpora we demonstrate thatour generalized language models lead to a substantial reduction of perplexitybetween 3.1% and 12.7% in comparison to traditional language models usingmodified Kneser-Ney smoothing. Furthermore, we investigate the behaviour overthree other languages and a domain specific corpus where we observed consistentimprovements. Finally, we also show that the strength of our approach lies inits ability to cope in particular with sparse training data. Using a very smalltraining data set of only 736 KB text we yield improvements of even 25.7%reduction of perplexity.
arxiv-5700-26 | Bayes Merging of Multiple Vocabularies for Scalable Image Retrieval | http://arxiv.org/pdf/1403.0284v2.pdf | author:Liang Zheng, Shengjin Wang, Wengang Zhou, Qi Tian category:cs.CV published:2014-03-03 summary:The Bag-of-Words (BoW) representation is well applied to recentstate-of-the-art image retrieval works. Typically, multiple vocabularies aregenerated to correct quantization artifacts and improve recall. However, thisroutine is corrupted by vocabulary correlation, i.e., overlapping amongdifferent vocabularies. Vocabulary correlation leads to an over-counting of theindexed features in the overlapped area, or the intersection set, thuscompromising the retrieval accuracy. In order to address the correlationproblem while preserve the benefit of high recall, this paper proposes a Bayesmerging approach to down-weight the indexed features in the intersection set.Through explicitly modeling the correlation problem in a probabilistic view, ajoint similarity on both image- and feature-level is estimated for the indexedfeatures in the intersection set. We evaluate our method through extensive experiments on three benchmarkdatasets. Albeit simple, Bayes merging can be well applied in various mergingtasks, and consistently improves the baselines on multi-vocabulary merging.Moreover, Bayes merging is efficient in terms of both time and memory cost, andyields competitive performance compared with the state-of-the-art methods.
arxiv-5700-27 | Packing and Padding: Coupled Multi-index for Accurate Image Retrieval | http://arxiv.org/pdf/1402.2681v2.pdf | author:Liang Zheng, Shengjin Wang, Ziqiong Liu, Qi Tian category:cs.CV published:2014-02-11 summary:In Bag-of-Words (BoW) based image retrieval, the SIFT visual word has a lowdiscriminative power, so false positive matches occur prevalently. Apart fromthe information loss during quantization, another cause is that the SIFTfeature only describes the local gradient distribution. To address thisproblem, this paper proposes a coupled Multi-Index (c-MI) framework to performfeature fusion at indexing level. Basically, complementary features are coupledinto a multi-dimensional inverted index. Each dimension of c-MI corresponds toone kind of feature, and the retrieval process votes for images similar in bothSIFT and other feature spaces. Specifically, we exploit the fusion of localcolor feature into c-MI. While the precision of visual match is greatlyenhanced, we adopt Multiple Assignment to improve recall. The joint cooperationof SIFT and color features significantly reduces the impact of false positivematches. Extensive experiments on several benchmark datasets demonstrate that c-MIimproves the retrieval accuracy significantly, while consuming only half of thequery time compared to the baseline. Importantly, we show that c-MI is wellcomplementary to many prior techniques. Assembling these methods, we haveobtained an mAP of 85.8% and N-S score of 3.85 on Holidays and Ukbenchdatasets, respectively, which compare favorably with the state-of-the-arts.
arxiv-5700-28 | Estimating Time-varying Brain Connectivity Networks from Functional MRI Time Series | http://arxiv.org/pdf/1310.3863v2.pdf | author:Ricardo Pio Monti, Peter Hellyer, David Sharp, Robert Leech, Christoforos Anagnostopoulos, Giovanni Montana category:stat.ML stat.AP published:2013-10-14 summary:Understanding the functional architecture of the brain in terms of networksis becoming increasingly common. In most fMRI applications functional networksare assumed to be stationary, resulting in a single network estimated for theentire time course. However recent results suggest that the connectivitybetween brain regions is highly non-stationary even at rest. As a result, thereis a need for new brain imaging methodologies that comprehensively account forthe dynamic (i.e., non-stationary) nature of the fMRI data. In this work wepropose the Smooth Incremental Graphical Lasso Estimation (SINGLE) algorithmwhich estimates dynamic brain networks from fMRI data. We apply the SINGLEalgorithm to functional MRI data from 24 healthy patients performing achoice-response task to demonstrate the dynamic changes in network structurethat accompany a simple but attentionally demanding cognitive task. Using graphtheoretic measures we show that the Right Inferior Frontal Gyrus, frequentlyreported as playing an important role in cognitive control, dynamically changeswith the task. Our results suggest that the Right Inferior Frontal Gyrus playsa fundamental role in the attention and executive function during cognitivelydemanding tasks and may play a key role in regulating the balance between otherbrain regions.
arxiv-5700-29 | Shrinkage Optimized Directed Information using Pictorial Structures for Action Recognition | http://arxiv.org/pdf/1404.3312v1.pdf | author:Xu Chen, Alfred Hero, Silvio Savarese category:cs.CV published:2014-04-12 summary:In this paper, we propose a novel action recognition framework. The methoduses pictorial structures and shrinkage optimized directed informationassessment (SODA) coupled with Markov Random Fields called SODA+MRF to modelthe directional temporal dependency and bidirectional spatial dependency. As avariant of mutual information, directional information captures the directionalinformation flow and temporal structure of video sequences across frames.Meanwhile, within each frame, Markov random fields are utilized to model thespatial relations among different parts of a human body and the body parts ofdifferent people. The proposed SODA+MRF model is robust to view pointtransformations and detect complex interactions accurately. We compare theproposed method against several baseline methods to highlight the effectivenessof the SODA+MRF model. We demonstrate that our algorithm has superior actionrecognition performance on the UCF action recognition dataset, the Olympicsports dataset and the collective activity dataset over severalstate-of-the-art methods.
arxiv-5700-30 | Cost-Effective HITs for Relative Similarity Comparisons | http://arxiv.org/pdf/1404.3291v1.pdf | author:Michael J. Wilber, Iljung S. Kwak, Serge J. Belongie category:cs.CV cs.LG published:2014-04-12 summary:Similarity comparisons of the form "Is object a more similar to b than to c?"are useful for computer vision and machine learning applications.Unfortunately, an embedding of $n$ points is specified by $n^3$ triplets,making collecting every triplet an expensive task. In noticing this difficulty,other researchers have investigated more intelligent triplet samplingtechniques, but they do not study their effectiveness or their potentialdrawbacks. Although it is important to reduce the number of collected triplets,it is also important to understand how best to display a triplet collectiontask to a user. In this work we explore an alternative display for collectingtriplets and analyze the monetary cost and speed of the display. We proposebest practices for creating cost effective human intelligence tasks forcollecting triplets. We show that rather than changing the sampling algorithm,simple changes to the crowdsourcing UI can lead to much higher qualityembeddings. We also provide a dataset as well as the labels collected fromcrowd workers.
arxiv-5700-31 | Motion-Compensated Coding and Frame-Rate Up-Conversion: Models and Analysis | http://arxiv.org/pdf/1404.3290v1.pdf | author:Yehuda Dar, Alfred M. Bruckstein category:cs.MM cs.CV published:2014-04-12 summary:Block-based motion estimation (ME) and compensation (MC) techniques arewidely used in modern video processing algorithms and compression systems. Thegreat variety of video applications and devices results in numerous compressionspecifications. Specifically, there is a diversity of frame-rates andbit-rates. In this paper, we study the effect of frame-rate and compressionbit-rate on block-based ME and MC as commonly utilized in inter-frame codingand frame-rate up conversion (FRUC). This joint examination yields acomprehensive foundation for comparing MC procedures in coding and FRUC. First,the video signal is modeled as a noisy translational motion of an image. Then,we theoretically model the motion-compensated prediction of an available andabsent frames as in coding and FRUC applications, respectively. The theoreticMC-prediction error is further analyzed and its autocorrelation function iscalculated for coding and FRUC applications. We show a linear relation betweenthe variance of the MC-prediction error and temporal-distance. While theaffecting distance in MC-coding is between the predicted and reference frames,MC-FRUC is affected by the distance between the available frames used for theinterpolation. Moreover, the dependency in temporal-distance implies an inverseeffect of the frame-rate. FRUC performance analysis considers the predictionerror variance, since it equals to the mean-squared-error of the interpolation.However, MC-coding analysis requires the entire autocorrelation function of theerror; hence, analytic simplicity is beneficial. Therefore, we propose twoconstructions of a separable autocorrelation function for prediction error inMC-coding. We conclude by comparing our estimations with experimental results.
arxiv-5700-32 | Pagination: It's what you say, not how long it takes to say it | http://arxiv.org/pdf/1404.3233v1.pdf | author:Joshua Hailpern, Niranjan Damera Venkata, Marina Danilevsky category:cs.CL cs.IR I.7.2; I.7.4 published:2014-04-11 summary:Pagination - the process of determining where to break an article acrosspages in a multi-article layout is a common layout challenge for mostcommercially printed newspapers and magazines. To date, no one has created analgorithm that determines a minimal pagination break point based on the contentof the article. Existing approaches for automatic multi-article layout focusexclusively on maximizing content (number of articles) and optimizing aestheticpresentation (e.g., spacing between articles). However, disregarding thesemantic information within the article can lead to overly aggressive cutting,thereby eliminating key content and potentially confusing the reader, orsetting too generous of a break point, thereby leaving in superfluous contentand making automatic layout more difficult. This is one of the remainingchallenges on the path from manual layouts to fully automated processes thatstill ensure article content quality. In this work, we present a new approachto calculating a document minimal break point for the task of pagination. Ourapproach uses a statistical language model to predict minimal break pointsbased on the semantic content of an article. We then compare 4 novel candidateapproaches, and 4 baselines (currently in use by layout algorithms). Resultsfrom this experiment show that one of our approaches strongly outperforms thebaselines and alternatives. Results from a second study suggest that humans arenot able to agree on a single "best" break point. Therefore, this work showsthat a semantic-based lower bound break point prediction is necessary for idealautomated document synthesis within a real-world context.
arxiv-5700-33 | Estimating nonlinear regression errors without doing regression | http://arxiv.org/pdf/1404.3219v1.pdf | author:Hong Pi, Carsten Peterson category:stat.ML nlin.CD q-fin.ST published:2014-04-11 summary:A method for estimating nonlinear regression errors and their distributionswithout performing regression is presented. Assuming continuity of the modelingfunction the variance is given in terms of conditional probabilities extractedfrom the data. For N data points the computational demand is N2. Comparing thepredicted residual errors with those derived from a linear model assumptionprovides a signal for nonlinearity. The method is successfully illustrated withdata generated by the Ikeda and Lorenz maps augmented with noise. As aby-product the embedding dimensions of these maps are also extracted.
arxiv-5700-34 | Compressive classification and the rare eclipse problem | http://arxiv.org/pdf/1404.3203v1.pdf | author:Afonso S. Bandeira, Dustin G. Mixon, Benjamin Recht category:cs.LG cs.IT math.IT math.ST stat.TH published:2014-04-11 summary:This paper addresses the fundamental question of when convex sets remaindisjoint after random projection. We provide an analysis using ideas fromhigh-dimensional convex geometry. For ellipsoids, we provide a bound in termsof the distance between these ellipsoids and simple functions of theirpolynomial coefficients. As an application, this theorem provides bounds forcompressive classification of convex sets. Rather than assuming that the datato be classified is sparse, our results show that the data can be acquired viavery few measurements yet will remain linearly separable. We demonstrate thefeasibility of this approach in the context of hyperspectral imaging.
arxiv-5700-35 | Pareto-Path Multi-Task Multiple Kernel Learning | http://arxiv.org/pdf/1404.3190v1.pdf | author:Cong Li, Michael Georgiopoulos, Georgios C. Anagnostopoulos category:cs.LG published:2014-04-11 summary:A traditional and intuitively appealing Multi-Task Multiple Kernel Learning(MT-MKL) method is to optimize the sum (thus, the average) of objectivefunctions with (partially) shared kernel function, which allows informationsharing amongst tasks. We point out that the obtained solution corresponds to asingle point on the Pareto Front (PF) of a Multi-Objective Optimization (MOO)problem, which considers the concurrent optimization of all task objectivesinvolved in the Multi-Task Learning (MTL) problem. Motivated by this lastobservation and arguing that the former approach is heuristic, we propose anovel Support Vector Machine (SVM) MT-MKL framework, that considers animplicitly-defined set of conic combinations of task objectives. We show thatsolving our framework produces solutions along a path on the aforementioned PFand that it subsumes the optimization of the average of objective functions asa special case. Using algorithms we derived, we demonstrate through a series ofexperimental results that the framework is capable of achieving betterclassification performance, when compared to other similar MTL approaches.
arxiv-5700-36 | Decreasing Weighted Sorted $\ell_1$ Regularization | http://arxiv.org/pdf/1404.3184v1.pdf | author:Xiangrong Zeng, Mário A. T. Figueiredo category:cs.CV cs.IT cs.LG math.IT published:2014-04-11 summary:We consider a new family of regularizers, termed {\it weighted sorted$\ell_1$ norms} (WSL1), which generalizes the recently introduced {\itoctagonal shrinkage and clustering algorithm for regression} (OSCAR) and alsocontains the $\ell_1$ and $\ell_{\infty}$ norms as particular instances. Wefocus on a special case of the WSL1, the {\sl decreasing WSL1} (DWSL1), wherethe elements of the argument vector are sorted in non-increasing order and theweights are also non-increasing. In this paper, after showing that the DWSL1 isindeed a norm, we derive two key tools for its use as a regularizer: the dualnorm and the Moreau proximity operator.
arxiv-5700-37 | The Sampling-and-Learning Framework: A Statistical View of Evolutionary Algorithms | http://arxiv.org/pdf/1401.6333v2.pdf | author:Yang Yu, Hong Qian category:cs.NE cs.LG published:2014-01-24 summary:Evolutionary algorithms (EAs), a large class of general purpose optimizationalgorithms inspired from the natural phenomena, are widely used in variousindustrial optimizations and often show excellent performance. This paperpresents an attempt towards revealing their general power from a statisticalview of EAs. By summarizing a large range of EAs into the sampling-and-learningframework, we show that the framework directly admits a general analysis on theprobable-absolute-approximate (PAA) query complexity. We particularly focus onthe framework with the learning subroutine being restricted as a binaryclassification, which results in the sampling-and-classification (SAC)algorithms. With the help of the learning theory, we obtain a general upperbound on the PAA query complexity of SAC algorithms. We further compare SACalgorithms with the uniform search in different situations. Under theerror-target independence condition, we show that SAC algorithms can achievepolynomial speedup to the uniform search, but not super-polynomial speedup.Under the one-side-error condition, we show that super-polynomial speedup canbe achieved. This work only touches the surface of the framework. Its powerunder other conditions is still open.
arxiv-5700-38 | On the Ground Validation of Online Diagnosis with Twitter and Medical Records | http://arxiv.org/pdf/1404.3026v1.pdf | author:Todd Bodnar, Victoria C Barclay, Nilam Ram, Conrad S Tucker, Marcel Salathé category:cs.SI cs.CL cs.LG I.2.1 published:2014-04-11 summary:Social media has been considered as a data source for tracking disease.However, most analyses are based on models that prioritize strong correlationwith population-level disease rates over determining whether or not specificindividual users are actually sick. Taking a different approach, we develop anovel system for social-media based disease detection at the individual levelusing a sample of professionally diagnosed individuals. Specifically, wedevelop a system for making an accurate influenza diagnosis based on anindividual's publicly available Twitter data. We find that about half (17/35 =48.57%) of the users in our sample that were sick explicitly discuss theirdisease on Twitter. By developing a meta classifier that combines textanalysis, anomaly detection, and social network analysis, we are able todiagnose an individual with greater than 99% accuracy even if she does notdiscuss her health.
arxiv-5700-39 | A Reverse Hierarchy Model for Predicting Eye Fixations | http://arxiv.org/pdf/1404.2999v1.pdf | author:Tianlin Shi, Liang Ming, Xiaolin Hu category:cs.CV published:2014-04-11 summary:A number of psychological and physiological evidences suggest that earlyvisual attention works in a coarse-to-fine way, which lays a basis for thereverse hierarchy theory (RHT). This theory states that attention propagatesfrom the top level of the visual hierarchy that processes gist and abstractinformation of input, to the bottom level that processes local details.Inspired by the theory, we develop a computational model for saliency detectionin images. First, the original image is downsampled to different scales toconstitute a pyramid. Then, saliency on each layer is obtained by imagesuper-resolution reconstruction from the layer above, which is defined asunpredictability from this coarse-to-fine reconstruction. Finally, saliency oneach layer of the pyramid is fused into stochastic fixations through aprobabilistic model, where attention initiates from the top layer andpropagates downward through the pyramid. Extensive experiments on two standardeye-tracking datasets show that the proposed method can achieve competitiveresults with state-of-the-art models.
arxiv-5700-40 | Automatic Detection of Reuses and Citations in Literary Texts | http://arxiv.org/pdf/1404.2997v1.pdf | author:Jean-Gabriel Ganascia, Pierre Glaudes, Andrea Del Lungo category:cs.CL cs.DL published:2014-04-11 summary:For more than forty years now, modern theories of literature (Compagnon,1979) insist on the role of paraphrases, rewritings, citations, reciprocalborrowings and mutual contributions of any kinds. The notions ofintertextuality, transtextuality, hypertextuality/hypotextuality, wereintroduced in the seventies and eighties to approach these phenomena. Thecareful analysis of these references is of particular interest in evaluatingthe distance that the creator voluntarily introduces with his/her masters.Phoebus is collaborative project that makes computer scientists from theUniversity Pierre and Marie Curie (LIP6-UPMC) collaborate with the literaryteams of Paris-Sorbonne University with the aim to develop efficient tools forliterary studies that take advantage of modern computer science techniques. Inthis context, we have developed a piece of software that automatically detectsand explores networks of textual reuses in classical literature. This paperdescribes the principles on which is based this program, the significantresults that have already been obtained and the perspectives for the nearfuture.
arxiv-5700-41 | Supersparse Linear Integer Models for Interpretable Classification | http://arxiv.org/pdf/1306.6677v6.pdf | author:Berk Ustun, Stefano Tracà, Cynthia Rudin category:stat.ML stat.AP published:2013-06-27 summary:Scoring systems are classification models that only require users to add,subtract and multiply a few meaningful numbers to make a prediction. Thesemodels are often used because they are practical and interpretable. In thispaper, we introduce an off-the-shelf tool to create scoring systems that bothaccurate and interpretable, known as a Supersparse Linear Integer Model (SLIM).SLIM is a discrete optimization problem that minimizes the 0-1 loss toencourage a high level of accuracy, regularizes the L0-norm to encourage a highlevel of sparsity, and constrains coefficients to a set of interpretablevalues. We illustrate the practical and interpretable nature of SLIM scoringsystems through applications in medicine and criminology, and show that theyare are accurate and sparse in comparison to state-of-the-art classificationmodels using numerical experiments.
arxiv-5700-42 | A Tutorial on Independent Component Analysis | http://arxiv.org/pdf/1404.2986v1.pdf | author:Jonathon Shlens category:cs.LG stat.ML published:2014-04-11 summary:Independent component analysis (ICA) has become a standard data analysistechnique applied to an array of problems in signal processing and machinelearning. This tutorial provides an introduction to ICA based on linear algebraformulating an intuition for ICA from first principles. The goal of thistutorial is to provide a solid foundation on this advanced topic so that onemight learn the motivation behind ICA, learn why and when to apply thistechnique and in the process gain an introduction to this exciting field ofactive research.
arxiv-5700-43 | Targeting HIV-related Medication Side Effects and Sentiment Using Twitter Data | http://arxiv.org/pdf/1404.3610v1.pdf | author:Cosme Adrover, Todd Bodnar, Marcel Salathe category:cs.SI cs.CL cs.IR published:2014-04-11 summary:We present a descriptive analysis of Twitter data. Our study focuses onextracting the main side effects associated with HIV treatments. The crux ofour work was the identification of personal tweets referring to HIV. Wesummarize our results in an infographic aimed at the general public. Inaddition, we present a measure of user sentiment based on hand-rated tweets.
arxiv-5700-44 | Multi-agent Inverse Reinforcement Learning for Zero-sum Games | http://arxiv.org/pdf/1403.6508v2.pdf | author:Xiaomin Lin, Peter A. Beling, Randy Cogill category:cs.GT cs.AI cs.LG published:2014-03-25 summary:In this paper we introduce a Bayesian framework for solving a class ofproblems termed Multi-agent Inverse Reinforcement Learning (MIRL). Compared tothe well-known Inverse Reinforcement Learning (IRL) problem, MIRL is formalizedin the context of a stochastic game rather than a Markov decision process(MDP). Games bring two primary challenges: First, the concept of optimality,central to MDPs, loses its meaning and must be replaced with a more generalsolution concept, such as the Nash equilibrium. Second, the non-uniqueness ofequilibria means that in MIRL, in addition to multiple reasonable solutions fora given inversion model, there may be multiple inversion models that are allequally sensible approaches to solving the problem. We establish a theoreticalfoundation for competitive two-agent MIRL problems and propose a Bayesianoptimization algorithm to solve the problem. We focus on the case of two-personzero-sum stochastic games, developing a generative model for the likelihood ofunknown rewards of agents given observed game play assuming that the two agentsfollow a minimax bipolicy. As a numerical illustration, we apply our method inthe context of an abstract soccer game. For the soccer game, we investigaterelationships between the extent of prior information and the quality oflearned rewards. Results suggest that covariance structure is more importantthan mean value in reward priors.
arxiv-5700-45 | Gradient-based Laplacian Feature Selection | http://arxiv.org/pdf/1404.2948v1.pdf | author:Bo Wang, Anna Goldenberg category:cs.LG published:2014-04-10 summary:Analysis of high dimensional noisy data is of essence across a variety ofresearch fields. Feature selection techniques are designed to find the relevantfeature subset that can facilitate classification or pattern detection.Traditional (supervised) feature selection methods utilize label information toguide the identification of relevant feature subsets. In this paper, however,we consider the unsupervised feature selection problem. Without the labelinformation, it is particularly difficult to identify a small set of relevantfeatures due to the noisy nature of real-world data which corrupts theintrinsic structure of the data. Our Gradient-based Laplacian Feature Selection(GLFS) selects important features by minimizing the variance of the Laplacianregularized least squares regression model. With $\ell_1$ relaxation, GLFS canfind a sparse subset of features that is relevant to the Laplacian manifolds.Extensive experiments on simulated, three real-world object recognition and twocomputational biology datasets, have illustrated the power and superiorperformance of our approach over multiple state-of-the-art unsupervised featureselection methods. Additionally, we show that GLFS selects a sparser set ofmore relevant features in a supervised setting outperforming the popularelastic net methodology.
arxiv-5700-46 | Clustering and Relational Ambiguity: from Text Data to Natural Data | http://arxiv.org/pdf/1311.5401v2.pdf | author:Nicolas Turenne category:cs.CL cs.IR published:2013-11-21 summary:Text data is often seen as "take-away" materials with little noise and easyto process information. Main questions are how to get data and transform theminto a good document format. But data can be sensitive to noise oftenly calledambiguities. Ambiguities are aware from a long time, mainly because polysemy isobvious in language and context is required to remove uncertainty. I claim inthis paper that syntactic context is not suffisant to improve interpretation.In this paper I try to explain that firstly noise can come from natural datathemselves, even involving high technology, secondly texts, seen as verifiedbut meaningless, can spoil content of a corpus; it may lead to contradictionsand background noise.
arxiv-5700-47 | Overview of Stemming Algorithms for Indian and Non-Indian Languages | http://arxiv.org/pdf/1404.2878v1.pdf | author:Dalwadi Bijal, Suthar Sanket category:cs.CL published:2014-04-10 summary:Stemming is a pre-processing step in Text Mining applications as well as avery common requirement of Natural Language processing functions. Stemming isthe process for reducing inflected words to their stem. The main purpose ofstemming is to reduce different grammatical forms / word forms of a word likeits noun, adjective, verb, adverb etc. to its root form. Stemming is widelyuses in Information Retrieval system and reduces the size of index files. Wecan say that the goal of stemming is to reduce inflectional forms and sometimesderivationally related forms of a word to a common base form. In this paper wehave discussed different stemming algorithm for non-Indian and Indian language,methods of stemming, accuracy and errors.
arxiv-5700-48 | Auto-Encoding Variational Bayes | http://arxiv.org/pdf/1312.6114v10.pdf | author:Diederik P Kingma, Max Welling category:stat.ML cs.LG published:2013-12-20 summary:How can we perform efficient inference and learning in directed probabilisticmodels, in the presence of continuous latent variables with intractableposterior distributions, and large datasets? We introduce a stochasticvariational inference and learning algorithm that scales to large datasets and,under some mild differentiability conditions, even works in the intractablecase. Our contributions is two-fold. First, we show that a reparameterizationof the variational lower bound yields a lower bound estimator that can bestraightforwardly optimized using standard stochastic gradient methods. Second,we show that for i.i.d. datasets with continuous latent variables perdatapoint, posterior inference can be made especially efficient by fitting anapproximate inference model (also called a recognition model) to theintractable posterior using the proposed lower bound estimator. Theoreticaladvantages are reflected in experimental results.
arxiv-5700-49 | Real Time Speckle Image De-Noising | http://arxiv.org/pdf/1405.6166v1.pdf | author:D. Sachin Kumar, P. R. Seshadri, N. Vaishnav, Dr. Saraswathi Janaki category:cs.CV published:2014-04-10 summary:The paper presents real time speckle de-noising based on activity computationalgorithm and wavelet transform. Speckles arise in an image when laser light isreflected from an illuminated surface. The process involves detection ofspeckles in an image by obtaining a number of frames of the same object underdifferent illumination or angle and comparing the frames for the granularcomputation and de-noising the same on presence of greater activity index. Theproject can be implemented in FPGA (Field Programmable Gate Array) technology.The results can be shown that the used activity computation algorithm andwavelet transform has better accuracy in the process of speckle detection andde-noising.
arxiv-5700-50 | A New Clustering Approach for Anomaly Intrusion Detection | http://arxiv.org/pdf/1404.2772v1.pdf | author:Ravi Ranjan, G. Sahoo category:cs.DC cs.CR cs.LG published:2014-04-10 summary:Recent advances in technology have made our work easier compare to earliertimes. Computer network is growing day by day but while discussing about thesecurity of computers and networks it has always been a major concerns fororganizations varying from smaller to larger enterprises. It is true thatorganizations are aware of the possible threats and attacks so they alwaysprepare for the safer side but due to some loopholes attackers are able to makeattacks. Intrusion detection is one of the major fields of research andresearchers are trying to find new algorithms for detecting intrusions.Clustering techniques of data mining is an interested area of research fordetecting possible intrusions and attacks. This paper presents a new clusteringapproach for anomaly intrusion detection by using the approach of K-medoidsmethod of clustering and its certain modifications. The proposed algorithm isable to achieve high detection rate and overcomes the disadvantages of K-meansalgorithm.
arxiv-5700-51 | Computational Limits for Matrix Completion | http://arxiv.org/pdf/1402.2331v2.pdf | author:Moritz Hardt, Raghu Meka, Prasad Raghavendra, Benjamin Weitz category:cs.CC cs.LG published:2014-02-10 summary:Matrix Completion is the problem of recovering an unknown real-valuedlow-rank matrix from a subsample of its entries. Important recent results showthat the problem can be solved efficiently under the assumption that theunknown matrix is incoherent and the subsample is drawn uniformly at random.Are these assumptions necessary? It is well known that Matrix Completion in its full generality is NP-hard.However, little is known if make additional assumptions such as incoherence andpermit the algorithm to output a matrix of slightly higher rank. In this paperwe prove that Matrix Completion remains computationally intractable even if theunknown matrix has rank $4$ but we are allowed to output any constant rankmatrix, and even if additionally we assume that the unknown matrix isincoherent and are shown $90%$ of the entries. This result relies on theconjectured hardness of the $4$-Coloring problem. We also consider the positivesemidefinite Matrix Completion problem. Here we show a similar hardness resultunder the standard assumption that $\mathrm{P}\ne \mathrm{NP}.$ Our results greatly narrow the gap between existing feasibility results andcomputational lower bounds. In particular, we believe that our results give thefirst complexity-theoretic justification for why distributional assumptions areneeded beyond the incoherence assumption in order to obtain positive results.On the technical side, we contribute several new ideas on how to encode hardcombinatorial problems in low-rank optimization problems. We hope that thesetechniques will be helpful in further understanding the computational limits ofMatrix Completion and related problems.
arxiv-5700-52 | Open problem: Tightness of maximum likelihood semidefinite relaxations | http://arxiv.org/pdf/1404.2655v1.pdf | author:Afonso S. Bandeira, Yuehaw Khoo, Amit Singer category:math.OC cs.LG stat.ML published:2014-04-10 summary:We have observed an interesting, yet unexplained, phenomenon: Semidefiniteprogramming (SDP) based relaxations of maximum likelihood estimators (MLE) tendto be tight in recovery problems with noisy data, even when MLE cannot exactlyrecover the ground truth. Several results establish tightness of SDP basedrelaxations in the regime where exact recovery from MLE is possible. However,to the best of our knowledge, their tightness is not understood beyond thisregime. As an illustrative example, we focus on the generalized Procrustesproblem.
arxiv-5700-53 | Joint optimization of fitting & matching in multi-view reconstruction | http://arxiv.org/pdf/1303.2607v2.pdf | author:Hossam Isack, Yuri Boykov category:cs.CV published:2013-03-11 summary:Many standard approaches for geometric model fitting are based on pre-matchedimage features. Typically, such pre-matching uses only feature appearances(e.g. SIFT) and a large number of non-unique features must be discarded inorder to control the false positive rate. In contrast, we solve featurematching and multi-model fitting problems in a joint optimization framework.This paper proposes several fit-&-match energy formulations based on ageneralization of the assignment problem. We developed an efficient solverbased on min-cost-max-flow algorithm that finds near optimal solutions. Ourapproach significantly increases the number of detected matches. In practice,energy-based joint fitting & matching allows to increase the distance betweenview-points previously restricted by robustness of local SIFT-matching and toimprove the model fitting accuracy when compared to state-of-the-artmulti-model fitting techniques.
arxiv-5700-54 | Modeling Radiometric Uncertainty for Vision with Tone-mapped Color Images | http://arxiv.org/pdf/1311.6887v2.pdf | author:Ayan Chakrabarti, Ying Xiong, Baochen Sun, Trevor Darrell, Daniel Scharstein, Todd Zickler, Kate Saenko category:cs.CV published:2013-11-27 summary:To produce images that are suitable for display, tone-mapping is widely usedin digital cameras to map linear color measurements into narrow gamuts withlimited dynamic range. This introduces non-linear distortion that must beundone, through a radiometric calibration process, before computer visionsystems can analyze such photographs radiometrically. This paper considers theinherent uncertainty of undoing the effects of tone-mapping. We observe thatthis uncertainty varies substantially across color space, making some pixelsmore reliable than others. We introduce a model for this uncertainty and amethod for fitting it to a given camera or imaging pipeline. Once fit, themodel provides for each pixel in a tone-mapped digital photograph a probabilitydistribution over linear scene colors that could have induced it. Wedemonstrate how these distributions can be useful for visual inference byincorporating them into estimation algorithms for a representative set ofvision tasks.
arxiv-5700-55 | RANCOR: Non-Linear Image Registration with Total Variation Regularization | http://arxiv.org/pdf/1404.2571v1.pdf | author:Martin Rajchl, John S. H. Baxter, Wu Qiu, Ali R. Khan, Aaron Fenster, Terry M. Peters, Jing Yuan category:cs.CV published:2014-04-09 summary:Optimization techniques have been widely used in deformable registration,allowing for the incorporation of similarity metrics with regularizationmechanisms. These regularization mechanisms are designed to mitigate theeffects of trivial solutions to ill-posed registration problems and tootherwise ensure the resulting deformation fields are well-behaved. This paperintroduces a novel deformable registration algorithm, RANCOR, which usesiterative convexification to address deformable registration problems undertotal-variation regularization. Initial comparative results against fourstate-of-the-art registration algorithms are presented using the Internet BrainSegmentation Repository (IBSR) database.
arxiv-5700-56 | Noisy Optimization: Convergence with a Fixed Number of Resamplings | http://arxiv.org/pdf/1404.2553v1.pdf | author:Marie-Liesse Cauwet category:math.OC stat.ML published:2014-04-09 summary:It is known that evolution strategies in continuous domains might notconverge in the presence of noise. It is also known that, under mildassumptions, and using an increasing number of resamplings, one can mitigatethe effect of additive noise and recover convergence. We show new sufficientconditions for the convergence of an evolutionary algorithm with constantnumber of resamplings; in particular, we get fast rates (log-linearconvergence) provided that the variance decreases around the optimum slightlyfaster than in the so-called multiplicative noise model. Keywords: Noisyoptimization, evolutionary algorithm, theory.
arxiv-5700-57 | A Compact Linear Programming Relaxation for Binary Sub-modular MRF | http://arxiv.org/pdf/1404.2268v1.pdf | author:Junyan Wang, Sai-Kit Yeung category:cs.CV published:2014-04-09 summary:We propose a novel compact linear programming (LP) relaxation for binarysub-modular MRF in the context of object segmentation. Our model is obtained bylinearizing an $l_1^+$-norm derived from the quadratic programming (QP) form ofthe MRF energy. The resultant LP model contains significantly fewer variablesand constraints compared to the conventional LP relaxation of the MRF energy.In addition, unlike QP which can produce ambiguous labels, our model can beviewed as a quasi-total-variation minimization problem, and it can thereforepreserve the discontinuities in the labels. We further establish a relaxationbound between our LP model and the conventional LP model. In the experiments,we demonstrate our method for the task of interactive object segmentation. OurLP model outperforms QP when converting the continuous labels to binary labelsusing different threshold values on the entire Oxford interactive segmentationdataset. The computational complexity of our LP is of the same order as that ofthe QP, and it is significantly lower than the conventional LP relaxation.
arxiv-5700-58 | Spectral Correlation Hub Screening of Multivariate Time Series | http://arxiv.org/pdf/1403.3371v2.pdf | author:Hamed Firouzi, Dennis Wei, Alfred O. Hero III category:stat.OT cs.LG stat.AP published:2014-03-13 summary:This chapter discusses correlation analysis of stationary multivariateGaussian time series in the spectral or Fourier domain. The goal is to identifythe hub time series, i.e., those that are highly correlated with a specifiednumber of other time series. We show that Fourier components of the time seriesat different frequencies are asymptotically statistically independent. Thisproperty permits independent correlation analysis at each frequency,alleviating the computational and statistical challenges of high-dimensionaltime series. To detect correlation hubs at each frequency, an existingcorrelation screening method is extended to the complex numbers to accommodatecomplex-valued Fourier components. We characterize the number of hubdiscoveries at specified correlation and degree thresholds in the regime ofincreasing dimension and fixed sample size. The theory specifies appropriatethresholds to apply to sample correlation matrices to detect hubs and alsoallows statistical significance to be attributed to hub discoveries. Numericalresults illustrate the accuracy of the theory and the usefulness of theproposed spectral framework.
arxiv-5700-59 | A comparison of linear and non-linear calibrations for speaker recognition | http://arxiv.org/pdf/1402.2447v2.pdf | author:Niko Brümmer, Albert Swart, David van Leeuwen category:stat.ML cs.LG published:2014-02-11 summary:In recent work on both generative and discriminative score tolog-likelihood-ratio calibration, it was shown that linear transforms give goodaccuracy only for a limited range of operating points. Moreover, these methodsrequired tailoring of the calibration training objective functions in order totarget the desired region of best accuracy. Here, we generalize the linearrecipes to non-linear ones. We experiment with a non-linear, non-parametric,discriminative PAV solution, as well as parametric, generative,maximum-likelihood solutions that use Gaussian, Student's T andnormal-inverse-Gaussian score distributions. Experiments on NIST SRE'12 scoressuggest that the non-linear methods provide wider ranges of optimal accuracyand can be trained without having to resort to objective function tailoring.
arxiv-5700-60 | Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error | http://arxiv.org/pdf/1312.6062v2.pdf | author:David Buchaca, Enrique Romero, Ferran Mazzanti, Jordi Delgado category:cs.LG published:2013-12-20 summary:Restricted Boltzmann Machines (RBMs) are general unsupervised learningdevices to ascertain generative models of data distributions. RBMs are oftentrained using the Contrastive Divergence learning algorithm (CD), anapproximation to the gradient of the data log-likelihood. A simplereconstruction error is often used to decide whether the approximation providedby the CD algorithm is good enough, though several authors (Schulz et al.,2010; Fischer & Igel, 2010) have raised doubts concerning the feasibility ofthis procedure. However, not many alternatives to the reconstruction error havebeen used in the literature. In this manuscript we investigate simplealternatives to the reconstruction error in order to detect as soon as possiblethe decrease in the log-likelihood during learning.
arxiv-5700-61 | A Dynamic Near-Optimal Algorithm for Online Linear Programming | http://arxiv.org/pdf/0911.2974v3.pdf | author:Shipra Agrawal, Zizhuo Wang, Yinyu Ye category:cs.DS cs.LG published:2009-11-16 summary:A natural optimization model that formulates many online resource allocationand revenue management problems is the online linear program (LP) in which theconstraint matrix is revealed column by column along with the correspondingobjective coefficient. In such a model, a decision variable has to be set eachtime a column is revealed without observing the future inputs and the goal isto maximize the overall objective function. In this paper, we provide anear-optimal algorithm for this general class of online problems under theassumption of random order of arrival and some mild conditions on the size ofthe LP right-hand-side input. Specifically, our learning-based algorithm worksby dynamically updating a threshold price vector at geometric time intervals,where the dual prices learned from the revealed columns in the previous periodare used to determine the sequential decisions in the current period. Due tothe feature of dynamic learning, the competitiveness of our algorithm improvesover the past study of the same problem. We also present a worst-case exampleshowing that the performance of our algorithm is near-optimal.
arxiv-5700-62 | Power System Parameters Forecasting Using Hilbert-Huang Transform and Machine Learning | http://arxiv.org/pdf/1404.2353v1.pdf | author:Victor Kurbatsky, Nikita Tomin, Vadim Spiryaev, Paul Leahy, Denis Sidorov, Alexei Zhukov category:cs.LG stat.ML 62M10, 91B84 published:2014-04-09 summary:A novel hybrid data-driven approach is developed for forecasting power systemparameters with the goal of increasing the efficiency of short-term forecastingstudies for non-stationary time-series. The proposed approach is based on modedecomposition and a feature analysis of initial retrospective data using theHilbert-Huang transform and machine learning algorithms. The random forests andgradient boosting trees learning techniques were examined. The decision treetechniques were used to rank the importance of variables employed in theforecasting models. The Mean Decrease Gini index is employed as an impurityfunction. The resulting hybrid forecasting models employ the radial basisfunction neural network and support vector regression. Apart from introductionand references the paper is organized as follows. The section 2 presents thebackground and the review of several approaches for short-term forecasting ofpower system parameters. In the third section a hybrid machine learning-basedalgorithm using Hilbert-Huang transform is developed for short-term forecastingof power system parameters. Fourth section describes the decision tree learningalgorithms used for the issue of variables importance. Finally in section sixthe experimental results in the following electric power problems arepresented: active power flow forecasting, electricity price forecasting and forthe wind speed and direction forecasting.
arxiv-5700-63 | A Networks and Machine Learning Approach to Determine the Best College Coaches of the 20th-21st Centuries | http://arxiv.org/pdf/1404.2885v1.pdf | author:Tian-Shun Jiang, Zachary Polizzi, Christopher Yuan category:stat.AP cs.LG cs.SI published:2014-04-08 summary:Our objective is to find the five best college sports coaches of past centuryfor three different sports. We decided to look at men's basketball, football,and baseball. We wanted to use an approach that could definitively determineteam skill from the games played, and then use a machine-learning algorithm tocalculate the correct coach skills for each team in a given year. We created anetworks-based model to calculate team skill from historical game data. Adigraph was created for each year in each sport. Nodes represented teams, andedges represented a game played between two teams. The arrowhead pointedtowards the losing team. We calculated the team skill of each graph using aright-hand eigenvector centrality measure. This way, teams that beat good teamswill be ranked higher than teams that beat mediocre teams. The eigenvectorcentrality rankings for most years were well correlated with tournamentperformance and poll-based rankings. We assumed that the relationship betweencoach skill $C_s$, player skill $P_s$, and team skill $T_s$ was $C_s \cdot P_s= T_s$. We then created a function to describe the probability that a givenscore difference would occur based on player skill and coach skill. Wemultiplied the probabilities of all edges in the network together to find theprobability that the correct network would occur with any given player skilland coach skill matrix. We was able to determine player skill as a function ofteam skill and coach skill, eliminating the need to optimize two unknownmatrices. The top five coaches in each year were noted, and the top coach ofall time was calculated by dividing the number of times that coach ranked inthe yearly top five by the years said coach had been active.
arxiv-5700-64 | Algorithms for Learning Kernels Based on Centered Alignment | http://arxiv.org/pdf/1203.0550v2.pdf | author:Corinna Cortes, Mehryar Mohri, Afshin Rostamizadeh category:cs.LG cs.AI published:2012-03-02 summary:This paper presents new and effective algorithms for learning kernels. Inparticular, as shown by our empirical results, these algorithms consistentlyoutperform the so-called uniform combination solution that has proven to bedifficult to improve upon in the past, as well as other algorithms for learningkernels based on convex combinations of base kernels in both classification andregression. Our algorithms are based on the notion of centered alignment whichis used as a similarity measure between kernels or kernel matrices. We presenta number of novel algorithmic, theoretical, and empirical results for learningkernels based on our notion of centered alignment. In particular, we describeefficient algorithms for learning a maximum alignment kernel by showing thatthe problem can be reduced to a simple QP and discuss a one-stage algorithm forlearning both a kernel and a hypothesis based on that kernel using analignment-based regularization. Our theoretical results include a novelconcentration bound for centered alignment between kernel matrices, the proofof the existence of effective predictors for kernels with high alignment, bothfor classification and for regression, and the proof of stability-basedgeneralization bounds for a broad family of algorithms for learning kernelsbased on centered alignment. We also report the results of experiments with ourcentered alignment-based algorithms in both classification and regression.
arxiv-5700-65 | Data mining for censored time-to-event data: A Bayesian network model for predicting cardiovascular risk from electronic health record data | http://arxiv.org/pdf/1404.2189v1.pdf | author:Sunayan Bandyopadhyay, Julian Wolfson, David M. Vock, Gabriela Vazquez-Benitez, Gediminas Adomavicius, Mohamed Elidrisi, Paul E. Johnson, Patrick J. O'Connor category:stat.ML stat.AP published:2014-04-08 summary:Models for predicting the risk of cardiovascular events based on individualpatient characteristics are important tools for managing patient care. Mostcurrent and commonly used risk prediction models have been built from carefullyselected epidemiological cohorts. However, the homogeneity and limited size ofsuch cohorts restricts the predictive power and generalizability of these riskmodels to other populations. Electronic health data (EHD) from large healthcare systems provide access to data on large, heterogeneous, andcontemporaneous patient populations. The unique features and challenges of EHD,including missing risk factor information, non-linear relationships betweenrisk factors and cardiovascular event outcomes, and differing effects fromdifferent patient subgroups, demand novel machine learning approaches to riskmodel development. In this paper, we present a machine learning approach basedon Bayesian networks trained on EHD to predict the probability of having acardiovascular event within five years. In such data, event status may beunknown for some individuals as the event time is right-censored due todisenrollment and incomplete follow-up. Since many traditional data miningmethods are not well-suited for such data, we describe how to modify bothmodelling and assessment techniques to account for censored observation times.We show that our approach can lead to better predictive performance than theCox proportional hazards model (i.e., a regression-based approach commonly usedfor censored, time-to-event data) or a Bayesian network with {\em{ad hoc}}approaches to right-censoring. Our techniques are motivated by and illustratedon data from a large U.S. Midwestern health care system.
arxiv-5700-66 | A Convolutional Neural Network for Modelling Sentences | http://arxiv.org/pdf/1404.2188v1.pdf | author:Nal Kalchbrenner, Edward Grefenstette, Phil Blunsom category:cs.CL published:2014-04-08 summary:The ability to accurately represent sentences is central to languageunderstanding. We describe a convolutional architecture dubbed the DynamicConvolutional Neural Network (DCNN) that we adopt for the semantic modelling ofsentences. The network uses Dynamic k-Max Pooling, a global pooling operationover linear sequences. The network handles input sentences of varying lengthand induces a feature graph over the sentence that is capable of explicitlycapturing short and long-range relations. The network does not rely on a parsetree and is easily applicable to any language. We test the DCNN in fourexperiments: small scale binary and multi-class sentiment prediction, six-wayquestion classification and Twitter sentiment prediction by distantsupervision. The network achieves excellent performance in the first threetasks and a greater than 25% error reduction in the last task with respect tothe strongest baseline.
arxiv-5700-67 | Minimum $n$-Rank Approximation via Iterative Hard Thresholding | http://arxiv.org/pdf/1311.4291v2.pdf | author:Min Zhang, Lei Yang, Zheng-Hai Huang category:math.OC stat.ML published:2013-11-18 summary:The problem of recovering a low $n$-rank tensor is an extension of sparserecovery problem from the low dimensional space (matrix space) to the highdimensional space (tensor space) and has many applications in computer visionand graphics such as image inpainting and video inpainting. In this paper, weconsider a new tensor recovery model, named as minimum $n$-rank approximation(MnRA), and propose an appropriate iterative hard thresholding algorithm withgiving the upper bound of the $n$-rank in advance. The convergence analysis ofthe proposed algorithm is also presented. Particularly, we show that for thenoiseless case, the linear convergence with rate $\frac{1}{2}$ can be obtainedfor the proposed algorithm under proper conditions. Additionally, combining aneffective heuristic for determining $n$-rank, we can also apply the proposedalgorithm to solve MnRA when $n$-rank is unknown in advance. Some preliminarynumerical results on randomly generated and real low $n$-rank tensor completionproblems are reported, which show the efficiency of the proposed algorithms.
arxiv-5700-68 | A Naive Bayes machine learning approach to risk prediction using censored, time-to-event data | http://arxiv.org/pdf/1404.2124v1.pdf | author:Julian Wolfson, Sunayan Bandyopadhyay, Mohamed Elidrisi, Gabriela Vazquez-Benitez, Donald Musgrove, Gediminas Adomavicius, Paul Johnson, Patrick O'Connor category:stat.ML published:2014-04-08 summary:Predicting an individual's risk of experiencing a future clinical outcome isa statistical task with important consequences for both practicing cliniciansand public health experts. Modern observational databases such as electronichealth records (EHRs) provide an alternative to the longitudinal cohort studiestraditionally used to construct risk models, bringing with them bothopportunities and challenges. Large sample sizes and detailed covariatehistories enable the use of sophisticated machine learning techniques touncover complex associations and interactions, but observational databases areoften ``messy,'' with high levels of missing data and incomplete patientfollow-up. In this paper, we propose an adaptation of the well-known NaiveBayes (NB) machine learning approach for classification to time-to-eventoutcomes subject to censoring. We compare the predictive performance of ourmethod to the Cox proportional hazards model which is commonly used for riskprediction in healthcare populations, and illustrate its application toprediction of cardiovascular risk using an EHR dataset from a large Midwestintegrated healthcare system.
arxiv-5700-69 | Efficiency of conformalized ridge regression | http://arxiv.org/pdf/1404.2083v1.pdf | author:Evgeny Burnaev, Vladimir Vovk category:cs.LG stat.ML published:2014-04-08 summary:Conformal prediction is a method of producing prediction sets that can beapplied on top of a wide range of prediction algorithms. The method has aguaranteed coverage probability under the standard IID assumption regardless ofwhether the assumptions (often considerably more restrictive) of the underlyingalgorithm are satisfied. However, for the method to be really useful it isdesirable that in the case where the assumptions of the underlying algorithmare satisfied, the conformal predictor loses little in efficiency as comparedwith the underlying algorithm (whereas being a conformal predictor, it has thestronger guarantee of validity). In this paper we explore the degree to whichthis additional requirement of efficiency is satisfied in the case of Bayesianridge regression; we find that asymptotically conformal prediction sets differlittle from ridge regression prediction intervals when the standard Bayesianassumptions are satisfied.
arxiv-5700-70 | Extracting a bilingual semantic grammar from FrameNet-annotated corpora | http://arxiv.org/pdf/1404.2071v1.pdf | author:Dana Dannélls, Normunds Grūzītis category:cs.CL published:2014-04-08 summary:We present the creation of an English-Swedish FrameNet-based grammar inGrammatical Framework. The aim of this research is to make existing framenetscomputationally accessible for multilingual natural language applications via acommon semantic grammar API, and to facilitate the porting of such grammar toother languages. In this paper, we describe the abstract syntax of the semanticgrammar while focusing on its automatic extraction possibilities. We haveextracted a shared abstract syntax from ~58,500 annotated sentences in BerkeleyFrameNet (BFN) and ~3,500 annotated sentences in Swedish FrameNet (SweFN). Theabstract syntax defines 769 frame-specific valence patterns that cover 77.8%examples in BFN and 74.9% in SweFN belonging to the shared set of 471 frames.As a side result, we provide a unified method for comparing semantic andsyntactic valence patterns across framenets.
arxiv-5700-71 | Entropy Computation of Document Images in Run-Length Compressed Domain | http://arxiv.org/pdf/1404.2014v1.pdf | author:P. Nagabhushan, Mohammed Javed, B. B. Chaudhuri category:cs.CV published:2014-04-08 summary:Compression of documents, images, audios and videos have been traditionallypracticed to increase the efficiency of data storage and transfer. However, inorder to process or carry out any analytical computations, decompression hasbecome an unavoidable pre-requisite. In this research work, we have attemptedto compute the entropy, which is an important document analytic directly fromthe compressed documents. We use Conventional Entropy Quantifier (CEQ) andSpatial Entropy Quantifiers (SEQ) for entropy computations [1]. The entropiesobtained are useful in applications like establishing equivalence, wordspotting and document retrieval. Experiments have been performed with all thedata sets of [1], at character, word and line levels taking compresseddocuments in run-length compressed domain. The algorithms developed arecomputational and space efficient, and results obtained match 100% with theresults reported in [1].
arxiv-5700-72 | A Permutation Approach for Selecting the Penalty Parameter in Penalized Model Selection | http://arxiv.org/pdf/1404.2007v1.pdf | author:Jeremy Sabourin, William Valdar, Andrew Nobel category:stat.ML published:2014-04-08 summary:We describe a simple, efficient, permutation based procedure for selectingthe penalty parameter in the LASSO. The procedure, which is intended forapplications where variable selection is the primary focus, can be applied in avariety of structural settings, including generalized linear models. We brieflydiscuss connections between permutation selection and existing theory for theLASSO. In addition, we present a simulation study and an analysis of three realdata sets in which permutation selection is compared with cross-validation(CV), the Bayesian information criterion (BIC), and a selection method based onrecently developed testing procedures for the LASSO.
arxiv-5700-73 | Automatic Tracker Selection w.r.t Object Detection Performance | http://arxiv.org/pdf/1404.2005v1.pdf | author:Duc Phu Chau, François Bremond, Monique Thonnat, Slawomir Bak category:cs.CV published:2014-04-08 summary:The tracking algorithm performance depends on video content. This paperpresents a new multi-object tracking approach which is able to cope with videocontent variations. First the object detection is improved using Kanade-Lucas-Tomasi (KLT) feature tracking. Second, for each mobile object, anappropriate tracker is selected among a KLT-based tracker and a discriminativeappearance-based tracker. This selection is supported by an online trackingevaluation. The approach has been experimented on three public video datasets.The experimental results show a better performance of the proposed approachcompared to recent state of the art trackers.
arxiv-5700-74 | Notes on Generalized Linear Models of Neurons | http://arxiv.org/pdf/1404.1999v1.pdf | author:Jonathon Shlens category:cs.NE cs.LG q-bio.NC published:2014-04-08 summary:Experimental neuroscience increasingly requires tractable models foranalyzing and predicting the behavior of neurons and networks. The generalizedlinear model (GLM) is an increasingly popular statistical framework foranalyzing neural data that is flexible, exhibits rich dynamic behavior and iscomputationally tractable (Paninski, 2004; Pillow et al., 2008; Truccolo etal., 2005). What follows is a brief summary of the primary equations governingthe application of GLM's to spike trains with a few sentences linking this workto the larger statistical literature. Latter sections include extensions of abasic GLM to model spatio-temporal receptive fields as well as network activityin an arbitrary numbers of neurons.
arxiv-5700-75 | Human Face as human single identity | http://arxiv.org/pdf/1405.6168v1.pdf | author:Spits Warnars category:cs.CV published:2014-04-08 summary:Human face as a physical human recognition can be used as a unique identityfor computer to recognize human by transforming human face with face algorithmas simple text number which can be primary key for human. Human face as singleidentity for human will be done by making a huge and large world centre humanface database, where the human face around the world will be recorded from timeto time and from generation to generation. Architecture database will bedivided become human face image database which will save human face images andhuman face output code which will save human face output code as atransformation human face image with face algorithm. As an improvement theslightly and simple human face output code database will make human facesearching process become more fast. Transaction with human face as atransaction without card can make human no need their card for the transactionand office automation and banking system as an example for implementationarchitecture. As an addition suspect human face database can be extended forfighting crime and terrorism by doing surveillance and searching suspect humanface around the world.
arxiv-5700-76 | A New Approach to Speeding Up Topic Modeling | http://arxiv.org/pdf/1204.0170v2.pdf | author:Jia Zeng, Zhi-Qiang Liu, Xiao-Qin Cao category:cs.LG cs.IR published:2012-04-01 summary:Latent Dirichlet allocation (LDA) is a widely-used probabilistic topicmodeling paradigm, and recently finds many applications in computer vision andcomputational biology. In this paper, we propose a fast and accurate batchalgorithm, active belief propagation (ABP), for training LDA. Usually batch LDAalgorithms require repeated scanning of the entire corpus and searching thecomplete topic space. To process massive corpora having a large number oftopics, the training iteration of batch LDA algorithms is often inefficient andtime-consuming. To accelerate the training speed, ABP actively scans the subsetof corpus and searches the subset of topic space for topic modeling, thereforesaves enormous training time in each iteration. To ensure accuracy, ABP selectsonly those documents and topics that contribute to the largest residuals withinthe residual belief propagation (RBP) framework. On four real-world corpora,ABP performs around $10$ to $100$ times faster than state-of-the-art batch LDAalgorithms with a comparable topic modeling accuracy.
arxiv-5700-77 | Aspect-Based Opinion Extraction from Customer reviews | http://arxiv.org/pdf/1404.1982v1.pdf | author:Amani K Samha, Yuefeng Li, Jinglan Zhang category:cs.CL cs.IR published:2014-04-08 summary:Text is the main method of communicating information in the digital age.Messages, blogs, news articles, reviews, and opinionated information abound onthe Internet. People commonly purchase products online and post their opinionsabout purchased items. This feedback is displayed publicly to assist otherswith their purchasing decisions, creating the need for a mechanism with whichto extract and summarize useful information for enhancing the decision-makingprocess. Our contribution is to improve the accuracy of extraction by combiningdifferent techniques from three major areas, named Data Mining, NaturalLanguage Processing techniques and Ontologies. The proposed frameworksequentially mines products aspects and users opinions, groups representativeaspects by similarity, and generates an output summary. This paper focuses onthe task of extracting product aspects and users opinions by extracting allpossible aspects and opinions from reviews using natural language, ontology,and frequent (tag) sets. The proposed framework, when compared with an existingbaseline model, yielded promising results.
arxiv-5700-78 | Coupled Item-based Matrix Factorization | http://arxiv.org/pdf/1405.6223v1.pdf | author:Fangfang Li, Guandong Xu, Longbing Cao category:cs.LG cs.IR published:2014-04-08 summary:The essence of the challenges cold start and sparsity in Recommender Systems(RS) is that the extant techniques, such as Collaborative Filtering (CF) andMatrix Factorization (MF), mainly rely on the user-item rating matrix, whichsometimes is not informative enough for predicting recommendations. To solvethese challenges, the objective item attributes are incorporated ascomplementary information. However, most of the existing methods for inferringthe relationships between items assume that the attributes are "independentlyand identically distributed (iid)", which does not always hold in reality. Infact, the attributes are more or less coupled with each other by some implicitrelationships. Therefore, in this pa-per we propose an attribute-based coupledsimilarity measure to capture the implicit relationships between items. We thenintegrate the implicit item coupling into MF to form the Coupled Item-basedMatrix Factorization (CIMF) model. Experimental results on two open data setsdemonstrate that CIMF outperforms the benchmark methods.
arxiv-5700-79 | Polish and English wordnets -- statistical analysis of interconnected networks | http://arxiv.org/pdf/1404.1890v1.pdf | author:Maksymilian Bujok, Piotr Fronczak, Agata Fronczak category:cs.CL physics.soc-ph published:2014-04-07 summary:Wordnets are semantic networks containing nouns, verbs, adjectives, andadverbs organized according to linguistic principles, by means of semanticrelations. In this work, we adopt a complex network perspective to perform acomparative analysis of the English and Polish wordnets. We determine theirsimilarities and show that the networks exhibit some of the typicalcharacteristics observed in other real-world networks. We analyse interlingualrelations between both wordnets and deliberate over the problem of mapping thePolish lexicon onto the English one.
arxiv-5700-80 | From Shading to Local Shape | http://arxiv.org/pdf/1310.2916v2.pdf | author:Ying Xiong, Ayan Chakrabarti, Ronen Basri, Steven J. Gortler, David W. Jacobs, Todd Zickler category:cs.CV published:2013-10-10 summary:We develop a framework for extracting a concise representation of the shapeinformation available from diffuse shading in a small image patch. Thisproduces a mid-level scene descriptor, comprised of local shape distributionsthat are inferred separately at every image patch across multiple scales. Theframework is based on a quadratic representation of local shape that, in theabsence of noise, has guarantees on recovering accurate local shape andlighting. And when noise is present, the inferred local shape distributionsprovide useful shape information without over-committing to any particularimage explanation. These local shape distributions naturally encode the factthat some smooth diffuse regions are more informative than others, and theyenable efficient and robust reconstruction of object-scale shape. Experimentalresults show that this approach to surface reconstruction compares well againstthe state-of-art on both synthetic images and captured photographs.
arxiv-5700-81 | A Hierarchical Graphical Model for Big Inverse Covariance Estimation with an Application to fMRI | http://arxiv.org/pdf/1403.4698v2.pdf | author:Xi Luo category:stat.ME stat.AP stat.ML published:2014-03-19 summary:Brain networks has attracted the interests of many neuroscientists. Fromfunctional MRI (fMRI) data, statistical tools have been developed to recoverbrain networks. However, the dimensionality of whole-brain fMRI, usually inhundreds of thousands, challenges the applicability of these methods. Wedevelop a hierarchical graphical model (HGM) to remediate this difficulty. Thismodel introduces a hidden layer of networks based on sparse Gaussian graphicalmodels, and the observed data are sampled from individual network nodes. InfMRI, the network layer models the underlying signals of different brainfunctional units, and how these units directly interact with each other. Theintroduction of this hierarchical structure not only provides a formal andinterpretable approach, but also enables efficient computation for inferringbig networks with hundreds of thousands of nodes. Based on the conditionalconvexity of our formulation, we develop an alternating update algorithm tocompute the HGM model parameters simultaneously. The effectiveness of thisapproach is demonstrated on simulated data and a real dataset from a stop/gofMRI experiment.
arxiv-5700-82 | Intégration des données d'un lexique syntaxique dans un analyseur syntaxique probabiliste | http://arxiv.org/pdf/1404.1872v1.pdf | author:Anthony Sigogne, Matthieu Constant, Eric Laporte category:cs.CL published:2014-04-07 summary:This article reports the evaluation of the integration of data from asyntactic-semantic lexicon, the Lexicon-Grammar of French, into a syntacticparser. We show that by changing the set of labels for verbs and predicationalnouns, we can improve the performance on French of a non-lexicalizedprobabilistic parser.
arxiv-5700-83 | DenseNet: Implementing Efficient ConvNet Descriptor Pyramids | http://arxiv.org/pdf/1404.1869v1.pdf | author:Forrest Iandola, Matt Moskewicz, Sergey Karayev, Ross Girshick, Trevor Darrell, Kurt Keutzer category:cs.CV published:2014-04-07 summary:Convolutional Neural Networks (CNNs) can provide accurate objectclassification. They can be extended to perform object detection by iteratingover dense or selected proposed object regions. However, the runtime of suchdetectors scales as the total number and/or area of regions to examine perimage, and training such detectors may be prohibitively slow. However, for someCNN classifier topologies, it is possible to share significant work amongoverlapping regions to be classified. This paper presents DenseNet, an opensource system that computes dense, multiscale features from the convolutionallayers of a CNN based object classifier. Future work will involve trainingefficient object detectors with DenseNet feature descriptors.
arxiv-5700-84 | Evaluation and Ranking of Machine Translated Output in Hindi Language using Precision and Recall Oriented Metrics | http://arxiv.org/pdf/1404.1847v1.pdf | author:Aditi Kalyani, Hemant Kumud, Shashi Pal Singh, Ajai Kumar, Hemant Darbari category:cs.CL published:2014-04-07 summary:Evaluation plays a crucial role in development of Machine translationsystems. In order to judge the quality of an existing MT system i.e. if thetranslated output is of human translation quality or not, various automaticmetrics exist. We here present the implementation results of different metricswhen used on Hindi language along with their comparisons, illustrating howeffective are these metrics on languages like Hindi (free word order language).
arxiv-5700-85 | Structural Intervention Distance (SID) for Evaluating Causal Graphs | http://arxiv.org/pdf/1306.1043v2.pdf | author:Jonas Peters, Peter Bühlmann category:stat.ML published:2013-06-05 summary:Causal inference relies on the structure of a graph, often a directed acyclicgraph (DAG). Different graphs may result in different causal inferencestatements and different intervention distributions. To quantify suchdifferences, we propose a (pre-) distance between DAGs, the structuralintervention distance (SID). The SID is based on a graphical criterion only andquantifies the closeness between two DAGs in terms of their correspondingcausal inference statements. It is therefore well-suited for evaluating graphsthat are used for computing interventions. Instead of DAGs it is also possibleto compare CPDAGs, completed partially directed acyclic graphs that representMarkov equivalence classes. Since it differs significantly from the popularStructural Hamming Distance (SHD), the SID constitutes a valuable additionalmeasure. We discuss properties of this distance and provide an efficientimplementation with software code available on the first author's homepage (anR package is under construction).
arxiv-5700-86 | Improving Bilayer Product Quantization for Billion-Scale Approximate Nearest Neighbors in High Dimensions | http://arxiv.org/pdf/1404.1831v1.pdf | author:Artem Babenko, Victor Lempitsky category:cs.CV H.3.3 published:2014-04-07 summary:The top-performing systems for billion-scale high-dimensional approximatenearest neighbor (ANN) search are all based on two-layer architectures thatinclude an indexing structure and a compressed datapoints layer. An indexingstructure is crucial as it allows to avoid exhaustive search, while the lossydata compression is needed to fit the dataset into RAM. Several of the mostsuccessful systems use product quantization (PQ) for both the indexing and thedataset compression layers. These systems are however limited in the way theyexploit the interaction of product quantization processes that happen atdifferent stages of these systems. Here we introduce and evaluate two approximate nearest neighbor searchsystems that both exploit the synergy of product quantization processes in amore efficient way. The first system, called Fast Bilayer Product Quantization(FBPQ), speeds up the runtime of the baseline system (Multi-D-ADC) by severaltimes, while achieving the same accuracy. The second system, HierarchicalBilayer Product Quantization (HBPQ) provides a significantly better recall forthe same runtime at a cost of small memory footprint increase. For the BIGANNdataset of billion SIFT descriptors, the 10% increase in Recall@1 and the 17%increase in Recall@10 is observed.
arxiv-5700-87 | Probabilistic Archetypal Analysis | http://arxiv.org/pdf/1312.7604v2.pdf | author:Sohan Seth, Manuel J. A. Eugster category:stat.ML published:2013-12-29 summary:Archetypal analysis represents a set of observations as convex combinationsof pure patterns, or archetypes. The original geometric formulation of findingarchetypes by approximating the convex hull of the observations assumes them tobe real valued. This, unfortunately, is not compatible with many practicalsituations. In this paper we revisit archetypal analysis from the basicprinciples, and propose a probabilistic framework that accommodates otherobservation types such as integers, binary, and probability vectors. Wecorroborate the proposed methodology with convincing real-world applications onfinding archetypal winter tourists based on binary survey data, archetypaldisaster-affected countries based on disaster count data, and documentarchetypes based on term-frequency data. We also present an appropriatevisualization tool to summarize archetypal analysis solution better.
arxiv-5700-88 | MBIS: Multivariate Bayesian Image Segmentation Tool | http://arxiv.org/pdf/1404.0600v2.pdf | author:Oscar Esteban, Gert Wollny, Subrahmanyam Gorthi, Maria-J. Ledesma-Carbayo, Jean-Philippe Thiran, Andres Santos, Meritxell Bach-Cuadra category:cs.CV 62P10, 62F15 published:2014-04-02 summary:We present MBIS (Multivariate Bayesian Image Segmentation tool), a clusteringtool based on the mixture of multivariate normal distributions model. MBISsupports multi-channel bias field correction based on a B-spline model. Asecond methodological novelty is the inclusion of graph-cuts optimization forthe stationary anisotropic hidden Markov random field model. Along with MBIS,we release an evaluation framework that contains three different experiments onmulti-site data. We first validate the accuracy of segmentation and theestimated bias field for each channel. MBIS outperforms a widely usedsegmentation tool in a cross-comparison evaluation. The second experimentdemonstrates the robustness of results on atlas-free segmentation of two imagesets from scan-rescan protocols on 21 healthy subjects. Multivariatesegmentation is more replicable than the monospectral counterpart onT1-weighted images. Finally, we provide a third experiment to illustrate howMBIS can be used in a large-scale study of tissue volume change with increasingage in 584 healthy subjects. This last result is meaningful as multivariatesegmentation performs robustly without the need for prior knowledge
arxiv-5700-89 | Pseudo-Marginal Bayesian Inference for Gaussian Processes | http://arxiv.org/pdf/1310.0740v4.pdf | author:Maurizio Filippone, Mark Girolami category:stat.ML cs.LG stat.ME published:2013-10-02 summary:The main challenges that arise when adopting Gaussian Process priors inprobabilistic modeling are how to carry out exact Bayesian inference and how toaccount for uncertainty on model parameters when making model-based predictionson out-of-sample data. Using probit regression as an illustrative workingexample, this paper presents a general and effective methodology based on thepseudo-marginal approach to Markov chain Monte Carlo that efficiently addressesboth of these issues. The results presented in this paper show improvementsover existing sampling methods to simulate from the posterior distribution overthe parameters defining the covariance function of the Gaussian Process prior.This is particularly important as it offers a powerful tool to carry out fullBayesian inference of Gaussian Process based hierarchic statistical models ingeneral. The results also demonstrate that Monte Carlo based integration of allmodel parameters is actually feasible in this class of models providing asuperior quantification of uncertainty in predictions. Extensive comparisonswith respect to state-of-the-art probabilistic classifiers confirm thisassertion.
arxiv-5700-90 | Successive Nonnegative Projection Algorithm for Robust Nonnegative Blind Source Separation | http://arxiv.org/pdf/1310.7529v3.pdf | author:Nicolas Gillis category:stat.ML cs.LG math.NA math.OC published:2013-10-28 summary:In this paper, we propose a new fast and robust recursive algorithm fornear-separable nonnegative matrix factorization, a particular nonnegative blindsource separation problem. This algorithm, which we refer to as the successivenonnegative projection algorithm (SNPA), is closely related to the popularsuccessive projection algorithm (SPA), but takes advantage of the nonnegativityconstraint in the decomposition. We prove that SNPA is more robust than SPA andcan be applied to a broader class of nonnegative matrices. This is illustratedon some synthetic data sets, and on a real-world hyperspectral image.
arxiv-5700-91 | Icon Based Information Retrieval and Disease Identification in Agriculture | http://arxiv.org/pdf/1404.1664v1.pdf | author:Namita Mittal, Basant Agarwal, Ajay Gupta, Hemant Madhur category:cs.HC cs.CV cs.CY cs.IR published:2014-04-07 summary:Recent developments in the ICT industry in past few decades has enabled thequick and easy access to the information available on the internet. But,digital literacy is the pre-requisite for its use. The main purpose of thispaper is to provide an interface for digitally illiterate users, especiallyfarmers to efficiently and effectively retrieve information through Internet.In addition, to enable the farmers to identify the disease in their crop, itscause and symptoms using digital image processing and pattern recognitioninstantly without waiting for an expert to visit the farms and identify thedisease.
arxiv-5700-92 | Penalty-regulated dynamics and robust learning procedures in games | http://arxiv.org/pdf/1303.2270v2.pdf | author:Pierre Coucheney, Bruno Gaujal, Panayotis Mertikopoulos category:math.OC cs.GT cs.LG published:2013-03-09 summary:Starting from a heuristic learning scheme for N-person games, we derive a newclass of continuous-time learning dynamics consisting of a replicator-likedrift adjusted by a penalty term that renders the boundary of the game'sstrategy space repelling. These penalty-regulated dynamics are equivalent toplayers keeping an exponentially discounted aggregate of their on-going payoffsand then using a smooth best response to pick an action based on theseperformance scores. Owing to this inherent duality, the proposed dynamicssatisfy a variant of the folk theorem of evolutionary game theory and theyconverge to (arbitrarily precise) approximations of Nash equilibria inpotential games. Motivated by applications to traffic engineering, we exploitthis duality further to design a discrete-time, payoff-based learning algorithmwhich retains these convergence properties and only requires players to observetheir in-game payoffs: moreover, the algorithm remains robust in the presenceof stochastic perturbations and observation errors, and it does not require anysynchronization between players.
arxiv-5700-93 | A Denoising Autoencoder that Guides Stochastic Search | http://arxiv.org/pdf/1404.1614v1.pdf | author:Alexander W. Churchill, Siddharth Sigtia, Chrisantha Fernando category:cs.NE cs.LG published:2014-04-06 summary:An algorithm is described that adaptively learns a non-linear mutationdistribution. It works by training a denoising autoencoder (DA) online at eachgeneration of a genetic algorithm to reconstruct a slowly decaying memory ofthe best genotypes so far. A compressed hidden layer forces the autoencoder tolearn hidden features in the training set that can be used to accelerate searchon novel problems with similar structure. Its output neurons define aprobability distribution that we sample from to produce offspring solutions.The algorithm outperforms a canonical genetic algorithm on severalcombinatorial optimisation problems, e.g. multidimensional 0/1 knapsackproblem, MAXSAT, HIFF, and on parameter optimisation problems, e.g. Rastriginand Rosenbrock functions.
arxiv-5700-94 | Convex Relaxations of SE(2) and SE(3) for Visual Pose Estimation | http://arxiv.org/pdf/1401.3700v2.pdf | author:Matanya B. Horowitz, Nikolai Matni, Joel W. Burdick category:cs.CV published:2014-01-15 summary:This paper proposes a new method for rigid body pose estimation based onspectrahedral representations of the tautological orbitopes of $SE(2)$ and$SE(3)$. The approach can use dense point cloud data from stereo vision or anRGB-D sensor (such as the Microsoft Kinect), as well as visual appearance data.The method is a convex relaxation of the classical pose estimation problem, andis based on explicit linear matrix inequality (LMI) representations for theconvex hulls of $SE(2)$ and $SE(3)$. Given these representations, the relaxedpose estimation problem can be framed as a robust least squares problem withthe optimization variable constrained to these convex sets. Although thisformulation is a relaxation of the original problem, numerical experimentsindicate that it is indeed exact - i.e. its solution is a member of $SE(2)$ or$SE(3)$ - in many interesting settings. We additionally show that this methodis guaranteed to be exact for a large class of pose estimation problems.
arxiv-5700-95 | Causal Discovery with Continuous Additive Noise Models | http://arxiv.org/pdf/1309.6779v4.pdf | author:Jonas Peters, Joris Mooij, Dominik Janzing, Bernhard Schölkopf category:stat.ML published:2013-09-26 summary:We consider the problem of learning causal directed acyclic graphs from anobservational joint distribution. One can use these graphs to predict theoutcome of interventional experiments, from which data are often not available.We show that if the observational distribution follows a structural equationmodel with an additive noise structure, the directed acyclic graph becomesidentifiable from the distribution under mild conditions. This constitutes aninteresting alternative to traditional methods that assume faithfulness andidentify only the Markov equivalence class of the graph, thus leaving someedges undirected. We provide practical algorithms for finitely many samples,RESIT (Regression with Subsequent Independence Test) and two methods based onan independence score. We prove that RESIT is correct in the population settingand provide an empirical evaluation.
arxiv-5700-96 | Sparse Coding: A Deep Learning using Unlabeled Data for High - Level Representation | http://arxiv.org/pdf/1404.1559v1.pdf | author:R. Vidya, Dr. G. M. Nasira, R. P. Jaia Priyankka category:cs.LG cs.NE published:2014-04-06 summary:Sparse coding algorithm is an learning algorithm mainly for unsupervisedfeature for finding succinct, a little above high - level Representation ofinputs, and it has successfully given a way for Deep learning. Our objective isto use High - Level Representation data in form of unlabeled category to helpunsupervised learning task. when compared with labeled data, unlabeled data iseasier to acquire because, unlike labeled data it does not follow someparticular class labels. This really makes the Deep learning wider andapplicable to practical problems and learning. The main problem with sparsecoding is it uses Quadratic loss function and Gaussian noise mode. So, itsperforms is very poor when binary or integer value or other Non- Gaussian typedata is applied. Thus first we propose an algorithm for solving the L1 -regularized convex optimization algorithm for the problem to allow High - LevelRepresentation of unlabeled data. Through this we derive a optimal solution fordescribing an approach to Deep learning algorithm by using sparse code.
arxiv-5700-97 | Text Based Approach For Indexing And Retrieval Of Image And Video: A Review | http://arxiv.org/pdf/1404.1514v1.pdf | author:Avinash N Bhute, B. B. Meshram category:cs.IR cs.CV cs.DL cs.MM published:2014-04-05 summary:Text data present in multimedia contain useful information for automaticannotation, indexing. Extracted information used for recognition of the overlayor scene text from a given video or image. The Extracted text can be used forretrieving the videos and images. In this paper, firstly, we are discussed thedifferent techniques for text extraction from images and videos. Secondly, weare reviewed the techniques for indexing and retrieval of image and videos byusing extracted text.
arxiv-5700-98 | A Compression Technique for Analyzing Disagreement-Based Active Learning | http://arxiv.org/pdf/1404.1504v1.pdf | author:Yair Wiener, Steve Hanneke, Ran El-Yaniv category:cs.LG stat.ML published:2014-04-05 summary:We introduce a new and improved characterization of the label complexity ofdisagreement-based active learning, in which the leading quantity is theversion space compression set size. This quantity is defined as the size of thesmallest subset of the training data that induces the same version space. Weshow various applications of the new characterization, including a tightanalysis of CAL and refined label complexity bounds for linear separators undermixtures of Gaussians and axis-aligned rectangles under product densities. Theversion space compression set size, as well as the new characterization of thelabel complexity, can be naturally extended to agnostic learning problems, forwhich we show new speedup results for two well known active learningalgorithms.
arxiv-5700-99 | Ensemble Committees for Stock Return Classification and Prediction | http://arxiv.org/pdf/1404.1492v1.pdf | author:James Brofos category:stat.ML cs.LG published:2014-04-05 summary:This paper considers a portfolio trading strategy formulated by algorithms inthe field of machine learning. The profitability of the strategy is measured bythe algorithm's capability to consistently and accurately identify stockindices with positive or negative returns, and to generate a preferredportfolio allocation on the basis of a learned model. Stocks are characterizedby time series data sets consisting of technical variables that reflect marketconditions in a previous time interval, which are utilized produce binaryclassification decisions in subsequent intervals. The learned model isconstructed as a committee of random forest classifiers, a non-linear supportvector machine classifier, a relevance vector machine classifier, and aconstituent ensemble of k-nearest neighbors classifiers. The Global IndustryClassification Standard (GICS) is used to explore the ensemble model's efficacywithin the context of various fields of investment including Energy, Materials,Financials, and Information Technology. Data from 2006 to 2012, inclusive, areconsidered, which are chosen for providing a range of market circumstances forevaluating the model. The model is observed to achieve an accuracy ofapproximately 70% when predicting stock price returns three months in advance.
arxiv-5700-100 | Sharpened Error Bounds for Random Sampling Based $\ell_2$ Regression | http://arxiv.org/pdf/1403.7737v2.pdf | author:Shusen Wang category:cs.LG cs.NA stat.ML published:2014-03-30 summary:Given a data matrix $X \in R^{n\times d}$ and a response vector $y \inR^{n}$, suppose $n>d$, it costs $O(n d^2)$ time and $O(n d)$ space to solve theleast squares regression (LSR) problem. When $n$ and $d$ are both large,exactly solving the LSR problem is very expensive. When $n \gg d$, one feasibleapproach to speeding up LSR is to randomly embed $y$ and all columns of $X$into a smaller subspace $R^c$; the induced LSR problem has the same number ofcolumns but much fewer number of rows, and it can be solved in $O(c d^2)$ timeand $O(c d)$ space. We discuss in this paper two random sampling based methods for solving LSRmore efficiently. Previous work showed that the leverage scores based samplingbased LSR achieves $1+\epsilon$ accuracy when $c \geq O(d \epsilon^{-2} \logd)$. In this paper we sharpen this error bound, showing that $c = O(d \log d +d \epsilon^{-1})$ is enough for achieving $1+\epsilon$ accuracy. We also showthat when $c \geq O(\mu d \epsilon^{-2} \log d)$, the uniform sampling basedLSR attains a $2+\epsilon$ bound with positive probability.
arxiv-5700-101 | The Fast Cauchy Transform and Faster Robust Linear Regression | http://arxiv.org/pdf/1207.4684v3.pdf | author:Kenneth L. Clarkson, Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, Xiangrui Meng, David P. Woodruff category:cs.DS stat.ML published:2012-07-19 summary:We provide fast algorithms for overconstrained $\ell_p$ regression andrelated problems: for an $n\times d$ input matrix $A$ and vector$b\in\mathbb{R}^n$, in $O(nd\log n)$ time we reduce the problem$\min_{x\in\mathbb{R}^d} \Ax-b\_p$ to the same problem with input matrix$\tilde A$ of dimension $s \times d$ and corresponding $\tilde b$ of dimension$s\times 1$. Here, $\tilde A$ and $\tilde b$ are a coreset for the problem,consisting of sampled and rescaled rows of $A$ and $b$; and $s$ is independentof $n$ and polynomial in $d$. Our results improve on the best previousalgorithms when $n\gg d$, for all $p\in[1,\infty)$ except $p=2$. We alsoprovide a suite of improved results for finding well-conditioned bases viaellipsoidal rounding, illustrating tradeoffs between running time andconditioning quality, including a one-pass conditioning algorithm for general$\ell_p$ problems. We also provide an empirical evaluation of implementations of our algorithmsfor $p=1$, comparing them with related algorithms. Our empirical results showthat, in the asymptotic regime, the theory is a very good guide to thepractical performance of these algorithms. Our algorithms use our fasterconstructions of well-conditioned bases for $\ell_p$ spaces and, for $p=1$, afast subspace embedding of independent interest that we call the Fast CauchyTransform: a distribution over matrices $\Pi:\mathbb{R}^n\mapsto\mathbb{R}^{O(d\log d)}$, found obliviously to $A$, that approximatelypreserves the $\ell_1$ norms: that is, with large probability, simultaneouslyfor all $x$, $\Ax\_1 \approx \\Pi Ax\_1$, with distortion $O(d^{2+\eta})$,for an arbitrarily small constant $\eta>0$; and, moreover, $\Pi A$ can becomputed in $O(nd\log d)$ time. The techniques underlying our Fast CauchyTransform include fast Johnson-Lindenstrauss transforms, low-coherencematrices, and rescaling by Cauchy random variables.
arxiv-5700-102 | Multiple Testing for Neuroimaging via Hidden Markov Random Field | http://arxiv.org/pdf/1404.1371v1.pdf | author:Hai Shu, Bin Nan, Robert Koeppe, the Alzheimer's Dise category:stat.AP stat.ML published:2014-04-04 summary:One of the important objectives that the Alzheimer's Disease NeuroimagingInitiative (ADNI) tries to achieve is to understand how the human brain changesover the course of disease progression. We consider voxel-level analysis forthe 18F-Fluorodeoxyglucose positron emission tomography (FDG-PET) imaging studyin ADNI for such a purpose. Traditional voxel-level multiple testing proceduresin neuroimaging, which are mostly p-value based, often ignore the spatialcorrelations among neighboring voxels and thus suffer from substantial loss ofpower. We extend the local-significance-index based procedure, which aims tominimize the false nondiscovery rate subject to a constraint on the falsediscovery rate, to three-dimensional neuroimaging data using a hidden Markovrandom field model. A generalized expectation-maximization algorithm isproposed for estimating the model parameters. Extensive simulations show thatthe proposed approach is more powerful than conventional false discovery rateprocedures. We apply the method to the comparison between mild cognitiveimpairment, a disease status with increased risk of developing Alzheimer's oranother dementia, and normal controls in the ADNI's FDG-PET imaging study.
arxiv-5700-103 | TACT: A Transfer Actor-Critic Learning Framework for Energy Saving in Cellular Radio Access Networks | http://arxiv.org/pdf/1211.6616v3.pdf | author:Rongpeng Li, Zhifeng Zhao, Xianfu Chen, Jacques Palicot, Honggang Zhang category:cs.NI cs.AI cs.IT cs.LG math.IT published:2012-11-28 summary:Recent works have validated the possibility of improving energy efficiency inradio access networks (RANs), achieved by dynamically turning on/off some basestations (BSs). In this paper, we extend the research over BS switchingoperations, which should match up with traffic load variations. Instead ofdepending on the dynamic traffic loads which are still quite challenging toprecisely forecast, we firstly formulate the traffic variations as a Markovdecision process. Afterwards, in order to foresightedly minimize the energyconsumption of RANs, we design a reinforcement learning framework based BSswitching operation scheme. Furthermore, to avoid the underlying curse ofdimensionality in reinforcement learning, a transfer actor-critic algorithm(TACT), which utilizes the transferred learning expertise in historical periodsor neighboring regions, is proposed and provably converges. In the end, weevaluate our proposed scheme by extensive simulations under various practicalconfigurations and show that the proposed TACT algorithm contributes to aperformance jumpstart and demonstrates the feasibility of significant energyefficiency improvement at the expense of tolerable delay performance.
arxiv-5700-104 | AIS-MACA- Z: MACA based Clonal Classifier for Splicing Site, Protein Coding and Promoter Region Identification in Eukaryotes | http://arxiv.org/pdf/1404.1144v1.pdf | author:Pokkuluri Kiran Sree, Inampudi Ramesh Babu, SSSN Usha Devi N category:cs.CE cs.LG published:2014-04-04 summary:Bioinformatics incorporates information regarding biological data storage,accessing mechanisms and presentation of characteristics within this data. Mostof the problems in bioinformatics and be addressed efficiently by computertechniques. This paper aims at building a classifier based on MultipleAttractor Cellular Automata (MACA) which uses fuzzy logic with version Z topredict splicing site, protein coding and promoter region identification ineukaryotes. It is strengthened with an artificial immune system technique(AIS), Clonal algorithm for choosing rules of best fitness. The proposedclassifier can handle DNA sequences of lengths 54,108,162,252,354. Thisclassifier gives the exact boundaries of both protein and promoter regions withan average accuracy of 90.6%. This classifier can predict the splicing sitewith 97% accuracy. This classifier was tested with 1, 97,000 data componentswhich were taken from Fickett & Toung , EPDnew, and other sequences from arenowned medical university.
arxiv-5700-105 | Resolving Multi-path Interference in Time-of-Flight Imaging via Modulation Frequency Diversity and Sparse Regularization | http://arxiv.org/pdf/1404.1116v1.pdf | author:Ayush Bhandari, Achuta Kadambi, Refael Whyte, Christopher Barsi, Micha Feigin, Adrian Dorrington, Ramesh Raskar category:cs.CV cs.IT math.IT physics.optics published:2014-04-03 summary:Time-of-flight (ToF) cameras calculate depth maps by reconstructing phaseshifts of amplitude-modulated signals. For broad illumination or transparentobjects, reflections from multiple scene points can illuminate a given pixel,giving rise to an erroneous depth map. We report here a sparsity regularizedsolution that separates K-interfering components using multiple modulationfrequency measurements. The method maps ToF imaging to the general framework ofspectral estimation theory and has applications in improving depth profiles andexploiting multiple scattering.
arxiv-5700-106 | Attribute-Efficient Evolvability of Linear Functions | http://arxiv.org/pdf/1309.4132v2.pdf | author:Elaine Angelino, Varun Kanade category:cs.LG q-bio.PE published:2013-09-16 summary:In a seminal paper, Valiant (2006) introduced a computational model forevolution to address the question of complexity that can arise throughDarwinian mechanisms. Valiant views evolution as a restricted form ofcomputational learning, where the goal is to evolve a hypothesis that is closeto the ideal function. Feldman (2008) showed that (correlational) statisticalquery learning algorithms could be framed as evolutionary mechanisms inValiant's model. P. Valiant (2012) considered evolvability of real-valuedfunctions and also showed that weak-optimization algorithms that useweak-evaluation oracles could be converted to evolutionary mechanisms. In this work, we focus on the complexity of representations of evolutionarymechanisms. In general, the reductions of Feldman and P. Valiant may result inintermediate representations that are arbitrarily complex (polynomial-sizedcircuits). We argue that biological constraints often dictate that therepresentations have low complexity, such as constant depth and fan-incircuits. We give mechanisms for evolving sparse linear functions under a largeclass of smooth distributions. These evolutionary algorithms areattribute-efficient in the sense that the size of the representations and thenumber of generations required depend only on the sparsity of the targetfunction and the accuracy parameter, but have no dependence on the total numberof attributes.
arxiv-5700-107 | A Tutorial on Principal Component Analysis | http://arxiv.org/pdf/1404.1100v1.pdf | author:Jonathon Shlens category:cs.LG stat.ML published:2014-04-03 summary:Principal component analysis (PCA) is a mainstay of modern data analysis - ablack box that is widely used but (sometimes) poorly understood. The goal ofthis paper is to dispel the magic behind this black box. This manuscriptfocuses on building a solid intuition for how and why principal componentanalysis works. This manuscript crystallizes this knowledge by deriving fromsimple intuitions, the mathematics behind PCA. This tutorial does not shy awayfrom explaining the ideas informally, nor does it shy away from themathematics. The hope is that by addressing both aspects, readers of all levelswill be able to gain a better understanding of PCA as well as the when, the howand the why of applying this technique.
arxiv-5700-108 | Optimal Schatten-q and Ky-Fan-k Norm Rate of Low Rank Matrix Estimation | http://arxiv.org/pdf/1403.6499v2.pdf | author:Dong Xia category:stat.ML published:2014-03-25 summary:In this paper, we consider low rank matrix estimation using eithermatrix-version Dantzig Selector $\hat{A}_{\lambda}^d$ or matrix-version LASSOestimator $\hat{A}_{\lambda}^L$. We consider sub-Gaussian measurements, $i.e.$,the measurements $X_1,\ldots,X_n\in\mathbb{R}^{m\times m}$ have $i.i.d.$sub-Gaussian entries. Suppose $\textrm{rank}(A_0)=r$. We proved that, when$n\geq Cm[r^2\vee r\log(m)\log(n)]$ for some $C>0$, both $\hat{A}_{\lambda}^d$and $\hat{A}_{\lambda}^L$ can obtain optimal upper bounds(except somelogarithmic terms) for estimation accuracy under spectral norm. By applyingmetric entropy of Grassmann manifolds, we construct (near) matching minimaxlower bound for estimation accuracy under spectral norm. We also give upperbounds and matching minimax lower bound(except some logarithmic terms) forestimation accuracy under Schatten-q norm for every $1\leq q\leq\infty$. As adirect corollary, we show both upper bounds and minimax lower bounds ofestimation accuracy under Ky-Fan-k norms for every $1\leq k\leq m$.
arxiv-5700-109 | Parallel Support Vector Machines in Practice | http://arxiv.org/pdf/1404.1066v1.pdf | author:Stephen Tyree, Jacob R. Gardner, Kilian Q. Weinberger, Kunal Agrawal, John Tran category:cs.LG published:2014-04-03 summary:In this paper, we evaluate the performance of various parallel optimizationmethods for Kernel Support Vector Machines on multicore CPUs and GPUs. Inparticular, we provide the first comparison of algorithms with explicit andimplicit parallelization. Most existing parallel implementations for multi-coreor GPU architectures are based on explicit parallelization of SequentialMinimal Optimization (SMO)---the programmers identified parallelizablecomponents and hand-parallelized them, specifically tuned for a particulararchitecture. We compare these approaches with each other and with implicitlyparallelized algorithms---where the algorithm is expressed such that most ofthe work is done within few iterations with large dense linear algebraoperations. These can be computed with highly-optimized libraries, that arecarefully parallelized for a large variety of parallel platforms. We highlightthe advantages and disadvantages of both approaches and compare them on variousbenchmark data sets. We find an approximate implicitly parallel algorithm whichis surprisingly efficient, permits a much simpler implementation, and leads tounprecedented speedups in SVM training.
arxiv-5700-110 | Bayes and Naive Bayes Classifier | http://arxiv.org/pdf/1404.0933v1.pdf | author:Vikramkumar, Vijaykumar B, Trilochan category:cs.LG published:2014-04-03 summary:The Bayesian Classification represents a supervised learning method as wellas a statistical method for classification. Assumes an underlying probabilisticmodel and it allows us to capture uncertainty about the model in a principledway by determining probabilities of the outcomes. This Classification is namedafter Thomas Bayes (1702-1761), who proposed the Bayes Theorem. Bayesianclassification provides practical learning algorithms and prior knowledge andobserved data can be combined. Bayesian Classification provides a usefulperspective for understanding and evaluating many learning algorithms. Itcalculates explicit probabilities for hypothesis and it is robust to noise ininput data. In statistical classification the Bayes classifier minimises theprobability of misclassification. That was a visual intuition for a simple caseof the Bayes classifier, also called: 1)Idiot Bayes 2)Naive Bayes 3)SimpleBayes
arxiv-5700-111 | A Novel Genetic Algorithm using Helper Objectives for the 0-1 Knapsack Problem | http://arxiv.org/pdf/1404.0868v1.pdf | author:Jun He, Feidun He, Hongbin Dong category:cs.NE published:2014-04-03 summary:The 0-1 knapsack problem is a well-known combinatorial optimisation problem.Approximation algorithms have been designed for solving it and they returnprovably good solutions within polynomial time. On the other hand, geneticalgorithms are well suited for solving the knapsack problem and they findreasonably good solutions quickly. A naturally arising question is whethergenetic algorithms are able to find solutions as good as approximationalgorithms do. This paper presents a novel multi-objective optimisation geneticalgorithm for solving the 0-1 knapsack problem. Experiment results show thatthe new algorithm outperforms its rivals, the greedy algorithm, mixed strategygenetic algorithm, and greedy algorithm + mixed strategy genetic algorithm.
arxiv-5700-112 | Application of Ontologies in Identifying Requirements Patterns in Use Cases | http://arxiv.org/pdf/1404.0850v1.pdf | author:Rui Couto, António Nestor Ribeiro, José Creissac Campos category:cs.SE cs.CL cs.IR published:2014-04-03 summary:Use case specifications have successfully been used for requirementsdescription. They allow joining, in the same modeling space, the expectationsof the stakeholders as well as the needs of the software engineer and analystinvolved in the process. While use cases are not meant to describe a system'simplementation, by formalizing their description we are able to extractimplementation relevant information from them. More specifically, we areinterested in identifying requirements patterns (common requirements withtypical implementation solutions) in support for a requirements based softwaredevelopment approach. In the paper we propose the transformation of Use Casedescriptions expressed in a Controlled Natural Language into an ontologyexpressed in the Web Ontology Language (OWL). OWL's query engines can then beused to identify requirements patterns expressed as queries over the ontology.We describe a tool that we have developed to support the approach and providean example of usage.
arxiv-5700-113 | Methods Of Measurement The Three-Dimensional Wind Waves Spectra, Based On The Processing Of Video Images Of The Sea Surface | http://arxiv.org/pdf/1303.5248v2.pdf | author:Boris M. Salin, Mikhail B. Salin category:physics.ao-ph cs.CV published:2013-03-21 summary:Optical instruments for measuring surface-wave characteristics provide abetter spatial and temporal resolution than other methods, but they facedifficulties while converting the results of indirect measurements intoabsolute levels of the waves. We have solved this problem to some extent. Inthis paper, we propose an optical method for measuring the 3D power spectraldensity of the surface waves and spatio-temporal samples of the wave profiles.The method involves, first, synchronous recording of the brightness field overa patch of a rough surface and measurement of surface oscillations at one ormore points and, second, filtering of the spatial image spectrum. Filterparameters are chosen to maximize the correlation of the surface oscillationsrecovered and measured at one or two points. In addition to the measurementprocedure, the paper provides experimental results of measuringmultidimensional spectra of roughness, which generally agree with theoreticalexpectations and the results of other authors.
arxiv-5700-114 | GPU Accelerated Fractal Image Compression for Medical Imaging in Parallel Computing Platform | http://arxiv.org/pdf/1404.0774v1.pdf | author:Md. Enamul Haque, Abdullah Al Kaisan, Mahmudur R Saniat, Aminur Rahman category:cs.DC cs.CV published:2014-04-03 summary:In this paper, we implemented both sequential and parallel version of fractalimage compression algorithms using CUDA (Compute Unified Device Architecture)programming model for parallelizing the program in Graphics Processing Unit formedical images, as they are highly similar within the image itself. There areseveral improvement in the implementation of the algorithm as well. Fractalimage compression is based on the self similarity of an image, meaning an imagehaving similarity in majority of the regions. We take this opportunity toimplement the compression algorithm and monitor the effect of it using bothparallel and sequential implementation. Fractal compression has the property ofhigh compression rate and the dimensionless scheme. Compression scheme forfractal image is of two kind, one is encoding and another is decoding. Encodingis very much computational expensive. On the other hand decoding is lesscomputational. The application of fractal compression to medical images wouldallow obtaining much higher compression ratios. While the fractal magnificationan inseparable feature of the fractal compression would be very useful inpresenting the reconstructed image in a highly readable form. However, like allirreversible methods, the fractal compression is connected with the problem ofinformation loss, which is especially troublesome in the medical imaging. Avery time consuming encoding pro- cess, which can last even several hours, isanother bothersome drawback of the fractal compression.
arxiv-5700-115 | An Efficient Search Strategy for Aggregation and Discretization of Attributes of Bayesian Networks Using Minimum Description Length | http://arxiv.org/pdf/1404.0752v1.pdf | author:Jem Corcoran, Daniel Tran, Nicholas Levine category:stat.ML published:2014-04-03 summary:Bayesian networks are convenient graphical expressions for high dimensionalprobability distributions representing complex relationships between a largenumber of random variables. They have been employed extensively in areas suchas bioinformatics, artificial intelligence, diagnosis, and risk management. Therecovery of the structure of a network from data is of prime importance for thepurposes of modeling, analysis, and prediction. Most recovery algorithms in theliterature assume either discrete of continuous but Gaussian data. For generalcontinuous data, discretization is usually employed but often destroys the verystructure one is out to recover. Friedman and Goldszmidt suggest an approachbased on the minimum description length principle that chooses a discretizationwhich preserves the information in the original data set, however it is onewhich is difficult, if not impossible, to implement for even moderately sizednetworks. In this paper we provide an extremely efficient search strategy whichallows one to use the Friedman and Goldszmidt discretization in practice.
arxiv-5700-116 | Subspace Learning from Extremely Compressed Measurements | http://arxiv.org/pdf/1404.0751v1.pdf | author:Akshay Krishnamurthy, Martin Azizyan, Aarti Singh category:stat.ML cs.LG published:2014-04-03 summary:We consider learning the principal subspace of a large set of vectors from anextremely small number of compressive measurements of each vector. Ourtheoretical results show that even a constant number of measurements per columnsuffices to approximate the principal subspace to arbitrary precision, providedthat the number of vectors is large. This result is achieved by a simplealgorithm that computes the eigenvectors of an estimate of the covariancematrix. The main insight is to exploit an averaging effect that arises fromapplying a different random projection to each vector. We provide a number ofsimulations confirming our theoretical results.
arxiv-5700-117 | Computational Optimization, Modelling and Simulation: Recent Trends and Challenges | http://arxiv.org/pdf/1404.0708v1.pdf | author:Xin-She Yang, Slawomir Koziel, Leifur Leifsson category:cs.NE math.OC 90C26 published:2014-04-02 summary:Modelling, simulation and optimization form an integrated part of moderndesign practice in engineering and industry. Tremendous progress has beenobserved for all three components over the last few decades. However, manychallenging issues remain unresolved, and the current trends tend to usenature-inspired algorithms and surrogate-based techniques for modelling andoptimization. This 4th workshop on Computational Optimization, Modelling andSimulation (COMS 2013) at ICCS 2013 will further summarize the latestdevelopments of optimization and modelling and their applications in science,engineering and industry. In this review paper, we will analyse the recenttrends in modelling and optimization, and their associated challenges. We willdiscuss important topics for further research, including parameter-tuning,large-scale problems, and the gaps between theory and applications.
arxiv-5700-118 | Multi-objective Flower Algorithm for Optimization | http://arxiv.org/pdf/1404.0695v1.pdf | author:Xin-She Yang, M. Karamanoglu, Xingshi He category:cs.NE math.OC 90C26 published:2014-04-02 summary:Flower pollination algorithm is a new nature-inspired algorithm, based on thecharacteristics of flowering plants. In this paper, we extend this floweralgorithm to solve multi-objective optimization problems in engineering. Byusing the weighted sum method with random weights, we show that the proposedmulti-objective flower algorithm can accurately find the Pareto fronts for aset of test functions. We then solve a bi-objective disc brake design problem,which indeed converges quickly.
arxiv-5700-119 | Active Deformable Part Models | http://arxiv.org/pdf/1404.0334v2.pdf | author:Menglong Zhu, Nikolay Atanasov, George J. Pappas, Kostas Daniilidis category:cs.CV cs.LG published:2014-04-01 summary:This paper presents an active approach for part-based object detection, whichoptimizes the order of part filter evaluations and the time at which to stopand make a prediction. Statistics, describing the part responses, are learnedfrom training data and are used to formalize the part scheduling problem as anoffline optimization. Dynamic programming is applied to obtain a policy, whichbalances the number of part evaluations with the classification accuracy.During inference, the policy is used as a look-up table to choose the partorder and the stopping time based on the observed filter responses. The methodis faster than cascade detection with deformable part models (which does notoptimize the part order) with negligible loss in accuracy when evaluated on thePASCAL VOC 2007 and 2010 datasets.
arxiv-5700-120 | Extraction of Projection Profile, Run-Histogram and Entropy Features Straight from Run-Length Compressed Text-Documents | http://arxiv.org/pdf/1404.0627v1.pdf | author:Mohammed Javed, P. Nagabhushan, B. B. Chaudhuri category:cs.CV published:2014-04-02 summary:Document Image Analysis, like any Digital Image Analysis requiresidentification and extraction of proper features, which are generally extractedfrom uncompressed images, though in reality images are made available incompressed form for the reasons such as transmission and storage efficiency.However, this implies that the compressed image should be decompressed, whichindents additional computing resources. This limitation induces the motivationto research in extracting features directly from the compressed image. In thisresearch, we propose to extract essential features such as projection profile,run-histogram and entropy for text document analysis directly from run-lengthcompressed text-documents. The experimentation illustrates that features areextracted directly from the compressed image without going through the stage ofdecompression, because of which the computing time is reduced. The featurevalues so extracted are exactly identical to those extracted from uncompressedimages.
arxiv-5700-121 | From ADP to the Brain: Foundations, Roadmap, Challenges and Research Priorities | http://arxiv.org/pdf/1404.0554v1.pdf | author:Paul J Werbos category:cs.NE published:2014-04-02 summary:This paper defines and discusses Mouse Level Computational Intelligence(MLCI) as a grand challenge for the coming century. It provides a specificroadmap to reach that target, citing relevant work and review papers anddiscussing the relation to funding priorities in two NSF funding activities:the ongoing Energy, Power and Adaptive Systems program (EPAS) and the recentinitiative in Cognitive Optimization and Prediction (COPN). It elaborates onthe first step, vector intelligence, a challenge in the development ofuniversal learning systems, which itself will require considerable new researchto attain. This in turn is a crucial prerequisite to true functionalunderstanding of how mammal brains achieve such general learning capabilities.
arxiv-5700-122 | A Comparative Study of Modern Inference Techniques for Structured Discrete Energy Minimization Problems | http://arxiv.org/pdf/1404.0533v1.pdf | author:Jörg H. Kappes, Bjoern Andres, Fred A. Hamprecht, Christoph Schnörr, Sebastian Nowozin, Dhruv Batra, Sungwoong Kim, Bernhard X. Kausler, Thorben Kröger, Jan Lellmann, Nikos Komodakis, Bogdan Savchynskyy, Carsten Rother category:cs.CV published:2014-04-02 summary:Szeliski et al. published an influential study in 2006 on energy minimizationmethods for Markov Random Fields (MRF). This study provided valuable insightsin choosing the best optimization technique for certain classes of problems.While these insights remain generally useful today, the phenomenal success ofrandom field models means that the kinds of inference problems that have to besolved changed significantly. Specifically, the models today often includehigher order interactions, flexible connectivity structures, largela\-bel-spaces of different cardinalities, or learned energy tables. To reflectthese changes, we provide a modernized and enlarged study. We present anempirical comparison of 32 state-of-the-art optimization techniques on a corpusof 2,453 energy minimization instances from diverse applications in computervision. To ensure reproducibility, we evaluate all methods in the OpenGM 2framework and report extensive results regarding runtime and solution quality.Key insights from our study agree with the results of Szeliski et al. for thetypes of models they studied. However, on new and challenging types of modelsour findings disagree and suggest that polyhedral methods and integerprogramming solvers are competitive in terms of runtime and solution qualityover a large range of model types.
arxiv-5700-123 | Constrained speaker linking | http://arxiv.org/pdf/1403.7084v2.pdf | author:David A. van Leeuwen, Niko Brümmer category:stat.ML cs.SD published:2014-03-26 summary:In this paper we study speaker linking (a.k.a.\ partitioning) givenconstraints of the distribution of speaker identities over speech recordings.Specifically, we show that the intractable partitioning problem becomestractable when the constraints pre-partition the data in smaller cliques withnon-overlapping speakers. The surprisingly common case where speakers intelephone conversations are known, but the assignment of channels to identitiesis unspecified, is treated in a Bayesian way. We show that for the Dutch CGNdatabase, where this channel assignment task is at hand, a lightweight speakerrecognition system can quite effectively solve the channel assignment problem,with 93% of the cliques solved. We further show that the posterior distributionover channel assignment configurations is well calibrated.
arxiv-5700-124 | Thoughts on a Recursive Classifier Graph: a Multiclass Network for Deep Object Recognition | http://arxiv.org/pdf/1404.2903v1.pdf | author:Marius Leordeanu, Rahul Sukthankar category:cs.CV cs.LG cs.NE published:2014-04-02 summary:We propose a general multi-class visual recognition model, termed theClassifier Graph, which aims to generalize and integrate ideas from many oftoday's successful hierarchical recognition approaches. Our graph-based modelhas the advantage of enabling rich interactions between classes from differentlevels of interpretation and abstraction. The proposed multi-class system isefficiently learned using step by step updates. The structure consists ofsimple logistic linear layers with inputs from features that are automaticallyselected from a large pool. Each newly learned classifier becomes a potentialnew feature. Thus, our feature pool can consist both of initial manuallydesigned features as well as learned classifiers from previous steps (graphnodes), each copied many times at different scales and locations. In thismanner we can learn and grow both a deep, complex graph of classifiers and arich pool of features at different levels of abstraction and interpretation.Our proposed graph of classifiers becomes a multi-class system with a recursivestructure, suitable for deep detection and recognition of several classessimultaneously.
arxiv-5700-125 | Cellular Automata and Its Applications in Bioinformatics: A Review | http://arxiv.org/pdf/1404.0453v1.pdf | author:Pokkuluri Kiran Sree, Inampudi Ramesh Babu, SSSN Usha Devi N category:cs.CE cs.LG published:2014-04-02 summary:This paper aims at providing a survey on the problems that can be easilyaddressed by cellular automata in bioinformatics. Some of the authors haveproposed algorithms for addressing some problems in bioinformatics but theapplication of cellular automata in bioinformatics is a virgin field inresearch. None of the researchers has tried to relate the major problems inbioinformatics and find a common solution. Extensive literature surveys wereconducted. We have considered some papers in various journals and conferencesfor conduct of our research. This paper provides intuition towards relatingvarious problems in bioinformatics logically and tries to attain a common framework for addressing the same.
arxiv-5700-126 | An Enhanced Features Extractor for a Portfolio of Constraint Solvers | http://arxiv.org/pdf/1308.0227v7.pdf | author:Roberto Amadini, Maurizio Gabbrielli, Jacopo Mauro category:cs.AI cs.LG published:2013-08-01 summary:Recent research has shown that a single arbitrarily efficient solver can besignificantly outperformed by a portfolio of possibly slower on-averagesolvers. The solver selection is usually done by means of (un)supervisedlearning techniques which exploit features extracted from the problemspecification. In this paper we present an useful and flexible framework thatis able to extract an extensive set of features from a Constraint(Satisfaction/Optimization) Problem defined in possibly different modelinglanguages: MiniZinc, FlatZinc or XCSP. We also report some empirical resultsshowing that the performances that can be obtained using these features areeffective and competitive with state of the art CSP portfolio techniques.
arxiv-5700-127 | Theory and Application of Shapelets to the Analysis of Surface Self-assembly Imaging | http://arxiv.org/pdf/1404.0437v1.pdf | author:Robert Suderman, Daniel Lizotte, Nasser Mohieddin Abukhdeir category:cs.CV published:2014-04-02 summary:A method for quantitative analysis of local pattern strength and defects insurface self-assembly imaging is presented and applied to images of stripe andhexagonal ordered domains. The presented method uses "shapelet" functions whichwere originally developed for quantitative analysis of images of galaxies($\propto 10^{20}\mathrm{m}$). In this work, they are used instead to quantifythe presence of translational order in surface self-assembled films ($\propto10^{-9}\mathrm{m}$) through reformulation into "steerable" filters. Theresulting method is both computationally efficient (with respect to the numberof filter evaluations), robust to variation in pattern feature shape, and,unlike previous approaches, is applicable to a wide variety of pattern types.An application of the method is presented which uses a nearest-neighbouranalysis to distinguish between uniform (defect-free) and non-uniform(strained, defect-containing) regions within imaged self-assembled domains,both with striped and hexagonal patterns.
arxiv-5700-128 | Confidence Intervals and Hypothesis Testing for High-Dimensional Regression | http://arxiv.org/pdf/1306.3171v2.pdf | author:Adel Javanmard, Andrea Montanari category:stat.ME cs.IT cs.LG math.IT published:2013-06-13 summary:Fitting high-dimensional statistical models often requires the use ofnon-linear parameter estimation procedures. As a consequence, it is generallyimpossible to obtain an exact characterization of the probability distributionof the parameter estimates. This in turn implies that it is extremelychallenging to quantify the \emph{uncertainty} associated with a certainparameter estimate. Concretely, no commonly accepted procedure exists forcomputing classical measures of uncertainty and statistical significance asconfidence intervals or $p$-values for these models. We consider here high-dimensional linear regression problem, and propose anefficient algorithm for constructing confidence intervals and $p$-values. Theresulting confidence intervals have nearly optimal size. When testing for thenull hypothesis that a certain parameter is vanishing, our method has nearlyoptimal power. Our approach is based on constructing a `de-biased' version of regularizedM-estimators. The new construction improves over recent work in the field inthat it does not assume a special structure on the design matrix. We test ourmethod on synthetic data and a high-throughput genomic data set aboutriboflavin production rate.
arxiv-5700-129 | Equivalence of Kernel Machine Regression and Kernel Distance Covariance for Multidimensional Trait Association Studies | http://arxiv.org/pdf/1402.2679v2.pdf | author:Wen-Yu Hua, Debashis Ghosh category:stat.ML stat.ME published:2014-02-11 summary:Associating genetic markers with a multidimensional phenotype is an importantyet challenging problem. In this work, we establish the equivalence between twopopular methods: kernel-machine regression (KMR), and kernel distancecovariance (KDC). KMR is a semiparametric regression frameworks that models thecovariate effects parametrically, while the genetic markers are considerednon-parametrically. KDC represents a class of methods that includes distancecovariance (DC) and Hilbert-Schmidt Independence Criterion (HSIC), which arenonparametric tests of independence. We show the equivalence between the scoretest of KMR and the KDC statistic under certain conditions. This result leadsto a novel generalization of the KDC test that incorporates the covariates. Ourcontributions are three-fold: (1) establishing the equivalence between KMR andKDC; (2) showing that the principles of kernel machine regression can beapplied to the interpretation of KDC; (3) the development of a broader class ofKDC statistics, that the members are the quantities of different kernels. Wedemonstrate the proposals using simulation studies. Data from the Alzheimer'sDisease Neuroimaging Initiative (ADNI) is used to explore the associationbetween the genetic variants on gene \emph{FLJ16124} and phenotypes representedin 3D structural brain MR images adjusting for age and gender. The resultssuggest that SNPs of \emph{FLJ16124} exhibit strong pairwise interactioneffects that are correlated to the changes of brain region volumes.
arxiv-5700-130 | A Deep Representation for Invariance And Music Classification | http://arxiv.org/pdf/1404.0400v1.pdf | author:Chiyuan Zhang, Georgios Evangelopoulos, Stephen Voinea, Lorenzo Rosasco, Tomaso Poggio category:cs.SD cs.LG stat.ML published:2014-04-01 summary:Representations in the auditory cortex might be based on mechanisms similarto the visual ventral stream; modules for building invariance totransformations and multiple layers for compositionality and selectivity. Inthis paper we propose the use of such computational modules for extractinginvariant and discriminative audio representations. Building on a theory ofinvariance in hierarchical architectures, we propose a novel, mid-levelrepresentation for acoustical signals, using the empirical distributions ofprojections on a set of templates and their transformations. Under theassumption that, by construction, this dictionary of templates is composed fromsimilar classes, and samples the orbit of variance-inducing signaltransformations (such as shift and scale), the resulting signature istheoretically guaranteed to be unique, invariant to transformations and stableto deformations. Modules of projection and pooling can then constitute layersof deep networks, for learning composite representations. We present the maintheoretical and computational aspects of a framework for unsupervised learningof invariant audio representations, empirically evaluated on music genreclassification.
arxiv-5700-131 | Toward computational cumulative biology by combining models of biological datasets | http://arxiv.org/pdf/1404.0329v1.pdf | author:Ali Faisal, Jaakko Peltonen, Elisabeth Georgii, Johan Rung, Samuel Kaski category:q-bio.QM q-bio.GN stat.ML published:2014-04-01 summary:A main challenge of data-driven sciences is how to make maximal use of theprogressively expanding databases of experimental datasets in order to keepresearch cumulative. We introduce the idea of a modeling-based datasetretrieval engine designed for relating a researcher's experimental dataset toearlier work in the field. The search is (i) data-driven to enable newfindings, going beyond the state of the art of keyword searches in annotations,(ii) modeling-driven, to both include biological knowledge and insights learnedfrom data, and (iii) scalable, as it is accomplished without building oneunified grand model of all data. Assuming each dataset has been modeledbeforehand, by the researchers or by database managers, we apply a rapidlycomputable and optimizable combination model to decompose a new dataset intocontributions from earlier relevant models. By using the data-drivendecomposition we identify a network of interrelated datasets from a largeannotated human gene expression atlas. While tissue type and disease were majordriving forces for determining relevant datasets, the found relationships werericher and the model-based search was more accurate than keyword search; itmoreover recovered biologically meaningful relationships that are notstraightforwardly visible from annotations, for instance, between cells indifferent developmental stages such as thymocytes and T-cells. Data-drivenlinks and citations matched to a large extent; the data-driven links evenuncovered corrections to the publication data, as two of the most linkeddatasets were not highly cited and turned out to have wrong publication entriesin the database.
arxiv-5700-132 | Stochastic processes and feedback-linearisation for online identification and Bayesian adaptive control of fully-actuated mechanical systems | http://arxiv.org/pdf/1311.4468v3.pdf | author:Jan-Peter Calliess, Antonis Papachristodoulou, Stephen J. Roberts category:cs.LG cs.SY stat.ML published:2013-11-18 summary:This work proposes a new method for simultaneous probabilistic identificationand control of an observable, fully-actuated mechanical system. Identificationis achieved by conditioning stochastic process priors on observations ofconfigurations and noisy estimates of configuration derivatives. In contrast toprevious work that has used stochastic processes for identification, weleverage the structural knowledge afforded by Lagrangian mechanics and learnthe drift and control input matrix functions of the control-affine systemseparately. We utilise feedback-linearisation to reduce, in expectation, theuncertain nonlinear control problem to one that is easy to regulate in adesired manner. Thereby, our method combines the flexibility of nonparametricBayesian learning with epistemological guarantees on the expected closed-looptrajectory. We illustrate our method in the context of torque-actuated pendulawhere the dynamics are learned with a combination of normal and log-normalprocesses.
arxiv-5700-133 | Household Electricity Demand Forecasting -- Benchmarking State-of-the-Art Methods | http://arxiv.org/pdf/1404.0200v1.pdf | author:Andreas Veit, Christoph Goebel, Rohit Tidke, Christoph Doblander, Hans-Arno Jacobsen category:cs.LG stat.AP I.2.6 published:2014-04-01 summary:The increasing use of renewable energy sources with variable output, such assolar photovoltaic and wind power generation, calls for Smart Grids thateffectively manage flexible loads and energy storage. The ability to forecastconsumption at different locations in distribution systems will be a keycapability of Smart Grids. The goal of this paper is to benchmarkstate-of-the-art methods for forecasting electricity demand on the householdlevel across different granularities and time scales in an explorative way,thereby revealing potential shortcomings and find promising directions forfuture research in this area. We apply a number of forecasting methodsincluding ARIMA, neural networks, and exponential smoothening using severalstrategies for training data selection, in particular day type and slidingwindow based strategies. We consider forecasting horizons ranging between 15minutes and 24 hours. Our evaluation is based on two data sets containing thepower usage of individual appliances at second time granularity collected overthe course of several months. The results indicate that forecasting accuracyvaries significantly depending on the choice of forecasting methods/strategyand the parameter configuration. Measured by the Mean Absolute Percentage Error(MAPE), the considered state-of-the-art forecasting methods rarely beatcorresponding persistence forecasts. Overall, we observed MAPEs in the rangebetween 5 and >100%. The average MAPE for the first data set was ~30%, while itwas ~85% for the other data set. These results show big room for improvement.Based on the identified trends and experiences from our experiments, wecontribute a detailed discussion of promising future research.
arxiv-5700-134 | Efficient Algorithms and Error Analysis for the Modified Nystrom Method | http://arxiv.org/pdf/1404.0138v1.pdf | author:Shusen Wang, Zhihua Zhang category:cs.LG published:2014-04-01 summary:Many kernel methods suffer from high time and space complexities and are thusprohibitive in big-data applications. To tackle the computational challenge,the Nystr\"om method has been extensively used to reduce time and spacecomplexities by sacrificing some accuracy. The Nystr\"om method speedupscomputation by constructing an approximation of the kernel matrix using only afew columns of the matrix. Recently, a variant of the Nystr\"om method calledthe modified Nystr\"om method has demonstrated significant improvement over thestandard Nystr\"om method in approximation accuracy, both theoretically andempirically. In this paper, we propose two algorithms that make the modified Nystr\"ommethod practical. First, we devise a simple column selection algorithm with aprovable error bound. Our algorithm is more efficient and easier to implementthan and nearly as accurate as the state-of-the-art algorithm. Second, with theselected columns at hand, we propose an algorithm that computes theapproximation in lower time complexity than the approach in the previous work.Furthermore, we prove that the modified Nystr\"om method is exact under certainconditions, and we establish a lower error bound for the modified Nystr\"ommethod.
arxiv-5700-135 | Traffic Monitoring Using M2M Communication | http://arxiv.org/pdf/1404.0106v1.pdf | author:Shiu Kumar, Eun Sik Ham, Seong Ro Lee category:cs.CV published:2014-04-01 summary:This paper presents an intelligent traffic monitoring system using wirelessvision sensor network that captures and processes the real-time video image toobtain the traffic flow rate and vehicle speeds along different urban roadways.This system will display the traffic states on the front roadways that canguide the drivers to select the right way and avoid potential trafficcongestions. On the other hand, it will also monitor the vehicle speeds andstore the vehicle details, for those breaking the roadway speed limits, in itsdatabase. The real-time traffic data is processed by the Personal Computer (PC)at the sub roadway station and the traffic flow rate data is transmitted to themain roadway station Arduino 3G via email, where the data is extracted andtraffic flow rate displayed.
arxiv-5700-136 | Venture: a higher-order probabilistic programming platform with programmable inference | http://arxiv.org/pdf/1404.0099v1.pdf | author:Vikash Mansinghka, Daniel Selsam, Yura Perov category:cs.AI cs.PL stat.CO stat.ML published:2014-04-01 summary:We describe Venture, an interactive virtual machine for probabilisticprogramming that aims to be sufficiently expressive, extensible, and efficientfor general-purpose use. Like Church, probabilistic models and inferenceproblems in Venture are specified via a Turing-complete, higher-orderprobabilistic language descended from Lisp. Unlike Church, Venture alsoprovides a compositional language for custom inference strategies built out ofscalable exact and approximate techniques. We also describe four key aspects ofVenture's implementation that build on ideas from probabilistic graphicalmodels. First, we describe the stochastic procedure interface (SPI) thatspecifies and encapsulates primitive random variables. The SPI supports customcontrol flow, higher-order probabilistic procedures, partially exchangeablesequences and ``likelihood-free'' stochastic simulators. It also supportsexternal models that do inference over latent variables hidden from Venture.Second, we describe probabilistic execution traces (PETs), which representexecution histories of Venture programs. PETs capture conditional dependencies,existential dependencies and exchangeable coupling. Third, we describepartitions of execution histories called scaffolds that factor global inferenceproblems into coherent sub-problems. Finally, we describe a family ofstochastic regeneration algorithms for efficiently modifying PET fragmentscontained within scaffolds. Stochastic regeneration linear runtime scaling incases where many previous approaches scaled quadratically. We show how to usestochastic regeneration and the SPI to implement general-purpose inferencestrategies such as Metropolis-Hastings, Gibbs sampling, and blocked proposalsbased on particle Markov chain Monte Carlo and mean-field variational inferencetechniques.
arxiv-5700-137 | Using HMM in Strategic Games | http://arxiv.org/pdf/1404.0086v1.pdf | author:Mario Benevides, Isaque Lima, Rafael Nader, Pedro Rougemont category:cs.GT cs.IR cs.LG I.2.8 published:2014-04-01 summary:In this paper we describe an approach to resolve strategic games in whichplayers can assume different types along the game. Our goal is to infer whichtype the opponent is adopting at each moment so that we can increase theplayer's odds. To achieve that we use Markov games combined with hidden Markovmodel. We discuss a hypothetical example of a tennis game whose solution can beapplied to any game with similar characteristics.
arxiv-5700-138 | Simultaneous Perturbation Algorithms for Batch Off-Policy Search | http://arxiv.org/pdf/1403.4514v2.pdf | author:Raphael Fonteneau, L. A. Prashanth category:math.OC cs.LG published:2014-03-18 summary:We propose novel policy search algorithms in the context of off-policy, batchmode reinforcement learning (RL) with continuous state and action spaces. Givena batch collection of trajectories, we perform off-line policy evaluation usingan algorithm similar to that by [Fonteneau et al., 2010]. Using thisMonte-Carlo like policy evaluator, we perform policy search in a class ofparameterized policies. We propose both first order policy gradient and secondorder policy Newton algorithms. All our algorithms incorporate simultaneousperturbation estimates for the gradient as well as the Hessian of thecost-to-go vector, since the latter is unknown and only biased estimates areavailable. We demonstrate their practicality on a simple 1-dimensionalcontinuous state space problem.
arxiv-5700-139 | Coding for Random Projections and Approximate Near Neighbor Search | http://arxiv.org/pdf/1403.8144v1.pdf | author:Ping Li, Michael Mitzenmacher, Anshumali Shrivastava category:cs.LG cs.DB cs.DS stat.CO published:2014-03-31 summary:This technical note compares two coding (quantization) schemes for randomprojections in the context of sub-linear time approximate near neighbor search.The first scheme is based on uniform quantization while the second schemeutilizes a uniform quantization plus a uniformly random offset (which has beenpopular in practice). The prior work compared the two schemes in the context ofsimilarity estimation and training linear classifiers, with the conclusion thatthe step of random offset is not necessary and may hurt the performance(depending on the similarity level). The task of near neighbor search isrelated to similarity estimation with importance distinctions and requires ownstudy. In this paper, we demonstrate that in the context of near neighborsearch, the step of random offset is not needed either and may hurt theperformance (sometimes significantly so, depending on the similarity and otherparameters).
arxiv-5700-140 | Privacy Tradeoffs in Predictive Analytics | http://arxiv.org/pdf/1403.8084v1.pdf | author:Stratis Ioannidis, Andrea Montanari, Udi Weinsberg, Smriti Bhagat, Nadia Fawaz, Nina Taft category:cs.CR cs.LG published:2014-03-31 summary:Online services routinely mine user data to predict user preferences, makerecommendations, and place targeted ads. Recent research has demonstrated thatseveral private user attributes (such as political affiliation, sexualorientation, and gender) can be inferred from such data. Can aprivacy-conscious user benefit from personalization while simultaneouslyprotecting her private attributes? We study this question in the context of arating prediction service based on matrix factorization. We construct aprotocol of interactions between the service and users that has remarkableoptimality properties: it is privacy-preserving, in that no inference algorithmcan succeed in inferring a user's private attribute with a probability betterthan random guessing; it has maximal accuracy, in that no otherprivacy-preserving protocol improves rating prediction; and, finally, itinvolves a minimal disclosure, as the prediction accuracy strictly decreaseswhen the service reveals less information. We extensively evaluate our protocolusing several rating datasets, demonstrating that it successfully blocks theinference of gender, age and political affiliation, while incurring less than5% decrease in the accuracy of rating prediction.
arxiv-5700-141 | Probabilistic Intra-Retinal Layer Segmentation in 3-D OCT Images Using Global Shape Regularization | http://arxiv.org/pdf/1403.8003v1.pdf | author:Fabian Rathke, Stefan Schmidt, Christoph Schnörr category:cs.CV published:2014-03-31 summary:With the introduction of spectral-domain optical coherence tomography (OCT),resulting in a significant increase in acquisition speed, the fast and accuratesegmentation of 3-D OCT scans has become evermore important. This paperpresents a novel probabilistic approach, that models the appearance of retinallayers as well as the global shape variations of layer boundaries. Given an OCTscan, the full posterior distribution over segmentations is approximatelyinferred using a variational method enabling efficient probabilistic inferencein terms of computationally tractable model components: Segmenting a full 3-Dvolume takes around a minute. Accurate segmentations demonstrate the benefit ofusing global shape regularization: We segmented 35 fovea-centered 3-D volumeswith an average unsigned error of 2.46 $\pm$ 0.22 {\mu}m as well as 80 normaland 66 glaucomatous 2-D circular scans with errors of 2.92 $\pm$ 0.53 {\mu}mand 4.09 $\pm$ 0.98 {\mu}m respectively. Furthermore, we utilized the inferredposterior distribution to rate the quality of the segmentation, point outpotentially erroneous regions and discriminate normal from pathological scans.No pre- or postprocessing was required and we used the same set of parametersfor all data sets, underlining the robustness and out-of-the-box nature of ourapproach.
arxiv-5700-142 | Implementation of an Automatic Sign Language Lexical Annotation Framework based on Propositional Dynamic Logic | http://arxiv.org/pdf/1403.6392v2.pdf | author:Arturo Curiel, Christophe Collet category:cs.CL I.2.7 published:2014-03-25 summary:In this paper, we present the implementation of an automatic Sign Language(SL) sign annotation framework based on a formal logic, the PropositionalDynamic Logic (PDL). Our system relies heavily on the use of a specific variantof PDL, the Propositional Dynamic Logic for Sign Language (PDLSL), which letsus describe SL signs as formulae and corpora videos as labeled transitionsystems (LTSs). Here, we intend to show how a generic annotation system can beconstructed upon these underlying theoretical principles, regardless of thetracking technologies available or the input format of corpora. With this inmind, we generated a development framework that adapts the system to specificuse cases. Furthermore, we present some results obtained by our applicationwhen adapted to one distinct case, 2D corpora analysis with pre-processedtracking information. We also present some insights on how such a technologycan be used to analyze 3D real-time data, captured with a depth device.
arxiv-5700-143 | Between Sense and Sensibility: Declarative narrativisation of mental models as a basis and benchmark for visuo-spatial cognition and computation focussed collaborative cognitive systems | http://arxiv.org/pdf/1307.3040v2.pdf | author:Mehul Bhatt category:cs.AI cs.CL cs.CV cs.HC cs.RO published:2013-07-11 summary:What lies between `\emph{sensing}' and `\emph{sensibility}'? In other words,what kind of cognitive processes mediate sensing capability, and the formationof sensible impressions ---e.g., abstractions, analogies, hypotheses and theoryformation, beliefs and their revision, argument formation--- in domain-specificproblem solving, or in regular activities of everyday living, working andsimply going around in the environment? How can knowledge and reasoning aboutsuch capabilities, as exhibited by humans in particular problem contexts, beused as a model and benchmark for the development of collaborative cognitive(interaction) systems concerned with human assistance, assurance, andempowerment? We pose these questions in the context of a range of assistive technologiesconcerned with \emph{visuo-spatial perception and cognition} tasks encompassingaspects such as commonsense, creativity, and the application of specialistdomain knowledge and problem-solving thought processes. Assistive technologiesbeing considered include: (a) human activity interpretation; (b) high-levelcognitive rovotics; (c) people-centred creative design in domains such asarchitecture & digital media creation, and (d) qualitative analyses geographicinformation systems. Computational narratives not only provide a rich cognitivebasis, but they also serve as a benchmark of functional performance in ourdevelopment of computational cognitive assistance systems. We posit thatcomputational narrativisation pertaining to space, actions, and change providesa useful model of \emph{visual} and \emph{spatio-temporal thinking} within awide-range of problem-solving tasks and application areas where collaborativecognitive systems could serve an assistive and empowering function.
arxiv-5700-144 | Sparse K-Means with $\ell_{\infty}/\ell_0$ Penalty for High-Dimensional Data Clustering | http://arxiv.org/pdf/1403.7890v1.pdf | author:Xiangyu Chang, Yu Wang, Rongjian Li, Zongben Xu category:stat.ML cs.LG stat.ME published:2014-03-31 summary:Sparse clustering, which aims to find a proper partition of an extremelyhigh-dimensional data set with redundant noise features, has been attractedmore and more interests in recent years. The existing studies commonly solvethe problem in a framework of maximizing the weighted feature contributionssubject to a $\ell_2/\ell_1$ penalty. Nevertheless, this framework has twoserious drawbacks: One is that the solution of the framework unavoidablyinvolves a considerable portion of redundant noise features in many situations,and the other is that the framework neither offers intuitive explanations onwhy this framework can select relevant features nor leads to any theoreticalguarantee for feature selection consistency. In this article, we attempt to overcome those drawbacks through developing anew sparse clustering framework which uses a $\ell_{\infty}/\ell_0$ penalty.First, we introduce new concepts on optimal partitions and noise features forthe high-dimensional data clustering problems, based on which the previouslyknown framework can be intuitively explained in principle. Then, we apply thesuggested $\ell_{\infty}/\ell_0$ framework to formulate a new sparse k-meansmodel with the $\ell_{\infty}/\ell_0$ penalty ($\ell_0$-k-means for short). Wepropose an efficient iterative algorithm for solving the $\ell_0$-k-means. Todeeply understand the behavior of $\ell_0$-k-means, we prove that the solutionyielded by the $\ell_0$-k-means algorithm has feature selection consistencywhenever the data matrix is generated from a high-dimensional Gaussian mixturemodel. Finally, we provide experiments with both synthetic data and the AllenDeveloping Mouse Brain Atlas data to support that the proposed $\ell_0$-k-meansexhibits better noise feature detection capacity over the previously knownsparse k-means with the $\ell_2/\ell_1$ penalty ($\ell_1$-k-means for short).
arxiv-5700-145 | Particle filter-based Gaussian process optimisation for parameter inference | http://arxiv.org/pdf/1311.0689v2.pdf | author:Johan Dahlin, Fredrik Lindsten category:stat.CO stat.ML published:2013-11-04 summary:We propose a novel method for maximum likelihood-based parameter inference innonlinear and/or non-Gaussian state space models. The method is an iterativeprocedure with three steps. At each iteration a particle filter is used toestimate the value of the log-likelihood function at the current parameteriterate. Using these log-likelihood estimates, a surrogate objective functionis created by utilizing a Gaussian process model. Finally, we use a heuristicprocedure to obtain a revised parameter iterate, providing an automatictrade-off between exploration and exploitation of the surrogate model. Themethod is profiled on two state space models with good performance bothconsidering accuracy and computational cost.
arxiv-5700-146 | Correlation Filters with Limited Boundaries | http://arxiv.org/pdf/1403.7876v1.pdf | author:Hamed Kiani Galoogahi, Terence Sim, Simon Lucey category:cs.CV published:2014-03-31 summary:Correlation filters take advantage of specific properties in the Fourierdomain allowing them to be estimated efficiently: O(NDlogD) in the frequencydomain, versus O(D^3 + ND^2) spatially where D is signal length, and N is thenumber of signals. Recent extensions to correlation filters, such as MOSSE,have reignited interest of their use in the vision community due to theirrobustness and attractive computational properties. In this paper wedemonstrate, however, that this computational efficiency comes at a cost.Specifically, we demonstrate that only 1/D proportion of shifted examples areunaffected by boundary effects which has a dramatic effect ondetection/tracking performance. In this paper, we propose a novel approach tocorrelation filter estimation that: (i) takes advantage of inherentcomputational redundancies in the frequency domain, and (ii) dramaticallyreduces boundary effects. Impressive object tracking and detection results arepresented in terms of both accuracy and computational efficiency.
arxiv-5700-147 | A probabilistic estimation and prediction technique for dynamic continuous social science models: The evolution of the attitude of the Basque Country population towards ETA as a case study | http://arxiv.org/pdf/1404.0649v1.pdf | author:Juan-Carlos Cortés, Francisco-J. Santonja, Ana-C. Tarazona, Rafael-J. Villanueva, Javier Villanueva-Oller category:cs.LG published:2014-03-30 summary:In this paper, we present a computational technique to deal with uncertaintyin dynamic continuous models in Social Sciences. Considering data from surveys,the method consists of determining the probability distribution of the surveyoutput and this allows to sample data and fit the model to the sampled datausing a goodness-of-fit criterion based on the chi-square-test. Taking thefitted parameters non-rejected by the chi-square-test, substituting them intothe model and computing their outputs, we build 95% confidence intervals ineach time instant capturing uncertainty of the survey data (probabilisticestimation). Using the same set of obtained model parameters, we also provide aprediction over the next few years with 95% confidence intervals (probabilisticprediction). This technique is applied to a dynamic social model describing theevolution of the attitude of the Basque Country population towards therevolutionary organization ETA.
arxiv-5700-148 | Exact Subspace Segmentation and Outlier Detection by Low-Rank Representation | http://arxiv.org/pdf/1109.1646v3.pdf | author:Guangcan Liu, Huan Xu, Shuicheng Yan category:cs.IT cs.CV math.IT published:2011-09-08 summary:In this work, we address the following matrix recovery problem: suppose weare given a set of data points containing two parts, one part consists ofsamples drawn from a union of multiple subspaces and the other part consists ofoutliers. We do not know which data points are outliers, or how many outliersthere are. The rank and number of the subspaces are unknown either. Can wedetect the outliers and segment the samples into their right subspaces,efficiently and exactly? We utilize a so-called {\em Low-Rank Representation}(LRR) method to solve this problem, and prove that under mild technicalconditions, any solution to LRR exactly recovers the row space of the samplesand detect the outliers as well. Since the subspace membership is provablydetermined by the row space, this further implies that LRR can perform exactsubspace segmentation and outlier detection, in an efficient way.
arxiv-5700-149 | Bio-Inspired Computation: Success and Challenges of IJBIC | http://arxiv.org/pdf/1403.7795v1.pdf | author:Xin-She Yang, Zhihua Cui category:math.OC cs.NE 78M32 published:2014-03-30 summary:It is now five years since the launch of the International Journal ofBio-Inspired Computation (IJBIC). At the same time, significant new progresshas been made in the area of bio-inspired computation. This review papersummarizes the success and achievements of IJBIC in the past five years, andalso highlights the challenges and key issues for further research.
arxiv-5700-150 | True Global Optimality of the Pressure Vessel Design Problem: A Benchmark for Bio-Inspired Optimisation Algorithms | http://arxiv.org/pdf/1403.7793v1.pdf | author:Xin-She Yang, Christian Huyck, Mehmet Karamanoglu, Nawaz Khan category:math.OC cs.NE nlin.AO published:2014-03-30 summary:The pressure vessel design problem is a well-known design benchmark forvalidating bio-inspired optimization algorithms. However, its global optimalityis not clear and there has been no mathematical proof put forward. In thispaper, a detailed mathematical analysis of this problem is provided that provesthat 6059.714335048436 is the global minimum. The Lagrange multiplier method isalso used as an alternative proof and this method is extended to find theglobal optimum of a cantilever beam design problem.
arxiv-5700-151 | Swarm Intelligence Based Algorithms: A Critical Analysis | http://arxiv.org/pdf/1403.7792v1.pdf | author:Xin-She Yang category:math.OC cs.NE nlin.AO 78M32 published:2014-03-30 summary:Many optimization algorithms have been developed by drawing inspiration fromswarm intelligence (SI). These SI-based algorithms can have some advantagesover traditional algorithms. In this paper, we carry out a critical analysis ofthese SI-based algorithms by analyzing their ways to mimic evolutionaryoperators. We also analyze the ways of achieving exploration and exploitationin algorithms by using mutation, crossover and selection. In addition, we alsolook at algorithms using dynamic systems, self-organization and Markov chainframework. Finally, we provide some discussions and topics for furtherresearch.
arxiv-5700-152 | Extraction of Line Word Character Segments Directly from Run Length Compressed Printed Text Documents | http://arxiv.org/pdf/1403.7783v1.pdf | author:Mohammed Javed, P. Nagabhushan, B. B. Chaudhuri category:cs.CV published:2014-03-30 summary:Segmentation of a text-document into lines, words and characters, which isconsidered to be the crucial pre-processing stage in Optical CharacterRecognition (OCR) is traditionally carried out on uncompressed documents,although most of the documents in real life are available in compressed form,for the reasons such as transmission and storage efficiency. However, thisimplies that the compressed image should be decompressed, which indentsadditional computing resources. This limitation has motivated us to take upresearch in document image analysis using compressed documents. In this paper,we think in a new way to carry out segmentation at line, word and characterlevel in run-length compressed printed-text-documents. We extract thehorizontal projection profile curve from the compressed file and using thelocal minima points perform line segmentation. However, tracing verticalinformation which leads to tracking words-characters in a run-length compressedfile is not very straight forward. Therefore, we propose a novel technique forcarrying out simultaneous word and character segmentation by popping out columnruns from each row in an intelligent sequence. The proposed algorithms havebeen validated with 1101 text-lines, 1409 words and 7582 characters from adata-set of 35 noise and skew free compressed documents of Bengali, Kannada andEnglish Scripts.
arxiv-5700-153 | Multi-label Ferns for Efficient Recognition of Musical Instruments in Recordings | http://arxiv.org/pdf/1403.7746v1.pdf | author:Miron B. Kursa, Alicja A. Wieczorkowska category:cs.LG cs.SD published:2014-03-30 summary:In this paper we introduce multi-label ferns, and apply this technique forautomatic classification of musical instruments in audio recordings. We comparethe performance of our proposed method to a set of binary random ferns, usingjazz recordings as input data. Our main result is obtaining much fasterclassification and higher F-score. We also achieve substantial reduction of themodel size.
arxiv-5700-154 | Relevant Feature Selection Model Using Data Mining for Intrusion Detection System | http://arxiv.org/pdf/1403.7726v1.pdf | author:Ayman I. Madbouly, Amr M. Gody, Tamer M. Barakat category:cs.CR cs.LG published:2014-03-30 summary:Network intrusions have become a significant threat in recent years as aresult of the increased demand of computer networks for critical systems.Intrusion detection system (IDS) has been widely deployed as a defense measurefor computer networks. Features extracted from network traffic can be used assign to detect anomalies. However with the huge amount of network traffic,collected data contains irrelevant and redundant features that affect thedetection rate of the IDS, consumes high amount of system resources, andslowdown the training and testing process of the IDS. In this paper, a newfeature selection model is proposed; this model can effectively select the mostrelevant features for intrusion detection. Our goal is to build a lightweightintrusion detection system by using a reduced features set. Deleting irrelevantand redundant features helps to build a faster training and testing process, tohave less resource consumption as well as to maintain high detection rates. Theeffectiveness and the feasibility of our feature selection model were verifiedby several experiments on KDD intrusion detection dataset. The experimentalresults strongly showed that our model is not only able to yield high detectionrates but also to speed up the detection process.
arxiv-5700-155 | Hybrid Q-Learning Applied to Ubiquitous recommender system | http://arxiv.org/pdf/1303.2651v2.pdf | author:Djallel Bouneffouf category:cs.LG cs.IR I.2 published:2013-03-10 summary:Ubiquitous information access becomes more and more important nowadays andresearch is aimed at making it adapted to users. Our work consists in applyingmachine learning techniques in order to bring a solution to some of theproblems concerning the acceptance of the system by users. To achieve this, wepropose a fundamental shift in terms of how we model the learning ofrecommender system: inspired by models of human reasoning developed in robotic,we combine reinforcement learning and case-base reasoning to define arecommendation process that uses these two approaches for generatingrecommendations on different context dimensions (social, temporal, geographic).We describe an implementation of the recommender system based on thisframework. We also present preliminary results from experiments with the systemand show how our approach increases the recommendation quality.
arxiv-5700-156 | Approximate Matrix Multiplication with Application to Linear Embeddings | http://arxiv.org/pdf/1403.7683v1.pdf | author:Anastasios Kyrillidis, Michail Vlachos, Anastasios Zouzias category:math.ST cs.IT math.IT stat.ML stat.TH published:2014-03-30 summary:In this paper, we study the problem of approximately computing the product oftwo real matrices. In particular, we analyze a dimensionality-reduction-basedapproximation algorithm due to Sarlos [1], introducing the notion of nuclearrank as the ratio of the nuclear norm over the spectral norm. The presentedbound has improved dependence with respect to the approximation error (ascompared to previous approaches), whereas the subspace -- on which we projectthe input matrices -- has dimensions proportional to the maximum of theirnuclear rank and it is independent of the input dimensions. In addition, weprovide an application of this result to linear low-dimensional embeddings.Namely, we show that any Euclidean point-set with bounded nuclear rank isamenable to projection onto number of dimensions that is independent of theinput dimensionality, while achieving additive error guarantees.
arxiv-5700-157 | Building A Large Concept Bank for Representing Events in Video | http://arxiv.org/pdf/1403.7591v1.pdf | author:Yin Cui, Dong Liu, Jiawei Chen, Shih-Fu Chang category:cs.MM cs.CV cs.IR H.3.1 published:2014-03-29 summary:Concept-based video representation has proven to be effective in complexevent detection. However, existing methods either manually design concepts ordirectly adopt concept libraries not specifically designed for events. In thispaper, we propose to build Concept Bank, the largest concept library consistingof 4,876 concepts specifically designed to cover 631 real-world events. Toconstruct the Concept Bank, we first gather a comprehensive event collectionfrom WikiHow, a collaborative writing project that aims to build the world'slargest manual for any possible How-To event. For each event, we then searchFlickr and discover relevant concepts from the tags of the returned images. Wetrain a Multiple Kernel Linear SVM for each discovered concept as a conceptdetector in Concept Bank. We organize the concepts into a five-layer treestructure, in which the higher-level nodes correspond to the event categorieswhile the leaf nodes are the event-specific concepts discovered for each event.Based on such tree ontology, we develop a semantic matching method to selectrelevant concepts for each textual event query, and then apply thecorresponding concept detectors to generate concept-based videorepresentations. We use TRECVID Multimedia Event Detection 2013 and ColumbiaConsumer Video open source event definitions and videos as our test sets andshow very promising results on two video event detection tasks: event modelingover concept space and zero-shot event retrieval. To the best of our knowledge,this is the largest concept library covering the largest number of real-worldevents.
arxiv-5700-158 | Scalable Robust Matrix Recovery: Frank-Wolfe Meets Proximal Methods | http://arxiv.org/pdf/1403.7588v1.pdf | author:Cun Mu, Yuqian Zhang, John Wright, Donald Goldfarb category:math.OC cs.CV cs.NA stat.ML published:2014-03-29 summary:Recovering matrices from compressive and grossly corrupted observations is afundamental problem in robust statistics, with rich applications in computervision and machine learning. In theory, under certain conditions, this problemcan be solved in polynomial time via a natural convex relaxation, known asCompressive Principal Component Pursuit (CPCP). However, all existing provablealgorithms for CPCP suffer from superlinear per-iteration cost, which severelylimits their applicability to large scale problems. In this paper, we proposeprovable, scalable and efficient methods to solve CPCP with (essentially)linear per-iteration cost. Our method combines classical ideas from Frank-Wolfeand proximal methods. In each iteration, we mainly exploit Frank-Wolfe toupdate the low-rank component with rank-one SVD and exploit the proximal stepfor the sparse term. Convergence results and implementation details are alsodiscussed. We demonstrate the scalability of the proposed approach withpromising numerical experiments on visual data.
arxiv-5700-159 | Confidence Intervals for Random Forests: The Jackknife and the Infinitesimal Jackknife | http://arxiv.org/pdf/1311.4555v2.pdf | author:Stefan Wager, Trevor Hastie, Bradley Efron category:stat.ML stat.CO stat.ME published:2013-11-18 summary:We study the variability of predictions made by bagged learners and randomforests, and show how to estimate standard errors for these methods. Our workbuilds on variance estimates for bagging proposed by Efron (1992, 2012) thatare based on the jackknife and the infinitesimal jackknife (IJ). In practice,bagged predictors are computed using a finite number B of bootstrap replicates,and working with a large B can be computationally expensive. Directapplications of jackknife and IJ estimators to bagging require B on the orderof n^{1.5} bootstrap replicates to converge, where n is the size of thetraining set. We propose improved versions that only require B on the order ofn replicates. Moreover, we show that the IJ estimator requires 1.7 times lessbootstrap replicates than the jackknife to achieve a given accuracy. Finally,we study the sampling distributions of the jackknife and IJ variance estimatesthemselves. We illustrate our findings with multiple experiments and simulationstudies.
arxiv-5700-160 | A sparse Kaczmarz solver and a linearized Bregman method for online compressed sensing | http://arxiv.org/pdf/1403.7543v1.pdf | author:Dirk A. Lorenz, Stephan Wenger, Frank Schöpfer, Marcus Magnor category:math.OC cs.CV cs.IT math.IT math.NA published:2014-03-28 summary:An algorithmic framework to compute sparse or minimal-TV solutions of linearsystems is proposed. The framework includes both the Kaczmarz method and thelinearized Bregman method as special cases and also several new methods such asa sparse Kaczmarz solver. The algorithmic framework has a variety ofapplications and is especially useful for problems in which the linearmeasurements are slow and expensive to obtain. We present examples for onlinecompressed sensing, TV tomographic reconstruction and radio interferometry.
arxiv-5700-161 | Hybrid Approach to English-Hindi Name Entity Transliteration | http://arxiv.org/pdf/1403.7455v1.pdf | author:Shruti Mathur, Varun Prakash Saxena category:cs.CL published:2014-03-28 summary:Machine translation (MT) research in Indian languages is still in itsinfancy. Not much work has been done in proper transliteration of name entitiesin this domain. In this paper we address this issue. We have used English-Hindilanguage pair for our experiments and have used a hybrid approach. At first wehave processed English words using a rule based approach which extractsindividual phonemes from the words and then we have applied statisticalapproach which converts the English into its equivalent Hindi phoneme and inturn the corresponding Hindi word. Through this approach we have attained83.40% accuracy.
arxiv-5700-162 | Distributed Reconstruction of Nonlinear Networks: An ADMM Approach | http://arxiv.org/pdf/1403.7429v1.pdf | author:Wei Pan, Aivar Sootla, Guy-Bart Stan category:math.OC cs.DC cs.LG cs.SY published:2014-03-28 summary:In this paper, we present a distributed algorithm for the reconstruction oflarge-scale nonlinear networks. In particular, we focus on the identificationfrom time-series data of the nonlinear functional forms and associatedparameters of large-scale nonlinear networks. Recently, a nonlinear networkreconstruction problem was formulated as a nonconvex optimisation problem basedon the combination of a marginal likelihood maximisation procedure withsparsity inducing priors. Using a convex-concave procedure (CCCP), an iterativereweighted lasso algorithm was derived to solve the initial nonconvexoptimisation problem. By exploiting the structure of the objective function ofthis reweighted lasso algorithm, a distributed algorithm can be designed. Tothis end, we apply the alternating direction method of multipliers (ADMM) todecompose the original problem into several subproblems. To illustrate theeffectiveness of the proposed methods, we use our approach to identify anetwork of interconnected Kuramoto oscillators with different network sizes(500~100,000 nodes).
arxiv-5700-163 | Lossy Compression via Sparse Linear Regression: Computationally Efficient Encoding and Decoding | http://arxiv.org/pdf/1212.1707v2.pdf | author:Ramji Venkataramanan, Tuhin Sarkar, Sekhar Tatikonda category:cs.IT math.IT stat.ML published:2012-12-07 summary:We propose computationally efficient encoders and decoders for lossycompression using a Sparse Regression Code. The codebook is defined by a designmatrix and codewords are structured linear combinations of columns of thismatrix. The proposed encoding algorithm sequentially chooses columns of thedesign matrix to successively approximate the source sequence. It is shown toachieve the optimal distortion-rate function for i.i.d Gaussian sources underthe squared-error distortion criterion. For a given rate, the parameters of thedesign matrix can be varied to trade off distortion performance with encodingcomplexity. An example of such a trade-off as a function of the block length nis the following. With computational resource (space or time) per source sampleof O((n/\log n)^2), for a fixed distortion-level above the Gaussiandistortion-rate function, the probability of excess distortion decaysexponentially in n. The Sparse Regression Code is robust in the followingsense: for any ergodic source, the proposed encoder achieves the optimaldistortion-rate function of an i.i.d Gaussian source with the same variance.Simulations show that the encoder has good empirical performance, especially atlow and moderate rates.
arxiv-5700-164 | Expectation-Maximization Technique and Spatial-Adaptation Applied to Pel-Recursive Motion Estimation | http://arxiv.org/pdf/1403.7365v1.pdf | author:Vania Vieira Estrela, Marcos Henrique da Silva Bassani category:cs.CV published:2014-03-28 summary:Pel-recursive motion estimation isa well-established approach. However, inthe presence of noise, it becomes an ill-posed problem that requiresregularization. In this paper, motion vectors are estimated in an iterativefashion by means of the Expectation-Maximization (EM) algorithm and a Gaussiandata model. Our proposed algorithm also utilizes the local image properties ofthe scene to improve the motion vector estimates following a spatially adaptiveapproach. Numerical experiments are presented that demonstrate the merits ofour method.
arxiv-5700-165 | Emotion Analysis Platform on Chinese Microblog | http://arxiv.org/pdf/1403.7335v1.pdf | author:Duyu Tang, Bing Qin, Ting Liu, Qiuhui Shi category:cs.CL cs.CY cs.IR published:2014-03-28 summary:Weibo, as the largest social media service in China, has billions of messagesgenerated every day. The huge number of messages contain rich sentimentalinformation. In order to analyze the emotional changes in accordance with timeand space, this paper presents an Emotion Analysis Platform (EAP), whichexplores the emotional distribution of each province, so that can monitor theglobal pulse of each province in China. The massive data of Weibo and thereal-time requirements make the building of EAP challenging. In order to solvethe above problems, emoticons, emotion lexicon and emotion-shifting rules areadopted in EAP to analyze the emotion of each tweet. In order to verify theeffectiveness of the platform, case study on the Sichuan earthquake is done,and the analysis result of the platform accords with the fact. In order toanalyze from quantity, we manually annotate a test set and conduct experimenton it. The experimental results show that the macro-Precision of EAP reaches80% and the EAP works effectively.
arxiv-5700-166 | Learning detectors quickly using structured covariance matrices | http://arxiv.org/pdf/1403.7321v1.pdf | author:Jack Valmadre, Sridha Sridharan, Simon Lucey category:cs.CV published:2014-03-28 summary:Computer vision is increasingly becoming interested in the rapid estimationof object detectors. Canonical hard negative mining strategies are slow as theyrequire multiple passes of the large negative training set. Recent work hasdemonstrated that if the distribution of negative examples is assumed to bestationary, then Linear Discriminant Analysis (LDA) can learn comparabledetectors without ever revisiting the negative set. Even with this insight,however, the time to learn a single object detector can still be on the orderof tens of seconds on a modern desktop computer. This paper proposes toleverage the resulting structured covariance matrix to obtain detectors withidentical performance in orders of magnitude less time and memory. We elucidatean important connection to the correlation filter literature, demonstratingthat these can also be trained without ever revisiting the negative set.
arxiv-5700-167 | Performance Evaluation of Raster Based Shape Vectors in Object Recognition | http://arxiv.org/pdf/1403.7311v1.pdf | author:Akbar Khan, Pratap Reddy L category:cs.CV published:2014-03-28 summary:Object recognition is still an impediment in the field of computer vision andmultimedia retrieval.Defining an object model is a critical task. Shapeinformation of an object play a critical role in the process of objectrecognition. Extraction of boundary information of an object from themultimedia data and classifying this information with associated objects is theprimary step towards object recognition. Rasters play an important role whilecomputing object boundary. The trade-off lies with the dimensionality of theobject versus computational cost while achieving maximum efficiency. In thistreatise an attempt is made to evaluate the performance of circular and spiralraster models in terms of average retrieval efficiency and computational cost.
arxiv-5700-168 | Data generator based on RBF network | http://arxiv.org/pdf/1403.7308v1.pdf | author:Marko Robnik-Šikonja category:stat.ML cs.AI cs.LG published:2014-03-28 summary:There are plenty of problems where the data available is scarce andexpensive. We propose a generator of semi-artificial data with similarproperties to the original data which enables development and testing ofdifferent data mining algorithms and optimization of their parameters. Thegenerated data allow a large scale experimentation and simulations withoutdanger of overfitting. The proposed generator is based on RBF networks whichlearn sets of Gaussian kernels. Learned Gaussian kernels can be used in agenerative mode to generate the data from the same distributions. To assesquality of the generated data we developed several workflows and used them toevaluate the statistical properties of the generated data, structuralsimilarity, and predictive similarity using supervised and unsupervisedlearning techniques. To determine usability of the proposed generator weconducted a large scale evaluation using 51 UCI data sets. The results show aconsiderable similarity between the original and generated data and indicatethat the method can be useful in several development and simulation scenarios.
arxiv-5700-169 | Random Binary Mappings for Kernel Learning and Efficient SVM | http://arxiv.org/pdf/1307.5161v2.pdf | author:Gemma Roig, Xavier Boix, Luc Van Gool category:cs.CV cs.LG stat.ML published:2013-07-19 summary:Support Vector Machines (SVMs) are powerful learners that have led tostate-of-the-art results in various computer vision problems. SVMs suffer fromvarious drawbacks in terms of selecting the right kernel, which depends on theimage descriptors, as well as computational and memory efficiency. This paperintroduces a novel kernel, which serves such issues well. The kernel is learnedby exploiting a large amount of low-complex, randomized binary mappings of theinput feature. This leads to an efficient SVM, while also alleviating the taskof kernel selection. We demonstrate the capabilities of our kernel on 6standard vision benchmarks, in which we combine several common imagedescriptors, namely histograms (Flowers17 and Daimler), attribute-likedescriptors (UCI, OSR, and a-VOC08), and Sparse Quantization (ImageNet).Results show that our kernel learning adapts well to the different descriptorstypes, achieving the performance of the kernels specifically tuned for eachimage descriptor, and with similar evaluation cost as efficient SVM methods.
arxiv-5700-170 | Non-Elitist Genetic Algorithm as a Local Search Method | http://arxiv.org/pdf/1307.3463v4.pdf | author:Anton Eremeev category:cs.NE published:2013-07-12 summary:Sufficient conditions are found under which the iterated non-elitist geneticalgorithm with tournament selection first visits a local optimum inpolynomially bounded time on average. It is shown that these conditions aresatisfied on a class of problems with guaranteed local optima (GLO) ifappropriate parameters of the algorithm are chosen.
arxiv-5700-171 | EXMOVES: Classifier-based Features for Scalable Action Recognition | http://arxiv.org/pdf/1312.5785v3.pdf | author:Du Tran, Lorenzo Torresani category:cs.CV published:2013-12-20 summary:This paper introduces EXMOVES, learned exemplar-based features for efficientrecognition of actions in videos. The entries in our descriptor are produced byevaluating a set of movement classifiers over spatial-temporal volumes of theinput sequence. Each movement classifier is a simple exemplar-SVM trained onlow-level features, i.e., an SVM learned using a single annotated positivespace-time volume and a large number of unannotated videos. Our representation offers two main advantages. First, since our mid-levelfeatures are learned from individual video exemplars, they require minimalamount of supervision. Second, we show that simple linear classification modelstrained on our global video descriptor yield action recognition accuracyapproaching the state-of-the-art but at orders of magnitude lower cost, sinceat test-time no sliding window is necessary and linear models are efficient totrain and test. This enables scalable action recognition, i.e., efficientclassification of a large number of different actions even in large videodatabases. We show the generality of our approach by building our mid-leveldescriptors from two different low-level feature representations. The accuracyand efficiency of the approach are demonstrated on several large-scale actionrecognition benchmarks.
arxiv-5700-172 | Systematic Ensemble Learning for Regression | http://arxiv.org/pdf/1403.7267v1.pdf | author:Roberto Aldave, Jean-Pierre Dussault category:stat.ML published:2014-03-28 summary:The motivation of this work is to improve the performance of standardstacking approaches or ensembles, which are composed of simple, heterogeneousbase models, through the integration of the generation and selection stages forregression problems. We propose two extensions to the standard stackingapproach. In the first extension we combine a set of standard stackingapproaches into an ensemble of ensembles using a two-step ensemble learning inthe regression setting. The second extension consists of two parts. In theinitial part a diversity mechanism is injected into the original training dataset, systematically generating different training subsets or partitions, andcorresponding ensembles of ensembles. In the final part after measuring thequality of the different partitions or ensembles, a max-min rule-basedselection algorithm is used to select the most appropriate ensemble/partitionon which to make the final prediction. We show, based on experiments over abroad range of data sets, that the second extension performs better than thebest of the standard stacking approaches, and is as good as the oracle ofdatabases, which has the best base model selected by cross-validation for eachdata set. In addition to that, the second extension performs better than twostate-of-the-art ensemble methods for regression, and it is as good as a thirdstate-of-the-art ensemble method.
arxiv-5700-173 | Accelerating MCMC via Parallel Predictive Prefetching | http://arxiv.org/pdf/1403.7265v1.pdf | author:Elaine Angelino, Eddie Kohler, Amos Waterland, Margo Seltzer, Ryan P. Adams category:stat.ML stat.CO published:2014-03-28 summary:We present a general framework for accelerating a large class of widely usedMarkov chain Monte Carlo (MCMC) algorithms. Our approach exploits fast,iterative approximations to the target density to speculatively evaluate manypotential future steps of the chain in parallel. The approach can acceleratecomputation of the target distribution of a Bayesian inference problem, withoutcompromising exactness, by exploiting subsets of data. It takes advantage ofwhatever parallel resources are available, but produces results exactlyequivalent to standard serial execution. In the initial burn-in phase of chainevaluation, it achieves speedup over serial evaluation that is close to linearin the number of available cores.
arxiv-5700-174 | Automated polyp detection in colon capsule endoscopy | http://arxiv.org/pdf/1305.1912v4.pdf | author:Alexander V. Mamonov, Isabel N. Figueiredo, Pedro N. Figueiredo, Yen-Hsi Richard Tsai category:cs.CV I.4.8 published:2013-05-08 summary:Colorectal polyps are important precursors to colon cancer, a major healthproblem. Colon capsule endoscopy (CCE) is a safe and minimally invasiveexamination procedure, in which the images of the intestine are obtained viadigital cameras on board of a small capsule ingested by a patient. The videosequence is then analyzed for the presence of polyps. We propose an algorithmthat relieves the labor of a human operator analyzing the frames in the videosequence. The algorithm acts as a binary classifier, which labels the frame aseither containing polyps or not, based on the geometrical analysis and thetexture content of the frame. The geometrical analysis is based on asegmentation of an image with the help of a mid-pass filter. The featuresextracted by the segmentation procedure are classified according to anassumption that the polyps are characterized as protrusions that are mostlyround in shape. Thus, we use a best fit ball radius as a decision parameter ofa binary classifier. We present a statistical study of the performance of ourapproach on a data set containing over 18,900 frames from the endoscopic videosequences of five adult patients. The algorithm demonstrates a solidperformance, achieving 47% sensitivity per frame and over 81% sensitivity perpolyp at a specificity level of 90%. On average, with a video sequence lengthof 3747 frames, only 367 false positive frames need to be inspected by a humanoperator.
arxiv-5700-175 | Offshore Wind Farm Layout Optimization Using Adapted Genetic Algorithm: A different perspective | http://arxiv.org/pdf/1403.7178v1.pdf | author:Feng Liu, Zhifang Wang category:cs.NE published:2014-03-27 summary:In this paper we study the problem of optimal layout of an offshore wind farmto minimize the wake effect impacts. Considering the specific requirements ofconcerned offshore wind farm, we propose an adaptive genetic algorithm (AGA)which introduces location swaps to replace random crossovers in conventionalGAs. That way the total number of turbines in the resulting layout will beeffectively kept to the initially specified value. We experiment the proposedAGA method on three cases with free wind speed of 12 m/s, 20 m/s, and a typicaloffshore wind distribution setting respectively. Numerical results verify theeffectiveness of our proposed algorithm which achieves a much fasterconvergence compared to conventional GA algorithms.
arxiv-5700-176 | Closed-Form Training of Conditional Random Fields for Large Scale Image Segmentation | http://arxiv.org/pdf/1403.7057v1.pdf | author:Alexander Kolesnikov, Matthieu Guillaumin, Vittorio Ferrari, Christoph H. Lampert category:cs.LG cs.CV published:2014-03-27 summary:We present LS-CRF, a new method for very efficient large-scale training ofConditional Random Fields (CRFs). It is inspired by existing closed-formexpressions for the maximum likelihood parameters of a generative graphicalmodel with tree topology. LS-CRF training requires only solving a set ofindependent regression problems, for which closed-form expression as well asefficient iterative solvers are available. This makes it orders of magnitudefaster than conventional maximum likelihood learning for CRFs that requirerepeated runs of probabilistic inference. At the same time, the models learnedby our method still allow for joint inference at test time. We apply LS-CRF tothe task of semantic image segmentation, showing that it is highly efficient,even for loopy models where probabilistic inference is problematic. It allowsthe training of image segmentation models from significantly larger trainingsets than had been used previously. We demonstrate this on two new datasetsthat form a second contribution of this paper. They consist of over 180,000images with figure-ground segmentation annotations. Our large-scale experimentsshow that the possibilities of CRF-based image segmentation are far fromexhausted, indicating, for example, that semi-supervised learning and the useof non-linear predictors are promising directions for achieving highersegmentation accuracy in the future.
arxiv-5700-177 | Compressive Pattern Matching on Multispectral Data | http://arxiv.org/pdf/1403.6958v1.pdf | author:S. Rousseau, D. Helbert, P. Carré, J. Blanc-Talon category:cs.CV published:2014-03-27 summary:We introduce a new constrained minimization problem that performs templateand pattern detection on a multispectral image in a compressive sensingcontext. We use an original minimization problem from Guo and Osher that uses$L_1$ minimization techniques to perform template detection in a multispectralimage. We first adapt this minimization problem to work with compressivesensing data. Then we extend it to perform pattern detection using a formaltransform called the spectralization along a pattern. That extension brings outthe problem of measurement reconstruction. We introduce shifted measurementsthat allow us to reconstruct all the measurement with a small overhead and wegive an optimality constraint for simple patterns. We present numerical resultsshowing the performances of the original minimization problem and thecompressed ones with different measurement rates and applied on remotely senseddata.
arxiv-5700-178 | Pyramidal Fisher Motion for Multiview Gait Recognition | http://arxiv.org/pdf/1403.6950v1.pdf | author:F. M. Castro, M. J. Marin-Jimenez, R. Medina-Carnicer category:cs.CV published:2014-03-27 summary:The goal of this paper is to identify individuals by analyzing their gait.Instead of using binary silhouettes as input data (as done in many previousworks) we propose and evaluate the use of motion descriptors based on denselysampled short-term trajectories. We take advantage of state-of-the-art peopledetectors to define custom spatial configurations of the descriptors around thetarget person. Thus, obtaining a pyramidal representation of the gait motion.The local motion features (described by the Divergence-Curl-Shear descriptor)extracted on the different spatial areas of the person are combined into asingle high-level gait descriptor by using the Fisher Vector encoding. Theproposed approach, coined Pyramidal Fisher Motion, is experimentally validatedon the recent `AVA Multiview Gait' dataset. The results show that this newapproach achieves promising results in the problem of gait recognition.
arxiv-5700-179 | An Optimization Method For Slice Interpolation Of Medical Images | http://arxiv.org/pdf/1402.0936v2.pdf | author:Ahmadreza Baghaie, Zeyun Yu category:cs.CV cs.CE published:2014-02-05 summary:Slice interpolation is a fast growing field in medical image processing.Intensity-based interpolation and object-based interpolation are two majorgroups of methods in the literature. In this paper, we describe anobject-oriented, optimization method based on a modified version ofcurvature-based image registration, in which a displacement field is computedfor the missing slice between two known slices and used to interpolate theintensities of the missing slice. The proposed approach is evaluatedquantitatively by using the Mean Squared Difference (MSD) as a metric. Theproduced results also show visual improvement in preserving sharp edges inimages.
arxiv-5700-180 | Automatic Segmentation of Broadcast News Audio using Self Similarity Matrix | http://arxiv.org/pdf/1403.6901v1.pdf | author:Sapna Soni, Ahmed Imran, Sunil Kumar Kopparapu category:cs.SD cs.LG cs.MM published:2014-03-27 summary:Generally audio news broadcast on radio is com- posed of music, commercials,news from correspondents and recorded statements in addition to the actual newsread by the newsreader. When news transcripts are available, automaticsegmentation of audio news broadcast to time align the audio with the texttranscription to build frugal speech corpora is essential. We address theproblem of identifying segmentation in the audio news broadcast correspondingto the news read by the newsreader so that they can be mapped to the texttranscripts. The existing techniques produce sub-optimal solutions when used toextract newsreader read segments. In this paper, we propose a new techniquewhich is able to identify the acoustic change points reliably using an acousticSelf Similarity Matrix (SSM). We describe the two pass technique in detail andverify its performance on real audio news broadcast of All India Radio fordifferent languages.
arxiv-5700-181 | Online Learning of k-CNF Boolean Functions | http://arxiv.org/pdf/1403.6863v1.pdf | author:Joel Veness, Marcus Hutter category:cs.LG published:2014-03-26 summary:This paper revisits the problem of learning a k-CNF Boolean function fromexamples in the context of online learning under the logarithmic loss. In doingso, we give a Bayesian interpretation to one of Valiant's celebrated PAClearning algorithms, which we then build upon to derive two efficient, online,probabilistic, supervised learning algorithms for predicting the output of anunknown k-CNF Boolean function. We analyze the loss of our methods, and showthat the cumulative log-loss can be upper bounded, ignoring logarithmicfactors, by a polynomial function of the size of each example.
arxiv-5700-182 | KPCA Spatio-temporal trajectory point cloud classifier for recognizing human actions in a CBVR system | http://arxiv.org/pdf/1403.6794v1.pdf | author:Iván Gómez-Conde, David N. Olivieri category:cs.IR cs.CV published:2014-03-26 summary:We describe a content based video retrieval (CBVR) software system foridentifying specific locations of a human action within a full length film, andretrieving similar video shots from a query. For this, we introduce the conceptof a trajectory point cloud for classifying unique actions, encoded in aspatio-temporal covariant eigenspace, where each point is characterized by itsspatial location, local Frenet-Serret vector basis, time averaged curvature andtorsion and the mean osculating hyperplane. Since each action can bedistinguished by their unique trajectories within this space, the trajectorypoint cloud is used to define an adaptive distance metric for classifyingqueries against stored actions. Depending upon the distance to othertrajectories, the distance metric uses either large scale structure of thetrajectory point cloud, such as the mean distance between cloud centroids orthe difference in hyperplane orientation, or small structure such as the timeaveraged curvature and torsion, to classify individual points in a fuzzy-KNN.Our system can function in real-time and has an accuracy greater than 93% formultiple action recognition within video repositories. We demonstrate the useof our CBVR system in two situations: by locating specific frame positions oftrained actions in two full featured films, and video shot retrieval from adatabase with a web search application.
arxiv-5700-183 | Optimized imaging using non-rigid registration | http://arxiv.org/pdf/1403.6774v1.pdf | author:Benjamin Berkels, Peter Binev, Douglas A. Blom, Wolfgang Dahmen, Robert C. Sharpley, Thomas Vogt category:cs.CV published:2014-03-26 summary:The extraordinary improvements of modern imaging devices offer access to datawith unprecedented information content. However, widely used image processingmethodologies fall far short of exploiting the full breadth of informationoffered by numerous types of scanning probe, optical, and electronmicroscopies. In many applications, it is necessary to keep measurementintensities below a desired threshold. We propose a methodology for extractingan increased level of information by processing a series of data setssuffering, in particular, from high degree of spatial uncertainty caused bycomplex multiscale motion during the acquisition process. An important role isplayed by a nonrigid pixel-wise registration method that can cope with lowsignal-to-noise ratios. This is accompanied by formulating objective qualitymeasures which replace human intervention and visual inspection in theprocessing chain. Scanning transmission electron microscopy of siliceouszeolite material exhibits the above-mentioned obstructions and therefore servesas orientation and a test of our procedures.
arxiv-5700-184 | Comparison of Multi-agent and Single-agent Inverse Learning on a Simulated Soccer Example | http://arxiv.org/pdf/1403.6822v1.pdf | author:Xiaomin Lin, Peter A. Beling, Randy Cogill category:cs.LG cs.GT published:2014-03-26 summary:We compare the performance of Inverse Reinforcement Learning (IRL) with therelative new model of Multi-agent Inverse Reinforcement Learning (MIRL). Beforecomparing the methods, we extend a published Bayesian IRL approach that is onlyapplicable to the case where the reward is only state dependent to a generalone capable of tackling the case where the reward depends on both state andaction. Comparison between IRL and MIRL is made in the context of an abstractsoccer game, using both a game model in which the reward depends only on stateand one in which it depends on both state and action. Results suggest that theIRL approach performs much worse than the MIRL approach. We speculate that theunderperformance of IRL is because it fails to capture equilibrium informationin the manner possible in MIRL.
arxiv-5700-185 | Beyond L2-Loss Functions for Learning Sparse Models | http://arxiv.org/pdf/1403.6706v1.pdf | author:Karthikeyan Natesan Ramamurthy, Aleksandr Y. Aravkin, Jayaraman J. Thiagarajan category:stat.ML cs.CV cs.LG math.OC I.2.6; G.1.6 published:2014-03-26 summary:Incorporating sparsity priors in learning tasks can give rise to simple, andinterpretable models for complex high dimensional data. Sparse models havefound widespread use in structure discovery, recovering data from corruptions,and a variety of large scale unsupervised and supervised learning problems.Assuming the availability of sufficient data, these methods infer dictionariesfor sparse representations by optimizing for high-fidelity reconstruction. Inmost scenarios, the reconstruction quality is measured using the squaredEuclidean distance, and efficient algorithms have been developed for both batchand online learning cases. However, new application domains motivate lookingbeyond conventional loss functions. For example, robust loss functions such as$\ell_1$ and Huber are useful in learning outlier-resilient models, and thequantile loss is beneficial in discovering structures that are therepresentative of a particular quantile. These new applications motivate ourwork in generalizing sparse learning to a broad class of convex loss functions.In particular, we consider the class of piecewise linear quadratic (PLQ) costfunctions that includes Huber, as well as $\ell_1$, quantile, Vapnik, hingeloss, and smoothed variants of these penalties. We propose an algorithm tolearn dictionaries and obtain sparse codes when the data reconstructionfidelity is measured using any smooth PLQ cost function. We provide convergenceguarantees for the proposed algorithm, and demonstrate the convergence behaviorusing empirical experiments. Furthermore, we present three case studies thatrequire the use of PLQ cost functions: (i) robust image modeling, (ii) tagrefinement for image annotation and retrieval and (iii) computing empiricalconfidence limits for subspace clustering.
arxiv-5700-186 | Sign Language Lexical Recognition With Propositional Dynamic Logic | http://arxiv.org/pdf/1403.6636v1.pdf | author:Arturo Curiel, Christophe Collet category:cs.CL I.2.7 published:2014-03-26 summary:This paper explores the use of Propositional Dynamic Logic (PDL) as asuitable formal framework for describing Sign Language (SL), the language ofdeaf people, in the context of natural language processing. SLs are visual,complete, standalone languages which are just as expressive as oral languages.Signs in SL usually correspond to sequences of highly specific body posturesinterleaved with movements, which make reference to real world objects,characters or situations. Here we propose a formal representation of SL signs,that will help us with the analysis of automatically-collected hand trackingdata from French Sign Language (FSL) video corpora. We further show how such arepresentation could help us with the design of computer aided SL verificationtools, which in turn would bring us closer to the development of an automaticrecognition system for these languages.
arxiv-5700-187 | QCMC: Quasi-conformal Parameterizations for Multiply-connected domains | http://arxiv.org/pdf/1403.6614v1.pdf | author:Kin Tat Ho, Lok Ming Lui category:cs.CG cs.CV math.DG published:2014-03-26 summary:This paper presents a method to compute the {\it quasi-conformalparameterization} (QCMC) for a multiply-connected 2D domain or surface. QCMCcomputes a quasi-conformal map from a multiply-connected domain $S$ onto apunctured disk $D_S$ associated with a given Beltrami differential. TheBeltrami differential, which measures the conformality distortion, is acomplex-valued function $\mu:S\to\mathbb{C}$ with supremum norm strictly lessthan 1. Every Beltrami differential gives a conformal structure of $S$. Hence,the conformal module of $D_S$, which are the radii and centers of the innercircles, can be fully determined by $\mu$, up to a M\"obius transformation. Inthis paper, we propose an iterative algorithm to simultaneously search for theconformal module and the optimal quasi-conformal parameterization. The key ideais to minimize the Beltrami energy subject to the boundary constraints. Theoptimal solution is our desired quasi-conformal parameterization onto apunctured disk. The parameterization of the multiply-connected domainsimplifies numerical computations and has important applications in variousfields, such as in computer graphics and vision. Experiments have been carriedout on synthetic data together with real multiply-connected Riemann surfaces.Results show that our proposed method can efficiently compute quasi-conformalparameterizations of multiply-connected domains and outperforms otherstate-of-the-art algorithms. Applications of the proposed parameterizationtechnique have also been explored.
arxiv-5700-188 | Can a biologically-plausible hierarchy effectively replace face detection, alignment, and recognition pipelines? | http://arxiv.org/pdf/1311.4082v3.pdf | author:Qianli Liao, Joel Z Leibo, Youssef Mroueh, Tomaso Poggio category:cs.CV published:2013-11-16 summary:The standard approach to unconstrained face recognition in naturalphotographs is via a detection, alignment, recognition pipeline. While thatapproach has achieved impressive results, there are several reasons to bedissatisfied with it, among them is its lack of biological plausibility. Arecent theory of invariant recognition by feedforward hierarchical networks,like HMAX, other convolutional networks, or possibly the ventral stream,implies an alternative approach to unconstrained face recognition. Thisapproach accomplishes detection and alignment implicitly by storingtransformations of training images (called templates) rather than explicitlydetecting and aligning faces at test time. Here we propose a particularlocality-sensitive hashing based voting scheme which we call "consensus ofcollisions" and show that it can be used to approximate the full 3-layerhierarchy implied by the theory. The resulting end-to-end system forunconstrained face recognition operates on photographs of faces taken undernatural conditions, e.g., Labeled Faces in the Wild (LFW), without aligning orcropping them, as is normally done. It achieves a drastic improvement in thestate of the art on this end-to-end task, reaching the same level ofperformance as the best systems operating on aligned, closely cropped images(no outside training data). It also performs well on two newer datasets,similar to LFW, but more difficult: LFW-jittered (new here) and SUFR-W.
arxiv-5700-189 | A study on cost behaviors of binary classification measures in class-imbalanced problems | http://arxiv.org/pdf/1403.7100v1.pdf | author:Bao-Gang Hu, Wei-Ming Dong category:cs.LG published:2014-03-26 summary:This work investigates into cost behaviors of binary classification measuresin a background of class-imbalanced problems. Twelve performance measures arestudied, such as F measure, G-means in terms of accuracy rates, and of recalland precision, balance error rate (BER), Matthews correlation coefficient(MCC), Kappa coefficient, etc. A new perspective is presented for thosemeasures by revealing their cost functions with respect to the class imbalanceratio. Basically, they are described by four types of cost functions. Thefunctions provides a theoretical understanding why some measures are suitablefor dealing with class-imbalanced problems. Based on their cost functions, weare able to conclude that G-means of accuracy rates and BER are suitablemeasures because they show "proper" cost behaviors in terms of "amisclassification from a small class will cause a greater cost than that from alarge class". On the contrary, F1 measure, G-means of recall and precision, MCCand Kappa coefficient measures do not produce such behaviors so that they areunsuitable to serve our goal in dealing with the problems properly.
arxiv-5700-190 | Evaluating topic coherence measures | http://arxiv.org/pdf/1403.6397v1.pdf | author:Frank Rosner, Alexander Hinneburg, Michael Röder, Martin Nettling, Andreas Both category:cs.LG cs.CL cs.IR published:2014-03-25 summary:Topic models extract representative word sets - called topics - from wordcounts in documents without requiring any semantic annotations. Topics are notguaranteed to be well interpretable, therefore, coherence measures have beenproposed to distinguish between good and bad topics. Studies of topic coherenceso far are limited to measures that score pairs of individual words. For thefirst time, we include coherence measures from scientific philosophy that scorepairs of more complex word subsets and apply them to topic scoring.
arxiv-5700-191 | Stabilizing dual-energy X-ray computed tomography reconstructions using patch-based regularization | http://arxiv.org/pdf/1403.6318v1.pdf | author:Brian H. Tracey, Eric L. Miller category:cs.CV physics.med-ph published:2014-03-25 summary:Recent years have seen growing interest in exploiting dual- and multi-energymeasurements in computed tomography (CT) in order to characterize materialproperties as well as object shape. Material characterization is performed bydecomposing the scene into constitutive basis functions, such as Comptonscatter and photoelectric absorption functions. While well motivatedphysically, the joint recovery of the spatial distribution of photoelectric andCompton properties is severely complicated by the fact that the data areseveral orders of magnitude more sensitive to Compton scatter coefficients thanto photoelectric absorption, so small errors in Compton estimates can createlarge artifacts in the photoelectric estimate. To address these issues, wepropose a model-based iterative approach which uses patch-based regularizationterms to stabilize inversion of photoelectric coefficients, and solve theresulting problem though use of computationally attractive AlternatingDirection Method of Multipliers (ADMM) solution techniques. Using simulationsand experimental data acquired on a commercial scanner, we demonstrate that theproposed processing can lead to more stable material property estimates whichshould aid materials characterization in future dual- and multi-energy CTsystems.
arxiv-5700-192 | Hierarchical Block Structures and High-resolution Model Selection in Large Networks | http://arxiv.org/pdf/1310.4377v6.pdf | author:Tiago P. Peixoto category:cs.SI physics.soc-ph stat.ML published:2013-10-16 summary:Discovering and characterizing the large-scale topological features inempirical networks are crucial steps in understanding how complex systemsfunction. However, most existing methods used to obtain the modular structureof networks suffer from serious problems, such as being oblivious to thestatistical evidence supporting the discovered patterns, which results in theinability to separate actual structure from noise. In addition to this, onealso observes a resolution limit on the size of communities, where smaller butwell-defined clusters are not detectable when the network becomes large. Thisphenomenon occurs not only for the very popular approach of modularityoptimization, which lacks built-in statistical validation, but also for moreprincipled methods based on statistical inference and model selection, which doincorporate statistical validation in a formally correct way. Here we constructa nested generative model that, through a complete description of the entirenetwork hierarchy at multiple scales, is capable of avoiding this limitation,and enables the detection of modular structure at levels far beyond thosepossible with current approaches. Even with this increased resolution, themethod is based on the principle of parsimony, and is capable of separatingsignal from noise, and thus will not lead to the identification of spuriousmodules even on sparse networks. Furthermore, it fully generalizes otherapproaches in that it is not restricted to purely assortative mixing patterns,directed or undirected graphs, and ad hoc hierarchical structures such asbinary trees. Despite its general character, the approach is tractable, and canbe combined with advanced techniques of community detection to yield anefficient algorithm that scales well for very large networks.
arxiv-5700-193 | A Tiered Move-making Algorithm for General Non-submodular Pairwise Energies | http://arxiv.org/pdf/1403.6275v1.pdf | author:Vibhav Vineet, Jonathan Warrell, Philip H. S. Torr category:cs.CV published:2014-03-25 summary:A large number of problems in computer vision can be modelled as energyminimization problems in a Markov Random Field (MRF) or Conditional RandomField (CRF) framework. Graph-cuts based $\alpha$-expansion is a standardmove-making method to minimize the energy functions with sub-modular pairwiseterms. However, certain problems require more complex pairwise terms where the$\alpha$-expansion method is generally not applicable. In this paper, we propose an iterative {\em tiered move making algorithm}which is able to handle general pairwise terms. Each move to the nextconfiguration is based on the current labeling and an optimal tiered move,where each tiered move requires one application of the dynamic programmingbased tiered labeling method introduced in Felzenszwalb et. al.\cite{tiered_cvpr_felzenszwalbV10}. The algorithm converges to a local minimumfor any general pairwise potential, and we give a theoretical analysis of theproperties of the algorithm, characterizing the situations in which we canexpect good performance. We first evaluate our method on an object-classsegmentation problem using the Pascal VOC-11 segmentation dataset where welearn general pairwise terms. Further we evaluate the algorithm on many otherbenchmark labeling problems such as stereo, image segmentation, image stitchingand image denoising. Our method consistently gets better accuracy and energyvalues than alpha-expansion, loopy belief propagation (LBP), quadraticpseudo-boolean optimization (QPBO), and is competitive with TRWS.
arxiv-5700-194 | Capturing and Recognizing Objects Appearance Employing Eigenspace | http://arxiv.org/pdf/1403.6260v1.pdf | author:M. Ashrafuzzaman, M. M . Rahman, M. M. A. Hashem category:cs.CV published:2014-03-25 summary:This paper presents a method of capturing objects appearances from itsenvironment and it also describes how to recognize unknown appearances creatingan eigenspace. This representation and recognition can be done automaticallytaking objects various appearances by using robotic vision from a definedenvironment. This technique also allows extracting objects from some sort ofcomplicated scenes. In this case, some of object appearances are taken withdefined occlusions and eigenspaces are created by accepting both ofnon-occluded and occluded appearances together. Eigenspace is constructedsuccessfully every times when a new object appears, and various appearancesaccumulated gradually. A sequence of appearances is generated from itsaccumulated shapes, which is used for recognition of the unknown objectsappearances. Various objects environments are shown in the experiment tocapture objects appearances and experimental results show effectiveness of theproposed approach.
arxiv-5700-195 | Classroom Video Assessment and Retrieval via Multiple Instance Learning | http://arxiv.org/pdf/1403.6248v1.pdf | author:Qifeng Qiao, Peter A. Beling category:cs.IR cs.CY cs.LG published:2014-03-25 summary:We propose a multiple instance learning approach to content-based retrievalof classroom video for the purpose of supporting human assessing the learningenvironment. The key element of our approach is a mapping between the semanticconcepts of the assessment system and features of the video that can bemeasured using techniques from the fields of computer vision and speechanalysis. We report on a formative experiment in content-based video retrievalinvolving trained experts in the Classroom Assessment Scoring System, a widelyused framework for assessment and improvement of learning environments. Theresults of this experiment suggest that our approach has potential applicationto productivity enhancement in assessment and to broader retrieval tasks.
arxiv-5700-196 | Random design analysis of ridge regression | http://arxiv.org/pdf/1106.2363v2.pdf | author:Daniel Hsu, Sham M. Kakade, Tong Zhang category:math.ST cs.AI cs.LG stat.ML stat.TH published:2011-06-13 summary:This work gives a simultaneous analysis of both the ordinary least squaresestimator and the ridge regression estimator in the random design setting undermild assumptions on the covariate/response distributions. In particular, theanalysis provides sharp results on the ``out-of-sample'' prediction error, asopposed to the ``in-sample'' (fixed design) error. The analysis also revealsthe effect of errors in the estimated covariance structure, as well as theeffect of modeling errors, neither of which effects are present in the fixeddesign setting. The proofs of the main results are based on a simpledecomposition lemma combined with concentration inequalities for random vectorsand matrices.
arxiv-5700-197 | Disease Prediction based on Functional Connectomes using a Scalable and Spatially-Informed Support Vector Machine | http://arxiv.org/pdf/1310.5415v2.pdf | author:Takanori Watanabe, Daniel Kessler, Clayton Scott, Michael Angstadt, Chandra Sripada category:stat.ML published:2013-10-21 summary:Substantial evidence indicates that major psychiatric disorders areassociated with distributed neural dysconnectivity, leading to strong interestin using neuroimaging methods to accurately predict disorder status. In thiswork, we are specifically interested in a multivariate approach that usesfeatures derived from whole-brain resting state functional connectomes.However, functional connectomes reside in a high dimensional space, whichcomplicates model interpretation and introduces numerous statistical andcomputational challenges. Traditional feature selection techniques are used toreduce data dimensionality, but are blind to the spatial structure of theconnectomes. We propose a regularization framework where the 6-D structure ofthe functional connectome is explicitly taken into account via the fused Lassoor the GraphNet regularizer. Our method only restricts the loss function to beconvex and margin-based, allowing non-differentiable loss functions such as thehinge-loss to be used. Using the fused Lasso or GraphNet regularizer with thehinge-loss leads to a structured sparse support vector machine (SVM) withembedded feature selection. We introduce a novel efficient optimizationalgorithm based on the augmented Lagrangian and the classical alternatingdirection method, which can solve both fused Lasso and GraphNet regularized SVMwith very little modification. We also demonstrate that the inner subproblemsof the algorithm can be solved efficiently in analytic form by coupling thevariable splitting strategy with a data augmentation scheme. Experiments onsimulated data and resting state scans from a large schizophrenia dataset showthat our proposed approach can identify predictive regions that are spatiallycontiguous in the 6-D "connectome space," offering an additional layer ofinterpretability that could provide new insights about various diseaseprocesses.
arxiv-5700-198 | Development and evaluation of a 3D model observer with nonlinear spatiotemporal contrast sensitivity | http://arxiv.org/pdf/1403.6183v1.pdf | author:Ali R. N. Avanaki, Kathryn S. Espig, Andrew D. A. Maidment, Cedric Marchessoux, Predrag R. Bakic, Tom R. L. Kimpe category:cs.CV published:2014-03-24 summary:We investigate improvements to our 3D model observer with the goal of bettermatching human observer performance as a function of viewing distance,effective contrast, maximum luminance, and browsing speed. Two nonlinearmethods of applying the human contrast sensitivity function (CSF) to a 3D modelobserver are proposed, namely the Probability Map (PM) and Monte Carlo (MC)methods. In the PM method, the visibility probability for each frequencycomponent of the image stack, p, is calculated taking into account Barten'sspatiotemporal CSF, the component modulation, and the human psychometricfunction. The probability p is considered to be equal to the perceivedamplitude of the frequency component and thus can be used by a traditionalmodel observer (e.g., LG-msCHO) in the space-time domain. In the MC method,each component is randomly kept with probability p or discarded with 1-p. Theamplitude of the retained components is normalized to unity. The methods weretested using DBT stacks of an anthropomorphic breast phantom processed in acomprehensive simulation pipeline. Our experiments indicate that both the PMand MC methods yield results that match human observer performance better thanthe linear filtering method as a function of viewing distance, effectivecontrast, maximum luminance, and browsing speed.
arxiv-5700-199 | Coherent Multi-Sentence Video Description with Variable Level of Detail | http://arxiv.org/pdf/1403.6173v1.pdf | author:Anna Senina, Marcus Rohrbach, Wei Qiu, Annemarie Friedrich, Sikandar Amin, Mykhaylo Andriluka, Manfred Pinkal, Bernt Schiele category:cs.CV cs.CL published:2014-03-24 summary:Humans can easily describe what they see in a coherent way and at varyinglevel of detail. However, existing approaches for automatic video descriptionare mainly focused on single sentence generation and produce descriptions at afixed level of detail. In this paper, we address both of these limitations: fora variable level of detail we produce coherent multi-sentence descriptions ofcomplex videos. We follow a two-step approach where we first learn to predict asemantic representation (SR) from video and then generate natural languagedescriptions from the SR. To produce consistent multi-sentence descriptions, wemodel across-sentence consistency at the level of the SR by enforcing aconsistent topic. We also contribute both to the visual recognition of objectsproposing a hand-centric approach as well as to the robust generation ofsentences using a word lattice. Human judges rate our multi-sentencedescriptions as more readable, correct, and relevant than related work. Tounderstand the difference between more detailed and shorter descriptions, wecollect and analyze a video description corpus of three levels of detail.
arxiv-5700-200 | New Algorithmic Approaches to Point Constellation Recognition | http://arxiv.org/pdf/1405.1402v1.pdf | author:Thomas Bourgeat, Julien Bringer, Herve Chabanne, Robin Champenois, Jeremie Clement, Houda Ferradi, Marc Heinrich, Paul Melotti, David Naccache, Antoine Voizard category:cs.CV published:2014-03-24 summary:Point constellation recognition is a common problem with many patternmatching applications. Whilst useful in many contexts, this work is mainlymotivated by fingerprint matching. Fingerprints are traditionally modelled asconstellations of oriented points called minutiae. The fingerprint verifier'stask consists in comparing two point constellations. The comparedconstellations may differ by rotation and translation or by much more involvedtransforms such as distortion or occlusion. This paper presents three newconstellation matching algorithms. The first two methods generalize analgorithm by Bringer and Despiegel. Our third proposal creates a veryinteresting analogy between mechanical system simulation and the constellationrecognition problem.
arxiv-5700-201 | Ensemble Detection of Single & Multiple Events at Sentence-Level | http://arxiv.org/pdf/1403.6023v1.pdf | author:Luís Marujo, Anatole Gershman, Jaime Carbonell, João P. Neto, David Martins de Matos category:cs.CL cs.LG published:2014-03-24 summary:Event classification at sentence level is an important Information Extractiontask with applications in several NLP, IR, and personalization systems.Multi-label binary relevance (BR) are the state-of-art methods. In this work,we explored new multi-label methods known for capturing relations between eventtypes. These new methods, such as the ensemble Chain of Classifiers, improvethe F1 on average across the 6 labels by 2.8% over the Binary Relevance. Thelow occurrence of multi-label sentences motivated the reduction of the hardimbalanced multi-label classification problem with low number of occurrences ofmultiple labels per instance to an more tractable imbalanced multiclass problemwith better results (+ 4.6%). We report the results of adding new features,such as sentiment strength, rhetorical signals, domain-id (source-id and date),and key-phrases in both single-label and multi-label event classificationscenarios.
arxiv-5700-202 | An Efficient Feature Selection in Classification of Audio Files | http://arxiv.org/pdf/1404.1491v1.pdf | author:Jayita Mitra, Diganta Saha category:cs.LG published:2014-03-24 summary:In this paper we have focused on an efficient feature selection method inclassification of audio files. The main objective is feature selection andextraction. We have selected a set of features for further analysis, whichrepresents the elements in feature vector. By extraction method we can computea numerical representation that can be used to characterize the audio using theexisting toolbox. In this study Gain Ratio (GR) is used as a feature selectionmeasure. GR is used to select splitting attribute which will separate thetuples into different classes. The pulse clarity is considered as a subjectivemeasure and it is used to calculate the gain of features of audio files. Thesplitting criterion is employed in the application to identify the class or themusic genre of a specific audio file from testing database. Experimentalresults indicate that by using GR the application can produce a satisfactoryresult for music genre classification. After dimensionality reduction bestthree features have been selected out of various features of audio file and inthis technique we will get more than 90% successful classification result.
arxiv-5700-203 | Brain Tumor Detection Based On Mathematical Analysis and Symmetry Information | http://arxiv.org/pdf/1403.6002v1.pdf | author:Narkhede Sachin G., Vaishali Khairnar, Sujata Kadu category:cs.CV published:2014-03-24 summary:Image segmentation some of the challenging issues on brain magnetic resonanceimage tumor segmentation caused by the weak correlation between magneticresonance imaging intensity and anatomical meaning.With the objective ofutilizing more meaningful information to improve brain tumor segmentation,anapproach which employs bilateral symmetry information as an additional featurefor segmentation is proposed.This is motivated by potential performanceimprovement in the general automatic brain tumor segmentation systems which areimportant for many medical and scientific applications.Brain Magnetic ResonanceImaging segmentation is a complex problem in the field of medical imagingdespite various presented methods.MR image of human brain can be divided intoseveral sub-regions especially soft tissues such as gray matter,white matterand cerebra spinal fluid.Although edge information is the main clue in imagesegmentation,it cannot get a better result in analysis the content of imageswithout combining other information.Our goal is to detect the position andboundary of tumors automatically.Experiments were conducted on realpictures,and the results show that the algorithm is flexible and convenient.
arxiv-5700-204 | First Order Methods for Robust Non-negative Matrix Factorization for Large Scale Noisy Data | http://arxiv.org/pdf/1403.5994v1.pdf | author:Jason Gejie Liu, Shuchin Aeron category:stat.ML published:2014-03-24 summary:Nonnegative matrix factorization (NMF) has been shown to be identifiableunder the separability assumption, under which all the columns(or rows) of theinput data matrix belong to the convex cone generated by only a few of thesecolumns(or rows) [1]. In real applications, however, such separabilityassumption is hard to satisfy. Following [4] and [5], in this paper, we look atthe Linear Programming (LP) based reformulation to locate the extreme rays ofthe convex cone but in a noisy setting. Furthermore, in order to deal with thelarge scale data, we employ First-Order Methods (FOM) to mitigate thecomputational complexity of LP, which primarily results from a large number ofconstraints. We show the performance of the algorithm on real and syntheticdata sets.
arxiv-5700-205 | Automatic recognition and tagging of topologically different regimes in dynamical systems | http://arxiv.org/pdf/1312.2482v2.pdf | author:Jesse Berwald, Marian Gidea, Mikael Vejdemo-Johansson category:cs.CG cs.LG math.DS nlin.CD published:2013-12-09 summary:Complex systems are commonly modeled using nonlinear dynamical systems. Thesemodels are often high-dimensional and chaotic. An important goal in studyingphysical systems through the lens of mathematical models is to determine whenthe system undergoes changes in qualitative behavior. A detailed description ofthe dynamics can be difficult or impossible to obtain for high-dimensional andchaotic systems. Therefore, a more sensible goal is to recognize and marktransitions of a system between qualitatively different regimes of behavior. Inpractice, one is interested in developing techniques for detection of suchtransitions from sparse observations, possibly contaminated by noise. In thispaper we develop a framework to accurately tag different regimes of complexsystems based on topological features. In particular, our framework works witha high degree of success in picking out a cyclically orbiting regime from astationary equilibrium regime in high-dimensional stochastic dynamical systems.
arxiv-5700-206 | AIS-INMACA: A Novel Integrated MACA Based Clonal Classifier for Protein Coding and Promoter Region Prediction | http://arxiv.org/pdf/1403.5933v1.pdf | author:Pokkuluri Kiran Sree, Inampudi Ramesh Babu category:cs.CE cs.LG published:2014-03-24 summary:Most of the problems in bioinformatics are now the challenges in computing.This paper aims at building a classifier based on Multiple Attractor CellularAutomata (MACA) which uses fuzzy logic. It is strengthened with an artificialImmune System Technique (AIS), Clonal algorithm for identifying a proteincoding and promoter region in a given DNA sequence. The proposed classifier isnamed as AIS-INMACA introduces a novel concept to combine CA with artificialimmune system to produce a better classifier which can address major problemsin bioinformatics. This will be the first integrated algorithm which canpredict both promoter and protein coding regions. To obtain good fitness rulesthe basic concept of Clonal selection algorithm was used. The proposedclassifier can handle DNA sequences of lengths 54,108,162,252,354. Thisclassifier gives the exact boundaries of both protein and promoter regions withan average accuracy of 89.6%. This classifier was tested with 97,000 datacomponents which were taken from Fickett & Toung, MPromDb, and other sequencesfrom a renowned medical university. This proposed classifier can handle hugedata sets and can find protein and promoter regions even in mixed andoverlapped DNA sequences. This work also aims at identifying the logicalitybetween the major problems in bioinformatics and tries to obtaining a commonframe work for addressing major problems in bioinformatics like proteinstructure prediction, RNA structure prediction, predicting the splicing patternof any primary transcript and analysis of information content in DNA, RNA,protein sequences and structure. This work will attract more researcherstowards application of CA as a potential pattern classifier to many importantproblems in bioinformatics
arxiv-5700-207 | SRA: Fast Removal of General Multipath for ToF Sensors | http://arxiv.org/pdf/1403.5919v1.pdf | author:Daniel Freedman, Eyal Krupka, Yoni Smolin, Ido Leichter, Mirko Schmidt category:cs.CV published:2014-03-24 summary:A major issue with Time of Flight sensors is the presence of multipathinterference. We present Sparse Reflections Analysis (SRA), an algorithm forremoving this interference which has two main advantages. First, it allows forvery general forms of multipath, including interference with three or morepaths, diffuse multipath resulting from Lambertian surfaces, and combinationsthereof. SRA removes this general multipath with robust techniques based on$L_1$ optimization. Second, due to a novel dimension reduction, we are able toproduce a very fast version of SRA, which is able to run at frame rate.Experimental results on both synthetic data with ground truth, as well as realimages of challenging scenes, validate the approach.
arxiv-5700-208 | The state of play of ASC-Inclusion: An Integrated Internet-Based Environment for Social Inclusion of Children with Autism Spectrum Conditions | http://arxiv.org/pdf/1403.5912v1.pdf | author:Björn Schuller, Erik Marchi, Simon Baron-Cohen, Helen O'Reilly, Delia Pigat, Peter Robinson, Ian Daves category:cs.HC cs.CV cs.CY published:2014-03-24 summary:Individuals with Autism Spectrum Conditions (ASC) have marked difficultiesusing verbal and non-verbal communication for social interaction. The runningASC-Inclusion project aims to help children with ASC by allowing them to learnhow emotions can be expressed and recognised via playing games in a virtualworld. The platform includes analysis of users' gestures, facial, and vocalexpressions using standard microphone and web-cam or a depth sensor, trainingthrough games, text communication with peers, animation, video and audio clips.We present the state of play in realising such a serious game platform andprovide results for the different modalities.
arxiv-5700-209 | Nonlinear Eigenproblems in Data Analysis - Balanced Graph Cuts and the RatioDCA-Prox | http://arxiv.org/pdf/1312.5192v2.pdf | author:Leonardo Jost, Simon Setzer, Matthias Hein category:stat.ML cs.LG math.OC published:2013-12-18 summary:It has been recently shown that a large class of balanced graph cuts allowsfor an exact relaxation into a nonlinear eigenproblem. We review briefly someof these results and propose a family of algorithms to compute nonlineareigenvectors which encompasses previous work as special cases. We provide adetailed analysis of the properties and the convergence behavior of thesealgorithms and then discuss their application in the area of balanced graphcuts.
arxiv-5700-210 | Non-uniform Feature Sampling for Decision Tree Ensembles | http://arxiv.org/pdf/1403.5877v1.pdf | author:Anastasios Kyrillidis, Anastasios Zouzias category:stat.ML cs.IT cs.LG math.IT stat.AP published:2014-03-24 summary:We study the effectiveness of non-uniform randomized feature selection indecision tree classification. We experimentally evaluate two feature selectionmethodologies, based on information extracted from the provided dataset: $(i)$\emph{leverage scores-based} and $(ii)$ \emph{norm-based} feature selection.Experimental evaluation of the proposed feature selection techniques indicatethat such approaches might be more effective compared to naive uniform featureselection and moreover having comparable performance to the random forestalgorithm [3]
arxiv-5700-211 | Block Motion Based Dynamic Texture Analysis: A Review | http://arxiv.org/pdf/1403.5869v1.pdf | author:Akhlaqur Rahman, Sumaira Tasnim category:cs.CV published:2014-03-24 summary:Dynamic texture refers to image sequences of non-rigid objects that exhibitsome regularity in their movement. Videos of smoke, fire etc. fall under thecategory of dynamic texture. Researchers have investigated different ways toanalyze dynamic textures since early nineties. Both appearance based (imageintensities) and motion based approaches are investigated. Motion basedapproaches turn out to be more effective. A group of researchers haveinvestigated ways to utilize the motion vectors readily available with theblocks in video codes like MGEG/H26X. In this paper we provide a review of thedynamic texture analysis methods using block motion. Research into dynamictexture analysis using block motion includes recognition, motion computation,segmentation, and synthesis. We provide a comprehensive review of theseapproaches.
arxiv-5700-212 | MCL-3D: a database for stereoscopic image quality assessment using 2D-image-plus-depth source | http://arxiv.org/pdf/1405.1403v1.pdf | author:Rui Song, Hyunsuk Ko, C. C. Jay Kuo category:cs.CV published:2014-03-23 summary:A new stereoscopic image quality assessment database rendered using the2D-image-plus-depth source, called MCL-3D, is described and the performancebenchmarking of several known 2D and 3D image quality metrics using the MCL-3Ddatabase is presented in this work. Nine image-plus-depth sources are firstselected, and a depth image-based rendering (DIBR) technique is used to renderstereoscopic image pairs. Distortions applied to either the texture image orthe depth image before stereoscopic image rendering include: Gaussian blur,additive white noise, down-sampling blur, JPEG and JPEG-2000 (JP2K) compressionand transmission error. Furthermore, the distortion caused by imperfectrendering is also examined. The MCL-3D database contains 693 stereoscopic imagepairs, where one third of them are of resolution 1024x728 and two thirds are ofresolution 1920x1080. The pair-wise comparison was adopted in the subjectivetest for user friendliness, and the Mean Opinion Score (MOS) can be computedaccordingly. Finally, we evaluate the performance of several 2D and 3D imagequality metrics applied to MCL-3D. All texture images, depth images, renderedimage pairs in MCL-3D and their MOS values obtained in the subjective test areavailable to the public (http://mcl.usc.edu/mcl-3d-database/) for futureresearch and development.
arxiv-5700-213 | Analysis of Distributed Stochastic Dual Coordinate Ascent | http://arxiv.org/pdf/1312.1031v2.pdf | author:Tianbao Yang, Shenghuo Zhu, Rong Jin, Yuanqing Lin category:cs.DC cs.LG published:2013-12-04 summary:In \citep{Yangnips13}, the author presented distributed stochastic dualcoordinate ascent (DisDCA) algorithms for solving large-scale regularized lossminimization. Extraordinary performances have been observed and reported forthe well-motivated updates, as referred to the practical updates, compared tothe naive updates. However, no serious analysis has been provided to understandthe updates and therefore the convergence rates. In the paper, we bridge thegap by providing a theoretical analysis of the convergence rates of thepractical DisDCA algorithm. Our analysis helped by empirical studies has shownthat it could yield an exponential speed-up in the convergence by increasingthe number of dual updates at each iteration. This result justifies thesuperior performances of the practical DisDCA as compared to the naive variant.As a byproduct, our analysis also reveals the convergence behavior of theone-communication DisDCA.
arxiv-5700-214 | Human brain distinctiveness based on EEG spectral coherence connectivity | http://arxiv.org/pdf/1403.6384v1.pdf | author:Daria La Rocca, Patrizio Campisi, Balazs Vegso, Peter Cserti, Gyorgy Kozmann, Fabio Babiloni, Fabrizio De Vico Fallani category:q-bio.NC stat.ML published:2014-03-23 summary:The use of EEG biometrics, for the purpose of automatic people recognition,has received increasing attention in the recent years. Most of current analysisrely on the extraction of features characterizing the activity of single brainregions, like power-spectrum estimates, thus neglecting possible temporaldependencies between the generated EEG signals. However, importantphysiological information can be extracted from the way different brain regionsare functionally coupled. In this study, we propose a novel approach that fusesspectral coherencebased connectivity between different brain regions as apossibly viable biometric feature. The proposed approach is tested on a largedataset of subjects (N=108) during eyes-closed (EC) and eyes-open (EO) restingstate conditions. The obtained recognition performances show that using brainconnectivity leads to higher distinctiveness with respect to power-spectrummeasurements, in both the experimental conditions. Notably, a 100% recognitionaccuracy is obtained in EC and EO when integrating functional connectivitybetween regions in the frontal lobe, while a lower 97.41% is obtained in EC(96.26% in EO) when fusing power spectrum information from centro-parietalregions. Taken together, these results suggest that functional connectivitypatterns represent effective features for improving EEG-based biometricsystems.
arxiv-5700-215 | Two Timescale Convergent Q-learning for Sleep--Scheduling in Wireless Sensor Networks | http://arxiv.org/pdf/1312.7292v2.pdf | author:Prashanth L. A., Abhranil Chatterjee, Shalabh Bhatnagar category:cs.SY cs.LG published:2013-12-27 summary:In this paper, we consider an intrusion detection application for WirelessSensor Networks (WSNs). We study the problem of scheduling the sleep times ofthe individual sensors to maximize the network lifetime while keeping thetracking error to a minimum. We formulate this problem as apartially-observable Markov decision process (POMDP) with continuousstate-action spaces, in a manner similar to (Fuemmeler and Veeravalli [2008]).However, unlike their formulation, we consider infinite horizon discounted andaverage cost objectives as performance criteria. For each criterion, we proposea convergent on-policy Q-learning algorithm that operates on two timescales,while employing function approximation to handle the curse of dimensionalityassociated with the underlying POMDP. Our proposed algorithm incorporates apolicy gradient update using a one-simulation simultaneous perturbationstochastic approximation (SPSA) estimate on the faster timescale, while theQ-value parameter (arising from a linear function approximation for theQ-values) is updated in an on-policy temporal difference (TD) algorithm-likefashion on the slower timescale. The feature selection scheme employed in eachof our algorithms manages the energy and tracking components in a manner thatassists the search for the optimal sleep-scheduling policy. For the sake ofcomparison, in both discounted and average settings, we also develop a functionapproximation analogue of the Q-learning algorithm. This algorithm, unlike thetwo-timescale variant, does not possess theoretical convergence guarantees.Finally, we also adapt our algorithms to include a stochastic iterativeestimation scheme for the intruder's mobility model. Our simulation results ona 2-dimensional network setting suggest that our algorithms result in bettertracking accuracy at the cost of only a few additional sensors, in comparisonto a recent prior work.
arxiv-5700-216 | SmartAnnotator: An Interactive Tool for Annotating RGBD Indoor Images | http://arxiv.org/pdf/1403.5718v1.pdf | author:Yu-Shiang Wong, Hung-Kuo Chu, Niloy J. Mitra category:cs.CV published:2014-03-23 summary:RGBD images with high quality annotations in the form of geometric (i.e.,segmentation) and structural (i.e., how do the segments are mutually related in3D) information provide valuable priors to a large number of scene and imagemanipulation applications. While it is now simple to acquire RGBD images,annotating them, automatically or manually, remains challenging especially incluttered noisy environments. We present SmartAnnotator, an interactive systemto facilitate annotating RGBD images. The system performs the tedious tasks ofgrouping pixels, creating potential abstracted cuboids, inferring objectinteractions in 3D, and comes up with various hypotheses. The user simply hasto flip through a list of suggestions for segment labels, finalize a selection,and the system updates the remaining hypotheses. As objects are finalized, theprocess speeds up with fewer ambiguities to resolve. Further, as more scenesare annotated, the system makes better suggestions based on structural andgeometric priors learns from the previous annotation sessions. We test oursystem on a large number of database scenes and report significant improvementsover naive low-level annotation tools.
arxiv-5700-217 | Neuronal Synchrony in Complex-Valued Deep Networks | http://arxiv.org/pdf/1312.6115v5.pdf | author:David P. Reichert, Thomas Serre category:stat.ML cs.LG cs.NE q-bio.NC published:2013-12-20 summary:Deep learning has recently led to great successes in tasks such as imagerecognition (e.g Krizhevsky et al., 2012). However, deep networks are stilloutmatched by the power and versatility of the brain, perhaps in part due tothe richer neuronal computations available to cortical circuits. The challengeis to identify which neuronal mechanisms are relevant, and to find suitableabstractions to model them. Here, we show how aspects of spike timing, longhypothesized to play a crucial role in cortical information processing, couldbe incorporated into deep networks to build richer, versatile representations. We introduce a neural network formulation based on complex-valued neuronalunits that is not only biologically meaningful but also amenable to a varietyof deep learning frameworks. Here, units are attributed both a firing rate anda phase, the latter indicating properties of spike timing. We show how thisformulation qualitatively captures several aspects thought to be related toneuronal synchrony, including gating of information processing and dynamicbinding of distributed object representations. Focusing on the latter, wedemonstrate the potential of the approach in several simple experiments. Thus,neuronal synchrony could be a flexible mechanism that fulfills multiplefunctional roles in deep networks.
arxiv-5700-218 | Firefly Monte Carlo: Exact MCMC with Subsets of Data | http://arxiv.org/pdf/1403.5693v1.pdf | author:Dougal Maclaurin, Ryan P. Adams category:stat.ML cs.LG stat.CO published:2014-03-22 summary:Markov chain Monte Carlo (MCMC) is a popular and successful general-purposetool for Bayesian inference. However, MCMC cannot be practically applied tolarge data sets because of the prohibitive cost of evaluating every likelihoodterm at every iteration. Here we present Firefly Monte Carlo (FlyMC) anauxiliary variable MCMC algorithm that only queries the likelihoods of apotentially small subset of the data at each iteration yet simulates from theexact posterior distribution, in contrast to recent proposals that areapproximate even in the asymptotic limit. FlyMC is compatible with a widevariety of modern MCMC algorithms, and only requires a lower bound on theper-datum likelihood factors. In experiments, we find that FlyMC generatessamples from the posterior more than an order of magnitude faster than regularMCMC, opening up MCMC methods to larger datasets than were previouslyconsidered feasible.
arxiv-5700-219 | k-Sparse Autoencoders | http://arxiv.org/pdf/1312.5663v2.pdf | author:Alireza Makhzani, Brendan Frey category:cs.LG published:2013-12-19 summary:Recently, it has been observed that when representations are learnt in a waythat encourages sparsity, improved performance is obtained on classificationtasks. These methods involve combinations of activation functions, samplingsteps and different kinds of penalties. To investigate the effectiveness ofsparsity by itself, we propose the k-sparse autoencoder, which is anautoencoder with linear activation function, where in hidden layers only the khighest activities are kept. When applied to the MNIST and NORB datasets, wefind that this method achieves better classification results than denoisingautoencoders, networks trained with dropout, and RBMs. k-sparse autoencodersare simple to train and the encoding stage is very fast, making themwell-suited to large problem sizes, where conventional sparse coding algorithmscannot be applied.
arxiv-5700-220 | CUR Algorithm with Incomplete Matrix Observation | http://arxiv.org/pdf/1403.5647v1.pdf | author:Rong Jin, Shenghuo Zhu category:cs.LG stat.ML published:2014-03-22 summary:CUR matrix decomposition is a randomized algorithm that can efficientlycompute the low rank approximation for a given rectangle matrix. One limitationwith the existing CUR algorithms is that they require an access to the fullmatrix A for computing U. In this work, we aim to alleviate this limitation. Inparticular, we assume that besides having an access to randomly sampled d rowsand d columns from A, we only observe a subset of randomly sampled entries fromA. Our goal is to develop a low rank approximation algorithm, similar to CUR,based on (i) randomly sampled rows and columns from A, and (ii) randomlysampled entries from A. The proposed algorithm is able to perfectly recover thetarget matrix A with only O(rn log n) number of observed entries. In addition,instead of having to solve an optimization problem involved trace normregularization, the proposed algorithm only needs to solve a standardregression problem. Finally, unlike most matrix completion theories that holdonly when the target matrix is of low rank, we show a strong guarantee for theproposed algorithm even when the target matrix is not low rank.
arxiv-5700-221 | Bayesian Optimization with Unknown Constraints | http://arxiv.org/pdf/1403.5607v1.pdf | author:Michael A. Gelbart, Jasper Snoek, Ryan P. Adams category:stat.ML cs.LG published:2014-03-22 summary:Recent work on Bayesian optimization has shown its effectiveness in globaloptimization of difficult black-box objective functions. Many real-worldoptimization problems of interest also have constraints which are unknown apriori. In this paper, we study Bayesian optimization for constrained problemsin the general case that noise may be present in the constraint functions, andthe objective and constraints may be evaluated independently. We providemotivating practical examples, and present a general framework to solve suchproblems. We demonstrate the effectiveness of our approach on optimizing theperformance of online latent Dirichlet allocation subject to topic sparsityconstraints, tuning a neural network given test-time memory constraints, andoptimizing Hamiltonian Monte Carlo to achieve maximal effectiveness in a fixedtime, subject to passing standard convergence diagnostics.
arxiv-5700-222 | Forecasting Popularity of Videos using Social Media | http://arxiv.org/pdf/1403.5603v1.pdf | author:Jie Xu, Mihaela van der Schaar, Jiangchuan Liu, Haitao Li category:cs.LG cs.SI published:2014-03-22 summary:This paper presents a systematic online prediction method (Social-Forecast)that is capable to accurately forecast the popularity of videos promoted bysocial media. Social-Forecast explicitly considers the dynamically changing andevolving propagation patterns of videos in social media when making popularityforecasts, thereby being situation and context aware. Social-Forecast aims tomaximize the forecast reward, which is defined as a tradeoff between thepopularity prediction accuracy and the timeliness with which a prediction isissued. The forecasting is performed online and requires no training phase or apriori knowledge. We analytically bound the prediction performance loss ofSocial-Forecast as compared to that obtained by an omniscient oracle and provethat the bound is sublinear in the number of video arrivals, therebyguaranteeing its short-term performance as well as its asymptotic convergenceto the optimal performance. In addition, we conduct extensive experiments usingreal-world data traces collected from the videos shared in RenRen, one of thelargest online social networks in China. These experiments show that ourproposed method outperforms existing view-based approaches for popularityprediction (which are not context-aware) by more than 30% in terms ofprediction rewards.
arxiv-5700-223 | A Lemma Based Evaluator for Semitic Language Text Summarization Systems | http://arxiv.org/pdf/1403.5596v1.pdf | author:Tarek El-Shishtawy, Fatma El-Ghannam category:cs.CL cs.IR published:2014-03-22 summary:Matching texts in highly inflected languages such as Arabic by simplestemming strategy is unlikely to perform well. In this paper, we present astrategy for automatic text matching technique for for inflectional languages,using Arabic as the test case. The system is an extension of ROUGE test inwhich texts are matched on token's lemma level. The experimental results showan enhancement of detecting similarities between different sentences havingsame semantics but written in different lexical forms..
arxiv-5700-224 | Zero-Shot Learning by Convex Combination of Semantic Embeddings | http://arxiv.org/pdf/1312.5650v3.pdf | author:Mohammad Norouzi, Tomas Mikolov, Samy Bengio, Yoram Singer, Jonathon Shlens, Andrea Frome, Greg S. Corrado, Jeffrey Dean category:cs.LG published:2013-12-19 summary:Several recent publications have proposed methods for mapping images intocontinuous semantic embedding spaces. In some cases the embedding space istrained jointly with the image transformation. In other cases the semanticembedding space is established by an independent natural language processingtask, and then the image transformation into that space is learned in a secondstage. Proponents of these image embedding systems have stressed theiradvantages over the traditional \nway{} classification framing of imageunderstanding, particularly in terms of the promise for zero-shot learning --the ability to correctly annotate images of previously unseen objectcategories. In this paper, we propose a simple method for constructing an imageembedding system from any existing \nway{} image classifier and a semantic wordembedding model, which contains the $\n$ class labels in its vocabulary. Ourmethod maps images into the semantic embedding space via convex combination ofthe class label embedding vectors, and requires no additional training. We showthat this simple and direct method confers many of the advantages associatedwith more complex image embedding schemes, and indeed outperforms state of theart methods on the ImageNet zero-shot learning task.
arxiv-5700-225 | Continuous Optimization for Fields of Experts Denoising Works | http://arxiv.org/pdf/1403.5590v1.pdf | author:Petter Strandmark, Sameer Agarwal category:cs.CV published:2014-03-21 summary:Several recent papers use image denoising with a Fields of Experts prior tobenchmark discrete optimization methods. We show that a non-linear leastsquares solver significantly outperforms all known discrete methods on thisproblem.
arxiv-5700-226 | Missing Data Prediction and Classification: The Use of Auto-Associative Neural Networks and Optimization Algorithms | http://arxiv.org/pdf/1403.5488v1.pdf | author:Collins Leke, Bhekisipho Twala, T. Marwala category:cs.NE cs.LG published:2014-03-21 summary:This paper presents methods which are aimed at finding approximations tomissing data in a dataset by using optimization algorithms to optimize thenetwork parameters after which prediction and classification tasks can beperformed. The optimization methods that are considered are genetic algorithm(GA), simulated annealing (SA), particle swarm optimization (PSO), randomforest (RF) and negative selection (NS) and these methods are individually usedin combination with auto-associative neural networks (AANN) for missing dataestimation and the results obtained are compared. The methods suggested use theoptimization algorithms to minimize an error function derived from training theauto-associative neural network during which the interrelationships between theinputs and the outputs are obtained and stored in the weights connecting thedifferent layers of the network. The error function is expressed as the squareof the difference between the actual observations and predicted values from anauto-associative neural network. In the event of missing data, all the valuesof the actual observations are not known hence, the error function isdecomposed to depend on the known and unknown variable values. Multi-layerperceptron (MLP) neural network is employed to train the neural networks usingthe scaled conjugate gradient (SCG) method. Prediction accuracy is determinedby mean squared error (MSE), root mean squared error (RMSE), mean absoluteerror (MAE), and correlation coefficient (r) computations. Accuracy inclassification is obtained by plotting ROC curves and calculating the areasunder these. Analysis of results depicts that the approach using RF with AANNproduces the most accurate predictions and classifications while on the otherend of the scale is the approach which entails using NS with AANN.
arxiv-5700-227 | Adaptive piecewise polynomial estimation via trend filtering | http://arxiv.org/pdf/1304.2986v2.pdf | author:Ryan J. Tibshirani category:math.ST stat.ME stat.ML stat.TH published:2013-04-10 summary:We study trend filtering, a recently proposed tool of Kim et al. [SIAM Rev.51 (2009) 339-360] for nonparametric regression. The trend filtering estimateis defined as the minimizer of a penalized least squares criterion, in whichthe penalty term sums the absolute $k$th order discrete derivatives over theinput points. Perhaps not surprisingly, trend filtering estimates appear tohave the structure of $k$th degree spline functions, with adaptively chosenknot points (we say ``appear'' here as trend filtering estimates are not reallyfunctions over continuous domains, and are only defined over the discrete setof inputs). This brings to mind comparisons to other nonparametric regressiontools that also produce adaptive splines; in particular, we compare trendfiltering to smoothing splines, which penalize the sum of squared derivativesacross input points, and to locally adaptive regression splines [Ann. Statist.25 (1997) 387-413], which penalize the total variation of the $k$th derivative.Empirically, we discover that trend filtering estimates adapt to the locallevel of smoothness much better than smoothing splines, and further, theyexhibit a remarkable similarity to locally adaptive regression splines. We alsoprovide theoretical support for these empirical findings; most notably, weprove that (with the right choice of tuning parameter) the trend filteringestimate converges to the true underlying function at the minimax rate forfunctions whose $k$th derivative is of bounded variation. This is done via anasymptotic pairing of trend filtering and locally adaptive regression splines,which have already been shown to converge at the minimax rate [Ann. Statist. 25(1997) 387-413]. At the core of this argument is a new result tying togetherthe fitted values of two lasso problems that share the same outcome vector, buthave different predictor matrices.
arxiv-5700-228 | The quasispecies regime for the simple genetic algorithm with ranking selection | http://arxiv.org/pdf/1403.5427v1.pdf | author:Raphaël Cerf category:math.PR cs.NE published:2014-03-21 summary:We study the simple genetic algorithm with a ranking selection mechanism(linear ranking or tournament). We denote by $\ell$ the length of thechromosomes, by $m$ the population size, by $p_C$ the crossover probability andby $p_M$ the mutation probability. We introduce a parameter $\sigma$, calledthe selection drift, which measures the selection intensity of the fittestchromosome. We show that the dynamics of the genetic algorithm depend in acritical way on the parameter $$\pi \,=\,\sigma(1-p_C)(1-p_M)^\ell\,.$$ If$\pi<1$, then the genetic algorithm operates in a disordered regime: anadvantageous mutant disappears with probability larger than $1-1/m^\beta$,where $\beta$ is a positive exponent. If $\pi>1$, then the genetic algorithmoperates in a quasispecies regime: an advantageous mutant invades a positivefraction of the population with probability larger than a constant $p^*$ (whichdoes not depend on $m$). We estimate next the probability of the occurrence ofa catastrophe (the whole population falls below a fitness level which waspreviously reached by a positive fraction of the population). The asymptoticresults suggest the following rules: $\pi=\sigma(1-p_C)(1-p_M)^\ell$ should beslightly larger than $1$; $p_M$ should be of order $1/\ell$; $m$ should belarger than $\ell\ln\ell$; the running time should be of exponential order in$m$. The first condition requires that $ \ell p_M +p_C< \ln\sigma$. Theseconclusions must be taken with great care: they come from an asymptotic regime,and it is a formidable task to understand the relevance of this regime for areal-world problem. At least, we hope that these conclusions provideinteresting guidelines for the practical implementation of the simple geneticalgorithm.
arxiv-5700-229 | Model-Driven Applications of Fractional Derivatives and Integrals | http://arxiv.org/pdf/1405.1999v1.pdf | author:William A. Sethares, Selçuk Ş. Bayın category:cs.CV published:2014-03-21 summary:Fractional order derivatives and integrals (differintegrals) are viewed froma frequency-domain perspective using the formalism of Riesz, providing acomputational tool as well as a way to interpret the operations in thefrequency domain. Differintegrals provide a logical extension of currenttechniques, generalizing the notion of integral and differential operators andacting as kind of frequency-domain filtering that has many of the advantages ofa nonlocal linear operator. Several important properties of differintegrals arepresented, and sample applications are given to one- and two-dimensionalsignals. Computer code to carry out the computations is made available on theauthor's website.
arxiv-5700-230 | Computing Entropy Rate Of Symbol Sources & A Distribution-free Limit Theorem | http://arxiv.org/pdf/1401.0711v2.pdf | author:Ishanu Chattopadhyay, Hod Lipson category:cs.IT cs.LG math.IT math.PR stat.CO stat.ML published:2014-01-03 summary:Entropy rate of sequential data-streams naturally quantifies the complexityof the generative process. Thus entropy rate fluctuations could be used as atool to recognize dynamical perturbations in signal sources, and couldpotentially be carried out without explicit background noise characterization.However, state of the art algorithms to estimate the entropy rate have markedlyslow convergence; making such entropic approaches non-viable in practice. Wepresent here a fundamentally new approach to estimate entropy rates, which isdemonstrated to converge significantly faster in terms of input data lengths,and is shown to be effective in diverse applications ranging from theestimation of the entropy rate of English texts to the estimation of complexityof chaotic dynamical systems. Additionally, the convergence rate of entropyestimates do not follow from any standard limit theorem, and reportedalgorithms fail to provide any confidence bounds on the computed values.Exploiting a connection to the theory of probabilistic automata, we establish aconvergence rate of $O(\log \vert s \vert/\sqrt[3]{\vert s \vert})$ as afunction of the input length $\vert s \vert$, which then yields explicituncertainty estimates, as well as required data lengths to satisfypre-specified confidence bounds.
arxiv-5700-231 | Using n-grams models for visual semantic place recognition | http://arxiv.org/pdf/1403.5370v1.pdf | author:Mathieu Dubois, Frenoux Emmanuelle, Philippe Tarroux category:stat.ML cs.CV cs.LG published:2014-03-21 summary:The aim of this paper is to present a new method for visual placerecognition. Our system combines global image characterization and visualwords, which allows to use efficient Bayesian filtering methods to integrateseveral images. More precisely, we extend the classical HMM model withtechniques inspired by the field of Natural Language Processing. This paperpresents our system and the Bayesian filtering algorithm. The performance ofour system and the influence of the main parameters are evaluated on a standarddatabase. The discussion highlights the interest of using such models andproposes improvements.
arxiv-5700-232 | An efficiency dependency parser using hybrid approach for tamil language | http://arxiv.org/pdf/1403.6381v1.pdf | author:K. Sureka, K. G. Srinivasagan, S. Suganthi category:cs.CL published:2014-03-21 summary:Natural language processing is a prompt research area across the country.Parsing is one of the very crucial tool in language analysis system which aimsto forecast the structural relationship among the words in a given sentence.Many researchers have already developed so many language tools but the accuracyis not meet out the human expectation level, thus the research is still exists.Machine translation is one of the major application area under Natural LanguageProcessing. While translation between one language to another language, thestructure identification of a sentence play a key role. This paper introducesthe hybrid way to solve the identification of relationship among the givenwords in a sentence. In existing system is implemented using rule basedapproach, which is not suited in huge amount of data. The machine learningapproaches is suitable for handle larger amount of data and also to get betteraccuracy via learning and training the system. The proposed approach takes aTamil sentence as an input and produce the result of a dependency relation as atree like structure using hybrid approach. This proposed tool is very helpfulfor researchers and act as an odd-on improve the quality of existingapproaches.
arxiv-5700-233 | Asymptotically Exact, Embarrassingly Parallel MCMC | http://arxiv.org/pdf/1311.4780v2.pdf | author:Willie Neiswanger, Chong Wang, Eric Xing category:stat.ML cs.DC cs.LG stat.CO published:2013-11-19 summary:Communication costs, resulting from synchronization requirements duringlearning, can greatly slow down many parallel machine learning algorithms. Inthis paper, we present a parallel Markov chain Monte Carlo (MCMC) algorithm inwhich subsets of data are processed independently, with very littlecommunication. First, we arbitrarily partition data onto multiple machines.Then, on each machine, any classical MCMC method (e.g., Gibbs sampling) may beused to draw samples from a posterior distribution given the data subset.Finally, the samples from each machine are combined to form samples from thefull posterior. This embarrassingly parallel algorithm allows each machine toact independently on a subset of the data (without communication) until thefinal combination stage. We prove that our algorithm generates asymptoticallyexact samples and empirically demonstrate its ability to parallelize burn-inand sampling in several models.
arxiv-5700-234 | SMML estimators for exponential families with continuous sufficient statistics | http://arxiv.org/pdf/1302.0581v2.pdf | author:James G. Dowty category:cs.IT math.IT math.ST stat.ML stat.TH published:2013-02-04 summary:The minimum message length principle is an information theoretic criterionthat links data compression with statistical inference. This paper studies thestrict minimum message length (SMML) estimator for $d$-dimensional exponentialfamilies with continuous sufficient statistics, for all $d \ge 1$. Thepartition of an SMML estimator is shown to consist of convex polytopes (i.e.convex polygons when $d=2$) which can be described explicitly in terms of theassertions and coding probabilities. While this result is known, we give a newproof based on the calculus of variations, and this approach gives someinteresting new inequalities for SMML estimators. We also use this result toconstruct an SMML estimator for a $2$-dimensional normal random variable withknown variance and a normal prior on its mean.
arxiv-5700-235 | A Physarum-Inspired Approach to Optimal Supply Chain Network Design at Minimum Total Cost with Demand Satisfaction | http://arxiv.org/pdf/1403.5345v1.pdf | author:Xiaoge Zhang, Andrew Adamatzky, Xin-She Yang, Hai Yang, Sankaran Mahadevan, Yong Deng category:cs.NE published:2014-03-21 summary:A supply chain is a system which moves products from a supplier to customers.The supply chains are ubiquitous. They play a key role in all economicactivities. Inspired by biological principles of nutrients' distribution inprotoplasmic networks of slime mould Physarum polycephalum we propose a novelalgorithm for a supply chain design. The algorithm handles the supply networkswhere capacity investments and product flows are variables. The networks areconstrained by a need to satisfy product demands. Two features of the slimemould are adopted in our algorithm. The first is the continuity of a fluxduring the iterative process, which is used in real-time update of the costsassociated with the supply links. The second feature is adaptivity. The supplychain can converge to an equilibrium state when costs are changed. Practicalityand flexibility of our algorithm is illustrated on numerical examples.
arxiv-5700-236 | Online Local Learning via Semidefinite Programming | http://arxiv.org/pdf/1403.5287v1.pdf | author:Paul Christiano category:cs.LG published:2014-03-20 summary:In many online learning problems we are interested in predicting localinformation about some universe of items. For example, we may want to knowwhether two items are in the same cluster rather than computing an assignmentof items to clusters; we may want to know which of two teams will win a gamerather than computing a ranking of teams. Although finding the optimalclustering or ranking is typically intractable, it may be possible to predictthe relationships between items as well as if you could solve the globaloptimization problem exactly. Formally, we consider an online learning problem in which a learnerrepeatedly guesses a pair of labels (l(x), l(y)) and receives an adversarialpayoff depending on those labels. The learner's goal is to receive a payoffnearly as good as the best fixed labeling of the items. We show that a simplealgorithm based on semidefinite programming can obtain asymptotically optimalregret in the case where the number of possible labels is O(1), resolving anopen problem posed by Hazan, Kale, and Shalev-Schwartz. Our main technicalcontribution is a novel use and analysis of the log determinant regularizer,exploiting the observation that log det(A + I) upper bounds the entropy of anydistribution with covariance matrix A.
arxiv-5700-237 | Review of Face Detection Systems Based Artificial Neural Networks Algorithms | http://arxiv.org/pdf/1404.1292v1.pdf | author:Omaima N. A. AL-Allaf category:cs.CV cs.NE published:2014-03-20 summary:Face detection is one of the most relevant applications of image processingand biometric systems. Artificial neural networks (ANN) have been used in thefield of image processing and pattern recognition. There is lack of literaturesurveys which give overview about the studies and researches related to theusing of ANN in face detection. Therefore, this research includes a generalreview of face detection studies and systems which based on different ANNapproaches and algorithms. The strengths and limitations of these literaturestudies and systems were included also.
arxiv-5700-238 | Smoothing Dynamic Systems with State-Dependent Covariance Matrices | http://arxiv.org/pdf/1211.4601v2.pdf | author:Aleksandr Y. Aravkin, James V. Burke category:math.OC stat.CO stat.ML 62F35, 65K10 published:2012-11-19 summary:Kalman filtering and smoothing algorithms are used in many areas, includingtracking and navigation, medical applications, and financial trend filtering.One of the basic assumptions required to apply the Kalman smoothing frameworkis that error covariance matrices are known and given. In this paper, we studya general class of inference problems where covariance matrices can dependfunctionally on unknown parameters. In the Kalman framework, this allowsmodeling situations where covariance matrices may depend functionally on thestate sequence being estimated. We present an extended formulation andgeneralized Gauss-Newton (GGN) algorithm for inference in this context. Whenapplied to dynamic systems inference, we show the algorithm can be implementedto preserve the computational efficiency of the classic Kalman smoother. Thenew approach is illustrated with a synthetic numerical example.
arxiv-5700-239 | Sparse Learning over Infinite Subgraph Features | http://arxiv.org/pdf/1403.5177v1.pdf | author:Ichigaku Takigawa, Hiroshi Mamitsuka category:stat.ML published:2014-03-20 summary:We present a supervised-learning algorithm from graph data (a set of graphs)for arbitrary twice-differentiable loss functions and sparse linear models overall possible subgraph features. To date, it has been shown that under allpossible subgraph features, several types of sparse learning, such as Adaboost,LPBoost, LARS/LASSO, and sparse PLS regression, can be performed. Particularlyemphasis is placed on simultaneous learning of relevant features from aninfinite set of candidates. We first generalize techniques used in all thesepreceding studies to derive an unifying bounding technique for arbitraryseparable functions. We then carefully use this bounding to make blockcoordinate gradient descent feasible over infinite subgraph features, resultingin a fast converging algorithm that can solve a wider class of sparse learningproblems over graph data. We also empirically study the differences from theexisting approaches in convergence property, selected subgraph features, andsearch-space sizes. We further discuss several unnoticed issues in sparselearning over all possible subgraph features.
arxiv-5700-240 | Multilingual Distributed Representations without Word Alignment | http://arxiv.org/pdf/1312.6173v4.pdf | author:Karl Moritz Hermann, Phil Blunsom category:cs.CL published:2013-12-20 summary:Distributed representations of meaning are a natural way to encode covariancerelationships between words and phrases in NLP. By overcoming data sparsityproblems, as well as providing information about semantic relatedness which isnot available in discrete representations, distributed representations haveproven useful in many NLP tasks. Recent work has shown how compositionalsemantic representations can successfully be applied to a number of monolingualapplications such as sentiment analysis. At the same time, there has been someinitial success in work on learning shared word-level representations acrosslanguages. We combine these two approaches by proposing a method for learningdistributed representations in a multilingual setup. Our model learns to assignsimilar embeddings to aligned sentences and dissimilar ones to sentence whichare not aligned while not requiring word alignments. We show that ourrepresentations are semantically informative and apply them to a cross-lingualdocument classification task where we outperform the previous state of the art.Further, by employing parallel corpora of multiple language pairs we find thatour model learns representations that capture semantic relationships acrosslanguages for which no parallel data was used.
arxiv-5700-241 | Unconfused Ultraconservative Multiclass Algorithms | http://arxiv.org/pdf/1403.5115v1.pdf | author:Ugo Louche, Liva Ralaivola category:cs.LG published:2014-03-20 summary:We tackle the problem of learning linear classifiers from noisy datasets in amulticlass setting. The two-class version of this problem was studied a fewyears ago by, e.g. Bylander (1994) and Blum et al. (1996): in thesecontributions, the proposed approaches to fight the noise revolve around aPerceptron learning scheme fed with peculiar examples computed through aweighted average of points from the noisy training set. We propose to buildupon these approaches and we introduce a new algorithm called UMA (forUnconfused Multiclass additive Algorithm) which may be seen as a generalizationto the multiclass setting of the previous approaches. In order to characterizethe noise we use the confusion matrix as a multiclass extension of theclassification noise studied in the aforementioned literature. Theoreticallywell-founded, UMA furthermore displays very good empirical noise robustness, asevidenced by numerical simulations conducted on both synthetic and real data.Keywords: Multiclass classification, Perceptron, Noisy labels, Confusion Matrix
arxiv-5700-242 | On The Sample Complexity of Sparse Dictionary Learning | http://arxiv.org/pdf/1403.5112v1.pdf | author:Matthias Seibert, Martin Kleinsteuber, Rémi Gribonval, Rodolphe Jenatton, Francis Bach category:stat.ML published:2014-03-20 summary:In the synthesis model signals are represented as a sparse combinations ofatoms from a dictionary. Dictionary learning describes the acquisition processof the underlying dictionary for a given set of training samples. While ideallythis would be achieved by optimizing the expectation of the factors over theunderlying distribution of the training data, in practice the necessaryinformation about the distribution is not available. Therefore, in real worldapplications it is achieved by minimizing an empirical average over theavailable samples. The main goal of this paper is to provide a samplecomplexity estimate that controls to what extent the empirical average deviatesfrom the cost function. This estimate then provides a suitable estimate to theaccuracy of the representation of the learned dictionary. The presentedapproach exemplifies the general results proposed by the authors in SampleComplexity of Dictionary Learning and other Matrix Factorizations, Gribonval etal. and gives more concrete bounds of the sample complexity of dictionarylearning. We cover a variety of sparsity measures employed in the learningprocedure.
arxiv-5700-243 | Robust Binary Fused Compressive Sensing using Adaptive Outlier Pursuit | http://arxiv.org/pdf/1402.5076v2.pdf | author:Xiangrong Zeng, Mário A. T. Figueiredo category:cs.CV cs.IT math.IT published:2014-02-20 summary:We propose a new method, {\it robust binary fused compressive sensing}(RoBFCS), to recover sparse piece-wise smooth signals from 1-bit compressivemeasurements. The proposed method is a modification of our previous {\it binaryfused compressive sensing} (BFCS) algorithm, which is based on the {\it binaryiterative hard thresholding} (BIHT) algorithm. As in BIHT, the data term of theobjective function is a one-sided $\ell_1$ (or $\ell_2$) norm. Experiments showthat the proposed algorithm is able to take advantage of the piece-wisesmoothness of the original signal and detect sign flips and correct them,achieving more accurate recovery than BFCS and BIHT.
arxiv-5700-244 | Clinical TempEval | http://arxiv.org/pdf/1403.4928v1.pdf | author:Steven Bethard, Leon Derczynski, James Pustejovsky, Marc Verhagen category:cs.CL published:2014-03-19 summary:We describe the Clinical TempEval task which is currently in preparation forthe SemEval-2015 evaluation exercise. This task involves identifying anddescribing events, times and the relations between them in clinical text. Sixdiscrete subtasks are included, focusing on recognising mentions of times andevents, describing those mentions for both entity types, identifying therelation between an event and the document creation time, and identifyingnarrative container relations.
arxiv-5700-245 | Using Entropy Estimates for DAG-Based Ontologies | http://arxiv.org/pdf/1403.4887v1.pdf | author:Andrew Warren, Joao Setubal category:cs.CL published:2014-03-19 summary:Motivation: Entropy measurements on hierarchical structures have been used inmethods for information retrieval and natural language modeling. Here weexplore its application to semantic similarity. By finding shared ontologyterms, semantic similarity can be established between annotated genes. A commonprocedure for establishing semantic similarity is to calculate thedescriptiveness (information content) of ontology terms and use these values todetermine the similarity of annotations. Most often information content iscalculated for an ontology term by analyzing its frequency in an annotationcorpus. The inherent problems in using these values to model functionalsimilarity motivates our work. Summary: We present a novel calculation forestablishing the entropy of a DAG-based ontology, which can be used in analternative method for establishing the information content of its terms. Wealso compare our IC metric to two others using semantic and sequencesimilarity.
arxiv-5700-246 | Evolutionary Algorithm for Drug Discovery Interim Design Report | http://arxiv.org/pdf/1403.4871v1.pdf | author:Mark Shackelford category:cs.NE cs.CE published:2014-03-19 summary:A software program which aims to provide an exploration capability over theSearch Space of potential drug molecules. The program explores the search spaceby generating random molecules, determining their fitness and then breeding anew generation from the fittest individuals. The search space, in theory anycombination of any elements in any order, is constrained by the use of a subsetof elements and a list of fragments, molecular parts that are known to beuseful in drug development. The resultant molecules from each generation arestored in a searchable database, so that the user can browse through previousgenerations looking for interesting molecules.
arxiv-5700-247 | Balancing Sparsity and Rank Constraints in Quadratic Basis Pursuit | http://arxiv.org/pdf/1403.4267v2.pdf | author:Cagdas Bilen, Gilles Puy, Rémi Gribonval, Laurent Daudet category:cs.NA cs.LG published:2014-03-17 summary:We investigate the methods that simultaneously enforce sparsity and low-rankstructure in a matrix as often employed for sparse phase retrieval problems orphase calibration problems in compressive sensing. We propose a new approachfor analyzing the trade off between the sparsity and low rank constraints inthese approaches which not only helps to provide guidelines to adjust theweights between the aforementioned constraints, but also enables new simulationstrategies for evaluating performance. We then provide simulation results forphase retrieval and phase calibration cases both to demonstrate the consistencyof the proposed method with other approaches and to evaluate the change ofperformance with different weights for the sparsity and low rank structureconstraints.
arxiv-5700-248 | A Split-and-Merge Dictionary Learning Algorithm for Sparse Representation | http://arxiv.org/pdf/1403.4781v1.pdf | author:Subhadip Mukherjee, Chandra Sekhar Seelamantula category:cs.LG stat.ML published:2014-03-19 summary:In big data image/video analytics, we encounter the problem of learning anovercomplete dictionary for sparse representation from a large trainingdataset, which can not be processed at once because of storage andcomputational constraints. To tackle the problem of dictionary learning in suchscenarios, we propose an algorithm for parallel dictionary learning. Thefundamental idea behind the algorithm is to learn a sparse representation intwo phases. In the first phase, the whole training dataset is partitioned intosmall non-overlapping subsets, and a dictionary is trained independently oneach small database. In the second phase, the dictionaries are merged to form aglobal dictionary. We show that the proposed algorithm is efficient in itsusage of memory and computational complexity, and performs on par with thestandard learning strategy operating on the entire data at a time. As anapplication, we consider the problem of image denoising. We present acomparative analysis of our algorithm with the standard learning techniques,that use the entire database at a time, in terms of training and denoisingperformance. We observe that the split-and-merge algorithm results in aremarkable reduction of training time, without significantly affecting thedenoising performance.
arxiv-5700-249 | MFCC based Enlargement of the Training Set for Emotion Recognition in Speech | http://arxiv.org/pdf/1403.4777v1.pdf | author:Inma Mohino-Herranz, Roberto Gil-Pita, Sagrario Alonso-Diaz, Manuel Rosa-Zurera category:cs.CV published:2014-03-19 summary:Emotional state recognition through speech is being a very interestingresearch topic nowadays. Using subliminal information of speech, denominated asprosody, it is possible to recognize the emotional state of the person. One ofthe main problems in the design of automatic emotion recognition systems is thesmall number of available patterns. This fact makes the learning process moredifficult, due to the generalization problems that arise under theseconditions. In this work we propose a solution to this problem consisting inenlarging the training set through the creation the new virtual patterns. Inthe case of emotional speech, most of the emotional information is included inspeed and pitch variations. So, a change in the average pitch that does notmodify neither the speed nor the pitch variations does not affect the expressedemotion. Thus, we use this prior information in order to create new patternsapplying a gender dependent pitch shift modification in the feature extractionprocess of the classification system. For this purpose, we propose a frequencyscaling modification of the Mel Frequency Cepstral Coefficients, used toclassify the emotion. For this purpose, we propose a gender dependent frequencyscaling modification. This proposed process allows us to synthetically increasethe number of available patterns in the training set, thus increasing thegeneralization capability of the system and reducing the test error. Resultscarried out with two different classifiers with different degree ofgeneralization capability demonstrate the suitability of the proposal.
arxiv-5700-250 | Spelling Error Trends and Patterns in Sindhi | http://arxiv.org/pdf/1403.4759v1.pdf | author:Zeeshan Bhatti, Imdad Ali Ismaili, Asad Ali Shaikh, Waseem Javaid category:cs.CL published:2014-03-19 summary:Statistical error Correction technique is the most accurate and widely usedapproach today, but for a language like Sindhi which is a low resourcedlanguage the trained corpora's are not available, so the statistical techniquesare not possible at all. Instead a useful alternative would be to exploitvarious spelling error trends in Sindhi by using a Rule based approach. Fordesigning such technique an essential prerequisite would be to study thevarious error patterns in a language. This pa per presents various studies ofspelling error trends and their types in Sindhi Language. The research showsthat the error trends common to all languages are also encountered in Sindhibut their do exist some error patters that are catered specifically to a Sindhilanguage.
arxiv-5700-251 | Study on performance improvement of oil paint image filter algorithm using parallel pattern library | http://arxiv.org/pdf/1405.1020v1.pdf | author:Siddhartha Mukherjee category:cs.CV cs.DC published:2014-03-19 summary:This paper gives a detailed study on the performance of oil paint imagefilter algorithm with various parameters applied on an image of RGB model. OilPaint image processing, being very performance hungry, current research triesto find improvement using parallel pattern library. With increasingkernel-size, the processing time of oil paint image filter algorithm increasesexponentially.
arxiv-5700-252 | A Proximal Stochastic Gradient Method with Progressive Variance Reduction | http://arxiv.org/pdf/1403.4699v1.pdf | author:Lin Xiao, Tong Zhang category:math.OC stat.ML published:2014-03-19 summary:We consider the problem of minimizing the sum of two convex functions: one isthe average of a large number of smooth component functions, and the other is ageneral convex function that admits a simple proximal mapping. We assume thewhole objective function is strongly convex. Such problems often arise inmachine learning, known as regularized empirical risk minimization. We proposeand analyze a new proximal stochastic gradient method, which uses a multi-stagescheme to progressively reduce the variance of the stochastic gradient. Whileeach iteration of this algorithm has similar cost as the classical stochasticgradient method (or incremental gradient method), we show that the expectedobjective value converges to the optimum at a geometric rate. The overallcomplexity of this method is much lower than both the proximal full gradientmethod and the standard proximal stochastic gradient method.
arxiv-5700-253 | Structured Sparse Method for Hyperspectral Unmixing | http://arxiv.org/pdf/1403.4682v1.pdf | author:Feiyun Zhu, Ying Wang, Shiming Xiang, Bin Fan, Chunhong Pan category:cs.CV cs.AI published:2014-03-19 summary:Hyperspectral Unmixing (HU) has received increasing attention in the pastdecades due to its ability of unveiling information latent in hyperspectraldata. Unfortunately, most existing methods fail to take advantage of thespatial information in data. To overcome this limitation, we propose aStructured Sparse regularized Nonnegative Matrix Factorization (SS-NMF) methodfrom the following two aspects. First, we incorporate a graph Laplacian toencode the manifold structures embedded in the hyperspectral data space. Inthis way, the highly similar neighboring pixels can be grouped together.Second, the lasso penalty is employed in SS-NMF for the fact that pixels in thesame manifold structure are sparsely mixed by a common set of relevant bases.These two factors act as a new structured sparse constraint. With thisconstraint, our method can learn a compact space, where highly similar pixelsare grouped to share correlated sparse representations. Experiments on realhyperspectral data sets with different noise levels demonstrate that our methodoutperforms the state-of-the-art methods significantly.
arxiv-5700-254 | Bayesian Source Separation Applied to Identifying Complex Organic Molecules in Space | http://arxiv.org/pdf/1403.4626v1.pdf | author:Kevin H. Knuth, Man Kit Tse, Joshua Choinsky, Haley A. Maunu, Duane F. Carbon category:astro-ph.IM stat.ML published:2014-03-18 summary:Emission from a class of benzene-based molecules known as Polycyclic AromaticHydrocarbons (PAHs) dominates the infrared spectrum of star-forming regions.The observed emission appears to arise from the combined emission of numerousPAH species, each with its unique spectrum. Linear superposition of the PAHspectra identifies this problem as a source separation problem. It is, however,of a formidable class of source separation problems given that different PAHsources potentially number in the hundreds, even thousands, and there is onlyone measured spectral signal for a given astrophysical site. Fortunately, thesource spectra of the PAHs are known, but the signal is also contaminated byother spectral sources. We describe our ongoing work in developing Bayesiansource separation techniques relying on nested sampling in conjunction with anON/OFF mechanism enabling simultaneous estimation of the probability that aparticular PAH species is present and its contribution to the spectrum.
arxiv-5700-255 | Can Cascades be Predicted? | http://arxiv.org/pdf/1403.4608v1.pdf | author:Justin Cheng, Lada A. Adamic, P. Alex Dow, Jon Kleinberg, Jure Leskovec category:cs.SI physics.soc-ph stat.ML H.2.8 published:2014-03-18 summary:On many social networking web sites such as Facebook and Twitter, resharingor reposting functionality allows users to share others' content with their ownfriends or followers. As content is reshared from user to user, large cascadesof reshares can form. While a growing body of research has focused on analyzingand characterizing such cascades, a recent, parallel line of work has arguedthat the future trajectory of a cascade may be inherently unpredictable. Inthis work, we develop a framework for addressing cascade prediction problems.On a large sample of photo reshare cascades on Facebook, we find strongperformance in predicting whether a cascade will continue to grow in thefuture. We find that the relative growth of a cascade becomes more predictableas we observe more of its reshares, that temporal and structural features arekey predictors of cascade size, and that initially, breadth, rather than depthin a cascade is a better indicator of larger cascades. This predictionperformance is robust in the sense that multiple distinct classes of featuresall achieve similar performance. We also discover that temporal features arepredictive of a cascade's eventual shape. Observing independent cascades of thesame content, we find that while these cascades differ greatly in size, we arestill able to predict which ends up the largest.
arxiv-5700-256 | Language Heedless of Logic - Philosophy Mindful of What? Failures of Distributive and Absorption Laws | http://arxiv.org/pdf/1403.3668v2.pdf | author:Arthur Merin category:cs.CL published:2014-03-14 summary:Much of philosophical logic and all of philosophy of language make empiricalclaims about the vernacular natural language. They presume semantics underwhich `and' and `or' are related by the dually paired distributive andabsorption laws. However, at least one of each pair of laws fails in thevernacular. `Implicature'-based auxiliary theories associated with theprogramme of H.P. Grice do not prove remedial. Conceivable alternatives thatmight replace the familiar logics as descriptive instruments are briefly noted:(i) substructural logics and (ii) meaning composition in linear algebras overthe reals, occasionally constrained by norms of classical logic. Alternative(ii) locates the problem in violations of one of the idempotent laws. Reasonsfor a lack of curiosity about elementary and easily testable implications ofthe received theory are considered. The concept of `reflective equilibrium' iscritically examined for its role in reconciling normative desiderata anddescriptive commitments.
arxiv-5700-257 | Complex-Valued Autoencoders | http://arxiv.org/pdf/1108.4135v2.pdf | author:Pierre Baldi, Zhiqin Lu category:cs.NE math.RA published:2011-08-20 summary:Autoencoders are unsupervised machine learning circuits whose learning goalis to minimize a distortion measure between inputs and outputs. Linearautoencoders can be defined over any field and only real-valued linearautoencoder have been studied so far. Here we study complex-valued linearautoencoders where the components of the training vectors and adjustablematrices are defined over the complex field with the $L_2$ norm. We providesimpler and more general proofs that unify the real-valued and complex-valuedcases, showing that in both cases the landscape of the error function isinvariant under certain groups of transformations. The landscape has no localminima, a family of global minima associated with Principal Component Analysis,and many families of saddle points associated with orthogonal projections ontosub-space spanned by sub-optimal subsets of eigenvectors of the covariancematrix. The theory yields several iterative, convergent, learning algorithms, aclear understanding of the generalization properties of the trainedautoencoders, and can equally be applied to the hetero-associative case whenexternal targets are provided. Partial results on deep architecture as well asthe differential geometry of autoencoders are also presented. The generalframework described here is useful to classify autoencoders and identifygeneral common properties that ought to be investigated for each class,illuminating some of the connections between information theory, unsupervisedlearning, clustering, Hebbian learning, and autoencoders.
arxiv-5700-258 | Similarity networks for classification: a case study in the Horse Colic problem | http://arxiv.org/pdf/1403.4540v1.pdf | author:Lluís Belanche, Jerónimo Hernández category:cs.LG cs.NE published:2014-03-18 summary:This paper develops a two-layer neural network in which the neuron modelcomputes a user-defined similarity function between inputs and weights. Theneuron transfer function is formed by composition of an adapted logisticfunction with the mean of the partial input-weight similarities. The resultingneuron model is capable of dealing directly with variables of potentiallydifferent nature (continuous, fuzzy, ordinal, categorical). There is alsoprovision for missing values. The network is trained using a two-stageprocedure very similar to that used to train a radial basis function (RBF)neural network. The network is compared to two types of RBF networks in anon-trivial dataset: the Horse Colic problem, taken as a case study andanalyzed in detail.
arxiv-5700-259 | Splitting Methods for Convex Clustering | http://arxiv.org/pdf/1304.0499v2.pdf | author:Eric C. Chi, Kenneth Lange category:stat.ML math.NA math.OC stat.CO published:2013-04-01 summary:Clustering is a fundamental problem in many scientific applications. Standardmethods such as $k$-means, Gaussian mixture models, and hierarchicalclustering, however, are beset by local minima, which are sometimes drasticallysuboptimal. Recently introduced convex relaxations of $k$-means andhierarchical clustering shrink cluster centroids toward one another and ensurea unique global minimizer. In this work we present two splitting methods forsolving the convex clustering problem. The first is an instance of thealternating direction method of multipliers (ADMM); the second is an instanceof the alternating minimization algorithm (AMA). In contrast to previouslyconsidered algorithms, our ADMM and AMA formulations provide simple and unifiedframeworks for solving the convex clustering problem under the previouslystudied norms and open the door to potentially novel norms. We demonstrate theperformance of our algorithm on both simulated and real data examples. Whilethe differences between the two algorithms appear to be minor on the surface,complexity analysis and numerical experiments show AMA to be significantly moreefficient.
arxiv-5700-260 | Parallel Coordinate Descent Newton Method for Efficient $\ell_1$-Regularized Minimization | http://arxiv.org/pdf/1306.4080v3.pdf | author:Yatao Bian, Xiong Li, Yuncai Liu, Ming-Hsuan Yang category:cs.LG cs.NA published:2013-06-18 summary:The recent years have witnessed advances in parallel algorithms for largescale optimization problems. Notwithstanding demonstrated success, existingalgorithms that parallelize over features are usually limited by divergenceissues under high parallelism or require data preprocessing to alleviate theseproblems. In this work, we propose a Parallel Coordinate Descent Newtonalgorithm using multidimensional approximate Newton steps (PCDN), where theoff-diagonal elements of the Hessian are set to zero to enable parallelization.It randomly partitions the feature set into $b$ bundles/subsets with size of$P$, and sequentially processes each bundle by first computing the descentdirections for each feature in parallel and then conducting $P$-dimensionalline search to obtain the step size. We show that: (1) PCDN is guaranteed toconverge globally despite increasing parallelism; (2) PCDN converges to thespecified accuracy $\epsilon$ within the limited iteration number of$T_\epsilon$, and $T_\epsilon$ decreases with increasing parallelism (bundlesize $P$). Using the implementation technique of maintaining intermediatequantities, we minimize the data transfer and synchronization cost of the$P$-dimensional line search. For concreteness, the proposed PCDN algorithm isapplied to $\ell_1$-regularized logistic regression and $\ell_2$-loss SVM.Experimental evaluations on six benchmark datasets show that the proposed PCDNalgorithm exploits parallelism well and outperforms the state-of-the-artmethods in speed without losing accuracy.
arxiv-5700-261 | Sign Language Gibberish for syntactic parsing evaluation | http://arxiv.org/pdf/1403.4473v1.pdf | author:Rémi Dubot, Christophe Collet category:cs.CL published:2014-03-18 summary:Sign Language (SL) automatic processing slowly progresses bottom-up. Thefield has seen proposition to handle the video signal, to recognize andsynthesize sublexical and lexical units. It starts to see the development ofsupra-lexical processing. But the recognition, at this level, lacks data. Thesyntax of SL appears very specific as it uses massively the multiplicity ofarticulators and its access to the spatial dimensions. Therefore new parsingtechniques are developed. However these need to be evaluated. The shortage onreal data restrains the corpus-based models to small sizes. We propose here asolution to produce data-sets for the evaluation of parsers on the specificproperties of SL. The article first describes the general model used togenerates dependency grammars and the phrase generation from these lasts. Itthen discusses the limits of approach. The solution shows to be of particularinterest to evaluate the scalability of the techniques on big models.
arxiv-5700-262 | Word Emdeddings through Hellinger PCA | http://arxiv.org/pdf/1312.5542v2.pdf | author:Rémi Lebret, Ronan Collobert category:cs.CL cs.LG published:2013-12-19 summary:Word embeddings resulting from neural language models have been shown to besuccessful for a large variety of NLP tasks. However, such architecture mightbe difficult to train and time-consuming. Instead, we propose to drasticallysimplify the word embeddings computation through a Hellinger PCA of the wordco-occurence matrix. We compare those new word embeddings with some well-knownembeddings on NER and movie review tasks and show that we can reach similar oreven better performance. Although deep learning is not really necessary forgenerating good word embeddings, we show that it can provide an easy way toadapt embeddings to specific tasks.
arxiv-5700-263 | Spectral Clustering with Jensen-type kernels and their multi-point extensions | http://arxiv.org/pdf/1403.4378v1.pdf | author:Debarghya Ghoshdastidar, Ambedkar Dukkipati, Ajay P. Adsul, Aparna S. Vijayan category:cs.LG published:2014-03-18 summary:Motivated by multi-distribution divergences, which originate in informationtheory, we propose a notion of `multi-point' kernels, and study theirapplications. We study a class of kernels based on Jensen type divergences andshow that these can be extended to measure similarity among multiple points. Westudy tensor flattening methods and develop a multi-point (kernel) spectralclustering (MSC) method. We further emphasize on a special case of the proposedkernels, which is a multi-point extension of the linear (dot-product) kerneland show the existence of cubic time tensor flattening algorithm in this case.Finally, we illustrate the usefulness of our contributions using standard datasets and image segmentation tasks.
arxiv-5700-264 | Concept Based vs. Pseudo Relevance Feedback Performance Evaluation for Information Retrieval System | http://arxiv.org/pdf/1403.4362v1.pdf | author:Mohammed El Amine Abderrahim category:cs.IR cs.CL published:2014-03-18 summary:This article evaluates the performance of two techniques for queryreformulation in a system for information retrieval, namely, the concept basedand the pseudo relevance feedback reformulation. The experiments performed on acorpus of Arabic text have allowed us to compare the contribution of these tworeformulation techniques in improving the performance of an informationretrieval system for Arabic texts.
arxiv-5700-265 | Transduction on Directed Graphs via Absorbing Random Walks | http://arxiv.org/pdf/1402.4566v2.pdf | author:Jaydeep De, Xiaowei Zhang, Li Cheng category:cs.CV cs.LG stat.ML published:2014-02-19 summary:In this paper we consider the problem of graph-based transductiveclassification, and we are particularly interested in the directed graphscenario which is a natural form for many real world applications. Differentfrom existing research efforts that either only deal with undirected graphs orcircumvent directionality by means of symmetrization, we propose a novel randomwalk approach on directed graphs using absorbing Markov chains, which can beregarded as maximizing the accumulated expected number of visits from theunlabeled transient states. Our algorithm is simple, easy to implement, andworks with large-scale graphs. In particular, it is capable of preserving thegraph structure even when the input graph is sparse and changes over time, aswell as retaining weak signals presented in the directed edges. We present itsintimate connections to a number of existing methods, including graph kernels,graph Laplacian based methods, and interestingly, spanning forest of graphs.Its computational complexity and the generalization error are also studied.Empirically our algorithm is systematically evaluated on a wide range ofapplications, where it has shown to perform competitively comparing to a suiteof state-of-the-art methods.
arxiv-5700-266 | Active Discovery of Network Roles for Predicting the Classes of Network Nodes | http://arxiv.org/pdf/1312.7258v2.pdf | author:Leto Peel category:cs.LG cs.SI stat.ML published:2013-12-27 summary:Nodes in real world networks often have class labels, or underlyingattributes, that are related to the way in which they connect to other nodes.Sometimes this relationship is simple, for instance nodes of the same class aremay be more likely to be connected. In other cases, however, this is not true,and the way that nodes link in a network exhibits a different, more complexrelationship to their attributes. Here, we consider networks in which we knowhow the nodes are connected, but we do not know the class labels of the nodesor how class labels relate to the network links. We wish to identify the bestsubset of nodes to label in order to learn this relationship between nodeattributes and network links. We can then use this discovered relationship toaccurately predict the class labels of the rest of the network nodes. We present a model that identifies groups of nodes with similar linkpatterns, which we call network roles, using a generative blockmodel. The modelthen predicts labels by learning the mapping from network roles to class labelsusing a maximum margin classifier. We choose a subset of nodes to labelaccording to an iterative margin-based active learning strategy. By integratingthe discovery of network roles with the classifier optimisation, the activelearning process can adapt the network roles to better represent the networkfor node classification. We demonstrate the model by exploring a selection ofreal world networks, including a marine food web and a network of Englishwords. We show that, in contrast to other network classifiers, this modelachieves good classification accuracy for a range of networks with differentrelationships between class labels and network links.
arxiv-5700-267 | Rotationally Invariant Image Representation for Viewing Direction Classification in Cryo-EM | http://arxiv.org/pdf/1309.7643v4.pdf | author:Zhizhen Zhao, Amit Singer category:q-bio.BM cs.CV published:2013-09-29 summary:We introduce a new rotationally invariant viewing angle classification methodfor identifying, among a large number of Cryo-EM projection images, similarviews without prior knowledge of the molecule. Our rotationally invariantfeatures are based on the bispectrum. Each image is denoised and compressedusing steerable principal component analysis (PCA) such that rotating an imageis equivalent to phase shifting the expansion coefficients. Thus we are able toextend the theory of bispectrum of 1D periodic signals to 2D images. Therandomized PCA algorithm is then used to efficiently reduce the dimensionalityof the bispectrum coefficients, enabling fast computation of the similaritybetween any pair of images. The nearest neighbors provide an initialclassification of similar viewing angles. In this way, rotational alignment isonly performed for images with their nearest neighbors. The initial nearestneighbor classification and alignment are further improved by a newclassification method called vector diffusion maps. Our pipeline for viewingangle classification and alignment is experimentally shown to be faster andmore accurate than reference-free alignment with rotationally invariant K-meansclustering, MSA/MRA 2D classification, and their modern approximations.
arxiv-5700-268 | Automatic Image Registration in Infrared-Visible Videos using Polygon Vertices | http://arxiv.org/pdf/1403.4232v1.pdf | author:Tanushri Chakravorty, Guillaume-Alexandre Bilodeau, Eric Granger category:cs.CV published:2014-03-17 summary:In this paper, an automatic method is proposed to perform image registrationin visible and infrared pair of video sequences for multiple targets. Inmultimodal image analysis like image fusion systems, color and IR sensors areplaced close to each other and capture a same scene simultaneously, but thevideos are not properly aligned by default because of different fields of view,image capturing information, working principle and other camera specifications.Because the scenes are usually not planar, alignment needs to be performedcontinuously by extracting relevant common information. In this paper, weapproximate the shape of the targets by polygons and use affine transformationfor aligning the two video sequences. After background subtraction, keypointson the contour of the foreground blobs are detected using DCE (Discrete CurveEvolution)technique. These keypoints are then described by the local shape ateach point of the obtained polygon. The keypoints are matched based on theconvexity of polygon's vertices and Euclidean distance between them. Only goodmatches for each local shape polygon in a frame, are kept. To achieve a globalaffine transformation that maximises the overlapping of infrared and visibleforeground pixels, the matched keypoints of each local shape polygon are storedtemporally in a buffer for a few number of frames. The matrix is evaluated ateach frame using the temporal buffer and the best matrix is selected, based onan overlapping ratio criterion. Our experimental results demonstrate that thismethod can provide highly accurate registered images and that we outperform aprevious related method.
arxiv-5700-269 | A reversible infinite HMM using normalised random measures | http://arxiv.org/pdf/1403.4206v1.pdf | author:Konstantina Palla, David A. Knowles, Zoubin Ghahramani category:stat.ML published:2014-03-17 summary:We present a nonparametric prior over reversible Markov chains. We usecompletely random measures, specifically gamma processes, to construct acountably infinite graph with weighted edges. By enforcing symmetry to make theedges undirected we define a prior over random walks on graphs that results ina reversible Markov chain. The resulting prior over infinite transitionmatrices is closely related to the hierarchical Dirichlet process but enforcesreversibility. A reinforcement scheme has recently been proposed with similarproperties, but the de Finetti measure is not well characterised. We take thealternative approach of explicitly constructing the mixing measure, whichallows more straightforward and efficient inference at the cost of no longerhaving a closed form predictive distribution. We use our process to construct areversible infinite HMM which we apply to two real datasets, one fromepigenomics and one ion channel recording.
arxiv-5700-270 | Computer Vision Accelerators for Mobile Systems based on OpenCL GPGPU Co-Processing | http://arxiv.org/pdf/1403.4238v1.pdf | author:Guohui Wang, Yingen Xiong, Jay Yun, Joseph R. Cavallaro category:cs.DC cs.CV cs.MS published:2014-03-17 summary:In this paper, we present an OpenCL-based heterogeneous implementation of acomputer vision algorithm -- image inpainting-based object removal algorithm --on mobile devices. To take advantage of the computation power of the mobileprocessor, the algorithm workflow is partitioned between the CPU and the GPUbased on the profiling results on mobile devices, so that thecomputationally-intensive kernels are accelerated by the mobile GPGPU(general-purpose computing using graphics processing units). By exploring theimplementation trade-offs and utilizing the proposed optimization strategies atdifferent levels including algorithm optimization, parallelism optimization,and memory access optimization, we significantly speed up the algorithm withthe CPU-GPU heterogeneous implementation, while preserving the quality of theoutput images. Experimental results show that heterogeneous computing based onGPGPU co-processing can significantly speed up the computer vision algorithmsand makes them practical on real-world mobile devices.
arxiv-5700-271 | A-infinity Persistence | http://arxiv.org/pdf/1403.2395v3.pdf | author:Francisco Belchí Guillamón, Aniceto Murillo Mas category:math.AT cs.CG cs.CV published:2014-03-10 summary:We introduce and study A-infinity persistence of a given homology filtrationof topological spaces. This is a family, one for each n > 0, of homologicalinvariants which provide information not readily available by the (persistent)Betti numbers of the given filtration. This may help to detect noise, not justin the simplicial structure of the filtration but in further geometricalproperties in which the higher codiagonals of the A-infinity structure aretranslated. Based in the classification of zigzag modules, a characterizationof the A-infinity persistence in terms of its associated barcode is given.
arxiv-5700-272 | Multi-task Feature Selection based Anomaly Detection | http://arxiv.org/pdf/1403.4017v1.pdf | author:Longqi Yang, Yibing Wang, Zhisong Pan, Guyu Hu category:stat.ML cs.LG published:2014-03-17 summary:Network anomaly detection is still a vibrant research area. As the fastgrowth of network bandwidth and the tremendous traffic on the network, therearises an extremely challengeable question: How to efficiently and accuratelydetect the anomaly on multiple traffic? In multi-task learning, the trafficconsisting of flows at different time periods is considered as a task. Multipletasks at different time periods performed simultaneously to detect anomalies.In this paper, we apply the multi-task feature selection in network anomalydetection area which provides a powerful method to gather information frommultiple traffic and detect anomalies on it simultaneously. In particular, themulti-task feature selection includes the well-known l1-norm based featureselection as a special case given only one task. Moreover, we show that themulti-task feature selection is more accurate by utilizing more informationsimultaneously than the l1-norm based method. At the evaluation stage, wepreprocess the raw data trace from trans-Pacific backbone link between Japanand the United States, label with anomaly communities, and generate a248-feature dataset. We show empirically that the multi-task feature selectionoutperforms independent l1-norm based feature selection on real trafficdataset.
arxiv-5700-273 | Image processing using miniKanren | http://arxiv.org/pdf/1403.3964v1.pdf | author:Hirotaka Niitsuma category:cs.CV cs.PL published:2014-03-16 summary:An integral image is one of the most efficient optimization technique forimage processing. However an integral image is only a special case of delayedstream or memoization. This research discusses generalizing concept of integralimage optimization technique, and how to generate an integral image optimizedprogram code automatically from abstracted image processing algorithm. In oderto abstruct algorithms, we forces to miniKanren.
arxiv-5700-274 | A General Framework for Interacting Bayes-Optimally with Self-Interested Agents using Arbitrary Parametric Model and Model Prior | http://arxiv.org/pdf/1304.2024v3.pdf | author:Trong Nghia Hoang, Kian Hsiang Low category:cs.LG cs.AI cs.MA stat.ML published:2013-04-07 summary:Recent advances in Bayesian reinforcement learning (BRL) have shown thatBayes-optimality is theoretically achievable by modeling the environment'slatent dynamics using Flat-Dirichlet-Multinomial (FDM) prior. Inself-interested multi-agent environments, the transition dynamics are mainlycontrolled by the other agent's stochastic behavior for which FDM'sindependence and modeling assumptions do not hold. As a result, FDM does notallow the other agent's behavior to be generalized across different states norspecified using prior domain knowledge. To overcome these practical limitationsof FDM, we propose a generalization of BRL to integrate the general class ofparametric models and model priors, thus allowing practitioners' domainknowledge to be exploited to produce a fine-grained and compact representationof the other agent's behavior. Empirical evaluation shows that our approachoutperforms existing multi-agent reinforcement learning algorithms.
arxiv-5700-275 | Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression | http://arxiv.org/pdf/1303.6149v3.pdf | author:Francis Bach category:math.ST cs.LG math.OC stat.TH published:2013-03-25 summary:In this paper, we consider supervised learning problems such as logisticregression and study the stochastic gradient method with averaging, in theusual stochastic approximation setting where observations are used only once.We show that after $N$ iterations, with a constant step-size proportional to$1/R^2 \sqrt{N}$ where $N$ is the number of observations and $R$ is the maximumnorm of the observations, the convergence rate is always of order$O(1/\sqrt{N})$, and improves to $O(R^2 / \mu N)$ where $\mu$ is the lowesteigenvalue of the Hessian at the global optimum (when this eigenvalue isgreater than $R^2/\sqrt{N}$). Since $\mu$ does not need to be known in advance,this shows that averaged stochastic gradient is adaptive to \emph{unknownlocal} strong convexity of the objective function. Our proof relies on thegeneralized self-concordance properties of the logistic loss and thus extendsto all generalized linear models with uniformly bounded features.
arxiv-5700-276 | Knapsack Constrained Contextual Submodular List Prediction with Application to Multi-document Summarization | http://arxiv.org/pdf/1308.3541v2.pdf | author:Jiaji Zhou, Stephane Ross, Yisong Yue, Debadeepta Dey, J. Andrew Bagnell category:cs.LG published:2013-08-16 summary:We study the problem of predicting a set or list of options under knapsackconstraint. The quality of such lists are evaluated by a submodular rewardfunction that measures both quality and diversity. Similar to DAgger (Ross etal., 2010), by a reduction to online learning, we show how to adapt twosequence prediction models to imitate greedy maximization under knapsackconstraint problems: CONSEQOPT (Dey et al., 2012) and SCP (Ross et al., 2013).Experiments on extractive multi-document summarization show that our approachoutperforms existing state-of-the-art methods.
arxiv-5700-277 | Geometric VLAD for Large Scale Image Search | http://arxiv.org/pdf/1403.3829v1.pdf | author:Zixuan Wang, Wei Di, Anurag Bhardwaj, Vignesh Jagadeesh, Robinson Piramuthu category:cs.CV published:2014-03-15 summary:We present a novel compact image descriptor for large scale image search. Ourproposed descriptor - Geometric VLAD (gVLAD) is an extension of VLAD (Vector ofLocally Aggregated Descriptors) that incorporates weak geometry informationinto the VLAD framework. The proposed geometry cues are derived as a membershipfunction over keypoint angles which contain evident and informative informationbut yet often discarded. A principled technique for learning the membershipfunction by clustering angles is also presented. Further, to address theoverhead of iterative codebook training over real-time datasets, a novelcodebook adaptation strategy is outlined. Finally, we demonstrate the efficacyof proposed gVLAD based retrieval framework where we achieve more than 15%improvement in mAP over existing benchmarks.
arxiv-5700-278 | Automatic Classification of Human Epithelial Type 2 Cell Indirect Immunofluorescence Images using Cell Pyramid Matching | http://arxiv.org/pdf/1403.3780v1.pdf | author:Arnold Wiliem, Conrad Sanderson, Yongkang Wong, Peter Hobson, Rodney F. Minchin, Brian C. Lovell category:q-bio.CB cs.CV q-bio.QM published:2014-03-15 summary:This paper describes a novel system for automatic classification of imagesobtained from Anti-Nuclear Antibody (ANA) pathology tests on Human Epithelialtype 2 (HEp-2) cells using the Indirect Immunofluorescence (IIF) protocol. TheIIF protocol on HEp-2 cells has been the hallmark method to identify thepresence of ANAs, due to its high sensitivity and the large range of antigensthat can be detected. However, it suffers from numerous shortcomings, such asbeing subjective as well as time and labour intensive. Computer AidedDiagnostic (CAD) systems have been developed to address these problems, whichautomatically classify a HEp-2 cell image into one of its known patterns (eg.speckled, homogeneous). Most of the existing CAD systems use handpickedfeatures to represent a HEp-2 cell image, which may only work in limitedscenarios. We propose a novel automatic cell image classification method termedCell Pyramid Matching (CPM), which is comprised of regional histograms ofvisual words coupled with the Multiple Kernel Learning framework. We present astudy of several variations of generating histograms and show the efficacy ofthe system on two publicly available datasets: the ICPR HEp-2 cellclassification contest dataset and the SNPHEp-2 dataset.
arxiv-5700-279 | A Geometric Algorithm for Scalable Multiple Kernel Learning | http://arxiv.org/pdf/1206.5580v2.pdf | author:John Moeller, Parasaran Raman, Avishek Saha, Suresh Venkatasubramanian category:cs.LG stat.ML published:2012-06-25 summary:We present a geometric formulation of the Multiple Kernel Learning (MKL)problem. To do so, we reinterpret the problem of learning kernel weights assearching for a kernel that maximizes the minimum (kernel) distance between twoconvex polytopes. This interpretation combined with novel structural insightsfrom our geometric formulation allows us to reduce the MKL problem to a simpleoptimization routine that yields provable convergence as well as qualityguarantees. As a result our method scales efficiently to much larger data setsthan most prior methods can handle. Empirical evaluation on eleven datasetsshows that we are significantly faster and even compare favorably with auniform unweighted combination of kernels.
arxiv-5700-280 | Learning the Latent State Space of Time-Varying Graphs | http://arxiv.org/pdf/1403.3707v1.pdf | author:Nesreen K. Ahmed, Christopher Cole, Jennifer Neville category:cs.SI cs.LG physics.soc-ph stat.ML published:2014-03-14 summary:From social networks to Internet applications, a wide variety of electroniccommunication tools are producing streams of graph data; where the nodesrepresent users and the edges represent the contacts between them over time.This has led to an increased interest in mechanisms to model the dynamicstructure of time-varying graphs. In this work, we develop a framework forlearning the latent state space of a time-varying email graph. We show how theframework can be used to find subsequences that correspond to global real-timeevents in the Email graph (e.g. vacations, breaks, ...etc.). These eventsimpact the underlying graph process to make its characteristics non-stationary.Within the framework, we compare two different representations of the temporalrelationships; discrete vs. probabilistic. We use the two representations asinputs to a mixture model to learn the latent state transitions that correspondto important changes in the Email graph structure over time.
arxiv-5700-281 | Removal and Contraction Operations in $n$D Generalized Maps for Efficient Homology Computation | http://arxiv.org/pdf/1403.3683v1.pdf | author:Guillaume Damiand, Rocio Gonzalez-Diaz, Samuel Peltier category:cs.CV published:2014-03-14 summary:In this paper, we show that contraction operations preserve the homology of$n$D generalized maps, under some conditions. Removal and contractionoperations are used to propose an efficient algorithm that compute homologygenerators of $n$D generalized maps. Its principle consists in simplifying ageneralized map as much as possible by using removal and contractionoperations. We obtain a generalized map having the same homology than theinitial one, while the number of cells decreased significantly. Keywords: $n$D Generalized Maps; Cellular Homology; Homology Generators;Contraction and Removal Operations.
arxiv-5700-282 | Test Set Selection using Active Information Acquisition for Predictive Models | http://arxiv.org/pdf/1312.0790v2.pdf | author:Sneha Chaudhari, Pankaj Dayama, Vinayaka Pandit, Indrajit Bhattacharya category:cs.AI cs.LG stat.ML published:2013-12-03 summary:In this paper, we consider active information acquisition when the predictionmodel is meant to be applied on a targeted subset of the population. The goalis to label a pre-specified fraction of customers in the target or test set byiteratively querying for information from the non-target or training set. Thenumber of queries is limited by an overall budget. Arising in the context oftwo rather disparate applications- banking and medical diagnosis, we pose theactive information acquisition problem as a constrained optimization problem.We propose two greedy iterative algorithms for solving the above problem. Weconduct experiments with synthetic data and compare results of our proposedalgorithms with few other baseline approaches. The experimental results showthat our proposed approaches perform better than the baseline schemes.
arxiv-5700-283 | Mixed-norm Regularization for Brain Decoding | http://arxiv.org/pdf/1403.3628v1.pdf | author:Rémi Flamary, Nisrine Jrad, Ronald Phlypo, Marco Congedo, Alain Rakotomamonjy category:cs.LG published:2014-03-14 summary:This work investigates the use of mixed-norm regularization for sensorselection in Event-Related Potential (ERP) based Brain-Computer Interfaces(BCI). The classification problem is cast as a discriminative optimizationframework where sensor selection is induced through the use of mixed-norms.This framework is extended to the multi-task learning situation where severalsimilar classification tasks related to different subjects are learnedsimultaneously. In this case, multi-task learning helps in leveraging datascarcity issue yielding to more robust classifiers. For this purpose, we haveintroduced a regularizer that induces both sensor selection and classifiersimilarities. The different regularization approaches are compared on three ERPdatasets showing the interest of mixed-norm regularization in terms of sensorselection. The multi-task approaches are evaluated when a small number oflearning examples are available yielding to significant performanceimprovements especially for subjects performing poorly.
arxiv-5700-284 | Patch-based Probabilistic Image Quality Assessment for Face Selection and Improved Video-based Face Recognition | http://arxiv.org/pdf/1304.0869v2.pdf | author:Yongkang Wong, Shaokang Chen, Sandra Mau, Conrad Sanderson, Brian C. Lovell category:cs.CV stat.AP published:2013-04-03 summary:In video based face recognition, face images are typically captured overmultiple frames in uncontrolled conditions, where head pose, illumination,shadowing, motion blur and focus change over the sequence. Additionally,inaccuracies in face localisation can also introduce scale and alignmentvariations. Using all face images, including images of poor quality, canactually degrade face recognition performance. While one solution it to useonly the "best" subset of images, current face selection techniques areincapable of simultaneously handling all of the abovementioned issues. Wepropose an efficient patch-based face image quality assessment algorithm whichquantifies the similarity of a face image to a probabilistic face model,representing an "ideal" face. Image characteristics that affect recognition aretaken into account, including variations in geometric alignment (shift,rotation and scale), sharpness, head pose and cast shadows. Experiments onFERET and PIE datasets show that the proposed algorithm is able to identifyimages which are simultaneously the most frontal, aligned, sharp and wellilluminated. Further experiments on a new video surveillance dataset (termedChokePoint) show that the proposed method provides better face subsets thanexisting face selection techniques, leading to significant improvements inrecognition accuracy.
arxiv-5700-285 | Spontaneous expression classification in the encrypted domain | http://arxiv.org/pdf/1403.3602v1.pdf | author:Segun Aina, Yogachandran Rahulamathavan, Raphael C. -W. Phan, Jonathon A. Chambers category:cs.CV cs.CR published:2014-03-14 summary:To date, most facial expression analysis have been based on posed imagedatabases and is carried out without being able to protect the identity of thesubjects whose expressions are being recognised. In this paper, we propose andimplement a system for classifying facial expressions of images in theencrypted domain based on a Paillier cryptosystem implementation of FisherLinear Discriminant Analysis and k-nearest neighbour (FLDA + kNN). We presentresults of experiments carried out on a recently developed natural visible andinfrared facial expression (NVIE) database of spontaneous images. To the bestof our knowledge, this is the first system that will allow the recog-nition ofencrypted spontaneous facial expressions by a remote server on behalf of aclient.
arxiv-5700-286 | Engaging with Massive Online Courses | http://arxiv.org/pdf/1403.3100v2.pdf | author:Ashton Anderson, Daniel Huttenlocher, Jon Kleinberg, Jure Leskovec category:cs.SI physics.soc-ph stat.ML H.2.8 published:2014-03-12 summary:The Web has enabled one of the most visible recent developments ineducation---the deployment of massive open online courses. With their globalreach and often staggering enrollments, MOOCs have the potential to become amajor new mechanism for learning. Despite this early promise, however, MOOCsare still relatively unexplored and poorly understood. In a MOOC, each student's complete interaction with the course materialstakes place on the Web, thus providing a record of learner activity ofunprecedented scale and resolution. In this work, we use such trace data todevelop a conceptual framework for understanding how users currently engagewith MOOCs. We develop a taxonomy of individual behavior, examine the differentbehavioral patterns of high- and low-achieving students, and investigate howforum participation relates to other parts of the course. We also report on a large-scale deployment of badges as incentives forengagement in a MOOC, including randomized experiments in which thepresentation of badges was varied across sub-populations. We find that makingbadges more salient produced increases in forum engagement.
arxiv-5700-287 | Scalable and Robust Construction of Topical Hierarchies | http://arxiv.org/pdf/1403.3460v1.pdf | author:Chi Wang, Xueqing Liu, Yanglei Song, Jiawei Han category:cs.LG cs.CL cs.DB cs.IR published:2014-03-13 summary:Automated generation of high-quality topical hierarchies for a textcollection is a dream problem in knowledge engineering with many valuableapplications. In this paper a scalable and robust algorithm is proposed forconstructing a hierarchy of topics from a text collection. We divide andconquer the problem using a top-down recursive framework, based on a tensororthogonal decomposition technique. We solve a critical challenge to performscalable inference for our newly designed hierarchical topic model. Experimentswith various real-world datasets illustrate its ability to generate robust,high-quality hierarchies efficiently. Our method reduces the time ofconstruction by several orders of magnitude, and its robust feature renders itpossible for users to interactively revise the hierarchy.
arxiv-5700-288 | Neighborhood Selection for Thresholding-based Subspace Clustering | http://arxiv.org/pdf/1403.3438v1.pdf | author:Reinhard Heckel, Eirikur Agustsson, Helmut Bölcskei category:stat.ML cs.IT math.IT published:2014-03-13 summary:Subspace clustering refers to the problem of clustering high-dimensional datapoints into a union of low-dimensional linear subspaces, where the number ofsubspaces, their dimensions and orientations are all unknown. In this paper, wepropose a variation of the recently introduced thresholding-based subspaceclustering (TSC) algorithm, which applies spectral clustering to an adjacencymatrix constructed from the nearest neighbors of each data point with respectto the spherical distance measure. The new element resides in an individual anddata-driven choice of the number of nearest neighbors. Previous performanceresults for TSC, as well as for other subspace clustering algorithms based onspectral clustering, come in terms of an intermediate performance measure,which does not address the clustering error directly. Our main analyticalcontribution is a performance analysis of the modified TSC algorithm (as wellas the original TSC algorithm) in terms of the clustering error directly.
arxiv-5700-289 | Controlling Recurrent Neural Networks by Conceptors | http://arxiv.org/pdf/1403.3369v1.pdf | author:Herbert Jaeger category:cs.NE I.2.6 published:2014-03-13 summary:The human brain is a dynamical system whose extremely complex sensor-drivenneural processes give rise to conceptual, logical cognition. Understanding theinterplay between nonlinear neural dynamics and concept-level cognition remainsa major scientific challenge. Here I propose a mechanism of neurodynamicalorganization, called conceptors, which unites nonlinear dynamics with basicprinciples of conceptual abstraction and logic. It becomes possible to learn,store, abstract, focus, morph, generalize, de-noise and recognize a largenumber of dynamical patterns within a single neural system; novel patterns canbe added without interfering with previously acquired ones; neural noise isautomatically filtered. Conceptors help explaining how conceptual-levelinformation processing emerges naturally and robustly in neural systems, andremove a number of roadblocks in the theory and applications of recurrentneural networks.
arxiv-5700-290 | Semantic Unification A sheaf theoretic approach to natural language | http://arxiv.org/pdf/1403.3351v1.pdf | author:Samson Abramsky, Mehrnoosh Sadrzadeh category:cs.CL published:2014-03-13 summary:Language is contextual and sheaf theory provides a high level mathematicalframework to model contextuality. We show how sheaf theory can model thecontextual nature of natural language and how gluing can be used to provide aglobal semantics for a discourse by putting together the local logicalsemantics of each sentence within the discourse. We introduce a presheafstructure corresponding to a basic form of Discourse Representation Structures.Within this setting, we formulate a notion of semantic unification --- gluingmeanings of parts of a discourse into a coherent whole --- as a form ofsheaf-theoretic gluing. We illustrate this idea with a number of examples whereit can used to represent resolutions of anaphoric references. We also discussmultivalued gluing, described using a distributions functor, which can be usedto represent situations where multiple gluings are possible, and where we mayneed to rank them using quantitative measures. Dedicated to Jim Lambek on the occasion of his 90th birthday.
arxiv-5700-291 | The Potential Benefits of Filtering Versus Hyper-Parameter Optimization | http://arxiv.org/pdf/1403.3342v1.pdf | author:Michael R. Smith, Tony Martinez, Christophe Giraud-Carrier category:stat.ML cs.LG published:2014-03-13 summary:The quality of an induced model by a learning algorithm is dependent on thequality of the training data and the hyper-parameters supplied to the learningalgorithm. Prior work has shown that improving the quality of the training data(i.e., by removing low quality instances) or tuning the learning algorithmhyper-parameters can significantly improve the quality of an induced model. Acomparison of the two methods is lacking though. In this paper, we estimate andcompare the potential benefits of filtering and hyper-parameter optimization.Estimating the potential benefit gives an overly optimistic estimate but alsoempirically demonstrates an approximation of the maximum potential benefit ofeach method. We find that, while both significantly improve the induced model,improving the quality of the training set has a greater potential effect thanhyper-parameter optimization.
arxiv-5700-292 | A Framework for the Analysis of Computational Imaging Systems with Practical Applications | http://arxiv.org/pdf/1308.1981v3.pdf | author:Kaushik Mitra, Oliver Cossairt, Ashok Veeraraghavan category:cs.CV I.4 published:2013-08-08 summary:Over the last decade, a number of Computational Imaging (CI) systems havebeen proposed for tasks such as motion deblurring, defocus deblurring andmultispectral imaging. These techniques increase the amount of light reachingthe sensor via multiplexing and then undo the deleterious effects ofmultiplexing by appropriate reconstruction algorithms. Given the widespreadappeal and the considerable enthusiasm generated by these techniques, adetailed performance analysis of the benefits conferred by this approach isimportant. Unfortunately, a detailed analysis of CI has proven to be a challengingproblem because performance depends equally on three components: (1) theoptical multiplexing, (2) the noise characteristics of the sensor, and (3) thereconstruction algorithm. A few recent papers have performed analysis takingmultiplexing and noise characteristics into account. However, analysis of CIsystems under state-of-the-art reconstruction algorithms, most of which exploitsignal prior models, has proven to be unwieldy. In this paper, we present acomprehensive analysis framework incorporating all three components. In order to perform this analysis, we model the signal priors using aGaussian Mixture Model (GMM). A GMM prior confers two unique characteristics.Firstly, GMM satisfies the universal approximation property which says that anyprior density function can be approximated to any fidelity using a GMM withappropriate number of mixtures. Secondly, a GMM prior lends itself toanalytical tractability allowing us to derive simple expressions for the`minimum mean square error' (MMSE), which we use as a metric to characterizethe performance of CI systems. We use our framework to analyze severalpreviously proposed CI techniques, giving conclusive answer to the question:`How much performance gain is due to use of a signal prior and how much is dueto multiplexing?
arxiv-5700-293 | Noise Facilitation in Associative Memories of Exponential Capacity | http://arxiv.org/pdf/1403.3305v1.pdf | author:Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi, Lav R. Varshney category:cs.NE published:2014-03-13 summary:Recent advances in associative memory design through structured pattern setsand graph-based inference algorithms have allowed reliable learning and recallof an exponential number of patterns. Although these designs correct externalerrors in recall, they assume neurons that compute noiselessly, in contrast tothe highly variable neurons in brain regions thought to operate associativelysuch as hippocampus and olfactory cortex. Here we consider associative memories with noisy internal computations andanalytically characterize performance. As long as the internal noise level isbelow a specified threshold, the error probability in the recall phase can bemade exceedingly small. More surprisingly, we show that internal noise actuallyimproves the performance of the recall phase while the pattern retrievalcapacity remains intact, i.e., the number of stored patterns does not reducewith noise (up to a threshold). Computational experiments lend additionalsupport to our theoretical analysis. This work suggests a functional benefit tonoisy neurons in biological neuronal networks.
arxiv-5700-294 | Ambiguity in language networks | http://arxiv.org/pdf/1402.4802v2.pdf | author:Ricard V. Solé, Luís F. Seoane category:physics.soc-ph cs.CL q-bio.NC published:2014-02-18 summary:Human language defines the most complex outcomes of evolution. The emergenceof such an elaborated form of communication allowed humans to create extremelystructured societies and manage symbols at different levels including, amongothers, semantics. All linguistic levels have to deal with an astronomiccombinatorial potential that stems from the recursive nature of languages. Thisrecursiveness is indeed a key defining trait. However, not all words areequally combined nor frequent. In breaking the symmetry between less and moreoften used and between less and more meaning-bearing units, universal scalinglaws arise. Such laws, common to all human languages, appear on differentstages from word inventories to networks of interacting words. Among theseseemingly universal traits exhibited by language networks, ambiguity appears tobe a specially relevant component. Ambiguity is avoided in most computationalapproaches to language processing, and yet it seems to be a crucial element oflanguage architecture. Here we review the evidence both from language networkarchitecture and from theoretical reasonings based on a least effort argument.Ambiguity is shown to play an essential role in providing a source of languageefficiency, and is likely to be an inevitable byproduct of network growth.
arxiv-5700-295 | Sensing-Aware Kernel SVM | http://arxiv.org/pdf/1312.0512v2.pdf | author:Weicong Ding, Prakash Ishwar, Venkatesh Saligrama, W. Clem Karl category:cs.LG published:2013-12-02 summary:We propose a novel approach for designing kernels for support vector machines(SVMs) when the class label is linked to the observation through a latent stateand the likelihood function of the observation given the state (the sensingmodel) is available. We show that the Bayes-optimum decision boundary is ahyperplane under a mapping defined by the likelihood function. Combining thiswith the maximum margin principle yields kernels for SVMs that leverageknowledge of the sensing model in an optimal way. We derive the optimum kernelfor the bag-of-words (BoWs) sensing model and demonstrate its superiorperformance over other kernels in document and image classification tasks.These results indicate that such optimum sensing-aware kernel SVMs can matchthe performance of rather sophisticated state-of-the-art approaches.
arxiv-5700-296 | Sentiment Analysis by Using Fuzzy Logic | http://arxiv.org/pdf/1403.3185v1.pdf | author:Md. Ansarul Haque category:cs.IR cs.CL published:2014-03-13 summary:How could a product or service is reasonably evaluated by anyone in theshortest time? A million dollar question but it is having a simple answer:Sentiment analysis. Sentiment analysis is consumers review on products andservices which helps both the producers and consumers (stakeholders) to takeeffective and efficient decision within a shortest period of time. Producerscan have better knowledge of their products and services through the sentimentanalysis (ex. positive and negative comments or consumers likes and dislikes)which will help them to know their products status (ex. product limitations ormarket status). Consumers can have better knowledge of their interestedproducts and services through the sentiment analysis (ex. positive and negativecomments or consumers likes and dislikes) which will help them to know theirdeserving products status (ex. product limitations or market status). For morespecification of the sentiment values, fuzzy logic could be introduced.Therefore, sentiment analysis with the help of fuzzy logic (deals withreasoning and gives closer views to the exact sentiment values) will help theproducers or consumers or any interested person for taking the effectivedecision according to their product or service interest.
arxiv-5700-297 | Self-similar prior and wavelet bases for hidden incompressible turbulent motion | http://arxiv.org/pdf/1302.5554v2.pdf | author:Patrick Héas, Frédéric Lavancier, Souleymane Kadri-Harouna category:stat.AP cs.CV cs.NA published:2013-02-22 summary:This work is concerned with the ill-posed inverse problem of estimatingturbulent flows from the observation of an image sequence. From a Bayesianperspective, a divergence-free isotropic fractional Brownian motion (fBm) ischosen as a prior model for instantaneous turbulent velocity fields. Thisself-similar prior characterizes accurately second-order statistics of velocityfields in incompressible isotropic turbulence. Nevertheless, the associatedmaximum a posteriori involves a fractional Laplacian operator which is delicateto implement in practice. To deal with this issue, we propose to decompose thedivergent-free fBm on well-chosen wavelet bases. As a first alternative, wepropose to design wavelets as whitening filters. We show that these filters arefractional Laplacian wavelets composed with the Leray projector. As a secondalternative, we use a divergence-free wavelet basis, which takes implicitlyinto account the incompressibility constraint arising from physics. Althoughthe latter decomposition involves correlated wavelet coefficients, we are ableto handle this dependence in practice. Based on these two waveletdecompositions, we finally provide effective and efficient algorithms toapproach the maximum a posteriori. An intensive numerical evaluation proves therelevance of the proposed wavelet-based self-similar priors.
arxiv-5700-298 | The Gaussian Radon Transform and Machine Learning | http://arxiv.org/pdf/1310.4794v2.pdf | author:Irina Holmes, Ambar Sengupta category:stat.ML math.FA published:2013-10-17 summary:There has been growing recent interest in probabilistic interpretations ofkernel-based methods as well as learning in Banach spaces. The absence of auseful Lebesgue measure on an infinite-dimensional reproducing kernel Hilbertspace is a serious obstacle for such stochastic models. We propose anestimation model for the ridge regression problem within the framework ofabstract Wiener spaces and show how the support vector machine solution to suchproblems can be interpreted in terms of the Gaussian Radon transform.
arxiv-5700-299 | On Combining Machine Learning with Decision Making | http://arxiv.org/pdf/1104.5061v2.pdf | author:Theja Tulabandhula, Cynthia Rudin category:math.OC cs.LG stat.ML published:2011-04-27 summary:We present a new application and covering number bound for the framework of"Machine Learning with Operational Costs (MLOC)," which is an exploratory formof decision theory. The MLOC framework incorporates knowledge about how apredictive model will be used for a subsequent task, thus combining machinelearning with the decision that is made afterwards. In this work, we use theMLOC framework to study a problem that has implications for power gridreliability and maintenance, called the Machine Learning and TravelingRepairman Problem ML&TRP. The goal of the ML&TRP is to determine a route for a"repair crew," which repairs nodes on a graph. The repair crew aims to minimizethe cost of failures at the nodes, but as in many real situations, the failureprobabilities are not known and must be estimated. The MLOC framework allows usto understand how this uncertainty influences the repair route. We also presentnew covering number generalization bounds for the MLOC framework.
arxiv-5700-300 | Parallel WiSARD object tracker: a ram-based tracking system | http://arxiv.org/pdf/1403.3118v1.pdf | author:Rodrigo da Silva Moreira, Nelson Francisco Favilla Ebecken category:cs.CV published:2014-03-12 summary:This paper proposes the Parallel WiSARD Object Tracker (PWOT), a new objecttracker based on the WiSARD weightless neural network that is robust againstquantization errors. Object tracking in video is an important and challengingtask in many applications. Difficulties can arise due to weather conditions,target trajectory and appearance, occlusions, lighting conditions and noise.Tracking is a high-level application and requires the object location frame byframe in real time. This paper proposes a fast hybrid image segmentation(threshold and edge detection) in YcbCr color model and a parallel RAM baseddiscriminator that improves efficiency when quantization errors occur. Theoriginal WiSARD training algorithm was changed to allow the tracking.
