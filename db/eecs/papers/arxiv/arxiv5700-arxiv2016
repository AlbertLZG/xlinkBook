arxiv-5700-1 | Efficient Background Modeling Based on Sparse Representation and Outlier Iterative Removal | http://arxiv.org/pdf/1401.6013v2.pdf | author:Linhao Li, Ping Wang, Qinghua Hu, Sijia Cai category:cs.CV published:2014-01-23 summary:Background modeling is a critical component for various vision-basedapplications. Most traditional methods tend to be inefficient when solvinglarge-scale problems. In this paper, we introduce sparse representation intothe task of large scale stable background modeling, and reduce the video sizeby exploring its 'discriminative' frames. A cyclic iteration process is thenproposed to extract the background from the discriminative frame set. The twoparts combine to form our Sparse Outlier Iterative Removal (SOIR) algorithm.The algorithm operates in tensor space to obey the natural data structure ofvideos. Experimental results show that a few discriminative frames determinethe performance of the background extraction. Further, SOIR can achieve highaccuracy and high speed simultaneously when dealing with real video sequences.Thus, SOIR has an advantage in solving large-scale tasks.
arxiv-5700-2 | Spatially regularized reconstruction of fibre orientation distributions in the presence of isotropic diffusion | http://arxiv.org/pdf/1401.6196v1.pdf | author:Q. Zhou, O. Michailovich, Y. Rathi category:cs.CV published:2014-01-23 summary:The connectivity and structural integrity of the white matter of the brain isnowadays known to be implicated into a wide range of brain-related disorders.However, it was not before the advent of diffusion Magnetic Resonance Imaging(dMRI) that researches have been able to examine the properties of white matterin vivo. Presently, among a range of various methods of dMRI, high angularresolution diffusion imaging (HARDI) is known to excel in its ability toprovide reliable information about the local orientations of neural fasciculi(aka fibre tracts). Moreover, as opposed to the more traditional diffusiontensor imaging (DTI), HARDI is capable of distinguishing the orientations ofmultiple fibres passing through a given spatial voxel. Unfortunately, theability of HARDI to discriminate between neural fibres that cross each other atacute angles is always limited, which is the main reason behind the developmentof numerous post-processing tools, aiming at the improvement of the directionalresolution of HARDI. Among such tools is spherical deconvolution (SD). Due toits ill-posed nature, however, SD standardly relies on a number of a prioriassumptions which are to render its results unique and stable. In this paper,we propose a different approach to the problem of SD in HARDI, which accountsfor the spatial continuity of neural fibres as well as the presence ofisotropic diffusion. Subsequently, we demonstrate how the proposed solution canbe used to successfully overcome the effect of partial voluming, whilepreserving the spatial coherency of cerebral diffusion at moderate-to-severenoise levels. In a series of both in silico and in vivo experiments, theperformance of the proposed method is compared with that of several availablealternatives, with the comparative results clearly supporting the viability andusefulness of our approach.
arxiv-5700-3 | Nonlinear hyperspectral unmixing with robust nonnegative matrix factorization | http://arxiv.org/pdf/1401.5649v2.pdf | author:Cédric Févotte, Nicolas Dobigeon category:stat.ME stat.ML published:2014-01-22 summary:This paper introduces a robust mixing model to describe hyperspectral dataresulting from the mixture of several pure spectral signatures. This new modelnot only generalizes the commonly used linear mixing model, but also allows forpossible nonlinear effects to be easily handled, relying on mild assumptionsregarding these nonlinearities. The standard nonnegativity and sum-to-oneconstraints inherent to spectral unmixing are coupled with a group-sparseconstraint imposed on the nonlinearity component. This results in a new form ofrobust nonnegative matrix factorization. The data fidelity term is expressed asa beta-divergence, a continuous family of dissimilarity measures that takes thesquared Euclidean distance and the generalized Kullback-Leibler divergence asspecial cases. The penalized objective is minimized with a block-coordinatedescent that involves majorization-minimization updates. Simulation resultsobtained on synthetic and real data show that the proposed strategy competeswith state-of-the-art linear and nonlinear unmixing methods.
arxiv-5700-4 | The Gabor-Einstein Wavelet: A Model for the Receptive Fields of V1 to MT Neurons | http://arxiv.org/pdf/1401.5589v1.pdf | author:Stephen G. Odaibo category:q-bio.NC cs.CV physics.bio-ph published:2014-01-22 summary:Our visual system is astonishingly efficient at detecting moving objects.This process is mediated by the neurons which connect the primary visual cortex(V1) to the middle temporal (MT) area. Interestingly, since Kuffler'spioneering experiments on retinal ganglion cells, mathematical models have beenvital for advancing our understanding of the receptive fields of visualneurons. However, existing models were not designed to describe the mostsalient attributes of the highly specialized neurons in the V1 to MT motionprocessing stream; and they have not been able to do so. Here, we introduce theGabor-Einstein wavelet, a new family of functions for representing thereceptive fields of V1 to MT neurons. We show that the way space and time aremixed in the visual cortex is analogous to the way they are mixed in thespecial theory of relativity (STR). Hence we constrained the Gabor-Einsteinmodel by requiring: (i) relativistic-invariance of the wave carrier, and (ii)the minimum possible number of parameters. From these two constraints, the sincfunction emerged as a natural descriptor of the wave carrier. The particulardistribution of lowpass to bandpass temporal frequency filtering properties ofV1 to MT neurons (Foster et al 1985; DeAngelis et al 1993b; Hawken et al 1996)is clearly explained by the Gabor-Einstein basis. Furthermore, it does so in amanner innately representative of the motion-processing stream's neuronalhierarchy. Our analysis and computer simulations show that the distribution oftemporal frequency filtering properties along the motion processing stream is adirect effect of the way the brain jointly encodes space and time. We uncoveredthis fundamental link by demonstrating that analogous mathematical structuresunderlie STR and joint cortical spacetime encoding. This link will provide newphysiological insights into how the brain represents visual information.
arxiv-5700-5 | Parsimonious Topic Models with Salient Word Discovery | http://arxiv.org/pdf/1401.6169v2.pdf | author:Hossein Soleimani, David J. Miller category:cs.LG cs.CL cs.IR stat.ML published:2014-01-22 summary:We propose a parsimonious topic model for text corpora. In related modelssuch as Latent Dirichlet Allocation (LDA), all words are modeledtopic-specifically, even though many words occur with similar frequenciesacross different topics. Our modeling determines salient words for each topic,which have topic-specific probabilities, with the rest explained by a universalshared model. Further, in LDA all topics are in principle present in everydocument. By contrast our model gives sparse topic representation, determiningthe (small) subset of relevant topics for each document. We derive a BayesianInformation Criterion (BIC), balancing model complexity and goodness of fit.Here, interestingly, we identify an effective sample size and correspondingpenalty specific to each parameter type in our model. We minimize BIC tojointly determine our entire model -- the topic-specific words,document-specific topics, all model parameter values, {\it and} the totalnumber of topics -- in a wholly unsupervised fashion. Results on three textcorpora and an image dataset show that our model achieves higher test setlikelihood and better agreement with ground-truth class labels, compared to LDAand to a model designed to incorporate sparsity.
arxiv-5700-6 | License Plate Recognition (LPR): A Review with Experiments for Malaysia Case Study | http://arxiv.org/pdf/1401.5559v1.pdf | author:Nuzulha Khilwani Ibrahim, Emaliana Kasmuri, Norazira A Jalil, Mohd Adili Norasikin, Sazilah Salam, Mohamad Riduwan Md Nawawi category:cs.CV published:2014-01-22 summary:Most vehicle license plate recognition use neural network techniques toenhance its computing capability. The image of the vehicle license plate iscaptured and processed to produce a textual output for further processing. Thispaper reviews image processing and neural network techniques applied atdifferent stages which are preprocessing, filtering, feature extraction,segmentation and recognition in such way to remove the noise of the image, toenhance the image quality and to expedite the computing process by convertingthe characters in the image into respective text. An exemplar experiment hasbeen done in MATLAB to show the basic process of the image processingespecially for license plate in Malaysia case study. An algorithm is adaptedinto the solution for parking management system. The solution then isimplemented as proof of concept to the algorithm.
arxiv-5700-7 | Learning Mid-Level Features and Modeling Neuron Selectivity for Image Classification | http://arxiv.org/pdf/1401.5535v2.pdf | author:Shu Kong, Zhuolin Jiang, Qiang Yang category:cs.CV cs.LG cs.NE cs.RO published:2014-01-22 summary:We now know that mid-level features can greatly enhance the performance ofimage learning, but how to automatically learn the image features efficientlyand in an unsupervised manner is still an open question. In this paper, wepresent a very efficient mid-level feature learning approach (MidFea), whichonly involves simple operations such as $k$-means clustering, convolution,pooling, vector quantization and random projection. We explain why this simplemethod generates the desired features, and argue that there is no need to spendmuch time in learning low-level feature extractors. Furthermore, to boost theperformance, we propose to model the neuron selectivity (NS) principle bybuilding an additional layer over the mid-level features before feeding thefeatures into the classifier. We show that the NS-layer learnscategory-specific neurons with both bottom-up inference and top-down analysis,and thus supports fast inference for a query image. We run extensiveexperiments on several public databases to demonstrate that our approach canachieve state-of-the-art performances for face recognition, genderclassification, age estimation and object categorization. In particular, wedemonstrate that our approach is more than an order of magnitude faster thansome recently proposed sparse coding based methods.
arxiv-5700-8 | Identifiability of an Integer Modular Acyclic Additive Noise Model and its Causal Structure Discovery | http://arxiv.org/pdf/1401.5625v1.pdf | author:Joe Suzuki, Takanori Inazumi, Takashi Washio, Shohei Shimizu category:stat.ML published:2014-01-22 summary:The notion of causality is used in many situations dealing with uncertainty.We consider the problem whether causality can be identified given data setgenerated by discrete random variables rather than continuous ones. Inparticular, for non-binary data, thus far it was only known that causality canbe identified except rare cases. In this paper, we present necessary andsufficient condition for an integer modular acyclic additive noise (IMAN) oftwo variables. In addition, we relate bivariate and multivariate causalidentifiability in a more explicit manner, and develop a practical algorithm tofind the order of variables and their parent sets. We demonstrate itsperformance in applications to artificial data and real world body motion datawith comparisons to conventional methods.
arxiv-5700-9 | Collaborative Regression | http://arxiv.org/pdf/1401.5823v1.pdf | author:Samuel M. Gross, Robert Tibshirani category:q-bio.QM stat.ML published:2014-01-22 summary:We consider the scenario where one observes an outcome variable and sets offeatures from multiple assays, all measured on the same set of samples. Oneapproach that has been proposed for dealing with this type of data is ``sparsemultiple canonical correlation analysis'' (sparse mCCA). All of the currentsparse mCCA techniques are biconvex and thus have no guarantees about reachinga global optimum. We propose a method for performing sparse supervisedcanonical correlation analysis (sparse sCCA), a specific case of sparse mCCAwhen one of the datasets is a vector. Our proposal for sparse sCCA is convexand thus does not face the same difficulties as the other methods. We deriveefficient algorithms for this problem, and illustrate their use on simulatedand real data.
arxiv-5700-10 | Enhancing Template Security of Face Biometrics by Using Edge Detection and Hashing | http://arxiv.org/pdf/1401.5632v1.pdf | author:Manoj Krishnaswamy, G. Hemantha Kumar category:cs.CV published:2014-01-22 summary:In this paper we address the issues of using edge detection techniques onfacial images to produce cancellable biometric templates and a novel method fortemplate verification against tampering. With increasing use of biometrics,there is a real threat for the conventional systems using face databases, whichstore images of users in raw and unaltered form. If compromised not only it isirrevocable, but can be misused for cross-matching across different databases.So it is desirable to generate and store revocable templates for the same userin different applications to prevent cross-matching and to enhance security,while maintaining privacy and ethics. By comparing different edge detectionmethods it has been observed that the edge detection based on the Roberts Crossoperator performs consistently well across multiple face datasets, in which theface images have been taken under a variety of conditions. We have proposed anovel scheme using hashing, for extra verification, in order to harden thesecurity of the stored biometric templates.
arxiv-5700-11 | A new keyphrases extraction method based on suffix tree data structure for arabic documents clustering | http://arxiv.org/pdf/1401.5644v1.pdf | author:Issam Sahmoudi, Hanane Froud, Abdelmonaime Lachkar category:cs.CL cs.IR H.2.3 published:2014-01-22 summary:Document Clustering is a branch of a larger area of scientific study known asdata mining .which is an unsupervised classification using to find a structurein a collection of unlabeled data. The useful information in the documents canbe accompanied by a large amount of noise words when using Full TextRepresentation, and therefore will affect negatively the result of theclustering process. So it is with great need to eliminate the noise words andkeeping just the useful information in order to enhance the quality of theclustering results. This problem occurs with different degree for any languagesuch as English, European, Hindi, Chinese, and Arabic Language. To overcomethis problem, in this paper, we propose a new and efficient Keyphrasesextraction method based on the Suffix Tree data structure (KpST), the extractedKeyphrases are then used in the clustering process instead of Full TextRepresentation. The proposed method for Keyphrases extraction is languageindependent and therefore it may be applied to any language. In thisinvestigation, we are interested to deal with the Arabic language which is oneof the most complex languages. To evaluate our method, we conduct anexperimental study on Arabic Documents using the most popular Clusteringapproach of Hierarchical algorithms: Agglomerative Hierarchical algorithm withseven linkage techniques and a variety of distance functions and similaritymeasures to perform Arabic Document Clustering task. The obtained results showthat our method for extracting Keyphrases increases the quality of theclustering results. We propose also to study the effect of using the stemmingfor the testing dataset to cluster it with the same documents clusteringtechniques and similarity/distance measures.
arxiv-5700-12 | Numerical weather prediction or stochastic modeling: an objective criterion of choice for the global radiation forecasting | http://arxiv.org/pdf/1401.6002v1.pdf | author:Cyril Voyant, Gilles Notton, Christophe Paoli, Marie Laure Nivet, Marc Muselli, Kahina Dahmani category:stat.AP cs.LG published:2014-01-22 summary:Numerous methods exist and were developed for global radiation forecasting.The two most popular types are the numerical weather predictions (NWP) and thepredictions using stochastic approaches. We propose to compute a parameternoted constructed in part from the mutual information which is a quantity thatmeasures the mutual dependence of two variables. Both of these are calculatedwith the objective to establish the more relevant method between NWP andstochastic models concerning the current problem.
arxiv-5700-13 | Causal Discovery in a Binary Exclusive-or Skew Acyclic Model: BExSAM | http://arxiv.org/pdf/1401.5636v1.pdf | author:Takanori Inazumi, Takashi Washio, Shohei Shimizu, Joe Suzuki, Akihiro Yamamoto, Yoshinobu Kawahara category:stat.ML cs.LG published:2014-01-22 summary:Discovering causal relations among observed variables in a given data set isa major objective in studies of statistics and artificial intelligence.Recently, some techniques to discover a unique causal model have been exploredbased on non-Gaussianity of the observed data distribution. However, most ofthese are limited to continuous data. In this paper, we present a novel causalmodel for binary data and propose an efficient new approach to deriving theunique causal model governing a given binary data set under skew distributionsof external binary noises. Experimental evaluation shows excellent performancefor both artificial and real world data sets.
arxiv-5700-14 | Multi-Directional Multi-Level Dual-Cross Patterns for Robust Face Recognition | http://arxiv.org/pdf/1401.5311v2.pdf | author:Changxing Ding, Jonghyun Choi, Dacheng Tao, Larry S. Davis category:cs.CV published:2014-01-21 summary:To perform unconstrained face recognition robust to variations inillumination, pose and expression, this paper presents a new scheme to extract"Multi-Directional Multi-Level Dual-Cross Patterns" (MDML-DCPs) from faceimages. Specifically, the MDMLDCPs scheme exploits the first derivative ofGaussian operator to reduce the impact of differences in illumination and thencomputes the DCP feature at both the holistic and component levels. DCP is anovel face image descriptor inspired by the unique textural structure of humanfaces. It is computationally efficient and only doubles the cost of computinglocal binary patterns, yet is extremely robust to pose and expressionvariations. MDML-DCPs comprehensively yet efficiently encodes the invariantcharacteristics of a face image from multiple levels into patterns that arehighly discriminative of inter-personal differences but robust tointra-personal variations. Experimental results on the FERET, CAS-PERL-R1, FRGC2.0, and LFW databases indicate that DCP outperforms the state-of-the-art localdescriptors (e.g. LBP, LTP, LPQ, POEM, tLBP, and LGXP) for both faceidentification and face verification tasks. More impressively, the bestperformance is achieved on the challenging LFW and FRGC 2.0 databases bydeploying MDML-DCPs in a simple recognition scheme.
arxiv-5700-15 | Alternating direction method of multipliers for penalized zero-variance discriminant analysis | http://arxiv.org/pdf/1401.5492v4.pdf | author:Brendan P. W. Ames, Mingyi Hong category:stat.ML math.OC published:2014-01-21 summary:We consider the task of classification in the high dimensional setting wherethe number of features of the given data is significantly greater than thenumber of observations. To accomplish this task, we propose a heuristic, calledsparse zero-variance discriminant analysis (SZVD), for simultaneouslyperforming linear discriminant analysis and feature selection on highdimensional data. This method combines classical zero-variance discriminantanalysis, where discriminant vectors are identified in the null space of thesample within-class covariance matrix, with penalization applied to inducesparse structures in the resulting vectors. To approximately solve theresulting nonconvex problem, we develop a simple algorithm based on thealternating direction method of multipliers. Further, we show that thisalgorithm is applicable to a larger class of penalized generalized eigenvalueproblems, including a particular relaxation of the sparse principal componentanalysis problem. Finally, we establish theoretical guarantees for convergenceof our algorithm to stationary points of the original nonconvex problem, andempirically demonstrate the effectiveness of our heuristic for classifyingsimulated data and data drawn from applications in time-series classification.
arxiv-5700-16 | Reaserchnig the Development of the Electrical Power System Using Systemically Evolutionary Algorithm | http://arxiv.org/pdf/1401.5789v1.pdf | author:Jerzy Tchorzewski, Emil Chyzy category:cs.NE cs.SY published:2014-01-21 summary:The paper contains the concept and the results of research concerning theevolutionary algorithm, identified based on the systems control theory, whichwas called the Systemically of Evolutionary Algorithm (SAE). Special attentionwas paid to two elements of evolutionary algorithms, which have not been fullysolved yet, i.e. to the methods used to create the initial population and themethod of creating the robustness (fitness) function. Other elements of the SEAalgorithm, i.a. cross-over, mutation, selection, etc. were also defined from asystemic point of view. Computational experiments were conducted using aselected subsystem of the Polish Electrical Power System and three programminglanguages: Java, C++ and Matlab. Selected comparative results for the SAEalgorithm in different implementations were also presented.
arxiv-5700-17 | Hilbert Space Methods for Reduced-Rank Gaussian Process Regression | http://arxiv.org/pdf/1401.5508v1.pdf | author:Arno Solin, Simo Särkkä category:stat.ML published:2014-01-21 summary:This paper proposes a novel scheme for reduced-rank Gaussian processregression. The method is based on an approximate series expansion of thecovariance function in terms of an eigenfunction expansion of the Laplaceoperator in a compact subset of $\mathbb{R}^d$. On this approximate eigenbasisthe eigenvalues of the covariance function can be expressed as simple functionsof the spectral density of the Gaussian process, which allows the GP inferenceto be solved under a computational cost scaling as $\mathcal{O}(nm^2)$(initial) and $\mathcal{O}(m^3)$ (hyperparameter learning) with $m$ basisfunctions and $n$ data points. The approach also allows for rigorous erroranalysis with Hilbert space theory, and we show that the approximation becomesexact when the size of the compact subset and the number of eigenfunctions goto infinity. The expansion generalizes to Hilbert spaces with an inner productwhich is defined as an integral over a specified input density. The method iscompared to previously proposed methods theoretically and through empiricaltests with simulated and real data.
arxiv-5700-18 | A Unifying Framework for Typical Multi-Task Multiple Kernel Learning Problems | http://arxiv.org/pdf/1401.5136v1.pdf | author:Cong Li, Michael Georgiopoulos, Georgios C. Anagnostopoulos category:cs.LG published:2014-01-21 summary:Over the past few years, Multi-Kernel Learning (MKL) has received significantattention among data-driven feature selection techniques in the context ofkernel-based learning. MKL formulations have been devised and solved for abroad spectrum of machine learning problems, including Multi-Task Learning(MTL). Solving different MKL formulations usually involves designing algorithmsthat are tailored to the problem at hand, which is, typically, a non-trivialaccomplishment. In this paper we present a general Multi-Task Multi-Kernel Learning(Multi-Task MKL) framework that subsumes well-known Multi-Task MKLformulations, as well as several important MKL approaches on single-taskproblems. We then derive a simple algorithm that can solve the unifyingframework. To demonstrate the flexibility of the proposed framework, weformulate a new learning problem, namely Partially-Shared Common Space (PSCS)Multi-Task MKL, and demonstrate its merits through experimentation.
arxiv-5700-19 | Multi-GPU parallel memetic algorithm for capacitated vehicle routing problem | http://arxiv.org/pdf/1401.5216v1.pdf | author:Michał Karpiński, Maciej Pacut category:cs.DC cs.NE published:2014-01-21 summary:The goal of this paper is to propose and test a new memetic algorithm for thecapacitated vehicle routing problem in parallel computing environment. In thispaper we consider simple variation of vehicle routing problem in which the onlyparameter is the capacity of the vehicle and each client only needs onepackage. We present simple reduction to prove the existence of polynomial-timealgorithm for capacity 2. We analyze the efficiency of the algorithm usinghierarchical Parallel Random Access Machine (PRAM) model and run experimentswith code written in CUDA (for capacities larger than 2).
arxiv-5700-20 | Optimal Intelligent Control for Wind Turbulence Rejection in WECS Using ANNs and Genetic Fuzzy Approach | http://arxiv.org/pdf/1401.5221v1.pdf | author:Hadi kasiri, hamid reza momeni, Atiyeh Kasiri category:cs.SY cs.NE published:2014-01-21 summary:One of the disadvantages in Connection of wind energy conversion systems(WECSs) to transmission networks is plentiful turbulence of wind speed.Therefore effects of this problem must be controlled. Nowadays,pitch-controlled WECSs are increasingly used for variable speed and pitch windturbines. Megawatt class wind turbines generally turn at variable speed in windfarm. Thus turbine operation must be controlled in order to maximize theconversion efficiency below rated power and reduce loading on the drive-train.Due to random and non-linear nature of the wind turbulence and the ability ofMulti-Layer Perceptron (MLP) and Radial Basis Function (RBF) Artificial NeuralNetworks (ANNs) in the modeling and control of this turbulence, in this study,widespread changes of wind have been perused using MLP and RBF artificial NNs.In addition in this study, a new genetic fuzzy system has been successfullyapplied to identify disturbance wind in turbine input. Thus output power hasbeen regulated in optimal and nominal range by pitch angle regulation.Consequently, our proposed approaches have regulated output aerodynamic powerand torque in the nominal rang.
arxiv-5700-21 | Edge detection of binary images using the method of masks | http://arxiv.org/pdf/1401.5245v1.pdf | author:Ayman M Bahaa-Eldeen, Abdel-Moneim A. Wahdan, Hani M. K. Mahdi category:cs.CV published:2014-01-21 summary:In this work the method of masks, creating and using of inverted image masks,together with binary operation of image data are used in edge detection ofbinary images, monochrome images, which yields about 300 times faster thanordinary methods. The method is divided into three stages: Mask construction,Fundamental edge detection, and Edge Construction Comparison with an ordinarymethod and a fuzzy based method is carried out.
arxiv-5700-22 | Genetic Algorithms and its use with back-propagation network | http://arxiv.org/pdf/1401.5246v1.pdf | author:Ayman M. Bahaa-Eldin, A. M. A. Wahdan, H. M. K. Mahdi category:cs.NE published:2014-01-21 summary:Genetic algorithms are considered as one of the most efficient searchtechniques. Although they do not offer an optimal solution, their ability toreach a suitable solution in considerably short time gives them theirrespectable role in many AI techniques. This work introduces genetic algorithmsand describes their characteristics. Then a novel method using geneticalgorithm in best training set generation and selection for a back-propagationnetwork is proposed. This work also offers a new extension to the originalgenetic algorithms
arxiv-5700-23 | The Why and How of Nonnegative Matrix Factorization | http://arxiv.org/pdf/1401.5226v2.pdf | author:Nicolas Gillis category:stat.ML cs.IR cs.LG math.OC published:2014-01-21 summary:Nonnegative matrix factorization (NMF) has become a widely used tool for theanalysis of high-dimensional data as it automatically extracts sparse andmeaningful features from a set of nonnegative data vectors. We first illustratethis property of NMF on three applications, in image processing, text miningand hyperspectral imaging --this is the why. Then we address the problem ofsolving NMF, which is NP-hard in general. We review some standard NMFalgorithms, and also present a recent subclass of NMF problems, referred to asnear-separable NMF, that can be solved efficiently (that is, in polynomialtime), even in the presence of noise --this is the how. Finally, we brieflydescribe some problems in mathematics and computer science closely related toNMF via the nonnegative rank.
arxiv-5700-24 | Increasing Server Availability for Overall System Security: A Preventive Maintenance Approach Based on Failure Prediction | http://arxiv.org/pdf/1401.5686v1.pdf | author:Ayman M. Bahaa-Eldin, Hoda K. Mohamead, Sally S. Deraz category:cs.DC cs.NE published:2014-01-21 summary:Server Availability (SA) is an important measure of overall systems security.Important security systems rely on the availability of their hosting servers todeliver critical security services. Many of these servers offer managementinterface through web mainly using an Apache server. This paper investigatesthe increase of Server Availability by the use of Artificial Neural Networks(ANN) to predict software aging phenomenon. Several resource usage data iscollected and analyzed on a typical long-running software system (a webserver). A Multi-Layer Perceptron feed forward Artificial Neural Network wastrained on an Apache web server data-set to predict future server resourceexhaustion through uni-variate time series forecasting. The results werebenchmarked against those obtained from non-parametric statistical techniques,parametric time series models and empirical modeling techniques reported in theliterature.
arxiv-5700-25 | Compositional Operators in Distributional Semantics | http://arxiv.org/pdf/1401.5327v1.pdf | author:Dimitri Kartsaklis category:cs.CL cs.AI math.CT published:2014-01-21 summary:This survey presents in some detail the main advances that have been recentlytaking place in Computational Linguistics towards the unification of the twoprominent semantic paradigms: the compositional formal semantics view and thedistributional models of meaning based on vector spaces. After an introductionto these two approaches, I review the most important models that aim to providecompositionality in distributional semantics. Then I proceed and present inmore detail a particular framework by Coecke, Sadrzadeh and Clark (2010) basedon the abstract mathematical setting of category theory, as a more completeexample capable to demonstrate the diversity of techniques and scientificdisciplines that this kind of research can draw from. This paper concludes witha discussion about important open issues that need to be addressed by theresearchers in the future.
arxiv-5700-26 | Study of Neural Network Algorithm for Straight-Line Drawings of Planar Graphs | http://arxiv.org/pdf/1401.5330v1.pdf | author:Mohamed A. El-Sayed, S. Abdel-Khalek, Hanan H. Amin category:cs.CG cs.NE published:2014-01-21 summary:Graph drawing addresses the problem of finding a layout of a graph thatsatisfies given aesthetic and understandability objectives. The most importantobjective in graph drawing is minimization of the number of crossings in thedrawing, as the aesthetics and readability of graph drawings depend on thenumber of edge crossings. VLSI layouts with fewer crossings are more easilyrealizable and consequently cheaper. A straight-line drawing of a planar graphG of n vertices is a drawing of G such that each edge is drawn as astraight-line segment without edge crossings. However, a problem with currentgraph layout methods which are capable of producing satisfactory results for awide range of graphs is that they often put an extremely high demand oncomputational resources. This paper introduces a new layout method, whichnicely draws internally convex of planar graph that consumes only littlecomputational resources and does not need any heavy duty preprocessing. Here,we use two methods: The first is self organizing map known from unsupervisedneural networks which is known as (SOM) and the second method is Inverse SelfOrganized Map (ISOM).
arxiv-5700-27 | HMACA: Towards Proposing a Cellular Automata Based Tool for Protein Coding, Promoter Region Identification and Protein Structure Prediction | http://arxiv.org/pdf/1401.5364v1.pdf | author:Pokkuluri Kiran Sree, Inampudi Ramesh Babu, SSSN Usha Devi N category:cs.CE cs.LG published:2014-01-21 summary:Human body consists of lot of cells, each cell consist of DeOxaRibo NucleicAcid (DNA). Identifying the genes from the DNA sequences is a very difficulttask. But identifying the coding regions is more complex task compared to theformer. Identifying the protein which occupy little place in genes is a reallychallenging issue. For understating the genes coding region analysis plays animportant role. Proteins are molecules with macro structure that areresponsible for a wide range of vital biochemical functions, which includesacting as oxygen, cell signaling, antibody production, nutrient transport andbuilding up muscle fibers. Promoter region identification and protein structureprediction has gained a remarkable attention in recent years. Even though thereare some identification techniques addressing this problem, the approximateaccuracy in identifying the promoter region is closely 68% to 72%. We havedeveloped a Cellular Automata based tool build with hybrid multiple attractorcellular automata (HMACA) classifier for protein coding region, promoter regionidentification and protein structure prediction which predicts the protein andpromoter regions with an accuracy of 76%. This tool also predicts the structureof protein with an accuracy of 80%.
arxiv-5700-28 | On change point detection using the fused lasso method | http://arxiv.org/pdf/1401.5408v1.pdf | author:Cristian R. Rojas, Bo Wahlberg category:math.ST stat.ML stat.TH 62G08, 62G20 published:2014-01-21 summary:In this paper we analyze the asymptotic properties of l1 penalized maximumlikelihood estimation of signals with piece-wise constant mean values and/orvariances. The focus is on segmentation of a non-stationary time series withrespect to changes in these model parameters. This change point detection andestimation problem is also referred to as total variation denoising or l1 -meanfiltering and has many important applications in most fields of science andengineering. We establish the (approximate) sparse consistency properties,including rate of convergence, of the so-called fused lasso signal approximator(FLSA). We show that this only holds if the sign of the correspondingconsecutive changes are all different, and that this estimator is otherwiseincapable of correctly detecting the underlying sparsity pattern. The key ideais to notice that the optimality conditions for this problem can be analyzedusing techniques related to brownian bridge theory.
arxiv-5700-29 | An Evolutionary Approach towards Clustering Airborne Laser Scanning Data | http://arxiv.org/pdf/1401.4848v1.pdf | author:Ronald Hochreiter, Christoph Waldhauser category:cs.NE published:2014-01-20 summary:In land surveying, the generation of maps was greatly simplified with theintroduction of orthophotos and at a later stage with airborne LiDAR laserscanning systems. While the original purpose of LiDAR systems was to determinethe altitude of ground elevations, newer full wave systems provide additionalinformation that can be used on classifying the type of ground cover and thegeneration of maps. The LiDAR resulting point clouds are huge, multidimensionaldata sets that need to be grouped in classes of ground cover. We propose agenetic algorithm that aids in classifying these data sets and thus make themusable for map generation. A key feature are tailor-made genetic operators andfitness functions for the subject. The algorithm is compared to a traditionalk-means clustering.
arxiv-5700-30 | Marginal Pseudo-Likelihood Learning of Markov Network structures | http://arxiv.org/pdf/1401.4988v2.pdf | author:Johan Pensar, Henrik Nyman, Juha Niiranen, Jukka Corander category:stat.ML published:2014-01-20 summary:Undirected graphical models known as Markov networks are popular for a widevariety of applications ranging from statistical physics to computationalbiology. Traditionally, learning of the network structure has been done underthe assumption of chordality which ensures that efficient scoring methods canbe used. In general, non-chordal graphs have intractable normalizing constantswhich renders the calculation of Bayesian and other scores difficult beyondvery small-scale systems. Recently, there has been a surge of interest towardsthe use of regularized pseudo-likelihood methods for structural learning oflarge-scale Markov network models, as such an approach avoids the assumption ofchordality. The currently available methods typically necessitate the use of atuning parameter to adapt the level of regularization for a particular dataset,which can be optimized for example by cross-validation. Here we introduce aBayesian version of pseudo-likelihood scoring of Markov networks, which enablesan automatic regularization through marginalization over the nuisanceparameters in the model. We prove consistency of the resulting MPL estimatorfor the network structure via comparison with the pseudo information criterion.Identification of the MPL-optimal network on a prescanned graph space isconsidered with both greedy hill climbing and exact pseudo-Boolean optimizationalgorithms. We find that for reasonable sample sizes the hill climbing approachmost often identifies networks that are at a negligible distance from therestricted global optimum. Using synthetic and existing benchmark networks, themarginal pseudo-likelihood method is shown to generally perform favorablyagainst recent popular inference methods for Markov networks.
arxiv-5700-31 | Classification of IDS Alerts with Data Mining Techniques | http://arxiv.org/pdf/1401.4872v1.pdf | author:Hany Nashat Gabra, Ayman Mohammad Bahaa-Eldin, Huda Korashy category:cs.CR cs.DB cs.LG published:2014-01-20 summary:A data mining technique to reduce the amount of false alerts within an IDSsystem is proposed. The new technique achieves an accuracy of 99% compared to97% by the current systems.
arxiv-5700-32 | Anomaly detection in reconstructed quantum states using a machine-learning technique | http://arxiv.org/pdf/1401.4785v1.pdf | author:Satoshi Hara, Takafumi Ono, Ryo Okamoto, Takashi Washio, Shigeki Takeuchi category:quant-ph stat.AP stat.ML 81V80 published:2014-01-20 summary:The accurate detection of small deviations in given density matrices isimportant for quantum information processing. Here we propose a new methodbased on the concept of data mining. We demonstrate that the proposed methodcan more accurately detect small erroneous deviations in reconstructed densitymatrices, which contain intrinsic fluctuations due to the limited number ofsamples, than a naive method of checking the trace distance from the average ofthe given density matrices. This method has the potential to be a key tool inbroad areas of physics where the detection of small deviations of quantumstates reconstructed using a limited number of samples are essential.
arxiv-5700-33 | An Identification System Using Eye Detection Based On Wavelets And Neural Networks | http://arxiv.org/pdf/1401.5108v1.pdf | author:Mohamed A. El-Sayed, Mohamed A. Khafagy category:cs.CV published:2014-01-20 summary:The randomness and uniqueness of human eye patterns is a major breakthroughin the search for quicker, easier and highly reliable forms of automatic humanidentification. It is being used extensively in security solutions. Thisincludes access control to physical facilities, security systems andinformation databases, Suspect tracking, surveillance and intrusion detectionand by various Intelligence agencies through out the world. We use theadvantage of human eye uniqueness to identify people and approve its validityas a biometric. . Eye detection involves first extracting the eye from adigital face image, and then encoding the unique patterns of the eye in such away that they can be compared with pre-registered eye patterns. The eyedetection system consists of an automatic segmentation system that is based onthe wavelet transform, and then the Wavelet analysis is used as a pre-processorfor a back propagation neural network with conjugate gradient learning. Theinputs to the neural network are the wavelet maxima neighborhood coefficientsof face images at a particular scale. The output of the neural network is theclassification of the input into an eye or non-eye region. An accuracy of 90%is observed for identifying test images under different conditions included intraining stage.
arxiv-5700-34 | Does Syntactic Knowledge help English-Hindi SMT? | http://arxiv.org/pdf/1401.4869v1.pdf | author:Taraka Rama, Karthik Gali, Avinesh PVS category:cs.CL cs.AI published:2014-01-20 summary:In this paper we explore various parameter settings of the state-of-artStatistical Machine Translation system to improve the quality of thetranslation for a `distant' language pair like English-Hindi. We proposed newtechniques for efficient reordering. A slight improvement over the baseline isreported using these techniques. We also show that a simple pre-processing stepcan improve the quality of the translation significantly.
arxiv-5700-35 | Análisis e implementación de algoritmos evolutivos para la optimización de simulaciones en ingeniería civil. (draft) | http://arxiv.org/pdf/1401.5054v3.pdf | author:José Alberto García Gutiérrez, Alejandro Mateo Hernández Díaz category:cs.NE cs.AI published:2014-01-20 summary:This paper studies the applicability of evolutionary algorithms,particularly, the evolution strategies family in order to estimate adegradation parameter in the shear design of reinforced concrete members. Thisproblem represents a great computational task and is highly relevant in theframework of the structural engineering that for the first time is solved usinggenetic algorithms. You are viewing a draft, the authors appreciate corrections, comments andsuggestions to this work.
arxiv-5700-36 | A Review of Verbal and Non-Verbal Human-Robot Interactive Communication | http://arxiv.org/pdf/1401.4994v1.pdf | author:Nikolaos Mavridis category:cs.RO cs.CL published:2014-01-20 summary:In this paper, an overview of human-robot interactive communication ispresented, covering verbal as well as non-verbal aspects of human-robotinteraction. Following a historical introduction, and motivation towards fluidhuman-robot communication, ten desiderata are proposed, which provide anorganizational axis both of recent as well as of future research on human-robotcommunication. Then, the ten desiderata are examined in detail, culminating toa unifying discussion, and a forward-looking conclusion.
arxiv-5700-37 | Study of Efficient Technique Based On 2D Tsallis Entropy For Image Thresholding | http://arxiv.org/pdf/1401.5098v1.pdf | author:Mohamed A. El-Sayed, S. Abdel-Khalek, Eman Abdel-Aziz category:cs.CV 68U10 published:2014-01-20 summary:Thresholding is an important task in image processing. It is a main tool inpattern recognition, image segmentation, edge detection and scene analysis. Inthis paper, we present a new thresholding technique based on two-dimensionalTsallis entropy. The two-dimensional Tsallis entropy was obtained from thetwodimensional histogram which was determined by using the gray value of thepixels and the local average gray value of the pixels, the work it was applieda generalized entropy formalism that represents a recent development instatistical mechanics. The effectiveness of the proposed method is demonstratedby using examples from the real-world and synthetic images. The performanceevaluation of the proposed technique in terms of the quality of the thresholdedimages are presented. Experimental results demonstrate that the proposed methodachieve better result than the Shannon method.
arxiv-5700-38 | A Genetic Algorithm to Optimize a Tweet for Retweetability | http://arxiv.org/pdf/1401.4857v1.pdf | author:Ronald Hochreiter, Christoph Waldhauser category:cs.NE cs.CY cs.SI physics.soc-ph published:2014-01-20 summary:Twitter is a popular microblogging platform. When users send out messages,other users have the ability to forward these messages to their own subgraph.Most research focuses on increasing retweetability from a node's perspective.Here, we center on improving message style to increase the chance of a messagebeing forwarded. To this end, we simulate an artificial Twitter-like networkwith nodes deciding deterministically on retweeting a message or not. A geneticalgorithm is used to optimize message composition, so that the reach of amessage is increased. When analyzing the algorithm's runtime behavior across aset of different node types, we find that the algorithm consistently succeedsin significantly improving the retweetability of a message.
arxiv-5700-39 | Generalized Bhattacharyya and Chernoff upper bounds on Bayes error using quasi-arithmetic means | http://arxiv.org/pdf/1401.4788v1.pdf | author:Frank Nielsen category:cs.CV cs.IT math.IT published:2014-01-20 summary:Bayesian classification labels observations based on given prior information,namely class-a priori and class-conditional probabilities. Bayes' risk is theminimum expected classification cost that is achieved by the Bayes' test, theoptimal decision rule. When no cost incurs for correct classification and unitcost is charged for misclassification, Bayes' test reduces to the maximum aposteriori decision rule, and Bayes risk simplifies to Bayes' error, theprobability of error. Since calculating this probability of error is oftenintractable, several techniques have been devised to bound it with closed-formformula, introducing thereby measures of similarity and divergence betweendistributions like the Bhattacharyya coefficient and its associatedBhattacharyya distance. The Bhattacharyya upper bound can further be tightenedusing the Chernoff information that relies on the notion of best errorexponent. In this paper, we first express Bayes' risk using the total variationdistance on scaled distributions. We then elucidate and extend theBhattacharyya and the Chernoff upper bound mechanisms using generalizedweighted means. We provide as a byproduct novel notions of statisticaldivergences and affinity coefficients. We illustrate our technique by derivingnew upper bounds for the univariate Cauchy and the multivariate$t$-distributions, and show experimentally that those bounds are not toodistant to the computationally intractable Bayes' error.
arxiv-5700-40 | Evolutionary Optimization for Decision Making under Uncertainty | http://arxiv.org/pdf/1401.4696v1.pdf | author:Ronald Hochreiter category:cs.NE published:2014-01-19 summary:Optimizing decision problems under uncertainty can be done using a variety ofsolution methods. Soft computing and heuristic approaches tend to be powerfulfor solving such problems. In this overview article, we survey EvolutionaryOptimization techniques to solve Stochastic Programming problems - both for thesingle-stage and multi-stage case.
arxiv-5700-41 | Evolving Accuracy: A Genetic Algorithm to Improve Election Night Forecasts | http://arxiv.org/pdf/1401.4674v1.pdf | author:Ronald Hochreiter, Christoph Waldhauser category:cs.NE published:2014-01-19 summary:In this paper, we apply genetic algorithms to the field of electoral studies.Forecasting election results is one of the most exciting and demanding tasks inthe area of market research, especially due to the fact that decisions have tobe made within seconds on live television. We show that the proposed methodoutperforms currently applied approaches and thereby provide an argument totighten the intersection between computer science and social science,especially political science, further. We scrutinize the performance of ouralgorithm's runtime behavior to evaluate its applicability in the field.Numerical results with real data from a local election in the Austrian provinceof Styria from 2010 substantiate the applicability of the proposed approach.
arxiv-5700-42 | On the Resilience of an Ant-based System in Fuzzy Environments. An Empirical Study | http://arxiv.org/pdf/1401.4660v2.pdf | author:Gloria Cerasela Crisan, Camelia-M. Pintea, Petrica C. Pop category:cs.NE published:2014-01-19 summary:The current work describes an empirical study conducted in order toinvestigate the behavior of an optimization method in a fuzzy environment.MAX-MIN Ant System, an efficient implementation of a heuristic method is usedfor solving an optimization problem derived from the Traveling Salesman Problem(TSP). Several publicly-available symmetric TSP instances and their fuzzyvariants are tested in order to extract some general features. The entry datawas adapted by introducing a two-dimensional systematic degree of fuzziness,proportional with the number of nodes, the dimension of the instance and alsowith the distances between nodes, the scale of the instance. The results showthat our proposed method can handle the data uncertainty, showing goodresilience and adaptability.
arxiv-5700-43 | Revolutionary Algorithms | http://arxiv.org/pdf/1401.4714v1.pdf | author:Ronald Hochreiter, Christoph Waldhauser category:cs.NE published:2014-01-19 summary:The optimization of dynamic problems is both widespread and difficult. Whenconducting dynamic optimization, a balance between reinitialization andcomputational expense has to be found. There are multiple approaches to this.In parallel genetic algorithms, multiple sub-populations concurrently try tooptimize a potentially dynamic problem. But as the number of sub-populationincreases, their efficiency decreases. Cultural algorithms provide a frameworkthat has the potential to make optimizations more efficient. But they adaptslowly to changing environments. We thus suggest a confluence of theseapproaches: revolutionary algorithms. These algorithms seek to extend theevolutionary and cultural aspects of the former to approaches with a notion ofthe political. By modeling how belief systems are changed by means ofrevolution, these algorithms provide a framework to model and optimize dynamicproblems in an efficient fashion.
arxiv-5700-44 | The Capacity of String-Replication Systems | http://arxiv.org/pdf/1401.4634v1.pdf | author:Farzad Farnoud, Moshe Schwartz, Jehoshua Bruck category:cs.IT cs.CL math.IT published:2014-01-19 summary:It is known that the majority of the human genome consists of repeatedsequences. Furthermore, it is believed that a significant part of the rest ofthe genome also originated from repeated sequences and has mutated to itscurrent form. In this paper, we investigate the possibility of constructing anexponentially large number of sequences from a short initial sequence andsimple replication rules, including those resembling genomic replicationprocesses. In other words, our goal is to find out the capacity, or theexpressive power, of these string-replication systems. Our results includeexact capacities, and bounds on the capacities, of four fundamentalstring-replication systems.
arxiv-5700-45 | Visual Tracking using Particle Swarm Optimization | http://arxiv.org/pdf/1401.4648v1.pdf | author:Rafid Siddiqui, Siamak Khatibi category:cs.CV published:2014-01-19 summary:The problem of robust extraction of visual odometry from a sequence of imagesobtained by an eye in hand camera configuration is addressed. A novel approachtoward solving planar template based tracking is proposed which performs anon-linear image alignment for successful retrieval of camera transformations.In order to obtain global optimum a bio-metaheuristic is used for optimizationof similarity among the planar regions. The proposed method is validated onimage sequences with real as well as synthetic transformations and found to beresilient to intensity variations. A comparative analysis of the varioussimilarity measures as well as various state-of-art methods reveal that thealgorithm succeeds in tracking the planar regions robustly and has goodpotential to be used in real applications.
arxiv-5700-46 | Excess Risk Bounds for Exponentially Concave Losses | http://arxiv.org/pdf/1401.4566v2.pdf | author:Mehrdad Mahdavi, Rong Jin category:cs.LG stat.ML published:2014-01-18 summary:The overarching goal of this paper is to derive excess risk bounds forlearning from exp-concave loss functions in passive and sequential learningsettings. Exp-concave loss functions encompass several fundamental problems inmachine learning such as squared loss in linear regression, logistic loss inclassification, and negative logarithm loss in portfolio management. In batchsetting, we obtain sharp bounds on the performance of empirical riskminimization performed in a linear hypothesis space and with respect to theexp-concave loss functions. We also extend the results to the online settingwhere the learner receives the training examples in a sequential manner. Wepropose an online learning algorithm that is a properly modified version ofonline Newton method to obtain sharp risk bounds. Under an additional mildassumption on the loss function, we show that in both settings we are able toachieve an excess risk bound of $O(d\log n/n)$ that holds with a highprobability.
arxiv-5700-47 | Semantic Similarity Measures Applied to an Ontology for Human-Like Interaction | http://arxiv.org/pdf/1401.4603v1.pdf | author:Esperanza Albacete, Javier Calle, Elena Castro, Dolores Cuadra category:cs.AI cs.CL published:2014-01-18 summary:The focus of this paper is the calculation of similarity between two conceptsfrom an ontology for a Human-Like Interaction system. In order to facilitatethis calculation, a similarity function is proposed based on five dimensions(sort, compositional, essential, restrictive and descriptive) constituting thestructure of ontological knowledge. The paper includes a proposal for computinga similarity function for each dimension of knowledge. Later on, the similarityvalues obtained are weighted and aggregated to obtain a global similaritymeasure. In order to calculate those weights associated to each dimension, fourtraining methods have been proposed. The training methods differ in the elementto fit: the user, concepts or pairs of concepts, and a hybrid approach. Forevaluating the proposal, the knowledge base was fed from WordNet and extendedby using a knowledge editing toolkit (Cognos). The evaluation of the proposalis carried out through the comparison of system responses with those given byhuman test subjects, both providing a measure of the soundness of the procedureand revealing ways in which the proposal may be improved.
arxiv-5700-48 | The CQC Algorithm: Cycling in Graphs to Semantically Enrich and Enhance a Bilingual Dictionary | http://arxiv.org/pdf/1402.2561v1.pdf | author:Tiziano Flati, Roberto Navigli category:cs.CL published:2014-01-18 summary:Bilingual machine-readable dictionaries are knowledge resources useful inmany automatic tasks. However, compared to monolingual computational lexiconslike WordNet, bilingual dictionaries typically provide a lower amount ofstructured information, such as lexical and semantic relations, and often donot cover the entire range of possible translations for a word of interest. Inthis paper we present Cycles and Quasi-Cycles (CQC), a novel algorithm for theautomated disambiguation of ambiguous translations in the lexical entries of abilingual machine-readable dictionary. The dictionary is represented as agraph, and cyclic patterns are sought in the graph to assign an appropriatesense tag to each translation in a lexical entry. Further, we use thealgorithms output to improve the quality of the dictionary itself, bysuggesting accurate solutions to structural problems such as misalignments,partial alignments and missing entries. Finally, we successfully apply CQC tothe task of synonym extraction.
arxiv-5700-49 | Combining Evaluation Metrics via the Unanimous Improvement Ratio and its Application to Clustering Tasks | http://arxiv.org/pdf/1401.4590v1.pdf | author:Enrique Amigó, Julio Gonzalo, Javier Artiles, Felisa Verdejo category:cs.AI cs.LG published:2014-01-18 summary:Many Artificial Intelligence tasks cannot be evaluated with a single qualitycriterion and some sort of weighted combination is needed to provide systemrankings. A problem of weighted combination measures is that slight changes inthe relative weights may produce substantial changes in the system rankings.This paper introduces the Unanimous Improvement Ratio (UIR), a measure thatcomplements standard metric combination criteria (such as van Rijsbergen'sF-measure) and indicates how robust the measured differences are to changes inthe relative weights of the individual metrics. UIR is meant to elucidatewhether a perceived difference between two systems is an artifact of howindividual metrics are weighted. Besides discussing the theoretical foundations of UIR, this paper presentsempirical results that confirm the validity and usefulness of the metric forthe Text Clustering problem, where there is a tradeoff between precision andrecall based metrics and results are particularly sensitive to the weightingscheme used to combine them. Remarkably, our experiments show that UIR can beused as a predictor of how well differences between systems measured on a giventest bed will also hold in a different test bed.
arxiv-5700-50 | General factorization framework for context-aware recommendations | http://arxiv.org/pdf/1401.4529v2.pdf | author:Balázs Hidasi, Domonkos Tikk category:cs.IR cs.LG published:2014-01-18 summary:Context-aware recommendation algorithms focus on refining recommendations byconsidering additional information, available to the system. This topic hasgained a lot of attention recently. Among others, several factorization methodswere proposed to solve the problem, although most of them assume explicitfeedback which strongly limits their real-world applicability. While thesealgorithms apply various loss functions and optimization strategies, thepreference modeling under context is less explored due to the lack of toolsallowing for easy experimentation with various models. As context dimensionsare introduced beyond users and items, the space of possible preference modelsand the importance of proper modeling largely increases. In this paper we propose a General Factorization Framework (GFF), a singleflexible algorithm that takes the preference model as an input and computeslatent feature matrices for the input dimensions. GFF allows us to easilyexperiment with various linear models on any context-aware recommendation task,be it explicit or implicit feedback based. The scaling properties makes itusable under real life circumstances as well. We demonstrate the framework's potential by exploring various preferencemodels on a 4-dimensional context-aware problem with contexts that areavailable for almost any real life datasets. We show in our experiments --performed on five real life, implicit feedback datasets -- that properpreference modelling significantly increases recommendation accuracy, andpreviously unused models outperform the traditional ones. Novel models in GFFalso outperform state-of-the-art factorization algorithms. We also extend the method to be fully compliant to the MultidimensionalDataspace Model, one of the most extensive data models of context-enricheddata. Extended GFF allows the seamless incorporation of information into thefac[truncated]
arxiv-5700-51 | Learning to Win by Reading Manuals in a Monte-Carlo Framework | http://arxiv.org/pdf/1401.5390v1.pdf | author:S. R. K. Branavan, David Silver, Regina Barzilay category:cs.CL cs.AI cs.LG published:2014-01-18 summary:Domain knowledge is crucial for effective performance in autonomous controlsystems. Typically, human effort is required to encode this knowledge into acontrol algorithm. In this paper, we present an approach to language groundingwhich automatically interprets text in the context of a complex controlapplication, such as a game, and uses domain knowledge extracted from the textto improve control performance. Both text analysis and control strategies arelearned jointly using only a feedback signal inherent to the application. Toeffectively leverage textual information, our method automatically extracts thetext segment most relevant to the current game state, and labels it with atask-centric predicate structure. This labeled text is then used to bias anaction selection policy for the game, guiding it towards promising regions ofthe action space. We encode our model for text analysis and game playing in amulti-layer neural network, representing linguistic decisions via latentvariables in the hidden layers, and game action quality via the output layer.Operating within the Monte-Carlo Search framework, we estimate model parametersusing feedback from simulated games. We apply our approach to the complexstrategy game Civilization II using the official game manual as the text guide.Our results show that a linguistically-informed game-playing agentsignificantly outperforms its language-unaware counterpart, yielding a 34%absolute improvement and winning over 65% of games when playing against thebuilt-in AI of Civilization.
arxiv-5700-52 | miRNA and Gene Expression based Cancer Classification using Self- Learning and Co-Training Approaches | http://arxiv.org/pdf/1401.4589v1.pdf | author:Rania Ibrahim, Noha A. Yousri, Mohamed A. Ismail, Nagwa M. El-Makky category:cs.CE cs.LG published:2014-01-18 summary:miRNA and gene expression profiles have been proved useful for classifyingcancer samples. Efficient classifiers have been recently sought and developed.A number of attempts to classify cancer samples using miRNA/gene expressionprofiles are known in literature. However, the use of semi-supervised learningmodels have been used recently in bioinformatics, to exploit the huge corpusesof publicly available sets. Using both labeled and unlabeled sets to trainsample classifiers, have not been previously considered when gene and miRNAexpression sets are used. Moreover, there is a motivation to integrate bothmiRNA and gene expression for a semi-supervised cancer classification as thatprovides more information on the characteristics of cancer samples. In thispaper, two semi-supervised machine learning approaches, namely self-learningand co-training, are adapted to enhance the quality of cancer sampleclassification. These approaches exploit the huge public corpuses to enrich thetraining data. In self-learning, miRNA and gene based classifiers are enhancedindependently. While in co-training, both miRNA and gene expression profilesare used simultaneously to provide different views of cancer samples. To ourknowledge, it is the first attempt to apply these learning approaches to cancerclassification. The approaches were evaluated using breast cancer,hepatocellular carcinoma (HCC) and lung cancer expression sets. Results show upto 20% improvement in F1-measure over Random Forests and SVM classifiers.Co-Training also outperforms Low Density Separation (LDS) approach by around25% improvement in F1-measure in breast cancer.
arxiv-5700-53 | Modelling Observation Correlations for Active Exploration and Robust Object Detection | http://arxiv.org/pdf/1401.4612v1.pdf | author:Javier Velez, Garrett Hemann, Albert S. Huang, Ingmar Posner, Nicholas Roy category:cs.RO cs.CV published:2014-01-18 summary:Today, mobile robots are expected to carry out increasingly complex tasks inmultifarious, real-world environments. Often, the tasks require a certainsemantic understanding of the workspace. Consider, for example, spokeninstructions from a human collaborator referring to objects of interest; therobot must be able to accurately detect these objects to correctly understandthe instructions. However, existing object detection, while competent, is notperfect. In particular, the performance of detection algorithms is commonlysensitive to the position of the sensor relative to the objects in the scene.This paper presents an online planning algorithm which learns an explicit modelof the spatial dependence of object detection and generates plans whichmaximize the expected performance of the detection, and by extension theoverall plan performance. Crucially, the learned sensor model incorporatesspatial correlations between measurements, capturing the fact that successivemeasurements taken at the same or nearby locations are not independent. We showhow this sensor model can be incorporated into an efficient forward searchalgorithm in the information space of detected objects, allowing the robot togenerate motion plans efficiently. We investigate the performance of ourapproach by addressing the tasks of door and text detection in indoorenvironments and demonstrate significant improvement in detection performanceduring task execution over alternative methods in simulated and real robotexperiments.
arxiv-5700-54 | Generalized Biwords for Bitext Compression and Translation Spotting | http://arxiv.org/pdf/1401.5674v1.pdf | author:Felipe Sánchez-Martínez, Rafael C. Carrasco, Miguel A. Martínez-Prieto, Joaquin Adiego category:cs.CL published:2014-01-18 summary:Large bilingual parallel texts (also known as bitexts) are usually stored ina compressed form, and previous work has shown that they can be moreefficiently compressed if the fact that the two texts are mutual translationsis exploited. For example, a bitext can be seen as a sequence of biwords---pairs of parallel words with a high probability of co-occurrence--- that canbe used as an intermediate representation in the compression process. However,the simple biword approach described in the literature can only exploitone-to-one word alignments and cannot tackle the reordering of words. Wetherefore introduce a generalization of biwords which can describe multi-wordexpressions and reorderings. We also describe some methods for the binarycompression of generalized biword sequences, and compare their performance whendifferent schemes are applied to the extraction of the biword sequence. Inaddition, we show that this generalization of biwords allows for theimplementation of an efficient algorithm to look on the compressed bitext forwords or text segments in one of the texts and retrieve their counterparttranslations in the other text ---an application usually referred to astranslation spotting--- with only some minor modifications in the compressionalgorithm.
arxiv-5700-55 | Monte Carlo Simulation for Lasso-Type Problems by Estimator Augmentation | http://arxiv.org/pdf/1401.4425v2.pdf | author:Qing Zhou category:stat.ME stat.ML published:2014-01-17 summary:Regularized linear regression under the $\ell_1$ penalty, such as the Lasso,has been shown to be effective in variable selection and sparse modeling. Thesampling distribution of an $\ell_1$-penalized estimator $\hat{\beta}$ is hardto determine as the estimator is defined by an optimization problem that ingeneral can only be solved numerically and many of its components may beexactly zero. Let $S$ be the subgradient of the $\ell_1$ norm of thecoefficient vector $\beta$ evaluated at $\hat{\beta}$. We find that the jointsampling distribution of $\hat{\beta}$ and $S$, together called an augmentedestimator, is much more tractable and has a closed-form density under a normalerror distribution in both low-dimensional ($p\leq n$) and high-dimensional($p>n$) settings. Given $\beta$ and the error variance $\sigma^2$, one mayemploy standard Monte Carlo methods, such as Markov chain Monte Carlo andimportance sampling, to draw samples from the distribution of the augmentedestimator and calculate expectations with respect to the sampling distributionof $\hat{\beta}$. We develop a few concrete Monte Carlo algorithms anddemonstrate with numerical examples that our approach may offer huge advantagesand great flexibility in studying sampling distributions in $\ell_1$-penalizedlinear regression. We also establish nonasymptotic bounds on the differencebetween the true sampling distribution of $\hat{\beta}$ and its estimatorobtained by plugging in estimated parameters, which justifies the validity ofMonte Carlo simulation from an estimated sampling distribution even when $p\ggn\to \infty$.
arxiv-5700-56 | An Analysis of Random Projections in Cancelable Biometrics | http://arxiv.org/pdf/1401.4489v3.pdf | author:Devansh Arpit, Ifeoma Nwogu, Gaurav Srivastava, Venu Govindaraju category:cs.CV cs.LG stat.ML published:2014-01-17 summary:With increasing concerns about security, the need for highly secure physicalbiometrics-based authentication systems utilizing \emph{cancelable biometric}technologies is on the rise. Because the problem of cancelable templategeneration deals with the trade-off between template security and matchingperformance, many state-of-the-art algorithms successful in generating highquality cancelable biometrics all have random projection as one of their earlyprocessing steps. This paper therefore presents a formal analysis of why randomprojections is an essential step in cancelable biometrics. By formally definingthe notion of an \textit{Independent Subspace Structure} for datasets, it canbe shown that random projection preserves the subspace structure of datavectors generated from a union of independent linear subspaces. The bound onthe minimum number of random vectors required for this to hold is also derivedand is shown to depend logarithmically on the number of data samples, not onlyin independent subspaces but in disjoint subspace settings as well. Thetheoretical analysis presented is supported in detail with empirical results onreal-world face recognition datasets.
arxiv-5700-57 | Embedding Graphs under Centrality Constraints for Network Visualization | http://arxiv.org/pdf/1401.4408v1.pdf | author:Brian Baingana, Georgios B. Giannakis category:stat.ML published:2014-01-17 summary:Visual rendering of graphs is a key task in the mapping of complex networkdata. Although most graph drawing algorithms emphasize aesthetic appeal,certain applications such as travel-time maps place more importance onvisualization of structural network properties. The present paper advocates twograph embedding approaches with centrality considerations to comply with nodehierarchy. The problem is formulated first as one of constrainedmulti-dimensional scaling (MDS), and it is solved via block coordinate descentiterations with successive approximations and guaranteed convergence to a KKTpoint. In addition, a regularization term enforcing graph smoothness isincorporated with the goal of reducing edge crossings. A second approachleverages the locally-linear embedding (LLE) algorithm which assumes that thegraph encodes data sampled from a low-dimensional manifold. Closed-formsolutions to the resulting centrality-constrained optimization problems aredetermined yielding meaningful embeddings. Experimental results demonstrate theefficacy of both approaches, especially for visualizing large networks on theorder of thousands of nodes.
arxiv-5700-58 | Distortion-driven Turbulence Effect Removal using Variational Model | http://arxiv.org/pdf/1401.4221v1.pdf | author:Yuan Xie, Wensheng Zhang, Dacheng Tao, Wenrui Hu, Yanyun Qu, Hanzi Wang category:cs.CV published:2014-01-17 summary:It remains a challenge to simultaneously remove geometric distortion andspace-time-varying blur in frames captured through a turbulent atmosphericmedium. To solve, or at least reduce these effects, we propose a new scheme torecover a latent image from observed frames by integrating a new variationalmodel and distortion-driven spatial-temporal kernel regression. The proposedscheme first constructs a high-quality reference image from the observed framesusing low-rank decomposition. Then, to generate an improved registeredsequence, the reference image is iteratively optimized using a variationalmodel containing a new spatial-temporal regularization. The proposed fastalgorithm efficiently solves this model without the use of partial differentialequations (PDEs). Next, to reduce blur variation, distortion-drivenspatial-temporal kernel regression is carried out to fuse the registeredsequence into one image by introducing the concept of the near-stationarypatch. Applying a blind deconvolution algorithm to the fused image produces thefinal output. Extensive experimental testing shows, both qualitatively andquantitatively, that the proposed method can effectively alleviate distortionand blur and recover details of the original scene compared to state-of-the-artmethods.
arxiv-5700-59 | Entropy analysis of word-length series of natural language texts: Effects of text language and genre | http://arxiv.org/pdf/1401.4205v1.pdf | author:Maria Kalimeri, Vassilios Constantoudis, Constantinos Papadimitriou, Kostantinos Karamanos, Fotis K. Diakonos, Haris Papageorgiou category:cs.CL published:2014-01-17 summary:We estimate the $n$-gram entropies of natural language texts in word-lengthrepresentation and find that these are sensitive to text language and genre. Weattribute this sensitivity to changes in the probability distribution of thelengths of single words and emphasize the crucial role of the uniformity ofprobabilities of having words with length between five and ten. Furthermore,comparison with the entropies of shuffled data reveals the impact of wordlength correlations on the estimated $n$-gram entropies.
arxiv-5700-60 | A bi-level view of inpainting - based image compression | http://arxiv.org/pdf/1401.4112v2.pdf | author:Yunjin Chen, René Ranftl, Thomas Pock category:cs.CV published:2014-01-16 summary:Inpainting based image compression approaches, especially linear andnon-linear diffusion models, are an active research topic for lossy imagecompression. The major challenge in these compression models is to find a smallset of descriptive supporting points, which allow for an accuratereconstruction of the original image. It turns out in practice that this is achallenging problem even for the simplest Laplacian interpolation model. Inthis paper, we revisit the Laplacian interpolation compression model andintroduce two fast algorithms, namely successive preconditioning primal dualalgorithm and the recently proposed iPiano algorithm, to solve this problemefficiently. Furthermore, we extend the Laplacian interpolation basedcompression model to a more general form, which is based on principles frombi-level optimization. We investigate two different variants of the Laplacianmodel, namely biharmonic interpolation and smoothed Total Variationregularization. Our numerical results show that significant improvements can beobtained from the biharmonic interpolation model, and it can recover an imagewith very high quality from only 5% pixels.
arxiv-5700-61 | Which Clustering Do You Want? Inducing Your Ideal Clustering with Minimal Feedback | http://arxiv.org/pdf/1401.5389v1.pdf | author:Sajib Dasgupta, Vincent Ng category:cs.IR cs.CL cs.LG published:2014-01-16 summary:While traditional research on text clustering has largely focused on groupingdocuments by topic, it is conceivable that a user may want to cluster documentsalong other dimensions, such as the authors mood, gender, age, or sentiment.Without knowing the users intention, a clustering algorithm will only groupdocuments along the most prominent dimension, which may not be the one the userdesires. To address the problem of clustering documents along the user-desireddimension, previous work has focused on learning a similarity metric from datamanually annotated with the users intention or having a human construct afeature space in an interactive manner during the clustering process. With thegoal of reducing reliance on human knowledge for fine-tuning the similarityfunction or selecting the relevant features required by these approaches, wepropose a novel active clustering algorithm, which allows a user to easilyselect the dimension along which she wants to cluster the documents byinspecting only a small number of words. We demonstrate the viability of ouralgorithm on a variety of commonly-used sentiment datasets.
arxiv-5700-62 | Stochastic Backpropagation and Approximate Inference in Deep Generative Models | http://arxiv.org/pdf/1401.4082v3.pdf | author:Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra category:stat.ML cs.AI cs.LG stat.CO stat.ME published:2014-01-16 summary:We marry ideas from deep neural networks and approximate Bayesian inferenceto derive a generalised class of deep, directed generative models, endowed witha new algorithm for scalable inference and learning. Our algorithm introduces arecognition model to represent approximate posterior distributions, and thatacts as a stochastic encoder of the data. We develop stochasticback-propagation -- rules for back-propagation through stochastic variables --and use this to develop an algorithm that allows for joint optimisation of theparameters of both the generative and recognition model. We demonstrate onseveral real-world data sets that the model generates realistic samples,provides accurate imputations of missing data and is a useful tool forhigh-dimensional data visualisation.
arxiv-5700-63 | Kalman Temporal Differences | http://arxiv.org/pdf/1406.3270v1.pdf | author:Matthieu Geist, Olivier Pietquin category:cs.LG published:2014-01-16 summary:Because reinforcement learning suffers from a lack of scalability, onlinevalue (and Q-) function approximation has received increasing interest thislast decade. This contribution introduces a novel approximation scheme, namelythe Kalman Temporal Differences (KTD) framework, that exhibits the followingfeatures: sample-efficiency, non-linear approximation, non-stationarityhandling and uncertainty management. A first KTD-based algorithm is providedfor deterministic Markov Decision Processes (MDP) which produces biasedestimates in the case of stochastic transitions. Than the eXtended KTDframework (XKTD), solving stochastic MDP, is described. Convergence is analyzedfor special cases for both deterministic and stochastic transitions. Relatedalgorithms are experimented on classical benchmarks. They compare favorably tothe state of the art while exhibiting the announced features.
arxiv-5700-64 | Evaluating Temporal Graphs Built from Texts via Transitive Reduction | http://arxiv.org/pdf/1401.3865v1.pdf | author:Xavier Tannier, Philippe Muller category:cs.CL cs.IR published:2014-01-16 summary:Temporal information has been the focus of recent attention in informationextraction, leading to some standardization effort, in particular for the taskof relating events in a text. This task raises the problem of comparing twoannotations of a given text, because relations between events in a story areintrinsically interdependent and cannot be evaluated separately. A properevaluation measure is also crucial in the context of a machine learningapproach to the problem. Finding a common comparison referent at the text levelis not obvious, and we argue here in favor of a shift from event-based measuresto measures on a unique textual object, a minimal underlying temporal graph, ormore formally the transitive reduction of the graph of relations between eventboundaries. We support it by an investigation of its properties on syntheticdata and on a well-know temporal corpus.
arxiv-5700-65 | Narrowing the Modeling Gap: A Cluster-Ranking Approach to Coreference Resolution | http://arxiv.org/pdf/1405.5202v1.pdf | author:Altaf Rahman, Vincent Ng category:cs.CL published:2014-01-16 summary:Traditional learning-based coreference resolvers operate by training themention-pair model for determining whether two mentions are coreferent or not.Though conceptually simple and easy to understand, the mention-pair model islinguistically rather unappealing and lags far behind the heuristic-basedcoreference models proposed in the pre-statistical NLP era in terms ofsophistication. Two independent lines of recent research have attempted toimprove the mention-pair model, one by acquiring the mention-ranking model torank preceding mentions for a given anaphor, and the other by training theentity-mention model to determine whether a preceding cluster is coreferentwith a given mention. We propose a cluster-ranking approach to coreferenceresolution, which combines the strengths of the mention-ranking model and theentity-mention model, and is therefore theoretically more appealing than bothof these models. In addition, we seek to improve cluster rankers via twoextensions: (1) lexicalization and (2) incorporating knowledge of anaphoricityby jointly modeling anaphoricity determination and coreference resolution.Experimental results on the ACE data sets demonstrate the superior performanceof cluster rankers to competing approaches as well as the effectiveness of ourtwo extensions.
arxiv-5700-66 | Learning to Make Predictions In Partially Observable Environments Without a Generative Model | http://arxiv.org/pdf/1401.3870v1.pdf | author:Erik Talvitie, Satinder Singh category:cs.LG cs.AI stat.ML published:2014-01-16 summary:When faced with the problem of learning a model of a high-dimensionalenvironment, a common approach is to limit the model to make only a restrictedset of predictions, thereby simplifying the learning problem. These partialmodels may be directly useful for making decisions or may be combined togetherto form a more complete, structured model. However, in partially observable(non-Markov) environments, standard model-learning methods learn generativemodels, i.e. models that provide a probability distribution over all possiblefutures (such as POMDPs). It is not straightforward to restrict such models tomake only certain predictions, and doing so does not always simplify thelearning problem. In this paper we present prediction profile models:non-generative partial models for partially observable systems that make only agiven set of predictions, and are therefore far simpler than generative modelsin some cases. We formalize the problem of learning a prediction profile modelas a transformation of the original model-learning problem, and showempirically that one can learn prediction profile models that make a small setof important predictions even in systems that are too complex for standardgenerative models.
arxiv-5700-67 | Non-Deterministic Policies in Markovian Decision Processes | http://arxiv.org/pdf/1401.3871v1.pdf | author:Mahdi Milani Fard, Joelle Pineau category:cs.AI cs.LG published:2014-01-16 summary:Markovian processes have long been used to model stochastic environments.Reinforcement learning has emerged as a framework to solve sequential planningand decision-making problems in such environments. In recent years, attemptswere made to apply methods from reinforcement learning to construct decisionsupport systems for action selection in Markovian environments. Althoughconventional methods in reinforcement learning have proved to be useful inproblems concerning sequential decision-making, they cannot be applied in theircurrent form to decision support systems, such as those in medical domains, asthey suggest policies that are often highly prescriptive and leave little roomfor the users input. Without the ability to provide flexible guidelines, it isunlikely that these methods can gain ground with users of such systems. Thispaper introduces the new concept of non-deterministic policies to allow moreflexibility in the users decision-making process, while constraining decisionsto remain near optimal solutions. We provide two algorithms to computenon-deterministic policies in discrete domains. We study the output and runningtime of these method on a set of synthetic and real-world problems. In anexperiment with human subjects, we show that humans assisted by hints based onnon-deterministic policies outperform both human-only and computer-only agentsin a web navigation task.
arxiv-5700-68 | Properties of Bethe Free Energies and Message Passing in Gaussian Models | http://arxiv.org/pdf/1401.3877v1.pdf | author:Botond Cseke, Tom Heskes category:cs.LG cs.AI stat.ML published:2014-01-16 summary:We address the problem of computing approximate marginals in Gaussianprobabilistic models by using mean field and fractional Bethe approximations.We define the Gaussian fractional Bethe free energy in terms of the momentparameters of the approximate marginals, derive a lower and an upper bound onthe fractional Bethe free energy and establish a necessary condition for thelower bound to be bounded from below. It turns out that the condition isidentical to the pairwise normalizability condition, which is known to be asufficient condition for the convergence of the message passing algorithm. Weshow that stable fixed points of the Gaussian message passing algorithm arelocal minima of the Gaussian Bethe free energy. By a counterexample, wedisprove the conjecture stating that the unboundedness of the free energyimplies the divergence of the message passing algorithm.
arxiv-5700-69 | Regression Conformal Prediction with Nearest Neighbours | http://arxiv.org/pdf/1401.3880v1.pdf | author:Harris Papadopoulos, Vladimir Vovk, Alex Gammerman category:cs.LG published:2014-01-16 summary:In this paper we apply Conformal Prediction (CP) to the k-Nearest NeighboursRegression (k-NNR) algorithm and propose ways of extending the typicalnonconformity measure used for regression so far. Unlike traditional regressionmethods which produce point predictions, Conformal Predictors output predictiveregions that satisfy a given confidence level. The regions produced by anyConformal Predictor are automatically valid, however their tightness andtherefore usefulness depends on the nonconformity measure used by each CP. Ineffect a nonconformity measure evaluates how strange a given example iscompared to a set of other examples based on some traditional machine learningalgorithm. We define six novel nonconformity measures based on the k-NearestNeighbours Regression algorithm and develop the corresponding CPs followingboth the original (transductive) and the inductive CP approaches. A comparisonof the predictive regions produced by our measures with those of the typicalregression measure suggests that a major improvement in terms of predictiveregion tightness is achieved by the new measures.
arxiv-5700-70 | Efficient Multi-Start Strategies for Local Search Algorithms | http://arxiv.org/pdf/1401.3894v1.pdf | author:András György, Levente Kocsis category:cs.LG cs.AI stat.ML published:2014-01-16 summary:Local search algorithms applied to optimization problems often suffer fromgetting trapped in a local optimum. The common solution for this deficiency isto restart the algorithm when no progress is observed. Alternatively, one canstart multiple instances of a local search algorithm, and allocatecomputational resources (in particular, processing time) to the instancesdepending on their behavior. Hence, a multi-start strategy has to decide(dynamically) when to allocate additional resources to a particular instanceand when to start new instances. In this paper we propose multi-startstrategies motivated by works on multi-armed bandit problems and Lipschitzoptimization with an unknown constant. The strategies continuously estimate thepotential performance of each algorithm instance by supposing a convergencerate of the local search algorithm up to an unknown constant, and in everyphase allocate resources to those instances that could converge to the optimumfor a particular range of the constant. Asymptotic bounds are given on theperformance of the strategies. In particular, we prove that at most a quadraticincrease in the number of times the target function is evaluated is needed toachieve the performance of a local search algorithm started from the attractionregion of the optimum. Experiments are provided using SPSA (SimultaneousPerturbation Stochastic Approximation) and k-means as local search algorithms,and the results indicate that the proposed strategies work well in practice,and, in all cases studied, need only logarithmically more evaluations of thetarget function as opposed to the theoretically suggested quadratic increase.
arxiv-5700-71 | Controlling Complexity in Part-of-Speech Induction | http://arxiv.org/pdf/1401.6131v1.pdf | author:João V. Graça, Kuzman Ganchev, Luisa Coheur, Fernando Pereira, Ben Taskar category:cs.CL cs.LG published:2014-01-16 summary:We consider the problem of fully unsupervised learning of grammatical(part-of-speech) categories from unlabeled text. The standardmaximum-likelihood hidden Markov model for this task performs poorly, becauseof its weak inductive bias and large model capacity. We address this problem byrefining the model and modifying the learning objective to control its capacityvia para- metric and non-parametric constraints. Our approach enforcesword-category association sparsity, adds morphological and orthographicfeatures, and eliminates hard-to-estimate parameters for rare words. We developan efficient learning algorithm that is not much more computationally intensivethan standard training. We also provide an open-source implementation of thealgorithm. Our experiments on five diverse languages (Bulgarian, Danish,English, Portuguese, Spanish) achieve significant improvements compared withprevious methods for the same task.
arxiv-5700-72 | Cause Identification from Aviation Safety Incident Reports via Weakly Supervised Semantic Lexicon Construction | http://arxiv.org/pdf/1401.4436v1.pdf | author:Muhammad Arshad Ul Abedin, Vincent Ng, Latifur Khan category:cs.CL cs.LG published:2014-01-16 summary:The Aviation Safety Reporting System collects voluntarily submitted reportson aviation safety incidents to facilitate research work aiming to reduce suchincidents. To effectively reduce these incidents, it is vital to accuratelyidentify why these incidents occurred. More precisely, given a set of possiblecauses, or shaping factors, this task of cause identification involvesidentifying all and only those shaping factors that are responsible for theincidents described in a report. We investigate two approaches to causeidentification. Both approaches exploit information provided by a semanticlexicon, which is automatically constructed via Thelen and Riloffs Basiliskframework augmented with our linguistic and algorithmic modifications. Thefirst approach labels a report using a simple heuristic, which looks for thewords and phrases acquired during the semantic lexicon learning process in thereport. The second approach recasts cause identification as a textclassification problem, employing supervised and transductive textclassification algorithms to learn models from incident reports labeled withshaping factors and using the models to label unseen reports. Our experimentsshow that both the heuristic-based approach and the learning-based approach(when given sufficient training data) outperform the baseline systemsignificantly.
arxiv-5700-73 | Policy Invariance under Reward Transformations for General-Sum Stochastic Games | http://arxiv.org/pdf/1401.3907v1.pdf | author:Xiaosong Lu, Howard M. Schwartz, Sidney N. Givigi Jr category:cs.GT cs.LG published:2014-01-16 summary:We extend the potential-based shaping method from Markov decision processesto multi-player general-sum stochastic games. We prove that the Nash equilibriain a stochastic game remains unchanged after potential-based shaping is appliedto the environment. The property of policy invariance provides a possible wayof speeding convergence when learning to play a stochastic game.
arxiv-5700-74 | Centrality-as-Relevance: Support Sets and Similarity as Geometric Proximity | http://arxiv.org/pdf/1401.3908v1.pdf | author:Ricardo Ribeiro, David Martins de Matos category:cs.IR cs.CL published:2014-01-16 summary:In automatic summarization, centrality-as-relevance means that the mostimportant content of an information source, or a collection of informationsources, corresponds to the most central passages, considering a representationwhere such notion makes sense (graph, spatial, etc.). We assess the mainparadigms, and introduce a new centrality-based relevance model for automaticsummarization that relies on the use of support sets to better estimate therelevant content. Geometric proximity is used to compute semantic relatedness.Centrality (relevance) is determined by considering the whole input source (andnot only local information), and by taking into account the existence of minortopics or lateral subjects in the information sources to be summarized. Themethod consists in creating, for each passage of the input source, a supportset consisting only of the most semantically related passages. Then, thedetermination of the most relevant content is achieved by selecting thepassages that occur in the largest number of support sets. This model producesextractive summaries that are generic, and language- and domain-independent.Thorough automatic evaluation shows that the method achieves state-of-the-artperformance, both in written text, and automatically transcribed speechsummarization, including when compared to considerably more complex approaches.
arxiv-5700-75 | Nonparametric Latent Tree Graphical Models: Inference, Estimation, and Structure Learning | http://arxiv.org/pdf/1401.3940v1.pdf | author:Le Song, Han Liu, Ankur Parikh, Eric Xing category:stat.ML published:2014-01-16 summary:Tree structured graphical models are powerful at expressing long range orhierarchical dependency among many variables, and have been widely applied indifferent areas of computer science and statistics. However, existing methodsfor parameter estimation, inference, and structure learning mainly rely on theGaussian or discrete assumptions, which are restrictive under manyapplications. In this paper, we propose new nonparametric methods based onreproducing kernel Hilbert space embeddings of distributions that can recoverthe latent tree structures, estimate the parameters, and perform inference forhigh dimensional continuous and non-Gaussian variables. The usefulness of theproposed methods are illustrated by thorough numerical results.
arxiv-5700-76 | An Empirical Evaluation of Similarity Measures for Time Series Classification | http://arxiv.org/pdf/1401.3973v1.pdf | author:Joan Serrà, Josep Lluis Arcos category:cs.LG cs.CV stat.ML published:2014-01-16 summary:Time series are ubiquitous, and a measure to assess their similarity is acore part of many computational systems. In particular, the similarity measureis the most essential ingredient of time series clustering and classificationsystems. Because of this importance, countless approaches to estimate timeseries similarity have been proposed. However, there is a lack of comparativestudies using empirical, rigorous, quantitative, and large-scale assessmentstrategies. In this article, we provide an extensive evaluation of similaritymeasures for time series classification following the aforementionedprinciples. We consider 7 different measures coming from alternative measure`families', and 45 publicly-available time series data sets coming from a widevariety of scientific domains. We focus on out-of-sample classificationaccuracy, but in-sample accuracies and parameter choices are also discussed.Our work is based on rigorous evaluation methodologies and includes the use ofpowerful statistical significance tests to derive meaningful conclusions. Theobtained results show the equivalence, in terms of accuracy, of a number ofmeasures, but with one single candidate outperforming the rest. Such findings,together with the followed methodology, invite researchers on the field toadopt a more consistent evaluation criteria and a more informed decisionregarding the baseline measures to which new developments should be compared.
arxiv-5700-77 | Using Local Alignments for Relation Recognition | http://arxiv.org/pdf/1405.7713v1.pdf | author:Sophia Katrenko, Pieter Adriaans, Maarten van Someren category:cs.CL cs.IR cs.LG published:2014-01-16 summary:This paper discusses the problem of marrying structural similarity withsemantic relatedness for Information Extraction from text. Aiming at accuraterecognition of relations, we introduce local alignment kernels and explorevarious possibilities of using them for this task. We give a definition of alocal alignment (LA) kernel based on the Smith-Waterman score as a sequencesimilarity measure and proceed with a range of possibilities for computingsimilarity between elements of sequences. We show how distributional similaritymeasures obtained from unlabeled data can be incorporated into the learningtask as semantic knowledge. Our experiments suggest that the LA kernel yieldspromising results on various biomedical corpora outperforming two baselines bya large margin. Additional series of experiments have been conducted on thedata sets of seven general relation types, where the performance of the LAkernel is comparable to the current state-of-the-art results.
arxiv-5700-78 | Community Detection in Networks using Graph Distance | http://arxiv.org/pdf/1401.3915v2.pdf | author:Sharmodeep Bhattacharyya, Peter J. Bickel category:stat.ML cs.SI published:2014-01-16 summary:The study of networks has received increased attention recently not only fromthe social sciences and statistics but also from physicists, computerscientists and mathematicians. One of the principal problem in networks iscommunity detection. Many algorithms have been proposed for community findingbut most of them do not have have theoretical guarantee for sparse networks andnetworks close to the phase transition boundary proposed by physicists. Thereare some exceptions but all have some incomplete theoretical basis. Here wepropose an algorithm based on the graph distance of vertices in the network. Wegive theoretical guarantees that our method works in identifying communitiesfor block models and can be extended for degree-corrected block models andblock models with the number of communities growing with number of vertices.Despite favorable simulation results, we are not yet able to conclude that ourmethod is satisfactory for worst possible case. We illustrate on a network ofpolitical blogs, Facebook networks and some other networks.
arxiv-5700-79 | An Active Learning Approach for Jointly Estimating Worker Performance and Annotation Reliability with Crowdsourced Data | http://arxiv.org/pdf/1401.3836v1.pdf | author:Liyue Zhao, Yu Zhang, Gita Sukthankar category:cs.LG cs.HC published:2014-01-16 summary:Crowdsourcing platforms offer a practical solution to the problem ofaffordably annotating large datasets for training supervised classifiers.Unfortunately, poor worker performance frequently threatens to compromiseannotation reliability, and requesting multiple labels for every instance canlead to large cost increases without guaranteeing good results. Minimizing therequired training samples using an active learning selection procedure reducesthe labeling requirement but can jeopardize classifier training by focusing onerroneous annotations. This paper presents an active learning approach in whichworker performance, task difficulty, and annotation reliability are jointlyestimated and used to compute the risk function guiding the sample selectionprocedure. We demonstrate that the proposed approach, which employs activelearning with Bayesian networks, significantly improves training accuracy andcorrectly ranks the expertise of unknown labelers in the presence of annotationnoise.
arxiv-5700-80 | Seeded Graph Matching Via Joint Optimization of Fidelity and Commensurability | http://arxiv.org/pdf/1401.3813v1.pdf | author:Vince Lyzinski, Sancar Adali, Joshua T. Vogelstein, Youngser Park, Carey E. Priebe category:stat.ML stat.AP stat.ME published:2014-01-16 summary:We present a novel approximate graph matching algorithm that incorporatesseeded data into the graph matching paradigm. Our Joint Optimization ofFidelity and Commensurability (JOFC) algorithm embeds two graphs into a commonEuclidean space where the matching inference task can be performed. Throughreal and simulated data examples, we demonstrate the versatility of ouralgorithm in matching graphs with various characteristics--weightedness,directedness, loopiness, many-to-one and many-to-many matchings, and softseedings.
arxiv-5700-81 | Learning $\ell_1$-based analysis and synthesis sparsity priors using bi-level optimization | http://arxiv.org/pdf/1401.4105v1.pdf | author:Yunjin Chen, Thomas Pock, Horst Bischof category:cs.CV published:2014-01-16 summary:We consider the analysis operator and synthesis dictionary learning problemsbased on the the $\ell_1$ regularized sparse representation model. We revealthe internal relations between the $\ell_1$-based analysis model and synthesismodel. We then introduce an approach to learn both analysis operator andsynthesis dictionary simultaneously by using a unified framework of bi-leveloptimization. Our aim is to learn a meaningful operator (dictionary) such thatthe minimum energy solution of the analysis (synthesis)-prior based model is asclose as possible to the ground-truth. We solve the bi-level optimizationproblem using the implicit differentiation technique. Moreover, we demonstratethe effectiveness of our leaning approach by applying the learned analysisoperator (dictionary) to the image denoising task and comparing its performancewith state-of-the-art methods. Under this unified framework, we can compare theperformance of the two types of priors.
arxiv-5700-82 | Structured Priors for Sparse-Representation-Based Hyperspectral Image Classification | http://arxiv.org/pdf/1401.3818v1.pdf | author:Xiaoxia Sun, Qing Qu, Nasser M. Nasrabadi, Trac D. Tran category:cs.CV cs.LG stat.ML published:2014-01-16 summary:Pixel-wise classification, where each pixel is assigned to a predefinedclass, is one of the most important procedures in hyperspectral image (HSI)analysis. By representing a test pixel as a linear combination of a smallsubset of labeled pixels, a sparse representation classifier (SRC) gives ratherplausible results compared with that of traditional classifiers such as thesupport vector machine (SVM). Recently, by incorporating additional structuredsparsity priors, the second generation SRCs have appeared in the literature andare reported to further improve the performance of HSI. These priors are basedon exploiting the spatial dependencies between the neighboring pixels, theinherent structure of the dictionary, or both. In this paper, we review andcompare several structured priors for sparse-representation-based HSIclassification. We also propose a new structured prior called the low rankgroup prior, which can be considered as a modification of the low rank prior.Furthermore, we will investigate how different structured priors improve theresult for the HSI classification.
arxiv-5700-83 | Revisiting loss-specific training of filter-based MRFs for image restoration | http://arxiv.org/pdf/1401.4107v1.pdf | author:Yunjin Chen, Thomas Pock, René Ranftl, Horst Bischof category:cs.CV published:2014-01-16 summary:It is now well known that Markov random fields (MRFs) are particularlyeffective for modeling image priors in low-level vision. Recent years have seenthe emergence of two main approaches for learning the parameters in MRFs: (1)probabilistic learning using sampling-based algorithms and (2) loss-specifictraining based on MAP estimate. After investigating existing trainingapproaches, it turns out that the performance of the loss-specific training hasbeen significantly underestimated in existing work. In this paper, we revisitthis approach and use techniques from bi-level optimization to solve it. Weshow that we can get a substantial gain in the final performance by solving thelower-level problem in the bi-level framework with high accuracy using ournewly proposed algorithm. As a result, our trained model is on par with highlyspecialized image denoising algorithms and clearly outperformsprobabilistically trained MRF models. Our findings suggest that for theloss-specific training scheme, solving the lower-level problem with higheraccuracy is beneficial. Our trained model comes along with the additionaladvantage, that inference is extremely efficient. Our GPU-based implementationtakes less than 1s to produce state-of-the-art performance.
arxiv-5700-84 | Constructing Reference Sets from Unstructured, Ungrammatical Text | http://arxiv.org/pdf/1401.3832v1.pdf | author:Matthew Michelson, Craig A. Knoblock category:cs.CL cs.IR published:2014-01-16 summary:Vast amounts of text on the Web are unstructured and ungrammatical, such asclassified ads, auction listings, forum postings, etc. We call such text"posts." Despite their inconsistent structure and lack of grammar, posts arefull of useful information. This paper presents work on semi-automaticallybuilding tables of relational information, called "reference sets," byanalyzing such posts directly. Reference sets can be applied to a number oftasks such as ontology maintenance and information extraction. Ourreference-set construction method starts with just a small amount of backgroundknowledge, and constructs tuples representing the entities in the posts to forma reference set. We also describe an extension to this approach for the specialcase where even this small amount of background knowledge is impossible todiscover and use. To evaluate the utility of the machine-constructed referencesets, we compare them to manually constructed reference sets in the context ofreference-set-based information extraction. Our results show the reference setsconstructed by our method outperform manually constructed reference sets. Wealso compare the reference-set-based extraction approach using themachine-constructed reference set to supervised extraction approaches usinggeneric features. These results demonstrate that using machine-constructedreference sets outperforms the supervised methods, even though the supervisedmethods require training data.
arxiv-5700-85 | Towards the selection of patients requiring ICD implantation by automatic classification from Holter monitoring indices | http://arxiv.org/pdf/1401.4128v1.pdf | author:Charles-Henri Cappelaere, R. Dubois, P. Roussel, G. Dreyfus category:cs.LG stat.AP published:2014-01-16 summary:The purpose of this study is to optimize the selection of prophylacticcardioverter defibrillator implantation candidates. Currently, the maincriterion for implantation is a low Left Ventricular Ejection Fraction (LVEF)whose specificity is relatively poor. We designed two classifiers aimed topredict, from long term ECG recordings (Holter), whether a low-LVEF patient islikely or not to undergo ventricular arrhythmia in the next six months. Oneclassifier is a single hidden layer neural network whose variables are the mostrelevant features extracted from Holter recordings, and the other classifierhas a structure that capitalizes on the physiological decomposition of thearrhythmogenic factors into three disjoint groups: the myocardial substrate,the triggers and the autonomic nervous system (ANS). In this ad hoc network,the features were assigned to each group; one neural network classifier pergroup was designed and its complexity was optimized. The outputs of theclassifiers were fed to a single neuron that provided the required probabilityestimate. The latter was thresholded for final discrimination A datasetcomposed of 186 pre-implantation 30-mn Holter recordings of patients equippedwith an implantable cardioverter defibrillator (ICD) in primary prevention wasused in order to design and test this classifier. 44 out of 186 patientsunderwent at least one treated ventricular arrhythmia during the six-monthfollow-up period. Performances of the designed classifier were evaluated usinga cross-test strategy that consists in splitting the database into severalcombinations of a training set and a test set. The average arrhythmiaprediction performances of the ad-hoc classifier are NPV = 77% $\pm$ 13% andPPV = 31% $\pm$ 19% (Negative Predictive Value $\pm$ std, Positive PredictiveValue $\pm$ std). According to our study, improving prophylacticICD-implantation candidate selection by automatic classification from ECGfeatures may be possible, but the availability of a sizable dataset appears tobe essential to decrease the number of False Negatives.
arxiv-5700-86 | Context-based Word Acquisition for Situated Dialogue in a Virtual World | http://arxiv.org/pdf/1401.6875v1.pdf | author:Shaolin Qu, Joyce Y. Chai category:cs.CL published:2014-01-16 summary:To tackle the vocabulary problem in conversational systems, previous work hasapplied unsupervised learning approaches on co-occurring speech and eye gazeduring interaction to automatically acquire new words. Although theseapproaches have shown promise, several issues related to human languagebehavior and human-machine conversation have not been addressed. First,psycholinguistic studies have shown certain temporal regularities between humaneye movement and language production. While these regularities can potentiallyguide the acquisition process, they have not been incorporated in the previousunsupervised approaches. Second, conversational systems generally have anexisting knowledge base about the domain and vocabulary. While the existingknowledge can potentially help bootstrap and constrain the acquired new words,it has not been incorporated in the previous models. Third, eye gaze couldserve different functions in human-machine conversation. Some gaze streams maynot be closely coupled with speech stream, and thus are potentially detrimentalto word acquisition. Automated recognition of closely-coupled speech-gazestreams based on conversation context is important. To address these issues, wedeveloped new approaches that incorporate user language behavior, domainknowledge, and conversation context in word acquisition. We evaluated theseapproaches in the context of situated dialogue in a virtual world. Ourexperimental results have shown that incorporating the above three types ofcontextual information significantly improves word acquisition performance.
arxiv-5700-87 | RoxyBot-06: Stochastic Prediction and Optimization in TAC Travel | http://arxiv.org/pdf/1401.3829v1.pdf | author:Amy Greenwald, Seong Jae Lee, Victor Naroditskiy category:cs.GT cs.LG published:2014-01-16 summary:In this paper, we describe our autonomous bidding agent, RoxyBot, who emergedvictorious in the travel division of the 2006 Trading Agent Competition in aphoto finish. At a high level, the design of many successful trading agents canbe summarized as follows: (i) price prediction: build a model of market prices;and (ii) optimization: solve for an approximately optimal set of bids, giventhis model. To predict, RoxyBot builds a stochastic model of market prices bysimulating simultaneous ascending auctions. To optimize, RoxyBot relies on thesample average approximation method, a stochastic optimization technique.
arxiv-5700-88 | Convex Optimization for Binary Classifier Aggregation in Multiclass Problems | http://arxiv.org/pdf/1401.4143v1.pdf | author:Sunho Park, TaeHyun Hwang, Seungjin Choi category:cs.LG published:2014-01-16 summary:Multiclass problems are often decomposed into multiple binary problems thatare solved by individual binary classifiers whose results are integrated into afinal answer. Various methods, including all-pairs (APs), one-versus-all (OVA),and error correcting output code (ECOC), have been studied, to decomposemulticlass problems into binary problems. However, little study has been madeto optimally aggregate binary problems to determine a final answer to themulticlass problem. In this paper we present a convex optimization method foran optimal aggregation of binary classifiers to estimate class membershipprobabilities in multiclass problems. We model the class membership probabilityas a softmax function which takes a conic combination of discrepancies inducedby individual binary classifiers, as an input. With this model, we formulatethe regularized maximum likelihood estimation as a convex optimization problem,which is solved by the primal-dual interior point method. Connections of ourmethod to large margin classifiers are presented, showing that the large marginformulation can be considered as a limiting case of our convex formulation.Numerical experiments on synthetic and real-world data sets demonstrate thatour method outperforms existing aggregation methods as well as direct methods,in terms of the classification accuracy and the quality of class membershipprobability estimates.
arxiv-5700-89 | Training a Multilingual Sportscaster: Using Perceptual Context to Learn Language | http://arxiv.org/pdf/1405.7711v1.pdf | author:David L. Chen, Joohyun Kim, Raymond J. Mooney category:cs.CL published:2014-01-16 summary:We present a novel framework for learning to interpret and generate languageusing only perceptual context as supervision. We demonstrate its capabilitiesby developing a system that learns to sportscast simulated robot soccer gamesin both English and Korean without any language-specific prior knowledge.Training employs only ambiguous supervision consisting of a stream ofdescriptive textual comments and a sequence of events extracted from thesimulation trace. The system simultaneously establishes correspondences betweenindividual comments and the events that they describe while building atranslation model that supports both parsing and generation. We also present anovel algorithm for learning which events are worth describing. Humanevaluations of the generated commentaries indicate they are of reasonablequality and in some cases even on par with those produced by humans for ourlimited domain.
arxiv-5700-90 | Amino Acid Interaction Network Prediction using Multi-objective Optimization | http://arxiv.org/pdf/1401.3446v1.pdf | author:Md. Shiplu Hawlader, Saifuddin Md. Tareeq category:cs.CE cs.NE published:2014-01-15 summary:Protein can be represented by amino acid interaction network. This network isa graph whose vertices are the proteins amino acids and whose edges are theinteractions between them. This interaction network is the first step ofproteins three-dimensional structure prediction. In this paper we present amulti-objective evolutionary algorithm for interaction prediction and antcolony probabilistic optimization algorithm is used to confirm the interaction.
arxiv-5700-91 | Transductive Rademacher Complexity and its Applications | http://arxiv.org/pdf/1401.3441v1.pdf | author:Ran El-Yaniv, Dmitry Pechyony category:cs.LG cs.AI stat.ML published:2014-01-15 summary:We develop a technique for deriving data-dependent error bounds fortransductive learning algorithms based on transductive Rademacher complexity.Our technique is based on a novel general error bound for transduction in termsof transductive Rademacher complexity, together with a novel bounding techniquefor Rademacher averages for particular algorithms, in terms of their"unlabeled-labeled" representation. This technique is relevant to many advancedgraph-based transductive algorithms and we demonstrate its effectiveness byderiving error bounds to three well known algorithms. Finally, we present a newPAC-Bayesian bound for mixtures of transductive algorithms based on ourRademacher bounds.
arxiv-5700-92 | Anytime Induction of Low-cost, Low-error Classifiers: a Sampling-based Approach | http://arxiv.org/pdf/1401.3447v1.pdf | author:Saher Esmeir, Shaul Markovitch category:cs.LG published:2014-01-15 summary:Machine learning techniques are gaining prevalence in the production of awide range of classifiers for complex real-world applications with nonuniformtesting and misclassification costs. The increasing complexity of theseapplications poses a real challenge to resource management during learning andclassification. In this work we introduce ACT (anytime cost-sensitive treelearner), a novel framework for operating in such complex environments. ACT isan anytime algorithm that allows learning time to be increased in return forlower classification costs. It builds a tree top-down and exploits additionaltime resources to obtain better estimations for the utility of the differentcandidate splits. Using sampling techniques, ACT approximates the cost of thesubtree under each candidate split and favors the one with a minimal cost. As astochastic algorithm, ACT is expected to be able to escape local minima, intowhich greedy methods may be trapped. Experiments with a variety of datasetswere conducted to compare ACT to the state-of-the-art cost-sensitive treelearners. The results show that for the majority of domains ACT producessignificantly less costly trees. ACT also exhibits good anytime behavior withdiminishing returns.
arxiv-5700-93 | Adaptive Stochastic Resource Control: A Machine Learning Approach | http://arxiv.org/pdf/1401.3434v1.pdf | author:Balázs Csanád Csáji, László Monostori category:cs.LG published:2014-01-15 summary:The paper investigates stochastic resource allocation problems with scarce,reusable resources and non-preemtive, time-dependent, interconnected tasks.This approach is a natural generalization of several standard resourcemanagement problems, such as scheduling and transportation problems. First,reactive solutions are considered and defined as control policies of suitablyreformulated Markov decision processes (MDPs). We argue that this reformulationhas several favorable properties, such as it has finite state and actionspaces, it is aperiodic, hence all policies are proper and the space of controlpolicies can be safely restricted. Next, approximate dynamic programming (ADP)methods, such as fitted Q-learning, are suggested for computing an efficientcontrol policy. In order to compactly maintain the cost-to-go function, tworepresentations are studied: hash tables and support vector regression (SVR),particularly, nu-SVRs. Several additional improvements, such as the applicationof limited-lookahead rollout algorithms in the initial phases, action spacedecomposition, task clustering and distributed sampling are investigated, too.Finally, experimental results on both benchmark and industry-related data arepresented.
arxiv-5700-94 | Identification of Pleonastic It Using the Web | http://arxiv.org/pdf/1401.5698v1.pdf | author:Yifan Li, Petr Musilek, Marek Reformat, Loren Wyard-Scott category:cs.CL published:2014-01-15 summary:In a significant minority of cases, certain pronouns, especially the pronounit, can be used without referring to any specific entity. This phenomenon ofpleonastic pronoun usage poses serious problems for systems aiming at even ashallow understanding of natural language texts. In this paper, a novelapproach is proposed to identify such uses of it: the extrapositional cases areidentified using a series of queries against the web, and the cleft cases areidentified using a simple set of syntactic rules. The system is evaluated withfour sets of news articles containing 679 extrapositional cases as well as 78cleft constructs. The identification results are comparable to those obtainedby human efforts.
arxiv-5700-95 | A Rigorously Bayesian Beam Model and an Adaptive Full Scan Model for Range Finders in Dynamic Environments | http://arxiv.org/pdf/1401.3432v1.pdf | author:Tinne De Laet, Joris De Schutter, Herman Bruyninckx category:cs.AI cs.LG published:2014-01-15 summary:This paper proposes and experimentally validates a Bayesian network model ofa range finder adapted to dynamic environments. All modeling assumptions arerigorously explained, and all model parameters have a physical interpretation.This approach results in a transparent and intuitive model. With respect to thestate of the art beam model this paper: (i) proposes a different functionalform for the probability of range measurements caused by unmodeled objects,(ii) intuitively explains the discontinuity encountered in te state of the artbeam model, and (iii) reduces the number of model parameters, while maintainingthe same representational power for experimental data. The proposed beam modelis called RBBM, short for Rigorously Bayesian Beam Model. A maximum likelihoodand a variational Bayesian estimator (both based on expectation-maximization)are proposed to learn the model parameters. Furthermore, the RBBM is extended to a full scan model in two steps: first,to a full scan model for static environments and next, to a full scan model forgeneral, dynamic environments. The full scan model accounts for the dependencybetween beams and adapts to the local sample density when using a particlefilter. In contrast to Gaussian-based state of the art models, the proposedfull scan model uses a sample-based approximation. This sample-basedapproximation enables handling dynamic environments and capturingmulti-modality, which occurs even in simple static environments.
arxiv-5700-96 | Latent Tree Models and Approximate Inference in Bayesian Networks | http://arxiv.org/pdf/1401.3429v1.pdf | author:Yi Wang, Nevin L. Zhang, Tao Chen category:cs.LG published:2014-01-15 summary:We propose a novel method for approximate inference in Bayesian networks(BNs). The idea is to sample data from a BN, learn a latent tree model (LTM)from the data offline, and when online, make inference with the LTM instead ofthe original BN. Because LTMs are tree-structured, inference takes linear time.In the meantime, they can represent complex relationship among leaf nodes andhence the approximation accuracy is often good. Empirical evidence shows thatour method can achieve good approximation accuracy at low online computationalcost.
arxiv-5700-97 | A Multiagent Reinforcement Learning Algorithm with Non-linear Dynamics | http://arxiv.org/pdf/1401.3454v1.pdf | author:Sherief Abdallah, Victor Lesser category:cs.LG cs.MA published:2014-01-15 summary:Several multiagent reinforcement learning (MARL) algorithms have beenproposed to optimize agents decisions. Due to the complexity of the problem,the majority of the previously developed MARL algorithms assumed agents eitherhad some knowledge of the underlying game (such as Nash equilibria) and/orobserved other agents actions and the rewards they received. We introduce a new MARL algorithm called the Weighted Policy Learner (WPL),which allows agents to reach a Nash Equilibrium (NE) in benchmark2-player-2-action games with minimum knowledge. Using WPL, the only feedback anagent needs is its own local reward (the agent does not observe other agentsactions or rewards). Furthermore, WPL does not assume that agents know theunderlying game or the corresponding Nash Equilibrium a priori. Weexperimentally show that our algorithm converges in benchmarktwo-player-two-action games. We also show that our algorithm converges in thechallenging Shapleys game where previous MARL algorithms failed to convergewithout knowing the underlying game or the NE. Furthermore, we show that WPLoutperforms the state-of-the-art algorithms in a more realistic setting of 100agents interacting and learning concurrently. An important aspect of understanding the behavior of a MARL algorithm isanalyzing the dynamics of the algorithm: how the policies of multiple learningagents evolve over time as agents interact with one another. Such an analysisnot only verifies whether agents using a given MARL algorithm will eventuallyconverge, but also reveals the behavior of the MARL algorithm prior toconvergence. We analyze our algorithm in two-player-two-action games and showthat symbolically proving WPLs convergence is difficult, because of thenon-linear nature of WPLs dynamics, unlike previous MARL algorithms that hadeither linear or piece-wise-linear dynamics. Instead, we numerically solve WPLsdynamics differential equations and compare the solution to the dynamics ofprevious MARL algorithms.
arxiv-5700-98 | Analogical Dissimilarity: Definition, Algorithms and Two Experiments in Machine Learning | http://arxiv.org/pdf/1401.3427v1.pdf | author:Laurent Miclet, Sabri Bayoudh, Arnaud Delhay category:cs.LG cs.AI published:2014-01-15 summary:This paper defines the notion of analogical dissimilarity between fourobjects, with a special focus on objects structured as sequences. Firstly, itstudies the case where the four objects have a null analogical dissimilarity,i.e. are in analogical proportion. Secondly, when one of these objects isunknown, it gives algorithms to compute it. Thirdly, it tackles the problem ofdefining analogical dissimilarity, which is a measure of how far four objectsare from being in analogical proportion. In particular, when objects aresequences, it gives a definition and an algorithm based on an optimal alignmentof the four sequences. It gives also learning algorithms, i.e. methods to findthe triple of objects in a learning sample which has the least analogicaldissimilarity with a given object. Two practical experiments are described: thefirst is a classification problem on benchmarks of binary and nominal data, thesecond shows how the generation of sequences by solving analogical equationsenables a handwritten character recognition system to rapidly be adapted to anew writer.
arxiv-5700-99 | Infinite Mixed Membership Matrix Factorization | http://arxiv.org/pdf/1401.3413v1.pdf | author:Avneesh Saluja, Mahdi Pakdaman, Dongzhen Piao, Ankur P. Parikh category:cs.LG cs.IR published:2014-01-15 summary:Rating and recommendation systems have become a popular application area forapplying a suite of machine learning techniques. Current approaches relyprimarily on probabilistic interpretations and extensions of matrixfactorization, which factorizes a user-item ratings matrix into latent user anditem vectors. Most of these methods fail to model significant variations initem ratings from otherwise similar users, a phenomenon known as the "NapoleonDynamite" effect. Recent efforts have addressed this problem by adding acontextual bias term to the rating, which captures the mood under which a userrates an item or the context in which an item is rated by a user. In this work,we extend this model in a nonparametric sense by learning the optimal number ofmoods or contexts from the data, and derive Gibbs sampling inference proceduresfor our model. We evaluate our approach on the MovieLens 1M dataset, and showsignificant improvements over the optimal parametric baseline, more than twicethe improvements previously encountered for this task. We also extract andevaluate a DBLP dataset, wherein we predict the number of papers co-authored bytwo authors, and present improvements over the parametric baseline on thisalternative domain as well.
arxiv-5700-100 | Learning Document-Level Semantic Properties from Free-Text Annotations | http://arxiv.org/pdf/1401.3457v1.pdf | author:S. R. K. Branavan, Harr Chen, Jacob Eisenstein, Regina Barzilay category:cs.CL cs.IR published:2014-01-15 summary:This paper presents a new method for inferring the semantic properties ofdocuments by leveraging free-text keyphrase annotations. Such annotations arebecoming increasingly abundant due to the recent dramatic growth insemi-structured, user-generated online content. One especially relevant domainis product reviews, which are often annotated by their authors with pros/conskeyphrases such as a real bargain or good value. These annotations arerepresentative of the underlying semantic properties; however, unlike expertannotations, they are noisy: lay authors may use different labels to denote thesame property, and some labels may be missing. To learn using such noisyannotations, we find a hidden paraphrase structure which clusters thekeyphrases. The paraphrase structure is linked with a latent topic model of thereview texts, enabling the system to predict the properties of unannotateddocuments and to effectively aggregate the semantic properties of multiplereviews. Our approach is implemented as a hierarchical Bayesian model withjoint inference. We find that joint inference increases the robustness of thekeyphrase clustering and encourages the latent topics to correlate withsemantically meaningful properties. Multiple evaluations demonstrate that ourmodel substantially outperforms alternative approaches for summarizing singleand multiple documents into a set of semantically salient keyphrases.
arxiv-5700-101 | Sentence Compression as Tree Transduction | http://arxiv.org/pdf/1401.5693v1.pdf | author:Trevor Anthony Cohn, Mirella Lapata category:cs.CL published:2014-01-15 summary:This paper presents a tree-to-tree transduction method for sentencecompression. Our model is based on synchronous tree substitution grammar, aformalism that allows local distortion of the tree topology and can thusnaturally capture structural mismatches. We describe an algorithm for decodingin this framework and show how the model can be trained discriminatively withina large margin framework. Experimental results on sentence compression bringsignificant improvements over a state-of-the-art model.
arxiv-5700-102 | Wikipedia-based Semantic Interpretation for Natural Language Processing | http://arxiv.org/pdf/1401.5697v1.pdf | author:Evgeniy Gabrilovich, Shaul Markovitch category:cs.CL published:2014-01-15 summary:Adequate representation of natural language semantics requires access to vastamounts of common sense and domain-specific world knowledge. Prior work in thefield was based on purely statistical techniques that did not make use ofbackground knowledge, on limited lexicographic knowledge bases such as WordNet,or on huge manual efforts such as the CYC project. Here we propose a novelmethod, called Explicit Semantic Analysis (ESA), for fine-grained semanticinterpretation of unrestricted natural language texts. Our method representsmeaning in a high-dimensional space of concepts derived from Wikipedia, thelargest encyclopedia in existence. We explicitly represent the meaning of anytext in terms of Wikipedia-based concepts. We evaluate the effectiveness of ourmethod on text categorization and on computing the degree of semanticrelatedness between fragments of natural language text. Using ESA results insignificant improvements over the previous state of the art in both tasks.Importantly, due to the use of natural concepts, the ESA model is easy toexplain to human users.
arxiv-5700-103 | Learning Bayesian Network Equivalence Classes with Ant Colony Optimization | http://arxiv.org/pdf/1401.3464v1.pdf | author:Rónán Daly, Qiang Shen category:cs.NE cs.AI cs.LG published:2014-01-15 summary:Bayesian networks are a useful tool in the representation of uncertainknowledge. This paper proposes a new algorithm called ACO-E, to learn thestructure of a Bayesian network. It does this by conducting a search throughthe space of equivalence classes of Bayesian networks using Ant ColonyOptimization (ACO). To this end, two novel extensions of traditional ACOtechniques are proposed and implemented. Firstly, multiple types of moves areallowed. Secondly, moves can be given in terms of indices that are not based onconstruction graph nodes. The results of testing show that ACO-E performsbetter than a greedy search and other state-of-the-art and metaheuristicalgorithms whilst searching in the space of equivalence classes.
arxiv-5700-104 | Inferring Shallow-Transfer Machine Translation Rules from Small Parallel Corpora | http://arxiv.org/pdf/1401.5700v1.pdf | author:Felipe Sánchez-Martínez, Mikel L. Forcada category:cs.CL published:2014-01-15 summary:This paper describes a method for the automatic inference of structuraltransfer rules to be used in a shallow-transfer machine translation (MT) systemfrom small parallel corpora. The structural transfer rules are based onalignment templates, like those used in statistical MT. Alignment templates areextracted from sentence-aligned parallel corpora and extended with a set ofrestrictions which are derived from the bilingual dictionary of the MT systemand control their application as transfer rules. The experiments conductedusing three different language pairs in the free/open-source MT platformApertium show that translation quality is improved as compared to word-for-wordtranslation (when no transfer rules are used), and that the resultingtranslation quality is close to that obtained using hand-coded transfer rules.The method we present is entirely unsupervised and benefits from information inthe rest of modules of the MT system in which the inferred rules are applied.
arxiv-5700-105 | Unsupervised Methods for Determining Object and Relation Synonyms on the Web | http://arxiv.org/pdf/1401.5696v1.pdf | author:Alexander Pieter Yates, Oren Etzioni category:cs.CL published:2014-01-15 summary:The task of identifying synonymous relations and objects, or synonymresolution, is critical for high-quality information extraction. This paperinvestigates synonym resolution in the context of unsupervised informationextraction, where neither hand-tagged training examples nor domain knowledge isavailable. The paper presents a scalable, fully-implemented system that runs inO(KN log N) time in the number of extractions, N, and the maximum number ofsynonyms per word, K. The system, called Resolver, introduces a probabilisticrelational model for predicting whether two strings are co-referential based onthe similarity of the assertions containing them. On a set of two millionassertions extracted from the Web, Resolver resolves objects with 78% precisionand 68% recall, and resolves relations with 90% precision and 35% recall.Several variations of resolvers probabilistic model are explored, andexperiments demonstrate that under appropriate conditions these variations canimprove F1 by 5%. An extension to the basic Resolver system allows it to handlepolysemous names with 97% precision and 95% recall on a data set from the TRECcorpus.
arxiv-5700-106 | Efficient Markov Network Structure Discovery Using Independence Tests | http://arxiv.org/pdf/1401.3478v1.pdf | author:Facundo Bromberg, Dimitris Margaritis, Vasant Honavar category:cs.LG cs.AI stat.ML published:2014-01-15 summary:We present two algorithms for learning the structure of a Markov network fromdata: GSMN* and GSIMN. Both algorithms use statistical independence tests toinfer the structure by successively constraining the set of structuresconsistent with the results of these tests. Until very recently, algorithms forstructure learning were based on maximum likelihood estimation, which has beenproved to be NP-hard for Markov networks due to the difficulty of estimatingthe parameters of the network, needed for the computation of the datalikelihood. The independence-based approach does not require the computation ofthe likelihood, and thus both GSMN* and GSIMN can compute the structureefficiently (as shown in our experiments). GSMN* is an adaptation of theGrow-Shrink algorithm of Margaritis and Thrun for learning the structure ofBayesian networks. GSIMN extends GSMN* by additionally exploiting Pearlswell-known properties of the conditional independence relation to infer novelindependences from known ones, thus avoiding the performance of statisticaltests to estimate them. To accomplish this efficiently GSIMN uses the Triangletheorem, also introduced in this work, which is a simplified version of the setof Markov axioms. Experimental comparisons on artificial and real-world datasets show GSIMN can yield significant savings with respect to GSMN*, whilegenerating a Markov network with comparable or in some cases improved quality.We also compare GSIMN to a forward-chaining implementation, called GSIMN-FCH,that produces all possible conditional independences resulting from repeatedlyapplying Pearls theorems on the known conditional independence tests. Theresults of this comparison show that GSIMN, by the sole use of the Triangletheorem, is nearly optimal in terms of the set of independences tests that itinfers.
arxiv-5700-107 | Complex Question Answering: Unsupervised Learning Approaches and Experiments | http://arxiv.org/pdf/1401.3479v1.pdf | author:Yllias Chali, Shafiq Rayhan Joty, Sadid A. Hasan category:cs.CL cs.IR cs.LG published:2014-01-15 summary:Complex questions that require inferencing and synthesizing information frommultiple documents can be seen as a kind of topic-oriented, informativemulti-document summarization where the goal is to produce a single text as acompressed version of a set of documents with a minimum loss of relevantinformation. In this paper, we experiment with one empirical method and twounsupervised statistical machine learning techniques: K-means and ExpectationMaximization (EM), for computing relative importance of the sentences. Wecompare the results of these approaches. Our experiments show that theempirical approach outperforms the other two techniques and EM performs betterthan K-means. However, the performance of these approaches depends entirely onthe feature set used and the weighting of these features. In order to measurethe importance and relevance to the user query we extract different kinds offeatures (i.e. lexical, lexical semantic, cosine similarity, basic element,tree kernel based syntactic and shallow-semantic) for each of the documentsentences. We use a local search technique to learn the weights of thefeatures. To the best of our knowledge, no study has used tree kernel functionsto encode syntactic/semantic information for more complex tasks such ascomputing the relatedness between the query sentences and the documentsentences in order to generate query-focused summaries (or answers to complexquestions). For each of our methods of generating summaries (i.e. empirical,K-means and EM) we show the effects of syntactic and shallow-semantic featuresover the bag-of-words (BOW) features.
arxiv-5700-108 | Low-Rank Modeling and Its Applications in Image Analysis | http://arxiv.org/pdf/1401.3409v3.pdf | author:Xiaowei Zhou, Can Yang, Hongyu Zhao, Weichuan Yu category:cs.CV cs.LG stat.ML published:2014-01-15 summary:Low-rank modeling generally refers to a class of methods that solve problemsby representing variables of interest as low-rank matrices. It has achievedgreat success in various fields including computer vision, data mining, signalprocessing and bioinformatics. Recently, much progress has been made intheories, algorithms and applications of low-rank modeling, such as exactlow-rank matrix recovery via convex programming and matrix completion appliedto collaborative filtering. These advances have brought more and moreattentions to this topic. In this paper, we review the recent advance oflow-rank modeling, the state-of-the-art algorithms, and related applications inimage analysis. We first give an overview to the concept of low-rank modelingand challenging problems in this area. Then, we summarize the models andalgorithms for low-rank matrix recovery and illustrate their advantages andlimitations with numerical experiments. Next, we introduce a few applicationsof low-rank modeling in the context of image analysis. Finally, we concludethis paper with some discussions.
arxiv-5700-109 | Enhancing QA Systems with Complex Temporal Question Processing Capabilities | http://arxiv.org/pdf/1401.3482v1.pdf | author:Estela Saquete, Jose Luis Vicedo, Patricio Martínez-Barco, Rafael Muñoz, Hector Llorens category:cs.CL cs.AI cs.IR published:2014-01-15 summary:This paper presents a multilayered architecture that enhances thecapabilities of current QA systems and allows different types of complexquestions or queries to be processed. The answers to these questions need to begathered from factual information scattered throughout different documents.Specifically, we designed a specialized layer to process the different types oftemporal questions. Complex temporal questions are first decomposed into simplequestions, according to the temporal relations expressed in the originalquestion. In the same way, the answers to the resulting simple questions arerecomposed, fulfilling the temporal restrictions of the original complexquestion. A novel aspect of this approach resides in the decomposition whichuses a minimal quantity of resources, with the final aim of obtaining aportable platform that is easily extensible to other languages. In this paperwe also present a methodology for evaluation of the decomposition of thequestions as well as the ability of the implemented temporal layer to performat a multilingual level. The temporal layer was first performed for English,then evaluated and compared with: a) a general purpose QA system (F-measure65.47% for QA plus English temporal layer vs. 38.01% for the general QAsystem), and b) a well-known QA system. Much better results were obtained fortemporal questions with the multilayered system. This system was thereforeextended to Spanish and very good results were again obtained in the evaluation(F-measure 40.36% for QA plus Spanish temporal layer vs. 22.94% for the generalQA system).
arxiv-5700-110 | Coordinate Descent with Online Adaptation of Coordinate Frequencies | http://arxiv.org/pdf/1401.3737v1.pdf | author:Tobias Glasmachers, Ürün Dogan category:stat.ML cs.LG published:2014-01-15 summary:Coordinate descent (CD) algorithms have become the method of choice forsolving a number of optimization problems in machine learning. They areparticularly popular for training linear models, including linear supportvector machine classification, LASSO regression, and logistic regression. We consider general CD with non-uniform selection of coordinates. Instead offixing selection frequencies beforehand we propose an online adaptationmechanism for this important parameter, called the adaptive coordinatefrequencies (ACF) method. This mechanism removes the need to estimate optimalcoordinate frequencies beforehand, and it automatically reacts to changingrequirements during an optimization run. We demonstrate the usefulness of our ACF-CD approach for a variety ofoptimization problems arising in machine learning contexts. Our algorithmoffers significant speed-ups over state-of-the-art training methods.
arxiv-5700-111 | Content Modeling Using Latent Permutations | http://arxiv.org/pdf/1401.3488v1.pdf | author:Harr Chen, S. R. K. Branavan, Regina Barzilay, David R. Karger category:cs.IR cs.CL cs.LG published:2014-01-15 summary:We present a novel Bayesian topic model for learning discourse-level documentstructure. Our model leverages insights from discourse theory to constrainlatent topic assignments in a way that reflects the underlying organization ofdocument topics. We propose a global model in which both topic selection andordering are biased to be similar across a collection of related documents. Weshow that this space of orderings can be effectively represented using adistribution over permutations called the Generalized Mallows Model. We applyour method to three complementary discourse-level tasks: cross-documentalignment, document segmentation, and information ordering. Our experimentsshow that incorporating our permutation-based model in these applicationsyields substantial improvements in performance over previously proposedmethods.
arxiv-5700-112 | Multilingual Part-of-Speech Tagging: Two Unsupervised Approaches | http://arxiv.org/pdf/1401.5695v1.pdf | author:Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, Regina Barzilay category:cs.CL published:2014-01-15 summary:We demonstrate the effectiveness of multilingual learning for unsupervisedpart-of-speech tagging. The central assumption of our work is that by combiningcues from multiple languages, the structure of each becomes more apparent. Weconsider two ways of applying this intuition to the problem of unsupervisedpart-of-speech tagging: a model that directly merges tag structures for a pairof languages into a single sequence and a second model which insteadincorporates multilingual context using latent variables. Both approaches areformulated as hierarchical Bayesian models, using Markov Chain Monte Carlosampling techniques for inference. Our results demonstrate that byincorporating multilingual evidence we can achieve impressive performance gainsacross a range of scenarios. We also found that performance improves steadilyas the number of available languages increases.
arxiv-5700-113 | A Brief History of Learning Classifier Systems: From CS-1 to XCS | http://arxiv.org/pdf/1401.3607v2.pdf | author:Larry Bull category:cs.NE cs.LG published:2014-01-15 summary:Modern Learning Classifier Systems can be characterized by their use of ruleaccuracy as the utility metric for the search algorithm(s) discovering usefulrules. Such searching typically takes place within the restricted space ofco-active rules for efficiency. This paper gives an historical overview of theevolution of such systems up to XCS, and then some of the subsequentdevelopments of XCS to different types of learning.
arxiv-5700-114 | Cross-lingual Annotation Projection for Semantic Roles | http://arxiv.org/pdf/1401.5694v1.pdf | author:Sebastian Pado, Mirella Lapata category:cs.CL published:2014-01-15 summary:This article considers the task of automatically inducing role-semanticannotations in the FrameNet paradigm for new languages. We propose a generalframework that is based on annotation projection, phrased as a graphoptimization problem. It is relatively inexpensive and has the potential toreduce the human effort involved in creating role-semantic resources. Withinthis framework, we present projection models that exploit lexical and syntacticinformation. We provide an experimental evaluation on an English-Germanparallel corpus which demonstrates the feasibility of inducing high-precisionGerman semantic role annotation both for manually and automatically annotatedEnglish data.
arxiv-5700-115 | Text Relatedness Based on a Word Thesaurus | http://arxiv.org/pdf/1401.5699v1.pdf | author:George Tsatsaronis, Iraklis Varlamis, Michalis Vazirgiannis category:cs.CL published:2014-01-15 summary:The computation of relatedness between two fragments of text in an automatedmanner requires taking into account a wide range of factors pertaining to themeaning the two fragments convey, and the pairwise relations between theirwords. Without doubt, a measure of relatedness between text segments must takeinto account both the lexical and the semantic relatedness between words. Sucha measure that captures well both aspects of text relatedness may help in manytasks, such as text retrieval, classification and clustering. In this paper wepresent a new approach for measuring the semantic relatedness between wordsbased on their implicit semantic links. The approach exploits only a wordthesaurus in order to devise implicit semantic links between words. Based onthis approach, we introduce Omiotis, a new measure of semantic relatednessbetween texts which capitalizes on the word-to-word semantic relatednessmeasure (SR) and extends it to measure the relatedness between texts. Wegradually validate our method: we first evaluate the performance of thesemantic relatedness measure between individual words, covering word-to-wordsimilarity and relatedness, synonym identification and word analogy; then, weproceed with evaluating the performance of our method in measuring text-to-textsemantic relatedness in two tasks, namely sentence-to-sentence similarity andparaphrase recognition. Experimental evaluation shows that the proposed methodoutperforms every lexicon-based method of semantic relatedness in the selectedtasks and the used data sets, and competes well against corpus-based and hybridapproaches.
arxiv-5700-116 | Improving Performance Of English-Hindi Cross Language Information Retrieval Using Transliteration Of Query Terms | http://arxiv.org/pdf/1401.3510v1.pdf | author:Saurabh Varshney, Jyoti Bajpai category:cs.IR cs.CL published:2014-01-15 summary:The main issue in Cross Language Information Retrieval (CLIR) is the poorperformance of retrieval in terms of average precision when compared tomonolingual retrieval performance. The main reasons behind poor performance ofCLIR are mismatching of query terms, lexical ambiguity and un-translated queryterms. The existing problems of CLIR are needed to be addressed in order toincrease the performance of the CLIR system. In this paper, we are putting oureffort to solve the given problem by proposed an algorithm for improving theperformance of English-Hindi CLIR system. We used all possible combination ofHindi translated query using transliteration of English query terms andchoosing the best query among them for retrieval of documents. The experimentis performed on FIRE 2010 (Forum of Information Retrieval Evaluation) datasets.The experimental result show that the proposed approach gives betterperformance of English-Hindi CLIR system and also helps in overcoming existingproblems and outperforms the existing English-Hindi CLIR system in terms ofaverage precision.
arxiv-5700-117 | Intelligent Systems for Information Security | http://arxiv.org/pdf/1401.3592v1.pdf | author:Ayman M. Bahaa-Eldin category:cs.NE cs.CR published:2014-01-15 summary:This thesis aims to use intelligent systems to extend and improve performanceand security of cryptographic techniques. Genetic algorithms framework forcryptanalysis problem is addressed. A novel extension to the differentialcryptanalysis using genetic algorithm is proposed and a fitness measure basedon the differential characteristics of the cipher being attacked is alsoproposed. The complexity of the proposed attack is shown to be less thanquarter of normal differential cryptanalysis of the same cipher by applying theproposed attack to both the basic Substitution Permutation Network and theFeistel Network. The basic models of modern block ciphers are attacked insteadof actual cipher to prove that the attack is applicable to other ciphersvulnerable to differential cryptanalysis. A new attack for block cipher basedon the ability of neural networks to perform an approximation of mapping isproposed. A complete problem formulation is explained and implementation of theattack on some hypothetical Feistel cipher not vulnerable to differential orlinear attacks is presented. A new block cipher based on the neural networks isproposed. A complete cipher structure is given and a key scheduling is alsoshown. The main properties of neural network being able to perform mappingbetween large dimension domains in a very fast and a very small memory comparedto S-Boxes is used as a base for the cipher.
arxiv-5700-118 | Hrebs and Cohesion Chains as similar tools for semantic text properties research | http://arxiv.org/pdf/1401.3669v1.pdf | author:D. Tatar, M. Lupea, E. Kapetanios category:cs.CL published:2014-01-15 summary:In this study it is proven that the Hrebs used in Denotation analysis oftexts and Cohesion Chains (de?ned as a fusion between Lexical Chains andCoreference Chains) represent similar linguistic tools. This result gives usthe possibility to extend to Cohesion Chains (CCs) some important indicatorsas, for example the Kernel of CCs, the topicality of a CC, text concentration,CC-di?useness and mean di?useness of the text. Let us mention that nowhere inthe Lexical Chains or Coreference Chains literature these kinds of indicatorsare introduced and used since now. Similarly, some applications of CCs in thestudy of a text (as for example segmentation or summarization of a text) couldbe realized starting from hrebs. As an illustration of the similarity betweenHrebs and CCs a detailed analyze of the poem "Lacul" by Mihai Eminescu isgiven.
arxiv-5700-119 | Highly comparative feature-based time-series classification | http://arxiv.org/pdf/1401.3531v2.pdf | author:Ben D. Fulcher, Nick S. Jones category:cs.LG cs.AI cs.DB q-bio.QM published:2014-01-15 summary:A highly comparative, feature-based approach to time series classification isintroduced that uses an extensive database of algorithms to extract thousandsof interpretable features from time series. These features are derived fromacross the scientific time-series analysis literature, and include summaries oftime series in terms of their correlation structure, distribution, entropy,stationarity, scaling properties, and fits to a range of time-series models.After computing thousands of features for each time series in a training set,those that are most informative of the class structure are selected usinggreedy forward feature selection with a linear classifier. The resultingfeature-based classifiers automatically learn the differences between classesusing a reduced number of time-series properties, and circumvent the need tocalculate distances between time series. Representing time series in this wayresults in orders of magnitude of dimensionality reduction, allowing the methodto perform well on very large datasets containing long time series or timeseries of different lengths. For many of the datasets studied, classificationperformance exceeded that of conventional instance-based classifiers, includingone nearest neighbor classifiers using Euclidean distances and dynamic timewarping and, most importantly, the features selected provide an understandingof the properties of the dataset, insight that can guide further scientificinvestigation.
arxiv-5700-120 | Convex Relaxations of SE(2) and SE(3) for Visual Pose Estimation | http://arxiv.org/pdf/1401.3700v2.pdf | author:Matanya B. Horowitz, Nikolai Matni, Joel W. Burdick category:cs.CV published:2014-01-15 summary:This paper proposes a new method for rigid body pose estimation based onspectrahedral representations of the tautological orbitopes of $SE(2)$ and$SE(3)$. The approach can use dense point cloud data from stereo vision or anRGB-D sensor (such as the Microsoft Kinect), as well as visual appearance data.The method is a convex relaxation of the classical pose estimation problem, andis based on explicit linear matrix inequality (LMI) representations for theconvex hulls of $SE(2)$ and $SE(3)$. Given these representations, the relaxedpose estimation problem can be framed as a robust least squares problem withthe optimization variable constrained to these convex sets. Although thisformulation is a relaxation of the original problem, numerical experimentsindicate that it is indeed exact - i.e. its solution is a member of $SE(2)$ or$SE(3)$ - in many interesting settings. We additionally show that this methodis guaranteed to be exact for a large class of pose estimation problems.
arxiv-5700-121 | Bayesian Conditional Density Filtering | http://arxiv.org/pdf/1401.3632v3.pdf | author:Shaan Qamar, Rajarshi Guhaniyogi, David B. Dunson category:stat.ML cs.LG stat.CO published:2014-01-15 summary:We propose a Conditional Density Filtering (C-DF) algorithm for efficientonline Bayesian inference. C-DF adapts MCMC sampling to the online setting,sampling from approximations to conditional posterior distributions obtained bypropagating surrogate conditional sufficient statistics (a function of data andparameter estimates) as new data arrive. These quantities eliminate the need tostore or process the entire dataset simultaneously and offer a number ofdesirable features. Often, these include a reduction in memory requirements andruntime and improved mixing, along with state-of-the-art parameter inferenceand prediction. These improvements are demonstrated through severalillustrative examples including an application to high dimensional compressedregression. Finally, we show that C-DF samples converge to the target posteriordistribution asymptotically as sampling proceeds and more data arrives.
arxiv-5700-122 | An Enhanced Method For Evaluating Automatic Video Summaries | http://arxiv.org/pdf/1401.3590v3.pdf | author:Karim M. Mahmoud category:cs.CV cs.IR published:2014-01-14 summary:Evaluation of automatic video summaries is a challenging problem. In the pastyears, some evaluation methods are presented that utilize only a single featurelike color feature to detect similarity between automatic video summaries andground-truth user summaries. One of the drawbacks of using a single feature isthat sometimes it gives a false similarity detection which makes the assessmentof the quality of the generated video summary less perceptual and not accurate.In this paper, a novel method for evaluating automatic video summaries ispresented. This method is based on comparing automatic video summariesgenerated by video summarization techniques with ground-truth user summaries.The objective of this evaluation method is to quantify the quality of videosummaries, and allow comparing different video summarization techniquesutilizing both color and texture features of the video frames and using theBhattacharya distance as a dissimilarity measure due to its advantages. OurExperiments show that the proposed evaluation method overcomes the drawbacks ofother methods and gives a more perceptual evaluation of the quality of theautomatic video summaries.
arxiv-5700-123 | Dynamic Topology Adaptation and Distributed Estimation for Smart Grids | http://arxiv.org/pdf/1401.3148v1.pdf | author:S. Xu, R. C. de Lamare, H. V. Poor category:cs.IT cs.LG math.IT published:2014-01-14 summary:This paper presents new dynamic topology adaptation strategies fordistributed estimation in smart grids systems. We propose a dynamic exhaustivesearch--based topology adaptation algorithm and a dynamic sparsity--inspiredtopology adaptation algorithm, which can exploit the topology of smart gridswith poor--quality links and obtain performance gains. We incorporate anoptimized combining rule, named Hastings rule into our proposed dynamictopology adaptation algorithms. Compared with the existing works in theliterature on distributed estimation, the proposed algorithms have a betterconvergence rate and significantly improve the system performance. Theperformance of the proposed algorithms is compared with that of existingalgorithms in the IEEE 14--bus system.
arxiv-5700-124 | Across neighbourhood search for numerical optimization | http://arxiv.org/pdf/1401.3376v3.pdf | author:Guohua Wu category:cs.NE published:2014-01-14 summary:Population-based search algorithms (PBSAs), including swarm intelligencealgorithms (SIAs) and evolutionary algorithms (EAs), are competitivealternatives for solving complex optimization problems and they have beenwidely applied to real-world optimization problems in different fields. In thisstudy, a novel population-based across neighbourhood search (ANS) is proposedfor numerical optimization. ANS is motivated by two straightforward assumptionsand three important issues raised in improving and designing efficient PBSAs.In ANS, a group of individuals collaboratively search the solution space for anoptimal solution of the optimization problem considered. A collection ofsuperior solutions found by individuals so far is maintained and updateddynamically. At each generation, an individual directly searches across theneighbourhoods of multiple superior solutions with the guidance of a Gaussiandistribution. This search manner is referred to as across neighbourhood search.The characteristics of ANS are discussed and the concept comparisons with otherPBSAs are given. The principle behind ANS is simple. Moreover, ANS is easy forimplementation and application with three parameters being required to tune.Extensive experiments on 18 benchmark optimization functions of different typesshow that ANS has well balanced exploration and exploitation capabilities andperforms competitively compared with many efficient PBSAs (Related Matlab codesused in the experiments are available fromhttp://guohuawunudt.gotoip2.com/publications.html).
arxiv-5700-125 | Online Markov decision processes with Kullback-Leibler control cost | http://arxiv.org/pdf/1401.3198v1.pdf | author:Peng Guan, Maxim Raginsky, Rebecca Willett category:math.OC cs.LG cs.SY published:2014-01-14 summary:This paper considers an online (real-time) control problem that involves anagent performing a discrete-time random walk over a finite state space. Theagent's action at each time step is to specify the probability distribution forthe next state given the current state. Following the set-up of Todorov, thestate-action cost at each time step is a sum of a state cost and a control costgiven by the Kullback-Leibler (KL) divergence between the agent's next-statedistribution and that determined by some fixed passive dynamics. The onlineaspect of the problem is due to the fact that the state cost functions aregenerated by a dynamic environment, and the agent learns the current state costonly after selecting an action. An explicit construction of a computationallyefficient strategy with small regret (i.e., expected difference between itsactual total cost and the smallest cost attainable using noncausal knowledge ofthe state costs) under mild regularity conditions is presented, along with ademonstration of the performance of the proposed strategy on a simulated targettracking problem. A number of new results on Markov decision processes with KLcontrol cost are also obtained.
arxiv-5700-126 | A Boosting Approach to Learning Graph Representations | http://arxiv.org/pdf/1401.3258v1.pdf | author:Rajmonda Caceres, Kevin Carter, Jeremy Kun category:cs.LG cs.SI stat.ML published:2014-01-14 summary:Learning the right graph representation from noisy, multisource data hasgarnered significant interest in recent years. A central tenet of this problemis relational learning. Here the objective is to incorporate the partialinformation each data source gives us in a way that captures the trueunderlying relationships. To address this challenge, we present a general,boosting-inspired framework for combining weak evidence of entity associationsinto a robust similarity metric. We explore the extent to which differentquality measurements yield graph representations that are suitable forcommunity detection. We then present empirical results on both synthetic andreal datasets demonstrating the utility of this framework. Our framework leadsto suitable global graph representations from quality measurements local toeach edge. Finally, we discuss future extensions and theoretical considerationsof learning useful graph representations from weak feedback in generalapplication settings.
arxiv-5700-127 | Survey On The Estimation Of Mutual Information Methods as a Measure of Dependency Versus Correlation Analysis | http://arxiv.org/pdf/1401.3358v1.pdf | author:D. Gencaga, N. K. Malakar, D. J. Lary category:stat.ML 94A17 published:2014-01-14 summary:In this survey, we present and compare different approaches to estimateMutual Information (MI) from data to analyse general dependencies betweenvariables of interest in a system. We demonstrate the performance difference ofMI versus correlation analysis, which is only optimal in case of lineardependencies. First, we use a piece-wise constant Bayesian methodology using ageneral Dirichlet prior. In this estimation method, we use a two-stage approachwhere we approximate the probability distribution first and then calculate themarginal and joint entropies. Here, we demonstrate the performance of thisBayesian approach versus the others for computing the dependency betweendifferent variables. We also compare these with linear correlation analysis.Finally, we apply MI and correlation analysis to the identification of the biasin the determination of the aerosol optical depth (AOD) by the satellite basedModerate Resolution Imaging Spectroradiometer (MODIS) and the ground basedAErosol RObotic NETwork (AERONET). Here, we observe that the AOD measurementsby these two instruments might be different for the same location. The reasonof this bias is explored by quantifying the dependencies between the bias and15 other variables including cloud cover, surface reflectivity and others.
arxiv-5700-128 | Use Case Point Approach Based Software Effort Estimation using Various Support Vector Regression Kernel Methods | http://arxiv.org/pdf/1401.3069v2.pdf | author:Shashank Mouli Satapathy, Santanu Kumar Rath category:cs.SE cs.LG published:2014-01-14 summary:The job of software effort estimation is a critical one in the early stagesof the software development life cycle when the details of requirements areusually not clearly identified. Various optimization techniques help inimproving the accuracy of effort estimation. The Support Vector Regression(SVR) is one of several different soft-computing techniques that help ingetting optimal estimated values. The idea of SVR is based upon the computationof a linear regression function in a high dimensional feature space where theinput data are mapped via a nonlinear function. Further, the SVR kernel methodscan be applied in transforming the input data and then based on thesetransformations, an optimal boundary between the possible outputs can beobtained. The main objective of the research work carried out in this paper isto estimate the software effort using use case point approach. The use casepoint approach relies on the use case diagram to estimate the size and effortof software projects. Then, an attempt has been made to optimize the resultsobtained from use case point analysis using various SVR kernel methods toachieve better prediction accuracy.
arxiv-5700-129 | Learning Language from a Large (Unannotated) Corpus | http://arxiv.org/pdf/1401.3372v1.pdf | author:Linas Vepstas, Ben Goertzel category:cs.CL cs.LG published:2014-01-14 summary:A novel approach to the fully automated, unsupervised extraction ofdependency grammars and associated syntax-to-semantic-relationship mappingsfrom large text corpora is described. The suggested approach builds on theauthors' prior work with the Link Grammar, RelEx and OpenCog systems, as wellas on a number of prior papers and approaches from the statistical languagelearning literature. If successful, this approach would enable the mining ofall the information needed to power a natural language comprehension andgeneration system, directly from a large, unannotated corpus.
arxiv-5700-130 | A programme to determine the exact interior of any connected digital picture | http://arxiv.org/pdf/1401.3385v1.pdf | author:Antonio Elias Fabris, Valério Ramos Batista category:cs.CG cs.CV cs.GR published:2014-01-14 summary:Region filling is one of the most important and fundamental operations incomputer graphics and image processing. Many filling algorithms and theirimplementations are based on the Euclidean geometry, which are then translatedinto computational models moving carelessly from the continuous to the finitediscrete space of the computer. The consequences of this approach is that mostimplementations fail when tested for challenging degenerate and nearlydegenerate regions. We present a correct integer-only procedure that works forall connected digital pictures. It finds all possible interior points, whichare then displayed and stored in a locating matrix. Namely, we present afilling and locating procedure that can be used in computer graphics and imageprocessing applications.
arxiv-5700-131 | Binary Classifier Calibration: Non-parametric approach | http://arxiv.org/pdf/1401.3390v1.pdf | author:Mahdi Pakdaman Naeini, Gregory F. Cooper, Milos Hauskrecht category:stat.ML cs.LG published:2014-01-14 summary:Accurate calibration of probabilistic predictive models learned is criticalfor many practical prediction and decision-making tasks. There are two maincategories of methods for building calibrated classifiers. One approach is todevelop methods for learning probabilistic models that are well-calibrated, abinitio. The other approach is to use some post-processing methods fortransforming the output of a classifier to be well calibrated, as for examplehistogram binning, Platt scaling, and isotonic regression. One advantage of thepost-processing approach is that it can be applied to any existingprobabilistic classification model that was constructed using anymachine-learning method. In this paper, we first introduce two measures for evaluating how well aclassifier is calibrated. We prove three theorems showing that using a simplehistogram binning post-processing method, it is possible to make a classifierbe well calibrated while retaining its discrimination capability. Also, bycasting the histogram binning method as a density-based non-parametric binaryclassifier, we can extend it using two simple non-parametric density estimationmethods. We demonstrate the performance of the proposed calibration methods onsynthetic and real datasets. Experimental results show that the proposedmethods either outperform or are comparable to existing calibration methods.
arxiv-5700-132 | Detection of Anomalous Crowd Behavior Using Spatio-Temporal Multiresolution Model and Kronecker Sum Decompositions | http://arxiv.org/pdf/1401.3291v2.pdf | author:Kristjan Greenewald, Alfred Hero category:stat.ML published:2014-01-14 summary:In this work we consider the problem of detecting anomalous spatio-temporalbehavior in videos. Our approach is to learn the normative multiframe pixeljoint distribution and detect deviations from it using a likelihood basedapproach. Due to the extreme lack of available training samples relative to thedimension of the distribution, we use a mean and covariance approach andconsider methods of learning the spatio-temporal covariance in the low-sampleregime. Our approach is to estimate the covariance using parameter reductionand sparse models. The first method considered is the representation of thecovariance as a sum of Kronecker products as in (Greenewald et al 2013), whichis found to be an accurate approximation in this setting. We propose learningalgorithms relevant to our problem. We then consider the sparse multiresolutionmodel of (Choi et al 2010) and apply the Kronecker product methods to it forfurther parameter reduction, as well as introducing modifications for enhancedefficiency and greater applicability to spatio-temporal covariance matrices. Weapply our methods to the detection of crowd behavior anomalies in theUniversity of Minnesota crowd anomaly dataset, and achieve competitive results.
arxiv-5700-133 | Stochastic Optimization with Importance Sampling | http://arxiv.org/pdf/1401.2753v2.pdf | author:Peilin Zhao, Tong Zhang category:stat.ML cs.LG published:2014-01-13 summary:Uniform sampling of training data has been commonly used in traditionalstochastic optimization algorithms such as Proximal Stochastic Gradient Descent(prox-SGD) and Proximal Stochastic Dual Coordinate Ascent (prox-SDCA). Althoughuniform sampling can guarantee that the sampled stochastic quantity is anunbiased estimate of the corresponding true quantity, the resulting estimatormay have a rather high variance, which negatively affects the convergence ofthe underlying optimization procedure. In this paper we study stochasticoptimization with importance sampling, which improves the convergence rate byreducing the stochastic variance. Specifically, we study prox-SGD (actually,stochastic mirror descent) with importance sampling and prox-SDCA withimportance sampling. For prox-SGD, instead of adopting uniform samplingthroughout the training process, the proposed algorithm employs importancesampling to minimize the variance of the stochastic gradient. For prox-SDCA,the proposed importance sampling scheme aims to achieve higher expected dualvalue at each dual coordinate ascent step. We provide extensive theoreticalanalysis to show that the convergence rates with the proposed importancesampling methods can be significantly improved under suitable conditions bothfor prox-SGD and for prox-SDCA. Experiments are provided to verify thetheoretical analysis.
arxiv-5700-134 | Tensor Representation and Manifold Learning Methods for Remote Sensing Images | http://arxiv.org/pdf/1401.2871v1.pdf | author:Lefei Zhang category:cs.CV 68 published:2014-01-13 summary:One of the main purposes of earth observation is to extract interestedinformation and knowledge from remote sensing (RS) images with high efficiencyand accuracy. However, with the development of RS technologies, RS systemprovide images with higher spatial and temporal resolution and more spectralchannels than before, and it is inefficient and almost impossible to manuallyinterpret these images. Thus, it is of great interests to explore automatic andintelligent algorithms to quickly process such massive RS data with highaccuracy. This thesis targets to develop some efficient information extractionalgorithms for RS images, by relying on the advanced technologies in machinelearning. More precisely, we adopt the manifold learning algorithms as themainline and unify the regularization theory, tensor-based method, sparselearning and transfer learning into the same framework. The main contributionsof this thesis are as follows.
arxiv-5700-135 | A variational Bayes framework for sparse adaptive estimation | http://arxiv.org/pdf/1401.2771v1.pdf | author:Konstantinos E. Themelis, Athanasios A. Rontogiannis, Konstantinos D. Koutroumbas category:stat.ML published:2014-01-13 summary:Recently, a number of mostly $\ell_1$-norm regularized least squares typedeterministic algorithms have been proposed to address the problem of\emph{sparse} adaptive signal estimation and system identification. From aBayesian perspective, this task is equivalent to maximum a posterioriprobability estimation under a sparsity promoting heavy-tailed prior for theparameters of interest. Following a different approach, this paper develops aunifying framework of sparse \emph{variational Bayes} algorithms that employheavy-tailed priors in conjugate hierarchical form to facilitate posteriorinference. The resulting fully automated variational schemes are firstpresented in a batch iterative form. Then it is shown that by properlyexploiting the structure of the batch estimation task, new sparse adaptivevariational Bayes algorithms can be derived, which have the ability to imposeand track sparsity during real-time processing in a time-varying environment.The most important feature of the proposed algorithms is that they completelyeliminate the need for computationally costly parameter fine-tuning, anecessary ingredient of sparse adaptive deterministic algorithms. Extensivesimulation results are provided to demonstrate the effectiveness of the newsparse variational Bayes algorithms against state-of-the-art deterministictechniques for adaptive channel estimation. The results show that the proposedalgorithms are numerically robust and exhibit in general superior estimationperformance compared to their deterministic counterparts.
arxiv-5700-136 | PSMACA: An Automated Protein Structure Prediction Using MACA (Multiple Attractor Cellular Automata) | http://arxiv.org/pdf/1401.2688v1.pdf | author:Pokkuluri Kiran Sree, Inamupudi Ramesh Babu, SSSN Usha Devi N category:cs.CE cs.LG published:2014-01-13 summary:Protein Structure Predication from sequences of amino acid has gained aremarkable attention in recent years. Even though there are some predictiontechniques addressing this problem, the approximate accuracy in predicting theprotein structure is closely 75%. An automated procedure was evolved with MACA(Multiple Attractor Cellular Automata) for predicting the structure of theprotein. Most of the existing approaches are sequential which will classify theinput into four major classes and these are designed for similar sequences.PSMACA is designed to identify ten classes from the sequences that sharetwilight zone similarity and identity with the training sequences. This methodalso predicts three states (helix, strand, and coil) for the structure. Ourcomprehensive design considers 10 feature selection methods and 4 classifiersto develop MACA (Multiple Attractor Cellular Automata) based classifiers thatare build for each of the ten classes. We have tested the proposed classifierwith twilight-zone and 1-high-similarity benchmark datasets with over threedozens of modern competing predictors shows that PSMACA provides the bestoverall accuracy that ranges between 77% and 88.7% depending on the dataset.
arxiv-5700-137 | A parameterless scale-space approach to find meaningful modes in histograms - Application to image and spectrum segmentation | http://arxiv.org/pdf/1401.2686v1.pdf | author:Jérôme Gilles, Kathryn Heal category:cs.CV published:2014-01-13 summary:In this paper, we present an algorithm to automatically detect meaningfulmodes in a histogram. The proposed method is based on the behavior of localminima in a scale-space representation. We show that the detection of suchmeaningful modes is equivalent in a two classes clustering problem on thelength of minima scale-space curves. The algorithm is easy to implement, fast,and does not require any parameters. We present several results on histogramand spectrum segmentation, grayscale image segmentation and color imagereduction.
arxiv-5700-138 | GPS-ABC: Gaussian Process Surrogate Approximate Bayesian Computation | http://arxiv.org/pdf/1401.2838v1.pdf | author:Edward Meeds, Max Welling category:cs.LG q-bio.QM stat.ML published:2014-01-13 summary:Scientists often express their understanding of the world through acomputationally demanding simulation program. Analyzing the posteriordistribution of the parameters given observations (the inverse problem) can beextremely challenging. The Approximate Bayesian Computation (ABC) framework isthe standard statistical tool to handle these likelihood free problems, butthey require a very large number of simulations. In this work we develop twonew ABC sampling algorithms that significantly reduce the number of simulationsnecessary for posterior inference. Both algorithms use confidence estimates forthe accept probability in the Metropolis Hastings step to adaptively choose thenumber of necessary simulations. Our GPS-ABC algorithm stores the informationobtained from every simulation in a Gaussian process which acts as a surrogatefunction for the simulated statistics. Experiments on a challenging realisticbiological problem illustrate the potential of these algorithms.
arxiv-5700-139 | Binary Classifier Calibration: Bayesian Non-Parametric Approach | http://arxiv.org/pdf/1401.2955v1.pdf | author:Mahdi Pakdaman Naeini, Gregory F. Cooper, Milos Hauskrecht category:stat.ML cs.LG published:2014-01-13 summary:A set of probabilistic predictions is well calibrated if the events that arepredicted to occur with probability p do in fact occur about p fraction of thetime. Well calibrated predictions are particularly important when machinelearning models are used in decision analysis. This paper presents two newnon-parametric methods for calibrating outputs of binary classification models:a method based on the Bayes optimal selection and a method based on theBayesian model averaging. The advantage of these methods is that they areindependent of the algorithm used to learn a predictive model, and they can beapplied in a post-processing step, after the model is learned. This makes themapplicable to a wide variety of machine learning models and methods. Thesecalibration methods, as well as other methods, are tested on a variety ofdatasets in terms of both discrimination and calibration performance. Theresults show the methods either outperform or are comparable in performance tothe state-of-the-art calibration methods.
arxiv-5700-140 | ONTS: "Optima" News Translation System | http://arxiv.org/pdf/1401.2943v1.pdf | author:Marco Turchi, Martin Atkinson, Alastair Wilcox, Brett Crawley, Stefano Bucci, Ralf Steinberger, Erik Van der Goot category:cs.CL published:2014-01-13 summary:We propose a real-time machine translation system that allows users to selecta news category and to translate the related live news articles from Arabic,Czech, Danish, Farsi, French, German, Italian, Polish, Portuguese, Spanish andTurkish into English. The Moses-based system was optimised for the news domainand differs from other available systems in four ways: (1) News items areautomatically categorised on the source side, before translation; (2) Namedentity translation is optimised by recognising and extracting them on thesource side and by re-inserting their translation in the target language,making use of a separate entity repository; (3) News titles are translated witha separate translation system which is optimised for the specific style of newstitles; (4) The system was optimised for speed in order to cope with the largevolume of daily news articles.
arxiv-5700-141 | Multilinear Wavelets: A Statistical Shape Space for Human Faces | http://arxiv.org/pdf/1401.2818v2.pdf | author:Alan Brunton, Timo Bolkart, Stefanie Wuhrer category:cs.CV cs.GR published:2014-01-13 summary:We present a statistical model for $3$D human faces in varying expression,which decomposes the surface of the face using a wavelet transform, and learnsmany localized, decorrelated multilinear models on the resulting coefficients.Using this model we are able to reconstruct faces from noisy and occluded $3$Dface scans, and facial motion sequences. Accurate reconstruction of face shapeis important for applications such as tele-presence and gaming. The localizedand multi-scale nature of our model allows for recovery of fine-scale detailwhile retaining robustness to severe noise and occlusion, and iscomputationally efficient and scalable. We validate these propertiesexperimentally on challenging data in the form of static scans and motionsequences. We show that in comparison to a global multilinear model, our modelbetter preserves fine detail and is computationally faster, while in comparisonto a localized PCA model, our model better handles variation in expression, isfaster, and allows us to fix identity parameters for a given subject.
arxiv-5700-142 | Insights into analysis operator learning: From patch-based sparse models to higher-order MRFs | http://arxiv.org/pdf/1401.2804v1.pdf | author:Yunjin Chen, René Ranftl, Thomas Pock category:cs.CV published:2014-01-13 summary:This paper addresses a new learning algorithm for the recently introducedco-sparse analysis model. First, we give new insights into the co-sparseanalysis model by establishing connections to filter-based MRF models, such asthe Field of Experts (FoE) model of Roth and Black. For training, we introducea technique called bi-level optimization to learn the analysis operators.Compared to existing analysis operator learning approaches, our trainingprocedure has the advantage that it is unconstrained with respect to theanalysis operator. We investigate the effect of different aspects of theco-sparse analysis model and show that the sparsity promoting function (alsocalled penalty function) is the most important factor in the model. In order todemonstrate the effectiveness of our training approach, we apply our trainedmodels to various classical image restoration problems. Numerical experimentsshow that our trained models clearly outperform existing analysis operatorlearning approaches and are on par with state-of-the-art image denoisingalgorithms. Our approach develops a framework that is intuitive to understandand easy to implement.
arxiv-5700-143 | A survey of methods to ease the development of highly multilingual text mining applications | http://arxiv.org/pdf/1401.2937v1.pdf | author:Ralf Steinberger category:cs.CL published:2014-01-13 summary:Multilingual text processing is useful because the information content foundin different languages is complementary, both regarding facts and opinions.While Information Extraction and other text mining software can, in principle,be developed for many languages, most text analysis tools have only beenapplied to small sets of languages because the development effort per languageis large. Self-training tools obviously alleviate the problem, but even theeffort of providing training data and of manually tuning the results is usuallyconsiderable. In this paper, we gather insights by various multilingual systemdevelopers on how to minimise the effort of developing natural languageprocessing applications for many languages. We also explain the main guidelinesunderlying our own effort to develop complex text mining software for tens oflanguages. While these guidelines - most of all: extreme simplicity - can bevery restrictive and limiting, we believe to have shown the feasibility of theapproach through the development of the Europe Media Monitor (EMM) family ofapplications (http://emm.newsbrief.eu/overview.html). EMM is a set of complexmedia monitoring tools that process and analyse up to 100,000 online newsarticles per day in between twenty and fifty languages. We will also touch uponthe kind of language resources that would make it easier for all to develophighly multilingual text mining applications. We will argue that - to achievethis - the most needed resources would be freely available, simple, paralleland uniform multilingual dictionaries, corpora and software tools.
arxiv-5700-144 | Sentiment Analysis Using Collaborated Opinion Mining | http://arxiv.org/pdf/1401.2618v1.pdf | author:Deepali Virmani, Vikrant Malhotra, Ridhi Tyagi category:cs.IR cs.CL published:2014-01-12 summary:Opinion mining and Sentiment analysis have emerged as a field of study sincethe widespread of World Wide Web and internet. Opinion refers to extraction ofthose lines or phrase in the raw and huge data which express an opinion.Sentiment analysis on the other hand identifies the polarity of the opinionbeing extracted. In this paper we propose the sentiment analysis incollaboration with opinion extraction, summarization, and tracking the recordsof the students. The paper modifies the existing algorithm in order to obtainthe collaborated opinion about the students. The resultant opinion isrepresented as very high, high, moderate, low and very low. The paper is basedon a case study where teachers give their remarks about the students and byapplying the proposed sentiment analysis algorithm the opinion is extracted andrepresented.
arxiv-5700-145 | MRFalign: Protein Homology Detection through Alignment of Markov Random Fields | http://arxiv.org/pdf/1401.2668v2.pdf | author:Jianzhu Ma, Sheng Wang, Zhiyong Wang, Jinbo Xu category:q-bio.QM cs.CE cs.LG published:2014-01-12 summary:Sequence-based protein homology detection has been extensively studied and sofar the most sensitive method is based upon comparison of protein sequenceprofiles, which are derived from multiple sequence alignment (MSA) of sequencehomologs in a protein family. A sequence profile is usually represented as aposition-specific scoring matrix (PSSM) or an HMM (Hidden Markov Model) andaccordingly PSSM-PSSM or HMM-HMM comparison is used for homolog detection. Thispaper presents a new homology detection method MRFalign, consisting of threekey components: 1) a Markov Random Fields (MRF) representation of a proteinfamily; 2) a scoring function measuring similarity of two MRFs; and 3) anefficient ADMM (Alternating Direction Method of Multipliers) algorithm aligningtwo MRFs. Compared to HMM that can only model very short-range residuecorrelation, MRFs can model long-range residue interaction pattern and thus,encode information for the global 3D structure of a protein family.Consequently, MRF-MRF comparison for remote homology detection shall be muchmore sensitive than HMM-HMM or PSSM-PSSM comparison. Experiments confirm thatMRFalign outperforms several popular HMM or PSSM-based methods in terms of bothalignment accuracy and remote homology detection and that MRFalign worksparticularly well for mainly beta proteins. For example, tested on thebenchmark SCOP40 (8353 proteins) for homology detection, PSSM-PSSM and HMM-HMMsucceed on 48% and 52% of proteins, respectively, at superfamily level, and on15% and 27% of proteins, respectively, at fold level. In contrast, MRFalignsucceeds on 57.3% and 42.5% of proteins at superfamily and fold level,respectively. This study implies that long-range residue interaction patternsare very helpful for sequence-based homology detection. The software isavailable for download at http://raptorx.uchicago.edu/download/.
arxiv-5700-146 | Towards a Generic Framework for the Development of Unicode Based Digital Sindhi Dictionaries | http://arxiv.org/pdf/1401.2641v1.pdf | author:Imdad Ali Ismaili, Zeeshan Bhatti, Azhar Ali Shah category:cs.CL published:2014-01-12 summary:Dictionaries are essence of any language providing vital linguistic recoursefor the language learners, researchers and scholars. This paper focuses on themethodology and techniques used in developing software architecture for aUBSESD (Unicode Based Sindhi to English and English to Sindhi Dictionary). Theproposed system provides an accurate solution for construction andrepresentation of Unicode based Sindhi characters in a dictionary implementingHash Structure algorithm and a custom java Object as its internal datastructure saved in a file. The System provides facilities for Insertion,Deletion and Editing of new records of Sindhi. Through this framework any typeof Sindhi to English and English to Sindhi Dictionary (belonging to differentdomains of knowledge, e.g. engineering, medicine, computer, biology etc.) couldbe developed easily with accurate representation of Unicode Characters in fontindependent manner.
arxiv-5700-147 | An Overview of Schema Theory | http://arxiv.org/pdf/1401.2651v1.pdf | author:David White category:cs.NE published:2014-01-12 summary:The purpose of this paper is to give an introduction to the field of SchemaTheory written by a mathematician and for mathematicians. In particular, weendeavor to to highlight areas of the field which might be of interest to amathematician, to point out some related open problems, and to suggest somelarge-scale projects. Schema theory seeks to give a theoretical justificationfor the efficacy of the field of genetic algorithms, so readers who havestudied genetic algorithms stand to gain the most from this paper. However,nothing beyond basic probability theory is assumed of the reader, and for thisreason we write in a fairly informal style. Because the mathematics behind the theorems in schema theory is relativelyelementary, we focus more on the motivation and philosophy. Many of theseresults have been proven elsewhere, so this paper is designed to serve aprimarily expository role. We attempt to cast known results in a new light,which makes the suggested future directions natural. This involves devoting asubstantial amount of time to the history of the field. We hope that this exposition will entice some mathematicians to do researchin this area, that it will serve as a road map for researchers new to thefield, and that it will help explain how schema theory developed. Furthermore,we hope that the results collected in this document will serve as a usefulreference. Finally, as far as the author knows, the questions raised in thefinal section are new.
arxiv-5700-148 | Dictionary-Based Concept Mining: An Application for Turkish | http://arxiv.org/pdf/1401.2663v1.pdf | author:Cem Rıfkı Aydın, Ali Erkan, Tunga Güngör, Hidayet Takçı category:cs.CL I.2.7 published:2014-01-12 summary:In this study, a dictionary-based method is used to extract expressiveconcepts from documents. So far, there have been many studies concerningconcept mining in English, but this area of study for Turkish, an agglutinativelanguage, is still immature. We used dictionary instead of WordNet, a lexicaldatabase grouping words into synsets that is widely used for conceptextraction. The dictionaries are rarely used in the domain of concept mining,but taking into account that dictionary entries have synonyms, hypernyms,hyponyms and other relationships in their meaning texts, the success rate hasbeen high for determining concepts. This concept extraction method isimplemented on documents, that are collected from different corpora.
arxiv-5700-149 | Inference in High Dimensions with the Penalized Score Test | http://arxiv.org/pdf/1401.2678v3.pdf | author:Arend Voorman, Ali Shojaie, Daniela Witten category:stat.ME stat.ML published:2014-01-12 summary:In recent years, there has been considerable theoretical developmentregarding variable selection consistency of penalized regression techniques,such as the lasso. However, there has been relatively little work onquantifying the uncertainty in these selection procedures. In this paper, wepropose a new method for inference in high dimensions using a score test basedon penalized regression. In this test, we perform penalized regression of anoutcome on all but a single feature, and test for correlation of the residualswith the held-out feature. This procedure is applied to each feature in turn.Interestingly, when an $\ell_1$ penalty is used, the sparsity pattern of thelasso corresponds exactly to a decision based on the proposed test. Further,when an $\ell_2$ penalty is used, the test corresponds precisely to a scoretest in a mixed effects model, in which the effects of all but one feature areassumed to be random. We formulate the hypothesis being tested as a compromisebetween the null hypotheses tested in simple linear regression on each featureand in multiple linear regression on all features, and develop referencedistributions for some well-known penalties. We also examine the behavior ofthe test on real and simulated data.
arxiv-5700-150 | The semantic similarity ensemble | http://arxiv.org/pdf/1401.2517v1.pdf | author:Andrea Ballatore, Michela Bertolotto, David C. Wilson category:cs.CL published:2014-01-11 summary:Computational measures of semantic similarity between geographic termsprovide valuable support across geographic information retrieval, data mining,and information integration. To date, a wide variety of approaches togeo-semantic similarity have been devised. A judgment of similarity is notintrinsically right or wrong, but obtains a certain degree of cognitiveplausibility, depending on how closely it mimics human behavior. Thus selectingthe most appropriate measure for a specific task is a significant challenge. Toaddress this issue, we make an analogy between computational similaritymeasures and soliciting domain expert opinions, which incorporate a subjectiveset of beliefs, perceptions, hypotheses, and epistemic biases. Following thisanalogy, we define the semantic similarity ensemble (SSE) as a composition ofdifferent similarity measures, acting as a panel of experts having to reach adecision on the semantic similarity of a set of geographic terms. The approachis evaluated in comparison to human judgments, and results indicate that an SSEperforms better than the average of its parts. Although the best member tendsto outperform the ensemble, all ensembles outperform the average performance ofeach ensemble's member. Hence, in contexts where the best measure is unknown,the ensemble provides a more cognitively plausible approach.
arxiv-5700-151 | Multiscale Shrinkage and Lévy Processes | http://arxiv.org/pdf/1401.2497v1.pdf | author:Xin Yuan, Vinayak Rao, Shaobo Han, Lawrence Carin category:stat.ML published:2014-01-11 summary:A new shrinkage-based construction is developed for a compressible vector$\boldsymbol{x}\in\mathbb{R}^n$, for cases in which the components of $\xv$ arenaturally associated with a tree structure. Important examples are when $\xv$corresponds to the coefficients of a wavelet or block-DCT representation ofdata. The method we consider in detail, and for which numerical results arepresented, is based on increments of a gamma process. However, we demonstratethat the general framework is appropriate for many other types of shrinkagepriors, all within the L\'{e}vy process family, with the gamma process aspecial case. Bayesian inference is carried out by approximating the posteriorwith samples from an MCMC algorithm, as well as by constructing a heuristicvariational approximation to the posterior. We also considerexpectation-maximization (EM) for a MAP (point) solution. State-of-the-artresults are manifested for compressive sensing and denoising applications, thelatter with spiky (non-Gaussian) noise.
arxiv-5700-152 | An Online Expectation-Maximisation Algorithm for Nonnegative Matrix Factorisation Models | http://arxiv.org/pdf/1401.2490v1.pdf | author:Sinan Yildirim, A. Taylan Cemgil, Sumeetpal S. Singh category:cs.LG stat.CO stat.ML published:2014-01-11 summary:In this paper we formulate the nonnegative matrix factorisation (NMF) problemas a maximum likelihood estimation problem for hidden Markov models and proposeonline expectation-maximisation (EM) algorithms to estimate the NMF and theother unknown static parameters. We also propose a sequential Monte Carloapproximation of our online EM algorithm. We show the performance of theproposed method with two numerical examples.
arxiv-5700-153 | Multi Terminal Probabilistic Compressed Sensing | http://arxiv.org/pdf/1401.2569v1.pdf | author:Saeid Haghighatshoar category:cs.IT math.IT stat.ML published:2014-01-11 summary:In this paper, the `Approximate Message Passing' (AMP) algorithm, initiallydeveloped for compressed sensing of signals under i.i.d. Gaussian measurementmatrices, has been extended to a multi-terminal setting (MAMP algorithm). Ithas been shown that similar to its single terminal counterpart, the behavior ofMAMP algorithm is fully characterized by a `State Evolution' (SE) equation forlarge block-lengths. This equation has been used to obtain the rate-distortioncurve of a multi-terminal memoryless source. It is observed that by spatiallycoupling the measurement matrices, the rate-distortion curve of MAMP algorithmundergoes a phase transition, where the measurement rate region correspondingto a low distortion (approximately zero distortion) regime is fullycharacterized by the joint and conditional Renyi information dimension (RID) ofthe multi-terminal source. This measurement rate region is very similar to therate region of the Slepian-Wolf distributed source coding problem where the RIDplays a role similar to the discrete entropy. Simulations have been done to investigate the empirical behavior of MAMPalgorithm. It is observed that simulation results match very well withpredictions of SE equation for reasonably large block-lengths.
arxiv-5700-154 | Multi-Step-Ahead Time Series Prediction using Multiple-Output Support Vector Regression | http://arxiv.org/pdf/1401.2504v1.pdf | author:Yukun Bao, Tao Xiong, Zhongyi Hu category:cs.LG stat.ML published:2014-01-11 summary:Accurate time series prediction over long future horizons is challenging andof great interest to both practitioners and academics. As a well-knownintelligent algorithm, the standard formulation of Support Vector Regression(SVR) could be taken for multi-step-ahead time series prediction, only relyingeither on iterated strategy or direct strategy. This study proposes a novelmultiple-step-ahead time series prediction approach which employsmultiple-output support vector regression (M-SVR) with multiple-inputmultiple-output (MIMO) prediction strategy. In addition, the rank of threeleading prediction strategies with SVR is comparatively examined, providingpractical implications on the selection of the prediction strategy formulti-step-ahead forecasting while taking SVR as modeling technique. Theproposed approach is validated with the simulated and real datasets. Thequantitative and comprehensive assessments are performed on the basis of theprediction accuracy and computational cost. The results indicate that: 1) theM-SVR using MIMO strategy achieves the best accurate forecasts with accreditedcomputational load, 2) the standard SVR using direct strategy achieves thesecond best accurate forecasts, but with the most expensive computational cost,and 3) the standard SVR using iterated strategy is the worst in terms ofprediction accuracy, but with the least computational cost.
arxiv-5700-155 | A Study of Image Analysis with Tangent Distance | http://arxiv.org/pdf/1401.2529v3.pdf | author:Elif Vural, Pascal Frossard category:cs.CV published:2014-01-11 summary:The computation of the geometric transformation between a reference and atarget image, known as registration or alignment, corresponds to the projectionof the target image onto the transformation manifold of the reference image(the set of images generated by its geometric transformations). It, however,often takes a nontrivial form such that the exact computation of projections onthe manifold is difficult. The tangent distance method is an effectivealgorithm to solve this problem by exploiting a linear approximation of themanifold. As theoretical studies about the tangent distance algorithm have beenlargely overlooked, we present in this work a detailed performance analysis ofthis useful algorithm, which can eventually help its implementation. Weconsider a popular image registration setting using a multiscale pyramid oflowpass filtered versions of the (possibly noisy) reference and target images,which is particularly useful for recovering large transformations. We firstshow that the alignment error has a nonmonotonic variation with the filtersize, due to the opposing effects of filtering on both manifold nonlinearityand image noise. We then study the convergence of the multiscale tangentdistance method to the optimal solution. We finally examine the performance ofthe tangent distance method in image classification applications. Ourtheoretical findings are confirmed by experiments on image transformationmodels involving translations, rotations and scalings. Our study is the firstdetailed study of the tangent distance algorithm that leads to a betterunderstanding of its efficacy and to the proper selection of its designparameters.
arxiv-5700-156 | Extension of Sparse Randomized Kaczmarz Algorithm for Multiple Measurement Vectors | http://arxiv.org/pdf/1401.2288v3.pdf | author:Hemant Kumar Aggarwal, Angshul Majumdar category:cs.NA cs.LG stat.ML published:2014-01-10 summary:The Kaczmarz algorithm is popular for iteratively solving an overdeterminedsystem of linear equations. The traditional Kaczmarz algorithm can approximatethe solution in few sweeps through the equations but a randomized version ofthe Kaczmarz algorithm was shown to converge exponentially and independent ofnumber of equations. Recently an algorithm for finding sparse solution to alinear system of equations has been proposed based on weighted randomizedKaczmarz algorithm. These algorithms solves single measurement vector problem;however there are applications were multiple-measurements are available. Inthis work, the objective is to solve a multiple measurement vector problem withcommon sparse support by modifying the randomized Kaczmarz algorithm. We havealso modeled the problem of face recognition from video as the multiplemeasurement vector problem and solved using our proposed technique. We havecompared the proposed algorithm with state-of-art spectral projected gradientalgorithm for multiple measurement vectors on both real and synthetic datasets.The Monte Carlo simulations confirms that our proposed algorithm have betterrecovery and convergence rate than the MMV version of spectral projectedgradient algorithm under fairness constraints.
arxiv-5700-157 | Satellite image classification and segmentation using non-additive entropy | http://arxiv.org/pdf/1401.2416v1.pdf | author:Lucas Assirati, Alexandre Souto Martinez, Odemir Martinez Bruno category:cs.CV published:2014-01-10 summary:Here we compare the Boltzmann-Gibbs-Shannon (standard) with the Tsallisentropy on the pattern recognition and segmentation of coloured images obtainedby satellites, via "Google Earth". By segmentation we mean split an image tolocate regions of interest. Here, we discriminate and define an image partitionclasses according to a training basis. This training basis consists of threepattern classes: aquatic, urban and vegetation regions. Our numericalexperiments demonstrate that the Tsallis entropy, used as a feature vectorcomposed of distinct entropic indexes $q$ outperforms the standard entropy.There are several applications of our proposed methodology, once satelliteimages can be used to monitor migration form rural to urban regions,agricultural activities, oil spreading on the ocean etc.
arxiv-5700-158 | N2Sky - Neural Networks as Services in the Clouds | http://arxiv.org/pdf/1401.2468v1.pdf | author:Erich Schikuta, Erwin Mann category:cs.NE H.3.5; I.2 published:2014-01-10 summary:We present the N2Sky system, which provides a framework for the exchange ofneural network specific knowledge, as neural network paradigms and objects, bya virtual organization environment. It follows the sky computing paradigmdelivering ample resources by the usage of federated Clouds. N2Sky is a novelCloud-based neural network simulation environment, which follows a pure serviceoriented approach. The system implements a transparent environment aiming toenable both novice and experienced users to do neural network research easilyand comfortably. N2Sky is built using the RAVO reference architecture ofvirtual organizations which allows itself naturally integrating into the Cloudservice stack (SaaS, PaaS, and IaaS) of service oriented architectures.
arxiv-5700-159 | Clustering, Coding, and the Concept of Similarity | http://arxiv.org/pdf/1401.2411v1.pdf | author:L. Thorne McCarty category:cs.LG published:2014-01-10 summary:This paper develops a theory of clustering and coding which combines ageometric model with a probabilistic model in a principled way. The geometricmodel is a Riemannian manifold with a Riemannian metric, ${g}_{ij}({\bf x})$,which we interpret as a measure of dissimilarity. The probabilistic modelconsists of a stochastic process with an invariant probability measure whichmatches the density of the sample input data. The link between the two modelsis a potential function, $U({\bf x})$, and its gradient, $\nabla U({\bf x})$.We use the gradient to define the dissimilarity metric, which guarantees thatour measure of dissimilarity will depend on the probability measure. Finally,we use the dissimilarity metric to define a coordinate system on the embeddedRiemannian manifold, which gives us a low-dimensional encoding of our originaldata.
arxiv-5700-160 | Exploiting generalisation symmetries in accuracy-based learning classifier systems: An initial study | http://arxiv.org/pdf/1401.2949v1.pdf | author:Larry Bull category:cs.NE cs.LG published:2014-01-10 summary:Modern learning classifier systems typically exploit a niched geneticalgorithm to facilitate rule discovery. When used for reinforcement learning,such rules represent generalisations over the state-action-reward space. Whilstencouraging maximal generality, the niching can potentially hinder theformation of generalisations in the state space which are symmetrical, or verysimilar, over different actions. This paper introduces the use of rules whichcontain multiple actions, maintaining accuracy and reward metrics for eachaction. It is shown that problem symmetries can be exploited, improvingperformance, whilst not degrading performance when symmetries are reduced.
arxiv-5700-161 | Online Matrix Completion Through Nuclear Norm Regularisation | http://arxiv.org/pdf/1401.2451v1.pdf | author:Charanpal Dhanjal, Romaric Gaudel, Stéphan Clémençon category:stat.ML published:2014-01-10 summary:It is the main goal of this paper to propose a novel method to perform matrixcompletion on-line. Motivated by a wide variety of applications, ranging fromthe design of recommender systems to sensor network localization throughseismic data reconstruction, we consider the matrix completion problem whenentries of the matrix of interest are observed gradually. Precisely, we placeourselves in the situation where the predictive rule should be refinedincrementally, rather than recomputed from scratch each time the sample ofobserved entries increases. The extension of existing matrix completion methodsto the sequential prediction context is indeed a major issue in the Big Dataera, and yet little addressed in the literature. The algorithm promoted in thisarticle builds upon the Soft Impute approach introduced in Mazumder et al.(2010). The major novelty essentially arises from the use of a randomisedtechnique for both computing and updating the Singular Value Decomposition(SVD) involved in the algorithm. Though of disarming simplicity, the methodproposed turns out to be very efficient, while requiring reduced computations.Several numerical experiments based on real datasets illustrating itsperformance are displayed, together with preliminary results giving it atheoretical basis.
arxiv-5700-162 | Lasso and equivalent quadratic penalized models | http://arxiv.org/pdf/1401.2304v1.pdf | author:Stefan Hummelsheim category:stat.ML cs.LG published:2014-01-10 summary:The least absolute shrinkage and selection operator (lasso) and ridgeregression produce usually different estimates although input, loss functionand parameterization of the penalty are identical. In this paper we look forridge and lasso models with identical solution set. It turns out, that the lasso model with shrink vector $\lambda$ and aquadratic penalized model with shrink matrix as outer product of $\lambda$ withitself are equivalent, in the sense that they have equal solutions. To achievethis, we have to restrict the estimates to be positive. This doesn't limit thearea of application since we can easily decompose every estimate in a positiveand negative part. The resulting problem can be solved with a non negativeleast square algorithm. Beside this quadratic penalized model, an augmented regression model withpositive bounded estimates is developed which is also equivalent to the lassomodel, but is probably faster to solve.
arxiv-5700-163 | Assessing Wikipedia-Based Cross-Language Retrieval Models | http://arxiv.org/pdf/1401.2258v1.pdf | author:Benjamin Roth category:cs.IR cs.CL published:2014-01-10 summary:This work compares concept models for cross-language retrieval: First, weadapt probabilistic Latent Semantic Analysis (pLSA) for multilingual documents.Experiments with different weighting schemes show that a weighting methodfavoring documents of similar length in both language sides gives best results.Considering that both monolingual and multilingual Latent Dirichlet Allocation(LDA) behave alike when applied for such documents, we use a training corpusbuilt on Wikipedia where all documents are length-normalized and obtainimprovements over previously reported scores for LDA. Another focus of our workis on model combination. For this end we include Explicit Semantic Analysis(ESA) in the experiments. We observe that ESA is not competitive with LDA in aquery based retrieval task on CLEF 2000 data. The combination of machinetranslation with concept models increased performance by 21.1% map incomparison to machine translation alone. Machine translation relies on parallelcorpora, which may not be available for many language pairs. We further explorehow much cross-lingual information can be carried over by a specificinformation source in Wikipedia, namely linked text. The best results areobtained using a language modeling approach, entirely without information fromparallel corpora. The need for smoothing raises interesting questions onsoundness and efficiency. Link models capture only a certain kind ofinformation and suggest weighting schemes to emphasize particular words. For acombined model, another interesting question is therefore how to integratedifferent weighting schemes. Using a very simple combination scheme, we obtainresults that compare favorably to previously reported results on the CLEF 2000dataset.
arxiv-5700-164 | A Comparative Study of Reservoir Computing for Temporal Signal Processing | http://arxiv.org/pdf/1401.2224v1.pdf | author:Alireza Goudarzi, Peter Banda, Matthew R. Lakin, Christof Teuscher, Darko Stefanovic category:cs.NE cs.LG published:2014-01-10 summary:Reservoir computing (RC) is a novel approach to time series prediction usingrecurrent neural networks. In RC, an input signal perturbs the intrinsicdynamics of a medium called a reservoir. A readout layer is then trained toreconstruct a target output from the reservoir's state. The multitude of RCarchitectures and evaluation metrics poses a challenge to both practitionersand theorists who study the task-solving performance and computational power ofRC. In addition, in contrast to traditional computation models, the reservoiris a dynamical system in which computation and memory are inseparable, andtherefore hard to analyze. Here, we compare echo state networks (ESN), apopular RC architecture, with tapped-delay lines (DL) and nonlinearautoregressive exogenous (NARX) networks, which we use to model systems withlimited computation and limited memory respectively. We compare the performanceof the three systems while computing three common benchmark time series:H{\'e}non Map, NARMA10, and NARMA20. We find that the role of the reservoir inthe reservoir computing paradigm goes beyond providing a memory of the pastinputs. The DL and the NARX network have higher memorization capability, butfall short of the generalization power of the ESN.
arxiv-5700-165 | Distinguishing noise from chaos: objective versus subjective criteria using Horizontal Visibility Graph | http://arxiv.org/pdf/1401.2139v1.pdf | author:Martín Gómez Ravetti, Laura C. Carpi, Bruna Amin Gonçalves, Alejandro C. Frery, Osvaldo A. Rosso category:stat.ML cs.IT math.IT nlin.CD published:2014-01-09 summary:A recently proposed methodology called the Horizontal Visibility Graph (HVG)[Luque {\it et al.}, Phys. Rev. E., 80, 046103 (2009)] that constitutes ageometrical simplification of the well known Visibility Graph algorithm [Lacasa{\it et al.\/}, Proc. Natl. Sci. U.S.A. 105, 4972 (2008)], has been used tostudy the distinction between deterministic and stochastic components in timeseries [L. Lacasa and R. Toral, Phys. Rev. E., 82, 036120 (2010)].Specifically, the authors propose that the node degree distribution of theseprocesses follows an exponential functional of the form $P(\kappa)\sim\exp(-\lambda~\kappa)$, in which $\kappa$ is the node degree and $\lambda$ is apositive parameter able to distinguish between deterministic (chaotic) andstochastic (uncorrelated and correlated) dynamics. In this work, we investigatethe characteristics of the node degree distributions constructed by using HVG,for time series corresponding to $28$ chaotic maps and $3$ different stochasticprocesses. We thoroughly study the methodology proposed by Lacasa and Toralfinding several cases for which their hypothesis is not valid. We propose amethodology that uses the HVG together with Information Theory quantifiers. Anextensive and careful analysis of the node degree distributions obtained byapplying HVG allow us to conclude that the Fisher-Shannon information plane isa remarkable tool able to graphically represent the different nature,deterministic or stochastic, of the systems under study.
arxiv-5700-166 | Statistical Analysis based Hypothesis Testing Method in Biological Knowledge Discovery | http://arxiv.org/pdf/1401.2851v1.pdf | author:Md. Naseef-Ur-Rahman Chowdhury, Suvankar Paul, Kazi Zakia Sultana category:cs.IR cs.CL published:2014-01-09 summary:The correlation and interactions among different biological entities comprisethe biological system. Although already revealed interactions contribute to theunderstanding of different existing systems, researchers face many questionseveryday regarding inter-relationships among entities. Their queries havepotential role in exploring new relations which may open up a new area ofinvestigation. In this paper, we introduce a text mining based method foranswering the biological queries in terms of statistical computation such thatresearchers can come up with new knowledge discovery. It facilitates user tosubmit their query in natural linguistic form which can be treated ashypothesis. Our proposed approach analyzes the hypothesis and measures thep-value of the hypothesis with respect to the existing literature. Based on themeasured value, the system either accepts or rejects the hypothesis fromstatistical point of view. Moreover, even it does not find any directrelationship among the entities of the hypothesis, it presents a network togive an integral overview of all the entities through which the entities mightbe related. This is also congenial for the researchers to widen their view andthus think of new hypothesis for further investigation. It assists researcherto get a quantitative evaluation of their assumptions such that they can reacha logical conclusion and thus aids in relevant re-searches of biologicalknowledge discovery. The system also provides the researchers a graphicalinteractive interface to submit their hypothesis for assessment in a moreconvenient way.
arxiv-5700-167 | DJ-MC: A Reinforcement-Learning Agent for Music Playlist Recommendation | http://arxiv.org/pdf/1401.1880v2.pdf | author:Elad Liebman, Maytal Saar-Tsechansky, Peter Stone category:cs.LG published:2014-01-09 summary:In recent years, there has been growing focus on the study of automatedrecommender systems. Music recommendation systems serve as a prominent domainfor such works, both from an academic and a commercial perspective. Afundamental aspect of music perception is that music is experienced in temporalcontext and in sequence. In this work we present DJ-MC, a novelreinforcement-learning framework for music recommendation that does notrecommend songs individually but rather song sequences, or playlists, based ona model of preferences for both songs and song transitions. The model islearned online and is uniquely adapted for each listener. To reduce explorationtime, DJ-MC exploits user feedback to initialize a model, which it subsequentlyupdates by reinforcement. We evaluate our framework with human participantsusing both real song and playlist data. Our results indicate that DJ-MC'sability to recommend sequences of songs provides a significant improvement overmore straightforward approaches, which do not take transitions into account.
arxiv-5700-168 | Gesture recognition based mouse events | http://arxiv.org/pdf/1401.2058v1.pdf | author:Rachit Puri category:cs.CV published:2014-01-09 summary:This paper presents the maneuver of mouse pointer and performs various mouseoperations such as left click, right click, double click, drag etc usinggestures recognition technique. Recognizing gestures is a complex task whichinvolves many aspects such as motion modeling, motion analysis, patternrecognition and machine learning. Keeping all the essential factors in mind asystem has been created which recognizes the movement of fingers and variouspatterns formed by them. Color caps have been used for fingers to distinguishit from the background color such as skin color. Thus recognizing the gesturesvarious mouse events have been performed. The application has been created onMATLAB environment with operating system as windows 7.
arxiv-5700-169 | Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts | http://arxiv.org/pdf/1401.1974v4.pdf | author:Vu Nguyen, Dinh Phung, XuanLong Nguyen, Svetha Venkatesh, Hung Hai Bui category:cs.LG stat.ML published:2014-01-09 summary:We present a Bayesian nonparametric framework for multilevel clustering whichutilizes group-level context information to simultaneously discoverlow-dimensional structures of the group contents and partitions groups intoclusters. Using the Dirichlet process as the building block, our modelconstructs a product base-measure with a nested structure to accommodatecontent and context observations at multiple levels. The proposed modelpossesses properties that link the nested Dirichlet processes (nDP) and theDirichlet process mixture models (DPM) in an interesting way: integrating outall contents results in the DPM over contexts, whereas integrating outgroup-specific contexts results in the nDP mixture over content variables. Weprovide a Polya-urn view of the model and an efficient collapsed Gibbsinference procedure. Extensive experiments on real-world datasets demonstratethe advantage of utilizing context information via our model in both text andimage domains.
arxiv-5700-170 | Enhancement performance of road recognition system of autonomous robots in shadow scenario | http://arxiv.org/pdf/1401.2051v1.pdf | author:Olusanya Y. Agunbiade, Tranos Zuva, Awosejo O. Johnson, Keneilwe Zuva category:cs.RO cs.CV published:2014-01-09 summary:Road region recognition is a main feature that is gaining increasingattention from intellectuals because it helps autonomous vehicle to achieve asuccessful navigation without accident. However, different techniques based oncamera sensor have been used by various researchers and outstanding resultshave been achieved. Despite their success, environmental noise like shadowleads to inaccurate recognition of road region which eventually leads toaccident for autonomous vehicle. In this research, we conducted aninvestigation on shadow and its effects, optimized the road region recognitionsystem of autonomous vehicle by introducing an algorithm capable of detectingand eliminating the effects of shadow. The experimental performance of oursystem was tested and compared using the following schemes: Total Positive Rate(TPR), False Negative Rate (FNR), Total Negative Rate (TNR), Error Rate (ERR)and False Positive Rate (FPR). The performance result of the system improved onroad recognition in shadow scenario and this advancement has added tremendouslyto successful navigation approaches for autonomous vehicle.
arxiv-5700-171 | A PSO and Pattern Search based Memetic Algorithm for SVMs Parameters Optimization | http://arxiv.org/pdf/1401.1926v1.pdf | author:Yukun Bao, Zhongyi Hu, Tao Xiong category:cs.LG cs.AI cs.NE stat.ML published:2014-01-09 summary:Addressing the issue of SVMs parameters optimization, this study proposes anefficient memetic algorithm based on Particle Swarm Optimization algorithm(PSO) and Pattern Search (PS). In the proposed memetic algorithm, PSO isresponsible for exploration of the search space and the detection of thepotential regions with optimum solutions, while pattern search (PS) is used toproduce an effective exploitation on the potential regions obtained by PSO.Moreover, a novel probabilistic selection strategy is proposed to select theappropriate individuals among the current population to undergo localrefinement, keeping a well balance between exploration and exploitation.Experimental results confirm that the local refinement with PS and our proposedselection strategy are effective, and finally demonstrate effectiveness androbustness of the proposed PSO-PS based MA for SVMs parameters optimization.
arxiv-5700-172 | Brazilian License Plate Detection Using Histogram of Oriented Gradients and Sliding Windows | http://arxiv.org/pdf/1401.1990v1.pdf | author:R. F. Prates, G. Cámara-Chávez, William R. Schwartz, D. Menotti category:cs.CV published:2014-01-09 summary:Due to the increasingly need for automatic traffic monitoring, vehiclelicense plate detection is of high interest to perform automatic tollcollection, traffic law enforcement, parking lot access control, among others.In this paper, a sliding window approach based on Histogram of OrientedGradients (HOG) features is used for Brazilian license plate detection. Thisapproach consists in scanning the whole image in a multiscale fashion such thatthe license plate is located precisely. The main contribution of this workconsists in a deep study of the best setup for HOG descriptors on the detectionof Brazilian license plates, in which HOG have never been applied before. Wealso demonstrate the reliability of this method ensured by a recall higher than98% (with a precision higher than 78%) in a publicly available data set.
arxiv-5700-173 | Hand-guided 3D surface acquisition by combining simple light sectioning with real-time algorithms | http://arxiv.org/pdf/1401.1946v1.pdf | author:Oliver Arold, Svenja Ettl, Florian Willomitzer, Gerd Häusler category:physics.optics cs.CV published:2014-01-09 summary:Precise 3D measurements of rigid surfaces are desired in many fields ofapplication like quality control or surgery. Often, views from all around theobject have to be acquired for a full 3D description of the object surface. Wepresent a sensor principle called "Flying Triangulation" which avoids anelaborate "stop-and-go" procedure. It combines a low-cost classicallight-section sensor with an algorithmic pipeline. A hand-guided sensorcaptures a continuous movie of 3D views while being moved around the object.The views are automatically aligned and the acquired 3D model is displayed inreal time. In contrast to most existing sensors no bandwidth is wasted forspatial or temporal encoding of the projected lines. Nor is an expensive colorcamera necessary for 3D acquisition. The achievable measurement uncertainty andlateral resolution of the generated 3D data is merely limited by physics. Analternating projection of vertical and horizontal lines guarantees theexistence of corresponding points in successive 3D views. This enables aprecise registration without surface interpolation. For registration, a variantof the iterative closest point algorithm - adapted to the specific nature ofour 3D views - is introduced. Furthermore, data reduction and smoothing withoutlosing lateral resolution as well as the acquisition and mapping of a colortexture is presented. The precision and applicability of the sensor isdemonstrated by simulation and measurement results.
arxiv-5700-174 | Radial basis function process neural network training based on generalized frechet distance and GA-SA hybrid strategy | http://arxiv.org/pdf/1405.7349v1.pdf | author:Bing Wang, Yao-hua Meng, Xiao-hong Yu category:cs.NE published:2014-01-09 summary:For learning problem of Radial Basis Function Process Neural Network(RBF-PNN), an optimization training method based on GA combined with SA isproposed in this paper. Through building generalized Fr\'echet distance tomeasure similarity between time-varying function samples, the learning problemof radial basis centre functions and connection weights is converted into thetraining on corresponding discrete sequence coefficients. Network trainingobjective function is constructed according to the least square errorcriterion, and global optimization solving of network parameters is implementedin feasible solution space by use of global optimization feature of GA andprobabilistic jumping property of SA . The experiment results illustrate thatthe training algorithm improves the network training efficiency and stability.
arxiv-5700-175 | Image reconstruction from few views by L0-norm optimization | http://arxiv.org/pdf/1401.1882v1.pdf | author:Yuli Sun, Jinxu Tao category:cs.IT cs.CV math.IT published:2014-01-09 summary:The L1-norm of the gradient-magnitude images (GMI), which is the well-knowntotal variation (TV) model, is widely used as regularization in the few viewsCT reconstruction. As the L1-norm TV regularization is tending to uniformlypenalize the image gradient and the low-contrast structures are sometimes oversmoothed, we proposed a new algorithm based on the L0-norm of the GMI to dealwith the few views problem. To rise to the challenges introduced by the L0-normDGT, the algorithm uses a pseudo-inverse transform of DGT and adapts aniterative hard thresholding (IHT) algorithm, whose convergence and effectiveefficiency have been theoretically proven. The simulation indicates that thealgorithm proposed in this paper can obviously improve the reconstructionquality.
arxiv-5700-176 | Efficient unimodality test in clustering by signature testing | http://arxiv.org/pdf/1401.1895v1.pdf | author:Mahdi Shahbaba, Soosan Beheshti category:cs.LG stat.ML published:2014-01-09 summary:This paper provides a new unimodality test with application in hierarchicalclustering methods. The proposed method denoted by signature test (Sigtest),transforms the data based on its statistics. The transformed data has muchsmaller variation compared to the original data and can be evaluated in asimple proposed unimodality test. Compared with the existing unimodality tests,Sigtest is more accurate in detecting the overlapped clusters and has a muchless computational complexity. Simulation results demonstrate the efficiency ofthis statistic test for both real and synthetic data sets.
arxiv-5700-177 | Multiple-output support vector regression with a firefly algorithm for interval-valued stock price index forecasting | http://arxiv.org/pdf/1401.1916v1.pdf | author:Tao Xiong, Yukun Bao, Zhongyi Hu category:cs.CE cs.LG q-fin.ST published:2014-01-09 summary:Highly accurate interval forecasting of a stock price index is fundamental tosuccessfully making a profit when making investment decisions, by providing arange of values rather than a point estimate. In this study, we investigate thepossibility of forecasting an interval-valued stock price index series overshort and long horizons using multi-output support vector regression (MSVR).Furthermore, this study proposes a firefly algorithm (FA)-based approach, builton the established MSVR, for determining the parameters of MSVR (abbreviated asFA-MSVR). Three globally traded broad market indices are used to compare theperformance of the proposed FA-MSVR method with selected counterparts. Thequantitative and comprehensive assessments are performed on the basis ofstatistical criteria, economic criteria, and computational cost. In terms ofstatistical criteria, we compare the out-of-sample forecasting usinggoodness-of-forecast measures and testing approaches. In terms of economiccriteria, we assess the relative forecast performance with a simple tradingstrategy. The results obtained in this study indicate that the proposed FA-MSVRmethod is a promising alternative for forecasting interval-valued financialtime series.
arxiv-5700-178 | A Parameterized Complexity Analysis of Bi-level Optimisation with Evolutionary Algorithms | http://arxiv.org/pdf/1401.1905v1.pdf | author:Dogan Corus, Per Kristian Lehre, Frank Neumann, Mojgan Pourhassan category:cs.NE published:2014-01-09 summary:Bi-level optimisation problems have gained increasing interest in the fieldof combinatorial optimisation in recent years. With this paper, we start theruntime analysis of evolutionary algorithms for bi-level optimisation problems.We examine two NP-hard problems, the generalised minimum spanning tree problem(GMST), and the generalised travelling salesman problem (GTSP) in the contextof parameterised complexity. For the generalised minimum spanning tree problem, we analyse the twoapproaches presented by Hu and Raidl (2012) with respect to the number ofclusters that distinguish each other by the chosen representation of possiblesolutions. Our results show that a (1+1) EA working with the spanning nodesrepresentation is not a fixed-parameter evolutionary algorithm for the problem,whereas the global structure representation enables to solve the problem infixed-parameter time. We present hard instances for each approach and show thatthe two approaches are highly complementary by proving that they solve eachother's hard instances very efficiently. For the generalised travelling salesman problem, we analyse the problem withrespect to the number of clusters in the problem instance. Our results showthat a (1+1) EA working with the global structure representation is afixed-parameter evolutionary algorithm for the problem.
arxiv-5700-179 | Optimal Demand Response Using Device Based Reinforcement Learning | http://arxiv.org/pdf/1401.1549v2.pdf | author:Zheng Wen, Daniel O'Neill, Hamid Reza Maei category:cs.LG cs.AI cs.SY published:2014-01-08 summary:Demand response (DR) for residential and small commercial buildings isestimated to account for as much as 65% of the total energy savings potentialof DR, and previous work shows that a fully automated Energy Management System(EMS) is a necessary prerequisite to DR in these areas. In this paper, wepropose a novel EMS formulation for DR problems in these sectors. Specifically,we formulate a fully automated EMS's rescheduling problem as a reinforcementlearning (RL) problem, and argue that this RL problem can be approximatelysolved by decomposing it over device clusters. Compared with existingformulations, our new formulation (1) does not require explicitly modeling theuser's dissatisfaction on job rescheduling, (2) enables the EMS toself-initiate jobs, (3) allows the user to initiate more flexible requests and(4) has a computational complexity linear in the number of devices. We alsodemonstrate the simulation results of applying Q-learning, one of the mostpopular and classical RL algorithms, to a representative example.
arxiv-5700-180 | A Solution of Degree Constrained Spanning Tree Using Hybrid GA | http://arxiv.org/pdf/1401.1753v2.pdf | author:Sounak Sadhukhan, Samar Sen Sarma category:cs.NE cs.DS published:2014-01-08 summary:In real life, it is always an urge to reach our goal in minimum effort i.e.,it should have a minimum constrained path. The path may be shortest route inpractical life, either physical or electronic medium. The scenario is torepresents the ambiance as a graph and to find a spanning tree with customdesign criteria. Here, we have chosen a minimum degree spanning tree, which canbe generated in real time with minimum turnaround time. The problem isNP-complete in nature [1, 2]. The solution approach, in general, isapproximate. We have used a heuristic approach, namely hybrid genetic algorithm(GA), with motivated criteria of encoded data structures of graph. We comparethe experimental result with the existing approximate algorithm and the resultis so encouraging that we are interested to use it in our future applications.
arxiv-5700-181 | The Continuity of Images by Transmission Imaging Revisited | http://arxiv.org/pdf/1401.1558v1.pdf | author:Zhitao Fan, Feng Guan, Chunlin Wu, Ming Yan category:math.DG cs.CV math.NA published:2014-01-08 summary:Transmission imaging, as an important imaging technique widely used inastronomy, medical diagnosis, and biology science, has been shown in [49] quitedifferent from reflection imaging used in our everyday life. Understanding thestructures of images (the prior information) is important for designing,testing, and choosing image processing methods, and good image processingmethods are helpful for further uses of the image data, e.g., increasing theaccuracy of the object reconstruction methods in transmission imagingapplications. In reflection imaging, the images are usually modeled asdiscontinuous functions and even piecewise constant functions. In transmissionimaging, it was shown very recently in [49] that almost all images arecontinuous functions. However, the author in [49] considered only the case ofparallel beam geometry and used some too strong assumptions in the proof, whichexclude some common cases such as cylindrical objects. In this paper, weconsider more general beam geometries and simplify the assumptions by usingtotally different techniques. In particular, we will prove that almost allimages in transmission imaging with both parallel and divergent beam geometries(two most typical beam geometries) are continuous functions, under much weakerassumptions than those in [49], which admit almost all practical cases.Besides, taking into accounts our analysis, we compare two image processingmethods for Poisson noise (which is the most significant noise in transmissionimaging) removal. Numerical experiments will be provided to demonstrate ouranalysis.
arxiv-5700-182 | Robust Large Scale Non-negative Matrix Factorization using Proximal Point Algorithm | http://arxiv.org/pdf/1401.1842v1.pdf | author:Jason Gejie Liu, Shuchin Aeron category:stat.ML cs.IT cs.LG cs.NA math.IT published:2014-01-08 summary:A robust algorithm for non-negative matrix factorization (NMF) is presentedin this paper with the purpose of dealing with large-scale data, where theseparability assumption is satisfied. In particular, we modify the LinearProgramming (LP) algorithm of [9] by introducing a reduced set of constraintsfor exact NMF. In contrast to the previous approaches, the proposed algorithmdoes not require the knowledge of factorization rank (extreme rays [3] ortopics [7]). Furthermore, motivated by a similar problem arising in the contextof metabolic network analysis [13], we consider an entirely different regimewhere the number of extreme rays or topics can be much larger than thedimension of the data vectors. The performance of the algorithm for differentsynthetic data sets are provided.
arxiv-5700-183 | Learning Multilingual Word Representations using a Bag-of-Words Autoencoder | http://arxiv.org/pdf/1401.1803v1.pdf | author:Stanislas Lauly, Alex Boulanger, Hugo Larochelle category:cs.CL cs.LG stat.ML published:2014-01-08 summary:Recent work on learning multilingual word representations usually relies onthe use of word-level alignements (e.g. infered with the help of GIZA++)between translated sentences, in order to align the word embeddings indifferent languages. In this workshop paper, we investigate an autoencodermodel for learning multilingual word representations that does without suchword-level alignements. The autoencoder is trained to reconstruct thebag-of-word representation of given sentence from an encoded representationextracted from its translation. We evaluate our approach on a multilingualdocument classification task, where labeled data is available only for onelanguage (e.g. English) while classification must be performed in a differentlanguage (e.g. French). In our experiments, we observe that our method comparesfavorably with a previously proposed method that exploits word-level alignmentsto learn word representations.
arxiv-5700-184 | Variations on Memetic Algorithms for Graph Coloring Problems | http://arxiv.org/pdf/1401.2184v1.pdf | author:Laurent Moalic, Alexandre Gondran category:cs.AI cs.NE math.OC published:2014-01-08 summary:Graph vertices coloring with a given number of colors is a famous andmuch-studied NP-complete problem. The best methods to solve this problem arehybrid algorithms such as memetic algorithms [Galinier99, Lu10, Wu12] orquantum annealing [Titiloye11a, Titiloye11b, Titiloye12]. Those hybridalgorithms use a powerful local search inside a population-based algorithm. Thebalance between intensification and diversification is essential for thosemetaheuristics but difficult to archieve. Customizing metaheuristics takes longtime and is one of the main weak points of these approaches. This paper studiesthe impact of the increase and the decrease of diversification in one of themost effective algorithms known: the Hybrid Evolutionary Algorithm (HEA) fromGalinier and Hao [Galinier99]. We then propose a modification of this memeticalgorithm in order to work with a population of only two individuals. This newalgorithm more effectively manages the correct 'dose' of diversification to addinto the included local search - TabuCol [Hertz87] in the case of the HEA. Ithas produced several good results for the well-known DIMACS benchmark graphs,such as 47-colorings for DSJC500.5, 82-colorings for DSJC1000.5, 222-coloringsfor DSJC1000.9 and 81-colorings for flat1000\_76\_0, which have so far onlybeen produced by quantum annealing [Titiloye12] in 2012 with massivemulti-CPUs.
arxiv-5700-185 | Large Scale Visual Recommendations From Street Fashion Images | http://arxiv.org/pdf/1401.1778v1.pdf | author:Vignesh Jagadeesh, Robinson Piramuthu, Anurag Bhardwaj, Wei Di, Neel Sundaresan category:cs.CV published:2014-01-08 summary:We describe a completely automated large scale visual recommendation systemfor fashion. Our focus is to efficiently harness the availability of largequantities of online fashion images and their rich meta-data. Specifically, wepropose four data driven models in the form of Complementary Nearest NeighborConsensus, Gaussian Mixture Models, Texture Agnostic Retrieval and Markov ChainLDA for solving this problem. We analyze relative merits and pitfalls of thesealgorithms through extensive experimentation on a large-scale data set andbaseline them against existing ideas from color science. We also illustrate keyfashion insights learned through these experiments and show how they can beemployed to design better recommendation systems. Finally, we also outline alarge-scale annotated data set of fashion images (Fashion-136K) that can beexploited for future vision research.
arxiv-5700-186 | Actor-Critic Algorithms for Learning Nash Equilibria in N-player General-Sum Games | http://arxiv.org/pdf/1401.2086v2.pdf | author:H. L Prasad, L. A. Prashanth, Shalabh Bhatnagar category:cs.GT cs.LG stat.ML published:2014-01-08 summary:We consider the problem of finding stationary Nash equilibria (NE) in afinite discounted general-sum stochastic game. We first generalize a non-linearoptimization problem from Filar and Vrieze [2004] to a $N$-player setting andbreak down this problem into simpler sub-problems that ensure there is noBellman error for a given state and an agent. We then provide acharacterization of solution points of these sub-problems that correspond toNash equilibria of the underlying game and for this purpose, we derive a set ofnecessary and sufficient SG-SP (Stochastic Game - Sub-Problem) conditions.Using these conditions, we develop two actor-critic algorithms: OFF-SGSP(model-based) and ON-SGSP (model-free). Both algorithms use a critic thatestimates the value function for a fixed policy and an actor that performsdescent in the policy space using a descent direction that avoids local minima.We establish that both algorithms converge, in self-play, to the equilibria ofa certain ordinary differential equation (ODE), whose stable limit pointscoincide with stationary NE of the underlying general-sum stochastic game. On asingle state non-generic game (see Hart and Mas-Colell [2005]) as well as on asynthetic two-player game setup with $810,000$ states, we establish thatON-SGSP consistently outperforms NashQ ([Hu and Wellman, 2003] and FFQ[Littman, 2001] algorithms.
arxiv-5700-187 | Content Based Image Indexing and Retrieval | http://arxiv.org/pdf/1401.1742v1.pdf | author:Avinash N Bhute, B. B. Meshram category:cs.CV cs.GR cs.IR cs.MM published:2014-01-08 summary:In this paper, we present the efficient content based image retrieval systemswhich employ the color, texture and shape information of images to facilitatethe retrieval process. For efficient feature extraction, we extract the color,texture and shape feature of images automatically using edge detection which iswidely used in signal processing and image compression. For facilitated thespeedy retrieval we are implements the antipole-tree algorithm for indexing theimages.
arxiv-5700-188 | Application of the Modified Fractal Signature Method for Terrain Classification from Synthetic Aperture Radar Images | http://arxiv.org/pdf/1401.2899v1.pdf | author:A. Malamou, C. Pandis, P. Frangos, P. Stefaneas, A. Karakasiliotis, D. Kodokostas category:cs.CV published:2014-01-08 summary:In this paper the Modified Fractal Signature method is applied to realSynthetic Aperture Radar images provided to our research group by SET 163Working Group on SAR radar techniques. This method uses the blanket techniqueto provide useful information for SAR image classification. It is based on thecalculation of the volume of a blanket, corresponding to the image to beclassified, and then on the calculation of the corresponding Fractal Area curveand Fractal Dimension curve of the image. The main idea concerning thisproposed technique is the fact that different terrain types encountered in SARimages yield different values of Fractal Area curves and Fractal Dimensioncurves, upon which classification of different types of terrain is possible. Asa result, a classification technique for five different terrain types, i.e.urban, suburban, rural, mountain and sea, is presented in this paper.
arxiv-5700-189 | Beyond One-Step-Ahead Forecasting: Evaluation of Alternative Multi-Step-Ahead Forecasting Models for Crude Oil Prices | http://arxiv.org/pdf/1401.1560v1.pdf | author:Tao Xiong, Yukun Bao, Zhongyi Hu category:cs.LG cs.AI published:2014-01-08 summary:An accurate prediction of crude oil prices over long future horizons ischallenging and of great interest to governments, enterprises, and investors.This paper proposes a revised hybrid model built upon empirical modedecomposition (EMD) based on the feed-forward neural network (FNN) modelingframework incorporating the slope-based method (SBM), which is capable ofcapturing the complex dynamic of crude oil prices. Three commonly usedmulti-step-ahead prediction strategies proposed in the literature, includingiterated strategy, direct strategy, and MIMO (multiple-input multiple-output)strategy, are examined and compared, and practical considerations for theselection of a prediction strategy for multi-step-ahead forecasting relating tocrude oil prices are identified. The weekly data from the WTI (West TexasIntermediate) crude oil spot price are used to compare the performance of thealternative models under the EMD-SBM-FNN modeling framework with selectedcounterparts. The quantitative and comprehensive assessments are performed onthe basis of prediction accuracy and computational cost. The results obtainedin this study indicate that the proposed EMD-SBM-FNN model using the MIMOstrategy is the best in terms of prediction accuracy with accreditedcomputational load.
arxiv-5700-190 | Fast nonparametric clustering of structured time-series | http://arxiv.org/pdf/1401.1605v2.pdf | author:James Hensman, Magnus Rattray, Neil D. Lawrence category:cs.LG cs.CV stat.ML published:2014-01-08 summary:In this publication, we combine two Bayesian non-parametric models: theGaussian Process (GP) and the Dirichlet Process (DP). Our innovation in the GPmodel is to introduce a variation on the GP prior which enables us to modelstructured time-series data, i.e. data containing groups where we wish to modelinter- and intra-group variability. Our innovation in the DP model is animplementation of a new fast collapsed variational inference procedure whichenables us to optimize our variationala pproximation significantly faster thanstandard VB approaches. In a biological time series application we show how ourmodel better captures salient features of the data, leading to betterconsistency with existing biological classifications, while the associatedinference algorithm provides a twofold speed-up over EM-based variationalinference.
arxiv-5700-191 | Cortical prediction markets | http://arxiv.org/pdf/1401.1465v1.pdf | author:David Balduzzi category:cs.AI cs.GT cs.LG cs.MA q-bio.NC published:2014-01-07 summary:We investigate cortical learning from the perspective of mechanism design.First, we show that discretizing standard models of neurons and synapticplasticity leads to rational agents maximizing simple scoring rules. Second,our main result is that the scoring rules are proper, implying that neuronsfaithfully encode expected utilities in their synaptic weights and encodehigh-scoring outcomes in their spikes. Third, with this foundation in hand, wepropose a biologically plausible mechanism whereby neurons backpropagateincentives which allows them to optimize their usefulness to the rest ofcortex. Finally, experiments show that networks that backpropagate incentivescan learn simple tasks.
arxiv-5700-192 | Time series forecasting using neural networks | http://arxiv.org/pdf/1401.1333v1.pdf | author:Bogdan Oancea, ŞTefan Cristian Ciucu category:cs.NE published:2014-01-07 summary:Recent studies have shown the classification and prediction power of theNeural Networks. It has been demonstrated that a NN can approximate anycontinuous function. Neural networks have been successfully used forforecasting of financial data series. The classical methods used for timeseries prediction like Box-Jenkins or ARIMA assumes that there is a linearrelationship between inputs and outputs. Neural Networks have the advantagethat can approximate nonlinear functions. In this paper we compared theperformances of different feed forward and recurrent neural networks andtraining algorithms for predicting the exchange rate EUR/RON and USD/RON. Weused data series with daily exchange rates starting from 2005 until 2013.
arxiv-5700-193 | Key point selection and clustering of swimmer coordination through Sparse Fisher-EM | http://arxiv.org/pdf/1401.1489v1.pdf | author:John Komar, Romain Hérault, Ludovic Seifert category:stat.ML cs.CV cs.LG stat.AP published:2014-01-07 summary:To answer the existence of optimal swimmer learning/teaching strategies, thiswork introduces a two-level clustering in order to analyze temporal dynamics ofmotor learning in breaststroke swimming. Each level have been performed throughSparse Fisher-EM, a unsupervised framework which can be applied efficiently onlarge and correlated datasets. The induced sparsity selects key points of thecoordination phase without any prior knowledge.
arxiv-5700-194 | Insights from the Wikipedia Contest (IEEE Contest for Data Mining 2011) | http://arxiv.org/pdf/1405.7393v1.pdf | author:Kalpit V Desai, Roopesh Ranjan category:cs.CY physics.soc-ph stat.ML published:2014-01-07 summary:The Wikimedia Foundation has recently observed that newly joining editors onWikipedia are increasingly failing to integrate into the Wikipedia editors'community, i.e. the community is becoming increasingly harder to penetrate. Tosustain healthy growth of the community, the Wikimedia Foundation aims toquantitatively understand the factors that determine the editing behavior, andexplain why most new editors become inactive soon after joining. As a steptowards this broader goal, the Wikimedia foundation sponsored the ICDM (IEEEInternational Conference for Data Mining) contest for the year 2011. The objective for the participants was to develop models to predict thenumber of edits that an editor will make in future five months based on theediting history of the editor. Here we describe the approach we followed fordeveloping predictive models towards this goal, the results that we obtainedand the modeling insights that we gained from this exercise. In addition,towards the broader goal of Wikimedia Foundation, we also summarize the factorsthat emerged during our model building exercise as powerful predictors offuture editing activity.
arxiv-5700-195 | Design & Development of the Graphical User Interface for Sindhi Language | http://arxiv.org/pdf/1401.1486v1.pdf | author:Imdad Ali Ismaili, Zeeshan Bhatti, Azhar Ali Shah category:cs.HC cs.CL published:2014-01-07 summary:This paper describes the design and implementation of a Unicode-based GUISL(Graphical User Interface for Sindhi Language). The idea is to provide asoftware platform to the people of Sindh as well as Sindhi diasporas livingacross the globe to make use of computing for basic tasks such as editing,composition, formatting, and printing of documents in Sindhi by using GUISL.The implementation of the GUISL has been done in the Java technology to makethe system platform independent. The paper describes several design issues ofSindhi GUI in the context of existing software tools and technologies andexplains how mapping and concatenation techniques have been employed to achievethe cursive shape of Sindhi script.
arxiv-5700-196 | Accelerating ABC methods using Gaussian processes | http://arxiv.org/pdf/1401.1436v2.pdf | author:Richard D Wilkinson category:stat.CO stat.ML published:2014-01-07 summary:Approximate Bayesian computation (ABC) methods are used to approximateposterior distributions using simulation rather than likelihood calculations.We introduce Gaussian process (GP) accelerated ABC, which we show cansignificantly reduce the number of simulations required. As computationalresource is usually the main determinant of accuracy in ABC, GP-acceleratedmethods can thus enable more accurate inference in some models. GP models ofthe unknown log-likelihood function are used to exploit continuity andsmoothness, reducing the required computation. We use a sequence of models thatincrease in accuracy, using intermediate models to rule out regions of theparameter space as implausible. The methods will not be suitable for allproblems, but when they can be used, can result in significant computationalsavings. For the Ricker model, we are able to achieve accurate approximationsto the posterior distribution using a factor of 100 fewer simulator evaluationsthan comparable Monte Carlo approaches, and for a population genetics model weare able to approximate the exact posterior for the first time.
arxiv-5700-197 | Effective Slot Filling Based on Shallow Distant Supervision Methods | http://arxiv.org/pdf/1401.1158v1.pdf | author:Benjamin Roth, Tassilo Barth, Michael Wiegand, Mittul Singh, Dietrich Klakow category:cs.CL published:2014-01-06 summary:Spoken Language Systems at Saarland University (LSV) participated this yearwith 5 runs at the TAC KBP English slot filling track. Effective algorithms forall parts of the pipeline, from document retrieval to relation prediction andresponse post-processing, are bundled in a modular end-to-end relationextraction system called RelationFactory. The main run solely focuses onshallow techniques and achieved significant improvements over LSV's last year'ssystem, while using the same training data and patterns. Improvements mainlyhave been obtained by a feature representation focusing on surface skip n-gramsand improved scoring for extracted distant supervision patterns. Importantfactors for effective extraction are the training and tuning scheme for distantsupervision classifiers, and the query expansion by a translation model basedon Wikipedia links. In the TAC KBP 2013 English Slotfilling evaluation, thesubmitted main run of the LSV RelationFactory system achieved the top-rankedF1-score of 37.3%.
arxiv-5700-198 | Bangla Text Recognition from Video Sequence: A New Focus | http://arxiv.org/pdf/1401.1190v1.pdf | author:Souvik Bhowmick, Purnendu Banerjee category:cs.CV published:2014-01-06 summary:Extraction and recognition of Bangla text from video frame images ischallenging due to complex color background, low-resolution etc. In this paper,we propose an algorithm for extraction and recognition of Bangla text form suchvideo frames with complex background. Here, a two-step approach has beenproposed. First, the text line is segmented into words using information basedon line contours. First order gradient value of the text blocks are used tofind the word gap. Next, a local binarization technique is applied on each wordand text line is reconstructed using those words. Secondly, this binarized textblock is sent to OCR for recognition purpose.
arxiv-5700-199 | Exploration vs Exploitation vs Safety: Risk-averse Multi-Armed Bandits | http://arxiv.org/pdf/1401.1123v1.pdf | author:Nicolas Galichet, Michèle Sebag, Olivier Teytaud category:cs.LG published:2014-01-06 summary:Motivated by applications in energy management, this paper presents theMulti-Armed Risk-Aware Bandit (MARAB) algorithm. With the goal of limiting theexploration of risky arms, MARAB takes as arm quality its conditional value atrisk. When the user-supplied risk level goes to 0, the arm quality tends towardthe essential infimum of the arm distribution density, and MARAB tends towardthe MIN multi-armed bandit algorithm, aimed at the arm with maximal minimalvalue. As a first contribution, this paper presents a theoretical analysis ofthe MIN algorithm under mild assumptions, establishing its robustnesscomparatively to UCB. The analysis is supported by extensive experimentalvalidation of MIN and MARAB compared to UCB and state-of-art risk-aware MABalgorithms on artificial and real-world problems.
arxiv-5700-200 | Differentially Private Data Releasing for Smooth Queries with Synthetic Database Output | http://arxiv.org/pdf/1401.0987v1.pdf | author:Chi Jin, Ziteng Wang, Junliang Huang, Yiqiao Zhong, Liwei Wang category:cs.DB stat.ML published:2014-01-06 summary:We consider accurately answering smooth queries while preserving differentialprivacy. A query is said to be $K$-smooth if it is specified by a functiondefined on $[-1,1]^d$ whose partial derivatives up to order $K$ are allbounded. We develop an $\epsilon$-differentially private mechanism for theclass of $K$-smooth queries. The major advantage of the algorithm is that itoutputs a synthetic database. In real applications, a synthetic database outputis appealing. Our mechanism achieves an accuracy of $O(n^{-\frac{K}{2d+K}}/\epsilon )$, and runs in polynomial time. We alsogeneralize the mechanism to preserve $(\epsilon, \delta)$-differential privacywith slightly improved accuracy. Extensive experiments on benchmark datasetsdemonstrate that the mechanisms have good accuracy and are efficient.
arxiv-5700-201 | Sparse graphs using exchangeable random measures | http://arxiv.org/pdf/1401.1137v3.pdf | author:François Caron, Emily B. Fox category:stat.ME cs.SI math.ST stat.ML stat.TH published:2014-01-06 summary:Statistical network modeling has focused on representing the graph as adiscrete structure, namely the adjacency matrix, and considering theexchangeability of this array. In such cases, the Aldous-Hoover representationtheorem (Aldous, 1981;Hoover, 1979} applies and informs us that the graph isnecessarily either dense or empty. In this paper, we instead considerrepresenting the graph as a measure on $\mathbb{R}_+^2$. For the associateddefinition of exchangeability in this continuous space, we rely on theKallenberg representation theorem (Kallenberg, 2005). We show that for certainchoices of such exchangeable random measures underlying our graph construction,our network process is sparse with power-law degree distribution. Inparticular, we build on the framework of completely random measures (CRMs) anduse the theory associated with such processes to derive important networkproperties, such as an urn representation for our analysis and networksimulation. Our theoretical results are explored empirically and compared tocommon network models. We then present a Hamiltonian Monte Carlo algorithm forefficient exploration of the posterior distribution and demonstrate that we areable to recover graphs ranging from dense to sparse--and perform associatedtests--based on our flexible CRM-based formulation. We explore networkproperties in a range of real datasets, including Facebook social circles, apolitical blogosphere, protein networks, citation networks, and world wide webnetworks, including networks with hundreds of thousands of nodes and millionsof edges.
arxiv-5700-202 | A binary differential evolution algorithm learning from explored solutions | http://arxiv.org/pdf/1401.1124v2.pdf | author:Yu Chen, Weicheng Xie, Xiufen Zou category:cs.NE published:2014-01-06 summary:Although real-coded differential evolution (DE) algorithms can perform wellon continuous optimization problems (CoOPs), it is still a challenging task todesign an efficient binary-coded DE algorithm. Inspired by the learningmechanism of particle swarm optimization (PSO) algorithms, we propose a binarylearning differential evolution (BLDE) algorithm that can efficiently locatethe global optimal solutions by learning from the last population. Then, wetheoretically prove the global convergence of BLDE, and compare it with someexisting binary-coded evolutionary algorithms (EAs) via numerical experiments.Numerical results show that BLDE is competitive to the compared EAs, andmeanwhile, further study is performed via the change curves of a renewal metricand a refinement metric to investigate why BLDE cannot outperform some comparedEAs for several selected benchmark problems. Finally, we employ BLDE solvingthe unit commitment problem (UCP) in power systems to show its applicability inpractical problems.
arxiv-5700-203 | Binary Linear Classification and Feature Selection via Generalized Approximate Message Passing | http://arxiv.org/pdf/1401.0872v3.pdf | author:Justin Ziniel, Philip Schniter, Per Sederberg category:cs.IT math.IT stat.ML published:2014-01-05 summary:For the problem of binary linear classification and feature selection, wepropose algorithmic approaches to classifier design based on the generalizedapproximate message passing (GAMP) algorithm, recently proposed in the contextof compressive sensing. We are particularly motivated by problems where thenumber of features greatly exceeds the number of training examples, but whereonly a few features suffice for accurate classification. We show thatsum-product GAMP can be used to (approximately) minimize the classificationerror rate and max-sum GAMP can be used to minimize a wide variety ofregularized loss functions. Furthermore, we describe anexpectation-maximization (EM)-based scheme to learn the associated modelparameters online, as an alternative to cross-validation, and we show thatGAMP's state-evolution framework can be used to accurately predict themisclassification rate. Finally, we present a detailed numerical study toconfirm the accuracy, speed, and flexibility afforded by our GAMP-basedapproaches to binary linear classification and feature selection.
arxiv-5700-204 | Feature Selection Using Classifier in High Dimensional Data | http://arxiv.org/pdf/1401.0898v1.pdf | author:Vijendra Singh, Shivani Pathak category:cs.CV cs.LG stat.ML published:2014-01-05 summary:Feature selection is frequently used as a pre-processing step to machinelearning. It is a process of choosing a subset of original features so that thefeature space is optimally reduced according to a certain evaluation criterion.The central objective of this paper is to reduce the dimension of the data byfinding a small set of important features which can give good classificationperformance. We have applied filter and wrapper approach with differentclassifiers QDA and LDA respectively. A widely-used filter method is used forbioinformatics data i.e. a univariate criterion separately on each feature,assuming that there is no interaction between features and then appliedSequential Feature Selection method. Experimental results show that filterapproach gives better performance in respect of Misclassification Error Rate.
arxiv-5700-205 | Factorized Point Process Intensities: A Spatial Analysis of Professional Basketball | http://arxiv.org/pdf/1401.0942v2.pdf | author:Andrew Miller, Luke Bornn, Ryan Adams, Kirk Goldsberry category:stat.ML stat.AP published:2014-01-05 summary:We develop a machine learning approach to represent and analyze theunderlying spatial structure that governs shot selection among professionalbasketball players in the NBA. Typically, NBA players are discussed andcompared in an heuristic, imprecise manner that relies on unmeasured intuitionsabout player behavior. This makes it difficult to draw comparisons betweenplayers and make accurate player specific predictions. Modeling shot attemptdata as a point process, we create a low dimensional representation ofoffensive player types in the NBA. Using non-negative matrix factorization(NMF), an unsupervised dimensionality reduction technique, we show that alow-rank spatial decomposition summarizes the shooting habits of NBA players.The spatial representations discovered by the algorithm correspond to intuitivedescriptions of NBA player types, and can be used to model other spatialeffects, such as shooting accuracy.
arxiv-5700-206 | Pectoral Muscles Suppression in Digital Mammograms using Hybridization of Soft Computing Methods | http://arxiv.org/pdf/1401.0870v1.pdf | author:I. Laurence Aroquiaraj, K. Thangavel category:cs.CV cs.CE published:2014-01-05 summary:Breast region segmentation is an essential prerequisite in computerizedanalysis of mammograms. It aims at separating the breast tissue from thebackground of the mammogram and it includes two independent segmentations. Thefirst segments the background region which usually contains annotations, labelsand frames from the whole breast region, while the second removes the pectoralmuscle portion (present in Medio Lateral Oblique (MLO) views) from the rest ofthe breast tissue. In this paper we propose hybridization of ConnectedComponent Labeling (CCL), Fuzzy, and Straight line methods. Our proposedmethods worked good for separating pectoral region. After removal pectoralmuscle from the mammogram, further processing is confined to the breast regionalone. To demonstrate the validity of our segmentation algorithm, it isextensively tested using over 322 mammographic images from the MammographicImage Analysis Society (MIAS) database. The segmentation results were evaluatedusing a Mean Absolute Error (MAE), Hausdroff Distance (HD), Probabilistic RandIndex (PRI), Local Consistency Error (LCE) and Tanimoto Coefficient (TC). Thehybridization of fuzzy with straight line method is given more than 96% of thecurve segmentations to be adequate or better. In addition a comparison withsimilar approaches from the state of the art has been given, obtaining slightlyimproved results. Experimental results demonstrate the effectiveness of theproposed approach.
arxiv-5700-207 | Schatten-$p$ Quasi-Norm Regularized Matrix Optimization via Iterative Reweighted Singular Value Minimization | http://arxiv.org/pdf/1401.0869v2.pdf | author:Zhaosong Lu, Yong Zhang category:math.OC cs.LG math.NA stat.CO stat.ML published:2014-01-05 summary:In this paper we study general Schatten-$p$ quasi-norm (SPQN) regularizedmatrix minimization problems. In particular, we first introduce a class offirst-order stationary points for them, and show that the first-orderstationary points introduced in [11] for an SPQN regularized $vector$minimization problem are equivalent to those of an SPQN regularized $matrix$minimization reformulation. We also show that any local minimizer of the SPQNregularized matrix minimization problems must be a first-order stationarypoint. Moreover, we derive lower bounds for nonzero singular values of thefirst-order stationary points and hence also of the local minimizers of theSPQN regularized matrix minimization problems. The iterative reweightedsingular value minimization (IRSVM) methods are then proposed to solve theseproblems, whose subproblems are shown to have a closed-form solution. Incontrast to the analogous methods for the SPQN regularized $vector$minimization problems, the convergence analysis of these methods issignificantly more challenging. We develop a novel approach to establishing theconvergence of these methods, which makes use of the expression of a specificsolution of their subproblems and avoids the intricate issue of finding theexplicit expression for the Clarke subdifferential of the objective of theirsubproblems. In particular, we show that any accumulation point of the sequencegenerated by the IRSVM methods is a first-order stationary point of theproblems. Our computational results demonstrate that the IRSVM methodsgenerally outperform some recently developed state-of-the-art methods in termsof solution quality and/or speed.
arxiv-5700-208 | Multimodal Optimization by Sparkling Squid Populations | http://arxiv.org/pdf/1401.0858v1.pdf | author:Videh Seksaria category:cs.NE published:2014-01-05 summary:The swarm intelligence of animals is a natural paradigm to apply tooptimization problems. Ant colony, bee colony, firefly and bat algorithms areamongst those that have been demonstrated to efficiently to optimize complexconstraints. This paper proposes the new Sparkling Squid Algorithm (SSA) formultimodal optimization, inspired by the intelligent swarm behavior of itsnamesake. After an introduction, formulation and discussion of itsimplementation, it will be compared to other popular metaheuristics. Finally,applications to well - known problems such as image registration and thetraveling salesperson problem will be discussed.
arxiv-5700-209 | Spectrum Hole Prediction Based On Historical Data: A Neural Network Approach | http://arxiv.org/pdf/1401.0886v1.pdf | author:Barau Gafai Najashi, Feng Wenjiang, Mohammed Dikko Almustapha category:cs.NE published:2014-01-05 summary:The concept of cognitive radio pioneered by Mitola promises to change thefuture of wireless communication especially in the area of spectrum management.Currently, the command and control strategy employed in spectrum assignment istoo rigid and needs to be reviewed. Recent studies have shown that assignedspectrum is underutilized spectrally and temporally. Cognitive radio provides aviable solution whereby licensed users can share the spectrum with unlicensedusers opportunistically without causing interference. Unlicensed users must beable to sense weather the channel is busy or idle, failure to do so will leadto interference to the licensed user. In this paper, a neural network basedprediction model for predicting the channel status using historical dataobtained during a spectrum occupancy measurement is presented. Geneticalgorithm is combined with LM BP for increasing the probability of obtainingthe best weights thus optimizing the network. The results obtained indicatehigh prediction accuracy over all bands considered
arxiv-5700-210 | Learning parametric dictionaries for graph signals | http://arxiv.org/pdf/1401.0887v1.pdf | author:Dorina Thanou, David I Shuman, Pascal Frossard category:cs.LG cs.SI stat.ML published:2014-01-05 summary:In sparse signal representation, the choice of a dictionary often involves atradeoff between two desirable properties -- the ability to adapt to specificsignal data and a fast implementation of the dictionary. To sparsely representsignals residing on weighted graphs, an additional design challenge is toincorporate the intrinsic geometric structure of the irregular data domain intothe atoms of the dictionary. In this work, we propose a parametric dictionarylearning algorithm to design data-adapted, structured dictionaries thatsparsely represent graph signals. In particular, we model graph signals ascombinations of overlapping local patterns. We impose the constraint that eachdictionary is a concatenation of subdictionaries, with each subdictionary beinga polynomial of the graph Laplacian matrix, representing a single patterntranslated to different areas of the graph. The learning algorithm adapts thepatterns to a training set of graph signals. Experimental results on bothsynthetic and real datasets demonstrate that the dictionaries learned by theproposed algorithm are competitive with and often better than unstructureddictionaries learned by state-of-the-art numerical learning algorithms in termsof sparse approximation of graph signals. In contrast to the unstructureddictionaries, however, the dictionaries learned by the proposed algorithmfeature localized atoms and can be implemented in a computationally efficientmanner in signal processing tasks such as compression, denoising, andclassification.
arxiv-5700-211 | Stylistic Clusters and the Syrian/South Syrian Tradition of First-Millennium BCE Levantine Ivory Carving: A Machine Learning Approach | http://arxiv.org/pdf/1401.0871v1.pdf | author:Amy Rebecca Gansell, Jan-Willem van de Meent, Sakellarios Zairis, Chris H. Wiggins category:stat.ML stat.AP published:2014-01-05 summary:Thousands of first-millennium BCE ivory carvings have been excavated fromNeo-Assyrian sites in Mesopotamia (primarily Nimrud, Khorsabad, and ArslanTash) hundreds of miles from their Levantine production contexts. At present,their specific manufacture dates and workshop localities are unknown. Relyingon subjective, visual methods, scholars have grappled with their classificationand regional attribution for over a century. This study combines visualapproaches with machine-learning techniques to offer data-driven perspectiveson the classification and attribution of this early Iron Age corpus. The studysample consisted of 162 sculptures of female figures. We have developed analgorithm that clusters the ivories based on a combination of descriptive andanthropometric data. The resulting categories, which are based on purelystatistical criteria, show good agreement with conventional art historicalclassifications, while revealing new perspectives, especially with regard tothe contested Syrian/South Syrian/Intermediate tradition. Specifically, we haveidentified that objects of the Syrian/South Syrian/Intermediate tradition maybe more closely related to Phoenician objects than to North Syrian objects; weoffer a reconsideration of a subset of Phoenician objects, and we confirmSyrian/South Syrian/Intermediate stylistic subgroups that might distinguishnetworks of acquisition among the sites of Nimrud, Khorsabad, Arslan Tash andthe Levant. We have also identified which features are most significant in ourcluster assignments and might thereby be most diagnostic of regional carvingtraditions. In short, our study both corroborates traditional visualclassification methods and demonstrates how machine-learning techniques may beemployed to reveal complementary information not accessible through theexclusively visual analysis of an archaeological corpus.
arxiv-5700-212 | Properties of phoneme N -grams across the world's language families | http://arxiv.org/pdf/1401.0794v1.pdf | author:Taraka Rama, Lars Borin category:cs.CL stat.CO published:2014-01-04 summary:In this article, we investigate the properties of phoneme N-grams across halfof the world's languages. We investigate if the sizes of three different N-gramdistributions of the world's language families obey a power law. Further, theN-gram distributions of language families parallel the sizes of the families,which seem to obey a power law distribution. The correlation between N-gramdistributions and language family sizes improves with increasing values of N.We applied statistical tests, originally given by physicists, to test thehypothesis of power law fit to twelve different datasets. The study also raisessome new questions about the use of N-gram distributions in linguisticresearch, which we answer by running a statistical test.
arxiv-5700-213 | From Kernel Machines to Ensemble Learning | http://arxiv.org/pdf/1401.0767v1.pdf | author:Chunhua Shen, Fayao Liu category:cs.LG cs.CV published:2014-01-04 summary:Ensemble methods such as boosting combine multiple learners to obtain betterprediction than could be obtained from any individual learner. Here we proposea principled framework for directly constructing ensemble learning methods fromkernel methods. Unlike previous studies showing the equivalence betweenboosting and support vector machines (SVMs), which needs a translationprocedure, we show that it is possible to design boosting-like procedure tosolve the SVM optimization problems. In other words, it is possible to design ensemble methods directly from SVMwithout any middle procedure. This finding not only enables us to design new ensemble learning methodsdirectly from kernel methods, but also makes it possible to take advantage ofthose highly-optimized fast linear SVM solvers for ensemble learning. We exemplify this framework for designing binary ensemble learning as well asa new multi-class ensemble learning methods. Experimental results demonstrate the flexibility and usefulness of theproposed framework.
arxiv-5700-214 | Least Squares Policy Iteration with Instrumental Variables vs. Direct Policy Search: Comparison Against Optimal Benchmarks Using Energy Storage | http://arxiv.org/pdf/1401.0843v1.pdf | author:Warren R. Scott, Warren B. Powell, Somayeh Moazehi category:math.OC cs.LG published:2014-01-04 summary:This paper studies approximate policy iteration (API) methods which useleast-squares Bellman error minimization for policy evaluation. We addressseveral of its enhancements, namely, Bellman error minimization usinginstrumental variables, least-squares projected Bellman error minimization, andprojected Bellman error minimization using instrumental variables. We provethat for a general discrete-time stochastic control problem, Bellman errorminimization using instrumental variables is equivalent to both variants ofprojected Bellman error minimization. An alternative to these API methods isdirect policy search based on knowledge gradient. The practical performance ofthese three approximate dynamic programming methods are then investigated inthe context of an application in energy storage, integrated with anintermittent wind energy supply to fully serve a stochastic time-varyingelectricity demand. We create a library of test problems using real-world dataand apply value iteration to find their optimal policies. These benchmarks arethen used to compare the developed policies. Our analysis indicates that APIwith instrumental variables Bellman error minimization prominently outperformsAPI with least-squares Bellman error minimization. However, these approachesunderperform our direct policy search implementation.
arxiv-5700-215 | Context-Aware Hypergraph Construction for Robust Spectral Clustering | http://arxiv.org/pdf/1401.0764v1.pdf | author:Xi Li, Weiming Hu, Chunhua Shen, Anthony Dick, Zhongfei Zhang category:cs.CV cs.LG published:2014-01-04 summary:Spectral clustering is a powerful tool for unsupervised data analysis. Inthis paper, we propose a context-aware hypergraph similarity measure (CAHSM),which leads to robust spectral clustering in the case of noisy data. Weconstruct three types of hypergraph---the pairwise hypergraph, thek-nearest-neighbor (kNN) hypergraph, and the high-order over-clusteringhypergraph. The pairwise hypergraph captures the pairwise similarity of datapoints; the kNN hypergraph captures the neighborhood of each point; and theclustering hypergraph encodes high-order contexts within the dataset. Bycombining the affinity information from these three hypergraphs, the CAHSMalgorithm is able to explore the intrinsic topological information of thedataset. Therefore, data clustering using CAHSM tends to be more robust.Considering the intra-cluster compactness and the inter-cluster separability ofvertices, we further design a discriminative hypergraph partitioning criterion(DHPC). Using both CAHSM and DHPC, a robust spectral clustering algorithm isdeveloped. Theoretical analysis and experimental evaluation demonstrate theeffectiveness and robustness of the proposed algorithm.
arxiv-5700-216 | Concave Penalized Estimation of Sparse Gaussian Bayesian Networks | http://arxiv.org/pdf/1401.0852v2.pdf | author:Bryon Aragam, Qing Zhou category:stat.ME cs.LG stat.ML published:2014-01-04 summary:We develop a penalized likelihood estimation framework to estimate thestructure of Gaussian Bayesian networks from observational data. In contrast torecent methods which accelerate the learning problem by restricting the searchspace, our main contribution is a fast algorithm for score-based structurelearning which does not restrict the search space in any way and works onhigh-dimensional datasets with thousands of variables. Our use of concaveregularization, as opposed to the more popular $\ell_0$ (e.g. BIC) penalty, isnew. Moreover, we provide theoretical guarantees which generalize existingasymptotic results when the underlying distribution is Gaussian. Most notably,our framework does not require the existence of a so-called faithful DAGrepresentation, and as a result the theory must handle the inherentnonidentifiability of the estimation problem in a novel way. Finally, as amatter of independent interest, we provide a comprehensive comparison of ourapproach to several standard structure learning methods using open-sourcepackages developed for the R language. Based on these experiments, we show thatour algorithm is significantly faster than other competing methods whileobtaining higher sensitivity with comparable false discovery rates forhigh-dimensional data. In particular, the total runtime for our method togenerate a solution path of 20 estimates for DAGs with 8000 nodes is around onehour.
arxiv-5700-217 | More Algorithms for Provable Dictionary Learning | http://arxiv.org/pdf/1401.0579v1.pdf | author:Sanjeev Arora, Aditya Bhaskara, Rong Ge, Tengyu Ma category:cs.DS cs.LG stat.ML published:2014-01-03 summary:In dictionary learning, also known as sparse coding, the algorithm is givensamples of the form $y = Ax$ where $x\in \mathbb{R}^m$ is an unknown randomsparse vector and $A$ is an unknown dictionary matrix in $\mathbb{R}^{n\timesm}$ (usually $m > n$, which is the overcomplete case). The goal is to learn $A$and $x$. This problem has been studied in neuroscience, machine learning,visions, and image processing. In practice it is solved by heuristic algorithmsand provable algorithms seemed hard to find. Recently, provable algorithms werefound that work if the unknown feature vector $x$ is $\sqrt{n}$-sparse or evensparser. Spielman et al. \cite{DBLP:journals/jmlr/SpielmanWW12} did this fordictionaries where $m=n$; Arora et al. \cite{AGM} gave an algorithm forovercomplete ($m >n$) and incoherent matrices $A$; and Agarwal et al.\cite{DBLP:journals/corr/AgarwalAN13} handled a similar case but with weakerguarantees. This raised the problem of designing provable algorithms that allow sparsity$\gg \sqrt{n}$ in the hidden vector $x$. The current paper designs algorithmsthat allow sparsity up to $n/poly(\log n)$. It works for a class of matriceswhere features are individually recoverable, a new notion identified in thispaper that may motivate further work. The algorithm runs in quasipolynomial time because they use limitedenumeration.
arxiv-5700-218 | Natural Language Processing in Biomedicine: A Unified System Architecture Overview | http://arxiv.org/pdf/1401.0569v2.pdf | author:Son Doan, Mike Conway, Tu Minh Phuong, Lucila Ohno-Machado category:cs.CL published:2014-01-03 summary:In modern electronic medical records (EMR) much of the clinically importantdata - signs and symptoms, symptom severity, disease status, etc. - are notprovided in structured data fields, but rather are encoded in cliniciangenerated narrative text. Natural language processing (NLP) provides a means of"unlocking" this important data source for applications in clinical decisionsupport, quality assurance, and public health. This chapter provides anoverview of representative NLP systems in biomedicine based on a unifiedarchitectural view. A general architecture in an NLP system consists of twomain components: background knowledge that includes biomedical knowledgeresources and a framework that integrates NLP tools to process text. Systemsdiffer in both components, which we will review briefly. Additionally,challenges facing current research efforts in biomedical NLP include thepaucity of large, publicly available annotated corpora, although initiativesthat facilitate data sharing, system evaluation, and collaborative work betweenresearchers in clinical NLP are starting to emerge.
arxiv-5700-219 | Data Smashing | http://arxiv.org/pdf/1401.0742v1.pdf | author:Ishanu Chattopadhyay, Hod Lipson category:cs.LG cs.AI cs.CE cs.IT math.IT stat.ML published:2014-01-03 summary:Investigation of the underlying physics or biology from empirical datarequires a quantifiable notion of similarity - when do two observed data setsindicate nearly identical generating processes, and when they do not. Thediscriminating characteristics to look for in data is often determined byheuristics designed by experts, $e.g.$, distinct shapes of "folded" lightcurvesmay be used as "features" to classify variable stars, while determination ofpathological brain states might require a Fourier analysis of brainwaveactivity. Finding good features is non-trivial. Here, we propose a universalsolution to this problem: we delineate a principle for quantifying similaritybetween sources of arbitrary data streams, without a priori knowledge, featuresor training. We uncover an algebraic structure on a space of symbolic modelsfor quantized data, and show that such stochastic generators may be added anduniquely inverted; and that a model and its inverse always sum to the generatorof flat white noise. Therefore, every data stream has an anti-stream: datagenerated by the inverse model. Similarity between two streams, then, is thedegree to which one, when summed to the other's anti-stream, mutuallyannihilates all statistical structure to noise. We call this data smashing. Wepresent diverse applications, including disambiguation of brainwaves pertainingto epileptic seizures, detection of anomalous cardiac rhythms, andclassification of astronomical objects from raw photometry. In our examples,the data smashing principle, without access to any domain knowledge, meets orexceeds the performance of specialized algorithms tuned by domain experts.
arxiv-5700-220 | Quantitative methods for Phylogenetic Inference in Historical Linguistics: An experimental case study of South Central Dravidian | http://arxiv.org/pdf/1401.0708v1.pdf | author:Taraka Rama, Sudheer Kolachina, Lakshmi Bai B category:cs.CL cs.AI published:2014-01-03 summary:In this paper we examine the usefulness of two classes of algorithms DistanceMethods, Discrete Character Methods (Felsenstein and Felsenstein 2003) widelyused in genetics, for predicting the family relationships among a set ofrelated languages and therefore, diachronic language change. Applying thesealgorithms to the data on the numbers of shared cognates- with-change andchanged as well as unchanged cognates for a group of six languages belonging toa Dravidian language sub-family given in Krishnamurti et al. (1983), weobserved that the resultant phylogenetic trees are largely in agreement withthe linguistic family tree constructed using the comparative method ofreconstruction with only a few minor differences. Furthermore, we studied theseminor differences and found that they were cases of genuine ambiguity even fora well-trained historical linguist. We evaluated the trees obtained through ourexperiments using a well-defined criterion and report the results here. Wefinally conclude that quantitative methods like the ones we examined are quiteuseful in predicting family relationships among languages. In addition, weconclude that a modest degree of confidence attached to the intuition thatthere could indeed exist a parallelism between the processes of linguistic andgenetic change is not totally misplaced.
arxiv-5700-221 | Adaptive-Rate Compressive Sensing Using Side Information | http://arxiv.org/pdf/1401.0583v1.pdf | author:Garrett Warnell, Sourabh Bhattacharya, Rama Chellappa, Tamer Basar category:cs.CV published:2014-01-03 summary:We provide two novel adaptive-rate compressive sensing (CS) strategies forsparse, time-varying signals using side information. Our first method utilizesextra cross-validation measurements, and the second one exploits extralow-resolution measurements. Unlike the majority of current CS techniques, wedo not assume that we know an upper bound on the number of significantcoefficients that comprise the images in the video sequence. Instead, we usethe side information to predict the number of significant coefficients in thesignal at the next time instant. For each image in the video sequence, ourtechniques specify a fixed number of spatially-multiplexed CS measurements toacquire, and adjust this quantity from image to image. Our strategies aredeveloped in the specific context of background subtraction for surveillancevideo, and we experimentally validate the proposed methods on real videosequences.
arxiv-5700-222 | Plurals: individuals and sets in a richly typed semantics | http://arxiv.org/pdf/1401.0660v1.pdf | author:Bruno Mery, Richard Moot, Christian Retoré category:cs.CL published:2014-01-03 summary:We developed a type-theoretical framework for natural lan- guage semanticsthat, in addition to the usual Montagovian treatment of compositionalsemantics, includes a treatment of some phenomena of lex- ical semantic:coercions, meaning, transfers, (in)felicitous co-predication. In this settingwe see how the various readings of plurals (collective, dis- tributive,coverings,...) can be modelled.
arxiv-5700-223 | What is usual in unusual videos? Trajectory snippet histograms for discovering unusualness | http://arxiv.org/pdf/1401.0730v2.pdf | author:Ahmet Iscen, Anil Armagan, Pinar Duygulu category:cs.CV published:2014-01-03 summary:Unusual events are important as being possible indicators of undesiredconsequences. Moreover, unusualness in everyday life activities may also beamusing to watch as proven by the popularity of such videos shared in socialmedia. Discovery of unusual events in videos is generally attacked as a problemof finding usual patterns, and then separating the ones that do not resemble tothose. In this study, we address the problem from the other side, and try toanswer what type of patterns are shared among unusual videos that make themresemble to each other regardless of the ongoing event. With this challengingproblem at hand, we propose a novel descriptor to encode the rapid motions invideos utilizing densely extracted trajectories. The proposed descriptor, whichis referred to as trajectory snipped histograms, is used to distinguish unusualvideos from usual videos, and further exploited to discover snapshots in whichunusualness happen. Experiments on domain specific people falling videos andunrestricted funny videos show the effectiveness of our method in capturingunusualness.
arxiv-5700-224 | ConceptVision: A Flexible Scene Classification Framework | http://arxiv.org/pdf/1401.0733v2.pdf | author:Ahmet Iscen, Eren Golge, Ilker Sarac, Pinar Duygulu category:cs.CV published:2014-01-03 summary:We introduce ConceptVision, a method that aims for high accuracy incategorizing large number of scenes, while keeping the model relatively simplerand efficient for scalability. The proposed method combines the advantages ofboth low-level representations and high-level semantic categories, andeliminates the distinctions between different levels through the definition ofconcepts. The proposed framework encodes the perspectives brought throughdifferent concepts by considering them in concept groups. Differentperspectives are ensembled for the final decision. Extensive experiments arecarried out on benchmark datasets to test the effects of different concepts,and methods used to ensemble. Comparisons with state-of-the-art studies showthat we can achieve better results with incorporation of concepts in differentlevels with different perspectives.
arxiv-5700-225 | Particle Gibbs with Ancestor Sampling | http://arxiv.org/pdf/1401.0604v1.pdf | author:Fredrik Lindsten, Michael I. Jordan, Thomas B. Schön category:stat.CO stat.ML published:2014-01-03 summary:Particle Markov chain Monte Carlo (PMCMC) is a systematic way of combiningthe two main tools used for Monte Carlo statistical inference: sequential MonteCarlo (SMC) and Markov chain Monte Carlo (MCMC). We present a novel PMCMCalgorithm that we refer to as particle Gibbs with ancestor sampling (PGAS).PGAS provides the data analyst with an off-the-shelf class of Markov kernelsthat can be used to simulate the typically high-dimensional and highlyautocorrelated state trajectory in a state-space model. The ancestor samplingprocedure enables fast mixing of the PGAS kernel even when using seemingly fewparticles in the underlying SMC sampler. This is important as it cansignificantly reduce the computational burden that is typically associated withusing SMC. PGAS is conceptually similar to the existing PG with backwardsimulation (PGBS) procedure. Instead of using separate forward and backwardsweeps as in PGBS, however, we achieve the same effect in a single forwardsweep. This makes PGAS well suited for addressing inference problems not onlyin state-space models, but also in models with more complex dependencies, suchas non-Markovian, Bayesian nonparametric, and general probabilistic graphicalmodels.
arxiv-5700-226 | Computing Entropy Rate Of Symbol Sources & A Distribution-free Limit Theorem | http://arxiv.org/pdf/1401.0711v2.pdf | author:Ishanu Chattopadhyay, Hod Lipson category:cs.IT cs.LG math.IT math.PR stat.CO stat.ML published:2014-01-03 summary:Entropy rate of sequential data-streams naturally quantifies the complexityof the generative process. Thus entropy rate fluctuations could be used as atool to recognize dynamical perturbations in signal sources, and couldpotentially be carried out without explicit background noise characterization.However, state of the art algorithms to estimate the entropy rate have markedlyslow convergence; making such entropic approaches non-viable in practice. Wepresent here a fundamentally new approach to estimate entropy rates, which isdemonstrated to converge significantly faster in terms of input data lengths,and is shown to be effective in diverse applications ranging from theestimation of the entropy rate of English texts to the estimation of complexityof chaotic dynamical systems. Additionally, the convergence rate of entropyestimates do not follow from any standard limit theorem, and reportedalgorithms fail to provide any confidence bounds on the computed values.Exploiting a connection to the theory of probabilistic automata, we establish aconvergence rate of $O(\log \vert s \vert/\sqrt[3]{\vert s \vert})$ as afunction of the input length $\vert s \vert$, which then yields explicituncertainty estimates, as well as required data lengths to satisfypre-specified confidence bounds.
arxiv-5700-227 | Multi-Topic Multi-Document Summarizer | http://arxiv.org/pdf/1401.0640v1.pdf | author:Fatma El-Ghannam, Tarek El-Shishtawy category:cs.CL published:2014-01-03 summary:Current multi-document summarization systems can successfully extract summarysentences, however with many limitations including: low coverage, inaccurateextraction to important sentences, redundancy and poor coherence among theselected sentences. The present study introduces a new concept of centroidapproach and reports new techniques for extracting summary sentences formulti-document. In both techniques keyphrases are used to weigh sentences anddocuments. The first summarization technique (Sen-Rich) prefers maximumrichness sentences. While the second (Doc-Rich), prefers sentences fromcentroid document. To demonstrate the new summarization system application toextract summaries of Arabic documents we performed two experiments. First, weapplied Rouge measure to compare the new techniques among systems presented atTAC2011. The results show that Sen-Rich outperformed all systems in ROUGE-S.Second, the system was applied to summarize multi-topic documents. Using humanevaluators, the results show that Doc-Rich is the superior, where summarysentences characterized by extra coverage and more cohesion.
arxiv-5700-228 | Reducing the Computational Cost in Multi-objective Evolutionary Algorithms by Filtering Worthless Individuals | http://arxiv.org/pdf/1401.5808v1.pdf | author:Zahra Pourbahman, Ali Hamzeh category:cs.NE published:2014-01-02 summary:The large number of exact fitness function evaluations makes evolutionaryalgorithms to have computational cost. In some real-world problems, reducingnumber of these evaluations is much more valuable even by increasingcomputational complexity and spending more time. To fulfill this target, weintroduce an effective factor, in spite of applied factor in Adaptive FuzzyFitness Granulation with Non-dominated Sorting Genetic Algorithm-II, to filterout worthless individuals more precisely. Our proposed approach is comparedwith respect to Adaptive Fuzzy Fitness Granulation with Non-dominated SortingGenetic Algorithm-II, using the Hyper volume and the Inverted GenerationalDistance performance measures. The proposed method is applied to 1 traditionaland 1 state-of-the-art benchmarks with considering 3 different dimensions. Froman average performance view, the results indicate that although decreasing thenumber of fitness evaluations leads to have performance reduction but it is nottangible compared to what we gain.
arxiv-5700-229 | A Hybrid NN/HMM Modeling Technique for Online Arabic Handwriting Recognition | http://arxiv.org/pdf/1401.0486v1.pdf | author:Najiba Tagougui, Houcine Boubaker, Monji Kherallah, Adel M. ALIMI category:cs.CV published:2014-01-02 summary:In this work we propose a hybrid NN/HMM model for online Arabic handwritingrecognition. The proposed system is based on Hidden Markov Models (HMMs) andMulti Layer Perceptron Neural Networks (MLPNNs). The input signal is segmentedto continuous strokes called segments based on the Beta-Elliptical strategy byinspecting the extremum points of the curvilinear velocity profile. A neuralnetwork trained with segment level contextual information is used to extractclass character probabilities. The output of this network is decoded by HMMs toprovide character level recognition. In evaluations on the ADAB database, weachieved 96.4% character recognition accuracy that is statisticallysignificantly important in comparison with character recognition accuraciesobtained from state-of-the-art online Arabic systems.8
arxiv-5700-230 | Solving Poisson Equation by Genetic Algorithms | http://arxiv.org/pdf/1401.0523v1.pdf | author:Khalid Jebari, Mohammed Madiafi, Abdelaziz El Moujahid category:cs.NE published:2014-01-02 summary:This paper deals with a method for solving Poisson Equation (PE) based ongenetic algorithms and grammatical evolution. The method forms generations ofsolutions expressed in an analytical form. Several examples of PE are testedand in most cases the exact solution is recovered. But, when the solutioncannot be expressed in an analytical form, our method produces a satisfactorysolution with a good level of accuracy
arxiv-5700-231 | Machine Assisted Authentication of Paper Currency: an Experiment on Indian Banknotes | http://arxiv.org/pdf/1401.0689v5.pdf | author:Ankush Roy, Biswajit Halder, Utpal Garain, David S. Doermann category:cs.CV published:2014-01-02 summary:Automatic authentication of paper money has been targeted. Indian bank notesare taken as reference to show how a system can be developed for discriminatingfake notes from genuine ones. Image processing and pattern recognitiontechniques are used to design the overall approach. The ability of the embeddedsecurity aspects is thoroughly analysed for detecting fake currencies. Realforensic samples are involved in the experiment that shows a high precisionmachine can be developed for authentication of paper money. The systemperformance is reported in terms of both accuracy and processing speed.Comparison with human subjects namely forensic experts and bank staffs clearlyshows its applicability for mass checking of currency notes in the real world.The analysis of security features to protect counterfeiting highlights somefacts that should be taken care of in future designing of currency notes.
arxiv-5700-232 | Hybrid Approach to Face Recognition System using Principle component and Independent component with score based fusion process | http://arxiv.org/pdf/1401.0395v1.pdf | author:Trupti M. Kodinariya category:cs.CV published:2014-01-02 summary:Hybrid approach has a special status among Face Recognition Systems as theycombine different recognition approaches in an either serial or parallel toovercome the shortcomings of individual methods. This paper explores the areaof Hybrid Face Recognition using score based strategy as a combiner/fusionprocess. In proposed approach, the recognition system operates in two modes:training and classification. Training mode involves normalization of the faceimages (training set), extracting appropriate features using PrincipleComponent Analysis (PCA) and Independent Component Analysis (ICA). Theextracted features are then trained in parallel using Back-propagation neuralnetworks (BPNNs) to partition the feature space in to different face classes.In classification mode, the trained PCA BPNN and ICA BPNN are fed with new faceimage(s). The score based strategy which works as a combiner is applied to theresults of both PCA BPNN and ICA BPNN to classify given new face image(s)according to face classes obtained during the training mode. The proposedapproach has been tested on ORL and other face databases; the experimentedresults show that the proposed system has higher accuracy than face recognitionsystems using single feature extractor.
arxiv-5700-233 | Structured Generative Models of Natural Source Code | http://arxiv.org/pdf/1401.0514v2.pdf | author:Chris J. Maddison, Daniel Tarlow category:cs.PL cs.LG stat.ML published:2014-01-02 summary:We study the problem of building generative models of natural source code(NSC); that is, source code written and understood by humans. Our primarycontribution is to describe a family of generative models for NSC that havethree key properties: First, they incorporate both sequential and hierarchicalstructure. Second, we learn a distributed representation of source codeelements. Finally, they integrate closely with a compiler, which allowsleveraging compiler logic and abstractions when building structure into themodel. We also develop an extension that includes more complex structure,refining how the model generates identifier tokens based on what variables arecurrently in scope. Our models can be learned efficiently, and we showempirically that including appropriate structure greatly improves the models,measured by the probability of generating test programs.
arxiv-5700-234 | Generalization Bounds for Representative Domain Adaptation | http://arxiv.org/pdf/1401.0376v1.pdf | author:Chao Zhang, Lei Zhang, Wei Fan, Jieping Ye category:cs.LG stat.ML published:2014-01-02 summary:In this paper, we propose a novel framework to analyze the theoreticalproperties of the learning process for a representative type of domainadaptation, which combines data from multiple sources and one target (orbriefly called representative domain adaptation). In particular, we use theintegral probability metric to measure the difference between the distributionsof two domains and meanwhile compare it with the H-divergence and thediscrepancy distance. We develop the Hoeffding-type, the Bennett-type and theMcDiarmid-type deviation inequalities for multiple domains respectively, andthen present the symmetrization inequality for representative domainadaptation. Next, we use the derived inequalities to obtain the Hoeffding-typeand the Bennett-type generalization bounds respectively, both of which arebased on the uniform entropy number. Moreover, we present the generalizationbounds based on the Rademacher complexity. Finally, we analyze the asymptoticconvergence and the rate of convergence of the learning process forrepresentative domain adaptation. We discuss the factors that affect theasymptotic behavior of the learning process and the numerical experimentssupport our theoretical findings as well. Meanwhile, we give a comparison withthe existing results of domain adaptation and the classical results under thesame-distribution assumption.
arxiv-5700-235 | EigenGP: Gaussian Process Models with Adaptive Eigenfunctions | http://arxiv.org/pdf/1401.0362v3.pdf | author:Hao Peng, Yuan Qi category:cs.LG published:2014-01-02 summary:Gaussian processes (GPs) provide a nonparametric representation of functions.However, classical GP inference suffers from high computational cost for bigdata. In this paper, we propose a new Bayesian approach, EigenGP, that learnsboth basis dictionary elements--eigenfunctions of a GP prior--and priorprecisions in a sparse finite model. It is well known that, among allorthogonal basis functions, eigenfunctions can provide the most compactrepresentation. Unlike other sparse Bayesian finite models where the basisfunction has a fixed form, our eigenfunctions live in a reproducing kernelHilbert space as a finite linear combination of kernel functions. We learn thedictionary elements--eigenfunctions--and the prior precisions over theseelements as well as all the other hyperparameters from data by maximizing themodel marginal likelihood. We explore computational linear algebra to simplifythe gradient computation significantly. Our experimental results demonstrateimproved predictive performance of EigenGP over alternative sparse GP methodsas well as relevance vector machine.
arxiv-5700-236 | Low-Complexity Particle Swarm Optimization for Time-Critical Applications | http://arxiv.org/pdf/1401.0546v1.pdf | author:Muhammad Saqib Sohail, Muhammad Omer Bin Saeed, Syed Zeeshan Rizvi, Mobien Shoaib, Asrar Ul Haq Sheikh category:cs.NE published:2014-01-02 summary:Particle swam optimization (PSO) is a popular stochastic optimization methodthat has found wide applications in diverse fields. However, PSO suffers fromhigh computational complexity and slow convergence speed. High computationalcomplexity hinders its use in applications that have limited power resourceswhile slow convergence speed makes it unsuitable for time criticalapplications. In this paper, we propose two techniques to overcome theselimitations. The first technique reduces the computational complexity of PSOwhile the second technique speeds up its convergence. These techniques can beapplied, either separately or in conjunction, to any existing PSO variant. Theproposed techniques are robust to the number of dimensions of the optimizationproblem. Simulation results are presented for the proposed techniques appliedto the standard PSO as well as to several PSO variants. The results show thatthe use of both these techniques in conjunction results in a reduction in thenumber of computations required as well as faster convergence speed whilemaintaining an acceptable error performance for time-critical applications.
arxiv-5700-237 | Convex optimization on Banach Spaces | http://arxiv.org/pdf/1401.0334v1.pdf | author:R. A. DeVore, V. N. Temlyakov category:stat.ML math.OC published:2014-01-01 summary:Greedy algorithms which use only function evaluations are applied to convexoptimization in a general Banach space $X$. Along with algorithms that useexact evaluations, algorithms with approximate evaluations are treated. Apriori upper bounds for the convergence rate of the proposed algorithms aregiven. These bounds depend on the smoothness of the objective function and thesparsity or compressibility (with respect to a given dictionary) of a point in$X$ where the minimum is attained.
arxiv-5700-238 | Modeling Attractiveness and Multiple Clicks in Sponsored Search Results | http://arxiv.org/pdf/1401.0255v1.pdf | author:Dinesh Govindaraj, Tao Wang, S. V. N. Vishwanathan category:cs.IR cs.LG published:2014-01-01 summary:Click models are an important tool for leveraging user feedback, and are usedby commercial search engines for surfacing relevant search results. However,existing click models are lacking in two aspects. First, they do not shareinformation across search results when computing attractiveness. Second, theyassume that users interact with the search results sequentially. Based on ouranalysis of the click logs of a commercial search engine, we observe that thesequential scan assumption does not always hold, especially for sponsoredsearch results. To overcome the above two limitations, we propose a new clickmodel. Our key insight is that sharing information across search results helpsin identifying important words or key-phrases which can then be used toaccurately compute attractiveness of a search result. Furthermore, we arguethat the click probability of a position as well as its attractiveness changesduring a user session and depends on the user's past click experience. Ourmodel seamlessly incorporates the effect of externalities (quality of othersearch results displayed in response to a user query), user fatigue, as well aspre and post-click relevance of a sponsored search result. We propose anefficient one-pass inference scheme and empirically evaluate the performance ofour model via extensive experiments using the click logs of a large commercialsearch engine.
arxiv-5700-239 | Learning without Concentration | http://arxiv.org/pdf/1401.0304v2.pdf | author:Shahar Mendelson category:cs.LG stat.ML published:2014-01-01 summary:We obtain sharp bounds on the performance of Empirical Risk Minimizationperformed in a convex class and with respect to the squared loss, withoutassuming that class members and the target are bounded functions or haverapidly decaying tails. Rather than resorting to a concentration-based argument, the method used hererelies on a `small-ball' assumption and thus holds for classes consisting ofheavy-tailed functions and for heavy-tailed targets. The resulting estimates scale correctly with the `noise level' of theproblem, and when applied to the classical, bounded scenario, always improvethe known bounds.
arxiv-5700-240 | Robust Hierarchical Clustering | http://arxiv.org/pdf/1401.0247v2.pdf | author:Maria-Florina Balcan, Yingyu Liang, Pramod Gupta category:cs.LG cs.DS published:2014-01-01 summary:One of the most widely used techniques for data clustering is agglomerativeclustering. Such algorithms have been long used across many different fieldsranging from computational biology to social sciences to computer vision inpart because their output is easy to interpret. Unfortunately, it is wellknown, however, that many of the classic agglomerative clustering algorithmsare not robust to noise. In this paper we propose and analyze a new robustalgorithm for bottom-up agglomerative clustering. We show that our algorithmcan be used to cluster accurately in cases where the data satisfies a number ofnatural properties and where the traditional agglomerative algorithms fail. Wealso show how to adapt our algorithm to the inductive setting where our givendata is only a small random sample of the entire data set. Experimentalevaluations on synthetic and real world data sets show that our algorithmachieves better performance than other hierarchical algorithms in the presenceof noise.
arxiv-5700-241 | Feature Augmentation via Nonparametrics and Selection (FANS) in High Dimensional Classification | http://arxiv.org/pdf/1401.0211v2.pdf | author:Jianqing Fan, Yang Feng, Jiancheng Jiang, Xin Tong category:stat.ME math.ST stat.AP stat.ML stat.TH published:2013-12-31 summary:We propose a high dimensional classification method that involvesnonparametric feature augmentation. Knowing that marginal density ratios arethe most powerful univariate classifiers, we use the ratio estimates totransform the original feature measurements. Subsequently, penalized logisticregression is invoked, taking as input the newly transformed or augmentedfeatures. This procedure trains models equipped with local complexity andglobal simplicity, thereby avoiding the curse of dimensionality while creatinga flexible nonlinear decision boundary. The resulting method is called FeatureAugmentation via Nonparametrics and Selection (FANS). We motivate FANS bygeneralizing the Naive Bayes model, writing the log ratio of joint densities asa linear combination of those of marginal densities. It is related togeneralized additive models, but has better interpretability and computability.Risk bounds are developed for FANS. In numerical analysis, FANS is comparedwith competing methods, so as to provide a guideline on its best applicationdomain. Real data analysis demonstrates that FANS performs very competitivelyon benchmark email spam and gene expression data sets. Moreover, FANS isimplemented by an extremely fast algorithm through parallel computing.
arxiv-5700-242 | Sparse Recovery with Very Sparse Compressed Counting | http://arxiv.org/pdf/1401.0201v1.pdf | author:Ping Li, Cun-Hui Zhang, Tong Zhang category:stat.ME cs.DS cs.IT cs.LG math.IT published:2013-12-31 summary:Compressed sensing (sparse signal recovery) often encounters nonnegative data(e.g., images). Recently we developed the methodology of using (dense)Compressed Counting for recovering nonnegative K-sparse signals. In this paper,we adopt very sparse Compressed Counting for nonnegative signal recovery. Ourdesign matrix is sampled from a maximally-skewed p-stable distribution (0<p<1),and we sparsify the design matrix so that on average (1-g)-fraction of theentries become zero. The idea is related to very sparse stable randomprojections (Li et al 2006 and Li 2007), the prior work for estimating summarystatistics of the data. In our theoretical analysis, we show that, when p->0, it suffices to use M=K/(1-exp(-gK) log N measurements, so that all coordinates can be recovered inone scan of the coordinates. If g = 1 (i.e., dense design), then M = K log N.If g= 1/K or 2/K (i.e., very sparse design), then M = 1.58K log N or M = 1.16Klog N. This means the design matrix can be indeed very sparse at only a minorinflation of the sample complexity. Interestingly, as p->1, the required number of measurements is essentially M= 2.7K log N, provided g= 1/K. It turns out that this result is a generalworst-case bound.
arxiv-5700-243 | Medical Image Fusion: A survey of the state of the art | http://arxiv.org/pdf/1401.0166v1.pdf | author:A. P. James, B. V. Dasarathy category:cs.CV cs.AI physics.med-ph published:2013-12-31 summary:Medical image fusion is the process of registering and combining multipleimages from single or multiple imaging modalities to improve the imagingquality and reduce randomness and redundancy in order to increase the clinicalapplicability of medical images for diagnosis and assessment of medicalproblems. Multi-modal medical image fusion algorithms and devices have shownnotable achievements in improving clinical accuracy of decisions based onmedical images. This review article provides a factual listing of methods andsummarizes the broad scientific challenges faced in the field of medical imagefusion. We characterize the medical image fusion research based on (1) thewidely used image fusion methods, (2) imaging modalities, and (3) imaging oforgans that are under study. This review concludes that even though thereexists several open ended technological and scientific challenges, the fusionof medical images has proved to be useful for advancing the clinicalreliability of using medical imaging for medical diagnostics and analysis, andis a scientific discipline that has the potential to significantly grow in thecoming years.
arxiv-5700-244 | Speeding-Up Convergence via Sequential Subspace Optimization: Current State and Future Directions | http://arxiv.org/pdf/1401.0159v1.pdf | author:Michael Zibulevsky category:cs.NA cs.LG published:2013-12-31 summary:This is an overview paper written in style of research proposal. In recentyears we introduced a general framework for large-scale unconstrainedoptimization -- Sequential Subspace Optimization (SESOP) and demonstrated itsusefulness for sparsity-based signal/image denoising, deconvolution,compressive sensing, computed tomography, diffraction imaging, support vectormachines. We explored its combination with Parallel Coordinate Descent andSeparable Surrogate Function methods, obtaining state of the art results inabove-mentioned areas. There are several methods, that are faster than plainSESOP under specific conditions: Trust region Newton method - for problems witheasily invertible Hessian matrix; Truncated Newton method - when fastmultiplication by Hessian is available; Stochastic optimization methods - forproblems with large stochastic-type data; Multigrid methods - for problems withnested multilevel structure. Each of these methods can be further improved bymerge with SESOP. One can also accelerate Augmented Lagrangian method forconstrained optimization problems and Alternating Direction Method ofMultipliers for problems with separable objective function and non-separableconstraints.
arxiv-5700-245 | A Novel Approach For Generating Face Template Using Bda | http://arxiv.org/pdf/1401.0092v1.pdf | author:Shraddha S. Shinde, Prof. Anagha P. Khedkar category:cs.CV published:2013-12-31 summary:In identity management system, commonly used biometric recognition systemneeds attention towards issue of biometric template protection as far as morereliable solution is concerned. In view of this biometric template protectionalgorithm should satisfy security, discriminability and cancelability. As nosingle template protection method is capable of satisfying the basicrequirements, a novel technique for face template generation and protection isproposed. The novel approach is proposed to provide security and accuracy innew user enrollment as well as authentication process. This novel techniquetakes advantage of both the hybrid approach and the binary discriminantanalysis algorithm. This algorithm is designed on the basis of randomprojection, binary discriminant analysis and fuzzy commitment scheme. Threepublicly available benchmark face databases are used for evaluation. Theproposed novel technique enhances the discriminability and recognition accuracyby 80% in terms of matching score of the face images and provides highsecurity.
arxiv-5700-246 | PSO-MISMO Modeling Strategy for Multi-Step-Ahead Time Series Prediction | http://arxiv.org/pdf/1401.0104v1.pdf | author:Yukun Bao, Tao Xiong, Zhongyi Hu category:cs.AI cs.LG cs.NE stat.ML published:2013-12-31 summary:Multi-step-ahead time series prediction is one of the most challengingresearch topics in the field of time series modeling and prediction, and iscontinually under research. Recently, the multiple-input severalmultiple-outputs (MISMO) modeling strategy has been proposed as a promisingalternative for multi-step-ahead time series prediction, exhibiting advantagescompared with the two currently dominating strategies, the iterated and thedirect strategies. Built on the established MISMO strategy, this study proposesa particle swarm optimization (PSO)-based MISMO modeling strategy, which iscapable of determining the number of sub-models in a self-adaptive mode, withvarying prediction horizons. Rather than deriving crisp divides with equal-sizes prediction horizons from the established MISMO, the proposed PSO-MISMOstrategy, implemented with neural networks, employs a heuristic to createflexible divides with varying sizes of prediction horizons and to generatecorresponding sub-models, providing considerable flexibility in modelconstruction, which has been validated with simulated and real datasets.
arxiv-5700-247 | Controlled Sparsity Kernel Learning | http://arxiv.org/pdf/1401.0116v1.pdf | author:Dinesh Govindaraj, Raman Sankaran, Sreedal Menon, Chiranjib Bhattacharyya category:cs.LG published:2013-12-31 summary:Multiple Kernel Learning(MKL) on Support Vector Machines(SVMs) has been apopular front of research in recent times due to its success in applicationproblems like Object Categorization. This success is due to the fact that MKLhas the ability to choose from a variety of feature kernels to identify theoptimal kernel combination. But the initial formulation of MKL was only able toselect the best of the features and misses out many other informative kernelspresented. To overcome this, the Lp norm based formulation was proposed byKloft et. al. This formulation is capable of choosing a non-sparse set ofkernels through a control parameter p. Unfortunately, the parameter p does nothave a direct meaning to the number of kernels selected. We have observed thatstricter control over the number of kernels selected gives us an edge overthese techniques in terms of accuracy of classification and also helps us tofine tune the algorithms to the time requirements at hand. In this work, wepropose a Controlled Sparsity Kernel Learning (CSKL) formulation that canstrictly control the number of kernels which we wish to select. The CSKLformulation introduces a parameter t which directly corresponds to the numberof kernels selected. It is important to note that a search in t space is finiteand fast as compared to p. We have also provided an efficient Reduced GradientDescent based algorithm to solve the CSKL formulation, which is proven toconverge. Through our experiments on the Caltech101 Object Categorizationdataset, we have also shown that one can achieve better accuracies than theprevious formulations through the right choice of t.
arxiv-5700-248 | Forward-Backward Greedy Algorithms for General Convex Smooth Functions over A Cardinality Constraint | http://arxiv.org/pdf/1401.0086v2.pdf | author:Ji Liu, Ryohei Fujimaki, Jieping Ye category:stat.ML published:2013-12-31 summary:We consider forward-backward greedy algorithms for solving sparse featureselection problems with general convex smooth functions. A state-of-the-artgreedy method, the Forward-Backward greedy algorithm (FoBa-obj) requires tosolve a large number of optimization problems, thus it is not scalable forlarge-size problems. The FoBa-gdt algorithm, which uses the gradientinformation for feature selection at each forward iteration, significantlyimproves the efficiency of FoBa-obj. In this paper, we systematically analyzethe theoretical properties of both forward-backward greedy algorithms. Our maincontributions are: 1) We derive better theoretical bounds than existinganalyses regarding FoBa-obj for general smooth convex functions; 2) We showthat FoBa-gdt achieves the same theoretical performance as FoBa-obj under thesame condition: restricted strong convexity condition. Our new bounds areconsistent with the bounds of a special case (least squares) and fills apreviously existing theoretical gap for general convex smooth functions; 3) Weshow that the restricted strong convexity condition is satisfied if the numberof independent samples is more than $\bar{k}\log d$ where $\bar{k}$ is thesparsity number and $d$ is the dimension of the variable; 4) We apply FoBa-gdt(with the conditional random field objective) to the sensor selection problemfor human indoor activity recognition and our results show that FoBa-gdtoutperforms other methods (including the ones based on forward greedy selectionand L1-regularization).
arxiv-5700-249 | Black Box Variational Inference | http://arxiv.org/pdf/1401.0118v1.pdf | author:Rajesh Ranganath, Sean Gerrish, David M. Blei category:stat.ML cs.LG stat.CO stat.ME published:2013-12-31 summary:Variational inference has become a widely used method to approximateposteriors in complex latent variables models. However, deriving a variationalinference algorithm generally requires significant model-specific analysis, andthese efforts can hinder and deter us from quickly developing and exploring avariety of models for a problem at hand. In this paper, we present a "blackbox" variational inference algorithm, one that can be quickly applied to manymodels with little additional derivation. Our method is based on a stochasticoptimization of the variational objective where the noisy gradient is computedfrom Monte Carlo samples from the variational distribution. We develop a numberof methods to reduce the variance of the gradient, always maintaining thecriterion that we want to avoid difficult model-based derivations. We evaluateour method against the corresponding black box sampling based methods. We findthat our method reaches better predictive likelihoods much faster than samplingmethods. Finally, we demonstrate that Black Box Variational Inference lets useasily explore a wide space of models by quickly constructing and evaluatingseveral models of longitudinal healthcare data.
arxiv-5700-250 | System Analysis And Design For Multimedia Retrieval Systems | http://arxiv.org/pdf/1401.0131v1.pdf | author:Avinash N Bhute, B B Meshram category:cs.IR cs.CV cs.MM published:2013-12-31 summary:Due to the extensive use of information technology and the recentdevelopments in multimedia systems, the amount of multimedia data available tousers has increased exponentially. Video is an example of multimedia data as itcontains several kinds of data such as text, image, meta-data, visual andaudio. Content based video retrieval is an approach for facilitating thesearching and browsing of large multimedia collections over WWW. In order tocreate an effective video retrieval system, visual perception must be takeninto account. We conjectured that a technique which employs multiple featuresfor indexing and retrieval would be more effective in the discrimination andsearch tasks of videos. In order to validate this, content based indexing andretrieval systems were implemented using color histogram, Texture feature(GLCM), edge density and motion..
arxiv-5700-251 | The combinatorial structure of beta negative binomial processes | http://arxiv.org/pdf/1401.0062v3.pdf | author:Creighton Heaukulani, Daniel M. Roy category:math.ST math.PR stat.ML stat.TH published:2013-12-31 summary:We characterize the combinatorial structure of conditionally-i.i.d. sequencesof negative binomial processes with a common beta process base measure. InBayesian nonparametric applications, such processes have served as models forlatent multisets of features underlying data. Analogously, random subsets arisefrom conditionally-i.i.d. sequences of Bernoulli processes with a common betaprocess base measure, in which case the combinatorial structure is described bythe Indian buffet process. Our results give a count analogue of the Indianbuffet process, which we call a negative binomial Indian buffet process. As anintermediate step toward this goal, we provide a construction for the betanegative binomial process that avoids a representation of the underlying betaprocess base measure. We describe the key Markov kernels needed to use a NB-IBPrepresentation in a Markov Chain Monte Carlo algorithm targeting a posteriordistribution.
arxiv-5700-252 | Constrained Parametric Proposals and Pooling Methods for Semantic Segmentation in RGB-D Images | http://arxiv.org/pdf/1312.7715v2.pdf | author:Dan Banica, Cristian Sminchisescu category:cs.CV published:2013-12-30 summary:We focus on the problem of semantic segmentation based on RGB-D data, withemphasis on analyzing cluttered indoor scenes containing many instances frommany visual categories. Our approach is based on a parametric figure-groundintensity and depth-constrained proposal process that generates spatial layouthypotheses at multiple locations and scales in the image followed by asequential inference algorithm that integrates the proposals into a completescene estimate. Our contributions can be summarized as proposing the following:(1) a generalization of parametric max flow figure-ground proposal methodologyto take advantage of intensity and depth information, in order tosystematically and efficiently generate the breakpoints of an underlyingspatial model in polynomial time, (2) new region description methods based onsecond-order pooling over multiple features constructed using both intensityand depth channels, (3) an inference procedure that can resolve conflicts inoverlapping spatial partitions, and handles scenes with a large number ofobjects category instances, of very different scales, (4) extensive evaluationof the impact of depth, as well as the effectiveness of a large number ofdescriptors, both pre-designed and automatically obtained using deep learning,in a difficult RGB-D semantic segmentation problem with 92 classes. We reportstate of the art results in the challenging NYU Depth v2 dataset, extended forRMRC 2013 Indoor Segmentation Challenge, where currently the proposed modelranks first, with an average score of 24.61% and a number of 39 classes won.Moreover, we show that by combining second-order and deep learning features,over 15% relative accuracy improvements can be additionally achieved. In ascene classification benchmark, our methodology further improves the state ofthe art by 24%.
arxiv-5700-253 | Response-Based Approachability and its Application to Generalized No-Regret Algorithms | http://arxiv.org/pdf/1312.7658v1.pdf | author:Andrey Bernstein, Nahum Shimkin category:cs.LG cs.GT published:2013-12-30 summary:Approachability theory, introduced by Blackwell (1956), provides fundamentalresults on repeated games with vector-valued payoffs, and has been usefullyapplied since in the theory of learning in games and to learning algorithms inthe online adversarial setup. Given a repeated game with vector payoffs, atarget set $S$ is approachable by a certain player (the agent) if he can ensurethat the average payoff vector converges to that set no matter what hisadversary opponent does. Blackwell provided two equivalent sets of conditionsfor a convex set to be approachable. The first (primary) condition is ageometric separation condition, while the second (dual) condition requires thatthe set be {\em non-excludable}, namely that for every mixed action of theopponent there exists a mixed action of the agent (a {\em response}) such thatthe resulting payoff vector belongs to $S$. Existing approachability algorithmsrely on the primal condition and essentially require to compute at each stage aprojection direction from a given point to $S$. In this paper, we introduce anapproachability algorithm that relies on Blackwell's {\em dual} condition.Thus, rather than projection, the algorithm relies on computation of theresponse to a certain action of the opponent at each stage. The utility of theproposed algorithm is demonstrated by applying it to certain generalizations ofthe classical regret minimization problem, which include regret minimizationwith side constraints and regret minimization for global cost functions. Inthese problems, computation of the required projections is generally complexbut a response is readily obtainable.
arxiv-5700-254 | Approximating the Bethe partition function | http://arxiv.org/pdf/1401.0044v1.pdf | author:Adrian Weller, Tony Jebara category:cs.LG published:2013-12-30 summary:When belief propagation (BP) converges, it does so to a stationary point ofthe Bethe free energy $F$, and is often strikingly accurate. However, it mayconverge only to a local optimum or may not converge at all. An algorithm wasrecently introduced for attractive binary pairwise MRFs which is guaranteed toreturn an $\epsilon$-approximation to the global minimum of $F$ in polynomialtime provided the maximum degree $\Delta=O(\log n)$, where $n$ is the number ofvariables. Here we significantly improve this algorithm and derive severalresults including a new approach based on analyzing first derivatives of $F$,which leads to performance that is typically far superior and yields a fullypolynomial-time approximation scheme (FPTAS) for attractive models without anydegree restriction. Further, the method applies to general (non-attractive)models, though with no polynomial time guarantee in this case, leading to theimportant result that approximating $\log$ of the Bethe partition function,$\log Z_B=-\min F$, for a general model to additive $\epsilon$-accuracy may bereduced to a discrete MAP inference problem. We explore an application topredicting equipment failure on an urban power network and demonstrate that theBethe approximation can perform well even when BP fails to converge.
arxiv-5700-255 | Total variation regularization for manifold-valued data | http://arxiv.org/pdf/1312.7710v1.pdf | author:Andreas Weinmann, Laurent Demaret, Martin Storath category:math.OC cs.CV physics.med-ph published:2013-12-30 summary:We consider total variation minimization for manifold valued data. We proposea cyclic proximal point algorithm and a parallel proximal point algorithm tominimize TV functionals with $\ell^p$-type data terms in the manifold case.These algorithms are based on iterative geodesic averaging which makes themeasily applicable to a large class of data manifolds. As an application, weconsider denoising images which take their values in a manifold. We apply ouralgorithms to diffusion tensor images, interferometric SAR images as well assphere and cylinder valued images. For the class of Cartan-Hadamard manifolds(which includes the data space in diffusion tensor imaging) we show theconvergence of the proposed TV minimizing algorithms to a global minimizer.
arxiv-5700-256 | Consistent Bounded-Asynchronous Parameter Servers for Distributed ML | http://arxiv.org/pdf/1312.7869v2.pdf | author:Jinliang Wei, Wei Dai, Abhimanu Kumar, Xun Zheng, Qirong Ho, Eric P. Xing category:stat.ML cs.DC cs.LG published:2013-12-30 summary:In distributed ML applications, shared parameters are usually replicatedamong computing nodes to minimize network overhead. Therefore, properconsistency model must be carefully chosen to ensure algorithm's correctnessand provide high throughput. Existing consistency models used ingeneral-purpose databases and modern distributed ML systems are either tooloose to guarantee correctness of the ML algorithms or too strict and thus failto fully exploit the computing power of the underlying distributed system. Many ML algorithms fall into the category of \emph{iterative convergentalgorithms} which start from a randomly chosen initial point and converge tooptima by repeating iteratively a set of procedures. We've found that many suchalgorithms are to a bounded amount of inconsistency and still convergecorrectly. This property allows distributed ML to relax strict consistencymodels to improve system performance while theoretically guarantees algorithmiccorrectness. In this paper, we present several relaxed consistency models forasynchronous parallel computation and theoretically prove their algorithmiccorrectness. The proposed consistency models are implemented in a distributedparameter server and evaluated in the context of a popular ML application:topic modeling.
arxiv-5700-257 | Identification of structural features in chemicals associated with cancer drug response: A systematic data-driven analysis | http://arxiv.org/pdf/1312.7734v2.pdf | author:Suleiman A Khan, Seppo Virtanen, Olli P Kallioniemi, Krister Wennerberg, Antti Poso, Samuel Kaski category:stat.ML q-bio.GN stat.AP published:2013-12-30 summary:Motivation: Analysis of relationships of drug structure to biologicalresponse is key to understanding off-target and unexpected drug effects, andfor developing hypotheses on how to tailor drug thera-pies. New methods arerequired for integrated analyses of a large number of chemical features ofdrugs against the corresponding genome-wide responses of multiple cell models.Results: In this paper, we present the first comprehensive multi-set analysison how the chemical structure of drugs impacts on ge-nome-wide gene expressionacross several cancer cell lines (CMap database). The task is formulated assearching for drug response components across multiple cancers to reveal sharedeffects of drugs and the chemical features that may be responsible. Thecom-ponents can be computed with an extension of a very recent ap-proach calledGroup Factor Analysis (GFA). We identify 11 compo-nents that link thestructural descriptors of drugs with specific gene expression responsesobserved in the three cell lines, and identify structural groups that may beresponsible for the responses. Our method quantitatively outperforms thelimited earlier studies on CMap and identifies both the previously reportedassociations and several interesting novel findings, by taking into accountmultiple cell lines and advanced 3D structural descriptors. The novelobservations include: previously unknown similarities in the effects induced by15-delta prostaglandin J2 and HSP90 inhibitors, which are linked to the 3Ddescriptors of the drugs; and the induction by simvastatin of leukemia-specificanti-inflammatory response, resem-bling the effects of corticosteroids.
arxiv-5700-258 | Bayesian Models of Graphs, Arrays and Other Exchangeable Random Structures | http://arxiv.org/pdf/1312.7857v2.pdf | author:Peter Orbanz, Daniel M. Roy category:math.ST stat.ML stat.TH published:2013-12-30 summary:The natural habitat of most Bayesian methods is data represented byexchangeable sequences of observations, for which de Finetti's theorem providesthe theoretical foundation. Dirichlet process clustering, Gaussian processregression, and many other parametric and nonparametric Bayesian models fallwithin the remit of this framework; many problems arising in modern dataanalysis do not. This article provides an introduction to Bayesian models ofgraphs, matrices, and other data that can be modeled by random structures. Wedescribe results in probability theory that generalize de Finetti's theorem tosuch data and discuss their relevance to nonparametric Bayesian modeling. Withthe basic ideas in place, we survey example models available in the literature;applications of such models include collaborative filtering, link prediction,and graph and network analysis. We also highlight connections to recentdevelopments in graph theory and probability, and sketch the more generalmathematical foundation of Bayesian methods for other types of data beyondsequences and arrays.
arxiv-5700-259 | A Fused Elastic Net Logistic Regression Model for Multi-Task Binary Classification | http://arxiv.org/pdf/1312.7750v1.pdf | author:Venelin Mitov, Manfred Claassen category:stat.ML published:2013-12-30 summary:Multi-task learning has shown to significantly enhance the performance ofmultiple related learning tasks in a variety of situations. We present thefused logistic regression, a sparse multi-task learning approach for binaryclassification. Specifically, we introduce sparsity inducing penalties overparameter differences of related logistic regression models to encodesimilarity across related tasks. The resulting joint learning task is cast intoa form that lends itself to be efficiently optimized with a recursive variantof the alternating direction method of multipliers. We show results onsynthetic data and describe the regime of settings where our multi-taskapproach achieves significant improvements over the single task learningapproach and discuss the implications on applying the fused logistic regressionin different real world settings.
arxiv-5700-260 | Evolutionary Design of Numerical Methods: Generating Finite Difference and Integration Schemes by Differential Evolution | http://arxiv.org/pdf/1312.7852v1.pdf | author:C. D. Erdbrink, V. V. Krzhizhanovskaya, P. M. A. Sloot category:cs.NE cs.NA published:2013-12-30 summary:Classical and new numerical schemes are generated using evolutionarycomputing. Differential Evolution is used to find the coefficients of finitedifference approximations of function derivatives, and of single and multi-stepintegration methods. The coefficients are reverse engineered based on samplesfrom a target function and its derivative used for training. The Runge-Kuttaschemes are trained using the order condition equations. An appealing featureof the evolutionary method is the low number of model parameters. Thepopulation size, termination criterion and number of training points aredetermined in a sensitivity analysis. Computational results show good agreementbetween evolved and analytical coefficients. In particular, a new fifth-orderRunge-Kutta scheme is computed which adheres to the order conditions with a sumof absolute errors of order 10^-14. Execution of the evolved schemes proved theintended orders of accuracy. The outcome of this study is valuable for futuredevelopments in the design of complex numerical methods that are out of reachby conventional means.
arxiv-5700-261 | Petuum: A New Platform for Distributed Machine Learning on Big Data | http://arxiv.org/pdf/1312.7651v2.pdf | author:Eric P. Xing, Qirong Ho, Wei Dai, Jin Kyu Kim, Jinliang Wei, Seunghak Lee, Xun Zheng, Pengtao Xie, Abhimanu Kumar, Yaoliang Yu category:stat.ML cs.LG cs.SY published:2013-12-30 summary:What is a systematic way to efficiently apply a wide spectrum of advanced MLprograms to industrial scale problems, using Big Models (up to 100s of billionsof parameters) on Big Data (up to terabytes or petabytes)? Modernparallelization strategies employ fine-grained operations and scheduling beyondthe classic bulk-synchronous processing paradigm popularized by MapReduce, oreven specialized graph-based execution that relies on graph representations ofML programs. The variety of approaches tends to pull systems and algorithmsdesign in different directions, and it remains difficult to find a universalplatform applicable to a wide range of ML programs at scale. We propose ageneral-purpose framework that systematically addresses data- andmodel-parallel challenges in large-scale ML, by observing that many ML programsare fundamentally optimization-centric and admit error-tolerant,iterative-convergent algorithmic solutions. This presents unique opportunitiesfor an integrative system design, such as bounded-error network synchronizationand dynamic scheduling based on ML program structure. We demonstrate theefficacy of these system designs versus well-known implementations of modern MLalgorithms, allowing ML programs to run in much less time and at considerablylarger model sizes, even on modestly-sized compute clusters.
arxiv-5700-262 | Distributed Policy Evaluation Under Multiple Behavior Strategies | http://arxiv.org/pdf/1312.7606v2.pdf | author:Sergio Valcarcel Macua, Jianshu Chen, Santiago Zazo, Ali H. Sayed category:cs.MA cs.AI cs.DC cs.LG published:2013-12-30 summary:We apply diffusion strategies to develop a fully-distributed cooperativereinforcement learning algorithm in which agents in a network communicate onlywith their immediate neighbors to improve predictions about their environment.The algorithm can also be applied to off-policy learning, meaning that theagents can predict the response to a behavior different from the actualpolicies they are following. The proposed distributed strategy is efficient,with linear complexity in both computation time and memory footprint. Weprovide a mean-square-error performance analysis and establish convergenceunder constant step-size updates, which endow the network with continuouslearning capabilities. The results show a clear gain from cooperation: when theindividual agents can estimate the solution, cooperation increases stabilityand reduces bias and variance of the prediction error; but, more importantly,the network is able to approach the optimal solution even when none of theindividual agents can (e.g., when the individual behavior policies restricteach agent to sample a small portion of the state space).
arxiv-5700-263 | Communication Efficient Distributed Optimization using an Approximate Newton-type Method | http://arxiv.org/pdf/1312.7853v4.pdf | author:Ohad Shamir, Nathan Srebro, Tong Zhang category:cs.LG math.OC stat.ML published:2013-12-30 summary:We present a novel Newton-type method for distributed optimization, which isparticularly well suited for stochastic optimization and learning problems. Forquadratic objectives, the method enjoys a linear rate of convergence whichprovably \emph{improves} with the data size, requiring an essentially constantnumber of iterations under reasonable assumptions. We provide theoretical andempirical evidence of the advantages of our method compared to otherapproaches, such as one-shot parameter averaging and ADMM.
arxiv-5700-264 | A model selection approach for clustering a multinomial sequence with non-negative factorization | http://arxiv.org/pdf/1312.7559v7.pdf | author:Nam H. Lee, Runze Tang, Carey E. Priebe, Michael Rosen category:stat.ML published:2013-12-29 summary:We consider a problem of clustering a sequence of multinomial observations byway of a model selection criterion. We propose a form of a penalty term for themodel selection procedure. Our approach subsumes both the conventional AIC andBIC criteria but also extends the conventional criteria in a way that it can beapplicable also to a sequence of sparse multinomial observations, where evenwithin a same cluster, the number of multinomial trials may be different fordifferent observations. In addition, as a preliminary estimation step tomaximum likelihood estimation, and more generally, to maximum $L_{q}$estimation, we propose to use reduced rank projection in combination withnon-negative factorization. We motivate our approach by showing that our modelselection criterion and preliminary estimation step yield consistent estimatesunder simplifying assumptions. We also illustrate our approach throughnumerical experiments using real and simulated data.
arxiv-5700-265 | A General Algorithm for Deciding Transportability of Experimental Results | http://arxiv.org/pdf/1312.7485v1.pdf | author:Elias Bareinboim, Judea Pearl category:cs.AI stat.ME stat.ML published:2013-12-29 summary:Generalizing empirical findings to new environments, settings, or populationsis essential in most scientific explorations. This article treats a particularproblem of generalizability, called "transportability", defined as a license totransfer information learned in experimental studies to a different population,on which only observational studies can be conducted. Given a set ofassumptions concerning commonalities and differences between the twopopulations, Pearl and Bareinboim (2011) derived sufficient conditions thatpermit such transfer to take place. This article summarizes their findings andsupplements them with an effective procedure for deciding when and howtransportability is feasible. It establishes a necessary and sufficientcondition for deciding when causal effects in the target population areestimable from both the statistical information available and the causalinformation transferred from the experiments. The article further provides acomplete algorithm for computing the transport formula, that is, a way ofcombining observational and experimental information to synthesize bias-freeestimate of the desired causal relation. Finally, the article examines thedifferences between transportability and other variants of generalizability.
arxiv-5700-266 | Learning Temporal Logical Properties Discriminating ECG models of Cardiac Arrhytmias | http://arxiv.org/pdf/1312.7523v1.pdf | author:Ezio Bartocci, Luca Bortolussi, Guido Sanguinetti category:cs.LO cs.CV q-bio.QM published:2013-12-29 summary:We present a novel approach to learn the formulae characterising the emergentbehaviour of a dynamical system from system observations. At a high level, theapproach starts by devising a statistical dynamical model of the system whichoptimally fits the observations. We then propose general optimisationstrategies for selecting high support formulae (under the learnt model of thesystem) either within a discrete set of formulae of bounded complexity, or aparametric family of formulae. We illustrate and apply the methodology on anin-depth case study of characterising cardiac malfunction fromelectro-cardiogram data, where our approach enables us to quantitativelydetermine the diagnostic power of a formula in discriminating between differentcardiac conditions.
arxiv-5700-267 | Probabilistic Archetypal Analysis | http://arxiv.org/pdf/1312.7604v2.pdf | author:Sohan Seth, Manuel J. A. Eugster category:stat.ML published:2013-12-29 summary:Archetypal analysis represents a set of observations as convex combinationsof pure patterns, or archetypes. The original geometric formulation of findingarchetypes by approximating the convex hull of the observations assumes them tobe real valued. This, unfortunately, is not compatible with many practicalsituations. In this paper we revisit archetypal analysis from the basicprinciples, and propose a probabilistic framework that accommodates otherobservation types such as integers, binary, and probability vectors. Wecorroborate the proposed methodology with convincing real-world applications onfinding archetypal winter tourists based on binary survey data, archetypaldisaster-affected countries based on disaster count data, and documentarchetypes based on term-frequency data. We also present an appropriatevisualization tool to summarize archetypal analysis solution better.
arxiv-5700-268 | A Novel Scheme for Generating Secure Face Templates Using BDA | http://arxiv.org/pdf/1312.7511v1.pdf | author:Shraddha S. Shinde, Prof. Anagha P. Khedkar category:cs.CV cs.CR published:2013-12-29 summary:In identity management system, frequently used biometric recognition systemneeds awareness towards issue of protecting biometric template as far as morereliable solution is apprehensive. In sight of this biometric templateprotection algorithm should gratify the basic requirements viz. security,discriminability and cancelability. As no single template protection method iscapable of satisfying these requirements, a novel scheme for face templategeneration and protection is proposed. The novel scheme is proposed to providesecurity and accuracy in new user enrolment and authentication process. Thisnovel scheme takes advantage of both the hybrid approach and the binarydiscriminant analysis algorithm. This algorithm is designed on the basis ofrandom projection, binary discriminant analysis and fuzzy commitment scheme.Publicly available benchmark face databases (FERET, FRGC, CMU-PIE) and otherdatasets are used for evaluation. The proposed novel scheme enhances thediscriminability and recognition accuracy in terms of matching score of theface images for each stage and provides high security against potential attacksnamely brute force and smart attacks. In this paper, we discuss results viz.averages matching score, computation time and security for hybrid approach andnovel approach.
arxiv-5700-269 | A Novel Method for Automatic Segmentation of Brain Tumors in MRI Images | http://arxiv.org/pdf/1312.7573v1.pdf | author:Saeid Fazli, Parisa Nadirkhanlou category:cs.CV published:2013-12-29 summary:The brain tumor segmentation on MRI images is a very difficult and importanttask which is used in surgical and medical planning and assessments. If expertsdo the segmentation manually with their own medical knowledge, it will betime-consuming. Therefore, researchers propose methods and systems which can dothe segmentation automatically and without any interference. In this article,an unsupervised automatic method for brain tumor segmentation on MRI images ispresented. In this method, at first in the pre-processing level, the extraparts which are outside the skull and don't have any helpful information areremoved and then anisotropic diffusion filter with 8-connected neighborhood isapplied to the MRI images to remove noise. By applying the fast boundingbox(FBB) algorithm, the tumor area is displayed on the MRI image with abounding box and the central part is selected as sample points for training ofa One Class SVM classifier. A database is also provided by the Zanjan MRICenter. The MRI images are related to 10 patients who have brain tumor. 100T2-weighted MRI images are used in this study. Experimental results show thehigh precision and dependability of the proposed algorithm. The results arealso highly helpful for specialists and radiologists to easily estimate thesize and position of a tumor.
arxiv-5700-270 | Actions in the Eye: Dynamic Gaze Datasets and Learnt Saliency Models for Visual Recognition | http://arxiv.org/pdf/1312.7570v1.pdf | author:Stefan Mathe, Cristian Sminchisescu category:cs.CV published:2013-12-29 summary:Systems based on bag-of-words models from image features collected at maximaof sparse interest point operators have been used successfully for bothcomputer visual object and action recognition tasks. While the sparse,interest-point based approach to recognition is not inconsistent with visualprocessing in biological systems that operate in `saccade and fixate' regimes,the methodology and emphasis in the human and the computer vision communitiesremains sharply distinct. Here, we make three contributions aiming to bridgethis gap. First, we complement existing state-of-the art large scale dynamiccomputer vision annotated datasets like Hollywood-2 and UCF Sports with humaneye movements collected under the ecological constraints of the visual actionrecognition task. To our knowledge these are the first large human eye trackingdatasets to be collected and made publicly available for video,vision.imar.ro/eyetracking (497,107 frames, each viewed by 16 subjects), uniquein terms of their (a) large scale and computer vision relevance, (b) dynamic,video stimuli, (c) task control, as opposed to free-viewing. Second, weintroduce novel sequential consistency and alignment measures, which underlinethe remarkable stability of patterns of visual search among subjects. Third, weleverage the significant amount of collected data in order to pursue studiesand build automatic, end-to-end trainable computer vision systems based onhuman eye movements. Our studies not only shed light on the differences betweencomputer vision spatio-temporal interest point image sampling strategies andthe human fixations, as well as their impact for visual recognitionperformance, but also demonstrate that human fixations can be accuratelypredicted, and when used in an end-to-end automatic system, leveraging some ofthe advanced computer vision practice, can lead to state of the art results.
arxiv-5700-271 | Nonparametric Inference For Density Modes | http://arxiv.org/pdf/1312.7567v1.pdf | author:Christopher Genovese, Marco Perone-Pacifico, Isabella Verdinelli, Larry Wasserman category:stat.ME cs.LG 62G07 published:2013-12-29 summary:We derive nonparametric confidence intervals for the eigenvalues of theHessian at modes of a density estimate. This provides information about thestrength and shape of modes and can also be used as a significance test. We usea data-splitting approach in which potential modes are identified using thefirst half of the data and inference is done with the second half of the data.To get valid confidence sets for the eigenvalues, we use a bootstrap based onan elementary-symmetric-polynomial (ESP) transformation. This leads to validbootstrap confidence sets regardless of any multiplicities in the eigenvalues.We also suggest a new method for bandwidth selection, namely, choosing thebandwidth to maximize the number of significant modes. We show by example thatthis method works well. Even when the true distribution is singular, and hencedoes not have a density, (in which case cross validation chooses a zerobandwidth), our method chooses a reasonable bandwidth.
arxiv-5700-272 | Implementation of Hand Detection based Techniques for Human Computer Interaction | http://arxiv.org/pdf/1312.7560v1.pdf | author:Amiraj Dhawan, Vipul Honrao category:cs.CV cs.HC published:2013-12-29 summary:The computer industry is developing at a fast pace. With this developmentalmost all of the fields under computers have advanced in the past couple ofdecades. But the same technology is being used for human computer interactionthat was used in 1970s. Even today the same type of keyboard and mouse is usedfor interacting with computer systems. With the recent boom in the mobilesegment touchscreens have become popular for interaction with cell phones. Butthese touchscreens are rarely used on traditional systems. This paper tries tointroduce methods for human computer interaction using the users hand which canbe used both on traditional computer platforms as well as cell phones. Themethods explain how the users detected hand can be used as input forapplications and also explain applications that can take advantage of this typeof interaction mechanism.
arxiv-5700-273 | A Novel Retinal Vessel Segmentation Based On Histogram Transformation Using 2-D Morlet Wavelet and Supervised Classification | http://arxiv.org/pdf/1312.7557v1.pdf | author:Saeid Fazli, Sevin Samadi category:cs.CV published:2013-12-29 summary:The appearance and structure of blood vessels in retinal images have animportant role in diagnosis of diseases. This paper proposes a method forautomatic retinal vessel segmentation. In this work, a novel preprocessingbased on local histogram equalization is used to enhance the original imagethen pixels are classified as vessel and non-vessel using a classifier. Forthis classification, special feature vectors are organized based on responsesto Morlet wavelet. Morlet wavelet is a continues transform which has theability to filter existing noises after preprocessing. Bayesian classifier isused and Gaussian mixture model (GMM) is its likelihood function. Theprobability distributions are approximated according to training set of manualthat has been segmented by a specialist. After this, morphological transformsare used in different directions to make the existing discontinuities uniformon the DRIVE database, it achieves the accuracy about 0.9571 which shows thatit is an accurate method among the available ones for retinal vesselsegmentation.
arxiv-5700-274 | Shape Primitive Histogram: A Novel Low-Level Face Representation for Face Recognition | http://arxiv.org/pdf/1312.7446v3.pdf | author:Sheng Huang, Dan Yang, Haopeng Zhang, Luwen Huangfu, Xiaohong Zhang category:cs.CV published:2013-12-28 summary:We further exploit the representational power of Haar wavelet and present anovel low-level face representation named Shape Primitives Histogram (SPH) forface recognition. Since human faces exist abundant shape features, we addressthe face representation issue from the perspective of the shape featureextraction. In our approach, we divide faces into a number of tiny shapefragments and reduce these shape fragments to several uniform atomic shapepatterns called Shape Primitives. A convolution with Haar Wavelet templates isapplied to each shape fragment to identify its belonging shape primitive. Afterthat, we do a histogram statistic of shape primitives in each spatial localimage patch for incorporating the spatial information. Finally, each face isrepresented as a feature vector via concatenating all the local histograms ofshape primitives. Four popular face databases, namely ORL, AR, Yale-B and LFW-adatabases, are employed to evaluate SPH and experimentally study the choices ofthe parameters. Extensive experimental results demonstrate that the proposedapproach outperform the state-of-the-arts.
arxiv-5700-275 | Collaborative Discriminant Locality Preserving Projections With its Application to Face Recognition | http://arxiv.org/pdf/1312.7469v2.pdf | author:Sheng Huang, Dan Yang, Dong Yang, Ahmed Elgammal category:cs.CV published:2013-12-28 summary:We present a novel Discriminant Locality Preserving Projections (DLPP)algorithm named Collaborative Discriminant Locality Preserving Projection(CDLPP). In our algorithm, the discriminating power of DLPP are furtherexploited from two aspects. On the one hand, the global optimum of classscattering is guaranteed via using the between-class scatter matrix to replacethe original denominator of DLPP. On the other hand, motivated by collaborativerepresentation, an $L_2$-norm constraint is imposed to the projections todiscover the collaborations of dimensions in the sample space. We apply ouralgorithm to face recognition. Three popular face databases, namely AR, ORL andLFW-A, are employed for evaluating the performance of CDLPP. Extensiveexperimental results demonstrate that CDLPP significantly improves thediscriminating power of DLPP and outperforms the state-of-the-arts.
arxiv-5700-276 | Rate-Distortion Auto-Encoders | http://arxiv.org/pdf/1312.7381v2.pdf | author:Luis G. Sanchez Giraldo, Jose C. Principe category:cs.LG published:2013-12-28 summary:A rekindled the interest in auto-encoder algorithms has been spurred byrecent work on deep learning. Current efforts have been directed towardseffective training of auto-encoder architectures with a large number of codingunits. Here, we propose a learning algorithm for auto-encoders based on arate-distortion objective that minimizes the mutual information between theinputs and the outputs of the auto-encoder subject to a fidelity constraint.The goal is to learn a representation that is minimally committed to the inputdata, but that is rich enough to reconstruct the inputs up to certain level ofdistortion. Minimizing the mutual information acts as a regularization termwhereas the fidelity constraint can be understood as a risk functional in theconventional statistical learning setting. The proposed algorithm uses arecently introduced measure of entropy based on infinitely divisible matricesthat avoids the plug in estimation of densities. Experiments usingover-complete bases show that the rate-distortion auto-encoders can learn aregularized input-output mapping in an implicit manner.
arxiv-5700-277 | Generalized Ambiguity Decomposition for Understanding Ensemble Diversity | http://arxiv.org/pdf/1312.7463v1.pdf | author:Kartik Audhkhasi, Abhinav Sethy, Bhuvana Ramabhadran, Shrikanth S. Narayanan category:stat.ML cs.CV cs.LG I.5 published:2013-12-28 summary:Diversity or complementarity of experts in ensemble pattern recognition andinformation processing systems is widely-observed by researchers to be crucialfor achieving performance improvement upon fusion. Understanding this linkbetween ensemble diversity and fusion performance is thus an important researchquestion. However, prior works have theoretically characterized ensemblediversity and have linked it with ensemble performance in very restrictedsettings. We present a generalized ambiguity decomposition (GAD) theorem as abroad framework for answering these questions. The GAD theorem applies to ageneric convex ensemble of experts for any arbitrary twice-differentiable lossfunction. It shows that the ensemble performance approximately decomposes intoa difference of the average expert performance and the diversity of theensemble. It thus provides a theoretical explanation for theempirically-observed benefit of fusing outputs from diverse classifiers andregressors. It also provides a loss function-dependent, ensemble-dependent, anddata-dependent definition of diversity. We present extensions of thisdecomposition to common regression and classification loss functions, andreport a simulation-based analysis of the diversity term and the accuracy ofthe decomposition. We finally present experiments on standard patternrecognition data sets which indicate the accuracy of the decomposition forreal-world classification and regression problems.
arxiv-5700-278 | Stopping Rules for Bag-of-Words Image Search and Its Application in Appearance-Based Localization | http://arxiv.org/pdf/1312.7414v1.pdf | author:Kiana Hajebi, Hong Zhang category:cs.CV cs.RO published:2013-12-28 summary:We propose a technique to improve the search efficiency of the bag-of-words(BoW) method for image retrieval. We introduce a notion of difficulty for theimage matching problems and propose methods that reduce the amount ofcomputations required for the feature vector-quantization task in BoW byexploiting the fact that easier queries need less computational resources.Measuring the difficulty of a query and stopping the search accordingly isformulated as a stopping problem. We introduce stopping rules that terminatethe image search depending on the difficulty of each query, therebysignificantly reducing the computational cost. Our experimental results showthe effectiveness of our approach when it is applied to appearance-basedlocalization problem.
arxiv-5700-279 | Learning Human Pose Estimation Features with Convolutional Networks | http://arxiv.org/pdf/1312.7302v6.pdf | author:Arjun Jain, Jonathan Tompson, Mykhaylo Andriluka, Graham W. Taylor, Christoph Bregler category:cs.CV cs.LG cs.NE published:2013-12-27 summary:This paper introduces a new architecture for human pose estimation using amulti- layer convolutional network architecture and a modified learningtechnique that learns low-level features and higher-level weak spatial models.Unconstrained human pose estimation is one of the hardest problems in computervision, and our new architecture and learning schema shows significantimprovement over the current state-of-the-art results. The main contribution ofthis paper is showing, for the first time, that a specific variation of deeplearning is able to outperform all existing traditional architectures on thistask. The paper also discusses several lessons learned while researchingalternatives, most notably, that it is possible to learn strong low-levelfeature detectors on features that might even just cover a few pixels in theimage. Higher-level spatial models improve somewhat the overall result, but toa much lesser extent then expected. Many researchers previously argued that thekinematic structure and top-down information is crucial for this domain, butwith our purely bottom up, and weak spatial model, we could improve other morecomplicated architectures that currently produce the best results. This mirrorswhat many other researchers, like those in the speech recognition, objectrecognition, and other domains have experienced.
arxiv-5700-280 | lil' UCB : An Optimal Exploration Algorithm for Multi-Armed Bandits | http://arxiv.org/pdf/1312.7308v1.pdf | author:Kevin Jamieson, Matthew Malloy, Robert Nowak, Sébastien Bubeck category:stat.ML cs.LG published:2013-12-27 summary:The paper proposes a novel upper confidence bound (UCB) procedure foridentifying the arm with the largest mean in a multi-armed bandit game in thefixed confidence setting using a small number of total samples. The procedurecannot be improved in the sense that the number of samples required to identifythe best arm is within a constant factor of a lower bound based on the law ofthe iterated logarithm (LIL). Inspired by the LIL, we construct our confidencebounds to explicitly account for the infinite time horizon of the algorithm. Inaddition, by using a novel stopping time for the algorithm we avoid a unionbound over the arms that has been observed in other UCB-type algorithms. Weprove that the algorithm is optimal up to constants and also show throughsimulations that it provides superior performance with respect to thestate-of-the-art.
arxiv-5700-281 | Active Discovery of Network Roles for Predicting the Classes of Network Nodes | http://arxiv.org/pdf/1312.7258v2.pdf | author:Leto Peel category:cs.LG cs.SI stat.ML published:2013-12-27 summary:Nodes in real world networks often have class labels, or underlyingattributes, that are related to the way in which they connect to other nodes.Sometimes this relationship is simple, for instance nodes of the same class aremay be more likely to be connected. In other cases, however, this is not true,and the way that nodes link in a network exhibits a different, more complexrelationship to their attributes. Here, we consider networks in which we knowhow the nodes are connected, but we do not know the class labels of the nodesor how class labels relate to the network links. We wish to identify the bestsubset of nodes to label in order to learn this relationship between nodeattributes and network links. We can then use this discovered relationship toaccurately predict the class labels of the rest of the network nodes. We present a model that identifies groups of nodes with similar linkpatterns, which we call network roles, using a generative blockmodel. The modelthen predicts labels by learning the mapping from network roles to class labelsusing a maximum margin classifier. We choose a subset of nodes to labelaccording to an iterative margin-based active learning strategy. By integratingthe discovery of network roles with the classifier optimisation, the activelearning process can adapt the network roles to better represent the networkfor node classification. We demonstrate the model by exploring a selection ofreal world networks, including a marine food web and a network of Englishwords. We show that, in contrast to other network classifiers, this modelachieves good classification accuracy for a range of networks with differentrelationships between class labels and network links.
arxiv-5700-282 | Quality Estimation of English-Hindi Outputs using Naive Bayes Classifier | http://arxiv.org/pdf/1312.7223v1.pdf | author:Rashmi Gupta, Nisheeth Joshi, Iti Mathur category:cs.CL published:2013-12-27 summary:In this paper we present an approach for estimating the quality of machinetranslation system. There are various methods for estimating the quality ofoutput sentences, but in this paper we focus on Na\"ive Bayes classifier tobuild model using features which are extracted from the input sentences. Thesefeatures are used for finding the likelihood of each of the sentences of thetraining data which are then further used for determining the scores of thetest data. On the basis of these scores we determine the class labels of thetest data.
arxiv-5700-283 | Sub-Classifier Construction for Error Correcting Output Code Using Minimum Weight Perfect Matching | http://arxiv.org/pdf/1312.7179v1.pdf | author:Patoomsiri Songsiri, Thimaporn Phetkaew, Ryutaro Ichise, Boonserm Kijsirikul category:cs.LG cs.IT math.IT published:2013-12-27 summary:Multi-class classification is mandatory for real world problems and one ofpromising techniques for multi-class classification is Error Correcting OutputCode. We propose a method for constructing the Error Correcting Output Code toobtain the suitable combination of positive and negative classes encoded torepresent binary classifiers. The minimum weight perfect matching algorithm isapplied to find the optimal pairs of subset of classes by using thegeneralization performance as a weighting criterion. Based on our method, eachsubset of classes with positive and negative labels is appropriately combinedfor learning the binary classifiers. Experimental results show that ourtechnique gives significantly higher performance compared to traditionalmethods including the dense random code and the sparse random code both interms of accuracy and classification times. Moreover, our method requiressignificantly smaller number of binary classifiers while maintaining accuracycompared to the One-Versus-One.
arxiv-5700-284 | Near-separable Non-negative Matrix Factorization with $\ell_1$- and Bregman Loss Functions | http://arxiv.org/pdf/1312.7167v1.pdf | author:Abhishek Kumar, Vikas Sindhwani category:stat.ML cs.CV cs.LG published:2013-12-27 summary:Recently, a family of tractable NMF algorithms have been proposed under theassumption that the data matrix satisfies a separability condition Donoho &Stodden (2003); Arora et al. (2012). Geometrically, this condition reformulatesthe NMF problem as that of finding the extreme rays of the conical hull of afinite set of vectors. In this paper, we develop several extensions of theconical hull procedures of Kumar et al. (2013) for robust ($\ell_1$)approximations and Bregman divergences. Our methods inherit all the advantagesof Kumar et al. (2013) including scalability and noise-tolerance. We show thaton foreground-background separation problems in computer vision, robustnear-separable NMFs match the performance of Robust PCA, considered state ofthe art on these problems, with an order of magnitude faster training time. Wealso demonstrate applications in exemplar selection settings.
arxiv-5700-285 | Combining persistent homology and invariance groups for shape comparison | http://arxiv.org/pdf/1312.7219v4.pdf | author:Patrizio Frosini, Grzegorz Jablonski category:math.AT cs.CG cs.CV I.4.7; I.5.1 published:2013-12-27 summary:In many applications concerning the comparison of data expressed by$\mathbb{R}^m$-valued functions defined on a topological space $X$, theinvariance with respect to a given group $G$ of self-homeomorphisms of $X$ isrequired. While persistent homology is quite efficient in the topological andqualitative comparison of this kind of data when the invariance group $G$ isthe group $\mathrm{Homeo}(X)$ of all self-homeomorphisms of $X$, this theory isnot tailored to manage the case in which $G$ is a proper subgroup of$\mathrm{Homeo}(X)$, and its invariance appears too general for several tasks.This paper proposes a way to adapt persistent homology in order to getinvariance just with respect to a given group of self-homeomorphisms of $X$.The main idea consists in a dual approach, based on considering the set of all$G$-invariant non-expanding operators defined on the space of the admissiblefiltering functions on $X$. Some theoretical results concerning this approachare proven and two experiments are presented. An experiment illustrates theapplication of the proposed technique to compare 1D-signals, when theinvariance is expressed by the group of affinities, the group oforientation-preserving affinities, the group of isometries, the group oftranslations and the identity group. Another experiment shows how our techniquecan be used for image comparison.
arxiv-5700-286 | Monte Carlo non local means: Random sampling for large-scale image filtering | http://arxiv.org/pdf/1312.7366v3.pdf | author:Stanley H. Chan, Todd Zickler, Yue M. Lu category:cs.CV stat.CO published:2013-12-27 summary:We propose a randomized version of the non-local means (NLM) algorithm forlarge-scale image filtering. The new algorithm, called Monte Carlo non-localmeans (MCNLM), speeds up the classical NLM by computing a small subset of imagepatch distances, which are randomly selected according to a designed samplingpattern. We make two contributions. First, we analyze the performance of theMCNLM algorithm and show that, for large images or large external imagedatabases, the random outcomes of MCNLM are tightly concentrated around thedeterministic full NLM result. In particular, our error probability bounds showthat, at any given sampling ratio, the probability for MCNLM to have a largedeviation from the original NLM solution decays exponentially as the size ofthe image or database grows. Second, we derive explicit formulas for optimalsampling patterns that minimize the error probability bound by exploitingpartial knowledge of the pairwise similarity weights. Numerical experimentsshow that MCNLM is competitive with other state-of-the-art fast NLM algorithmsfor single-image denoising. When applied to denoising images using an externaldatabase containing ten billion patches, MCNLM returns a randomized solutionthat is within 0.2 dB of the full NLM solution while reducing the runtime bythree orders of magnitude.
arxiv-5700-287 | Two Timescale Convergent Q-learning for Sleep--Scheduling in Wireless Sensor Networks | http://arxiv.org/pdf/1312.7292v2.pdf | author:Prashanth L. A., Abhranil Chatterjee, Shalabh Bhatnagar category:cs.SY cs.LG published:2013-12-27 summary:In this paper, we consider an intrusion detection application for WirelessSensor Networks (WSNs). We study the problem of scheduling the sleep times ofthe individual sensors to maximize the network lifetime while keeping thetracking error to a minimum. We formulate this problem as apartially-observable Markov decision process (POMDP) with continuousstate-action spaces, in a manner similar to (Fuemmeler and Veeravalli [2008]).However, unlike their formulation, we consider infinite horizon discounted andaverage cost objectives as performance criteria. For each criterion, we proposea convergent on-policy Q-learning algorithm that operates on two timescales,while employing function approximation to handle the curse of dimensionalityassociated with the underlying POMDP. Our proposed algorithm incorporates apolicy gradient update using a one-simulation simultaneous perturbationstochastic approximation (SPSA) estimate on the faster timescale, while theQ-value parameter (arising from a linear function approximation for theQ-values) is updated in an on-policy temporal difference (TD) algorithm-likefashion on the slower timescale. The feature selection scheme employed in eachof our algorithms manages the energy and tracking components in a manner thatassists the search for the optimal sleep-scheduling policy. For the sake ofcomparison, in both discounted and average settings, we also develop a functionapproximation analogue of the Q-learning algorithm. This algorithm, unlike thetwo-timescale variant, does not possess theoretical convergence guarantees.Finally, we also adapt our algorithms to include a stochastic iterativeestimation scheme for the intruder's mobility model. Our simulation results ona 2-dimensional network setting suggest that our algorithms result in bettertracking accuracy at the cost of only a few additional sensors, in comparisonto a recent prior work.
arxiv-5700-288 | Shape-constrained Estimation of Value Functions | http://arxiv.org/pdf/1312.7035v1.pdf | author:Mohammad Mousavi, Peter W. Glynn category:math.PR cs.CE math.OC stat.ML published:2013-12-26 summary:We present a fully nonparametric method to estimate the value function, viasimulation, in the context of expected infinite-horizon discounted rewards forMarkov chains. Estimating such value functions plays an important role inapproximate dynamic programming and applied probability in general. Weincorporate "soft information" into the estimation algorithm, such as knowledgeof convexity, monotonicity, or Lipchitz constants. In the presence of suchinformation, a nonparametric estimator for the value function can be computedthat is provably consistent as the simulated time horizon tends to infinity. Asan application, we implement our method on price tolling agreement contracts inenergy markets.
arxiv-5700-289 | Finding More Relevance: Propagating Similarity on Markov Random Field for Image Retrieval | http://arxiv.org/pdf/1312.7085v1.pdf | author:Peng Lu, Xujun Peng, Xinshan Zhu, Xiaojie Wang category:cs.CV published:2013-12-26 summary:To effectively retrieve objects from large corpus with high accuracy is achallenge task. In this paper, we propose a method that propagates visualfeature level similarities on a Markov random field (MRF) to obtain a highlevel correspondence in image space for image pairs. The proposedcorrespondence between image pair reflects not only the similarity of low-levelvisual features but also the relations built through other images in thedatabase and it can be easily integrated into the existingbag-of-visual-words(BoW) based systems to reduce the missing rate. We evaluateour method on the standard Oxford-5K, Oxford-105K and Paris-6K dataset. Theexperiment results show that the proposed method significantly improves theretrieval accuracy on three datasets and exceeds the current state-of-the-artretrieval performance.
arxiv-5700-290 | Language Modeling with Power Low Rank Ensembles | http://arxiv.org/pdf/1312.7077v2.pdf | author:Ankur P. Parikh, Avneesh Saluja, Chris Dyer, Eric P. Xing category:cs.CL cs.LG stat.ML published:2013-12-26 summary:We present power low rank ensembles (PLRE), a flexible framework for n-gramlanguage modeling where ensembles of low rank matrices and tensors are used toobtain smoothed probability estimates of words in context. Our method can beunderstood as a generalization of n-gram modeling to non-integer n, andincludes standard techniques such as absolute discounting and Kneser-Neysmoothing as special cases. PLRE training is efficient and our approachoutperforms state-of-the-art modified Kneser Ney baselines in terms ofperplexity on large corpora as well as on BLEU score in a downstream machinetranslation task.
arxiv-5700-291 | Lesion Border Detection in Dermoscopy Images Using Ensembles of Thresholding Methods | http://arxiv.org/pdf/1312.7345v1.pdf | author:M. Emre Celebi, Quan Wen, Sae Hwang, Hitoshi Iyatomi, Gerald Schaefer category:cs.CV I.4.6 published:2013-12-26 summary:Dermoscopy is one of the major imaging modalities used in the diagnosis ofmelanoma and other pigmented skin lesions. Due to the difficulty andsubjectivity of human interpretation, automated analysis of dermoscopy imageshas become an important research area. Border detection is often the first stepin this analysis. In many cases, the lesion can be roughly separated from thebackground skin using a thresholding method applied to the blue channel.However, no single thresholding method appears to be robust enough tosuccessfully handle the wide variety of dermoscopy images encountered inclinical practice. In this paper, we present an automated method for detectinglesion borders in dermoscopy images using ensembles of thresholding methods.Experiments on a difficult set of 90 images demonstrate that the proposedmethod is robust, fast, and accurate when compared to nine state-of-the-artmethods.
arxiv-5700-292 | A Convex Formulation for Mixed Regression with Two Components: Minimax Optimal Rates | http://arxiv.org/pdf/1312.7006v2.pdf | author:Yudong Chen, Xinyang Yi, Constantine Caramanis category:stat.ML cs.IT cs.LG math.IT published:2013-12-25 summary:We consider the mixed regression problem with two components, underadversarial and stochastic noise. We give a convex optimization formulationthat provably recovers the true solution, and provide upper bounds on therecovery errors for both arbitrary noise and stochastic noise settings. We alsogive matching minimax lower bounds (up to log factors), showing that undercertain assumptions, our algorithm is information-theoretically optimal. Ourresults represent the first tractable algorithm guaranteeing successfulrecovery with tight bounds on recovery errors and sample complexity.
arxiv-5700-293 | Piecewise regression mixture for simultaneous functional data clustering and optimal segmentation | http://arxiv.org/pdf/1312.6974v2.pdf | author:Faicel Chamroukhi category:stat.ME cs.LG math.ST stat.ML stat.TH published:2013-12-25 summary:This paper introduces a novel mixture model-based approach for simultaneousclustering and optimal segmentation of functional data which are curvespresenting regime changes. The proposed model consists in a finite mixture ofpiecewise polynomial regression models. Each piecewise polynomial regressionmodel is associated with a cluster, and within each cluster, each piecewisepolynomial component is associated with a regime (i.e., a segment). We derivetwo approaches for learning the model parameters. The former is an estimationapproach and consists in maximizing the observed-data likelihood via adedicated expectation-maximization (EM) algorithm. A fuzzy partition of thecurves in K clusters is then obtained at convergence by maximizing theposterior cluster probabilities. The latter however is a classificationapproach and optimizes a specific classification likelihood criterion through adedicated classification expectation-maximization (CEM) algorithm. The optimalcurve segmentation is performed by using dynamic programming. In theclassification approach, both the curve clustering and the optimal segmentationare performed simultaneously as the CEM learning proceeds. We show that theclassification approach is the probabilistic version that generalizes thedeterministic K-means-like algorithm proposed in H\'ebrail et al. (2010). Theproposed approach is evaluated using simulated curves and real-world curves.Comparisons with alternatives including regression mixture models and theK-means like algorithm for piecewise regression demonstrate the effectivenessof the proposed approach.
arxiv-5700-294 | Formal Ontology Learning on Factual IS-A Corpus in English using Description Logics | http://arxiv.org/pdf/1312.6947v2.pdf | author:Sourish Dasgupta, Ankur Padia, Kushal Shah, Prasenjit Majumder category:cs.CL cs.AI published:2013-12-25 summary:Ontology Learning (OL) is the computational task of generating a knowledgebase in the form of an ontology given an unstructured corpus whose content isin natural language (NL). Several works can be found in this area most of whichare limited to statistical and lexico-syntactic pattern matching basedtechniques Light-Weight OL. These techniques do not lead to very accuratelearning mostly because of several linguistic nuances in NL. Formal OL is analternative (less explored) methodology were deep linguistics analysis is madeusing theory and tools found in computational linguistics to generate formalaxioms and definitions instead simply inducing a taxonomy. In this paper wepropose "Description Logic (DL)" based formal OL framework for learning factualIS-A type sentences in English. We claim that semantic construction of IS-Asentences is non trivial. Hence, we also claim that such sentences requiresspecial studies in the context of OL before any truly formal OL can beproposed. We introduce a learner tool, called DLOL_IS-A, that generated suchontologies in the owl format. We have adopted "Gold Standard" based OLevaluation on IS-A rich WCL v.1.1 dataset and our own Community representativeIS-A dataset. We observed significant improvement of DLOL_IS-A when compared tothe light-weight OL tool Text2Onto and formal OL tool FRED.
arxiv-5700-295 | Model-based clustering with Hidden Markov Model regression for time series with regime changes | http://arxiv.org/pdf/1312.7024v1.pdf | author:Faicel Chamroukhi, Allou Samé, Patrice Aknin, Gérard Govaert category:stat.ML cs.LG stat.ME published:2013-12-25 summary:This paper introduces a novel model-based clustering approach for clusteringtime series which present changes in regime. It consists of a mixture ofpolynomial regressions governed by hidden Markov chains. The underlying hiddenprocess for each cluster activates successively several polynomial regimesduring time. The parameter estimation is performed by the maximum likelihoodmethod through a dedicated Expectation-Maximization (EM) algorithm. Theproposed approach is evaluated using simulated time series and real-world timeseries issued from a railway diagnosis application. Comparisons with existingapproaches for time series clustering, including the stand EM for Gaussianmixtures, $K$-means clustering, the standard mixture of regression models andmixture of Hidden Markov Models, demonstrate the effectiveness of the proposedapproach.
arxiv-5700-296 | Robust EM algorithm for model-based curve clustering | http://arxiv.org/pdf/1312.7022v1.pdf | author:Faicel Chamroukhi category:stat.ME cs.LG stat.ML published:2013-12-25 summary:Model-based clustering approaches concern the paradigm of exploratory dataanalysis relying on the finite mixture model to automatically find a latentstructure governing observed data. They are one of the most popular andsuccessful approaches in cluster analysis. The mixture density estimation isgenerally performed by maximizing the observed-data log-likelihood by using theexpectation-maximization (EM) algorithm. However, it is well-known that the EMalgorithm initialization is crucial. In addition, the standard EM algorithmrequires the number of clusters to be known a priori. Some solutions have beenprovided in [31, 12] for model-based clustering with Gaussian mixture modelsfor multivariate data. In this paper we focus on model-based curve clusteringapproaches, when the data are curves rather than vectorial data, based onregression mixtures. We propose a new robust EM algorithm for clusteringcurves. We extend the model-based clustering approach presented in [31] forGaussian mixture models, to the case of curve clustering by regressionmixtures, including polynomial regression mixtures as well as spline orB-spline regressions mixtures. Our approach both handles the problem ofinitialization and the one of choosing the optimal number of clusters as the EMlearning proceeds, rather than in a two-fold scheme. This is achieved byoptimizing a penalized log-likelihood criterion. A simulation study confirmsthe potential benefit of the proposed algorithm in terms of robustnessregarding initialization and funding the actual number of clusters.
arxiv-5700-297 | Mixture model-based functional discriminant analysis for curve classification | http://arxiv.org/pdf/1312.7018v1.pdf | author:Faicel Chamroukhi, Hervé Glotin category:stat.ME cs.LG stat.ML published:2013-12-25 summary:Statistical approaches for Functional Data Analysis concern the paradigm forwhich the individuals are functions or curves rather than finite dimensionalvectors. In this paper, we particularly focus on the modeling and theclassification of functional data which are temporal curves presenting regimechanges over time. More specifically, we propose a new mixture model-baseddiscriminant analysis approach for functional data using a specific hiddenprocess regression model. Our approach is particularly adapted to both handlethe problem of complex-shaped classes of curves, where each class is composedof several sub-classes, and to deal with the regime changes within eachhomogeneous sub-class. The model explicitly integrates the heterogeneity ofeach class of curves via a mixture model formulation, and the regime changeswithin each sub-class through a hidden logistic process. The approach allowstherefore for fitting flexible curve-models to each class of complex-shapedcurves presenting regime changes through an unsupervised learning scheme, toautomatically summarize it into a finite number of homogeneous clusters, eachof them is decomposed into several regimes. The model parameters are learned bymaximizing the observed-data log-likelihood for each class by using a dedicatedexpectation-maximization (EM) algorithm. Comparisons on simulated data and realdata with alternative approaches, including functional linear discriminantanalysis and functional mixture discriminant analysis with polynomialregression mixtures and spline regression mixtures, show that the proposedapproach provides better results regarding the discrimination results andsignificantly improves the curves approximation.
arxiv-5700-298 | Classification automatique de données temporelles en classes ordonnées | http://arxiv.org/pdf/1312.7011v1.pdf | author:Faicel Chamroukhi, Allou Samé, Gérard Govaert, Patrice Aknin category:stat.ML stat.ME published:2013-12-25 summary:This paper proposes a method of segmenting temporal data into orderedclasses. It is based on mixture models and a discrete latent process, whichenables to successively activates the classes. The classification can beperformed by maximizing the likelihood via the EM algorithm or bysimultaneously optimizing the model parameters and the partition by the CEMalgorithm. These two algorithms can be seen as alternatives to Fisher'salgorithm, which improve its computing time.
arxiv-5700-299 | Description Logics based Formalization of Wh-Queries | http://arxiv.org/pdf/1312.6948v1.pdf | author:Sourish Dasgupta, Rupali KaPatel, Ankur Padia, Kushal Shah category:cs.CL cs.AI published:2013-12-25 summary:The problem of Natural Language Query Formalization (NLQF) is to translate agiven user query in natural language (NL) into a formal language so that thesemantic interpretation has equivalence with the NL interpretation.Formalization of NL queries enables logic based reasoning during informationretrieval, database query, question-answering, etc. Formalization also helps inWeb query normalization and indexing, query intent analysis, etc. In this paperwe are proposing a Description Logics based formal methodology for wh-queryintent (also called desire) identification and corresponding formaltranslation. We evaluated the scalability of our proposed formalism usingMicrosoft Encarta 98 query dataset and OWL-S TC v.4.0 dataset.
arxiv-5700-300 | Model-based clustering and segmentation of time series with changes in regime | http://arxiv.org/pdf/1312.6967v1.pdf | author:Allou Samé, Faicel Chamroukhi, Gérard Govaert, Patrice Aknin category:stat.ME cs.LG math.ST stat.ML stat.TH published:2013-12-25 summary:Mixture model-based clustering, usually applied to multidimensional data, hasbecome a popular approach in many data analysis problems, both for its goodstatistical properties and for the simplicity of implementation of theExpectation-Maximization (EM) algorithm. Within the context of a railwayapplication, this paper introduces a novel mixture model for dealing with timeseries that are subject to changes in regime. The proposed approach consists inmodeling each cluster by a regression model in which the polynomialcoefficients vary according to a discrete hidden process. In particular, thisapproach makes use of logistic functions to model the (smooth or abrupt)transitions between regimes. The model parameters are estimated by the maximumlikelihood method solved by an Expectation-Maximization algorithm. The proposedapproach can also be regarded as a clustering approach which operates byfinding groups of time series having common changes in regime. In addition toproviding a time series partition, it therefore provides a time seriessegmentation. The problem of selecting the optimal numbers of clusters andsegments is solved by means of the Bayesian Information Criterion (BIC). Theproposed approach is shown to be efficient using a variety of simulated timeseries and real-world time series of electrical power consumption from railswitching operations.
