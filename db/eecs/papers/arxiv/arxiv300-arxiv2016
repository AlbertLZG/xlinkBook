arxiv-300-1 | A Scalable Bootstrap for Massive Data | http://arxiv.org/pdf/1112.5016v2.pdf | author:Ariel Kleiner, Ameet Talwalkar, Purnamrita Sarkar, Michael I. Jordan category:stat.ME stat.CO stat.ML published:2011-12-21 summary:The bootstrap provides a simple and powerful means of assessing the qualityof estimators. However, in settings involving large datasets---which areincreasingly prevalent---the computation of bootstrap-based quantities can beprohibitively demanding computationally. While variants such as subsampling andthe $m$ out of $n$ bootstrap can be used in principle to reduce the cost ofbootstrap computations, we find that these methods are generally not robust tospecification of hyperparameters (such as the number of subsampled datapoints), and they often require use of more prior information (such as rates ofconvergence of estimators) than the bootstrap. As an alternative, we introducethe Bag of Little Bootstraps (BLB), a new procedure which incorporates featuresof both the bootstrap and subsampling to yield a robust, computationallyefficient means of assessing the quality of estimators. BLB is well suited tomodern parallel and distributed computing architectures and furthermore retainsthe generic applicability and statistical efficiency of the bootstrap. Wedemonstrate BLB's favorable statistical performance via a theoretical analysiselucidating the procedure's properties, as well as a simulation study comparingBLB to the bootstrap, the $m$ out of $n$ bootstrap, and subsampling. Inaddition, we present results from a large-scale distributed implementation ofBLB demonstrating its computational superiority on massive data, a method foradaptively selecting BLB's hyperparameters, an empirical study applying BLB toseveral real datasets, and an extension of BLB to time series data.
arxiv-300-2 | Modeling transition dynamics in MDPs with RKHS embeddings of conditional distributions | http://arxiv.org/pdf/1112.4722v2.pdf | author:Steffen Grünewälder, Luca Baldassarre, Massimiliano Pontil, Arthur Gretton, Guy Lever category:cs.LG published:2011-12-20 summary:We propose a new, nonparametric approach to estimating the value function inreinforcement learning. This approach makes use of a recently developedrepresentation of conditional distributions as functions in a reproducingkernel Hilbert space. Such representations bypass the need for estimatingtransition probabilities, and apply to any domain on which kernels can bedefined. Our approach avoids the need to approximate intractable integralssince expectations are represented as RKHS inner products whose computation haslinear complexity in the sample size. Thus, we can efficiently perform valuefunction estimation in a wide variety of settings, including finite statespaces, continuous states spaces, and partially observable tasks where onlysensor measurements are available. A second advantage of the approach is thatwe learn the conditional distribution representation from a training sample,and do not require an exhaustive exploration of the state space. We proveconvergence of our approach either to the optimal policy, or to the closestprojection of the optimal policy in our model class, under reasonableassumptions. In experiments, we demonstrate the performance of our algorithm ona learning task in a continuous state space (the under-actuated pendulum), andon a navigation problem where only images from a sensor are observed. Wecompare with least-squares policy iteration where a Gaussian process is usedfor value function estimation. Our algorithm achieves better performance inboth tasks.
arxiv-300-3 | A Novel M-Estimator for Robust PCA | http://arxiv.org/pdf/1112.4863v4.pdf | author:Teng Zhang, Gilad Lerman category:stat.ML math.OC published:2011-12-20 summary:We study the basic problem of robust subspace recovery. That is, we assume adata set that some of its points are sampled around a fixed subspace and therest of them are spread in the whole ambient space, and we aim to recover thefixed underlying subspace. We first estimate "robust inverse sample covariance"by solving a convex minimization procedure; we then recover the subspace by thebottom eigenvectors of this matrix (their number correspond to the number ofeigenvalues close to 0). We guarantee exact subspace recovery under someconditions on the underlying data. Furthermore, we propose a fast iterativealgorithm, which linearly converges to the matrix minimizing the convexproblem. We also quantify the effect of noise and regularization and discussmany other practical and theoretical issues for improving the subspace recoveryin various settings. When replacing the sum of terms in the convex energyfunction (that we minimize) with the sum of squares of terms, we obtain thatthe new minimizer is a scaled version of the inverse sample covariance (whenexists). We thus interpret our minimizer and its subspace (spanned by itsbottom eigenvectors) as robust versions of the empirical inverse covariance andthe PCA subspace respectively. We compare our method with many other algorithmsfor robust PCA on synthetic and real data sets and demonstrate state-of-the-artspeed and accuracy.
arxiv-300-4 | A geometric analysis of subspace clustering with outliers | http://arxiv.org/pdf/1112.4258v5.pdf | author:Mahdi Soltanolkotabi, Emmanuel J. Candés category:cs.IT cs.LG math.IT math.ST stat.ML stat.TH published:2011-12-19 summary:This paper considers the problem of clustering a collection of unlabeled datapoints assumed to lie near a union of lower-dimensional planes. As is common incomputer vision or unsupervised learning applications, we do not know inadvance how many subspaces there are nor do we have any information about theirdimensions. We develop a novel geometric analysis of an algorithm named sparsesubspace clustering (SSC) [In IEEE Conference on Computer Vision and PatternRecognition, 2009. CVPR 2009 (2009) 2790-2797. IEEE], which significantlybroadens the range of problems where it is provably effective. For instance, weshow that SSC can recover multiple subspaces, each of dimension comparable tothe ambient dimension. We also prove that SSC can correctly cluster data pointseven when the subspaces of interest intersect. Further, we develop an extensionof SSC that succeeds when the data set is corrupted with possiblyoverwhelmingly many outliers. Underlying our analysis are clear geometricinsights, which may bear on other sparse recovery problems. A numerical studycomplements our theoretical analysis and demonstrates the effectiveness ofthese methods.
arxiv-300-5 | Oracle inequalities and minimax rates for non-local means and related adaptive kernel-based methods | http://arxiv.org/pdf/1112.4434v2.pdf | author:Ery Arias-Castro, Joseph Salmon, Rebecca Willett category:math.ST cs.CV cs.IT math.IT stat.TH published:2011-12-19 summary:This paper describes a novel theoretical characterization of the performanceof non-local means (NLM) for noise removal. NLM has proven effective in avariety of empirical studies, but little is understood fundamentally about howit performs relative to classical methods based on wavelets or how variousparameters (e.g., patch size) should be chosen. For cartoon images and imageswhich may contain thin features and regular textures, the error decay rates ofNLM are derived and compared with those of linear filtering, oracle estimators,variable-bandwidth kernel methods, Yaroslavsky's filter and waveletthresholding estimators. The trade-off between global and local search formatching patches is examined, and the bias reduction associated with the localpolynomial regression version of NLM is analyzed. The theoretical results arevalidated via simulations for 2D images corrupted by additive white Gaussiannoise.
arxiv-300-6 | A Geometric Approach For Fully Automatic Chromosome Segmentation | http://arxiv.org/pdf/1112.4164v5.pdf | author:Shervin Minaee, Mehran Fotouhi, Babak Hossein Khalaj category:cs.CV published:2011-12-18 summary:A fundamental task in human chromosome analysis is chromosome segmentation.Segmentation plays an important role in chromosome karyotyping. The first stepin segmentation is to remove intrusive objects such as stain debris and othernoises. The next step is detection of touching and overlapping chromosomes, andthe final step is separation of such chromosomes. Common methods for separationbetween touching chromosomes are interactive and require human intervention forcorrect separation between touching and overlapping chromosomes. In this paper,a geometric-based method is used for automatic detection of touching andoverlapping chromosomes and separating them. The proposed scheme performssegmentation in two phases. In the first phase, chromosome clusters aredetected using three geometric criteria, and in the second phase, chromosomeclusters are separated using a cut-line. Most of earlier methods did not workproperly in case of chromosome clusters that contained more than twochromosomes. Our method, on the other hand, is quite efficient in separation ofsuch chromosome clusters. At each step, one separation will be performed andthis algorithm is repeated until all individual chromosomes are separated.Another important point about the proposed method is that it uses the geometricfeatures of chromosomes which are independent of the type of images and it caneasily be applied to any type of images such as binary images and does notrequire multispectral images as well. We have applied our method to a databasecontaining 62 touching and partially overlapping chromosomes and a success rateof 91.9% is achieved.
arxiv-300-7 | Ensemble Models with Trees and Rules | http://arxiv.org/pdf/1112.3699v8.pdf | author:Deniz Akdemir category:stat.ML published:2011-12-16 summary:In this article, we have proposed several approaches for post processing alarge ensemble of prediction models or rules. The results from our simulationsshow that the post processing methods we have considered here are promising. Wehave used the techniques developed here for estimation of quantitative traitsfrom markers, on the benchmark "Bostob Housing"data set and in somesimulations. In most cases, the produced models had better predictionperformance than, for example, the ones produced by the random forest or therulefit algorithms.
arxiv-300-8 | Echoes of power: Language effects and power differences in social interaction | http://arxiv.org/pdf/1112.3670v3.pdf | author:Cristian Danescu-Niculescu-Mizil, Lillian Lee, Bo Pang, Jon Kleinberg category:cs.SI cs.CL physics.soc-ph published:2011-12-15 summary:Understanding social interaction within groups is key to analyzing onlinecommunities. Most current work focuses on structural properties: who talks towhom, and how such interactions form larger network structures. Theinteractions themselves, however, generally take place in the form of naturallanguage --- either spoken or written --- and one could reasonably suppose thatsignals manifested in language might also provide information about roles,status, and other aspects of the group's dynamics. To date, however, findingsuch domain-independent language-based signals has been a challenge. Here, we show that in group discussions power differentials betweenparticipants are subtly revealed by how much one individual immediately echoesthe linguistic style of the person they are responding to. Starting from thisobservation, we propose an analysis framework based on linguistic coordinationthat can be used to shed light on power relationships and that worksconsistently across multiple types of power --- including a more "static" formof power based on status differences, and a more "situational" form of power inwhich one individual experiences a type of dependence on another. Using thisframework, we study how conversational behavior can reveal power relationshipsin two very different settings: discussions among Wikipedians and argumentsbefore the U.S. Supreme Court.
arxiv-300-9 | Higher-Order Momentum Distributions and Locally Affine LDDMM Registration | http://arxiv.org/pdf/1112.3166v2.pdf | author:Stefan Sommer, Mads Nielsen, Sune Darkner, Xavier Pennec category:cs.CV cs.NA published:2011-12-14 summary:To achieve sparse parametrizations that allows intuitive analysis, we aim torepresent deformation with a basis containing interpretable elements, and wewish to use elements that have the description capacity to represent thedeformation compactly. To accomplish this, we introduce in this paperhigher-order momentum distributions in the LDDMM registration framework. Whilethe zeroth order moments previously used in LDDMM only describe localdisplacement, the first-order momenta that are proposed here represent a basisthat allows local description of affine transformations and subsequent compactdescription of non-translational movement in a globally non-rigid deformation.The resulting representation contains directly interpretable information fromboth mathematical and modeling perspectives. We develop the mathematicalconstruction of the registration framework with higher-order momenta, we showthe implications for sparse image registration and deformation description, andwe provide examples of how the parametrization enables registration with a verylow number of parameters. The capacity and interpretability of theparametrization using higher-order momenta lead to natural modeling ofarticulated movement, and the method promises to be useful for quantifyingventricle expansion and progressing atrophy during Alzheimer's disease.
arxiv-300-10 | Semi-Supervised Anomaly Detection - Towards Model-Independent Searches of New Physics | http://arxiv.org/pdf/1112.3329v3.pdf | author:Mikael Kuusela, Tommi Vatanen, Eric Malmi, Tapani Raiko, Timo Aaltonen, Yoshikazu Nagai category:hep-ex stat.AP stat.ML published:2011-12-14 summary:Most classification algorithms used in high energy physics fall under thecategory of supervised machine learning. Such methods require a training setcontaining both signal and background events and are prone to classificationerrors should this training data be systematically inaccurate for example dueto the assumed MC model. To complement such model-dependent searches, wepropose an algorithm based on semi-supervised anomaly detection techniques,which does not require a MC training sample for the signal data. We first modelthe background using a multivariate Gaussian mixture model. We then search fordeviations from this model by fitting to the observations a mixture of thebackground model and a number of additional Gaussians. This allows us toperform pattern recognition of any anomalous excess over the background. Weshow by a comparison to neural network classifiers that such an approach is alot more robust against misspecification of the signal MC than supervisedclassification. In cases where there is an unexpected signal, a neural networkmight fail to correctly identify it, while anomaly detection does not sufferfrom such a limitation. On the other hand, when there are no systematic errorsin the training data, both methods perform comparably.
arxiv-300-11 | Supervised Generative Reconstruction: An Efficient Way To Flexibly Store and Recognize Patterns | http://arxiv.org/pdf/1112.2988v2.pdf | author:Tsvi Achler category:cs.CV published:2011-12-13 summary:Matching animal-like flexibility in recognition and the ability to quicklyincorporate new information remains difficult. Limits are yet to be adequatelyaddressed in neural models and recognition algorithms. This work proposes aconfiguration for recognition that maintains the same function of conventionalalgorithms but avoids combinatorial problems. Feedforward recognitionalgorithms such as classical artificial neural networks and machine learningalgorithms are known to be subject to catastrophic interference and forgetting.Modifying or learning new information (associations between patterns andlabels) causes loss of previously learned information. I demonstrate usingmathematical analysis how supervised generative models, with feedforward andfeedback connections, can emulate feedforward algorithms yet avoid catastrophicinterference and forgetting. Learned information in generative models is storedin a more intuitive form that represents the fixed points or solutions of thenetwork and moreover displays similar difficulties as cognitive phenomena.Brain-like capabilities and limits associated with generative models suggestthe brain may perform recognition and store information using a similarapproach. Because of the central role of recognition, progress understandingthe underlying principles may reveal significant insight on how to better studyand integrate with the brain.
arxiv-300-12 | A new variational principle for the Euclidean distance function: Linear approach to the non-linear eikonal problem | http://arxiv.org/pdf/1112.3010v4.pdf | author:Karthik S. Gurumoorthy, Anand Rangarajan category:cs.CV math.NA 65D18, 65M80 published:2011-12-13 summary:We present a fast convolution-based technique for computing an approximate,signed Euclidean distance function $S$ on a set of 2D and 3D grid locations.Instead of solving the non-linear, static Hamilton-Jacobi equation ($\\nablaS\=1$), our solution stems from first solving for a scalar field $\phi$ in alinear differential equation and then deriving the solution for $S$ by takingthe negative logarithm. In other words, when $S$ and $\phi$ are related by$\phi = \exp \left(-\frac{S}{\tau} \right)$ and $\phi$ satisfies a specificlinear differential equation corresponding to the extremum of a variationalproblem, we obtain the approximate Euclidean distance function $S = -\tau\log(\phi)$ which converges to the true solution in the limit as $\tau\rightarrow 0$. This is in sharp contrast to techniques like the fast marchingand fast sweeping methods which directly solve the Hamilton-Jacobi equation bythe Godunov upwind discretization scheme. Our linear formulation results in aclosed-form solution to the approximate Euclidean distance function expressibleas a discrete convolution, and hence efficiently computable using the fastFourier transform (FFT). Our solution also circumvents the need for spatialdiscretization of the derivative operator. As $\tau\rightarrow0$ we show theconvergence of our results to the true solution and also bound the error for agiven value of $\tau$. The differentiability of our solution allows us tocompute---using a set of convolutions---the first and second derivatives of theapproximate distance function. In order to determine the sign of the distancefunction (defined to be positive inside a closed region and negative outside),we compute the winding number in 2D and the topological degree in 3D, whosecomputations can also be performed via fast convolutions. We demonstrate theefficacy of our method through a set of experimental results.
arxiv-300-13 | Low-rank optimization with trace norm penalty | http://arxiv.org/pdf/1112.2318v2.pdf | author:B. Mishra, G. Meyer, F. Bach, R. Sepulchre category:math.OC cs.LG published:2011-12-11 summary:The paper addresses the problem of low-rank trace norm minimization. Wepropose an algorithm that alternates between fixed-rank optimization andrank-one updates. The fixed-rank optimization is characterized by an efficientfactorization that makes the trace norm differentiable in the search space andthe computation of duality gap numerically tractable. The search space isnonlinear but is equipped with a particular Riemannian structure that leads toefficient computations. We present a second-order trust-region algorithm with aguaranteed quadratic rate of convergence. Overall, the proposed optimizationscheme converges super-linearly to the global solution while maintainingcomplexity that is linear in the number of rows and columns of the matrix. Tocompute a set of solutions efficiently for a grid of regularization parameterswe propose a predictor-corrector approach that outperforms the naivewarm-restart approach on the fixed-rank quotient manifold. The performance ofthe proposed algorithm is illustrated on problems of low-rank matrix completionand multivariate linear regression.
arxiv-300-14 | Green's function based unparameterised multi-dimensional kernel density and likelihood ratio estimator | http://arxiv.org/pdf/1112.2093v2.pdf | author:Peter Kovesarki, Ian C. Brock, A. Elizabeth Nuncio Quiroz category:stat.ML math.ST stat.TH published:2011-12-09 summary:This paper introduces a probability density estimator based on Green'sfunction identities. A density model is constructed under the sole assumptionthat the probability density is differentiable. The method is implemented as abinary likelihood estimator for classification purposes, so issues such asmis-modeling and overtraining are also discussed. The identity behind thedensity estimator can be interpreted as a real-valued, non-scalar kernel methodwhich is able to reconstruct differentiable density functions.
arxiv-300-15 | Pure Strategy or Mixed Strategy? | http://arxiv.org/pdf/1112.1517v4.pdf | author:Jun He, Feidun He, Hongbin Dong category:cs.NE published:2011-12-07 summary:Mixed strategy EAs aim to integrate several mutation operators into a singlealgorithm. However few theoretical analysis has been made to answer thequestion whether and when the performance of mixed strategy EAs is better thanthat of pure strategy EAs. In theory, the performance of EAs can be measured byasymptotic convergence rate and asymptotic hitting time. In this paper, it isproven that given a mixed strategy (1+1) EAs consisting of several mutationoperators, its performance (asymptotic convergence rate and asymptotic hittingtime)is not worse than that of the worst pure strategy (1+1) EA using onemutation operator; if these mutation operators are mutually complementary, thenit is possible to design a mixed strategy (1+1) EA whose performance is betterthan that of any pure strategy (1+1) EA using one mutation operator.
arxiv-300-16 | A recursive procedure for density estimation on the binary hypercube | http://arxiv.org/pdf/1112.1450v2.pdf | author:Maxim Raginsky, Jorge Silva, Svetlana Lazebnik, Rebecca Willett category:math.ST stat.ML stat.TH published:2011-12-07 summary:This paper describes a recursive estimation procedure for multivariate binarydensities (probability distributions of vectors of Bernoulli random variables)using orthogonal expansions. For $d$ covariates, there are $2^d$ basiscoefficients to estimate, which renders conventional approaches computationallyprohibitive when $d$ is large. However, for a wide class of densities thatsatisfy a certain sparsity condition, our estimator runs in probabilisticpolynomial time and adapts to the unknown sparsity of the underlying density intwo key ways: (1) it attains near-minimax mean-squared error for moderatesample sizes, and (2) the computational complexity is lower for sparserdensities. Our method also allows for flexible control of the trade-off betweenmean-squared error and computational complexity.
arxiv-300-17 | Re-initialization Free Level Set Evolution via Reaction Diffusion | http://arxiv.org/pdf/1112.1496v3.pdf | author:Kaihua Zhang, Lei Zhang, Huihui Song, David Zhang category:cs.CV published:2011-12-07 summary:This paper presents a novel reaction-diffusion (RD) method for implicitactive contours, which is completely free of the costly re-initializationprocedure in level set evolution (LSE). A diffusion term is introduced intoLSE, resulting in a RD-LSE equation, to which a piecewise constant solution canbe derived. In order to have a stable numerical solution of the RD based LSE,we propose a two-step splitting method (TSSM) to iteratively solve the RD-LSEequation: first iterating the LSE equation, and then solving the diffusionequation. The second step regularizes the level set function obtained in thefirst step to ensure stability, and thus the complex and costlyre-initialization procedure is completely eliminated from LSE. By successfullyapplying diffusion to LSE, the RD-LSE model is stable by means of the simplefinite difference method, which is very easy to implement. The proposed RDmethod can be generalized to solve the LSE for both variational level setmethod and PDE-based level set method. The RD-LSE method shows very goodperformance on boundary anti-leakage, and it can be readily extended to highdimensional level set method. The extensive and promising experimental resultson synthetic and real images validate the effectiveness of the proposed RD-LSEapproach.
arxiv-300-18 | Multi-timescale Nexting in a Reinforcement Learning Robot | http://arxiv.org/pdf/1112.1133v3.pdf | author:Joseph Modayil, Adam White, Richard S. Sutton category:cs.LG cs.RO published:2011-12-06 summary:The term "nexting" has been used by psychologists to refer to the propensityof people and many other animals to continually predict what will happen nextin an immediate, local, and personal sense. The ability to "next" constitutes abasic kind of awareness and knowledge of one's environment. In this paper wepresent results with a robot that learns to next in real time, predictingthousands of features of the world's state, including all sensory inputs, attimescales from 0.1 to 8 seconds. This was achieved by treating each statefeature as a reward-like target and applying temporal-difference methods tolearn a corresponding value function with a discount rate corresponding to thetimescale. We show that two thousand predictions, each dependent on sixthousand state features, can be learned and updated online at better than 10Hzon a laptop computer, using the standard TD(lambda) algorithm with linearfunction approximation. We show that this approach is efficient enough to bepractical, with most of the learning complete within 30 minutes. We also showthat a single tile-coded feature representation suffices to accurately predictmany different signals at a significant range of timescales. Finally, we showthat the accuracy of our learned predictions compares favorably with theoptimal off-line solution.
arxiv-300-19 | Clustering under Perturbation Resilience | http://arxiv.org/pdf/1112.0826v4.pdf | author:Maria Florina Balcan, Yingyu Liang category:cs.LG cs.DS published:2011-12-05 summary:Motivated by the fact that distances between data points in many real-worldclustering instances are often based on heuristic measures, Bilu andLinial~\cite{BL} proposed analyzing objective based clustering problems underthe assumption that the optimum clustering to the objective is preserved undersmall multiplicative perturbations to distances between points. The hope isthat by exploiting the structure in such instances, one can overcome worst casehardness results. In this paper, we provide several strong results within this framework. Forcenter-based objectives, we present an algorithm that can optimally clusterinstances resilient to perturbations of factor $(1 + \sqrt{2})$, solving anopen problem of Awasthi et al.~\cite{ABS10}. For $k$-median, a center-basedobjective of special interest, we additionally give algorithms for a morerelaxed assumption in which we allow the optimal solution to change in a small$\epsilon$ fraction of the points after perturbation. We give the first boundsknown for $k$-median under this more realistic and more general assumption. Wealso provide positive results for min-sum clustering which is a generally muchharder objective than center-based objectives. Our algorithms are based on newlinkage criteria that may be of independent interest. Additionally, we give sublinear-time algorithms, showing algorithms that canreturn an implicit clustering from only access to a small random sample.
arxiv-300-20 | On best subset regression | http://arxiv.org/pdf/1112.0918v2.pdf | author:Shifeng Xiong category:stat.ME stat.CO stat.ML 62J07, 62F12 G.3 published:2011-12-05 summary:In this paper we discuss the variable selection method from \ell0-normconstrained regression, which is equivalent to the problem of finding the bestsubset of a fixed size. Our study focuses on two aspects, consistency andcomputation. We prove that the sparse estimator from such a method can retainall of the important variables asymptotically for even exponentially growingdimensionality under regularity conditions. This indicates that the best subsetregression method can efficiently shrink the full model down to a submodel of asize less than the sample size, which can be analyzed by well-developedregression techniques for such cases in a follow-up study. We provide aniterative algorithm, called orthogonalizing subset selection (OSS), to addresscomputational issues in best subset regression. OSS is an EM algorithm, andthus possesses the monotonicity property. For any sparse estimator, OSS canimprove its fit of the model by putting it as an initial point. After thisimprovement, the sparsity of the estimator is kept. Another appealing featureof OSS is that, similarly to an effective algorithm for a continuousoptimization problem, OSS can converge to the global solution to the \ell0-normconstrained regression problem if the initial point lies in a neighborhood ofthe global solution. An accelerating algorithm of OSS and its combination withforward stepwise selection are also investigated. Simulations and a realexample are presented to evaluate the performances of the proposed methods.
arxiv-300-21 | Machine Learning with Operational Costs | http://arxiv.org/pdf/1112.0698v4.pdf | author:Theja Tulabandhula, Cynthia Rudin category:stat.ML cs.AI math.OC published:2011-12-03 summary:This work proposes a way to align statistical modeling with decision making.We provide a method that propagates the uncertainty in predictive modeling tothe uncertainty in operational cost, where operational cost is the amount spentby the practitioner in solving the problem. The method allows us to explore therange of operational costs associated with the set of reasonable statisticalmodels, so as to provide a useful way for practitioners to understanduncertainty. To do this, the operational cost is cast as a regularization termin a learning algorithm's objective function, allowing either an optimistic orpessimistic view of possible costs, depending on the regularization parameter.From another perspective, if we have prior knowledge about the operationalcost, for instance that it should be low, this knowledge can help to restrictthe hypothesis space, and can help with generalization. We provide atheoretical generalization bound for this scenario. We also show that learningwith operational costs is related to robust optimization.
arxiv-300-22 | Merging Belief Propagation and the Mean Field Approximation: A Free Energy Approach | http://arxiv.org/pdf/1112.0467v3.pdf | author:Erwin Riegler, Gunvor Elisabeth Kirkelund, Carles Navarro Manchón, Mihai-Alin Badiu, Bernard Henry Fleury category:cs.IT math.IT stat.ML published:2011-12-02 summary:We present a joint message passing approach that combines belief propagationand the mean field approximation. Our analysis is based on the region-basedfree energy approximation method proposed by Yedidia et al. We show that themessage passing fixed-point equations obtained with this combination correspondto stationary points of a constrained region-based free energy approximation.Moreover, we present a convergent implementation of these message passingfixedpoint equations provided that the underlying factor graph fulfills certaintechnical conditions. In addition, we show how to include hard constraints inthe part of the factor graph corresponding to belief propagation. Finally, wedemonstrate an application of our method to iterative channel estimation anddecoding in an orthogonal frequency division multiplexing (OFDM) system.
arxiv-300-23 | Bandit Market Makers | http://arxiv.org/pdf/1112.0076v4.pdf | author:Nicolas Della Penna, Mark D. Reid category:q-fin.TR cs.GT stat.ML published:2011-12-01 summary:We introduce a modular framework for market making. It combines cost-functionbased automated market makers with bandit algorithms. We obtain worst-caseprofits guarantee's relative to the best in hindsight within a class of natural"overround" cost functions . This combination allow us to havedistribution-free guarantees on the regret of profits while preserving thebounded worst-case losses and computational tractability over combinatorialspaces of the cost function based approach. We present simulation results tobetter understand the practical behaviour of market makers from the framework.
arxiv-300-24 | Determining a rotation of a tetrahedron from a projection | http://arxiv.org/pdf/1111.7100v2.pdf | author:Richard J. Gardner, Paolo Gronchi, Thorsten Theobald category:math.MG cs.CG cs.CV published:2011-11-30 summary:The following problem, arising from medical imaging, is addressed: Supposethat $T$ is a known tetrahedron in $\R^3$ with centroid at the origin. Alsoknown is the orthogonal projection $U$ of the vertices of the image $\phi T$ of$T$ under an unknown rotation $\phi$ about the origin. Under what circumstancescan $\phi$ be determined from $T$ and $U$?
arxiv-300-25 | Efficient Discovery of Association Rules and Frequent Itemsets through Sampling with Tight Performance Guarantees | http://arxiv.org/pdf/1111.6937v6.pdf | author:Matteo Riondato, Eli Upfal category:cs.DS cs.DB cs.LG H.2.8 published:2011-11-29 summary:The tasks of extracting (top-$K$) Frequent Itemsets (FI's) and AssociationRules (AR's) are fundamental primitives in data mining and databaseapplications. Exact algorithms for these problems exist and are widely used,but their running time is hindered by the need of scanning the entire dataset,possibly multiple times. High quality approximations of FI's and AR's aresufficient for most practical uses, and a number of recent works explored theapplication of sampling for fast discovery of approximate solutions to theproblems. However, these works do not provide satisfactory performanceguarantees on the quality of the approximation, due to the difficulty ofbounding the probability of under- or over-sampling any one of an unknownnumber of frequent itemsets. In this work we circumvent this issue by applyingthe statistical concept of \emph{Vapnik-Chervonenkis (VC) dimension} to developa novel technique for providing tight bounds on the sample size that guaranteesapproximation within user-specified parameters. Our technique applies both toabsolute and to relative approximations of (top-$K$) FI's and AR's. Theresulting sample size is linearly dependent on the VC-dimension of a rangespace associated with the dataset to be mined. The main theoreticalcontribution of this work is a proof that the VC-dimension of this range spaceis upper bounded by an easy-to-compute characteristic quantity of the datasetwhich we call \emph{d-index}, and is the maximum integer $d$ such that thedataset contains at least $d$ transactions of length at least $d$ such that noone of them is a superset of or equal to another. We show that this bound isstrict for a large class of datasets.
arxiv-300-26 | Developing Embodied Multisensory Dialogue Agents | http://arxiv.org/pdf/1111.7190v3.pdf | author:Michał B. Paradowski category:cs.AI cs.CL published:2011-11-29 summary:A few decades of work in the AI field have focused efforts on developing anew generation of systems which can acquire knowledge via interaction with theworld. Yet, until very recently, most such attempts were underpinned byresearch which predominantly regarded linguistic phenomena as separated fromthe brain and body. This could lead one into believing that to emulatelinguistic behaviour, it suffices to develop 'software' operating on abstractrepresentations that will work on any computational machine. This picture isinaccurate for several reasons, which are elucidated in this paper and extendbeyond sensorimotor and semantic resonance. Beginning with a review ofresearch, I list several heterogeneous arguments against disembodied language,in an attempt to draw conclusions for developing embodied multisensory agentswhich communicate verbally and non-verbally with their environment. Withouttaking into account both the architecture of the human brain, and embodiment,it is unrealistic to replicate accurately the processes which take place duringlanguage acquisition, comprehension, production, or during non-linguisticactions. While robots are far from isomorphic with humans, they could benefitfrom strengthened associative connections in the optimization of theirprocesses and their reactivity and sensitivity to environmental stimuli, and insituated human-machine interaction. The concept of multisensory integrationshould be extended to cover linguistic input and the complementary informationcombined from temporally coincident sensory impressions.
arxiv-300-27 | Gaussian Probabilities and Expectation Propagation | http://arxiv.org/pdf/1111.6832v2.pdf | author:John P. Cunningham, Philipp Hennig, Simon Lacoste-Julien category:stat.ML published:2011-11-29 summary:While Gaussian probability densities are omnipresent in applied mathematics,Gaussian cumulative probabilities are hard to calculate in any but theunivariate case. We study the utility of Expectation Propagation (EP) as anapproximate integration method for this problem. For rectangular integrationregions, the approximation is highly accurate. We also extend the derivationsto the more general case of polyhedral integration regions. However, we findthat in this polyhedral case, EP's answer, though often accurate, can be almostarbitrarily wrong. We consider these unexpected results empirically andtheoretically, both for the problem of Gaussian probabilities and for EP moregenerally. These results elucidate an interesting and non-obvious feature of EPnot yet studied in detail.
arxiv-300-28 | Multivariate information measures: an experimentalist's perspective | http://arxiv.org/pdf/1111.6857v5.pdf | author:Nicholas Timme, Wesley Alford, Benjamin Flecker, John M. Beggs category:cs.IT cs.LG math.IT stat.AP 94A15 published:2011-11-28 summary:Information theory is widely accepted as a powerful tool for analyzingcomplex systems and it has been applied in many disciplines. Recently, somecentral components of information theory - multivariate information measures -have found expanded use in the study of several phenomena. These informationmeasures differ in subtle yet significant ways. Here, we will review theinformation theory behind each measure, as well as examine the differencesbetween these measures by applying them to several simple model systems. Inaddition to these systems, we will illustrate the usefulness of the informationmeasures by analyzing neural spiking data from a dissociated culture throughearly stages of its development. We hope that this work will aid otherresearchers as they seek the best multivariate information measure for theirspecific research goals and system. Finally, we have made software availableonline which allows the user to calculate all of the information measuresdiscussed within this paper.
arxiv-300-29 | Regret Bound by Variation for Online Convex Optimization | http://arxiv.org/pdf/1111.6337v4.pdf | author:Tianbao Yang, Mehrdad Mahdavi, Rong Jin, Shenghuo Zhu category:cs.LG published:2011-11-28 summary:In citep{Hazan-2008-extract}, the authors showed that the regret of onlinelinear optimization can be bounded by the total variation of the cost vectors.In this paper, we extend this result to general online convex optimization. Wefirst analyze the limitations of the algorithm in \citep{Hazan-2008-extract}when applied it to online convex optimization. We then present two algorithmsfor online convex optimization whose regrets are bounded by the variation ofcost functions. We finally consider the bandit setting, and present arandomized algorithm for online bandit convex optimization with avariation-based regret bound. We show that the regret bound for online banditconvex optimization is optimal when the variation of cost functions isindependent of the number of trials.
arxiv-300-30 | Learning with Submodular Functions: A Convex Optimization Perspective | http://arxiv.org/pdf/1111.6453v2.pdf | author:Francis Bach category:cs.LG math.OC published:2011-11-28 summary:Submodular functions are relevant to machine learning for at least tworeasons: (1) some problems may be expressed directly as the optimization ofsubmodular functions and (2) the lovasz extension of submodular functionsprovides a useful set of regularization functions for supervised andunsupervised learning. In this monograph, we present the theory of submodularfunctions from a convex analysis perspective, presenting tight links betweencertain polyhedra, combinatorial optimization and convex optimization problems.In particular, we show how submodular function minimization is equivalent tosolving a wide variety of convex optimization problems. This allows thederivation of new efficient algorithms for approximate and exact submodularfunction minimization with theoretical guarantees and good practicalperformance. By listing many examples of submodular functions, we reviewvarious applications to machine learning, such as clustering, experimentaldesign, sensor placement, graphical model structure learning or subsetselection, as well as a family of structured sparsity-inducing norms that canbe derived and used from submodular functions.
arxiv-300-31 | Learning a Factor Model via Regularized PCA | http://arxiv.org/pdf/1111.6201v4.pdf | author:Yi-Hao Kao, Benjamin Van Roy category:cs.LG stat.ML published:2011-11-26 summary:We consider the problem of learning a linear factor model. We propose aregularized form of principal component analysis (PCA) and demonstrate throughexperiments with synthetic and real data the superiority of resulting estimatesto those produced by pre-existing factor analysis approaches. We also establishtheoretical results that explain how our algorithm corrects the biases inducedby conventional approaches. An important feature of our algorithm is that itscomputational requirements are similar to those of PCA, which enjoys wide usein large part due to its efficiency.
arxiv-300-32 | Automatic Relevance Determination in Nonnegative Matrix Factorization with the β-Divergence | http://arxiv.org/pdf/1111.6085v3.pdf | author:Vincent Y. F. Tan, Cédric Févotte category:stat.ML stat.ME published:2011-11-25 summary:This paper addresses the estimation of the latent dimensionality innonnegative matrix factorization (NMF) with the \beta-divergence. The\beta-divergence is a family of cost functions that includes the squaredEuclidean distance, Kullback-Leibler and Itakura-Saito divergences as specialcases. Learning the model order is important as it is necessary to strike theright balance between data fidelity and overfitting. We propose a Bayesianmodel based on automatic relevance determination in which the columns of thedictionary matrix and the rows of the activation matrix are tied togetherthrough a common scale parameter in their prior. A family ofmajorization-minimization algorithms is proposed for maximum a posteriori (MAP)estimation. A subset of scale parameters is driven to a small lower bound inthe course of inference, with the effect of pruning the corresponding spuriouscomponents. We demonstrate the efficacy and robustness of our algorithms byperforming extensive experiments on synthetic data, the swimmer dataset, amusic decomposition example and a stock price prediction task.
arxiv-300-33 | Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints | http://arxiv.org/pdf/1111.6082v3.pdf | author:Mehrdad Mahdavi, Rong Jin, Tianbao Yang category:cs.LG published:2011-11-25 summary:In this paper we propose a framework for solving constrained online convexoptimization problem. Our motivation stems from the observation that mostalgorithms proposed for online convex optimization require a projection ontothe convex set $\mathcal{K}$ from which the decisions are made. While forsimple shapes (e.g. Euclidean ball) the projection is straightforward, forarbitrary complex sets this is the main computational challenge and may beinefficient in practice. In this paper, we consider an alternative onlineconvex optimization problem. Instead of requiring decisions belong to$\mathcal{K}$ for all rounds, we only require that the constraints which definethe set $\mathcal{K}$ be satisfied in the long run. We show that our frameworkcan be utilized to solve a relaxed version of online learning with sideconstraints addressed in \cite{DBLP:conf/colt/MannorT06} and\cite{DBLP:conf/aaai/KvetonYTM08}. By turning the problem into an onlineconvex-concave optimization problem, we propose an efficient algorithm whichachieves $\tilde{\mathcal{O}}(\sqrt{T})$ regret bound and$\tilde{\mathcal{O}}(T^{3/4})$ bound for the violation of constraints. Then wemodify the algorithm in order to guarantee that the constraints are satisfiedin the long run. This gain is achieved at the price of getting$\tilde{\mathcal{O}}(T^{3/4})$ regret bound. Our second algorithm is based onthe Mirror Prox method \citep{nemirovski-2005-prox} to solve variationalinequalities which achieves $\tilde{\mathcal{\mathcal{O}}}(T^{2/3})$ bound forboth regret and the violation of constraints when the domain $\K$ can bedescribed by a finite number of linear constraints. Finally, we extend theresult to the setting where we only have partial access to the convex set$\mathcal{K}$ and propose a multipoint bandit feedback algorithm with the samebounds in expectation as our first algorithm.
arxiv-300-34 | The Graphical Lasso: New Insights and Alternatives | http://arxiv.org/pdf/1111.5479v2.pdf | author:Rahul Mazumder, Trevor Hastie category:stat.ML cs.LG published:2011-11-23 summary:The graphical lasso \citep{FHT2007a} is an algorithm for learning thestructure in an undirected Gaussian graphical model, using $\ell_1$regularization to control the number of zeros in the precision matrix${\B\Theta}={\B\Sigma}^{-1}$ \citep{BGA2008,yuan_lin_07}. The {\texttt R}package \GL\ \citep{FHT2007a} is popular, fast, and allows one to efficientlybuild a path of models for different values of the tuning parameter.Convergence of \GL\ can be tricky; the converged precision matrix might not bethe inverse of the estimated covariance, and occasionally it fails to convergewith warm starts. In this paper we explain this behavior, and propose newalgorithms that appear to outperform \GL. By studying the "normal equations" we see that, \GL\ is solving the {\emdual} of the graphical lasso penalized likelihood, by block coordinate ascent;a result which can also be found in \cite{BGA2008}. In this dual, the target of estimation is $\B\Sigma$, the covariance matrix,rather than the precision matrix $\B\Theta$. We propose similar primalalgorithms \PGL\ and \DPGL, that also operate by block-coordinate descent,where $\B\Theta$ is the optimization target. We study all of these algorithms,and in particular different approaches to solving their coordinatesub-problems. We conclude that \DPGL\ is superior from several points of view.
arxiv-300-35 | Contextually Guided Semantic Labeling and Search for 3D Point Clouds | http://arxiv.org/pdf/1111.5358v3.pdf | author:Abhishek Anand, Hema Swetha Koppula, Thorsten Joachims, Ashutosh Saxena category:cs.RO cs.AI cs.CV published:2011-11-22 summary:RGB-D cameras, which give an RGB image to- gether with depths, are becomingincreasingly popular for robotic perception. In this paper, we address the taskof detecting commonly found objects in the 3D point cloud of indoor scenesobtained from such cameras. Our method uses a graphical model that capturesvarious features and contextual relations, including the local visualappearance and shape cues, object co-occurence relationships and geometricrelationships. With a large number of object classes and relations, the model'sparsimony becomes important and we address that by using multiple types of edgepotentials. We train the model using a maximum-margin learning approach. In ourexperiments over a total of 52 3D scenes of homes and offices (composed fromabout 550 views), we get a performance of 84.06% and 73.38% in labeling officeand home scenes respectively for 17 object classes each. We also present amethod for a robot to search for an object using the learned model and thecontextual information available from the current labelings of the scene. Weapplied this algorithm successfully on a mobile robot for the task of finding12 object classes in 10 different offices and achieved a precision of 97.56%with 78.43% recall.
arxiv-300-36 | Stochastic gradient descent on Riemannian manifolds | http://arxiv.org/pdf/1111.5280v4.pdf | author:Silvere Bonnabel category:math.OC cs.LG stat.ML published:2011-11-22 summary:Stochastic gradient descent is a simple approach to find the local minima ofa cost function whose evaluations are corrupted by noise. In this paper, wedevelop a procedure extending stochastic gradient descent algorithms to thecase where the function is defined on a Riemannian manifold. We prove that, asin the Euclidian case, the gradient descent algorithm converges to a criticalpoint of the cost function. The algorithm has numerous potential applications,and is illustrated here by four examples. In particular a novel gossipalgorithm on the set of covariance matrices is derived and tested numerically.
arxiv-300-37 | Distributed Multi-view Matching in Networks with Limited Communications | http://arxiv.org/pdf/1111.4840v4.pdf | author:Eduardo Montijano, Rosario Aragues, Carlos Sagues category:cs.CV cs.MA cs.RO published:2011-11-21 summary:We address the problem of distributed matching of features in networks withvision systems. Every camera in the network has limited communicationcapabilities and can only exchange local matches with its neighbors. We proposea distributed algorithm that takes these local matches and computes globalcorrespondences by a proper propagation in the network. When the algorithmfinishes, each camera knows the global correspondences between its features andthe features of all the cameras in the network. The presence of spuriousintroduced by the local matcher may produce inconsistent globalcorrespondences, which are association paths between features from the samecamera. The contributions of this work are the propagation of the local matchesand the detection and resolution of these inconsistencies by deleting localmatches. Our resolution algorithm considers the quality of each local match,when this information is provided by the local matcher. We formally prove thatafter executing the algorithm, the network finishes with a global dataassociation free of inconsistencies. We provide a fully decentralized solutionto the problem which does not rely on any particular communication topology.Simulations and experimental results with real images show the performance ofthe method considering different features, matching functions and scenarios.
arxiv-300-38 | Non-Asymptotic Analysis of Tangent Space Perturbation | http://arxiv.org/pdf/1111.4601v4.pdf | author:Daniel N. Kaslovsky, Francois G. Meyer category:cs.NA stat.ML published:2011-11-20 summary:Constructing an efficient parameterization of a large, noisy data set ofpoints lying close to a smooth manifold in high dimension remains a fundamentalproblem. One approach consists in recovering a local parameterization using thelocal tangent plane. Principal component analysis (PCA) is often the tool ofchoice, as it returns an optimal basis in the case of noise-free samples from alinear subspace. To process noisy data samples from a nonlinear manifold, PCAmust be applied locally, at a scale small enough such that the manifold isapproximately linear, but at a scale large enough such that structure may bediscerned from noise. Using eigenspace perturbation theory and non-asymptoticrandom matrix theory, we study the stability of the subspace estimated by PCAas a function of scale, and bound (with high probability) the angle it formswith the true tangent space. By adaptively selecting the scale that minimizesthis bound, our analysis reveals an appropriate scale for local tangent planerecovery. We also introduce a geometric uncertainty principle quantifying thelimits of noise-curvature perturbation for stable recovery. With the purpose ofproviding perturbation bounds that can be used in practice, we propose plug-inestimates that make it possible to directly apply the theoretical results toreal data sets.
arxiv-300-39 | Equivalence of History and Generator Epsilon-Machines | http://arxiv.org/pdf/1111.4500v2.pdf | author:Nicholas F. Travers, James P. Crutchfield category:math.PR cs.IT math.IT nlin.CD stat.ML published:2011-11-18 summary:Epsilon-machines are minimal, unifilar presentations of stationary stochasticprocesses. They were originally defined in the history machine sense, as hiddenMarkov models whose states are the equivalence classes of infinite pasts withthe same probability distribution over futures. In analyzing synchronization,though, an alternative generator definition was given: unifilar, edge-emittinghidden Markov models with probabilistically distinct states. The key differenceis that history epsilon-machines are defined by a process, whereas generatorepsilon-machines define a process. We show here that these two definitions areequivalent in the finite-state case.
arxiv-300-40 | Efficient Regression in Metric Spaces via Approximate Lipschitz Extension | http://arxiv.org/pdf/1111.4470v2.pdf | author:Lee-Ad Gottlieb, Aryeh Kontorovich, Robert Krauthgamer category:cs.LG published:2011-11-18 summary:We present a framework for performing efficient regression in general metricspaces. Roughly speaking, our regressor predicts the value at a new point bycomputing a Lipschitz extension --- the smoothest function consistent with theobserved data --- after performing structural risk minimization to avoidoverfitting. We obtain finite-sample risk bounds with minimal structural andnoise assumptions, and a natural speed-precision tradeoff. The offline(learning) and online (prediction) stages can be solved by convex programming,but this naive approach has runtime complexity $O(n^3)$, which is prohibitivefor large datasets. We design instead a regression algorithm whose speed andgeneralization performance depend on the intrinsic dimension of the data, towhich the algorithm adapts. While our main innovation is algorithmic, thestatistical results may also be of independent interest.
arxiv-300-41 | A note on the lack of symmetry in the graphical lasso | http://arxiv.org/pdf/1111.2667v2.pdf | author:Benjamin T. Rolfs, Bala Rajaratnam category:stat.ML stat.CO published:2011-11-11 summary:The graphical lasso (glasso) is a widely-used fast algorithm for estimatingsparse inverse covariance matrices. The glasso solves an L1 penalized maximumlikelihood problem and is available as an R library on CRAN. The output fromthe glasso, a regularized covariance matrix estimate a sparse inversecovariance matrix estimate, not only identify a graphical model but can alsoserve as intermediate inputs into multivariate procedures such as PCA, LDA,MANOVA, and others. The glasso indeed produces a covariance matrix estimatewhich solves the L1 penalized optimization problem in a dual sense; however,the method for producing the inverse covariance matrix estimator after thisoptimization is inexact and may produce asymmetric estimates. This problem isexacerbated when the amount of L1 regularization that is applied is small,which in turn is more likely to occur if the true underlying inverse covariancematrix is not sparse. The lack of symmetry can potentially have consequences.First, it implies that the covariance and inverse covariance estimates are notnumerical inverses of one another, and second, asymmetry can possibly lead tonegative or complex eigenvalues,rendering many multivariate procedures whichmay depend on the inverse covariance estimator unusable. We demonstrate thisproblem, explain its causes, and propose possible remedies.
arxiv-300-42 | Accuracy guaranties for $\ell_1$ recovery of block-sparse signals | http://arxiv.org/pdf/1111.2546v2.pdf | author:Anatoli Juditsky, Fatma Kılınç Karzan, Arkadi Nemirovski, Boris Polyak category:math.ST math.OC stat.ML stat.TH published:2011-11-10 summary:We introduce a general framework to handle structured models (sparse andblock-sparse with possibly overlapping blocks). We discuss new methods fortheir recovery from incomplete observation, corrupted with deterministic andstochastic noise, using block-$\ell_1$ regularization. While the current theoryprovides promising bounds for the recovery errors under a number of different,yet mostly hard to verify conditions, our emphasis is on verifiable conditionson the problem parameters (sensing matrix and the block structure) whichguarantee accurate recovery. Verifiability of our conditions not only leads toefficiently computable bounds for the recovery error but also allows us tooptimize these error bounds with respect to the method parameters, andtherefore construct estimators with improved statistical properties. To justifyour approach, we also provide an oracle inequality, which links the propertiesof the proposed recovery algorithms and the best estimation performance.Furthermore, utilizing these verifiable conditions, we develop acomputationally cheap alternative to block-$\ell_1$ minimization, thenon-Euclidean Block Matching Pursuit algorithm. We close by presenting anumerical study to investigate the effect of different block regularizationsand demonstrate the performance of the proposed recoveries.
arxiv-300-43 | Improved Bound for the Nystrom's Method and its Application to Kernel Classification | http://arxiv.org/pdf/1111.2262v4.pdf | author:Rong Jin, Tianbao Yang, Mehrdad Mahdavi, Yu-Feng Li, Zhi-Hua Zhou category:cs.LG cs.NA published:2011-11-09 summary:We develop two approaches for analyzing the approximation error bound for theNystr\"{o}m method, one based on the concentration inequality of integraloperator, and one based on the compressive sensing theory. We show that theapproximation error, measured in the spectral norm, can be improved from$O(N/\sqrt{m})$ to $O(N/m^{1 - \rho})$ in the case of large eigengap, where $N$is the total number of data points, $m$ is the number of sampled data points,and $\rho \in (0, 1/2)$ is a positive constant that characterizes the eigengap.When the eigenvalues of the kernel matrix follow a $p$-power law, our analysisbased on compressive sensing theory further improves the bound to $O(N/m^{p -1})$ under an incoherence assumption, which explains why the Nystr\"{o}m methodworks well for kernel matrix with skewed eigenvalues. We present a kernelclassification approach based on the Nystr\"{o}m method and derive itsgeneralization performance using the improved bound. We show that when theeigenvalues of kernel matrix follow a $p$-power law, we can reduce the numberof support vectors to $N^{2p/(p^2 - 1)}$, a number less than $N$ when $p >1+\sqrt{2}$, without seriously sacrificing its generalization performance.
arxiv-300-44 | Ag-dependent (in silico) approach implies a deterministic kinetics for homeostatic memory cell turnover | http://arxiv.org/pdf/1111.2085v3.pdf | author:Alexandre de Castro category:q-bio.CB cs.NE published:2011-11-09 summary:Verhulst-like mathematical modeling has been used to investigate severalcomplex biological issues, such as immune memory equilibrium and cell-mediatedimmunity in mammals. The regulation mechanisms of both these processes arestill not sufficiently understood. In a recent paper, Choo et al. [J. Immunol.,v. 185, pp. 3436-44, 2010], used an Ag-independent approach to quantitativelyanalyze memory cell turnover from some empirical data, and concluded thatimmune homeostasis behaves stochastically, rather than deterministically. Inthe paper here presented, we use an in silico Ag-dependent approach to simulatethe process of antigenic mutation and study its implications for memorydynamics. Our results have suggested a deterministic kinetics for homeostaticequilibrium, what contradicts the Choo et al. findings. Accordingly, ourcalculations are an indication that a more extensive empirical protocol forstudying the homeostatic turnover should be considered.
arxiv-300-45 | Analysis of Thompson Sampling for the multi-armed bandit problem | http://arxiv.org/pdf/1111.1797v3.pdf | author:Shipra Agrawal, Navin Goyal category:cs.LG cs.DS 68W40, 68Q25 F.2.0 published:2011-11-08 summary:The multi-armed bandit problem is a popular model for studyingexploration/exploitation trade-off in sequential decision problems. Manyalgorithms are now available for this well-studied problem. One of the earliestalgorithms, given by W. R. Thompson, dates back to 1933. This algorithm,referred to as Thompson Sampling, is a natural Bayesian algorithm. The basicidea is to choose an arm to play according to its probability of being the bestarm. Thompson Sampling algorithm has experimentally been shown to be close tooptimal. In addition, it is efficient to implement and exhibits severaldesirable properties such as small regret for delayed feedback. However,theoretical understanding of this algorithm was quite limited. In this paper,for the first time, we show that Thompson Sampling algorithm achieveslogarithmic expected regret for the multi-armed bandit problem. More precisely,for the two-armed bandit problem, the expected regret in time $T$ is$O(\frac{\ln T}{\Delta} + \frac{1}{\Delta^3})$. And, for the $N$-armed banditproblem, the expected regret in time $T$ is $O([(\sum_{i=2}^N\frac{1}{\Delta_i^2})^2] \ln T)$. Our bounds are optimal but for the dependenceon $\Delta_i$ and the constant factors in big-Oh.
arxiv-300-46 | Combinatorial clustering and the beta negative binomial process | http://arxiv.org/pdf/1111.1802v5.pdf | author:Tamara Broderick, Lester Mackey, John Paisley, Michael I. Jordan category:stat.ME stat.ML published:2011-11-08 summary:We develop a Bayesian nonparametric approach to a general family of latentclass problems in which individuals can belong simultaneously to multipleclasses and where each class can be exhibited multiple times by an individual.We introduce a combinatorial stochastic process known as the negative binomialprocess (NBP) as an infinite-dimensional prior appropriate for such problems.We show that the NBP is conjugate to the beta process, and we characterize theposterior distribution under the beta-negative binomial process (BNBP) andhierarchical models based on the BNBP (the HBNBP). We study the asymptoticproperties of the BNBP and develop a three-parameter extension of the BNBP thatexhibits power-law behavior. We derive MCMC algorithms for posterior inferenceunder the HBNBP, and we present experiments using these algorithms in thedomains of image segmentation, object recognition, and document analysis.
arxiv-300-47 | Stochastic Belief Propagation: A Low-Complexity Alternative to the Sum-Product Algorithm | http://arxiv.org/pdf/1111.1020v2.pdf | author:Nima Noorshams, Martin J. Wainwright category:cs.IT math.IT stat.ML published:2011-11-04 summary:The sum-product or belief propagation (BP) algorithm is a widely-usedmessage-passing algorithm for computing marginal distributions in graphicalmodels with discrete variables. At the core of the BP message updates, whenapplied to a graphical model with pairwise interactions, lies a matrix-vectorproduct with complexity that is quadratic in the state dimension $d$, andrequires transmission of a $(d-1)$-dimensional vector of real numbers(messages) to its neighbors. Since various applications involve very largestate dimensions, such computation and communication complexities can beprohibitively complex. In this paper, we propose a low-complexity variant ofBP, referred to as stochastic belief propagation (SBP). As suggested by thename, it is an adaptively randomized version of the BP message updates in whicheach node passes randomly chosen information to each of its neighbors. The SBPmessage updates reduce the computational complexity (per iteration) fromquadratic to linear in $d$, without assuming any particular structure of thepotentials, and also reduce the communication complexity significantly,requiring only $\log{d}$ bits transmission per edge. Moreover, we establish anumber of theoretical guarantees for the performance of SBP, showing that itconverges almost surely to the BP fixed point for any tree-structured graph,and for graphs with cycles satisfying a contractivity condition. In addition,for these graphical models, we provide non-asymptotic upper bounds on theconvergence rate, showing that the $\ell_{\infty}$ norm of the error vectordecays no slower than $O(1/\sqrt{t})$ with the number of iterations $t$ ontrees and the mean square error decays as $O(1/t)$ for general graphs. Theseanalysis show that SBP can provably yield reductions in computational andcommunication complexities for various classes of graphical models.
arxiv-300-48 | Towards Analyzing Crossover Operators in Evolutionary Search via General Markov Chain Switching Theorem | http://arxiv.org/pdf/1111.0907v2.pdf | author:Yang Yu, Chao Qian, Zhi-Hua Zhou category:cs.NE published:2011-11-03 summary:Evolutionary algorithms (EAs), simulating the evolution process of naturalspecies, are used to solve optimization problems. Crossover (also calledrecombination), originated from simulating the chromosome exchange phenomena inzoogamy reproduction, is widely employed in EAs to generate offspringsolutions, of which the effectiveness has been examined empirically inapplications. However, due to the irregularity of crossover operators and thecomplicated interactions to mutation, crossover operators are hard to analyzeand thus have few theoretical results. Therefore, analyzing crossover not onlyhelps in understanding EAs, but also helps in developing novel techniques foranalyzing sophisticated metaheuristic algorithms. In this paper, we derive the General Markov Chain Switching Theorem (GMCST)to facilitate theoretical studies of crossover-enabled EAs. The theorem allowsus to analyze the running time of a sophisticated EA from an easy-to-analyzeEA. Using this tool, we analyze EAs with several crossover operators on theLeadingOnes and OneMax problems, which are noticeably two well studied problemsfor mutation-only EAs but with few results for crossover-enabled EAs. We firstderive the bounds of running time of the (2+2)-EA with crossover operators;then we study the running time gap between the mutation-only (2:2)-EA and the(2:2)-EA with crossover operators; finally, we develop strategies that applycrossover operators only when necessary, which improve from the mutation-onlyas well as the crossover-all-the-time (2:2)-EA. The theoretical results areverified by experiments.
arxiv-300-49 | Distributed Lossy Source Coding Using Real-Number Codes | http://arxiv.org/pdf/1111.0654v2.pdf | author:Mojtaba Vaezi, Fabrice Labeau category:cs.IT cs.CV cs.NI math.IT published:2011-11-02 summary:We show how real-number codes can be used to compress correlated sources, andestablish a new framework for lossy distributed source coding, in which wequantize compressed sources instead of compressing quantized sources. Thischange in the order of binning and quantization blocks makes it possible tomodel correlation between continuous-valued sources more realistically andcorrect quantization error when the sources are completely correlated. Theencoding and decoding procedures are described in detail, for discrete Fouriertransform (DFT) codes. Reconstructed signal, in the mean squared error sense,is seen to be better than that in the conventional approach.
arxiv-300-50 | Revisiting k-means: New Algorithms via Bayesian Nonparametrics | http://arxiv.org/pdf/1111.0352v2.pdf | author:Brian Kulis, Michael I. Jordan category:cs.LG stat.ML published:2011-11-02 summary:Bayesian models offer great flexibility for clusteringapplications---Bayesian nonparametrics can be used for modeling infinitemixtures, and hierarchical Bayesian models can be utilized for sharing clustersacross multiple data sets. For the most part, such flexibility is lacking inclassical clustering methods such as k-means. In this paper, we revisit thek-means clustering algorithm from a Bayesian nonparametric viewpoint. Inspiredby the asymptotic connection between k-means and mixtures of Gaussians, we showthat a Gibbs sampling algorithm for the Dirichlet process mixture approaches ahard clustering algorithm in the limit, and further that the resultingalgorithm monotonically minimizes an elegant underlying k-means-like clusteringobjective that includes a penalty for the number of clusters. We generalizethis analysis to the case of clustering multiple data sets through a similarasymptotic argument with the hierarchical Dirichlet process. We also discussfurther extensions that highlight the benefits of our analysis: i) a spectralrelaxation involving thresholded eigenvectors, and ii) a normalized cut graphclustering algorithm that does not fix the number of clusters in the graph.
arxiv-300-51 | Diffusion Adaptation Strategies for Distributed Optimization and Learning over Networks | http://arxiv.org/pdf/1111.0034v3.pdf | author:Jianshu Chen, Ali H. Sayed category:math.OC cs.IT cs.LG cs.SI math.IT physics.soc-ph published:2011-10-31 summary:We propose an adaptive diffusion mechanism to optimize a global cost functionin a distributed manner over a network of nodes. The cost function is assumedto consist of a collection of individual components. Diffusion adaptationallows the nodes to cooperate and diffuse information in real-time; it alsohelps alleviate the effects of stochastic gradient noise and measurement noisethrough a continuous learning process. We analyze the mean-square-errorperformance of the algorithm in some detail, including its transient andsteady-state behavior. We also apply the diffusion algorithm to two problems:distributed estimation with sparse parameters and distributed localization.Compared to well-studied incremental methods, diffusion methods do not requirethe use of a cyclic path over the nodes and are robust to node and linkfailure. Diffusion methods also endow networks with adaptation abilities thatenable the individual nodes to continue learning even when the cost functionchanges with time. Examples involving such dynamic cost functions with movingtargets are common in the context of biological networks.
arxiv-300-52 | PAC-Bayesian Inequalities for Martingales | http://arxiv.org/pdf/1110.6886v3.pdf | author:Yevgeny Seldin, François Laviolette, Nicolò Cesa-Bianchi, John Shawe-Taylor, Peter Auer category:cs.LG cs.IT math.IT stat.ML published:2011-10-31 summary:We present a set of high-probability inequalities that control theconcentration of weighted averages of multiple (possibly uncountably many)simultaneously evolving and interdependent martingales. Our results extend thePAC-Bayesian analysis in learning theory from the i.i.d. setting to martingalesopening the way for its application to importance weighted sampling,reinforcement learning, and other interactive learning domains, as well as manyother domains in probability theory and statistics, where martingales areencountered. We also present a comparison inequality that bounds the expectation of aconvex function of a martingale difference sequence shifted to the [0,1]interval by the expectation of the same function of independent Bernoullivariables. This inequality is applied to derive a tighter analog ofHoeffding-Azuma's inequality.
arxiv-300-53 | Risk-sensitive Markov control processes | http://arxiv.org/pdf/1110.6317v5.pdf | author:Yun Shen, Wilhelm Stannat, Klaus Obermayer category:math.OC cs.CE math.DS stat.ML published:2011-10-28 summary:We introduce a general framework for measuring risk in the context of Markovcontrol processes with risk maps on general Borel spaces that generalize knownconcepts of risk measures in mathematical finance, operations research andbehavioral economics. Within the framework, applying weighted norm spaces toincorporate also unbounded costs, we study two types of infinite-horizonrisk-sensitive criteria, discounted total risk and average risk, and solve theassociated optimization problems by dynamic programming. For the discountedcase, we propose a new discount scheme, which is different from theconventional form but consistent with the existing literature, while for theaverage risk criterion, we state Lyapunov-like stability conditions thatgeneralize known conditions for Markov chains to ensure the existence ofsolutions to the optimality equation.
arxiv-300-54 | The AdaBoost Flow | http://arxiv.org/pdf/1110.6228v4.pdf | author:A. Lykov, S. Muzychka, K. Vaninsky category:stat.ML math-ph math.MP 62-07 published:2011-10-28 summary:We introduce a dynamical system which we call the AdaBoost flow. The flow isdefined by a system of ODEs with control. We show that three algorithms of theAdaBoost family (i) the AdaBoost algorithm of Schapire and Freund (ii) thearc-gv algorithm of Breiman (iii) the confidence rated prediction of Schapireand Singer can be can be embedded in the AdaBoost flow. The nontrivial part of the AdaBoost flow equations coincides with theequations of dynamics of nonperiodic Toda system written in terms of spectralvariables. We provide a novel invariant geometrical description of the AdaBoostalgorithm as a gradient flow on a foliation defined by level sets of thepotential function. We propose a new approach for constructing boosting algorithms as acontinuous time gradient flow on measures defined by various metrics andpotential functions. Finally we explain similarity of the AdaBoost algorithmwith the Perelman's construction for the Ricci flow.
arxiv-300-55 | The multi-armed bandit problem with covariates | http://arxiv.org/pdf/1110.6084v3.pdf | author:Vianney Perchet, Philippe Rigollet category:math.ST cs.LG stat.ML stat.TH published:2011-10-27 summary:We consider a multi-armed bandit problem in a setting where each arm producesa noisy reward realization which depends on an observable random covariate. Asopposed to the traditional static multi-armed bandit problem, this settingallows for dynamically changing rewards that better describe applications whereside information is available. We adopt a nonparametric model where theexpected rewards are smooth functions of the covariate and where the hardnessof the problem is captured by a margin parameter. To maximize the expectedcumulative reward, we introduce a policy called Adaptively Binned SuccessiveElimination (abse) that adaptively decomposes the global problem into suitably"localized" static bandit problems. This policy constructs an adaptivepartition using a variant of the Successive Elimination (se) policy. Ourresults include sharper regret bounds for the se policy in a static banditproblem and minimax optimal regret bounds for the abse policy in the dynamicproblem.
arxiv-300-56 | Distance Dependent Infinite Latent Feature Models | http://arxiv.org/pdf/1110.5454v2.pdf | author:Samuel J. Gershman, Peter I. Frazier, David M. Blei category:stat.ML math.ST stat.TH published:2011-10-25 summary:Latent feature models are widely used to decompose data into a small numberof components. Bayesian nonparametric variants of these models, which use theIndian buffet process (IBP) as a prior over latent features, allow the numberof features to be determined from the data. We present a generalization of theIBP, the distance dependent Indian buffet process (dd-IBP), for modelingnon-exchangeable data. It relies on distances defined between data points,biasing nearby data to share more features. The choice of distance measureallows for many kinds of dependencies, including temporal and spatial. Further,the original IBP is a special case of the dd-IBP. In this paper, we develop thedd-IBP and theoretically characterize its feature-sharing properties. We derivea Markov chain Monte Carlo sampler for a linear Gaussian model with a dd-IBPprior and study its performance on several non-exchangeable data sets.
arxiv-300-57 | Absolute Uniqueness of Phase Retrieval with Random Illumination | http://arxiv.org/pdf/1110.5097v7.pdf | author:Albert Fannjiang category:physics.optics cs.CV math-ph math.MP published:2011-10-23 summary:Random illumination is proposed to enforce absolute uniqueness and resolveall types of ambiguity, trivial or nontrivial, from phase retrieval. Almostsure irreducibility is proved for any complex-valued object of a full ranksupport. While the new irreducibility result can be viewed as a probabilisticversion of the classical result by Bruck, Sodin and Hayes, it provides a novelperspective and an effective method for phase retrieval. In particular, almost sure uniqueness, up to a global phase, is proved forcomplex-valued objects under general two-point conditions. Under a tight sectorconstraint absolute uniqueness is proved to hold with probability exponentiallyclose to unity as the object sparsity increases. Under a magnitude constraintwith random amplitude illumination, uniqueness modulo global phase is proved tohold with probability exponentially close to unity as object sparsityincreases. For general complex-valued objects without any constraint, almostsure uniqueness up to global phase is established with two sets of Fouriermagnitude data under two independent illuminations. Numerical experimentssuggest that random illumination essentially alleviates most, if not all,numerical problems commonly associated with the standard phasing algorithms.
arxiv-300-58 | Web search queries can predict stock market volumes | http://arxiv.org/pdf/1110.4784v3.pdf | author:Ilaria Bordino, Stefano Battiston, Guido Caldarelli, Matthieu Cristelli, Antti Ukkonen, Ingmar Weber category:q-fin.ST cs.LG physics.soc-ph published:2011-10-21 summary:We live in a computerized and networked society where many of our actionsleave a digital trace and affect other people's actions. This has lead to theemergence of a new data-driven research field: mathematical methods of computerscience, statistical physics and sociometry provide insights on a wide range ofdisciplines ranging from social science to human mobility. A recent importantdiscovery is that query volumes (i.e., the number of requests submitted byusers to search engines on the www) can be used to track and, in some cases, toanticipate the dynamics of social phenomena. Successful exemples includeunemployment levels, car and home sales, and epidemics spreading. Few recentworks applied this approach to stock prices and market sentiment. However, itremains unclear if trends in financial markets can be anticipated by thecollective wisdom of on-line users on the web. Here we show that tradingvolumes of stocks traded in NASDAQ-100 are correlated with the volumes ofqueries related to the same stocks. In particular, query volumes anticipate inmany cases peaks of trading by one day or more. Our analysis is carried out ona unique dataset of queries, submitted to an important web search engine, whichenable us to investigate also the user behavior. We show that the query volumedynamics emerges from the collective but seemingly uncoordinated activity ofmany users. These findings contribute to the debate on the identification ofearly warnings of financial systemic risk, based on the activity of users ofthe www.
arxiv-300-59 | Regression for sets of polynomial equations | http://arxiv.org/pdf/1110.4531v4.pdf | author:Franz Johannes Király, Paul von Bünau, Jan Saputra Müller, Duncan Blythe, Frank Meinecke, Klaus-Robert Müller category:stat.ML published:2011-10-20 summary:We propose a method called ideal regression for approximating an arbitrarysystem of polynomial equations by a system of a particular type. Usingtechniques from approximate computational algebraic geometry, we show how wecan solve ideal regression directly without resorting to numericaloptimization. Ideal regression is useful whenever the solution to a learningproblem can be described by a system of polynomial equations. As an example, wedemonstrate how to formulate Stationary Subspace Analysis (SSA), a sourceseparation problem, in terms of ideal regression, which also yields aconsistent estimator for SSA. We then compare this estimator in simulationswith previous optimization-based approaches for SSA.
arxiv-300-60 | Readouts for Echo-state Networks Built using Locally Regularized Orthogonal Forward Regression | http://arxiv.org/pdf/1110.4304v3.pdf | author:Ján Dolinský, Kei Hirose, Sadanori Konishi category:stat.ML stat.ME published:2011-10-19 summary:Echo state network (ESN) is viewed as a temporal non-orthogonal expansionwith pseudo-random parameters. Such expansions naturally give rise toregressors of various relevance to a teacher output. We illustrate that oftenonly a certain amount of the generated echo-regressors effectively explain thevariance of the teacher output and also that sole local regularization is notable to provide in-depth information concerning the importance of the generatedregressors. The importance is therefore determined by a joint calculation ofthe individual variance contributions and Bayesian relevance using locallyregularized orthogonal forward regression (LROFR) algorithm. This informationcan be advantageously used in a variety of ways for an in-depth analysis of anESN structure and its state-space parameters in relation to the unknowndynamics of the underlying problem. We present locally regularized linearreadout built using LROFR. The readout may have a different dimensionality thanan ESN model itself, and besides improving robustness and accuracy of an ESN itrelates the echo-regressors to different features of the training data and maydetermine what type of an additional readout is suitable for a task at hand.Moreover, as flexibility of the linear readout has limitations and mightsometimes be insufficient for certain tasks, we also present a radial basisfunction (RBF) readout built using LROFR. It is a flexible and parsimoniousreadout with excellent generalization abilities and is a viable alternative toreadouts based on a feed-forward neural network (FFNN) or an RBF net builtusing relevance vector machine (RVM).
arxiv-300-61 | Stable mixed graphs | http://arxiv.org/pdf/1110.4168v3.pdf | author:Kayvan Sadeghi category:stat.OT math.ST stat.ML stat.TH published:2011-10-19 summary:In this paper, we study classes of graphs with three types of edges thatcapture the modified independence structure of a directed acyclic graph (DAG)after marginalisation over unobserved variables and conditioning on selectionvariables using the $m$-separation criterion. These include MC, summary, andancestral graphs. As a modification of MC graphs, we define the class ofribbonless graphs (RGs) that permits the use of the $m$-separation criterion.RGs contain summary and ancestral graphs as subclasses, and each RG can begenerated by a DAG after marginalisation and conditioning. We derive simplealgorithms to generate RGs, from given DAGs or RGs, and also to generatesummary and ancestral graphs in a simple way by further extension of theRG-generating algorithm. This enables us to develop a parallel theory on thesethree classes and to study the relationships between them as well as the use ofeach class.
arxiv-300-62 | A Reliable Effective Terascale Linear Learning System | http://arxiv.org/pdf/1110.4198v3.pdf | author:Alekh Agarwal, Olivier Chapelle, Miroslav Dudik, John Langford category:cs.LG stat.ML published:2011-10-19 summary:We present a system and a set of techniques for learning linear predictorswith convex losses on terascale datasets, with trillions of features, {Thenumber of features here refers to the number of non-zero entries in the datamatrix.} billions of training examples and millions of parameters in an hourusing a cluster of 1000 machines. Individually none of the component techniquesare new, but the careful synthesis required to obtain an efficientimplementation is. The result is, up to our knowledge, the most scalable andefficient linear learning system reported in the literature (as of 2011 whenour experiments were conducted). We describe and thoroughly evaluate thecomponents of the system, showing the importance of the various design choices.
arxiv-300-63 | Is the k-NN classifier in high dimensions affected by the curse of dimensionality? | http://arxiv.org/pdf/1110.4347v3.pdf | author:Vladimir Pestov category:stat.ML 62H30, 68H05 I.2.6 published:2011-10-19 summary:There is an increasing body of evidence suggesting that exact nearestneighbour search in high-dimensional spaces is affected by the curse ofdimensionality at a fundamental level. Does it necessarily mean that the sameis true for k nearest neighbours based learning algorithms such as the k-NNclassifier? We analyse this question at a number of levels and show that theanswer is different at each of them. As our first main observation, we show theconsistency of a k approximate nearest neighbour classifier. However, theperformance of the classifier in very high dimensions is provably unstable. Asour second main observation, we point out that the existing model forstatistical learning is oblivious of dimension of the domain and so everylearning problem admits a universally consistent deterministic reduction to theone-dimensional case by means of a Borel isomorphism.
arxiv-300-64 | AOSO-LogitBoost: Adaptive One-Vs-One LogitBoost for Multi-Class Problem | http://arxiv.org/pdf/1110.3907v3.pdf | author:Peng Sun, Mark D. Reid, Jie Zhou category:stat.ML cs.AI cs.CV published:2011-10-18 summary:This paper presents an improvement to model learning when using multi-classLogitBoost for classification. Motivated by the statistical view, LogitBoostcan be seen as additive tree regression. Two important factors in this settingare: 1) coupled classifier output due to a sum-to-zero constraint, and 2) thedense Hessian matrices that arise when computing tree node split gain and nodevalue fittings. In general, this setting is too complicated for a tractablemodel learning algorithm. However, too aggressive simplification of the settingmay lead to degraded performance. For example, the original LogitBoost isoutperformed by ABC-LogitBoost due to the latter's more careful treatment ofthe above two factors. In this paper we propose techniques to address the two main difficulties ofthe LogitBoost setting: 1) we adopt a vector tree (i.e. each node value isvector) that enforces a sum-to-zero constraint, and 2) we use an adaptive blockcoordinate descent that exploits the dense Hessian when computing tree splitgain and node values. Higher classification accuracy and faster convergencerates are observed for a range of public data sets when compared to both theoriginal and the ABC-LogitBoost implementations.
arxiv-300-65 | Positive words carry less information than negative words | http://arxiv.org/pdf/1110.4123v4.pdf | author:David Garcia, Antonios Garas, Frank Schweitzer category:cs.CL cs.IR physics.soc-ph published:2011-10-18 summary:We show that the frequency of word use is not only determined by the wordlength \cite{Zipf1935} and the average information content\cite{Piantadosi2011}, but also by its emotional content. We have analyzedthree established lexica of affective word usage in English, German, andSpanish, to verify that these lexica have a neutral, unbiased, emotionalcontent. Taking into account the frequency of word usage, we find that wordswith a positive emotional content are more frequently used. This lends supportto Pollyanna hypothesis \cite{Boucher1969} that there should be a positive biasin human expression. We also find that negative words contain more informationthan positive words, as the informativeness of a word increases uniformly withits valence decrease. Our findings support earlier conjectures about (i) therelation between word frequency and information content, and (ii) the impact ofpositive emotions on communication and social links.
arxiv-300-66 | Budget-Optimal Task Allocation for Reliable Crowdsourcing Systems | http://arxiv.org/pdf/1110.3564v4.pdf | author:David R. Karger, Sewoong Oh, Devavrat Shah category:cs.LG cs.DS cs.HC stat.ML published:2011-10-17 summary:Crowdsourcing systems, in which numerous tasks are electronically distributedto numerous "information piece-workers", have emerged as an effective paradigmfor human-powered solving of large scale problems in domains such as imageclassification, data entry, optical character recognition, recommendation, andproofreading. Because these low-paid workers can be unreliable, nearly all suchsystems must devise schemes to increase confidence in their answers, typicallyby assigning each task multiple times and combining the answers in anappropriate manner, e.g. majority voting. In this paper, we consider a general model of such crowdsourcing tasks andpose the problem of minimizing the total price (i.e., number of taskassignments) that must be paid to achieve a target overall reliability. We givea new algorithm for deciding which tasks to assign to which workers and forinferring correct answers from the workers' answers. We show that ouralgorithm, inspired by belief propagation and low-rank matrix approximation,significantly outperforms majority voting and, in fact, is optimal throughcomparison to an oracle that knows the reliability of every worker. Further, wecompare our approach with a more general class of algorithms which candynamically assign tasks. By adaptively deciding which questions to ask to thenext arriving worker, one might hope to reduce uncertainty more efficiently. Weshow that, perhaps surprisingly, the minimum price necessary to achieve atarget reliability scales in the same manner under both adaptive andnon-adaptive scenarios. Hence, our non-adaptive approach is order-optimal underboth scenarios. This strongly relies on the fact that workers are fleeting andcan not be exploited. Therefore, architecturally, our results suggest thatbuilding a reliable worker-reputation system is essential to fully harnessingthe potential of adaptive designs.
arxiv-300-67 | Period-halving Bifurcation of a Neuronal Recurrence Equation | http://arxiv.org/pdf/1110.3586v3.pdf | author:René Ndoundam category:cs.NE math.DS nlin.CD published:2011-10-17 summary:We study the sequences generated by neuronal recurrence equations of the form$x(n) = {\bf 1}[\sum_{j=1}^{h} a_{j} x(n-j)- \theta]$. From a neuronalrecurrence equation of memory size $h$ which describes a cycle of length$\rho(m) \times lcm(p_0, p_1,..., p_{-1+\rho(m)})$, we construct a set of$\rho(m)$ neuronal recurrence equations whose dynamics describe respectivelythe transient of length $O(\rho(m) \times lcm(p_0, ..., p_{d}))$ and the cycleof length $O(\rho(m) \times lcm(p_{d+1}, ..., p_{-1+\rho(m)}))$ if $0 \leq d\leq -2+\rho(m)$ and 1 if $d=\rho(m)-1$. This result shows the exponential time of the convergence of neuronalrecurrence equation to fixed points and the existence of the period-halvingbifurcation.
arxiv-300-68 | Joint variable and rank selection for parsimonious estimation of high-dimensional matrices | http://arxiv.org/pdf/1110.3556v4.pdf | author:Florentina Bunea, Yiyuan She, Marten H. Wegkamp category:math.ST stat.ME stat.ML stat.TH published:2011-10-17 summary:We propose dimension reduction methods for sparse, high-dimensionalmultivariate response regression models. Both the number of responses and thatof the predictors may exceed the sample size. Sometimes viewed ascomplementary, predictor selection and rank reduction are the most popularstrategies for obtaining lower-dimensional approximations of the parametermatrix in such models. We show in this article that important gains inprediction accuracy can be obtained by considering them jointly. We motivate anew class of sparse multivariate regression models, in which the coefficientmatrix has low rank and zero rows or can be well approximated by such a matrix.Next, we introduce estimators that are based on penalized least squares, withnovel penalties that impose simultaneous row and rank restrictions on thecoefficient matrix. We prove that these estimators indeed adapt to the unknownmatrix sparsity and have fast rates of convergence. We support our theoreticalresults with an extensive simulation study and two data analyses.
arxiv-300-69 | Multi-criteria Anomaly Detection using Pareto Depth Analysis | http://arxiv.org/pdf/1110.3741v3.pdf | author:Ko-Jen Hsiao, Kevin S. Xu, Jeff Calder, Alfred O. Hero III category:cs.LG cs.CV cs.DB stat.ML published:2011-10-17 summary:We consider the problem of identifying patterns in a data set that exhibitanomalous behavior, often referred to as anomaly detection. In most anomalydetection algorithms, the dissimilarity between data samples is calculated by asingle criterion, such as Euclidean distance. However, in many cases there maynot exist a single dissimilarity measure that captures all possible anomalouspatterns. In such a case, multiple criteria can be defined, and one can testfor anomalies by scalarizing the multiple criteria using a linear combinationof them. If the importance of the different criteria are not known in advance,the algorithm may need to be executed multiple times with different choices ofweights in the linear combination. In this paper, we introduce a novelnon-parametric multi-criteria anomaly detection method using Pareto depthanalysis (PDA). PDA uses the concept of Pareto optimality to detect anomaliesunder multiple criteria without having to run an algorithm multiple times withdifferent choices of weights. The proposed PDA approach scales linearly in thenumber of criteria and is provably better than linear combinations of thecriteria.
arxiv-300-70 | Robust Image Analysis by L1-Norm Semi-supervised Learning | http://arxiv.org/pdf/1110.3109v2.pdf | author:Zhiwu Lu, Yuxin Peng category:cs.CV cs.LG published:2011-10-14 summary:This paper presents a novel L1-norm semi-supervised learning algorithm forrobust image analysis by giving new L1-norm formulation of Laplacianregularization which is the key step of graph-based semi-supervised learning.Since our L1-norm Laplacian regularization is defined directly over theeigenvectors of the normalized Laplacian matrix, we successfully formulatesemi-supervised learning as an L1-norm linear reconstruction problem which canbe effectively solved with sparse coding. By working with only a small subsetof eigenvectors, we further develop a fast sparse coding algorithm for ourL1-norm semi-supervised learning. Due to the sparsity induced by sparse coding,the proposed algorithm can deal with the noise in the data to some extent andthus has important applications to robust image analysis, such as noise-robustimage classification and noise reduction for visual and textual bag-of-words(BOW) models. In particular, this paper is the first attempt to obtain robustimage representation by sparse co-refinement of visual and textual BOW models.The experimental results have shown the promising performance of the proposedalgorithm.
arxiv-300-71 | Randomized Dimensionality Reduction for k-means Clustering | http://arxiv.org/pdf/1110.2897v3.pdf | author:Christos Boutsidis, Anastasios Zouzias, Michael W. Mahoney, Petros Drineas category:cs.DS cs.LG published:2011-10-13 summary:We study the topic of dimensionality reduction for $k$-means clustering.Dimensionality reduction encompasses the union of two approaches: \emph{featureselection} and \emph{feature extraction}. A feature selection based algorithmfor $k$-means clustering selects a small subset of the input features and thenapplies $k$-means clustering on the selected features. A feature extractionbased algorithm for $k$-means clustering constructs a small set of newartificial features and then applies $k$-means clustering on the constructedfeatures. Despite the significance of $k$-means clustering as well as thewealth of heuristic methods addressing it, provably accurate feature selectionmethods for $k$-means clustering are not known. On the other hand, two provablyaccurate feature extraction methods for $k$-means clustering are known in theliterature; one is based on random projections and the other is based on thesingular value decomposition (SVD). This paper makes further progress towards a better understanding ofdimensionality reduction for $k$-means clustering. Namely, we present the firstprovably accurate feature selection method for $k$-means clustering and, inaddition, we present two feature extraction methods. The first featureextraction method is based on random projections and it improves upon theexisting results in terms of time complexity and number of features needed tobe extracted. The second feature extraction method is based on fast approximateSVD factorizations and it also improves upon the existing results in terms oftime complexity. The proposed algorithms are randomized and provideconstant-factor approximation guarantees with respect to the optimal $k$-meansobjective value.
arxiv-300-72 | Efficient Tracking of Large Classes of Experts | http://arxiv.org/pdf/1110.2755v3.pdf | author:András Gyorgy, Tamás Linder, Gábor Lugosi category:cs.LG cs.IT math.IT 68Q32, 68P30 I.2.6; E.4 published:2011-10-12 summary:In the framework of prediction of individual sequences, sequential predictionmethods are to be constructed that perform nearly as well as the best expertfrom a given class. We consider prediction strategies that compete with theclass of switching strategies that can segment a given sequence into severalblocks, and follow the advice of a different "base" expert in each block. Asusual, the performance of the algorithm is measured by the regret defined asthe excess loss relative to the best switching strategy selected in hindsightfor the particular sequence to be predicted. In this paper we constructprediction strategies of low computational cost for the case where the set ofbase experts is large. In particular we provide a method that can transform anyprediction algorithm $\A$ that is designed for the base class into a trackingalgorithm. The resulting tracking algorithm can take advantage of theprediction performance and potential computational efficiency of $\A$ in thesense that it can be implemented with time and space complexity only$O(n^{\gamma} \ln n)$ times larger than that of $\A$, where $n$ is the timehorizon and $\gamma \ge 0$ is a parameter of the algorithm. With $\A$ properlychosen, our algorithm achieves a regret bound of optimal order for $\gamma>0$,and only $O(\ln n)$ times larger than the optimal order for $\gamma=0$ for alltypical regret bound types we examined. For example, for predicting binarysequences with switching parameters under the logarithmic loss, our methodachieves the optimal $O(\ln n)$ regret rate with time complexity$O(n^{1+\gamma}\ln n)$ for any $\gamma\in (0,1)$.
arxiv-300-73 | The Generalization Ability of Online Algorithms for Dependent Data | http://arxiv.org/pdf/1110.2529v2.pdf | author:Alekh Agarwal, John C. Duchi category:stat.ML cs.LG math.OC published:2011-10-11 summary:We study the generalization performance of online learning algorithms trainedon samples coming from a dependent source of data. We show that thegeneralization error of any stable online algorithm concentrates around itsregret--an easily computable statistic of the online performance of thealgorithm--when the underlying ergodic process is $\beta$- or $\phi$-mixing. Weshow high probability error bounds assuming the loss function is convex, and wealso establish sharp convergence rates and deviation bounds for strongly convexlosses and several linear prediction problems such as linear and logisticregression, least-squares SVM, and boosting on dependent data. In addition, ourresults have straightforward applications to stochastic optimization withdependent data, and our analysis requires only martingale convergencearguments; we need not rely on more powerful statistical tools such asempirical process theory.
arxiv-300-74 | Steps Towards a Theory of Visual Information: Active Perception, Signal-to-Symbol Conversion and the Interplay Between Sensing and Control | http://arxiv.org/pdf/1110.2053v3.pdf | author:Stefano Soatto category:cs.CV published:2011-10-10 summary:This manuscript describes the elements of a theory of information tailored tocontrol and decision tasks and specifically to visual data. The concept ofActionable Information is described, that relates to a notion of informationchampioned by J. Gibson, and a notion of "complete information" that relates tothe minimal sufficient statistics of a complete representation. It is shownthat the "actionable information gap" between the two can be reduced byexercising control on the sensing process. Thus, senging, control andinformation are inextricably tied. This has consequences in the so-called"signal-to-symbol barrier" problem, as well as in the analysis and design ofactive sensing systems. It has ramifications in vision-based control,navigation, 3-D reconstruction and rendering, as well as detection,localization, recognition and categorization of objects and scenes in livevideo. This manuscript has been developed from a set of lecture notes for a summercourse at the First International Computer Vision Summer School (ICVSS) inScicli, Italy, in July of 2008. They were later expanded and amended forsubsequent lectures in the same School in July 2009. Starting on November 1,2009, they were further expanded for a special topics course, CS269, taught atUCLA in the Spring term of 2010.
arxiv-300-75 | Active Learning Using Smooth Relative Regret Approximations with Applications | http://arxiv.org/pdf/1110.2136v3.pdf | author:Nir Ailon, Ron Begleiter, Esther Ezra category:cs.LG published:2011-10-10 summary:The disagreement coefficient of Hanneke has become a central data independentinvariant in proving active learning rates. It has been shown in various waysthat a concept class with low complexity together with a bound on thedisagreement coefficient at an optimal solution allows active learning ratesthat are superior to passive learning ones. We present a different tool for pool based active learning which follows fromthe existence of a certain uniform version of low disagreement coefficient, butis not equivalent to it. In fact, we present two fundamental active learningproblems of significant interest for which our approach allows nontrivialactive learning bounds. However, any general purpose method relying on thedisagreement coefficient bounds only fails to guarantee any useful bounds forthese problems. The tool we use is based on the learner's ability to compute an estimator ofthe difference between the loss of any hypotheses and some fixed "pivotal"hypothesis to within an absolute error of at most $\eps$ times the
arxiv-300-76 | Dynamic Matrix Factorization: A State Space Approach | http://arxiv.org/pdf/1110.2098v3.pdf | author:John Z. Sun, Kush R. Varshney, Karthik Subbian category:cs.LG published:2011-10-10 summary:Matrix factorization from a small number of observed entries has recentlygarnered much attention as the key ingredient of successful recommendationsystems. One unresolved problem in this area is how to adapt current methods tohandle changing user preferences over time. Recent proposals to address thisissue are heuristic in nature and do not fully exploit the time-dependentstructure of the problem. As a principled and general temporal formulation, wepropose a dynamical state space model of matrix factorization. Our proposalbuilds upon probabilistic matrix factorization, a Bayesian model with Gaussianpriors. We utilize results in state tracking, such as the Kalman filter, toprovide accurate recommendations in the presence of both process andmeasurement noise. We show how system parameters can be learned viaexpectation-maximization and provide comparisons to current publishedtechniques.
arxiv-300-77 | The proximal point method for a hybrid model in image restoration | http://arxiv.org/pdf/1110.1804v2.pdf | author:Zhi-Feng Pang, Li-Lian Wang, Yu-Fei Yang category:cs.CV cs.IT math.IT math.OC published:2011-10-09 summary:Models including two $L^1$ -norm terms have been widely used in imagerestoration. In this paper we first propose the alternating direction method ofmultipliers (ADMM) to solve this class of models. Based on ADMM, we thenpropose the proximal point method (PPM), which is more efficient than ADMM.Following the operator theory, we also give the convergence analysis of theproposed methods. Furthermore, we use the proposed methods to solve a class ofhybrid models combining the ROF model with the LLT model. Some numericalresults demonstrate the viability and efficiency of the proposed methods.
arxiv-300-78 | Positive definite matrices and the S-divergence | http://arxiv.org/pdf/1110.1773v4.pdf | author:Suvrit Sra category:math.FA stat.ML published:2011-10-08 summary:Positive definite matrices abound in a dazzling variety of applications. Thisubiquity can be in part attributed to their rich geometric structure: positivedefinite matrices form a self-dual convex cone whose strict interior is aRiemannian manifold. The manifold view is endowed with a "natural" distancefunction while the conic view is not. Nevertheless, drawing motivation from theconic view, we introduce the S-Divergence as a "natural" distance-like functionon the open cone of positive definite matrices. We motivate the S-divergencevia a sequence of results that connect it to the Riemannian distance. Inparticular, we show that (a) this divergence is the square of a distance; and(b) that it has several geometric properties similar to those of the Riemanniandistance, though without being computationally as demanding. The S-divergenceis even more intriguing: although nonconvex, we can still compute matrix meansand medians using it to global optimality. We complement our results with somenumerical experiments illustrating our theorems and our optimization algorithmfor computing matrix medians.
arxiv-300-79 | Runtime Guarantees for Regression Problems | http://arxiv.org/pdf/1110.1358v2.pdf | author:Hui Han Chin, Aleksander Madry, Gary Miller, Richard Peng category:cs.DS cs.CV published:2011-10-06 summary:We study theoretical runtime guarantees for a class of optimization problemsthat occur in a wide variety of inference problems. these problems aremotivated by the lasso framework and have applications in machine learning andcomputer vision. Our work shows a close connection between these problems and core questionsin algorithmic graph theory. While this connection demonstrates thedifficulties of obtaining runtime guarantees, it also suggests an approach ofusing techniques originally developed for graph algorithms. We then show that most of these problems can be formulated as a grouped leastsquares problem, and give efficient algorithms for this formulation. Ouralgorithms rely on routines for solving quadratic minimization problems, whichin turn are equivalent to solving linear systems. Finally we present someexperimental results on applying our approximation algorithm to imageprocessing problems.
arxiv-300-80 | Learning to relate images: Mapping units, complex cells and simultaneous eigenspaces | http://arxiv.org/pdf/1110.0107v2.pdf | author:Roland Memisevic category:cs.CV cs.AI nlin.AO stat.ML published:2011-10-01 summary:A fundamental operation in many vision tasks, including motion understanding,stereopsis, visual odometry, or invariant recognition, is establishingcorrespondences between images or between images and data from othermodalities. We present an analysis of the role that multiplicative interactionsplay in learning such correspondences, and we show how learning and inferringrelationships between images can be viewed as detecting rotations in theeigenspaces shared among a set of orthogonal matrices. We review a variety ofrecent multiplicative sparse coding methods in light of this observation. Wealso review how the squaring operation performed by energy models and by modelsof complex cells can be thought of as a way to implement multiplicativeinteractions. This suggests that the main utility of including complex cells incomputational models of vision may be that they can encode relations notinvariances.
arxiv-300-81 | Robust Parametric Classification and Variable Selection by a Minimum Distance Criterion | http://arxiv.org/pdf/1109.6090v3.pdf | author:Eric C. Chi, David W. Scott category:stat.ME stat.CO stat.ML published:2011-09-28 summary:We investigate a robust penalized logistic regression algorithm based on aminimum distance criterion. Influential outliers are often associated with theexplosion of parameter vector estimates, but in the context of standardlogistic regression, the bias due to outliers always causes the parametervector to implode, that is shrink towards the zero vector. Thus, usingLASSO-like penalties to perform variable selection in the presence of outlierscan result in missed detections of relevant covariates. We show that bychoosing a minimum distance criterion together with an Elastic Net penalty, wecan simultaneously find a parsimonious model and avoid estimation implosioneven in the presence of many outliers in the important small $n$ large $p$situation. Implementation using an MM algorithm is described and performanceevaluated.
arxiv-300-82 | Markov properties for mixed graphs | http://arxiv.org/pdf/1109.5909v5.pdf | author:Kayvan Sadeghi, Steffen Lauritzen category:stat.OT math.ST stat.ML stat.TH published:2011-09-27 summary:In this paper, we unify the Markov theory of a variety of different types ofgraphs used in graphical Markov models by introducing the class of looplessmixed graphs, and show that all independence models induced by $m$-separationon such graphs are compositional graphoids. We focus in particular on thesubclass of ribbonless graphs which as special cases include undirected graphs,bidirected graphs, and directed acyclic graphs, as well as ancestral graphs andsummary graphs. We define maximality of such graphs as well as a pairwise and aglobal Markov property. We prove that the global and pairwise Markov propertiesof a maximal ribbonless graph are equivalent for any independence model that isa compositional graphoid.
arxiv-300-83 | Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization | http://arxiv.org/pdf/1109.5647v7.pdf | author:Alexander Rakhlin, Ohad Shamir, Karthik Sridharan category:cs.LG math.OC published:2011-09-26 summary:Stochastic gradient descent (SGD) is a simple and popular method to solvestochastic optimization problems which arise in machine learning. For stronglyconvex problems, its convergence rate was known to be O(\log(T)/T), by runningSGD for T iterations and returning the average point. However, recent resultsshowed that using a different algorithm, one can get an optimal O(1/T) rate.This might lead one to believe that standard SGD is suboptimal, and maybeshould even be replaced as a method of choice. In this paper, we investigatethe optimality of SGD in a stochastic setting. We show that for smoothproblems, the algorithm attains the optimal O(1/T) rate. However, fornon-smooth problems, the convergence rate with averaging might really be\Omega(\log(T)/T), and this is not just an artifact of the analysis. On theflip side, we show that a simple modification of the averaging step suffices torecover the O(1/T) rate, and no other change of the algorithm is necessary. Wealso present experimental results which support our findings, and point outopen problems.
arxiv-300-84 | Deterministic Feature Selection for $k$-means Clustering | http://arxiv.org/pdf/1109.5664v4.pdf | author:Christos Boutsidis, Malik Magdon-Ismail category:cs.LG cs.DS published:2011-09-26 summary:We study feature selection for $k$-means clustering. Although the literaturecontains many methods with good empirical performance, algorithms with provabletheoretical behavior have only recently been developed. Unfortunately, thesealgorithms are randomized and fail with, say, a constant probability. Weaddress this issue by presenting a deterministic feature selection algorithmfor k-means with theoretical guarantees. At the heart of our algorithm lies adeterministic method for decompositions of the identity.
arxiv-300-85 | Noise Tolerance under Risk Minimization | http://arxiv.org/pdf/1109.5231v4.pdf | author:Naresh Manwani, P. S. Sastry category:cs.LG published:2011-09-24 summary:In this paper we explore noise tolerant learning of classifiers. We formulatethe problem as follows. We assume that there is an ${\bf unobservable}$training set which is noise-free. The actual training set given to the learningalgorithm is obtained from this ideal data set by corrupting the class label ofeach example. The probability that the class label of an example is corruptedis a function of the feature vector of the example. This would account for mostkinds of noisy data one encounters in practice. We say that a learning methodis noise tolerant if the classifiers learnt with the ideal noise-free data andwith noisy data, both have the same classification accuracy on the noise-freedata. In this paper we analyze the noise tolerance properties of riskminimization (under different loss functions), which is a generic method forlearning classifiers. We show that risk minimization under 0-1 loss functionhas impressive noise tolerance properties and that under squared error loss istolerant only to uniform noise; risk minimization under other loss functions isnot noise tolerant. We conclude the paper with some discussion on implicationsof these theoretical results.
arxiv-300-86 | Simultaneous Codeword Optimization (SimCO) for Dictionary Update and Learning | http://arxiv.org/pdf/1109.5302v3.pdf | author:Wei Dai, Tao Xu, Wenwu Wang category:cs.LG cs.IT math.IT published:2011-09-24 summary:We consider the data-driven dictionary learning problem. The goal is to seekan over-complete dictionary from which every training signal can be bestapproximated by a linear combination of only a few codewords. This task isoften achieved by iteratively executing two operations: sparse coding anddictionary update. In the literature, there are two benchmark mechanisms toupdate a dictionary. The first approach, such as the MOD algorithm, ischaracterized by searching for the optimal codewords while fixing the sparsecoefficients. In the second approach, represented by the K-SVD method, onecodeword and the related sparse coefficients are simultaneously updated whileall other codewords and coefficients remain unchanged. We propose a novelframework that generalizes the aforementioned two methods. The unique featureof our approach is that one can update an arbitrary set of codewords and thecorresponding sparse coefficients simultaneously: when sparse coefficients arefixed, the underlying optimization problem is similar to that in the MODalgorithm; when only one codeword is selected for update, it can be proved thatthe proposed algorithm is equivalent to the K-SVD method; and more importantly,our method allows us to update all codewords and all sparse coefficientssimultaneously, hence the term simultaneous codeword optimization (SimCO).Under the proposed framework, we design two algorithms, namely, primitive andregularized SimCO. We implement these two algorithms based on a simple gradientdescent mechanism. Simulations are provided to demonstrate the performance ofthe proposed algorithms, as compared with two baseline algorithms MOD andK-SVD. Results show that regularized SimCO is particularly appealing in termsof both learning performance and running speed.
arxiv-300-87 | RPA: Probabilistic analysis of probe performance and robust summarization | http://arxiv.org/pdf/1109.4928v2.pdf | author:Leo Lahti, Laura L. Elo, Tero Aittokallio, Samuel Kaski category:cs.CE stat.AP stat.ML published:2011-09-22 summary:Probe-level models have led to improved performance in microarray studies butthe various sources of probe-level contamination are still poorly understood.Data-driven analysis of probe performance can be used to quantify theuncertainty in individual probes and to highlight the relative contribution ofdifferent noise sources. Improved understanding of the probe-level effects canlead to improved preprocessing techniques and microarray design. We have implemented probabilistic tools for probe performance analysis andsummarization on short oligonucleotide arrays. In contrast to standardpreprocessing approaches, the methods provide quantitative estimates ofprobe-specific noise and affinity terms and tools to investigate theseparameters. Tools to incorporate prior information of the probes in theanalysis are provided as well. Comparisons to known probe-level error sourcesand spike-in data sets validate the approach. Implementation is freely available in R/BioConductor:http://www.bioconductor.org/packages/release/bioc/html/RPA.html
arxiv-300-88 | Manifold estimation and singular deconvolution under Hausdorff loss | http://arxiv.org/pdf/1109.4540v2.pdf | author:Christopher R. Genovese, Marco Perone-Pacifico, Isabella Verdinelli, Larry Wasserman category:math.ST cs.LG stat.ML stat.TH published:2011-09-21 summary:We find lower and upper bounds for the risk of estimating a manifold inHausdorff distance under several models. We also show that there are closeconnections between manifold estimation and the problem of deconvolving asingular measure.
arxiv-300-89 | Fast approximation of matrix coherence and statistical leverage | http://arxiv.org/pdf/1109.3843v2.pdf | author:Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, David P. Woodruff category:cs.DS cs.DM cs.LG published:2011-09-18 summary:The statistical leverage scores of a matrix $A$ are the squared row-norms ofthe matrix containing its (top) left singular vectors and the coherence is thelargest leverage score. These quantities are of interest in recently-popularproblems such as matrix completion and Nystr\"{o}m-based low-rank matrixapproximation as well as in large-scale statistical data analysis applicationsmore generally; moreover, they are of interest since they define the keystructural nonuniformity that must be dealt with in developing fast randomizedmatrix algorithms. Our main result is a randomized algorithm that takes asinput an arbitrary $n \times d$ matrix $A$, with $n \gg d$, and that returns asoutput relative-error approximations to all $n$ of the statistical leveragescores. The proposed algorithm runs (under assumptions on the precise values of$n$ and $d$) in $O(n d \log n)$ time, as opposed to the $O(nd^2)$ time requiredby the na\"{i}ve algorithm that involves computing an orthogonal basis for therange of $A$. Our analysis may be viewed in terms of computing a relative-errorapproximation to an underconstrained least-squares approximation problem, or,relatedly, it may be viewed as an application of Johnson-Lindenstrauss typeideas. Several practically-important extensions of our basic result are alsodescribed, including the approximation of so-called cross-leverage scores, theextension of these ideas to matrices with $n \approx d$, and the extension tostreaming environments.
arxiv-300-90 | A KdV-like advection-dispersion equation with some remarkable properties | http://arxiv.org/pdf/1109.3745v3.pdf | author:Abhijit Sen, Dilip P. Ahalpara, Anantanarayanan Thyagaraja, Govind S. Krishnaswami category:nlin.PS cs.NE math.AP published:2011-09-17 summary:We discuss a new non-linear PDE, u_t + (2 u_xx/u) u_x = epsilon u_xxx,invariant under scaling of dependent variable and referred to here as SIdV. Itis one of the simplest such translation and space-time reflection-symmetricfirst order advection-dispersion equations. This PDE (with dispersioncoefficient unity) was discovered in a genetic programming search for equationssharing the KdV solitary wave solution. It provides a bridge between non-linearadvection, diffusion and dispersion. Special cases include the mKdV and lineardispersive equations. We identify two conservation laws, though initialinvestigations indicate that SIdV does not follow from a polynomial Lagrangianof the KdV sort. Nevertheless, it possesses solitary and periodic travellingwaves. Moreover, numerical simulations reveal recurrence properties usuallyassociated with integrable systems. KdV and SIdV are the simplest in aninfinite dimensional family of equations sharing the KdV solitary wave. SIdVand its generalizations may serve as a testing ground for numerical andanalytical techniques and be a rich source for further explorations.
arxiv-300-91 | High-dimensional regression with noisy and missing data: Provable guarantees with nonconvexity | http://arxiv.org/pdf/1109.3714v4.pdf | author:Po-Ling Loh, Martin J. Wainwright category:math.ST cs.IT math.IT stat.ML stat.TH published:2011-09-16 summary:Although the standard formulations of prediction problems involvefully-observed and noiseless data drawn in an i.i.d. manner, many applicationsinvolve noisy and/or missing data, possibly involving dependence, as well. Westudy these issues in the context of high-dimensional sparse linear regression,and propose novel estimators for the cases of noisy, missing and/or dependentdata. Many standard approaches to noisy or missing data, such as those usingthe EM algorithm, lead to optimization problems that are inherently nonconvex,and it is difficult to establish theoretical guarantees on practicalalgorithms. While our approach also involves optimizing nonconvex programs, weare able to both analyze the statistical error associated with any globaloptimum, and more surprisingly, to prove that a simple algorithm based onprojected gradient descent will converge in polynomial time to a smallneighborhood of the set of all global minimizers. On the statistical side, weprovide nonasymptotic bounds that hold with high probability for the cases ofnoisy, missing and/or dependent data. On the computational side, we prove thatunder the same types of conditions required for statistical consistency, theprojected gradient descent algorithm is guaranteed to converge at a geometricrate to a near-global minimizer. We illustrate these theoretical predictionswith simulations, showing close agreement with the predicted scalings.
arxiv-300-92 | Convergence of latent mixing measures in finite and infinite mixture models | http://arxiv.org/pdf/1109.3250v5.pdf | author:XuanLong Nguyen category:math.ST stat.ML stat.TH published:2011-09-15 summary:This paper studies convergence behavior of latent mixing measures that arisein finite and infinite mixture models, using transportation distances (i.e.,Wasserstein metrics). The relationship between Wasserstein distances on thespace of mixing measures and f-divergence functionals such as Hellinger andKullback-Leibler distances on the space of mixture distributions isinvestigated in detail using various identifiability conditions. Convergence inWasserstein metrics for discrete measures implies convergence of individualatoms that provide support for the measures, thereby providing a naturalinterpretation of convergence of clusters in clustering applications wheremixture models are typically employed. Convergence rates of posteriordistributions for latent mixing measures are established, for both finitemixtures of multivariate distributions and infinite mixtures based on theDirichlet process.
arxiv-300-93 | Distributed User Profiling via Spectral Methods | http://arxiv.org/pdf/1109.3318v2.pdf | author:Dan-Cristian Tomozei, Laurent Massoulié category:cs.LG 60B20 G.3 published:2011-09-15 summary:User profiling is a useful primitive for constructing personalised services,such as content recommendation. In the present paper we investigate thefeasibility of user profiling in a distributed setting, with no centralauthority and only local information exchanges between users. We compute aprofile vector for each user (i.e., a low-dimensional vector that characterisesher taste) via spectral transformation of observed user-produced ratings foritems. Our two main contributions follow: i) We consider a low-rankprobabilistic model of user taste. More specifically, we consider that usersand items are partitioned in a constant number of classes, such that users anditems within the same class are statistically identical. We prove that withoutprior knowledge of the compositions of the classes, based solely on few randomobserved ratings (namely $O(N\log N)$ such ratings for $N$ users), we canpredict user preference with high probability for unrated items by running alocal vote among users with similar profile vectors. In addition, we provideempirical evaluations characterising the way in which spectral profilingperformance depends on the dimension of the profile space. Such evaluations areperformed on a data set of real user ratings provided by Netflix. ii) Wedevelop distributed algorithms which provably achieve an embedding of usersinto a low-dimensional space, based on spectral transformation. These involvesimple message passing among users, and provably converge to the desiredembedding. Our method essentially relies on a novel combination of gossipingand the algorithm proposed by Oja and Karhunen.
arxiv-300-94 | Sampled forms of functional PCA in reproducing kernel Hilbert spaces | http://arxiv.org/pdf/1109.3336v2.pdf | author:Arash A. Amini, Martin J. Wainwright category:math.ST stat.ML stat.TH published:2011-09-15 summary:We consider the sampling problem for functional PCA (fPCA), where thesimplest example is the case of taking time samples of the underlyingfunctional components. More generally, we model the sampling operation as acontinuous linear map from $\mathcal{H}$ to $\mathbb{R}^m$, where thefunctional components to lie in some Hilbert subspace $\mathcal{H}$ of $L^2$,such as a reproducing kernel Hilbert space of smooth functions. This modelincludes time and frequency sampling as special cases. In contrast to classicalapproach in fPCA in which access to entire functions is assumed, having alimited number m of functional samples places limitations on the performance ofstatistical procedures. We study these effects by analyzing the rate ofconvergence of an M-estimator for the subspace spanned by the leadingcomponents in a multi-spiked covariance model. The estimator takes the form ofregularized PCA, and hence is computationally attractive. We analyze thebehavior of this estimator within a nonasymptotic framework, and provide boundsthat hold with high probability as a function of the number of statisticalsamples n and the number of functional samples m. We also derive lower boundsshowing that the rates obtained are minimax optimal.
arxiv-300-95 | Nominal Association Vector and Matrix | http://arxiv.org/pdf/1109.2553v2.pdf | author:Wenxue Huang, Yong Shi, Xiaogang Wang category:stat.ML published:2011-09-12 summary:When response variables are nominal and populations are cross-classified withrespect to multiple polytomies, questions often arise about the degree ofassociation of the responses with explanatory variables. When populations areknown, we introduce a nominal association vector and matrix to evaluate thedependence of a response variable with an explanatory variable. These measuresprovide detailed evaluations of nominal associations at both local and globallevels. We also define a general class of global association measures whichembraces the well known association measure by Goodman-Kruskal (1954). Theproposed association matrix also gives rise to the expected generalizedconfusion matrix in classification. The hierarchy of equivalence relationsdefined by the association vector and matrix are also shown.
arxiv-300-96 | Structured sparsity through convex optimization | http://arxiv.org/pdf/1109.2397v2.pdf | author:Francis Bach, Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski category:cs.LG stat.ML published:2011-09-12 summary:Sparse estimation methods are aimed at using or obtaining parsimoniousrepresentations of data or models. While naturally cast as a combinatorialoptimization problem, variable or feature selection admits a convex relaxationthrough the regularization by the $\ell_1$-norm. In this paper, we considersituations where we are not only interested in sparsity, but where somestructural prior knowledge is available as well. We show that the $\ell_1$-normcan then be extended to structured norms built on either disjoint oroverlapping groups of variables, leading to a flexible framework that can dealwith various structures. We present applications to unsupervised learning, forstructured sparse principal component analysis and hierarchical dictionarylearning, and to supervised learning in the context of non-linear variableselection.
arxiv-300-97 | The Bayesian Bridge | http://arxiv.org/pdf/1109.2279v2.pdf | author:Nicholas G. Polson, James G. Scott, Jesse Windle category:stat.ME stat.CO stat.ML published:2011-09-11 summary:We propose the Bayesian bridge estimator for regularized regression andclassification. Two key mixture representations for the Bayesian bridge modelare developed: (1) a scale mixture of normals with respect to an alpha-stablerandom variable; and (2) a mixture of Bartlett--Fejer kernels (or triangledensities) with respect to a two-component mixture of gamma random variables.Both lead to MCMC methods for posterior simulation, and these methods turn outto have complementary domains of maximum efficiency. The first representationis a well known result due to West (1987), and is the better choice forcollinear design matrices. The second representation is new, and is moreefficient for orthogonal problems, largely because it avoids the need to dealwith exponentially tilted stable random variables. It also provides insightinto the multimodality of the joint posterior distribution, a feature of thebridge model that is notably absent under ridge or lasso-type priors. We provea theorem that extends this representation to a wider class of densitiesrepresentable as scale mixtures of betas, and provide an explicit inversionformula for the mixing distribution. The connections with slice sampling andscale mixtures of normals are explored. On the practical side, we find that theBayesian bridge model outperforms its classical cousin in estimation andprediction across a variety of data sets, both simulated and real. We also showthat the MCMC for fitting the bridge model exhibits excellent mixingproperties, particularly for the global scale parameter. This makes for afavorable contrast with analogous MCMC algorithms for other sparse Bayesianmodels. All methods described in this paper are implemented in the R packageBayesBridge. An extensive set of simulation results are provided in twosupplemental files.
arxiv-300-98 | Learning Sequence Neighbourhood Metrics | http://arxiv.org/pdf/1109.2034v2.pdf | author:Justin Bayer, Christian Osendorfer, Patrick van der Smagt category:cs.NE cs.LG published:2011-09-09 summary:Recurrent neural networks (RNNs) in combination with a pooling operator andthe neighbourhood components analysis (NCA) objective function are able todetect the characterizing dynamics of sequences and embed them into afixed-length vector space of arbitrary dimensionality. Subsequently, theresulting features are meaningful and can be used for visualization or nearestneighbour classification in linear time. This kind of metric learning forsequential data enables the use of algorithms tailored towards fixed lengthvector spaces such as R^n.
arxiv-300-99 | Exact Subspace Segmentation and Outlier Detection by Low-Rank Representation | http://arxiv.org/pdf/1109.1646v3.pdf | author:Guangcan Liu, Huan Xu, Shuicheng Yan category:cs.IT cs.CV math.IT published:2011-09-08 summary:In this work, we address the following matrix recovery problem: suppose weare given a set of data points containing two parts, one part consists ofsamples drawn from a union of multiple subspaces and the other part consists ofoutliers. We do not know which data points are outliers, or how many outliersthere are. The rank and number of the subspaces are unknown either. Can wedetect the outliers and segment the samples into their right subspaces,efficiently and exactly? We utilize a so-called {\em Low-Rank Representation}(LRR) method to solve this problem, and prove that under mild technicalconditions, any solution to LRR exactly recovers the row space of the samplesand detect the outliers as well. Since the subspace membership is provablydetermined by the row space, this further implies that LRR can perform exactsubspace segmentation and outlier detection, in an efficient way.
arxiv-300-100 | Gossip Learning with Linear Models on Fully Distributed Data | http://arxiv.org/pdf/1109.1396v3.pdf | author:Róbert Ormándi, István Hegedüs, Márk Jelasity category:cs.LG cs.DC published:2011-09-07 summary:Machine learning over fully distributed data poses an important problem inpeer-to-peer (P2P) applications. In this model we have one data record at eachnetwork node, but without the possibility to move raw data due to privacyconsiderations. For example, user profiles, ratings, history, or sensorreadings can represent this case. This problem is difficult, because there isno possibility to learn local models, the system model offers almost noguarantees for reliability, yet the communication cost needs to be kept low.Here we propose gossip learning, a generic approach that is based on multiplemodels taking random walks over the network in parallel, while applying anonline learning algorithm to improve themselves, and getting combined viaensemble learning methods. We present an instantiation of this approach for thecase of classification with linear models. Our main contribution is an ensemblelearning method which---through the continuous combination of the models in thenetwork---implements a virtual weighted voting mechanism over an exponentialnumber of models at practically no extra cost as compared to independent randomwalks. We prove the convergence of the method theoretically, and performextensive experiments on benchmark datasets. Our experimental analysisdemonstrates the performance and robustness of the proposed approach.
arxiv-300-101 | Nonparametric Link Prediction in Large Scale Dynamic Networks | http://arxiv.org/pdf/1109.1077v3.pdf | author:Purnamrita Sarkar, Deepayan Chakrabarti, Michael Jordan category:stat.ML cs.SI physics.soc-ph published:2011-09-06 summary:We propose a nonparametric approach to link prediction in large-scale dynamicnetworks. Our model uses graph-based features of pairs of nodes as well asthose of their local neighborhoods to predict whether those nodes will belinked at each time step. The model allows for different types of evolution indifferent parts of the graph (e.g, growing or shrinking communities). We focuson large-scale graphs and present an implementation of our model that makes useof locality-sensitive hashing to allow it to be scaled to large problems.Experiments with simulated data as well as five real-world dynamic graphs showthat we outperform the state of the art, especially when sharp fluctuations ornonlinearities are present. We also establish theoretical properties of ourestimator, in particular consistency and weak convergence, the latter makinguse of an elaboration of Stein's method for dependency graphs.
arxiv-300-102 | Moving Object Detection by Detecting Contiguous Outliers in the Low-Rank Representation | http://arxiv.org/pdf/1109.0882v2.pdf | author:Xiaowei Zhou, Can Yang, Weichuan Yu category:cs.CV published:2011-09-05 summary:Object detection is a fundamental step for automated video analysis in manyvision applications. Object detection in a video is usually performed by objectdetectors or background subtraction techniques. Often, an object detectorrequires manually labeled examples to train a binary classifier, whilebackground subtraction needs a training sequence that contains no objects tobuild a background model. To automate the analysis, object detection without aseparate training phase becomes a critical task. People have tried to tacklethis task by using motion information. But existing motion-based methods areusually limited when coping with complex scenarios such as nonrigid motion anddynamic background. In this paper, we show that above challenges can beaddressed in a unified framework named DEtecting Contiguous Outliers in theLOw-rank Representation (DECOLOR). This formulation integrates object detectionand background learning into a single process of optimization, which can besolved by an alternating algorithm efficiently. We explain the relationsbetween DECOLOR and other sparsity-based methods. Experiments on both simulateddata and real sequences demonstrate that DECOLOR outperforms thestate-of-the-art approaches and it can work effectively on a wide range ofcomplex scenarios.
arxiv-300-103 | Learning Nonlinear Functions Using Regularized Greedy Forest | http://arxiv.org/pdf/1109.0887v7.pdf | author:Rie Johnson, Tong Zhang category:stat.ML published:2011-09-05 summary:We consider the problem of learning a forest of nonlinear decision rules withgeneral loss functions. The standard methods employ boosted decision trees suchas Adaboost for exponential loss and Friedman's gradient boosting for generalloss. In contrast to these traditional boosting algorithms that treat a treelearner as a black box, the method we propose directly learns decision forestsvia fully-corrective regularized greedy search using the underlying foreststructure. Our method achieves higher accuracy and smaller models than gradientboosting (and Adaboost with exponential loss) on many datasets.
arxiv-300-104 | The Stick-Breaking Construction of the Beta Process as a Poisson Process | http://arxiv.org/pdf/1109.0343v2.pdf | author:John Paisley, David Blei, Michael I. Jordan category:math.ST math.PR stat.ML stat.TH published:2011-09-02 summary:We show that the stick-breaking construction of the beta process due toPaisley, et al. (2010) can be obtained from the characterization of the betaprocess as a Poisson process. Specifically, we show that the mean measure ofthe underlying Poisson process is equal to that of the beta process. We usethis underlying representation to derive error bounds on truncated betaprocesses that are tighter than those in the literature. We also develop a newMCMC inference algorithm for beta processes, based in part on our new Poissonprocess construction.
arxiv-300-105 | The Variational Garrote | http://arxiv.org/pdf/1109.0486v3.pdf | author:Hilbert J. Kappen, Vicenç Gómez category:stat.ME cs.LG published:2011-09-02 summary:In this paper, we present a new variational method for sparse regressionusing $L_0$ regularization. The variational parameters appear in theapproximate model in a way that is similar to Breiman's Garrote model. We referto this method as the variational Garrote (VG). We show that the combination ofthe variational approximation and $L_0$ regularization has the effect of makingthe problem effectively of maximal rank even when the number of samples issmall compared to the number of variables. The VG is compared numerically withthe Lasso method, ridge regression and the recently introduced paired meanfield method (PMF) (M. Titsias & M. L\'azaro-Gredilla., NIPS 2012). Numericalresults show that the VG and PMF yield more accurate predictions and moreaccurately reconstruct the true model than the other methods. It is shown thatthe VG finds correct solutions when the Lasso solution is inconsistent due tolarge input correlations. Globally, VG is significantly faster than PMF andtends to perform better as the problems become denser and in problems withstrongly correlated inputs. The naive implementation of the VG scales cubicwith the number of features. By introducing Lagrange multipliers we obtain adual formulation of the problem that scales cubic in the number of samples, butclose to linear in the number of features.
arxiv-300-106 | Inter-rater Agreement on Sentence Formality | http://arxiv.org/pdf/1109.0069v2.pdf | author:Shibamouli Lahiri, Xiaofei Lu category:cs.CL H.3.1; I.2.7 published:2011-09-01 summary:Formality is one of the most important dimensions of writing style variation.In this study we conducted an inter-rater reliability experiment for assessingsentence formality on a five-point Likert scale, and obtained good agreementresults as well as different rating distributions for different sentencecategories. We also performed a difficulty analysis to identify the bottlenecksof our rating procedure. Our main objective is to design an automatic scoringmechanism for sentence-level formality, and this study is important for thatpurpose.
arxiv-300-107 | Local Component Analysis | http://arxiv.org/pdf/1109.0093v4.pdf | author:Nicolas Le Roux, Francis Bach category:cs.LG published:2011-09-01 summary:Kernel density estimation, a.k.a. Parzen windows, is a popular densityestimation method, which can be used for outlier detection or clustering. Withmultivariate data, its performance is heavily reliant on the metric used withinthe kernel. Most earlier work has focused on learning only the bandwidth of thekernel (i.e., a scalar multiplicative factor). In this paper, we propose tolearn a full Euclidean metric through an expectation-minimization (EM)procedure, which can be seen as an unsupervised counterpart to neighbourhoodcomponent analysis (NCA). In order to avoid overfitting with a fullynonparametric density estimator in high dimensions, we also consider asemi-parametric Gaussian-Parzen density model, where some of the variables aremodelled through a jointly Gaussian density, while others are modelled throughParzen windows. For these two models, EM leads to simple closed-form updatesbased on matrix inversions and eigenvalue decompositions. We show empiricallythat our method leads to density estimators with higher test-likelihoods thannatural competing methods, and that the metrics may be used within mostunsupervised learning techniques that rely on such metrics, such as spectralclustering or manifold learning methods. Finally, we present a stochasticapproximation scheme which allows for the use of this method in a large-scalesetting.
arxiv-300-108 | Nonconvex proximal splitting: batch and incremental algorithms | http://arxiv.org/pdf/1109.0258v2.pdf | author:Suvrit Sra category:math.OC stat.ML published:2011-09-01 summary:Within the unmanageably large class of nonconvex optimization, we considerthe rich subclass of nonsmooth problems that have composite objectives---thisalready includes the extensively studied convex, composite objective problemsas a special case. For this subclass, we introduce a powerful, new frameworkthat permits asymptotically non-vanishing perturbations. In particular, wedevelop perturbation-based batch and incremental (online like) nonconvexproximal splitting algorithms. To our knowledge, this is the first time thatsuch perturbation-based nonconvex splitting algorithms are being proposed andanalyzed. While the main contribution of the paper is the theoreticalframework, we complement our results by presenting some empirical results onmatrix factorization.
arxiv-300-109 | Off-grid Direction of Arrival Estimation Using Sparse Bayesian Inference | http://arxiv.org/pdf/1108.5838v4.pdf | author:Zai Yang, Lihua Xie, Cishen Zhang category:stat.AP cs.IT math.IT stat.ML published:2011-08-30 summary:Direction of arrival (DOA) estimation is a classical problem in signalprocessing with many practical applications. Its research has recently beenadvanced owing to the development of methods based on sparse signalreconstruction. While these methods have shown advantages over conventionalones, there are still difficulties in practical situations where true DOAs arenot on the discretized sampling grid. To deal with such an off-grid DOAestimation problem, this paper studies an off-grid model that takes intoaccount effects of the off-grid DOAs and has a smaller modeling error. Aniterative algorithm is developed based on the off-grid model from a Bayesianperspective while joint sparsity among different snapshots is exploited byassuming a Laplace prior for signals at all snapshots. The new approach appliesto both single snapshot and multi-snapshot cases. Numerical simulations showthat the proposed algorithm has improved accuracy in terms of mean squaredestimation error. The algorithm can maintain high estimation accuracy evenunder a very coarse sampling grid.
arxiv-300-110 | Solving Principal Component Pursuit in Linear Time via $l_1$ Filtering | http://arxiv.org/pdf/1108.5359v4.pdf | author:Risheng Liu, Zhouchen Lin, Siming Wei, Zhixun Su category:cs.NA cs.CV published:2011-08-26 summary:In the past decades, exactly recovering the intrinsic data structure fromcorrupted observations, which is known as robust principal component analysis(RPCA), has attracted tremendous interests and found many applications incomputer vision. Recently, this problem has been formulated as recovering alow-rank component and a sparse component from the observed data matrix. It isproved that under some suitable conditions, this problem can be exactly solvedby principal component pursuit (PCP), i.e., minimizing a combination of nuclearnorm and $l_1$ norm. Most of the existing methods for solving PCP requiresingular value decompositions (SVD) of the data matrix, resulting in a highcomputational complexity, hence preventing the applications of RPCA to verylarge scale computer vision problems. In this paper, we propose a novelalgorithm, called $l_1$ filtering, for \emph{exactly} solving PCP with an$O(r^2(m+n))$ complexity, where $m\times n$ is the size of data matrix and $r$is the rank of the matrix to recover, which is supposed to be much smaller than$m$ and $n$. Moreover, $l_1$ filtering is \emph{highly parallelizable}. It isthe first algorithm that can \emph{exactly} solve a nuclear norm minimizationproblem in \emph{linear time} (with respect to the data size). Experiments onboth synthetic data and real applications testify to the great advantage of$l_1$ filtering in speed over state-of-the-art algorithms.
arxiv-300-111 | Semi-supervised logistic discrimination via labeled data and unlabeled data from different sampling distributions | http://arxiv.org/pdf/1108.5244v3.pdf | author:Shuichi Kawano category:stat.ML stat.ME published:2011-08-26 summary:This article addresses the problem of classification method based on bothlabeled and unlabeled data, where we assume that a density function for labeleddata is different from that for unlabeled data. We propose a semi-supervisedlogistic regression model for classification problem along with the techniqueof covariate shift adaptation. Unknown parameters involved in proposed modelsare estimated by regularization with EM algorithm. A crucial issue in themodeling process is the choices of tuning parameters in our semi-supervisedlogistic models. In order to select the parameters, a model selection criterionis derived from an information-theoretic approach. Some numerical studies showthat our modeling procedure performs well in various cases.
arxiv-300-112 | Learning from Complex Systems: On the Roles of Entropy and Fisher Information in Pairwise Isotropic Gaussian Markov Random Fields | http://arxiv.org/pdf/1108.4973v12.pdf | author:Alexandre L. M. Levada category:cs.IT cs.AI cs.CV math.IT stat.CO published:2011-08-25 summary:Markov Random Field models are powerful tools for the study of complexsystems. However, little is known about how the interactions between theelements of such systems are encoded, especially from an information-theoreticperspective. In this paper, our goal is to enlight the connection betweenFisher information, Shannon entropy, information geometry and the behavior ofcomplex systems modeled by isotropic pairwise Gaussian Markov random fields. Wepropose analytical expressions to compute local and global versions of thesemeasures using Besag's pseudo-likelihood function, characterizing the system'sbehavior through its \emph{Fisher curve}, a parametric trajectory accross theinformation space that provides a geometric representation for the study ofcomplex systems. Computational experiments show how the proposed tools can beuseful in extrating relevant information from complex patterns. The obtainedresults quantify and support our main conclusion, which is: in terms ofinformation, moving towards higher entropy states (A --> B) is different frommoving towards lower entropy states (B --> A), since the \emph{Fisher curves}are not the same given a natural orientation (the direction of time).
arxiv-300-113 | Novel Analysis of Population Scalability in Evolutionary Algorithms | http://arxiv.org/pdf/1108.4531v4.pdf | author:Jun He, Tianshi Chen, Boris Mitavskiy category:cs.NE published:2011-08-23 summary:Population-based evolutionary algorithms (EAs) have been widely applied tosolve various optimization problems. The question of how the performance of apopulation-based EA depends on the population size arises naturally. Theperformance of an EA may be evaluated by different measures, such as theaverage convergence rate to the optimal set per generation or the expectednumber of generations to encounter an optimal solution for the first time.Population scalability is the performance ratio between a benchmark EA andanother EA using identical genetic operators but a larger population size.Although intuitively the performance of an EA may improve if its populationsize increases, currently there exist only a few case studies for simplefitness functions. This paper aims at providing a general study for discreteoptimisation. A novel approach is introduced to analyse population scalabilityusing the fundamental matrix. The following two contributions summarize themajor results of the current article. (1) We demonstrate rigorously that forelitist EAs with identical global mutation, using a lager population sizealways increases the average rate of convergence to the optimal set; and yet,sometimes, the expected number of generations needed to find an optimalsolution (measured by either the maximal value or the average value) mayincrease, rather than decrease. (2) We establish sufficient and/or necessaryconditions for the superlinear scalability, that is, when the averageconvergence rate of a $(\mu+\mu)$ EA (where $\mu\ge2$) is bigger than $\mu$times that of a $(1+1)$ EA.
arxiv-300-114 | Optimal Algorithms for Ridge and Lasso Regression with Partially Observed Attributes | http://arxiv.org/pdf/1108.4559v2.pdf | author:Elad Hazan, Tomer Koren category:cs.LG published:2011-08-23 summary:We consider the most common variants of linear regression, including Ridge,Lasso and Support-vector regression, in a setting where the learner is allowedto observe only a fixed number of attributes of each example at training time.We present simple and efficient algorithms for these problems: for Lasso andRidge regression they need the same total number of attributes (up toconstants) as do full-information algorithms, for reaching a certain accuracy.For Support-vector regression, we require exponentially less attributescompared to the state of the art. By that, we resolve an open problem recentlyposed by Cesa-Bianchi et al. (2010). Experiments show the theoretical bounds tobe justified by superior performance compared to the state of the art.
arxiv-300-115 | Sparse Estimation using Bayesian Hierarchical Prior Modeling for Real and Complex Linear Models | http://arxiv.org/pdf/1108.4324v3.pdf | author:Niels Lovmand Pedersen, Carles Navarro Manchón, Mihai-Alin Badiu, Dmitriy Shutin, Bernard Henri Fleury category:stat.ML published:2011-08-22 summary:In sparse Bayesian learning (SBL), Gaussian scale mixtures (GSMs) have beenused to model sparsity-inducing priors that realize a class of concave penaltyfunctions for the regression task in real-valued signal models. Motivated bythe relative scarcity of formal tools for SBL in complex-valued models, thispaper proposes a GSM model - the Bessel K model - that induces concave penaltyfunctions for the estimation of complex sparse signals. The properties of theBessel K model are analyzed when it is applied to Type I and Type IIestimation. This analysis reveals that, by tuning the parameters of the mixingpdf different penalty functions are invoked depending on the estimation typeused, the value of the noise variance, and whether real or complex signals areestimated. Using the Bessel K model, we derive a sparse estimator based on amodification of the expectation-maximization algorithm formulated for Type IIestimation. The estimator includes as a special instance the algorithmsproposed by Tipping and Faul [1] and by Babacan et al. [2]. Numerical resultsshow the superiority of the proposed estimator over these state-of-the-artestimators in terms of convergence speed, sparseness, reconstruction error, androbustness in low and medium signal-to-noise ratio regimes.
arxiv-300-116 | Complex-Valued Autoencoders | http://arxiv.org/pdf/1108.4135v2.pdf | author:Pierre Baldi, Zhiqin Lu category:cs.NE math.RA published:2011-08-20 summary:Autoencoders are unsupervised machine learning circuits whose learning goalis to minimize a distortion measure between inputs and outputs. Linearautoencoders can be defined over any field and only real-valued linearautoencoder have been studied so far. Here we study complex-valued linearautoencoders where the components of the training vectors and adjustablematrices are defined over the complex field with the $L_2$ norm. We providesimpler and more general proofs that unify the real-valued and complex-valuedcases, showing that in both cases the landscape of the error function isinvariant under certain groups of transformations. The landscape has no localminima, a family of global minima associated with Principal Component Analysis,and many families of saddle points associated with orthogonal projections ontosub-space spanned by sub-optimal subsets of eigenvectors of the covariancematrix. The theory yields several iterative, convergent, learning algorithms, aclear understanding of the generalization properties of the trainedautoencoders, and can equally be applied to the hetero-associative case whenexternal targets are provided. Partial results on deep architecture as well asthe differential geometry of autoencoders are also presented. The generalframework described here is useful to classify autoencoders and identifygeneral common properties that ought to be investigated for each class,illuminating some of the connections between information theory, unsupervisedlearning, clustering, Hebbian learning, and autoencoders.
arxiv-300-117 | Dynamic Pricing with Limited Supply | http://arxiv.org/pdf/1108.4142v3.pdf | author:Moshe Babaioff, Shaddin Dughmi, Robert Kleinberg, Aleksandrs Slivkins category:cs.GT cs.DS cs.LG published:2011-08-20 summary:We consider the problem of dynamic pricing with limited supply. A seller has$k$ identical items for sale and is facing $n$ potential buyers ("agents") thatare arriving sequentially. Each agent is interested in buying one item. Eachagent's value for an item is an IID sample from some fixed distribution withsupport $[0,1]$. The seller offers a take-it-or-leave-it price to each arrivingagent (possibly different for different agents), and aims to maximize hisexpected revenue. We focus on "prior-independent" mechanisms -- ones that do not use anyinformation about the distribution. They are desirable because knowing thedistribution is unrealistic in many practical scenarios. We study how therevenue of such mechanisms compares to the revenue of the optimal offlinemechanism that knows the distribution ("offline benchmark"). We present a prior-independent dynamic pricing mechanism whose revenue is atmost $O((k \log n)^{2/3})$ less than the offline benchmark, for everydistribution that is regular. In fact, this guarantee holds without *any*assumptions if the benchmark is relaxed to fixed-price mechanisms. Further, weprove a matching lower bound. The performance guarantee for the same mechanismcan be improved to $O(\sqrt{k} \log n)$, with a distribution-dependentconstant, if $k/n$ is sufficiently small. We show that, in the worst case overall demand distributions, this is essentially the best rate that can beobtained with a distribution-specific constant. On a technical level, we exploit the connection to multi-armed bandits (MAB).While dynamic pricing with unlimited supply can easily be seen as an MABproblem, the intuition behind MAB approaches breaks when applied to the settingwith limited supply. Our high-level conceptual contribution is that even thelimited supply setting can be fruitfully treated as a bandit problem.
arxiv-300-118 | Simulation-based optimal Bayesian experimental design for nonlinear systems | http://arxiv.org/pdf/1108.4146v3.pdf | author:Xun Huan, Youssef M. Marzouk category:stat.ML stat.CO stat.ME published:2011-08-20 summary:The optimal selection of experimental conditions is essential to maximizingthe value of data for inference and prediction, particularly in situationswhere experiments are time-consuming and expensive to conduct. We propose ageneral mathematical framework and an algorithmic approach for optimalexperimental design with nonlinear simulation-based models; in particular, wefocus on finding sets of experiments that provide the most information abouttargeted sets of parameters. Our framework employs a Bayesian statistical setting, which provides afoundation for inference from noisy, indirect, and incomplete data, and anatural mechanism for incorporating heterogeneous sources of information. Anobjective function is constructed from information theoretic measures,reflecting expected information gain from proposed combinations of experiments.Polynomial chaos approximations and a two-stage Monte Carlo sampling method areused to evaluate the expected information gain. Stochastic approximationalgorithms are then used to make optimization feasible in computationallyintensive and high-dimensional settings. These algorithms are demonstrated onmodel problems and on nonlinear parameter estimation problems arising indetailed combustion kinetics.
arxiv-300-119 | Hierarchical Object Parsing from Structured Noisy Point Clouds | http://arxiv.org/pdf/1108.3605v2.pdf | author:Adrian Barbu category:cs.CV published:2011-08-18 summary:Object parsing and segmentation from point clouds are challenging tasksbecause the relevant data is available only as thin structures along objectboundaries or other features, and is corrupted by large amounts of noise. Tohandle this kind of data, flexible shape models are desired that can accuratelyfollow the object boundaries. Popular models such as Active Shape and ActiveAppearance models lack the necessary flexibility for this task, while recentapproaches such as the Recursive Compositional Models make modelsimplifications in order to obtain computational guarantees. This paperinvestigates a hierarchical Bayesian model of shape and appearance in agenerative setting. The input data is explained by an object parsing layer,which is a deformation of a hidden PCA shape model with Gaussian prior. Thepaper also introduces a novel efficient inference algorithm that uses informeddata-driven proposals to initialize local searches for the hidden variables.Applied to the problem of object parsing from structured point clouds such asedge detection images, the proposed approach obtains state of the art parsingerrors on two standard datasets without using any intensity information.
arxiv-300-120 | Premise Selection for Mathematics by Corpus Analysis and Kernel Methods | http://arxiv.org/pdf/1108.3446v2.pdf | author:Jesse Alama, Tom Heskes, Daniel Kühlwein, Evgeni Tsivtsivadze, Josef Urban category:cs.LG cs.AI 68T05 I.2.6; I.2.3 published:2011-08-17 summary:Smart premise selection is essential when using automated reasoning as a toolfor large-theory formal proof development. A good method for premise selectionin complex mathematical libraries is the application of machine learning tolarge corpora of proofs. This work develops learning-based premise selection intwo ways. First, a newly available minimal dependency analysis of existinghigh-level formal mathematical proofs is used to build a large knowledge baseof proof dependencies, providing precise data for ATP-based re-verification andfor training premise selection algorithms. Second, a new machine learningalgorithm for premise selection based on kernel methods is proposed andimplemented. To evaluate the impact of both techniques, a benchmark consistingof 2078 large-theory mathematical problems is constructed,extending the olderMPTP Challenge benchmark. The combined effect of the techniques results in a50% improvement on the benchmark over the Vampire/SInE state-of-the-art systemfor automated reasoning in large theories.
arxiv-300-121 | Kernel Methods for the Approximation of Nonlinear Systems | http://arxiv.org/pdf/1108.2903v3.pdf | author:Jake Bouvrie, Boumediene Hamzi category:math.OC cs.SY math.DS stat.ML published:2011-08-14 summary:We introduce a data-driven order reduction method for nonlinear controlsystems, drawing on recent progress in machine learning and statisticaldimensionality reduction. The method rests on the assumption that the nonlinearsystem behaves linearly when lifted into a high (or infinite) dimensionalfeature space where balanced truncation may be carried out implicitly. Thisleads to a nonlinear reduction map which can be combined with a representationof the system belonging to a reproducing kernel Hilbert space to give a closed,reduced order dynamical system which captures the essential input-outputcharacteristics of the original model. Empirical simulations illustrating theapproach are also provided.
arxiv-300-122 | Adaptive sequential Monte Carlo by means of mixture of experts | http://arxiv.org/pdf/1108.2836v2.pdf | author:J. Cornebise, E. Moulines, J. Olsson category:stat.ME stat.CO stat.ML published:2011-08-14 summary:Appropriately designing the proposal kernel of particle filters is an issueof significant importance, since a bad choice may lead to deterioration of theparticle sample and, consequently, waste of computational power. In this paperwe introduce a novel algorithm adaptively approximating the so-called optimalproposal kernel by a mixture of integrated curved exponential distributionswith logistic weights. This family of distributions, referred to as mixtures ofexperts, is broad enough to be used in the presence of multi-modality orstrongly skewed distributions. The mixtures are fitted, via online-EM methods,to the optimal kernel through minimisation of the Kullback-Leibler divergencebetween the auxiliary target and instrumental distributions of the particlefilter. At each iteration of the particle filter, the algorithm is required tosolve only a single optimisation problem for the whole particle sample,yielding an algorithm with only linear complexity. In addition, we illustratein a simulation study how the method can be successfully applied to optimalfiltering in nonlinear state-space models.
arxiv-300-123 | A More Powerful Two-Sample Test in High Dimensions using Random Projection | http://arxiv.org/pdf/1108.2401v3.pdf | author:Miles E. Lopes, Laurent J. Jacob, Martin J. Wainwright category:math.ST stat.ME stat.ML stat.TH published:2011-08-11 summary:We consider the hypothesis testing problem of detecting a shift between themeans of two multivariate normal distributions in the high-dimensional setting,allowing for the data dimension p to exceed the sample size n. Specifically, wepropose a new test statistic for the two-sample test of means that integrates arandom projection with the classical Hotelling T^2 statistic. Working under ahigh-dimensional framework with (p,n) tending to infinity, we first derive anasymptotic power function for our test, and then provide sufficient conditionsfor it to achieve greater power than other state-of-the-art tests. Using ROCcurves generated from synthetic data, we demonstrate superior performanceagainst competing tests in the parameter regimes anticipated by our theoreticalresults. Lastly, we illustrate an advantage of our procedure's false positiverate with comparisons on high-dimensional gene expression data involving thediscrimination of different types of cancer.
arxiv-300-124 | A survey on independence-based Markov networks learning | http://arxiv.org/pdf/1108.2283v2.pdf | author:Federico Schlüter category:cs.AI cs.LG published:2011-08-10 summary:This work reports the most relevant technical aspects in the problem oflearning the \emph{Markov network structure} from data. Such problem has becomeincreasingly important in machine learning, and many other application fieldsof machine learning. Markov networks, together with Bayesian networks, areprobabilistic graphical models, a widely used formalism for handlingprobability distributions in intelligent systems. Learning graphical modelsfrom data have been extensively applied for the case of Bayesian networks, butfor Markov networks learning it is not tractable in practice. However, thissituation is changing with time, given the exponential growth of computerscapacity, the plethora of available digital data, and the researching on newlearning technologies. This work stresses on a technology calledindependence-based learning, which allows the learning of the independencestructure of those networks from data in an efficient and sound manner,whenever the dataset is sufficiently large, and data is a representativesampling of the target distribution. In the analysis of such technology, thiswork surveys the current state-of-the-art algorithms for learning Markovnetworks structure, discussing its current limitations, and proposing a seriesof open problems where future works may produce some advances in the area interms of quality and efficiency. The paper concludes by opening a discussionabout how to develop a general formalism for improving the quality of thestructures learned, when data is scarce.
arxiv-300-125 | A consistent adjacency spectral embedding for stochastic blockmodel graphs | http://arxiv.org/pdf/1108.2228v3.pdf | author:Daniel L. Sussman, Minh Tang, Donniell E. Fishkind, Carey E. Priebe category:stat.ML published:2011-08-10 summary:We present a method to estimate block membership of nodes in a random graphgenerated by a stochastic blockmodel. We use an embedding procedure motivatedby the random dot product graph model, a particular example of the latentposition model. The embedding associates each node with a vector; these vectorsare clustered via minimization of a square error criterion. We prove that thismethod is consistent for assigning nodes to blocks, as only a negligible numberof nodes will be mis-assigned. We prove consistency of the method for directedand undirected graphs. The consistent block assignment makes possibleconsistent parameter estimation for a stochastic blockmodel. We extend theresult in the setting where the number of blocks grows slowly with the numberof nodes. Our method is also computationally feasible even for very largegraphs. We compare our method to Laplacian spectral clustering through analysisof simulated data and a graph derived from Wikipedia documents.
arxiv-300-126 | An application of the stationary phase method for estimating probability densities of function derivatives | http://arxiv.org/pdf/1108.1783v4.pdf | author:Karthik S. Gurumoorthy, Anand Rangarajan, Arunava Banerjee category:stat.ML math.ST stat.TH published:2011-08-08 summary:We prove a novel result wherein the density function of thegradients---corresponding to density function of the derivatives in onedimension---of a thrice differentiable function S (obtained via a randomvariable transformation of a uniformly distributed random variable) defined ona closed, bounded interval \Omega \subset R is accurately approximated by thenormalized power spectrum of \phi=exp(iS/\tau) as the free parameter \tau-->0.The result is shown using the well known stationary phase approximation andstandard integration techniques and requires proper ordering of limits.Experimental results provide anecdotal visual evidence corroborating theresult.
arxiv-300-127 | Serialising the ISO SynAF Syntactic Object Model | http://arxiv.org/pdf/1108.0631v3.pdf | author:Laurent Romary, Amir Zeldes, Florian Zipser category:cs.CL published:2011-08-02 summary:This paper introduces, an XML format developed to serialise the object modeldefined by the ISO Syntactic Annotation Framework SynAF. Based on widespreadbest practices we adapt a popular XML format for syntactic annotation,TigerXML, with additional features to support a variety of syntactic phenomenaincluding constituent and dependency structures, binding, and different nodetypes such as compounds or empty elements. We also define interfaces to otherformats and standards including the Morpho-syntactic Annotation Framework MAFand the ISOCat Data Category Registry. Finally a case study of the GermanTreebank TueBa-D/Z is presented, showcasing the handling of constituentstructures, topological fields and coreference annotation in tandem.
arxiv-300-128 | Cross-moments computation for stochastic context-free grammars | http://arxiv.org/pdf/1108.0353v2.pdf | author:Velimir M. Ilic, Miroslav D. Ciric, Miomir S. Stankovic category:cs.CL published:2011-08-01 summary:In this paper we consider the problem of efficient computation ofcross-moments of a vector random variable represented by a stochasticcontext-free grammar. Two types of cross-moments are discussed. The samplespace for the first one is the set of all derivations of the context-freegrammar, and the sample space for the second one is the set of all derivationswhich generate a string belonging to the language of the grammar. In the past,this problem was widely studied, but mainly for the cross-moments of scalarvariables and up to the second order. This paper presents new algorithms forcomputing the cross-moments of an arbitrary order, and the previously developedones are derived as special cases.
arxiv-300-129 | Expectation-Propagation for Likelihood-Free Inference | http://arxiv.org/pdf/1107.5959v2.pdf | author:Simon Barthelmé, Nicolas Chopin category:stat.CO stat.ML published:2011-07-29 summary:Many models of interest in the natural and social sciences have noclosed-form likelihood function, which means that they cannot be treated usingthe usual techniques of statistical inference. In the case where such modelscan be efficiently simulated, Bayesian inference is still possible thanks tothe Approximate Bayesian Computation (ABC) algorithm. Although many refinementshave been suggested, ABC inference is still far from routine. ABC is oftenexcruciatingly slow due to very low acceptance rates. In addition, ABC requiresintroducing a vector of "summary statistics", the choice of which is relativelyarbitrary, and often require some trial and error, making the whole processquite laborious for the user. We introduce in this work the EP-ABC algorithm, which is an adaptation to thelikelihood-free context of the variational approximation algorithm known asExpectation Propagation (Minka, 2001). The main advantage of EP-ABC is that itis faster by a few orders of magnitude than standard algorithms, whileproducing an overall approximation error which is typically negligible. Asecond advantage of EP-ABC is that it replaces the usual global ABC constrainton the vector of summary statistics computed on the whole dataset, by n localconstraints of the form that apply separately to each data-point. As aconsequence, it is often possible to do away with summary statistics entirely.In that case, EP-ABC approximates directly the evidence (marginal likelihood)of the model. Comparisons are performed in three real-world applications which are typicalof likelihood-free inference, including one application in neuroscience whichis novel, and possibly too challenging for standard ABC techniques.
arxiv-300-130 | Confidence-Based Dynamic Classifier Combination For Mean-Shift Tracking | http://arxiv.org/pdf/1107.5850v2.pdf | author:Ibrahim Saygin Topkaya, Hakan Erdogan category:cs.CV I.4.8 published:2011-07-29 summary:We introduce a novel tracking technique which uses dynamic confidence-basedfusion of two different information sources for robust and efficient trackingof visual objects. Mean-shift tracking is a popular and well known method usedin object tracking problems. Originally, the algorithm uses a similaritymeasure which is optimized by shifting a search area to the center of agenerated weight image to track objects. Recent improvements on the originalmean-shift algorithm involves using a classifier that differentiates the objectfrom its surroundings. We adopt this classifier-based approach and propose anapplication of a classifier fusion technique within this classifier-basedcontext in this work. We use two different classifiers, where one comes from abackground modeling method, to generate the weight image and we calculatecontributions of the classifiers dynamically using their confidences togenerate a final weight image to be used in tracking. The contributions of theclassifiers are calculated by using correlations between histograms of theirweight images and histogram of a defined ideal weight image in the previousframe. We show with experiments that our dynamic combination scheme selectsgood contributions for classifiers for different cases and improves trackingaccuracy significantly.
arxiv-300-131 | The Divergence of Reinforcement Learning Algorithms with Value-Iteration and Function Approximation | http://arxiv.org/pdf/1107.4606v2.pdf | author:Michael Fairbank, Eduardo Alonso category:cs.LG published:2011-07-22 summary:This paper gives specific divergence examples of value-iteration for severalmajor Reinforcement Learning and Adaptive Dynamic Programming algorithms, whenusing a function approximator for the value function. These divergence examplesdiffer from previous divergence examples in the literature, in that they areapplicable for a greedy policy, i.e. in a "value iteration" scenario. Perhapssurprisingly, with a greedy policy, it is also possible to get divergence forthe algorithms TD(1) and Sarsa(1). In addition to these divergences, we alsoachieve divergence for the Adaptive Dynamic Programming algorithms HDP, DHP andGDHP.
arxiv-300-132 | A Unifying Analysis of Projected Gradient Descent for $\ell_p$-constrained Least Squares | http://arxiv.org/pdf/1107.4623v5.pdf | author:Sohail Bahmani, Bhiksha Raj category:math.NA cs.IT math.IT math.OC stat.ML published:2011-07-22 summary:In this paper we study the performance of the Projected Gradient Descent(PGD)algorithm for $\ell_{p}$-constrained least squares problems that arise in theframework of Compressed Sensing. Relying on the Restricted Isometry Property,we provide convergence guarantees for this algorithm for the entire range of$0\leq p\leq1$, that include and generalize the existing results for theIterative Hard Thresholding algorithm and provide a new accuracy guarantee forthe Iterative Soft Thresholding algorithm as special cases. Our results suggestthat in this group of algorithms, as $p$ increases from zero to one, conditionsrequired to guarantee accuracy become stricter and robustness to noisedeteriorates.
arxiv-300-133 | Multi-Task Averaging | http://arxiv.org/pdf/1107.4390v4.pdf | author:Sergey Feldman, Bela A. Frigyik, Maya R. Gupta category:stat.ML stat.ME published:2011-07-21 summary:We present a multi-task learning approach to jointly estimate the means ofmultiple independent data sets. The proposed multi-task averaging (MTA)algorithm results in a convex combination of the single-task maximum likelihoodestimates. We derive the optimal minimum risk estimator and the minimaxestimator, and show that these estimators can be efficiently estimated.Simulations and real data experiments demonstrate that MTA estimators oftenoutperform both single-task and James-Stein estimators.
arxiv-300-134 | Optimal Adaptive Learning in Uncontrolled Restless Bandit Problems | http://arxiv.org/pdf/1107.4042v3.pdf | author:Cem Tekin, Mingyan Liu category:math.OC cs.LG published:2011-07-20 summary:In this paper we consider the problem of learning the optimal policy foruncontrolled restless bandit problems. In an uncontrolled restless banditproblem, there is a finite set of arms, each of which when pulled yields apositive reward. There is a player who sequentially selects one of the arms ateach time step. The goal of the player is to maximize its undiscounted rewardover a time horizon T. The reward process of each arm is a finite state Markovchain, whose transition probabilities are unknown by the player. Statetransitions of each arm is independent of the selection of the player. Wepropose a learning algorithm with logarithmic regret uniformly over time withrespect to the optimal finite horizon policy. Our results extend the optimaladaptive learning of MDPs to POMDPs.
arxiv-300-135 | On the Computational Complexity of Stochastic Controller Optimization in POMDPs | http://arxiv.org/pdf/1107.3090v2.pdf | author:Nikos Vlassis, Michael L. Littman, David Barber category:cs.CC cs.LG cs.SY math.OC F.2.1 published:2011-07-15 summary:We show that the problem of finding an optimal stochastic 'blind' controllerin a Markov decision process is an NP-hard problem. The corresponding decisionproblem is NP-hard, in PSPACE, and SQRT-SUM-hard, hence placing it in NP wouldimply breakthroughs in long-standing open problems in computer science. Ourresult establishes that the more general problem of stochastic controlleroptimization in POMDPs is also NP-hard. Nonetheless, we outline a special casethat is convex and admits efficient global solutions.
arxiv-300-136 | From Small-World Networks to Comparison-Based Search | http://arxiv.org/pdf/1107.3059v3.pdf | author:Amin Karbasi, Stratis Ioannidis, Laurent Massoulie category:cs.LG cs.DS cs.IT cs.SI math.IT stat.ML published:2011-07-15 summary:The problem of content search through comparisons has recently receivedconsiderable attention. In short, a user searching for a target objectnavigates through a database in the following manner: the user is asked toselect the object most similar to her target from a small list of objects. Anew object list is then presented to the user based on her earlier selection.This process is repeated until the target is included in the list presented, atwhich point the search terminates. This problem is known to be strongly relatedto the small-world network design problem. However, contrary to prior work, which focuses on cases where objects in thedatabase are equally popular, we consider here the case where the demand forobjects may be heterogeneous. We show that, under heterogeneous demand, thesmall-world network design problem is NP-hard. Given the above negative result,we propose a novel mechanism for small-world design and provide an upper boundon its performance under heterogeneous demand. The above mechanism has anatural equivalent in the context of content search through comparisons, and weestablish both an upper bound and a lower bound for the performance of thismechanism. These bounds are intuitively appealing, as they depend on theentropy of the demand as well as its doubling constant, a quantity capturingthe topology of the set of target objects. They also illustrate interestingconnections between comparison-based search to classic results from informationtheory. Finally, we propose an adaptive learning algorithm for content searchthat meets the performance guarantees achieved by the above mechanisms.
arxiv-300-137 | Learning $k$-Modal Distributions via Testing | http://arxiv.org/pdf/1107.2700v3.pdf | author:Constantinos Daskalakis, Ilias Diakonikolas, Rocco A. Servedio category:cs.DS cs.LG math.ST stat.TH published:2011-07-13 summary:A $k$-modal probability distribution over the discrete domain $\{1,...,n\}$is one whose histogram has at most $k$ "peaks" and "valleys." Suchdistributions are natural generalizations of monotone ($k=0$) and unimodal($k=1$) probability distributions, which have been intensively studied inprobability theory and statistics. In this paper we consider the problem of \emph{learning} (i.e., performingdensity estimation of) an unknown $k$-modal distribution with respect to the$L_1$ distance. The learning algorithm is given access to independent samplesdrawn from an unknown $k$-modal distribution $p$, and it must output ahypothesis distribution $\widehat{p}$ such that with high probability the totalvariation distance between $p$ and $\widehat{p}$ is at most $\epsilon.$ Ourmain goal is to obtain \emph{computationally efficient} algorithms for thisproblem that use (close to) an information-theoretically optimal number ofsamples. We give an efficient algorithm for this problem that runs in time$\mathrm{poly}(k,\log(n),1/\epsilon)$. For $k \leq \tilde{O}(\log n)$, thenumber of samples used by our algorithm is very close (within an$\tilde{O}(\log(1/\epsilon))$ factor) to being information-theoreticallyoptimal. Prior to this work computationally efficient algorithms were knownonly for the cases $k=0,1$ \cite{Birge:87b,Birge:97}. A novel feature of our approach is that our learning algorithm crucially usesa new algorithm for \emph{property testing of probability distributions} as akey subroutine. The learning algorithm uses the property tester to efficientlydecompose the $k$-modal distribution into $k$ (near-)monotone distributions,which are easier to learn.
arxiv-300-138 | Provably Safe and Robust Learning-Based Model Predictive Control | http://arxiv.org/pdf/1107.2487v2.pdf | author:Anil Aswani, Humberto Gonzalez, S. Shankar Sastry, Claire Tomlin category:math.OC cs.LG cs.SY math.ST stat.TH published:2011-07-13 summary:Controller design faces a trade-off between robustness and performance, andthe reliability of linear controllers has caused many practitioners to focus onthe former. However, there is renewed interest in improving system performanceto deal with growing energy constraints. This paper describes a learning-basedmodel predictive control (LBMPC) scheme that provides deterministic guaranteeson robustness, while statistical identification tools are used to identifyricher models of the system in order to improve performance; the benefits ofthis framework are that it handles state and input constraints, optimizessystem performance with respect to a cost function, and can be designed to usea wide variety of parametric or nonparametric statistical tools. The maininsight of LBMPC is that safety and performance can be decoupled underreasonable conditions in an optimization framework by maintaining two models ofthe system. The first is an approximate model with bounds on its uncertainty,and the second model is updated by statistical methods. LBMPC improvesperformance by choosing inputs that minimize a cost subject to the learneddynamics, and it ensures safety and robustness by checking whether these sameinputs keep the approximate model stable when it is subject to uncertainty.Furthermore, we show that if the system is sufficiently excited, then the LBMPCcontrol action probabilistically converges to that of an MPC computed using thetrue dynamics.
arxiv-300-139 | Learning Poisson Binomial Distributions | http://arxiv.org/pdf/1107.2702v4.pdf | author:Constantinos Daskalakis, Ilias Diakonikolas, Rocco A. Servedio category:cs.DS cs.LG math.ST stat.TH published:2011-07-13 summary:We consider a basic problem in unsupervised learning: learning an unknown\emph{Poisson Binomial Distribution}. A Poisson Binomial Distribution (PBD)over $\{0,1,\dots,n\}$ is the distribution of a sum of $n$ independentBernoulli random variables which may have arbitrary, potentially non-equal,expectations. These distributions were first studied by S. Poisson in 1837\cite{Poisson:37} and are a natural $n$-parameter generalization of thefamiliar Binomial Distribution. Surprisingly, prior to our work this basiclearning problem was poorly understood, and known results for it were far fromoptimal. We essentially settle the complexity of the learning problem for this basicclass of distributions. As our first main result we give a highly efficientalgorithm which learns to $\eps$-accuracy (with respect to the total variationdistance) using $\tilde{O}(1/\eps^3)$ samples \emph{independent of $n$}. Therunning time of the algorithm is \emph{quasilinear} in the size of its inputdata, i.e., $\tilde{O}(\log(n)/\eps^3)$ bit-operations. (Observe that each drawfrom the distribution is a $\log(n)$-bit string.) Our second main result is a{\em proper} learning algorithm that learns to $\eps$-accuracy using$\tilde{O}(1/\eps^2)$ samples, and runs in time $(1/\eps)^{\poly (\log(1/\eps))} \cdot \log n$. This is nearly optimal, since any algorithm {for thisproblem} must use $\Omega(1/\eps^2)$ samples. We also give positive andnegative results for some extensions of this learning problem to weighted sumsof independent Bernoulli random variables.
arxiv-300-140 | Data Stability in Clustering: A Closer Look | http://arxiv.org/pdf/1107.2379v5.pdf | author:Shalev Ben-David, Lev Reyzin category:cs.LG cs.DS published:2011-07-12 summary:We consider the model introduced by Bilu and Linial (2010), who studyproblems for which the optimal clustering does not change when distances areperturbed. They show that even when a problem is NP-hard, it is sometimespossible to obtain efficient algorithms for instances resilient to certainmultiplicative perturbations, e.g. on the order of $O(\sqrt{n})$ for max-cutclustering. Awasthi et al. (2010) consider center-based objectives, and Balcanand Liang (2011) analyze the $k$-median and min-sum objectives, givingefficient algorithms for instances resilient to certain constant multiplicativeperturbations. Here, we are motivated by the question of to what extent these assumptionscan be relaxed while allowing for efficient algorithms. We show there is littleroom to improve these results by giving NP-hardness lower bounds for both the$k$-median and min-sum objectives. On the other hand, we show that constantmultiplicative resilience parameters can be so strong as to make the clusteringproblem trivial, leaving only a narrow range of resilience parameters for whichclustering is interesting. We also consider a model of additive perturbationsand give a correspondence between additive and multiplicative notions ofstability. Our results provide a close examination of the consequences ofassuming stability in data.
arxiv-300-141 | An estimation of distribution algorithm with adaptive Gibbs sampling for unconstrained global optimization | http://arxiv.org/pdf/1107.2104v2.pdf | author:Jonás Velasco, Mario A. Saucedo-Espinosa, Hugo Jair Escalante, Karlo Mendoza, César Emilio Villarreal-Rodríguez, Óscar L. Chacón-Mondragón, Adrián Rodríguez, Arturo Berrones category:cs.NE math.OC stat.ML published:2011-07-11 summary:In this paper is proposed a new heuristic approach belonging to the field ofevolutionary Estimation of Distribution Algorithms (EDAs). EDAs builds aprobability model and a set of solutions is sampled from the model whichcharacterizes the distribution of such solutions. The main framework of theproposed method is an estimation of distribution algorithm, in which anadaptive Gibbs sampling is used to generate new promising solutions and, incombination with a local search strategy, it improves the individual solutionsproduced in each iteration. The Estimation of Distribution Algorithm withAdaptive Gibbs Sampling we are proposing in this paper is called AGEDA. Weexperimentally evaluate and compare this algorithm against two deterministicprocedures and several stochastic methods in three well known test problems forunconstrained global optimization. It is empirically shown that our heuristicis robust in problems that involve three central aspects that mainly determinethe difficulty of global optimization problems, namely high-dimensionality,multi-modality and non-smoothness.
arxiv-300-142 | Multi-Instance Learning with Any Hypothesis Class | http://arxiv.org/pdf/1107.2021v3.pdf | author:Sivan Sabato, Naftali Tishby category:cs.LG stat.ML published:2011-07-11 summary:In the supervised learning setting termed Multiple-Instance Learning (MIL),the examples are bags of instances, and the bag label is a function of thelabels of its instances. Typically, this function is the Boolean OR. Thelearner observes a sample of bags and the bag labels, but not the instancelabels that determine the bag labels. The learner is then required to emit aclassification rule for bags based on the sample. MIL has numerousapplications, and many heuristic algorithms have been used successfully on thisproblem, each adapted to specific settings or applications. In this work weprovide a unified theoretical analysis for MIL, which holds for any underlyinghypothesis class, regardless of a specific application or problem domain. Weshow that the sample complexity of MIL is only poly-logarithmically dependenton the size of the bag, for any underlying hypothesis class. In addition, weintroduce a new PAC-learning algorithm for MIL, which uses a regular supervisedlearning algorithm as an oracle. We prove that efficient PAC-learning for MILcan be generated from any efficient non-MIL supervised learning algorithm thathandles one-sided error. The computational complexity of the resultingalgorithm is only polynomially dependent on the bag size.
arxiv-300-143 | High-dimensional structure estimation in Ising models: Local separation criterion | http://arxiv.org/pdf/1107.1736v4.pdf | author:Animashree Anandkumar, Vincent Y. F. Tan, Furong Huang, Alan S. Willsky category:stat.ML cs.LG math.ST stat.TH published:2011-07-08 summary:We consider the problem of high-dimensional Ising (graphical) modelselection. We propose a simple algorithm for structure estimation based on thethresholding of the empirical conditional variation distances. We introduce anovel criterion for tractable graph families, where this method is efficient,based on the presence of sparse local separators between node pairs in theunderlying graph. For such graphs, the proposed algorithm has a samplecomplexity of $n=\Omega(J_{\min}^{-2}\log p)$, where $p$ is the number ofvariables, and $J_{\min}$ is the minimum (absolute) edge potential in themodel. We also establish nonasymptotic necessary and sufficient conditions forstructure estimation.
arxiv-300-144 | Polyceptron: A Polyhedral Learning Algorithm | http://arxiv.org/pdf/1107.1564v3.pdf | author:Naresh Manwani, P. S. Sastry category:cs.LG cs.NE published:2011-07-08 summary:In this paper we propose a new algorithm for learning polyhedral classifierswhich we call as Polyceptron. It is a Perception like algorithm which updatesthe parameters only when the current classifier misclassifies any trainingdata. We give both batch and online version of Polyceptron algorithm. Finallywe give experimental results to show the effectiveness of our approach.
arxiv-300-145 | On the information-theoretic structure of distributed measurements | http://arxiv.org/pdf/1107.1222v2.pdf | author:David Balduzzi category:cs.IT cs.DC cs.NE math.CT math.IT nlin.CG published:2011-07-06 summary:The internal structure of a measuring device, which depends on what itscomponents are and how they are organized, determines how it categorizes itsinputs. This paper presents a geometric approach to studying the internalstructure of measurements performed by distributed systems such asprobabilistic cellular automata. It constructs the quale, a family of sectionsof a suitably defined presheaf, whose elements correspond to the measurementsperformed by all subsystems of a distributed system. Using the quale wequantify (i) the information generated by a measurement; (ii) the extent towhich a measurement is context-dependent; and (iii) whether a measurement isdecomposable into independent submeasurements, which turns out to be equivalentto context-dependence. Finally, we show that only indecomposable measurementsare more informative than the sum of their submeasurements.
arxiv-300-146 | Automatic Road Lighting System (ARLS) Model Based on Image Processing of Moving Object | http://arxiv.org/pdf/1107.0845v4.pdf | author:Suprijadi, Thomas Muliawan, Sparisoma Viridi category:cs.CV published:2011-07-05 summary:Using a vehicle toy (in next future called vehicle) as a moving object anautomatic road lighting system (ARLS) model is constructed. A digital videocamera with 25 fps is used to capture the vehicle motion as it moves in thetest segment of the road. Captured images are then processed to calculatevehicle speed. This information of the speed together with position of vehicleis then used to control the lighting system along the path that passes by thevehicle. Length of the road test segment is 1 m, the video camera is positionedabout 1.1 m above the test segment, and the vehicle toy dimension is 13 cm\times 9.3 cm. In this model, the maximum speed that ARLS can handle is about1.32 m/s, and the highest performance is obtained about 91% at speed 0.93 m/s.
arxiv-300-147 | Distributed Matrix Completion and Robust Factorization | http://arxiv.org/pdf/1107.0789v7.pdf | author:Lester Mackey, Ameet Talwalkar, Michael I. Jordan category:cs.LG cs.DS cs.NA math.NA stat.ML published:2011-07-05 summary:If learning methods are to scale to the massive sizes of modern datasets, itis essential for the field of machine learning to embrace parallel anddistributed computing. Inspired by the recent development of matrixfactorization methods with rich theory but poor computational complexity and bythe relative ease of mapping matrices onto distributed architectures, weintroduce a scalable divide-and-conquer framework for noisy matrixfactorization. We present a thorough theoretical analysis of this framework inwhich we characterize the statistical errors introduced by the "divide" stepand control their magnitude in the "conquer" step, so that the overallalgorithm enjoys high-probability estimation guarantees comparable to those ofits base algorithm. We also present experiments in collaborative filtering andvideo background modeling that demonstrate the near-linear to superlinearspeed-ups attainable with this approach.
arxiv-300-148 | "Memory foam" approach to unsupervised learning | http://arxiv.org/pdf/1107.0674v3.pdf | author:Natalia B. Janson, Christopher J. Marsden category:nlin.AO cs.LG 34F05, 60G99 published:2011-07-04 summary:We propose an alternative approach to construct an artificial learningsystem, which naturally learns in an unsupervised manner. Its mathematicalprototype is a dynamical system, which automatically shapes its vector field inresponse to the input signal. The vector field converges to a gradient of amulti-dimensional probability density distribution of the input process, takenwith negative sign. The most probable patterns are represented by the stablefixed points, whose basins of attraction are formed automatically. Theperformance of this system is illustrated with musical signals.
arxiv-300-149 | Vision-Based Navigation I: A navigation filter for fusing DTM/correspondence updates | http://arxiv.org/pdf/1107.0399v4.pdf | author:Oleg Kupervasser, Vladimir Voronov category:cs.CV cs.AI 68T45 published:2011-07-02 summary:An algorithm for pose and motion estimation using corresponding features inimages and a digital terrain map is proposed. Using a Digital Terrain (orDigital Elevation) Map (DTM/DEM) as a global reference enables recovering theabsolute position and orientation of the camera. In order to do this, the DTMis used to formulate a constraint between corresponding features in twoconsecutive frames. The utilization of data is shown to improve the robustnessand accuracy of the inertial navigation algorithm. Extended Kalman filter wasused to combine results of inertial navigation algorithm and proposedvision-based navigation algorithm. The feasibility of this algorithms isestablished through numerical simulations.
arxiv-300-150 | On the origin of ambiguity in efficient communication | http://arxiv.org/pdf/1107.0193v3.pdf | author:Jordi Fortuny, Bernat Corominas-Murtra category:cs.CL published:2011-07-01 summary:This article studies the emergence of ambiguity in communication through theconcept of logical irreversibility and within the framework of Shannon'sinformation theory. This leads us to a precise and general expression of theintuition behind Zipf's vocabulary balance in terms of a symmetry equationbetween the complexities of the coding and the decoding processes that imposesan unavoidable amount of logical uncertainty in natural communication.Accordingly, the emergence of irreversible computations is required if thecomplexities of the coding and the decoding processes are balanced in asymmetric scenario, which means that the emergence of ambiguous codes is anecessary condition for natural communication to succeed.
arxiv-300-151 | A Note on Improved Loss Bounds for Multiple Kernel Learning | http://arxiv.org/pdf/1106.6258v2.pdf | author:Zakria Hussain, John Shawe-Taylor, Mario Marchand category:cs.LG published:2011-06-30 summary:In this paper, we correct an upper bound, presented in~\cite{hs-11}, on thegeneralisation error of classifiers learned through multiple kernel learning.The bound in~\cite{hs-11} uses Rademacher complexity and has an\emph{additive}dependence on the logarithm of the number of kernels and the margin achieved bythe classifier. However, there are some errors in parts of the proof which arecorrected in this paper. Unfortunately, the final result turns out to be a riskbound which has a \emph{multiplicative} dependence on the logarithm of thenumber of kernels and the margin achieved by the classifier.
arxiv-300-152 | Deterministic Sequencing of Exploration and Exploitation for Multi-Armed Bandit Problems | http://arxiv.org/pdf/1106.6104v3.pdf | author:Sattar Vakili, Keqin Liu, Qing Zhao category:math.OC cs.LG cs.SY math.PR math.ST stat.TH published:2011-06-30 summary:In the Multi-Armed Bandit (MAB) problem, there is a given set of arms withunknown reward models. At each time, a player selects one arm to play, aimingto maximize the total expected reward over a horizon of length T. An approachbased on a Deterministic Sequencing of Exploration and Exploitation (DSEE) isdeveloped for constructing sequential arm selection policies. It is shown thatfor all light-tailed reward distributions, DSEE achieves the optimallogarithmic order of the regret, where regret is defined as the total expectedreward loss against the ideal case with known reward models. For heavy-tailedreward distributions, DSEE achieves O(T^1/p) regret when the moments of thereward distributions exist up to the pth order for 1<p<=2 and O(T^1/(1+p/2))for p>2. With the knowledge of an upperbound on a finite moment of theheavy-tailed reward distributions, DSEE offers the optimal logarithmic regretorder. The proposed DSEE approach complements existing work on MAB by providingcorresponding results for general reward distributions. Furthermore, with aclearly defined tunable parameter-the cardinality of the exploration sequence,the DSEE approach is easily extendable to variations of MAB, including MAB withvarious objectives, decentralized MAB with multiple players and incompletereward observations under collisions, MAB with unknown Markov dynamics, andcombinatorial MAB with dependent arms that often arise in network optimizationproblems such as the shortest path, the minimum spanning, and the dominatingset problems under unknown random weights.
arxiv-300-153 | Kernels for Vector-Valued Functions: a Review | http://arxiv.org/pdf/1106.6251v2.pdf | author:Mauricio A. Alvarez, Lorenzo Rosasco, Neil D. Lawrence category:stat.ML cs.AI math.ST stat.TH published:2011-06-30 summary:Kernel methods are among the most popular techniques in machine learning.From a frequentist/discriminative perspective they play a central role inregularization theory as they provide a natural choice for the hypotheses spaceand the regularization functional through the notion of reproducing kernelHilbert spaces. From a Bayesian/generative perspective they are the key in thecontext of Gaussian processes, where the kernel function is also known as thecovariance function. Traditionally, kernel methods have been used in supervisedlearning problem with scalar outputs and indeed there has been a considerableamount of work devoted to designing and learning kernels. More recently therehas been an increasing interest in methods that deal with multiple outputs,motivated partly by frameworks like multitask learning. In this paper, wereview different methods to design or learn valid kernel functions for multipleoutputs, paying particular attention to the connection between probabilisticand functional methods.
arxiv-300-154 | Learning XML Twig Queries | http://arxiv.org/pdf/1106.3725v3.pdf | author:Sławomir Staworko, Piotr Wieczorek category:cs.DB cs.LG published:2011-06-19 summary:We investigate the problem of learning XML queries, path queries and treepattern queries, from examples given by the user. A learning algorithm takes onthe input a set of XML documents with nodes annotated by the user and returns aquery that selects the nodes in a manner consistent with the annotation. Westudy two learning settings that differ with the types of annotations. In thefirst setting the user may only indicate required nodes that the query mustreturn. In the second, more general, setting, the user may also indicateforbidden nodes that the query must not return. The query may or may not returnany node with no annotation. We formalize what it means for a class of queriesto be \emph{learnable}. One requirement is the existence of a learningalgorithm that is sound i.e., always returns a query consistent with theexamples given by the user. Furthermore, the learning algorithm should becomplete i.e., able to produce every query with a sufficiently rich example.Other requirements involve tractability of learning and its robustness tononessential examples. We show that the classes of simple path queries andpath-subsumption-free tree queries are learnable from positive examples. Thelearnability of the full class of tree pattern queries (and the full class ofpath queries) remains an open question. We show also that adding negativeexamples to the picture renders the learning unfeasible. Published in ICDT 2012, Berlin.
arxiv-300-155 | Prediction and Modularity in Dynamical Systems | http://arxiv.org/pdf/1106.3703v2.pdf | author:Artemy Kolchinsky, Luis M. Rocha category:nlin.AO cs.AI cs.IT cs.LG cs.SY math.IT q-bio.QM stat.ME G.3 published:2011-06-19 summary:Identifying and understanding modular organizations is centrally important inthe study of complex systems. Several approaches to this problem have beenadvanced, many framed in information-theoretic terms. Our treatment starts fromthe complementary point of view of statistical modeling and prediction ofdynamical systems. It is known that for finite amounts of training data,simpler models can have greater predictive power than more complex ones. We usethe trade-off between model simplicity and predictive accuracy to generateoptimal multiscale decompositions of dynamical networks into weakly-coupled,simple modules. State-dependent and causal versions of our method are alsoproposed.
arxiv-300-156 | ANOVA kernels and RKHS of zero mean functions for model-based sensitivity analysis | http://arxiv.org/pdf/1106.3571v2.pdf | author:Nicolas Durrande, David Ginsbourger, Olivier Roustant, Laurent Carraro category:stat.ML published:2011-06-17 summary:Given a reproducing kernel Hilbert space H of real-valued functions and asuitable measure mu over the source space D (subset of R), we decompose H asthe sum of a subspace of centered functions for mu and its orthogonal in H.This decomposition leads to a special case of ANOVA kernels, for which thefunctional ANOVA representation of the best predictor can be elegantly derived,either in an interpolation or regularization framework. The proposed kernelsappear to be particularly convenient for analyzing the e ffect of each (groupof) variable(s) and computing sensitivity indices without recursivity.
arxiv-300-157 | Nested Graph Words for Object Recognition | http://arxiv.org/pdf/1106.2729v2.pdf | author:Svebor Karaman, Jenny Benois-Pineau, Rémi Mégret category:cs.MM cs.CV published:2011-06-14 summary:In this paper, we propose a new, scalable approach for the task of objectbased image search or object recognition. Despite the very large literatureexisting on the scalability issues in CBIR in the sense of retrievalapproaches, the scalability of media and scalability of features remain anissue. In our work we tackle the problem of scalability and structuralorganization of features. The proposed features are nested local graphs builtupon sets of SURF feature points with Delaunay triangulation. ABag-of-Visual-Words (BoVW) framework is applied on these graphs, giving birthto a Bag-of-Graph-Words representation. The nested nature of the descriptorsconsists in scaling from trivial Delaunay graphs - isolated feature points - byincreasing the number of nodes layer by layer up to graphs with maximal numberof nodes. For each layer of graphs its proper visual dictionary is built. Theexperiments conducted on the SIVAL data set reveal that the graph features atdifferent layers exhibit complementary performances on the same content. Thenested approach, the combination of all existing layers, yields significantimprovement of the object recognition performance compared to single levelapproaches.
arxiv-300-158 | Random design analysis of ridge regression | http://arxiv.org/pdf/1106.2363v2.pdf | author:Daniel Hsu, Sham M. Kakade, Tong Zhang category:math.ST cs.AI cs.LG stat.ML stat.TH published:2011-06-13 summary:This work gives a simultaneous analysis of both the ordinary least squaresestimator and the ridge regression estimator in the random design setting undermild assumptions on the covariate/response distributions. In particular, theanalysis provides sharp results on the ``out-of-sample'' prediction error, asopposed to the ``in-sample'' (fixed design) error. The analysis also revealsthe effect of errors in the estimated covariance structure, as well as theeffect of modeling errors, neither of which effects are present in the fixeddesign setting. The proofs of the main results are based on a simpledecomposition lemma combined with concentration inequalities for random vectorsand matrices.
arxiv-300-159 | Efficient Transductive Online Learning via Randomized Rounding | http://arxiv.org/pdf/1106.2429v4.pdf | author:Nicolò Cesa-Bianchi, Ohad Shamir category:cs.LG stat.ML published:2011-06-13 summary:Most traditional online learning algorithms are based on variants of mirrordescent or follow-the-leader. In this paper, we present an online algorithmbased on a completely different approach, tailored for transductive settings,which combines "random playout" and randomized rounding of loss subgradients.As an application of our approach, we present the first computationallyefficient online algorithm for collaborative filtering with trace-normconstrained matrices. As a second application, we solve an open questionlinking batch learning and transductive online learning
arxiv-300-160 | Lyapunov stochastic stability and control of robust dynamic coalitional games with transferable utilities | http://arxiv.org/pdf/1106.1933v2.pdf | author:Dario Bauso, Puduru Viswanadha Reddy, Tamer Basar category:cs.GT cs.LG cs.SY math.OC published:2011-06-09 summary:This paper considers a dynamic game with transferable utilities (TU), wherethe characteristic function is a continuous-time bounded mean ergodic process.A central planner interacts continuously over time with the players by choosingthe instantaneous allocations subject to budget constraints. Before the gamestarts, the central planner knows the nature of the process (bounded meanergodic), the bounded set from which the coalitions' values are sampled, andthe long run average coalitions' values. On the other hand, he has no knowledgeof the underlying probability function generating the coalitions' values. Ourgoal is to find allocation rules that use a measure of the extra reward that acoalition has received up to the current time by re-distributing the budgetamong the players. The objective is two-fold: i) guaranteeing convergence ofthe average allocations to the core (or a specific point in the core) of theaverage game, ii) driving the coalitions' excesses to an a priori given cone.The resulting allocation rules are robust as they guarantee the aforementionedconvergence properties despite the uncertain and time-varying nature of thecoaltions' values. We highlight three main contributions. First, we design anallocation rule based on full observation of the extra reward so that theaverage allocation approaches a specific point in the core of the average game,while the coalitions' excesses converge to an a priori given direction. Second,we design a new allocation rule based on partial observation on the extrareward so that the average allocation converges to the core of the averagegame, while the coalitions' excesses converge to an a priori given cone. Andthird, we establish connections to approachability theory and attainabilitytheory.
arxiv-300-161 | Learning the Dependence Graph of Time Series with Latent Factors | http://arxiv.org/pdf/1106.1887v4.pdf | author:Ali Jalali, Sujay Sanghavi category:cs.LG published:2011-06-09 summary:This paper considers the problem of learning, from samples, the dependencystructure of a system of linear stochastic differential equations, when some ofthe variables are latent. In particular, we observe the time evolution of somevariables, and never observe other variables; from this, we would like to findthe dependency structure between the observed variables - separating out thespurious interactions caused by the (marginalizing out of the) latentvariables' time series. We develop a new method, based on convex optimization,to do so in the case when the number of latent variables is smaller than thenumber of observed ones. For the case when the dependency structure between theobserved variables is sparse, we theoretically establish a high-dimensionalscaling result for structure recovery. We verify our theoretical result withboth synthetic and real data (from the stock market).
arxiv-300-162 | A Unified Framework for Approximating and Clustering Data | http://arxiv.org/pdf/1106.1379v3.pdf | author:Dan Feldman, Michael Langberg category:cs.LG published:2011-06-07 summary:Given a set $F$ of $n$ positive functions over a ground set $X$, we considerthe problem of computing $x^*$ that minimizes the expression $\sum_{f\inF}f(x)$, over $x\in X$. A typical application is \emph{shape fitting}, where wewish to approximate a set $P$ of $n$ elements (say, points) by a shape $x$ froma (possibly infinite) family $X$ of shapes. Here, each point $p\in P$corresponds to a function $f$ such that $f(x)$ is the distance from $p$ to $x$,and we seek a shape $x$ that minimizes the sum of distances from each point in$P$. In the $k$-clustering variant, each $x\in X$ is a tuple of $k$ shapes, and$f(x)$ is the distance from $p$ to its closest shape in $x$. Our main result is a unified framework for constructing {\em coresets} and{\em approximate clustering} for such general sets of functions. To achieve ourresults, we forge a link between the classic and well defined notion of$\varepsilon$-approximations from the theory of PAC Learning and VC dimension,to the relatively new (and not so consistent) paradigm of coresets, which aresome kind of "compressed representation" of the input set $F$. Usingtraditional techniques, a coreset usually implies an LTAS (linear timeapproximation scheme) for the corresponding optimization problem, which can becomputed in parallel, via one pass over the data, and using onlypolylogarithmic space (i.e, in the streaming model). We show how to generalize the results of our framework for squared distances(as in $k$-mean), distances to the $q$th power, and deterministicconstructions.
arxiv-300-163 | Bayesian and L1 Approaches to Sparse Unsupervised Learning | http://arxiv.org/pdf/1106.1157v3.pdf | author:Shakir Mohamed, Katherine Heller, Zoubin Ghahramani category:cs.LG cs.AI stat.ML published:2011-06-06 summary:The use of L1 regularisation for sparse learning has generated immenseresearch interest, with successful application in such diverse areas as signalacquisition, image coding, genomics and collaborative filtering. While existingwork highlights the many advantages of L1 methods, in this paper we find thatL1 regularisation often dramatically underperforms in terms of predictiveperformance when compared with other methods for inferring sparsity. We focuson unsupervised latent variable models, and develop L1 minimising factormodels, Bayesian variants of "L1", and Bayesian models with a stronger L0-likesparsity induced through spike-and-slab distributions. These spike-and-slabBayesian factor models encourage sparsity while accounting for uncertainty in aprincipled manner and avoiding unnecessary shrinkage of non-zero values. Wedemonstrate on a number of data sets that in practice spike-and-slab Bayesianmethods outperform L1 minimisation, even on a computational budget. We thushighlight the need to re-assess the wide use of L1 methods in sparsity-reliantapplications, particularly when we care about generalising to previously unseendata, and provide an alternative that, over many varying conditions, providesimproved generalisation performance.
arxiv-300-164 | Constructing Runge-Kutta Methods with the Use of Artificial Neural Networks | http://arxiv.org/pdf/1106.1194v2.pdf | author:Angelos A. Anastassi category:cs.NE math.NA 68T05, 65L06 published:2011-06-06 summary:A methodology that can generate the optimal coefficients of a numericalmethod with the use of an artificial neural network is presented in this work.The network can be designed to produce a finite difference algorithm thatsolves a specific system of ordinary differential equations numerically. Thecase we are examining here concerns an explicit two-stage Runge-Kutta methodfor the numerical solution of the two-body problem. Following theimplementation of the network, the latter is trained to obtain the optimalvalues for the coefficients of the Runge-Kutta method. The comparison of thenew method to others that are well known in the literature proves itsefficiency and demonstrates the capability of the network to provide efficientalgorithms for specific problems.
arxiv-300-165 | Recovering Epipolar Geometry from Images of Smooth Surfaces | http://arxiv.org/pdf/1106.0823v4.pdf | author:Oleg Kupervasser category:cs.CV cs.AI 68T45 published:2011-06-04 summary:We present four methods for recovering the epipolar geometry from images ofsmooth surfaces. In the existing methods for recovering epipolar geometrycorresponding feature points are used that cannot be found in such images. Thefirst method is based on finding corresponding characteristic points created byillumination (ICPM - illumination characteristic points' method (PM)). Thesecond method is based on correspondent tangency points created by tangentsfrom epipoles to outline of smooth bodies (OTPM - outline tangent PM). Thesetwo methods are exact and give correct results for real images, becausepositions of the corresponding illumination characteristic points andcorresponding outline are known with small errors. But the second method islimited either to special type of scenes or to restricted camera motion. Wealso consider two more methods which are termed CCPM (curve characteristic PM)and CTPM (curve tangent PM), for searching epipolar geometry for images ofsmooth bodies based on a set of level curves with constant illuminationintensity. The CCPM method is based on searching correspondent points onisophoto curves with the help of correlation of curvatures between these lines.The CTPM method is based on property of the tangential to isophoto curveepipolar line to map into the tangential to correspondent isophoto curvesepipolar line. The standard method (SM) based on knowledge of pairs of thealmost exact correspondent points. The methods have been implemented and testedby SM on pairs of real images. Unfortunately, the last two methods give us onlya finite subset of solutions including "good" solution. Exception is "epipolesin infinity". The main reason is inaccuracy of assumption of constantbrightness for smooth bodies. But outline and illumination characteristicpoints are not influenced by this inaccuracy. So, the first pair of methodsgives exact results.
arxiv-300-166 | PAC learnability under non-atomic measures: a problem by Vidyasagar | http://arxiv.org/pdf/1105.5669v3.pdf | author:Vladimir Pestov category:stat.ML 68T05 I.2.6 published:2011-05-27 summary:In response to a 1997 problem of M. Vidyasagar, we state a criterion for PAClearnability of a concept class $\mathscr C$ under the family of all non-atomic(diffuse) measures on the domain $\Omega$. The uniform Glivenko--Cantelliproperty with respect to non-atomic measures is no longer a necessarycondition, and consistent learnability cannot in general be expected. Ourcriterion is stated in terms of a combinatorial parameter $\VC({\mathscrC}\,{\mathrm{mod}}\,\omega_1)$ which we call the VC dimension of $\mathscr C$modulo countable sets. The new parameter is obtained by "thickening up" singlepoints in the definition of VC dimension to uncountable "clusters".Equivalently, $\VC(\mathscr C\modd\omega_1)\leq d$ if and only if everycountable subclass of $\mathscr C$ has VC dimension $\leq d$ outside acountable subset of $\Omega$. The new parameter can be also expressed as theclassical VC dimension of $\mathscr C$ calculated on a suitable subset of acompactification of $\Omega$. We do not make any measurability assumptions on$\mathscr C$, assuming instead the validity of Martin's Axiom (MA). Similarresults are obtained for function learning in terms of fat-shattering dimensionmodulo countable sets, but, just like in the classical distribution-free case,the finiteness of this parameter is sufficient but not necessary for PAClearnability under non-atomic measures.
arxiv-300-167 | Multidimensional Scaling in the Poincare Disk | http://arxiv.org/pdf/1105.5332v3.pdf | author:Andrej Cvetkovski, Mark Crovella category:stat.ML cs.SI published:2011-05-26 summary:Multidimensional scaling (MDS) is a class of projective algorithmstraditionally used in Euclidean space to produce two- or three-dimensionalvisualizations of datasets of multidimensional points or point distances. Morerecently however, several authors have pointed out that for certain datasets,hyperbolic target space may provide a better fit than Euclidean space. In this paper we develop PD-MDS, a metric MDS algorithm designed specificallyfor the Poincare disk (PD) model of the hyperbolic plane. Emphasizing theimportance of proceeding from first principles in spite of the availability ofvarious black box optimizers, our construction is based on an elementaryhyperbolic line search and reveals numerous particulars that need to becarefully addressed when implementing this as well as more sophisticatediterative optimization methods in a hyperbolic space model.
arxiv-300-168 | Ergodic Mirror Descent | http://arxiv.org/pdf/1105.4681v3.pdf | author:John C. Duchi, Alekh Agarwal, Mikael Johansson, Michael I. Jordan category:math.OC stat.ML published:2011-05-24 summary:We generalize stochastic subgradient descent methods to situations in whichwe do not receive independent samples from the distribution over which weoptimize, but instead receive samples that are coupled over time. We show thatas long as the source of randomness is suitably ergodic---it converges quicklyenough to a stationary distribution---the method enjoys strong convergenceguarantees, both in expectation and with high probability. This result hasimplications for stochastic optimization in high-dimensional spaces,peer-to-peer distributed optimization schemes, decision problems with dependentdata, and stochastic optimization problems over combinatorial spaces.
arxiv-300-169 | Is the Multiverse Hypothesis capable of explaining the Fine Tuning of Nature Laws and Constants? The Case of Cellular Automata | http://arxiv.org/pdf/1105.4278v3.pdf | author:Francisco José Soler Gil, Manuel Alfonseca category:nlin.CG astro-ph.CO cs.NE published:2011-05-21 summary:The objective of this paper is analyzing to which extent the multiversehypothesis provides a real explanation of the peculiarities of the laws andconstants in our universe. First we argue in favor of the thesis that allmultiverses except Tegmark's <<mathematical multiverse>> are too small toexplain the fine tuning, so that they merely shift the problem up one level.But the <<mathematical multiverse>> is surely too large. To prove thisassessment, we have performed a number of experiments with cellular automata ofcomplex behavior, which can be considered as universes in the mathematicalmultiverse. The analogy between what happens in some automata (in particularConway's <<Game of Life>>) and the real world is very strong. But if theresults of our experiments can be extrapolated to our universe, we shouldexpect to inhabit -- in the context of the multiverse -- a world in which atleast some of the laws and constants of nature should show a certain timedependence. Actually, the probability of our existence in a world such as ourswould be mathematically equal to zero. In consequence, the results presented inthis paper can be considered as an inkling that the hypothesis of themultiverse, whatever its type, does not offer an adequate explanation for thepeculiarities of the physical laws in our world. A slightly reduced version ofthis paper has been published in the Journal for General Philosophy of Science,Springer, March 2013, DOI: 10.1007/s10838-013-9215-7.
arxiv-300-170 | An Algorithmic Solution to the Five-Point Pose Problem Based on the Cayley Representation of Rotations | http://arxiv.org/pdf/1105.3828v2.pdf | author:Evgeniy Martyushev category:cs.CV published:2011-05-19 summary:We give a new algorithmic solution to the well-known five-point relative poseproblem. Our approach does not deal with the famous cubic constraint on anessential matrix. Instead, we use the Cayley representation of rotations inorder to obtain a polynomial system from epipolar constraints. Solving thatsystem, we directly get relative rotation and translation parameters of thecameras in terms of roots of a 10th degree polynomial.
arxiv-300-171 | Ant Colony Optimization and Hypergraph Covering Problems | http://arxiv.org/pdf/1105.2894v2.pdf | author:Ankit Pat, Ashish Ranjan Hota category:cs.NE published:2011-05-14 summary:Ant Colony Optimization (ACO) is a very popular metaheuristic for solvingcomputationally hard combinatorial optimization problems. Runtime analysis ofACO with respect to various pseudo-boolean functions and different graph basedcombinatorial optimization problems has been taken up in recent years. In thispaper, we investigate the runtime behavior of an MMAS*(Max-Min Ant System) ACOalgorithm on some well known hypergraph covering problems that are NP-Hard. Inparticular, we have addressed the Minimum Edge Cover problem, the MinimumVertex Cover problem and the Maximum Weak- Independent Set problem. Theinfluence of pheromone values and heuristic information on the running time isanalysed. The results indicate that the heuristic information has greaterimpact towards improving the expected optimization time as compared topheromone values. For certain instances of hypergraphs, we show that the MMAS*algorithm gives a constant order expected optimization time when the dominanceof heuristic information is suitably increased.
arxiv-300-172 | A Risk Comparison of Ordinary Least Squares vs Ridge Regression | http://arxiv.org/pdf/1105.0875v2.pdf | author:Paramveer S. Dhillon, Dean P. Foster, Sham M. Kakade, Lyle H. Ungar category:stat.ML published:2011-05-04 summary:We compare the risk of ridge regression to a simple variant of ordinary leastsquares, in which one simply projects the data onto a finite dimensionalsubspace (as specified by a Principal Component Analysis) and then performs anordinary (un-regularized) least squares regression in this subspace. This noteshows that the risk of this ordinary least squares method is within a constantfactor (namely 4) of the risk of ridge regression.
arxiv-300-173 | SERAPH: Semi-supervised Metric Learning Paradigm with Hyper Sparsity | http://arxiv.org/pdf/1105.0167v3.pdf | author:Gang Niu, Bo Dai, Makoto Yamada, Masashi Sugiyama category:stat.ML cs.AI published:2011-05-01 summary:We propose a general information-theoretic approach called Seraph(SEmi-supervised metRic leArning Paradigm with Hyper-sparsity) for metriclearning that does not rely upon the manifold assumption. Given the probabilityparameterized by a Mahalanobis distance, we maximize the entropy of thatprobability on labeled data and minimize it on unlabeled data following entropyregularization, which allows the supervised and unsupervised parts to beintegrated in a natural and meaningful way. Furthermore, Seraph is regularizedby encouraging a low-rank projection induced from the metric. The optimizationof Seraph is solved efficiently and stably by an EM-like scheme with theanalytical E-Step and convex M-Step. Experiments demonstrate that Seraphcompares favorably with many well-known global and local metric learningmethods.
arxiv-300-174 | Learning high-dimensional directed acyclic graphs with latent and selection variables | http://arxiv.org/pdf/1104.5617v3.pdf | author:Diego Colombo, Marloes H. Maathuis, Markus Kalisch, Thomas S. Richardson category:stat.ME cs.LG math.ST stat.TH published:2011-04-29 summary:We consider the problem of learning causal information between randomvariables in directed acyclic graphs (DAGs) when allowing arbitrarily manylatent and selection variables. The FCI (Fast Causal Inference) algorithm hasbeen explicitly designed to infer conditional independence and causalinformation in such settings. However, FCI is computationally infeasible forlarge graphs. We therefore propose the new RFCI algorithm, which is much fasterthan FCI. In some situations the output of RFCI is slightly less informative,in particular with respect to conditional independence information. However, weprove that any causal information in the output of RFCI is correct in theasymptotic limit. We also define a class of graphs on which the outputs of FCIand RFCI are identical. We prove consistency of FCI and RFCI in sparsehigh-dimensional settings, and demonstrate in simulations that the estimationperformances of the algorithms are very similar. All software is implemented inthe R-package pcalg.
arxiv-300-175 | On Combining Machine Learning with Decision Making | http://arxiv.org/pdf/1104.5061v2.pdf | author:Theja Tulabandhula, Cynthia Rudin category:math.OC cs.LG stat.ML published:2011-04-27 summary:We present a new application and covering number bound for the framework of"Machine Learning with Operational Costs (MLOC)," which is an exploratory formof decision theory. The MLOC framework incorporates knowledge about how apredictive model will be used for a subsequent task, thus combining machinelearning with the decision that is made afterwards. In this work, we use theMLOC framework to study a problem that has implications for power gridreliability and maintenance, called the Machine Learning and TravelingRepairman Problem ML&TRP. The goal of the ML&TRP is to determine a route for a"repair crew," which repairs nodes on a graph. The repair crew aims to minimizethe cost of failures at the nodes, but as in many real situations, the failureprobabilities are not known and must be estimated. The MLOC framework allows usto understand how this uncertainty influences the repair route. We also presentnew covering number generalization bounds for the MLOC framework.
arxiv-300-176 | Fast global convergence of gradient methods for high-dimensional statistical recovery | http://arxiv.org/pdf/1104.4824v3.pdf | author:Alekh Agarwal, Sahand N. Negahban, Martin J. Wainwright category:stat.ML cs.IT math.IT published:2011-04-25 summary:Many statistical $M$-estimators are based on convex optimization problemsformed by the combination of a data-dependent loss function with a norm-basedregularizer. We analyze the convergence rates of projected gradient andcomposite gradient methods for solving such problems, working within ahigh-dimensional framework that allows the data dimension $\pdim$ to grow with(and possibly exceed) the sample size $\numobs$. This high-dimensionalstructure precludes the usual global assumptions---namely, strong convexity andsmoothness conditions---that underlie much of classical optimization analysis.We define appropriately restricted versions of these conditions, and show thatthey are satisfied with high probability for various statistical models. Underthese conditions, our theory guarantees that projected gradient descent has aglobally geometric rate of convergence up to the \emph{statistical precision}of the model, meaning the typical distance between the true unknown parameter$\theta^*$ and an optimal solution $\hat{\theta}$. This result is substantiallysharper than previous convergence results, which yielded sublinear convergence,or linear convergence only up to the noise level. Our analysis applies to awide range of $M$-estimators and statistical models, including sparse linearregression using Lasso ($\ell_1$-regularized regression); group Lasso for blocksparsity; log-linear models with regularization; low-rank matrix recovery usingnuclear norm regularization; and matrix decomposition. Overall, our analysisreveals interesting connections between statistical precision and computationalefficiency in high-dimensional estimation.
arxiv-300-177 | Clustering Partially Observed Graphs via Convex Optimization | http://arxiv.org/pdf/1104.4803v4.pdf | author:Yudong Chen, Ali Jalali, Sujay Sanghavi, Huan Xu category:cs.LG stat.ML published:2011-04-25 summary:This paper considers the problem of clustering a partially observedunweighted graph---i.e., one where for some node pairs we know there is an edgebetween them, for some others we know there is no edge, and for the remainingwe do not know whether or not there is an edge. We want to organize the nodesinto disjoint clusters so that there is relatively dense (observed)connectivity within clusters, and sparse across clusters. We take a novel yet natural approach to this problem, by focusing on findingthe clustering that minimizes the number of "disagreements"---i.e., the sum ofthe number of (observed) missing edges within clusters, and (observed) presentedges across clusters. Our algorithm uses convex optimization; its basis is areduction of disagreement minimization to the problem of recovering an(unknown) low-rank matrix and an (unknown) sparse matrix from their partiallyobserved sum. We evaluate the performance of our algorithm on the classicalPlanted Partition/Stochastic Block Model. Our main theorem provides sufficientconditions for the success of our algorithm as a function of the minimumcluster size, edge density and observation probability; in particular, theresults characterize the tradeoff between the observation probability and theedge density gap. When there are a constant number of clusters of equal size,our results are optimal up to logarithmic factors.
arxiv-300-178 | Positive Semidefinite Metric Learning Using Boosting-like Algorithms | http://arxiv.org/pdf/1104.4704v2.pdf | author:Chunhua Shen, Junae Kim, Lei Wang, Anton van den Hengel category:cs.CV published:2011-04-25 summary:The success of many machine learning and pattern recognition methods reliesheavily upon the identification of an appropriate distance metric on the inputdata. It is often beneficial to learn such a metric from the input trainingdata, instead of using a default one such as the Euclidean distance. In thiswork, we propose a boosting-based technique, termed BoostMetric, for learning aquadratic Mahalanobis distance metric. Learning a valid Mahalanobis distancemetric requires enforcing the constraint that the matrix parameter to themetric remains positive definite. Semidefinite programming is often used toenforce this constraint, but does not scale well and easy to implement.BoostMetric is instead based on the observation that any positive semidefinitematrix can be decomposed into a linear combination of trace-one rank-onematrices. BoostMetric thus uses rank-one positive semidefinite matrices as weaklearners within an efficient and scalable boosting-based learning process. Theresulting methods are easy to implement, efficient, and can accommodate varioustypes of constraints. We extend traditional boosting algorithms in that itsweak learner is a positive semidefinite matrix with trace and rank being onerather than a classifier or regressor. Experiments on various datasetsdemonstrate that the proposed algorithms compare favorably to thosestate-of-the-art methods in terms of classification accuracy and running time.
arxiv-300-179 | Scaled Sparse Linear Regression | http://arxiv.org/pdf/1104.4595v2.pdf | author:Tingni Sun, Cun-Hui Zhang category:stat.ML math.ST stat.TH published:2011-04-24 summary:Scaled sparse linear regression jointly estimates the regression coefficientsand noise level in a linear model. It chooses an equilibrium with a sparseregression method by iteratively estimating the noise level via the meanresidual square and scaling the penalty in proportion to the estimated noiselevel. The iterative algorithm costs little beyond the computation of a path orgrid of the sparse regression estimator for penalty levels above a properthreshold. For the scaled lasso, the algorithm is a gradient descent in aconvex minimization of a penalized joint loss function for the regressioncoefficients and noise level. Under mild regularity conditions, we prove thatthe scaled lasso simultaneously yields an estimator for the noise level and anestimated coefficient vector satisfying certain oracle inequalities forprediction, the estimation of the noise level and the regression coefficients.These inequalities provide sufficient conditions for the consistency andasymptotic normality of the noise level estimator, including certain caseswhere the number of variables is of greater order than the sample size.Parallel results are provided for the least squares estimation after modelselection by the scaled lasso. Numerical results demonstrate the superiorperformance of the proposed methods over an earlier proposal of joint convexminimization.
arxiv-300-180 | Curved Gabor Filters for Fingerprint Image Enhancement | http://arxiv.org/pdf/1104.4298v2.pdf | author:Carsten Gottschlich category:cs.CV published:2011-04-21 summary:Gabor filters play an important role in many application areas for theenhancement of various types of images and the extraction of Gabor features.For the purpose of enhancing curved structures in noisy images, we introducecurved Gabor filters which locally adapt their shape to the direction of flow.These curved Gabor filters enable the choice of filter parameters whichincrease the smoothing power without creating artifacts in the enhanced image.In this paper, curved Gabor filters are applied to the curved ridge and valleystructure of low-quality fingerprint images. First, we combine two orientationfield estimation methods in order to obtain a more robust estimation for verynoisy images. Next, curved regions are constructed by following the respectivelocal orientation and they are used for estimating the local ridge frequency.Lastly, curved Gabor filters are defined based on curved regions and they areapplied for the enhancement of low-quality fingerprint images. Experimentalresults on the FVC2004 databases show improvements of this approach incomparison to state-of-the-art enhancement methods.
arxiv-300-181 | Emergent Criticality Through Adaptive Information Processing in Boolean Networks | http://arxiv.org/pdf/1104.4141v2.pdf | author:Alireza Goudarzi, Christof Teuscher, Natali Gulbahce, Thimo Rohlf category:cs.NE nlin.AO published:2011-04-20 summary:We study information processing in populations of Boolean networks withevolving connectivity and systematically explore the interplay between thelearning capability, robustness, the network topology, and the task complexity.We solve a long-standing open question and find computationally that, for largesystem sizes $N$, adaptive information processing drives the networks to acritical connectivity $K_{c}=2$. For finite size networks, the connectivityapproaches the critical value with a power-law of the system size $N$. We showthat network learning and generalization are optimized near criticality, giventask complexity and the amount of information provided threshold values. Bothrandom and evolved networks exhibit maximal topological diversity near $K_{c}$.We hypothesize that this supports efficient exploration and robustness ofsolutions. Also reflected in our observation is that the variance of the valuesis maximal in critical network populations. Finally, we discuss implications ofour results for determining the optimal topology of adaptive dynamical networksthat solve computational tasks.
arxiv-300-182 | Distance Transform Gradient Density Estimation using the Stationary Phase Approximation | http://arxiv.org/pdf/1104.3621v4.pdf | author:Karthik S. Gurumoorthy, Anand Rangarajan category:stat.ML math.PR 42B10, 41A60 published:2011-04-19 summary:The complex wave representation (CWR) converts unsigned 2D distancetransforms into their corresponding wave functions. Here, the distancetransform S(X) appears as the phase of the wave function\phi(X)---specifically, \phi(X)=exp(iS(X)/\tau where \tau is a free parameter.In this work, we prove a novel result using the higher-order stationary phaseapproximation: we show convergence of the normalized power spectrum (squaredmagnitude of the Fourier transform) of the wave function to the densityfunction of the distance transform gradients as the free parameter \tau-->0. Incolloquial terms, spatial frequencies are gradient histogram bins. Since thedistance transform gradients have only orientation information (as theirmagnitudes are identically equal to one almost everywhere), as \tau-->0, the 2DFourier transform values mainly lie on the unit circle in the spatial frequencydomain. The proof of the result involves standard integration techniques andrequires proper ordering of limits. Our mathematical relation indicates thatthe CWR of distance transforms is an intriguing, new representation.
arxiv-300-183 | Cluster Forests | http://arxiv.org/pdf/1104.2930v3.pdf | author:Donghui Yan, Aiyou Chen, Michael I. Jordan category:stat.ME cs.LG stat.ML published:2011-04-14 summary:With inspiration from Random Forests (RF) in the context of classification, anew clustering ensemble method---Cluster Forests (CF) is proposed.Geometrically, CF randomly probes a high-dimensional data cloud to obtain "goodlocal clusterings" and then aggregates via spectral clustering to obtaincluster assignments for the whole dataset. The search for good localclusterings is guided by a cluster quality measure kappa. CF progressivelyimproves each local clustering in a fashion that resembles the tree growth inRF. Empirical studies on several real-world datasets under two differentperformance metrics show that CF compares favorably to its competitors.Theoretical analysis reveals that the kappa measure makes it possible to growthe local clustering in a desirable way---it is "noise-resistant". Aclosed-form expression is obtained for the mis-clustering rate of spectralclustering under a perturbation model, which yields new insights into someaspects of spectral clustering.
arxiv-300-184 | Hybrid Deterministic-Stochastic Methods for Data Fitting | http://arxiv.org/pdf/1104.2373v4.pdf | author:Michael P. Friedlander, Mark Schmidt category:cs.NA cs.SY math.OC stat.ML published:2011-04-13 summary:Many structured data-fitting applications require the solution of anoptimization problem involving a sum over a potentially large number ofmeasurements. Incremental gradient algorithms offer inexpensive iterations bysampling a subset of the terms in the sum. These methods can make greatprogress initially, but often slow as they approach a solution. In contrast,full-gradient methods achieve steady convergence at the expense of evaluatingthe full objective and gradient on each iteration. We explore hybrid methodsthat exhibit the benefits of both approaches. Rate-of-convergence analysisshows that by controlling the sample size in an incremental gradient algorithm,it is possible to maintain the steady convergence rates of full-gradientmethods. We detail a practical quasi-Newton implementation based on thisapproach. Numerical experiments illustrate its potential benefits.
arxiv-300-185 | Adaptive Evolutionary Clustering | http://arxiv.org/pdf/1104.1990v3.pdf | author:Kevin S. Xu, Mark Kliger, Alfred O. Hero III category:cs.LG stat.ML published:2011-04-11 summary:In many practical applications of clustering, the objects to be clusteredevolve over time, and a clustering result is desired at each time step. In suchapplications, evolutionary clustering typically outperforms traditional staticclustering by producing clustering results that reflect long-term trends whilebeing robust to short-term variations. Several evolutionary clusteringalgorithms have recently been proposed, often by adding a temporal smoothnesspenalty to the cost function of a static clustering method. In this paper, weintroduce a different approach to evolutionary clustering by accuratelytracking the time-varying proximities between objects followed by staticclustering. We present an evolutionary clustering framework that adaptivelyestimates the optimal smoothing parameter using shrinkage estimation, astatistical approach that improves a naive estimate using additionalinformation. The proposed framework can be used to extend a variety of staticclustering algorithms, including hierarchical, k-means, and spectralclustering, into evolutionary clustering algorithms. Experiments on syntheticand real data sets indicate that the proposed framework outperforms staticclustering and existing evolutionary clustering algorithms in many scenarios.
arxiv-300-186 | Generalized double Pareto shrinkage | http://arxiv.org/pdf/1104.0861v4.pdf | author:Artin Armagan, David Dunson, Jaeyong Lee category:stat.ME math.ST stat.ML stat.TH published:2011-04-05 summary:We propose a generalized double Pareto prior for Bayesian shrinkageestimation and inferences in linear models. The prior can be obtained via ascale mixture of Laplace or normal distributions, forming a bridge between theLaplace and Normal-Jeffreys' priors. While it has a spike at zero like theLaplace density, it also has a Student's $t$-like tail behavior. Bayesiancomputation is straightforward via a simple Gibbs sampling algorithm. Weinvestigate the properties of the maximum a posteriori estimator, as sparseestimation plays an important role in many problems, reveal connections withsome well-established regularization procedures, and show some asymptoticresults. The performance of the prior is tested through simulations and anapplication.
arxiv-300-187 | On Identifying Significant Edges in Graphical Models of Molecular Networks | http://arxiv.org/pdf/1104.0896v5.pdf | author:Marco Scutari, Radhakrishnan Nagarajan category:stat.ML stat.ME published:2011-04-05 summary:Objective: Modelling the associations from high-throughput experimentalmolecular data has provided unprecedented insights into biological pathways andsignalling mechanisms. Graphical models and networks have especially proven tobe useful abstractions in this regard. Ad-hoc thresholds are often used inconjunction with structure learning algorithms to determine significantassociations. The present study overcomes this limitation by proposing astatistically-motivated approach for identifying significant associations in anetwork. Methods and Materials: A new method that identifies significant associationsin graphical models by estimating the threshold minimising the $L_{\mathrm{1}}$norm between the cumulative distribution function (CDF) of the observed edgeconfidences and those of its asymptotic counterpart is proposed. Theeffectiveness of the proposed method is demonstrated on popular synthetic datasets as well as publicly available experimental molecular data corresponding togene and protein expression profiles. Results: The improved performance of the proposed approach is demonstratedacross the synthetic data sets using sensitivity, specificity and accuracy asperformance metrics. The results are also demonstrated across varying samplesizes and three different structure learning algorithms with widely varyingassumptions. In all cases, the proposed approach has specificity and accuracyclose to 1, while sensitivity increases linearly in the logarithm of the samplesize. The estimated threshold systematically outperforms common ad-hoc ones interms of sensitivity while maintaining comparable levels of specificity andaccuracy. Networks from experimental data sets are reconstructed accuratelywith respect to the results from the original papers.
arxiv-300-188 | Block-Sparse Recovery via Convex Optimization | http://arxiv.org/pdf/1104.0654v3.pdf | author:Ehsan Elhamifar, Rene Vidal category:math.OC cs.CV cs.IT math.IT published:2011-04-04 summary:Given a dictionary that consists of multiple blocks and a signal that livesin the range space of only a few blocks, we study the problem of finding ablock-sparse representation of the signal, i.e., a representation that uses theminimum number of blocks. Motivated by signal/image processing and computervision applications, such as face recognition, we consider the block-sparserecovery problem in the case where the number of atoms in each block isarbitrary, possibly much larger than the dimension of the underlying subspace.To find a block-sparse representation of a signal, we propose two classes ofnon-convex optimization programs, which aim to minimize the number of nonzerocoefficient blocks and the number of nonzero reconstructed vectors from theblocks, respectively. Since both classes of problems are NP-hard, we proposeconvex relaxations and derive conditions under which each class of the convexprograms is equivalent to the original non-convex formulation. Our conditionsdepend on the notions of mutual and cumulative subspace coherence of adictionary, which are natural generalizations of existing notions of mutual andcumulative coherence. We evaluate the performance of the proposed convexprograms through simulations as well as real experiments on face recognition.We show that treating the face recognition problem as a block-sparse recoveryproblem improves the state-of-the-art results by 10% with only 25% of thetraining data.
arxiv-300-189 | Visualization techniques for data mining of Latur district satellite imagery | http://arxiv.org/pdf/1104.3571v2.pdf | author:B. G. Kodge, P. S. Hiremath category:cs.CE cs.CV published:2011-03-28 summary:This study presents a new visualization tool for classification of satelliteimagery. Visualization of feature space allows exploration of patterns in theimage data and insight into the classification process and related uncertainty.Visual Data Mining provides added value to image classifications as the usercan be involved in the classification process providing increased confidence inand understanding of the results. In this study, we present a prototypevisualization tool for visual data mining (VDM) of satellite imagery. Thevisualization tool is showcased in a classification study of highresolutionimageries of Latur district in Maharashtra state of India.
arxiv-300-190 | The Discrete Infinite Logistic Normal Distribution | http://arxiv.org/pdf/1103.4789v3.pdf | author:John Paisley, Chong Wang, David Blei category:stat.ML published:2011-03-24 summary:We present the discrete infinite logistic normal distribution (DILN), aBayesian nonparametric prior for mixed membership models. DILN is ageneralization of the hierarchical Dirichlet process (HDP) that modelscorrelation structure between the weights of the atoms at the group level. Wederive a representation of DILN as a normalized collection of gamma-distributedrandom variables, and study its statistical properties. We considerapplications to topic modeling and derive a variational inference algorithm forapproximate posterior inference. We study the empirical performance of the DILNtopic model on four corpora, comparing performance with the HDP and thecorrelated topic model (CTM). To deal with large-scale data sets, we alsodevelop an online inference algorithm for DILN and compare with online HDP andonline LDA on the Nature magazine, which contains approximately 350,000articles.
arxiv-300-191 | Randomized Smoothing for Stochastic Optimization | http://arxiv.org/pdf/1103.4296v2.pdf | author:John C. Duchi, Peter L. Bartlett, Martin J. Wainwright category:math.OC stat.ML published:2011-03-22 summary:We analyze convergence rates of stochastic optimization procedures fornon-smooth convex optimization problems. By combining randomized smoothingtechniques with accelerated gradient methods, we obtain convergence rates ofstochastic optimization procedures, both in expectation and with highprobability, that have optimal dependence on the variance of the gradientestimates. To the best of our knowledge, these are the first variance-basedrates for non-smooth optimization. We give several applications of our resultsto statistical estimation problems, and provide experimental results thatdemonstrate the effectiveness of the proposed algorithms. We also describe howa combination of our algorithm with recent work on decentralized optimizationyields a distributed stochastic optimization algorithm that is order-optimal.
arxiv-300-192 | Localization from Incomplete Noisy Distance Measurements | http://arxiv.org/pdf/1103.1417v4.pdf | author:Adel Javanmard, Andrea Montanari category:math.ST cs.LG cs.SY math.OC math.PR stat.TH published:2011-03-08 summary:We consider the problem of positioning a cloud of points in the Euclideanspace $\mathbb{R}^d$, using noisy measurements of a subset of pairwisedistances. This task has applications in various areas, such as sensor networklocalization and reconstruction of protein conformations from NMR measurements.Also, it is closely related to dimensionality reduction problems and manifoldlearning, where the goal is to learn the underlying global geometry of a dataset using local (or partial) metric information. Here we propose areconstruction algorithm based on semidefinite programming. For a randomgeometric graph model and uniformly bounded noise, we provide a precisecharacterization of the algorithm's performance: In the noiseless case, we finda radius $r_0$ beyond which the algorithm reconstructs the exact positions (upto rigid transformations). In the presence of noise, we obtain upper and lowerbounds on the reconstruction error that match up to a factor that depends onlyon the dimension $d$, and the average degree of the nodes in the graph.
arxiv-300-193 | A Feature Selection Method for Multivariate Performance Measures | http://arxiv.org/pdf/1103.1013v2.pdf | author:Qi Mao, Ivor W. Tsang category:cs.LG published:2011-03-05 summary:Feature selection with specific multivariate performance measures is the keyto the success of many applications, such as image retrieval and textclassification. The existing feature selection methods are usually designed forclassification error. In this paper, we propose a generalized sparseregularizer. Based on the proposed regularizer, we present a unified featureselection framework for general loss functions. In particular, we study thenovel feature selection paradigm by optimizing multivariate performancemeasures. The resultant formulation is a challenging problem forhigh-dimensional data. Hence, a two-layer cutting plane algorithm is proposedto solve this problem, and the convergence is presented. In addition, we adaptthe proposed method to optimize multivariate measures for multiple instancelearning problems. The analyses by comparing with the state-of-the-art featureselection methods show that the proposed method is superior to others.Extensive experiments on large-scale and high-dimensional real world datasetsshow that the proposed method outperforms $l_1$-SVM and SVM-RFE when choosing asmall subset of features, and achieves significantly improved performances overSVM$^{perf}$ in terms of $F_1$-score.
arxiv-300-194 | Efficient Multi-Template Learning for Structured Prediction | http://arxiv.org/pdf/1103.0890v2.pdf | author:Qi Mao, Ivor W. Tsang category:cs.LG cs.CL published:2011-03-04 summary:Conditional random field (CRF) and Structural Support Vector Machine(Structural SVM) are two state-of-the-art methods for structured predictionwhich captures the interdependencies among output variables. The success ofthese methods is attributed to the fact that their discriminative models areable to account for overlapping features on the whole input observations. Thesefeatures are usually generated by applying a given set of templates on labeleddata, but improper templates may lead to degraded performance. To alleviatethis issue, in this paper, we propose a novel multiple template learningparadigm to learn structured prediction and the importance of each templatesimultaneously, so that hundreds of arbitrary templates could be added into thelearning model without caution. This paradigm can be formulated as a specialmultiple kernel learning problem with exponential number of constraints. Thenwe introduce an efficient cutting plane algorithm to solve this problem in theprimal, and its convergence is presented. We also evaluate the proposedlearning paradigm on two widely-studied structured prediction tasks,\emph{i.e.} sequence labeling and dependency parsing. Extensive experimentalresults show that the proposed method outperforms CRFs and Structural SVMs dueto exploiting the importance of each template. Our complexity analysis andempirical results also show that our proposed method is more efficient thanOnlineMKL on very sparse and high-dimensional data. We further extend thisparadigm for structured prediction using generalized $p$-block normregularization with $p>1$, and experiments show competitive performances when$p \in [1,2)$.
arxiv-300-195 | Multiple Kernel Learning: A Unifying Probabilistic Viewpoint | http://arxiv.org/pdf/1103.0897v3.pdf | author:Hannes Nickisch, Matthias Seeger category:stat.ML published:2011-03-04 summary:We present a probabilistic viewpoint to multiple kernel learning unifyingwell-known regularised risk approaches and recent advances in approximateBayesian inference relaxations. The framework proposes a general objectivefunction suitable for regression, robust regression and classification that islower bound of the marginal likelihood and contains many regularised riskapproaches as special cases. Furthermore, we derive an efficient and provablyconvergent optimisation algorithm.
arxiv-300-196 | Semi-supervised logistic discrimination for functional data | http://arxiv.org/pdf/1102.4399v3.pdf | author:Shuichi Kawano, Sadanori Konishi category:stat.ME stat.ML published:2011-02-22 summary:Multi-class classification methods based on both labeled and unlabeledfunctional data sets are discussed. We present a semi-supervised logistic modelfor classification in the context of functional data analysis. Unknownparameters in our proposed model are estimated by regularization with the helpof EM algorithm. A crucial point in the modeling procedure is the choice of aregularization parameter involved in the semi-supervised functional logisticmodel. In order to select the adjusted parameter, we introduce model selectioncriteria from information-theoretic and Bayesian viewpoints. Monte Carlosimulations and a real data analysis are given to examine the effectiveness ofour proposed modeling strategy.
arxiv-300-197 | Joint and individual variation explained (JIVE) for integrated analysis of multiple data types | http://arxiv.org/pdf/1102.4110v2.pdf | author:Eric F. Lock, Katherine A. Hoadley, J. S. Marron, Andrew B. Nobel category:stat.ML stat.AP stat.ME published:2011-02-20 summary:Research in several fields now requires the analysis of data sets in whichmultiple high-dimensional types of data are available for a common set ofobjects. In particular, The Cancer Genome Atlas (TCGA) includes data fromseveral diverse genomic technologies on the same cancerous tumor samples. Inthis paper we introduce Joint and Individual Variation Explained (JIVE), ageneral decomposition of variation for the integrated analysis of such datasets. The decomposition consists of three terms: a low-rank approximationcapturing joint variation across data types, low-rank approximations forstructured variation individual to each data type, and residual noise. JIVEquantifies the amount of joint variation between data types, reduces thedimensionality of the data and provides new directions for the visualexploration of joint and individual structures. The proposed method representsan extension of Principal Component Analysis and has clear advantages overpopular two-block methods such as Canonical Correlation Analysis and PartialLeast Squares. A JIVE analysis of gene expression and miRNA data onGlioblastoma Multiforme tumor samples reveals gene-miRNA associations andprovides better characterization of tumor types. Data and software areavailable at https://genome.unc.edu/jive/
arxiv-300-198 | Transductive Ordinal Regression | http://arxiv.org/pdf/1102.2808v5.pdf | author:Chun-Wei Seah, Ivor W. Tsang, Yew-Soon Ong category:cs.LG published:2011-02-14 summary:Ordinal regression is commonly formulated as a multi-class problem withordinal constraints. The challenge of designing accurate classifiers forordinal regression generally increases with the number of classes involved, dueto the large number of labeled patterns that are needed. The availability ofordinal class labels, however, is often costly to calibrate or difficult toobtain. Unlabeled patterns, on the other hand, often exist in much greaterabundance and are freely available. To take benefits from the abundance ofunlabeled patterns, we present a novel transductive learning paradigm forordinal regression in this paper, namely Transductive Ordinal Regression (TOR).The key challenge of the present study lies in the precise estimation of boththe ordinal class label of the unlabeled data and the decision functions of theordinal classes, simultaneously. The core elements of the proposed TOR includean objective function that caters to several commonly used loss functionscasted in transductive settings, for general ordinal regression. A labelswapping scheme that facilitates a strictly monotonic decrease in the objectivefunction value is also introduced. Extensive numerical studies on commonly usedbenchmark datasets including the real world sentiment prediction problem arethen presented to showcase the characteristics and efficacies of the proposedtransductive ordinal regression. Further, comparisons to recentstate-of-the-art ordinal regression methods demonstrate the introducedtransductive learning paradigm for ordinal regression led to the robust andimproved performance.
arxiv-300-199 | The KL-UCB Algorithm for Bounded Stochastic Bandits and Beyond | http://arxiv.org/pdf/1102.2490v5.pdf | author:Aurélien Garivier, Olivier Cappé category:math.ST cs.LG cs.SY math.OC stat.TH 93E35 published:2011-02-12 summary:This paper presents a finite-time analysis of the KL-UCB algorithm, anonline, horizon-free index policy for stochastic bandit problems. We prove twodistinct results: first, for arbitrary bounded rewards, the KL-UCB algorithmsatisfies a uniformly better regret bound than UCB or UCB2; second, in thespecial case of Bernoulli rewards, it reaches the lower bound of Lai andRobbins. Furthermore, we show that simple adaptations of the KL-UCB algorithmare also optimal for specific classes of (possibly unbounded) rewards,including those generated from exponential families of distributions. Alarge-scale numerical study comparing KL-UCB with its main competitors (UCB,UCB2, UCB-Tuned, UCB-V, DMED) shows that KL-UCB is remarkably efficient andstable, including for short time horizons. KL-UCB is also the only method thatalways performs better than the basic UCB policy. Our regret bounds rely ondeviations results of independent interest which are stated and proved in theAppendix. As a by-product, we also obtain an improved regret bound for thestandard UCB algorithm.
arxiv-300-200 | Matrix completion with column manipulation: Near-optimal sample-robustness-rank tradeoffs | http://arxiv.org/pdf/1102.2254v2.pdf | author:Yudong Chen, Huan Xu, Constantine Caramanis, Sujay Sanghavi category:stat.ML cs.IT math.IT published:2011-02-10 summary:This paper considers the problem of matrix completion when some number of thecolumns are completely and arbitrarily corrupted, potentially by a maliciousadversary. It is well-known that standard algorithms for matrix completion canreturn arbitrarily poor results, if even a single column is corrupted. Onedirect application comes from robust collaborative filtering. Here, some numberof users are so-called manipulators who try to skew the predictions of thealgorithm by calibrating their inputs to the system. In this paper, we developan efficient algorithm for this problem based on a combination of a trimmingprocedure and a convex program that minimizes the nuclear norm and the$\ell_{1,2}$ norm. Our theoretical results show that given a vanishing fractionof observed entries, it is nevertheless possible to complete the underlyingmatrix even when the number of corrupted columns grows. Significantly, ourresults hold without any assumptions on the locations or values of the observedentries of the manipulated columns. Moreover, we show by aninformation-theoretic argument that our guarantees are nearly optimal in termsof the fraction of sampled entries on the authentic columns, the fraction ofcorrupted columns, and the rank of the underlying matrix. Our results thereforesharply characterize the tradeoffs between sample, robustness and rank inmatrix completion.
arxiv-300-201 | An Introduction to Artificial Prediction Markets for Classification | http://arxiv.org/pdf/1102.1465v6.pdf | author:Adrian Barbu, Nathan Lay category:stat.ML cs.LG math.ST stat.TH published:2011-02-07 summary:Prediction markets are used in real life to predict outcomes of interest suchas presidential elections. This paper presents a mathematical theory ofartificial prediction markets for supervised learning of conditionalprobability estimators. The artificial prediction market is a novel method forfusing the prediction information of features or trained classifiers, where thefusion result is the contract price on the possible outcomes. The market can betrained online by updating the participants' budgets using training examples.Inspired by the real prediction markets, the equations that govern the marketare derived from simple and reasonable assumptions. Efficient numericalalgorithms are presented for solving these equations. The obtained artificialprediction market is shown to be a maximum likelihood estimator. It generalizeslinear aggregation, existent in boosting and random forest, as well as logisticregression and some kernel methods. Furthermore, the market mechanism allowsthe aggregation of specialized classifiers that participate only on specificinstances. Experimental comparisons show that the artificial prediction marketsoften outperform random forest and implicit online learning on synthetic dataand real UCI datasets. Moreover, an extensive evaluation for pelvic andabdominal lymph node detection in CT data shows that the prediction marketimproves adaboost's detection rate from 79.6% to 81.2% at 3 falsepositives/volume.
arxiv-300-202 | Ranking-Based Black-Box Complexity | http://arxiv.org/pdf/1102.1140v3.pdf | author:Benjamin Doerr, Carola Winzen category:cs.NE cs.CC cs.DS published:2011-02-06 summary:Randomized search heuristics such as evolutionary algorithms, simulatedannealing, and ant colony optimization are a broadly used class ofgeneral-purpose algorithms. Analyzing them via classical methods of theoreticalcomputer science is a growing field. While several strong runtime analysisresults have appeared in the last 20 years, a powerful complexity theory forsuch algorithms is yet to be developed. We enrich the existing notions ofblack-box complexity by the additional restriction that not the actualobjective values, but only the relative quality of the previously evaluatedsolutions may be taken into account by the black-box algorithm. Many randomizedsearch heuristics belong to this class of algorithms. We show that the new ranking-based model gives more realistic complexityestimates for some problems. For example, the class of all binary-valuefunctions has a black-box complexity of $O(\log n)$ in the previous black-boxmodels, but has a ranking-based complexity of $\Theta(n)$. For the class of all OneMax functions, we present a ranking-based black-boxalgorithm that has a runtime of $\Theta(n / \log n)$, which shows that theOneMax problem does not become harder with the additional ranking-basednessrestriction.
arxiv-300-203 | Statistical methods for tissue array images - algorithmic scoring and co-training | http://arxiv.org/pdf/1102.0059v2.pdf | author:Donghui Yan, Pei Wang, Michael Linden, Beatrice Knudsen, Timothy Randolph category:stat.ME cs.CE cs.CV cs.LG q-bio.QM published:2011-02-01 summary:Recent advances in tissue microarray technology have allowedimmunohistochemistry to become a powerful medium-to-high throughput analysistool, particularly for the validation of diagnostic and prognostic biomarkers.However, as study size grows, the manual evaluation of these assays becomes aprohibitive limitation; it vastly reduces throughput and greatly increasesvariability and expense. We propose an algorithm - Tissue Array Co-OccurrenceMatrix Analysis (TACOMA) - for quantifying cellular phenotypes based ontextural regularity summarized by local inter-pixel relationships. Thealgorithm can be easily trained for any staining pattern, is absent ofsensitive tuning parameters and has the ability to report salient pixels in animage that contribute to its score. Pathologists' input via informativetraining patches is an important aspect of the algorithm that allows thetraining for any specific marker or cell type. With co-training, the error rateof TACOMA can be reduced substantially for a very small training sample (e.g.,with size 30). We give theoretical insights into the success of co-training viathinning of the feature set in a high-dimensional setting when there is"sufficient" redundancy among the features. TACOMA is flexible, transparent andprovides a scoring process that can be evaluated with clarity and confidence.In a study based on an estrogen receptor (ER) marker, we show that TACOMA iscomparable to, or outperforms, pathologists' performance in terms of accuracyand repeatability.
arxiv-300-204 | Bayesian Network Structure Learning with Permutation Tests | http://arxiv.org/pdf/1101.5184v3.pdf | author:Marco Scutari, Adriana Brogini category:stat.ML stat.ME published:2011-01-27 summary:In literature there are several studies on the performance of Bayesiannetwork structure learning algorithms. The focus of these studies is almostalways the heuristics the learning algorithms are based on, i.e. themaximisation algorithms (in score-based algorithms) or the techniques forlearning the dependencies of each variable (in constraint-based algorithms). Inthis paper we investigate how the use of permutation tests instead ofparametric ones affects the performance of Bayesian network structure learningfrom discrete data. Shrinkage tests are also covered to provide a broadoverview of the techniques developed in current literature.
arxiv-300-205 | Geometric representations for minimalist grammars | http://arxiv.org/pdf/1101.5076v6.pdf | author:Peter beim Graben, Sabrina Gerth category:cs.CL published:2011-01-26 summary:We reformulate minimalist grammars as partial functions on term algebras forstrings and trees. Using filler/role bindings and tensor productrepresentations, we construct homomorphisms for these data structures intogeometric vector spaces. We prove that the structure-building functions as wellas simple processors for minimalist languages can be realized by piecewiselinear operators in representation space. We also propose harmony, i.e. thedistance of an intermediate processing step from the final well-formed state inrepresentation space, as a measure of processing complexity. Finally, weillustrate our findings by means of two particular arithmetic and fractalrepresentations.
arxiv-300-206 | Close the Gaps: A Learning-while-Doing Algorithm for a Class of Single-Product Revenue Management Problems | http://arxiv.org/pdf/1101.4681v6.pdf | author:Zizhuo Wang, Shiming Deng, Yinyu Ye category:cs.LG 93E35 published:2011-01-24 summary:We consider a retailer selling a single product with limited on-handinventory over a finite selling season. Customer demand arrives according to aPoisson process, the rate of which is influenced by a single action taken bythe retailer (such as price adjustment, sales commission, advertisementintensity, etc.). The relationship between the action and the demand rate isnot known in advance. However, the retailer is able to learn the optimal action"on the fly" as she maximizes her total expected revenue based on the observeddemand reactions. Using the pricing problem as an example, we propose a dynamic"learning-while-doing" algorithm that only involves function value estimationto achieve a near-optimal performance. Our algorithm employs a series ofshrinking price intervals and iteratively tests prices within that intervalusing a set of carefully chosen parameters. We prove that the convergence rateof our algorithm is among the fastest of all possible algorithms in terms ofasymptotic "regret" (the relative loss comparing to the full informationoptimal solution). Our result closes the performance gaps between parametricand non-parametric learning and between a post-price mechanism and acustomer-bidding mechanism. Important managerial insight from this research isthat the values of information on both the parametric form of the demandfunction as well as each customer's exact reservation price are less importantthan prior literature suggests. Our results also suggest that firms would bebetter off to perform dynamic learning and action concurrently rather thansequentially.
arxiv-300-207 | Transductive-Inductive Cluster Approximation Via Multivariate Chebyshev Inequality | http://arxiv.org/pdf/1101.3755v2.pdf | author:Shriprakash Sinha category:cs.CV cs.AI published:2011-01-19 summary:Approximating adequate number of clusters in multidimensional data is an openarea of research, given a level of compromise made on the quality of acceptableresults. The manuscript addresses the issue by formulating a transductiveinductive learning algorithm which uses multivariate Chebyshev inequality.Considering clustering problem in imaging, theoretical proofs for a particularlevel of compromise are derived to show the convergence of the reconstructionerror to a finite value with increasing (a) number of unseen examples and (b)the number of clusters, respectively. Upper bounds for these error rates arealso proved. Non-parametric estimates of these error from a random sample ofsequences empirically point to a stable number of clusters. Lastly, thegeneralization of algorithm can be applied to multidimensional data sets fromdifferent fields.
arxiv-300-208 | Generic identification of binary-valued hidden Markov processes | http://arxiv.org/pdf/1101.3712v6.pdf | author:Alexander Schönhuth category:math.ST math.AG stat.ML stat.TH published:2011-01-19 summary:The generic identification problem is to decide whether a stochastic process$(X_t)$ is a hidden Markov process and if yes to infer its parameters for allbut a subset of parametrizations that form a lower-dimensional subvariety inparameter space. Partial answers so far available depend on extra assumptionson the processes, which are usually centered around stationarity. Here wepresent a general solution for binary-valued hidden Markov processes. Ourapproach is rooted in algebraic statistics hence it is geometric in nature. Wefind that the algebraic varieties associated with the probability distributionsof binary-valued hidden Markov processes are zero sets of determinantalequations which draws a connection to well-studied objects from algebra. As aconsequence, our solution allows for algorithmic implementation based onelementary (linear) algebraic routines.
arxiv-300-209 | Group Invariant Scattering | http://arxiv.org/pdf/1101.2286v3.pdf | author:Stéphane Mallat category:math.FA cs.CV published:2011-01-12 summary:This paper constructs translation invariant operators on L2(R^d), which areLipschitz continuous to the action of diffeomorphisms. A scattering propagatoris a path ordered product of non-linear and non-commuting operators, each ofwhich computes the modulus of a wavelet transform. A local integration definesa windowed scattering transform, which is proved to be Lipschitz continuous tothe action of diffeomorphisms. As the window size increases, it converges to awavelet scattering transform which is translation invariant. Scatteringcoefficients also provide representations of stationary processes. Expectedvalues depend upon high order moments and can discriminate processes having thesame power spectrum. Scattering operators are extended on L2 (G), where G is acompact Lie group, and are invariant under the action of G. Combining ascattering on L2(R^d) and on Ld (SO(d)) defines a translation and rotationinvariant scattering on L2(R^d).
arxiv-300-210 | Sparsity regret bounds for individual sequences in online linear regression | http://arxiv.org/pdf/1101.1057v3.pdf | author:Sébastien Gerchinovitz category:stat.ML cs.LG math.ST stat.TH published:2011-01-05 summary:We consider the problem of online linear regression on arbitrarydeterministic sequences when the ambient dimension d can be much larger thanthe number of time rounds T. We introduce the notion of sparsity regret bound,which is a deterministic online counterpart of recent risk bounds derived inthe stochastic setting under a sparsity scenario. We prove such regret boundsfor an online-learning algorithm called SeqSEW and based on exponentialweighting and data-driven truncation. In a second part we apply aparameter-free version of this algorithm to the stochastic setting (regressionmodel with random design). This yields risk bounds of the same flavor as inDalalyan and Tsybakov (2011) but which solve two questions left open therein.In particular our risk bounds are adaptive (up to a logarithmic factor) to theunknown variance of the noise if the latter is Gaussian. We also address theregression model with fixed design.
arxiv-300-211 | Sparse recovery with unknown variance: a LASSO-type approach | http://arxiv.org/pdf/1101.0434v5.pdf | author:Stéphane Chrétien, Sébastien Darses category:math.ST stat.ML stat.TH published:2011-01-02 summary:We address the issue of estimating the regression vector $\beta$ in thegeneric $s$-sparse linear model $y = X\beta+z$, with $\beta\in\R^{p}$,$y\in\R^{n}$, $z\sim\mathcal N(0,\sg^2 I)$ and $p> n$ when the variance$\sg^{2}$ is unknown. We study two LASSO-type methods that jointly estimate$\beta$ and the variance. These estimators are minimizers of the $\ell_1$penalized least-squares functional, where the relaxation parameter is tunedaccording to two different strategies. In the first strategy, the relaxationparameter is of the order $\ch{\sigma} \sqrt{\log p}$, where $\ch{\sigma}^2$ isthe empirical variance. %The resulting optimization problem can be solved byrunning only a few successive LASSO instances with %recursive updating of therelaxation parameter. In the second strategy, the relaxation parameter ischosen so as to enforce a trade-off between the fidelity and the penalty termsat optimality. For both estimators, our assumptions are similar to the onesproposed by Cand\`es and Plan in {\it Ann. Stat. (2009)}, for the case where$\sg^{2}$ is known. We prove that our estimators ensure exact recovery of thesupport and sign pattern of $\beta$ with high probability. We presentsimulations results showing that the first estimator enjoys nearly the sameperformances in practice as the standard LASSO (known variance case) for a widerange of the signal to noise ratio. Our second estimator is shown to outperformboth in terms of false detection, when the signal to noise ratio is low.
arxiv-300-212 | lp-Recovery of the Most Significant Subspace among Multiple Subspaces with Outliers | http://arxiv.org/pdf/1012.4116v4.pdf | author:Gilad Lerman, Teng Zhang category:stat.ML cs.CV math.FA published:2010-12-18 summary:We assume data sampled from a mixture of d-dimensional linear subspaces withspherically symmetric distributions within each subspace and an additionaloutlier component with spherically symmetric distribution within the ambientspace (for simplicity we may assume that all distributions are uniform on theircorresponding unit spheres). We also assume mixture weights for the differentcomponents. We say that one of the underlying subspaces of the model is mostsignificant if its mixture weight is higher than the sum of the mixture weightsof all other subspaces. We study the recovery of the most significant subspaceby minimizing the lp-averaged distances of data points from d-dimensionalsubspaces, where p>0. Unlike other lp minimization problems, this minimizationis non-convex for all p>0 and thus requires different methods for its analysis.We show that if 0<p<=1, then for any fraction of outliers the most significantsubspace can be recovered by lp minimization with overwhelming probability(which depends on the generating distribution and its parameters). We show thatwhen adding small noise around the underlying subspaces the most significantsubspace can be nearly recovered by lp minimization for any 0<p<=1 with anerror proportional to the noise level. On the other hand, if p>1 and there ismore than one underlying subspace, then with overwhelming probability the mostsignificant subspace cannot be recovered or nearly recovered. This last resultdoes not require spherically symmetric outliers.
arxiv-300-213 | Analysis of Agglomerative Clustering | http://arxiv.org/pdf/1012.3697v4.pdf | author:Marcel R. Ackermann, Johannes Blömer, Daniel Kuntze, Christian Sohler category:cs.DS cs.CG cs.LG published:2010-12-16 summary:The diameter $k$-clustering problem is the problem of partitioning a finitesubset of $\mathbb{R}^d$ into $k$ subsets called clusters such that the maximumdiameter of the clusters is minimized. One early clustering algorithm thatcomputes a hierarchy of approximate solutions to this problem (for all valuesof $k$) is the agglomerative clustering algorithm with the complete linkagestrategy. For decades, this algorithm has been widely used by practitioners.However, it is not well studied theoretically. In this paper, we analyze theagglomerative complete linkage clustering algorithm. Assuming that thedimension $d$ is a constant, we show that for any $k$ the solution computed bythis algorithm is an $O(\log k)$-approximation to the diameter $k$-clusteringproblem. Our analysis does not only hold for the Euclidean distance but for anymetric that is based on a norm. Furthermore, we analyze the closely related$k$-center and discrete $k$-center problem. For the corresponding agglomerativealgorithms, we deduce an approximation factor of $O(\log k)$ as well.
arxiv-300-214 | Inverse-Category-Frequency based supervised term weighting scheme for text categorization | http://arxiv.org/pdf/1012.2609v4.pdf | author:Deqing Wang, Hui Zhang category:cs.LG cs.AI published:2010-12-13 summary:Term weighting schemes often dominate the performance of many classifiers,such as kNN, centroid-based classifier and SVMs. The widely used term weightingscheme in text categorization, i.e., tf.idf, is originated from informationretrieval (IR) field. The intuition behind idf for text categorization seemsless reasonable than IR. In this paper, we introduce inverse category frequency(icf) into term weighting scheme and propose two novel approaches, i.e., tf.icfand icf-based supervised term weighting schemes. The tf.icf adopts icf tosubstitute idf factor and favors terms occurring in fewer categories, ratherthan fewer documents. And the icf-based approach combines icf and relevancefrequency (rf) to weight terms in a supervised way. Our cross-classifier andcross-corpus experiments have shown that our proposed approaches are superioror comparable to six supervised term weighting schemes and three traditionalschemes in terms of macro-F1 and micro-F1.
arxiv-300-215 | Generalized Species Sampling Priors with Latent Beta reinforcements | http://arxiv.org/pdf/1012.0866v4.pdf | author:Edoardo M. Airoldi, Thiago Costa, Federico Bassetti, Fabrizio Leisen, Michele Guindani category:math.ST cs.LG stat.ME stat.TH published:2010-12-04 summary:Many popular Bayesian nonparametric priors can be characterized in terms ofexchangeable species sampling sequences. However, in some applications,exchangeability may not be appropriate. We introduce a {novel andprobabilistically coherent family of non-exchangeable species samplingsequences characterized by a tractable predictive probability function withweights driven by a sequence of independent Beta random variables. We comparetheir theoretical clustering properties with those of the Dirichlet Process andthe two parameters Poisson-Dirichlet process. The proposed constructionprovides a complete characterization of the joint process, differently fromexisting work. We then propose the use of such process as prior distribution ina hierarchical Bayes modeling framework, and we describe a Markov Chain MonteCarlo sampler for posterior inference. We evaluate the performance of the priorand the robustness of the resulting inference in a simulation study, providinga comparison with popular Dirichlet Processes mixtures and Hidden MarkovModels. Finally, we develop an application to the detection of chromosomalaberrations in breast cancer by leveraging array CGH data.
arxiv-300-216 | Efficient Optimization of Performance Measures by Classifier Adaptation | http://arxiv.org/pdf/1012.0930v3.pdf | author:Nan Li, Ivor W. Tsang, Zhi-Hua Zhou category:cs.LG cs.AI published:2010-12-04 summary:In practical applications, machine learning algorithms are often needed tolearn classifiers that optimize domain specific performance measures.Previously, the research has focused on learning the needed classifier inisolation, yet learning nonlinear classifier for nonlinear and nonsmoothperformance measures is still hard. In this paper, rather than learning theneeded classifier by optimizing specific performance measure directly, wecircumvent this problem by proposing a novel two-step approach called as CAPO,namely to first train nonlinear auxiliary classifiers with existing learningmethods, and then to adapt auxiliary classifiers for specific performancemeasures. In the first step, auxiliary classifiers can be obtained efficientlyby taking off-the-shelf learning algorithms. For the second step, we show thatthe classifier adaptation problem can be reduced to a quadratic programproblem, which is similar to linear SVMperf and can be efficiently solved. Byexploiting nonlinear auxiliary classifiers, CAPO can generate nonlinearclassifier which optimizes a large variety of performance measures includingall the performance measure based on the contingency table and AUC, whilstkeeping high computational efficiency. Empirical studies show that CAPO iseffective and of high computational efficiency, and even it is more efficientthan linear SVMperf.
arxiv-300-217 | Optimal measures and Markov transition kernels | http://arxiv.org/pdf/1012.0366v7.pdf | author:Roman V. Belavkin category:math.OC cs.CC cs.IT math-ph math.FA math.IT math.MP stat.ML published:2010-12-02 summary:We study optimal solutions to an abstract optimization problem for measures,which is a generalization of classical variational problems in informationtheory and statistical physics. In the classical problems, information andrelative entropy are defined using the Kullback-Leibler divergence, and forthis reason optimal measures belong to a one-parameter exponential family.Measures within such a family have the property of mutual absolute continuity.Here we show that this property characterizes other families of optimalpositive measures if a functional representing information has a strictlyconvex dual. Mutual absolute continuity of optimal probability measures allowsus to strictly separate deterministic and non-deterministic Markov transitionkernels, which play an important role in theories of decisions, estimation,control, communication and computation. We show that deterministic transitionsare strictly sub-optimal, unless information resource with a strictly convexdual is unconstrained. For illustration, we construct an example where, unlikenon-deterministic, any deterministic kernel either has negatively infiniteexpected utility (unbounded expected error) or communicates infiniteinformation.
arxiv-300-218 | Nuclear norm penalization and optimal rates for noisy low rank matrix completion | http://arxiv.org/pdf/1011.6256v4.pdf | author:Vladimir Koltchinskii, Alexandre B. Tsybakov, Karim Lounici category:math.ST stat.ML stat.TH published:2010-11-29 summary:This paper deals with the trace regression model where $n$ entries or linearcombinations of entries of an unknown $m_1\times m_2$ matrix $A_0$ corrupted bynoise are observed. We propose a new nuclear norm penalized estimator of $A_0$and establish a general sharp oracle inequality for this estimator forarbitrary values of $n,m_1,m_2$ under the condition of isometry in expectation.Then this method is applied to the matrix completion problem. In this case, theestimator admits a simple explicit form and we prove that it satisfies oracleinequalities with faster rates of convergence than in the previous works. Theyare valid, in particular, in the high-dimensional setting $m_1m_2\gg n$. Weshow that the obtained rates are optimal up to logarithmic factors in a minimaxsense and also derive, for any fixed matrix $A_0$, a non-minimax lower bound onthe rate of convergence of our estimator, which coincides with the upper boundup to a constant factor. Finally, we show that our procedure provides an exactrecovery of the rank of $A_0$ with probability close to 1. We also discuss thestatistical learning setting where there is no underlying model determined by$A_0$ and the aim is to find the best trace regression model approximating thedata.
arxiv-300-219 | Tight Sample Complexity of Large-Margin Learning | http://arxiv.org/pdf/1011.5053v2.pdf | author:Sivan Sabato, Nathan Srebro, Naftali Tishby category:cs.LG math.PR math.ST stat.ML stat.TH published:2010-11-23 summary:We obtain a tight distribution-specific characterization of the samplecomplexity of large-margin classification with L_2 regularization: We introducethe \gamma-adapted-dimension, which is a simple function of the spectrum of adistribution's covariance matrix, and show distribution-specific upper andlower bounds on the sample complexity, both governed by the\gamma-adapted-dimension of the source distribution. We conclude that this newquantity tightly characterizes the true sample complexity of large-marginclassification. The bounds hold for a rich family of sub-Gaussiandistributions.
arxiv-300-220 | Clustering and Latent Semantic Indexing Aspects of the Singular Value Decomposition | http://arxiv.org/pdf/1011.4104v4.pdf | author:Andri Mirzal category:cs.LG cs.NA math.SP 15A18, 65F15 published:2010-11-17 summary:This paper discusses clustering and latent semantic indexing (LSI) aspects ofthe singular value decomposition (SVD). The purpose of this paper is twofold.The first is to give an explanation on how and why the singular vectors can beused in clustering. And the second is to show that the two seemingly unrelatedSVD aspects actually originate from the same source: related vertices tend tobe more clustered in the graph representation of lower rank approximate matrixusing the SVD than in the original semantic graph. Accordingly, the SVD canimprove retrieval performance of an information retrieval system since queriesmade to the approximate matrix can retrieve more relevant documents and filterout more irrelevant documents than the same queries made to the originalmatrix. By utilizing this fact, we will devise an LSI algorithm that mimicksSVD capability in clustering related vertices. Convergence analysis shows thatthe algorithm is convergent and produces a unique solution for each input.Experimental results using some standard datasets in LSI research show thatretrieval performances of the algorithm are comparable to the SVD's. Inaddition, the algorithm is more practical and easier to use because there is noneed to determine decomposition rank which is crucial in driving retrievalperformance of the SVD.
arxiv-300-221 | Warping Peirce Quincuncial Panoramas | http://arxiv.org/pdf/1011.3189v5.pdf | author:Chamberlain Fong, Brian K. Vogel category:cs.CV cs.GR published:2010-11-14 summary:The Peirce quincuncial projection is a mapping of the surface of a sphere tothe interior of a square. It is a conformal map except for four points on theequator. These points of non-conformality cause significant artifacts inphotographic applications. In this paper, we propose an algorithm anduser-interface to mitigate these artifacts. Moreover, in order to facilitate aninteractive user-interface, we present a fast algorithm for calculating thePeirce quincuncial projection of spherical imagery. We then promote the Peircequincuncial projection as a viable alternative to the more popularstereographic projection in some scenarios.
arxiv-300-222 | Classification with Scattering Operators | http://arxiv.org/pdf/1011.3023v4.pdf | author:Joan Bruna, Stéphane Mallat category:cs.CV published:2010-11-12 summary:A scattering vector is a local descriptor including multiscale andmulti-direction co-occurrence information. It is computed with a cascade ofwavelet decompositions and complex modulus. This scattering representation islocally translation invariant and linearizes deformations. A supervisedclassification algorithm is computed with a PCA model selection on scatteringvectors. State of the art results are obtained for handwritten digitrecognition and texture classification.
arxiv-300-223 | Multiarmed Bandit Problems with Delayed Feedback | http://arxiv.org/pdf/1011.1161v3.pdf | author:Sudipto Guha, Kamesh Munagala, Martin Pal category:cs.DS cs.LG published:2010-11-04 summary:In this paper we initiate the study of optimization of bandit type problemsin scenarios where the feedback of a play is not immediately known. This arisesnaturally in allocation problems which have been studied extensively in theliterature, albeit in the absence of delays in the feedback. We study thisproblem in the Bayesian setting. In presence of delays, no solution withprovable guarantees is known to exist with sub-exponential running time. We show that bandit problems with delayed feedback that arise in allocationsettings can be forced to have significant structure, with a slight loss inoptimality. This structure gives us the ability to reason about therelationship of single arm policies to the entangled optimum policy, andeventually leads to a O(1) approximation for a significantly general class ofpriors. The structural insights we develop are of key interest and carry overto the setting where the feedback of an action is available instantaneously,and we improve all previous results in this setting as well.
arxiv-300-224 | Local Component Analysis for Nonparametric Bayes Classifier | http://arxiv.org/pdf/1010.4951v2.pdf | author:Mahmoud Khademi, Mohammad T. Manzuri-Shalmani, Meharn safayani category:cs.CV cs.LG published:2010-10-24 summary:The decision boundaries of Bayes classifier are optimal because they lead tomaximum probability of correct decision. It means if we knew the priorprobabilities and the class-conditional densities, we could design a classifierwhich gives the lowest probability of error. However, in classification basedon nonparametric density estimation methods such as Parzen windows, thedecision regions depend on the choice of parameters such as window width.Moreover, these methods suffer from curse of dimensionality of the featurespace and small sample size problem which severely restricts their practicalapplications. In this paper, we address these problems by introducing a noveldimension reduction and classification method based on local componentanalysis. In this method, by adopting an iterative cross-validation algorithm,we simultaneously estimate the optimal transformation matrices (for dimensionreduction) and classifier parameters based on local information. The proposedmethod can classify the data with complicated boundary and also alleviate thecourse of dimensionality dilemma. Experiments on real data show the superiorityof the proposed algorithm in term of classification accuracies for patternclassification applications like age, facial expression and characterrecognition. Keywords: Bayes classifier, curse of dimensionality dilemma,Parzen window, pattern classification, subspace learning.
arxiv-300-225 | Reading Dependencies from Covariance Graphs | http://arxiv.org/pdf/1010.4504v3.pdf | author:Jose M. Peña category:stat.ML cs.AI math.ST stat.TH published:2010-10-21 summary:The covariance graph (aka bi-directed graph) of a probability distribution$p$ is the undirected graph $G$ where two nodes are adjacent iff theircorresponding random variables are marginally dependent in $p$. In this paper,we present a graphical criterion for reading dependencies from $G$, under theassumption that $p$ satisfies the graphoid properties as well as weaktransitivity and composition. We prove that the graphical criterion is soundand complete in certain sense. We argue that our assumptions are not toorestrictive. For instance, all the regular Gaussian probability distributionssatisfy them.
arxiv-300-226 | Hybrid Linear Modeling via Local Best-fit Flats | http://arxiv.org/pdf/1010.3460v2.pdf | author:Teng Zhang, Arthur Szlam, Yi Wang, Gilad Lerman category:cs.CV stat.ML published:2010-10-17 summary:We present a simple and fast geometric method for modeling data by a union ofaffine subspaces. The method begins by forming a collection of local best-fitaffine subspaces, i.e., subspaces approximating the data in localneighborhoods. The correct sizes of the local neighborhoods are determinedautomatically by the Jones' $\beta_2$ numbers (we prove under certain geometricconditions that our method finds the optimal local neighborhoods). Thecollection of subspaces is further processed by a greedy selection procedure ora spectral method to generate the final model. We discuss applications totracking-based motion segmentation and clustering of faces under differentilluminating conditions. We give extensive experimental evidence demonstratingthe state of the art accuracy and speed of the suggested algorithms on theseproblems and also on synthetic hybrid linear data as well as the MNISThandwritten digits data; and we demonstrate how to use our algorithms for fastdetermination of the number of affine subspaces.
arxiv-300-227 | Near-Optimal Bayesian Active Learning with Noisy Observations | http://arxiv.org/pdf/1010.3091v2.pdf | author:Daniel Golovin, Andreas Krause, Debajyoti Ray category:cs.LG cs.AI cs.DS published:2010-10-15 summary:We tackle the fundamental problem of Bayesian active learning with noise,where we need to adaptively select from a number of expensive tests in order toidentify an unknown hypothesis sampled from a known prior distribution. In thecase of noise-free observations, a greedy algorithm called generalized binarysearch (GBS) is known to perform near-optimally. We show that if theobservations are noisy, perhaps surprisingly, GBS can perform very poorly. Wedevelop EC2, a novel, greedy active learning algorithm and prove that it iscompetitive with the optimal policy, thus obtaining the first competitivenessguarantees for Bayesian active learning with noisy observations. Our boundsrely on a recently discovered diminishing returns property called adaptivesubmodularity, generalizing the classical notion of submodular set functions toadaptive policies. Our results hold even if the tests have non-uniform cost andtheir noise is correlated. We also propose EffECXtive, a particularly fastapproximation of EC2, and evaluate it on a Bayesian experimental design probleminvolving human subjects, intended to tease apart competing economic theoriesof how people make decisions under uncertainty.
arxiv-300-228 | Robust Recovery of Subspace Structures by Low-Rank Representation | http://arxiv.org/pdf/1010.2955v6.pdf | author:Guangcan Liu, Zhouchen Lin, Shuicheng Yan, Ju Sun, Yong Yu, Yi Ma category:cs.IT cs.CV cs.LG math.IT published:2010-10-14 summary:In this work we address the subspace recovery problem. Given a set of datasamples (vectors) approximately drawn from a union of multiple subspaces, ourgoal is to segment the samples into their respective subspaces and correct thepossible errors as well. To this end, we propose a novel method termed Low-RankRepresentation (LRR), which seeks the lowest-rank representation among all thecandidates that can represent the data samples as linear combinations of thebases in a given dictionary. It is shown that LRR well solves the subspacerecovery problem: when the data is clean, we prove that LRR exactly capturesthe true subspace structures; for the data contaminated by outliers, we provethat under certain conditions LRR can exactly recover the row space of theoriginal data and detect the outlier as well; for the data corrupted byarbitrary errors, LRR can also approximately recover the row space withtheoretical guarantees. Since the subspace membership is provably determined bythe row space, these further imply that LRR can perform robust subspacesegmentation and error correction, in an efficient way.
arxiv-300-229 | Optimal designs for Lasso and Dantzig selector using Expander Codes | http://arxiv.org/pdf/1010.2457v6.pdf | author:Yohann de Castro category:math.ST cs.IT math.IT math.PR stat.ME stat.ML stat.TH published:2010-10-12 summary:We investigate the high-dimensional regression problem using adjacencymatrices of unbalanced expander graphs. In this frame, we prove that the$\ell_{2}$-prediction error and the $\ell_{1}$-risk of the lasso and theDantzig selector are optimal up to an explicit multiplicative constant. Thus wecan estimate a high-dimensional target vector with an error term similar to theone obtained in a situation where one knows the support of the largestcoordinates in advance. Moreover, we show that these design matrices have an explicit restrictedeigenvalue. Precisely, they satisfy the restricted eigenvalue assumption andthe compatibility condition with an explicit constant. Eventually, we capitalize on the recent construction of unbalanced expandergraphs due to Guruswami, Umans, and Vadhan, to provide a deterministicpolynomial time construction of these design matrices.
arxiv-300-230 | Time Series Classification by Class-Specific Mahalanobis Distance Measures | http://arxiv.org/pdf/1010.1526v6.pdf | author:Zoltán Prekopcsák, Daniel Lemire category:cs.LG published:2010-10-07 summary:To classify time series by nearest neighbors, we need to specify or learn oneor several distance measures. We consider variations of the Mahalanobisdistance measures which rely on the inverse covariance matrix of the data.Unfortunately --- for time series data --- the covariance matrix has often lowrank. To alleviate this problem we can either use a pseudoinverse, covarianceshrinking or limit the matrix to its diagonal. We review these alternatives andbenchmark them against competitive methods such as the related Large MarginNearest Neighbor Classification (LMNN) and the Dynamic Time Warping (DTW)distance. As we expected, we find that the DTW is superior, but the Mahalanobisdistance measures are one to two orders of magnitude faster. To get bestresults with Mahalanobis distance measures, we recommend learning one distancemeasure per class using either covariance shrinking or the diagonal approach.
arxiv-300-231 | Genetic Algorithm for Mulicriteria Optimization of a Multi-Pickup and Delivery Problem with Time Windows | http://arxiv.org/pdf/1010.0771v2.pdf | author:Imen Harbaoui Dridi, Ryan Kammarti, Mekki Ksouri, Pierre Borne category:cs.NE published:2010-10-05 summary:In This paper we present a genetic algorithm for mulicriteria optimization ofa multipickup and delivery problem with time windows (m-PDPTW). The m-PDPTW isan optimization vehicles routing problem which must meet requests for transportbetween suppliers and customers satisfying precedence, capacity and timeconstraints. This paper purposes a brief literature review of the PDPTW,present an approach based on genetic algorithms and Pareto dominance method togive a set of satisfying solutions to the m-PDPTW minimizing total travel cost,total tardiness time and the vehicles number.
arxiv-300-232 | Fast Reinforcement Learning for Energy-Efficient Wireless Communications | http://arxiv.org/pdf/1009.5773v4.pdf | author:Nicholas Mastronarde, Mihaela van der Schaar category:cs.LG published:2010-09-29 summary:We consider the problem of energy-efficient point-to-point transmission ofdelay-sensitive data (e.g. multimedia data) over a fading channel. Existingresearch on this topic utilizes either physical-layer centric solutions, namelypower-control and adaptive modulation and coding (AMC), or system-levelsolutions based on dynamic power management (DPM); however, there is currentlyno rigorous and unified framework for simultaneously utilizing bothphysical-layer centric and system-level techniques to achieve the minimumpossible energy consumption, under delay constraints, in the presence ofstochastic and a priori unknown traffic and channel conditions. In this report,we propose such a framework. We formulate the stochastic optimization problemas a Markov decision process (MDP) and solve it online using reinforcementlearning. The advantages of the proposed online method are that (i) it does notrequire a priori knowledge of the traffic arrival and channel statistics todetermine the jointly optimal power-control, AMC, and DPM policies; (ii) itexploits partial information about the system so that less information needs tobe learned than when using conventional reinforcement learning algorithms; and(iii) it obviates the need for action exploration, which severely limits theadaptation speed and run-time performance of conventional reinforcementlearning algorithms. Our results show that the proposed learning algorithms canconverge up to two orders of magnitude faster than a state-of-the-art learningalgorithm for physical layer power-control and up to three orders of magnitudefaster than conventional reinforcement learning algorithms.
arxiv-300-233 | Task-Driven Dictionary Learning | http://arxiv.org/pdf/1009.5358v2.pdf | author:Julien Mairal, Francis Bach, Jean Ponce category:stat.ML published:2010-09-27 summary:Modeling data with linear combinations of a few elements from a learneddictionary has been the focus of much recent research in machine learning,neuroscience and signal processing. For signals such as natural images thatadmit such sparse representations, it is now well established that these modelsare well suited to restoration tasks. In this context, learning the dictionaryamounts to solving a large-scale matrix factorization problem, which can bedone efficiently with classical optimization tools. The same approach has alsobeen used for learning features from data for other purposes, e.g., imageclassification, but tuning the dictionary in a supervised way for these taskshas proven to be more difficult. In this paper, we present a generalformulation for supervised dictionary learning adapted to a wide variety oftasks, and present an efficient algorithm for solving the correspondingoptimization problem. Experiments on handwritten digit classification, digitalart identification, nonlinear inverse image problems, and compressed sensingdemonstrate that our approach is effective in large-scale settings, and is wellsuited to supervised and semi-supervised classification, as well as regressiontasks for data that admit sparse representations.
arxiv-300-234 | Optimistic Rates for Learning with a Smooth Loss | http://arxiv.org/pdf/1009.3896v2.pdf | author:Nathan Srebro, Karthik Sridharan, Ambuj Tewari category:cs.LG published:2010-09-20 summary:We establish an excess risk bound of O(H R_n^2 + R_n \sqrt{H L*}) forempirical risk minimization with an H-smooth loss function and a hypothesisclass with Rademacher complexity R_n, where L* is the best risk achievable bythe hypothesis class. For typical hypothesis classes where R_n = \sqrt{R/n},this translates to a learning rate of O(RH/n) in the separable (L*=0) case andO(RH/n + \sqrt{L^* RH/n}) more generally. We also provide similar guaranteesfor online and stochastic convex optimization with a smooth non-negativeobjective.
arxiv-300-235 | Geometric Decision Tree | http://arxiv.org/pdf/1009.3604v5.pdf | author:Naresh Manwani, P. S. Sastry category:cs.LG published:2010-09-19 summary:In this paper we present a new algorithm for learning oblique decision trees.Most of the current decision tree algorithms rely on impurity measures toassess the goodness of hyperplanes at each node while learning a decision treein a top-down fashion. These impurity measures do not properly capture thegeometric structures in the data. Motivated by this, our algorithm uses astrategy to assess the hyperplanes in such a way that the geometric structurein the data is taken into account. At each node of the decision tree, we findthe clustering hyperplanes for both the classes and use their angle bisectorsas the split rule at that node. We show through empirical studies that thisidea leads to small decision trees and better performance. We also present someanalysis to show that the angle bisectors of clustering hyperplanes that we useas the split rules at each node, are solutions of an interesting optimizationproblem and hence argue that this is a principled method of learning a decisiontree.
arxiv-300-236 | On the Doubt about Margin Explanation of Boosting | http://arxiv.org/pdf/1009.3613v5.pdf | author:Wei Gao, Zhi-Hua Zhou category:cs.LG published:2010-09-19 summary:Margin theory provides one of the most popular explanations to the success of\texttt{AdaBoost}, where the central point lies in the recognition that\textit{margin} is the key for characterizing the performance of\texttt{AdaBoost}. This theory has been very influential, e.g., it has beenused to argue that \texttt{AdaBoost} usually does not overfit since it tends toenlarge the margin even after the training error reaches zero. Previously the\textit{minimum margin bound} was established for \texttt{AdaBoost}, however,\cite{Breiman1999} pointed out that maximizing the minimum margin does notnecessarily lead to a better generalization. Later, \cite{Reyzin:Schapire2006}emphasized that the margin distribution rather than minimum margin is crucialto the performance of \texttt{AdaBoost}. In this paper, we first present the\textit{$k$th margin bound} and further study on its relationship to previouswork such as the minimum margin bound and Emargin bound. Then, we improve theprevious empirical Bernstein bounds\citep{Maurer:Pontil2009,Audibert:Munos:Szepesvari2009}, and based on suchfindings, we defend the margin-based explanation against Breiman's doubts byproving a new generalization error bound that considers exactly the samefactors as \cite{Schapire:Freund:Bartlett:Lee1998} but is sharper than\cite{Breiman1999}'s minimum margin bound. By incorporating factors such asaverage margin and variance, we present a generalization error bound that isheavily related to the whole margin distribution. We also provide margindistribution bounds for generalization error of voting classifiers in finiteVC-dimension space.
arxiv-300-237 | Penalty Decomposition Methods for $L0$-Norm Minimization | http://arxiv.org/pdf/1008.5372v2.pdf | author:Zhaosong Lu, Yong Zhang category:math.OC cs.CV cs.IT cs.LG cs.NA math.IT stat.ME published:2010-08-31 summary:In this paper we consider general l0-norm minimization problems, that is, theproblems with l0-norm appearing in either objective function or constraint. Inparticular, we first reformulate the l0-norm constrained problem as anequivalent rank minimization problem and then apply the penalty decomposition(PD) method proposed in [33] to solve the latter problem. By utilizing thespecial structures, we then transform all matrix operations of this method tovector operations and obtain a PD method that only involves vector operations.Under some suitable assumptions, we establish that any accumulation point ofthe sequence generated by the PD method satisfies a first-order optimalitycondition that is generally stronger than one natural optimality condition. Wefurther extend the PD method to solve the problem with the l0-norm appearing inobjective function. Finally, we test the performance of our PD methods byapplying them to compressed sensing, sparse logistic regression and sparseinverse covariance selection. The computational results demonstrate that ourmethods generally outperform the existing methods in terms of solution qualityand/or speed.
arxiv-300-238 | Penalty Decomposition Methods for Rank Minimization | http://arxiv.org/pdf/1008.5373v4.pdf | author:Zhaosong Lu, Yong Zhang category:math.OC cs.LG cs.NA cs.SY q-fin.CP q-fin.ST published:2010-08-31 summary:In this paper we consider general rank minimization problems with rankappearing in either objective function or constraint. We first establish that aclass of special rank minimization problems has closed-form solutions. Usingthis result, we then propose penalty decomposition methods for general rankminimization problems in which each subproblem is solved by a block coordinatedescend method. Under some suitable assumptions, we show that any accumulationpoint of the sequence generated by the penalty decomposition methods satisfiesthe first-order optimality conditions of a nonlinear reformulation of theproblems. Finally, we test the performance of our methods by applying them tothe matrix completion and nearest low-rank correlation matrix problems. Thecomputational results demonstrate that our methods are generally comparable orsuperior to the existing methods in terms of solution quality.
arxiv-300-239 | Nonlinear Quality of Life Index | http://arxiv.org/pdf/1008.4063v3.pdf | author:A. Zinovyev, A. N. Gorban category:cs.NE stat.AP published:2010-08-24 summary:We present details of the analysis of the nonlinear quality of life index for171 countries. This index is based on four indicators: GDP per capita byPurchasing Power Parities, Life expectancy at birth, Infant mortality rate, andTuberculosis incidence. We analyze the structure of the data in order to findthe optimal and independent on expert's opinion way to map several numericalindicators from a multidimensional space onto the one-dimensional space of thequality of life. In the 4D space we found a principal curve that goes "throughthe middle" of the dataset and project the data points on this curve. The orderalong this principal curve gives us the ranking of countries. Projection ontothe principal curve provides a solution to the classical problem ofunsupervised ranking of objects. It allows us to find the independent onexpert's opinion way to project several numerical indicators from amultidimensional space onto the one-dimensional space of the index values. Thisprojection is, in some sense, optimal and preserves as much information aspossible. For computation we used ViDaExpert, a tool for visualization andanalysis of multidimensional vectorial data (arXiv:1406.5550).
arxiv-300-240 | Bottleneck of using single memristor as a synapse and its solution | http://arxiv.org/pdf/1008.3450v3.pdf | author:Farnood Merrikh-Bayat, Saeed Bagheri Shouraki, Iman Esmaili Paeen Afrakoti category:cs.NE published:2010-08-20 summary:It is now widely accepted that memristive devices are perfect candidates forthe emulation of biological synapses in neuromorphic systems. This is mainlybecause of the fact that like the strength of synapse, memristance of thememristive device can be tuned actively (e.g., by the application of volt- ageor current). In addition, it is also possible to fabricate very high density ofmemristive devices (comparable to the number of synapses in real biologicalsystem) through the nano-crossbar structures. However, in this paper we willshow that there are some problems associated with memristive synapses(memristive devices which are playing the role of biological synapses). Forexample, we show that the variation rate of the memristance of memristivedevice depends completely on the current memristance of the device andtherefore it can change significantly with time during the learning phase. Thisphenomenon can degrade the performance of learning methods like SpikeTiming-Dependent Plasticity (STDP) and cause the corresponding neuromorphicsystems to become unstable. Finally, at the end of this paper, we illustratethat using two serially connected memristive devices with different polaritiesas a synapse can somewhat fix the aforementioned problem.
arxiv-300-241 | Submodular Functions: Learnability, Structure, and Optimization | http://arxiv.org/pdf/1008.2159v3.pdf | author:Maria-Florina Balcan, Nicholas J. A. Harvey category:cs.DS cs.DM cs.LG published:2010-08-12 summary:Submodular functions are discrete functions that model laws of diminishingreturns and enjoy numerous algorithmic applications. They have been used inmany areas, including combinatorial optimization, machine learning, andeconomics. In this work we study submodular functions from a learning theoreticangle. We provide algorithms for learning submodular functions, as well aslower bounds on their learnability. In doing so, we uncover several novelstructural results revealing ways in which submodular functions can be bothsurprisingly structured and surprisingly unstructured. We provide severalconcrete implications of our work in other domains including algorithmic gametheory and combinatorial optimization. At a technical level, this research combines ideas from many areas, includinglearning theory (distributional learning and PAC-style analyses), combinatoricsand optimization (matroids and submodular functions), and pseudorandomness(lossless expander graphs).
arxiv-300-242 | Separate Training for Conditional Random Fields Using Co-occurrence Rate Factorization | http://arxiv.org/pdf/1008.1566v5.pdf | author:Zhemin Zhu, Djoerd Hiemstra, Peter Apers, Andreas Wombacher category:cs.LG cs.AI published:2010-08-09 summary:The standard training method of Conditional Random Fields (CRFs) is very slowfor large-scale applications. As an alternative, piecewise training divides thefull graph into pieces, trains them independently, and combines the learnedweights at test time. In this paper, we present \emph{separate} training forundirected models based on the novel Co-occurrence Rate Factorization (CR-F).Separate training is a local training method. In contrast to MEMMs, separatetraining is unaffected by the label bias problem. Experiments show thatseparate training (i) is unaffected by the label bias problem; (ii) reduces thetraining time from weeks to seconds; and (iii) obtains competitive results tothe standard and piecewise training on linear-chain CRFs.
arxiv-300-243 | Mixture decompositions of exponential families using a decomposition of their sample spaces | http://arxiv.org/pdf/1008.0204v4.pdf | author:Guido Montufar category:math.ST stat.ML stat.TH published:2010-08-01 summary:We study the problem of finding the smallest $m$ such that every element ofan exponential family can be written as a mixture of $m$ elements of anotherexponential family. We propose an approach based on coverings and packings ofthe face lattice of the corresponding convex support polytopes and results fromcoding theory. We show that $m=q^{N-1}$ is the smallest number for which anydistribution of $N$ $q$-ary variables can be written as mixture of $m$independent $q$-ary variables. Furthermore, we show that any distribution of$N$ binary variables is a mixture of $m = 2^{N-(k+1)}(1+ 1/(2^k-1))$ elementsof the $k$-interaction exponential family.
arxiv-300-244 | Fast L1-Minimization Algorithms For Robust Face Recognition | http://arxiv.org/pdf/1007.3753v4.pdf | author:Allen Y. Yang, Zihan Zhou, Arvind Ganesh, S. Shankar Sastry, Yi Ma category:cs.CV cs.NA published:2010-07-21 summary:L1-minimization refers to finding the minimum L1-norm solution to anunderdetermined linear system b=Ax. Under certain conditions as described incompressive sensing theory, the minimum L1-norm solution is also the sparsestsolution. In this paper, our study addresses the speed and scalability of itsalgorithms. In particular, we focus on the numerical implementation of asparsity-based classification framework in robust face recognition, wheresparse representation is sought to recover human identities from veryhigh-dimensional facial images that may be corrupted by illumination, facialdisguise, and pose variation. Although the underlying numerical problem is alinear program, traditional algorithms are known to suffer poor scalability forlarge-scale applications. We investigate a new solution based on a classicalconvex optimization framework, known as Augmented Lagrangian Methods (ALM). Thenew convex solvers provide a viable solution to real-world, time-criticalapplications such as face recognition. We conduct extensive experiments tovalidate and compare the performance of the ALM algorithms against severalpopular L1-minimization solvers, including interior-point method, Homotopy,FISTA, SESOP-PCD, approximate message passing (AMP) and TFOCS. To aid peerevaluation, the code for all the algorithms has been made publicly available.
arxiv-300-245 | A generalized risk approach to path inference based on hidden Markov models | http://arxiv.org/pdf/1007.3622v4.pdf | author:Jüri Lember, Alexey A. Koloydenko category:stat.ML cs.LG stat.CO published:2010-07-21 summary:Motivated by the unceasing interest in hidden Markov models (HMMs), thispaper re-examines hidden path inference in these models, using primarily arisk-based framework. While the most common maximum a posteriori (MAP), orViterbi, path estimator and the minimum error, or Posterior Decoder (PD), havelong been around, other path estimators, or decoders, have been either onlyhinted at or applied more recently and in dedicated applications generallyunfamiliar to the statistical learning community. Over a decade ago, however, afamily of algorithmically defined decoders aiming to hybridize the two standardones was proposed (Brushe et al., 1998). The present paper gives a carefulanalysis of this hybridization approach, identifies several problems and issueswith it and other previously proposed approaches, and proposes practicalresolutions of those. Furthermore, simple modifications of the classicalcriteria for hidden path recognition are shown to lead to a new class ofdecoders. Dynamic programming algorithms to compute these decoders in the usualforward-backward manner are presented. A particularly interesting subclass ofsuch estimators can be also viewed as hybrids of the MAP and PD estimators.Similar to previously proposed MAP-PD hybrids, the new class is parameterizedby a small number of tunable parameters. Unlike their algorithmic predecessors,the new risk-based decoders are more clearly interpretable, and, mostimportantly, work "out of the box" in practice, which is demonstrated on somereal bioinformatics tasks and data. Some further generalizations andapplications are discussed in conclusion.
arxiv-300-246 | Reduced Rank Vector Generalized Linear Models for Feature Extraction | http://arxiv.org/pdf/1007.3098v3.pdf | author:Yiyuan She category:stat.ML published:2010-07-19 summary:Supervised linear feature extraction can be achieved by fitting a reducedrank multivariate model. This paper studies rank penalized and rank constrainedvector generalized linear models. From the perspective of thresholding rules,we build a framework for fitting singular value penalized models and use it forfeature extraction. Through solving the rank constraint form of the problem, wepropose progressive feature space reduction for fast computation in highdimensions with little performance loss. A novel projective cross-validation isproposed for parameter tuning in such nonconvex setups. Real data applicationsare given to show the power of the methodology in supervised dimensionreduction and feature extraction.
arxiv-300-247 | Computational Model of Music Sight Reading: A Reinforcement Learning Approach | http://arxiv.org/pdf/1007.0546v4.pdf | author:Keyvan Yahya, Pouyan Rafiei Fard category:cs.AI cs.LG cs.NE math.OC published:2010-07-04 summary:Although the Music Sight Reading process has been studied from the cognitivepsychology view points, but the computational learning methods like theReinforcement Learning have not yet been used to modeling of such processes. Inthis paper, with regards to essential properties of our specific problem, weconsider the value function concept and will indicate that the optimum policycan be obtained by the method we offer without to be getting involved withcomputing of the complex value functions. Also, we will offer a normativebehavioral model for the interaction of the agent with the musical pitchenvironment and by using a slightly different version of Partially observableMarkov decision processes we will show that our method helps for fasterlearning of state-action pairs in our implemented agents.
arxiv-300-248 | Uncertainty of visual measurement and efficient allocation of sensory resources | http://arxiv.org/pdf/1007.0210v2.pdf | author:Sergei Gepshtein, Ivan Tyukin category:q-bio.NC cs.CV cs.IT math.IT published:2010-07-01 summary:We review the reasoning underlying two approaches to combination of sensoryuncertainties. First approach is noncommittal, making no assumptions aboutproperties of uncertainty or parameters of stimulation. Then we explain therelationship between this approach and the one commonly used in modeling"higher level" aspects of sensory systems, such as in visual cue integration,where assumptions are made about properties of stimulation. The two approachesfollow similar logic, except in one case maximal uncertainty is minimized, andin the other minimal certainty is maximized. Then we demonstrate how optimalsolutions are found to the problem of resource allocation under uncertainty.
arxiv-300-249 | Statistical Inference in Dynamic Treatment Regimes | http://arxiv.org/pdf/1006.5831v3.pdf | author:Eric B. Laber, Min Qian, Dan J. Lizotte, William E. Pelham, Susan A. Murphy category:stat.ME stat.ML stat.OT 47N30 published:2010-06-30 summary:Dynamic treatment regimes are of growing interest across the clinicalsciences as these regimes provide one way to operationalize and thus informsequential personalized clinical decision making. A dynamic treatment regime isa sequence of decision rules, with a decision rule per stage of clinicalintervention; each decision rule maps up-to-date patient information to arecommended treatment. We briefly review a variety of approaches for using datato construct the decision rules. We then review an interesting challenge, thatof nonregularity that often arises in this area. By nonregularity, we mean theparameters indexing the optimal dynamic treatment regime are nonsmoothfunctionals of the underlying generative distribution. A consequence is that no regular or asymptotically unbiased estimator ofthese parameters exists. Nonregularity arises in inference for parameters inthe optimal dynamic treatment regime; we illustrate the effect of nonregularityon asymptotic bias and via sensitivity of asymptotic, limiting, distributionsto local perturbations. We propose and evaluate a locally consistent AdaptiveConfidence Interval (ACI) for the parameters of the optimal dynamic treatmentregime. We use data from the Adaptive Interventions for Children with ADHDstudy as an illustrative example. We conclude by highlighting and discussingemerging theoretical problems in this area.
arxiv-300-250 | 3D Visual Tracking with Particle and Kalman Filters | http://arxiv.org/pdf/1006.4910v2.pdf | author:Burak Bayramli category:cs.CV published:2010-06-25 summary:One of the most visually demonstrable and straightforward uses of filteringis in the field of Computer Vision. In this document we will try to outline theissues encountered while designing and implementing a particle and kalmanfilter based tracking system.
arxiv-300-251 | Complete Complementary Results Report of the MARF's NLP Approach to the DEFT 2010 Competition | http://arxiv.org/pdf/1006.3787v7.pdf | author:Serguei A. Mokhov category:cs.CL I.2.7; I.5 published:2010-06-18 summary:This companion paper complements the main DEFT'10 article describing the MARFapproach (arXiv:0905.1235) to the DEFT'10 NLP challenge (described athttp://www.groupes.polymtl.ca/taln2010/deft.php in French). This paper is aimedto present the complete result sets of all the conducted experiments and theirsettings in the resulting tables highlighting the approach and the bestresults, but also showing the worse and the worst and their subsequentanalysis. This particular work focuses on application of the MARF's classicaland NLP pipelines to identification tasks within various francophone corpora toidentify decades when certain articles were published for the first track(Piste 1) and place of origin of a publication (Piste 2), such as the journaland location (France vs. Quebec). This is the sixth iteration of the release ofthe results.
arxiv-300-252 | Approximated Structured Prediction for Learning Large Scale Graphical Models | http://arxiv.org/pdf/1006.2899v2.pdf | author:Tamir Hazan, Raquel Urtasun category:cs.LG cs.AI published:2010-06-15 summary:This manuscripts contains the proofs for "A Primal-Dual Message-PassingAlgorithm for Approximated Large Scale Structured Prediction".
arxiv-300-253 | On the Achievability of Cramér-Rao Bound In Noisy Compressed Sensing | http://arxiv.org/pdf/1006.2513v3.pdf | author:Rad Niazadeh, Masoud Babaie-Zadeh, Christian Jutten category:cs.IT cs.LG math.IT published:2010-06-13 summary:Recently, it has been proved in Babadi et al. that in noisy compressedsensing, a joint typical estimator can asymptotically achieve the Cramer-Raolower bound of the problem.To prove this result, this paper used a lemma,whichis provided in Akcakaya et al,that comprises the main building block of theproof. This lemma is based on the assumption of Gaussianity of the measurementmatrix and its randomness in the domain of noise. In this correspondence, wegeneralize the results obtained in Babadi et al by dropping the Gaussianityassumption on the measurement matrix. In fact, by considering the measurementmatrix as a deterministic matrix in our analysis, we find a theorem similar tothe main theorem of Babadi et al for a family of randomly generated (butdeterministic in the noise domain) measurement matrices that satisfy ageneralized condition known as The Concentration of Measures Inequality. Bythis, we finally show that under our generalized assumptions, the Cramer-Raobound of the estimation is achievable by using the typical estimator introducedin Babadi et al.
arxiv-300-254 | Online Learning via Sequential Complexities | http://arxiv.org/pdf/1006.1138v3.pdf | author:Alexander Rakhlin, Karthik Sridharan, Ambuj Tewari category:cs.LG stat.ML published:2010-06-06 summary:We consider the problem of sequential prediction and provide tools to studythe minimax value of the associated game. Classical statistical learning theoryprovides several useful complexity measures to study learning with i.i.d. data.Our proposed sequential complexities can be seen as extensions of thesemeasures to the sequential setting. The developed theory is shown to yieldprecise learning guarantees for the problem of sequential prediction. Inparticular, we show necessary and sufficient conditions for online learnabilityin the setting of supervised learning. Several examples show the utility of ourframework: we can establish learnability without having to exhibit an explicitonline learning algorithm.
arxiv-300-255 | On the Relation between Realizable and Nonrealizable Cases of the Sequence Prediction Problem | http://arxiv.org/pdf/1005.5603v3.pdf | author:Daniil Ryabko category:cs.LG cs.IT math.IT math.ST stat.TH published:2010-05-31 summary:A sequence $x_1,\dots,x_n,\dots$ of discrete-valued observations is generatedaccording to some unknown probabilistic law (measure) $\mu$. After observingeach outcome, one is required to give conditional probabilities of the nextobservation. The realizable case is when the measure $\mu$ belongs to anarbitrary but known class $\mathcal C$ of process measures. The non-realizablecase is when $\mu$ is completely arbitrary, but the prediction performance ismeasured with respect to a given set $\mathcal C$ of process measures. We areinterested in the relations between these problems and between their solutions,as well as in characterizing the cases when a solution exists and finding thesesolutions. We show that if the quality of prediction is measured using thetotal variation distance, then these problems coincide, while if it is measuredusing the expected average KL divergence, then they are different. For some ofthe formalizations we also show that when a solution exists, it can be obtainedas a Bayes mixture over a countable subset of $\mathcal C$. We also obtainseveral characterization of those sets $\mathcal C$ for which solutions to theconsidered problems exist. As an illustration to the general results obtained,we show that a solution to the non-realizable case of the sequence predictionproblem exists for the set of all finite-memory processes, but does not existfor the set of all stationary processes. It should be emphasized that the framework is completely general: theprocesses measures considered are not required to be i.i.d., mixing,stationary, or to belong to any parametric family.
arxiv-300-256 | Ranked bandits in metric spaces: learning optimally diverse rankings over large document collections | http://arxiv.org/pdf/1005.5197v2.pdf | author:Aleksandrs Slivkins, Filip Radlinski, Sreenivas Gollapudi category:cs.LG cs.DS published:2010-05-28 summary:Most learning to rank research has assumed that the utility of differentdocuments is independent, which results in learned ranking functions thatreturn redundant results. The few approaches that avoid this have ratherunsatisfyingly lacked theoretical foundations, or do not scale. We present alearning-to-rank formulation that optimizes the fraction of satisfied users,with several scalable algorithms that explicitly takes document similarity andranking context into account. Our formulation is a non-trivial commongeneralization of two multi-armed bandit models from the literature: "rankedbandits" (Radlinski et al., ICML 2008) and "Lipschitz bandits" (Kleinberg etal., STOC 2008). We present theoretical justifications for this approach, aswell as a near-optimal algorithm. Our evaluation adds optimizations thatimprove empirical performance, and shows that our algorithms learn orders ofmagnitude more quickly than previous approaches.
arxiv-300-257 | On Recursive Edit Distance Kernels with Application to Time Series Classification | http://arxiv.org/pdf/1005.5141v12.pdf | author:Pierre-François Marteau, Sylvie Gibet category:cs.LG cs.IR published:2010-05-27 summary:This paper proposes some extensions to the work on kernels dedicated tostring or time series global alignment based on the aggregation of scoresobtained by local alignments. The extensions we propose allow to construct,from classical recursive definition of elastic distances, recursive editdistance (or time-warp) kernels that are positive definite if some sufficientconditions are satisfied. The sufficient conditions we end-up with are originaland weaker than those proposed in earlier works, although a recursiveregularizing term is required to get the proof of the positive definiteness asa direct consequence of the Haussler's convolution theorem. The classificationexperiment we conducted on three classical time warp distances (two of whichbeing metrics), using Support Vector Machine classifier, leads to concludethat, when the pairwise distance matrix obtained from the training data is\textit{far} from definiteness, the positive definite recursive elastic kernelsoutperform in general the distance substituting kernels for the classicalelastic distances we have tested.
arxiv-300-258 | Bayesian clustering in decomposable graphs | http://arxiv.org/pdf/1005.5081v2.pdf | author:Luke Bornn, François Caron category:stat.ME stat.AP stat.ML published:2010-05-27 summary:In this paper we propose a class of prior distributions on decomposablegraphs, allowing for improved modeling flexibility. While existing methodssolely penalize the number of edges, the proposed work empowers practitionersto control clustering, level of separation, and other features of the graph.Emphasis is placed on a particular prior distribution which derives itsmotivation from the class of product partition models; the properties of thisprior relative to existing priors is examined through theory and simulation. Wethen demonstrate the use of graphical models in the field of agriculture,showing how the proposed prior distribution alleviates the inflexibility ofprevious approaches in properly modeling the interactions between the yield ofdifferent crop varieties.
arxiv-300-259 | Smoothing proximal gradient method for general structured sparse regression | http://arxiv.org/pdf/1005.4717v4.pdf | author:Xi Chen, Qihang Lin, Seyoung Kim, Jaime G. Carbonell, Eric P. Xing category:stat.ML cs.LG math.OC stat.AP stat.CO published:2010-05-26 summary:We study the problem of estimating high-dimensional regression modelsregularized by a structured sparsity-inducing penalty that encodes priorstructural information on either the input or output variables. We consider twowidely adopted types of penalties of this kind as motivating examples: (1) thegeneral overlapping-group-lasso penalty, generalized from the group-lassopenalty; and (2) the graph-guided-fused-lasso penalty, generalized from thefused-lasso penalty. For both types of penalties, due to their nonseparabilityand nonsmoothness, developing an efficient optimization method remains achallenging problem. In this paper we propose a general optimization approach,the smoothing proximal gradient (SPG) method, which can solve structured sparseregression problems with any smooth convex loss under a wide spectrum ofstructured sparsity-inducing penalties. Our approach combines a smoothingtechnique with an effective proximal gradient method. It achieves a convergencerate significantly faster than the standard first-order methods, subgradientmethods, and is much more scalable than the most widely used interior-pointmethods. The efficiency and scalability of our method are demonstrated on bothsimulation experiments and real genetic data sets.
arxiv-300-260 | Clustering processes | http://arxiv.org/pdf/1005.0826v2.pdf | author:Daniil Ryabko category:cs.LG cs.IT math.IT stat.ML published:2010-05-05 summary:The problem of clustering is considered, for the case when each data point isa sample generated by a stationary ergodic process. We propose a very naturalasymptotic notion of consistency, and show that simple consistent algorithmsexist, under most general non-parametric assumptions. The notion of consistencyis as follows: two samples should be put into the same cluster if and only ifthey were generated by the same distribution. With this notion of consistency,clustering generalizes such classical statistical problems as homogeneitytesting and process classification. We show that, for the case of a knownnumber of clusters, consistency can be achieved under the only assumption thatthe joint distribution of the data is stationary ergodic (no parametric orMarkovian assumptions, no assumptions of independence, neither between norwithin the samples). If the number of clusters is unknown, consistency can beachieved under appropriate assumptions on the mixing rates of the processes.(again, no parametric or independence assumptions). In both cases we giveexamples of simple (at most quadratic in each argument) algorithms which areconsistent.
arxiv-300-261 | Evolutionary Inference for Function-valued Traits: Gaussian Process Regression on Phylogenies | http://arxiv.org/pdf/1004.4668v3.pdf | author:Nick S. Jones, John Moriarty category:q-bio.QM cs.LG stat.ML published:2010-04-26 summary:Biological data objects often have both of the following features: (i) theyare functions rather than single numbers or vectors, and (ii) they arecorrelated due to phylogenetic relationships. In this paper we give a flexiblestatistical model for such data, by combining assumptions from phylogeneticswith Gaussian processes. We describe its use as a nonparametric Bayesian priordistribution, both for prediction (placing posterior distributions on ancestralfunctions) and model selection (comparing rates of evolution across aphylogeny, or identifying the most likely phylogenies consistent with theobserved data). Our work is integrative, extending the popular phylogeneticBrownian Motion and Ornstein-Uhlenbeck models to functional data and Bayesianinference, and extending Gaussian Process regression to phylogenies. We providea brief illustration of the application of our method.
arxiv-300-262 | Facial Expression Representation and Recognition Using 2DHLDA, Gabor Wavelets, and Ensemble Learning | http://arxiv.org/pdf/1004.0378v7.pdf | author:Mahmoud Khademi, Mohammad H. Kiapour, Mehran Safayani, Mohammad T. Manzuri, M. Shojaei category:cs.CV cs.LG I.5 published:2010-04-02 summary:In this paper, a novel method for representation and recognition of thefacial expressions in two-dimensional image sequences is presented. We apply avariation of two-dimensional heteroscedastic linear discriminant analysis(2DHLDA) algorithm, as an efficient dimensionality reduction technique, toGabor representation of the input sequence. 2DHLDA is an extension of thetwo-dimensional linear discriminant analysis (2DLDA) approach and it removesthe equal within-class covariance. By applying 2DHLDA in two directions, weeliminate the correlations between both image columns and image rows. Then, weperform a one-dimensional LDA on the new features. This combined method canalleviate the small sample size problem and instability encountered by HLDA.Also, employing both geometric and appearance features and using an ensemblelearning scheme based on data fusion, we create a classifier which canefficiently classify the facial expressions. The proposed method is robust toillumination changes and it can properly represent temporal information as wellas subtle changes in facial muscles. We provide experiments on Cohn-Kanadedatabase that show the superiority of the proposed method. KEYWORDS:two-dimensional heteroscedastic linear discriminant analysis (2DHLDA), subspacelearning, facial expression analysis, Gabor wavelets, ensemble learning.
arxiv-300-263 | Visualization of Manifold-Valued Elements by Multidimensional Scaling | http://arxiv.org/pdf/1004.0314v2.pdf | author:Simone Fiori category:stat.ML published:2010-04-02 summary:The present contribution suggests the use of a multidimensional scaling (MDS)algorithm as a visualization tool for manifold-valued elements. A visualizationtool of this kind is useful in signal processing and machine learning wheneverlearning/adaptation algorithms insist on high-dimensional parameter manifolds.
arxiv-300-264 | A novel scheme for binarization of vehicle images using hierarchical histogram equalization technique | http://arxiv.org/pdf/1003.6059v2.pdf | author:Satadal Saha, Subhadip Basu, Mita Nasipuri, Dipak Kumar Basu category:cs.CV published:2010-03-31 summary:Automatic License Plate Recognition system is a challenging area of researchnow-a-days and binarization is an integral and most important part of it. Incase of a real life scenario, most of existing methods fail to properlybinarize the image of a vehicle in a congested road, captured through a CCDcamera. In the current work we have applied histogram equalization techniqueover the complete image and also over different hierarchy of imagepartitioning. A novel scheme is formulated for giving the membership value toeach pixel for each hierarchy of histogram equalization. Then the image isbinarized depending on the net membership value of each pixel. The technique isexhaustively evaluated on the vehicle image dataset as well as the licenseplate dataset, giving satisfactory performances.
arxiv-300-265 | Development of an automated Red Light Violation Detection System (RLVDS) for Indian vehicles | http://arxiv.org/pdf/1003.6052v2.pdf | author:Satadal Saha, Subhadip Basu, Mita Nasipuri, Dipak Kumar Basu category:cs.CV published:2010-03-31 summary:Integrated Traffic Management Systems (ITMS) are now implemented in differentcities in India to primarily address the concerns of road-safety and security.An automated Red Light Violation Detection System (RLVDS) is an integral partof the ITMS. In our present work we have designed and developed a completesystem for generating the list of all stop-line violating vehicle imagesautomatically from video snapshots of road-side surveillance cameras. Thesystem first generates adaptive background images for each camera view,subtracts captured images from the corresponding background images and analysespotential occlusions over the stop-line in a traffic signal. Consideringround-the-clock operations in a real-life test environment, the developedsystem could successfully track 92% images of vehicles with violations on thestop-line in a "Red" traffic signal.
arxiv-300-266 | Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization | http://arxiv.org/pdf/1003.3967v4.pdf | author:Daniel Golovin, Andreas Krause category:cs.LG cs.AI cs.DS published:2010-03-21 summary:Solving stochastic optimization problems under partial observability, whereone needs to adaptively make decisions with uncertain outcomes, is afundamental but notoriously difficult challenge. In this paper, we introducethe concept of adaptive submodularity, generalizing submodular set functions toadaptive policies. We prove that if a problem satisfies this property, a simpleadaptive greedy algorithm is guaranteed to be competitive with the optimalpolicy. In addition to providing performance guarantees for both stochasticmaximization and coverage, adaptive submodularity can be exploited todrastically speed up the greedy algorithm by using lazy evaluations. Weillustrate the usefulness of the concept by giving several examples of adaptivesubmodular objectives arising in diverse applications including sensorplacement, viral marketing and active learning. Proving adaptive submodularityfor these problems allows us to recover existing results in these applicationsas special cases, improve approximation guarantees and handle naturalgeneralizations.
arxiv-300-267 | Local Space-Time Smoothing for Version Controlled Documents | http://arxiv.org/pdf/1003.1410v2.pdf | author:Seungyeon Kim, Guy Lebanon category:cs.GR cs.CL cs.LG published:2010-03-06 summary:Unlike static documents, version controlled documents are continuously editedby one or more authors. Such collaborative revision process makes traditionalmodeling and visualization techniques inappropriate. In this paper we propose anew representation based on local space-time smoothing that captures importantrevision patterns. We demonstrate the applicability of our framework usingexperiments on synthetic and real-world data.
arxiv-300-268 | An Offline Technique for Localization of License Plates for Indian Commercial Vehicles | http://arxiv.org/pdf/1003.1072v2.pdf | author:Satadal Saha, Subhadip Basu, Mita Nasipuri, Dipak Kumar Basu category:cs.CV published:2010-03-04 summary:Automatic License Plate Recognition (ALPR) is a challenging area of researchdue to its importance to variety of commercial applications. The overallproblem may be subdivided into two key modules, firstly, localization oflicense plates from vehicle images, and secondly, optical character recognitionof extracted license plates. In the current work, we have concentrated on thefirst part of the problem, i.e., localization of license plate regions fromIndian commercial vehicles as a significant step towards development of acomplete ALPR system for Indian vehicles. The technique is based on color basedsegmentation of vehicle images and identification of potential license plateregions. True license plates are finally localized based on four spatial andhorizontal contrast features. The technique successfully localizes the actuallicense plates in 73.4% images.
arxiv-300-269 | A Complete Characterization of Statistical Query Learning with Applications to Evolvability | http://arxiv.org/pdf/1002.3183v3.pdf | author:Vitaly Feldman category:cs.CC cs.LG published:2010-02-16 summary:Statistical query (SQ) learning model of Kearns (1993) is a naturalrestriction of the PAC learning model in which a learning algorithm is allowedto obtain estimates of statistical properties of the examples but cannot seethe examples themselves. We describe a new and simple characterization of thequery complexity of learning in the SQ learning model. Unlike the previouslyknown bounds on SQ learning our characterization preserves the accuracy and theefficiency of learning. The preservation of accuracy implies that that ourcharacterization gives the first characterization of SQ learning in theagnostic learning framework. The preservation of efficiency is achieved using anew boosting technique and allows us to derive a new approach to the design ofevolutionary algorithms in Valiant's (2006) model of evolvability. We use thisapproach to demonstrate the existence of a large class of monotone evolutionarylearning algorithms based on square loss performance estimation. These resultsdiffer significantly from the few known evolutionary algorithms and giveevidence that evolvability in Valiant's model is a more versatile phenomenonthan there had been previous reason to suspect.
arxiv-300-270 | Probabilistic Recovery of Multiple Subspaces in Point Clouds by Geometric lp Minimization | http://arxiv.org/pdf/1002.1994v3.pdf | author:Gilad Lerman, Teng Zhang category:stat.ML published:2010-02-09 summary:We assume data independently sampled from a mixture distribution on the unitball of the D-dimensional Euclidean space with K+1 components: the firstcomponent is a uniform distribution on that ball representing outliers and theother K components are uniform distributions along K d-dimensional linearsubspaces restricted to that ball. We study both the simultaneous recovery ofall K underlying subspaces and the recovery of the best l0 subspace (i.e., withlargest number of points) by minimizing the lp-averaged distances of datapoints from d-dimensional subspaces of the D-dimensional space. Unlike other lpminimization problems, this minimization is non-convex for all p>0 and thusrequires different methods for its analysis. We show that if 0<p <= 1, thenboth all underlying subspaces and the best l0 subspace can be preciselyrecovered by lp minimization with overwhelming probability. This result extendsto additive homoscedastic uniform noise around the subspaces (i.e., uniformdistribution in a strip around them) and near recovery with an errorproportional to the noise level. On the other hand, if K>1 and p>1, then weshow that both all underlying subspaces and the best l0 subspace cannot berecovered and even nearly recovered. Further relaxations are also discussed. Weuse the results of this paper for partially justifying recent effectivealgorithms for modeling data by mixtures of multiple subspaces as well as fordiscussing the effect of using variants of lp minimizations in RANSAC-typestrategies for single subspace recovery.
arxiv-300-271 | An Unsupervised Algorithm For Learning Lie Group Transformations | http://arxiv.org/pdf/1001.1027v4.pdf | author:Jascha Sohl-Dickstein, Ching Ming Wang, Bruno A. Olshausen category:cs.CV cs.LG published:2010-01-07 summary:We present several theoretical contributions which allow Lie groups to be fitto high dimensional datasets. Transformation operators are represented in theireigen-basis, reducing the computational complexity of parameter estimation tothat of training a linear transformation model. A transformation specific"blurring" operator is introduced that allows inference to escape local minimavia a smoothing of the transformation space. A penalty on traversed manifolddistance is added which encourages the discovery of sparse, minimal distance,transformations between states. Both learning and inference are demonstratedusing these methods for the full set of affine transformations on natural imagepatches. Transformation operators are then trained on natural video sequences.It is shown that the learned video transformations provide a better descriptionof inter-frame differences than the standard motion model based on rigidtranslation.
arxiv-300-272 | Ranking relations using analogies in biological and information networks | http://arxiv.org/pdf/0912.5193v3.pdf | author:Ricardo Silva, Katherine Heller, Zoubin Ghahramani, Edoardo M. Airoldi category:stat.ME cs.LG physics.soc-ph q-bio.QM stat.AP published:2009-12-28 summary:Analogical reasoning depends fundamentally on the ability to learn andgeneralize about relations between objects. We develop an approach torelational learning which, given a set of pairs of objects$\mathbf{S}=\{A^{(1)}:B^{(1)},A^{(2)}:B^{(2)},\ldots,A^{(N)}:B ^{(N)}\}$,measures how well other pairs A:B fit in with the set $\mathbf{S}$. Our workaddresses the following question: is the relation between objects A and Banalogous to those relations found in $\mathbf{S}$? Such questions areparticularly relevant in information retrieval, where an investigator mightwant to search for analogous pairs of objects that match the query set ofinterest. There are many ways in which objects can be related, making the taskof measuring analogies very challenging. Our approach combines a similaritymeasure on function spaces with Bayesian analysis to produce a ranking. Itrequires data containing features of the objects of interest and a link matrixspecifying which relationships exist; no further attributes of suchrelationships are necessary. We illustrate the potential of our method on textanalysis and information networks. An application on discovering functionalinteractions between pairs of proteins is discussed in detail, where we showthat our approach can work in practice even if a small set of protein pairs isprovided.
arxiv-300-273 | An Invariance Principle for Polytopes | http://arxiv.org/pdf/0912.4884v2.pdf | author:Prahladh Harsha, Adam Klivans, Raghu Meka category:cs.CC cs.CG cs.DM cs.LG math.PR published:2009-12-24 summary:Let X be randomly chosen from {-1,1}^n, and let Y be randomly chosen from thestandard spherical Gaussian on R^n. For any (possibly unbounded) polytope Pformed by the intersection of k halfspaces, we prove that Pr [X belongs to P] - Pr [Y belongs to P] < log^{8/5}k * Delta, where Deltais a parameter that is small for polytopes formed by the intersection of"regular" halfspaces (i.e., halfspaces with low influence). The novelty of ourinvariance principle is the polylogarithmic dependence on k. Previously, onlybounds that were at least linear in k were known. We give two importantapplications of our main result: (1) A polylogarithmic in k bound on theBoolean noise sensitivity of intersections of k "regular" halfspaces (previouswork gave bounds linear in k). (2) A pseudorandom generator (PRG) with seedlength O((log n)*poly(log k,1/delta)) that delta-fools all polytopes with kfaces with respect to the Gaussian distribution. We also obtain PRGs withsimilar parameters that fool polytopes formed by intersection of regularhalfspaces over the hypercube. Using our PRG constructions, we obtain the firstdeterministic quasi-polynomial time algorithms for approximately counting thenumber of solutions to a broad class of integer programs, including densecovering problems and contingency tables.
arxiv-300-274 | Geometric Representations of Random Hypergraphs | http://arxiv.org/pdf/0912.3648v3.pdf | author:Simón Lunagómez, Sayan Mukherjee, Robert L. Wolpert, Edoardo M. Airoldi category:math.ST math.PR stat.ML stat.TH 60K35 published:2009-12-18 summary:A parametrization of hypergraphs based on the geometry of points in$\mathbf{R}^d$ is developed. Informative prior distributions on hypergraphs areinduced through this parametrization by priors on point configurations viaspatial processes. This prior specification is used to infer conditionalindependence models or Markov structure of multivariate distributions.Specifically, we can recover both the junction tree factorization as well asthe hyper Markov law. This approach offers greater control on the distributionof graph features than Erd\"os-R\'enyi random graphs, supports inference offactorizations that cannot be retrieved by a graph alone, and leads to newMetropolis\slash Hastings Markov chain Monte Carlo algorithms with both localand global moves in graph space. We illustrate the utility of thisparametrization and prior specification using simulations.
arxiv-300-275 | Automated languages phylogeny from Levenshtein distance | http://arxiv.org/pdf/0911.3280v7.pdf | author:Maurizio Serva category:cs.CL q-bio.PE q-bio.QM published:2009-11-17 summary:Languages evolve over time in a process in which reproduction, mutation andextinction are all possible, similar to what happens to living organisms. Usingthis similarity it is possible, in principle, to build family trees which showthe degree of relatedness between languages. The method used by modern glottochronology, developed by Swadesh in the1950s, measures distances from the percentage of words with a common historicalorigin. The weak point of this method is that subjective judgment plays arelevant role. Recently we proposed an automated method that avoids the subjectivity, whoseresults can be replicated by studies that use the same database and thatdoesn't require a specific linguistic knowledge. Moreover, the method allows aquick comparison of a large number of languages. We applied our method to the Indo-European and Austronesian families,considering in both cases, fifty different languages. The resulting trees aresimilar to those of previous studies, but with some important differences inthe position of few languages and subgroups. We believe that these differencescarry new information on the structure of the tree and on the phylogeneticrelationships within families.
arxiv-300-276 | A Dynamic Near-Optimal Algorithm for Online Linear Programming | http://arxiv.org/pdf/0911.2974v3.pdf | author:Shipra Agrawal, Zizhuo Wang, Yinyu Ye category:cs.DS cs.LG published:2009-11-16 summary:A natural optimization model that formulates many online resource allocationand revenue management problems is the online linear program (LP) in which theconstraint matrix is revealed column by column along with the correspondingobjective coefficient. In such a model, a decision variable has to be set eachtime a column is revealed without observing the future inputs and the goal isto maximize the overall objective function. In this paper, we provide anear-optimal algorithm for this general class of online problems under theassumption of random order of arrival and some mild conditions on the size ofthe LP right-hand-side input. Specifically, our learning-based algorithm worksby dynamically updating a threshold price vector at geometric time intervals,where the dual prices learned from the revealed columns in the previous periodare used to determine the sequential decisions in the current period. Due tothe feature of dynamic learning, the competitiveness of our algorithm improvesover the past study of the same problem. We also present a worst-case exampleshowing that the performance of our algorithm is near-optimal.
arxiv-300-277 | Kullback-Leibler aggregation and misspecified generalized linear models | http://arxiv.org/pdf/0911.2919v5.pdf | author:Philippe Rigollet category:stat.ML math.ST stat.TH published:2009-11-16 summary:In a regression setup with deterministic design, we study the pureaggregation problem and introduce a natural extension from the Gaussiandistribution to distributions in the exponential family. While this extensionbears strong connections with generalized linear models, it does not requireidentifiability of the parameter or even that the model on the systematiccomponent is true. It is shown that this problem can be solved by constrainedand/or penalized likelihood maximization and we derive sharp oracleinequalities that hold both in expectation and with high probability. Finallyall the bounds are proved to be optimal in a minimax sense.
arxiv-300-278 | Learning Exponential Families in High-Dimensions: Strong Convexity and Sparsity | http://arxiv.org/pdf/0911.0054v2.pdf | author:Sham M. Kakade, Ohad Shamir, Karthik Sridharan, Ambuj Tewari category:cs.LG stat.ML I.2.6 published:2009-10-31 summary:The versatility of exponential families, along with their attendant convexityproperties, make them a popular and effective statistical model. A centralissue is learning these models in high-dimensions, such as when there is somesparsity pattern of the optimal parameter. This work characterizes a certainstrong convexity property of general exponential families, which allow theirgeneralization ability to be quantified. In particular, we show how thisproperty can be used to analyze generic exponential families under L_1regularization.
arxiv-300-279 | The Geometry of Generalized Binary Search | http://arxiv.org/pdf/0910.4397v5.pdf | author:Robert D. Nowak category:stat.ML cs.IT math.IT math.ST stat.TH published:2009-10-22 summary:This paper investigates the problem of determining a binary-valued functionthrough a sequence of strategically selected queries. The focus is an algorithmcalled Generalized Binary Search (GBS). GBS is a well-known greedy algorithmfor determining a binary-valued function through a sequence of strategicallyselected queries. At each step, a query is selected that most evenly splits thehypotheses under consideration into two disjoint subsets, a naturalgeneralization of the idea underlying classic binary search. This paperdevelops novel incoherence and geometric conditions under which GBS achievesthe information-theoretically optimal query complexity; i.e., given acollection of N hypotheses, GBS terminates with the correct function after nomore than a constant times log N queries. Furthermore, a noise-tolerant versionof GBS is developed that also achieves the optimal query complexity. Theseresults are applied to learning halfspaces, a problem arising routinely inimage processing and machine learning.
arxiv-300-280 | Fractional differentiation based image processing | http://arxiv.org/pdf/0910.2381v4.pdf | author:Amelia Carolina Sparavigna category:cs.CV published:2009-10-13 summary:There are many resources useful for processing images, most of them freelyavailable and quite friendly to use. In spite of this abundance of tools, astudy of the processing methods is still worthy of efforts. Here, we want todiscuss the possibilities arising from the use of fractional differentialcalculus. This calculus evolved in the research field of pure mathematics until1920, when applied science started to use it. Only recently, fractionalcalculus was involved in image processing methods. As we shall see, thefractional calculation is able to enhance the quality of images, withinteresting possibilities in edge detection and image restoration. We suggestalso the fractional differentiation as a tool to reveal faint objects inastronomical images.
arxiv-300-281 | Sparsity and `Something Else': An Approach to Encrypted Image Folding | http://arxiv.org/pdf/0909.2017v5.pdf | author:James Bowley, Laura Rebollo-Neira category:cs.CV cs.IT math.IT published:2009-09-10 summary:A property of sparse representations in relation to their capacity forinformation storage is discussed. It is shown that this feature can be used foran application that we term Encrypted Image Folding. The proposed procedure isrealizable through any suitable transformation. In particular, in this paper weillustrate the approach by recourse to the Discrete Cosine Transform and acombination of redundant Cosine and Dirac dictionaries. The main advantage ofthe proposed technique is that both storage and encryption can be achievedsimultaneously using simple processing steps.
arxiv-300-282 | Tree-guided group lasso for multi-response regression with structured sparsity, with an application to eQTL mapping | http://arxiv.org/pdf/0909.1373v3.pdf | author:Seyoung Kim, Eric P. Xing category:stat.ML q-bio.GN q-bio.QM stat.AP stat.ME published:2009-09-08 summary:We consider the problem of estimating a sparse multi-response regressionfunction, with an application to expression quantitative trait locus (eQTL)mapping, where the goal is to discover genetic variations that influencegene-expression levels. In particular, we investigate a shrinkage techniquecapable of capturing a given hierarchical structure over the responses, such asa hierarchical clustering tree with leaf nodes for responses and internal nodesfor clusters of related responses at multiple granularity, and we seek toleverage this structure to recover covariates relevant to eachhierarchically-defined cluster of responses. We propose a tree-guided grouplasso, or tree lasso, for estimating such structured sparsity undermulti-response regression by employing a novel penalty function constructedfrom the tree. We describe a systematic weighting scheme for the overlappinggroups in the tree-penalty such that each regression coefficient is penalizedin a balanced manner despite the inhomogeneous multiplicity of groupmemberships of the regression coefficients due to overlaps among groups. Forefficient optimization, we employ a smoothing proximal gradient method that wasoriginally developed for a general class of structured-sparsity-inducingpenalties. Using simulated and yeast data sets, we demonstrate that our methodshows a superior performance in terms of both prediction errors and recovery oftrue sparsity patterns, compared to other methods for learning amultivariate-response regression.
arxiv-300-283 | Efficient algorithms for training the parameters of hidden Markov models using stochastic expectation maximization EM training and Viterbi training | http://arxiv.org/pdf/0909.0737v2.pdf | author:Tin Yin Lam, Irmtraud M. Meyer category:q-bio.QM cs.LG q-bio.GN published:2009-09-03 summary:Background: Hidden Markov models are widely employed by numerousbioinformatics programs used today. Applications range widely from comparativegene prediction to time-series analyses of micro-array data. The parameters ofthe underlying models need to be adjusted for specific data sets, for examplethe genome of a particular species, in order to maximize the predictionaccuracy. Computationally efficient algorithms for parameter training are thuskey to maximizing the usability of a wide range of bioinformatics applications. Results: We introduce two computationally efficient training algorithms, onefor Viterbi training and one for stochastic expectation maximization (EM)training, which render the memory requirements independent of the sequencelength. Unlike the existing algorithms for Viterbi and stochastic EM trainingwhich require a two-step procedure, our two new algorithms require only onestep and scan the input sequence in only one direction. We also implement thesetwo new algorithms and the already published linear-memory algorithm for EMtraining into the hidden Markov model compiler HMM-Converter and examine theirrespective practical merits for three small example models. Conclusions: Bioinformatics applications employing hidden Markov models canuse the two algorithms in order to make Viterbi training and stochastic EMtraining more computationally efficient. Using these algorithms, parametertraining can thus be attempted for more complex models and longer trainingsequences. The two new algorithms have the added advantage of being easier toimplement than the corresponding default algorithms for Viterbi training andstochastic EM training.
arxiv-300-284 | Another Look at Quantum Neural Computing | http://arxiv.org/pdf/0908.3148v2.pdf | author:Subhash Kak category:cs.NE cs.AI published:2009-08-21 summary:The term quantum neural computing indicates a unity in the functioning of thebrain. It assumes that the neural structures perform classical processing andthat the virtual particles associated with the dynamical states of thestructures define the underlying quantum state. We revisit the concept and alsosummarize new arguments related to the learning modes of the brain in responseto sensory input that may be aggregated in three types: associative,reorganizational, and quantum. The associative and reorganizational types arequite apparent based on experimental findings; it is much harder to establishthat the brain as an entity exhibits quantum properties. We argue that thereorganizational behavior of the brain may be viewed as inner adjustmentcorresponding to its quantum behavior at the system level. Not only neuralstructures but their higher abstractions also may be seen as whole entities. Weconsider the dualities associated with the behavior of the brain and how thesedualities are bridged.
arxiv-300-285 | Contextual Bandits with Similarity Information | http://arxiv.org/pdf/0907.3986v5.pdf | author:Aleksandrs Slivkins category:cs.DS cs.LG F.2.2; F.1.2 published:2009-07-23 summary:In a multi-armed bandit (MAB) problem, an online algorithm makes a sequenceof choices. In each round it chooses from a time-invariant set of alternativesand receives the payoff associated with this alternative. While the case ofsmall strategy sets is by now well-understood, a lot of recent work has focusedon MAB problems with exponentially or infinitely large strategy sets, where oneneeds to assume extra structure in order to make the problem tractable. Inparticular, recent literature considered information on similarity betweenarms. We consider similarity information in the setting of "contextual bandits", anatural extension of the basic MAB problem where before each round an algorithmis given the "context" -- a hint about the payoffs in this round. Contextualbandits are directly motivated by placing advertisements on webpages, one ofthe crucial problems in sponsored search. A particularly simple way torepresent similarity information in the contextual bandit setting is via a"similarity distance" between the context-arm pairs which gives an upper boundon the difference between the respective expected payoffs. Prior work on contextual bandits with similarity uses "uniform" partitions ofthe similarity space, which is potentially wasteful. We design more efficientalgorithms that are based on adaptive partitions adjusted to "popular" contextand "high-payoff" arms.
arxiv-300-286 | Sparsistent Estimation of Time-Varying Discrete Markov Random Fields | http://arxiv.org/pdf/0907.2337v2.pdf | author:Mladen Kolar, Eric P. Xing category:stat.ML published:2009-07-14 summary:Network models have been popular for modeling and representing complexrelationships and dependencies between observed variables. When data comes froma dynamic stochastic process, a single static network model cannot adequatelycapture transient dependencies, such as, gene regulatory dependenciesthroughout a developmental cycle of an organism. Kolar et al (2010b) proposed amethod based on kernel-smoothing l1-penalized logistic regression forestimating time-varying networks from nodal observations collected from atime-series of observational data. In this paper, we establish conditions underwhich the proposed method consistently recovers the structure of a time-varyingnetwork. This work complements previous empirical findings by providing soundtheoretical guarantees for the proposed estimation procedure. For completeness,we include numerical simulations in the paper.
arxiv-300-287 | Matrix Completion from Noisy Entries | http://arxiv.org/pdf/0906.2027v2.pdf | author:Raghunandan H. Keshavan, Andrea Montanari, Sewoong Oh category:cs.LG stat.ML published:2009-06-11 summary:Given a matrix M of low-rank, we consider the problem of reconstructing itfrom noisy observations of a small, random subset of its entries. The problemarises in a variety of applications, from collaborative filtering (the `Netflixproblem') to structure-from-motion and positioning. We study a low complexityalgorithm introduced by Keshavan et al.(2009), based on a combination ofspectral techniques and manifold optimization, that we call here OptSpace. Weprove performance guarantees that are order-optimal in a number ofcircumstances.
arxiv-300-288 | Sure independence screening in generalized linear models with NP-dimensionality | http://arxiv.org/pdf/0903.5255v5.pdf | author:Jianqing Fan, Rui Song category:stat.ME math.ST stat.ML stat.TH published:2009-03-30 summary:Ultrahigh-dimensional variable selection plays an increasingly important rolein contemporary scientific discoveries and statistical research. Among others,Fan and Lv [J. R. Stat. Soc. Ser. B Stat. Methodol. 70 (2008) 849-911] proposean independent screening framework by ranking the marginal correlations. Theyshowed that the correlation ranking procedure possesses a sure independencescreening property within the context of the linear model with Gaussiancovariates and responses. In this paper, we propose a more general version ofthe independent learning with ranking the maximum marginal likelihood estimatesor the maximum marginal likelihood itself in generalized linear models. We showthat the proposed methods, with Fan and Lv [J. R. Stat. Soc. Ser. B Stat.Methodol. 70 (2008) 849-911] as a very special case, also possess the surescreening property with vanishing false selection rate. The conditions underwhich the independence learning possesses a sure screening is surprisinglysimple. This justifies the applicability of such a simple method in a widespectrum. We quantify explicitly the extent to which the dimensionality can bereduced by independence screening, which depends on the interactions of thecovariance matrix of covariates and true parameters. Simulation studies areused to illustrate the utility of the proposed approaches. In addition, weestablish an exponential inequality for the quasi-maximum likelihood estimatorwhich is useful for high-dimensional statistical learning.
arxiv-300-289 | An Exponential Lower Bound on the Complexity of Regularization Paths | http://arxiv.org/pdf/0903.4817v3.pdf | author:Bernd Gärtner, Martin Jaggi, Clément Maria category:cs.LG cs.CG cs.CV math.OC stat.ML 90C20 F.2.2; I.5.1 published:2009-03-27 summary:For a variety of regularized optimization problems in machine learning,algorithms computing the entire solution path have been developed recently.Most of these methods are quadratic programs that are parameterized by a singleparameter, as for example the Support Vector Machine (SVM). Solution pathalgorithms do not only compute the solution for one particular value of theregularization parameter but the entire path of solutions, making the selectionof an optimal parameter much easier. It has been assumed that these piecewise linear solution paths have onlylinear complexity, i.e. linearly many bends. We prove that for the supportvector machine this complexity can be exponential in the number of trainingpoints in the worst case. More strongly, we construct a single instance of ninput points in d dimensions for an SVM such that at least \Theta(2^{n/2}) =\Theta(2^d) many distinct subsets of support vectors occur as theregularization parameter changes.
arxiv-300-290 | Differential Contrastive Divergence | http://arxiv.org/pdf/0903.2299v3.pdf | author:David McAllester category:cs.LG published:2009-03-13 summary:This paper has been retracted.
arxiv-300-291 | Google distance between words | http://arxiv.org/pdf/0901.4180v2.pdf | author:Bjørn Kjos-Hanssen, Alberto J. Evangelista category:cs.CL I.2.7 published:2009-01-27 summary:Cilibrasi and Vitanyi have demonstrated that it is possible to extract themeaning of words from the world-wide web. To achieve this, they rely on thenumber of webpages that are found through a Google search containing a givenword and they associate the page count to the probability that the word appearson a webpage. Thus, conditional probabilities allow them to correlate one wordwith another word's meaning. Furthermore, they have developed a similaritydistance function that gauges how closely related a pair of words is. Wepresent a specific counterexample to the triangle inequality for thissimilarity distance function.
arxiv-300-292 | The Offset Tree for Learning with Partial Labels | http://arxiv.org/pdf/0812.4044v3.pdf | author:Alina Beygelzimer, John Langford category:cs.LG cs.AI published:2008-12-21 summary:We present an algorithm, called the Offset Tree, for learning to makedecisions in situations where the payoff of only one choice is observed, ratherthan all choices. The algorithm reduces this setting to binary classification,allowing one to reuse of any existing, fully supervised binary classificationalgorithm in this partial information setting. We show that the Offset Tree isan optimal reduction to binary classification. In particular, it has regret atmost $(k-1)$ times the regret of the binary classifier it uses (where $k$ isthe number of choices), and no reduction to binary classification can dobetter. This reduction is also computationally optimal, both at training andtest time, requiring just $O(\log_2 k)$ work to train on an example or make aprediction. Experiments with the Offset Tree show that it generally performs better thanseveral alternative approaches.
arxiv-300-293 | Characterizing Truthful Multi-Armed Bandit Mechanisms | http://arxiv.org/pdf/0812.2291v7.pdf | author:Moshe Babaioff, Yogeshwer Sharma, Aleksandrs Slivkins category:cs.DS cs.GT cs.LG published:2008-12-12 summary:We consider a multi-round auction setting motivated by pay-per-click auctionsfor Internet advertising. In each round the auctioneer selects an advertiserand shows her ad, which is then either clicked or not. An advertiser derivesvalue from clicks; the value of a click is her private information. Initially,neither the auctioneer nor the advertisers have any information about thelikelihood of clicks on the advertisements. The auctioneer's goal is to designa (dominant strategies) truthful mechanism that (approximately) maximizes thesocial welfare. If the advertisers bid their true private values, our problem is equivalentto the "multi-armed bandit problem", and thus can be viewed as a strategicversion of the latter. In particular, for both problems the quality of analgorithm can be characterized by "regret", the difference in social welfarebetween the algorithm and the benchmark which always selects the same "best"advertisement. We investigate how the design of multi-armed bandit algorithmsis affected by the restriction that the resulting mechanism must be truthful.We find that truthful mechanisms have certain strong structural properties --essentially, they must separate exploration from exploitation -- and they incurmuch higher regret than the optimal multi-armed bandit algorithms. Moreover, weprovide a truthful mechanism which (essentially) matches our lower bound onregret.
arxiv-300-294 | A Spectral Algorithm for Learning Hidden Markov Models | http://arxiv.org/pdf/0811.4413v6.pdf | author:Daniel Hsu, Sham M. Kakade, Tong Zhang category:cs.LG cs.AI published:2008-11-26 summary:Hidden Markov Models (HMMs) are one of the most fundamental and widely usedstatistical tools for modeling discrete time series. In general, learning HMMsfrom data is computationally hard (under cryptographic assumptions), andpractitioners typically resort to search heuristics which suffer from the usuallocal optima issues. We prove that under a natural separation condition (boundson the smallest singular value of the HMM parameters), there is an efficientand provably correct algorithm for learning HMMs. The sample complexity of thealgorithm does not explicitly depend on the number of distinct (discrete)observations---it implicitly depends on this quantity through spectralproperties of the underlying HMM. This makes the algorithm particularlyapplicable to settings with a large number of observations, such as those innatural language processing where the space of observation is sometimes thewords in a language. The algorithm is also simple, employing only a singularvalue decomposition and matrix multiplications.
arxiv-300-295 | Corpus sp{é}cialis{é} et ressource de sp{é}cialit{é} | http://arxiv.org/pdf/0801.1179v2.pdf | author:Bernard Jacquemin, Sabine Ploux category:cs.IR cs.CL published:2008-01-08 summary:"Semantic Atlas" is a mathematic and statistic model to visualise word sensesaccording to relations between words. The model, that has been applied toproximity relations from a corpus, has shown its ability to distinguish wordsenses as the corpus' contributors comprehend them. We propose to use the modeland a specialised corpus in order to create automatically a specialiseddictionary relative to the corpus' domain. A morpho-syntactic analysisperformed on the corpus makes it possible to create the dictionary fromsyntactic relations between lexical units. The semantic resource can be used tonavigate semantically - and not only lexically - through the corpus, to createclassical dictionaries or for diachronic studies of the language.
arxiv-300-296 | Creating a Digital Ecosystem: Service-Oriented Architectures with Distributed Evolutionary Computing | http://arxiv.org/pdf/0712.4159v5.pdf | author:G Briscoe category:cs.NE published:2007-12-26 summary:We start with a discussion of the relevant literature, including NatureInspired Computing as a framework in which to understand this work, and theprocess of biomimicry to be used in mimicking the necessary biologicalprocesses to create Digital Ecosystems. We then consider the relevanttheoretical ecology in creating the digital counterpart of a biologicalecosystem, including the topological structure of ecosystems, and evolutionaryprocesses within distributed environments. This leads to a discussion of therelevant fields from computer science for the creation of Digital Ecosystems,including evolutionary computing, Multi-Agent Systems, and Service-OrientedArchitectures. We then define Ecosystem-Oriented Architectures for the creationof Digital Ecosystems, imbibed with the properties of self-organisation andscalability from biological ecosystems, including a novel form of distributedevolutionary computing.
arxiv-300-297 | Preconditioned Temporal Difference Learning | http://arxiv.org/pdf/0704.1409v3.pdf | author:Yao HengShuai category:cs.LG cs.AI published:2007-04-11 summary:This paper has been withdrawn by the author. This draft is withdrawn for itspoor quality in english, unfortunately produced by the author when he was juststarting his science route. Look at the ICML version instead:http://icml2008.cs.helsinki.fi/papers/111.pdf
arxiv-300-298 | One-Pass, One-Hash n-Gram Statistics Estimation | http://arxiv.org/pdf/cs/0610010v4.pdf | author:Daniel Lemire, Owen Kaser category:cs.DB cs.CL published:2006-10-03 summary:In multimedia, text or bioinformatics databases, applications query sequencesof n consecutive symbols called n-grams. Estimating the number of distinctn-grams is a view-size estimation problem. While view sizes can be estimated bysampling under statistical assumptions, we desire an unassuming algorithm withuniversally valid accuracy bounds. Most related work has focused on repeatedlyhashing the data, which is prohibitive for large data sources. We prove that aone-pass one-hash algorithm is sufficient for accurate estimates if the hashingis sufficiently independent. To reduce costs further, we investigate recursiverandom hashing algorithms and show that they are sufficiently independent inpractice. We compare our running times with exact counts using suffix arraysand show that, while we use hardly any storage, we are an order of magnitudefaster. The approach further is extended to a one-pass/one-hash computation ofn-gram entropy and iceberg counts. The experiments use a large collection ofEnglish text from the Gutenberg Project as well as synthetic data.
arxiv-300-299 | Get out the vote: Determining support or opposition from Congressional floor-debate transcripts | http://arxiv.org/pdf/cs/0607062v3.pdf | author:Matt Thomas, Bo Pang, Lillian Lee category:cs.CL cs.SI physics.soc-ph I.2.7 published:2006-07-12 summary:We investigate whether one can determine from the transcripts of U.S.Congressional floor debates whether the speeches represent support of oropposition to proposed legislation. To address this problem, we exploit thefact that these speeches occur as part of a discussion; this allows us to usesources of information regarding relationships between discourse segments, suchas whether a given utterance indicates agreement with the opinion expressed byanother. We find that the incorporation of such information yields substantialimprovements over classifying speeches in isolation.
arxiv-300-300 | The Mysterious Optimality of Naive Bayes: Estimation of the Probability in the System of "Classifiers" | http://arxiv.org/pdf/cs/0202020v3.pdf | author:Oleg Kupervasser, Alexsander Vardy category:cs.CV cs.AI published:2002-02-17 summary:Bayes Classifiers are widely used currently for recognition, identificationand knowledge discovery. The fields of application are, for example, imageprocessing, medicine, chemistry (QSAR). But by mysterious way the Naive BayesClassifier usually gives a very nice and good presentation of a recognition. Itcan not be improved considerably by more complex models of Bayes Classifier. Wedemonstrate here a very nice and simple proof of the Naive Bayes Classifieroptimality, that can explain this interesting fact.The derivation in thecurrent paper is based on arXiv:cs/0202020v1
