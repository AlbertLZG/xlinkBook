arxiv-6300-1 | A swarm optimization algorithm inspired in the behavior of the social-spider | http://arxiv.org/pdf/1406.3282v1.pdf | author:Erik Cuevas, Miguel Cienfuegos, Daniel Zaldivar, Marco Perez category:cs.NE published:2014-06-12 summary:Swarm intelligence is a research field that models the collective behavior inswarms of insects or animals. Several algorithms arising from such models havebeen proposed to solve a wide range of complex optimization problems. In thispaper, a novel swarm algorithm called the Social Spider Optimization (SSO) isproposed for solving optimization tasks. The SSO algorithm is based on thesimulation of cooperative behavior of social-spiders. In the proposedalgorithm, individuals emulate a group of spiders which interact to each otherbased on the biological laws of the cooperative colony. The algorithm considerstwo different search agents (spiders): males and females. Depending on gender,each individual is conducted by a set of different evolutionary operators whichmimic different cooperative behaviors that are typically found in the colony.In order to illustrate the proficiency and robustness of the proposed approach,it is compared to other well-known evolutionary methods. The comparisonexamines several standard benchmark functions that are commonly consideredwithin the literature of evolutionary algorithms. The outcome shows a highperformance of the proposed method for searching a global optimum with severalbenchmark functions.
arxiv-6300-2 | Approximate Decentralized Bayesian Inference | http://arxiv.org/pdf/1403.7471v3.pdf | author:Trevor Campbell, Jonathan P. How category:cs.LG published:2014-03-28 summary:This paper presents an approximate method for performing Bayesian inferencein models with conditional independence over a decentralized network oflearning agents. The method first employs variational inference on eachindividual learning agent to generate a local approximate posterior, the agentstransmit their local posteriors to other agents in the network, and finallyeach agent combines its set of received local posteriors. The key insight inthis work is that, for many Bayesian models, approximate inference schemesdestroy symmetry and dependencies in the model that are crucial to the correctapplication of Bayes' rule when combining the local posteriors. The proposedmethod addresses this issue by including an additional optimization step in thecombination procedure that accounts for these broken dependencies. Experimentson synthetic and real data demonstrate that the decentralized method providesadvantages in computational performance and predictive test likelihood overprevious batch and distributed methods.
arxiv-6300-3 | A hybrid neuro--wavelet predictor for QoS control and stability | http://arxiv.org/pdf/1406.3156v1.pdf | author:Christian Napoli, Giuseppe Pappalardo, Emiliano Tramontana category:cs.NE cs.DC cs.NI cs.PF cs.SY 68T05 published:2014-06-12 summary:For distributed systems to properly react to peaks of requests, theiradaptation activities would benefit from the estimation of the amount ofrequests. This paper proposes a solution to produce a short-term forecast basedon data characterising user behaviour of online services. We use \emph{waveletanalysis}, providing compression and denoising on the observed time series ofthe amount of past user requests; and a \emph{recurrent neural network} trainedwith observed data and designed so as to provide well-timed estimations offuture requests. The said ensemble has the ability to predict the amount offuture user requests with a root mean squared error below 0.06\%. Thanks toprediction, advance resource provision can be performed for the duration of arequest peak and for just the right amount of resources, hence avoidingover-provisioning and associated costs. Moreover, reliable provision lets usersenjoy a level of availability of services unaffected by load variations.
arxiv-6300-4 | A Cascade Neural Network Architecture investigating Surface Plasmon Polaritons propagation for thin metals in OpenMP | http://arxiv.org/pdf/1406.3149v1.pdf | author:Francesco Bonanno, Giacomo Capizzi, Grazia Lo Sciuto, Christian Napoli, Giuseppe Pappalardo, Emiliano Tramontana category:cs.NE cs.DC cs.LG 68T05 published:2014-06-12 summary:Surface plasmon polaritons (SPPs) confined along metal-dielectric interfacehave attracted a relevant interest in the area of ultracompact photoniccircuits, photovoltaic devices and other applications due to their strong fieldconfinement and enhancement. This paper investigates a novel cascade neuralnetwork (NN) architecture to find the dependance of metal thickness on the SPPpropagation. Additionally, a novel training procedure for the proposed cascadeNN has been developed using an OpenMP-based framework, thus greatly reducingtraining time. The performed experiments confirm the effectiveness of theproposed NN architecture for the problem at hand.
arxiv-6300-5 | Expressive Power and Approximation Errors of Restricted Boltzmann Machines | http://arxiv.org/pdf/1406.3140v1.pdf | author:Guido Montufar, Johannes Rauh, Nihat Ay category:stat.ML math.PR 82C32, 68Q99 published:2014-06-12 summary:We present explicit classes of probability distributions that can be learnedby Restricted Boltzmann Machines (RBMs) depending on the number of units thatthey contain, and which are representative for the expressive power of themodel. We use this to show that the maximal Kullback-Leibler divergence to theRBM model with $n$ visible and $m$ hidden units is bounded from above by $n -\left\lfloor \log(m+1) \right\rfloor -\frac{m+1}{2^{\left\lfloor\log(m+1)\right\rfloor}} \approx (n -1) - \log(m+1)$.In this way we can specify the number of hidden units that guarantees asufficiently rich model containing different classes of distributions andrespecting a given error tolerance.
arxiv-6300-6 | Learning ELM network weights using linear discriminant analysis | http://arxiv.org/pdf/1406.3100v1.pdf | author:Philip de Chazal, Jonathan Tapson, Andr√© van Schaik category:cs.NE cs.LG stat.ML published:2014-06-12 summary:We present an alternative to the pseudo-inverse method for determining thehidden to output weight values for Extreme Learning Machines performingclassification tasks. The method is based on linear discriminant analysis andprovides Bayes optimal single point estimates for the weight values.
arxiv-6300-7 | A Brief State of the Art for Ontology Authoring | http://arxiv.org/pdf/1406.2903v2.pdf | author:Hazem Safwat, Brian Davis category:cs.CL published:2014-06-11 summary:One of the main challenges for building the Semantic web is OntologyAuthoring. Controlled Natural Languages CNLs offer a user friendly means fornon-experts to author ontologies. This paper provides a snapshot of thestate-of-the-art for the core CNLs for ontology authoring and reviews theirrespective evaluations.
arxiv-6300-8 | Gradient Sliding for Composite Optimization | http://arxiv.org/pdf/1406.0919v2.pdf | author:Guanghui Lan category:math.OC cs.CC stat.ML published:2014-06-04 summary:We consider in this paper a class of composite optimization problems whoseobjective function is given by the summation of a general smooth and nonsmoothcomponent, together with a relatively simple nonsmooth term. We present a newclass of first-order methods, namely the gradient sliding algorithms, which canskip the computation of the gradient for the smooth component from time totime. As a consequence, these algorithms require only ${\calO}(1/\sqrt{\epsilon})$ gradient evaluations for the smooth component in orderto find an $\epsilon$-solution for the composite problem, while stillmaintaining the optimal ${\cal O}(1/\epsilon^2)$ bound on the total number ofsubgradient evaluations for the nonsmooth component. We then present astochastic counterpart for these algorithms and establish similar complexitybounds for solving an important class of stochastic composite optimizationproblems. Moreover, if the smooth component in the composite function isstrongly convex, the developed gradient sliding algorithms can significantlyreduce the number of graduate and subgradient evaluations for the smooth andnonsmooth component to ${\cal O} (\log (1/\epsilon))$ and ${\calO}(1/\epsilon)$, respectively. Finally, we generalize these algorithms to thecase when the smooth component is replaced by a nonsmooth one possessing acertain bi-linear saddle point structure.
arxiv-6300-9 | Parsing Semantic Parts of Cars Using Graphical Models and Segment Appearance Consistency | http://arxiv.org/pdf/1406.2375v2.pdf | author:Wenhao Lu, Xiaochen Lian, Alan Yuille category:cs.CV published:2014-06-09 summary:This paper addresses the problem of semantic part parsing (segmentation) ofcars, i.e.assigning every pixel within the car to one of the parts (e.g.body,window, lights, license plates and wheels). We formulate this as a landmarkidentification problem, where a set of landmarks specifies the boundaries ofthe parts. A novel mixture of graphical models is proposed, which dynamicallycouples the landmarks to a hierarchy of segments. When modeling pairwiserelation between landmarks, this coupling enables our model to exploit thelocal image contents in addition to spatial deformation, an aspect that mostexisting graphical models ignore. In particular, our model enforces appearanceconsistency between segments within the same part. Parsing the car, includingfinding the optimal coupling between landmarks and segments in the hierarchy,is performed by dynamic programming. We evaluate our method on a subset ofPASCAL VOC 2010 car images and on the car subset of 3D Object Category dataset(CAR3D). We show good results and, in particular, quantify the effectiveness ofusing the segment appearance consistency in terms of accuracy of partlocalization and segmentation.
arxiv-6300-10 | Distributed Parameter Estimation in Probabilistic Graphical Models | http://arxiv.org/pdf/1406.3070v1.pdf | author:Yariv Dror Mizrahi, Misha Denil, Nando de Freitas category:stat.ML published:2014-06-11 summary:This paper presents foundational theoretical results on distributed parameterestimation for undirected probabilistic graphical models. It introduces ageneral condition on composite likelihood decompositions of these models whichguarantees the global consistency of distributed estimators, provided the localestimators are consistent.
arxiv-6300-11 | Input Warping for Bayesian Optimization of Non-stationary Functions | http://arxiv.org/pdf/1402.0929v3.pdf | author:Jasper Snoek, Kevin Swersky, Richard S. Zemel, Ryan P. Adams category:stat.ML cs.LG published:2014-02-05 summary:Bayesian optimization has proven to be a highly effective methodology for theglobal optimization of unknown, expensive and multimodal functions. The abilityto accurately model distributions over functions is critical to theeffectiveness of Bayesian optimization. Although Gaussian processes provide aflexible prior over functions which can be queried efficiently, there arevarious classes of functions that remain difficult to model. One of the mostfrequently occurring of these is the class of non-stationary functions. Theoptimization of the hyperparameters of machine learning algorithms is a problemdomain in which parameters are often manually transformed a priori, for exampleby optimizing in "log-space," to mitigate the effects of spatially-varyinglength scale. We develop a methodology for automatically learning a wide familyof bijective transformations or warpings of the input space using the Betacumulative distribution function. We further extend the warping framework tomulti-task Bayesian optimization so that multiple tasks can be warped into ajointly stationary space. On a set of challenging benchmark optimization tasks,we observe that the inclusion of warping greatly improves on thestate-of-the-art, producing better results faster and more reliably.
arxiv-6300-12 | Truncated Nuclear Norm Minimization for Image Restoration Based On Iterative Support Detection | http://arxiv.org/pdf/1406.2969v1.pdf | author:Yilun Wang, Xinhua Su category:cs.CV cs.LG stat.ML published:2014-06-11 summary:Recovering a large matrix from limited measurements is a challenging taskarising in many real applications, such as image inpainting, compressivesensing and medical imaging, and this kind of problems are mostly formulated aslow-rank matrix approximation problems. Due to the rank operator beingnon-convex and discontinuous, most of the recent theoretical studies use thenuclear norm as a convex relaxation and the low-rank matrix recovery problem issolved through minimization of the nuclear norm regularized problem. However, amajor limitation of nuclear norm minimization is that all the singular valuesare simultaneously minimized and the rank may not be well approximated\cite{hu2012fast}. Correspondingly, in this paper, we propose a new multi-stagealgorithm, which makes use of the concept of Truncated Nuclear NormRegularization (TNNR) proposed in \citep{hu2012fast} and Iterative SupportDetection (ISD) proposed in \citep{wang2010sparse} to overcome the abovelimitation. Besides matrix completion problems considered in\citep{hu2012fast}, the proposed method can be also extended to the generallow-rank matrix recovery problems. Extensive experiments well validate thesuperiority of our new algorithms over other state-of-the-art methods.
arxiv-6300-13 | Bird Species Categorization Using Pose Normalized Deep Convolutional Nets | http://arxiv.org/pdf/1406.2952v1.pdf | author:Steve Branson, Grant Van Horn, Serge Belongie, Pietro Perona category:cs.CV published:2014-06-11 summary:We propose an architecture for fine-grained visual categorization thatapproaches expert human performance in the classification of bird species. Ourarchitecture first computes an estimate of the object's pose; this is used tocompute local image features which are, in turn, used for classification. Thefeatures are computed by applying deep convolutional nets to image patches thatare located and normalized by the pose. We perform an empirical study of anumber of pose normalization schemes, including an investigation of higherorder geometric warping functions. We propose a novel graph-based clusteringalgorithm for learning a compact pose normalization space. We perform adetailed investigation of state-of-the-art deep convolutional featureimplementations and fine-tuning feature learning for fine-grainedclassification. We observe that a model that integrates lower-level featurelayers with pose-normalized extraction routines and higher-level feature layerswith unaligned image features works best. Our experiments advancestate-of-the-art performance on bird species recognition, with a largeimprovement of correct classification rates over previous methods (75% vs.55-65%).
arxiv-6300-14 | Acoustic Gait-based Person Identification using Hidden Markov Models | http://arxiv.org/pdf/1406.2895v1.pdf | author:J√ºrgen T. Geiger, Maximilian Knei√ül, Bj√∂rn Schuller, Gerhard Rigoll category:cs.HC cs.CV published:2014-06-11 summary:We present a system for identifying humans by their walking sounds. Thisproblem is also known as acoustic gait recognition. The goal of the system isto analyse sounds emitted by walking persons (mostly the step sounds) andidentify those persons. These sounds are characterised by the gait pattern andare influenced by the movements of the arms and legs, but also depend on thetype of shoe. We extract cepstral features from the recorded audio signals anduse hidden Markov models for dynamic classification. A cyclic model topology isemployed to represent individual gait cycles. This topology allows to model anddetect individual steps, leading to very promising identification rates. Forexperimental validation, we use the publicly available TUM GAID database, whichis a large gait recognition database containing 3050 recordings of 305 subjectsin three variations. In the best setup, an identification rate of 65.5 % isachieved out of 155 subjects. This is a relative improvement of almost 30 %compared to our previous work, which used various audio features and supportvector machines.
arxiv-6300-15 | Explicit Computation of Input Weights in Extreme Learning Machines | http://arxiv.org/pdf/1406.2889v1.pdf | author:Jonathan Tapson, Philip de Chazal, Andr√© van Schaik category:cs.NE published:2014-06-11 summary:We present a closed form expression for initializing the input weights in amulti-layer perceptron, which can be used as the first step in synthesis of anExtreme Learning Ma-chine. The expression is based on the standard function fora separating hyperplane as computed in multilayer perceptrons and linearSupport Vector Machines; that is, as a linear combination of input datasamples. In the absence of supervised training for the input weights, randomlinear combinations of training data samples are used to project the input datato a higher dimensional hidden layer. The hidden layer weights are solved inthe standard ELM fashion by computing the pseudoinverse of the hidden layeroutputs and multiplying by the desired output values. All weights for thismethod can be computed in a single pass, and the resulting networks are moreaccurate and more consistent on some standard problems than regular ELMnetworks of the same size.
arxiv-6300-16 | POS Tagging and its Applications for Mathematics | http://arxiv.org/pdf/1406.2880v1.pdf | author:Ulf Sch√∂neberg, Wolfram Sperber category:cs.DL cs.CL cs.IR published:2014-06-11 summary:Content analysis of scientific publications is a nontrivial task, but auseful and important one for scientific information services. In the Gutenbergera it was a domain of human experts; in the digital age many machine-basedmethods, e.g., graph analysis tools and machine-learning techniques, have beendeveloped for it. Natural Language Processing (NLP) is a powerfulmachine-learning approach to semiautomatic speech and language processing,which is also applicable to mathematics. The well established methods of NLPhave to be adjusted for the special needs of mathematics, in particular forhandling mathematical formulae. We demonstrate a mathematics-aware part ofspeech tagger and give a short overview about our adaptation of NLP methods formathematical publications. We show the use of the tools developed for keyphrase extraction and classification in the database zbMATH.
arxiv-6300-17 | Algebraic-Combinatorial Methods for Low-Rank Matrix Completion with Application to Athletic Performance Prediction | http://arxiv.org/pdf/1406.2864v1.pdf | author:Duncan A. J. Blythe, Louis Theran, Franz Kiraly category:stat.ML published:2014-06-11 summary:This paper presents novel algorithms which exploit the intrinsic algebraicand combinatorial structure of the matrix completion task for estimatingmissing en- tries in the general low rank setting. For positive data, weachieve results out- performing the state of the art nuclear norm, both inaccuracy and computational efficiency, in simulations and in the task ofpredicting athletic performance from partially observed data.
arxiv-6300-18 | Maximum Likelihood-based Online Adaptation of Hyper-parameters in CMA-ES | http://arxiv.org/pdf/1406.2623v2.pdf | author:Ilya Loshchilov, Marc Schoenauer, Mich√®le Sebag, Nikolaus Hansen category:cs.NE cs.AI published:2014-06-10 summary:The Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is widelyaccepted as a robust derivative-free continuous optimization algorithm fornon-linear and non-convex optimization problems. CMA-ES is well known to bealmost parameterless, meaning that only one hyper-parameter, the populationsize, is proposed to be tuned by the user. In this paper, we propose aprincipled approach called self-CMA-ES to achieve the online adaptation ofCMA-ES hyper-parameters in order to improve its overall performance.Experimental results show that for larger-than-default population size, thedefault settings of hyper-parameters of CMA-ES are far from being optimal, andthat self-CMA-ES allows for dynamically approaching optimal settings.
arxiv-6300-19 | A Hitchhiker's Guide to Search-Based Software Engineering for Software Product Lines | http://arxiv.org/pdf/1406.2823v1.pdf | author:Roberto E. Lopez-Herrejon, Javier Ferrer, Francisco Chicano, Lukas Linsbauer, Alexander Egyed, Enrique Alba category:cs.SE cs.NE published:2014-06-11 summary:Search Based Software Engineering (SBSE) is an emerging discipline thatfocuses on the application of search-based optimization techniques to softwareengineering problems. The capacity of SBSE techniques to tackle problemsinvolving large search spaces make their application attractive for SoftwareProduct Lines (SPLs). In recent years, several publications have appeared thatapply SBSE techniques to SPL problems. In this paper, we present the results ofa systematic mapping study of such publications. We identified the stages ofthe SPL life cycle where SBSE techniques have been used, what case studies havebeen employed and how they have been analysed. This mapping study revealedpotential venues for further research as well as common misunderstanding andpitfalls when applying SBSE techniques that we address by providing a guidelinefor researchers and practitioners interested in exploiting these techniques.
arxiv-6300-20 | Bregman Divergences for Infinite Dimensional Covariance Matrices | http://arxiv.org/pdf/1403.4334v3.pdf | author:Mehrtash Harandi, Mathieu Salzmann, Fatih Porikli category:cs.CV published:2014-03-18 summary:We introduce an approach to computing and comparing Covariance Descriptors(CovDs) in infinite-dimensional spaces. CovDs have become increasingly popularto address classification problems in computer vision. While CovDs offer somerobustness to measurement variations, they also throw away part of theinformation contained in the original data by only retaining the second-orderstatistics over the measurements. Here, we propose to overcome this limitationby first mapping the original data to a high-dimensional Hilbert space, andonly then compute the CovDs. We show that several Bregman divergences can becomputed between the resulting CovDs in Hilbert space via the use of kernels.We then exploit these divergences for classification purposes. Our experimentsdemonstrate the benefits of our approach on several tasks, such as material andtexture recognition, person re-identification, and action recognition frommotion capture data.
arxiv-6300-21 | Provable Tensor Factorization with Missing Data | http://arxiv.org/pdf/1406.2784v1.pdf | author:Prateek Jain, Sewoong Oh category:stat.ML published:2014-06-11 summary:We study the problem of low-rank tensor factorization in the presence ofmissing data. We ask the following question: how many sampled entries do weneed, to efficiently and exactly reconstruct a tensor with a low-rankorthogonal decomposition? We propose a novel alternating minimization basedmethod which iteratively refines estimates of the singular vectors. We showthat under certain standard assumptions, our method can recover a three-mode$n\times n\times n$ dimensional rank-$r$ tensor exactly from $O(n^{3/2} r^5\log^4 n)$ randomly sampled entries. In the process of proving this result, wesolve two challenging sub-problems for tensors with missing data. First, in theprocess of analyzing the initialization step, we prove a generalization of acelebrated result by Szemer\'edie et al. on the spectrum of random graphs.Next, we prove global convergence of alternating minimization with a goodinitialization. Simulations suggest that the dependence of the sample size ondimensionality $n$ is indeed tight.
arxiv-6300-22 | Deep Epitomic Convolutional Neural Networks | http://arxiv.org/pdf/1406.2732v1.pdf | author:George Papandreou category:cs.CV cs.LG published:2014-06-10 summary:Deep convolutional neural networks have recently proven extremely competitivein challenging image recognition tasks. This paper proposes the epitomicconvolution as a new building block for deep neural networks. An epitomicconvolution layer replaces a pair of consecutive convolution and max-poolinglayers found in standard deep convolutional neural networks. The main versionof the proposed model uses mini-epitomes in place of filters and computesresponses invariant to small translations by epitomic search instead ofmax-pooling over image positions. The topographic version of the proposed modeluses large epitomes to learn filter maps organized in translationaltopographies. We show that error back-propagation can successfully learnmultiple epitomic layers in a supervised fashion. The effectiveness of theproposed method is assessed in image classification tasks on standardbenchmarks. Our experiments on Imagenet indicate improved recognitionperformance compared to standard convolutional neural networks of similararchitecture. Our models pre-trained on Imagenet perform excellently onCaltech-101. We also obtain competitive image classification results on thesmall-image MNIST and CIFAR-10 datasets.
arxiv-6300-23 | Learning Latent Variable Gaussian Graphical Models | http://arxiv.org/pdf/1406.2721v1.pdf | author:Zhaoshi Meng, Brian Eriksson, Alfred O. Hero III category:stat.ML cs.LG math.ST stat.TH published:2014-06-10 summary:Gaussian graphical models (GGM) have been widely used in manyhigh-dimensional applications ranging from biological and financial data torecommender systems. Sparsity in GGM plays a central role both statisticallyand computationally. Unfortunately, real-world data often does not fit well tosparse graphical models. In this paper, we focus on a family of latent variableGaussian graphical models (LVGGM), where the model is conditionally sparsegiven latent variables, but marginally non-sparse. In LVGGM, the inversecovariance matrix has a low-rank plus sparse structure, and can be learned in aregularized maximum likelihood framework. We derive novel parameter estimationerror bounds for LVGGM under mild conditions in the high-dimensional setting.These results complement the existing theory on the structural learning, andopen up new possibilities of using LVGGM for statistical inference.
arxiv-6300-24 | A Multiplicative Model for Learning Distributed Text-Based Attribute Representations | http://arxiv.org/pdf/1406.2710v1.pdf | author:Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov category:cs.LG cs.CL published:2014-06-10 summary:In this paper we propose a general framework for learning distributedrepresentations of attributes: characteristics of text whose representationscan be jointly learned with word embeddings. Attributes can correspond todocument indicators (to learn sentence vectors), language indicators (to learndistributed language representations), meta-data and side information (such asthe age, gender and industry of a blogger) or representations of authors. Wedescribe a third-order model where word context and attribute vectors interactmultiplicatively to predict the next word in a sequence. This leads to thenotion of conditional word similarity: how meanings of words change whenconditioned on different attributes. We perform several experimental tasksincluding sentiment classification, cross-lingual document classification, andblog authorship attribution. We also qualitatively evaluate conditional wordneighbours and attribute-conditioned text generation.
arxiv-6300-25 | Augur: a Modeling Language for Data-Parallel Probabilistic Inference | http://arxiv.org/pdf/1312.3613v2.pdf | author:Jean-Baptiste Tristan, Daniel Huang, Joseph Tassarotti, Adam Pocock, Stephen J. Green, Guy L. Steele Jr category:stat.ML cs.AI cs.DC cs.PL published:2013-12-12 summary:It is time-consuming and error-prone to implement inference procedures foreach new probabilistic model. Probabilistic programming addresses this problemby allowing a user to specify the model and having a compiler automaticallygenerate an inference procedure for it. For this approach to be practical, itis important to generate inference code that has reasonable performance. Inthis paper, we present a probabilistic programming language and compiler forBayesian networks designed to make effective use of data-parallel architecturessuch as GPUs. Our language is fully integrated within the Scala programminglanguage and benefits from tools such as IDE support, type-checking, and codecompletion. We show that the compiler can generate data-parallel inference codescalable to thousands of GPU cores by making use of the conditionalindependence relationships in the Bayesian network.
arxiv-6300-26 | Conceptors: an easy introduction | http://arxiv.org/pdf/1406.2671v1.pdf | author:Herbert Jaeger category:cs.NE published:2014-06-10 summary:Conceptors provide an elementary neuro-computational mechanism which sheds afresh and unifying light on a diversity of cognitive phenomena. A number ofdemanding learning and processing tasks can be solved with unprecedented ease,robustness and accuracy. Some of these tasks were impossible to solve before.This entirely informal paper introduces the basic principles of conceptors andhighlights some of their usages.
arxiv-6300-27 | Generative Adversarial Networks | http://arxiv.org/pdf/1406.2661v1.pdf | author:Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio category:stat.ML cs.LG published:2014-06-10 summary:We propose a new framework for estimating generative models via anadversarial process, in which we simultaneously train two models: a generativemodel G that captures the data distribution, and a discriminative model D thatestimates the probability that a sample came from the training data rather thanG. The training procedure for G is to maximize the probability of D making amistake. This framework corresponds to a minimax two-player game. In the spaceof arbitrary functions G and D, a unique solution exists, with G recovering thetraining data distribution and D equal to 1/2 everywhere. In the case where Gand D are defined by multilayer perceptrons, the entire system can be trainedwith backpropagation. There is no need for any Markov chains or unrolledapproximate inference networks during either training or generation of samples.Experiments demonstrate the potential of the framework through qualitative andquantitative evaluation of the generated samples.
arxiv-6300-28 | Learning with Cross-Kernels and Ideal PCA | http://arxiv.org/pdf/1406.2646v1.pdf | author:Franz J Kir√°ly, Martin Kreuzer, Louis Theran category:cs.LG math.AC stat.ML published:2014-06-10 summary:We describe how cross-kernel matrices, that is, kernel matrices between thedata and a custom chosen set of `feature spanning points' can be used forlearning. The main potential of cross-kernels lies in the fact that (a) onlyone side of the matrix scales with the number of data points, and (b)cross-kernels, as opposed to the usual kernel matrices, can be used to certifyfor the data manifold. Our theoretical framework, which is based on a dualityinvolving the feature space and vanishing ideals, indicates that cross-kernelshave the potential to be used for any kind of kernel learning. We present anovel algorithm, Ideal PCA (IPCA), which cross-kernelizes PCA. We demonstrateon real and synthetic data that IPCA allows to (a) obtain PCA-like featuresfaster and (b) to extract novel and empirically validated features certifyingfor the data manifold.
arxiv-6300-29 | Equivalence of Learning Algorithms | http://arxiv.org/pdf/1406.2622v1.pdf | author:Julien Audiffren, Hachem Kadri category:cs.LG stat.ML published:2014-06-10 summary:The purpose of this paper is to introduce a concept of equivalence betweenmachine learning algorithms. We define two notions of algorithmic equivalence,namely, weak and strong equivalence. These notions are of paramount importancefor identifying when learning prop erties from one learning algorithm can betransferred to another. Using regularized kernel machines as a case study, weillustrate the importance of the introduced equivalence concept by analyzingthe relation between kernel ridge regression (KRR) and m-power regularizedleast squares regression (M-RLSR) algorithms.
arxiv-6300-30 | Graph Approximation and Clustering on a Budget | http://arxiv.org/pdf/1406.2602v1.pdf | author:Ethan Fetaya, Ohad Shamir, Shimon Ullman category:stat.ML cs.AI cs.CV cs.LG published:2014-06-10 summary:We consider the problem of learning from a similarity matrix (such asspectral clustering and lowd imensional embedding), when computing pairwisesimilarities are costly, and only a limited number of entries can be observed.We provide a theoretical analysis using standard notions of graphapproximation, significantly generalizing previous results (which focused onspectral clustering with two clusters). We also propose a new algorithmicapproach based on adaptive sampling, which experimentally matches or improveson previous methods, while being considerably more general and computationallycheaper.
arxiv-6300-31 | Identification of Orchid Species Using Content-Based Flower Image Retrieval | http://arxiv.org/pdf/1406.2580v1.pdf | author:D. H. Apriyanti, A. A. Arymurthy, L. T. Handoko category:cs.CV cs.IR cs.LG published:2014-06-10 summary:In this paper, we developed the system for recognizing the orchid species byusing the images of flower. We used MSRM (Maximal Similarity based on RegionMerging) method for segmenting the flower object from the background andextracting the shape feature such as the distance from the edge to the centroidpoint of the flower, aspect ratio, roundness, moment invariant, fractaldimension and also extract color feature. We used HSV color feature withignoring the V value. To retrieve the image, we used Support Vector Machine(SVM) method. Orchid is a unique flower. It has a part of flower called lip(labellum) that distinguishes it from other flowers even from other types oforchids. Thus, in this paper, we proposed to do feature extraction not only onflower region but also on lip (labellum) region. The result shows that ourproposed method can increase the accuracy value of content based flower imageretrieval for orchid species up to $\pm$ 14%. The most dominant feature isCentroid Contour Distance, Moment Invariant and HSV Color. The system accuracyis 85,33% in validation phase and 79,33% in testing phase.
arxiv-6300-32 | Identifying and attacking the saddle point problem in high-dimensional non-convex optimization | http://arxiv.org/pdf/1406.2572v1.pdf | author:Yann Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, Yoshua Bengio category:cs.LG math.OC stat.ML published:2014-06-10 summary:A central challenge to many fields of science and engineering involvesminimizing non-convex error functions over continuous, high dimensional spaces.Gradient descent or quasi-Newton methods are almost ubiquitously used toperform such minimizations, and it is often thought that a main source ofdifficulty for these local methods to find the global minimum is theproliferation of local minima with much higher error than the global minimum.Here we argue, based on results from statistical physics, random matrix theory,neural network theory, and empirical evidence, that a deeper and more profounddifficulty originates from the proliferation of saddle points, not localminima, especially in high dimensional problems of practical interest. Suchsaddle points are surrounded by high error plateaus that can dramatically slowdown learning, and give the illusory impression of the existence of a localminimum. Motivated by these arguments, we propose a new approach tosecond-order optimization, the saddle-free Newton method, that can rapidlyescape high dimensional saddle points, unlike gradient descent and quasi-Newtonmethods. We apply this algorithm to deep or recurrent neural network training,and provide numerical evidence for its superior optimization performance.
arxiv-6300-33 | A Flexible Fitness Function for Community Detection in Complex Networks | http://arxiv.org/pdf/1406.2545v1.pdf | author:Fabricio Olivetti de Franca, Guilherme Palermo Coelho category:cs.NE cs.SI physics.soc-ph published:2014-06-10 summary:Most community detection algorithms from the literature work as optimizationtools that minimize a given \textit{fitness function}, while assuming that eachnode belongs to a single community. Since there is no hard concept of what acommunity is, most proposed fitness functions focus on a particular definition.As such, these functions do not always lead to partitions that correspond tothose observed in practice. This paper proposes a new flexible fitness functionthat allows the identification of communities with distinct characteristics.Such flexibility was evaluated through the adoption of an immune-inspiredoptimization algorithm, named cob-aiNet[C], to identify both disjoint andoverlapping communities in a set of benchmark networks. The results have shownthat the obtained partitions are much closer to the ground-truth than thoseobtained by the optimization of the modularity function.
arxiv-6300-34 | Predictive Entropy Search for Efficient Global Optimization of Black-box Functions | http://arxiv.org/pdf/1406.2541v1.pdf | author:Jos√© Miguel Hern√°ndez-Lobato, Matthew W. Hoffman, Zoubin Ghahramani category:stat.ML cs.LG published:2014-06-10 summary:We propose a novel information-theoretic approach for Bayesian optimizationcalled Predictive Entropy Search (PES). At each iteration, PES selects the nextevaluation point that maximizes the expected information gained with respect tothe global maximum. PES codifies this intractable acquisition function in termsof the expected reduction in the differential entropy of the predictivedistribution. This reformulation allows PES to obtain approximations that areboth more accurate and efficient than other alternatives such as Entropy Search(ES). Furthermore, PES can easily perform a fully Bayesian treatment of themodel hyperparameters while ES cannot. We evaluate PES in both synthetic andreal-world applications, including optimization problems in machine learning,finance, biotechnology, and robotics. We show that the increased accuracy ofPES leads to significant gains in optimization performance.
arxiv-6300-35 | Maximizing Diversity for Multimodal Optimization | http://arxiv.org/pdf/1406.2539v1.pdf | author:Fabricio Olivetti de Franca category:cs.NE published:2014-06-10 summary:Most multimodal optimization algorithms use the so called \textit{nichingmethods}~\cite{mahfoud1995niching} in order to promote diversity duringoptimization, while others, like \textit{Artificial ImmuneSystems}~\cite{de2010conceptual} try to find multiple solutions as its mainobjective. One of such algorithms, called\textit{dopt-aiNet}~\cite{de2005artificial}, introduced the Line Distance thatmeasures the distance between two solutions regarding their basis ofattraction. In this short abstract I propose the use of the Line Distancemeasure as the main objective-function in order to locate multiple optima atonce in a population.
arxiv-6300-36 | FrameNet CNL: a Knowledge Representation and Information Extraction Language | http://arxiv.org/pdf/1406.2538v1.pdf | author:Guntis Barzdins category:cs.CL cs.AI cs.IR cs.LG published:2014-06-10 summary:The paper presents a FrameNet-based information extraction and knowledgerepresentation framework, called FrameNet-CNL. The framework is used on naturallanguage documents and represents the extracted knowledge in a tailor-madeFrame-ontology from which unambiguous FrameNet-CNL paraphrase text can begenerated automatically in multiple languages. This approach brings togetherthe fields of information extraction and CNL, because a source text can beconsidered belonging to FrameNet-CNL, if information extraction parser producesthe correct knowledge representation as a result. We describe astate-of-the-art information extraction parser used by a national news agencyand speculate that FrameNet-CNL eventually could shape the natural languagesubset used for writing the newswire articles.
arxiv-6300-37 | Denosing Using Wavelets and Projections onto the L1-Ball | http://arxiv.org/pdf/1406.2528v1.pdf | author:A. Enis Cetin, Mohammad Tofighi category:math.OC cs.CV published:2014-06-10 summary:Both wavelet denoising and denosing methods using the concept of sparsity arebased on soft-thresholding. In sparsity based denoising methods, it is assumedthat the original signal is sparse in some transform domains such as thewavelet domain and the wavelet subsignals of the noisy signal are projectedonto L1-balls to reduce noise. In this lecture note, it is shown that the sizeof the L1-ball or equivalently the soft threshold value can be determined usinglinear algebra. The key step is an orthogonal projection onto the epigraph setof the L1-norm cost function.
arxiv-6300-38 | Hyperspectral image superresolution: An edge-preserving convex formulation | http://arxiv.org/pdf/1403.8098v2.pdf | author:Miguel Sim√µes, Jos√© Bioucas-Dias, Luis B. Almeida, Jocelyn Chanussot category:cs.CV stat.ML published:2014-03-31 summary:Hyperspectral remote sensing images (HSIs) are characterized by having a lowspatial resolution and a high spectral resolution, whereas multispectral images(MSIs) are characterized by low spectral and high spatial resolutions. Thesecomplementary characteristics have stimulated active research in the inferenceof images with high spatial and spectral resolutions from HSI-MSI pairs. In this paper, we formulate this data fusion problem as the minimization of aconvex objective function containing two data-fitting terms and anedge-preserving regularizer. The data-fitting terms are quadratic and accountfor blur, different spatial resolutions, and additive noise; the regularizer, aform of vector Total Variation, promotes aligned discontinuities across thereconstructed hyperspectral bands. The optimization described above is rather hard, owing to itsnon-diagonalizable linear operators, to the non-quadratic and non-smooth natureof the regularizer, and to the very large size of the image to be inferred. Wetackle these difficulties by tailoring the Split Augmented Lagrangian ShrinkageAlgorithm (SALSA)---an instance of the Alternating Direction Method ofMultipliers (ADMM)---to this optimization problem. By using a convenientvariable splitting and by exploiting the fact that HSIs generally "live" in alow-dimensional subspace, we obtain an effective algorithm that yieldsstate-of-the-art results, as illustrated by experiments.
arxiv-6300-39 | Bayesian calibration for forensic evidence reporting | http://arxiv.org/pdf/1403.5997v3.pdf | author:Niko Br√ºmmer, Albert Swart category:stat.ML cs.LG published:2014-03-24 summary:We introduce a Bayesian solution for the problem in forensic speakerrecognition, where there may be very little background material for estimatingscore calibration parameters. We work within the Bayesian paradigm of evidencereporting and develop a principled probabilistic treatment of the problem,which results in a Bayesian likelihood-ratio as the vehicle for reportingweight of evidence. We show in contrast, that reporting a likelihood-ratiodistribution does not solve this problem. Our solution is experimentallyexercised on a simulated forensic scenario, using NIST SRE'12 scores, whichdemonstrates a clear advantage for the proposed method compared to thetraditional plugin calibration recipe.
arxiv-6300-40 | Detecting Directionality in Random Fields Using the Monogenic Signal | http://arxiv.org/pdf/1304.2998v3.pdf | author:Sofia Olhede, David Ram√≠rez, Peter J. Schreier category:cs.IT cs.CV math.IT published:2013-04-10 summary:Detecting and analyzing directional structures in images is important in manyapplications since one-dimensional patterns often correspond to importantfeatures such as object contours or trajectories. Classifying a structure asdirectional or non-directional requires a measure to quantify the degree ofdirectionality and a threshold, which needs to be chosen based on thestatistics of the image. In order to do this, we model the image as a randomfield. So far, little research has been performed on analyzing directionalityin random fields. In this paper, we propose a measure to quantify the degree ofdirectionality based on the random monogenic signal, which enables a uniquedecomposition of a 2D signal into local amplitude, local orientation, and localphase. We investigate the second-order statistical properties of the monogenicsignal for isotropic, anisotropic, and unidirectional random fields. We analyzeour measure of directionality for finite-size sample images, and determine athreshold to distinguish between unidirectional and non-unidirectional randomfields, which allows the automatic classification of images.
arxiv-6300-41 | Online Convex Optimization Against Adversaries with Memory and Application to Statistical Arbitrage | http://arxiv.org/pdf/1302.6937v2.pdf | author:Oren Anava, Elad Hazan, Shie Mannor category:cs.LG published:2013-02-27 summary:The framework of online learning with memory naturally captures learningproblems with temporal constraints, and was previously studied for the expertssetting. In this work we extend the notion of learning with memory to thegeneral Online Convex Optimization (OCO) framework, and present two algorithmsthat attain low regret. The first algorithm applies to Lipschitz continuousloss functions, obtaining optimal regret bounds for both convex and stronglyconvex losses. The second algorithm attains the optimal regret bounds andapplies more broadly to convex losses without requiring Lipschitz continuity,yet is more complicated to implement. We complement our theoretic results withan application to statistical arbitrage in finance: we devise algorithms forconstructing mean-reverting portfolios.
arxiv-6300-42 | Why do linear SVMs trained on HOG features perform so well? | http://arxiv.org/pdf/1406.2419v1.pdf | author:Hilton Bristow, Simon Lucey category:cs.CV cs.LG published:2014-06-10 summary:Linear Support Vector Machines trained on HOG features are now a de factostandard across many visual perception tasks. Their popularisation can largelybe attributed to the step-change in performance they brought to pedestriandetection, and their subsequent successes in deformable parts models. Thispaper explores the interactions that make the HOG-SVM symbiosis perform sowell. By connecting the feature extraction and learning processes rather thantreating them as disparate plugins, we show that HOG features can be viewed asdoing two things: (i) inducing capacity in, and (ii) adding prior to a linearSVM trained on pixels. From this perspective, preserving second-orderstatistics and locality of interactions are key to good performance. Wedemonstrate surprising accuracy on expression recognition and pedestriandetection tasks, by assuming only the importance of preserving such localsecond-order interactions.
arxiv-6300-43 | Unsupervised Feature Learning through Divergent Discriminative Feature Accumulation | http://arxiv.org/pdf/1406.1833v2.pdf | author:Paul A. Szerlip, Gregory Morse, Justin K. Pugh, Kenneth O. Stanley category:cs.NE cs.LG published:2014-06-06 summary:Unlike unsupervised approaches such as autoencoders that learn to reconstructtheir inputs, this paper introduces an alternative approach to unsupervisedfeature learning called divergent discriminative feature accumulation (DDFA)that instead continually accumulates features that make novel discriminationsamong the training set. Thus DDFA features are inherently discriminative fromthe start even though they are trained without knowledge of the ultimateclassification problem. Interestingly, DDFA also continues to add new featuresindefinitely (so it does not depend on a hidden layer size), is not based onminimizing error, and is inherently divergent instead of convergent, therebyproviding a unique direction of research for unsupervised feature learning. Inthis paper the quality of its learned features is demonstrated on the MNISTdataset, where its performance confirms that indeed DDFA is a viable techniquefor learning useful features.
arxiv-6300-44 | Optimization Methods for Convolutional Sparse Coding | http://arxiv.org/pdf/1406.2407v1.pdf | author:Hilton Bristow, Simon Lucey category:cs.CV published:2014-06-10 summary:Sparse and convolutional constraints form a natural prior for manyoptimization problems that arise from physical processes. Detecting motifs inspeech and musical passages, super-resolving images, compressing videos, andreconstructing harmonic motions can all leverage redundancies introduced byconvolution. Solving problems involving sparse and convolutional constraintsremains a difficult computational problem, however. In this paper we present anoverview of convolutional sparse coding in a consistent framework. Theobjective involves iteratively optimizing a convolutional least-squares termfor the basis functions, followed by an L1-regularized least squares term forthe sparse coefficients. We discuss a range of optimization methods for solvingthe convolutional sparse coding objective, and the properties that make eachmethod suitable for different applications. In particular, we concentrate oncomputational complexity, speed to {\epsilon} convergence, memory usage, andthe effect of implied boundary conditions. We present a broad suite of examplescovering different signal and application domains to illustrate the generalapplicability of convolutional sparse coding, and the efficacy of the availableoptimization methods.
arxiv-6300-45 | Controlled Natural Language Generation from a Multilingual FrameNet-based Grammar | http://arxiv.org/pdf/1406.2400v1.pdf | author:Dana Dann√©lls, Normunds Gr≈´zƒ´tis category:cs.CL published:2014-06-10 summary:This paper presents a currently bilingual but potentially multilingualFrameNet-based grammar library implemented in Grammatical Framework. Thecontribution of this paper is two-fold. First, it offers a methodologicalapproach to automatically generate the grammar based on semantico-syntacticvalence patterns extracted from FrameNet-annotated corpora. Second, it providesa proof of concept for two use cases illustrating how the acquired multilingualgrammar can be exploited in different CNL applications in the domains of artsand tourism.
arxiv-6300-46 | ExpertBayes: Automatically refining manually built Bayesian networks | http://arxiv.org/pdf/1406.2395v1.pdf | author:Ezilda Almeida, Pedro Ferreira, Tiago Vinhoza, In√™s Dutra, Jingwei Li, Yirong Wu, Elizabeth Burnside category:cs.AI cs.LG stat.ML published:2014-06-10 summary:Bayesian network structures are usually built using only the data andstarting from an empty network or from a naive Bayes structure. Very often, insome domains, like medicine, a prior structure knowledge is already known. Thisstructure can be automatically or manually refined in search for betterperformance models. In this work, we take Bayesian networks built byspecialists and show that minor perturbations to this original network canyield better classifiers with a very small computational cost, whilemaintaining most of the intended meaning of the original model.
arxiv-6300-47 | Automatic Labeling for Entity Extraction in Cyber Security | http://arxiv.org/pdf/1308.4941v3.pdf | author:Robert A. Bridges, Corinne L. Jones, Michael D. Iannacone, Kelly M. Testa, John R. Goodall category:cs.IR cs.CL published:2013-08-22 summary:Timely analysis of cyber-security information necessitates automatedinformation extraction from unstructured text. While state-of-the-artextraction methods produce extremely accurate results, they require ampletraining data, which is generally unavailable for specialized applications,such as detecting security related entities; moreover, manual annotation ofcorpora is very costly and often not a viable solution. In response, we developa very precise method to automatically label text from several data sources byleveraging related, domain-specific, structured data and provide public accessto a corpus annotated with cyber-security entities. Next, we implement aMaximum Entropy Model trained with the average perceptron on a portion of ourcorpus ($\sim$750,000 words) and achieve near perfect precision, recall, andaccuracy, with training times under 17 seconds.
arxiv-6300-48 | Explaining Violation Traces with Finite State Natural Language Generation Models | http://arxiv.org/pdf/1406.2298v1.pdf | author:Gordon J. Pace, Michael Rosner category:cs.SE cs.CL published:2014-06-09 summary:An essential element of any verification technique is that of identifying andcommunicating to the user, system behaviour which leads to a deviation from theexpected behaviour. Such behaviours are typically made available as long tracesof system actions which would benefit from a natural language explanation ofthe trace and especially in the context of business logic level specifications.In this paper we present a natural language generation model which can be usedto explain such traces. A key idea is that the explanation language is a CNLthat is, formally speaking, regular language susceptible transformations thatcan be expressed with finite state machinery. At the same time it admitsvarious forms of abstraction and simplification which contribute to thenaturalness of explanations that are communicated to the user.
arxiv-6300-49 | Depth Map Prediction from a Single Image using a Multi-Scale Deep Network | http://arxiv.org/pdf/1406.2283v1.pdf | author:David Eigen, Christian Puhrsch, Rob Fergus category:cs.CV published:2014-06-09 summary:Predicting depth is an essential component in understanding the 3D geometryof a scene. While for stereo images local correspondence suffices forestimation, finding depth relations from a single image is lessstraightforward, requiring integration of both global and local informationfrom various cues. Moreover, the task is inherently ambiguous, with a largesource of uncertainty coming from the overall scale. In this paper, we presenta new method that addresses this task by employing two deep network stacks: onethat makes a coarse global prediction based on the entire image, and anotherthat refines this prediction locally. We also apply a scale-invariant error tohelp measure depth relations rather than scale. By leveraging the raw datasetsas large sources of training data, our method achieves state-of-the-art resultson both NYU Depth and KITTI, and matches detailed depth boundaries without theneed for superpixelation.
arxiv-6300-50 | Robust Estimation of 3D Human Poses from a Single Image | http://arxiv.org/pdf/1406.2282v1.pdf | author:Chunyu Wang, Yizhou Wang, Zhouchen Lin, Alan L. Yuille, Wen Gao category:cs.CV published:2014-06-09 summary:Human pose estimation is a key step to action recognition. We propose amethod of estimating 3D human poses from a single image, which works inconjunction with an existing 2D pose/joint detector. 3D pose estimation ischallenging because multiple 3D poses may correspond to the same 2D pose afterprojection due to the lack of depth information. Moreover, current 2D poseestimators are usually inaccurate which may cause errors in the 3D estimation.We address the challenges in three ways: (i) We represent a 3D pose as a linearcombination of a sparse set of bases learned from 3D human skeletons. (ii) Weenforce limb length constraints to eliminate anthropomorphically implausibleskeletons. (iii) We estimate a 3D pose by minimizing the $L_1$-norm errorbetween the projection of the 3D pose and the corresponding 2D detection. The$L_1$-norm loss term is robust to inaccurate 2D joint estimations. We use thealternating direction method (ADM) to solve the optimization problemefficiently. Our approach outperforms the state-of-the-arts on three benchmarkdatasets.
arxiv-6300-51 | Feature Selection For High-Dimensional Clustering | http://arxiv.org/pdf/1406.2240v1.pdf | author:Larry Wasserman, Martin Azizyan, Aarti Singh category:math.ST stat.ML stat.TH published:2014-06-09 summary:We present a nonparametric method for selecting informative features inhigh-dimensional clustering problems. We start with a screening step that usesa test for multimodality. Then we apply kernel density estimation and modeclustering to the selected features. The output of the method consists of alist of relevant features, and cluster assignments. We provide explicit boundson the error rate of the resulting clustering. In addition, we provide thefirst error bounds on mode based clustering.
arxiv-6300-52 | A Hybrid Latent Variable Neural Network Model for Item Recommendation | http://arxiv.org/pdf/1406.2235v1.pdf | author:Michael R. Smith, Tony Martinez, Michael Gashler category:cs.LG cs.IR cs.NE stat.ML published:2014-06-09 summary:Collaborative filtering is used to recommend items to a user withoutrequiring a knowledge of the item itself and tends to outperform othertechniques. However, collaborative filtering suffers from the cold-startproblem, which occurs when an item has not yet been rated or a user has notrated any items. Incorporating additional information, such as item or userdescriptions, into collaborative filtering can address the cold-start problem.In this paper, we present a neural network model with latent input variables(latent neural network or LNN) as a hybrid collaborative filtering techniquethat addresses the cold-start problem. LNN outperforms a broad selection ofcontent-based filters (which make recommendations based on item descriptions)and other hybrid approaches while maintaining the accuracy of state-of-the-artcollaborative filtering techniques.
arxiv-6300-53 | Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation | http://arxiv.org/pdf/1404.0736v2.pdf | author:Emily Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, Rob Fergus category:cs.CV cs.LG published:2014-04-02 summary:We present techniques for speeding up the test-time evaluation of largeconvolutional networks, designed for object recognition tasks. These modelsdeliver impressive accuracy but each image evaluation requires millions offloating point operations, making their deployment on smartphones andInternet-scale clusters problematic. The computation is dominated by theconvolution operations in the lower layers of the model. We exploit the linearstructure present within the convolutional filters to derive approximationsthat significantly reduce the required computation. Using largestate-of-the-art models, we demonstrate we demonstrate speedups ofconvolutional layers on both CPU and GPU by a factor of 2x, while keeping theaccuracy within 1% of the original model.
arxiv-6300-54 | Efficient Sparse Clustering of High-Dimensional Non-spherical Gaussian Mixtures | http://arxiv.org/pdf/1406.2206v1.pdf | author:Martin Azizyan, Aarti Singh, Larry Wasserman category:math.ST stat.ML stat.TH published:2014-06-09 summary:We consider the problem of clustering data points in high dimensions, i.e.when the number of data points may be much smaller than the number ofdimensions. Specifically, we consider a Gaussian mixture model (GMM) withnon-spherical Gaussian components, where the clusters are distinguished by onlya few relevant dimensions. The method we propose is a combination of a recentapproach for learning parameters of a Gaussian mixture model and sparse lineardiscriminant analysis (LDA). In addition to cluster assignments, the methodreturns an estimate of the set of features relevant for clustering. Our resultsindicate that the sample complexity of clustering depends on the sparsity ofthe relevant feature set, while only scaling logarithmically with the ambientdimension. Additionally, we require much milder assumptions than existing workon clustering in high dimensions. In particular, we do not require sphericalclusters nor necessitate mean separation along relevant dimensions.
arxiv-6300-55 | How Easy is it to Learn a Controlled Natural Language for Building a Knowledge Base? | http://arxiv.org/pdf/1406.2204v1.pdf | author:Sandra Williams, Richard Power, Allan Third category:cs.CL published:2014-06-09 summary:Recent developments in controlled natural language editors for knowledgeengineering (KE) have given rise to expectations that they will make KE tasksmore accessible and perhaps even enable non-engineers to build knowledge bases.This exploratory research focussed on novices and experts in knowledgeengineering during their attempts to learn a controlled natural language (CNL)known as OWL Simplified English and use it to build a small knowledge base.Participants' behaviours during the task were observed through eye-tracking andscreen recordings. This was an attempt at a more ambitious user study than inprevious research because we used a naturally occurring text as the source ofdomain knowledge, and left them without guidance on which information toselect, or how to encode it. We have identified a number of skills(competencies) required for this difficult task and key problems that authorsface.
arxiv-6300-56 | Adaptive Stochastic Alternating Direction Method of Multipliers | http://arxiv.org/pdf/1312.4564v4.pdf | author:Peilin Zhao, Jinwei Yang, Tong Zhang, Ping Li category:stat.ML cs.LG I.2.6; G.1.6 published:2013-12-16 summary:The Alternating Direction Method of Multipliers (ADMM) has been studied foryears. The traditional ADMM algorithm needs to compute, at each iteration, an(empirical) expected loss function on all training examples, resulting in acomputational complexity proportional to the number of training examples. Toreduce the time complexity, stochastic ADMM algorithms were proposed to replacethe expected function with a random loss function associated with one uniformlydrawn example plus a Bregman divergence. The Bregman divergence, however, isderived from a simple second order proximal function, the half squared norm,which could be a suboptimal choice. In this paper, we present a new family of stochastic ADMM algorithms withoptimal second order proximal functions, which produce a new family of adaptivesubgradient methods. We theoretically prove that their regret bounds are asgood as the bounds which could be achieved by the best proximal function thatcan be chosen in hindsight. Encouraging empirical results on a variety ofreal-world datasets confirm the effectiveness and efficiency of the proposedalgorithms.
arxiv-6300-57 | Unsupervised Pretraining Encourages Moderate-Sparseness | http://arxiv.org/pdf/1312.5813v2.pdf | author:Jun Li, Wei Luo, Jian Yang, Xiaotong Yuan category:cs.LG cs.NE published:2013-12-20 summary:It is well known that direct training of deep neural networks will generallylead to poor results. A major progress in recent years is the invention ofvarious pretraining methods to initialize network parameters and it was shownthat such methods lead to good prediction performance. However, the reason forthe success of pretraining has not been fully understood, although it wasargued that regularization and better optimization play certain roles. Thispaper provides another explanation for the effectiveness of pretraining, wherewe show pretraining leads to a sparseness of hidden unit activation in theresulting neural networks. The main reason is that the pretraining models canbe interpreted as an adaptive sparse coding. Compared to deep neural networkwith sigmoid function, our experimental results on MNIST and Birdsong furthersupport this sparseness observation.
arxiv-6300-58 | Learning directed acyclic graphs via bootstrap aggregating | http://arxiv.org/pdf/1406.2098v1.pdf | author:Ru Wang, Jie Peng category:stat.ML stat.CO stat.ME published:2014-06-09 summary:Probabilistic graphical models are graphical representations of probabilitydistributions. Graphical models have applications in many fields includingbiology, social sciences, linguistic, neuroscience. In this paper, we proposedirected acyclic graphs (DAGs) learning via bootstrap aggregating. The proposedprocedure is named as DAGBag. Specifically, an ensemble of DAGs is firstlearned based on bootstrap resamples of the data and then an aggregated DAG isderived by minimizing the overall distance to the entire ensemble. A family ofmetrics based on the structural hamming distance is defined for the space ofDAGs (of a given node set) and is used for aggregation. Under thehigh-dimensional-low-sample size setting, the graph learned on one data setoften has excessive number of false positive edges due to over-fitting of thenoise. Aggregation overcomes over-fitting through variance reduction and thusgreatly reduces false positives. We also develop an efficient implementation ofthe hill climbing search algorithm of DAG learning which makes the proposedmethod computationally competitive for the high-dimensional regime. The DAGBagprocedure is implemented in the R package dagbag.
arxiv-6300-59 | RuleCNL: A Controlled Natural Language for Business Rule Specifications | http://arxiv.org/pdf/1406.2096v1.pdf | author:Paul Brillant Feuto Njonko, Sylviane Cardey, Peter Greenfield, Walid El Abed category:cs.SE cs.CL published:2014-06-09 summary:Business rules represent the primary means by which companies define theirbusiness, perform their actions in order to reach their objectives. Thus, theyneed to be expressed unambiguously to avoid inconsistencies between businessstakeholders and formally in order to be machine-processed. A promisingsolution is the use of a controlled natural language (CNL) which is a goodmediator between natural and formal languages. This paper presents RuleCNL,which is a CNL for defining business rules. Its core feature is the alignmentof the business rule definition with the business vocabulary which ensurestraceability and consistency with the business domain. The RuleCNL toolprovides editors that assist end-users in the writing process and automaticmappings into the Semantics of Business Vocabulary and Business Rules (SBVR)standard. SBVR is grounded in first order logic and includes constructs calledsemantic formulations that structure the meaning of rules.
arxiv-6300-60 | Image Tag Completion by Low-rank Factorization with Dual Reconstruction Structure Preserved | http://arxiv.org/pdf/1406.2049v1.pdf | author:Xue Li, Yu-Jin Zhang, Bin Shen, Bao-Di Liu category:cs.CV cs.IR published:2014-06-09 summary:A novel tag completion algorithm is proposed in this paper, which is designedwith the following features: 1) Low-rank and error s-parsity: the incompleteinitial tagging matrix D is decomposed into the complete tagging matrix A and asparse error matrix E. However, instead of minimizing its nuclear norm, A isfurther factor-ized into a basis matrix U and a sparse coefficient matrix V,i.e. D=UV+E. This low-rank formulation encapsulating sparse coding enables ouralgorithm to recover latent structures from noisy initial data and avoidperforming too much denoising; 2) Local reconstruction structure consistency:to steer the completion of D, the local linear reconstruction structures infeature space and tag space are obtained and preserved by U and V respectively.Such a scheme could alleviate the negative effect of distances measured bylow-level features and incomplete tags. Thus, we can seek a balance betweenexploiting as much information and not being mislead to suboptimal performance.Experiments conducted on Corel5k dataset and the newly issued Flickr30Conceptsdataset demonstrate the effectiveness and efficiency of the proposed method.
arxiv-6300-61 | Detect What You Can: Detecting and Representing Objects using Holistic Models and Body Parts | http://arxiv.org/pdf/1406.2031v1.pdf | author:Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler, Raquel Urtasun, Alan Yuille category:cs.CV published:2014-06-08 summary:Detecting objects becomes difficult when we need to deal with large shapedeformation, occlusion and low resolution. We propose a novel approach to i)handle large deformations and partial occlusions in animals (as examples ofhighly deformable objects), ii) describe them in terms of body parts, and iii)detect them when their body parts are hard to detect (e.g., animals depicted atlow resolution). We represent the holistic object and body parts separately anduse a fully connected model to arrange templates for the holistic object andbody parts. Our model automatically decouples the holistic object or body partsfrom the model when they are hard to detect. This enables us to represent alarge number of holistic object and body part combinations to better deal withdifferent "detectability" patterns caused by deformations, occlusion and/or lowresolution. We apply our method to the six animal categories in the PASCAL VOC datasetand show that our method significantly improves state-of-the-art (by 4.1% AP)and provides a richer representation for objects. During training we useannotations for body parts (e.g., head, torso, etc), making use of a newdataset of fully annotated object parts for PASCAL VOC 2010, which provides amask for each part.
arxiv-6300-62 | Two-dimensional Sentiment Analysis of text | http://arxiv.org/pdf/1406.2022v1.pdf | author:Rahul Tejwani category:cs.IR cs.CL published:2014-06-08 summary:Sentiment Analysis aims to get the underlying viewpoint of the text, whichcould be anything that holds a subjective opinion, such as an online review,Movie rating, Comments on Blog posts etc. This paper presents a novel approachthat classify text in two-dimensional Emotional space, based on the sentimentsof the author. The approach uses existing lexical resources to extract featureset, which is trained using Supervised Learning techniques.
arxiv-6300-63 | Structured Dictionary Learning for Classification | http://arxiv.org/pdf/1406.1943v1.pdf | author:Yuanming Suo, Minh Dao, Umamahesh Srinivas, Vishal Monga, Trac D. Tran category:cs.CV published:2014-06-08 summary:Sparsity driven signal processing has gained tremendous popularity in thelast decade. At its core, the assumption is that the signal of interest issparse with respect to either a fixed transformation or a signal dependentdictionary. To better capture the data characteristics, various dictionarylearning methods have been proposed for both reconstruction and classificationtasks. For classification particularly, most approaches proposed so far havefocused on designing explicit constraints on the sparse code to improveclassification accuracy while simply adopting $l_0$-norm or $l_1$-norm forsparsity regularization. Motivated by the success of structured sparsity in thearea of Compressed Sensing, we propose a structured dictionary learningframework (StructDL) that incorporates the structure information on both groupand task levels in the learning process. Its benefits are two-fold: (i) thelabel consistency between dictionary atoms and training data are implicitlyenforced; and (ii) the classification performance is more robust in the casesof a small dictionary size or limited training data than other techniques.Using the subspace model, we derive the conditions for StructDL to guaranteethe performance and show theoretically that StructDL is superior to $l_0$-normor $l_1$-norm regularized dictionary learning for classification. Extensiveexperiments have been performed on both synthetic simulations and real worldapplications, such as face recognition and object classification, todemonstrate the validity of the proposed DL framework.
arxiv-6300-64 | Simulation based Hardness Evaluation of a Multi-Objective Genetic Algorithm | http://arxiv.org/pdf/1406.2613v1.pdf | author:Shahab U. Ansari, Sameen Mansha category:cs.NE published:2014-06-07 summary:Studies have shown that multi-objective optimization problems are hardproblems. Such problems either require longer time to converge to an optimumsolution, or may not converge at all. Recently some researchers have claimedthat real culprit for increasing the hardness of multi-objective problems arenot the number of objectives themselves rather it is the increased size ofsolution set, incompatibility of solutions, and high probability of findingsuboptimal solution due to increased number of local maxima. In this work, wehave setup a simple framework for the evaluation of hardness of multi-objectivegenetic algorithms (MOGA). The algorithm is designed for a pray-predator gamewhere a player is to improve its lifespan, challenging level and usability ofthe game arena through number of generations. A rigorous set of experiments areperformed for quantifying the hardness in terms of evolution for increasingnumber of objective functions. In genetic algorithm, crossover and mutationwith equal probability are applied to create offspring in each generation.First, each objective function is maximized individually by ranking thecompeting players on the basis of the fitness (cost) function, and then amulti-objective cost function (sum of individual cost functions) is maximizedwith ranking, and also without ranking where dominated solutions are alsoallowed to evolve.
arxiv-6300-65 | On the Number of Linear Regions of Deep Neural Networks | http://arxiv.org/pdf/1402.1869v2.pdf | author:Guido Mont√∫far, Razvan Pascanu, Kyunghyun Cho, Yoshua Bengio category:stat.ML cs.LG cs.NE published:2014-02-08 summary:We study the complexity of functions computable by deep feedforward neuralnetworks with piecewise linear activations in terms of the symmetries and thenumber of linear regions that they have. Deep networks are able to sequentiallymap portions of each layer's input-space to the same output. In this way, deepmodels compute functions that react equally to complicated patterns ofdifferent inputs. The compositional structure of these functions enables themto re-use pieces of computation exponentially often in terms of the network'sdepth. This paper investigates the complexity of such compositional maps andcontributes new theoretical results regarding the advantage of depth for neuralnetworks with piecewise linear activation functions. In particular, ouranalysis is not specific to a single family of models, and as an example, weemploy it for rectifier and maxout networks. We improve complexity bounds frompre-existing work and investigate the behavior of units in higher layers.
arxiv-6300-66 | Shape-from-intrinsic operator | http://arxiv.org/pdf/1406.1925v1.pdf | author:Davide Boscaini, Davide Eynard, Michael M. Bronstein category:cs.CV published:2014-06-07 summary:Shape-from-X is an important class of problems in the fields of geometryprocessing, computer graphics, and vision, attempting to recover the structureof a shape from some observations. In this paper, we formulate the problem ofshape-from-operator (SfO), recovering an embedding of a mesh from intrinsicdifferential operators defined on the mesh. Particularly interesting instancesof our SfO problem include synthesis of shape analogies, shape-from-Laplacianreconstruction, and shape exaggeration. Numerically, we approach the SfOproblem by splitting it into two optimization sub-problems that are applied inan alternating scheme: metric-from-operator (reconstruction of the discretemetric from the intrinsic operator) and embedding-from-metric (finding a shapeembedding that would realize a given metric, a setting of the multidimensionalscaling problem).
arxiv-6300-67 | Compressed Gaussian Process | http://arxiv.org/pdf/1406.1916v1.pdf | author:Rajarshi Guhaniyogi, David B. Dunson category:stat.ML published:2014-06-07 summary:Nonparametric regression for massive numbers of samples (n) and features (p)is an increasingly important problem. In big n settings, a common strategy isto partition the feature space, and then separately apply simple models to eachpartition set. We propose an alternative approach, which avoids suchpartitioning and the associated sensitivity to neighborhood choice and distancemetrics, by using random compression combined with Gaussian process regression.The proposed approach is particularly motivated by the setting in which theresponse is conditionally independent of the features given the projection to alow dimensional manifold. Conditionally on the random compression matrix and asmoothness parameter, the posterior distribution for the regression surface andposterior predictive distributions are available analytically. Running theanalysis in parallel for many random compression matrices and smoothnessparameters, model averaging is used to combine the results. The algorithm canbe implemented rapidly even in very big n and p problems, has strongtheoretical justification, and is found to yield state of the art predictiveperformance.
arxiv-6300-68 | Refinement-Cut: User-Guided Segmentation Algorithm for Translational Science | http://arxiv.org/pdf/1406.1906v1.pdf | author:Jan Egger category:cs.CV published:2014-06-07 summary:In this contribution, a semi-automatic segmentation algorithm for (medical)image analysis is presented. More precise, the approach belongs to the categoryof interactive contouring algorithms, which provide real-time feedback of thesegmentation result. However, even with interactive real-time contouringapproaches there are always cases where the user cannot find a satisfyingsegmentation, e.g. due to homogeneous appearances between the object and thebackground, or noise inside the object. For these difficult cases the algorithmstill needs additional user support. However, this additional user supportshould be intuitive and rapid integrated into the segmentation process, withoutbreaking the interactive real-time segmentation feedback. I propose a solutionwhere the user can support the algorithm by an easy and fast placement of oneor more seed points to guide the algorithm to a satisfying segmentation resultalso in difficult cases. These additional seed(s) restrict(s) the calculationof the segmentation for the algorithm, but at the same time, still enable tocontinue with the interactive real-time feedback segmentation. For a practicaland genuine application in translational science, the approach has been testedon medical data from the clinical routine in 2D and 3D.
arxiv-6300-69 | Box Drawings for Learning with Imbalanced Data | http://arxiv.org/pdf/1403.3378v2.pdf | author:Siong Thye Goh, Cynthia Rudin category:stat.ML cs.LG published:2014-03-13 summary:The vast majority of real world classification problems are imbalanced,meaning there are far fewer data from the class of interest (the positiveclass) than from other classes. We propose two machine learning algorithms tohandle highly imbalanced classification problems. The classifiers constructedby both methods are created as unions of parallel axis rectangles around thepositive examples, and thus have the benefit of being interpretable. The firstalgorithm uses mixed integer programming to optimize a weighted balance betweenpositive and negative class accuracies. Regularization is introduced to improvegeneralization performance. The second method uses an approximation in order toassist with scalability. Specifically, it follows a \textit{characterize thendiscriminate} approach, where the positive class is characterized first byboxes, and then each box boundary becomes a separate discriminative classifier.This method has the computational advantages that it can be easilyparallelized, and considers only the relevant regions of feature space.
arxiv-6300-70 | Toward verbalizing ontologies in isiZulu | http://arxiv.org/pdf/1406.1870v1.pdf | author:C. Maria Keet, Langa Khumalo category:cs.CL I.2.1 published:2014-06-07 summary:IsiZulu is one of the eleven official languages of South Africa and roughlyhalf the population can speak it. It is the first (home) language for over 10million people in South Africa. Only a few computational resources exist forisiZulu and its related Nguni languages, yet the imperative for tooldevelopment exists. We focus on natural language generation, and the grammaroptions and preferences in particular, which will inform verbalization ofknowledge representation languages and could contribute to machine translation.The verbalization pattern specification shows that the grammar rules areelaborate and there are several options of which one may have preference. Wedevised verbalization patterns for subsumption, basic disjointness, existentialand universal quantification, and conjunction. This was evaluated in a surveyamong linguists and non-linguists. Some differences between linguists andnon-linguists can be observed, with the former much more in agreement, andpreferences depend on the overall structure of the sentence, such as singularfor subsumption and plural in other cases.
arxiv-6300-71 | An Inequality with Applications to Structured Sparsity and Multitask Dictionary Learning | http://arxiv.org/pdf/1402.1864v2.pdf | author:Andreas Maurer, Massimiliano Pontil, Bernardino Romera-Paredes category:cs.LG stat.ML published:2014-02-08 summary:From concentration inequalities for the suprema of Gaussian or Rademacherprocesses an inequality is derived. It is applied to sharpen existing and toderive novel bounds on the empirical Rademacher complexities of unit balls invarious norms appearing in the context of structured sparsity and multitaskdictionary learning or matrix factorization. A key role is played by thelargest eigenvalue of the data covariance matrix.
arxiv-6300-72 | Analyzing noise in autoencoders and deep networks | http://arxiv.org/pdf/1406.1831v1.pdf | author:Ben Poole, Jascha Sohl-Dickstein, Surya Ganguli category:cs.NE cs.LG published:2014-06-06 summary:Autoencoders have emerged as a useful framework for unsupervised learning ofinternal representations, and a wide variety of apparently conceptuallydisparate regularization techniques have been proposed to generate usefulfeatures. Here we extend existing denoising autoencoders to additionally injectnoise before the nonlinearity, and at the hidden unit activations. We show thata wide variety of previous methods, including denoising, contractive, andsparse autoencoders, as well as dropout can be interpreted using thisframework. This noise injection framework reaps practical benefits by providinga unified strategy to develop new internal representations by designing thenature of the injected noise. We show that noisy autoencoders outperformdenoising autoencoders at the very task of denoising, and are competitive withother single-layer techniques on MNIST, and CIFAR-10. We also show that typesof noise other than dropout improve performance in a deep network throughsparsifying, decorrelating, and spreading information across representations.
arxiv-6300-73 | A New 2.5D Representation for Lymph Node Detection using Random Sets of Deep Convolutional Neural Network Observations | http://arxiv.org/pdf/1406.2639v1.pdf | author:Holger R. Roth, Le Lu, Ari Seff, Kevin M. Cherry, Joanne Hoffman, Shijun Wang, Jiamin Liu, Evrim Turkbey, Ronald M. Summers category:cs.CV cs.LG cs.NE published:2014-06-06 summary:Automated Lymph Node (LN) detection is an important clinical diagnostic taskbut very challenging due to the low contrast of surrounding structures inComputed Tomography (CT) and to their varying sizes, poses, shapes and sparselydistributed locations. State-of-the-art studies show the performance range of52.9% sensitivity at 3.1 false-positives per volume (FP/vol.), or 60.9% at 6.1FP/vol. for mediastinal LN, by one-shot boosting on 3D HAAR features. In thispaper, we first operate a preliminary candidate generation stage, towards 100%sensitivity at the cost of high FP levels (40 per patient), to harvest volumesof interest (VOI). Our 2.5D approach consequently decomposes any 3D VOI byresampling 2D reformatted orthogonal views N times, via scale, randomtranslations, and rotations with respect to the VOI centroid coordinates. Theserandom views are then used to train a deep Convolutional Neural Network (CNN)classifier. In testing, the CNN is employed to assign LN probabilities for allN random views that can be simply averaged (as a set) to compute the finalclassification probability per VOI. We validate the approach on two datasets:90 CT volumes with 388 mediastinal LNs and 86 patients with 595 abdominal LNs.We achieve sensitivities of 70%/83% at 3 FP/vol. and 84%/90% at 6 FP/vol. inmediastinum and abdomen respectively, which drastically improves over theprevious state-of-the-art work.
arxiv-6300-74 | Advances in Learning Bayesian Networks of Bounded Treewidth | http://arxiv.org/pdf/1406.1411v2.pdf | author:Siqi Nie, Denis Deratani Maua, Cassio Polpo de Campos, Qiang Ji category:cs.AI cs.LG stat.ML 68T37 published:2014-06-05 summary:This work presents novel algorithms for learning Bayesian network structureswith bounded treewidth. Both exact and approximate methods are developed. Theexact method combines mixed-integer linear programming formulations forstructure learning and treewidth computation. The approximate method consistsin uniformly sampling $k$-trees (maximal graphs of treewidth $k$), andsubsequently selecting, exactly or approximately, the best structure whosemoral graph is a subgraph of that $k$-tree. Some properties of these methodsare discussed and proven. The approaches are empirically compared to each otherand to a state-of-the-art method for learning bounded treewidth structures on acollection of public data sets with up to 100 variables. The experiments showthat our exact algorithm outperforms the state of the art, and that theapproximate approach is fairly accurate.
arxiv-6300-75 | Computational role of eccentricity dependent cortical magnification | http://arxiv.org/pdf/1406.1770v1.pdf | author:Tomaso Poggio, Jim Mutch, Leyla Isik category:cs.LG q-bio.NC published:2014-06-06 summary:We develop a sampling extension of M-theory focused on invariance to scaleand translation. Quite surprisingly, the theory predicts an architecture ofearly vision with increasing receptive field sizes and a high resolution fovea-- in agreement with data about the cortical magnification factor, V1 and theretina. From the slope of the inverse of the magnification factor, M-theorypredicts a cortical "fovea" in V1 in the order of $40$ by $40$ basic units ateach receptive field size -- corresponding to a foveola of size around $26$minutes of arc at the highest resolution, $\approx 6$ degrees at the lowestresolution. It also predicts uniform scale invariance over a fixed range ofscales independently of eccentricity, while translation invariance shoulddepend linearly on spatial frequency. Bouma's law of crowding follows in thetheory as an effect of cortical area-by-cortical area pooling; the Boumaconstant is the value expected if the signature responsible for recognition inthe crowding experiments originates in V2. From a broader perspective, theemerging picture suggests that visual recognition under natural conditionstakes place by composing information from a set of fixations, with eachfixation providing recognition from a space-scale image fragment -- that is animage patch represented at a set of increasing sizes and decreasingresolutions.
arxiv-6300-76 | Linguistic Analysis of Requirements of a Space Project and their Conformity with the Recommendations Proposed by a Controlled Natural Language | http://arxiv.org/pdf/1406.1765v1.pdf | author:Anne Condamines, Maxime Warnier category:cs.SE cs.CL published:2014-06-06 summary:The long term aim of the project carried out by the French National SpaceAgency (CNES) is to design a writing guide based on the real and regularwriting of requirements. As a first step in the project, this paper proposes alin-guistic analysis of requirements written in French by CNES engineers. Theaim is to determine to what extent they conform to two rules laid down inINCOSE, a recent guide for writing requirements. Although CNES engineers arenot obliged to follow any Controlled Natural Language in their writing ofrequirements, we believe that language regularities are likely to emerge fromthis task, mainly due to the writers' experience. The issue is approached usingnatural language processing tools to identify sentences that do not comply withINCOSE rules. We further review these sentences to understand why therecommendations cannot (or should not) always be applied when specifyinglarge-scale projects.
arxiv-6300-77 | Towards a Better Understanding of the Local Attractor in Particle Swarm Optimization: Speed and Solution Quality | http://arxiv.org/pdf/1406.1691v1.pdf | author:Vanessa Lange, Manuel Schmitt, Rolf Wanka category:cs.NE I.2.8 published:2014-06-06 summary:Particle Swarm Optimization (PSO) is a popular nature-inspired meta-heuristicfor solving continuous optimization problems. Although this technique is widelyused, the understanding of the mechanisms that make swarms so successful isstill limited. We present the first substantial experimental investigation ofthe influence of the local attractor on the quality of exploration andexploitation. We compare in detail classical PSO with the social-only variantwhere local attractors are ignored. To measure the exploration capabilities, wedetermine how frequently both variants return results in the neighborhood ofthe global optimum. We measure the quality of exploitation by considering onlyfunction values from runs that reached a search point sufficiently close to theglobal optimum and then comparing in how many digits such values still deviatefrom the global minimum value. It turns out that the local attractorsignificantly improves the exploration, but sometimes reduces the quality ofthe exploitation. As a compromise, we propose and evaluate a hybrid PSO whichswitches off its local attractors at a certain point in time. The effectsmentioned can also be observed by measuring the potential of the swarm.
arxiv-6300-78 | Online Clustering of Bandits | http://arxiv.org/pdf/1401.8257v3.pdf | author:Claudio Gentile, Shuai Li, Giovanni Zappella category:cs.LG stat.ML published:2014-01-31 summary:We introduce a novel algorithmic approach to content recommendation based onadaptive clustering of exploration-exploitation ("bandit") strategies. Weprovide a sharp regret analysis of this algorithm in a standard stochasticnoise setting, demonstrate its scalability properties, and prove itseffectiveness on a number of artificial and real-world datasets. Ourexperiments show a significant increase in prediction performance overstate-of-the-art methods for bandit problems.
arxiv-6300-79 | Ant Colony Optimization for Inferring Key Gene Interactions | http://arxiv.org/pdf/1406.1626v1.pdf | author:Khalid Raza, Mahish Kohli category:cs.NE cs.CE published:2014-06-06 summary:Inferring gene interaction network from gene expression data is an importanttask in systems biology research. The gene interaction network, especially keyinteractions, plays an important role in identifying biomarkers for diseasethat further helps in drug design. Ant colony optimization is an optimizationalgorithm based on natural evolution and has been used in many optimizationproblems. In this paper, we applied ant colony optimization algorithm forinferring the key gene interactions from gene expression data. The algorithmhas been tested on two different kinds of benchmark datasets and observed thatit successfully identify some key gene interactions.
arxiv-6300-80 | Separable Cosparse Analysis Operator Learning | http://arxiv.org/pdf/1406.1621v1.pdf | author:Matthias Seibert, Julian W√∂rmann, R√©mi Gribonval, Martin Kleinsteuber category:cs.LG stat.ML published:2014-06-06 summary:The ability of having a sparse representation for a certain class of signalshas many applications in data analysis, image processing, and other researchfields. Among sparse representations, the cosparse analysis model has recentlygained increasing interest. Many signals exhibit a multidimensional structure,e.g. images or three-dimensional MRI scans. Most data analysis and learningalgorithms use vectorized signals and thereby do not account for thisunderlying structure. The drawback of not taking the inherent structure intoaccount is a dramatic increase in computational cost. We propose an algorithmfor learning a cosparse Analysis Operator that adheres to the preexistingstructure of the data, and thus allows for a very efficient implementation.This is achieved by enforcing a separable structure on the learned operator.Our learning algorithm is able to deal with multidimensional data of arbitraryorder. We evaluate our method on volumetric data at the example ofthree-dimensional MRI scans.
arxiv-6300-81 | Machine learning approach for text and document mining | http://arxiv.org/pdf/1406.1580v1.pdf | author:Vishwanath Bijalwan, Pinki Kumari, Jordan Pascual, Vijay Bhaskar Semwal category:cs.IR cs.LG published:2014-06-06 summary:Text Categorization (TC), also known as Text Classification, is the task ofautomatically classifying a set of text documents into different categoriesfrom a predefined set. If a document belongs to exactly one of the categories,it is a single-label classification task; otherwise, it is a multi-labelclassification task. TC uses several tools from Information Retrieval (IR) andMachine Learning (ML) and has received much attention in the last years fromboth researchers in the academia and industry developers. In this paper, wefirst categorize the documents using KNN based machine learning approach andthen return the most relevant documents.
arxiv-6300-82 | Consistent procedures for cluster tree estimation and pruning | http://arxiv.org/pdf/1406.1546v1.pdf | author:Kamalika Chaudhuri, Sanjoy Dasgupta, Samory Kpotufe, Ulrike von Luxburg category:stat.ML published:2014-06-05 summary:For a density $f$ on ${\mathbb R}^d$, a {\it high-density cluster} is anyconnected component of $\{x: f(x) \geq \lambda\}$, for some $\lambda > 0$. Theset of all high-density clusters forms a hierarchy called the {\it clustertree} of $f$. We present two procedures for estimating the cluster tree givensamples from $f$. The first is a robust variant of the single linkage algorithmfor hierarchical clustering. The second is based on the $k$-nearest neighborgraph of the samples. We give finite-sample convergence rates for thesealgorithms which also imply consistency, and we derive lower bounds on thesample complexity of cluster tree estimation. Finally, we study a tree pruningprocedure that guarantees, under milder conditions than usual, to removeclusters that are spurious while recovering those that are salient.
arxiv-6300-83 | Towards building a Crowd-Sourced Sky Map | http://arxiv.org/pdf/1406.1528v1.pdf | author:Dustin Lang, David W. Hogg, Bernhard Scholkopf category:cs.CV astro-ph.IM published:2014-06-05 summary:We describe a system that builds a high dynamic-range and wide-angle image ofthe night sky by combining a large set of input images. The method makes use ofpixel-rank information in the individual input images to improve a "consensus"pixel rank in the combined image. Because it only makes use of ranks and thecomplexity of the algorithm is linear in the number of images, the method isuseful for large sets of uncalibrated images that might have undergone unknownnon-linear tone mapping transformations for visualization or aesthetic reasons.We apply the method to images of the night sky (of unknown provenance)discovered on the Web. The method permits discovery of astronomical objects orfeatures that are not visible in any of the input images taken individually.More importantly, however, it permits scientific exploitation of a huge sourceof astronomical images that would not be available to astronomical researchwithout our automatic system.
arxiv-6300-84 | A Continuous Max-Flow Approach to Multi-Labeling Problems under Arbitrary Region Regularization | http://arxiv.org/pdf/1405.0892v2.pdf | author:John S. H. Baxter, Martin Rajchl, Jing Yuan, Terry M. Peters category:cs.CV published:2014-05-05 summary:The incorporation of region regularization into max-flow segmentation hastraditionally focused on ordering and part-whole relationships. A side effectof the development of such models is that it constrained regularization only tothose cases, rather than allowing for arbitrary region regularization. DirectedAcyclic Graphical Max-Flow (DAGMF) segmentation overcomes these limitations byallowing for the algorithm designer to specify an arbitrary directed acyclicgraph to structure a max-flow segmentation. This allows for individual 'parts'to be a member of multiple distinct 'wholes.'
arxiv-6300-85 | A Continuous Max-Flow Approach to General Hierarchical Multi-Labeling Problems | http://arxiv.org/pdf/1404.0336v2.pdf | author:John S. H. Baxter, Martin Rajchl, Jing Yuan, Terry M. Peters category:cs.CV published:2014-04-01 summary:Multi-region segmentation algorithms often have the onus of incorporatingcomplex anatomical knowledge representing spatial or geometric relationshipsbetween objects, and general-purpose methods of addressing this knowledge in anoptimization-based manner have thus been lacking. This paper presentsGeneralized Hierarchical Max-Flow (GHMF) segmentation, which captures simpleanatomical part-whole relationships in the form of an unconstrained hierarchy.Regularization can then be applied to both parts and wholes independently,allowing for spatial grouping and clustering of labels in a globally optimalconvex optimization framework. For the purposes of ready integration into avariety of segmentation tasks, the hierarchies can be presented in run-time,allowing for the segmentation problem to be readily specified and alternativesexplored without undue programming effort or recompilation.
arxiv-6300-86 | Generating Sequences With Recurrent Neural Networks | http://arxiv.org/pdf/1308.0850v5.pdf | author:Alex Graves category:cs.NE cs.CL published:2013-08-04 summary:This paper shows how Long Short-term Memory recurrent neural networks can beused to generate complex sequences with long-range structure, simply bypredicting one data point at a time. The approach is demonstrated for text(where the data are discrete) and online handwriting (where the data arereal-valued). It is then extended to handwriting synthesis by allowing thenetwork to condition its predictions on a text sequence. The resulting systemis able to generate highly realistic cursive handwriting in a wide variety ofstyles.
arxiv-6300-87 | An Easy to Use Repository for Comparing and Improving Machine Learning Algorithm Usage | http://arxiv.org/pdf/1405.7292v2.pdf | author:Michael R. Smith, Andrew White, Christophe Giraud-Carrier, Tony Martinez category:stat.ML cs.LG published:2014-05-28 summary:The results from most machine learning experiments are used for a specificpurpose and then discarded. This results in a significant loss of informationand requires rerunning experiments to compare learning algorithms. This alsorequires implementation of another algorithm for comparison, that may notalways be correctly implemented. By storing the results from previousexperiments, machine learning algorithms can be compared easily and theknowledge gained from them can be used to improve their performance. Thepurpose of this work is to provide easy access to previous experimental resultsfor learning and comparison. These stored results are comprehensive -- storingthe prediction for each test instance as well as the learning algorithm,hyperparameters, and training set that were used. Previous results areparticularly important for meta-learning, which, in a broad sense, is theprocess of learning from previous machine learning results such that thelearning process is improved. While other experiment databases do exist, one ofour focuses is on easy access to the data. We provide meta-learning data setsthat are ready to be downloaded for meta-learning experiments. In addition,queries to the underlying database can be made if specific information isdesired. We also differ from previous experiment databases in that ourdatabases is designed at the instance level, where an instance is an example ina data set. We store the predictions of a learning algorithm trained on aspecific training set for each instance in the test set. Data set levelinformation can then be obtained by aggregating the results from the instances.The instance level information can be used for many tasks such as determiningthe diversity of a classifier or algorithmically determining the optimal subsetof training instances for a learning algorithm.
arxiv-6300-88 | Learning the Information Divergence | http://arxiv.org/pdf/1406.1385v1.pdf | author:Onur Dikmen, Zhirong Yang, Erkki Oja category:cs.LG published:2014-06-05 summary:Information divergence that measures the difference between two nonnegativematrices or tensors has found its use in a variety of machine learningproblems. Examples are Nonnegative Matrix/Tensor Factorization, StochasticNeighbor Embedding, topic models, and Bayesian network optimization. Thesuccess of such a learning task depends heavily on a suitable divergence. Alarge variety of divergences have been suggested and analyzed, but very fewresults are available for an objective choice of the optimal divergence for agiven task. Here we present a framework that facilitates automatic selection ofthe best divergence among a given family, based on standard maximum likelihoodestimation. We first propose an approximated Tweedie distribution for thebeta-divergence family. Selecting the best beta then becomes a machine learningproblem solved by maximum likelihood. Next, we reformulate alpha-divergence interms of beta-divergence, which enables automatic selection of alpha by maximumlikelihood with reuse of the learning principle for beta-divergence.Furthermore, we show the connections between gamma and beta-divergences as wellas R\'enyi and alpha-divergences, such that our automatic selection frameworkis extended to non-separable divergences. Experiments on both synthetic andreal-world data demonstrate that our method can quite accurately selectinformation divergence across different learning problems and variousdivergence families.
arxiv-6300-89 | Basis Identification for Automatic Creation of Pronunciation Lexicon for Proper Names | http://arxiv.org/pdf/1406.1280v1.pdf | author:Sunil Kumar Kopparapu, M Laxminarayana category:cs.CL published:2014-06-05 summary:Development of a proper names pronunciation lexicon is usually a manualeffort which can not be avoided. Grapheme to phoneme (G2P) conversion modules,in literature, are usually rule based and work best for non-proper names in aparticular language. Proper names are foreign to a G2P module. We follow anoptimization approach to enable automatic construction of proper namespronunciation lexicon. The idea is to construct a small orthogonal set of words(basis) which can span the set of names in a given database. We propose twoalgorithms for the construction of this basis. The transcription lexicon of allthe proper names in a database can be produced by the manual transcription ofonly the small set of basis words. We first construct a cost function and showthat the minimization of the cost function results in a basis. We deriveconditions for convergence of this cost function and validate themexperimentally on a very large proper name database. Experiments show thetranscription can be achieved by transcribing a set of small number of basiswords. The algorithms proposed are generic and independent of language; howeverperformance is better if the proper names have same origin, namely, samelanguage or geographical region.
arxiv-6300-90 | Illusory Shapes via Phase Transition | http://arxiv.org/pdf/1406.1265v1.pdf | author:Yoon Mo Jung, Jianhong Jackie Shen category:math.OC cs.CV q-bio.NC published:2014-06-05 summary:We propose a new variational illusory shape (VIS) model via phase fields andphase transitions. It is inspired by the first-order variational illusorycontour (VIC) model proposed by Jung and Shen [{\em J. Visual Comm. ImageRepres.}, {\bf 19}:42-55, 2008]. Under the new VIS model, illusory shapes arerepresented by phase values close to 1 while the rest by values close to 0. The0-1 transition is achieved by an elliptic energy with a double-well potential,as in the theory of $\Gamma$-convergence. The VIS model is non-convex, with thezero field as its trivial global optimum. To seek visually meaningful localoptima that can induce illusory shapes, an iterative algorithm is designed andits convergence behavior is closely studied. Several generic numerical examplesconfirm the versatility of the model and the algorithm.
arxiv-6300-91 | Image retrieval with hierarchical matching pursuit | http://arxiv.org/pdf/1406.0588v2.pdf | author:Shasha Bu, Yu-Jin Zhang category:cs.CV published:2014-06-03 summary:A novel representation of images for image retrieval is introduced in thispaper, by using a new type of feature with remarkable discriminative power.Despite the multi-scale nature of objects, most existing models perform featureextraction on a fixed scale, which will inevitably degrade the performance ofthe whole system. Motivated by this, we introduce a hierarchical sparse codingarchitecture for image retrieval to explore multi-scale cues. Sparse codesextracted on lower layers are transmitted to higher layers recursively. Withthis mechanism, cues from different scales are fused. Experiments on theHolidays dataset show that the proposed method achieves an excellent retrievalperformance with a small code length.
arxiv-6300-92 | Shared Representation Learning for Heterogeneous Face Recognition | http://arxiv.org/pdf/1406.1247v1.pdf | author:Dong Yi, Zhen Lei, Shengcai Liao, Stan Z. Li category:cs.CV published:2014-06-05 summary:After intensive research, heterogenous face recognition is still achallenging problem. The main difficulties are owing to the complexrelationship between heterogenous face image spaces. The heterogeneity isalways tightly coupled with other variations, which makes the relationship ofheterogenous face images highly nonlinear. Many excellent methods have beenproposed to model the nonlinear relationship, but they apt to overfit to thetraining set, due to limited samples. Inspired by the unsupervised algorithmsin deep learning, this paper proposes an novel framework for heterogeneous facerecognition. We first extract Gabor features at some localized facial points,and then use Restricted Boltzmann Machines (RBMs) to learn a sharedrepresentation locally to remove the heterogeneity around each facial point.Finally, the shared representations of local RBMs are connected together andprocessed by PCA. Two problems (Sketch-Photo and NIR-VIS) and three databasesare selected to evaluate the proposed method. For Sketch-Photo problem, weobtain perfect results on the CUFS database. For NIR-VIS problem, we producenew state-of-the-art performance on the CASIA HFB and NIR-VIS 2.0 databases.
arxiv-6300-93 | Towards the Safety of Human-in-the-Loop Robotics: Challenges and Opportunities for Safety Assurance of Robotic Co-Workers | http://arxiv.org/pdf/1404.2229v3.pdf | author:Kerstin Eder, Chris Harper, Ute Leonards category:cs.RO cs.LG I.2.9 published:2014-04-08 summary:The success of the human-robot co-worker team in a flexible manufacturingenvironment where robots learn from demonstration heavily relies on the correctand safe operation of the robot. How this can be achieved is a challenge thatrequires addressing both technical as well as human-centric research questions.In this paper we discuss the state of the art in safety assurance, existing aswell as emerging standards in this area, and the need for new approaches tosafety assurance in the context of learning machines. We then focus on roboticlearning from demonstration, the challenges these techniques pose to safetyassurance and indicate opportunities to integrate safety considerations intoalgorithms "by design". Finally, from a human-centric perspective, we stipulatethat, to achieve high levels of safety and ultimately trust, the roboticco-worker must meet the innate expectations of the humans it works with. It isour aim to stimulate a discussion focused on the safety aspects ofhuman-in-the-loop robotics, and to foster multidisciplinary collaboration toaddress the research challenges identified.
arxiv-6300-94 | The Best Templates Match Technique For Example Based Machine Translation | http://arxiv.org/pdf/1406.1241v1.pdf | author:T. El-Shishtawy, A. El-Sammak category:cs.CL published:2014-06-04 summary:It has been proved that large scale realistic Knowledge Based MachineTranslation applications require acquisition of huge knowledge about languageand about the world. This knowledge is encoded in computational grammars,lexicons and domain models. Another approach which avoids the need forcollecting and analyzing massive knowledge, is the Example Based approach,which is the topic of this paper. We show through the paper that using ExampleBased in its native form is not suitable for translating into Arabic. Thereforea modification to the basic approach is presented to improve the accuracy ofthe translation process. The basic idea of the new approach is to improve thetechnique by which template-based approaches select the appropriate templates.
arxiv-6300-95 | A Geometric Method to Obtain the Generation Probability of a Sentence | http://arxiv.org/pdf/1406.1234v1.pdf | author:Chen Lijiang category:cs.CL cs.AI math.ST stat.CO stat.ME stat.TH published:2014-06-04 summary:"How to generate a sentence" is the most critical and difficult problem inall the natural language processing technologies. In this paper, we present anew approach to explain the generation process of a sentence from theperspective of mathematics. Our method is based on the premise that in ourbrain a sentence is a part of a word network which is formed by many wordnodes. Experiments show that the probability of the entire sentence can beobtained by the probabilities of single words and the probabilities of theco-occurrence of word pairs, which indicate that human use the synthesis methodto generate a sentence.
arxiv-6300-96 | Multi-task Neural Networks for QSAR Predictions | http://arxiv.org/pdf/1406.1231v1.pdf | author:George E. Dahl, Navdeep Jaitly, Ruslan Salakhutdinov category:stat.ML cs.LG cs.NE published:2014-06-04 summary:Although artificial neural networks have occasionally been used forQuantitative Structure-Activity/Property Relationship (QSAR/QSPR) studies inthe past, the literature has of late been dominated by other machine learningtechniques such as random forests. However, a variety of new neural nettechniques along with successful applications in other domains have renewedinterest in network approaches. In this work, inspired by the winning team'suse of neural networks in a recent QSAR competition, we used an artificialneural network to learn a function that predicts activities of compounds formultiple assays at the same time. We conducted experiments leveraging recentmethods for dealing with overfitting in neural networks as well as other tricksfrom the neural networks literature. We compared our methods to alternativemethods reported to perform well on these tasks and found that our neural netmethods provided superior performance.
arxiv-6300-97 | A Semantic Approach to Summarization | http://arxiv.org/pdf/1406.1203v1.pdf | author:Divyanshu Bhartiya, Ashudeep Singh category:cs.CL published:2014-06-04 summary:Sentence extraction based summarization methods has some limitations as itdoesn't go into the semantics of the document. Also, it lacks the capability ofsentence generation which is intuitive to humans. Here we present a novelmethod to summarize text documents taking the process to semantic levels withthe use of WordNet and other resources, and using a technique for sentencegeneration. We involve semantic role labeling to get the semanticrepresentation of text and use of segmentation to form clusters of the relatedpieces of text. Picking out the centroids and sentence generation completes thetask. We evaluate our system against human composed summaries and also presentan evaluation done by humans to measure the quality attributes of oursummaries.
arxiv-6300-98 | Identifying Duplicate and Contradictory Information in Wikipedia | http://arxiv.org/pdf/1406.1143v1.pdf | author:Sarah Weissman, Samet Ayhan, Joshua Bradley, Jimmy Lin category:cs.IR cs.CL cs.DL cs.SI published:2014-06-04 summary:Our study identifies sentences in Wikipedia articles that are eitheridentical or highly similar by applying techniques for near-duplicate detectionof web pages. This is accomplished with a MapReduce implementation of minhashto identify clusters of sentences with high Jaccard similarity. We show thatthese clusters can be categorized into six different types, two of which areparticularly interesting: identical sentences quantify the extent to whichcontent in Wikipedia is copied and pasted, and near-duplicate sentences thatstate contradictory facts point to quality issues in Wikipedia.
arxiv-6300-99 | Neural Variational Inference and Learning in Belief Networks | http://arxiv.org/pdf/1402.0030v2.pdf | author:Andriy Mnih, Karol Gregor category:cs.LG stat.ML published:2014-01-31 summary:Highly expressive directed latent variable models, such as sigmoid beliefnetworks, are difficult to train on large datasets because exact inference inthem is intractable and none of the approximate inference methods that havebeen applied to them scale well. We propose a fast non-iterative approximateinference method that uses a feedforward network to implement efficient exactsampling from the variational posterior. The model and this inference networkare trained jointly by maximizing a variational lower bound on thelog-likelihood. Although the naive estimator of the inference model gradient istoo high-variance to be useful, we make it practical by applying severalstraightforward model-independent variance reduction techniques. Applying ourapproach to training sigmoid belief networks and deep autoregressive networks,we show that it outperforms the wake-sleep algorithm on MNIST and achievesstate-of-the-art results on the Reuters RCV1 document dataset.
arxiv-6300-100 | PAC Learning, VC Dimension, and the Arithmetic Hierarchy | http://arxiv.org/pdf/1406.1111v1.pdf | author:Wesley Calvert category:math.LO cs.LG cs.LO 03D80, 03D45 I.2.6 published:2014-06-04 summary:We compute that the index set of PAC-learnable concept classes is$m$-complete $\Sigma^0_3$ within the set of indices for all concept classes ofa reasonable form. All concept classes considered are computable enumerationsof computable $\Pi^0_1$ classes, in a sense made precise here. This family ofconcept classes is sufficient to cover all standard examples, and also has theproperty that PAC learnability is equivalent to finite VC dimension.
arxiv-6300-101 | A variational approach to stable principal component pursuit | http://arxiv.org/pdf/1406.1089v1.pdf | author:Aleksandr Aravkin, Stephen Becker, Volkan Cevher, Peder Olsen category:math.OC stat.ML published:2014-06-04 summary:We introduce a new convex formulation for stable principal component pursuit(SPCP) to decompose noisy signals into low-rank and sparse representations. Fornumerical solutions of our SPCP formulation, we first develop a convexvariational framework and then accelerate it with quasi-Newton methods. Weshow, via synthetic and real data experiments, that our approach offersadvantages over the classical SPCP formulations in scalability and practicalparameter selection.
arxiv-6300-102 | Learning to Diversify via Weighted Kernels for Classifier Ensemble | http://arxiv.org/pdf/1406.1167v1.pdf | author:Xu-Cheng Yin, Chun Yang, Hong-Wei Hao category:cs.LG cs.CV I.5 published:2014-06-04 summary:Classifier ensemble generally should combine diverse component classifiers.However, it is difficult to give a definitive connection between diversitymeasure and ensemble accuracy. Given a list of available component classifiers,how to adaptively and diversely ensemble classifiers becomes a big challenge inthe literature. In this paper, we argue that diversity, not direct diversity onsamples but adaptive diversity with data, is highly correlated to ensembleaccuracy, and we propose a novel technology for classifier ensemble, learningto diversify, which learns to adaptively combine classifiers by consideringboth accuracy and diversity. Specifically, our approach, Learning TO Diversifyvia Weighted Kernels (L2DWK), performs classifier combination by optimizing adirect but simple criterion: maximizing ensemble accuracy and adaptivediversity simultaneously by minimizing a convex loss function. Given a measureformulation, the diversity is calculated with weighted kernels (i.e., thediversity is measured on the component classifiers' outputs which are kernelledand weighted), and the kernel weights are automatically learned. We minimizethis loss function by estimating the kernel weights in conjunction with theclassifier weights, and propose a self-training algorithm for conducting thisconvex optimization procedure iteratively. Extensive experiments on a varietyof 32 UCI classification benchmark datasets show that the proposed approachconsistently outperforms state-of-the-art ensembles such as Bagging, AdaBoost,Random Forests, Gasen, Regularized Selective Ensemble, and Ensemble Pruning viaSemi-Definite Programming.
arxiv-6300-103 | Optimal Data Collection For Informative Rankings Expose Well-Connected Graphs | http://arxiv.org/pdf/1207.6430v2.pdf | author:Braxton Osting, Christoph Brune, Stanley J. Osher category:stat.ML cs.LG stat.AP published:2012-07-26 summary:Given a graph where vertices represent alternatives and arcs representpairwise comparison data, the statistical ranking problem is to find apotential function, defined on the vertices, such that the gradient of thepotential function agrees with the pairwise comparisons. Our goal in this paperis to develop a method for collecting data for which the least squaresestimator for the ranking problem has maximal Fisher information. Our approach,based on experimental design, is to view data collection as a bi-leveloptimization problem where the inner problem is the ranking problem and theouter problem is to identify data which maximizes the informativeness of theranking. Under certain assumptions, the data collection problem decouples,reducing to a problem of finding multigraphs with large algebraic connectivity.This reduction of the data collection problem to graph-theoretic questions isone of the primary contributions of this work. As an application, we study theYahoo! Movie user rating dataset and demonstrate that the addition of a smallnumber of well-chosen pairwise comparisons can significantly increase theFisher informativeness of the ranking. As another application, we study the2011-12 NCAA football schedule and propose schedules with the same number ofgames which are significantly more informative. Using spectral clusteringmethods to identify highly-connected communities within the division, we arguethat the NCAA could improve its notoriously poor rankings by simply schedulingmore out-of-conference games.
arxiv-6300-104 | Integration of a Predictive, Continuous Time Neural Network into Securities Market Trading Operations | http://arxiv.org/pdf/1406.0968v1.pdf | author:Christopher S Kirk category:q-fin.CP cs.CE cs.NE published:2014-06-04 summary:This paper describes recent development and test implementation of acontinuous time recurrent neural network that has been configured to predictrates of change in securities. It presents outcomes in the context of populartechnical analysis indicators and highlights the potential impact of continuouspredictive capability on securities market trading operations.
arxiv-6300-105 | A Game-theoretic Machine Learning Approach for Revenue Maximization in Sponsored Search | http://arxiv.org/pdf/1406.0728v2.pdf | author:Di He, Wei Chen, Liwei Wang, Tie-Yan Liu category:cs.GT cs.LG published:2014-06-03 summary:Sponsored search is an important monetization channel for search engines, inwhich an auction mechanism is used to select the ads shown to users anddetermine the prices charged from advertisers. There have been several piecesof work in the literature that investigate how to design an auction mechanismin order to optimize the revenue of the search engine. However, due to someunrealistic assumptions used, the practical values of these studies are notvery clear. In this paper, we propose a novel \emph{game-theoretic machinelearning} approach, which naturally combines machine learning and game theory,and learns the auction mechanism using a bilevel optimization framework. Inparticular, we first learn a Markov model from historical data to describe howadvertisers change their bids in response to an auction mechanism, and then forany given auction mechanism, we use the learnt model to predict itscorresponding future bid sequences. Next we learn the auction mechanism throughempirical revenue maximization on the predicted bid sequences. We show that theempirical revenue will converge when the prediction period approaches infinity,and a Genetic Programming algorithm can effectively optimize this empiricalrevenue. Our experiments indicate that the proposed approach is able to producea much more effective auction mechanism than several baselines.
arxiv-6300-106 | Beyond $œá^2$ Difference: Learning Optimal Metric for Boundary Detection | http://arxiv.org/pdf/1406.0946v1.pdf | author:Fei He, Shengjin Wang category:cs.CV published:2014-06-04 summary:This letter focuses on solving the challenging problem of detecting naturalimage boundaries. A boundary usually refers to the border between two regionswith different semantic meanings. Therefore, a measurement of dissimilaritybetween image regions plays a pivotal role in boundary detection of naturalimages. To improve the performance of boundary detection, a Learning-basedBoundary Metric (LBM) is proposed to replace $\chi^2$ difference adopted by theclassical algorithm mPb. Compared with $\chi^2$ difference, LBM is composed ofa single layer neural network and an RBF kernel, and is fine-tuned bysupervised learning rather than human-crafted. It is more effective indescribing the dissimilarity between natural image regions while toleratinglarge variance of image data. After substituting $\chi^2$ difference with LBM,the F-measure metric of mPb on the BSDS500 benchmark is increased from 0.69 to0.71. Moreover, when image features are computed on a single scale, theproposed LBM algorithm still achieves competitive results compared with\emph{mPb}, which makes use of multi-scale image features.
arxiv-6300-107 | ACO Implementation for Sequence Alignment with Genetic Algorithms | http://arxiv.org/pdf/1406.0930v1.pdf | author:Aaron Lee, Livia King category:cs.CE cs.NE published:2014-06-04 summary:In this paper, we implement Ant Colony Optimization (ACO) for sequencealignment. ACO is a meta-heuristic recently developed for nearest neighborapproximations in large, NP-hard search spaces. Here we use a genetic algorithmapproach to evolve the best parameters for an ACO designed to align twosequences. We then used the best parameters found to interpolate approximateoptimal parameters for a given string length within a range. The basis of ourcomparison is the alignment given by the Needleman-Wunsch algorithm. We foundthat ACO can indeed be applied to sequence alignment. While it iscomputationally expensive compared to other equivalent algorithms, it is apromising algorithm that can be readily applied to a variety of otherbiological problems.
arxiv-6300-108 | Improvement Tracking Dynamic Programming using Replication Function for Continuous Sign Language Recognition | http://arxiv.org/pdf/1406.0909v1.pdf | author:S. Ildarabadi, M. Ebrahimi, H. R. Pourreza category:cs.CV published:2014-06-04 summary:In this paper we used a Replication Function (R. F.)for improvement trackingwith dynamic programming. The R. F. transforms values of gray level [0 255] to[0 1]. The resulting images of R. F. are more striking and visible in skinregions. The R. F. improves Dynamic Programming (D. P.) in overlapping hand andface. Results show that Tracking Error Rate 11% and Average Tracked Distance 7%reduced
arxiv-6300-109 | Supervised classification-based stock prediction and portfolio optimization | http://arxiv.org/pdf/1406.0824v1.pdf | author:Sercan Arik, Sukru Burc Eryilmaz, Adam Goldberg category:q-fin.ST cs.CE cs.LG q-fin.PM stat.ML published:2014-06-03 summary:As the number of publicly traded companies as well as the amount of theirfinancial data grows rapidly, it is highly desired to have tracking, analysis,and eventually stock selections automated. There have been few works focusingon estimating the stock prices of individual companies. However, many of thosehave worked with very small number of financial parameters. In this work, weapply machine learning techniques to address automated stock picking, whileusing a larger number of financial parameters for individual companies than theprevious studies. Our approaches are based on the supervision of predictionparameters using company fundamentals, time-series properties, and correlationinformation between different stocks. We examine a variety of supervisedlearning techniques and found that using stock fundamentals is a usefulapproach for the classification problem, when combined with the highdimensional data handling capabilities of support vector machine. The portfolioour system suggests by predicting the behavior of stocks results in a 3% largergrowth on average than the overall market within a 3-month time period, as theout-of-sample test suggests.
arxiv-6300-110 | Learning Latent Block Structure in Weighted Networks | http://arxiv.org/pdf/1404.0431v2.pdf | author:Christopher Aicher, Abigail Z. Jacobs, Aaron Clauset category:stat.ML cs.SI physics.soc-ph published:2014-04-02 summary:Community detection is an important task in network analysis, in which we aimto learn a network partition that groups together vertices with similarcommunity-level connectivity patterns. By finding such groups of vertices withsimilar structural roles, we extract a compact representation of the network'slarge-scale structure, which can facilitate its scientific interpretation andthe prediction of unknown or future interactions. Popular approaches, includingthe stochastic block model, assume edges are unweighted, which limits theirutility by throwing away potentially useful information. We introduce the`weighted stochastic block model' (WSBM), which generalizes the stochasticblock model to networks with edge weights drawn from any exponential familydistribution. This model learns from both the presence and weight of edges,allowing it to discover structure that would otherwise be hidden when weightsare discarded or thresholded. We describe a Bayesian variational algorithm forefficiently approximating this model's posterior distribution over latent blockstructures. We then evaluate the WSBM's performance on both edge-existence andedge-weight prediction tasks for a set of real-world weighted networks. In allcases, the WSBM performs as well or better than the best alternatives on thesetasks.
arxiv-6300-111 | Visual Reranking with Improved Image Graph | http://arxiv.org/pdf/1406.0680v1.pdf | author:Ziqiong Liu, Shengjin Wang, Liang Zheng, Qi Tian category:cs.CV published:2014-06-03 summary:This paper introduces an improved reranking method for the Bag-of-Words (BoW)based image search. Built on [1], a directed image graph robust to outlierdistraction is proposed. In our approach, the relevance among images is encodedin the image graph, based on which the initial rank list is refined. Moreover,we show that the rank-level feature fusion can be adopted in this rerankingmethod as well. Taking advantage of the complementary nature of variousfeatures, the reranking performance is further enhanced. Particularly, weexploit the reranking method combining the BoW and color information.Experiments on two benchmark datasets demonstrate that ourmethod yieldssignificant improvements and the reranking results are competitive to thestate-of-the-art methods.
arxiv-6300-112 | An Ordered Lasso and Sparse Time-Lagged Regression | http://arxiv.org/pdf/1405.6447v2.pdf | author:Xiaotong Suo, Robert Tibshirani category:stat.AP stat.ML published:2014-05-26 summary:We consider regression scenarios where it is natural to impose an orderconstraint on the coefficients. We propose an order-constrained version ofL1-regularized regression for this problem, and show how to solve itefficiently using the well-known Pool Adjacent Violators Algorithm as itsproximal operator. The main application of this idea is time-lagged regression,where we predict an outcome at time t from features at the previous K timepoints. In this setting it is natural to assume that the coefficients decay aswe move farther away from t, and hence the order constraint is reasonable.Potential applications include financial time series and prediction of dynamicpatient out- comes based on clinical measurements. We illustrate this idea onreal and simulated data.
arxiv-6300-113 | Experimental Demonstration of Array-level Learning with Phase Change Synaptic Devices | http://arxiv.org/pdf/1405.7716v2.pdf | author:S. Burc Eryilmaz, Duygu Kuzum, Rakesh G. D. Jeyasingh, SangBum Kim, Matthew BrightSky, Chung Lam, H. -S. Philip Wong category:cs.NE cs.AI published:2014-05-29 summary:The computational performance of the biological brain has long attractedsignificant interest and has led to inspirations in operating principles,algorithms, and architectures for computing and signal processing. In thiswork, we focus on hardware implementation of brain-like learning in abrain-inspired architecture. We demonstrate, in hardware, that 2-D crossbararrays of phase change synaptic devices can achieve associative learning andperform pattern recognition. Device and array-level studies using anexperimental 10x10 array of phase change synaptic devices have shown thatpattern recognition is robust against synaptic resistance variations and largevariations can be tolerated by increasing the number of training iterations.Our measurements show that increase in initial variation from 9 % to 60 %causes required training iterations to increase from 1 to 11.
arxiv-6300-114 | Provable Deterministic Leverage Score Sampling | http://arxiv.org/pdf/1404.1530v3.pdf | author:Dimitris Papailiopoulos, Anastasios Kyrillidis, Christos Boutsidis category:cs.DS cs.IT cs.NA math.IT math.ST stat.ML stat.TH published:2014-04-06 summary:We explain theoretically a curious empirical phenomenon: "Approximating amatrix by deterministically selecting a subset of its columns with thecorresponding largest leverage scores results in a good low-rank matrixsurrogate". To obtain provable guarantees, previous work requires randomizedsampling of the columns with probabilities proportional to their leveragescores. In this work, we provide a novel theoretical analysis of deterministicleverage score sampling. We show that such deterministic sampling can beprovably as accurate as its randomized counterparts, if the leverage scoresfollow a moderately steep power-law decay. We support this power-law assumptionby providing empirical evidence that such decay laws are abundant in real-worlddata sets. We then demonstrate empirically the performance of deterministicleverage score sampling, which many times matches or outperforms thestate-of-the-art techniques.
arxiv-6300-115 | Universal Convexification via Risk-Aversion | http://arxiv.org/pdf/1406.0554v1.pdf | author:Krishnamurthy Dvijotham, Maryam Fazel, Emanuel Todorov category:cs.SY cs.LG math.OC published:2014-06-03 summary:We develop a framework for convexifying a fairly general class ofoptimization problems. Under additional assumptions, we analyze thesuboptimality of the solution to the convexified problem relative to theoriginal nonconvex problem and prove additive approximation guarantees. We thendevelop algorithms based on stochastic gradient methods to solve the resultingoptimization problems and show bounds on convergence rates. %We show a simpleapplication of this framework to supervised learning, where one can performintegration explicitly and can use standard (non-stochastic) optimizationalgorithms with better convergence guarantees. We then extend this framework toapply to a general class of discrete-time dynamical systems. In this context,our convexification approach falls under the well-studied paradigm ofrisk-sensitive Markov Decision Processes. We derive the first known model-basedand model-free policy gradient optimization algorithms with guaranteedconvergence to the optimal solution. Finally, we present numerical resultsvalidating our formulation in different applications.
arxiv-6300-116 | More Bang For Your Buck: Quorum-Sensing Capabilities Improve the Efficacy of Suicidal Altruism | http://arxiv.org/pdf/1406.0416v1.pdf | author:Anya Elaine Johnson, Eli Strauss, Rodney Pickett, Christoph Adami, Ian Dworkin, Heather J. Goldsby category:cs.NE cs.CE q-bio.PE published:2014-06-02 summary:Within the context of evolution, an altruistic act that benefits thereceiving individual at the expense of the acting individual is a puzzlingphenomenon. An extreme form of altruism can be found in colicinogenic E. coli.These suicidal altruists explode, releasing colicins that kill unrelatedindividuals, which are not colicin resistant. By committing suicide, thealtruist makes it more likely that its kin will have less competition. Thebenefits of this strategy rely on the number of competitors and kin nearby. Ifthe organism explodes at an inopportune time, the suicidal act may not harm anycompetitors. Communication could enable organisms to act altruistically whenenvironmental conditions suggest that that strategy would be most beneficial.Quorum sensing is a form of communication in which bacteria produce a proteinand gauge the amount of that protein around them. Quorum sensing is one meansby which bacteria sense the biotic factors around them and determine when toproduce products, such as antibiotics, that influence competition. Suicidalaltruists could use quorum sensing to determine when exploding is mostbeneficial, but it is challenging to study the selective forces at work inmicrobes. To address these challenges, we use digital evolution (a form ofexperimental evolution that uses self-replicating computer programs asorganisms) to investigate the effects of enabling altruistic organisms tocommunicate via quorum sensing. We found that quorum-sensing altruists killed agreater number of competitors per explosion, winning competitions againstnon-communicative altruists. These findings indicate that quorum sensing couldincrease the beneficial effect of altruism and the suite of conditions underwhich it will evolve.
arxiv-6300-117 | Linear Algorithm for Digital Euclidean Connected Skeleton | http://arxiv.org/pdf/1310.2418v3.pdf | author:Aur√©lie Leborgne, Julien Mille, Laure Tougne category:cs.CV published:2013-10-09 summary:The skeleton is an essential shape characteristic providing a compactrepresentation of the studied shape. Its computation on the image grid raisesmany issues. Due to the effects of discretization, the required properties ofthe skeleton - thinness, homotopy to the shape, reversibility, connectivity -may become incompatible. However, as regards practical use, the choice of aspecific skeletonization algorithm depends on the application. This allows toclassify the desired properties by order of importance, and tend towards themost critical ones. Our goal is to make a skeleton dedicated to shape matchingfor recognition. So, the discrete skeleton has to be thin - so that it can berepresented by a graph -, robust to noise, reversible - so that the initialshape can be fully reconstructed - and homotopic to the shape. We propose alinear-time skeletonization algorithm based on the squared Euclidean distancemap from which we extract the maximal balls and ridges. After a thinning andpruning process, we obtain the skeleton. The proposed method is finallycompared to fairly recent methods.
arxiv-6300-118 | Generalized Max Pooling | http://arxiv.org/pdf/1406.0312v1.pdf | author:Naila Murray, Florent Perronnin category:cs.CV published:2014-06-02 summary:State-of-the-art patch-based image representations involve a poolingoperation that aggregates statistics computed from local descriptors. Standardpooling operations include sum- and max-pooling. Sum-pooling lacksdiscriminability because the resulting representation is strongly influenced byfrequent yet often uninformative descriptors, but only weakly influenced byrare yet potentially highly-informative ones. Max-pooling equalizes theinfluence of frequent and rare descriptors but is only applicable torepresentations that rely on count statistics, such as the bag-of-visual-words(BOV) and its soft- and sparse-coding extensions. We propose a novel poolingmechanism that achieves the same effect as max-pooling but is applicable beyondthe BOV and especially to the state-of-the-art Fisher Vector -- hence the nameGeneralized Max Pooling (GMP). It involves equalizing the similarity betweeneach patch and the pooled representation, which is shown to be equivalent tore-weighting the per-patch statistics. We show on five public imageclassification benchmarks that the proposed GMP can lead to significantperformance gains with respect to heuristic alternatives.
arxiv-6300-119 | Transductive Learning for Multi-Task Copula Processes | http://arxiv.org/pdf/1406.0304v1.pdf | author:Markus Schneider, Fabio Ramos category:cs.LG stat.ML published:2014-06-02 summary:We tackle the problem of multi-task learning with copula process.Multivariable prediction in spatial and spatial-temporal processes such asnatural resource estimation and pollution monitoring have been typicallyaddressed using techniques based on Gaussian processes and co-Kriging. Whilethe Gaussian prior assumption is convenient from analytical and computationalperspectives, nature is dominated by non-Gaussian likelihoods. Copula processesare an elegant and flexible solution to handle various non-Gaussian likelihoodsby capturing the dependence structure of random variables with cumulativedistribution functions rather than their marginals. We show how multi-tasklearning for copula processes can be used to improve multivariable predictionfor problems where the simple Gaussianity prior assumption does not hold. Then,we present a transductive approximation for multi-task learning and deriveanalytical expressions for the copula process model. The approach is evaluatedand compared to other techniques in one artificial dataset and two publiclyavailable datasets for natural resource estimation and concrete slumpprediction.
arxiv-6300-120 | The constitution of visual perceptual units in the functional architecture of V1 | http://arxiv.org/pdf/1406.0289v1.pdf | author:Alessandro Sarti, Giovanna Citti category:cs.CV published:2014-06-02 summary:Scope of this paper is to consider a mean field neural model which takes intoaccount the functional neurogeometry of the visual cortex modelled as a groupof rotations and translations. The model generalizes well known results ofBressloff and Cowan which, in absence of input, accounts for hallucinationpatterns. The main result of our study consists in showing that in presence ofa visual input, the eigenmodes of the linearized operator which become stablerepresent perceptual units present in the image. The result is strictly relatedto dimensionality reduction and clustering problems.
arxiv-6300-121 | Continuous Action Recognition Based on Sequence Alignment | http://arxiv.org/pdf/1406.0288v1.pdf | author:Kaustubh Kulkarni, Georgios Evangelidis, Jan Cech, Radu Horaud category:cs.CV published:2014-06-02 summary:Continuous action recognition is more challenging than isolated recognitionbecause classification and segmentation must be simultaneously carried out. Webuild on the well known dynamic time warping (DTW) framework and devise a novelvisual alignment technique, namely dynamic frame warping (DFW), which performsisolated recognition based on per-frame representation of videos, and onaligning a test sequence with a model sequence. Moreover, we propose twoextensions which enable to perform recognition concomitant with segmentation,namely one-pass DFW and two-pass DFW. These two methods have their roots in thedomain of continuous recognition of speech and, to the best of our knowledge,their extension to continuous visual action recognition has been overlooked. Wetest and illustrate the proposed techniques with a recently released dataset(RAVEL) and with two public-domain datasets widely used in action recognition(Hollywood-1 and Hollywood-2). We also compare the performances of the proposedisolated and continuous recognition algorithms with several recently publishedmethods.
arxiv-6300-122 | Ambiguous Proximity Distribution | http://arxiv.org/pdf/1406.0231v1.pdf | author:Quanquan Wang, Yongping Li category:cs.CV published:2014-06-02 summary:Proximity Distribution Kernel is an effective method for bag-of-featues basedimage representation. In this paper, we investigate the soft assignment ofvisual words to image features for proximity distribution. Visual wordcontribution function is proposed to model ambiguous proximity distributions.Three ambiguous proximity distributions is developed by three ambiguouscontribution functions. The experiments are conducted on both classificationand retrieval of medical image data sets. The results show that the performanceof the proposed methods, Proximity Distribution Kernel (PDK), is better orcomparable to the state-of-the-art bag-of-features based image representationmethods.
arxiv-6300-123 | Holistic Measures for Evaluating Prediction Models in Smart Grids | http://arxiv.org/pdf/1406.0223v1.pdf | author:Saima Aman, Yogesh Simmhan, Viktor K. Prasanna category:cs.LG published:2014-06-02 summary:The performance of prediction models is often based on "abstract metrics"that estimate the model's ability to limit residual errors between the observedand predicted values. However, meaningful evaluation and selection ofprediction models for end-user domains requires holistic andapplication-sensitive performance measures. Inspired by energy consumptionprediction models used in the emerging "big data" domain of Smart Power Grids,we propose a suite of performance measures to rationally compare models alongthe dimensions of scale independence, reliability, volatility and cost. Weinclude both application independent and dependent measures, the latterparameterized to allow customization by domain experts to fit their scenario.While our measures are generalizable to other domains, we offer an empiricalanalysis using real energy use data for three Smart Grid applications:planning, customer education and demand response, which are relevant for energysustainability. Our results underscore the value of the proposed measures tooffer a deeper insight into models' behavior and their impact on realapplications, which benefit both data mining researchers and practitioners.
arxiv-6300-124 | Topological and Statistical Behavior Classifiers for Tracking Applications | http://arxiv.org/pdf/1406.0214v1.pdf | author:Paul Bendich, Sang Chin, Jesse Clarke, Jonathan deSena, John Harer, Elizabeth Munch, Andrew Newman, David Porter, David Rouse, Nate Strawn, Adam Watkins category:cs.SY math.AT stat.ML published:2014-06-01 summary:We introduce the first unified theory for target tracking using MultipleHypothesis Tracking, Topological Data Analysis, and machine learning. Ourstring of innovations are 1) robust topological features are used to encodebehavioral information, 2) statistical models are fitted to distributions overthese topological features, and 3) the target type classification methods ofWigren and Bar Shalom et al. are employed to exploit the resulting likelihoodsfor topological features inside of the tracking procedure. To demonstrate theefficacy of our approach, we test our procedure on synthetic vehicular datagenerated by the Simulation of Urban Mobility package.
arxiv-6300-125 | Inference of Sparse Networks with Unobserved Variables. Application to Gene Regulatory Networks | http://arxiv.org/pdf/1406.0193v1.pdf | author:Nikolai Slavov category:stat.ML cs.LG q-bio.MN q-bio.QM stat.AP published:2014-06-01 summary:Networks are a unifying framework for modeling complex systems and networkinference problems are frequently encountered in many fields. Here, I developand apply a generative approach to network inference (RCweb) for the case whenthe network is sparse and the latent (not observed) variables affect theobserved ones. From all possible factor analysis (FA) decompositions explainingthe variance in the data, RCweb selects the FA decomposition that is consistentwith a sparse underlying network. The sparsity constraint is imposed by a novelmethod that significantly outperforms (in terms of accuracy, robustness tonoise, complexity scaling, and computational efficiency) Bayesian methods andMLE methods using l1 norm relaxation such as K-SVD and l1--based sparseprinciple component analysis (PCA). Results from simulated models demonstratethat RCweb recovers exactly the model structures for sparsity as low (asnon-sparse) as 50% and with ratio of unobserved to observed variables as highas 2. RCweb is robust to noise, with gradual decrease in the parameter rangesas the noise level increases.
arxiv-6300-126 | Convex Total Least Squares | http://arxiv.org/pdf/1406.0189v1.pdf | author:Dmitry Malioutov, Nikolai Slavov category:stat.ML cs.LG q-bio.GN q-bio.QM stat.AP published:2014-06-01 summary:We study the total least squares (TLS) problem that generalizes least squaresregression by allowing measurement errors in both dependent and independentvariables. TLS is widely used in applied fields including computer vision,system identification and econometrics. The special case when all dependent andindependent variables have the same level of uncorrelated Gaussian noise, knownas ordinary TLS, can be solved by singular value decomposition (SVD). However,SVD cannot solve many important practical TLS problems with realistic noisestructure, such as having varying measurement noise, known structure on theerrors, or large outliers requiring robust error-norms. To solve such problems,we develop convex relaxation approaches for a general class of structured TLS(STLS). We show both theoretically and experimentally, that while the plainnuclear norm relaxation incurs large approximation errors for STLS, there-weighted nuclear norm approach is very effective, and achieves betteraccuracy on challenging STLS problems than popular non-convex solvers. Wedescribe a fast solution based on augmented Lagrangian formulation, and applyour approach to an important class of biological problems that use populationaverage measurements to infer cell-type and physiological-state specificexpression levels that are very hard to measure directly.
arxiv-6300-127 | Evolutionary Search in the Space of Rules for Creation of New Two-Player Board Games | http://arxiv.org/pdf/1406.0175v1.pdf | author:Zahid Halim category:cs.NE cs.AI published:2014-06-01 summary:Games have always been a popular test bed for artificial intelligencetechniques. Game developers are always in constant search for techniques thatcan automatically create computer games minimizing the developer's task. Inthis work we present an evolutionary strategy based solution towards theautomatic generation of two player board games. To guide the evolutionaryprocess towards games, which are entertaining, we propose a set of metrics.These metrics are based upon different theories of entertainment in computergames. This work also compares the entertainment value of the evolved gameswith the existing popular board based games. Further to verify theentertainment value of the evolved games with the entertainment value of thehuman user a human user survey is conducted. In addition to the user survey wecheck the learnability of the evolved games using an artificial neural networkbased controller. The proposed metrics and the evolutionary process can beemployed for generating new and entertaining board games, provided an initialsearch space is given to the evolutionary algorithm.
arxiv-6300-128 | Seeing the Big Picture: Deep Embedding with Contextual Evidences | http://arxiv.org/pdf/1406.0132v1.pdf | author:Liang Zheng, Shengjin Wang, Fei He, Qi Tian category:cs.CV published:2014-06-01 summary:In the Bag-of-Words (BoW) model based image retrieval task, the precision ofvisual matching plays a critical role in improving retrieval performance.Conventionally, local cues of a keypoint are employed. However, such strategydoes not consider the contextual evidences of a keypoint, a problem which wouldlead to the prevalence of false matches. To address this problem, this paperdefines "true match" as a pair of keypoints which are similar on three levels,i.e., local, regional, and global. Then, a principled probabilistic frameworkis established, which is capable of implicitly integrating discriminative cuesfrom all these feature levels. Specifically, the Convolutional Neural Network (CNN) is employed to extractfeatures from regional and global patches, leading to the so-called "DeepEmbedding" framework. CNN has been shown to produce excellent performance on adozen computer vision tasks such as image classification and detection, but fewworks have been done on BoW based image retrieval. In this paper, firstly weshow that proper pre-processing techniques are necessary for effective usage ofCNN feature. Then, in the attempt to fit it into our model, a novel indexingstructure called "Deep Indexing" is introduced, which dramatically reducesmemory usage. Extensive experiments on three benchmark datasets demonstrate that, theproposed Deep Embedding method greatly promotes the retrieval accuracy when CNNfeature is integrated. We show that our method is efficient in terms of bothmemory and time cost, and compares favorably with the state-of-the-art methods.
arxiv-6300-129 | A Corpus of Sentence-level Revisions in Academic Writing: A Step towards Understanding Statement Strength in Communication | http://arxiv.org/pdf/1405.1439v2.pdf | author:Chenhao Tan, Lillian Lee category:cs.CL published:2014-05-06 summary:The strength with which a statement is made can have a significant impact onthe audience. For example, international relations can be strained by how themedia in one country describes an event in another; and papers can be rejectedbecause they overstate or understate their findings. It is thus important tounderstand the effects of statement strength. A first step is to be able todistinguish between strong and weak statements. However, even this problem isunderstudied, partly due to a lack of data. Since strength is inherentlyrelative, revisions of texts that make claims are a natural source of data onstrength differences. In this paper, we introduce a corpus of sentence-levelrevisions from academic writing. We also describe insights gained from ourannotation efforts for this task.
arxiv-6300-130 | Improved graph Laplacian via geometric self-consistency | http://arxiv.org/pdf/1406.0118v1.pdf | author:Dominique Perrault-Joncas, Marina Meila category:stat.ML cs.LG published:2014-05-31 summary:We address the problem of setting the kernel bandwidth used by ManifoldLearning algorithms to construct the graph Laplacian. Exploiting the connectionbetween manifold geometry, represented by the Riemannian metric, and theLaplace-Beltrami operator, we set the bandwidth by optimizing the Laplacian'sability to preserve the geometry of the data. Experiments show that thisprincipled approach is effective and robust.
arxiv-6300-131 | Bridging the gap between Legal Practitioners and Knowledge Engineers using semi-formal KR | http://arxiv.org/pdf/1406.0079v1.pdf | author:Shashishekar Ramakrishna, Adrian Paschke category:cs.CL cs.AI published:2014-05-31 summary:The use of Structured English as a computation independent knowledgerepresentation format for non-technical users in business rules representationhas been proposed in OMGs Semantics and Business Vocabulary Representation(SBVR). In the legal domain we face a similar problem. Formal representationlanguages, such as OASIS LegalRuleML and legal ontologies (LKIF, legal OWL2ontologies etc.) support the technical knowledge engineer and the automatedreasoning. But, they can be hardly used directly by the legal domain expertswho do not have a computer science background. In this paper we adapt the SBVRStructured English approach for the legal domain and implement aproof-of-concept, called KR4IPLaw, which enables legal domain experts torepresent their knowledge in Structured English in a computational independentand hence, for them, more usable way. The benefit of this approach is that theunderlying pre-defined semantics of the Structured English approach makestransformations into formal languages such as OASIS LegalRuleML and OWL2ontologies possible. We exemplify our approach in the domain of patent law.
arxiv-6300-132 | Combined Approach for Image Segmentation | http://arxiv.org/pdf/1406.0074v1.pdf | author:Shradha Dakhare, Harshal Chowhan, Manoj B. Chandak category:cs.CV published:2014-05-31 summary:Many image segmentation techniques have been developed over the past twodecades for segmenting the images, which help for object recognition, occlusionboundary estimation within motion or stereo systems, image compression, imageediting. In this, there is a combined approach for segmenting the image. By usinghistogram equalization to the input image, from which it gives contrastenhancement output image .After that by applying median filtering,which willremove noise from contrast output image . At last I applied fuzzy c-meanclustering algorithm to denoising output image, which give segmented outputimage. In this way it produce better segmented image with less computationtime.
arxiv-6300-133 | Adaptive Reconfiguration Moves for Dirichlet Mixtures | http://arxiv.org/pdf/1406.0071v1.pdf | author:Tue Herlau, Morten M√∏rup, Yee Whye Teh, Mikkel N. Schmidt category:stat.ML published:2014-05-31 summary:Bayesian mixture models are widely applied for unsupervised learning andexploratory data analysis. Markov chain Monte Carlo based on Gibbs sampling andsplit-merge moves are widely used for inference in these models. However, bothmethods are restricted to limited types of transitions and suffer from torpidmixing and low accept rates even for problems of modest size. We propose amethod that considers a broader range of transitions that are close toequilibrium by exploiting multiple chains in parallel and using the past statesadaptively to inform the proposal distribution. The method significantlyimproves on Gibbs and split-merge sampling as quantified using convergencediagnostics and acceptance rates. Adaptive MCMC methods which use past statesto inform the proposal distribution has given rise to many ingenious samplingschemes for continuous problems and the present work can be seen as animportant first step in bringing these benefits to partition-based problems
arxiv-6300-134 | Average Drift Analysis and its Application | http://arxiv.org/pdf/1308.3080v3.pdf | author:Jun He, Tianshi Chen, Xin Yao category:cs.NE published:2013-08-14 summary:Drift analysis is a useful tool for estimating the running time ofevolutionary algorithms. A new representation of drift analysis, called averagedrift analysis, is described in this paper. It takes a weaker requirement thanpoint-wise drift analysis does. Point-wise drift theorems are corollaries ofour average drift theorems. Therefore average drift analysis is more powerfulthan point-wise drift analysis. To demonstrate the application of average driftanalysis, we choose a (1+N) evolutionary algorithms for linear-like functionsas a case study. Linear-like functions are proposed as a natural extension oflinear functions. For the (1+N) evolutionary algorithms to maximise linear-likefunctions, the lower and upper bounds on their running time have been derivedusing the average drift analysis.
arxiv-6300-135 | Comparing and Combining Sentiment Analysis Methods | http://arxiv.org/pdf/1406.0032v1.pdf | author:Pollyanna Gon√ßalves, Matheus Ara√∫jo, Fabr√≠cio Benevenuto, Meeyoung Cha category:cs.CL published:2014-05-30 summary:Several messages express opinions about events, products, and services,political views or even their author's emotional state and mood. Sentimentanalysis has been used in several applications including analysis of therepercussions of events in social networks, analysis of opinions about productsand services, and simply to better understand aspects of social communicationin Online Social Networks (OSNs). There are multiple methods for measuringsentiments, including lexical-based approaches and supervised machine learningmethods. Despite the wide use and popularity of some methods, it is unclearwhich method is better for identifying the polarity (i.e., positive ornegative) of a message as the current literature does not provide a method ofcomparison among existing methods. Such a comparison is crucial forunderstanding the potential limitations, advantages, and disadvantages ofpopular methods in analyzing the content of OSNs messages. Our study aims atfilling this gap by presenting comparisons of eight popular sentiment analysismethods in terms of coverage (i.e., the fraction of messages whose sentiment isidentified) and agreement (i.e., the fraction of identified sentiments that arein tune with ground truth). We develop a new method that combines existingapproaches, providing the best coverage results and competitive agreement. Wealso present a free Web service called iFeel, which provides an open API foraccessing and comparing results across different sentiment methods for a giventext.
arxiv-6300-136 | Circle detection using electro-magnetism optimization | http://arxiv.org/pdf/1406.0023v1.pdf | author:Erik Cuevas, Diego Oliva, Daniel Zaldivar, Marco Perez-Cisneros, Humberto Sossa category:cs.CV published:2014-05-30 summary:This paper describes a circle detection method based on Electromagnetism-LikeOptimization (EMO). Circle detection has received considerable attention overthe last years thanks to its relevance for many computer vision tasks. EMO is aheuristic method for solving complex optimization problems inspired inelectromagnetism principles. This algorithm searches a solution based in theattraction and repulsion among prototype candidates. In this paper thedetection process is considered to be similar to an optimization problem, thealgorithm uses the combination of three edge points (x, y, r) as parameters todetermine circles candidates in the scene. An objective function determines ifsuch circle candidates are actually present in the image. The EMO algorithm isused to find the circle candidate that is better related with the real circlepresent in the image according to the objective function. The final algorithmis a fast circle detector that locates circles with sub-pixel accuracy evenconsidering complicated conditions and noisy images.
arxiv-6300-137 | Estimating Vector Fields on Manifolds and the Embedding of Directed Graphs | http://arxiv.org/pdf/1406.0013v1.pdf | author:Dominique Perrault-Joncas, Marina Meila category:stat.ML cs.LG published:2014-05-30 summary:This paper considers the problem of embedding directed graphs in Euclideanspace while retaining directional information. We model a directed graph as afinite set of observations from a diffusion on a manifold endowed with a vectorfield. This is the first generative model of its kind for directed graphs. Weintroduce a graph embedding algorithm that estimates all three features of thismodel: the low-dimensional embedding of the manifold, the data density and thevector field. In the process, we also obtain new theoretical results on thelimits of "Laplacian type" matrices derived from directed graphs. Theapplication of our method to both artificially constructed and real datahighlights its strengths.
arxiv-6300-138 | The Infinite Degree Corrected Stochastic Block Model | http://arxiv.org/pdf/1311.2520v3.pdf | author:Tue Herlau, Mikkel N. Schmidt, Morten M√∏rup category:stat.ML published:2013-11-11 summary:In Stochastic blockmodels, which are among the most prominent statisticalmodels for cluster analysis of complex networks, clusters are defined as groupsof nodes with statistically similar link probabilities within and betweengroups. A recent extension by Karrer and Newman incorporates a node degreecorrection to model degree heterogeneity within each group. Although thisdemonstrably leads to better performance on several networks it is not obviouswhether modelling node degree is always appropriate or necessary. We formulatethe degree corrected stochastic blockmodel as a non-parametric Bayesian model,incorporating a parameter to control the amount of degree correction which canthen be inferred from data. Additionally, our formulation yields principledways of inferring the number of groups as well as predicting missing links inthe network which can be used to quantify the model's predictive performance.On synthetic data we demonstrate that including the degree correction yieldsbetter performance both on recovering the true group structure and predictingmissing links when degree heterogeneity is present, whereas performance is onpar for data with no degree heterogeneity within clusters. On seven realnetworks (with no ground truth group structure available) we show thatpredictive performance is about equal whether or not degree correction isincluded; however, for some networks significantly fewer clusters arediscovered when correcting for degree indicating that the data can be morecompactly explained by clusters of heterogenous degree nodes.
arxiv-6300-139 | Semantic Composition and Decomposition: From Recognition to Generation | http://arxiv.org/pdf/1405.7908v1.pdf | author:Peter D. Turney category:cs.CL cs.AI cs.LG published:2014-05-30 summary:Semantic composition is the task of understanding the meaning of text bycomposing the meanings of the individual words in the text. Semanticdecomposition is the task of understanding the meaning of an individual word bydecomposing it into various aspects (factors, constituents, components) thatare latent in the meaning of the word. We take a distributional approach tosemantics, in which a word is represented by a context vector. Much recent workhas considered the problem of recognizing compositions and decompositions, butwe tackle the more difficult generation problem. For simplicity, we focus onnoun-modifier bigrams and noun unigrams. A test for semantic composition is,given context vectors for the noun and modifier in a noun-modifier bigram ("redsalmon"), generate a noun unigram that is synonymous with the given bigram("sockeye"). A test for semantic decomposition is, given a context vector for anoun unigram ("snifter"), generate a noun-modifier bigram that is synonymouswith the given unigram ("brandy glass"). With a vocabulary of about 73,000unigrams from WordNet, there are 73,000 candidate unigram compositions for abigram and 5,300,000,000 (73,000 squared) candidate bigram decompositions for aunigram. We generate ranked lists of potential solutions in two passes. A fastunsupervised learning algorithm generates an initial list of candidates andthen a slower supervised learning algorithm refines the list. We evaluate thecandidate solutions by comparing them to WordNet synonym sets. Fordecomposition (unigram to bigram), the top 100 most highly ranked bigramsinclude a WordNet synonym of the given unigram 50.7% of the time. Forcomposition (bigram to unigram), the top 100 most highly ranked unigramsinclude a WordNet synonym of the given bigram 77.8% of the time.
arxiv-6300-140 | The Shortlist Method for Fast Computation of the Earth Mover's Distance and Finding Optimal Solutions to Transportation Problems | http://arxiv.org/pdf/1405.7903v1.pdf | author:Carsten Gottschlich, Dominic Schuhmacher category:cs.CV published:2014-05-30 summary:Finding solutions to the classical transportation problem is of greatimportance, since this optimization problem arises in many engineering andcomputer science applications. Especially the Earth Mover's Distance is used ina plethora of applications ranging from content-based image retrieval, shapematching, fingerprint recognition, object tracking and phishing web pagedetection to computing color differences in linguistics and biology. Ourstarting point is the well-known revised simplex algorithm, which iterativelyimproves a feasible solution to optimality. The Shortlist Method that wepropose substantially reduces the number of candidates inspected for improvingthe solution, while at the same time balancing the number of pivots required.Tests on simulated benchmarks demonstrate a considerable reduction incomputation time for the new method as compared to the usual revised simplexalgorithm implemented with state-of-the-art initialization and pivotstrategies. As a consequence, the Shortlist Method facilitates the computationof large scale transportation problems in viable time. In addition we describea novel method for finding an initial feasible solution which we coin ModifiedRussell's Method.
arxiv-6300-141 | Flip-Flop Sublinear Models for Graphs: Proof of Theorem 1 | http://arxiv.org/pdf/1405.7897v1.pdf | author:Brijnesh Jain category:cs.LG published:2014-05-30 summary:We prove that there is no class-dual for almost all sublinear models ongraphs.
arxiv-6300-142 | Stochastic Backpropagation and Approximate Inference in Deep Generative Models | http://arxiv.org/pdf/1401.4082v3.pdf | author:Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra category:stat.ML cs.AI cs.LG stat.CO stat.ME published:2014-01-16 summary:We marry ideas from deep neural networks and approximate Bayesian inferenceto derive a generalised class of deep, directed generative models, endowed witha new algorithm for scalable inference and learning. Our algorithm introduces arecognition model to represent approximate posterior distributions, and thatacts as a stochastic encoder of the data. We develop stochasticback-propagation -- rules for back-propagation through stochastic variables --and use this to develop an algorithm that allows for joint optimisation of theparameters of both the generative and recognition model. We demonstrate onseveral real-world data sets that the model generates realistic samples,provides accurate imputations of missing data and is a useful tool forhigh-dimensional data visualisation.
arxiv-6300-143 | ELM Solutions for Event-Based Systems | http://arxiv.org/pdf/1405.7780v1.pdf | author:Jonathan Tapson, Andr√© van Schaik category:cs.NE published:2014-05-30 summary:Whilst most engineered systems use signals that are continuous in time, thereis a domain of systems in which signals consist of events. Events, like Diracdelta functions, have no meaningful time duration. Many important real-worldsystems are intrinsically event-based, including the mammalian brain, in whichthe primary packets of data are spike events, or action potentials. In thisdomain, signal processing requires responses to spatio-temporal patterns ofevents. We show that some straightforward modifications to the standard ELMtopology produce networks that are able to perform spatio-temporal eventprocessing online with a high degree of accuracy. The modifications involve there-definition of hidden layer units as synaptic kernels, in which the inputdelta functions are transformed into continuous-valued signals using a varietyof impulse-response functions. This permits the use of linear solution methodsin the output layer, which can produce events as output, if modeled as aclassifier; the output classes are 'event' or 'no event'. We illustrate themethod in application to a spike-processing problem.
arxiv-6300-144 | Online and Adaptive Pseudoinverse Solutions for ELM Weights | http://arxiv.org/pdf/1405.7777v1.pdf | author:Andr√© van Schaik, Jonathan Tapson category:cs.NE published:2014-05-30 summary:The ELM method has become widely used for classification and regressionsproblems as a result of its accuracy, simplicity and ease of use. The solutionof the hidden layer weights by means of a matrix pseudoinverse operation is asignificant contributor to the utility of the method; however, the conventionalcalculation of the pseudoinverse by means of a singular value decomposition(SVD) is not always practical for large data sets or for online updates to thesolution. In this paper we discuss incremental methods for solving thepseudoinverse which are suitable for ELM. We show that careful choice ofmethods allows us to optimize for accuracy, ease of computation, oradaptability of the solution.
arxiv-6300-145 | DEM Registration and Error Analysis using ASCII values | http://arxiv.org/pdf/1405.7771v1.pdf | author:Suma Dawn, Vikas Saxena, Bhu Dev Sharma category:cs.CV published:2014-05-30 summary:Digital Elevation Model (DEM), while providing a bare earth look, is heavilyused in many applications including construction modeling, visualization, andGIS. Their registration techniques have not been explored much. Methods likeCoarse-to-fine or pyramid making are common in DEM-to-image or DEM-to-mapregistration. Self-consistency measure is used to detect any change in terrainelevation and hence was used for DEM-to-DEM registration. But these methodsapart from being time and complexity intensive, lack in error matrixevaluation. This paper gives a method of registration of DEMs using specifiedheight values as control points by initially converting these DEMs to ASCIIfiles. These control points may be found by two mannerisms - either by directdetection of appropriate height data in ASCII files or by edge matching alongcongruous quadrangle of the control point, followed by sub-graph matching.Error analysis for the same has also been done.
arxiv-6300-146 | Efficient State-Space Inference of Periodic Latent Force Models | http://arxiv.org/pdf/1310.6319v2.pdf | author:Steven Reece, Stephen Roberts, Siddhartha Ghosh, Alex Rogers, Nicholas Jennings category:stat.ML published:2013-10-23 summary:Latent force models (LFM) are principled approaches to incorporatingsolutions to differential equations within non-parametric inference methods.Unfortunately, the development and application of LFMs can be inhibited bytheir computational cost, especially when closed-form solutions for the LFM areunavailable, as is the case in many real world problems where these latentforces exhibit periodic behaviour. Given this, we develop a new sparserepresentation of LFMs which considerably improves their computationalefficiency, as well as broadening their applicability, in a principled way, todomains with periodic or near periodic latent forces. Our approach uses alinear basis model to approximate one generative model for each periodic force.We assume that the latent forces are generated from Gaussian process priors anddevelop a linear basis model which fully expresses these priors. We apply ourapproach to model the thermal dynamics of domestic buildings and show that itis effective at predicting day-ahead temperatures within the homes. We alsoapply our approach within queueing theory in which quasi-periodic arrival ratesare modelled as latent forces. In both cases, we demonstrate that our approachcan be implemented efficiently using state-space methods which encode thelinear dynamic systems via LFMs. Further, we show that state estimates obtainedusing periodic latent force models can reduce the root mean squared error to17% of that from non-periodic models and 27% of the nearest rival approachwhich is the resonator model.
arxiv-6300-147 | Classification of Basmati Rice Grain Variety using Image Processing and Principal Component Analysis | http://arxiv.org/pdf/1405.7626v1.pdf | author:Rubi Kambo, Amit Yerpude category:cs.CV published:2014-05-29 summary:All important decisions about the variety of rice grain end product are basedon the different features of rice grain.There are various methods available forclassification of basmati rice. This paper proposed a new principal componentanalysis based approach for classification of different variety of basmatirice. The experimental result shows the effectiveness of the proposedmethodology for various samples of different variety of basmati rice.
arxiv-6300-148 | Simultaneous Feature and Expert Selection within Mixture of Experts | http://arxiv.org/pdf/1405.7624v1.pdf | author:Billy Peralta category:cs.LG published:2014-05-29 summary:A useful strategy to deal with complex classification scenarios is the"divide and conquer" approach. The mixture of experts (MOE) technique makes useof this strategy by joinly training a set of classifiers, or experts, that arespecialized in different regions of the input space. A global model, or gatefunction, complements the experts by learning a function that weights theirrelevance in different parts of the input space. Local feature selectionappears as an attractive alternative to improve the specialization of expertsand gate function, particularly, for the case of high dimensional data. Ourmain intuition is that particular subsets of dimensions, or subspaces, areusually more appropriate to classify instances located in different regions ofthe input space. Accordingly, this work contributes with a regularized variantof MoE that incorporates an embedded process for local feature selection using$L1$ regularization, with a simultaneous expert selection. The experiments arestill pending.
arxiv-6300-149 | Functional Gaussian processes for regression with linear PDE models | http://arxiv.org/pdf/1405.7569v1.pdf | author:Ngoc-Cuong Nguyen, Jaime Peraire category:math.AP math.PR stat.CO stat.ML published:2014-05-29 summary:In this paper, we present a new statistical approach to the problem ofincorporating experimental observations into a mathematical model described bylinear partial differential equations (PDEs) to improve the prediction of thestate of a physical system. We augment the linear PDE with a functional thataccounts for the uncertainty in the mathematical model and is modeled as a {\emGaussian process}. This gives rise to a stochastic PDE which is characterizedby the Gaussian functional. We develop a {\em functional Gaussian processregression} method to determine the posterior mean and covariance of theGaussian functional, thereby solving the stochastic PDE to obtain the posteriordistribution for our prediction of the physical state. Our method has thefollowing features which distinguish itself from other regression methods.First, it incorporates both the mathematical model and the observations intothe regression procedure. Second, it can handle the observations given in theform of linear functionals of the field variable. Third, the method isnon-parametric in the sense that it provides a systematic way to optimallydetermine the prior covariance operator of the Gaussian functional based on theobservations. Fourth, it provides the posterior distribution quantifying themagnitude of uncertainty in our prediction of the physical state. We presentnumerical results to illustrate these features of the method and compare itsperformance to that of the standard Gaussian process regression.
arxiv-6300-150 | Feature sampling and partitioning for visual vocabulary generation on large action classification datasets | http://arxiv.org/pdf/1405.7545v1.pdf | author:Michael Sapienza, Fabio Cuzzolin, Philip H. S. Torr category:cs.CV published:2014-05-29 summary:The recent trend in action recognition is towards larger datasets, anincreasing number of action classes and larger visual vocabularies.State-of-the-art human action classification in challenging video data iscurrently based on a bag-of-visual-words pipeline in which space-time featuresare aggregated globally to form a histogram. The strategies chosen to samplefeatures and construct a visual vocabulary are critical to performance, in factoften dominating performance. In this work we provide a critical evaluation ofvarious approaches to building a vocabulary and show that good practises dohave a significant impact. By subsampling and partitioning featuresstrategically, we are able to achieve state-of-the-art results on 5 majoraction recognition datasets using relatively small visual vocabularies.
arxiv-6300-151 | Aspect Based Sentiment Analysis to Extract Meticulous Opinion Value | http://arxiv.org/pdf/1405.7519v1.pdf | author:Deepali Virmani, Vikrant Malhotra, Ridhi Tyagi category:cs.IR cs.CL published:2014-05-29 summary:Opinion Mining and Sentiment Analysis is a process of identifying opinions inlarge unstructured/structured data and then analysing polarity of thoseopinions. Opinion mining and sentiment analysis have found vast application inanalysing online ratings, analysing product based reviews, e-governance, andmanaging hostile content over the internet. This paper proposes an algorithm toimplement aspect level sentiment analysis. The algorithm takes input from theremarks submitted by various teachers of a student. An aspect tree is formedwhich has various levels and weights are assigned to each branch to identifylevel of aspect. Aspect value is calculated by the algorithm by means of theproposed aspect tree. Dictionary based method is implemented to evaluate thepolarity of the remark. The algorithm returns the aspect value clubbed withopinion value and sentiment value which helps in concluding the summarizedvalue of remark.
arxiv-6300-152 | Effect of Different Distance Measures on the Performance of K-Means Algorithm: An Experimental Study in Matlab | http://arxiv.org/pdf/1405.7471v1.pdf | author:Mr. Dibya Jyoti Bora, Dr. Anil Kumar Gupta category:cs.LG published:2014-05-29 summary:K-means algorithm is a very popular clustering algorithm which is famous forits simplicity. Distance measure plays a very important rule on the performanceof this algorithm. We have different distance measure techniques available. Butchoosing a proper technique for distance calculation is totally dependent onthe type of the data that we are going to cluster. In this paper anexperimental study is done in Matlab to cluster the iris and wine data setswith different distance measures and thereby observing the variation of theperformances shown.
arxiv-6300-153 | Universal Compression of Envelope Classes: Tight Characterization via Poisson Sampling | http://arxiv.org/pdf/1405.7460v1.pdf | author:Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, Ananda Theertha Suresh category:cs.IT cs.LG math.IT published:2014-05-29 summary:The Poisson-sampling technique eliminates dependencies among symbolappearances in a random sequence. It has been used to simplify the analysis andstrengthen the performance guarantees of randomized algorithms. Applying thismethod to universal compression, we relate the redundancies of fixed-length andPoisson-sampled sequences, use the relation to derive a simple single-letterformula that approximates the redundancy of any envelope class to within anadditive logarithmic term. As a first application, we consider i.i.d.distributions over a small alphabet as a step-envelope class, and provide ashort proof that determines the redundancy of discrete distributions over asmall al- phabet up to the first order terms. We then show the strength of ourmethod by applying the formula to tighten the existing bounds on the redundancyof exponential and power-law classes, in particular answering a question posedby Boucheron, Garivier and Gassiat.
arxiv-6300-154 | Linearized Alternating Direction Method with Parallel Splitting and Adaptive Penalty for Separable Convex Programs in Machine Learning | http://arxiv.org/pdf/1310.5035v2.pdf | author:Zhouchen Lin, Risheng Liu, Huan Li category:cs.NA cs.LG math.OC stat.ML published:2013-10-18 summary:Many problems in machine learning and other fields can be (re)for-mulated aslinearly constrained separable convex programs. In most of the cases, there aremultiple blocks of variables. However, the traditional alternating directionmethod (ADM) and its linearized version (LADM, obtained by linearizing thequadratic penalty term) are for the two-block case and cannot be naivelygeneralized to solve the multi-block case. So there is great demand onextending the ADM based methods for the multi-block case. In this paper, wepropose LADM with parallel splitting and adaptive penalty (LADMPSAP) to solvemulti-block separable convex programs efficiently. When all the componentobjective functions have bounded subgradients, we obtain convergence resultsthat are stronger than those of ADM and LADM, e.g., allowing the penaltyparameter to be unbounded and proving the sufficient and necessary conditions}for global convergence. We further propose a simple optimality measure andreveal the convergence rate of LADMPSAP in an ergodic sense. For programs withextra convex set constraints, with refined parameter estimation we devise apractical version of LADMPSAP for faster convergence. Finally, we generalizeLADMPSAP to handle programs with more difficult objective functions bylinearizing part of the objective function as well. LADMPSAP is particularlysuitable for sparse representation and low-rank recovery problems because itssubproblems have closed form solutions and the sparsity and low-rankness of theiterates can be preserved during the iteration. It is also highlyparallelizable and hence fits for parallel or distributed computing. Numericalexperiments testify to the advantages of LADMPSAP in speed and numericalaccuracy.
arxiv-6300-155 | BayesOpt: A Bayesian Optimization Library for Nonlinear Optimization, Experimental Design and Bandits | http://arxiv.org/pdf/1405.7430v1.pdf | author:Ruben Martinez-Cantin category:cs.LG published:2014-05-29 summary:BayesOpt is a library with state-of-the-art Bayesian optimization methods tosolve nonlinear optimization, stochastic bandits or sequential experimentaldesign problems. Bayesian optimization is sample efficient by building aposterior distribution to capture the evidence and prior knowledge for thetarget function. Built in standard C++, the library is extremely efficientwhile being portable and flexible. It includes a common interface for C, C++,Python, Matlab and Octave.
arxiv-6300-156 | A Comparison of Nature Inspired Algorithms for Multi-threshold Image Segmentation | http://arxiv.org/pdf/1405.7406v1.pdf | author:Valent√≠n Osuna-Enciso, Erik Cuevas, Humberto Sossa category:cs.CV cs.NE published:2014-05-28 summary:In the field of image analysis, segmentation is one of the most importantpreprocessing steps. One way to achieve segmentation is by mean of thresholdselection, where each pixel that belongs to a determined class islabeledaccording to the selected threshold, giving as a result pixel groups that sharevisual characteristics in the image. Several methods have been proposed inorder to solve threshold selectionproblems; in this work, it is used the methodbased on the mixture of Gaussian functions to approximate the 1D histogram of agray level image and whose parameters are calculated using three natureinspired algorithms (Particle Swarm Optimization, Artificial Bee ColonyOptimization and Differential Evolution). Each Gaussian function approximatesthehistogram, representing a pixel class and therefore a threshold point.Experimental results are shown, comparing in quantitative and qualitativefashion as well as the main advantages and drawbacks of each algorithm, appliedto multi-threshold problem.
arxiv-6300-157 | An HMM Based Named Entity Recognition System for Indian Languages: The JU System at ICON 2013 | http://arxiv.org/pdf/1405.7397v1.pdf | author:Vivekananda Gayen, Kamal Sarkar category:cs.CL published:2014-05-28 summary:This paper reports about our work in the ICON 2013 NLP TOOLS CONTEST on NamedEntity Recognition. We submitted runs for Bengali, English, Hindi, Marathi,Punjabi, Tamil and Telugu. A statistical HMM (Hidden Markov Models) based modelhas been used to implement our system. The system has been trained and testedon the NLP TOOLS CONTEST: ICON 2013 datasets. Our system obtains F-measures of0.8599, 0.7704, 0.7520, 0.4289, 0.5455, 0.4466, and 0.4003 for Bengali,English, Hindi, Marathi, Punjabi, Tamil and Telugu respectively.
arxiv-6300-158 | Seeing What You're Told: Sentence-Guided Activity Recognition In Video | http://arxiv.org/pdf/1308.4189v2.pdf | author:N. Siddharth, Andrei Barbu, Jeffrey Mark Siskind category:cs.CV cs.AI cs.CL published:2013-08-19 summary:We present a system that demonstrates how the compositional structure ofevents, in concert with the compositional structure of language, can interplaywith the underlying focusing mechanisms in video action recognition, therebyproviding a medium, not only for top-down and bottom-up integration, but alsofor multi-modal integration between vision and language. We show how the rolesplayed by participants (nouns), their characteristics (adjectives), the actionsperformed (verbs), the manner of such actions (adverbs), and changing spatialrelations between participants (prepositions) in the form of whole sententialdescriptions mediated by a grammar, guides the activity-recognition process.Further, the utility and expressiveness of our framework is demonstrated byperforming three separate tasks in the domain of multi-activity videos:sentence-guided focus of attention, generation of sentential descriptions ofvideo, and query-based video search, simply by leveraging the framework indifferent manners.
arxiv-6300-159 | Circle detection using Discrete Differential Evolution Optimization | http://arxiv.org/pdf/1405.7362v1.pdf | author:Erik Cuevas, Daniel Zaldivar, Marco Perez, Marte Ramirez category:cs.CV published:2014-05-28 summary:This paper introduces a circle detection method based on DifferentialEvolution (DE) optimization. Just as circle detection has been latelyconsidered as a fundamental component for many computer vision algorithms, DEhas evolved as a successful heuristic method for solving complex optimizationproblems, still keeping a simple structure and an easy implementation. It hasalso shown advantageous convergence properties and remarkable robustness. Thedetection process is considered similar to a combinational optimizationproblem. The algorithm uses the combination of three edge points as parametersto determine circles candidates in the scene yielding a reduction of the searchspace. The objective function determines if some circle candidates are actuallypresent in the image. This paper focuses particularly on one DE-based algorithmknown as the Discrete Differential Evolution (DDE), which eventually has shownbetter results than the original DE in particular for solving combinatorialproblems. In the DDE, suitable conversion routines are incorporated into theDE, aiming to operate from integer values to real values and then gettinginteger values back, following the crossover operation. The final algorithm isa fast circle detector that locates circles with sub-pixel accuracy evenconsidering complicated conditions and noisy images. Experimental results onseveral synthetic and natural images with varying range of complexity validatethe efficiency of the proposed technique considering accuracy, speed, androbustness.
arxiv-6300-160 | Adaptive Feature Ranking for Unsupervised Transfer Learning | http://arxiv.org/pdf/1312.6190v2.pdf | author:Son N. Tran, Artur d'Avila Garcez category:cs.LG published:2013-12-21 summary:Transfer Learning is concerned with the application of knowledge gained fromsolving a problem to a different but related problem domain. In this paper, wepropose a method and efficient algorithm for ranking and selectingrepresentations from a Restricted Boltzmann Machine trained on a source domainto be transferred onto a target domain. Experiments carried out using theMNIST, ICDAR and TiCC image datasets show that the proposed adaptive featureranking and transfer learning method offers statistically significantimprovements on the training of RBMs. Our method is general in that theknowledge chosen by the ranking function does not depend on its relation to anyspecific target domain, and it works with unsupervised learning andknowledge-based transfer.
arxiv-6300-161 | Reconstructing Native Language Typology from Foreign Language Usage | http://arxiv.org/pdf/1404.6312v2.pdf | author:Yevgeni Berzak, Roi Reichart, Boris Katz category:cs.CL published:2014-04-25 summary:Linguists and psychologists have long been studying cross-linguistictransfer, the influence of native language properties on linguistic performancein a foreign language. In this work we provide empirical evidence for thisprocess in the form of a strong correlation between language similaritiesderived from structural features in English as Second Language (ESL) texts andequivalent similarities obtained from the typological features of the nativelanguages. We leverage this finding to recover native language typologicalsimilarity structure directly from ESL text, and perform prediction oftypological features in an unsupervised fashion with respect to the targetlanguages. Our method achieves 72.2% accuracy on the typology prediction task,a result that is highly competitive with equivalent methods that rely ontypological resources.
arxiv-6300-162 | Seeking multi-thresholds for image segmentation with Learning Automata | http://arxiv.org/pdf/1405.7361v1.pdf | author:Erik Cuevas, Daniel Zaldivar, Marco Perez category:cs.CV published:2014-05-28 summary:This paper explores the use of the Learning Automata (LA) algorithm tocompute threshold selection for image segmentation as it is a criticalpreprocessing step for image analysis, pattern recognition and computer vision.LA is a heuristic method which is able to solve complex optimization problemswith interesting results in parameter estimation. Despite other techniquescommonly seek through the parameter map, LA explores in the probability spaceproviding appropriate convergence properties and robustness. The segmentationtask is therefore considered as an optimization problem and the LA is used togenerate the image multi-threshold separation. In this approach, one 1Dhistogram of a given image is approximated through a Gaussian mixture modelwhose parameters are calculated using the LA algorithm. Each Gaussian functionapproximating the histogram represents a pixel class and therefore a thresholdpoint. The method shows fast convergence avoiding the typical sensitivity toinitial conditions such as the Expectation Maximization (EM) algorithm or thecomplex time-consuming computations commonly found in gradient methods.Experimental results demonstrate the algorithm ability to perform automaticmulti-threshold selection and show interesting advantages as it is compared toother algorithms solving the same task.
arxiv-6300-163 | Circle detection by Harmony Search Optimization | http://arxiv.org/pdf/1405.7242v1.pdf | author:Erik Cuevas, Noe Ortega, Daniel Zaldivar, Marco Perez category:cs.CV published:2014-05-28 summary:Automatic circle detection in digital images has received considerableattention over the last years in computer vision as several efforts have aimedfor an optimal circle detector. This paper presents an algorithm for automaticdetection of circular shapes that considers the overall process as anoptimization problem. The approach is based on the Harmony Search Algorithm(HSA), a derivative free meta-heuristic optimization algorithm inspired bymusicians while improvising new harmonies. The algorithm uses the encoding ofthree points as candidate circles (harmonies) over the edge-only image. Anobjective function evaluates (harmony quality) if such candidate circles areactually present in the edge image. Guided by the values of this objectivefunction, the set of encoded candidate circles are evolved using the HSA sothat they can fit to the actual circles on the edge map of the image (optimalharmony). Experimental results from several tests on synthetic and naturalimages with a varying complexity range have been included to validate theefficiency of the proposed technique regarding accuracy, speed and robustness.
arxiv-6300-164 | Machine Learner for Automated Reasoning 0.4 and 0.5 | http://arxiv.org/pdf/1402.2359v2.pdf | author:Cezary Kaliszyk, Josef Urban, Ji≈ô√≠ Vyskoƒçil category:cs.LG cs.AI cs.LO published:2014-02-11 summary:Machine Learner for Automated Reasoning (MaLARea) is a learning and reasoningsystem for proving in large formal libraries where thousands of theorems areavailable when attacking a new conjecture, and a large number of relatedproblems and proofs can be used to learn specific theorem-proving knowledge.The last version of the system has by a large margin won the 2013 CASC LTBcompetition. This paper describes the motivation behind the methods used inMaLARea, discusses the general approach and the issues arising in evaluation ofsuch system, and describes the Mizar@Turing100 and CASC'24 versions of MaLARea.
arxiv-6300-165 | A Multi-threshold Segmentation Approach Based on Artificial Bee Colony Optimization | http://arxiv.org/pdf/1405.7229v1.pdf | author:Erik Cuevas, Felipe Sencion, Daniel Zaldivar, Marco Perez, Humberto Sossa category:cs.CV cs.NE published:2014-05-28 summary:This paper explores the use of the Artificial Bee Colony (ABC) algorithm tocompute threshold selection for image segmentation. ABC is a heuristicalgorithm motivated by the intelligent behavior of honey-bees which has beensuccessfully employed to solve complex optimization problems. In this approach,an image 1D histogram is approximated through a Gaussian mixture model whoseparameters are calculated by the ABC algorithm. For the approximation scheme,each Gaussian function represents a pixel class and therefore a threshold.Unlike the Expectation Maximization (EM) algorithm, the ABC based method showsfast convergence and low sensitivity to initial conditions. Remarkably, it alsoimproves complex time consuming computations commonly required bygradient-based methods. Experimental results demonstrate the algorithms abilityto perform automatic multi threshold selection yet showing interestingadvantages by comparison to other well known algorithms.
arxiv-6300-166 | BliStr: The Blind Strategymaker | http://arxiv.org/pdf/1301.2683v2.pdf | author:Josef Urban category:cs.AI cs.LG cs.LO published:2013-01-12 summary:BliStr is a system that automatically develops strategies for E prover on alarge set of problems. The main idea is to interleave (i) iteratedlow-timelimit local search for new strategies on small sets of similar easyproblems with (ii) higher-timelimit evaluation of the new strategies on allproblems. The accumulated results of the global higher-timelimit runs are usedto define and evolve the notion of "similar easy problems", and to control theselection of the next strategy to be improved. The technique was used tosignificantly strengthen the set of E strategies used by the MaLARea, PS-E,E-MaLeS, and E systems in the CASC@Turing 2012 competition, particularly in theMizar division. Similar improvement was obtained on the problems created fromthe Flyspeck corpus.
arxiv-6300-167 | On the saddle point problem for non-convex optimization | http://arxiv.org/pdf/1405.4604v2.pdf | author:Razvan Pascanu, Yann N. Dauphin, Surya Ganguli, Yoshua Bengio category:cs.LG cs.NE published:2014-05-19 summary:A central challenge to many fields of science and engineering involvesminimizing non-convex error functions over continuous, high dimensional spaces.Gradient descent or quasi-Newton methods are almost ubiquitously used toperform such minimizations, and it is often thought that a main source ofdifficulty for the ability of these local methods to find the global minimum isthe proliferation of local minima with much higher error than the globalminimum. Here we argue, based on results from statistical physics, randommatrix theory, and neural network theory, that a deeper and more profounddifficulty originates from the proliferation of saddle points, not localminima, especially in high dimensional problems of practical interest. Suchsaddle points are surrounded by high error plateaus that can dramatically slowdown learning, and give the illusory impression of the existence of a localminimum. Motivated by these arguments, we propose a new algorithm, thesaddle-free Newton method, that can rapidly escape high dimensional saddlepoints, unlike gradient descent and quasi-Newton methods. We apply thisalgorithm to deep neural network training, and provide preliminary numericalevidence for its superior performance.
arxiv-6300-168 | Layered Logic Classifiers: Exploring the `And' and `Or' Relations | http://arxiv.org/pdf/1405.6804v2.pdf | author:Zhuowen Tu, Piotr Dollar, Yingnian Wu category:stat.ML cs.LG published:2014-05-27 summary:Designing effective and efficient classifier for pattern analysis is a keyproblem in machine learning and computer vision. Many the solutions to theproblem require to perform logic operations such as `and', `or', and `not'.Classification and regression tree (CART) include these operations explicitly.Other methods such as neural networks, SVM, and boosting learn/compute aweighted sum on features (weak classifiers), which weakly perform the 'and' and'or' operations. However, it is hard for these classifiers to deal with the'xor' pattern directly. In this paper, we propose layered logic classifiers forpatterns of complicated distributions by combining the `and', `or', and `not'operations. The proposed algorithm is very general and easy to implement. Wetest the classifiers on several typical datasets from the Irvine repository andtwo challenging vision applications, object segmentation and pedestriandetection. We observe significant improvements on all the datasets over thewidely used decision stump based AdaBoost algorithm. The resulting classifiershave much less training complexity than decision tree based AdaBoost, and canbe applied in a wide range of domains.
arxiv-6300-169 | Learning Coverage Functions and Private Release of Marginals | http://arxiv.org/pdf/1304.2079v3.pdf | author:Vitaly Feldman, Pravesh Kothari category:cs.LG cs.CC cs.DS published:2013-04-08 summary:We study the problem of approximating and learning coverage functions. Afunction $c: 2^{[n]} \rightarrow \mathbf{R}^{+}$ is a coverage function, ifthere exists a universe $U$ with non-negative weights $w(u)$ for each $u \in U$and subsets $A_1, A_2, \ldots, A_n$ of $U$ such that $c(S) = \sum_{u \in\cup_{i \in S} A_i} w(u)$. Alternatively, coverage functions can be describedas non-negative linear combinations of monotone disjunctions. They are anatural subclass of submodular functions and arise in a number of applications. We give an algorithm that for any $\gamma,\delta>0$, given random and uniformexamples of an unknown coverage function $c$, finds a function $h$ thatapproximates $c$ within factor $1+\gamma$ on all but $\delta$-fraction of thepoints in time $poly(n,1/\gamma,1/\delta)$. This is the first fully-polynomialalgorithm for learning an interesting class of functions in the demanding PMACmodel of Balcan and Harvey (2011). Our algorithms are based on several newstructural properties of coverage functions. Using the results in (Feldman andKothari, 2014), we also show that coverage functions are learnable agnosticallywith excess $\ell_1$-error $\epsilon$ over all product and symmetricdistributions in time $n^{\log(1/\epsilon)}$. In contrast, we show that,without assumptions on the distribution, learning coverage functions is atleast as hard as learning polynomial-size disjoint DNF formulas, a class offunctions for which the best known algorithm runs in time$2^{\tilde{O}(n^{1/3})}$ (Klivans and Servedio, 2004). As an application of our learning results, we give simpledifferentially-private algorithms for releasing monotone conjunction countingqueries with low average error. In particular, for any $k \leq n$, we obtainprivate release of $k$-way marginals with average error $\bar{\alpha}$ in time$n^{O(\log(1/\bar{\alpha}))}$.
arxiv-6300-170 | Fast Supervised Hashing with Decision Trees for High-Dimensional Data | http://arxiv.org/pdf/1404.1561v2.pdf | author:Guosheng Lin, Chunhua Shen, Qinfeng Shi, Anton van den Hengel, David Suter category:cs.CV cs.LG published:2014-04-06 summary:Supervised hashing aims to map the original features to compact binary codesthat are able to preserve label based similarity in the Hamming space.Non-linear hash functions have demonstrated the advantage over linear ones dueto their powerful generalization capability. In the literature, kernelfunctions are typically used to achieve non-linearity in hashing, which achieveencouraging retrieval performance at the price of slow evaluation and trainingtime. Here we propose to use boosted decision trees for achieving non-linearityin hashing, which are fast to train and evaluate, hence more suitable forhashing with high dimensional data. In our approach, we first proposesub-modular formulations for the hashing binary code inference problem and anefficient GraphCut based block search method for solving large-scale inference.Then we learn hash functions by training boosted decision trees to fit thebinary codes. Experiments demonstrate that our proposed method significantlyoutperforms most state-of-the-art methods in retrieval precision and trainingtime. Especially for high-dimensional data, our method is orders of magnitudefaster than many methods in terms of training time.
arxiv-6300-171 | An FPGA-based Parallel Architecture for Face Detection using Mixed Color Models | http://arxiv.org/pdf/1405.7032v1.pdf | author:Luo Tao, Shi zaifeng category:cs.CV 68U10 published:2014-05-27 summary:In this paper, a reliable method for detecting human faces in color images isproposed. This system firstly detects skin color in YCgCr and YIQ color space,then filters binary texture and the result is morphological processed, finallyconverts skin tone to the preferred skin color configured by users in YIQ colorspace. The real-time adjusting circuit is implemented and some of simulationresults are given out. Experimental results demonstrate that the method hasachieved high rates and low false positives, another advantage is itssimplicity and minor computational costs.
arxiv-6300-172 | Sum-of-squares proofs and the quest toward optimal algorithms | http://arxiv.org/pdf/1404.5236v2.pdf | author:Boaz Barak, David Steurer category:cs.DS cs.CC cs.LG math.OC published:2014-04-21 summary:In order to obtain the best-known guarantees, algorithms are traditionallytailored to the particular problem we want to solve. Two recent developments,the Unique Games Conjecture (UGC) and the Sum-of-Squares (SOS) method,surprisingly suggest that this tailoring is not necessary and that a singleefficient algorithm could achieve best possible guarantees for a wide range ofdifferent problems. The Unique Games Conjecture (UGC) is a tantalizing conjecture incomputational complexity, which, if true, will shed light on the complexity ofa great many problems. In particular this conjecture predicts that a singleconcrete algorithm provides optimal guarantees among all efficient algorithmsfor a large class of computational problems. The Sum-of-Squares (SOS) method is a general approach for solving systems ofpolynomial constraints. This approach is studied in several scientificdisciplines, including real algebraic geometry, proof complexity, controltheory, and mathematical programming, and has found applications in fields asdiverse as quantum information theory, formal verification, game theory andmany others. We survey some connections that were recently uncovered between the UniqueGames Conjecture and the Sum-of-Squares method. In particular, we discuss newtools to rigorously bound the running time of the SOS method for obtainingapproximate solutions to hard optimization problems, and how these tools givethe potential for the sum-of-squares method to provide new guarantees for manyproblems of interest, and possibly to even refute the UGC.
arxiv-6300-173 | Futility Analysis in the Cross-Validation of Machine Learning Models | http://arxiv.org/pdf/1405.6974v1.pdf | author:Max Kuhn category:stat.ML cs.LG published:2014-05-27 summary:Many machine learning models have important structural tuning parameters thatcannot be directly estimated from the data. The common tactic for setting theseparameters is to use resampling methods, such as cross--validation or thebootstrap, to evaluate a candidate set of values and choose the best based onsome pre--defined criterion. Unfortunately, this process can be time consuming.However, the model tuning process can be streamlined by adaptively resamplingcandidate values so that settings that are clearly sub-optimal can bediscarded. The notion of futility analysis is introduced in this context. Anexample is shown that illustrates how adaptive resampling can be used to reducetraining time. Simulation studies are used to understand how the potentialspeed--up is affected by parallel processing techniques.
arxiv-6300-174 | Large Scale, Large Margin Classification using Indefinite Similarity Measures | http://arxiv.org/pdf/1405.6922v1.pdf | author:Omid Aghazadeh, Stefan Carlsson category:cs.LG cs.CV stat.ML published:2014-05-27 summary:Despite the success of the popular kernelized support vector machines, theyhave two major limitations: they are restricted to Positive Semi-Definite (PSD)kernels, and their training complexity scales at least quadratically with thesize of the data. Many natural measures of similarity between pairs of samplesare not PSD e.g. invariant kernels, and those that are implicitly or explicitlydefined by latent variable models. In this paper, we investigate scalableapproaches for using indefinite similarity measures in large margin frameworks.In particular we show that a normalization of similarity to a subset of thedata points constitutes a representation suitable for linear classifiers. Theresult is a classifier which is competitive to kernelized SVM in terms ofaccuracy, despite having better training and test time complexities.Experimental results demonstrate that on CIFAR-10 dataset, the model equippedwith similarity measures invariant to rigid and non-rigid deformations, can bemade more than 5 times sparser while being more accurate than kernelized SVMusing RBF kernels.
arxiv-6300-175 | Supervised Dictionary Learning by a Variational Bayesian Group Sparse Nonnegative Matrix Factorization | http://arxiv.org/pdf/1405.6914v1.pdf | author:Ivan Ivek category:cs.CV cs.LG stat.ML published:2014-05-27 summary:Nonnegative matrix factorization (NMF) with group sparsity constraints isformulated as a probabilistic graphical model and, assuming some observed datahave been generated by the model, a feasible variational Bayesian algorithm isderived for learning model parameters. When used in a supervised learningscenario, NMF is most often utilized as an unsupervised feature extractorfollowed by classification in the obtained feature subspace. Having mapped theclass labels to a more general concept of groups which underlie sparsity of thecoefficients, what the proposed group sparse NMF model allows is incorporatingclass label information to find low dimensional label-driven dictionaries whichnot only aim to represent the data faithfully, but are also suitable for classdiscrimination. Experiments performed in face recognition and facial expressionrecognition domains point to advantages of classification in such label-drivenfeature subspaces over classification in feature subspaces obtained in anunsupervised manner.
arxiv-6300-176 | A Topic Model Approach to Multi-Modal Similarity | http://arxiv.org/pdf/1405.6886v1.pdf | author:Rasmus Troelsg√•rd, Bj√∏rn Sand Jensen, Lars Kai Hansen category:cs.IR stat.ML published:2014-05-27 summary:Calculating similarities between objects defined by many heterogeneous datamodalities is an important challenge in many multimedia applications. We use amulti-modal topic model as a basis for defining such a similarity betweenobjects. We propose to compare the resulting similarities from different modelrealizations using the non-parametric Mantel test. The approach is evaluated ona music dataset.
arxiv-6300-177 | Human Pose Estimation from RGB Input Using Synthetic Training Data | http://arxiv.org/pdf/1405.1213v2.pdf | author:Oscar Danielsson, Omid Aghazadeh category:cs.CV published:2014-05-06 summary:We address the problem of estimating the pose of humans using RGB imageinput. More specifically, we are using a random forest classifier to classifypixels into joint-based body part categories, much similar to the famous Kinectpose estimator [11], [12]. However, we are using pure RGB input, i.e. no depth.Since the random forest requires a large number of training examples, we areusing computer graphics generated, synthetic training data. In addition, weassume that we have access to a large number of real images with bounding boxlabels, extracted for example by a pedestrian detector or a tracking system. Wepropose a new objective function for random forest training that uses theweakly labeled data from the target domain to encourage the learner to selectfeatures that generalize from the synthetic source domain to the real targetdomain. We demonstrate on a publicly available dataset [6] that the proposedobjective function yields a classifier that significantly outperforms abaseline classifier trained using the standard entropy objective [10].
arxiv-6300-178 | Understanding Machine-learned Density Functionals | http://arxiv.org/pdf/1404.1333v2.pdf | author:Li Li, John C. Snyder, Isabelle M. Pelaschier, Jessica Huang, Uma-Naresh Niranjan, Paul Duncan, Matthias Rupp, Klaus-Robert M√ºller, Kieron Burke category:cs.LG stat.ML published:2014-04-04 summary:Kernel ridge regression is used to approximate the kinetic energy ofnon-interacting fermions in a one-dimensional box as a functional of theirdensity. The properties of different kernels and methods of cross-validationare explored, and highly accurate energies are achieved. Accurate {\emconstrained optimal densities} are found via a modified Euler-Lagrangeconstrained minimization of the total energy. A projected gradient descentalgorithm is derived using local principal component analysis. Additionally, asparse grid representation of the density can be used without degrading theperformance of the methods. The implications for machine-learned densityfunctional approximations are discussed.
arxiv-6300-179 | Proximal Reinforcement Learning: A New Theory of Sequential Decision Making in Primal-Dual Spaces | http://arxiv.org/pdf/1405.6757v1.pdf | author:Sridhar Mahadevan, Bo Liu, Philip Thomas, Will Dabney, Steve Giguere, Nicholas Jacek, Ian Gemp, Ji Liu category:cs.LG published:2014-05-26 summary:In this paper, we set forth a new vision of reinforcement learning developedby us over the past few years, one that yields mathematically rigoroussolutions to longstanding important questions that have remained unresolved:(i) how to design reliable, convergent, and robust reinforcement learningalgorithms (ii) how to guarantee that reinforcement learning satisfiespre-specified "safety" guarantees, and remains in a stable region of theparameter space (iii) how to design "off-policy" temporal difference learningalgorithms in a reliable and stable manner, and finally (iv) how to integratethe study of reinforcement learning into the rich theory of stochasticoptimization. In this paper, we provide detailed answers to all these questionsusing the powerful framework of proximal operators. The key idea that emerges is the use of primal dual spaces connected throughthe use of a Legendre transform. This allows temporal difference updates tooccur in dual spaces, allowing a variety of important technical advantages. TheLegendre transform elegantly generalizes past algorithms for solvingreinforcement learning problems, such as natural gradient methods, which weshow relate closely to the previously unconnected framework of mirror descentmethods. Equally importantly, proximal operator theory enables the systematicdevelopment of operator splitting methods that show how to safely and reliablydecompose complex products of gradients that occur in recent variants ofgradient-based temporal difference learning. This key technical innovationmakes it possible to finally design "true" stochastic gradient methods forreinforcement learning. Finally, Legendre transforms enable a variety of otherbenefits, including modeling sparsity and domain geometry. Our work buildsextensively on recent work on the convergence of saddle-point algorithms, andon the theory of monotone operators.
arxiv-6300-180 | Visualizing Random Forest with Self-Organising Map | http://arxiv.org/pdf/1405.6684v1.pdf | author:Piotr P≈Ço≈Ñski, Krzysztof Zaremba category:cs.LG published:2014-05-26 summary:Random Forest (RF) is a powerful ensemble method for classification andregression tasks. It consists of decision trees set. Although, a single tree iswell interpretable for human, the ensemble of trees is a black-box model. Thepopular technique to look inside the RF model is to visualize a RF proximitymatrix obtained on data samples with Multidimensional Scaling (MDS) method.Herein, we present a novel method based on Self-Organising Maps (SOM) forrevealing intrinsic relationships in data that lay inside the RF used forclassification tasks. We propose an algorithm to learn the SOM with theproximity matrix obtained from the RF. The visualization of RF proximity matrixwith MDS and SOM is compared. What is more, the SOM learned with the RFproximity matrix has better classification accuracy in comparison to SOMlearned with Euclidean distance. Presented approach enables betterunderstanding of the RF and additionally improves accuracy of the SOM.
arxiv-6300-181 | Optimality Theory as a Framework for Lexical Acquisition | http://arxiv.org/pdf/1405.6682v1.pdf | author:Thierry Poibeau category:cs.CL published:2014-05-26 summary:This paper re-investigates a lexical acquisition system initially developedfor French.We show that, interestingly, the architecture of the systemreproduces and implements the main components of Optimality Theory. However, weformulate the hypothesis that some of its limitations are mainly due to a poorrepresentation of the constraints used. Finally, we show how a betterrepresentation of the constraints used would yield better results.
arxiv-6300-182 | Hybrid Type-Logical Grammars, First-Order Linear Logic and the Descriptive Inadequacy of Lambda Grammars | http://arxiv.org/pdf/1405.6678v1.pdf | author:Richard Moot category:cs.LO cs.CL published:2014-05-26 summary:In this article we show that hybrid type-logical grammars are a fragment offirst-order linear logic. This embedding result has several importantconsequences: it not only provides a simple new proof theory for the calculus,thereby clarifying the proof-theoretic foundations of hybrid type-logicalgrammars, but, since the translation is simple and direct, it also providesseveral new parsing strategies for hybrid type-logical grammars. Second,NP-completeness of hybrid type-logical grammars follows immediately. The mainembedding result also sheds new light on problems with lambda grammars/abstractcategorial grammars and shows lambda grammars/abstract categorial grammarssuffer from problems of over-generation and from problems at thesyntax-semantics interface unlike any other categorial grammar.
arxiv-6300-183 | Self-tuned Visual Subclass Learning with Shared Samples An Incremental Approach | http://arxiv.org/pdf/1405.5732v2.pdf | author:Hossein Azizpour, Stefan Carlsson category:cs.CV published:2014-05-22 summary:Computer vision tasks are traditionally defined and evaluated using semanticcategories. However, it is known to the field that semantic classes do notnecessarily correspond to a unique visual class (e.g. inside and outside of acar). Furthermore, many of the feasible learning techniques at hand cannotmodel a visual class which appears consistent to the human eye. These problemshave motivated the use of 1) Unsupervised or supervised clustering as apreprocessing step to identify the visual subclasses to be used in amixture-of-experts learning regime. 2) Felzenszwalb et al. part model and otherworks model mixture assignment with latent variables which is optimized duringlearning 3) Highly non-linear classifiers which are inherently capable ofmodelling multi-modal input space but are inefficient at the test time. In thiswork, we promote an incremental view over the recognition of semantic classeswith varied appearances. We propose an optimization technique whichincrementally finds maximal visual subclasses in a regularized riskminimization framework. Our proposed approach unifies the clustering andclassification steps in a single algorithm. The importance of this approach isits compliance with the classification via the fact that it does not need toknow about the number of clusters, the representation and similarity measuresused in pre-processing clustering methods a priori. Following this approach weshow both qualitatively and quantitatively significant results. We show thatthe visual subclasses demonstrate a long tail distribution. Finally, we showthat state of the art object detection methods (e.g. DPM) are unable to use thetails of this distribution comprising 50\% of the training samples. In fact weshow that DPM performance slightly increases on average by the removal of thishalf of the data.
arxiv-6300-184 | Inferring gender of a Twitter user using celebrities it follows | http://arxiv.org/pdf/1405.6667v1.pdf | author:Puneet Singh Ludu category:cs.IR cs.CL published:2014-05-26 summary:This paper addresses the task of user gender classification in social media,with an application to Twitter. The approach automatically predicts gender byleveraging observable information such as the tweet behavior, linguisticcontent of the user's Twitter feed and the celebrities followed by the user.This paper first evaluates linguistic content based features using LIWCdictionary and popular neighborhood features using Wikipedia and Freebase. Thenaugments both features which yielded a significant increase in the accuracy forgender prediction. Results show that rich linguistic features combined withpopular neighborhood prove valuables and promising for additional userclassification needs.
arxiv-6300-185 | New Algorithms for Learning Incoherent and Overcomplete Dictionaries | http://arxiv.org/pdf/1308.6273v5.pdf | author:Sanjeev Arora, Rong Ge, Ankur Moitra category:cs.DS cs.LG stat.ML published:2013-08-28 summary:In sparse recovery we are given a matrix $A$ (the dictionary) and a vector ofthe form $A X$ where $X$ is sparse, and the goal is to recover $X$. This is acentral notion in signal processing, statistics and machine learning. But inapplications such as sparse coding, edge detection, compression and superresolution, the dictionary $A$ is unknown and has to be learned from randomexamples of the form $Y = AX$ where $X$ is drawn from an appropriatedistribution --- this is the dictionary learning problem. In most settings, $A$is overcomplete: it has more columns than rows. This paper presents apolynomial-time algorithm for learning overcomplete dictionaries; the onlypreviously known algorithm with provable guarantees is the recent work ofSpielman, Wang and Wright who gave an algorithm for the full-rank case, whichis rarely the case in applications. Our algorithm applies to incoherentdictionaries which have been a central object of study since they wereintroduced in seminal work of Donoho and Huo. In particular, a dictionary is$\mu$-incoherent if each pair of columns has inner product at most $\mu /\sqrt{n}$. The algorithm makes natural stochastic assumptions about the unknown sparsevector $X$, which can contain $k \leq c \min(\sqrt{n}/\mu \log n, m^{1/2-\eta})$ non-zero entries (for any $\eta > 0$). This is close to the best $k$allowable by the best sparse recovery algorithms even if one knows thedictionary $A$ exactly. Moreover, both the running time and sample complexitydepend on $\log 1/\epsilon$, where $\epsilon$ is the target accuracy, and soour algorithms converge very quickly to the true dictionary. Our algorithm canalso tolerate substantial amounts of noise provided it is incoherent withrespect to the dictionary (e.g., Gaussian). In the noisy setting, our runningtime and sample complexity depend polynomially on $1/\epsilon$, and this isnecessary.
arxiv-6300-186 | Bayesian Inference for Gaussian Process Classifiers with Annealing and Pseudo-Marginal MCMC | http://arxiv.org/pdf/1311.7320v2.pdf | author:Maurizio Filippone category:stat.ME stat.ML published:2013-11-28 summary:Kernel methods have revolutionized the fields of pattern recognition andmachine learning. Their success, however, critically depends on the choice ofkernel parameters. Using Gaussian process (GP) classification as a workingexample, this paper focuses on Bayesian inference of covariance (kernel)parameters using Markov chain Monte Carlo (MCMC) methods. The motivation isthat, compared to standard optimization of kernel parameters, they have beensystematically demonstrated to be superior in quantifying uncertainty inpredictions. Recently, the Pseudo-Marginal MCMC approach has been proposed as apractical inference tool for GP models. In particular, it amounts in replacingthe analytically intractable marginal likelihood by an unbiased estimateobtainable by approximate methods and importance sampling. After discussing thepotential drawbacks in employing importance sampling, this paper proposes theapplication of annealed importance sampling. The results empiricallydemonstrate that compared to importance sampling, annealed importance samplingcan reduce the variance of the estimate of the marginal likelihoodexponentially in the number of data at a computational cost that scales onlypolynomially. The results on real data demonstrate that employing annealedimportance sampling in the Pseudo-Marginal MCMC approach represents a stepforward in the development of fully automated exact inference engines for GPmodels.
arxiv-6300-187 | Robust Temporally Coherent Laplacian Protrusion Segmentation of 3D Articulated Bodies | http://arxiv.org/pdf/1405.6563v1.pdf | author:Fabio Cuzzolin, Diana Mateus, Radu Horaud category:cs.CV cs.GR cs.LG published:2014-05-26 summary:In motion analysis and understanding it is important to be able to fit asuitable model or structure to the temporal series of observed data, in orderto describe motion patterns in a compact way, and to discriminate between them.In an unsupervised context, i.e., no prior model of the moving object(s) isavailable, such a structure has to be learned from the data in a bottom-upfashion. In recent times, volumetric approaches in which the motion is capturedfrom a number of cameras and a voxel-set representation of the body is builtfrom the camera views, have gained ground due to attractive features such asinherent view-invariance and robustness to occlusions. Automatic, unsupervisedsegmentation of moving bodies along entire sequences, in a temporally-coherentand robust way, has the potential to provide a means of constructing abottom-up model of the moving body, and track motion cues that may be laterexploited for motion classification. Spectral methods such as locally linearembedding (LLE) can be useful in this context, as they preserve "protrusions",i.e., high-curvature regions of the 3D volume, of articulated shapes, whileimproving their separation in a lower dimensional space, making them in thisway easier to cluster. In this paper we therefore propose a spectral approachto unsupervised and temporally-coherent body-protrusion segmentation along timesequences. Volumetric shapes are clustered in an embedding space, clusters arepropagated in time to ensure coherence, and merged or split to accommodatechanges in the body's topology. Experiments on both synthetic and realsequences of dense voxel-set data are shown. This supports the ability of theproposed method to cluster body-parts consistently over time in a totallyunsupervised fashion, its robustness to sampling density and shape quality, andits potential for bottom-up model construction
arxiv-6300-188 | Automatic large-scale classification of bird sounds is strongly improved by unsupervised feature learning | http://arxiv.org/pdf/1405.6524v1.pdf | author:Dan Stowell, Mark D. Plumbley category:cs.SD cs.LG published:2014-05-26 summary:Automatic species classification of birds from their sound is a computationaltool of increasing importance in ecology, conservation monitoring and vocalcommunication studies. To make classification useful in practice, it is crucialto improve its accuracy while ensuring that it can run at big data scales. Manyapproaches use acoustic measures based on spectrogram-type data, such as theMel-frequency cepstral coefficient (MFCC) features which represent amanually-designed summary of spectral information. However, recent work inmachine learning has demonstrated that features learnt automatically from datacan often outperform manually-designed feature transforms. Feature learning canbe performed at large scale and "unsupervised", meaning it requires no manualdata labelling, yet it can improve performance on "supervised" tasks such asclassification. In this work we introduce a technique for feature learning fromlarge volumes of bird sound recordings, inspired by techniques that have provenuseful in other domains. We experimentally compare twelve different featurerepresentations derived from the Mel spectrum (of which six use thistechnique), using four large and diverse databases of bird vocalisations, witha random forest classifier. We demonstrate that MFCCs are of limited power inthis context, leading to worse performance than the raw Mel spectral data.Conversely, we demonstrate that unsupervised feature learning provides asubstantial boost over MFCCs and Mel spectra without adding computationalcomplexity after the model has been trained. The boost is particularly notablefor single-label classification tasks at large scale. The spectro-temporalactivations learned through our procedure resemble spectro-temporal receptivefields calculated from avian primary auditory forebrain.
arxiv-6300-189 | Fast and Robust Archetypal Analysis for Representation Learning | http://arxiv.org/pdf/1405.6472v1.pdf | author:Yuansi Chen, Julien Mairal, Zaid Harchaoui category:cs.CV cs.LG stat.ML published:2014-05-26 summary:We revisit a pioneer unsupervised learning technique called archetypalanalysis, which is related to successful data analysis methods such as sparsecoding and non-negative matrix factorization. Since it was proposed, archetypalanalysis did not gain a lot of popularity even though it produces moreinterpretable models than other alternatives. Because no efficientimplementation has ever been made publicly available, its application toimportant scientific problems may have been severely limited. Our goal is tobring back into favour archetypal analysis. We propose a fast optimizationscheme using an active-set strategy, and provide an efficient open-sourceimplementation interfaced with Matlab, R, and Python. Then, we demonstrate theusefulness of archetypal analysis for computer vision tasks, such as codebooklearning, signal classification, and large image collection visualization.
arxiv-6300-190 | Optimal interval clustering: Application to Bregman clustering and statistical mixture learning | http://arxiv.org/pdf/1403.2485v2.pdf | author:Frank Nielsen, Richard Nock category:cs.IT cs.LG math.IT published:2014-03-11 summary:We present a generic dynamic programming method to compute the optimalclustering of $n$ scalar elements into $k$ pairwise disjoint intervals. Thiscase includes 1D Euclidean $k$-means, $k$-medoids, $k$-medians, $k$-centers,etc. We extend the method to incorporate cluster size constraints and show howto choose the appropriate $k$ by model selection. Finally, we illustrate andrefine the method on two case studies: Bregman clustering and statisticalmixture learning maximizing the complete likelihood.
arxiv-6300-191 | The role of dimensionality reduction in linear classification | http://arxiv.org/pdf/1405.6444v1.pdf | author:Weiran Wang, Miguel √Å. Carreira-Perpi√±√°n category:cs.LG math.OC stat.ML published:2014-05-26 summary:Dimensionality reduction (DR) is often used as a preprocessing step inclassification, but usually one first fixes the DR mapping, possibly usinglabel information, and then learns a classifier (a filter approach). Bestperformance would be obtained by optimizing the classification error jointlyover DR mapping and classifier (a wrapper approach), but this is a difficultnonconvex problem, particularly with nonlinear DR. Using the method ofauxiliary coordinates, we give a simple, efficient algorithm to train acombination of nonlinear DR and a classifier, and apply it to a RBF mappingwith a linear SVM. This alternates steps where we train the RBF mapping and alinear SVM as usual regression and classification, respectively, with aclosed-form step that coordinates both. The resulting nonlinear low-dimensionalclassifier achieves classification errors competitive with the state-of-the-artbut is fast at training and testing, and allows the user to trade off runtimefor classification accuracy easily. We then study the role of nonlinear DR inlinear classification, and the interplay between the DR mapping, the number oflatent dimensions and the number of classes. When trained jointly, the DRmapping takes an extreme role in eliminating variation: it tends to collapseclasses in latent space, erasing all manifold structure, and lay out classcentroids so they are linearly separable with maximum margin.
arxiv-6300-192 | TempEval-3: Evaluating Events, Time Expressions, and Temporal Relations | http://arxiv.org/pdf/1206.5333v2.pdf | author:Naushad UzZaman, Hector Llorens, James Allen, Leon Derczynski, Marc Verhagen, James Pustejovsky category:cs.CL published:2012-06-22 summary:We describe the TempEval-3 task which is currently in preparation for theSemEval-2013 evaluation exercise. The aim of TempEval is to advance research ontemporal information processing. TempEval-3 follows on from previous TempEvalevents, incorporating: a three-part task structure covering event, temporalexpression and temporal relation extraction; a larger dataset; and singleoverall task quality scores.
arxiv-6300-193 | Parallelizing MCMC via Weierstrass Sampler | http://arxiv.org/pdf/1312.4605v2.pdf | author:Xiangyu Wang, David B. Dunson category:stat.CO cs.DC stat.ML published:2013-12-17 summary:With the rapidly growing scales of statistical problems, subset basedcommunication-free parallel MCMC methods are a promising future for large scaleBayesian analysis. In this article, we propose a new Weierstrass sampler forparallel MCMC based on independent subsets. The new sampler approximates thefull data posterior samples via combining the posterior draws from independentsubset MCMC chains, and thus enjoys a higher computational efficiency. We showthat the approximation error for the Weierstrass sampler is bounded by sometuning parameters and provide suggestions for choice of the values. Simulationstudy shows the Weierstrass sampler is very competitive compared to othermethods for combining MCMC chains generated for subsets, including averagingand kernel smoothing.
arxiv-6300-194 | Sparse Estimation From Noisy Observations of an Overdetermined Linear System | http://arxiv.org/pdf/1402.2864v2.pdf | author:Liang Dai, Kristiaan Pelckmans category:cs.SY stat.ML published:2014-02-12 summary:This note studies a method for the efficient estimation of a finite number ofunknown parameters from linear equations, which are perturbed by Gaussiannoise. In case the unknown parameters have only few nonzero entries, the proposedestimator performs more efficiently than a traditional approach. The method consists of three steps: (1) a classical Least Squares Estimate (LSE), (2) the support is recovered through a Linear Programming (LP) optimizationproblem which can be computed using a soft-thresholding step, (3) a de-biasing step using a LSE on the estimated support set. The main contribution of this note is a formal derivation of an associatedORACLE property of the final estimate. That is, when the number of samples is large enough, the estimate is shown toequal the LSE based on the support of the {\em true} parameters.
arxiv-6300-195 | Learning the Irreducible Representations of Commutative Lie Groups | http://arxiv.org/pdf/1402.4437v2.pdf | author:Taco Cohen, Max Welling category:cs.LG published:2014-02-18 summary:We present a new probabilistic model of compact commutative Lie groups thatproduces invariant-equivariant and disentangled representations of data. Todefine the notion of disentangling, we borrow a fundamental principle fromphysics that is used to derive the elementary particles of a system from itssymmetries. Our model employs a newfound Bayesian conjugacy relation thatenables fully tractable probabilistic inference over compact commutative Liegroups -- a class that includes the groups that describe the rotation andcyclic translation of images. We train the model on pairs of transformed imagepatches, and show that the learned invariant representation is highly effectivefor classification.
arxiv-6300-196 | Volumetric Spanners: an Efficient Exploration Basis for Learning | http://arxiv.org/pdf/1312.6214v3.pdf | author:Elad Hazan, Zohar Karnin, Raghu Mehka category:cs.LG cs.AI cs.DS published:2013-12-21 summary:Numerous machine learning problems require an exploration basis - a mechanismto explore the action space. We define a novel geometric notion of explorationbasis with low variance, called volumetric spanners, and give efficientalgorithms to construct such a basis. We show how efficient volumetric spanners give rise to the first efficientand optimal regret algorithm for bandit linear optimization over general convexsets. Previously such results were known only for specific convex sets, orunder special conditions such as the existence of an efficient self-concordantbarrier for the underlying set.
arxiv-6300-197 | Active causation and the origin of meaning | http://arxiv.org/pdf/1310.2063v4.pdf | author:J. H. van Hateren category:q-bio.PE cs.NE nlin.AO q-bio.NC published:2013-10-08 summary:Purpose and meaning are necessary concepts for understanding mind andculture, but appear to be absent from the physical world and are not part ofthe explanatory framework of the natural sciences. Understanding how meaning(in the broad sense of the term) could arise from a physical world has provento be a tough problem. The basic scheme of Darwinian evolution producesadaptations that only represent apparent ("as if") goals and meaning. Here Iuse evolutionary models to show that a slight, evolvable extension of the basicscheme is sufficient to produce genuine goals. The extension, targetedmodulation of mutation rate, is known to be generally present in biologicalcells, and gives rise to two phenomena that are absent from the non-livingworld: intrinsic meaning and the ability to initiate goal-directed chains ofcausation (active causation). The extended scheme accomplishes this byutilizing randomness modulated by a feedback loop that is itself regulated byevolutionary pressure. The mechanism can be extended to behavioural variabilityas well, and thus shows how freedom of behaviour is possible. A furtherextension to communication suggests that the active exchange of intrinsicmeaning between organisms may be the origin of consciousness, which incombination with active causation can provide a physical basis for thephenomenon of free will.
arxiv-6300-198 | A Novel Stochastic Decoding of LDPC Codes with Quantitative Guarantees | http://arxiv.org/pdf/1405.6353v1.pdf | author:Nima Noorshams, Aravind Iyengar category:cs.IT math.IT stat.ML published:2014-05-25 summary:Low-density parity-check codes, a class of capacity-approaching linear codes,are particularly recognized for their efficient decoding scheme. The decodingscheme, known as the sum-product, is an iterative algorithm consisting ofpassing messages between variable and check nodes of the factor graph. Thesum-product algorithm is fully parallelizable, owing to the fact that allmessages can be update concurrently. However, since it requires extensivenumber of highly interconnected wires, the fully-parallel implementation of thesum-product on chips is exceedingly challenging. Stochastic decodingalgorithms, which exchange binary messages, are of great interest formitigating this challenge and have been the focus of extensive research overthe past decade. They significantly reduce the required wiring andcomputational complexity of the message-passing algorithm. Even thoughstochastic decoders have been shown extremely effective in practice, thetheoretical aspect and understanding of such algorithms remains limited atlarge. Our main objective in this paper is to address this issue. We firstpropose a novel algorithm referred to as the Markov based stochastic decoding.Then, we provide concrete quantitative guarantees on its performance fortree-structured as well as general factor graphs. More specifically, we provideupper-bounds on the first and second moments of the error, illustrating thatthe proposed algorithm is an asymptotically consistent estimate of thesum-product algorithm. We also validate our theoretical predictions withexperimental results, showing we achieve comparable performance to otherpractical stochastic decoders.
arxiv-6300-199 | Efficient Model Learning for Human-Robot Collaborative Tasks | http://arxiv.org/pdf/1405.6341v1.pdf | author:Stefanos Nikolaidis, Keren Gu, Ramya Ramakrishnan, Julie Shah category:cs.RO cs.AI cs.LG cs.SY published:2014-05-24 summary:We present a framework for learning human user models from joint-actiondemonstrations that enables the robot to compute a robust policy for acollaborative task with a human. The learning takes place completelyautomatically, without any human intervention. First, we describe theclustering of demonstrated action sequences into different human types using anunsupervised learning algorithm. These demonstrated sequences are also used bythe robot to learn a reward function that is representative for each type,through the employment of an inverse reinforcement learning algorithm. Thelearned model is then used as part of a Mixed Observability Markov DecisionProcess formulation, wherein the human type is a partially observable variable.With this framework, we can infer, either offline or online, the human type ofa new user that was not included in the training set, and can compute a policyfor the robot that will be aligned to the preference of this new user and willbe robust to deviations of the human actions from prior demonstrations. Finallywe validate the approach using data collected in human subject experiments, andconduct proof-of-concept demonstrations in which a person performs acollaborative task with a small industrial robot.
arxiv-6300-200 | Four Classes of Morphogenetic Collective Systems | http://arxiv.org/pdf/1405.6296v1.pdf | author:Hiroki Sayama category:nlin.AO cs.NE published:2014-05-24 summary:We studied the roles of morphogenetic principles---heterogeneity ofcomponents, dynamic differentiation/re-differentiation of components, and localinformation sharing among components---in the self-organization ofmorphogenetic collective systems. By incrementally introducing these principlesto collectives, we defined four distinct classes of morphogenetic collectivesystems. Monte Carlo simulations were conducted using an extended version ofthe Swarm Chemistry model that was equipped with dynamicdifferentiation/re-differentiation and local information sharing capabilities.Self-organization of swarms was characterized by several kinetic andtopological measurements, the latter of which were facilitated by a newlydeveloped network-based method. Results of simulations revealed that, whileheterogeneity of components had a strong impact on the structure and behaviorof the swarms, dynamic differentiation/re-differentiation of components andlocal information sharing helped the swarms maintain spatially adjacent,coherent organization.
arxiv-6300-201 | Cross-Language Personal Name Mapping | http://arxiv.org/pdf/1405.6293v1.pdf | author:Ahmed H. Yousef category:cs.CL published:2014-05-24 summary:Name matching between multiple natural languages is an important step incross-enterprise integration applications and data mining. It is difficult todecide whether or not two syntactic values (names) from two heterogeneous datasources are alternative designation of the same semantic entity (person), thisprocess becomes more difficult with Arabic language due to several factorsincluding spelling and pronunciation variation, dialects and special vowel andconsonant distinction and other linguistic characteristics. This paper proposesa new framework for name matching between the Arabic language and otherlanguages. The framework uses a dictionary based on a new proposed version ofthe Soundex algorithm to encapsulate the recognition of special features ofArabic names. The framework proposes a new proximity matching algorithm to suitthe high importance of order sensitivity in Arabic name matching. Newperformance evaluation metrics are proposed as well. The framework isimplemented and verified empirically in several case studies demonstratingsubstantial improvements compared to other well-known techniques found inliterature.
arxiv-6300-202 | Online Learning with Predictable Sequences | http://arxiv.org/pdf/1208.3728v2.pdf | author:Alexander Rakhlin, Karthik Sridharan category:stat.ML cs.LG published:2012-08-18 summary:We present methods for online linear optimization that take advantage ofbenign (as opposed to worst-case) sequences. Specifically if the sequenceencountered by the learner is described well by a known "predictable process",the algorithms presented enjoy tighter bounds as compared to the typical worstcase bounds. Additionally, the methods achieve the usual worst-case regretbounds if the sequence is not benign. Our approach can be seen as a way ofadding prior knowledge about the sequence within the paradigm of onlinelearning. The setting is shown to encompass partial and side information.Variance and path-length bounds can be seen as particular examples of onlinelearning with simple predictable sequences. We further extend our methods and results to include competing with a set ofpossible predictable processes (models), that is "learning" the predictableprocess itself concurrently with using it to obtain better regret guarantees.We show that such model selection is possible under various assumptions on theavailable feedback. Our results suggest a promising direction of furtherresearch with potential applications to stock market and time seriesprediction.
arxiv-6300-203 | Kernel Estimation from Salient Structure for Robust Motion Deblurring | http://arxiv.org/pdf/1212.1073v2.pdf | author:Jinshan Pan, Risheng Liu, Zhixun Su, Xianfeng Gu category:cs.CV published:2012-12-05 summary:Blind image deblurring algorithms have been improving steadily in the pastyears. Most state-of-the-art algorithms, however, still cannot performperfectly in challenging cases, especially in large blur setting. In thispaper, we focus on how to estimate a good kernel estimate from a single blurredimage based on the image structure. We found that image details caused byblurring could adversely affect the kernel estimation, especially when the blurkernel is large. One effective way to eliminate these details is to apply imagedenoising model based on the Total Variation (TV). First, we developed a novelmethod for computing image structures based on TV model, such that thestructures undermining the kernel estimation will be removed. Second, tomitigate the possible adverse effect of salient edges and improve therobustness of kernel estimation, we applied a gradient selection method. Third,we proposed a novel kernel estimation method, which is capable of preservingthe continuity and sparsity of the kernel and reducing the noises. Finally, wedeveloped an adaptive weighted spatial prior, for the purpose of preservingsharp edges in latent image restoration. The effectiveness of our method isdemonstrated by experiments on various kinds of challenging examples.
arxiv-6300-204 | Traversing News with Ant Colony Optimisation and Negative Pheromones | http://arxiv.org/pdf/1405.6285v1.pdf | author:David M. S. Rodrigues, Vitorino Ramos category:cs.IR cs.NE published:2014-05-24 summary:The past decade has seen the rapid development of the online newsroom. Newspublished online are the main outlet of news surpassing traditional printednewspapers. This poses challenges to the production and to the consumption ofthose news. With those many sources of information available it is important tofind ways to cluster and organise the documents if one wants to understand thisnew system. A novel bio inspired approach to the problem of traversing the newsis presented. It finds Hamiltonian cycles over documents published by thenewspaper The Guardian. A Second Order Swarm Intelligence algorithm based onAnt Colony Optimisation was developed that uses a negative pheromone to markunrewarding paths with a "no-entry" signal. This approach follows recentfindings of negative pheromone usage in real ants.
arxiv-6300-205 | Improvements and Experiments of a Compact Statistical Background Model | http://arxiv.org/pdf/1405.6275v1.pdf | author:Dong Liang, Shun'ichi Kaneko category:cs.CV published:2014-05-24 summary:Change detection plays an important role in most video-based applications.The first stage is to build appropriate background model, which is now becomingincreasingly complex as more sophisticated statistical approaches areintroduced to cover challenging situations and provide reliable detection. Thispaper reports a simple and intuitive statistical model based on deeper learningspatial correlation among pixels: For each observed pixel, we select a group ofsupporting pixels with high correlation, and then use a single Gaussian tomodel the intensity deviations between the observed pixel and the supportingones. In addition, a multi-channel model updating is integrated on-line and atemporal intensity constraint for each pixel is defined. Although this methodis mainly designed for coping with sudden illumination changes, experimentalresults using all the video sequences provided on changedetection.net validateit is comparable with other recent methods under various situations.
arxiv-6300-206 | Geometric Polynomial Constraints in Higher-Order Graph Matching | http://arxiv.org/pdf/1405.6261v1.pdf | author:Mayank Bansal, Kostas Daniilidis category:cs.CV published:2014-05-24 summary:Correspondence is a ubiquitous problem in computer vision and graph matchinghas been a natural way to formalize correspondence as an optimization problem.Recently, graph matching solvers have included higher-order terms representingaffinities beyond the unary and pairwise level. Such higher-order terms have aparticular appeal for geometric constraints that include three or morecorrespondences like the PnP 2D-3D pose problems. In this paper, we address theproblem of finding correspondences in the absence of unary or pairwiseconstraints as it emerges in problems where unary appearance similarity likeSIFT matches is not available. Current higher order matching approaches havetargeted problems where higher order affinity can simply be formulated as adifference of invariances such as lengths, angles, or cross-ratios. In thispaper, we present a method of how to apply geometric constraints modeled aspolynomial equation systems. As opposed to RANSAC where such systems have to besolved and then tested for inlier hypotheses, our constraints are derived as asingle affinity weight based on $n>2$ hypothesized correspondences withoutsolving the polynomial system. Since the result is directly a correspondencewithout a transformation model, our approach supports correspondence matchingin the presence of multiple geometric transforms like articulated motions.
arxiv-6300-207 | Deep Generative Stochastic Networks Trainable by Backprop | http://arxiv.org/pdf/1306.1091v5.pdf | author:Yoshua Bengio, √âric Thibodeau-Laufer, Guillaume Alain, Jason Yosinski category:cs.LG published:2013-06-05 summary:We introduce a novel training principle for probabilistic models that is analternative to maximum likelihood. The proposed Generative Stochastic Networks(GSN) framework is based on learning the transition operator of a Markov chainwhose stationary distribution estimates the data distribution. The transitiondistribution of the Markov chain is conditional on the previous state,generally involving a small move, so this conditional distribution has fewerdominant modes, being unimodal in the limit of small moves. Thus, it is easierto learn because it is easier to approximate its partition function, more likelearning to perform supervised function approximation, with gradients that canbe obtained by backprop. We provide theorems that generalize recent work on theprobabilistic interpretation of denoising autoencoders and obtain along the wayan interesting justification for dependency networks and generalizedpseudolikelihood, along with a definition of an appropriate joint distributionand sampling mechanism even when the conditionals are not consistent. GSNs canbe used with missing inputs and can be used to sample subsets of variablesgiven the rest. We validate these theoretical results with experiments on twoimage datasets using an architecture that mimics the Deep Boltzmann MachineGibbs sampler but allows training to proceed with simple backprop, without theneed for layerwise pretraining.
arxiv-6300-208 | Connection graph Laplacian methods can be made robust to noise | http://arxiv.org/pdf/1405.6231v1.pdf | author:Noureddine El Karoui, Hau-tieng Wu category:math.ST math.SP stat.ME stat.ML stat.TH 60F99, 53A99 published:2014-05-23 summary:Recently, several data analytic techniques based on connection graphlaplacian (CGL) ideas have appeared in the literature. At this point, theproperties of these methods are starting to be understood in the setting wherethe data is observed without noise. We study the impact of additive noise onthese methods, and show that they are remarkably robust. As a by-product of ouranalysis, we propose modifications of the standard algorithms that increasetheir robustness to noise. We illustrate our results in numerical simulations.
arxiv-6300-209 | Convex Banding of the Covariance Matrix | http://arxiv.org/pdf/1405.6210v1.pdf | author:Jacob Bien, Florentina Bunea, Luo Xiao category:math.ST stat.CO stat.ME stat.ML stat.TH published:2014-05-23 summary:We introduce a new sparse estimator of the covariance matrix forhigh-dimensional models in which the variables have a known ordering. Ourestimator, which is the solution to a convex optimization problem, isequivalently expressed as an estimator which tapers the sample covariancematrix by a Toeplitz, sparsely-banded, data-adaptive matrix. As a result ofthis adaptivity, the convex banding estimator enjoys theoretical optimalityproperties not attained by previous banding or tapered estimators. Inparticular, our convex banding estimator is minimax rate adaptive in Frobeniusand operator norms, up to log factors, over commonly-studied classes ofcovariance matrices, and over more general classes. Furthermore, it correctlyrecovers the bandwidth when the true covariance is exactly banded. Our convexformulation admits a simple and efficient algorithm. Empirical studiesdemonstrate its practical effectiveness and illustrate that our exactly-bandedestimator works well even when the true covariance matrix is only close to abanded matrix, confirming our theoretical results. Our method comparesfavorably with all existing methods, in terms of accuracy and speed. Weillustrate the practical merits of the convex banding estimator by showing thatit can be used to improve the performance of discriminant analysis forclassifying sound recordings.
arxiv-6300-210 | Evaluating the fully automatic multi-language translation of the Swiss avalanche bulletin | http://arxiv.org/pdf/1405.6103v1.pdf | author:Kurt Winkler, Tobias Kuhn, Martin Volk category:cs.CL published:2014-05-23 summary:The Swiss avalanche bulletin is produced twice a day in four languages. Dueto the lack of time available for manual translation, a fully automatedtranslation system is employed, based on a catalogue of predefined phrases andpredetermined rules of how these phrases can be combined to produce sentences.The system is able to automatically translate such sentences from German intothe target languages French, Italian and English without subsequentproofreading or correction. Our catalogue of phrases is limited to a smallsublanguage. The reduction of daily translation costs is expected to offset theinitial development costs within a few years. After being operational for twowinter seasons, we assess here the quality of the produced texts based on anevaluation where participants rate real danger descriptions from both origins,the catalogue of phrases versus the manually written and translated texts. Witha mean recognition rate of 55%, users can hardly distinguish between the twotypes of texts, and give similar ratings with respect to their languagequality. Overall, the output from the catalogue system can be consideredvirtually equivalent to a text written by avalanche forecasters and thenmanually translated by professional translators. Furthermore, forecastersdeclared that all relevant situations were captured by the system withsufficient accuracy and within the limited time available.
arxiv-6300-211 | Online Linear Optimization via Smoothing | http://arxiv.org/pdf/1405.6076v1.pdf | author:Jacob Abernethy, Chansoo Lee, Abhinav Sinha, Ambuj Tewari category:cs.LG published:2014-05-23 summary:We present a new optimization-theoretic approach to analyzingFollow-the-Leader style algorithms, particularly in the setting whereperturbations are used as a tool for regularization. We show that adding astrongly convex penalty function to the decision rule and adding stochasticperturbations to data correspond to deterministic and stochastic smoothingoperations, respectively. We establish an equivalence between "Follow theRegularized Leader" and "Follow the Perturbed Leader" up to the smoothnessproperties. This intuition leads to a new generic analysis framework thatrecovers and improves the previous known regret bounds of the class ofalgorithms commonly known as Follow the Perturbed Leader.
arxiv-6300-212 | Building of Networks of Natural Hierarchies of Terms Based on Analysis of Texts Corpora | http://arxiv.org/pdf/1405.6068v1.pdf | author:Dmitry Lande category:cs.CL published:2014-05-23 summary:The technique of building of networks of hierarchies of terms based on theanalysis of chosen text corpora is offered. The technique is based on themethodology of horizontal visibility graphs. Constructed and investigatedlanguage network, formed on the basis of electronic preprints arXiv on topicsof information retrieval.
arxiv-6300-213 | Robust subspace clustering | http://arxiv.org/pdf/1301.2603v3.pdf | author:Mahdi Soltanolkotabi, Ehsan Elhamifar, Emmanuel J. Cand√®s category:cs.LG cs.IT math.IT math.OC math.ST stat.ML stat.TH published:2013-01-11 summary:Subspace clustering refers to the task of finding a multi-subspacerepresentation that best fits a collection of points taken from ahigh-dimensional space. This paper introduces an algorithm inspired by sparsesubspace clustering (SSC) [In IEEE Conference on Computer Vision and PatternRecognition, CVPR (2009) 2790-2797] to cluster noisy data, and develops somenovel theory demonstrating its correctness. In particular, the theory usesideas from geometric functional analysis to show that the algorithm canaccurately recover the underlying subspaces under minimal requirements on theirorientation, and on the number of samples per subspace. Synthetic as well asreal data experiments complement our theoretical study, illustrating ourapproach and demonstrating its effectiveness.
arxiv-6300-214 | Large Margin Distribution Machine | http://arxiv.org/pdf/1311.0989v2.pdf | author:Teng Zhang, Zhi-Hua Zhou category:cs.LG published:2013-11-05 summary:Support vector machine (SVM) has been one of the most popular learningalgorithms, with the central idea of maximizing the minimum margin, i.e., thesmallest distance from the instances to the classification boundary. Recenttheoretical results, however, disclosed that maximizing the minimum margin doesnot necessarily lead to better generalization performances, and instead, themargin distribution has been proven to be more crucial. In this paper, wepropose the Large margin Distribution Machine (LDM), which tries to achieve abetter generalization performance by optimizing the margin distribution. Wecharacterize the margin distribution by the first- and second-order statistics,i.e., the margin mean and variance. The LDM is a general learning approachwhich can be used in any place where SVM can be applied, and its superiority isverified both theoretically and empirically in this paper.
arxiv-6300-215 | Gemini: Graph estimation with matrix variate normal instances | http://arxiv.org/pdf/1209.5075v2.pdf | author:Shuheng Zhou category:stat.ML math.ST stat.TH published:2012-09-23 summary:Undirected graphs can be used to describe matrix variate distributions. Inthis paper, we develop new methods for estimating the graphical structures andunderlying parameters, namely, the row and column covariance and inversecovariance matrices from the matrix variate data. Under sparsity conditions, weshow that one is able to recover the graphs and covariance matrices with asingle random matrix from the matrix variate normal distribution. Our methodextends, with suitable adaptation, to the general setting where replicates areavailable. We establish consistency and obtain the rates of convergence in theoperator and the Frobenius norm. We show that having replicates will allow oneto estimate more complicated graphical structures and achieve faster rates ofconvergence. We provide simulation evidence showing that we can recovergraphical structures as well as estimating the precision matrices, as predictedby theory.
arxiv-6300-216 | On the Optimal Solution of Weighted Nuclear Norm Minimization | http://arxiv.org/pdf/1405.6012v1.pdf | author:Qi Xie, Deyu Meng, Shuhang Gu, Lei Zhang, Wangmeng Zuo, Xiangchu Feng, Zongben Xu category:cs.CV cs.LG stat.ML published:2014-05-23 summary:In recent years, the nuclear norm minimization (NNM) problem has beenattracting much attention in computer vision and machine learning. The NNMproblem is capitalized on its convexity and it can be solved efficiently. Thestandard nuclear norm regularizes all singular values equally, which is howevernot flexible enough to fit real scenarios. Weighted nuclear norm minimization(WNNM) is a natural extension and generalization of NNM. By assigning properlydifferent weights to different singular values, WNNM can lead tostate-of-the-art results in applications such as image denoising. Nevertheless,so far the global optimal solution of WNNM problem is not completely solved yetdue to its non-convexity in general cases. In this article, we study thetheoretical properties of WNNM and prove that WNNM can be equivalentlytransformed into a quadratic programming problem with linear constraints. Thisimplies that WNNM is equivalent to a convex problem and its global optimum canbe readily achieved by off-the-shelf convex optimization solvers. We furthershow that when the weights are non-descending, the globally optimal solution ofWNNM can be obtained in closed-form.
arxiv-6300-217 | LASS: a simple assignment model with Laplacian smoothing | http://arxiv.org/pdf/1405.5960v1.pdf | author:Miguel √Å. Carreira-Perpi√±√°n, Weiran Wang category:cs.LG math.OC stat.ML published:2014-05-23 summary:We consider the problem of learning soft assignments of $N$ items to $K$categories given two sources of information: an item-category similaritymatrix, which encourages items to be assigned to categories they are similar to(and to not be assigned to categories they are dissimilar to), and an item-itemsimilarity matrix, which encourages similar items to have similar assignments.We propose a simple quadratic programming model that captures this intuition.We give necessary conditions for its solution to be unique, define anout-of-sample mapping, and derive a simple, effective training algorithm basedon the alternating direction method of multipliers. The model predictsreasonable assignments from even a few similarity values, and can be seen as ageneralization of semisupervised learning. It is particularly useful when itemsnaturally belong to multiple categories, as for example when annotatingdocuments with keywords or pictures with tags, with partially tagged items, orwhen the categories have complex interrelations (e.g. hierarchical) that areunknown.
arxiv-6300-218 | Distributed Representations of Sentences and Documents | http://arxiv.org/pdf/1405.4053v2.pdf | author:Quoc V. Le, Tomas Mikolov category:cs.CL cs.AI cs.LG published:2014-05-16 summary:Many machine learning algorithms require the input to be represented as afixed-length feature vector. When it comes to texts, one of the most commonfixed-length features is bag-of-words. Despite their popularity, bag-of-wordsfeatures have two major weaknesses: they lose the ordering of the words andthey also ignore semantics of the words. For example, "powerful," "strong" and"Paris" are equally distant. In this paper, we propose Paragraph Vector, anunsupervised algorithm that learns fixed-length feature representations fromvariable-length pieces of texts, such as sentences, paragraphs, and documents.Our algorithm represents each document by a dense vector which is trained topredict words in the document. Its construction gives our algorithm thepotential to overcome the weaknesses of bag-of-words models. Empirical resultsshow that Paragraph Vectors outperform bag-of-words models as well as othertechniques for text representations. Finally, we achieve new state-of-the-artresults on several text classification and sentiment analysis tasks.
arxiv-6300-219 | Computerization of African languages-French dictionaries | http://arxiv.org/pdf/1405.5893v1.pdf | author:Chantal Enguehard, Mathieu Mangeot category:cs.CL published:2014-05-22 summary:This paper relates work done during the DiLAF project. It consists inconverting 5 bilingual African language-French dictionaries originally in Wordformat into XML following the LMF model. The languages processed are Bambara,Hausa, Kanuri, Tamajaq and Songhai-zarma, still considered as under-resourcedlanguages concerning Natural Language Processing tools. Once converted, thedictionaries are available online on the Jibiki platform for lookup andmodification. The DiLAF project is first presented. A description of eachdictionary follows. Then, the conversion methodology from .doc format to XMLfiles is presented. A specific point on the usage of Unicode follows. Then,each step of the conversion into XML and LMF is detailed. The last partpresents the Jibiki lexical resources management platform used for the project.
arxiv-6300-220 | Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS) | http://arxiv.org/pdf/1405.5869v1.pdf | author:Anshumali Shrivastava, Ping Li category:stat.ML cs.DS cs.IR cs.LG published:2014-05-22 summary:We present the first provably sublinear time algorithm for approximate\emph{Maximum Inner Product Search} (MIPS). Our proposal is also the firsthashing algorithm for searching with (un-normalized) inner product as theunderlying similarity measure. Finding hashing schemes for MIPS was consideredhard. We formally show that the existing Locality Sensitive Hashing (LSH)framework is insufficient for solving MIPS, and then we extend the existing LSHframework to allow asymmetric hashing schemes. Our proposal is based on aninteresting mathematical phenomenon in which inner products, after independentasymmetric transformations, can be converted into the problem of approximatenear neighbor search. This key observation makes efficient sublinear hashingscheme for MIPS possible. In the extended asymmetric LSH (ALSH) framework, weprovide an explicit construction of provably fast hashing scheme for MIPS. Theproposed construction and the extended LSH framework could be of independenttheoretical interest. Our proposed algorithm is simple and easy to implement.We evaluate the method, for retrieving inner products, in the collaborativefiltering task of item recommendations on Netflix and Movielens datasets.
arxiv-6300-221 | Node Classification in Uncertain Graphs | http://arxiv.org/pdf/1405.5829v1.pdf | author:Michele Dallachiesa, Charu Aggarwal, Themis Palpanas category:cs.DB cs.LG published:2014-05-22 summary:In many real applications that use and analyze networked data, the links inthe network graph may be erroneous, or derived from probabilistic techniques.In such cases, the node classification problem can be challenging, since theunreliability of the links may affect the final results of the classificationprocess. If the information about link reliability is not used explicitly, theclassification accuracy in the underlying network may be affected adversely. Inthis paper, we focus on situations that require the analysis of the uncertaintythat is present in the graph structure. We study the novel problem of nodeclassification in uncertain graphs, by treating uncertainty as a first-classcitizen. We propose two techniques based on a Bayes model and automaticparameter selection, and show that the incorporation of uncertainty in theclassification process as a first-class citizen is beneficial. Weexperimentally evaluate the proposed approach using different real data sets,and study the behavior of the algorithms under different conditions. Theresults demonstrate the effectiveness and efficiency of our approach.
arxiv-6300-222 | Compressive Mining: Fast and Optimal Data Mining in the Compressed Domain | http://arxiv.org/pdf/1405.5873v1.pdf | author:Michail Vlachos, Nikolaos Freris, Anastasios Kyrillidis category:stat.ML cs.DS cs.IT math.IT published:2014-05-22 summary:Real-world data typically contain repeated and periodic patterns. Thissuggests that they can be effectively represented and compressed using only afew coefficients of an appropriate basis (e.g., Fourier, Wavelets, etc.).However, distance estimation when the data are represented using different setsof coefficients is still a largely unexplored area. This work studies theoptimization problems related to obtaining the \emph{tightest} lower/upperbound on Euclidean distances when each data object is potentially compressedusing a different set of orthonormal coefficients. Our technique leads totighter distance estimates, which translates into more accurate search,learning and mining operations \textit{directly} in the compressed domain. We formulate the problem of estimating lower/upper distance bounds as anoptimization problem. We establish the properties of optimal solutions, andleverage the theoretical analysis to develop a fast algorithm to obtain an\emph{exact} solution to the problem. The suggested solution provides thetightest estimation of the $L_2$-norm or the correlation. We show that typicaldata-analysis operations, such as k-NN search or k-Means clustering, canoperate more accurately using the proposed compression and distancereconstruction technique. We compare it with many other prevalent compressionand reconstruction techniques, including random projections and PCA-basedtechniques. We highlight a surprising result, namely that when the data arehighly sparse in some basis, our technique may even outperform PCA-basedcompression. The contributions of this work are generic as our methodology is applicableto any sequential or high-dimensional data as well as to any orthogonal datatransformation used for the underlying data compression scheme.
arxiv-6300-223 | Mot√†Mot project: conversion of a French-Khmer published dictionary for building a multilingual lexical system | http://arxiv.org/pdf/1405.5674v1.pdf | author:Mathieu Mangeot category:cs.CL published:2014-05-22 summary:Economic issues related to the information processing techniques are veryimportant. The development of such technologies is a major asset for developingcountries like Cambodia and Laos, and emerging ones like Vietnam, Malaysia andThailand. The MotAMot project aims to computerize an under-resourced language:Khmer, spoken mainly in Cambodia. The main goal of the project is thedevelopment of a multilingual lexical system targeted for Khmer. Themacrostructure is a pivot one with each word sense of each language linked to apivot axi. The microstructure comes from a simplification of the explanatoryand combinatory dictionary. The lexical system has been initialized with datacoming mainly from the conversion of the French-Khmer bilingual dictionary ofDenis Richer from Word to XML format. The French part was completed withpronunciation and parts-of-speech coming from the FeM French-english-Malaydictionary. The Khmer headwords noted in IPA in the Richer dictionary wereconverted to Khmer writing with OpenFST, a finite state transducer tool. Theresulting resource is available online for lookup, editing, download and remoteprogramming via a REST API on a Jibiki platform.
arxiv-6300-224 | Machine Translation Model based on Non-parallel Corpus and Semi-supervised Transductive Learning | http://arxiv.org/pdf/1405.5654v1.pdf | author:Lijiang Chen category:cs.CL published:2014-05-22 summary:Although the parallel corpus has an irreplaceable role in machinetranslation, its scale and coverage is still beyond the actual needs.Non-parallel corpus resources on the web have an inestimable potential value inmachine translation and other natural language processing tasks. This articleproposes a semi-supervised transductive learning method for expanding thetraining corpus in statistical machine translation system by extractingparallel sentences from the non-parallel corpus. This method only requires asmall amount of labeled corpus and a large unlabeled corpus to build ahigh-performance classifier, especially for when there is short of labeledcorpus. The experimental results show that by combining the non-parallel corpusalignment and the semi-supervised transductive learning method, we can moreeffectively use their respective strengths to improve the performance ofmachine translation system.
arxiv-6300-225 | ImageSpirit: Verbal Guided Image Parsing | http://arxiv.org/pdf/1310.4389v2.pdf | author:Ming-Ming Cheng, Shuai Zheng, Wen-Yan Lin, Jonathan Warrell, Vibhav Vineet, Paul Sturgess, Nigel Crook, Niloy Mitra, Philip Torr category:cs.GR cs.CV I.3.6; I.4.8 published:2013-10-16 summary:Humans describe images in terms of nouns and adjectives while algorithmsoperate on images represented as sets of pixels. Bridging this gap between howhumans would like to access images versus their typical representation is thegoal of image parsing, which involves assigning object and attribute labels topixel. In this paper we propose treating nouns as object labels and adjectivesas visual attribute labels. This allows us to formulate the image parsingproblem as one of jointly estimating per-pixel object and attribute labels froma set of training images. We propose an efficient (interactive time) solution.Using the extracted labels as handles, our system empowers a user to verballyrefine the results. This enables hands-free parsing of an image into pixel-wiseobject/attribute labels that correspond to human semantics. Verbally selectingobjects of interests enables a novel and natural interaction modality that canpossibly be used to interact with new generation devices (e.g. smart phones,Google Glass, living room devices). We demonstrate our system on a large numberof real-world images with varying complexity. To help understand the tradeoffscompared to traditional mouse based interactions, results are reported for botha large scale quantitative evaluation and a user study.
arxiv-6300-226 | New Perspectives in Sinographic Language Processing Through the Use of Character Structure | http://arxiv.org/pdf/1405.5474v1.pdf | author:Yannis Haralambous category:cs.CL published:2014-05-21 summary:Chinese characters have a complex and hierarchical graphical structurecarrying both semantic and phonetic information. We use this structure toenhance the text model and obtain better results in standard NLP operations.First of all, to tackle the problem of graphical variation we defineallographic classes of characters. Next, the relation of inclusion of asubcharacter in a characters, provides us with a directed graph of allographicclasses. We provide this graph with two weights: semanticity (semantic relationbetween subcharacter and character) and phoneticity (phonetic relation) andcalculate "most semantic subcharacter paths" for each character. Finally,adding the information contained in these paths to unigrams we claim toincrease the efficiency of text mining methods. We evaluate our method on atext classification task on two corpora (Chinese and Japanese) of a total of 18million characters and get an improvement of 3% on an already high baseline of89.6% precision, obtained by a linear SVM classifier. Other possibleapplications and perspectives of the system are discussed.
arxiv-6300-227 | Spectral Networks and Locally Connected Networks on Graphs | http://arxiv.org/pdf/1312.6203v3.pdf | author:Joan Bruna, Wojciech Zaremba, Arthur Szlam, Yann LeCun category:cs.LG cs.CV cs.NE published:2013-12-21 summary:Convolutional Neural Networks are extremely efficient architectures in imageand audio recognition tasks, thanks to their ability to exploit the localtranslational invariance of signal classes over their domain. In this paper weconsider possible generalizations of CNNs to signals defined on more generaldomains without the action of a translation group. In particular, we proposetwo constructions, one based upon a hierarchical clustering of the domain, andanother based on the spectrum of the graph Laplacian. We show throughexperiments that for low-dimensional graphs it is possible to learnconvolutional layers with a number of parameters independent of the input size,resulting in efficient deep architectures.
arxiv-6300-228 | Unconstrained Online Linear Learning in Hilbert Spaces: Minimax Algorithms and Normal Approximations | http://arxiv.org/pdf/1403.0628v2.pdf | author:H. Brendan McMahan, Francesco Orabona category:cs.LG published:2014-03-03 summary:We study algorithms for online linear optimization in Hilbert spaces,focusing on the case where the player is unconstrained. We develop a novelcharacterization of a large class of minimax algorithms, recovering, and evenimproving, several previous results as immediate corollaries. Moreover, usingour tools, we develop an algorithm that provides a regret bound of$\mathcal{O}\Big(U \sqrt{T \log(U \sqrt{T} \log^2 T +1)}\Big)$, where $U$ isthe $L_2$ norm of an arbitrary comparator and both $T$ and $U$ are unknown tothe player. This bound is optimal up to $\sqrt{\log \log T}$ terms. When $T$ isknown, we derive an algorithm with an optimal regret bound (up to constantfactors). For both the known and unknown $T$ case, a Normal approximation tothe conditional value of the game proves to be the key analysis tool.
arxiv-6300-229 | Robust Fuzzy corner detector | http://arxiv.org/pdf/1405.5422v1.pdf | author:Erik Cuevas, Daniel Zaldivar, Marco Perez, Edgar Sanchez, Marte Ramirez category:cs.CV published:2014-05-21 summary:Reliable corner detection is an important task in determining the shape ofdifferent regions within an image. Real-life image data are always imprecisedue to inherent uncertainties that may arise from the imaging process such asdefocusing, illumination changes, noise, etc. Therefore, the localization anddetection of corners has become a difficult task to accomplish under suchimperfect situations. On the other hand, Fuzzy systems are well known for theirefficient handling of impreciseness and incompleteness, which make theminherently suitable for modelling corner properties by means of a rule-basedfuzzy system. The paper presents a corner detection algorithm which employssuch fuzzy reasoning. The robustness of the proposed algorithm is compared towell-known conventional corner detectors and its performance is also testedover a number of benchmark images to illustrate the efficiency of the algorithmunder uncertainty.
arxiv-6300-230 | Fast algorithm for Multiple-Circle detection on images using Learning Automata | http://arxiv.org/pdf/1405.5531v1.pdf | author:Erik Cuevas, Fernando Wario, Valentin Osuna, Daniel Zaldivar, Marco Perez category:cs.CV published:2014-05-21 summary:Hough transform (HT) has been the most common method for circle detectionexhibiting robustness but adversely demanding a considerable computational loadand large storage. Alternative approaches include heuristic methods that employiterative optimization procedures for detecting multiple circles under theinconvenience that only one circle can be marked at each optimization cycledemanding a longer execution time. On the other hand, Learning Automata (LA) isa heuristic method to solve complex multi-modal optimization problems. AlthoughLA converges to just one global minimum, the final probability distributionholds valuable information regarding other local minima which have emergedduring the optimization process. The detection process is considered as amulti-modal optimization problem, allowing the detection of multiple circularshapes through only one optimization procedure. The algorithm uses acombination of three edge points as parameters to determine circles candidates.A reinforcement signal determines if such circle candidates are actuallypresent at the image. Guided by the values of such reinforcement signal, theset of encoded candidate circles are evolved using the LA so that they can fitinto actual circular shapes over the edge-only map of the image. The overallapproach is a fast multiple-circle detector despite facing complicatedconditions.
arxiv-6300-231 | Circle detection on images using Learning Automata | http://arxiv.org/pdf/1405.5406v1.pdf | author:Erik Cuevas, Fernando Wario, Daniel Zaldivar, Marco Perez category:cs.CV published:2014-05-21 summary:Circle detection over digital images has received considerable attention fromthe computer vision community over the last few years devoting a tremendousamount of research seeking for an optimal detector. This article presents analgorithm for the automatic detection of circular shapes from complicated andnoisy images with no consideration of conventional Hough transform principles.The proposed algorithm is based on Learning Automata (LA) which is aprobabilistic optimization method that explores an unknown random environmentby progressively improving the performance via a reinforcement signal(objective function). The approach uses the encoding of three non-collinearpoints as a candidate circle over the edge image. A reinforcement signal(matching function) indicates if such candidate circles are actually present inthe edge map. Guided by the values of such reinforcement signal, theprobability set of the encoded candidate circles is modified through the LAalgorithm so that they can fit to the actual circles on the edge map.Experimental results over several complex synthetic and natural images havevalidated the efficiency of the proposed technique regarding accuracy, speedand robustness.
arxiv-6300-232 | Multi Modal Face Recognition Using Block Based Curvelet Features | http://arxiv.org/pdf/1405.2641v2.pdf | author:Jyothi K, Prabhakar C. J category:cs.CV published:2014-05-12 summary:In this paper, we present multimodal 2D +3D face recognition method usingblock based curvelet features. The 3D surface of face (Depth Map) is computedfrom the stereo face images using stereo vision technique. The statisticalmeasures such as mean, standard deviation, variance and entropy are extractedfrom each block of curvelet subband for both depth and intensity imagesindependently.In order to compute the decision score, the KNN classifier isemployed independently for both intensity and depth map. Further, computeddecision scoresof intensity and depth map are combined at decision level toimprove the face recognition rate. The combination of intensity and depth mapis verified experimentally using benchmark face database. The experimentalresults show that the proposed multimodal method is better than individualmodality.
arxiv-6300-233 | Off-Policy Shaping Ensembles in Reinforcement Learning | http://arxiv.org/pdf/1405.5358v1.pdf | author:Anna Harutyunyan, Tim Brys, Peter Vrancx, Ann Nowe category:cs.AI cs.LG published:2014-05-21 summary:Recent advances of gradient temporal-difference methods allow to learnoff-policy multiple value functions in parallel with- out sacrificingconvergence guarantees or computational efficiency. This opens up newpossibilities for sound ensemble techniques in reinforcement learning. In thiswork we propose learning an ensemble of policies related throughpotential-based shaping rewards. The ensemble induces a combination policy byusing a voting mechanism on its components. Learning happens in real time, andwe empirically show the combination policy to outperform the individualpolicies of the ensemble.
arxiv-6300-234 | Compressive Sampling Using EM Algorithm | http://arxiv.org/pdf/1405.5311v1.pdf | author:Atanu Kumar Ghosh, Arnab Chakraborty category:stat.ME cs.LG stat.ML published:2014-05-21 summary:Conventional approaches of sampling signals follow the celebrated theorem ofNyquist and Shannon. Compressive sampling, introduced by Donoho, Romberg andTao, is a new paradigm that goes against the conventional methods in dataacquisition and provides a way of recovering signals using fewer samples thanthe traditional methods use. Here we suggest an alternative way ofreconstructing the original signals in compressive sampling using EM algorithm.We first propose a naive approach which has certain computational difficultiesand subsequently modify it to a new approach which performs better than theconventional methods of compressive sampling. The comparison of the differentapproaches and the performance of the new approach has been studied usingsimulated data.
arxiv-6300-235 | Fast and Fuzzy Private Set Intersection | http://arxiv.org/pdf/1405.3272v2.pdf | author:Nicholas Kersting category:cs.CR cs.CL published:2014-05-13 summary:Private Set Intersection (PSI) is usually implemented as a sequence ofencryption rounds between pairs of users, whereas the present work implementsPSI in a simpler fashion: each set only needs to be encrypted once, after whicheach pair of users need only one ordinary set comparison. This is typicallyorders of magnitude faster than ordinary PSI at the cost of some ``fuzziness"in the matching, which may nonetheless be tolerable or even desirable. This isdemonstrated in the case where the sets consist of English words processed withWordNet.
arxiv-6300-236 | Dynamic Hierarchical Bayesian Network for Arabic Handwritten Word Recognition | http://arxiv.org/pdf/1405.5248v1.pdf | author:Khaoula jayech, Nesrine Trimech, Mohamed Ali Mahjoub, Najoua Essoukri Ben Amara category:cs.CV published:2014-05-20 summary:This paper presents a new probabilistic graphical model used to model andrecognize words representing the names of Tunisian cities. In fact, this workis based on a dynamic hierarchical Bayesian network. The aim is to find thebest model of Arabic handwriting to reduce the complexity of the recognitionprocess by permitting the partial recognition. Actually, we propose asegmentation of the word based on smoothing the vertical histogram projectionusing different width values to reduce the error of segmentation. Then, weextract the characteristics of each cell using the Zernike and HU moments,which are invariant to rotation, translation and scaling. Our approach istested using the IFN / ENIT database, and the experiment results are verypromising.
arxiv-6300-237 | Robust and Scalable Bayes via a Median of Subset Posterior Measures | http://arxiv.org/pdf/1403.2660v2.pdf | author:Stanislav Minsker, Sanvesh Srivastava, Lizhen Lin, David B. Dunson category:math.ST cs.DC cs.LG stat.TH published:2014-03-11 summary:We propose a novel approach to Bayesian analysis that is provably robust tooutliers in the data and often has computational advantages over standardmethods. Our technique is based on splitting the data into non-overlappingsubgroups, evaluating the posterior distribution given each independentsubgroup, and then combining the resulting measures. The main novelty of ourapproach is the proposed aggregation step, which is based on the evaluation ofa median in the space of probability measures equipped with a suitablecollection of distances that can be quickly and efficiently evaluated inpractice. We present both theoretical and numerical evidence illustrating theimprovements achieved by our method.
arxiv-6300-238 | Sequential Advantage Selection for Optimal Treatment Regimes | http://arxiv.org/pdf/1405.5239v1.pdf | author:Ailin Fan, Wenbin Lu, Rui Song category:stat.ME stat.ML published:2014-05-20 summary:Variable selection for optimal treatment regime in a clinical trial or anobservational study is getting more attention. Most existing variable selectiontechniques focused on selecting variables that are important for prediction,therefore some variables that are poor in prediction but are critical fordecision-making may be ignored. A qualitative interaction of a variable withtreatment arises when treatment effect changes direction as the value of thisvariable varies. The qualitative interaction indicates the importance of thisvariable for decision-making. Gunter et al. (2011) proposed S-score whichcharacterizes the magnitude of qualitative interaction of each variable withtreatment individually. In this article, we developed a sequential advantageselection method based on the modified S-score. Our method selectsqualitatively interacted variables sequentially, and hence excludes marginallyimportant but jointly unimportant variables {or vice versa}. The optimaltreatment regime based on variables selected via joint model is morecomprehensive and reliable. With the proposed stopping criteria, our method canhandle a large amount of covariates even if sample size is small. Simulationresults show our method performs well in practical settings. We further appliedour method to data from a clinical trial for depression.
arxiv-6300-239 | Scalable Recommendation with Poisson Factorization | http://arxiv.org/pdf/1311.1704v3.pdf | author:Prem Gopalan, Jake M. Hofman, David M. Blei category:cs.IR cs.AI cs.LG stat.ML published:2013-11-07 summary:We develop a Bayesian Poisson matrix factorization model for formingrecommendations from sparse user behavior data. These data are large user/itemmatrices where each user has provided feedback on only a small subset of items,either explicitly (e.g., through star ratings) or implicitly (e.g., throughviews or purchases). In contrast to traditional matrix factorizationapproaches, Poisson factorization implicitly models each user's limitedattention to consume items. Moreover, because of the mathematical form of thePoisson likelihood, the model needs only to explicitly consider the observedentries in the matrix, leading to both scalable computation and good predictiveperformance. We develop a variational inference algorithm for approximateposterior inference that scales up to massive data sets. This is an efficientalgorithm that iterates over the observed entries and adjusts an approximateposterior over the user/item representations. We apply our method to largereal-world user data containing users rating movies, users listening to songs,and users reading scientific papers. In all these settings, Bayesian Poissonfactorization outperforms state-of-the-art matrix factorization methods.
arxiv-6300-240 | Constructing Time Series Shape Association Measures: Minkowski Distance and Data Standardization | http://arxiv.org/pdf/1311.1958v3.pdf | author:Ildar Batyrshin category:cs.LG published:2013-11-07 summary:It is surprising that last two decades many works in time series data miningand clustering were concerned with measures of similarity of time series butnot with measures of association that can be used for measuring possible directand inverse relationships between time series. Inverse relationships can existbetween dynamics of prices and sell volumes, between growth patterns ofcompetitive companies, between well production data in oilfields, between windvelocity and air pollution concentration etc. The paper develops a theoreticalbasis for analysis and construction of time series shape association measures.Starting from the axioms of time series shape association measures it studiesthe methods of construction of measures satisfying these axioms. Severalgeneral methods of construction of such measures suitable for measuring timeseries shape similarity and shape association are proposed. Time series shapeassociation measures based on Minkowski distance and data standardizationmethods are considered. The cosine similarity and the Pearsons correlationcoefficient are obtained as particular cases of the proposed general methodsthat can be used also for construction of new association measures in dataanalysis.
arxiv-6300-241 | Multi-ellipses detection on images inspired by collective animal behavior | http://arxiv.org/pdf/1405.5164v1.pdf | author:Erik Cuevas, Maurici Gonzalez, Daniel Zaldivar, Marco Perez category:cs.CV published:2014-05-20 summary:This paper presents a novel and effective technique for extracting multipleellipses from an image. The approach employs an evolutionary algorithm to mimicthe way animals behave collectively assuming the overall detection process as amulti-modal optimization problem. In the algorithm, searcher agents emulate agroup of animals that interact to each other using simple biological ruleswhich are modeled as evolutionary operators. In turn, such operators areapplied to each agent considering that the complete group has a memory to storeoptimal solutions (ellipses) seen so-far by applying a competition principle.The detector uses a combination of five edge points as parameters to determineellipse candidates (possible solutions) while a matching function determines ifsuch ellipse candidates are actually present in the image. Guided by the valuesof such matching functions, the set of encoded candidate ellipses are evolvedthrough the evolutionary algorithm so that the best candidates can be fittedinto the actual ellipses within the image. Just after the optimization processends, an analysis over the embedded memory is executed in order to find thebest obtained solution (the best ellipse) and significant local minima(remaining ellipses). Experimental results over several complex synthetic andnatural images have validated the efficiency of the proposed techniqueregarding accuracy, speed and robustness.
arxiv-6300-242 | Gaussian Approximation of Collective Graphical Models | http://arxiv.org/pdf/1405.5156v1.pdf | author:Li-Ping Liu, Daniel Sheldon, Thomas G. Dietterich category:cs.LG cs.AI stat.ML published:2014-05-20 summary:The Collective Graphical Model (CGM) models a population of independent andidentically distributed individuals when only collective statistics (i.e.,counts of individuals) are observed. Exact inference in CGMs is intractable,and previous work has explored Markov Chain Monte Carlo (MCMC) and MAPapproximations for learning and inference. This paper studies Gaussianapproximations to the CGM. As the population grows large, we show that the CGMdistribution converges to a multivariate Gaussian distribution (GCGM) thatmaintains the conditional independence properties of the original CGM. If theobservations are exact marginals of the CGM or marginals that are corrupted byGaussian noise, inference in the GCGM approximation can be computed efficientlyin closed form. If the observations follow a different noise model (e.g.,Poisson), then expectation propagation provides efficient and accurateapproximate inference. The accuracy and speed of GCGM inference is compared tothe MCMC and MAP methods on a simulated bird migration problem. The GCGMmatches or exceeds the accuracy of the MAP method while being significantlyfaster.
arxiv-6300-243 | Predicting Online Video Engagement Using Clickstreams | http://arxiv.org/pdf/1405.5147v1.pdf | author:Everaldo Aguiar, Saurabh Nagrecha, Nitesh V. Chawla category:cs.LG cs.IR I.5.2 published:2014-05-20 summary:In the nascent days of e-content delivery, having a superior product wasenough to give companies an edge against the competition. With today's fiercelycompetitive market, one needs to be multiple steps ahead, especially when itcomes to understanding consumers. Focusing on a large set of web portals ownedand managed by a private communications company, we propose methods by whichthese sites' clickstream data can be used to provide a deep understanding oftheir visitors, as well as their interests and preferences. We further expandthe use of this data to show that it can be effectively used to predict userengagement to video streams.
arxiv-6300-244 | Deep AutoRegressive Networks | http://arxiv.org/pdf/1310.8499v2.pdf | author:Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, Daan Wierstra category:cs.LG stat.ML published:2013-10-31 summary:We introduce a deep, generative autoencoder capable of learning hierarchiesof distributed representations from data. Successive deep stochastic hiddenlayers are equipped with autoregressive connections, which enable the model tobe sampled from quickly and exactly via ancestral sampling. We derive anefficient approximate parameter estimation method based on the minimumdescription length (MDL) principle, which can be seen as maximising avariational lower bound on the log-likelihood, with a feedforward neuralnetwork implementing approximate inference. We demonstrate state-of-the-artgenerative performance on a number of classic data sets: several UCI data sets,MNIST and Atari 2600 games.
arxiv-6300-245 | Unimodal Bandits: Regret Lower Bounds and Optimal Algorithms | http://arxiv.org/pdf/1405.5096v1.pdf | author:Richard Combes, Alexandre Proutiere category:cs.LG stat.ML published:2014-05-20 summary:We consider stochastic multi-armed bandits where the expected reward is aunimodal function over partially ordered arms. This important class of problemshas been recently investigated in (Cope 2009, Yu 2011). The set of arms iseither discrete, in which case arms correspond to the vertices of a finitegraph whose structure represents similarity in rewards, or continuous, in whichcase arms belong to a bounded interval. For discrete unimodal bandits, wederive asymptotic lower bounds for the regret achieved under any algorithm, andpropose OSUB, an algorithm whose regret matches this lower bound. Our algorithmoptimally exploits the unimodal structure of the problem, and surprisingly, itsasymptotic regret does not depend on the number of arms. We also provide aregret upper bound for OSUB in non-stationary environments where the expectedrewards smoothly evolve over time. The analytical results are supported bynumerical experiments showing that OSUB performs significantly better than thestate-of-the-art algorithms. For continuous sets of arms, we provide a briefdiscussion. We show that combining an appropriate discretization of the set ofarms with the UCB algorithm yields an order-optimal regret, and in practice,outperforms recently proposed algorithms designed to exploit the unimodalstructure.
arxiv-6300-246 | A Genetic Algorithm for solving Quadratic Assignment Problem(QAP) | http://arxiv.org/pdf/1405.5050v1.pdf | author:Hosein Azarbonyad, Reza Babazadeh category:cs.NE published:2014-05-20 summary:The Quadratic Assignment Problem (QAP) is one of the models used for themulti-row layout problem with facilities of equal area. There are a set of nfacilities and a set of n locations. For each pair of locations, a distance isspecified and for each pair of facilities a weight or flow is specified (e.g.,the amount of supplies transported between the two facilities). The problem isto assign all facilities to different locations with the aim of minimizing thesum of the distances multiplied by the corresponding flows. The QAP is amongthe most difficult NP-hard combinatorial optimization problems. Because ofthis, this paper presents an efficient Genetic algorithm (GA) to solve thisproblem in reasonable time. For validation the proposed GA some examples areselected from QAP library. The obtained results in reasonable time show theefficiency of proposed GA.
arxiv-6300-247 | A Parallel Way to Select the Parameters of SVM Based on the Ant Optimization Algorithm | http://arxiv.org/pdf/1405.4589v2.pdf | author:Chao Zhang, Hong-cen Mei, Hao Yang category:cs.NE cs.LG published:2014-05-19 summary:A large number of experimental data shows that Support Vector Machine (SVM)algorithm has obvious advantages in text classification, handwritingrecognition, image classification, bioinformatics, and some other fields. Tosome degree, the optimization of SVM depends on its kernel function and Slackvariable, the determinant of which is its parameters $\delta$ and c in theclassification function. That is to say,to optimize the SVM algorithm, theoptimization of the two parameters play a huge role. Ant Colony Optimization(ACO) is optimization algorithm which simulate ants to find the optimal path.Inthe available literature, we mix the ACO algorithm and Parallel algorithmtogether to find a well parameters.
arxiv-6300-248 | Learning to Exploit Different Translation Resources for Cross Language Information Retrieval | http://arxiv.org/pdf/1405.5447v1.pdf | author:Hosein Azarbonyad, Azadeh Shakery, Heshaam Faili category:cs.IR cs.CL published:2014-05-20 summary:One of the important factors that affects the performance of Cross LanguageInformation Retrieval(CLIR)is the quality of translations being employed inCLIR. In order to improve the quality of translations, it is important toexploit available resources efficiently. Employing different translationresources with different characteristics has many challenges. In this paper, wepropose a method for exploiting available translation resources simultaneously.This method employs Learning to Rank(LTR) for exploiting different translationresources. To apply LTR methods for query translation, we define differenttranslation relation based features in addition to context based features. Weuse the contextual information contained in translation resources forextracting context based features.The proposed method uses LTR to construct atranslation ranking model based on defined features. The constructed model isused for ranking translation candidates of query words. To evaluate theproposed method we do English-Persian CLIR, in which we employ the translationranking model to find translations of English queries and employ thetranslations to retrieve Persian documents. Experimental results show that ourapproach significantly outperforms single resource based CLIR methods.
arxiv-6300-249 | Auto-adaptative Laplacian Pyramids for High-dimensional Data Analysis | http://arxiv.org/pdf/1311.6594v2.pdf | author:√Ångela Fern√°ndez, Neta Rabin, Dalia Fishelov, Jos√© R. Dorronsoro category:cs.AI cs.LG stat.ML published:2013-11-26 summary:Non-linear dimensionality reduction techniques such as manifold learningalgorithms have become a common way for processing and analyzinghigh-dimensional patterns that often have attached a target that corresponds tothe value of an unknown function. Their application to new points consists intwo steps: first, embedding the new data point into the low dimensional spaceand then, estimating the function value on the test point from its neighbors inthe embedded space. However, finding the low dimension representation of a test point, while easyfor simple but often not powerful enough procedures such as PCA, can be muchmore complicated for methods that rely on some kind of eigenanalysis, such asSpectral Clustering (SC) or Diffusion Maps (DM). Similarly, when a targetfunction is to be evaluated, averaging methods like nearest neighbors may giveunstable results if the function is noisy. Thus, the smoothing of the targetfunction with respect to the intrinsic, low-dimensional representation thatdescribes the geometric structure of the examined data is a challenging task. In this paper we propose Auto-adaptive Laplacian Pyramids (ALP), an extensionof the standard Laplacian Pyramids model that incorporates a modified LOOCVprocedure that avoids the large cost of the standard one and offers thefollowing advantages: (i) it selects automatically the optimal functionresolution (stopping time) adapted to the data and its noise, (ii) it is easyto apply as it does not require parameterization, (iii) it does not overfit thetraining set and (iv) it adds no extra cost compared to other classicalinterpolation methods. We illustrate numerically ALP's behavior on a syntheticproblem and apply it to the computation of the DM projection of new patternsand to the extension to them of target function values on a radiationforecasting problem over very high dimensional patterns.
arxiv-6300-250 | Secure Friend Discovery via Privacy-Preserving and Decentralized Community Detection | http://arxiv.org/pdf/1405.4951v1.pdf | author:Pili Hu, Sherman S. M. Chow, Wing Cheong Lau category:cs.CR cs.SI stat.ML published:2014-05-20 summary:The problem of secure friend discovery on a social network has long beenproposed and studied. The requirement is that a pair of nodes can makebefriending decisions with minimum information exposed to the other party. Inthis paper, we propose to use community detection to tackle the problem ofsecure friend discovery. We formulate the first privacy-preserving anddecentralized community detection problem as a multi-objective optimization. Wedesign the first protocol to solve this problem, which transforms communitydetection to a series of Private Set Intersection (PSI) instances usingTruncated Random Walk (TRW). Preliminary theoretical results show that ourprotocol can uncover communities with overwhelming probability and preserveprivacy. We also discuss future works, potential extensions and variations.
arxiv-6300-251 | Minimal Dirichlet energy partitions for graphs | http://arxiv.org/pdf/1308.4915v2.pdf | author:Braxton Osting, Chris D. White, Edouard Oudet category:math.OC cs.LG stat.ML published:2013-08-22 summary:Motivated by a geometric problem, we introduce a new non-convex graphpartitioning objective where the optimality criterion is given by the sum ofthe Dirichlet eigenvalues of the partition components. A relaxed formulation isidentified and a novel rearrangement algorithm is proposed, which we show isstrictly decreasing and converges in a finite number of iterations to a localminimum of the relaxed objective function. Our method is applied to severalclustering problems on graphs constructed from synthetic data, MNISThandwritten digits, and manifold discretizations. The model has asemi-supervised extension and provides a natural representative for theclusters as well.
arxiv-6300-252 | Contextual Bandits with Similarity Information | http://arxiv.org/pdf/0907.3986v5.pdf | author:Aleksandrs Slivkins category:cs.DS cs.LG F.2.2; F.1.2 published:2009-07-23 summary:In a multi-armed bandit (MAB) problem, an online algorithm makes a sequenceof choices. In each round it chooses from a time-invariant set of alternativesand receives the payoff associated with this alternative. While the case ofsmall strategy sets is by now well-understood, a lot of recent work has focusedon MAB problems with exponentially or infinitely large strategy sets, where oneneeds to assume extra structure in order to make the problem tractable. Inparticular, recent literature considered information on similarity betweenarms. We consider similarity information in the setting of "contextual bandits", anatural extension of the basic MAB problem where before each round an algorithmis given the "context" -- a hint about the payoffs in this round. Contextualbandits are directly motivated by placing advertisements on webpages, one ofthe crucial problems in sponsored search. A particularly simple way torepresent similarity information in the contextual bandit setting is via a"similarity distance" between the context-arm pairs which gives an upper boundon the difference between the respective expected payoffs. Prior work on contextual bandits with similarity uses "uniform" partitions ofthe similarity space, which is potentially wasteful. We design more efficientalgorithms that are based on adaptive partitions adjusted to "popular" contextand "high-payoff" arms.
arxiv-6300-253 | Bayesian estimation of possible causal direction in the presence of latent confounders using a linear non-Gaussian acyclic structural equation model with individual-specific effects | http://arxiv.org/pdf/1310.6778v2.pdf | author:Shohei Shimizu, Kenneth Bollen category:stat.ML published:2013-10-24 summary:We consider learning the possible causal direction of two observed variablesin the presence of latent confounding variables. Several existing methods havebeen shown to consistently estimate causal direction assuming linear or sometype of nonlinear relationship and no latent confounders. However, theestimation results could be distorted if either assumption is actuallyviolated. In this paper, we first propose a new linear non-Gaussian acyclicstructural equation model with individual-specific effects that allows latentconfounders to be considered. We then propose an empirical Bayesian approachfor estimating possible causal direction using the new model. We demonstratethe effectiveness of our method using artificial and real-world data.
arxiv-6300-254 | Fighting Authorship Linkability with Crowdsourcing | http://arxiv.org/pdf/1405.4918v1.pdf | author:Mishari Almishari, Ekin Oguz, Gene Tsudik category:cs.DL cs.CL published:2014-05-19 summary:Massive amounts of contributed content -- including traditional literature,blogs, music, videos, reviews and tweets -- are available on the Internettoday, with authors numbering in many millions. Textual information, such asproduct or service reviews, is an important and increasingly popular type ofcontent that is being used as a foundation of many trendy community-basedreviewing sites, such as TripAdvisor and Yelp. Some recent results have shownthat, due partly to their specialized/topical nature, sets of reviews authoredby the same person are readily linkable based on simple stylometric features.In practice, this means that individuals who author more than a few reviewsunder different accounts (whether within one site or across multiple sites) canbe linked, which represents a significant loss of privacy. In this paper, we start by showing that the problem is actually worse thanpreviously believed. We then explore ways to mitigate authorship linkability incommunity-based reviewing. We first attempt to harness the global power ofcrowdsourcing by engaging random strangers into the process of re-writingreviews. As our empirical results (obtained from Amazon Mechanical Turk)clearly demonstrate, crowdsourcing yields impressively sensible reviews thatreflect sufficiently different stylometric characteristics such that priorstylometric linkability techniques become largely ineffective. We also considerusing machine translation to automatically re-write reviews. Contrary to whatwas previously believed, our results show that translation decreases authorshiplinkability as the number of intermediate languages grows. Finally, we explorethe combination of crowdsourcing and machine translation and report on theresults.
arxiv-6300-255 | Inference in High Dimensions with the Penalized Score Test | http://arxiv.org/pdf/1401.2678v3.pdf | author:Arend Voorman, Ali Shojaie, Daniela Witten category:stat.ME stat.ML published:2014-01-12 summary:In recent years, there has been considerable theoretical developmentregarding variable selection consistency of penalized regression techniques,such as the lasso. However, there has been relatively little work onquantifying the uncertainty in these selection procedures. In this paper, wepropose a new method for inference in high dimensions using a score test basedon penalized regression. In this test, we perform penalized regression of anoutcome on all but a single feature, and test for correlation of the residualswith the held-out feature. This procedure is applied to each feature in turn.Interestingly, when an $\ell_1$ penalty is used, the sparsity pattern of thelasso corresponds exactly to a decision based on the proposed test. Further,when an $\ell_2$ penalty is used, the test corresponds precisely to a scoretest in a mixed effects model, in which the effects of all but one feature areassumed to be random. We formulate the hypothesis being tested as a compromisebetween the null hypotheses tested in simple linear regression on each featureand in multiple linear regression on all features, and develop referencedistributions for some well-known penalties. We also examine the behavior ofthe test on real and simulated data.
arxiv-6300-256 | Screening Tests for Lasso Problems | http://arxiv.org/pdf/1405.4897v1.pdf | author:Zhen James Xiang, Yun Wang, Peter J. Ramadge category:cs.LG stat.ML published:2014-05-19 summary:This paper is a survey of dictionary screening for the lasso problem. Thelasso problem seeks a sparse linear combination of the columns of a dictionaryto best match a given target vector. This sparse representation has provenuseful in a variety of subsequent processing and decision tasks. For a giventarget vector, dictionary screening quickly identifies a subset of dictionarycolumns that will receive zero weight in a solution of the corresponding lassoproblem. These columns can be removed from the dictionary, prior to solving thelasso problem, without impacting the optimality of the solution obtained. Thishas two potential advantages: it reduces the size of the dictionary, allowingthe lasso problem to be solved with less resources, and it may speed upobtaining a solution. Using a geometrically intuitive framework, we providebasic insights for understanding useful lasso screening tests and theirlimitations. We also provide illustrative numerical studies on severaldatasets.
arxiv-6300-257 | Online Stochastic Optimization under Correlated Bandit Feedback | http://arxiv.org/pdf/1402.0562v3.pdf | author:Mohammad Gheshlaghi Azar, Alessandro Lazaric, Emma Brunskill category:stat.ML cs.LG cs.SY published:2014-02-04 summary:In this paper we consider the problem of online stochastic optimization of alocally smooth function under bandit feedback. We introduce the high-confidencetree (HCT) algorithm, a novel any-time $\mathcal{X}$-armed bandit algorithm,and derive regret bounds matching the performance of existing state-of-the-artin terms of dependency on number of steps and smoothness factor. The mainadvantage of HCT is that it handles the challenging case of correlated rewards,whereas existing methods require that the reward-generating process of each armis an identically and independent distributed (iid) random process. HCT alsoimproves on the state-of-the-art in terms of its memory requirement as well asrequiring a weaker smoothness assumption on the mean-reward function in compareto the previous anytime algorithms. Finally, we discuss how HCT can be appliedto the problem of policy search in reinforcement learning and we reportpreliminary empirical results.
arxiv-6300-258 | Scalable Semidefinite Relaxation for Maximum A Posterior Estimation | http://arxiv.org/pdf/1405.4807v1.pdf | author:Qixing Huang, Yuxin Chen, Leonidas Guibas category:cs.LG cs.CV cs.IT math.IT math.OC stat.ML published:2014-05-19 summary:Maximum a posteriori (MAP) inference over discrete Markov random fields is afundamental task spanning a wide spectrum of real-world applications, which isknown to be NP-hard for general graphs. In this paper, we propose a novelsemidefinite relaxation formulation (referred to as SDR) to estimate the MAPassignment. Algorithmically, we develop an accelerated variant of thealternating direction method of multipliers (referred to as SDPAD-LR) that caneffectively exploit the special structure of the new relaxation. Encouragingly,the proposed procedure allows solving SDR for large-scale problems, e.g.,problems on a grid graph comprising hundreds of thousands of variables withmultiple states per node. Compared with prior SDP solvers, SDPAD-LR is capableof attaining comparable accuracy while exhibiting remarkably improvedscalability, in contrast to the commonly held belief that semidefiniterelaxation can only been applied on small-scale MRF problems. We have evaluatedthe performance of SDR on various benchmark datasets including OPENGM2 and PICin terms of both the quality of the solutions and computation time.Experimental results demonstrate that for a broad class of problems, SDPAD-LRoutperforms state-of-the-art algorithms in producing better MAP assignment inan efficient manner.
arxiv-6300-259 | Lipschitz Bandits: Regret Lower Bounds and Optimal Algorithms | http://arxiv.org/pdf/1405.4758v1.pdf | author:Stefan Magureanu, Richard Combes, Alexandre Proutiere category:cs.LG published:2014-05-19 summary:We consider stochastic multi-armed bandit problems where the expected rewardis a Lipschitz function of the arm, and where the set of arms is eitherdiscrete or continuous. For discrete Lipschitz bandits, we derive asymptoticproblem specific lower bounds for the regret satisfied by any algorithm, andpropose OSLB and CKL-UCB, two algorithms that efficiently exploit the Lipschitzstructure of the problem. In fact, we prove that OSLB is asymptoticallyoptimal, as its asymptotic regret matches the lower bound. The regret analysisof our algorithms relies on a new concentration inequality for weighted sums ofKL divergences between the empirical distributions of rewards and their truedistributions. For continuous Lipschitz bandits, we propose to first discretizethe action space, and then apply OSLB or CKL-UCB, algorithms that provablyexploit the structure efficiently. This approach is shown, through numericalexperiments, to significantly outperform existing algorithms that directly dealwith the continuous set of arms. Finally the results and algorithms areextended to contextual bandits with similarities.
arxiv-6300-260 | Faster and Sample Near-Optimal Algorithms for Proper Learning Mixtures of Gaussians | http://arxiv.org/pdf/1312.1054v3.pdf | author:Constantinos Daskalakis, Gautam Kamath category:cs.DS cs.LG math.PR math.ST stat.TH published:2013-12-04 summary:We provide an algorithm for properly learning mixtures of twosingle-dimensional Gaussians without any separability assumptions. Given$\tilde{O}(1/\varepsilon^2)$ samples from an unknown mixture, our algorithmoutputs a mixture that is $\varepsilon$-close in total variation distance, intime $\tilde{O}(1/\varepsilon^5)$. Our sample complexity is optimal up tologarithmic factors, and significantly improves upon both Kalai et al., whosealgorithm has a prohibitive dependence on $1/\varepsilon$, and Feldman et al.,whose algorithm requires bounds on the mixture parameters and dependspseudo-polynomially in these parameters. One of our main contributions is an improved and generalized algorithm forselecting a good candidate distribution from among competing hypotheses.Namely, given a collection of $N$ hypotheses containing at least one candidatethat is $\varepsilon$-close to an unknown distribution, our algorithm outputs acandidate which is $O(\varepsilon)$-close to the distribution. The algorithmrequires ${O}(\log{N}/\varepsilon^2)$ samples from the unknown distribution and${O}(N \log N/\varepsilon^2)$ time, which improves previous such results (suchas the Scheff\'e estimator) from a quadratic dependence of the running time on$N$ to quasilinear. Given the wide use of such results for the purpose ofhypothesis selection, our improved algorithm implies immediate improvements toany such use.
arxiv-6300-261 | Vesselness via Multiple Scale Orientation Scores | http://arxiv.org/pdf/1402.4963v4.pdf | author:Julius Hannink, Remco Duits, Erik Bekkers category:cs.CV published:2014-02-20 summary:The multi-scale Frangi vesselness filter is an established tool in (retinal)vascular imaging. However, it cannot cope with crossings or bifurcations, sinceit only looks for elongated structures. Therefore, we disentangle crossingstructures in the image via (multiple scale) invertible orientation scores. Thedescribed vesselness filter via scale-orientation scores performs considerablybetter at enhancing vessels throughout crossings and bifurcations than theFrangi version. Both methods are evaluated on a public dataset. Performance ismeasured by comparing ground truth data to the segmentation results obtained bybasic thresholding and morphological component analysis of the filtered images.
arxiv-6300-262 | Modelling Data Dispersion Degree in Automatic Robust Estimation for Multivariate Gaussian Mixture Models with an Application to Noisy Speech Processing | http://arxiv.org/pdf/1405.4599v1.pdf | author:Dalei Wu, Haiqing Wu category:cs.CL cs.LG stat.ML published:2014-05-19 summary:The trimming scheme with a prefixed cutoff portion is known as a method ofimproving the robustness of statistical models such as multivariate Gaussianmixture models (MG- MMs) in small scale tests by alleviating the impacts ofoutliers. However, when this method is applied to real- world data, such asnoisy speech processing, it is hard to know the optimal cut-off portion toremove the outliers and sometimes removes useful data samples as well. In thispaper, we propose a new method based on measuring the dispersion degree (DD) ofthe training data to avoid this problem, so as to realise automatic robustestimation for MGMMs. The DD model is studied by using two different measures.For each one, we theoretically prove that the DD of the data samples in acontext of MGMMs approximately obeys a specific (chi or chi-square)distribution. The proposed method is evaluated on a real-world application witha moderately-sized speaker recognition task. Experiments show that the proposedmethod can significantly improve the robustness of the conventional trainingmethod of GMMs for speaker recognition.
arxiv-6300-263 | Multi-borders classification | http://arxiv.org/pdf/1404.4095v3.pdf | author:Peter Mills category:stat.ML cs.LG published:2014-04-15 summary:The number of possible methods of generalizing binary classification tomulti-class classification increases exponentially with the number of classlabels. Often, the best method of doing so will be highly problem dependent.Here we present classification software in which the partitioning ofmulti-class classification problems into binary classification problems isspecified using a recursive control language.
arxiv-6300-264 | ESSP: An Efficient Approach to Minimizing Dense and Nonsubmodular Energy Functions | http://arxiv.org/pdf/1405.4583v1.pdf | author:Wei Feng, Jiaya Jia, Zhi-Qiang Liu category:cs.CV cs.LG published:2014-05-19 summary:Many recent advances in computer vision have demonstrated the impressivepower of dense and nonsubmodular energy functions in solving visual labelingproblems. However, minimizing such energies is challenging. None of existingtechniques (such as s-t graph cut, QPBO, BP and TRW-S) can individually do thiswell. In this paper, we present an efficient method, namely ESSP, to optimizebinary MRFs with arbitrary pairwise potentials, which could be nonsubmodularand with dense connectivity. We also provide a comparative study of ourapproach and several recent promising methods. From our study, we make somereasonable recommendations of combining existing methods that perform the bestin different situations for this challenging problem. Experimental resultsvalidate that for dense and nonsubmodular energy functions, the proposedapproach can usually obtain lower energies than the best combination of othertechniques using comparably reasonable time.
arxiv-6300-265 | Kronecker PCA Based Spatio-Temporal Modeling of Video for Dismount Classification | http://arxiv.org/pdf/1405.4574v1.pdf | author:Kristjan H. Greenewald, Alfred O. Hero III category:cs.CV stat.ME published:2014-05-19 summary:We consider the application of KronPCA spatio-temporal modeling techniques[Greenewald et al 2013, Tsiligkaridis et al 2013] to the extraction ofspatiotemporal features for video dismount classification. KronPCA performs alow-rank type of dimensionality reduction that is adapted to spatio-temporaldata and is characterized by the T frame multiframe mean and covariance of pspatial features. For further regularization and improved inverse estimation,we also use the diagonally corrected KronPCA shrinkage methods we presented in[Greenewald et al 2013]. We apply this very general method to the modeling ofthe multivariate temporal behavior of HOG features extracted from pedestrianbounding boxes in video, with gender classification in a challenging datasetchosen as a specific application. The learned covariances for each class areused to extract spatiotemporal features which are then classified, achievingcompetitive classification performance.
arxiv-6300-266 | A Distributed Algorithm for Training Nonlinear Kernel Machines | http://arxiv.org/pdf/1405.4543v1.pdf | author:Dhruv Mahajan, S. Sathiya Keerthi, S. Sundararajan category:cs.LG published:2014-05-18 summary:This paper concerns the distributed training of nonlinear kernel machines onMap-Reduce. We show that a re-formulation of Nystr\"om approximation basedsolution which is solved using gradient based techniques is well suited forthis, especially when it is necessary to work with a large number of basispoints. The main advantages of this approach are: avoidance of computing thepseudo-inverse of the kernel sub-matrix corresponding to the basis points;simplicity and efficiency of the distributed part of the computations; and,friendliness to stage-wise addition of basis points. We implement the methodusing an AllReduce tree on Hadoop and demonstrate its value on a few largebenchmark datasets.
arxiv-6300-267 | A Memetic Algorithm for the Linear Ordering Problem with Cumulative Costs | http://arxiv.org/pdf/1405.4510v1.pdf | author:Tao Ye, Kan Zhou, Zhipeng Lu, Jin-Kao Hao category:cs.NE published:2014-05-18 summary:This paper introduces an effective memetic algorithm for the linear orderingproblem with cumulative costs. The proposed algorithm combines an order-basedrecombination operator with an improved forward-backward local search procedureand employs a solution quality based replacement criterion for pool updating.Extensive experiments on 118 well-known benchmark instances show that theproposed algorithm achieves competitive results by identifying 46 new upperbounds. Furthermore, some critical ingredients of our algorithm are analyzed tounderstand the source of its performance.
arxiv-6300-268 | A Multi-parent Memetic Algorithm for the Linear Ordering Problem | http://arxiv.org/pdf/1405.4507v1.pdf | author:Tao Ye, Tao Wang, Zhipeng Lu, Jin-Kao Hao category:cs.NE math.OC published:2014-05-18 summary:In this paper, we present a multi-parent memetic algorithm (denoted by MPM)for solving the classic Linear Ordering Problem (LOP). The MPM algorithmintegrates in particular a multi-parent recombination operator for generatingoffspring solutions and a distance-and-quality based criterion for poolupdating. Our MPM algorithm is assessed on 8 sets of 484 widely used LOPinstances and compared with several state-of-the-art algorithms in theliterature, showing the efficacy of the MPM algorithm. Specifically, for the255 instances whose optimal solutions are unknown, the MPM is able to detectbetter solutions than the previous best-known ones for 66 instances, whilematching the previous best-known results for 163 instances. Furthermore, someadditional experiments are carried out to analyze the key elements andimportant parameters of MPM.
arxiv-6300-269 | Bag of Visual Words and Fusion Methods for Action Recognition: Comprehensive Study and Good Practice | http://arxiv.org/pdf/1405.4506v1.pdf | author:Xiaojiang Peng, Limin Wang, Xingxing Wang, Yu Qiao category:cs.CV published:2014-05-18 summary:Video based action recognition is one of the important and challengingproblems in computer vision research. Bag of Visual Words model (BoVW) withlocal features has become the most popular method and obtained thestate-of-the-art performance on several realistic datasets, such as the HMDB51,UCF50, and UCF101. BoVW is a general pipeline to construct a globalrepresentation from a set of local features, which is mainly composed of fivesteps: (i) feature extraction, (ii) feature pre-processing, (iii) codebookgeneration, (iv) feature encoding, and (v) pooling and normalization. Manyefforts have been made in each step independently in different scenarios andtheir effect on action recognition is still unknown. Meanwhile, video dataexhibits different views of visual pattern, such as static appearance andmotion dynamics. Multiple descriptors are usually extracted to represent thesedifferent views. Many feature fusion methods have been developed in other areasand their influence on action recognition has never been investigated before.This paper aims to provide a comprehensive study of all steps in BoVW anddifferent fusion methods, and uncover some good practice to produce astate-of-the-art action recognition system. Specifically, we explore two kindsof local features, ten kinds of encoding methods, eight kinds of pooling andnormalization strategies, and three kinds of fusion methods. We conclude thatevery step is crucial for contributing to the final recognition rate.Furthermore, based on our comprehensive study, we propose a simple yeteffective representation, called hybrid representation, by exploring thecomplementarity of different BoVW frameworks and local descriptors. Using thisrepresentation, we obtain the state-of-the-art on the three challengingdatasets: HMDB51 (61.1%), UCF50 (92.3%), and UCF101 (87.9%).
arxiv-6300-270 | Online Learning with Composite Loss Functions | http://arxiv.org/pdf/1405.4471v1.pdf | author:Ofer Dekel, Jian Ding, Tomer Koren, Yuval Peres category:cs.LG published:2014-05-18 summary:We study a new class of online learning problems where each of the onlinealgorithm's actions is assigned an adversarial value, and the loss of thealgorithm at each step is a known and deterministic function of the valuesassigned to its recent actions. This class includes problems where thealgorithm's loss is the minimum over the recent adversarial values, the maximumover the recent values, or a linear combination of the recent values. Weanalyze the minimax regret of this class of problems when the algorithmreceives bandit feedback, and prove that when the minimum or maximum functionsare used, the minimax regret is $\tilde \Omega(T^{2/3})$ (so called hard onlinelearning problems), and when a linear function is used, the minimax regret is$\tilde O(\sqrt{T})$ (so called easy learning problems). Previously, the onlyonline learning problem that was known to be provably hard was the multi-armedbandit with switching costs.
arxiv-6300-271 | Multi-layered graph-based multi-document summarization model | http://arxiv.org/pdf/1405.7975v1.pdf | author:Ercan Canhasi category:cs.IR cs.CL published:2014-05-17 summary:Multi-document summarization is a process of automatic generation of acompressed version of the given collection of documents. Recently, thegraph-based models and ranking algorithms have been actively investigated bythe extractive document summarization community. While most work to datefocuses on homogeneous connecteness of sentences and heterogeneous connectenessof documents and sentences (e.g. sentence similarity weighted by documentimportance), in this paper we present a novel 3-layered graph model thatemphasizes not only sentence and document level relations but also theinfluence of under sentence level relations (e.g. a part of sentencesimilarity).
arxiv-6300-272 | Preliminary Report on the Structure of Croatian Linguistic Co-occurrence Networks | http://arxiv.org/pdf/1405.4433v1.pdf | author:Domagoj Margan, Sanda Martinƒçiƒá-Ip≈°iƒá, Ana Me≈°troviƒá category:cs.CL cs.SI physics.soc-ph published:2014-05-17 summary:In this article, we investigate the structure of Croatian linguisticco-occurrence networks. We examine the change of network structure propertiesby systematically varying the co-occurrence window sizes, the corpus sizes andremoving stopwords. In a co-occurrence window of size $n$ we establish a linkbetween the current word and $n-1$ subsequent words. The results point out thatthe increase of the co-occurrence window size is followed by a decrease indiameter, average path shortening and expectedly condensing the averageclustering coefficient. The same can be noticed for the removal of thestopwords. Finally, since the size of texts is reflected in the networkproperties, our results suggest that the corpus influence can be reduced byincreasing the co-occurrence window size.
arxiv-6300-273 | Learning Mixtures of Discrete Product Distributions using Spectral Decompositions | http://arxiv.org/pdf/1311.2972v2.pdf | author:Prateek Jain, Sewoong Oh category:stat.ML cs.CC cs.IT cs.LG math.IT published:2013-11-12 summary:We study the problem of learning a distribution from samples, when theunderlying distribution is a mixture of product distributions over discretedomains. This problem is motivated by several practical applications such ascrowd-sourcing, recommendation systems, and learning Boolean functions. Theexisting solutions either heavily rely on the fact that the number ofcomponents in the mixtures is finite or have sample/time complexity that isexponential in the number of components. In this paper, we introduce apolynomial time/sample complexity method for learning a mixture of $r$ discreteproduct distributions over $\{1, 2, \dots, \ell\}^n$, for general $\ell$ and$r$. We show that our approach is statistically consistent and further providefinite sample guarantees. We use techniques from the recent work on tensor decompositions forhigher-order moment matching. A crucial step in these moment matching methodsis to construct a certain matrix and a certain tensor with low-rank spectraldecompositions. These tensors are typically estimated directly from thesamples. The main challenge in learning mixtures of discrete productdistributions is that these low-rank tensors cannot be obtained directly fromthe sample moments. Instead, we reduce the tensor estimation problem to: $a$)estimating a low-rank matrix using only off-diagonal block elements; and $b$)estimating a tensor using a small number of linear measurements. Leveraging onrecent developments in matrix completion, we give an alternating minimizationbased method to estimate the low-rank matrix, and formulate the tensorcompletion problem as a least-squares problem.
arxiv-6300-274 | A two-step learning approach for solving full and almost full cold start problems in dyadic prediction | http://arxiv.org/pdf/1405.4423v1.pdf | author:Tapio Pahikkala, Michiel Stock, Antti Airola, Tero Aittokallio, Bernard De Baets, Willem Waegeman category:cs.LG published:2014-05-17 summary:Dyadic prediction methods operate on pairs of objects (dyads), aiming toinfer labels for out-of-sample dyads. We consider the full and almost full coldstart problem in dyadic prediction, a setting that occurs when both objects inan out-of-sample dyad have not been observed during training, or if one of themhas been observed, but very few times. A popular approach for addressing thisproblem is to train a model that makes predictions based on a pairwise featurerepresentation of the dyads, or, in case of kernel methods, based on a tensorproduct pairwise kernel. As an alternative to such a kernel approach, weintroduce a novel two-step learning algorithm that borrows ideas from thefields of pairwise learning and spectral filtering. We show theoretically thatthe two-step method is very closely related to the tensor product kernelapproach, and experimentally that it yields a slightly better predictiveperformance. Moreover, unlike existing tensor product kernel methods, thetwo-step method allows closed-form solutions for training and parameterselection via cross-validation estimates both in the full and almost full coldstart settings, making the approach much more efficient and straightforward toimplement.
arxiv-6300-275 | Identification of functionally related enzymes by learning-to-rank methods | http://arxiv.org/pdf/1405.4394v1.pdf | author:Michiel Stock, Thomas Fober, Eyke H√ºllermeier, Serghei Glinca, Gerhard Klebe, Tapio Pahikkala, Antti Airola, Bernard De Baets, Willem Waegeman category:cs.LG cs.CE q-bio.QM stat.ML published:2014-05-17 summary:Enzyme sequences and structures are routinely used in the biological sciencesas queries to search for functionally related enzymes in online databases. Tothis end, one usually departs from some notion of similarity, comparing twoenzymes by looking for correspondences in their sequences, structures orsurfaces. For a given query, the search operation results in a ranking of theenzymes in the database, from very similar to dissimilar enzymes, whileinformation about the biological function of annotated database enzymes isignored. In this work we show that rankings of that kind can be substantially improvedby applying kernel-based learning algorithms. This approach enables thedetection of statistical dependencies between similarities of the active cleftand the biological function of annotated enzymes. This is in contrast tosearch-based approaches, which do not take annotated training data intoaccount. Similarity measures based on the active cleft are known to outperformsequence-based or structure-based measures under certain conditions. Weconsider the Enzyme Commission (EC) classification hierarchy for obtainingannotated enzymes during the training phase. The results of a set of sizeableexperiments indicate a consistent and significant improvement for a set ofsimilarity measures that exploit information about small cavities in thesurface of enzymes.
arxiv-6300-276 | That's sick dude!: Automatic identification of word sense change across different timescales | http://arxiv.org/pdf/1405.4392v1.pdf | author:Sunny Mitra, Ritwik Mitra, Martin Riedl, Chris Biemann, Animesh Mukherjee, Pawan Goyal category:cs.CL cs.AI 68T50 published:2014-05-17 summary:In this paper, we propose an unsupervised method to identify noun sensechanges based on rigorous analysis of time-varying text data available in theform of millions of digitized books. We construct distributional thesauri basednetworks from data at different time points and cluster each of them separatelyto obtain word-centric sense clusters corresponding to the different timepoints. Subsequently, we compare these sense clusters of two different timepoints to find if (i) there is birth of a new sense or (ii) if an older sensehas got split into more than one sense or (iii) if a newer sense has beenformed from the joining of older senses or (iv) if a particular sense has died.We conduct a thorough evaluation of the proposed methodology both manually aswell as through comparison with WordNet. Manual evaluation indicates that thealgorithm could correctly identify 60.4% birth cases from a set of 48 randomlypicked samples and 57% split/join cases from a set of 21 randomly pickedsamples. Remarkably, in 44% cases the birth of a novel sense is attested byWordNet, while in 46% cases and 43% cases split and join are respectivelyconfirmed by WordNet. Our approach can be applied for lexicography, as well asfor applications like word sense disambiguation or semantic search.
arxiv-6300-277 | Real Time Object Tracking Based on Inter-frame Coding: A Review | http://arxiv.org/pdf/1405.4390v1.pdf | author:Shraddha Mehta, Vaishali Kalariya category:cs.CV published:2014-05-17 summary:Inter-frame Coding plays significant role for video Compression and ComputerVision. Computer vision systems have been incorporated in many real lifeapplications (e.g. surveillance systems, medical imaging, robot navigation andidentity verification systems). Object tracking is a key computer vision topic,which aims at detecting the position of a moving object from a video sequence.The application of Inter-frame Coding for low frame rate video, as well as forlow resolution video. Various methods based on Top-down approach just likekernel based or mean shift technique are used to track the object for video,So, Inter-frame Coding algorithms are widely adopted by video coding standards,mainly due to their simplicity and good distortion performance for objecttracking.
arxiv-6300-278 | Efficient Tracking of a Moving Object using Inter-Frame Coding | http://arxiv.org/pdf/1405.4389v1.pdf | author:Shraddha Mehta, Vaishali Kalariya category:cs.CV published:2014-05-17 summary:Video surveillance has long been in use to monitor security sensitive areassuch as banks, department stores, highways, crowded public places andborders.The advance in computing power, availability of large-capacity storagedevices and high speed network infrastructure paved the way for cheaper,multi-sensor video surveillance systems.Traditionally, the video outputs areprocessed online by human operators and are usually saved to tapes for lateruse only after a forensic event.The increase in the number of cameras inordinary surveillance systems overloaded both the human operators and thestorage devices with high volumes of data and made it in-feasible to ensureproper monitoring of sensitive areas for long times.
arxiv-6300-279 | Thematically Reinforced Explicit Semantic Analysis | http://arxiv.org/pdf/1405.4364v1.pdf | author:Yannis Haralambous, Vitaly Klyuev category:cs.CL 68T50 published:2014-05-17 summary:We present an extended, thematically reinforced version of Gabrilovich andMarkovitch's Explicit Semantic Analysis (ESA), where we obtain thematicinformation through the category structure of Wikipedia. For this we firstdefine a notion of categorical tfidf which measures the relevance of terms incategories. Using this measure as a weight we calculate a maximal spanning treeof the Wikipedia corpus considered as a directed graph of pages and categories.This tree provides us with a unique path of "most related categories" betweeneach page and the top of the hierarchy. We reinforce tfidf of words in a pageby aggregating it with categorical tfidfs of the nodes of these paths, anddefine a thematically reinforced ESA semantic relatedness measure which is morerobust than standard ESA and less sensitive to noise caused by out-of-contextwords. We apply our method to the French Wikipedia corpus, evaluate it througha text classification on a 37.5 MB corpus of 20 French newsgroups and obtain aprecision increase of 9-10% compared with standard ESA.
arxiv-6300-280 | Active Semi-Supervised Learning Using Sampling Theory for Graph Signals | http://arxiv.org/pdf/1405.4324v1.pdf | author:Akshay Gadde, Aamir Anis, Antonio Ortega category:cs.LG stat.ML published:2014-05-16 summary:We consider the problem of offline, pool-based active semi-supervisedlearning on graphs. This problem is important when the labeled data is scarceand expensive whereas unlabeled data is easily available. The data points arerepresented by the vertices of an undirected graph with the similarity betweenthem captured by the edge weights. Given a target number of nodes to label, thegoal is to choose those nodes that are most informative and then predict theunknown labels. We propose a novel framework for this problem based on ourrecent results on sampling theory for graph signals. A graph signal is areal-valued function defined on each node of the graph. A notion of frequencyfor such signals can be defined using the spectrum of the graph Laplacianmatrix. The sampling theory for graph signals aims to extend the traditionalNyquist-Shannon sampling theory by allowing us to identify the class of graphsignals that can be reconstructed from their values on a subset of vertices.This approach allows us to define a criterion for active learning based onsampling set selection which aims at maximizing the frequency of the signalsthat can be reconstructed from their samples on the set. Experiments show theeffectiveness of our method.
arxiv-6300-281 | Leveraging Evolutionary Search to Discover Self-Adaptive and Self-Organizing Cellular Automata | http://arxiv.org/pdf/1405.4322v1.pdf | author:David B. Knoester, Heather J. Goldsby, Christoph Adami category:cs.NE nlin.CG published:2014-05-16 summary:Building self-adaptive and self-organizing (SASO) systems is a challengingproblem, in part because SASO principles are not yet well understood and fewplatforms exist for exploring them. Cellular automata (CA) are a well-studiedapproach to exploring the principles underlying self-organization. A CAcomprises a lattice of cells whose states change over time based on a discreteupdate function. One challenge to developing CA is that the relationship of anupdate function, which describes the local behavior of each cell, to the globalbehavior of the entire CA is often unclear. As a result, many researchers haveused stochastic search techniques, such as evolutionary algorithms, toautomatically discover update functions that produce a desired global behavior.However, these update functions are typically defined in a way that does notprovide for self-adaptation. Here we describe an approach to discovering CAupdate functions that are both self-adaptive and self-organizing. Specifically,we use a novel evolutionary algorithm-based approach to discover finite statemachines (FSMs) that implement update functions for CA. We show how thisapproach is able to evolve FSM-based update functions that perform well on thedensity classification task for 1-, 2-, and 3-dimensional CA. Moreover, we showthat these FSMs are self-adaptive, self-organizing, and highly scalable, oftenperforming well on CA that are orders of magnitude larger than those used toevaluate performance during the evolutionary search. These results demonstratethat CA are a viable platform for studying the integration of self-adaptationand self-organization, and strengthen the case for using evolutionaryalgorithms as a component of SASO systems.
arxiv-6300-282 | Coarse-to-Fine Classification via Parametric and Nonparametric Models for Computer-Aided Diagnosis | http://arxiv.org/pdf/1405.4308v1.pdf | author:Meizhu Liu, Le Lu, Xiaojing Ye, Shipeng Yu category:cs.CV published:2014-05-16 summary:Classification is one of the core problems in Computer-Aided Diagnosis (CAD),targeting for early cancer detection using 3D medical imaging interpretation.High detection sensitivity with desirably low false positive (FP) rate iscritical for a CAD system to be accepted as a valuable or even indispensabletool in radiologists' workflow. Given various spurious imagery noises whichcause observation uncertainties, this remains a very challenging task. In thispaper, we propose a novel, two-tiered coarse-to-fine (CTF) classificationcascade framework to tackle this problem. We first obtainclassification-critical data samples (e.g., samples on the decision boundary)extracted from the holistic data distributions using a robust parametric model(e.g., \cite{Raykar08}); then we build a graph-embedding based nonparametricclassifier on sampled data, which can more accurately preserve or formulate thecomplex classification boundary. These two steps can also be considered aseffective "sample pruning" and "feature pursuing + $k$NN/template matching",respectively. Our approach is validated comprehensively in colorectal polypdetection and lung nodule detection CAD systems, as the top two deadly cancers,using hospital scale, multi-site clinical datasets. The results show that ourmethod achieves overall better classification/detection performance thanexisting state-of-the-art algorithms using single-layer classifiers, such asthe support vector machine variants \cite{Wang08}, boosting \cite{Slabaugh10},logistic regression \cite{Ravesteijn10}, relevance vector machine\cite{Raykar08}, $k$-nearest neighbor \cite{Murphy09} or spectral projectionson graph \cite{Cai08}.
arxiv-6300-283 | Compositional Morphology for Word Representations and Language Modelling | http://arxiv.org/pdf/1405.4273v1.pdf | author:Jan A. Botha, Phil Blunsom category:cs.CL 68T50 I.2.7; I.2.6 published:2014-05-16 summary:This paper presents a scalable method for integrating compositionalmorphological representations into a vector-based probabilistic language model.Our approach is evaluated in the context of log-bilinear language models,rendered suitably efficient for implementation inside a machine translationdecoder by factoring the vocabulary. We perform both intrinsic and extrinsicevaluations, presenting results on a range of languages which demonstrate thatour model learns morphological representations that both perform well on wordsimilarity tasks and lead to substantial reductions in perplexity. When usedfor translation into morphologically rich languages with large vocabularies,our models obtain improvements of up to 1.2 BLEU points relative to a baselinesystem using back-off n-gram models.
arxiv-6300-284 | Les math√©matiques de la langue : l'approche formelle de Montague | http://arxiv.org/pdf/1405.4248v1.pdf | author:Yannis Haralambous category:cs.CL 68T50 published:2014-05-16 summary:We present a natural language modelization method which is strongely relyingon mathematics. This method, called "Formal Semantics," has been initiated bythe American linguist Richard M. Montague in the 1970's. It uses mathematicaltools such as formal languages and grammars, first-order logic, type theory and$\lambda$-calculus. Our goal is to have the reader discover both Montagovianformal semantics and the mathematical tools that he used in his method. ----- Nous pr\'esentons une m\'ethode de mod\'elisation de la langue naturelle quiest fortement bas\'ee sur les math\'ematiques. Cette m\'ethode, appel\'ee{\guillemotleft}s\'emantique formelle{\guillemotright}, a \'et\'e initi\'ee parle linguiste am\'ericain Richard M. Montague dans les ann\'ees 1970. Elleutilise des outils math\'ematiques tels que les langages et grammaires formels,la logique du 1er ordre, la th\'eorie de types et le $\lambda$-calcul. Nousnous proposons de faire d\'ecouvrir au lecteur tant la s\'emantique formelle deMontague que les outils math\'ematiques dont il s'est servi.
arxiv-6300-285 | Matrix Completion via Max-Norm Constrained Optimization | http://arxiv.org/pdf/1303.0341v2.pdf | author:T. Tony Cai, Wen-Xin Zhou category:cs.LG cs.IT math.IT stat.ML 62H12, 15A83 published:2013-03-02 summary:Matrix completion has been well studied under the uniform sampling model andthe trace-norm regularized methods perform well both theoretically andnumerically in such a setting. However, the uniform sampling model isunrealistic for a range of applications and the standard trace-norm relaxationcan behave very poorly when the sampling distribution is non-uniform. In this paper we propose and analyze a max-norm constrained empirical riskminimization method for noisy matrix completion under a general sampling model.The optimal rate of convergence is established under the Frobenius norm loss inthe context of approximately low-rank matrix reconstruction. It is shown thatthe max-norm constrained method is minimax rate-optimal and it yields a uni?edand robust approximate recovery guarantee, with respect to the samplingdistributions. The computational effectiveness of this method is also studied,based on a first-order algorithm for solving convex programs involving amax-norm constraint.
arxiv-6300-286 | FTVd is beyond Fast Total Variation regularized Deconvolution | http://arxiv.org/pdf/1402.3869v2.pdf | author:Yilun Wang category:cs.CV published:2014-02-17 summary:In this paper, we revisit the "FTVd" algorithm for Fast Total VariationRegularized Deconvolution, which has been widely used in the past few years.Both its original version implemented in the MATLAB software FTVd 3.0 and itsrelated variant implemented in the latter version FTVd 4.0 are considered\cite{Wang08FTVdsoftware}. We propose that the intermediate results during theiterations are the solutions of a series of combined Tikhonov and totalvariation regularized image deconvolution models and therefore some of themoften have even better image quality than the final solution, which iscorresponding to the pure total variation regularized model.
arxiv-6300-287 | Optimized Cartesian $K$-Means | http://arxiv.org/pdf/1405.4054v1.pdf | author:Jianfeng Wang, Jingdong Wang, Jingkuan Song, Xin-Shun Xu, Heng Tao Shen, Shipeng Li category:cs.CV published:2014-05-16 summary:Product quantization-based approaches are effective to encodehigh-dimensional data points for approximate nearest neighbor search. The spaceis decomposed into a Cartesian product of low-dimensional subspaces, each ofwhich generates a sub codebook. Data points are encoded as compact binary codesusing these sub codebooks, and the distance between two data points can beapproximated efficiently from their codes by the precomputed lookup tables.Traditionally, to encode a subvector of a data point in a subspace, only onesub codeword in the corresponding sub codebook is selected, which may imposestrict restrictions on the search accuracy. In this paper, we propose a novelapproach, named Optimized Cartesian $K$-Means (OCKM), to better encode the datapoints for more accurate approximate nearest neighbor search. In OCKM, multiplesub codewords are used to encode the subvector of a data point in a subspace.Each sub codeword stems from different sub codebooks in each subspace, whichare optimally generated with regards to the minimization of the distortionerrors. The high-dimensional data point is then encoded as the concatenation ofthe indices of multiple sub codewords from all the subspaces. This can providemore flexibility and lower distortion errors than traditional methods.Experimental results on the standard real-life datasets demonstrate thesuperiority over state-of-the-art approaches for approximate nearest neighborsearch.
arxiv-6300-288 | Iterative Non-Local Shrinkage Algorithm for MR Image Reconstruction | http://arxiv.org/pdf/1405.5494v1.pdf | author:Yasir Q. Moshin, Greg Ongie, Mathews Jacob category:cs.CV published:2014-05-15 summary:We introduce a fast iterative non-local shrinkage algorithm to recover MRIdata from undersampled Fourier measurements. This approach is enabled by thereformulation of current non-local schemes as an alternating algorithm tominimize a global criterion. The proposed algorithm alternates between anon-local shrinkage step and a quadratic subproblem. We derive analyticalshrinkage rules for several penalties that are relevant in non-localregularization. The redundancy in the searches used to evaluate the shrinkagesteps are exploited using filtering operations. The resulting algorithm isobserved to be considerably faster than current alternating non-localalgorithms. The comparisons of the proposed scheme with state-of-the-artregularization schemes show a considerable reduction in alias artifacts andpreservation of edges.
arxiv-6300-289 | On learning to localize objects with minimal supervision | http://arxiv.org/pdf/1403.1024v4.pdf | author:Hyun Oh Song, Ross Girshick, Stefanie Jegelka, Julien Mairal, Zaid Harchaoui, Trevor Darrell category:cs.CV cs.LG published:2014-03-05 summary:Learning to localize objects with minimal supervision is an important problemin computer vision, since large fully annotated datasets are extremely costlyto obtain. In this paper, we propose a new method that achieves this goal withonly image-level labels of whether the objects are present or not. Our approachcombines a discriminative submodular cover problem for automaticallydiscovering a set of positive object windows with a smoothed latent SVMformulation. The latter allows us to leverage efficient quasi-Newtonoptimization techniques. Our experiments demonstrate that the proposed approachprovides a 50% relative improvement in mean average precision over the currentstate-of-the-art on PASCAL VOC 2007 detection.
arxiv-6300-290 | Fast Ridge Regression with Randomized Principal Component Analysis and Gradient Descent | http://arxiv.org/pdf/1405.3952v1.pdf | author:Yichao Lu, Dean P. Foster category:stat.ML published:2014-05-15 summary:We propose a new two stage algorithm LING for large scale regressionproblems. LING has the same risk as the well known Ridge Regression under thefixed design setting and can be computed much faster. Our experiments haveshown that LING performs well in terms of both prediction accuracy andcomputational efficiency compared with other large scale regression algorithmslike Gradient Descent, Stochastic Gradient Descent and Principal ComponentRegression on both simulated and real datasets.
arxiv-6300-291 | M√©thodes pour la repr√©sentation informatis√©e de donn√©es lexicales / Methoden der Speicherung lexikalischer Daten | http://arxiv.org/pdf/1405.3925v1.pdf | author:Laurent Romary, Andreas Witt category:cs.CL published:2014-05-15 summary:In recent years, new developments in the area of lexicography have alterednot only the management, processing and publishing of lexicographical data, butalso created new types of products such as electronic dictionaries andthesauri. These expand the range of possible uses of lexical data and supportusers with more flexibility, for instance in assisting human translation. Inthis article, we give a short and easy-to-understand introduction to theproblematic nature of the storage, display and interpretation of lexical data.We then describe the main methods and specifications used to build andrepresent lexical data. This paper is targeted for the following groups ofpeople: linguists, lexicographers, IT specialists, computer linguists and allothers who wish to learn more about the modelling, representation andvisualization of lexical knowledge. This paper is written in two languages:French and German.
arxiv-6300-292 | Randomized Approximation of the Gram Matrix: Exact Computation and Probabilistic Bounds | http://arxiv.org/pdf/1310.1502v3.pdf | author:John T. Holodnak, Ilse C. F. Ipsen category:math.NA cs.LG stat.ML published:2013-10-05 summary:Given a real matrix A with n columns, the problem is to approximate the Gramproduct AA^T by c << n weighted outer products of columns of A. Necessary andsufficient conditions for the exact computation of AA^T (in exact arithmetic)from c >= rank(A) columns depend on the right singular vector matrix of A. Fora Monte-Carlo matrix multiplication algorithm by Drineas et al. that samplesouter products, we present probabilistic bounds for the 2-norm relative errordue to randomization. The bounds depend on the stable rank or the rank of A,but not on the matrix dimensions. Numerical experiments illustrate that thebounds are informative, even for stringent success probabilities and matricesof small dimension. We also derive bounds for the smallest singular value andthe condition number of matrices obtained by sampling rows from orthonormalmatrices.
arxiv-6300-293 | Speeding up Convolutional Neural Networks with Low Rank Expansions | http://arxiv.org/pdf/1405.3866v1.pdf | author:Max Jaderberg, Andrea Vedaldi, Andrew Zisserman category:cs.CV published:2014-05-15 summary:The focus of this paper is speeding up the evaluation of convolutional neuralnetworks. While delivering impressive results across a range of computer visionand machine learning tasks, these networks are computationally demanding,limiting their deployability. Convolutional layers generally consume the bulkof the processing time, and so in this work we present two simple schemes fordrastically speeding up these layers. This is achieved by exploitingcross-channel or filter redundancy to construct a low rank basis of filtersthat are rank-1 in the spatial domain. Our methods are architecture agnostic,and can be easily applied to existing CPU and GPU convolutional frameworks fortuneable speedup performance. We demonstrate this with a real world networkdesigned for scene text character recognition, showing a possible 2.5x speedupwith no loss in accuracy, and 4.5x speedup with less than 1% drop in accuracy,still achieving state-of-the-art on standard benchmarks.
arxiv-6300-294 | Logistic Regression: Tight Bounds for Stochastic and Online Optimization | http://arxiv.org/pdf/1405.3843v1.pdf | author:Elad Hazan, Tomer Koren, Kfir Y. Levy category:cs.LG published:2014-05-15 summary:The logistic loss function is often advocated in machine learning andstatistics as a smooth and strictly convex surrogate for the 0-1 loss. In thispaper we investigate the question of whether these smoothness and convexityproperties make the logistic loss preferable to other widely considered optionssuch as the hinge loss. We show that in contrast to known asymptotic bounds, aslong as the number of prediction/optimization iterations is sub exponential,the logistic loss provides no improvement over a generic non-smooth lossfunction such as the hinge loss. In particular we show that the convergencerate of stochastic logistic optimization is bounded from below by a polynomialin the diameter of the decision set and the number of prediction iterations,and provide a matching tight upper bound. This resolves the COLT open problemof McMahan and Streeter (2012).
arxiv-6300-295 | Large-scale Multi-label Text Classification - Revisiting Neural Networks | http://arxiv.org/pdf/1312.5419v3.pdf | author:Jinseok Nam, Jungi Kim, Eneldo Loza Menc√≠a, Iryna Gurevych, Johannes F√ºrnkranz category:cs.LG published:2013-12-19 summary:Neural networks have recently been proposed for multi-label classificationbecause they are able to capture and model label dependencies in the outputlayer. In this work, we investigate limitations of BP-MLL, a neural network(NN) architecture that aims at minimizing pairwise ranking error. Instead, wepropose to use a comparably simple NN approach with recently proposed learningtechniques for large-scale multi-label text classification tasks. Inparticular, we show that BP-MLL's ranking loss minimization can be efficientlyand effectively replaced with the commonly used cross entropy error function,and demonstrate that several advances in neural network training that have beendeveloped in the realm of deep learning can be effectively employed in thissetting. Our experimental results show that simple NN models equipped withadvanced techniques such as rectified linear units, dropout, and AdaGradperform as well as or even outperform state-of-the-art approaches on sixlarge-scale textual datasets with diverse characteristics.
arxiv-6300-296 | Complex Networks Measures for Differentiation between Normal and Shuffled Croatian Texts | http://arxiv.org/pdf/1405.3786v1.pdf | author:Domagoj Margan, Ana Me≈°troviƒá, Sanda Martinƒçiƒá-Ip≈°iƒá category:cs.CL physics.soc-ph published:2014-05-15 summary:This paper studies the properties of the Croatian texts via complex networks.We present network properties of normal and shuffled Croatian texts fordifferent shuffling principles: on the sentence level and on the text level. Inboth experiments we preserved the vocabulary size, word and sentence frequencydistributions. Additionally, in the first shuffling approach we preserved thesentence structure of the text and the number of words per sentence. Obtainedresults showed that degree rank distributions exhibit no substantial deviationin shuffled networks, and strength rank distributions are preserved due to thesame word frequencies. Therefore, standard approach to study the structure oflinguistic co-occurrence networks showed no clear difference among thetopologies of normal and shuffled texts. Finally, we showed that the in- andout- selectivity values from shuffled texts are constantly below selectivityvalues calculated from normal texts. Our results corroborate that the nodeselectivity measure can capture structural differences between original andshuffled Croatian texts.
arxiv-6300-297 | Oracle Inequalities for High Dimensional Vector Autoregressions | http://arxiv.org/pdf/1311.0811v2.pdf | author:Anders Bredahl Kock, Laurent A. F. Callot category:math.ST stat.ML stat.TH published:2013-11-04 summary:This paper establishes non-asymptotic oracle inequalities for the predictionerror and estimation accuracy of the LASSO in stationary vector autoregressivemodels. These inequalities are used to establish consistency of the LASSO evenwhen the number of parameters is of a much larger order of magnitude than thesample size. We also give conditions under which no relevant variables areexcluded. Next, non-asymptotic probabilities are given for the Adaptive LASSO to selectthe correct sparsity pattern. We then give conditions under which the AdaptiveLASSO reveals the correct sparsity pattern asymptotically. We establish thatthe estimates of the non-zero coefficients are asymptotically equivalent to theoracle assisted least squares estimator. This is used to show that the rate ofconvergence of the estimates of the non-zero coefficients is identical to theone of least squares only including the relevant covariates.
arxiv-6300-298 | INAUT, a Controlled Language for the French Coast Pilot Books Instructions nautiques | http://arxiv.org/pdf/1405.3772v1.pdf | author:Yannis Haralambous, Julie Sauvage-Vincent, John Puentes category:cs.CL I.3.5 published:2014-05-15 summary:We describe INAUT, a controlled natural language dedicated to collaborativeupdate of a knowledge base on maritime navigation and to automatic generationof coast pilot books (Instructions nautiques) of the French NationalHydrographic and Oceanographic Service SHOM. INAUT is based on French languageand abundantly uses georeferenced entities. After describing the structure ofthe overall system, giving details on the language and on its generation, anddiscussing the three major applications of INAUT (document production,interaction with ENCs and collaborative updates of the knowledge base), weconclude with future extensions and open problems.
arxiv-6300-299 | Effective Bayesian Modeling of Groups of Related Count Time Series | http://arxiv.org/pdf/1405.3738v1.pdf | author:Nicolas Chapados category:stat.ML stat.AP published:2014-05-15 summary:Time series of counts arise in a variety of forecasting applications, forwhich traditional models are generally inappropriate. This paper introduces ahierarchical Bayesian formulation applicable to count time series that caneasily account for explanatory variables and share statistical strength acrossgroups of related time series. We derive an efficient approximate inferencetechnique, and illustrate its performance on a number of datasets from supplychain planning.
arxiv-6300-300 | Topic words analysis based on LDA model | http://arxiv.org/pdf/1405.3726v1.pdf | author:Xi Qiu, Christopher Stewart category:cs.SI cs.DC cs.IR cs.LG stat.ML published:2014-05-15 summary:Social network analysis (SNA), which is a research field describing andmodeling the social connection of a certain group of people, is popular amongnetwork services. Our topic words analysis project is a SNA method to visualizethe topic words among emails from Obama.com to accounts registered in Columbus,Ohio. Based on Latent Dirichlet Allocation (LDA) model, a popular topic modelof SNA, our project characterizes the preference of senders for target group ofreceptors. Gibbs sampling is used to estimate topic and word distribution. Ourtraining and testing data are emails from the carbon-free serverDatagreening.com. We use parallel computing tool BashReduce for word processingand generate related words under each latent topic to discovers typicalinformation of political news sending specially to local Columbus receptors.Running on two instances using paralleling tool BashReduce, our projectcontributes almost 30% speedup processing the raw contents, comparing withprocessing contents on one instance locally. Also, the experimental resultshows that the LDA model applied in our project provides precision rate 53.96%higher than TF-IDF model finding target words, on the condition thatappropriate size of topic words list is selected.
