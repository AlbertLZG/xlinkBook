arxiv-6300-1 | Spectral Sparse Representation for Clustering: Evolved from PCA, K-means, Laplacian Eigenmap, and Ratio Cut | http://arxiv.org/abs/1403.6290 | author:Zhenfang Hu, Gang Pan, Yueming Wang, Zhaohui Wu category:cs.CV published:2014-03-25 summary:Dimensionality reduction methods, e.g. PCA and Laplacian eigenmap (LE), andcluster analysis methods, e.g. K-means and ratio cut (Rcut), are two kinds ofwidely-used unsupervised data analysis tools. The former concerns highrepresentation fidelity, while the latter semantics. Some preliminary relationsbetween these methods have been established in the literature, but they are notyet integrated into a unified framework. In this paper, we show that under anideal condition, the four methods: PCA, K-means, LE, and Rcut, are unifiedtogether; and when the ideal condition is relaxed, the unification evolves to anew sparse representation method, called spectral sparse representation (SSR).It achieves the same representation fidelity as PCA/LE, and is able to revealthe cluster structure of data as K-means/Rcut. SSR is inherently related tocluster analysis, and the sparse codes can be directly used to do clustering.An efficient algorithm NSCrt is developed to solve the sparse codes of SSR. Itis observed that NSCrt is able to effectively recover the underlying solutions.As a direct application of SSR, a new clustering algorithm Scut is devised. Itreaches the start-of-the-art performance among spectral clustering methods.Compared with K-means based clustering methods, Scut does not depend oninitialization and avoids getting trapped in local minima; and the solutionsare comparable to the optimal ones of K-means run many times. Extensiveexperiments using data sets of different nature demonstrate the properties andstrengths of SSR, NSCrt, and Scut.
arxiv-6300-2 | Variance-Constrained Actor-Critic Algorithms for Discounted and Average Reward MDPs | http://arxiv.org/abs/1403.6530 | author:Prashanth L. A., Mohammad Ghavamzadeh category:cs.LG math.OC stat.ML published:2014-03-25 summary:In many sequential decision-making problems we may want to manage risk byminimizing some measure of variability in rewards in addition to maximizing astandard criterion. Variance related risk measures are among the most commonrisk-sensitive criteria in finance and operations research. However, optimizingmany such criteria is known to be a hard problem. In this paper, we considerboth discounted and average reward Markov decision processes. For eachformulation, we first define a measure of variability for a policy, which inturn gives us a set of risk-sensitive criteria to optimize. For each of thesecriteria, we derive a formula for computing its gradient. We then deviseactor-critic algorithms that operate on three timescales - a TD critic on thefastest timescale, a policy gradient (actor) on the intermediate timescale, anda dual ascent for Lagrange multipliers on the slowest timescale. In thediscounted setting, we point out the difficulty in estimating the gradient ofthe variance of the return and incorporate simultaneous perturbation approachesto alleviate this. The average setting, on the other hand, allows for an actorupdate using compatible features to estimate the gradient of the variance. Weestablish the convergence of our algorithms to locally risk-sensitive optimalpolicies. Finally, we demonstrate the usefulness of our algorithms in a trafficsignal control application.
arxiv-6300-3 | Arguments for Nested Patterns in Neural Ensembles | http://arxiv.org/abs/1403.6274 | author:Kieran Greer category:cs.NE q-bio.NC published:2014-03-25 summary:This paper describes a relatively simple way of allowing a brain model toself-organise its concept patterns through nested structures. Time is a keyelement and a simulator would be able to show how patterns may form and thenfire in sequence, as part of a search or thought process. It uses a very simpleequation to show how the inhibitors in particular, can switch off certainareas, to allow other areas to become the prominent ones and thereby define thecurrent brain state. This allows for a small amount of control over whatappears to be a chaotic structure inside of the brain. It is attractive becauseit is still mostly mechanical and therefore can be added as an automaticprocess, or the modelling of that. The paper also describes how the nestedpattern structure can be used as a basic counting mechanism.
arxiv-6300-4 | AIS-INMACA: A Novel Integrated MACA Based Clonal Classifier for Protein Coding and Promoter Region Prediction | http://arxiv.org/abs/1403.5933 | author:Pokkuluri Kiran Sree, Inampudi Ramesh Babu category:cs.CE cs.LG published:2014-03-24 summary:Most of the problems in bioinformatics are now the challenges in computing.This paper aims at building a classifier based on Multiple Attractor CellularAutomata (MACA) which uses fuzzy logic. It is strengthened with an artificialImmune System Technique (AIS), Clonal algorithm for identifying a proteincoding and promoter region in a given DNA sequence. The proposed classifier isnamed as AIS-INMACA introduces a novel concept to combine CA with artificialimmune system to produce a better classifier which can address major problemsin bioinformatics. This will be the first integrated algorithm which canpredict both promoter and protein coding regions. To obtain good fitness rulesthe basic concept of Clonal selection algorithm was used. The proposedclassifier can handle DNA sequences of lengths 54,108,162,252,354. Thisclassifier gives the exact boundaries of both protein and promoter regions withan average accuracy of 89.6%. This classifier was tested with 97,000 datacomponents which were taken from Fickett & Toung, MPromDb, and other sequencesfrom a renowned medical university. This proposed classifier can handle hugedata sets and can find protein and promoter regions even in mixed andoverlapped DNA sequences. This work also aims at identifying the logicalitybetween the major problems in bioinformatics and tries to obtaining a commonframe work for addressing major problems in bioinformatics like proteinstructure prediction, RNA structure prediction, predicting the splicing patternof any primary transcript and analysis of information content in DNA, RNA,protein sequences and structure. This work will attract more researcherstowards application of CA as a potential pattern classifier to many importantproblems in bioinformatics
arxiv-6300-5 | SRA: Fast Removal of General Multipath for ToF Sensors | http://arxiv.org/abs/1403.5919 | author:Daniel Freedman, Eyal Krupka, Yoni Smolin, Ido Leichter, Mirko Schmidt category:cs.CV published:2014-03-24 summary:A major issue with Time of Flight sensors is the presence of multipathinterference. We present Sparse Reflections Analysis (SRA), an algorithm forremoving this interference which has two main advantages. First, it allows forvery general forms of multipath, including interference with three or morepaths, diffuse multipath resulting from Lambertian surfaces, and combinationsthereof. SRA removes this general multipath with robust techniques based on$L_1$ optimization. Second, due to a novel dimension reduction, we are able toproduce a very fast version of SRA, which is able to run at frame rate.Experimental results on both synthetic data with ground truth, as well as realimages of challenging scenes, validate the approach.
arxiv-6300-6 | First Order Methods for Robust Non-negative Matrix Factorization for Large Scale Noisy Data | http://arxiv.org/abs/1403.5994 | author:Jason Gejie Liu, Shuchin Aeron category:stat.ML published:2014-03-24 summary:Nonnegative matrix factorization (NMF) has been shown to be identifiableunder the separability assumption, under which all the columns(or rows) of theinput data matrix belong to the convex cone generated by only a few of thesecolumns(or rows) [1]. In real applications, however, such separabilityassumption is hard to satisfy. Following [4] and [5], in this paper, we look atthe Linear Programming (LP) based reformulation to locate the extreme rays ofthe convex cone but in a noisy setting. Furthermore, in order to deal with thelarge scale data, we employ First-Order Methods (FOM) to mitigate thecomputational complexity of LP, which primarily results from a large number ofconstraints. We show the performance of the algorithm on real and syntheticdata sets.
arxiv-6300-7 | Brain Tumor Detection Based On Mathematical Analysis and Symmetry Information | http://arxiv.org/abs/1403.6002 | author:Narkhede Sachin G., Vaishali Khairnar, Sujata Kadu category:cs.CV published:2014-03-24 summary:Image segmentation some of the challenging issues on brain magnetic resonanceimage tumor segmentation caused by the weak correlation between magneticresonance imaging intensity and anatomical meaning.With the objective ofutilizing more meaningful information to improve brain tumor segmentation,anapproach which employs bilateral symmetry information as an additional featurefor segmentation is proposed.This is motivated by potential performanceimprovement in the general automatic brain tumor segmentation systems which areimportant for many medical and scientific applications.Brain Magnetic ResonanceImaging segmentation is a complex problem in the field of medical imagingdespite various presented methods.MR image of human brain can be divided intoseveral sub-regions especially soft tissues such as gray matter,white matterand cerebra spinal fluid.Although edge information is the main clue in imagesegmentation,it cannot get a better result in analysis the content of imageswithout combining other information.Our goal is to detect the position andboundary of tumors automatically.Experiments were conducted on realpictures,and the results show that the algorithm is flexible and convenient.
arxiv-6300-8 | Development and evaluation of a 3D model observer with nonlinear spatiotemporal contrast sensitivity | http://arxiv.org/abs/1403.6183 | author:Ali R. N. Avanaki, Kathryn S. Espig, Andrew D. A. Maidment, Cedric Marchessoux, Predrag R. Bakic, Tom R. L. Kimpe category:cs.CV published:2014-03-24 summary:We investigate improvements to our 3D model observer with the goal of bettermatching human observer performance as a function of viewing distance,effective contrast, maximum luminance, and browsing speed. Two nonlinearmethods of applying the human contrast sensitivity function (CSF) to a 3D modelobserver are proposed, namely the Probability Map (PM) and Monte Carlo (MC)methods. In the PM method, the visibility probability for each frequencycomponent of the image stack, p, is calculated taking into account Barten'sspatiotemporal CSF, the component modulation, and the human psychometricfunction. The probability p is considered to be equal to the perceivedamplitude of the frequency component and thus can be used by a traditionalmodel observer (e.g., LG-msCHO) in the space-time domain. In the MC method,each component is randomly kept with probability p or discarded with 1-p. Theamplitude of the retained components is normalized to unity. The methods weretested using DBT stacks of an anthropomorphic breast phantom processed in acomprehensive simulation pipeline. Our experiments indicate that both the PMand MC methods yield results that match human observer performance better thanthe linear filtering method as a function of viewing distance, effectivecontrast, maximum luminance, and browsing speed.
arxiv-6300-9 | Coherent Multi-Sentence Video Description with Variable Level of Detail | http://arxiv.org/abs/1403.6173 | author:Anna Senina, Marcus Rohrbach, Wei Qiu, Annemarie Friedrich, Sikandar Amin, Mykhaylo Andriluka, Manfred Pinkal, Bernt Schiele category:cs.CV cs.CL published:2014-03-24 summary:Humans can easily describe what they see in a coherent way and at varyinglevel of detail. However, existing approaches for automatic video descriptionare mainly focused on single sentence generation and produce descriptions at afixed level of detail. In this paper, we address both of these limitations: fora variable level of detail we produce coherent multi-sentence descriptions ofcomplex videos. We follow a two-step approach where we first learn to predict asemantic representation (SR) from video and then generate natural languagedescriptions from the SR. To produce consistent multi-sentence descriptions, wemodel across-sentence consistency at the level of the SR by enforcing aconsistent topic. We also contribute both to the visual recognition of objectsproposing a hand-centric approach as well as to the robust generation ofsentences using a word lattice. Human judges rate our multi-sentencedescriptions as more readable, correct, and relevant than related work. Tounderstand the difference between more detailed and shorter descriptions, wecollect and analyze a video description corpus of three levels of detail.
arxiv-6300-10 | The state of play of ASC-Inclusion: An Integrated Internet-Based Environment for Social Inclusion of Children with Autism Spectrum Conditions | http://arxiv.org/abs/1403.5912 | author:Björn Schuller, Erik Marchi, Simon Baron-Cohen, Helen O'Reilly, Delia Pigat, Peter Robinson, Ian Daves category:cs.HC cs.CV cs.CY published:2014-03-24 summary:Individuals with Autism Spectrum Conditions (ASC) have marked difficultiesusing verbal and non-verbal communication for social interaction. The runningASC-Inclusion project aims to help children with ASC by allowing them to learnhow emotions can be expressed and recognised via playing games in a virtualworld. The platform includes analysis of users' gestures, facial, and vocalexpressions using standard microphone and web-cam or a depth sensor, trainingthrough games, text communication with peers, animation, video and audio clips.We present the state of play in realising such a serious game platform andprovide results for the different modalities.
arxiv-6300-11 | New Algorithmic Approaches to Point Constellation Recognition | http://arxiv.org/abs/1405.1402 | author:Thomas Bourgeat, Julien Bringer, Herve Chabanne, Robin Champenois, Jeremie Clement, Houda Ferradi, Marc Heinrich, Paul Melotti, David Naccache, Antoine Voizard category:cs.CV published:2014-03-24 summary:Point constellation recognition is a common problem with many patternmatching applications. Whilst useful in many contexts, this work is mainlymotivated by fingerprint matching. Fingerprints are traditionally modelled asconstellations of oriented points called minutiae. The fingerprint verifier'stask consists in comparing two point constellations. The comparedconstellations may differ by rotation and translation or by much more involvedtransforms such as distortion or occlusion. This paper presents three newconstellation matching algorithms. The first two methods generalize analgorithm by Bringer and Despiegel. Our third proposal creates a veryinteresting analogy between mechanical system simulation and the constellationrecognition problem.
arxiv-6300-12 | Non-uniform Feature Sampling for Decision Tree Ensembles | http://arxiv.org/abs/1403.5877 | author:Anastasios Kyrillidis, Anastasios Zouzias category:stat.ML cs.IT cs.LG math.IT stat.AP published:2014-03-24 summary:We study the effectiveness of non-uniform randomized feature selection indecision tree classification. We experimentally evaluate two feature selectionmethodologies, based on information extracted from the provided dataset: $(i)$\emph{leverage scores-based} and $(ii)$ \emph{norm-based} feature selection.Experimental evaluation of the proposed feature selection techniques indicatethat such approaches might be more effective compared to naive uniform featureselection and moreover having comparable performance to the random forestalgorithm [3]
arxiv-6300-13 | Ensemble Detection of Single & Multiple Events at Sentence-Level | http://arxiv.org/abs/1403.6023 | author:Luís Marujo, Anatole Gershman, Jaime Carbonell, João P. Neto, David Martins de Matos category:cs.CL cs.LG published:2014-03-24 summary:Event classification at sentence level is an important Information Extractiontask with applications in several NLP, IR, and personalization systems.Multi-label binary relevance (BR) are the state-of-art methods. In this work,we explored new multi-label methods known for capturing relations between eventtypes. These new methods, such as the ensemble Chain of Classifiers, improvethe F1 on average across the 6 labels by 2.8% over the Binary Relevance. Thelow occurrence of multi-label sentences motivated the reduction of the hardimbalanced multi-label classification problem with low number of occurrences ofmultiple labels per instance to an more tractable imbalanced multiclass problemwith better results (+ 4.6%). We report the results of adding new features,such as sentiment strength, rhetorical signals, domain-id (source-id and date),and key-phrases in both single-label and multi-label event classificationscenarios.
arxiv-6300-14 | Block Motion Based Dynamic Texture Analysis: A Review | http://arxiv.org/abs/1403.5869 | author:Akhlaqur Rahman, Sumaira Tasnim category:cs.CV published:2014-03-24 summary:Dynamic texture refers to image sequences of non-rigid objects that exhibitsome regularity in their movement. Videos of smoke, fire etc. fall under thecategory of dynamic texture. Researchers have investigated different ways toanalyze dynamic textures since early nineties. Both appearance based (imageintensities) and motion based approaches are investigated. Motion basedapproaches turn out to be more effective. A group of researchers haveinvestigated ways to utilize the motion vectors readily available with theblocks in video codes like MGEG/H26X. In this paper we provide a review of thedynamic texture analysis methods using block motion. Research into dynamictexture analysis using block motion includes recognition, motion computation,segmentation, and synthesis. We provide a comprehensive review of theseapproaches.
arxiv-6300-15 | An Efficient Feature Selection in Classification of Audio Files | http://arxiv.org/abs/1404.1491 | author:Jayita Mitra, Diganta Saha category:cs.LG published:2014-03-24 summary:In this paper we have focused on an efficient feature selection method inclassification of audio files. The main objective is feature selection andextraction. We have selected a set of features for further analysis, whichrepresents the elements in feature vector. By extraction method we can computea numerical representation that can be used to characterize the audio using theexisting toolbox. In this study Gain Ratio (GR) is used as a feature selectionmeasure. GR is used to select splitting attribute which will separate thetuples into different classes. The pulse clarity is considered as a subjectivemeasure and it is used to calculate the gain of features of audio files. Thesplitting criterion is employed in the application to identify the class or themusic genre of a specific audio file from testing database. Experimentalresults indicate that by using GR the application can produce a satisfactoryresult for music genre classification. After dimensionality reduction bestthree features have been selected out of various features of audio file and inthis technique we will get more than 90% successful classification result.
arxiv-6300-16 | Bayesian calibration for forensic evidence reporting | http://arxiv.org/abs/1403.5997 | author:Niko Brümmer, Albert Swart category:stat.ML cs.LG published:2014-03-24 summary:We introduce a Bayesian solution for the problem in forensic speakerrecognition, where there may be very little background material for estimatingscore calibration parameters. We work within the Bayesian paradigm of evidencereporting and develop a principled probabilistic treatment of the problem,which results in a Bayesian likelihood-ratio as the vehicle for reportingweight of evidence. We show in contrast, that reporting a likelihood-ratiodistribution does not solve this problem. Our solution is experimentallyexercised on a simulated forensic scenario, using NIST SRE'12 scores, whichdemonstrates a clear advantage for the proposed method compared to thetraditional plugin calibration recipe.
arxiv-6300-17 | Simultaneous sparse estimation of canonical vectors in the p>>N setting | http://arxiv.org/abs/1403.6095 | author:Irina Gaynanova, James G. Booth, Martin T. Wells category:stat.ME stat.ML published:2014-03-24 summary:This article considers the problem of sparse estimation of canonical vectorsin linear discriminant analysis when $p\gg N$. Several methods have beenproposed in the literature that estimate one canonical vector in the two-groupcase. However, $G-1$ canonical vectors can be considered if the number ofgroups is $G$. In the multi-group context, it is common to estimate canonicalvectors in a sequential fashion. Moreover, separate prior estimation of thecovariance structure is often required. We propose a novel methodology fordirect estimation of canonical vectors. In contrast to existing techniques, theproposed method estimates all canonical vectors at once, performs variableselection across all the vectors and comes with theoretical guarantees on thevariable selection and classification consistency. First, we highlight the factthat in the $N>p$ setting the canonical vectors can be expressed in a closedform up to an orthogonal transformation. Secondly, we propose an extension ofthis form to the $p\gg N$ setting and achieve feature selection by using agroup penalty. The resulting optimization problem is convex and can be solvedusing a block-coordinate descent algorithm. The practical performance of themethod is evaluated through simulation studies as well as real dataapplications.
arxiv-6300-18 | Human brain distinctiveness based on EEG spectral coherence connectivity | http://arxiv.org/abs/1403.6384 | author:Daria La Rocca, Patrizio Campisi, Balazs Vegso, Peter Cserti, Gyorgy Kozmann, Fabio Babiloni, Fabrizio De Vico Fallani category:q-bio.NC stat.ML published:2014-03-23 summary:The use of EEG biometrics, for the purpose of automatic people recognition,has received increasing attention in the recent years. Most of current analysisrely on the extraction of features characterizing the activity of single brainregions, like power-spectrum estimates, thus neglecting possible temporaldependencies between the generated EEG signals. However, importantphysiological information can be extracted from the way different brain regionsare functionally coupled. In this study, we propose a novel approach that fusesspectral coherencebased connectivity between different brain regions as apossibly viable biometric feature. The proposed approach is tested on a largedataset of subjects (N=108) during eyes-closed (EC) and eyes-open (EO) restingstate conditions. The obtained recognition performances show that using brainconnectivity leads to higher distinctiveness with respect to power-spectrummeasurements, in both the experimental conditions. Notably, a 100% recognitionaccuracy is obtained in EC and EO when integrating functional connectivitybetween regions in the frontal lobe, while a lower 97.41% is obtained in EC(96.26% in EO) when fusing power spectrum information from centro-parietalregions. Taken together, these results suggest that functional connectivitypatterns represent effective features for improving EEG-based biometricsystems.
arxiv-6300-19 | CNN Features off-the-shelf: an Astounding Baseline for Recognition | http://arxiv.org/abs/1403.6382 | author:Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, Stefan Carlsson category:cs.CV published:2014-03-23 summary:Recent results indicate that the generic descriptors extracted from theconvolutional neural networks are very powerful. This paper adds to themounting evidence that this is indeed the case. We report on a series ofexperiments conducted for different recognition tasks using the publiclyavailable code and model of the \overfeat network which was trained to performobject classification on ILSVRC13. We use features extracted from the \overfeatnetwork as a generic image representation to tackle the diverse range ofrecognition tasks of object image classification, scene recognition, finegrained recognition, attribute detection and image retrieval applied to adiverse set of datasets. We selected these tasks and datasets as they graduallymove further away from the original task and data the \overfeat network wastrained to solve. Astonishingly, we report consistent superior results comparedto the highly tuned state-of-the-art systems in all the visual classificationtasks on various datasets. For instance retrieval it consistently outperformslow memory footprint methods except for sculptures dataset. The results areachieved using a linear SVM classifier (or $L2$ distance in case of retrieval)applied to a feature representation of size 4096 extracted from a layer in thenet. The representations are further modified using simple augmentationtechniques e.g. jittering. The results strongly suggest that features obtainedfrom deep learning with convolutional nets should be the primary candidate inmost visual recognition tasks.
arxiv-6300-20 | MCL-3D: a database for stereoscopic image quality assessment using 2D-image-plus-depth source | http://arxiv.org/abs/1405.1403 | author:Rui Song, Hyunsuk Ko, C. C. Jay Kuo category:cs.CV published:2014-03-23 summary:A new stereoscopic image quality assessment database rendered using the2D-image-plus-depth source, called MCL-3D, is described and the performancebenchmarking of several known 2D and 3D image quality metrics using the MCL-3Ddatabase is presented in this work. Nine image-plus-depth sources are firstselected, and a depth image-based rendering (DIBR) technique is used to renderstereoscopic image pairs. Distortions applied to either the texture image orthe depth image before stereoscopic image rendering include: Gaussian blur,additive white noise, down-sampling blur, JPEG and JPEG-2000 (JP2K) compressionand transmission error. Furthermore, the distortion caused by imperfectrendering is also examined. The MCL-3D database contains 693 stereoscopic imagepairs, where one third of them are of resolution 1024x728 and two thirds are ofresolution 1920x1080. The pair-wise comparison was adopted in the subjectivetest for user friendliness, and the Mean Opinion Score (MOS) can be computedaccordingly. Finally, we evaluate the performance of several 2D and 3D imagequality metrics applied to MCL-3D. All texture images, depth images, renderedimage pairs in MCL-3D and their MOS values obtained in the subjective test areavailable to the public (http://mcl.usc.edu/mcl-3d-database/) for futureresearch and development.
arxiv-6300-21 | SmartAnnotator: An Interactive Tool for Annotating RGBD Indoor Images | http://arxiv.org/abs/1403.5718 | author:Yu-Shiang Wong, Hung-Kuo Chu, Niloy J. Mitra category:cs.CV published:2014-03-23 summary:RGBD images with high quality annotations in the form of geometric (i.e.,segmentation) and structural (i.e., how do the segments are mutually related in3D) information provide valuable priors to a large number of scene and imagemanipulation applications. While it is now simple to acquire RGBD images,annotating them, automatically or manually, remains challenging especially incluttered noisy environments. We present SmartAnnotator, an interactive systemto facilitate annotating RGBD images. The system performs the tedious tasks ofgrouping pixels, creating potential abstracted cuboids, inferring objectinteractions in 3D, and comes up with various hypotheses. The user simply hasto flip through a list of suggestions for segment labels, finalize a selection,and the system updates the remaining hypotheses. As objects are finalized, theprocess speeds up with fewer ambiguities to resolve. Further, as more scenesare annotated, the system makes better suggestions based on structural andgeometric priors learns from the previous annotation sessions. We test oursystem on a large number of database scenes and report significant improvementsover naive low-level annotation tools.
arxiv-6300-22 | Scalable detection of statistically significant communities and hierarchies, using message-passing for modularity | http://arxiv.org/abs/1403.5787 | author:Pan Zhang, Cristopher Moore category:physics.soc-ph cs.SI stat.ML published:2014-03-23 summary:Modularity is a popular measure of community structure. However, maximizingthe modularity can lead to many competing partitions, with almost the samemodularity, that are poorly correlated with each other. It can also produceillusory "communities" in random graphs where none exist. We address thisproblem by using the modularity as a Hamiltonian at finite temperature, andusing an efficient Belief Propagation algorithm to obtain the consensus of manypartitions with high modularity, rather than looking for a single partitionthat maximizes it. We show analytically and numerically that the proposedalgorithm works all the way down to the detectability transition in networksgenerated by the stochastic block model. It also performs well on real-worldnetworks, revealing large communities in some networks where previous work hasclaimed no communities exist. Finally we show that by applying our algorithmrecursively, subdividing communities until no statistically-significantsubcommunities can be found, we can detect hierarchical structure in real-worldnetworks more efficiently than previous methods.
arxiv-6300-23 | Hierarchical Dirichlet Scaling Process | http://arxiv.org/abs/1404.1282 | author:Dongwoo Kim, Alice Oh category:cs.LG published:2014-03-22 summary:We present the \textit{hierarchical Dirichlet scaling process} (HDSP), aBayesian nonparametric mixed membership model. The HDSP generalizes thehierarchical Dirichlet process (HDP) to model the correlation structure betweenmetadata in the corpus and mixture components. We construct the HDSP based onthe normalized gamma representation of the Dirichlet process, and thisconstruction allows incorporating a scaling function that controls themembership probabilities of the mixture components. We develop two scalingmethods to demonstrate that different modeling assumptions can be expressed inthe HDSP. We also derive the corresponding approximate posterior inferencealgorithms using variational Bayes. Through experiments on datasets ofnewswire, medical journal articles, conference proceedings, and productreviews, we show that the HDSP results in a better predictive performance thanlabeled LDA, partially labeled LDA, and author topic model and a betternegative review classification performance than the supervised topic model andSVM.
arxiv-6300-24 | Firefly Monte Carlo: Exact MCMC with Subsets of Data | http://arxiv.org/abs/1403.5693 | author:Dougal Maclaurin, Ryan P. Adams category:stat.ML cs.LG stat.CO published:2014-03-22 summary:Markov chain Monte Carlo (MCMC) is a popular and successful general-purposetool for Bayesian inference. However, MCMC cannot be practically applied tolarge data sets because of the prohibitive cost of evaluating every likelihoodterm at every iteration. Here we present Firefly Monte Carlo (FlyMC) anauxiliary variable MCMC algorithm that only queries the likelihoods of apotentially small subset of the data at each iteration yet simulates from theexact posterior distribution, in contrast to recent proposals that areapproximate even in the asymptotic limit. FlyMC is compatible with a widevariety of modern MCMC algorithms, and only requires a lower bound on theper-datum likelihood factors. In experiments, we find that FlyMC generatessamples from the posterior more than an order of magnitude faster than regularMCMC, opening up MCMC methods to larger datasets than were previouslyconsidered feasible.
arxiv-6300-25 | CUR Algorithm with Incomplete Matrix Observation | http://arxiv.org/abs/1403.5647 | author:Rong Jin, Shenghuo Zhu category:cs.LG stat.ML published:2014-03-22 summary:CUR matrix decomposition is a randomized algorithm that can efficientlycompute the low rank approximation for a given rectangle matrix. One limitationwith the existing CUR algorithms is that they require an access to the fullmatrix A for computing U. In this work, we aim to alleviate this limitation. Inparticular, we assume that besides having an access to randomly sampled d rowsand d columns from A, we only observe a subset of randomly sampled entries fromA. Our goal is to develop a low rank approximation algorithm, similar to CUR,based on (i) randomly sampled rows and columns from A, and (ii) randomlysampled entries from A. The proposed algorithm is able to perfectly recover thetarget matrix A with only O(rn log n) number of observed entries. In addition,instead of having to solve an optimization problem involved trace normregularization, the proposed algorithm only needs to solve a standardregression problem. Finally, unlike most matrix completion theories that holdonly when the target matrix is of low rank, we show a strong guarantee for theproposed algorithm even when the target matrix is not low rank.
arxiv-6300-26 | Bayesian Optimization with Unknown Constraints | http://arxiv.org/abs/1403.5607 | author:Michael A. Gelbart, Jasper Snoek, Ryan P. Adams category:stat.ML cs.LG published:2014-03-22 summary:Recent work on Bayesian optimization has shown its effectiveness in globaloptimization of difficult black-box objective functions. Many real-worldoptimization problems of interest also have constraints which are unknown apriori. In this paper, we study Bayesian optimization for constrained problemsin the general case that noise may be present in the constraint functions, andthe objective and constraints may be evaluated independently. We providemotivating practical examples, and present a general framework to solve suchproblems. We demonstrate the effectiveness of our approach on optimizing theperformance of online latent Dirichlet allocation subject to topic sparsityconstraints, tuning a neural network given test-time memory constraints, andoptimizing Hamiltonian Monte Carlo to achieve maximal effectiveness in a fixedtime, subject to passing standard convergence diagnostics.
arxiv-6300-27 | Forecasting Popularity of Videos using Social Media | http://arxiv.org/abs/1403.5603 | author:Jie Xu, Mihaela van der Schaar, Jiangchuan Liu, Haitao Li category:cs.LG cs.SI published:2014-03-22 summary:This paper presents a systematic online prediction method (Social-Forecast)that is capable to accurately forecast the popularity of videos promoted bysocial media. Social-Forecast explicitly considers the dynamically changing andevolving propagation patterns of videos in social media when making popularityforecasts, thereby being situation and context aware. Social-Forecast aims tomaximize the forecast reward, which is defined as a tradeoff between thepopularity prediction accuracy and the timeliness with which a prediction isissued. The forecasting is performed online and requires no training phase or apriori knowledge. We analytically bound the prediction performance loss ofSocial-Forecast as compared to that obtained by an omniscient oracle and provethat the bound is sublinear in the number of video arrivals, therebyguaranteeing its short-term performance as well as its asymptotic convergenceto the optimal performance. In addition, we conduct extensive experiments usingreal-world data traces collected from the videos shared in RenRen, one of thelargest online social networks in China. These experiments show that ourproposed method outperforms existing view-based approaches for popularityprediction (which are not context-aware) by more than 30% in terms ofprediction rewards.
arxiv-6300-28 | A Lemma Based Evaluator for Semitic Language Text Summarization Systems | http://arxiv.org/abs/1403.5596 | author:Tarek El-Shishtawy, Fatma El-Ghannam category:cs.CL cs.IR published:2014-03-22 summary:Matching texts in highly inflected languages such as Arabic by simplestemming strategy is unlikely to perform well. In this paper, we present astrategy for automatic text matching technique for for inflectional languages,using Arabic as the test case. The system is an extension of ROUGE test inwhich texts are matched on token's lemma level. The experimental results showan enhancement of detecting similarities between different sentences havingsame semantics but written in different lexical forms..
arxiv-6300-29 | Continuous Optimization for Fields of Experts Denoising Works | http://arxiv.org/abs/1403.5590 | author:Petter Strandmark, Sameer Agarwal category:cs.CV published:2014-03-21 summary:Several recent papers use image denoising with a Fields of Experts prior tobenchmark discrete optimization methods. We show that a non-linear leastsquares solver significantly outperforms all known discrete methods on thisproblem.
arxiv-6300-30 | Model-Driven Applications of Fractional Derivatives and Integrals | http://arxiv.org/abs/1405.1999 | author:William A. Sethares, Selçuk Ş. Bayın category:cs.CV published:2014-03-21 summary:Fractional order derivatives and integrals (differintegrals) are viewed froma frequency-domain perspective using the formalism of Riesz, providing acomputational tool as well as a way to interpret the operations in thefrequency domain. Differintegrals provide a logical extension of currenttechniques, generalizing the notion of integral and differential operators andacting as kind of frequency-domain filtering that has many of the advantages ofa nonlocal linear operator. Several important properties of differintegrals arepresented, and sample applications are given to one- and two-dimensionalsignals. Computer code to carry out the computations is made available on theauthor's website.
arxiv-6300-31 | Using n-grams models for visual semantic place recognition | http://arxiv.org/abs/1403.5370 | author:Mathieu Dubois, Frenoux Emmanuelle, Philippe Tarroux category:stat.ML cs.CV cs.LG published:2014-03-21 summary:The aim of this paper is to present a new method for visual placerecognition. Our system combines global image characterization and visualwords, which allows to use efficient Bayesian filtering methods to integrateseveral images. More precisely, we extend the classical HMM model withtechniques inspired by the field of Natural Language Processing. This paperpresents our system and the Bayesian filtering algorithm. The performance ofour system and the influence of the main parameters are evaluated on a standarddatabase. The discussion highlights the interest of using such models andproposes improvements.
arxiv-6300-32 | A Physarum-Inspired Approach to Optimal Supply Chain Network Design at Minimum Total Cost with Demand Satisfaction | http://arxiv.org/abs/1403.5345 | author:Xiaoge Zhang, Andrew Adamatzky, Xin-She Yang, Hai Yang, Sankaran Mahadevan, Yong Deng category:cs.NE published:2014-03-21 summary:A supply chain is a system which moves products from a supplier to customers.The supply chains are ubiquitous. They play a key role in all economicactivities. Inspired by biological principles of nutrients' distribution inprotoplasmic networks of slime mould Physarum polycephalum we propose a novelalgorithm for a supply chain design. The algorithm handles the supply networkswhere capacity investments and product flows are variables. The networks areconstrained by a need to satisfy product demands. Two features of the slimemould are adopted in our algorithm. The first is the continuity of a fluxduring the iterative process, which is used in real-time update of the costsassociated with the supply links. The second feature is adaptivity. The supplychain can converge to an equilibrium state when costs are changed. Practicalityand flexibility of our algorithm is illustrated on numerical examples.
arxiv-6300-33 | The quasispecies regime for the simple genetic algorithm with ranking selection | http://arxiv.org/abs/1403.5427 | author:Raphaël Cerf category:math.PR cs.NE published:2014-03-21 summary:We study the simple genetic algorithm with a ranking selection mechanism(linear ranking or tournament). We denote by $\ell$ the length of thechromosomes, by $m$ the population size, by $p_C$ the crossover probability andby $p_M$ the mutation probability. We introduce a parameter $\sigma$, calledthe selection drift, which measures the selection intensity of the fittestchromosome. We show that the dynamics of the genetic algorithm depend in acritical way on the parameter $$\pi \,=\,\sigma(1-p_C)(1-p_M)^\ell\,.$$ If$\pi<1$, then the genetic algorithm operates in a disordered regime: anadvantageous mutant disappears with probability larger than $1-1/m^\beta$,where $\beta$ is a positive exponent. If $\pi>1$, then the genetic algorithmoperates in a quasispecies regime: an advantageous mutant invades a positivefraction of the population with probability larger than a constant $p^*$ (whichdoes not depend on $m$). We estimate next the probability of the occurrence ofa catastrophe (the whole population falls below a fitness level which waspreviously reached by a positive fraction of the population). The asymptoticresults suggest the following rules: $\pi=\sigma(1-p_C)(1-p_M)^\ell$ should beslightly larger than $1$; $p_M$ should be of order $1/\ell$; $m$ should belarger than $\ell\ln\ell$; the running time should be of exponential order in$m$. The first condition requires that $ \ell p_M +p_C< \ln\sigma$. Theseconclusions must be taken with great care: they come from an asymptotic regime,and it is a formidable task to understand the relevance of this regime for areal-world problem. At least, we hope that these conclusions provideinteresting guidelines for the practical implementation of the simple geneticalgorithm.
arxiv-6300-34 | Missing Data Prediction and Classification: The Use of Auto-Associative Neural Networks and Optimization Algorithms | http://arxiv.org/abs/1403.5488 | author:Collins Leke, Bhekisipho Twala, T. Marwala category:cs.NE cs.LG published:2014-03-21 summary:This paper presents methods which are aimed at finding approximations tomissing data in a dataset by using optimization algorithms to optimize thenetwork parameters after which prediction and classification tasks can beperformed. The optimization methods that are considered are genetic algorithm(GA), simulated annealing (SA), particle swarm optimization (PSO), randomforest (RF) and negative selection (NS) and these methods are individually usedin combination with auto-associative neural networks (AANN) for missing dataestimation and the results obtained are compared. The methods suggested use theoptimization algorithms to minimize an error function derived from training theauto-associative neural network during which the interrelationships between theinputs and the outputs are obtained and stored in the weights connecting thedifferent layers of the network. The error function is expressed as the squareof the difference between the actual observations and predicted values from anauto-associative neural network. In the event of missing data, all the valuesof the actual observations are not known hence, the error function isdecomposed to depend on the known and unknown variable values. Multi-layerperceptron (MLP) neural network is employed to train the neural networks usingthe scaled conjugate gradient (SCG) method. Prediction accuracy is determinedby mean squared error (MSE), root mean squared error (RMSE), mean absoluteerror (MAE), and correlation coefficient (r) computations. Accuracy inclassification is obtained by plotting ROC curves and calculating the areasunder these. Analysis of results depicts that the approach using RF with AANNproduces the most accurate predictions and classifications while on the otherend of the scale is the approach which entails using NS with AANN.
arxiv-6300-35 | An efficiency dependency parser using hybrid approach for tamil language | http://arxiv.org/abs/1403.6381 | author:K. Sureka, K. G. Srinivasagan, S. Suganthi category:cs.CL published:2014-03-21 summary:Natural language processing is a prompt research area across the country.Parsing is one of the very crucial tool in language analysis system which aimsto forecast the structural relationship among the words in a given sentence.Many researchers have already developed so many language tools but the accuracyis not meet out the human expectation level, thus the research is still exists.Machine translation is one of the major application area under Natural LanguageProcessing. While translation between one language to another language, thestructure identification of a sentence play a key role. This paper introducesthe hybrid way to solve the identification of relationship among the givenwords in a sentence. In existing system is implemented using rule basedapproach, which is not suited in huge amount of data. The machine learningapproaches is suitable for handle larger amount of data and also to get betteraccuracy via learning and training the system. The proposed approach takes aTamil sentence as an input and produce the result of a dependency relation as atree like structure using hybrid approach. This proposed tool is very helpfulfor researchers and act as an odd-on improve the quality of existingapproaches.
arxiv-6300-36 | An Information-Theoretic Analysis of Thompson Sampling | http://arxiv.org/abs/1403.5341 | author:Daniel Russo, Benjamin Van Roy category:cs.LG published:2014-03-21 summary:We provide an information-theoretic analysis of Thompson sampling thatapplies across a broad range of online optimization problems in which adecision-maker must learn from partial feedback. This analysis inherits thesimplicity and elegance of information theory and leads to regret bounds thatscale with the entropy of the optimal-action distribution. This strengthenspreexisting results and yields new insight into how information improvesperformance.
arxiv-6300-37 | A Non-Local Structure Tensor Based Approach for Multicomponent Image Recovery Problems | http://arxiv.org/abs/1403.5403 | author:Giovanni Chierchia, Nelly Pustelnik, Beatrice Pesquet-Popescu, Jean-Christophe Pesquet category:cs.CV cs.NA math.OC published:2014-03-21 summary:Non-Local Total Variation (NLTV) has emerged as a useful tool in variationalmethods for image recovery problems. In this paper, we extend the NLTV-basedregularization to multicomponent images by taking advantage of the StructureTensor (ST) resulting from the gradient of a multicomponent image. The proposedapproach allows us to penalize the non-local variations, jointly for thedifferent components, through various $\ell_{1,p}$ matrix norms with $p \ge 1$.To facilitate the choice of the hyper-parameters, we adopt a constrained convexoptimization approach in which we minimize the data fidelity term subject to aconstraint involving the ST-NLTV regularization. The resulting convexoptimization problem is solved with a novel epigraphical projection method.This formulation can be efficiently implemented thanks to the flexibilityoffered by recent primal-dual proximal algorithms. Experiments are carried outfor multispectral and hyperspectral images. The results demonstrate theinterest of introducing a non-local structure tensor regularization and showthat the proposed approach leads to significant improvements in terms ofconvergence speed over current state-of-the-art methods.
arxiv-6300-38 | Learning to Optimize via Information-Directed Sampling | http://arxiv.org/abs/1403.5556 | author:Daniel Russo, Benjamin Van Roy category:cs.LG published:2014-03-21 summary:We propose information-directed sampling -- a new algorithm for onlineoptimization problems in which a decision-maker must balance betweenexploration and exploitation while learning from partial feedback. Each actionis sampled in a manner that minimizes the ratio between squared expectedsingle-period regret and a measure of information gain: the mutual informationbetween the optimal action and the next observation. We establish an expected regret bound for information-directed sampling thatapplies across a very general class of models and scales with the entropy ofthe optimal action distribution. For the widely studied Bernoulli, Gaussian,and linear bandit problems, we demonstrate simulation performance surpassingpopular approaches, including upper confidence bound algorithms, Thompsonsampling, and the knowledge gradient algorithm. Further, we present simpleanalytic examples illustrating that, due to the way it measures informationgain, information-directed sampling can dramatically outperform upperconfidence bound algorithms and Thompson sampling.
arxiv-6300-39 | On The Sample Complexity of Sparse Dictionary Learning | http://arxiv.org/abs/1403.5112 | author:Matthias Seibert, Martin Kleinsteuber, Rémi Gribonval, Rodolphe Jenatton, Francis Bach category:stat.ML published:2014-03-20 summary:In the synthesis model signals are represented as a sparse combinations ofatoms from a dictionary. Dictionary learning describes the acquisition processof the underlying dictionary for a given set of training samples. While ideallythis would be achieved by optimizing the expectation of the factors over theunderlying distribution of the training data, in practice the necessaryinformation about the distribution is not available. Therefore, in real worldapplications it is achieved by minimizing an empirical average over theavailable samples. The main goal of this paper is to provide a samplecomplexity estimate that controls to what extent the empirical average deviatesfrom the cost function. This estimate then provides a suitable estimate to theaccuracy of the representation of the learned dictionary. The presentedapproach exemplifies the general results proposed by the authors in SampleComplexity of Dictionary Learning and other Matrix Factorizations, Gribonval etal. and gives more concrete bounds of the sample complexity of dictionarylearning. We cover a variety of sparsity measures employed in the learningprocedure.
arxiv-6300-40 | Network-based Isoform Quantification with RNA-Seq Data for Cancer Transcriptome Analysis | http://arxiv.org/abs/1403.5029 | author:Wei Zhang, Jae-Woong Chang, Lilong Lin, Kay Minn, Baolin Wu, Jeremy Chien, Jeongsik Yong, Hui Zheng, Rui Kuang category:cs.CE cs.AI cs.LG published:2014-03-20 summary:High-throughput mRNA sequencing (RNA-Seq) is widely used for transcriptquantification of gene isoforms. Since RNA-Seq data alone is often notsufficient to accurately identify the read origins from the isoforms forquantification, we propose to explore protein domain-domain interactions asprior knowledge for integrative analysis with RNA-seq data. We introduce aNetwork-based method for RNA-Seq-based Transcript Quantification (Net-RSTQ) tointegrate protein domain-domain interaction network with short read alignmentsfor transcript abundance estimation. Based on our observation that theabundances of the neighboring isoforms by domain-domain interactions in thenetwork are positively correlated, Net-RSTQ models the expression of theneighboring transcripts as Dirichlet priors on the likelihood of the observedread alignments against the transcripts in one gene. The transcript abundancesof all the genes are then jointly estimated with alternating optimization ofmultiple EM problems. In simulation Net-RSTQ effectively improved isoformtranscript quantifications when isoform co-expressions correlate with theirinteractions. qRT-PCR results on 25 multi-isoform genes in a stem cell line, anovarian cancer cell line, and a breast cancer cell line also showed thatNet-RSTQ estimated more consistent isoform proportions with RNA-Seq data. Inthe experiments on the RNA-Seq data in The Cancer Genome Atlas (TCGA), thetranscript abundances estimated by Net-RSTQ are more informative for patientsample classification of ovarian cancer, breast cancer and lung cancer. Allexperimental results collectively support that Net-RSTQ is a promising approachfor isoform quantification.
arxiv-6300-41 | Unconfused Ultraconservative Multiclass Algorithms | http://arxiv.org/abs/1403.5115 | author:Ugo Louche, Liva Ralaivola category:cs.LG published:2014-03-20 summary:We tackle the problem of learning linear classifiers from noisy datasets in amulticlass setting. The two-class version of this problem was studied a fewyears ago by, e.g. Bylander (1994) and Blum et al. (1996): in thesecontributions, the proposed approaches to fight the noise revolve around aPerceptron learning scheme fed with peculiar examples computed through aweighted average of points from the noisy training set. We propose to buildupon these approaches and we introduce a new algorithm called UMA (forUnconfused Multiclass additive Algorithm) which may be seen as a generalizationto the multiclass setting of the previous approaches. In order to characterizethe noise we use the confusion matrix as a multiclass extension of theclassification noise studied in the aforementioned literature. Theoreticallywell-founded, UMA furthermore displays very good empirical noise robustness, asevidenced by numerical simulations conducted on both synthetic and real data.Keywords: Multiclass classification, Perceptron, Noisy labels, Confusion Matrix
arxiv-6300-42 | Review of Face Detection Systems Based Artificial Neural Networks Algorithms | http://arxiv.org/abs/1404.1292 | author:Omaima N. A. AL-Allaf category:cs.CV cs.NE published:2014-03-20 summary:Face detection is one of the most relevant applications of image processingand biometric systems. Artificial neural networks (ANN) have been used in thefield of image processing and pattern recognition. There is lack of literaturesurveys which give overview about the studies and researches related to theusing of ANN in face detection. Therefore, this research includes a generalreview of face detection studies and systems which based on different ANNapproaches and algorithms. The strengths and limitations of these literaturestudies and systems were included also.
arxiv-6300-43 | Online Local Learning via Semidefinite Programming | http://arxiv.org/abs/1403.5287 | author:Paul Christiano category:cs.LG published:2014-03-20 summary:In many online learning problems we are interested in predicting localinformation about some universe of items. For example, we may want to knowwhether two items are in the same cluster rather than computing an assignmentof items to clusters; we may want to know which of two teams will win a gamerather than computing a ranking of teams. Although finding the optimalclustering or ranking is typically intractable, it may be possible to predictthe relationships between items as well as if you could solve the globaloptimization problem exactly. Formally, we consider an online learning problem in which a learnerrepeatedly guesses a pair of labels (l(x), l(y)) and receives an adversarialpayoff depending on those labels. The learner's goal is to receive a payoffnearly as good as the best fixed labeling of the items. We show that a simplealgorithm based on semidefinite programming can obtain asymptotically optimalregret in the case where the number of possible labels is O(1), resolving anopen problem posed by Hazan, Kale, and Shalev-Schwartz. Our main technicalcontribution is a novel use and analysis of the log determinant regularizer,exploiting the observation that log det(A + I) upper bounds the entropy of anydistribution with covariance matrix A.
arxiv-6300-44 | Sparse Learning over Infinite Subgraph Features | http://arxiv.org/abs/1403.5177 | author:Ichigaku Takigawa, Hiroshi Mamitsuka category:stat.ML published:2014-03-20 summary:We present a supervised-learning algorithm from graph data (a set of graphs)for arbitrary twice-differentiable loss functions and sparse linear models overall possible subgraph features. To date, it has been shown that under allpossible subgraph features, several types of sparse learning, such as Adaboost,LPBoost, LARS/LASSO, and sparse PLS regression, can be performed. Particularlyemphasis is placed on simultaneous learning of relevant features from aninfinite set of candidates. We first generalize techniques used in all thesepreceding studies to derive an unifying bounding technique for arbitraryseparable functions. We then carefully use this bounding to make blockcoordinate gradient descent feasible over infinite subgraph features, resultingin a fast converging algorithm that can solve a wider class of sparse learningproblems over graph data. We also empirically study the differences from theexisting approaches in convergence property, selected subgraph features, andsearch-space sizes. We further discuss several unnoticed issues in sparselearning over all possible subgraph features.
arxiv-6300-45 | Matroid Bandits: Fast Combinatorial Optimization with Learning | http://arxiv.org/abs/1403.5045 | author:Branislav Kveton, Zheng Wen, Azin Ashkan, Hoda Eydgahi, Brian Eriksson category:cs.LG cs.AI cs.SY stat.ML published:2014-03-20 summary:A matroid is a notion of independence in combinatorial optimization which isclosely related to computational efficiency. In particular, it is well knownthat the maximum of a constrained modular function can be found greedily if andonly if the constraints are associated with a matroid. In this paper, we bringtogether the ideas of bandits and matroids, and propose a new class ofcombinatorial bandits, matroid bandits. The objective in these problems is tolearn how to maximize a modular function on a matroid. This function isstochastic and initially unknown. We propose a practical algorithm for solvingour problem, Optimistic Matroid Maximization (OMM); and prove two upper bounds,gap-dependent and gap-free, on its regret. Both bounds are sublinear in timeand at most linear in all other quantities of interest. The gap-dependent upperbound is tight and we prove a matching lower bound on a partition matroidbandit. Finally, we evaluate our method on three real-world problems and showthat it is practical.
arxiv-6300-46 | Clinical TempEval | http://arxiv.org/abs/1403.4928 | author:Steven Bethard, Leon Derczynski, James Pustejovsky, Marc Verhagen category:cs.CL published:2014-03-19 summary:We describe the Clinical TempEval task which is currently in preparation forthe SemEval-2015 evaluation exercise. This task involves identifying anddescribing events, times and the relations between them in clinical text. Sixdiscrete subtasks are included, focusing on recognising mentions of times andevents, describing those mentions for both entity types, identifying therelation between an event and the document creation time, and identifyingnarrative container relations.
arxiv-6300-47 | Using Entropy Estimates for DAG-Based Ontologies | http://arxiv.org/abs/1403.4887 | author:Andrew Warren, Joao Setubal category:cs.CL published:2014-03-19 summary:Motivation: Entropy measurements on hierarchical structures have been used inmethods for information retrieval and natural language modeling. Here weexplore its application to semantic similarity. By finding shared ontologyterms, semantic similarity can be established between annotated genes. A commonprocedure for establishing semantic similarity is to calculate thedescriptiveness (information content) of ontology terms and use these values todetermine the similarity of annotations. Most often information content iscalculated for an ontology term by analyzing its frequency in an annotationcorpus. The inherent problems in using these values to model functionalsimilarity motivates our work. Summary: We present a novel calculation forestablishing the entropy of a DAG-based ontology, which can be used in analternative method for establishing the information content of its terms. Wealso compare our IC metric to two others using semantic and sequencesimilarity.
arxiv-6300-48 | Evolutionary Algorithm for Drug Discovery Interim Design Report | http://arxiv.org/abs/1403.4871 | author:Mark Shackelford category:cs.NE cs.CE published:2014-03-19 summary:A software program which aims to provide an exploration capability over theSearch Space of potential drug molecules. The program explores the search spaceby generating random molecules, determining their fitness and then breeding anew generation from the fittest individuals. The search space, in theory anycombination of any elements in any order, is constrained by the use of a subsetof elements and a list of fragments, molecular parts that are known to beuseful in drug development. The resultant molecules from each generation arestored in a searchable database, so that the user can browse through previousgenerations looking for interesting molecules.
arxiv-6300-49 | A Hierarchical Graphical Model for Big Inverse Covariance Estimation with an Application to fMRI | http://arxiv.org/abs/1403.4698 | author:Xi Luo category:stat.ME stat.AP stat.ML published:2014-03-19 summary:Brain networks has attracted the interests of many neuroscientists. Fromfunctional MRI (fMRI) data, statistical tools have been developed to recoverbrain networks. However, the dimensionality of whole-brain fMRI, usually inhundreds of thousands, challenges the applicability of these methods. Wedevelop a hierarchical graphical model (HGM) to remediate this difficulty. Thismodel introduces a hidden layer of networks based on sparse Gaussian graphicalmodels, and the observed data are sampled from individual network nodes. InfMRI, the network layer models the underlying signals of different brainfunctional units, and how these units directly interact with each other. Theintroduction of this hierarchical structure not only provides a formal andinterpretable approach, but also enables efficient computation for inferringbig networks with hundreds of thousands of nodes. Based on the conditionalconvexity of our formulation, we develop an alternating update algorithm tocompute the HGM model parameters simultaneously. The effectiveness of thisapproach is demonstrated on simulated data and a real dataset from a stop/gofMRI experiment.
arxiv-6300-50 | A Split-and-Merge Dictionary Learning Algorithm for Sparse Representation | http://arxiv.org/abs/1403.4781 | author:Subhadip Mukherjee, Chandra Sekhar Seelamantula category:cs.LG stat.ML published:2014-03-19 summary:In big data image/video analytics, we encounter the problem of learning anovercomplete dictionary for sparse representation from a large trainingdataset, which can not be processed at once because of storage andcomputational constraints. To tackle the problem of dictionary learning in suchscenarios, we propose an algorithm for parallel dictionary learning. Thefundamental idea behind the algorithm is to learn a sparse representation intwo phases. In the first phase, the whole training dataset is partitioned intosmall non-overlapping subsets, and a dictionary is trained independently oneach small database. In the second phase, the dictionaries are merged to form aglobal dictionary. We show that the proposed algorithm is efficient in itsusage of memory and computational complexity, and performs on par with thestandard learning strategy operating on the entire data at a time. As anapplication, we consider the problem of image denoising. We present acomparative analysis of our algorithm with the standard learning techniques,that use the entire database at a time, in terms of training and denoisingperformance. We observe that the split-and-merge algorithm results in aremarkable reduction of training time, without significantly affecting thedenoising performance.
arxiv-6300-51 | Structured Sparse Method for Hyperspectral Unmixing | http://arxiv.org/abs/1403.4682 | author:Feiyun Zhu, Ying Wang, Shiming Xiang, Bin Fan, Chunhong Pan category:cs.CV cs.AI published:2014-03-19 summary:Hyperspectral Unmixing (HU) has received increasing attention in the pastdecades due to its ability of unveiling information latent in hyperspectraldata. Unfortunately, most existing methods fail to take advantage of thespatial information in data. To overcome this limitation, we propose aStructured Sparse regularized Nonnegative Matrix Factorization (SS-NMF) methodfrom the following two aspects. First, we incorporate a graph Laplacian toencode the manifold structures embedded in the hyperspectral data space. Inthis way, the highly similar neighboring pixels can be grouped together.Second, the lasso penalty is employed in SS-NMF for the fact that pixels in thesame manifold structure are sparsely mixed by a common set of relevant bases.These two factors act as a new structured sparse constraint. With thisconstraint, our method can learn a compact space, where highly similar pixelsare grouped to share correlated sparse representations. Experiments on realhyperspectral data sets with different noise levels demonstrate that our methodoutperforms the state-of-the-art methods significantly.
arxiv-6300-52 | A Proximal Stochastic Gradient Method with Progressive Variance Reduction | http://arxiv.org/abs/1403.4699 | author:Lin Xiao, Tong Zhang category:math.OC stat.ML published:2014-03-19 summary:We consider the problem of minimizing the sum of two convex functions: one isthe average of a large number of smooth component functions, and the other is ageneral convex function that admits a simple proximal mapping. We assume thewhole objective function is strongly convex. Such problems often arise inmachine learning, known as regularized empirical risk minimization. We proposeand analyze a new proximal stochastic gradient method, which uses a multi-stagescheme to progressively reduce the variance of the stochastic gradient. Whileeach iteration of this algorithm has similar cost as the classical stochasticgradient method (or incremental gradient method), we show that the expectedobjective value converges to the optimum at a geometric rate. The overallcomplexity of this method is much lower than both the proximal full gradientmethod and the standard proximal stochastic gradient method.
arxiv-6300-53 | Study on performance improvement of oil paint image filter algorithm using parallel pattern library | http://arxiv.org/abs/1405.1020 | author:Siddhartha Mukherjee category:cs.CV cs.DC published:2014-03-19 summary:This paper gives a detailed study on the performance of oil paint imagefilter algorithm with various parameters applied on an image of RGB model. OilPaint image processing, being very performance hungry, current research triesto find improvement using parallel pattern library. With increasingkernel-size, the processing time of oil paint image filter algorithm increasesexponentially.
arxiv-6300-54 | Spelling Error Trends and Patterns in Sindhi | http://arxiv.org/abs/1403.4759 | author:Zeeshan Bhatti, Imdad Ali Ismaili, Asad Ali Shaikh, Waseem Javaid category:cs.CL published:2014-03-19 summary:Statistical error Correction technique is the most accurate and widely usedapproach today, but for a language like Sindhi which is a low resourcedlanguage the trained corpora's are not available, so the statistical techniquesare not possible at all. Instead a useful alternative would be to exploitvarious spelling error trends in Sindhi by using a Rule based approach. Fordesigning such technique an essential prerequisite would be to study thevarious error patterns in a language. This pa per presents various studies ofspelling error trends and their types in Sindhi Language. The research showsthat the error trends common to all languages are also encountered in Sindhibut their do exist some error patters that are catered specifically to a Sindhilanguage.
arxiv-6300-55 | MFCC based Enlargement of the Training Set for Emotion Recognition in Speech | http://arxiv.org/abs/1403.4777 | author:Inma Mohino-Herranz, Roberto Gil-Pita, Sagrario Alonso-Diaz, Manuel Rosa-Zurera category:cs.CV published:2014-03-19 summary:Emotional state recognition through speech is being a very interestingresearch topic nowadays. Using subliminal information of speech, denominated asprosody, it is possible to recognize the emotional state of the person. One ofthe main problems in the design of automatic emotion recognition systems is thesmall number of available patterns. This fact makes the learning process moredifficult, due to the generalization problems that arise under theseconditions. In this work we propose a solution to this problem consisting inenlarging the training set through the creation the new virtual patterns. Inthe case of emotional speech, most of the emotional information is included inspeed and pitch variations. So, a change in the average pitch that does notmodify neither the speed nor the pitch variations does not affect the expressedemotion. Thus, we use this prior information in order to create new patternsapplying a gender dependent pitch shift modification in the feature extractionprocess of the classification system. For this purpose, we propose a frequencyscaling modification of the Mel Frequency Cepstral Coefficients, used toclassify the emotion. For this purpose, we propose a gender dependent frequencyscaling modification. This proposed process allows us to synthetically increasethe number of available patterns in the training set, thus increasing thegeneralization capability of the system and reducing the test error. Resultscarried out with two different classifiers with different degree ofgeneralization capability demonstrate the suitability of the proposal.
arxiv-6300-56 | Spectral Clustering with Jensen-type kernels and their multi-point extensions | http://arxiv.org/abs/1403.4378 | author:Debarghya Ghoshdastidar, Ambedkar Dukkipati, Ajay P. Adsul, Aparna S. Vijayan category:cs.LG published:2014-03-18 summary:Motivated by multi-distribution divergences, which originate in informationtheory, we propose a notion of `multi-point' kernels, and study theirapplications. We study a class of kernels based on Jensen type divergences andshow that these can be extended to measure similarity among multiple points. Westudy tensor flattening methods and develop a multi-point (kernel) spectralclustering (MSC) method. We further emphasize on a special case of the proposedkernels, which is a multi-point extension of the linear (dot-product) kerneland show the existence of cubic time tensor flattening algorithm in this case.Finally, we illustrate the usefulness of our contributions using standard datasets and image segmentation tasks.
arxiv-6300-57 | Bayesian Source Separation Applied to Identifying Complex Organic Molecules in Space | http://arxiv.org/abs/1403.4626 | author:Kevin H. Knuth, Man Kit Tse, Joshua Choinsky, Haley A. Maunu, Duane F. Carbon category:astro-ph.IM stat.ML published:2014-03-18 summary:Emission from a class of benzene-based molecules known as Polycyclic AromaticHydrocarbons (PAHs) dominates the infrared spectrum of star-forming regions.The observed emission appears to arise from the combined emission of numerousPAH species, each with its unique spectrum. Linear superposition of the PAHspectra identifies this problem as a source separation problem. It is, however,of a formidable class of source separation problems given that different PAHsources potentially number in the hundreds, even thousands, and there is onlyone measured spectral signal for a given astrophysical site. Fortunately, thesource spectra of the PAHs are known, but the signal is also contaminated byother spectral sources. We describe our ongoing work in developing Bayesiansource separation techniques relying on nested sampling in conjunction with anON/OFF mechanism enabling simultaneous estimation of the probability that aparticular PAH species is present and its contribution to the spectrum.
arxiv-6300-58 | Can Cascades be Predicted? | http://arxiv.org/abs/1403.4608 | author:Justin Cheng, Lada A. Adamic, P. Alex Dow, Jon Kleinberg, Jure Leskovec category:cs.SI physics.soc-ph stat.ML H.2.8 published:2014-03-18 summary:On many social networking web sites such as Facebook and Twitter, resharingor reposting functionality allows users to share others' content with their ownfriends or followers. As content is reshared from user to user, large cascadesof reshares can form. While a growing body of research has focused on analyzingand characterizing such cascades, a recent, parallel line of work has arguedthat the future trajectory of a cascade may be inherently unpredictable. Inthis work, we develop a framework for addressing cascade prediction problems.On a large sample of photo reshare cascades on Facebook, we find strongperformance in predicting whether a cascade will continue to grow in thefuture. We find that the relative growth of a cascade becomes more predictableas we observe more of its reshares, that temporal and structural features arekey predictors of cascade size, and that initially, breadth, rather than depthin a cascade is a better indicator of larger cascades. This predictionperformance is robust in the sense that multiple distinct classes of featuresall achieve similar performance. We also discover that temporal features arepredictive of a cascade's eventual shape. Observing independent cascades of thesame content, we find that while these cascades differ greatly in size, we arestill able to predict which ends up the largest.
arxiv-6300-59 | Communication Communities in MOOCs | http://arxiv.org/abs/1403.4640 | author:Nabeel Gillani, Rebecca Eynon, Michael Osborne, Isis Hjorth, Stephen Roberts category:cs.CY cs.SI stat.ML published:2014-03-18 summary:Massive Open Online Courses (MOOCs) bring together thousands of people fromdifferent geographies and demographic backgrounds -- but to date, little isknown about how they learn or communicate. We introduce a new content-analysedMOOC dataset and use Bayesian Non-negative Matrix Factorization (BNMF) toextract communities of learners based on the nature of their online forumposts. We see that BNMF yields a superior probabilistic generative model foronline discussions when compared to other models, and that the communities itlearns are differentiated by their composite students' demographic and courseperformance indicators. These findings suggest that computationally efficientprobabilistic generative modelling of MOOCs can reveal important insights foreducational researchers and practitioners and help to develop more intelligentand responsive online learning environments.
arxiv-6300-60 | Simultaneous Perturbation Algorithms for Batch Off-Policy Search | http://arxiv.org/abs/1403.4514 | author:Raphael Fonteneau, L. A. Prashanth category:math.OC cs.LG published:2014-03-18 summary:We propose novel policy search algorithms in the context of off-policy, batchmode reinforcement learning (RL) with continuous state and action spaces. Givena batch collection of trajectories, we perform off-line policy evaluation usingan algorithm similar to that by [Fonteneau et al., 2010]. Using thisMonte-Carlo like policy evaluator, we perform policy search in a class ofparameterized policies. We propose both first order policy gradient and secondorder policy Newton algorithms. All our algorithms incorporate simultaneousperturbation estimates for the gradient as well as the Hessian of thecost-to-go vector, since the latter is unknown and only biased estimates areavailable. We demonstrate their practicality on a simple 1-dimensionalcontinuous state space problem.
arxiv-6300-61 | A hybrid formalism to parse Sign Languages | http://arxiv.org/abs/1403.4467 | author:Rémi Dubot, Christophe Collet category:cs.CL published:2014-03-18 summary:Sign Language (SL) linguistic is dependent on the expensive task ofannotating. Some automation is already available for low-level information (eg.body part tracking) and the lexical level has shown significant progresses. Thesyntactic level lacks annotated corpora as well as complete and consistentmodels. This article presents a solution for the automatic annotation of SLsyntactic elements. It exposes a formalism able to represent bothconstituency-based and dependency-based models. The first enable therepresentation the structures one may want to annotate, the second aims atfulfilling the holes of the first. A parser is presented and used to conducttwo experiments on the solution. One experiment is on a real corpus, the otheris on a synthetic corpus.
arxiv-6300-62 | Similarity networks for classification: a case study in the Horse Colic problem | http://arxiv.org/abs/1403.4540 | author:Lluís Belanche, Jerónimo Hernández category:cs.LG cs.NE published:2014-03-18 summary:This paper develops a two-layer neural network in which the neuron modelcomputes a user-defined similarity function between inputs and weights. Theneuron transfer function is formed by composition of an adapted logisticfunction with the mean of the partial input-weight similarities. The resultingneuron model is capable of dealing directly with variables of potentiallydifferent nature (continuous, fuzzy, ordinal, categorical). There is alsoprovision for missing values. The network is trained using a two-stageprocedure very similar to that used to train a radial basis function (RBF)neural network. The network is compared to two types of RBF networks in anon-trivial dataset: the Horse Colic problem, taken as a case study andanalyzed in detail.
arxiv-6300-63 | Sign Language Gibberish for syntactic parsing evaluation | http://arxiv.org/abs/1403.4473 | author:Rémi Dubot, Christophe Collet category:cs.CL published:2014-03-18 summary:Sign Language (SL) automatic processing slowly progresses bottom-up. Thefield has seen proposition to handle the video signal, to recognize andsynthesize sublexical and lexical units. It starts to see the development ofsupra-lexical processing. But the recognition, at this level, lacks data. Thesyntax of SL appears very specific as it uses massively the multiplicity ofarticulators and its access to the spatial dimensions. Therefore new parsingtechniques are developed. However these need to be evaluated. The shortage onreal data restrains the corpus-based models to small sizes. We propose here asolution to produce data-sets for the evaluation of parsers on the specificproperties of SL. The article first describes the general model used togenerates dependency grammars and the phrase generation from these lasts. Itthen discusses the limits of approach. The solution shows to be of particularinterest to evaluate the scalability of the techniques on big models.
arxiv-6300-64 | On the Sensitivity of the Lasso to the Number of Predictor Variables | http://arxiv.org/abs/1403.4544 | author:Cheryl J. Flynn, Clifford M. Hurvich, Jeffrey S. Simonoff category:stat.ML published:2014-03-18 summary:The Lasso is a computationally efficient procedure that can produce sparseestimators when the number of predictors (p) is large. Oracle inequalitiesprovide probability loss bounds for the Lasso estimator at a deterministicchoice of the regularization parameter. These bounds tend to zero if p isappropriately controlled, and are thus commonly cited as theoreticaljustification for the Lasso and its ability to handle high-dimensionalsettings. Unfortunately, in practice the regularization parameter is notselected to be a deterministic quantity, but is instead chosen using a random,data-dependent procedure. To address this shortcoming of previous theoreticalwork, we study the loss of the Lasso estimator when tuned optimally forprediction. Assuming orthonormal predictors and a sparse true model, we provethat the probability that the best possible predictive performance of the Lassodeteriorates as p increases can be arbitrarily close to one given asufficiently high signal to noise ratio and sufficiently large p. We furtherdemonstrate empirically that the deterioration in performance can be far worsethan is commonly suggested in the literature and provide a real data examplewhere deterioration is observed.
arxiv-6300-65 | Concept Based vs. Pseudo Relevance Feedback Performance Evaluation for Information Retrieval System | http://arxiv.org/abs/1403.4362 | author:Mohammed El Amine Abderrahim category:cs.IR cs.CL published:2014-03-18 summary:This article evaluates the performance of two techniques for queryreformulation in a system for information retrieval, namely, the concept basedand the pseudo relevance feedback reformulation. The experiments performed on acorpus of Arabic text have allowed us to compare the contribution of these tworeformulation techniques in improving the performance of an informationretrieval system for Arabic texts.
arxiv-6300-66 | Bregman Divergences for Infinite Dimensional Covariance Matrices | http://arxiv.org/abs/1403.4334 | author:Mehrtash Harandi, Mathieu Salzmann, Fatih Porikli category:cs.CV published:2014-03-18 summary:We introduce an approach to computing and comparing Covariance Descriptors(CovDs) in infinite-dimensional spaces. CovDs have become increasingly popularto address classification problems in computer vision. While CovDs offer somerobustness to measurement variations, they also throw away part of theinformation contained in the original data by only retaining the second-orderstatistics over the measurements. Here, we propose to overcome this limitationby first mapping the original data to a high-dimensional Hilbert space, andonly then compute the CovDs. We show that several Bregman divergences can becomputed between the resulting CovDs in Hilbert space via the use of kernels.We then exploit these divergences for classification purposes. Our experimentsdemonstrate the benefits of our approach on several tasks, such as material andtexture recognition, person re-identification, and action recognition frommotion capture data.
arxiv-6300-67 | Learning Negative Mixture Models by Tensor Decompositions | http://arxiv.org/abs/1403.4224 | author:Guillaume Rabusseau, François Denis category:cs.LG published:2014-03-17 summary:This work considers the problem of estimating the parameters of negativemixture models, i.e. mixture models that possibly involve negative weights. Thecontributions of this paper are as follows. (i) We show that every rationalprobability distributions on strings, a representation which occurs naturallyin spectral learning, can be computed by a negative mixture of at most twoprobabilistic automata (or HMMs). (ii) We propose a method to estimate theparameters of negative mixture models having a specific tensor structure intheir low order observable moments. Building upon a recent paper on tensordecompositions for learning latent variable models, we extend this work to thebroader setting of tensors having a symmetric decomposition with positive andnegative weights. We introduce a generalization of the tensor power method forcomplex valued tensors, and establish theoretical convergence guarantees. (iii)We show how our approach applies to negative Gaussian mixture models, for whichwe provide some experiments.
arxiv-6300-68 | Measuring Global Similarity between Texts | http://arxiv.org/abs/1403.4024 | author:Uli Fahrenberg, Fabrizio Biondi, Kevin Corre, Cyrille Jegourel, Simon Kongshøj, Axel Legay category:cs.CL published:2014-03-17 summary:We propose a new similarity measure between texts which, contrary to thecurrent state-of-the-art approaches, takes a global view of the texts to becompared. We have implemented a tool to compute our textual distance andconducted experiments on several corpuses of texts. The experiments show thatour methods can reliably identify different global types of texts.
arxiv-6300-69 | Computer Vision Accelerators for Mobile Systems based on OpenCL GPGPU Co-Processing | http://arxiv.org/abs/1403.4238 | author:Guohui Wang, Yingen Xiong, Jay Yun, Joseph R. Cavallaro category:cs.DC cs.CV cs.MS published:2014-03-17 summary:In this paper, we present an OpenCL-based heterogeneous implementation of acomputer vision algorithm -- image inpainting-based object removal algorithm --on mobile devices. To take advantage of the computation power of the mobileprocessor, the algorithm workflow is partitioned between the CPU and the GPUbased on the profiling results on mobile devices, so that thecomputationally-intensive kernels are accelerated by the mobile GPGPU(general-purpose computing using graphics processing units). By exploring theimplementation trade-offs and utilizing the proposed optimization strategies atdifferent levels including algorithm optimization, parallelism optimization,and memory access optimization, we significantly speed up the algorithm withthe CPU-GPU heterogeneous implementation, while preserving the quality of theoutput images. Experimental results show that heterogeneous computing based onGPGPU co-processing can significantly speed up the computer vision algorithmsand makes them practical on real-world mobile devices.
arxiv-6300-70 | Balancing Sparsity and Rank Constraints in Quadratic Basis Pursuit | http://arxiv.org/abs/1403.4267 | author:Cagdas Bilen, Gilles Puy, Rémi Gribonval, Laurent Daudet category:cs.NA cs.LG published:2014-03-17 summary:We investigate the methods that simultaneously enforce sparsity and low-rankstructure in a matrix as often employed for sparse phase retrieval problems orphase calibration problems in compressive sensing. We propose a new approachfor analyzing the trade off between the sparsity and low rank constraints inthese approaches which not only helps to provide guidelines to adjust theweights between the aforementioned constraints, but also enables new simulationstrategies for evaluating performance. We then provide simulation results forphase retrieval and phase calibration cases both to demonstrate the consistencyof the proposed method with other approaches and to evaluate the change ofperformance with different weights for the sparsity and low rank structureconstraints.
arxiv-6300-71 | Multi-task Feature Selection based Anomaly Detection | http://arxiv.org/abs/1403.4017 | author:Longqi Yang, Yibing Wang, Zhisong Pan, Guyu Hu category:stat.ML cs.LG published:2014-03-17 summary:Network anomaly detection is still a vibrant research area. As the fastgrowth of network bandwidth and the tremendous traffic on the network, therearises an extremely challengeable question: How to efficiently and accuratelydetect the anomaly on multiple traffic? In multi-task learning, the trafficconsisting of flows at different time periods is considered as a task. Multipletasks at different time periods performed simultaneously to detect anomalies.In this paper, we apply the multi-task feature selection in network anomalydetection area which provides a powerful method to gather information frommultiple traffic and detect anomalies on it simultaneously. In particular, themulti-task feature selection includes the well-known l1-norm based featureselection as a special case given only one task. Moreover, we show that themulti-task feature selection is more accurate by utilizing more informationsimultaneously than the l1-norm based method. At the evaluation stage, wepreprocess the raw data trace from trans-Pacific backbone link between Japanand the United States, label with anomaly communities, and generate a248-feature dataset. We show empirically that the multi-task feature selectionoutperforms independent l1-norm based feature selection on real trafficdataset.
arxiv-6300-72 | High-speed detection of emergent market clustering via an unsupervised parallel genetic algorithm | http://arxiv.org/abs/1403.4099 | author:Dieter Hendricks, Diane Wilcox, Tim Gebbie category:q-fin.CP cs.DC cs.NE published:2014-03-17 summary:We implement a master-slave parallel genetic algorithm (PGA) with a bespokelog-likelihood fitness function to identify emergent clusters within priceevolutions. We use graphics processing units (GPUs) to implement a PGA andvisualise the results using disjoint minimal spanning trees (MSTs). Wedemonstrate that our GPU PGA, implemented on a commercially available generalpurpose GPU, is able to recover stock clusters in sub-second speed, based on asubset of stocks in the South African market. This represents a pragmaticchoice for low-cost, scalable parallel computing and is significantly fasterthan a prototype serial implementation in an optimised C-basedfourth-generation programming language, although the results are not directlycomparable due to compiler differences. Combined with fast online intradaycorrelation matrix estimation from high frequency data for clusteridentification, the proposed implementation offers cost-effective,near-real-time risk assessment for financial practitioners.
arxiv-6300-73 | Automatic Image Registration in Infrared-Visible Videos using Polygon Vertices | http://arxiv.org/abs/1403.4232 | author:Tanushri Chakravorty, Guillaume-Alexandre Bilodeau, Eric Granger category:cs.CV published:2014-03-17 summary:In this paper, an automatic method is proposed to perform image registrationin visible and infrared pair of video sequences for multiple targets. Inmultimodal image analysis like image fusion systems, color and IR sensors areplaced close to each other and capture a same scene simultaneously, but thevideos are not properly aligned by default because of different fields of view,image capturing information, working principle and other camera specifications.Because the scenes are usually not planar, alignment needs to be performedcontinuously by extracting relevant common information. In this paper, weapproximate the shape of the targets by polygons and use affine transformationfor aligning the two video sequences. After background subtraction, keypointson the contour of the foreground blobs are detected using DCE (Discrete CurveEvolution)technique. These keypoints are then described by the local shape ateach point of the obtained polygon. The keypoints are matched based on theconvexity of polygon's vertices and Euclidean distance between them. Only goodmatches for each local shape polygon in a frame, are kept. To achieve a globalaffine transformation that maximises the overlapping of infrared and visibleforeground pixels, the matched keypoints of each local shape polygon are storedtemporally in a buffer for a few number of frames. The matrix is evaluated ateach frame using the temporal buffer and the best matrix is selected, based onan overlapping ratio criterion. Our experimental results demonstrate that thismethod can provide highly accurate registered images and that we outperform aprevious related method.
arxiv-6300-74 | A reversible infinite HMM using normalised random measures | http://arxiv.org/abs/1403.4206 | author:Konstantina Palla, David A. Knowles, Zoubin Ghahramani category:stat.ML published:2014-03-17 summary:We present a nonparametric prior over reversible Markov chains. We usecompletely random measures, specifically gamma processes, to construct acountably infinite graph with weighted edges. By enforcing symmetry to make theedges undirected we define a prior over random walks on graphs that results ina reversible Markov chain. The resulting prior over infinite transitionmatrices is closely related to the hierarchical Dirichlet process but enforcesreversibility. A reinforcement scheme has recently been proposed with similarproperties, but the de Finetti measure is not well characterised. We take thealternative approach of explicitly constructing the mixing measure, whichallows more straightforward and efficient inference at the cost of no longerhaving a closed form predictive distribution. We use our process to construct areversible infinite HMM which we apply to two real datasets, one fromepigenomics and one ion channel recording.
arxiv-6300-75 | Image processing using miniKanren | http://arxiv.org/abs/1403.3964 | author:Hirotaka Niitsuma category:cs.CV cs.PL published:2014-03-16 summary:An integral image is one of the most efficient optimization technique forimage processing. However an integral image is only a special case of delayedstream or memoization. This research discusses generalizing concept of integralimage optimization technique, and how to generate an integral image optimizedprogram code automatically from abstracted image processing algorithm. In oderto abstruct algorithms, we forces to miniKanren.
arxiv-6300-76 | Near-optimal Reinforcement Learning in Factored MDPs | http://arxiv.org/abs/1403.3741 | author:Ian Osband, Benjamin Van Roy category:stat.ML cs.LG published:2014-03-15 summary:Any reinforcement learning algorithm that applies to all Markov decisionprocesses (MDPs) will suffer $\Omega(\sqrt{SAT})$ regret on some MDP, where $T$is the elapsed time and $S$ and $A$ are the cardinalities of the state andaction spaces. This implies $T = \Omega(SA)$ time to guarantee a near-optimalpolicy. In many settings of practical interest, due to the curse ofdimensionality, $S$ and $A$ can be so enormous that this learning time isunacceptable. We establish that, if the system is known to be a \emph{factored}MDP, it is possible to achieve regret that scales polynomially in the number of\emph{parameters} encoding the factored MDP, which may be exponentially smallerthan $S$ or $A$. We provide two algorithms that satisfy near-optimal regretbounds in this context: posterior sampling reinforcement learning (PSRL) and anupper confidence bound algorithm (UCRL-Factored).
arxiv-6300-77 | Automatic Classification of Human Epithelial Type 2 Cell Indirect Immunofluorescence Images using Cell Pyramid Matching | http://arxiv.org/abs/1403.3780 | author:Arnold Wiliem, Conrad Sanderson, Yongkang Wong, Peter Hobson, Rodney F. Minchin, Brian C. Lovell category:q-bio.CB cs.CV q-bio.QM published:2014-03-15 summary:This paper describes a novel system for automatic classification of imagesobtained from Anti-Nuclear Antibody (ANA) pathology tests on Human Epithelialtype 2 (HEp-2) cells using the Indirect Immunofluorescence (IIF) protocol. TheIIF protocol on HEp-2 cells has been the hallmark method to identify thepresence of ANAs, due to its high sensitivity and the large range of antigensthat can be detected. However, it suffers from numerous shortcomings, such asbeing subjective as well as time and labour intensive. Computer AidedDiagnostic (CAD) systems have been developed to address these problems, whichautomatically classify a HEp-2 cell image into one of its known patterns (eg.speckled, homogeneous). Most of the existing CAD systems use handpickedfeatures to represent a HEp-2 cell image, which may only work in limitedscenarios. We propose a novel automatic cell image classification method termedCell Pyramid Matching (CPM), which is comprised of regional histograms ofvisual words coupled with the Multiple Kernel Learning framework. We present astudy of several variations of generating histograms and show the efficacy ofthe system on two publicly available datasets: the ICPR HEp-2 cellclassification contest dataset and the SNPHEp-2 dataset.
arxiv-6300-78 | Geometric VLAD for Large Scale Image Search | http://arxiv.org/abs/1403.3829 | author:Zixuan Wang, Wei Di, Anurag Bhardwaj, Vignesh Jagadeesh, Robinson Piramuthu category:cs.CV published:2014-03-15 summary:We present a novel compact image descriptor for large scale image search. Ourproposed descriptor - Geometric VLAD (gVLAD) is an extension of VLAD (Vector ofLocally Aggregated Descriptors) that incorporates weak geometry informationinto the VLAD framework. The proposed geometry cues are derived as a membershipfunction over keypoint angles which contain evident and informative informationbut yet often discarded. A principled technique for learning the membershipfunction by clustering angles is also presented. Further, to address theoverhead of iterative codebook training over real-time datasets, a novelcodebook adaptation strategy is outlined. Finally, we demonstrate the efficacyof proposed gVLAD based retrieval framework where we achieve more than 15%improvement in mAP over existing benchmarks.
arxiv-6300-79 | Making Risk Minimization Tolerant to Label Noise | http://arxiv.org/abs/1403.3610 | author:Aritra Ghosh, Naresh Manwani, P. S. Sastry category:cs.LG published:2014-03-14 summary:In many applications, the training data, from which one needs to learn aclassifier, is corrupted with label noise. Many standard algorithms such as SVMperform poorly in presence of label noise. In this paper we investigate therobustness of risk minimization to label noise. We prove a sufficient conditionon a loss function for the risk minimization under that loss to be tolerant touniform label noise. We show that the $0-1$ loss, sigmoid loss, ramp loss andprobit loss satisfy this condition though none of the standard convex lossfunctions satisfy it. We also prove that, by choosing a sufficiently largevalue of a parameter in the loss function, the sigmoid loss, ramp loss andprobit loss can be made tolerant to non-uniform label noise also if we canassume the classes to be separable under noise-free data distribution. Throughextensive empirical studies, we show that risk minimization under the $0-1$loss, the sigmoid loss and the ramp loss has much better robustness to labelnoise when compared to the SVM algorithm.
arxiv-6300-80 | Language Heedless of Logic - Philosophy Mindful of What? Failures of Distributive and Absorption Laws | http://arxiv.org/abs/1403.3668 | author:Arthur Merin category:cs.CL published:2014-03-14 summary:Much of philosophical logic and all of philosophy of language make empiricalclaims about the vernacular natural language. They presume semantics underwhich `and' and `or' are related by the dually paired distributive andabsorption laws. However, at least one of each pair of laws fails in thevernacular. `Implicature'-based auxiliary theories associated with theprogramme of H.P. Grice do not prove remedial. Conceivable alternatives thatmight replace the familiar logics as descriptive instruments are briefly noted:(i) substructural logics and (ii) meaning composition in linear algebras overthe reals, occasionally constrained by norms of classical logic. Alternative(ii) locates the problem in violations of one of the idempotent laws. Reasonsfor a lack of curiosity about elementary and easily testable implications ofthe received theory are considered. The concept of `reflective equilibrium' iscritically examined for its role in reconciling normative desiderata anddescriptive commitments.
arxiv-6300-81 | Learning the Latent State Space of Time-Varying Graphs | http://arxiv.org/abs/1403.3707 | author:Nesreen K. Ahmed, Christopher Cole, Jennifer Neville category:cs.SI cs.LG physics.soc-ph stat.ML published:2014-03-14 summary:From social networks to Internet applications, a wide variety of electroniccommunication tools are producing streams of graph data; where the nodesrepresent users and the edges represent the contacts between them over time.This has led to an increased interest in mechanisms to model the dynamicstructure of time-varying graphs. In this work, we develop a framework forlearning the latent state space of a time-varying email graph. We show how theframework can be used to find subsequences that correspond to global real-timeevents in the Email graph (e.g. vacations, breaks, ...etc.). These eventsimpact the underlying graph process to make its characteristics non-stationary.Within the framework, we compare two different representations of the temporalrelationships; discrete vs. probabilistic. We use the two representations asinputs to a mixture model to learn the latent state transitions that correspondto important changes in the Email graph structure over time.
arxiv-6300-82 | Removal and Contraction Operations in $n$D Generalized Maps for Efficient Homology Computation | http://arxiv.org/abs/1403.3683 | author:Guillaume Damiand, Rocio Gonzalez-Diaz, Samuel Peltier category:cs.CV published:2014-03-14 summary:In this paper, we show that contraction operations preserve the homology of$n$D generalized maps, under some conditions. Removal and contractionoperations are used to propose an efficient algorithm that compute homologygenerators of $n$D generalized maps. Its principle consists in simplifying ageneralized map as much as possible by using removal and contractionoperations. We obtain a generalized map having the same homology than theinitial one, while the number of cells decreased significantly. Keywords: $n$D Generalized Maps; Cellular Homology; Homology Generators;Contraction and Removal Operations.
arxiv-6300-83 | Mixed-norm Regularization for Brain Decoding | http://arxiv.org/abs/1403.3628 | author:Rémi Flamary, Nisrine Jrad, Ronald Phlypo, Marco Congedo, Alain Rakotomamonjy category:cs.LG published:2014-03-14 summary:This work investigates the use of mixed-norm regularization for sensorselection in Event-Related Potential (ERP) based Brain-Computer Interfaces(BCI). The classification problem is cast as a discriminative optimizationframework where sensor selection is induced through the use of mixed-norms.This framework is extended to the multi-task learning situation where severalsimilar classification tasks related to different subjects are learnedsimultaneously. In this case, multi-task learning helps in leveraging datascarcity issue yielding to more robust classifiers. For this purpose, we haveintroduced a regularizer that induces both sensor selection and classifiersimilarities. The different regularization approaches are compared on three ERPdatasets showing the interest of mixed-norm regularization in terms of sensorselection. The multi-task approaches are evaluated when a small number oflearning examples are available yielding to significant performanceimprovements especially for subjects performing poorly.
arxiv-6300-84 | Spontaneous expression classification in the encrypted domain | http://arxiv.org/abs/1403.3602 | author:Segun Aina, Yogachandran Rahulamathavan, Raphael C. -W. Phan, Jonathon A. Chambers category:cs.CV cs.CR published:2014-03-14 summary:To date, most facial expression analysis have been based on posed imagedatabases and is carried out without being able to protect the identity of thesubjects whose expressions are being recognised. In this paper, we propose andimplement a system for classifying facial expressions of images in theencrypted domain based on a Paillier cryptosystem implementation of FisherLinear Discriminant Analysis and k-nearest neighbour (FLDA + kNN). We presentresults of experiments carried out on a recently developed natural visible andinfrared facial expression (NVIE) database of spontaneous images. To the bestof our knowledge, this is the first system that will allow the recog-nition ofencrypted spontaneous facial expressions by a remote server on behalf of aclient.
arxiv-6300-85 | An inertial forward-backward algorithm for monotone inclusions | http://arxiv.org/abs/1403.3522 | author:Dirk A. Lorenz, Thomas Pock category:cs.CV cs.NA math.NA math.OC published:2014-03-14 summary:In this paper, we propose an inertial forward backward splitting algorithm tocompute a zero of the sum of two monotone operators, with one of the twooperators being co-coercive. The algorithm is inspired by the acceleratedgradient method of Nesterov, but can be applied to a much larger class ofproblems including convex-concave saddle point problems and general monotoneinclusions. We prove convergence of the algorithm in a Hilbert space settingand show that several recently proposed first-order methods can be obtained asspecial cases of the general algorithm. Numerical results show that theproposed algorithm converges faster than existing methods, while keeping thecomputational cost of each iteration basically unchanged.
arxiv-6300-86 | A Survey of Algorithms and Analysis for Adaptive Online Learning | http://arxiv.org/abs/1403.3465 | author:H. Brendan McMahan category:cs.LG published:2014-03-14 summary:We present tools for the analysis of Follow-The-Regularized-Leader (FTRL),Dual Averaging, and Mirror Descent algorithms when the regularizer(equivalently, prox-function or learning rate schedule) is chosen adaptivelybased on the data. Adaptivity can be used to prove regret bounds that hold onevery round, and also allows for data-dependent regret bounds as inAdaGrad-style algorithms (e.g., Online Gradient Descent with adaptiveper-coordinate learning rates). We present results from a large number of priorworks in a unified manner, using a modular and tight analysis that isolates thekey arguments in easily re-usable lemmas. This approach strengthens pre-viouslyknown FTRL analysis techniques to produce bounds as tight as those achieved bypotential functions or primal-dual analysis. Further, we prove a general andexact equivalence between an arbitrary adaptive Mirror Descent algorithm and acorrespond- ing FTRL update, which allows us to analyze any Mirror Descentalgorithm in the same framework. The key to bridging the gap between DualAveraging and Mirror Descent algorithms lies in an analysis of theFTRL-Proximal algorithm family. Our regret bounds are proved in the mostgeneral form, holding for arbitrary norms and non-smooth regularizers withtime-varying weight.
arxiv-6300-87 | VESICLE: Volumetric Evaluation of Synaptic Interfaces using Computer vision at Large Scale | http://arxiv.org/abs/1403.3724 | author:William Gray Roncal, Michael Pekala, Verena Kaynig-Fittkau, Dean M. Kleissas, Joshua T. Vogelstein, Hanspeter Pfister, Randal Burns, R. Jacob Vogelstein, Mark A. Chevillet, Gregory D. Hager category:cs.CV cs.CE q-bio.QM published:2014-03-14 summary:An open challenge problem at the forefront of modern neuroscience is toobtain a comprehensive mapping of the neural pathways that underlie human brainfunction; an enhanced understanding of the wiring diagram of the brain promisesto lead to new breakthroughs in diagnosing and treating neurological disorders.Inferring brain structure from image data, such as that obtained via electronmicroscopy (EM), entails solving the problem of identifying biologicalstructures in large data volumes. Synapses, which are a key communicationstructure in the brain, are particularly difficult to detect due to their smallsize and limited contrast. Prior work in automated synapse detection has reliedupon time-intensive biological preparations (post-staining, isotropic slicethicknesses) in order to simplify the problem. This paper presents VESICLE, the first known approach designed for mammaliansynapse detection in anisotropic, non-post-stained data. Our methods explicitlyleverage biological context, and the results exceed existing synapse detectionmethods in terms of accuracy and scalability. We provide two differentapproaches - one a deep learning classifier (VESICLE-CNN) and one a lightweightRandom Forest approach (VESICLE-RF) to offer alternatives in theperformance-scalability space. Addressing this synapse detection challengeenables the analysis of high-throughput imaging data soon expected to reachpetabytes of data, and provide tools for more rapid estimation of brain-graphs.Finally, to facilitate community efforts, we developed tools for large-scaleobject detection, and demonstrated this framework to find $\approx$ 50,000synapses in 60,000 $\mu m ^3$ (220 GB on disk) of electron microscopy data.
arxiv-6300-88 | Controlling Recurrent Neural Networks by Conceptors | http://arxiv.org/abs/1403.3369 | author:Herbert Jaeger category:cs.NE I.2.6 published:2014-03-13 summary:The human brain is a dynamical system whose extremely complex sensor-drivenneural processes give rise to conceptual, logical cognition. Understanding theinterplay between nonlinear neural dynamics and concept-level cognition remainsa major scientific challenge. Here I propose a mechanism of neurodynamicalorganization, called conceptors, which unites nonlinear dynamics with basicprinciples of conceptual abstraction and logic. It becomes possible to learn,store, abstract, focus, morph, generalize, de-noise and recognize a largenumber of dynamical patterns within a single neural system; novel patterns canbe added without interfering with previously acquired ones; neural noise isautomatically filtered. Conceptors help explaining how conceptual-levelinformation processing emerges naturally and robustly in neural systems, andremove a number of roadblocks in the theory and applications of recurrentneural networks.
arxiv-6300-89 | Neighborhood Selection for Thresholding-based Subspace Clustering | http://arxiv.org/abs/1403.3438 | author:Reinhard Heckel, Eirikur Agustsson, Helmut Bölcskei category:stat.ML cs.IT math.IT published:2014-03-13 summary:Subspace clustering refers to the problem of clustering high-dimensional datapoints into a union of low-dimensional linear subspaces, where the number ofsubspaces, their dimensions and orientations are all unknown. In this paper, wepropose a variation of the recently introduced thresholding-based subspaceclustering (TSC) algorithm, which applies spectral clustering to an adjacencymatrix constructed from the nearest neighbors of each data point with respectto the spherical distance measure. The new element resides in an individual anddata-driven choice of the number of nearest neighbors. Previous performanceresults for TSC, as well as for other subspace clustering algorithms based onspectral clustering, come in terms of an intermediate performance measure,which does not address the clustering error directly. Our main analyticalcontribution is a performance analysis of the modified TSC algorithm (as wellas the original TSC algorithm) in terms of the clustering error directly.
arxiv-6300-90 | Semantic Unification A sheaf theoretic approach to natural language | http://arxiv.org/abs/1403.3351 | author:Samson Abramsky, Mehrnoosh Sadrzadeh category:cs.CL published:2014-03-13 summary:Language is contextual and sheaf theory provides a high level mathematicalframework to model contextuality. We show how sheaf theory can model thecontextual nature of natural language and how gluing can be used to provide aglobal semantics for a discourse by putting together the local logicalsemantics of each sentence within the discourse. We introduce a presheafstructure corresponding to a basic form of Discourse Representation Structures.Within this setting, we formulate a notion of semantic unification --- gluingmeanings of parts of a discourse into a coherent whole --- as a form ofsheaf-theoretic gluing. We illustrate this idea with a number of examples whereit can used to represent resolutions of anaphoric references. We also discussmultivalued gluing, described using a distributions functor, which can be usedto represent situations where multiple gluings are possible, and where we mayneed to rank them using quantitative measures. Dedicated to Jim Lambek on the occasion of his 90th birthday.
arxiv-6300-91 | The Potential Benefits of Filtering Versus Hyper-Parameter Optimization | http://arxiv.org/abs/1403.3342 | author:Michael R. Smith, Tony Martinez, Christophe Giraud-Carrier category:stat.ML cs.LG published:2014-03-13 summary:The quality of an induced model by a learning algorithm is dependent on thequality of the training data and the hyper-parameters supplied to the learningalgorithm. Prior work has shown that improving the quality of the training data(i.e., by removing low quality instances) or tuning the learning algorithmhyper-parameters can significantly improve the quality of an induced model. Acomparison of the two methods is lacking though. In this paper, we estimate andcompare the potential benefits of filtering and hyper-parameter optimization.Estimating the potential benefit gives an overly optimistic estimate but alsoempirically demonstrates an approximation of the maximum potential benefit ofeach method. We find that, while both significantly improve the induced model,improving the quality of the training set has a greater potential effect thanhyper-parameter optimization.
arxiv-6300-92 | Scalable and Robust Construction of Topical Hierarchies | http://arxiv.org/abs/1403.3460 | author:Chi Wang, Xueqing Liu, Yanglei Song, Jiawei Han category:cs.LG cs.CL cs.DB cs.IR published:2014-03-13 summary:Automated generation of high-quality topical hierarchies for a textcollection is a dream problem in knowledge engineering with many valuableapplications. In this paper a scalable and robust algorithm is proposed forconstructing a hierarchy of topics from a text collection. We divide andconquer the problem using a top-down recursive framework, based on a tensororthogonal decomposition technique. We solve a critical challenge to performscalable inference for our newly designed hierarchical topic model. Experimentswith various real-world datasets illustrate its ability to generate robust,high-quality hierarchies efficiently. Our method reduces the time ofconstruction by several orders of magnitude, and its robust feature renders itpossible for users to interactively revise the hierarchy.
arxiv-6300-93 | Spectral Correlation Hub Screening of Multivariate Time Series | http://arxiv.org/abs/1403.3371 | author:Hamed Firouzi, Dennis Wei, Alfred O. Hero III category:stat.OT cs.LG stat.AP published:2014-03-13 summary:This chapter discusses correlation analysis of stationary multivariateGaussian time series in the spectral or Fourier domain. The goal is to identifythe hub time series, i.e., those that are highly correlated with a specifiednumber of other time series. We show that Fourier components of the time seriesat different frequencies are asymptotically statistically independent. Thisproperty permits independent correlation analysis at each frequency,alleviating the computational and statistical challenges of high-dimensionaltime series. To detect correlation hubs at each frequency, an existingcorrelation screening method is extended to the complex numbers to accommodatecomplex-valued Fourier components. We characterize the number of hubdiscoveries at specified correlation and degree thresholds in the regime ofincreasing dimension and fixed sample size. The theory specifies appropriatethresholds to apply to sample correlation matrices to detect hubs and alsoallows statistical significance to be attributed to hub discoveries. Numericalresults illustrate the accuracy of the theory and the usefulness of theproposed spectral framework.
arxiv-6300-94 | Sentiment Analysis by Using Fuzzy Logic | http://arxiv.org/abs/1403.3185 | author:Md. Ansarul Haque category:cs.IR cs.CL published:2014-03-13 summary:How could a product or service is reasonably evaluated by anyone in theshortest time? A million dollar question but it is having a simple answer:Sentiment analysis. Sentiment analysis is consumers review on products andservices which helps both the producers and consumers (stakeholders) to takeeffective and efficient decision within a shortest period of time. Producerscan have better knowledge of their products and services through the sentimentanalysis (ex. positive and negative comments or consumers likes and dislikes)which will help them to know their products status (ex. product limitations ormarket status). Consumers can have better knowledge of their interestedproducts and services through the sentiment analysis (ex. positive and negativecomments or consumers likes and dislikes) which will help them to know theirdeserving products status (ex. product limitations or market status). For morespecification of the sentiment values, fuzzy logic could be introduced.Therefore, sentiment analysis with the help of fuzzy logic (deals withreasoning and gives closer views to the exact sentiment values) will help theproducers or consumers or any interested person for taking the effectivedecision according to their product or service interest.
arxiv-6300-95 | Spectral Unmixing via Data-guided Sparsity | http://arxiv.org/abs/1403.3155 | author:Feiyun Zhu, Ying Wang, Bin Fan, Gaofeng Meng, Shiming Xiang, Chunhong Pan category:cs.CV published:2014-03-13 summary:Hyperspectral unmixing, the process of estimating a common set of spectralbases and their corresponding composite percentages at each pixel, is animportant task for hyperspectral analysis, visualization and understanding.From an unsupervised learning perspective, this problem is verychallenging---both the spectral bases and their composite percentages areunknown, making the solution space too large. To reduce the solution space,many approaches have been proposed by exploiting various priors. In practice,these priors would easily lead to some unsuitable solution. This is becausethey are achieved by applying an identical strength of constraints to all thefactors, which does not hold in practice. To overcome this limitation, wepropose a novel sparsity based method by learning a data-guided map to describethe individual mixed level of each pixel. Through this data-guided map, the$\ell_{p}(0<p<1)$ constraint is applied in an adaptive manner. Suchimplementation not only meets the practical situation, but also guides thespectral bases toward the pixels under highly sparse constraint. What's more,an elegant optimization scheme as well as its convergence proof have beenprovided in this paper. Extensive experiments on several datasets alsodemonstrate that the data-guided map is feasible, and high quality unmixingresults could be obtained by our method.
arxiv-6300-96 | Numerical Approaches for Linear Left-invariant Diffusions on SE(2), their Comparison to Exact Solutions, and their Applications in Retinal Imaging | http://arxiv.org/abs/1403.3320 | author:Jiong Zhang, Remco Duits, Gonzalo Sanguinetti, Bart M. ter Haar Romeny category:math.NA cs.CV published:2014-03-13 summary:Left-invariant PDE-evolutions on the roto-translation group $SE(2)$ (andtheir resolvent equations) have been widely studied in the fields of corticalmodeling and image analysis. They include hypo-elliptic diffusion (for contourenhancement) proposed by Citti & Sarti, and Petitot, and they include thedirection process (for contour completion) proposed by Mumford. This paperpresents a thorough study and comparison of the many numerical approaches,which, remarkably, is missing in the literature. Existing numerical approachescan be classified into 3 categories: Finite difference methods, Fourier basedmethods (equivalent to $SE(2)$-Fourier methods), and stochastic methods (MonteCarlo simulations). There are also 3 types of exact solutions to thePDE-evolutions that were derived explicitly (in the spatial Fourier domain) inprevious works by Duits and van Almsick in 2005. Here we provide an overview ofthese 3 types of exact solutions and explain how they relate to each of the 3numerical approaches. We compute relative errors of all numerical approaches tothe exact solutions, and the Fourier based methods show us the best performancewith smallest relative errors. We also provide an improvement of Mathematicaalgorithms for evaluating Mathieu-functions, crucial in implementations of theexact solutions. Furthermore, we include an asymptotical analysis of thesingularities within the kernels and we propose a probabilistic extension ofunderlying stochastic processes that overcomes the singular behavior in theorigin of time-integrated kernels. Finally, we show retinal imagingapplications of combining left-invariant PDE-evolutions with invertibleorientation scores.
arxiv-6300-97 | Noise Facilitation in Associative Memories of Exponential Capacity | http://arxiv.org/abs/1403.3305 | author:Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi, Lav R. Varshney category:cs.NE published:2014-03-13 summary:Recent advances in associative memory design through structured pattern setsand graph-based inference algorithms have allowed reliable learning and recallof an exponential number of patterns. Although these designs correct externalerrors in recall, they assume neurons that compute noiselessly, in contrast tothe highly variable neurons in brain regions thought to operate associativelysuch as hippocampus and olfactory cortex. Here we consider associative memories with noisy internal computations andanalytically characterize performance. As long as the internal noise level isbelow a specified threshold, the error probability in the recall phase can bemade exceedingly small. More surprisingly, we show that internal noise actuallyimproves the performance of the recall phase while the pattern retrievalcapacity remains intact, i.e., the number of stored patterns does not reducewith noise (up to a threshold). Computational experiments lend additionalsupport to our theoretical analysis. This work suggests a functional benefit tonoisy neurons in biological neuronal networks.
arxiv-6300-98 | A Novel Method to Extract Rocks from Mars Images | http://arxiv.org/abs/1403.3083 | author:Shuliang Wang, Yasen Chen category:cs.CV published:2014-03-13 summary:In this paper, a novel method is proposed to extract rocks from Martiansurface images by using 8 data field. It models the interaction between twopixels of an image in the context of imagery 9 characteristics. First,foreground rocks are differed from background information by binarizing 10image on roughly partitioned images. Second, foreground rocks are grouped intoclusters by 11 locating the centers and edges of clusters in data field viahierarchical grids. Third, the target 12 rocks are discovered for the MarsExploration Rover (MER) to keep healthy paths. The 13 experiment with imagestaken by MER shows the proposed method is practical and potential.
arxiv-6300-99 | Box Drawings for Learning with Imbalanced Data | http://arxiv.org/abs/1403.3378 | author:Siong Thye Goh, Cynthia Rudin category:stat.ML cs.LG published:2014-03-13 summary:The vast majority of real world classification problems are imbalanced,meaning there are far fewer data from the class of interest (the positiveclass) than from other classes. We propose two machine learning algorithms tohandle highly imbalanced classification problems. The classifiers constructedby both methods are created as unions of parallel axis rectangles around thepositive examples, and thus have the benefit of being interpretable. The firstalgorithm uses mixed integer programming to optimize a weighted balance betweenpositive and negative class accuracies. Regularization is introduced to improvegeneralization performance. The second method uses an approximation in order toassist with scalability. Specifically, it follows a \textit{characterize thendiscriminate} approach, where the positive class is characterized first byboxes, and then each box boundary becomes a separate discriminative classifier.This method has the computational advantages that it can be easilyparallelized, and considers only the relevant regions of feature space.
arxiv-6300-100 | ARSENAL: Automatic Requirements Specification Extraction from Natural Language | http://arxiv.org/abs/1403.3142 | author:Shalini Ghosh, Daniel Elenius, Wenchao Li, Patrick Lincoln, Natarajan Shankar, Wilfried Steiner category:cs.CL cs.SE published:2014-03-13 summary:Requirements are informal and semi-formal descriptions of the expectedbehavior of a complex system from the viewpoints of its stakeholders(customers, users, operators, designers, and engineers). However, for thepurpose of design, testing, and verification for critical systems, we cantransform requirements into formal models that can be analyzed automatically.ARSENAL is a framework and methodology for systematically transforming naturallanguage (NL) requirements into analyzable formal models and logicspecifications. These models can be analyzed for consistency andimplementability. The ARSENAL methodology is specialized to individual domains,but the approach is general enough to be adapted to new domains.
arxiv-6300-101 | Efficiently inferring community structure in bipartite networks | http://arxiv.org/abs/1403.2933 | author:Daniel B. Larremore, Aaron Clauset, Abigail Z. Jacobs category:cs.SI physics.soc-ph q-bio.QM stat.ML published:2014-03-12 summary:Bipartite networks are a common type of network data in which there are twotypes of vertices, and only vertices of different types can be connected. Whilebipartite networks exhibit community structure like their unipartitecounterparts, existing approaches to bipartite community detection havedrawbacks, including implicit parameter choices, loss of information throughone-mode projections, and lack of interpretability. Here we solve the communitydetection problem for bipartite networks by formulating a bipartite stochasticblock model, which explicitly includes vertex type information and may betrivially extended to $k$-partite networks. This bipartite stochastic blockmodel yields a projection-free and statistically principled method forcommunity detection that makes clear assumptions and parameter choices andyields interpretable results. We demonstrate this model's ability toefficiently and accurately find community structure in synthetic bipartitenetworks with known structure and in real-world bipartite networks with unknownstructure, and we characterize its performance in practical contexts.
arxiv-6300-102 | Memory Capacity of Neural Networks using a Circulant Weight Matrix | http://arxiv.org/abs/1403.3115 | author:Vamsi Sashank Kotagiri category:cs.NE published:2014-03-12 summary:This paper presents results on the memory capacity of a generalized feedbackneural network using a circulant matrix. Children are capable of learning soonafter birth which indicates that the neural networks of the brain have priorlearnt capacity that is a consequence of the regular structures in the brain'sorganization. Motivated by this idea, we consider the capacity of circulantmatrices as weight matrices in a feedback network.
arxiv-6300-103 | Adaptive Representations for Tracking Breaking News on Twitter | http://arxiv.org/abs/1403.2923 | author:Igor Brigadir, Derek Greene, Pádraig Cunningham category:cs.IR cs.NE published:2014-03-12 summary:Twitter is often the most up-to-date source for finding and tracking breakingnews stories. Therefore, there is considerable interest in developing filtersfor tweet streams in order to track and summarize stories. This is anon-trivial text analytics task as tweets are short, and standard retrievalmethods often fail as stories evolve over time. In this paper we examine theeffectiveness of adaptive mechanisms for tracking and summarizing breaking newsstories. We evaluate the effectiveness of these mechanisms on a number ofrecent news events for which manually curated timelines are available.Assessments based on ROUGE metrics indicate that an adaptive approaches arebest suited for tracking evolving stories on Twitter.
arxiv-6300-104 | Sparse Recovery with Linear and Nonlinear Observations: Dependent and Noisy Data | http://arxiv.org/abs/1403.3109 | author:Cem Aksoylar, Venkatesh Saligrama category:cs.IT cs.LG math.IT math.ST stat.TH published:2014-03-12 summary:We formulate sparse support recovery as a salient set identification problemand use information-theoretic analyses to characterize the recovery performanceand sample complexity. We consider a very general model where we are notrestricted to linear models or specific distributions. We state non-asymptoticbounds on recovery probability and a tight mutual information formula forsample complexity. We evaluate our bounds for applications such as sparselinear regression and explicitly characterize effects of correlation or noisyfeatures on recovery performance. We show improvements upon previous work andidentify gaps between the performance of recovery algorithms and fundamentalinformation.
arxiv-6300-105 | Evaluation of Image Segmentation and Filtering With ANN in the Papaya Leaf | http://arxiv.org/abs/1403.3057 | author:Maicon A. Sartin, Alexandre C. R. da Silva category:cs.NE cs.CV published:2014-03-12 summary:Precision agriculture is area with lack of cheap technology. The refinementof the production system brings large advantages to the producer and the use ofimages makes the monitoring a more cheap methodology. Macronutrients monitoringcan to determine the health and vulnerability of the plant in specific stages.In this paper is analyzed the method based on computational intelligence towork with image segmentation in the identification of symptoms of plantnutrient deficiency. Artificial neural networks are evaluated for imagesegmentation and filtering, several variations of parameters and insertionimpulsive noise were evaluated too. Satisfactory results are achieved withartificial neural for segmentation same with high noise levels.
arxiv-6300-106 | Efficient Legendre moment computation for grey level images | http://arxiv.org/abs/1403.3022 | author:Guanyu Yang, Huazhong Shu, Christine Toumoulin, Guo-Niu Han, Limin M. Luo category:cs.CV math.NA published:2014-03-12 summary:Legendre orthogonal moments have been widely used in the field of imageanalysis. Because their computation by a direct method is very time expensive,recent efforts have been devoted to the reduction of computational complexity.Nevertheless, the existing algorithms are mainly focused on binary images. Wepropose here a new fast method for computing the Legendre moments, which is notonly suitable for binary images but also for grey levels. We first set up therecurrence formula of one-dimensional (1D) Legendre moments by using therecursive property of Legendre polynomials. As a result, the 1D Legendremoments of order p, Lp = Lp(0), can be expressed as a linear combination ofLp-1(1) and Lp-2(0). Based on this relationship, the 1D Legendre moments Lp(0)is thus obtained from the array of L1(a) and L0(a) where a is an integer numberless than p. To further decrease the computation complexity, an algorithm, inwhich no multiplication is required, is used to compute these quantities. Themethod is then extended to the calculation of the two-dimensional Legendremoments Lpq. We show that the proposed method is more efficient than the directmethod.
arxiv-6300-107 | Image reconstruction from limited range projections using orthogonal moments | http://arxiv.org/abs/1403.3021 | author:Huazhong Shu, Jian Zhou, Guo-Niu Han, Limin M. Luo, Jean-Louis Coatrieux category:cs.CV math.NA published:2014-03-12 summary:A set of orthonormal polynomials is proposed for image reconstruction fromprojection data. The relationship between the projection moments and imagemoments is discussed in detail, and some interesting properties aredemonstrated. Simulation results are provided to validate the method and tocompare its performance with previous works.
arxiv-6300-108 | 3D Well-composed Polyhedral Complexes | http://arxiv.org/abs/1403.2980 | author:Rocio Gonzalez-Diaz, Maria-Jose Jimenez, Belen Medrano category:cs.CV published:2014-03-12 summary:A binary three-dimensional (3D) image $I$ is well-composed if the boundarysurface of its continuous analog is a 2D manifold. Since 3D images are notoften well-composed, there are several voxel-based methods ("repairing"algorithms) for turning them into well-composed ones but these methods eitherdo not guarantee the topological equivalence between the original image and itscorresponding well-composed one or involve sub-sampling the whole image. In this paper, we present a method to locally "repair" the cubical complex$Q(I)$ (embedded in $\mathbb{R}^3$) associated to $I$ to obtain a polyhedralcomplex $P(I)$ homotopy equivalent to $Q(I)$ such that the boundary of everyconnected component of $P(I)$ is a 2D manifold. The reparation is performed viaa new codification system for $P(I)$ under the form of a 3D grayscale imagethat allows an efficient access to cells and their faces.
arxiv-6300-109 | Cancer Prognosis Prediction Using Balanced Stratified Sampling | http://arxiv.org/abs/1403.2950 | author:J S Saleema, N Bhagawathi, S Monica, P Deepa Shenoy, K R Venugopal, L M Patnaik category:cs.LG 62D05 I.2.6; H.2.8 published:2014-03-12 summary:High accuracy in cancer prediction is important to improve the quality of thetreatment and to improve the rate of survivability of patients. As the datavolume is increasing rapidly in the healthcare research, the analyticalchallenge exists in double. The use of effective sampling technique inclassification algorithms always yields good prediction accuracy. The SEERpublic use cancer database provides various prominent class labels forprognosis prediction. The main objective of this paper is to find the effect ofsampling techniques in classifying the prognosis variable and propose an idealsampling method based on the outcome of the experimentation. In the first phaseof this work the traditional random sampling and stratified sampling techniqueshave been used. At the next level the balanced stratified sampling withvariations as per the choice of the prognosis class labels have been tested.Much of the initial time has been focused on performing the pre_processing ofthe SEER data set. The classification model for experimentation has been builtusing the breast cancer, respiratory cancer and mixed cancer data sets withthree traditional classifiers namely Decision Tree, Naive Bayes and K-NearestNeighbor. The three prognosis factors survival, stage and metastasis have beenused as class labels for experimental comparisons. The results shows a steadyincrease in the prediction accuracy of balanced stratified model as the samplesize increases, but the traditional approach fluctuates before the optimumresults.
arxiv-6300-110 | Uav Route Planning For Maximum Target Coverage | http://arxiv.org/abs/1403.2906 | author:Murat Karakaya category:cs.RO cs.NE published:2014-03-12 summary:Utilization of Unmanned Aerial Vehicles (UAVs) in military and civiloperations is getting popular. One of the challenges in effectively taskingthese expensive vehicles is planning the flight routes to monitor the targets.In this work, we aim to develop an algorithm which produces routing plans for alimited number of UAVs to cover maximum number of targets considering theirflight range. The proposed solution for this practical optimization problem isdesigned by modifying the Max-Min Ant System (MMAS) algorithm. To evaluate thesuccess of the proposed method, an alternative approach, based on the NearestNeighbour (NN) heuristic, has been developed as well. The results showed thesuccess of the proposed MMAS method by increasing the number of covered targetscompared to the solution based on the NN heuristic.
arxiv-6300-111 | Indoor 3D Video Monitoring Using Multiple Kinect Depth-Cameras | http://arxiv.org/abs/1403.2895 | author:M. Martínez-Zarzuela, M. Pedraza-Hueso, F. J. Díaz-Pernas, D. González-Ortega, M. Antón-Rodríguez category:cs.CV published:2014-03-12 summary:This article describes the design and development of a system for remoteindoor 3D monitoring using an undetermined number of Microsoft(R) Kinectsensors. In the proposed client-server system, the Kinect cameras can beconnected to different computers, addressing this way the hardware limitationof one sensor per USB controller. The reason behind this limitation is the highbandwidth needed by the sensor, which becomes also an issue for the distributedsystem TCP/IP communications. Since traffic volume is too high, 3D data has tobe compressed before it can be sent over the network. The solution consists inselfcoding the Kinect data into RGB images and then using a standard multimediacodec to compress color maps. Information from different sources is collectedinto a central client computer, where point clouds are transformed toreconstruct the scene in 3D. An algorithm is proposed to merge the skeletonsdetected locally by each Kinect conveniently, so that monitoring of people isrobust to self and inter-user occlusions. Final skeletons are labeled andtrajectories of every joint can be saved for event reconstruction or furtheranalysis.
arxiv-6300-112 | A survey of dimensionality reduction techniques | http://arxiv.org/abs/1403.2877 | author:C. O. S. Sorzano, J. Vargas, A. Pascual Montano category:stat.ML cs.LG q-bio.QM published:2014-03-12 summary:Experimental life sciences like biology or chemistry have seen in the recentdecades an explosion of the data available from experiments. Laboratoryinstruments become more and more complex and report hundreds or thousandsmeasurements for a single experiment and therefore the statistical methods facechallenging tasks when dealing with such high dimensional data. However, muchof the data is highly redundant and can be efficiently brought down to a muchsmaller number of variables without a significant loss of information. Themathematical procedures making possible this reduction are calleddimensionality reduction techniques; they have widely been developed by fieldslike Statistics or Machine Learning, and are currently a hot research topic. Inthis review we categorize the plethora of dimension reduction techniquesavailable and give the mathematical insight behind them.
arxiv-6300-113 | Shape-Based Plagiarism Detection for Flowchart Figures in Texts | http://arxiv.org/abs/1403.2871 | author:Senosy Arrish, Fadhil Noer Afif, Ahmadu Maidorawa, Naomie Salim category:cs.CV cs.IR published:2014-03-12 summary:Plagiarism detection is well known phenomenon in the academic arena. Copyingother people is considered as serious offence that needs to be checked. Thereare many plagiarism detection systems such as turn-it-in that has beendeveloped to provide this checks. Most, if not all, discard the figures andcharts before checking for plagiarism. Discarding the figures and chartsresults in look holes that people can take advantage. That means people canplagiarized figures and charts easily without the current plagiarism systemsdetecting it. There are very few papers which talks about flowcharts plagiarismdetection. Therefore, there is a need to develop a system that will detectplagiarism in figures and charts. This paper presents a method for detectingflow chart figure plagiarism based on shape-based image processing andmultimedia retrieval. The method managed to retrieve flowcharts with rankedsimilarity according to different matching sets.
arxiv-6300-114 | Application of Particle Swarm Optimization to Microwave Tapered Microstrip Lines | http://arxiv.org/abs/1403.2842 | author:Ezgi Deniz Ulker, Sadik Ulker category:cs.NE published:2014-03-12 summary:Application of metaheuristic algorithms has been of continued interest in thefield of electrical engineering because of their powerful features. In thiswork special design is done for a tapered transmission line used for matchingan arbitrary real load to a 50{\Omega} line. The problem at hand is to matchthis arbitrary load to 50 {\Omega} line using three section taperedtransmission line with impedances in decreasing order from the load. So theproblem becomes optimizing an equation with three unknowns with variousconditions. The optimized values are obtained using Particle SwarmOptimization. It can easily be shown that PSO is very strong in solving thiskind of multiobjective optimization problems.
arxiv-6300-115 | HPS: a hierarchical Persian stemming method | http://arxiv.org/abs/1403.2837 | author:Ayshe Rashidi, Mina Zolfy Lighvan category:cs.CL published:2014-03-12 summary:In this paper, a novel hierarchical Persian stemming approach based on thePart-Of-Speech of the word in a sentence is presented. The implemented stemmerincludes hash tables and several deterministic finite automata in its differentlevels of hierarchy for removing the prefixes and suffixes of the words. We hadtwo intentions in using hash tables in our method. The first one is that theDFA don't support some special words, so hash table can partly solve theaddressed problem. the second goal is to speed up the implemented stemmer withomitting the time that deterministic finite automata need. Because of thehierarchical organization, this method is fast and flexible enough. Ourexperiments on test sets from Hamshahri collection and security news (istna.ir)show that our method has the average accuracy of 95.37% which is even improvedin using the method on a test set with common topics.
arxiv-6300-116 | Learning Deep Face Representation | http://arxiv.org/abs/1403.2802 | author:Haoqiang Fan, Zhimin Cao, Yuning Jiang, Qi Yin, Chinchilla Doudou category:cs.CV cs.LG published:2014-03-12 summary:Face representation is a crucial step of face recognition systems. An optimalface representation should be discriminative, robust, compact, and veryeasy-to-implement. While numerous hand-crafted and learning-basedrepresentations have been proposed, considerable room for improvement is stillpresent. In this paper, we present a very easy-to-implement deep learningframework for face representation. Our method bases on a new structure of deepnetwork (called Pyramid CNN). The proposed Pyramid CNN adopts agreedy-filter-and-down-sample operation, which enables the training procedureto be very fast and computation-efficient. In addition, the structure ofPyramid CNN can naturally incorporate feature sharing across multi-scale facerepresentations, increasing the discriminative ability of resultingrepresentation. Our basic network is capable of achieving high recognitionaccuracy ($85.8\%$ on LFW benchmark) with only 8 dimension representation. Whenextended to feature-sharing Pyramid CNN, our system achieves thestate-of-the-art performance ($97.3\%$) on LFW benchmark. We also introduce anew benchmark of realistic face images on social network and validate ourproposed representation has a good ability of generalization.
arxiv-6300-117 | Engaging with Massive Online Courses | http://arxiv.org/abs/1403.3100 | author:Ashton Anderson, Daniel Huttenlocher, Jon Kleinberg, Jure Leskovec category:cs.SI physics.soc-ph stat.ML H.2.8 published:2014-03-12 summary:The Web has enabled one of the most visible recent developments ineducation---the deployment of massive open online courses. With their globalreach and often staggering enrollments, MOOCs have the potential to become amajor new mechanism for learning. Despite this early promise, however, MOOCsare still relatively unexplored and poorly understood. In a MOOC, each student's complete interaction with the course materialstakes place on the Web, thus providing a record of learner activity ofunprecedented scale and resolution. In this work, we use such trace data todevelop a conceptual framework for understanding how users currently engagewith MOOCs. We develop a taxonomy of individual behavior, examine the differentbehavioral patterns of high- and low-achieving students, and investigate howforum participation relates to other parts of the course. We also report on a large-scale deployment of badges as incentives forengagement in a MOOC, including randomized experiments in which thepresentation of badges was varied across sub-populations. We find that makingbadges more salient produced increases in forum engagement.
arxiv-6300-118 | Parallel WiSARD object tracker: a ram-based tracking system | http://arxiv.org/abs/1403.3118 | author:Rodrigo da Silva Moreira, Nelson Francisco Favilla Ebecken category:cs.CV published:2014-03-12 summary:This paper proposes the Parallel WiSARD Object Tracker (PWOT), a new objecttracker based on the WiSARD weightless neural network that is robust againstquantization errors. Object tracking in video is an important and challengingtask in many applications. Difficulties can arise due to weather conditions,target trajectory and appearance, occlusions, lighting conditions and noise.Tracking is a high-level application and requires the object location frame byframe in real time. This paper proposes a fast hybrid image segmentation(threshold and edge detection) in YcbCr color model and a parallel RAM baseddiscriminator that improves efficiency when quantization errors occur. Theoriginal WiSARD training algorithm was changed to allow the tracking.
arxiv-6300-119 | Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling | http://arxiv.org/abs/1403.3080 | author:Xi Chen, Qihang Lin, Dengyong Zhou category:cs.LG math.OC stat.ML published:2014-03-12 summary:In crowd labeling, a large amount of unlabeled data instances are outsourcedto a crowd of workers. Workers will be paid for each label they provide, butthe labeling requester usually has only a limited amount of the budget. Sincedata instances have different levels of labeling difficulty and workers havedifferent reliability, it is desirable to have an optimal policy to allocatethe budget among all instance-worker pairs such that the overall labelingaccuracy is maximized. We consider categorical labeling tasks and formulate thebudget allocation problem as a Bayesian Markov decision process (MDP), whichsimultaneously conducts learning and decision making. Using the dynamicprogramming (DP) recurrence, one can obtain the optimal allocation policy.However, DP quickly becomes computationally intractable when the size of theproblem increases. To solve this challenge, we propose a computationallyefficient approximate policy, called optimistic knowledge gradient policy. OurMDP is a quite general framework, which applies to both pull crowdsourcingmarketplaces with homogeneous workers and push marketplaces with heterogeneousworkers. It can also incorporate the contextual information of instances whenthey are available. The experiments on both simulated and real data show thatthe proposed policy achieves a higher labeling accuracy than other existingpolicies at the same budget level.
arxiv-6300-120 | Optimal interval clustering: Application to Bregman clustering and statistical mixture learning | http://arxiv.org/abs/1403.2485 | author:Frank Nielsen, Richard Nock category:cs.IT cs.LG math.IT published:2014-03-11 summary:We present a generic dynamic programming method to compute the optimalclustering of $n$ scalar elements into $k$ pairwise disjoint intervals. Thiscase includes 1D Euclidean $k$-means, $k$-medoids, $k$-medians, $k$-centers,etc. We extend the method to incorporate cluster size constraints and show howto choose the appropriate $k$ by model selection. Finally, we illustrate andrefine the method on two case studies: Bregman clustering and statisticalmixture learning maximizing the complete likelihood.
arxiv-6300-121 | Removing Mixture of Gaussian and Impulse Noise by Patch-Based Weighted Means | http://arxiv.org/abs/1403.2482 | author:Haijuan Hu, Bing Li, Quansheng Liu category:cs.CV published:2014-03-11 summary:We first establish a law of large numbers and a convergence theorem indistribution to show the rate of convergence of the non-local means filter forremoving Gaussian noise. We then introduce the notion of degree of similarityto measure the role of similarity for the non-local means filter. Based on theconvergence theorems, we propose a patch-based weighted means filter forremoving impulse noise and its mixture with Gaussian noise by combining theessential idea of the trilateral filter and that of the non-local means filter.Our experiments show that our filter is competitive compared to recentlyproposed methods.
arxiv-6300-122 | Robust and Scalable Bayes via a Median of Subset Posterior Measures | http://arxiv.org/abs/1403.2660 | author:Stanislav Minsker, Sanvesh Srivastava, Lizhen Lin, David B. Dunson category:math.ST cs.DC cs.LG stat.TH published:2014-03-11 summary:We propose a novel approach to Bayesian analysis that is provably robust tooutliers in the data and often has computational advantages over standardmethods. Our technique is based on splitting the data into non-overlappingsubgroups, evaluating the posterior distribution given each independentsubgroup, and then combining the resulting measures. The main novelty of ourapproach is the proposed aggregation step, which is based on the evaluation ofa median in the space of probability measures equipped with a suitablecollection of distances that can be quickly and efficiently evaluated inpractice. We present both theoretical and numerical evidence illustrating theimprovements achieved by our method.
arxiv-6300-123 | Transfer Learning across Networks for Collective Classification | http://arxiv.org/abs/1403.2484 | author:Meng Fang, Jie Yin, Xingquan Zhu category:cs.LG cs.SI published:2014-03-11 summary:This paper addresses the problem of transferring useful knowledge from asource network to predict node labels in a newly formed target network. Whileexisting transfer learning research has primarily focused on vector-based data,in which the instances are assumed to be independent and identicallydistributed, how to effectively transfer knowledge across different informationnetworks has not been well studied, mainly because networks may have theirdistinct node features and link relationships between nodes. In this paper, wepropose a new transfer learning algorithm that attempts to transfer commonlatent structure features across the source and target networks. The proposedalgorithm discovers these latent features by constructing label propagationmatrices in the source and target networks, and mapping them into a sharedlatent feature space. The latent features capture common structure patternsshared by two networks, and serve as domain-independent features to betransferred between networks. Together with domain-dependent node features, wethereafter propose an iterative classification algorithm that leverages labelcorrelations to predict node labels in the target network. Experiments onreal-world networks demonstrate that our proposed algorithm can successfullyachieve knowledge transfer between networks to help improve the accuracy ofclassifying nodes in the target network.
arxiv-6300-124 | Image Fusion Techniques in Remote Sensing | http://arxiv.org/abs/1403.5473 | author:Reham Gharbia, Ahmad Taher Azar, Ali El Baz, Aboul Ella Hassanien category:cs.CV published:2014-03-11 summary:Remote sensing image fusion is an effective way to use a large volume of datafrom multisensor images. Most earth satellites such as SPOT, Landsat 7, IKONOSand QuickBird provide both panchromatic (Pan) images at a higher spatialresolution and multispectral (MS) images at a lower spatial resolution and manyremote sensing applications require both high spatial and high spectralresolutions, especially for GIS based applications. An effective image fusiontechnique can produce such remotely sensed images. Image fusion is thecombination of two or more different images to form a new image by using acertain algorithm to obtain more and better information about an object or astudy area than. The image fusion is performed at three different processinglevels which are pixel level, feature level and decision level according to thestage at which the fusion takes place. There are many image fusion methods thatcan be used to produce high resolution multispectral images from a highresolution pan image and low resolution multispectral images. This paperexplores the major remote sensing data fusion techniques at pixel level andreviews the concept, principals, limitations and advantages for each technique.This paper focused on traditional techniques like intensity hue-saturation-(HIS), Brovey, principal component analysis (PCA) and Wavelet.
arxiv-6300-125 | The Bursty Dynamics of the Twitter Information Network | http://arxiv.org/abs/1403.2732 | author:Seth A. Myers, Jure Leskovec category:cs.SI physics.soc-ph stat.ML published:2014-03-11 summary:In online social media systems users are not only posting, consuming, andresharing content, but also creating new and destroying existing connections inthe underlying social network. While each of these two types of dynamics hasindividually been studied in the past, much less is known about the connectionbetween the two. How does user information posting and seeking behaviorinteract with the evolution of the underlying social network structure? Here, we study ways in which network structure reacts to users posting andsharing content. We examine the complete dynamics of the Twitter informationnetwork, where users post and reshare information while they also create anddestroy connections. We find that the dynamics of network structure can becharacterized by steady rates of change, interrupted by sudden bursts.Information diffusion in the form of cascades of post re-sharing often createssuch sudden bursts of new connections, which significantly change users' localnetwork structure. These bursts transform users' networks of followers tobecome structurally more cohesive as well as more homogenous in terms offollower interests. We also explore the effect of the information content onthe dynamics of the network and find evidence that the appearance of new topicsand real-world events can lead to significant changes in edge creations anddeletions. Lastly, we develop a model that quantifies the dynamics of thenetwork and the occurrence of these bursts as a function of the informationspreading through the network. The model can successfully predict whichinformation diffusion events will lead to bursts in network dynamics.
arxiv-6300-126 | Flying Insect Classification with Inexpensive Sensors | http://arxiv.org/abs/1403.2654 | author:Yanping Chen, Adena Why, Gustavo Batista, Agenor Mafra-Neto, Eamonn Keogh category:cs.LG cs.CE 68T00 I.2.6 published:2014-03-11 summary:The ability to use inexpensive, noninvasive sensors to accurately classifyflying insects would have significant implications for entomological research,and allow for the development of many useful applications in vector control forboth medical and agricultural entomology. Given this, the last sixty years haveseen many research efforts on this task. To date, however, none of thisresearch has had a lasting impact. In this work, we explain this lack ofprogress. We attribute the stagnation on this problem to several factors,including the use of acoustic sensing devices, the over-reliance on the singlefeature of wingbeat frequency, and the attempts to learn complex models withrelatively little data. In contrast, we show that pseudo-acoustic opticalsensors can produce vastly superior data, that we can exploit additionalfeatures, both intrinsic and extrinsic to the insect's flight behavior, andthat a Bayesian classification approach allows us to efficiently learnclassification models that are very robust to over-fitting. We demonstrate ourfindings with large scale experiments that dwarf all previous works combined,as measured by the number of insects and the number of species considered.
arxiv-6300-127 | A-infinity Persistence | http://arxiv.org/abs/1403.2395 | author:Francisco Belchí Guillamón, Aniceto Murillo Mas category:math.AT cs.CG cs.CV published:2014-03-10 summary:We introduce and study A-infinity persistence of a given homology filtrationof topological spaces. This is a family, one for each n > 0, of homologicalinvariants which provide information not readily available by the (persistent)Betti numbers of the given filtration. This may help to detect noise, not justin the simplicial structure of the filtration but in further geometricalproperties in which the higher codiagonals of the A-infinity structure aretranslated. Based in the classification of zigzag modules, a characterizationof the A-infinity persistence in terms of its associated barcode is given.
arxiv-6300-128 | Adaptive Penalized Estimation of Directed Acyclic Graphs From Categorical Data | http://arxiv.org/abs/1403.2310 | author:Jiaying Gu, Fei Fu, Qing Zhou category:stat.ME stat.ML published:2014-03-10 summary:We develop in this article a penalized likelihood method to estimate sparseBayesian networks from categorical data. The structure of a Bayesian network isrepresented by a directed acyclic graph (DAG). We model the conditionaldistribution of a node given its parents by multi-logit regression and estimatethe structure of a DAG via maximizing a regularized likelihood. The adaptivegroup Lasso penalty is employed to encourage sparsity by selecting groupeddummy variables encoding the level of a factor. We develop a blockwisecoordinate descent algorithm to solve the penalized likelihood problem subjectto the acyclicity constraint of a DAG. When intervention data are available,our method may construct a causal network, in which a directed edge representsa causal relation. We apply our method to various simulated networks and a realbiological network. The results show that our method is very competitive,compared to other existing methods, in DAG estimation from both interventionaland high-dimensional observational data. We also establish consistency inparameter and structure estimation for our method when the number of nodes isfixed.
arxiv-6300-129 | Generating Music from Literature | http://arxiv.org/abs/1403.2124 | author:Hannah Davis, Saif M. Mohammad category:cs.CL published:2014-03-10 summary:We present a system, TransProse, that automatically generates musical piecesfrom text. TransProse uses known relations between elements of music such astempo and scale, and the emotions they evoke. Further, it uses a novelmechanism to determine sequences of notes that capture the emotional activityin the text. The work has applications in information visualization, increating audio-visual e-books, and in developing music apps.
arxiv-6300-130 | Generalised Mixability, Constant Regret, and Bayesian Updating | http://arxiv.org/abs/1403.2433 | author:Mark D. Reid, Rafael M. Frongillo, Robert C. Williamson category:cs.LG stat.ML published:2014-03-10 summary:Mixability of a loss is known to characterise when constant regret bounds areachievable in games of prediction with expert advice through the use of Vovk'saggregating algorithm. We provide a new interpretation of mixability via convexanalysis that highlights the role of the Kullback-Leibler divergence in itsdefinition. This naturally generalises to what we call $\Phi$-mixability wherethe Bregman divergence $D_\Phi$ replaces the KL divergence. We prove thatlosses that are $\Phi$-mixable also enjoy constant regret bounds via ageneralised aggregating algorithm that is similar to mirror descent.
arxiv-6300-131 | Phase Retrieval using Lipschitz Continuous Maps | http://arxiv.org/abs/1403.2301 | author:Radu Balan, Dongmian Zou category:math.FA cs.IT math.IT stat.ML published:2014-03-10 summary:In this note we prove that reconstruction from magnitudes of framecoefficients (the so called "phase retrieval problem") can be performed usingLipschitz continuous maps. Specifically we show that when the nonlinearanalysis map $\alpha:{\mathcal H}\rightarrow\mathbb{R}^m$ is injective, with$(\alpha(x))_k=<x,f_k>^2$, where $\{f_1,\ldots,f_m\}$ is a frame for theHilbert space ${\mathcal H}$, then there exists a left inverse map$\omega:\mathbb{R}^m\rightarrow {\mathcal H}$ that is Lipschitz continuous.Additionally we obtain the Lipschitz constant of this inverse map in terms ofthe lower Lipschitz constant of $\alpha$. Surprisingly the increase inLipschitz constant is independent of the space dimension or frame redundancy.
arxiv-6300-132 | Sublinear Models for Graphs | http://arxiv.org/abs/1403.2295 | author:Brijnesh J. Jain category:cs.LG cs.CV published:2014-03-10 summary:This contribution extends linear models for feature vectors to sublinearmodels for graphs and analyzes their properties. The results are (i) ageometric interpretation of sublinear classifiers, (ii) a generic learning rulebased on the principle of empirical risk minimization, (iii) a convergencetheorem for the margin perceptron in the sublinearly separable case, and (iv)the VC-dimension of sublinear functions. Empirical results on graph data showthat sublinear models on graphs have similar properties as linear models forfeature vectors.
arxiv-6300-133 | Parsing using a grammar of word association vectors | http://arxiv.org/abs/1403.2152 | author:Robert John Freeman category:cs.CL cs.NE published:2014-03-10 summary:This paper was was first drafted in 2001 as a formalization of the systemdescribed in U.S. patent U.S. 7,392,174. It describes a system for implementinga parser based on a kind of cross-product over vectors of contextually similarwords. It is being published now in response to nascent interest in vectorcombination models of syntax and semantics. The method used aggressivesubstitution of contextually similar words and word groups to enable productvectors to stay in the same space as their operands and make entire sentencescomparable syntactically, and potentially semantically. The vectors generatedhad sufficient representational strength to generate parse trees at leastcomparable with contemporary symbolic parsers.
arxiv-6300-134 | Constraint-based Causal Discovery from Multiple Interventions over Overlapping Variable Sets | http://arxiv.org/abs/1403.2150 | author:Sofia Triantafillou, Ioannis Tsamardinos category:stat.ML cs.AI published:2014-03-10 summary:Scientific practice typically involves repeatedly studying a system, eachtime trying to unravel a different perspective. In each study, the scientistmay take measurements under different experimental conditions (interventions,manipulations, perturbations) and measure different sets of quantities(variables). The result is a collection of heterogeneous data sets coming fromdifferent data distributions. In this work, we present algorithm COmbINE, whichaccepts a collection of data sets over overlapping variable sets underdifferent experimental conditions; COmbINE then outputs a summary of all causalmodels indicating the invariant and variant structural characteristics of allmodels that simultaneously fit all of the input data sets. COmbINE convertsestimated dependencies and independencies in the data into path constraints onthe data-generating causal model and encodes them as a SAT instance. Thealgorithm is sound and complete in the sample limit. To account for conflictingconstraints arising from statistical errors, we introduce a general method forsorting constraints in order of confidence, computed as a function of theircorresponding p-values. In our empirical evaluation, COmbINE outperforms interms of efficiency the only pre-existing similar algorithm; the latteradditionally admits feedback cycles, but does not admit conflicting constraintswhich hinders the applicability on real data. As a proof-of-concept, COmbINE isemployed to co-analyze 4 real, mass-cytometry data sets measuringphosphorylated protein concentrations of overlapping protein sets under 3different interventions.
arxiv-6300-135 | Categorization Axioms for Clustering Results | http://arxiv.org/abs/1403.2065 | author:Jian Yu, Zongben Xu category:cs.LG published:2014-03-09 summary:Cluster analysis has attracted more and more attention in the field ofmachine learning and data mining. Numerous clustering algorithms have beenproposed and are being developed due to diverse theories and variousrequirements of emerging applications. Therefore, it is very worth establishingan unified axiomatic framework for data clustering. In the literature, it is anopen problem and has been proved very challenging. In this paper, clusteringresults are axiomatized by assuming that an proper clustering result shouldsatisfy categorization axioms. The proposed axioms not only introduceclassification of clustering results and inequalities of clustering results,but also are consistent with prototype theory and exemplar theory ofcategorization models in cognitive science. Moreover, the proposed axioms leadto three principles of designing clustering algorithm and cluster validityindex, which follow many popular clustering algorithms and cluster validityindices.
arxiv-6300-136 | Generalized Canonical Correlation Analysis and Its Application to Blind Source Separation Based on a Dual-Linear Predictor Structure | http://arxiv.org/abs/1403.2073 | author:Wei Liu category:math.NA stat.ML published:2014-03-09 summary:Blind source separation (BSS) is one of the most important and establishedresearch topics in signal processing and many algorithms have been proposedbased on different statistical properties of the source signals. Forsecond-order statistics (SOS) based methods, canonical correlation analysis(CCA) has been proved to be an effective solution to the problem. In this work,the CCA approach is generalized to accommodate the case with added white noiseand it is then applied to the BSS problem for noisy mixtures. In this approach,the noise component is assumed to be spatially and temporally white, but thevariance information of noise is not required. An adaptive blind sourceextraction algorithm is derived based on this idea and a further extension isproposed by employing a dual-linear predictor structure for blind sourceextraction (BSE).
arxiv-6300-137 | Texture Defect Detection in Gradient Space | http://arxiv.org/abs/1403.2031 | author:V. Asha, N. U. Bhajantri, P. Nagabhushan category:cs.CV published:2014-03-09 summary:In this paper, we propose a machine vision algorithm for automaticallydetecting defects in patterned textures with the help of gradient space and itsenergy. Experiments on real fabric images with defects show that the proposedmethod can be used for automatic detection of fabric defects in textileindustries.
arxiv-6300-138 | Predictive Overlapping Co-Clustering | http://arxiv.org/abs/1403.1942 | author:Chandrima Sarkar, Jaideep Srivastava category:cs.LG published:2014-03-08 summary:In the past few years co-clustering has emerged as an important data miningtool for two way data analysis. Co-clustering is more advantageous overtraditional one dimensional clustering in many ways such as, ability to findhighly correlated sub-groups of rows and columns. However, one of theoverlooked benefits of co-clustering is that, it can be used to extractmeaningful knowledge for various other knowledge extraction purposes. Forexample, building predictive models with high dimensional data andheterogeneous population is a non-trivial task. Co-clusters extracted from suchdata, which shows similar pattern in both the dimension, can be used for a moreaccurate predictive model building. Several applications such as findingpatient-disease cohorts in health care analysis, finding user-genre groups inrecommendation systems and community detection problems can benefit fromco-clustering technique that utilizes the predictive power of the data togenerate co-clusters for improved data analysis. In this paper, we present the novel idea of Predictive OverlappingCo-Clustering (POCC) as an optimization problem for a more effective andimproved predictive analysis. Our algorithm generates optimal co-clusters bymaximizing predictive power of the co-clusters subject to the constraints onthe number of row and column clusters. In this paper precision, recall andf-measure have been used as evaluation measures of the resulting co-clusters.Results of our algorithm has been compared with two other well-known techniques- K-means and Spectral co-clustering, over four real data set namely, Leukemia,Internet-Ads, Ovarian cancer and MovieLens data set. The results demonstratethe effectiveness and utility of our algorithm POCC in practice.
arxiv-6300-139 | Natural Language Feature Selection via Cooccurrence | http://arxiv.org/abs/1403.2004 | author:Michael Stewart category:cs.CL published:2014-03-08 summary:Specificity is important for extracting collocations, keyphrases, multi-wordand index terms [Newman et al. 2012]. It is also useful for tagging, ontologyconstruction [Ryu and Choi 2006], and automatic summarization of documents[Louis and Nenkova 2011, Chali and Hassan 2012]. Term frequency andinverse-document frequency (TF-IDF) are typically used to do this, but fail totake advantage of the semantic relationships between terms [Church and Gale1995]. The result is that general idiomatic terms are mistaken for specificterms. We demonstrate use of relational data for estimation of termspecificity. The specificity of a term can be learned from its distribution ofrelations with other terms. This technique is useful for identifying relevantwords or terms for other natural language processing tasks.
arxiv-6300-140 | Designing an FPGA Synthesizable Computer Vision Algorithm to Detect the Greening of Potatoes | http://arxiv.org/abs/1403.1974 | author:Jaspinder Pal Singh category:cs.CV published:2014-03-08 summary:Potato quality control has improved in the last years thanks to automationtechniques like machine vision, mainly making the classification task betweendifferent quality degrees faster, safer and less subjective. In our study weare going to design a computer vision algorithm for grading of potatoesaccording to the greening of the surface color of potato. The ratio of greenpixels to the total number of pixels of the potato surface is found. The higherthe ratio the worse is the potato. First the image is converted into serialdata and then processing is done in RGB colour space. Green part of the potatois also shown by de-serializing the output. The same algorithm is thensynthesized on FPGA and the result shows thousand times speed improvement incase of hardware synthesis.
arxiv-6300-141 | Combination of PCA with SMOTE Resampling to Boost the Prediction Rate in Lung Cancer Dataset | http://arxiv.org/abs/1403.1949 | author:Mehdi Naseriparsa, Mohammad Mansour Riahi Kashani category:cs.LG cs.CE published:2014-03-08 summary:Classification algorithms are unable to make reliable models on the datasetswith huge sizes. These datasets contain many irrelevant and redundant featuresthat mislead the classifiers. Furthermore, many huge datasets have imbalancedclass distribution which leads to bias over majority class in theclassification process. In this paper combination of unsuperviseddimensionality reduction methods with resampling is proposed and the resultsare tested on Lung-Cancer dataset. In the first step PCA is applied onLung-Cancer dataset to compact the dataset and eliminate irrelevant featuresand in the second step SMOTE resampling is carried out to balance the classdistribution and increase the variety of sample domain. Finally, Naive Bayesclassifier is applied on the resulting dataset and the results are compared andevaluation metrics are calculated. The experiments show the effectiveness ofthe proposed method across four evaluation metrics: Overall accuracy, FalsePositive Rate, Precision, Recall.
arxiv-6300-142 | A Hybrid Feature Selection Method to Improve Performance of a Group of Classification Algorithms | http://arxiv.org/abs/1403.2372 | author:Mehdi Naseriparsa, Amir-Masoud Bidgoli, Touraj Varaee category:cs.LG published:2014-03-08 summary:In this paper a hybrid feature selection method is proposed which takesadvantages of wrapper subset evaluation with a lower cost and improves theperformance of a group of classifiers. The method uses combination of sampledomain filtering and resampling to refine the sample domain and two featuresubset evaluation methods to select reliable features. This method utilizesboth feature space and sample domain in two phases. The first phase filters andresamples the sample domain and the second phase adopts a hybrid procedure byinformation gain, wrapper subset evaluation and genetic search to find theoptimal feature space. Experiments carried out on different types of datasetsfrom UCI Repository of Machine Learning databases and the results show a risein the average performance of five classifiers (Naive Bayes, Logistic,Multilayer Perceptron, Best First Decision Tree and JRIP) simultaneously andthe classification error for these classifiers decreases considerably. Theexperiments also show that this method outperforms other feature selectionmethods with a lower cost.
arxiv-6300-143 | Improving Performance of a Group of Classification Algorithms Using Resampling and Feature Selection | http://arxiv.org/abs/1403.1946 | author:Mehdi Naseriparsa, Amir-masoud Bidgoli, Touraj Varaee category:cs.LG published:2014-03-08 summary:In recent years the importance of finding a meaningful pattern from hugedatasets has become more challenging. Data miners try to adopt innovativemethods to face this problem by applying feature selection methods. In thispaper we propose a new hybrid method in which we use a combination ofresampling, filtering the sample domain and wrapper subset evaluation methodwith genetic search to reduce dimensions of Lung-Cancer dataset that wereceived from UCI Repository of Machine Learning databases. Finally, we applysome well- known classification algorithms (Na\"ive Bayes, Logistic, MultilayerPerceptron, Best First Decision Tree and JRIP) to the resulting dataset andcompare the results and prediction rates before and after the application ofour feature selection method on that dataset. The results show a substantialprogress in the average performance of five classification algorithmssimultaneously and the classification error for these classifiers decreasesconsiderably. The experiments also show that this method outperforms otherfeature selection methods with a lower cost.
arxiv-6300-144 | Quality-based Multimodal Classification Using Tree-Structured Sparsity | http://arxiv.org/abs/1403.1902 | author:Soheil Bahrampour, Asok Ray, Nasser M. Nasrabadi, Kenneth W. Jenkins category:cs.CV published:2014-03-08 summary:Recent studies have demonstrated advantages of information fusion based onsparsity models for multimodal classification. Among several sparsity models,tree-structured sparsity provides a flexible framework for extraction ofcross-correlated information from different sources and for enforcing groupsparsity at multiple granularities. However, the existing algorithm only solvesan approximated version of the cost functional and the resulting solution isnot necessarily sparse at group levels. This paper reformulates thetree-structured sparse model for multimodal classification task. An acceleratedproximal algorithm is proposed to solve the optimization problem, which is anefficient tool for feature-level fusion among either homogeneous orheterogeneous sources of information. In addition, a (fuzzy-set-theoretic)possibilistic scheme is proposed to weight the available modalities, based ontheir respective reliability, in a joint optimization problem for finding thesparsity codes. This approach provides a general framework for quality-basedfusion that offers added robustness to several sparsity-based multimodalclassification algorithms. To demonstrate their efficacy, the proposed methodsare evaluated on three different applications - multiview face recognition,multimodal face recognition, and target classification.
arxiv-6300-145 | A fast eikonal equation solver using the Schrodinger wave equation | http://arxiv.org/abs/1403.1937 | author:Karthik S. Gurumoorthy, Adrian M. Peter, Birmingham Hang Guan, Anand Rangarajan category:math.NA cs.CV cs.NA published:2014-03-08 summary:We use a Schr\"odinger wave equation formalism to solve the eikonal equation.In our framework, a solution to the eikonal equation is obtained in the limitas Planck's constant $\hbar$ (treated as a free parameter) tends to zero of thesolution to the corresponding linear Schr\"odinger equation. The Schr\"odingerequation corresponding to the eikonal turns out to be a \emph{generalized,screened Poisson equation}. Despite being linear, it does not have aclosed-form solution for arbitrary forcing functions. We present two differenttechniques to solve the screened Poisson equation. In the first approach we usea standard perturbation analysis approach to derive a new algorithm which isguaranteed to converge provided the forcing function is bounded and positive.The perturbation technique requires a sequence of discrete convolutions whichcan be performed in $O(N\log N)$ using the Fast Fourier Transform (FFT) where$N$ is the number of grid points. In the second method we discretize the linearLaplacian operator by the finite difference method leading to a sparse linearsystem of equations which can be solved using the plethora of sparse solvers.The eikonal solution is recovered from the exponent of the resultant scalarfield. Our approach eliminates the need to explicitly construct viscositysolutions as customary with direct solutions to the eikonal. Since the linearequation is computed for a small but non-zero $\hbar$, the obtained solution isan approximation. Though our solution framework is applicable to the generalclass of eikonal problems, we detail specifics for the popular visionapplications of shape-from-shading, vessel segmentation, and path planning.
arxiv-6300-146 | Multi-label ensemble based on variable pairwise constraint projection | http://arxiv.org/abs/1403.1944 | author:Ping Li, Hong Li, Min Wu category:cs.LG cs.CV stat.ML I.2.6 published:2014-03-08 summary:Multi-label classification has attracted an increasing amount of attention inrecent years. To this end, many algorithms have been developed to classifymulti-label data in an effective manner. However, they usually do not considerthe pairwise relations indicated by sample labels, which actually playimportant roles in multi-label classification. Inspired by this, we naturallyextend the traditional pairwise constraints to the multi-label scenario via aflexible thresholding scheme. Moreover, to improve the generalization abilityof the classifier, we adopt a boosting-like strategy to construct a multi-labelensemble from a group of base classifiers. To achieve these goals, this paperpresents a novel multi-label classification framework named Variable PairwiseConstraint projection for Multi-label Ensemble (VPCME). Specifically, we takeadvantage of the variable pairwise constraint projection to learn alower-dimensional data representation, which preserves the correlations betweensamples and labels. Thereafter, the base classifiers are trained in the newdata space. For the boosting-like strategy, we employ both the variablepairwise constraints and the bootstrap steps to diversify the base classifiers.Empirical studies have shown the superiority of the proposed method incomparison with other approaches.
arxiv-6300-147 | Rigid-Motion Scattering for Texture Classification | http://arxiv.org/abs/1403.1687 | author:Laurent SIfre, Stéphane Mallat category:cs.CV published:2014-03-07 summary:A rigid-motion scattering computes adaptive invariants along translations androtations, with a deep convolutional network. Convolutions are calculated onthe rigid-motion group, with wavelets defined on the translation and rotationvariables. It preserves joint rotation and translation information, whileproviding global invariants at any desired scale. Texture classification isstudied, through the characterization of stationary processes from a singlerealization. State-of-the-art results are obtained on multiple texture databases, with important rotation and scaling variabilities.
arxiv-6300-148 | Compressive Hyperspectral Imaging Using Progressive Total Variation | http://arxiv.org/abs/1403.1697 | author:Simeon Kamdem Kuiteing, Giulio Coluccia, Alessandro Barducci, Mauro Barni, Enrico Magli category:cs.IT cs.CV math.IT published:2014-03-07 summary:Compressed Sensing (CS) is suitable for remote acquisition of hyperspectralimages for earth observation, since it could exploit the strong spatial andspectral correlations, llowing to simplify the architecture of the onboardsensors. Solutions proposed so far tend to decouple spatial and spectraldimensions to reduce the complexity of the reconstruction, not taking intoaccount that onboard sensors progressively acquire spectral rows rather thanacquiring spectral channels. For this reason, we propose a novel progressive CSarchitecture based on separate sensing of spectral rows and jointreconstruction employing Total Variation. Experimental results run on rawAVIRIS and AIRS images confirm the validity of the proposed system.
arxiv-6300-149 | On the Sequence of State Configurations in the Garden of Eden | http://arxiv.org/abs/1403.1727 | author:Yukihiro Kamada, Kiyonori Miyasaki category:cs.NE published:2014-03-07 summary:Autonomous threshold element circuit networks are used to investigate thestructure of neural networks. With these circuits, as the transition functionsare threshold functions, it is necessary to consider the existence of sequencesof state configurations that cannot be transitioned. In this study, we focus onall logical functions of four or fewer variables, and we discuss the periodicsequences and transient series that transition from all sequences of stateconfigurations. Furthermore, by using the sequences of state configurations inthe Garden of Eden, we show that it is easy to obtain functions that determinethe operation of circuit networks.
arxiv-6300-150 | Continuous Features Discretization for Anomaly Intrusion Detectors Generation | http://arxiv.org/abs/1403.1729 | author:Amira Sayed A. Aziz, Ahmad Taher Azar, Aboul Ella Hassanien, Sanaa Al-Ola Hanafy category:cs.NI cs.CR cs.NE published:2014-03-07 summary:Network security is a growing issue, with the evolution of computer systemsand expansion of attacks. Biological systems have been inspiring scientists anddesigns for new adaptive solutions, such as genetic algorithms. In this paper,we present an approach that uses the genetic algorithm to generate anomaly net-work intrusion detectors. In this paper, an algorithm propose use adiscretization method for the continuous features selected for the intrusiondetection, to create some homogeneity between values, which have different datatypes. Then,the intrusion detection system is tested against the NSL-KDD dataset using different distance methods. A comparison is held amongst the results,and it is shown by the end that this proposed approach has good results, andrecommendations is given for future experiments.
arxiv-6300-151 | Ant Colony based Feature Selection Heuristics for Retinal Vessel Segmentation | http://arxiv.org/abs/1403.1735 | author:Ahmed. H. Asad, Ahmad Taher Azar, Nashwa El-Bendary, Aboul Ella Hassaanien category:cs.NE cs.CV published:2014-03-07 summary:Features selection is an essential step for successful data classification,since it reduces the data dimensionality by removing redundant features.Consequently, that minimizes the classification complexity and time in additionto maximizing its accuracy. In this article, a comparative study consideringsix features selection heuristics is conducted in order to select the bestrelevant features subset. The tested features vector consists of fourteenfeatures that are computed for each pixel in the field of view of retinalimages in the DRIVE database. The comparison is assessed in terms ofsensitivity, specificity, and accuracy measurements of the recommended featuressubset resulted by each heuristic when applied with the ant colony system.Experimental results indicated that the features subset recommended by therelief heuristic outperformed the subsets recommended by the other experiencedheuristics.
arxiv-6300-152 | Feature Extraction of ECG Signal Using HHT Algorithm | http://arxiv.org/abs/1403.1660 | author:Neha Soorma, Jaikaran Singh, Mukesh Tiwari category:cs.CV published:2014-03-07 summary:This paper describe the features extraction algorithm for electrocardiogram(ECG) signal using Huang Hilbert Transform and Wavelet Transform. ECG signalfor an individual human being is different due to unique heart structure. Thepurpose of feature extraction of ECG signal would allow successful abnormalitydetection and efficient prognosis due to heart disorder. Some major importantfeatures will be extracted from ECG signals such as amplitude, duration,pre-gradient, post-gradient and so on. Therefore, we need a strong mathematicalmodel to extract such useful parameter. Here an adaptive mathematical analysismodel is Hilbert-Huang transform (HHT). This new approach, the Hilbert-Huangtransform, is implemented to analyze the non-linear and nonstationary data. Itis unique and different from the existing methods of data analysis and does notrequire an a priori functional basis. The effectiveness of the proposed schemeis verified through the simulation.
arxiv-6300-153 | Finding Eyewitness Tweets During Crises | http://arxiv.org/abs/1403.1773 | author:Fred Morstatter, Nichola Lubold, Heather Pon-Barry, Jürgen Pfeffer, Huan Liu category:cs.CL cs.CY published:2014-03-07 summary:Disaster response agencies have started to incorporate social media as asource of fast-breaking information to understand the needs of people affectedby the many crises that occur around the world. These agencies look for tweetsfrom within the region affected by the crisis to get the latest updates of thestatus of the affected region. However only 1% of all tweets are geotagged withexplicit location information. First responders lose valuable informationbecause they cannot assess the origin of many of the tweets they collect. Inthis work we seek to identify non-geotagged tweets that originate from withinthe crisis region. Towards this, we address three questions: (1) is there adifference between the language of tweets originating within a crisis regionand tweets originating outside the region, (2) what are the linguistic patternsthat can be used to differentiate within-region and outside-region tweets, and(3) for non-geotagged tweets, can we automatically identify those originatingwithin the crisis region in real-time?
arxiv-6300-154 | Home Location Identification of Twitter Users | http://arxiv.org/abs/1403.2345 | author:Jalal Mahmud, Jeffrey Nichols, Clemens Drews category:cs.SI cs.CL cs.CY published:2014-03-07 summary:We present a new algorithm for inferring the home location of Twitter usersat different granularities, including city, state, time zone or geographicregion, using the content of users tweets and their tweeting behavior. Unlikeexisting approaches, our algorithm uses an ensemble of statistical andheuristic classifiers to predict locations and makes use of a geographicgazetteer dictionary to identify place-name entities. We find that ahierarchical classification approach, where time zone, state or geographicregion is predicted first and city is predicted next, can improve predictionaccuracy. We have also analyzed movement variations of Twitter users, built aclassifier to predict whether a user was travelling in a certain period of timeand use that to further improve the location detection accuracy. Experimentalevidence suggests that our algorithm works well in practice and outperforms thebest existing algorithms for predicting the home location of Twitter users.
arxiv-6300-155 | Statistical Structure Learning, Towards a Robust Smart Grid | http://arxiv.org/abs/1403.1863 | author:Hanie Sedghi, Edmond Jonckheere category:cs.LG cs.SY published:2014-03-07 summary:Robust control and maintenance of the grid relies on accurate data. Both PMUsand state estimators are prone to false data injection attacks. Thus, it iscrucial to have a mechanism for fast and accurate detection of an agentmaliciously tampering with the data---for both preventing attacks that may leadto blackouts, and for routine monitoring and control tasks of current andfuture grids. We propose a decentralized false data injection detection schemebased on Markov graph of the bus phase angles. We utilize the ConditionalCovariance Test (CCT) to learn the structure of the grid. Using the DC powerflow model, we show that under normal circumstances, and because ofwalk-summability of the grid graph, the Markov graph of the voltage angles canbe determined by the power grid graph. Therefore, a discrepancy betweencalculated Markov graph and learned structure should trigger the alarm. Localgrid topology is available online from the protection system and we exploit itto check for mismatch. Should a mismatch be detected, we use correlationanomaly score to detect the set of attacked nodes. Our method can detect themost recent stealthy deception attack on the power grid that assumes knowledgeof bus-branch model of the system and is capable of deceiving the stateestimator, damaging power network observatory, control, monitoring, demandresponse and pricing schemes. Specifically, under the stealthy deceptionattack, the Markov graph of phase angles changes. In addition to detect a stateof attack, our method can detect the set of attacked nodes. To the best of ourknowledge, our remedy is the first to comprehensively detect this sophisticatedattack and it does not need additional hardware. Moreover, our detection schemeis successful no matter the size of the attacked subset. Simulation of variouspower networks confirms our claims.
arxiv-6300-156 | Becoming More Robust to Label Noise with Classifier Diversity | http://arxiv.org/abs/1403.1893 | author:Michael R. Smith, Tony Martinez category:stat.ML cs.AI cs.LG published:2014-03-07 summary:It is widely known in the machine learning community that class noise can be(and often is) detrimental to inducing a model of the data. Many currentapproaches use a single, often biased, measurement to determine if an instanceis noisy. A biased measure may work well on certain data sets, but it can alsobe less effective on a broader set of data sets. In this paper, we presentnoise identification using classifier diversity (NICD) -- a method for derivinga less biased noise measurement and integrating it into the learning process.To lessen the bias of the noise measure, NICD selects a diverse set ofclassifiers (based on their predictions of novel instances) to determine whichinstances are noisy. We examine NICD as a technique for filtering, instanceweighting, and selecting the base classifiers of a voting ensemble. We compareNICD with several other noise handling techniques that do not considerclassifier diversity on a set of 54 data sets and 5 learning algorithms. NICDsignificantly increases the classification accuracy over the other consideredapproaches and is effective across a broad set of data sets and learningalgorithms.
arxiv-6300-157 | Automated Tracking and Estimation for Control of Non-rigid Cloth | http://arxiv.org/abs/1403.1653 | author:Marc D. Killpack category:cs.CV published:2014-03-07 summary:This report is a summary of research conducted on cloth tracking forautomated textile manufacturing during a two semester long research course atGeorgia Tech. This work was completed in 2009. Advances in current sensingtechnology such as the Microsoft Kinect would now allow me to relax certainassumptions and generally improve the tracking performance. This is because amajor part of my approach described in this paper was to track features in a 2Dimage and use these to estimate the cloth deformation. Innovations such as theKinect would improve estimation due to the automatic depth information obtainedwhen tracking 2D pixel locations. Additionally, higher resolution camera imageswould probably give better quality feature tracking. However, although I woulduse different technology now to implement this tracker, the algorithm describedand implemented in this paper is still a viable approach which is why I ampublishing this as a tech report for reference. In addition, although therelated work is a bit exhaustive, it will be useful to a reader who is new tomethods for tracking and estimation as well as modeling of cloth.
arxiv-6300-158 | Subspace Clustering by Exploiting a Low-Rank Representation with a Symmetric Constraint | http://arxiv.org/abs/1403.2330 | author:Jie Chen, Zhang Yi category:cs.CV published:2014-03-07 summary:In this paper, we propose a low-rank representation with symmetric constraint(LRRSC) method for robust subspace clustering. Given a collection of datapoints approximately drawn from multiple subspaces, the proposed technique cansimultaneously recover the dimension and members of each subspace. LRRSCextends the original low-rank representation algorithm by integrating asymmetric constraint into the low-rankness property of high-dimensional datarepresentation. The symmetric low-rank representation, which preserves thesubspace structures of high-dimensional data, guarantees weight consistency foreach pair of data points so that highly correlated data points of subspaces arerepresented together. Moreover, it can be efficiently calculated by solving aconvex optimization problem. We provide a rigorous proof for minimizing thenuclear-norm regularized least square problem with a symmetric constraint. Theaffinity matrix for spectral clustering can be obtained by further exploitingthe angular information of the principal directions of the symmetric low-rankrepresentation. This is a critical step towards evaluating the membershipsbetween data points. Experimental results on benchmark databases demonstratethe effectiveness and robustness of LRRSC compared with severalstate-of-the-art subspace clustering algorithms.
arxiv-6300-159 | Counterfactual Estimation and Optimization of Click Metrics for Search Engines | http://arxiv.org/abs/1403.1891 | author:Lihong Li, Shunbao Chen, Jim Kleban, Ankur Gupta category:cs.LG cs.AI stat.AP stat.ML G.3; H.3.3 published:2014-03-07 summary:Optimizing an interactive system against a predefined online metric isparticularly challenging, when the metric is computed from user feedback suchas clicks and payments. The key challenge is the counterfactual nature: in thecase of Web search, any change to a component of the search engine may resultin a different search result page for the same query, but we normally cannotinfer reliably from search log how users would react to the new result page.Consequently, it appears impossible to accurately estimate online metrics thatdepend on user feedback, unless the new engine is run to serve users andcompared with a baseline in an A/B test. This approach, while valid andsuccessful, is unfortunately expensive and time-consuming. In this paper, wepropose to address this problem using causal inference techniques, under thecontextual-bandit framework. This approach effectively allows one to run(potentially infinitely) many A/B tests offline from search log, making itpossible to estimate and optimize online metrics quickly and inexpensively.Focusing on an important component in a commercial search engine, we show howthese ideas can be instantiated and applied, and obtain very promising resultsthat suggest the wide applicability of these techniques.
arxiv-6300-160 | Multi-scale Orderless Pooling of Deep Convolutional Activation Features | http://arxiv.org/abs/1403.1840 | author:Yunchao Gong, Liwei Wang, Ruiqi Guo, Svetlana Lazebnik category:cs.CV published:2014-03-07 summary:Deep convolutional neural networks (CNN) have shown their promise as auniversal representation for recognition. However, global CNN activations lackgeometric invariance, which limits their robustness for classification andmatching of highly variable scenes. To improve the invariance of CNNactivations without degrading their discriminative power, this paper presents asimple but effective scheme called multi-scale orderless pooling (MOP-CNN).This scheme extracts CNN activations for local patches at multiple scalelevels, performs orderless VLAD pooling of these activations at each levelseparately, and concatenates the result. The resulting MOP-CNN representationcan be used as a generic feature for either supervised or unsupervisedrecognition tasks, from image classification to instance-level retrieval; itconsistently outperforms global CNN activations without requiring any jointtraining of prediction layers for a particular target dataset. In absoluteterms, it achieves state-of-the-art results on the challenging SUN397 and MITIndoor Scenes classification datasets, and competitive results onILSVRC2012/2013 classification and INRIA Holidays retrieval datasets.
arxiv-6300-161 | Can Image-Level Labels Replace Pixel-Level Labels for Image Parsing | http://arxiv.org/abs/1403.1626 | author:Zhiwu Lu, Zhenyong Fu, Tao Xiang, Liwei Wang, Ji-Rong Wen category:cs.CV published:2014-03-07 summary:This paper presents a weakly supervised sparse learning approach to theproblem of noisily tagged image parsing, or segmenting all the objects within anoisily tagged image and identifying their categories (i.e. tags). Differentfrom the traditional image parsing that takes pixel-level labels as strongsupervisory information, our noisily tagged image parsing is provided withnoisy tags of all the images (i.e. image-level labels), which is a naturalsetting for social image collections (e.g. Flickr). By oversegmenting all theimages into regions, we formulate noisily tagged image parsing as a weaklysupervised sparse learning problem over all the regions, where the initiallabels of each region are inferred from image-level labels. Furthermore, wedevelop an efficient algorithm to solve such weakly supervised sparse learningproblem. The experimental results on two benchmark datasets show theeffectiveness of our approach. More notably, the reported surprising resultsshed some light on answering the question: can image-level labels replacepixel-level labels (hard to access) as supervisory information for imageparsing.
arxiv-6300-162 | Real-Time Classification of Twitter Trends | http://arxiv.org/abs/1403.1451 | author:Arkaitz Zubiaga, Damiano Spina, Raquel Martínez, Víctor Fresno category:cs.IR cs.CL cs.SI published:2014-03-06 summary:Social media users give rise to social trends as they share about commoninterests, which can be triggered by different reasons. In this work, weexplore the types of triggers that spark trends on Twitter, introducing atypology with following four types: 'news', 'ongoing events', 'memes', and'commemoratives'. While previous research has analyzed trending topics in along term, we look at the earliest tweets that produce a trend, with the aim ofcategorizing trends early on. This would allow to provide a filtered subset oftrends to end users. We analyze and experiment with a set of straightforwardlanguage-independent features based on the social spread of trends tocategorize them into the introduced typology. Our method provides an efficientway to accurately categorize trending topics without need of external data,enabling news organizations to discover breaking news in real-time, or toquickly identify viral memes that might enrich marketing decisions, amongothers. The analysis of social features also reveals patterns associated witheach type of trend, such as tweets about ongoing events being shorter as manywere likely sent from mobile devices, or memes having more retweets originatingfrom a few trend-setters.
arxiv-6300-163 | Illumination,Expression and Occlusion Invariant Pose-Adaptive Face Recognition System for Real-Time Applications | http://arxiv.org/abs/1403.1362 | author:Shireesha Chintalapati, M. V. Raghunadh category:cs.CV published:2014-03-06 summary:Face recognition in real-time scenarios is mainly affected by illumination,expression and pose variations and also by occlusion. This paper presents theframework for pose adaptive component-based face recognition system. Theframework proposed deals with all the above mentioned issues. The stepsinvolved in the presented framework are (i) facial landmark localisation, (ii)facial component extraction, (iii) pre-processing of facial image (iv) facialpose estimation (v) feature extraction using Local Binary Pattern Histograms ofeach component followed by (vi) fusion of pose adaptive classification ofcomponents. By employing pose adaptive classification, the recognition processis carried out on some part of database, based on estimated pose, instead ofapplying the recognition process on the whole database. Pre-processingtechniques employed to overcome the problems due to illumination variation arealso discussed in this paper. Component-based techniques provide betterrecognition rates when face images are occluded compared to the holisticmethods. Our method is simple, feasible and provides better results whencompared to other holistic methods.
arxiv-6300-164 | New Perspectives on k-Support and Cluster Norms | http://arxiv.org/abs/1403.1481 | author:Andrew M. McDonald, Massimiliano Pontil, Dimitris Stamos category:stat.ML published:2014-03-06 summary:The $k$-support norm is a regularizer which has been successfully applied tosparse vector prediction problems. We show that it belongs to a general classof norms which can be formulated as a parameterized infimum over quadratics. Wefurther extend the $k$-support norm to matrices, and we observe that it is aspecial case of the matrix cluster norm. Using this formulation we derive anefficient algorithm to compute the proximity operator of both norms. Thisimproves upon the standard algorithm for the $k$-support norm and allows us toapply proximal gradient methods to the cluster norm. We also describe how tosolve regularization problems which employ centered versions of these norms.Finally, we apply the matrix regularizers to different matrix completion andmultitask learning datasets. Our results indicate that the spectral $k$-supportnorm and the cluster norm give state of the art performance on these problems,significantly outperforming trace norm and elastic net penalties.
arxiv-6300-165 | Collaborative Representation for Classification, Sparse or Non-sparse? | http://arxiv.org/abs/1403.1353 | author:Yang Wu, Vansteenberge Jarich, Masayuki Mukunoki, Michihiko Minoh category:cs.CV cs.AI cs.LG published:2014-03-06 summary:Sparse representation based classification (SRC) has been proved to be asimple, effective and robust solution to face recognition. As it gets popular,doubts on the necessity of enforcing sparsity starts coming up, and primaryexperimental results showed that simply changing the $l_1$-norm basedregularization to the computationally much more efficient $l_2$-norm basednon-sparse version would lead to a similar or even better performance. However,that's not always the case. Given a new classification task, it's still unclearwhich regularization strategy (i.e., making the coefficients sparse ornon-sparse) is a better choice without trying both for comparison. In thispaper, we present as far as we know the first study on solving this issue,based on plenty of diverse classification experiments. We propose a scoringfunction for pre-selecting the regularization strategy using only the datasetsize, the feature dimensionality and a discrimination score derived from agiven feature representation. Moreover, we show that when dictionary learningis taking into account, non-sparse representation has a more significantsuperiority to sparse representation. This work is expected to enrich ourunderstanding of sparse/non-sparse collaborative representation forclassification and motivate further research activities.
arxiv-6300-166 | Deep Supervised and Convolutional Generative Stochastic Network for Protein Secondary Structure Prediction | http://arxiv.org/abs/1403.1347 | author:Jian Zhou, Olga G. Troyanskaya category:q-bio.QM cs.CE cs.LG published:2014-03-06 summary:Predicting protein secondary structure is a fundamental problem in proteinstructure prediction. Here we present a new supervised generative stochasticnetwork (GSN) based method to predict local secondary structure with deephierarchical representations. GSN is a recently proposed deep learningtechnique (Bengio & Thibodeau-Laufer, 2013) to globally train deep generativemodel. We present the supervised extension of GSN, which learns a Markov chainto sample from a conditional distribution, and applied it to protein structureprediction. To scale the model to full-sized, high-dimensional data, likeprotein sequences with hundreds of amino acids, we introduce a convolutionalarchitecture, which allows efficient learning across multiple layers ofhierarchical representations. Our architecture uniquely focuses on predictingstructured low-level labels informed with both low and high-levelrepresentations learned by the model. In our application this corresponds tolabeling the secondary structure state of each amino-acid residue. We trainedand tested the model on separate sets of non-homologous proteins sharing lessthan 30% sequence identity. Our model achieves 66.4% Q8 accuracy on the CB513dataset, better than the previously reported best performance 64.9% (Wang etal., 2011) for this challenging secondary structure prediction problem.
arxiv-6300-167 | Minimax Optimal Bayesian Aggregation | http://arxiv.org/abs/1403.1345 | author:Yun Yang, David B. Dunson category:math.ST stat.ME stat.ML stat.TH published:2014-03-06 summary:It is generally believed that ensemble approaches, which combine multiplealgorithms or models, can outperform any single algorithm at machine learningtasks, such as prediction. In this paper, we propose Bayesian convex and linearaggregation approaches motivated by regression applications. We show that theproposed approach is minimax optimal when the true data-generating model is aconvex or linear combination of models in the list. Moreover, the method canadapt to sparsity structure in which certain models should receive zeroweights, and the method is tuning parameter free unlike competitors. Moregenerally, under an M-open view when the truth falls outside the space of allconvex/linear combinations, our theory suggests that the posterior measuretends to concentrate on the best approximation of the truth at the minimaxrate. We illustrate the method through simulation studies and severalapplications.
arxiv-6300-168 | An Extensive Repot on the Efficiency of AIS-INMACA (A Novel Integrated MACA based Clonal Classifier for Protein Coding and Promoter Region Prediction) | http://arxiv.org/abs/1403.1336 | author:Pokkuluri Kiran Sree, Inampudi Ramesh Babu category:cs.CE cs.LG published:2014-03-06 summary:This paper exclusively reports the efficiency of AIS-INMACA. AIS-INMACA hascreated good impact on solving major problems in bioinformatics like proteinregion identification and promoter region prediction with less time (PokkuluriKiran Sree, 2014). This AIS-INMACA is now came with several variations(Pokkuluri Kiran Sree, 2014) towards projecting it as a tool in bioinformaticsfor solving many problems in bioinformatics. So this paper will be very muchuseful for so many researchers who are working in the domain of bioinformaticswith cellular automata.
arxiv-6300-169 | Integer Programming Relaxations for Integrated Clustering and Outlier Detection | http://arxiv.org/abs/1403.1329 | author:Lionel Ott, Linsey Pang, Fabio Ramos, David Howe, Sanjay Chawla category:cs.LG published:2014-03-06 summary:In this paper we present methods for exemplar based clustering with outlierselection based on the facility location formulation. Given a distance functionand the number of outliers to be found, the methods automatically determine thenumber of clusters and outliers. We formulate the problem as an integer programto which we present relaxations that allow for solutions that scale to largedata sets. The advantages of combining clustering and outlier selectioninclude: (i) the resulting clusters tend to be compact and semanticallycoherent (ii) the clusters are more robust against data perturbations and (iii)the outliers are contextualised by the clusters and more interpretable, i.e. itis easier to distinguish between outliers which are the result of data errorsfrom those that may be indicative of a new pattern emergent in the data. Wepresent and contrast three relaxations to the integer program formulation: (i)a linear programming formulation (LP) (ii) an extension of affinity propagationto outlier detection (APOC) and (iii) a Lagrangian duality based formulation(LD). Evaluation on synthetic as well as real data shows the quality andscalability of these different methods.
arxiv-6300-170 | Collaborative Filtering with Information-Rich and Information-Sparse Entities | http://arxiv.org/abs/1403.1600 | author:Kai Zhu, Rui Wu, Lei Ying, R. Srikant category:stat.ML cs.IT cs.LG math.IT published:2014-03-06 summary:In this paper, we consider a popular model for collaborative filtering inrecommender systems where some users of a website rate some items, such asmovies, and the goal is to recover the ratings of some or all of the unrateditems of each user. In particular, we consider both the clustering model, whereonly users (or items) are clustered, and the co-clustering model, where bothusers and items are clustered, and further, we assume that some users rate manyitems (information-rich users) and some users rate only a few items(information-sparse users). When users (or items) are clustered, our algorithmcan recover the rating matrix with $\omega(MK \log M)$ noisy entries while $MK$entries are necessary, where $K$ is the number of clusters and $M$ is thenumber of items. In the case of co-clustering, we prove that $K^2$ entries arenecessary for recovering the rating matrix, and our algorithm achieves thislower bound within a logarithmic factor when $K$ is sufficiently large. Wecompare our algorithms with a well-known algorithms called alternatingminimization (AM), and a similarity score-based algorithm known as thepopularity-among-friends (PAF) algorithm by applying all three to the MovieLensand Netflix data sets. Our co-clustering algorithm and AM have similar overallerror rates when recovering the rating matrix, both of which are lower than theerror rate under PAF. But more importantly, the error rate of our co-clusteringalgorithm is significantly lower than AM and PAF in the scenarios of interestin recommender systems: when recommending a few items to each user or whenrecommending items to users who only rated a few items (these users are themajority of the total user population). The performance difference increaseseven more when noise is added to the datasets.
arxiv-6300-171 | Multi-view Face Analysis Based on Gabor Features | http://arxiv.org/abs/1403.1327 | author:Hongli Liu, Weifeng Liu, Yanjiang Wang category:cs.CV published:2014-03-06 summary:Facial analysis has attracted much attention in the technology forhuman-machine interface. Different methods of classification based on sparserepresentation and Gabor kernels have been widely applied in the fields offacial analysis. However, most of these methods treat face from a whole viewstandpoint. In terms of the importance of different facial views, in thispaper, we present multi-view face analysis based on sparse representation andGabor wavelet coefficients. To evaluate the performance, we conduct faceanalysis experiments including face recognition (FR) and face expressionrecognition (FER) on JAFFE database. Experiments are conducted from two parts:(1) Face images are divided into three facial parts which are forehead, eye andmouth. (2) Face images are divided into 8 parts by the orientation of Gaborkernels. Experimental results demonstrate that the proposed methods cansignificantly boost the performance and perform better than the other methods.
arxiv-6300-172 | Authorship detection of SMS messages using unigrams | http://arxiv.org/abs/1403.1314 | author:R. G. Ragel, P. Herath, U. Senanayake category:cs.CL cs.IR published:2014-03-06 summary:SMS messaging is a popular media of communication. Because of its popularityand privacy, it could be used for many illegal purposes. Additionally, sincethey are part of the day to day life, SMSes can be used as evidence for manylegal disputes. Since a cellular phone might be accessible to people close tothe owner, it is important to establish the fact that the sender of the messageis indeed the owner of the phone. For this purpose, the straight forwardsolutions seem to be the use of popular stylometric methods. However, incomparison with the data used for stylometry in the literature, SMSes haveunusual characteristics making it hard or impossible to apply these methods ina conventional way. Our target is to come up with a method of authorshipdetection of SMS messages that could still give a usable accuracy. We arguethat, considering the methods of author attribution, the best method that couldbe applied to SMS messages is an n-gram method. To prove our point, we checkedtwo different methods of distribution comparison with varying number oftraining and testing data. We specifically try to compare how well ouralgorithms work under less amount of testing data and large number of candidateauthors (which we believe to be the real world scenario) against controlledtests with less number of authors and selected SMSes with large number ofwords. To counter the lack of information in an SMS message, we propose themethod of stacking together few SMSes.
arxiv-6300-173 | AntiPlag: Plagiarism Detection on Electronic Submissions of Text Based Assignments | http://arxiv.org/abs/1403.1310 | author:M. A. C. Jiffriya, M. A. C. Akmal Jahan, R. G. Ragel, S. Deegalla category:cs.IR cs.CL cs.DL published:2014-03-06 summary:Plagiarism is one of the growing issues in academia and is always a concernin Universities and other academic institutions. The situation is becoming evenworse with the availability of ample resources on the web. This paper focuseson creating an effective and fast tool for plagiarism detection for text basedelectronic assignments. Our plagiarism detection tool named AntiPlag isdeveloped using the tri-gram sequence matching technique. Three sets of textbased assignments were tested by AntiPlag and the results were compared againstan existing commercial plagiarism detection tool. AntiPlag showed betterresults in terms of false positives compared to the commercial tool due to thepre-processing steps performed in AntiPlag. In addition, to improve thedetection latency, AntiPlag applies a data clustering technique making it fourtimes faster than the commercial tool considered. AntiPlag could be used toisolate plagiarized text based assignments from non-plagiarised assignmentseasily. Therefore, we present AntiPlag, a fast and effective tool forplagiarism detection on text based electronic assignments.
arxiv-6300-174 | Design a Persian Automated Plagiarism Detector (AMZPPD) | http://arxiv.org/abs/1403.1618 | author:Maryam Mahmoodi, Mohammad Mahmoodi Varnamkhasti category:cs.AI cs.CL published:2014-03-06 summary:Currently there are lots of plagiarism detection approaches. But few of themimplemented and adapted for Persian languages. In this paper, our work ondesigning and implementation of a plagiarism detection system based onpre-processing and NLP technics will be described. And the results of testingon a corpus will be presented.
arxiv-6300-175 | Inducing Language Networks from Continuous Space Word Representations | http://arxiv.org/abs/1403.1252 | author:Bryan Perozzi, Rami Al-Rfou, Vivek Kulkarni, Steven Skiena category:cs.LG cs.CL cs.SI published:2014-03-06 summary:Recent advancements in unsupervised feature learning have developed powerfullatent representations of words. However, it is still not clear what makes onerepresentation better than another and how we can learn the idealrepresentation. Understanding the structure of latent spaces attained is key toany future advancement in unsupervised learning. In this work, we introduce anew view of continuous space word representations as language networks. Weexplore two techniques to create language networks from learned features byinducing them for two popular word representation methods and examining theproperties of their resulting networks. We find that the induced networksdiffer from other methods of creating language networks, and that they containmeaningful community structure.
arxiv-6300-176 | Ubic: Bridging the gap between digital cryptography and the physical world | http://arxiv.org/abs/1403.1343 | author:Mark Simkin, Dominique Schroeder, Andreas Bulling, Mario Fritz category:cs.CR cs.CV published:2014-03-06 summary:Advances in computing technology increasingly blur the boundary between thedigital domain and the physical world. Although the research community hasdeveloped a large number of cryptographic primitives and has demonstrated theirusability in all-digital communication, many of them have not yet made theirway into the real world due to usability aspects. We aim to make another steptowards a tighter integration of digital cryptography into real worldinteractions. We describe Ubic, a framework that allows users to bridge the gapbetween digital cryptography and the physical world. Ubic relies onhead-mounted displays, like Google Glass, resource-friendly computer visiontechniques as well as mathematically sound cryptographic primitives to provideusers with better security and privacy guarantees. The framework covers keycryptographic primitives, such as secure identification, document verificationusing a novel secure physical document format, as well as content hiding. Tomake a contribution of practical value, we focused on making Ubic as simple,easily deployable, and user friendly as possible.
arxiv-6300-177 | Learning Soft Linear Constraints with Application to Citation Field Extraction | http://arxiv.org/abs/1403.1349 | author:Sam Anzaroot, Alexandre Passos, David Belanger, Andrew McCallum category:cs.CL cs.DL cs.IR published:2014-03-06 summary:Accurately segmenting a citation string into fields for authors, titles, etc.is a challenging task because the output typically obeys various globalconstraints. Previous work has shown that modeling soft constraints, where themodel is encouraged, but not require to obey the constraints, can substantiallyimprove segmentation performance. On the other hand, for imposing hardconstraints, dual decomposition is a popular technique for efficient predictiongiven existing algorithms for unconstrained inference. We extend the techniqueto perform prediction subject to soft constraints. Moreover, with a techniquefor performing inference given soft constraints, it is easy to automaticallygenerate large families of constraints and learn their costs with a simpleconvex optimization problem during training. This allows us to obtainsubstantial gains in accuracy on a new, challenging citation extractiondataset.
arxiv-6300-178 | Sparse Principal Component Analysis via Rotation and Truncation | http://arxiv.org/abs/1403.1430 | author:Zhenfang Hu, Gang Pan, Yueming Wang, Zhaohui Wu category:cs.LG cs.CV stat.ML published:2014-03-06 summary:Sparse principal component analysis (sparse PCA) aims at finding a sparsebasis to improve the interpretability over the dense basis of PCA, meanwhilethe sparse basis should cover the data subspace as much as possible. Incontrast to most of existing work which deal with the problem by adding somesparsity penalties on various objectives of PCA, in this paper, we propose anew method SPCArt, whose motivation is to find a rotation matrix and a sparsebasis such that the sparse basis approximates the basis of PCA after therotation. The algorithm of SPCArt consists of three alternating steps: rotatePCA basis, truncate small entries, and update the rotation matrix. Itsperformance bounds are also given. SPCArt is efficient, with each iterationscaling linearly with the data dimension. It is easy to choose parameters inSPCArt, due to its explicit physical explanations. Besides, we give a unifiedview to several existing sparse PCA methods and discuss the connection withSPCArt. Some ideas in SPCArt are extended to GPower, a popular sparse PCAalgorithm, to overcome its drawback. Experimental results demonstrate thatSPCArt achieves the state-of-the-art performance. It also achieves a goodtradeoff among various criteria, including sparsity, explained variance,orthogonality, balance of sparsity among loadings, and computational speed.
arxiv-6300-179 | Rate Prediction and Selection in LTE systems using Modified Source Encoding Techniques | http://arxiv.org/abs/1403.1412 | author:K. P. Saishankar, Sheetal Kalyani, K. Narendran category:stat.AP cs.IT cs.LG math.IT published:2014-03-06 summary:In current wireless systems, the base-Station (eNodeB) tries to serve itsuser-equipment (UE) at the highest possible rate that the UE can reliablydecode. The eNodeB obtains this rate information as a quantized feedback fromthe UE at time n and uses this, for rate selection till the next feedback isreceived at time n + {\delta}. The feedback received at n can become outdatedbefore n + {\delta}, because of a) Doppler fading, and b) Change in the set ofactive interferers for a UE. Therefore rate prediction becomes essential.Since, the rates belong to a discrete set, we propose a discrete sequenceprediction approach, wherein, frequency trees for the discrete sequences arebuilt using source encoding algorithms like Prediction by Partial Match (PPM).Finding the optimal depth of the frequency tree used for prediction is cast asa model order selection problem. The rate sequence complexity is analysed toprovide an upper bound on model order. Information-theoretic criteria are thenused to solve the model order problem. Finally, two prediction algorithms areproposed, using the PPM with optimal model order and system level simulationsdemonstrate the improvement in packet loss and throughput due to thesealgorithms.
arxiv-6300-180 | Latent Semantic Word Sense Disambiguation Using Global Co-occurrence Information | http://arxiv.org/abs/1403.1194 | author:Minoru Sasaki category:cs.CL cs.IR published:2014-03-05 summary:In this paper, I propose a novel word sense disambiguation method based onthe global co-occurrence information using NMF. When I calculate the dependencyrelation matrix, the existing method tends to produce very sparse co-occurrencematrix from a small training set. Therefore, the NMF algorithm sometimes doesnot converge to desired solutions. To obtain a large number of co-occurrencerelations, I propose to use co-occurrence frequencies of dependency relationsbetween word features in the whole training set. This enables us to solve datasparseness problem and induce more effective latent features. To evaluate theefficiency of the method of word sense disambiguation, I make some experimentsto compare with the result of the two baseline methods. The results of theexperiments show this method is effective for word sense disambiguation incomparison with the all baseline methods. Moreover, the proposed method iseffective for obtaining a stable effect by analyzing the global co-occurrenceinformation.
arxiv-6300-181 | Artificial Neuron Modelling Based on Wave Shape | http://arxiv.org/abs/1403.1073 | author:Kieran Greer category:cs.NE published:2014-03-05 summary:This paper describes a new model for an artificial neural network processingunit or neuron. It is slightly different to a traditional feedforward networkby the fact that it favours a mechanism of trying to match the wave-like'shape' of the input with the shape of the output against specific value errorcorrections. The expectation is then that a best fit shape can be transposedinto the desired output values more easily. This allows for notions ofreinforcement through resonance and also the construction of synapses.
arxiv-6300-182 | K-Tangent Spaces on Riemannian Manifolds for Improved Pedestrian Detection | http://arxiv.org/abs/1403.1056 | author:Andres Sanin, Conrad Sanderson, Mehrtash T. Harandi, Brian C. Lovell category:cs.CV published:2014-03-05 summary:For covariance-based image descriptors, taking into account the curvature ofthe corresponding feature space has been shown to improve discriminationperformance. This is often done through representing the descriptors as pointson Riemannian manifolds, with the discrimination accomplished on a tangentspace. However, such treatment is restrictive as distances between arbitrarypoints on the tangent space do not represent true geodesic distances, and hencedo not represent the manifold structure accurately. In this paper we propose ageneral discriminative model based on the combination of several tangentspaces, in order to preserve more details of the structure. The model can beused as a weak learner in a boosting-based pedestrian detection framework.Experiments on the challenging INRIA and DaimlerChrysler datasets show that theproposed model leads to considerably higher performance than methods based onhistograms of oriented gradients as well as previous Riemannian-basedtechniques.
arxiv-6300-183 | On learning to localize objects with minimal supervision | http://arxiv.org/abs/1403.1024 | author:Hyun Oh Song, Ross Girshick, Stefanie Jegelka, Julien Mairal, Zaid Harchaoui, Trevor Darrell category:cs.CV cs.LG published:2014-03-05 summary:Learning to localize objects with minimal supervision is an important problemin computer vision, since large fully annotated datasets are extremely costlyto obtain. In this paper, we propose a new method that achieves this goal withonly image-level labels of whether the objects are present or not. Our approachcombines a discriminative submodular cover problem for automaticallydiscovering a set of positive object windows with a smoothed latent SVMformulation. The latter allows us to leverage efficient quasi-Newtonoptimization techniques. Our experiments demonstrate that the proposed approachprovides a 50% relative improvement in mean average precision over the currentstate-of-the-art on PASCAL VOC 2007 detection.
arxiv-6300-184 | Detecting change points in the large-scale structure of evolving networks | http://arxiv.org/abs/1403.0989 | author:Leto Peel, Aaron Clauset category:cs.SI physics.soc-ph stat.ML published:2014-03-05 summary:Interactions among people or objects are often dynamic in nature and can berepresented as a sequence of networks, each providing a snapshot of theinteractions over a brief period of time. An important task in analyzing suchevolving networks is change-point detection, in which we both identify thetimes at which the large-scale pattern of interactions changes fundamentallyand quantify how large and what kind of change occurred. Here, we formalize forthe first time the network change-point detection problem within an onlineprobabilistic learning framework and introduce a method that can reliably solveit. This method combines a generalized hierarchical random graph model with aBayesian hypothesis test to quantitatively determine if, when, and preciselyhow a change point has occurred. We analyze the detectability of our methodusing synthetic data with known change points of different types andmagnitudes, and show that this method is more accurate than several previouslyused alternatives. Applied to two high-resolution evolving social networks,this method identifies a sequence of change points that align with knownexternal "shocks" to these networks.
arxiv-6300-185 | Estimating complex causal effects from incomplete observational data | http://arxiv.org/abs/1403.1124 | author:Juha Karvanen category:stat.ME cs.LG math.ST stat.ML stat.TH published:2014-03-05 summary:Despite the major advances taken in causal modeling, causality is still anunfamiliar topic for many statisticians. In this paper, it is demonstrated fromthe beginning to the end how causal effects can be estimated from observationaldata assuming that the causal structure is known. To make the problem morechallenging, the causal effects are highly nonlinear and the data are missingat random. The tools used in the estimation include causal models with design,causal calculus, multiple imputation and generalized additive models. The mainmessage is that a trained statistician can estimate causal effects byjudiciously combining existing tools.
arxiv-6300-186 | A Novel Method for Vectorization | http://arxiv.org/abs/1403.0728 | author:Tolga Birdal, Emrah Bala category:cs.CV cs.CG cs.GR published:2014-03-04 summary:Vectorization of images is a key concern uniting computer graphics andcomputer vision communities. In this paper we are presenting a novel idea forefficient, customizable vectorization of raster images, based on Catmull Romspline fitting. The algorithm maintains a good balance between photo-realismand photo abstraction, and hence is applicable to applications with artisticconcerns or applications where less information loss is crucial. The resultingalgorithm is fast, parallelizable and can satisfy general soft realtimerequirements. Moreover, the smoothness of the vectorized images aestheticallyoutperforms outputs of many polygon-based methods
arxiv-6300-187 | An Efficient Method for Face Recognition System In Various Assorted Conditions | http://arxiv.org/abs/1403.5475 | author:V. Karthikeyan, K. Vijayalakshmi, P. Jeyakumar category:cs.CV published:2014-03-04 summary:In the beginning stage, face verification is done using easy method ofgeometric algorithm models, but the verification route has now developed into ascientific progress of complicated geometric representation and identicalprocedure. In recent years the technologies have boosted face recognitionsystem into the healthy focus. Researchers currently undergoing strong researchon finding face recognition system for wider area information taken underhysterical elucidation dissimilarity. The proposed face recognition systemconsists of a narrative expositionindiscreet preprocessing method, a hybridFourier-based facial feature extraction and a score fusion scheme. We haveverified the face recognition in different lightening conditions (day or night)and at different locations (indoor or outdoor). Preprocessing, Image detection,Feature- extraction and Face recognition are the methods used for faceverification system. This paper focuses mainly on the issue of toughness tolighting variations. The proposed system has obtained an average of 88.1%verification rate on Two-Dimensional images under different lighteningconditions.
arxiv-6300-188 | Dynamic stochastic blockmodels for time-evolving social networks | http://arxiv.org/abs/1403.0921 | author:Kevin S. Xu, Alfred O. Hero III category:cs.SI cs.LG physics.soc-ph stat.ME G.3; G.2.2 published:2014-03-04 summary:Significant efforts have gone into the development of statistical models foranalyzing data in the form of networks, such as social networks. Most existingwork has focused on modeling static networks, which represent either a singletime snapshot or an aggregate view over time. There has been recent interest instatistical modeling of dynamic networks, which are observed at multiple pointsin time and offer a richer representation of many complex phenomena. In thispaper, we present a state-space model for dynamic networks that extends thewell-known stochastic blockmodel for static networks to the dynamic setting. Wefit the model in a near-optimal manner using an extended Kalman filter (EKF)augmented with a local search. We demonstrate that the EKF-based algorithmperforms competitively with a state-of-the-art algorithm based on Markov chainMonte Carlo sampling but is significantly less computationally demanding.
arxiv-6300-189 | Is getting the right answer just about choosing the right words? The role of syntactically-informed features in short answer scoring | http://arxiv.org/abs/1403.0801 | author:Derrick Higgins, Chris Brew, Michael Heilman, Ramon Ziai, Lei Chen, Aoife Cahill, Michael Flor, Nitin Madnani, Joel Tetreault, Daniel Blanchard, Diane Napolitano, Chong Min Lee, John Blackmore category:cs.CL published:2014-03-04 summary:Developments in the educational landscape have spurred greater interest inthe problem of automatically scoring short answer questions. A recent sharedtask on this topic revealed a fundamental divide in the modeling approachesthat have been applied to this problem, with the best-performing systems splitbetween those that employ a knowledge engineering approach and those thatalmost solely leverage lexical information (as opposed to higher-levelsyntactic information) in assigning a score to a given response. This paperaims to introduce the NLP community to the largest corpus currently availablefor short-answer scoring, provide an overview of methods used in the sharedtask using this data, and explore the extent to which moresyntactically-informed features can contribute to the short answer scoring taskin a way that avoids the question-specific manual effort of the knowledgeengineering approach.
arxiv-6300-190 | Fast Prediction with SVM Models Containing RBF Kernels | http://arxiv.org/abs/1403.0736 | author:Marc Claesen, Frank De Smet, Johan A. K. Suykens, Bart De Moor category:stat.ML cs.LG published:2014-03-04 summary:We present an approximation scheme for support vector machine models that usean RBF kernel. A second-order Maclaurin series approximation is used forexponentials of inner products between support vectors and test instances. Theapproximation is applicable to all kernel methods featuring sums of kernelevaluations and makes no assumptions regarding data normalization. Theprediction speed of approximated models no longer relates to the amount ofsupport vectors but is quadratic in terms of the number of input dimensions. Ifthe number of input dimensions is small compared to the amount of supportvectors, the approximated model is significantly faster in prediction and has asmaller memory footprint. An optimized C++ implementation was made to assessthe gain in prediction speed in a set of practical tests. We additionallyprovide a method to verify the approximation accuracy, prior to training modelsor during run-time, to ensure the loss in accuracy remains acceptable andwithin known bounds.
arxiv-6300-191 | Multi-Shot Person Re-Identification via Relational Stein Divergence | http://arxiv.org/abs/1403.0699 | author:Azadeh Alavi, Yan Yang, Mehrtash Harandi, Conrad Sanderson category:cs.CV stat.ML published:2014-03-04 summary:Person re-identification is particularly challenging due to significantappearance changes across separate camera views. In order to re-identifypeople, a representative human signature should effectively handle differencesin illumination, pose and camera parameters. While general appearance-basedmethods are modelled in Euclidean spaces, it has been argued that someapplications in image and video analysis are better modelled via non-Euclideanmanifold geometry. To this end, recent approaches represent images ascovariance matrices, and interpret such matrices as points on Riemannianmanifolds. As direct classification on such manifolds can be difficult, in thispaper we propose to represent each manifold point as a vector of similaritiesto class representers, via a recently introduced form of Bregman matrixdivergence known as the Stein divergence. This is followed by using adiscriminative mapping of similarity vectors for final classification. The useof similarity vectors is in contrast to the traditional approach of embeddingmanifolds into tangent spaces, which can suffer from representing the manifoldstructure inaccurately. Comparative evaluations on benchmark ETHZ and iLIDSdatasets for the person re-identification task show that the proposed approachobtains better performance than recent techniques such as Histogram PlusEpitome, Partial Least Squares, and Symmetry-Driven Accumulation of LocalFeatures.
arxiv-6300-192 | Multi-period Trading Prediction Markets with Connections to Machine Learning | http://arxiv.org/abs/1403.0648 | author:Jinli Hu, Amos Storkey category:cs.GT cs.LG q-fin.TR stat.ML published:2014-03-04 summary:We present a new model for prediction markets, in which we use risk measuresto model agents and introduce a market maker to describe the trading process.This specific choice on modelling tools brings us mathematical convenience. Theanalysis shows that the whole market effectively approaches a global objective,despite that the market is designed such that each agent only cares about itsown goal. Additionally, the market dynamics provides a sensible algorithm foroptimising the global objective. An intimate connection between machinelearning and our markets is thus established, such that we could 1) analyse amarket by applying machine learning methods to the global objective, and 2)solve machine learning problems by setting up and running certain markets.
arxiv-6300-193 | Random Projections on Manifolds of Symmetric Positive Definite Matrices for Image Classification | http://arxiv.org/abs/1403.0700 | author:Azadeh Alavi, Arnold Wiliem, Kun Zhao, Brian C. Lovell, Conrad Sanderson category:cs.CV stat.ML published:2014-03-04 summary:Recent advances suggest that encoding images through Symmetric PositiveDefinite (SPD) matrices and then interpreting such matrices as points onRiemannian manifolds can lead to increased classification performance. Takinginto account manifold geometry is typically done via (1) embedding themanifolds in tangent spaces, or (2) embedding into Reproducing Kernel HilbertSpaces (RKHS). While embedding into tangent spaces allows the use of existingEuclidean-based learning algorithms, manifold shape is only approximated whichcan cause loss of discriminatory information. The RKHS approach retains more ofthe manifold structure, but may require non-trivial effort to kerneliseEuclidean-based learning algorithms. In contrast to the above approaches, inthis paper we offer a novel solution that allows SPD matrices to be used withunmodified Euclidean-based learning algorithms, with the true manifold shapewell-preserved. Specifically, we propose to project SPD matrices using a set ofrandom projection hyperplanes over RKHS into a random projection space, whichleads to representing each matrix as a vector of projection coefficients.Experiments on face recognition, person re-identification and textureclassification show that the proposed approach outperforms several recentmethods, such as Tensor Sparse Coding, Histogram Plus Epitome, RiemannianLocality Preserving Projection and Relational Divergence Classification.
arxiv-6300-194 | The Hidden Convexity of Spectral Clustering | http://arxiv.org/abs/1403.0667 | author:James Voss, Mikhail Belkin, Luis Rademacher category:cs.LG stat.ML published:2014-03-04 summary:In recent years, spectral clustering has become a standard method for dataanalysis used in a broad range of applications. In this paper we propose a newclass of algorithms for multiway spectral clustering based on optimization of acertain "contrast function" over the unit sphere. These algorithms, partlyinspired by certain Independent Component Analysis techniques, are simple, easyto implement and efficient. Geometrically, the proposed algorithms can be interpreted as hidden basisrecovery by means of function optimization. We give a complete characterizationof the contrast functions admissible for provable basis recovery. We show howthese conditions can be interpreted as a "hidden convexity" of our optimizationproblem on the sphere; interestingly, we use efficient convex maximizationrather than the more common convex minimization. We also show encouragingexperimental results on real and simulated data.
arxiv-6300-195 | EnsembleSVM: A Library for Ensemble Learning Using Support Vector Machines | http://arxiv.org/abs/1403.0745 | author:Marc Claesen, Frank De Smet, Johan Suykens, Bart De Moor category:stat.ML cs.LG published:2014-03-04 summary:EnsembleSVM is a free software package containing efficient routines toperform ensemble learning with support vector machine (SVM) base models. Itcurrently offers ensemble methods based on binary SVM models. Ourimplementation avoids duplicate storage and evaluation of support vectors whichare shared between constituent models. Experimental results show that usingensemble approaches can drastically reduce training complexity whilemaintaining high predictive accuracy. The EnsembleSVM software package isfreely available online at http://esat.kuleuven.be/stadius/ensemblesvm.
arxiv-6300-196 | Dynamic Move Chains -- a Forward Pruning Approach to Tree Search in Computer Chess | http://arxiv.org/abs/1403.0778 | author:Kieran Greer category:cs.AI cs.NE published:2014-03-04 summary:This paper proposes a new mechanism for pruning a search game-tree incomputer chess. The algorithm stores and then reuses chains or sequences ofmoves, built up from previous searches. These move sequences have a built-inforward-pruning mechanism that can radically reduce the search space. A typicalsearch process might retrieve a move from a Transposition Table, where thedecision of what move to retrieve would be based on the position itself. Thisalgorithm stores move sequences based on what previous sequences were better,or caused cutoffs. This is therefore position independent and so it could alsobe useful in games with imperfect information or uncertainty, where the wholesituation is not known at any one time. Over a small set of tests, thealgorithm was shown to clearly out-perform Transposition Tables, both in termsof search reduction and game-play results.
arxiv-6300-197 | Matroid Regression | http://arxiv.org/abs/1403.0873 | author:Franz J Király, Louis Theran category:math.ST cs.DM cs.LG stat.ME stat.ML stat.TH published:2014-03-04 summary:We propose an algebraic combinatorial method for solving large sparse linearsystems of equations locally - that is, a method which can compute singleevaluations of the signal without computing the whole signal. The method scalesonly in the sparsity of the system and not in its size, and allows to provideerror estimates for any solution method. At the heart of our approach is theso-called regression matroid, a combinatorial object associated to sparsitypatterns, which allows to replace inversion of the large matrix with theinversion of a kernel matrix that is constant size. We show that our methodprovides the best linear unbiased estimator (BLUE) for this setting and theminimum variance unbiased estimator (MVUE) under Gaussian noise assumptions,and furthermore we show that the size of the kernel matrix which is to beinverted can be traded off with accuracy.
arxiv-6300-198 | Geometry-based Adaptive Symbolic Approximation for Fast Sequence Matching on Manifolds | http://arxiv.org/abs/1403.0820 | author:Rushil Anirudh, Pavan Turaga category:cs.CV math.DG published:2014-03-04 summary:In this paper, we consider the problem of fast and efficient indexingtechniques for sequences evolving in non-Euclidean spaces. This problem hasseveral applications in the areas of human activity analysis, where there is aneed to perform fast search, and recognition in very high dimensional spaces.The problem is made more challenging when representations such as landmarks,contours, and human skeletons etc. are naturally studied in a non-Euclideansetting where even simple operations are much more computationally intensivethan their Euclidean counterparts. We propose a geometry and data adaptivesymbolic framework that is shown to enable the deployment of fast and accuratealgorithms for activity recognition, dynamic texture recognition, motifdiscovery. Toward this end, we present generalizations of key concepts ofpiece-wise aggregation and symbolic approximation for the case of non-Euclideanmanifolds. We show that one can replace expensive geodesic computations withmuch faster symbolic computations with little loss of accuracy in activityrecognition and discovery applications. The framework is general enough to workacross both Euclidean and non-Euclidean spaces, depending on appropriatefeature representations without compromising on the ultra-low bandwidth, highspeed and high accuracy. The proposed methods are ideally suited for real-timesystems and low complexity scenarios.
arxiv-6300-199 | Support Vector Machine Model for Currency Crisis Discrimination | http://arxiv.org/abs/1403.0481 | author:Arindam Chaudhuri category:cs.LG stat.ML published:2014-03-03 summary:Support Vector Machine (SVM) is powerful classification technique based onthe idea of structural risk minimization. Use of kernel function enables curseof dimensionality to be addressed. However, proper kernel function for certainproblem is dependent on specific dataset and as such there is no good method onchoice of kernel function. In this paper, SVM is used to build empirical modelsof currency crisis in Argentina. An estimation technique is developed bytraining model on real life data set which provides reasonably accurate modeloutputs and helps policy makers to identify situations in which currency crisismay happen. The third and fourth order polynomial kernel is generally bestchoice to achieve high generalization of classifier performance. SVM has highlevel of maturity with algorithms that are simple, easy to implement, toleratescurse of dimensionality and good empirical performance. The satisfactoryresults show that currency crisis situation is properly emulated using onlysmall fraction of database and could be used as an evaluation tool as well asan early warning system. To the best of knowledge this is the first work on SVMapproach for currency crisis evaluation of Argentina.
arxiv-6300-200 | Face Recognition Methods & Applications | http://arxiv.org/abs/1403.0485 | author:Divyarajsinh N. Parmar, Brijesh B. Mehta category:cs.CV published:2014-03-03 summary:Face recognition presents a challenging problem in the field of imageanalysis and computer vision. The security of information is becoming verysignificant and difficult. Security cameras are presently common in airports,Offices, University, ATM, Bank and in any locations with a security system.Face recognition is a biometric system used to identify or verify a person froma digital image. Face Recognition system is used in security. Face recognitionsystem should be able to automatically detect a face in an image. This involvesextracts its features and then recognize it, regardless of lighting,expression, illumination, ageing, transformations (translate, rotate and scaleimage) and pose, which is a difficult task. This paper contains three sections.The first section describes the common methods like holistic matching method,feature extraction method and hybrid methods. The second section describesapplications with examples and finally third section describes the futureresearch directions of face recognition.
arxiv-6300-201 | A Primal Dual Active Set with Continuation Algorithm for the \ell^0-Regularized Optimization Problem | http://arxiv.org/abs/1403.0515 | author:Yuling Jiao, Bangti Jin, Xiliang Lu category:math.OC cs.IT math.IT stat.ML published:2014-03-03 summary:We develop a primal dual active set with continuation algorithm for solvingthe \ell^0-regularized least-squares problem that frequently arises incompressed sensing. The algorithm couples the the primal dual active set methodwith a continuation strategy on the regularization parameter. At each inneriteration, it first identifies the active set from both primal and dualvariables, and then updates the primal variable by solving a (typically small)least-squares problem defined on the active set, from which the dual variablecan be updated explicitly. Under certain conditions on the sensing matrix,i.e., mutual incoherence property or restricted isometry property, and thenoise level, the finite step global convergence of the algorithm isestablished. Extensive numerical examples are presented to illustrate theefficiency and accuracy of the algorithm and the convergence analysis.
arxiv-6300-202 | We Tweet Like We Talk and Other Interesting Observations: An Analysis of English Communication Modalities | http://arxiv.org/abs/1403.0531 | author:Josiah P. Zayner category:cs.CL published:2014-03-03 summary:Modalities of communication for human beings are gradually increasing innumber with the advent of new forms of technology. Many human beings canreadily transition between these different forms of communication with littleor no effort, which brings about the question: How similar are these differentcommunication modalities? To understand technology$\text{'}$s influence onEnglish communication, four different corpora were analyzed and compared:Writing from Books using the 1-grams database from the Google Books project,Twitter, IRC Chat, and transcribed Talking. Multi-word confusion matricesrevealed that Talking has the most similarity when compared to the other modesof communication, while 1-grams were the least similar form of communicationanalyzed. Based on the analysis of word usage, word usage frequencydistributions, and word class usage, among other things, Talking is also themost similar to Twitter and IRC Chat. This suggests that communicating usingTwitter and IRC Chat evolved from Talking rather than Writing. When wecommunicate online, even though we are writing, we do not Tweet or Chat how wewrite books; we Tweet and Chat how we Speak. Nonfiction and Fiction writingwere clearly differentiable from our analysis with Twitter and Chat being muchmore similar to Fiction than Nonfiction writing. These hypotheses were thentested using author and journalists Cory Doctorow. Mr. Doctorow$\text{'}$sWriting, Twitter usage, and Talking were all found to have very similarvocabulary usage patterns as the amalgamized populations, as long as thewriting was Fiction. However, Mr. Doctorow$\text{'}$s Nonfiction writing isdifferent from 1-grams and other collected Nonfiction writings. This data couldperhaps be used to create more entertaining works of Nonfiction.
arxiv-6300-203 | Representing, reasoning and answering questions about biological pathways - various applications | http://arxiv.org/abs/1403.0541 | author:Saadat Anwar category:cs.AI cs.CE cs.CL published:2014-03-03 summary:Biological organisms are composed of numerous interconnected biochemicalprocesses. Diseases occur when normal functionality of these processes isdisrupted. Thus, understanding these biochemical processes and theirinterrelationships is a primary task in biomedical research and a prerequisitefor diagnosing diseases, and drug development. Scientists studying theseprocesses have identified various pathways responsible for drug metabolism, andsignal transduction, etc. Newer techniques and speed improvements have resulted in deeper knowledgeabout these pathways, resulting in refined models that tend to be large andcomplex, making it difficult for a person to remember all aspects of it. Thus,computer models are needed to analyze them. We want to build such a system thatallows modeling of biological systems and pathways in such a way that we cananswer questions about them. Many existing models focus on structural and/or factoid questions, usingsurface-level knowledge that does not require understanding the underlyingmodel. We believe these are not the kind of questions that a biologist may asksomeone to test their understanding of the biological processes. We want oursystem to answer the kind of questions a biologist may ask. Such questionsappear in early college level text books. Thus the main goal of our thesis is to develop a system that allows us toencode knowledge about biological pathways and answer such questions about themdemonstrating understanding of the pathway. To that end, we develop a languagethat will allow posing such questions and illustrate the utility of ourframework with various applications in the biological domain. We use someexisting tools with modifications to accomplish our goal. Finally, we apply our system to real world applications by extracting pathwayknowledge from text and answering questions related to drug development.
arxiv-6300-204 | Matching Image Sets via Adaptive Multi Convex Hull | http://arxiv.org/abs/1403.0320 | author:Shaokang Chen, Arnold Wiliem, Conrad Sanderson, Brian C. Lovell category:cs.CV stat.ML published:2014-03-03 summary:Traditional nearest points methods use all the samples in an image set toconstruct a single convex or affine hull model for classification. However,strong artificial features and noisy data may be generated from combinations oftraining samples when significant intra-class variations and/or noise occur inthe image set. Existing multi-model approaches extract local models byclustering each image set individually only once, with fixed clusters used formatching with various image sets. This may not be optimal for discrimination,as undesirable environmental conditions (eg. illumination and pose variations)may result in the two closest clusters representing different characteristicsof an object (eg. frontal face being compared to non-frontal face). To addressthe above problem, we propose a novel approach to enhance nearest points basedmethods by integrating affine/convex hull classification with an adaptedmulti-model approach. We first extract multiple local convex hulls from a queryimage set via maximum margin clustering to diminish the artificial variationsand constrain the noise in local convex hulls. We then propose adaptivereference clustering (ARC) to constrain the clustering of each gallery imageset by forcing the clusters to have resemblance to the clusters in the queryimage set. By applying ARC, noisy clusters in the query set can be discarded.Experiments on Honda, MoBo and ETH-80 datasets show that the proposed methodoutperforms single model approaches and other recent techniques, such as SparseApproximated Nearest Points, Mutual Subspace Method and Manifold DiscriminantAnalysis.
arxiv-6300-205 | The Structurally Smoothed Graphlet Kernel | http://arxiv.org/abs/1403.0598 | author:Pinar Yanardag, S. V. N. Vishwanathan category:cs.LG published:2014-03-03 summary:A commonly used paradigm for representing graphs is to use a vector thatcontains normalized frequencies of occurrence of certain motifs or sub-graphs.This vector representation can be used in a variety of applications, such as,for computing similarity between graphs. The graphlet kernel of Shervashidze etal. [32] uses induced sub-graphs of k nodes (christened as graphlets by Przulj[28]) as motifs in the vector representation, and computes the kernel via a dotproduct between these vectors. One can easily show that this is a valid kernelbetween graphs. However, such a vector representation suffers from a fewdrawbacks. As k becomes larger we encounter the sparsity problem; most higherorder graphlets will not occur in a given graph. This leads to diagonaldominance, that is, a given graph is similar to itself but not to any othergraph in the dataset. On the other hand, since lower order graphlets tend to bemore numerous, using lower values of k does not provide enough discriminationability. We propose a smoothing technique to tackle the above problems. Ourmethod is based on a novel extension of Kneser-Ney and Pitman-Yor smoothingtechniques from natural language processing to graphs. We use the relationshipsbetween lower order and higher order graphlets in order to derive our method.Consequently, our smoothing algorithm not only respects the dependency betweensub-graphs but also tackles the diagonal dominance problem by distributing theprobability mass across graphlets. In our experiments, the smoothed graphletkernel outperforms graph kernels based on raw frequency counts.
arxiv-6300-206 | Cross-Scale Cost Aggregation for Stereo Matching | http://arxiv.org/abs/1403.0316 | author:Kang Zhang, Yuqiang Fang, Dongbo Min, Lifeng Sun, Shiqiang Yang. Shuicheng Yan, Qi Tian category:cs.CV published:2014-03-03 summary:Human beings process stereoscopic correspondence across multiple scales.However, this bio-inspiration is ignored by state-of-the-art cost aggregationmethods for dense stereo correspondence. In this paper, a generic cross-scalecost aggregation framework is proposed to allow multi-scale interaction in costaggregation. We firstly reformulate cost aggregation from a unifiedoptimization perspective and show that different cost aggregation methodsessentially differ in the choices of similarity kernels. Then, an inter-scaleregularizer is introduced into optimization and solving this new optimizationproblem leads to the proposed framework. Since the regularization term isindependent of the similarity kernel, various cost aggregation methods can beintegrated into the proposed general framework. We show that the cross-scaleframework is important as it effectively and efficiently expandsstate-of-the-art cost aggregation methods and leads to significantimprovements, when evaluated on Middlebury, KITTI and New Tsukuba datasets.
arxiv-6300-207 | Global solar irradiation prediction using a multi-gene genetic programming approach | http://arxiv.org/abs/1403.0623 | author:Indranil Pan, Daya Shankar Pandey, Saptarshi Das category:cs.NE cs.CE stat.AP published:2014-03-03 summary:In this paper, a nonlinear symbolic regression technique using anevolutionary algorithm known as multi-gene genetic programming (MGGP) isapplied for a data-driven modelling between the dependent and the independentvariables. The technique is applied for modelling the measured global solarirradiation and validated through numerical simulations. The proposed modellingtechnique shows improved results over the fuzzy logic and artificial neuralnetwork (ANN) based approaches as attempted by contemporary researchers. Themethod proposed here results in nonlinear analytical expressions, unlike thosewith neural networks which is essentially a black box modelling approach. Thisadditional flexibility is an advantage from the modelling perspective and helpsto discern the important variables which affect the prediction. Due to theevolutionary nature of the algorithm, it is able to get out of local minima andconverge to a global optimum unlike the back-propagation (BP) algorithm usedfor training neural networks. This results in a better percentage fit than theones obtained using neural networks by contemporary researchers. Also ahold-out cross validation is done on the obtained genetic programming (GP)results which show that the results generalize well to new data and do notover-fit the training samples. The multi-gene GP results are compared withthose, obtained using its single-gene version and also the same with fourclassical regression models in order to show the effectiveness of the adoptedapproach.
arxiv-6300-208 | Multiview Hessian regularized logistic regression for action recognition | http://arxiv.org/abs/1403.0829 | author:W. Liu, H. Liu, D. Tao, Y. Wang, Ke Lu category:cs.CV cs.LG stat.ML published:2014-03-03 summary:With the rapid development of social media sharing, people often need tomanage the growing volume of multimedia data such as large scale videoclassification and annotation, especially to organize those videos containinghuman activities. Recently, manifold regularized semi-supervised learning(SSL), which explores the intrinsic data probability distribution and thenimproves the generalization ability with only a small number of labeled data,has emerged as a promising paradigm for semiautomatic video classification. Inaddition, human action videos often have multi-modal content and differentrepresentations. To tackle the above problems, in this paper we proposemultiview Hessian regularized logistic regression (mHLR) for human actionrecognition. Compared with existing work, the advantages of mHLR lie in threefolds: (1) mHLR combines multiple Hessian regularization, each of whichobtained from a particular representation of instance, to leverage theexploring of local geometry; (2) mHLR naturally handle multi-view instanceswith multiple representations; (3) mHLR employs a smooth loss function and thencan be effectively optimized. We carefully conduct extensive experiments on theunstructured social activity attribute (USAA) dataset and the experimentalresults demonstrate the effectiveness of the proposed multiview Hessianregularized logistic regression for human action recognition.
arxiv-6300-209 | Blind and fully constrained unmixing of hyperspectral images | http://arxiv.org/abs/1403.0289 | author:Rita Ammanouil, André Ferrari, Cédric Richard, David Mary category:stat.AP stat.ML published:2014-03-03 summary:This paper addresses the problem of blind and fully constrained unmixing ofhyperspectral images. Unmixing is performed without the use of any dictionary,and assumes that the number of constituent materials in the scene and theirspectral signatures are unknown. The estimated abundances satisfy the desiredsum-to-one and nonnegativity constraints. Two models with increasing complexityare developed to achieve this challenging task, depending on how noiseinteracts with hyperspectral data. The first one leads to a convex optimizationproblem, and is solved with the Alternating Direction Method of Multipliers.The second one accounts for signal-dependent noise, and is addressed with aReweighted Least Squares algorithm. Experiments on synthetic and real datademonstrate the effectiveness of our approach.
arxiv-6300-210 | A Compilation Target for Probabilistic Programming Languages | http://arxiv.org/abs/1403.0504 | author:Brooks Paige, Frank Wood category:cs.AI cs.PL stat.ML published:2014-03-03 summary:Forward inference techniques such as sequential Monte Carlo and particleMarkov chain Monte Carlo for probabilistic programming can be implemented inany programming language by creative use of standardized operating systemfunctionality including processes, forking, mutexes, and shared memory.Exploiting this we have defined, developed, and tested a probabilisticprogramming language intermediate representation language we call probabilisticC, which itself can be compiled to machine code by standard compilers andlinked to operating system libraries yielding an efficient, scalable, portableprobabilistic programming compilation target. This opens up a new hardware andsystems research path for optimizing probabilistic programming systems.
arxiv-6300-211 | Bayes Merging of Multiple Vocabularies for Scalable Image Retrieval | http://arxiv.org/abs/1403.0284 | author:Liang Zheng, Shengjin Wang, Wengang Zhou, Qi Tian category:cs.CV published:2014-03-03 summary:The Bag-of-Words (BoW) representation is well applied to recentstate-of-the-art image retrieval works. Typically, multiple vocabularies aregenerated to correct quantization artifacts and improve recall. However, thisroutine is corrupted by vocabulary correlation, i.e., overlapping amongdifferent vocabularies. Vocabulary correlation leads to an over-counting of theindexed features in the overlapped area, or the intersection set, thuscompromising the retrieval accuracy. In order to address the correlationproblem while preserve the benefit of high recall, this paper proposes a Bayesmerging approach to down-weight the indexed features in the intersection set.Through explicitly modeling the correlation problem in a probabilistic view, ajoint similarity on both image- and feature-level is estimated for the indexedfeatures in the intersection set. We evaluate our method through extensive experiments on three benchmarkdatasets. Albeit simple, Bayes merging can be well applied in various mergingtasks, and consistently improves the baselines on multi-vocabulary merging.Moreover, Bayes merging is efficient in terms of both time and memory cost, andyields competitive performance compared with the state-of-the-art methods.
arxiv-6300-212 | Object Tracking via Non-Euclidean Geometry: A Grassmann Approach | http://arxiv.org/abs/1403.0309 | author:Sareh Shirazi, Mehrtash T. Harandi, Brian C. Lovell, Conrad Sanderson category:cs.CV math.MG stat.ML published:2014-03-03 summary:A robust visual tracking system requires an object appearance model that isable to handle occlusion, pose, and illumination variations in the videostream. This can be difficult to accomplish when the model is trained usingonly a single image. In this paper, we first propose a tracking approach basedon affine subspaces (constructed from several images) which are able toaccommodate the abovementioned variations. We use affine subspaces not only torepresent the object, but also the candidate areas that the object may occupy.We furthermore propose a novel approach to measure affine subspace-to-subspacedistance via the use of non-Euclidean geometry of Grassmann manifolds. Thetracking problem is then considered as an inference task in a Markov ChainMonte Carlo framework via particle filtering. Quantitative evaluation onchallenging video sequences indicates that the proposed approach obtainsconsiderably better performance than several recent state-of-the-art methodssuch as Tracking-Learning-Detection and MILtrack.
arxiv-6300-213 | Unconstrained Online Linear Learning in Hilbert Spaces: Minimax Algorithms and Normal Approximations | http://arxiv.org/abs/1403.0628 | author:H. Brendan McMahan, Francesco Orabona category:cs.LG published:2014-03-03 summary:We study algorithms for online linear optimization in Hilbert spaces,focusing on the case where the player is unconstrained. We develop a novelcharacterization of a large class of minimax algorithms, recovering, and evenimproving, several previous results as immediate corollaries. Moreover, usingour tools, we develop an algorithm that provides a regret bound of$\mathcal{O}\Big(U \sqrt{T \log(U \sqrt{T} \log^2 T +1)}\Big)$, where $U$ isthe $L_2$ norm of an arbitrary comparator and both $T$ and $U$ are unknown tothe player. This bound is optimal up to $\sqrt{\log \log T}$ terms. When $T$ isknown, we derive an algorithm with an optimal regret bound (up to constantfactors). For both the known and unknown $T$ case, a Normal approximation tothe conditional value of the game proves to be the key analysis tool.
arxiv-6300-214 | Summarisation of Short-Term and Long-Term Videos using Texture and Colour | http://arxiv.org/abs/1403.0315 | author:Johanna Carvajal, Chris McCool, Conrad Sanderson category:cs.CV stat.AP published:2014-03-03 summary:We present a novel approach to video summarisation that makes use of aBag-of-visual-Textures (BoT) approach. Two systems are proposed, one basedsolely on the BoT approach and another which exploits both colour informationand BoT features. On 50 short-term videos from the Open Video Project we showthat our BoT and fusion systems both achieve state-of-the-art performance,obtaining an average F-measure of 0.83 and 0.86 respectively, a relativeimprovement of 9% and 13% when compared to the previous state-of-the-art. Whenapplied to a new underwater surveillance dataset containing 33 long-termvideos, the proposed system reduces the amount of footage by a factor of 27,with only minor degradation in the information content. This order of magnitudereduction in video data represents significant savings in terms of time andpotential labour cost when manually reviewing such footage.
arxiv-6300-215 | On the Intersection Property of Conditional Independence and its Application to Causal Discovery | http://arxiv.org/abs/1403.0408 | author:Jonas Peters category:math.PR stat.ML published:2014-03-03 summary:This work investigates the intersection property of conditional independence.It states that for random variables $A,B,C$ and $X$ we have that $X$independent of $A$ given $B,C$ and $X$ independent of $B$ given $A,C$ implies$X$ independent of $(A,B)$ given $C$. Under the assumption that the jointdistribution has a continuous density, we provide necessary and sufficientconditions under which the intersection property holds. The result has directapplications to causal inference: it leads to strictly weaker conditions underwhich the graphical structure becomes identifiable from the joint distributionof an additive noise model.
arxiv-6300-216 | Cascading Randomized Weighted Majority: A New Online Ensemble Learning Algorithm | http://arxiv.org/abs/1403.0388 | author:Mohammadzaman Zamani, Hamid Beigy, Amirreza Shaban category:stat.ML cs.LG published:2014-03-03 summary:With the increasing volume of data in the world, the best approach forlearning from this data is to exploit an online learning algorithm. Onlineensemble methods are online algorithms which take advantage of an ensemble ofclassifiers to predict labels of data. Prediction with expert advice is awell-studied problem in the online ensemble learning literature. The WeightedMajority algorithm and the randomized weighted majority (RWM) are the mostwell-known solutions to this problem, aiming to converge to the best expert.Since among some expert, the best one does not necessarily have the minimumerror in all regions of data space, defining specific regions and converging tothe best expert in each of these regions will lead to a better result. In thispaper, we aim to resolve this defect of RWM algorithms by proposing a novelonline ensemble algorithm to the problem of prediction with expert advice. Wepropose a cascading version of RWM to achieve not only better experimentalresults but also a better error bound for sufficiently large datasets.
arxiv-6300-217 | Sleep Analytics and Online Selective Anomaly Detection | http://arxiv.org/abs/1403.0156 | author:Tahereh Babaie, Sanjay Chawla, Romesh Abeysuriya category:cs.LG published:2014-03-02 summary:We introduce a new problem, the Online Selective Anomaly Detection (OSAD), tomodel a specific scenario emerging from research in sleep science. Scientistshave segmented sleep into several stages and stage two is characterized by twopatterns (or anomalies) in the EEG time series recorded on sleep subjects.These two patterns are sleep spindle (SS) and K-complex. The OSAD problem wasintroduced to design a residual system, where all anomalies (known and unknown)are detected but the system only triggers an alarm when non-SS anomaliesappear. The solution of the OSAD problem required us to combine techniques fromboth machine learning and control theory. Experiments on data from realsubjects attest to the effectiveness of our approach.
arxiv-6300-218 | Network Traffic Decomposition for Anomaly Detection | http://arxiv.org/abs/1403.0157 | author:Tahereh Babaie, Sanjay Chawla, Sebastien Ardon category:cs.LG cs.NI published:2014-03-02 summary:In this paper we focus on the detection of network anomalies like Denial ofService (DoS) attacks and port scans in a unified manner. While there has beenan extensive amount of research in network anomaly detection, current state ofthe art methods are only able to detect one class of anomalies at the cost ofothers. The key tool we will use is based on the spectral decomposition of atrajectory/hankel matrix which is able to detect deviations from both betweenand within correlation present in the observed network traffic data. Detailedexperiments on synthetic and real network traces shows a significantimprovement in detection capability over competing approaches. In the processwe also address the issue of robustness of anomaly detection systems in aprincipled fashion.
arxiv-6300-219 | Particle methods enable fast and simple approximation of Sobolev gradients in image segmentation | http://arxiv.org/abs/1403.0240 | author:Ivo F. Sbalzarini, Sophie Schneider, Janick Cardinale category:cs.CV cs.CE cs.NA q-bio.QM published:2014-03-02 summary:Bio-image analysis is challenging due to inhomogeneous intensitydistributions and high levels of noise in the images. Bayesian inferenceprovides a principled way for regularizing the problem using prior knowledge. Afundamental choice is how one measures "distances" between shapes in an image.It has been shown that the straightforward geometric L2 distance is degenerateand leads to pathological situations. This is avoided when using Sobolevgradients, rendering the segmentation problem less ill-posed. The highcomputational cost and implementation overhead of Sobolev gradients, however,have hampered practical applications. We show how particle methods as appliedto image segmentation allow for a simple and computationally efficientimplementation of Sobolev gradients. We show that the evaluation of Sobolevgradients amounts to particle-particle interactions along the contour in animage. We extend an existing particle-based segmentation algorithm to usingSobolev gradients. Using synthetic and real-world images, we benchmark theresults for both 2D and 3D images using piecewise smooth and piecewise constantregion models. The present particle approximation of Sobolev gradients is 2.8to 10 times faster than the previous reference implementation, but retains theknown favorable properties of Sobolev gradients. This speedup is achieved byusing local particle-particle interactions instead of solving a global Poissonequation at each iteration. The computational time per iteration is higher forSobolev gradients than for L2 gradients. Since Sobolev gradients preconditionthe optimization problem, however, a smaller number of overall iterations maybe necessary for the algorithm to converge, which can in some cases amortizethe higher per-iteration cost.
arxiv-6300-220 | Real-time Topic-aware Influence Maximization Using Preprocessing | http://arxiv.org/abs/1403.0057 | author:Wei Chen, Tian Lin, Cheng Yang category:cs.SI cs.LG F.2.2 published:2014-03-01 summary:Influence maximization is the task of finding a set of seed nodes in a socialnetwork such that the influence spread of these seed nodes based on certaininfluence diffusion model is maximized. Topic-aware influence diffusion modelshave been recently proposed to address the issue that influence between a pairof users are often topic-dependent and information, ideas, innovations etc.being propagated in networks (referred collectively as items in this paper) aretypically mixtures of topics. In this paper, we focus on the topic-awareinfluence maximization task. In particular, we study preprocessing methods forthese topics to avoid redoing influence maximization for each item fromscratch. We explore two preprocessing algorithms with theoreticaljustifications. Our empirical results on data obtained in a couple of existingstudies demonstrate that one of our algorithms stands out as a strong candidateproviding microsecond online response time and competitive influence spread,with reasonable preprocessing effort.
arxiv-6300-221 | TBX goes TEI -- Implementing a TBX basic extension for the Text Encoding Initiative guidelines | http://arxiv.org/abs/1403.0052 | author:Laurent Romary category:cs.CL published:2014-03-01 summary:This paper presents an attempt to customise the TEI (Text EncodingInitiative) guidelines in order to offer the possibility to incorporate TBX(TermBase eXchange) based terminological entries within any kind of TEIdocuments. After presenting the general historical, conceptual and technicalcontexts, we describe the various design choices we had to take while creatingthis customisation, which in turn have led to make various changes in theactual TBX serialisation. Keeping in mind the objective to provide the TEIguidelines with, again, an onomasiological model, we try to identify the bestcomprise in maintaining both the isomorphism with the existing TBX Basicstandard and the characteristics of the TEI framework.
arxiv-6300-222 | Temporal Image Fusion | http://arxiv.org/abs/1403.0087 | author:Francisco J. Estrada category:cs.CV cs.GR published:2014-03-01 summary:This paper introduces temporal image fusion. The proposed technique buildsupon previous research in exposure fusion and expands it to deal with thelimited Temporal Dynamic Range of existing sensors and camera technologies. Inparticular, temporal image fusion enables the rendering of long-exposureeffects on full frame-rate video, as well as the generation of arbitrarily longexposures from a sequence of images of the same scene taken over time. Weexplore the problem of temporal under-exposure, and show how it can beaddressed by selectively enhancing dynamic structure. Finally, we show that theuse of temporal image fusion together with content-selective image filters canproduce a range of striking visual effects on a given input sequence.
arxiv-6300-223 | Learning Graphical Models With Hubs | http://arxiv.org/abs/1402.7349 | author:Kean Ming Tan, Palma London, Karthik Mohan, Su-In Lee, Maryam Fazel, Daniela Witten category:stat.ML stat.CO stat.ME published:2014-02-28 summary:We consider the problem of learning a high-dimensional graphical model inwhich certain hub nodes are highly-connected to many other nodes. Many authorshave studied the use of an l1 penalty in order to learn a sparse graph inhigh-dimensional setting. However, the l1 penalty implicitly assumes that eachedge is equally likely and independent of all other edges. We propose a generalframework to accommodate more realistic networks with hub nodes, using a convexformulation that involves a row-column overlap norm penalty. We apply thisgeneral framework to three widely-used probabilistic graphical models: theGaussian graphical model, the covariance graph model, and the binary Isingmodel. An alternating direction method of multipliers algorithm is used tosolve the corresponding convex optimization problems. On synthetic data, wedemonstrate that our proposed framework outperforms competitors that do notexplicitly model hub nodes. We illustrate our proposal on a webpage data setand a gene expression data set.
arxiv-6300-224 | An Incidence Geometry approach to Dictionary Learning | http://arxiv.org/abs/1402.7344 | author:Meera Sitharam, Mohamad Tarifi, Menghan Wang category:cs.LG stat.ML published:2014-02-28 summary:We study the Dictionary Learning (aka Sparse Coding) problem of obtaining asparse representation of data points, by learning \emph{dictionary vectors}upon which the data points can be written as sparse linear combinations. Weview this problem from a geometry perspective as the spanning set of a subspacearrangement, and focus on understanding the case when the underlying hypergraphof the subspace arrangement is specified. For this Fitted Dictionary Learningproblem, we completely characterize the combinatorics of the associatedsubspace arrangements (i.e.\ their underlying hypergraphs). Specifically, acombinatorial rigidity-type theorem is proven for a type of geometric incidencesystem. The theorem characterizes the hypergraphs of subspace arrangements thatgenerically yield (a) at least one dictionary (b) a locally unique dictionary(i.e.\ at most a finite number of isolated dictionaries) of the specified size.We are unaware of prior application of combinatorial rigidity techniques in thesetting of Dictionary Learning, or even in machine learning. We also provide asystematic classification of problems related to Dictionary Learning togetherwith various algorithms, their assumptions and performance.
arxiv-6300-225 | A Machine Learning Model for Stock Market Prediction | http://arxiv.org/abs/1402.7351 | author:Osman Hegazy, Omar S. Soliman, Mustafa Abdul Salam category:cs.CE cs.NE published:2014-02-28 summary:Stock market prediction is the act of trying to determine the future value ofa company stock or other financial instrument traded on a financial exchange.
arxiv-6300-226 | Semantics, Modelling, and the Problem of Representation of Meaning -- a Brief Survey of Recent Literature | http://arxiv.org/abs/1402.7265 | author:Yarin Gal category:cs.CL published:2014-02-28 summary:Over the past 50 years many have debated what representation should be usedto capture the meaning of natural language utterances. Recently new needs ofsuch representations have been raised in research. Here I survey some of theinteresting representations suggested to answer for these new needs.
arxiv-6300-227 | Neural Network Approach to Railway Stand Lateral Skew Control | http://arxiv.org/abs/1402.7136 | author:Peter Mark Benes, Ivo Bukovsky, Matous Cejnek, Jan Kalivoda category:cs.SY cs.NE published:2014-02-28 summary:The paper presents a study of an adaptive approach to lateral skew controlfor an experimental railway stand. The preliminary experiments with the realexperimental railway stand and simulations with its 3-D mechanical model,indicates difficulties of model-based control of the device. Thus, use ofneural networks for identification and control of lateral skew shall beinvestigated. This paper focuses on real-data based modeling of the railwaystand by various neural network models, i.e; linear neural unit and quadraticneural unit architectures. Furthermore, training methods of these neuralarchitectures as such, real-time-recurrent-learning and a variation ofback-propagation-through-time are examined, accompanied by a discussion of theproduced experimental results.
arxiv-6300-228 | Visual Saliency Model using SIFT and Comparison of Learning Approaches | http://arxiv.org/abs/1402.7162 | author:Hamdi Yalin Yalic category:cs.CV I.2.10; I.5.4 published:2014-02-28 summary:Humans' ability to detect and locate salient objects on images is remarkablyfast and successful. Performing this process by using eye tracking equipment isexpensive and cannot be easily applied, and computer modeling of this humanbehavior is still a problem to be solved. In our study, one of the largestpublic eye-tracking databases which has fixation points of 15 observers on 1003images is used. In addition to low, medium and high-level features which havebeen used in previous studies, SIFT features extracted from the images are usedto improve the classification accuracy of the models. A second contribution ofthis paper is the comparison and statistical analysis of different machinelearning methods that can be used to train our model. As a result, a bestfeature set and learning model to predict where humans look at images, isdetermined.
arxiv-6300-229 | Addendum on the scoring of Gaussian directed acyclic graphical models | http://arxiv.org/abs/1402.6863 | author:Jack Kuipers, Giusi Moffa, David Heckerman category:stat.ML published:2014-02-27 summary:We provide a correction to the expression for scoring Gaussian directedacyclic graphical models derived in Geiger and Heckerman [Ann. Statist. 30(2002) 1414-1440] and discuss how to evaluate the score efficiently.
arxiv-6300-230 | Data-driven HRF estimation for encoding and decoding models | http://arxiv.org/abs/1402.7015 | author:Fabian Pedregosa, Michael Eickenberg, Philippe Ciuciu, Bertrand Thirion, Alexandre Gramfort category:cs.CE cs.LG published:2014-02-27 summary:Despite the common usage of a canonical, data-independent, hemodynamicresponse function (HRF), it is known that the shape of the HRF varies acrossbrain regions and subjects. This suggests that a data-driven estimation of thisfunction could lead to more statistical power when modeling BOLD fMRI data.However, unconstrained estimation of the HRF can yield highly unstable resultswhen the number of free parameters is large. We develop a method for the jointestimation of activation and HRF using a rank constraint causing the estimatedHRF to be equal across events/conditions, yet permitting it to be differentacross voxels. Model estimation leads to an optimization problem that wepropose to solve with an efficient quasi-Newton method exploiting fast gradientcomputations. This model, called GLM with Rank-1 constraint (R1-GLM), can beextended to the setting of GLM with separate designs which has been shown toimprove decoding accuracy in brain activity decoding experiments. We compare 10different HRF modeling methods in terms of encoding and decoding score in twodifferent datasets. Our results show that the R1-GLM model significantlyoutperforms competing methods in both encoding and decoding settings,positioning it as an attractive method both from the points of view of accuracyand computational efficiency.
arxiv-6300-231 | Sequential Complexity as a Descriptor for Musical Similarity | http://arxiv.org/abs/1402.6926 | author:Peter Foster, Matthias Mauch, Simon Dixon category:cs.IR cs.LG cs.SD published:2014-02-27 summary:We propose string compressibility as a descriptor of temporal structure inaudio, for the purpose of determining musical similarity. Our descriptors arebased on computing track-wise compression rates of quantised audio features,using multiple temporal resolutions and quantisation granularities. To verifythat our descriptors capture musically relevant information, we incorporate ourdescriptors into similarity rating prediction and song year prediction tasks.We base our evaluation on a dataset of 15500 track excerpts of Western popularmusic, for which we obtain 7800 web-sourced pairwise similarity ratings. Toassess the agreement among similarity ratings, we perform an evaluation undercontrolled conditions, obtaining a rank correlation of 0.33 between intersectedsets of ratings. Combined with bag-of-features descriptors, we obtainperformance gains of 31.1% and 10.9% for similarity rating prediction and songyear prediction. For both tasks, analysis of selected descriptors reveals thatrepresenting features at multiple time scales benefits prediction accuracy.
arxiv-6300-232 | Modeling the Complex Dynamics and Changing Correlations of Epileptic Events | http://arxiv.org/abs/1402.6951 | author:Drausin F. Wulsin, Emily B. Fox, Brian Litt category:stat.ML q-bio.NC stat.AP published:2014-02-27 summary:Patients with epilepsy can manifest short, sub-clinical epileptic "bursts" inaddition to full-blown clinical seizures. We believe the relationship betweenthese two classes of events---something not previously studiedquantitatively---could yield important insights into the nature and intrinsicdynamics of seizures. A goal of our work is to parse these complex epilepticevents into distinct dynamic regimes. A challenge posed by the intracranial EEG(iEEG) data we study is the fact that the number and placement of electrodescan vary between patients. We develop a Bayesian nonparametric Markov switchingprocess that allows for (i) shared dynamic regimes between a variable number ofchannels, (ii) asynchronous regime-switching, and (iii) an unknown dictionaryof dynamic regimes. We encode a sparse and changing set of dependencies betweenthe channels using a Markov-switching Gaussian graphical model for theinnovations process driving the channel dynamics and demonstrate the importanceof this model in parsing and out-of-sample predictions of iEEG data. We showthat our model produces intuitive state assignments that can help automateclinical analysis of seizures and enable the comparison of sub-clinical burstsand full clinical seizures.
arxiv-6300-233 | Bayesian Multi-Scale Optimistic Optimization | http://arxiv.org/abs/1402.7005 | author:Ziyu Wang, Babak Shakibi, Lin Jin, Nando de Freitas category:stat.ML cs.LG published:2014-02-27 summary:Bayesian optimization is a powerful global optimization technique forexpensive black-box functions. One of its shortcomings is that it requiresauxiliary optimization of an acquisition function at each iteration. Thisauxiliary optimization can be costly and very hard to carry out in practice.Moreover, it creates serious theoretical concerns, as most of the convergenceresults assume that the exact optimum of the acquisition function can be found.In this paper, we introduce a new technique for efficient global optimizationthat combines Gaussian process confidence bounds and treed simultaneousoptimistic optimization to eliminate the need for auxiliary optimization ofacquisition functions. The experiments with global optimization benchmarks anda novel application to automatic information extraction demonstrate that theresulting technique is more efficient than the two approaches from which itdraws inspiration. Unlike most theoretical analyses of Bayesian optimizationwith Gaussian processes, our finite-time convergence rate proofs do not requireexact optimization of an acquisition function. That is, our approach eliminatesthe unsatisfactory assumption that a difficult, potentially NP-hard, problemhas to be solved in order to obtain vanishing regret rates.
arxiv-6300-234 | Marginalizing Corrupted Features | http://arxiv.org/abs/1402.7001 | author:Laurens van der Maaten, Minmin Chen, Stephen Tyree, Kilian Weinberger category:cs.LG published:2014-02-27 summary:The goal of machine learning is to develop predictors that generalize well totest data. Ideally, this is achieved by training on an almost infinitely largetraining data set that captures all variations in the data distribution. Inpractical learning settings, however, we do not have infinite data and ourpredictors may overfit. Overfitting may be combatted, for example, by adding aregularizer to the training objective or by defining a prior over the modelparameters and performing Bayesian inference. In this paper, we propose athird, alternative approach to combat overfitting: we extend the training setwith infinitely many artificial training examples that are obtained bycorrupting the original training data. We show that this approach is practicaland efficient for a range of predictors and corruption models. Our approach,called marginalized corrupted features (MCF), trains robust predictors byminimizing the expected value of the loss function under the corruption model.We show empirically on a variety of data sets that MCF classifiers can betrained efficiently, may generalize substantially better to test data, and arealso more robust to feature deletion at test time.
arxiv-6300-235 | Resourceful Contextual Bandits | http://arxiv.org/abs/1402.6779 | author:Ashwinkumar Badanidiyuru, John Langford, Aleksandrs Slivkins category:cs.LG cs.DS cs.GT published:2014-02-27 summary:We study contextual bandits with ancillary constraints on resources, whichare common in real-world applications such as choosing ads or dynamic pricingof items. We design the first algorithm for solving these problems that handlesconstrained resources other than time, and improves over a trivial reduction tothe non-contextual case. We consider very general settings for both contextualbandits (arbitrary policy sets, e.g. Dudik et al. (UAI'11)) and bandits withresource constraints (bandits with knapsacks, Badanidiyuru et al. (FOCS'13)),and prove a regret guarantee with near-optimal statistical properties.
arxiv-6300-236 | Scalable methods for nonnegative matrix factorizations of near-separable tall-and-skinny matrices | http://arxiv.org/abs/1402.6964 | author:Austin R. Benson, Jason D. Lee, Bartek Rajwa, David F. Gleich category:cs.LG cs.DC cs.NA stat.ML G.1.3; G.1.6 published:2014-02-27 summary:Numerous algorithms are used for nonnegative matrix factorization under theassumption that the matrix is nearly separable. In this paper, we show how tomake these algorithms efficient for data matrices that have many more rows thancolumns, so-called "tall-and-skinny matrices". One key component to theseimproved methods is an orthogonal matrix transformation that preserves theseparability of the NMF problem. Our final methods need a single pass over thedata matrix and are suitable for streaming, multi-core, and MapReducearchitectures. We demonstrate the efficacy of these algorithms onterabyte-sized synthetic matrices and real-world matrices from scientificcomputing and bioinformatics.
arxiv-6300-237 | A Parallel Memetic Algorithm to Solve the Vehicle Routing Problem with Time Windows | http://arxiv.org/abs/1402.6942 | author:Jakub Nalepa, Zbigniew J. Czech category:cs.DC cs.NE published:2014-02-27 summary:This paper presents a parallel memetic algorithm for solving the vehiclerouting problem with time windows (VRPTW). The VRPTW is a well-known NP-harddiscrete optimization problem with two objectives. The main objective is tominimize the number of vehicles serving customers scattered on the map, and thesecond one is to minimize the total distance traveled by the vehicles. Here,the fleet size is minimized in the first phase of the proposed method using theparallel heuristic algorithm (PHA), and the traveled distance is minimized inthe second phase by the parallel memetic algorithm (PMA). In both parallelalgorithms, the parallel components co-operate periodically in order toexchange the best solutions found so far. An extensive experimental studyperformed on the Gehring and Homberger's benchmark proves the high convergencecapabilities and robustness of both PHA and PMA. Also, we present the speedupanalysis of the PMA.
arxiv-6300-238 | Low-Cost Compressive Sensing for Color Video and Depth | http://arxiv.org/abs/1402.6932 | author:Xin Yuan, Patrick Llull, Xuejun Liao, Jianbo Yang, Guillermo Sapiro, David J. Brady, Lawrence Carin category:cs.CV published:2014-02-27 summary:A simple and inexpensive (low-power and low-bandwidth) modification is madeto a conventional off-the-shelf color video camera, from which we recover{multiple} color frames for each of the original measured frames, and each ofthe recovered frames can be focused at a different depth. The recovery ofmultiple frames for each measured frame is made possible via high-speed coding,manifested via translation of a single coded aperture; the inexpensivetranslation is constituted by mounting the binary code on a piezoelectricdevice. To simultaneously recover depth information, a {liquid} lens ismodulated at high speed, via a variable voltage. Consequently, during theaforementioned coding process, the liquid lens allows the camera to sweep thefocus through multiple depths. In addition to designing and implementing thecamera, fast recovery is achieved by an anytime algorithm exploiting thegroup-sparsity of wavelet/DCT coefficients.
arxiv-6300-239 | CriPS: Critical Dynamics in Particle Swarm Optimization | http://arxiv.org/abs/1402.6888 | author:Adam Erskine, J Michael Herrmann category:cs.NE published:2014-02-27 summary:Particle Swarm Optimisation (PSO) makes use of a dynamical system for solvinga search task. Instead of adding search biases in order to improve performancein certain problems, we aim to remove algorithm-induced scales by controllingthe swarm with a mechanism that is scale-free except possibly for a suppressionof scales beyond the system size. In this way a very promising performance isachieved due to the balance of large-scale exploration and local search. Theresulting algorithm shows evidence for self-organised criticality, broughtabout via the intrinsic dynamics of the swarm as it interacts with theobjective function, rather than being explicitly specified. The CriticalParticle Swarm (CriPS) can be easily combined with many existing extensionssuch as chaotic exploration, additional force terms or non-trivial topologies.
arxiv-6300-240 | It's distributions all the way down!: Second order changes in statistical distributions also occur | http://arxiv.org/abs/1402.6880 | author:M. T. Keane, A. Gerow category:cs.CL published:2014-02-27 summary:The textual, big-data literature misses Bentley, OBrien, & Brocks (Bentley etals) message on distributions; it largely examines the first-order effects ofhow a single, signature distribution can predict population behaviour,neglecting second-order effects involving distributional shifts, either betweensignature distributions or within a given signature distribution. Indeed,Bentley et al. themselves under-emphasise the potential richness of the latter,within-distribution effects.
arxiv-6300-241 | Outlier Detection using Improved Genetic K-means | http://arxiv.org/abs/1402.6859 | author:M. H. Marghny, Ahmed I. Taloba category:cs.LG cs.DB published:2014-02-27 summary:The outlier detection problem in some cases is similar to the classificationproblem. For example, the main concern of clustering-based outlier detectionalgorithms is to find clusters and outliers, which are often regarded as noisethat should be removed in order to make more reliable clustering. In thisarticle, we present an algorithm that provides outlier detection and dataclustering simultaneously. The algorithmimprovesthe estimation of centroids ofthe generative distribution during the process of clustering and outlierdiscovery. The proposed algorithm consists of two stages. The first stageconsists of improved genetic k-means algorithm (IGK) process, while the secondstage iteratively removes the vectors which are far from their clustercentroids.
arxiv-6300-242 | An Effective Evolutionary Clustering Algorithm: Hepatitis C Case Study | http://arxiv.org/abs/1405.6173 | author:M. H. Marghny, Rasha M. Abd El-Aziz, Ahmed I. Taloba category:cs.NE cs.CE published:2014-02-27 summary:Clustering analysis plays an important role in scientific research andcommercial application. K-means algorithm is a widely used partition method inclustering. However, it is known that the K-means algorithm may get stuck atsuboptimal solutions, depending on the choice of the initial cluster centers.In this article, we propose a technique to handle large scale data, which canselect initial clustering center purposefully using Genetic algorithms (GAs),reduce the sensitivity to isolated point, avoid dissevering big cluster, andovercome deflexion of data in some degree that caused by the disproportion indata partitioning owing to adoption of multi-sampling. We applied our method tosome public datasets these show the advantages of the proposed approach forexample Hepatitis C dataset that has been taken from the machine learningwarehouse of University of California. Our aim is to evaluate hepatitisdataset. In order to evaluate this dataset we did some preprocessing operation,the reason to preprocessing is to summarize the data in the best and suitableway for our algorithm. Missing values of the instances are adjusted using localmean method.
arxiv-6300-243 | A method to identify potential ambiguous Malay words through Ambiguity Attributes mapping: An exploratory Study | http://arxiv.org/abs/1402.6764 | author:Hazlina Haron, Abdul Azim Abd. Ghani category:cs.SE cs.CL published:2014-02-27 summary:We describe here a methodology to identify a list of ambiguous Malay wordsthat are commonly being used in Malay documentations such as RequirementSpecification. We compiled several relevant and appropriate requirement qualityattributes and sentence rules from previous literatures and adopt it to comeout with a set of ambiguity attributes that most suit Malay words. Theextracted Malay ambiguous words (potential) are then being mapped onto theconstructed ambiguity attributes to confirm their vagueness. The list is thenverified by Malay linguist experts. This paper aims to identify a list ofpotential ambiguous words in Malay as an attempt to assist writers to avoidusing the vague words while documenting Malay Requirement Specification as wellas to any other related Malay documentation. The result of this study is a listof 120 potential ambiguous Malay words that could act as guidelines in writingMalay sentences
arxiv-6300-244 | Synthesis of Parametric Programs using Genetic Programming and Model Checking | http://arxiv.org/abs/1402.6785 | author:Gal Katz, Doron Peled category:cs.SE cs.AI cs.NE published:2014-02-27 summary:Formal methods apply algorithms based on mathematical principles to enhancethe reliability of systems. It would only be natural to try to progress fromverification, model checking or testing a system against its formalspecification into constructing it automatically. Classical algorithmicsynthesis theory provides interesting algorithms but also alarming highcomplexity and undecidability results. The use of genetic programming, incombination with model checking and testing, provides a powerful heuristic tosynthesize programs. The method is not completely automatic, as it is finetuned by a user that sets up the specification and parameters. It also does notguarantee to always succeed and converge towards a solution that satisfies allthe required properties. However, we applied it successfully on quitenontrivial examples and managed to find solutions to hard programmingchallenges, as well as to improve and to correct code. We describe here severalversions of our method for synthesizing sequential and concurrent systems.
arxiv-6300-245 | Information Evolution in Social Networks | http://arxiv.org/abs/1402.6792 | author:Lada A. Adamic, Thomas M. Lento, Eytan Adar, Pauline C. Ng category:cs.SI cs.CL physics.soc-ph published:2014-02-27 summary:Social networks readily transmit information, albeit with less than perfectfidelity. We present a large-scale measurement of this imperfect informationcopying mechanism by examining the dissemination and evolution of thousands ofmemes, collectively replicated hundreds of millions of times in the onlinesocial network Facebook. The information undergoes an evolutionary process thatexhibits several regularities. A meme's mutation rate characterizes thepopulation distribution of its variants, in accordance with the Yule process.Variants further apart in the diffusion cascade have greater edit distance, aswould be expected in an iterative, imperfect replication process. Some textsequences can confer a replicative advantage; these sequences are abundant andtransfer "laterally" between different memes. Subpopulations of the socialnetwork can preferentially transmit a specific variant of a meme if the variantmatches their beliefs or culture. Understanding the mechanism driving change indiffusing information has important implications for how we interpret andharness the information that reaches us through our social networks.
arxiv-6300-246 | Robust Asymmetric Clustering | http://arxiv.org/abs/1402.6744 | author:Katherine Morris, Paul D. McNicholas, Antonio Punzo, Ryan P. Browne category:stat.ME stat.CO stat.ML published:2014-02-26 summary:Contaminated mixture models are developed for model-based clustering of datawith asymmetric clusters as well as spurious points, outliers, and/or noise.Specifically, we introduce a contaminated mixture of contaminated shiftedasymmetric Laplace distributions and a contaminated mixture of contaminatedskew-normal distributions. In each case, mixture components have a parametercontrolling the proportion of bad points (i.e., spurious points, outliers,and/or noise) and one specifying the degree of contamination. A very importantfeature of our approaches is that these parameters do not have to be specifieda priori. Expectation-conditional maximization algorithms are outlined forparameter estimation and the number of components is selected using theBayesian information criterion. The performance of our approaches isillustrated on artificial and real data.
arxiv-6300-247 | Why Are You More Engaged? Predicting Social Engagement from Word Use | http://arxiv.org/abs/1402.6690 | author:Jalal Mahmud, Jilin Chen, Jeffrey Nichols category:cs.SI cs.CL cs.CY published:2014-02-26 summary:We present a study to analyze how word use can predict social engagementbehaviors such as replies and retweets in Twitter. We compute psycholinguisticcategory scores from word usage, and investigate how people with differentscores exhibited different reply and retweet behaviors on Twitter. We alsofound psycholinguistic categories that show significant correlations with suchsocial engagement behaviors. In addition, we have built predictive models ofreplies and retweets from such psycholinguistic category based features. Ourexperiments using a real world dataset collected from Twitter validates thatsuch predictions can be done with reasonable accuracy.
arxiv-6300-248 | A Novel Method for the Recognition of Isolated Handwritten Arabic Characters | http://arxiv.org/abs/1402.6650 | author:Ahmed Sahlol, Cheng Suen category:cs.CV 68T10 published:2014-02-26 summary:There are many difficulties facing a handwritten Arabic recognition systemsuch as unlimited variation in human handwriting, similarities of distinctcharacter shapes, interconnections of neighbouring characters and theirposition in the word. The typical Optical Character Recognition (OCR) systemsare based mainly on three stages, preprocessing, features extraction andrecognition. This paper proposes new methods for handwritten Arabic characterrecognition which is based on novel preprocessing operations includingdifferent kinds of noise removal also different kind of features likestructural, Statistical and Morphological features from the main body of thecharacter and also from the secondary components. Evaluation of the accuracy ofthe selected features is made. The system was trained and tested by backpropagation neural network with CENPRMI dataset. The proposed algorithmobtained promising results as it is able to recognize 88% of our test setaccurately. In Comparable with other related works we find that our result isthe highest among other published works.
arxiv-6300-249 | Evolutionary solving of the debts' clearing problem | http://arxiv.org/abs/1402.6556 | author:Csaba Patcas, Attila Bartha category:cs.NE cs.AI 97R40 I.2.8; G.2.3 published:2014-02-26 summary:The debts' clearing problem is about clearing all the debts in a group of nentities (persons, companies etc.) using a minimal number of money transactionoperations. The problem is known to be NP-hard in the strong sense. As for manyintractable problems, techniques from the field of artificial intelligence areuseful in finding solutions close to optimum for large inputs. An evolutionaryalgorithm for solving the debts' clearing problem is proposed.
arxiv-6300-250 | Renewable Energy Prediction using Weather Forecasts for Optimal Scheduling in HPC Systems | http://arxiv.org/abs/1402.6552 | author:Ankur Sahai category:cs.LG published:2014-02-26 summary:The objective of the GreenPAD project is to use green energy (wind, solar andbiomass) for powering data-centers that are used to run HPC jobs. As a part ofthis it is important to predict the Renewable (Wind) energy for efficientscheduling (executing jobs that require higher energy when there is more greenenergy available and vice-versa). For predicting the wind energy we firstanalyze the historical data to find a statistical model that gives relationbetween wind energy and weather attributes. Then we use this model based on theweather forecast data to predict the green energy availability in the future.Using the green energy prediction obtained from the statistical model we areable to precompute job schedules for maximizing the green energy utilization inthe future. We propose a model which uses live weather data in addition tomachine learning techniques (which can predict future deviations in weatherconditions based on current deviations from the forecast) to make on-the-flychanges to the precomputed schedule (based on green energy prediction). For this we first analyze the data using histograms and simple statisticaltools such as correlation. In addition we build (correlation) regression modelfor finding the relation between wind energy availability and weatherattributes (temperature, cloud cover, air pressure, wind speed / direction,precipitation and sunshine). We also analyze different algorithms and machinelearning techniques for optimizing the job schedules for maximizing the greenenergy utilization.
arxiv-6300-251 | Modelling the Lexicon in Unsupervised Part of Speech Induction | http://arxiv.org/abs/1402.6516 | author:Greg Dubbin, Phil Blunsom category:cs.CL published:2014-02-26 summary:Automatically inducing the syntactic part-of-speech categories for words intext is a fundamental task in Computational Linguistics. While the performanceof unsupervised tagging models has been slowly improving, currentstate-of-the-art systems make the obviously incorrect assumption that alltokens of a given word type must share a single part-of-speech tag. Thisone-tag-per-type heuristic counters the tendency of Hidden Markov Model basedtaggers to over generate tags for a given word type. However, it is clearlyincompatible with basic syntactic theory. In this paper we extend astate-of-the-art Pitman-Yor Hidden Markov Model tagger with an explicit modelof the lexicon. In doing so we are able to incorporate a soft bias towardsinducing few tags per type. We develop a particle filter for drawing samplesfrom the posterior of our model and present empirical results that show thatour model is competitive with and faster than the state-of-the-art withoutmaking any unrealistic restrictions.
arxiv-6300-252 | Clustering Multidimensional Data with PSO based Algorithm | http://arxiv.org/abs/1402.6428 | author:Jayshree Ghorpade-Aher, Vishakha A. Metre category:cs.NE published:2014-02-26 summary:Data clustering is a recognized data analysis method in data mining whereasK-Means is the well known partitional clustering method, possessing pleasantfeatures. We observed that, K-Means and other partitional clustering techniquessuffer from several limitations such as initial cluster centre selection,preknowledge of number of clusters, dead unit problem, multiple clustermembership and premature convergence to local optima. Several optimizationmethods are proposed in the literature in order to solve clusteringlimitations, but Swarm Intelligence (SI) has achieved its remarkable positionin the concerned area. Particle Swarm Optimization (PSO) is the most popular SItechnique and one of the favorite areas of researchers. In this paper, wepresent a brief overview of PSO and applicability of its variants to solveclustering challenges. Also, we propose an advanced PSO algorithm named asSubtractive Clustering based Boundary Restricted Adaptive Particle SwarmOptimization (SC-BR-APSO) algorithm for clustering multidimensional data. Forcomparison purpose, we have studied and analyzed various algorithms such asK-Means, PSO, K-Means-PSO, Hybrid Subtractive + PSO, BRAPSO, and proposedalgorithm on nine different datasets. The motivation behind proposingSC-BR-APSO algorithm is to deal with multidimensional data clustering, withminimum error rate and maximum convergence rate.
arxiv-6300-253 | Deconstruction of compound objects from image sets | http://arxiv.org/abs/1402.6416 | author:Anton van den Hengel, John Bastian, Anthony Dick, Lachlan Fleming category:cs.CV published:2014-02-26 summary:We propose a method to recover the structure of a compound object frommultiple silhouettes. Structure is expressed as a collection of 3D primitiveschosen from a pre-defined library, each with an associated pose. This hasseveral advantages over a volume or mesh representation both for estimation andthe utility of the recovered model. The main challenge in recovering such amodel is the combinatorial number of possible arrangements of parts. We addressthis issue by exploiting the sparse nature of the problem, and show that ourmethod scales to objects constructed from large libraries of parts.
arxiv-6300-254 | Active spline model: A shape based model-interactive segmentation | http://arxiv.org/abs/1402.6387 | author:Jen Hong Tan, U. Rajendra Acharya category:cs.CV published:2014-02-26 summary:Rarely in literature a method of segmentation cares for the edit after thealgorithm delivers. They provide no solution when segmentation goes wrong. Wepropose to formulate point distribution model in terms ofcentripetal-parameterized Catmull-Rom spline. Such fusion brings interactivityto model-based segmentation, so that edit is better handled. When the deliveredsegment is unsatisfactory, user simply shifts points to vary the curve. We ranthe method on three disparate imaging modalities and achieved an averageoverlap of 0.879 for automated lung segmentation on chest radiographs. The editafterward improved the average overlap to 0.945, with a minimum of 0.925. Thesource code and the demo video are available at http://wp.me/p3vCKy-2S
arxiv-6300-255 | Exploiting the Statistics of Learning and Inference | http://arxiv.org/abs/1402.7025 | author:Max Welling category:cs.LG published:2014-02-26 summary:When dealing with datasets containing a billion instances or with simulationsthat require a supercomputer to execute, computational resources become part ofthe equation. We can improve the efficiency of learning and inference byexploiting their inherent statistical nature. We propose algorithms thatexploit the redundancy of data relative to a model by subsampling data-casesfor every update and reasoning about the uncertainty created in this process.In the context of learning we propose to test for the probability that astochastically estimated gradient points more than 180 degrees in the wrongdirection. In the context of MCMC sampling we use stochastic gradients toimprove the efficiency of MCMC updates, and hypothesis tests based on adaptivemini-batches to decide whether to accept or reject a proposed parameter update.Finally, we argue that in the context of likelihood free MCMC one needs tostore all the information revealed by all simulations, for instance in aGaussian process. We conclude that Bayesian methods will remain to play acrucial role in the era of big data and big simulations, but only if weovercome a number of computational challenges.
arxiv-6300-256 | Sparse principal component regression with adaptive loading | http://arxiv.org/abs/1402.6455 | author:Shuichi Kawano, Hironori Fujisawa, Toyoyuki Takada, Toshihiko Shiroishi category:stat.ML stat.ME 62H25, 62J07 published:2014-02-26 summary:Principal component regression (PCR) is a two-stage procedure that selectssome principal components and then constructs a regression model regarding themas new explanatory variables. Note that the principal components are obtainedfrom only explanatory variables and not considered with the response variable.To address this problem, we propose the sparse principal component regression(SPCR) that is a one-stage procedure for PCR. SPCR enables us to adaptivelyobtain sparse principal component loadings that are related to the responsevariable and select the number of principal components simultaneously. SPCR canbe obtained by the convex optimization problem for each of parameters with thecoordinate descent algorithm. Monte Carlo simulations and real data analysesare performed to illustrate the effectiveness of SPCR.
arxiv-6300-257 | Large-margin Learning of Compact Binary Image Encodings | http://arxiv.org/abs/1402.6383 | author:Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel category:cs.CV published:2014-02-26 summary:The use of high-dimensional features has become a normal practice in manycomputer vision applications. The large dimension of these features is alimiting factor upon the number of data points which may be effectively storedand processed, however. We address this problem by developing a novel approachto learning a compact binary encoding, which exploits both pair-wise proximityand class-label information on training data set. Exploiting this extrainformation allows the development of encodings which, although compact,outperform the original high-dimensional features in terms of finalclassification or retrieval performance. The method is general, in that it isapplicable to both non-parametric and parametric learning methods. Thisgenerality means that the embedded features are suitable for a wide variety ofcomputer vision tasks, such as image classification and content-based imageretrieval. Experimental results demonstrate that the new compact descriptorachieves an accuracy comparable to, and in some cases better than, the visualdescriptor in the original space despite being significantly more compact.Moreover, any convex loss function and convex regularization penalty (e.g., $\ell_p $ norm with $ p \ge 1 $) can be incorporated into the framework, whichprovides future flexibility.
arxiv-6300-258 | A DCT Approximation for Image Compression | http://arxiv.org/abs/1402.6034 | author:R. J. Cintra, F. M. Bayer category:cs.MM cs.CV stat.ME published:2014-02-25 summary:An orthogonal approximation for the 8-point discrete cosine transform (DCT)is introduced. The proposed transformation matrix contains only zeros and ones;multiplications and bit-shift operations are absent. Close spectral behaviorrelative to the DCT was adopted as design criterion. The proposed algorithm issuperior to the signed discrete cosine transform. It could also outperformstate-of-the-art algorithms in low and high image compression scenarios,exhibiting at the same time a comparable computational complexity.
arxiv-6300-259 | Machine Learning at Scale | http://arxiv.org/abs/1402.6076 | author:Sergei Izrailev, Jeremy M. Stanley category:cs.LG cs.MS stat.ML I.5.2 published:2014-02-25 summary:It takes skill to build a meaningful predictive model even with the abundanceof implementations of modern machine learning algorithms and readily availablecomputing resources. Building a model becomes challenging if hundreds ofterabytes of data need to be processed to produce the training data set. In adigital advertising technology setting, we are faced with the need to buildthousands of such models that predict user behavior and power advertisingcampaigns in a 24/7 chaotic real-time production environment. As datascientists, we also have to convince other internal departments critical toimplementation success, our management, and our customers that our machinelearning system works. In this paper, we present the details of the design andimplementation of an automated, robust machine learning platform that impactsbillions of advertising impressions monthly. This platform enables us tocontinuously optimize thousands of campaigns over hundreds of millions ofusers, on multiple continents, against varying performance objectives.
arxiv-6300-260 | LSSVM-ABC Algorithm for Stock Price prediction | http://arxiv.org/abs/1402.6366 | author:Osman Hegazy, Omar S. Soliman, Mustafa Abdul Salam category:cs.CE cs.NE published:2014-02-25 summary:In this paper, Artificial Bee Colony (ABC) algorithm which inspired from thebehavior of honey bees swarm is presented. ABC is a stochastic population-basedevolutionary algorithm for problem solving. ABC algorithm, which is consideredone of the most recently swarm intelligent techniques, is proposed to optimizeleast square support vector machine (LSSVM) to predict the daily stock prices.The proposed model is based on the study of stocks historical data, technicalindicators and optimizing LSSVM with ABC algorithm. ABC selects best freeparameters combination for LSSVM to avoid over-fitting and local minimaproblems and improve prediction accuracy. LSSVM optimized by Particle swarmoptimization (PSO) algorithm, LSSVM, and ANN techniques are used for comparisonwith proposed model. Proposed model tested with twenty datasets representingdifferent sectors in S&P 500 stock market. Results presented in this paper showthat the proposed model has fast convergence speed, and it also achieves betteraccuracy than compared techniques in most cases.
arxiv-6300-261 | Oracle-Based Robust Optimization via Online Learning | http://arxiv.org/abs/1402.6361 | author:Aharon Ben-Tal, Elad Hazan, Tomer Koren, Shie Mannor category:math.OC cs.LG published:2014-02-25 summary:Robust optimization is a common framework in optimization under uncertaintywhen the problem parameters are not known, but it is rather known that theparameters belong to some given uncertainty set. In the robust optimizationframework the problem solved is a min-max problem where a solution is judgedaccording to its performance on the worst possible realization of theparameters. In many cases, a straightforward solution of the robustoptimization problem of a certain type requires solving an optimization problemof a more complicated type, and in some cases even NP-hard. For example,solving a robust conic quadratic program, such as those arising in robust SVM,ellipsoidal uncertainty leads in general to a semidefinite program. In thispaper we develop a method for approximately solving a robust optimizationproblem using tools from online convex optimization, where in every stage astandard (non-robust) optimization program is solved. Our algorithms find anapproximate robust solution using a number of calls to an oracle that solvesthe original (non-robust) problem that is inversely proportional to the squareof the target accuracy.
arxiv-6300-262 | Precision Enhancement of 3D Surfaces from Multiple Compressed Depth Maps | http://arxiv.org/abs/1405.2062 | author:Pengfei Wan, Gene Cheung, Philip A. Chou, Dinei Florencio, Cha Zhang, Oscar C. Au category:cs.CV published:2014-02-25 summary:In texture-plus-depth representation of a 3D scene, depth maps from differentcamera viewpoints are typically lossily compressed via the classical transformcoding / coefficient quantization paradigm. In this paper we propose to reducedistortion of the decoded depth maps due to quantization. The key observationis that depth maps from different viewpoints constitute multiple descriptions(MD) of the same 3D scene. Considering the MD jointly, we perform a POCS-likeiterative procedure to project a reconstructed signal from one depth map to theother and back, so that the converged depth maps have higher precision than theoriginal quantized versions.
arxiv-6300-263 | Improving Collaborative Filtering based Recommenders using Topic Modelling | http://arxiv.org/abs/1402.6238 | author:Jobin Wilson, Santanu Chaudhury, Brejesh Lall, Prateek Kapadia category:cs.IR cs.CL cs.LG published:2014-02-25 summary:Standard Collaborative Filtering (CF) algorithms make use of interactionsbetween users and items in the form of implicit or explicit ratings alone forgenerating recommendations. Similarity among users or items is calculatedpurely based on rating overlap in this case,without considering explicitproperties of users or items involved, limiting their applicability in domainswith very sparse rating spaces. In many domains such as movies, news orelectronic commerce recommenders, considerable contextual data in text formdescribing item properties is available along with the rating data, which couldbe utilized to improve recommendation quality.In this paper, we propose a novelapproach to improve standard CF based recommenders by utilizing latentDirichlet allocation (LDA) to learn latent properties of items, expressed interms of topic proportions, derived from their textual description. We inferuser's topic preferences or persona in the same latent space,based on herhistorical ratings. While computing similarity between users, we make use of acombined similarity measure involving rating overlap as well as similarity inthe latent topic space. This approach alleviates sparsity problem as it allowscalculation of similarity between users even if they have not rated any itemsin common. Our experiments on multiple public datasets indicate that theproposed hybrid approach significantly outperforms standard user Based and itemBased CF recommenders in terms of classification accuracy metrics such asprecision, recall and f-measure.
arxiv-6300-264 | Novel Deviation Bounds for Mixture of Independent Bernoulli Variables with Application to the Missing Mass | http://arxiv.org/abs/1402.6262 | author:Bahman Yari Saeed Khanloo category:stat.ML published:2014-02-25 summary:In this paper, we are concerned with obtaining distribution-freeconcentration inequalities for mixture of independent Bernoulli variables thatincorporate a notion of variance. Missing mass is the total probability massassociated to the outcomes that have not been seen in a given sample which isan important quantity that connects density estimates obtained from a sample tothe population for discrete distributions. Therefore, we are specificallymotivated to apply our method to study the concentration of missing mass -which can be expressed as a mixture of Bernoulli - in a novel way. We not only derive - for the first time - Bernstein-like large deviationbounds for the missing mass whose exponents behave almost linearly with respectto deviation size, but also sharpen McAllester and Ortiz (2003) and Berend andKontorovich (2013) for large sample sizes in the case of small deviations whichis the most interesting case in learning theory. In the meantime, our approachshows that the heterogeneity issue introduced in McAllester and Ortiz (2003) isresolvable in the case of missing mass in the sense that one can use standardinequalities but it may not lead to strong results. Thus, we postulate that ourresults are general and can be applied to provide potentially sharpBernstein-like bounds under some constraints.
arxiv-6300-265 | Algorithms for multi-armed bandit problems | http://arxiv.org/abs/1402.6028 | author:Volodymyr Kuleshov, Doina Precup category:cs.AI cs.LG published:2014-02-25 summary:Although many algorithms for the multi-armed bandit problem arewell-understood theoretically, empirical confirmation of their effectiveness isgenerally scarce. This paper presents a thorough empirical study of the mostpopular multi-armed bandit algorithms. Three important observations can be madefrom our results. Firstly, simple heuristics such as epsilon-greedy andBoltzmann exploration outperform theoretically sound algorithms on mostsettings by a significant margin. Secondly, the performance of most algorithmsvaries dramatically with the parameters of the bandit problem. Our studyidentifies for each algorithm the settings where it performs well, and thesettings where it performs poorly. Thirdly, the algorithms' performancerelative each to other is affected only by the number of bandit arms and thevariance of the rewards. This finding may guide the design of subsequentempirical evaluations. In the second part of the paper, we turn our attentionto an important area of application of bandit algorithms: clinical trials.Although the design of clinical trials has been one of the principal practicalproblems motivating research on multi-armed bandits, bandit algorithms havenever been evaluated as potential treatment allocation strategies. Using datafrom a real study, we simulate the outcome that a 2001-2002 clinical trialwould have had if bandit algorithms had been used to allocate patients totreatments. We find that an adaptive trial would have successfully treated atleast 50% more patients, while significantly reducing the number of adverseeffects and increasing patient retention. At the end of the trial, the besttreatment could have still been identified with a high level of statisticalconfidence. Our findings demonstrate that bandit algorithms are attractivealternatives to current adaptive treatment allocation strategies.
arxiv-6300-266 | Sample Complexity Bounds on Differentially Private Learning via Communication Complexity | http://arxiv.org/abs/1402.6278 | author:Vitaly Feldman, David Xiao category:cs.DS cs.CC cs.LG published:2014-02-25 summary:In this work we analyze the sample complexity of classification bydifferentially private algorithms. Differential privacy is a strong andwell-studied notion of privacy introduced by Dwork et al. (2006) that ensuresthat the output of an algorithm leaks little information about the data pointprovided by any of the participating individuals. Sample complexity of privatePAC and agnostic learning was studied in a number of prior works starting with(Kasiviswanathan et al., 2008) but a number of basic questions still remainopen, most notably whether learning with privacy requires more samples thanlearning without privacy. We show that the sample complexity of learning with (pure) differentialprivacy can be arbitrarily higher than the sample complexity of learningwithout the privacy constraint or the sample complexity of learning withapproximate differential privacy. Our second contribution and the main tool isan equivalence between the sample complexity of (pure) differentially privatelearning of a concept class $C$ (or $SCDP(C)$) and the randomized one-waycommunication complexity of the evaluation problem for concepts from $C$. Usingthis equivalence we prove the following bounds: 1. $SCDP(C) = \Omega(LDim(C))$, where $LDim(C)$ is the Littlestone's (1987)dimension characterizing the number of mistakes in the online-mistake-boundlearning model. Known bounds on $LDim(C)$ then imply that $SCDP(C)$ can be muchhigher than the VC-dimension of $C$. 2. For any $t$, there exists a class $C$ such that $LDim(C)=2$ but $SCDP(C)\geq t$. 3. For any $t$, there exists a class $C$ such that the sample complexity of(pure) $\alpha$-differentially private PAC learning is $\Omega(t/\alpha)$ butthe sample complexity of the relaxed $(\alpha,\beta)$-differentially privatePAC learning is $O(\log(1/\beta)/\alpha)$. This resolves an open problem ofBeimel et al. (2013b).
arxiv-6300-267 | Inductive Logic Boosting | http://arxiv.org/abs/1402.6077 | author:Wang-Zhou Dai, Zhi-Hua Zhou category:cs.LG cs.AI published:2014-02-25 summary:Recent years have seen a surge of interest in Probabilistic Logic Programming(PLP) and Statistical Relational Learning (SRL) models that combine logic withprobabilities. Structure learning of these systems is an intersection area ofInductive Logic Programming (ILP) and statistical learning (SL). However, ILPcannot deal with probabilities, SL cannot model relational hypothesis. Thebiggest challenge of integrating these two machine learning frameworks is howto estimate the probability of a logic clause only from the observation ofgrounded logic atoms. Many current methods models a joint probability byrepresenting clause as graphical model and literals as vertices in it. Thismodel is still too complicate and only can be approximate by pseudo-likelihood.We propose Inductive Logic Boosting framework to transform the relationaldataset into a feature-based dataset, induces logic rules by boosting ProblogRule Trees and relaxes the independence constraint of pseudo-likelihood.Experimental evaluation on benchmark datasets demonstrates that the AUC-PR andAUC-ROC value of ILP learned rules are higher than current state-of-the-art SRLmethods.
arxiv-6300-268 | Bayesian Sample Size Determination of Vibration Signals in Machine Learning Approach to Fault Diagnosis of Roller Bearings | http://arxiv.org/abs/1402.6133 | author:Siddhant Sahu, V. Sugumaran category:stat.ML cs.LG published:2014-02-25 summary:Sample size determination for a data set is an important statistical processfor analyzing the data to an optimum level of accuracy and using minimumcomputational work. The applications of this process are credible in everydomain which deals with large data sets and high computational work. This studyuses Bayesian analysis for determination of minimum sample size of vibrationsignals to be considered for fault diagnosis of a bearing using pre-definedparameters such as the inverse standard probability and the acceptable marginof error. Thus an analytical formula for sample size determination isintroduced. The fault diagnosis of the bearing is done using a machine learningapproach using an entropy-based J48 algorithm. The following method will helpresearchers involved in fault diagnosis to determine minimum sample size ofdata for analysis for a good statistical stability and precision.
arxiv-6300-269 | Avoiding pathologies in very deep networks | http://arxiv.org/abs/1402.5836 | author:David Duvenaud, Oren Rippel, Ryan P. Adams, Zoubin Ghahramani category:stat.ML cs.LG published:2014-02-24 summary:Choosing appropriate architectures and regularization strategies for deepnetworks is crucial to good predictive performance. To shed light on thisproblem, we analyze the analogous problem of constructing useful priors oncompositions of functions. Specifically, we study the deep Gaussian process, atype of infinitely-wide, deep neural network. We show that in standardarchitectures, the representational capacity of the network tends to capturefewer degrees of freedom as the number of layers increases, retaining only asingle degree of freedom in the limit. We propose an alternate networkarchitecture which does not suffer from this pathology. We also examine deepcovariance functions, obtained by composing infinitely many feature transforms.Lastly, we characterize the class of models obtained by performing dropout onGaussian processes.
arxiv-6300-270 | Predictive Interval Models for Non-parametric Regression | http://arxiv.org/abs/1402.5874 | author:Mohammad Ghasemi Hamed, Mathieu Serrurier, Nicolas Durand category:cs.LG stat.ML published:2014-02-24 summary:Having a regression model, we are interested in finding two-sided intervalsthat are guaranteed to contain at least a desired proportion of the conditionaldistribution of the response variable given a specific combination ofpredictors. We name such intervals predictive intervals. This work presents anew method to find two-sided predictive intervals for non-parametric leastsquares regression without the homoscedasticity assumption. Our predictiveintervals are built by using tolerance intervals on prediction errors in thequery point's neighborhood. We proposed a predictive interval model test and wealso used it as a constraint in our hyper-parameter tuning algorithm. Thisgives an algorithm that finds the smallest reliable predictive intervals for agiven dataset. We also introduce a measure for comparing different intervalprediction methods yielding intervals having different size and coverage. Theseexperiments show that our methods are more reliable, effective and precise thanother interval prediction methods.
arxiv-6300-271 | Incremental Learning of Event Definitions with Inductive Logic Programming | http://arxiv.org/abs/1402.5988 | author:Nikos Katzouris, Alexander Artikis, George Paliouras category:cs.LG cs.AI published:2014-02-24 summary:Event recognition systems rely on properly engineered knowledge bases ofevent definitions to infer occurrences of events in time. The manualdevelopment of such knowledge is a tedious and error-prone task, thusevent-based applications may benefit from automated knowledge constructiontechniques, such as Inductive Logic Programming (ILP), which combines machinelearning with the declarative and formal semantics of First-Order Logic.However, learning temporal logical formalisms, which are typically utilized bylogic-based Event Recognition systems is a challenging task, which most ILPsystems cannot fully undertake. In addition, event-based data is usuallymassive and collected at different times and under various circumstances.Ideally, systems that learn from temporal data should be able to operate in anincremental mode, that is, revise prior constructed knowledge in the face ofnew evidence. Most ILP systems are batch learners, in the sense that in orderto account for new evidence they have no alternative but to forget pastknowledge and learn from scratch. Given the increased inherent complexity ofILP and the volumes of real-life temporal data, this results to algorithms thatscale poorly. In this work we present an incremental method for learning andrevising event-based knowledge, in the form of Event Calculus programs. Theproposed algorithm relies on abductive-inductive learning and comprises ascalable clause refinement methodology, based on a compressive summarization ofclause coverage in a stream of examples. We present an empirical evaluation ofour approach on real and synthetic data from activity recognition and citytransport applications.
arxiv-6300-272 | Open science in machine learning | http://arxiv.org/abs/1402.6013 | author:Joaquin Vanschoren, Mikio L. Braun, Cheng Soon Ong category:cs.LG cs.DL published:2014-02-24 summary:We present OpenML and mldata, open science platforms that provides easyaccess to machine learning data, software and results to encourage furtherstudy and application. They go beyond the more traditional repositories fordata sets and software packages in that they allow researchers to also easilyshare the results they obtained in experiments and to compare their solutionswith those of others.
arxiv-6300-273 | A Novel Scheme for Intelligent Recognition of Pornographic Images | http://arxiv.org/abs/1402.5792 | author:Seyed Mostafa Kia, Hossein Rahmani, Reza Mortezaei, Mohsen Ebrahimi Moghaddam, Amer Namazi category:cs.CV published:2014-02-24 summary:Harmful contents are rising in internet day by day and this motivates theessence of more research in fast and reliable obscene and immoral materialfiltering. Pornographic image recognition is an important component in eachfiltering system. In this paper, a new approach for detecting pornographicimages is introduced. In this approach, two new features are suggested. Thesetwo features in combination with other simple traditional features providedecent difference between porn and non-porn images. In addition, we appliedfuzzy integral based information fusion to combine MLP (Multi-Layer Perceptron)and NF (Neuro-Fuzzy) outputs. To test the proposed method, performance ofsystem was evaluated over 18354 download images from internet. The attainedprecision was 93% in TP and 8% in FP on training dataset, and 87% and 5.5% ontest dataset. Achieved results verify the performance of proposed system versusother related works.
arxiv-6300-274 | Information-Theoretic Bounds for Adaptive Sparse Recovery | http://arxiv.org/abs/1402.5731 | author:Cem Aksoylar, Venkatesh Saligrama category:cs.IT cs.LG math.IT math.ST stat.TH published:2014-02-24 summary:We derive an information-theoretic lower bound for sample complexity insparse recovery problems where inputs can be chosen sequentially andadaptively. This lower bound is in terms of a simple mutual informationexpression and unifies many different linear and nonlinear observation models.Using this formula we derive bounds for adaptive compressive sensing (CS),group testing and 1-bit CS problems. We show that adaptivity cannot decreasesample complexity in group testing, 1-bit CS and CS with linear sparsity. Incontrast, we show there might be mild performance gains for CS in the sublinearregime. Our unified analysis also allows characterization of gains due toadaptivity from a wider perspective on sparse problems.
arxiv-6300-275 | Near Optimal Bayesian Active Learning for Decision Making | http://arxiv.org/abs/1402.5886 | author:Shervin Javdani, Yuxin Chen, Amin Karbasi, Andreas Krause, J. Andrew Bagnell, Siddhartha Srinivasa category:cs.LG cs.AI published:2014-02-24 summary:How should we gather information to make effective decisions? We addressBayesian active learning and experimental design problems, where wesequentially select tests to reduce uncertainty about a set of hypotheses.Instead of minimizing uncertainty per se, we consider a set of overlappingdecision regions of these hypotheses. Our goal is to drive uncertainty into asingle decision region as quickly as possible. We identify necessary and sufficient conditions for correctly identifying adecision region that contains all hypotheses consistent with observations. Wedevelop a novel Hyperedge Cutting (HEC) algorithm for this problem, and provethat is competitive with the intractable optimal policy. Our efficientimplementation of the algorithm relies on computing subsets of the completehomogeneous symmetric polynomials. Finally, we demonstrate its effectiveness ontwo practical applications: approximate comparison-based learning and activelocalization using a robot manipulator.
arxiv-6300-276 | A hybrid swarm-based algorithm for single-objective optimization problems involving high-cost analyses | http://arxiv.org/abs/1402.5830 | author:Enrico Ampellio, Luca Vassio category:math.OC cs.AI cs.DC cs.NE published:2014-02-24 summary:In many technical fields, single-objective optimization procedures incontinuous domains involve expensive numerical simulations. In this context, animprovement of the Artificial Bee Colony (ABC) algorithm, called the Artificialsuper-Bee enhanced Colony (AsBeC), is presented. AsBeC is designed to providefast convergence speed, high solution accuracy and robust performance over awide range of problems. It implements enhancements of the ABC structure andhybridizations with interpolation strategies. The latter are inspired by thequadratic trust region approach for local investigation and by an efficientglobal optimizer for separable problems. Each modification and their combinedeffects are studied with appropriate metrics on a numerical benchmark, which isalso used for comparing AsBeC with some effective ABC variants and otherderivative-free algorithms. In addition, the presented algorithm is validatedon two recent benchmarks adopted for competitions in international conferences.Results show remarkable competitiveness and robustness for AsBeC.
arxiv-6300-277 | Manifold Gaussian Processes for Regression | http://arxiv.org/abs/1402.5876 | author:Roberto Calandra, Jan Peters, Carl Edward Rasmussen, Marc Peter Deisenroth category:stat.ML cs.LG published:2014-02-24 summary:Off-the-shelf Gaussian Process (GP) covariance functions encode smoothnessassumptions on the structure of the function to be modeled. To model complexand non-differentiable functions, these smoothness assumptions are often toorestrictive. One way to alleviate this limitation is to find a differentrepresentation of the data by introducing a feature space. This feature spaceis often learned in an unsupervised way, which might lead to datarepresentations that are not useful for the overall regression task. In thispaper, we propose Manifold Gaussian Processes, a novel supervised method thatjointly learns a transformation of the data into a feature space and a GPregression from the feature space to observed space. The Manifold GP is a fullGP and allows to learn data representations, which are useful for the overallregression task. As a proof-of-concept, we evaluate our approach on complexnon-smooth functions where standard GPs perform poorly, such as step functionsand robotics tasks with contacts.
arxiv-6300-278 | A Novel Face Recognition Method using Nearest Line Projection | http://arxiv.org/abs/1402.5859 | author:Huanguo Zhang, Sha Lv, Wei Li, Xun Qu category:cs.CV published:2014-02-24 summary:Face recognition is a popular application of pat- tern recognition methods,and it faces challenging problems including illumination, expression, and pose.The most popular way is to learn the subspaces of the face images so that itcould be project to another discriminant space where images of differentpersons can be separated. In this paper, a nearest line projection algorithm isdeveloped to represent the face images for face recognition. Instead ofprojecting an image to its nearest image, we try to project it to its nearestline spanned by two different face images. The subspaces are learned so thateach face image to its nearest line is minimized. We evaluated the proposedalgorithm on some benchmark face image database, and also compared it to someother image projection algorithms. The experiment results showed that theproposed algorithm outperforms other ones.
arxiv-6300-279 | DCT-like Transform for Image and Video Compression Requires 10 Additions Only | http://arxiv.org/abs/1402.5979 | author:R. J. Cintra, F. M. Bayer, V. A. Coutinho, S. Kulasekera, A. Madanayake category:cs.MM cs.CV stat.ME published:2014-02-24 summary:A multiplierless pruned approximate 8-point discrete cosine transform (DCT)requiring only 10 additions is introduced. The proposed algorithm was assessedin image and video compression, showing competitive performance withstate-of-the-art methods. Digital implementation in 45 nm CMOS technology up toplace-and-route level indicates clock speed of 255 MHz at a 1.1 V supply. The8x8 block rate is 31.875 MHz.The DCT approximation was embedded into HEVCreference software; resulting video frames, at up to 327 Hz for 8-bit RGB HEVC,presented negligible image degradation.
arxiv-6300-280 | Automatic Estimation of Live Coffee Leaf Infection based on Image Processing Techniques | http://arxiv.org/abs/1402.5805 | author:Eric Hitimana, Oubong Gwun category:cs.CV published:2014-02-24 summary:Image segmentation is the most challenging issue in computer visionapplications. And most difficulties for crops management in agriculture are thelack of appropriate methods for detecting the leaf damage for pests treatment.In this paper we proposed an automatic method for leaf damage detection andseverity estimation of coffee leaf by avoiding defoliation. After enhancing thecontrast of the original image using LUT based gamma correction, the image isprocessed to remove the background, and the output leaf is clustered usingFuzzy c-means segmentation in V channel of YUV color space to maximize all leafdamage detection, and finally, the severity of leaf is estimated in terms ofratio for leaf pixel distribution between the normal and the detected leafdamage. The results in each proposed method was compared to the currentresearches and the accuracy is obvious either in the background removal ordamage detection.
arxiv-6300-281 | Tripartite Graph Clustering for Dynamic Sentiment Analysis on Social Media | http://arxiv.org/abs/1402.6010 | author:Linhong Zhu, Aram Galstyan, James Cheng, Kristina Lerman category:cs.SI cs.CL cs.IR published:2014-02-24 summary:The growing popularity of social media (e.g, Twitter) allows users to easilyshare information with each other and influence others by expressing their ownsentiments on various subjects. In this work, we propose an unsupervised\emph{tri-clustering} framework, which analyzes both user-level and tweet-levelsentiments through co-clustering of a tripartite graph. A compelling feature ofthe proposed framework is that the quality of sentiment clustering of tweets,users, and features can be mutually improved by joint clustering. We furtherinvestigate the evolution of user-level sentiments and latent feature vectorsin an online framework and devise an efficient online algorithm to sequentiallyupdate the clustering of tweets, users and features with newly arrived data.The online framework not only provides better quality of both dynamicuser-level and tweet-level sentiment analysis, but also improves thecomputational and storage efficiency. We verified the effectiveness andefficiency of the proposed approaches on the November 2012 California ballotTwitter data.
arxiv-6300-282 | Sparse phase retrieval via group-sparse optimization | http://arxiv.org/abs/1402.5803 | author:Fabien Lauer, Henrik Ohlsson category:cs.IT cs.LG math.IT published:2014-02-24 summary:This paper deals with sparse phase retrieval, i.e., the problem of estimatinga vector from quadratic measurements under the assumption that few componentsare nonzero. In particular, we consider the problem of finding the sparsestvector consistent with the measurements and reformulate it as a group-sparseoptimization problem with linear constraints. Then, we analyze the convexrelaxation of the latter based on the minimization of a block l1-norm and showvarious exact recovery and stability results in the real and complex cases.Invariance to circular shifts and reflections are also discussed for realvectors measured via complex matrices.
arxiv-6300-283 | Representation as a Service | http://arxiv.org/abs/1404.4108 | author:Ouais Alsharif, Philip Bachman, Joelle Pineau category:cs.LG published:2014-02-24 summary:Consider a Machine Learning Service Provider (MLSP) designed to rapidlycreate highly accurate learners for a never-ending stream of new tasks. Thechallenge is to produce task-specific learners that can be trained from fewlabeled samples, even if tasks are not uniquely identified, and the number oftasks and input dimensionality are large. In this paper, we argue that the MLSPshould exploit knowledge from previous tasks to build a good representation ofthe environment it is in, and more precisely, that useful representations forsuch a service are ones that minimize generalization error for a new hypothesistrained on a new task. We formalize this intuition with a novel method thatminimizes an empirical proxy of the intra-task small-sample generalizationerror. We present several empirical results showing state-of-the artperformance on single-task transfer, multitask learning, and the full lifelonglearning problem.
arxiv-6300-284 | A Testbed for Cross-Dataset Analysis | http://arxiv.org/abs/1402.5923 | author:Tatiana Tommasi, Tinne Tuytelaars, Barbara Caputo category:cs.CV published:2014-02-24 summary:Since its beginning visual recognition research has tried to capture the hugevariability of the visual world in several image collections. The number ofavailable datasets is still progressively growing together with the amount ofsamples per object category. However, this trend does not correspond directlyto an increasing in the generalization capabilities of the developedrecognition systems. Each collection tends to have its specific characteristicsand to cover just some aspects of the visual world: these biases often narrowthe effect of the methods defined and tested separately over each image set.Our work makes a first step towards the analysis of the dataset bias problem ona large scale. We organize twelve existing databases in a unique corpus and wepresent the visual community with a useful feature repository for futureresearch.
arxiv-6300-285 | No more meta-parameter tuning in unsupervised sparse feature learning | http://arxiv.org/abs/1402.5766 | author:Adriana Romero, Petia Radeva, Carlo Gatta category:cs.LG cs.CV published:2014-02-24 summary:We propose a meta-parameter free, off-the-shelf, simple and fast unsupervisedfeature learning algorithm, which exploits a new way of optimizing forsparsity. Experiments on STL-10 show that the method presents state-of-the-artperformance and provides discriminative features that generalize well.
arxiv-6300-286 | On Learning from Label Proportions | http://arxiv.org/abs/1402.5902 | author:Felix X. Yu, Krzysztof Choromanski, Sanjiv Kumar, Tony Jebara, Shih-Fu Chang category:stat.ML cs.LG published:2014-02-24 summary:Learning from Label Proportions (LLP) is a learning setting, where thetraining data is provided in groups, or "bags", and only the proportion of eachclass in each bag is known. The task is to learn a model to predict the classlabels of the individual instances. LLP has broad applications in politicalscience, marketing, healthcare, and computer vision. This work answers thefundamental question, when and why LLP is possible, by introducing a generalframework, Empirical Proportion Risk Minimization (EPRM). EPRM learns aninstance label classifier to match the given label proportions on the trainingdata. Our result is based on a two-step analysis. First, we provide a VC boundon the generalization error of the bag proportions. We show that the bag samplecomplexity is only mildly sensitive to the bag size. Second, we show that undersome mild assumptions, good bag proportion prediction guarantees good instancelabel prediction. The results together provide a formal guarantee that theindividual labels can indeed be learned in the LLP setting. We discussapplications of the analysis, including justification of LLP algorithms,learning with population proportions, and a paradigm for learning algorithmswith privacy guarantees. We also demonstrate the feasibility of LLP based on acase study in real-world setting: predicting income based on census data.
arxiv-6300-287 | Bandits with concave rewards and convex knapsacks | http://arxiv.org/abs/1402.5758 | author:Shipra Agrawal, Nikhil R. Devanur category:cs.LG published:2014-02-24 summary:In this paper, we consider a very general model for exploration-exploitationtradeoff which allows arbitrary concave rewards and convex constraints on thedecisions across time, in addition to the customary limitation on the timehorizon. This model subsumes the classic multi-armed bandit (MAB) model, andthe Bandits with Knapsacks (BwK) model of Badanidiyuru et al.[2013]. We alsoconsider an extension of this model to allow linear contexts, similar to thelinear contextual extension of the MAB model. We demonstrate that a natural andsimple extension of the UCB family of algorithms for MAB provides a polynomialtime algorithm that has near-optimal regret guarantees for this substantiallymore general model, and matches the bounds provided by Badanidiyuru etal.[2013] for the special case of BwK, which is quite surprising. We alsoprovide computationally more efficient algorithms by establishing interestingconnections between this problem and other well studied problems/algorithmssuch as the Blackwell approachability problem, online convex optimization, andthe Frank-Wolfe technique for convex optimization. We give examples of severalconcrete applications, where this more general model of bandits allows forricher and/or more efficient formulations of the problem.
arxiv-6300-288 | Machine Learning Methods in the Computational Biology of Cancer | http://arxiv.org/abs/1402.5728 | author:Mathukumalli Vidyasagar category:q-bio.QM cs.LG stat.ML 62P10 published:2014-02-24 summary:The objectives of this "perspective" paper are to review some recent advancesin sparse feature selection for regression and classification, as well ascompressed sensing, and to discuss how these might be used to develop tools toadvance personalized cancer therapy. As an illustration of the possibilities, anew algorithm for sparse regression is presented, and is applied to predict thetime to tumor recurrence in ovarian cancer. A new algorithm for sparse featureselection in classification problems is presented, and its validation inendometrial cancer is briefly discussed. Some open problems are also presented.
arxiv-6300-289 | The Cerebellum: New Computational Model that Reveals its Primary Function to Calculate Multibody Dynamics Conform to Lagrange-Euler Formulation | http://arxiv.org/abs/1402.5708 | author:Lavdim Kurtaj, Ilir Limani, Vjosa Shatri, Avni Skeja category:cs.NE cs.CE cs.RO q-bio.NC published:2014-02-24 summary:Cerebellum is part of the brain that occupies only 10% of the brain volume,but it contains about 80% of total number of brain neurons. New cerebellarfunction model is developed that sets cerebellar circuits in context ofmultibody dynamics model computations, as important step in controlling balanceand movement coordination, functions performed by two oldest parts of thecerebellum. Model gives new functional interpretation for granule cells-Golgicell circuit, including distinct function for upper and lower Golgi celldendritc trees, and resolves issue of sharing Granule cells between Purkinjecells. Sets new function for basket cells, and for stellate cells according toposition in molecular layer. New model enables easily and direct integration ofsensory information from vestibular system and cutaneous mechanoreceptors, forbalance, movement and interaction with environments. Model gives explanation ofPurkinje cells convergence on deep-cerebellar nuclei.
arxiv-6300-290 | Exemplar-based Linear Discriminant Analysis for Robust Object Tracking | http://arxiv.org/abs/1402.5697 | author:Changxin Gao, Feifei Chen, Jin-Gang Yu, Rui Huang, Nong Sang category:cs.CV published:2014-02-24 summary:Tracking-by-detection has become an attractive tracking technique, whichtreats tracking as a category detection problem. However, the task in trackingis to search for a specific object, rather than an object category as indetection. In this paper, we propose a novel tracking framework based onexemplar detector rather than category detector. The proposed tracker is anensemble of exemplar-based linear discriminant analysis (ELDA) detectors. Eachdetector is quite specific and discriminative, because it is trained by asingle object instance and massive negatives. To improve its adaptivity, weupdate both object and background models. Experimental results on severalchallenging video sequences demonstrate the effectiveness and robustness of ourtracking algorithm.
arxiv-6300-291 | Variational Particle Approximations | http://arxiv.org/abs/1402.5715 | author:Ardavan Saeedi, Tejas D Kulkarni, Vikash Mansinghka, Samuel Gershman category:stat.ML cs.LG published:2014-02-24 summary:Approximate inference in high-dimensional, discrete probabilistic models is acentral problem in computational statistics and machine learning. This paperdescribes discrete particle variational inference (DPVI), a new approach thatcombines key strengths of Monte Carlo, variational and search-based techniques.DPVI is based on a novel family of particle-based variational approximationsthat can be fit using simple, fast, deterministic search techniques. Like MonteCarlo, DPVI can handle multiple modes, and yields exact results in awell-defined limit. Like unstructured mean-field, DPVI is based on optimizing alower bound on the partition function; when this quantity is not of intrinsicinterest, it facilitates convergence assessment and debugging. Like both MonteCarlo and combinatorial search, DPVI can take advantage of factorization,sequential structure, and custom search operators. This paper defines DPVIparticle-based approximation family and partition function lower bounds, alongwith the sequential DPVI and local DPVI algorithm templates for optimizingthem. DPVI is illustrated and evaluated via experiments on lattice MarkovRandom Fields, nonparametric Bayesian mixtures and block-models, and parametricas well as non-parametric hidden Markov models. Results include applications toreal-world spike-sorting and relational modeling problems, and show that DPVIcan offer appealing time/accuracy trade-offs as compared to multiplealternatives.
arxiv-6300-292 | Exact Post Model Selection Inference for Marginal Screening | http://arxiv.org/abs/1402.5596 | author:Jason D Lee, Jonathan E Taylor category:stat.ME cs.LG math.ST stat.ML stat.TH published:2014-02-23 summary:We develop a framework for post model selection inference, via marginalscreening, in linear regression. At the core of this framework is a result thatcharacterizes the exact distribution of linear functions of the response $y$,conditional on the model being selected (``condition on selection" framework).This allows us to construct valid confidence intervals and hypothesis tests forregression coefficients that account for the selection procedure. In contrastto recent work in high-dimensional statistics, our results are exact(non-asymptotic) and require no eigenvalue-like assumptions on the designmatrix $X$. Furthermore, the computational cost of marginal regression,constructing confidence intervals and hypothesis testing is negligible comparedto the cost of linear regression, thus making our methods particularly suitablefor extremely large datasets. Although we focus on marginal screening toillustrate the applicability of the condition on selection framework, thisframework is much more broadly applicable. We show how to apply the proposedframework to several other selection procedures including orthogonal matchingpursuit, non-negative least squares, and marginal screening+Lasso.
arxiv-6300-293 | To go deep or wide in learning? | http://arxiv.org/abs/1402.5634 | author:Gaurav Pandey, Ambedkar Dukkipati category:cs.LG published:2014-02-23 summary:To achieve acceptable performance for AI tasks, one can either usesophisticated feature extraction methods as the first layer in a two-layeredsupervised learning model, or learn the features directly using a deep(multi-layered) model. While the first approach is very problem-specific, thesecond approach has computational overheads in learning multiple layers andfine-tuning of the model. In this paper, we propose an approach called widelearning based on arc-cosine kernels, that learns a single layer of infinitewidth. We propose exact and inexact learning strategies for wide learning andshow that wide learning with single layer outperforms single layer as well asdeep architectures of finite width for some benchmark datasets.
arxiv-6300-294 | Localization of License Plate Using Morphological Operations | http://arxiv.org/abs/1402.5623 | author:V. Karthikeyan, V. J. Vijayalakshmi category:cs.CV published:2014-02-23 summary:It is believed that there are currently millions of vehicles on the roadsworldwide. The over speed of vehicles,theft of vehicles, disobeying trafficrules in public, an unauthorized person entering the restricted area are keepon increasing. In order restrict against these criminal activities, we need anautomatic public security system. Each vehicle has their own VehicleIdentification Number (VIN) as their primary identifier. The VIN is actually aLicense Number which states a legal license to participate in the publictraffic. The proposed paper is to identify the vehicle with the help ofvehicles License Plate (LP).LPRS is one the most important part of theIntelligent Transportation System (ITS) to locate the LP. In this paper certainexisting algorithm drawbacks are overcome by the proposed morphologicaloperations for LPRS. Morphological operation is chosen due to its higherefficiency, noise filter capacity, accuracy, exact localization of LP andspeed.
arxiv-6300-295 | Dynamic Rate and Channel Selection in Cognitive Radio Systems | http://arxiv.org/abs/1402.5666 | author:Richard Combes, Alexandre Proutiere category:cs.IT cs.LG math.IT published:2014-02-23 summary:In this paper, we investigate dynamic channel and rate selection in cognitiveradio systems which exploit a large number of channels free from primary users.In such systems, transmitters may rapidly change the selected (channel, rate)pair to opportunistically learn and track the pair offering the highestthroughput. We formulate the problem of sequential channel and rate selectionas an online optimization problem, and show its equivalence to a {\itstructured} Multi-Armed Bandit problem. The structure stems from inherentproperties of the achieved throughput as a function of the selected channel andrate. We derive fundamental performance limits satisfied by {\it any} channeland rate adaptation algorithm, and propose algorithms that achieve (orapproach) these limits. In turn, the proposed algorithms optimally exploit theinherent structure of the throughput. We illustrate the efficiency of ouralgorithms using both test-bed and simulation experiments, in both stationaryand non-stationary radio environments. In stationary environments, the packetsuccessful transmission probabilities at the various channel and rate pairs donot evolve over time, whereas in non-stationary environments, they may evolve.In practical scenarios, the proposed algorithms are able to track the bestchannel and rate quite accurately without the need of any explicit measurementand feedback of the quality of the various channels.
arxiv-6300-296 | A Novel Histogram Based Robust Image Registration Technique | http://arxiv.org/abs/1402.5619 | author:V. Karthikeyan category:cs.CV published:2014-02-23 summary:In this paper, a method for Automatic Image Registration (AIR) throughhistogram is proposed. Automatic image registration is one of the crucial stepsin the analysis of remotely sensed data. A new acquired image must betransformed, using image registration techniques, to match the orientation andscale of previous related images. This new approach combines severalsegmentations of the pair of images to be registered. A relaxation parameter onthe histogram modes delineation is introduced. It is followed bycharacterization of the extracted objects through the objects area, axis ratio,and perimeter and fractal dimension. The matched objects are used for rotationand translation estimation. It allows for the registration of pairs of imageswith differences in rotation and translation. This method contributes tosubpixel accuracy.
arxiv-6300-297 | Path Thresholding: Asymptotically Tuning-Free High-Dimensional Sparse Regression | http://arxiv.org/abs/1402.5584 | author:Divyanshu Vats, Richard G. Baraniuk category:math.ST cs.IT math.IT stat.ML stat.TH published:2014-02-23 summary:In this paper, we address the challenging problem of selecting tuningparameters for high-dimensional sparse regression. We propose a simple andcomputationally efficient method, called path thresholding (PaTh), thattransforms any tuning parameter-dependent sparse regression algorithm into anasymptotically tuning-free sparse regression algorithm. More specifically, weprove that, as the problem size becomes large (in the number of variables andin the number of observations), PaTh performs accurate sparse regression, underappropriate conditions, without specifying a tuning parameter. Infinite-dimensional settings, we demonstrate that PaTh can alleviate thecomputational burden of model selection algorithms by significantly reducingthe search space of tuning parameters.
arxiv-6300-298 | Semi-Supervised Nonlinear Distance Metric Learning via Forests of Max-Margin Cluster Hierarchies | http://arxiv.org/abs/1402.5565 | author:David M. Johnson, Caiming Xiong, Jason J. Corso category:stat.ML cs.IR cs.LG I.5.3; H.2.8 published:2014-02-23 summary:Metric learning is a key problem for many data mining and machine learningapplications, and has long been dominated by Mahalanobis methods. Recentadvances in nonlinear metric learning have demonstrated the potential power ofnon-Mahalanobis distance functions, particularly tree-based functions. Wepropose a novel nonlinear metric learning method that uses an iterative,hierarchical variant of semi-supervised max-margin clustering to construct aforest of cluster hierarchies, where each individual hierarchy can beinterpreted as a weak metric over the data. By introducing randomness duringhierarchy training and combining the output of many of the resultingsemi-random weak hierarchy metrics, we can obtain a powerful and robustnonlinear metric model. This method has two primary contributions: first, it issemi-supervised, incorporating information from both constrained andunconstrained points. Second, we take a relaxed approach to constraintsatisfaction, allowing the method to satisfy different subsets of theconstraints at different levels of the hierarchy rather than attempting tosimultaneously satisfy all of them. This leads to a more robust learningalgorithm. We compare our method to a number of state-of-the-art benchmarks on$k$-nearest neighbor classification, large-scale image retrieval andsemi-supervised clustering problems, and find that our algorithm yields resultscomparable or superior to the state-of-the-art, and is significantly morerobust to noise.
arxiv-6300-299 | Discriminative Functional Connectivity Measures for Brain Decoding | http://arxiv.org/abs/1402.5684 | author:Orhan Firat, Mete Ozay, Ilke Oztekin, Fatos T. Yarman Vural category:cs.AI cs.CE cs.CV cs.LG published:2014-02-23 summary:We propose a statistical learning model for classifying cognitive processesbased on distributed patterns of neural activation in the brain, acquired viafunctional magnetic resonance imaging (fMRI). In the proposed learning method,local meshes are formed around each voxel. The distance between voxels in themesh is determined by using a functional neighbourhood concept. In order todefine the functional neighbourhood, the similarities between the time seriesrecorded for voxels are measured and functional connectivity matrices areconstructed. Then, the local mesh for each voxel is formed by including thefunctionally closest neighbouring voxels in the mesh. The relationship betweenthe voxels within a mesh is estimated by using a linear regression model. Theserelationship vectors, called Functional Connectivity aware Local RelationalFeatures (FC-LRF) are then used to train a statistical learning machine. Theproposed method was tested on a recognition memory experiment, including datapertaining to encoding and retrieval of words belonging to ten differentsemantic categories. Two popular classifiers, namely k-nearest neighbour (k-nn)and Support Vector Machine (SVM), are trained in order to predict the semanticcategory of the item being retrieved, based on activation patterns duringencoding. The classification performance of the Functional Mesh Learning model,which range in 62%-71% is superior to the classical multi-voxel patternanalysis (MVPA) methods, which range in 40%-48%, for ten semantic categories.
arxiv-6300-300 | Scaling Nonparametric Bayesian Inference via Subsample-Annealing | http://arxiv.org/abs/1402.5473 | author:Fritz Obermeyer, Jonathan Glidden, Eric Jonas category:stat.ML stat.CO published:2014-02-22 summary:We describe an adaptation of the simulated annealing algorithm tononparametric clustering and related probabilistic models. This new algorithmlearns nonparametric latent structure over a growing and constantly churningsubsample of training data, where the portion of data subsampled can beinterpreted as the inverse temperature beta(t) in an annealing schedule. Gibbssampling at high temperature (i.e., with a very small subsample) can morequickly explore sketches of the final latent state by (a) making longer jumpsaround latent space (as in block Gibbs) and (b) lowering energy barriers (as insimulated annealing). We prove subsample annealing speeds up mixing time N^2 ->N in a simple clustering model and exp(N) -> N in another class of models,where N is data size. Empirically subsample-annealing outperforms naive Gibbssampling in accuracy-per-wallclock time, and can scale to larger datasets anddeeper hierarchical models. We demonstrate improved inference on million-rowsubsamples of US Census data and network log data and a 307-row hospital ratingdataset, using a Pitman-Yor generalization of the Cross Categorization model.
