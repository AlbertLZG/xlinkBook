arxiv-1602-06577 | 2-Bit Random Projections, NonLinear Estimators, and Approximate Near Neighbor Search |  http://arxiv.org/abs/1602.06577  | author:Ping Li, Michael Mitzenmacher, Anshumali Shrivastava category:stat.ML cs.DS cs.LG published:2016-02-21 summary:The method of random projections has become a standard tool for machinelearning, data mining, and search with massive data at Web scale. The effectiveuse of random projections requires efficient coding schemes for quantizing(real-valued) projected data into integers. In this paper, we focus on a simple2-bit coding scheme. In particular, we develop accurate nonlinear estimators ofdata similarity based on the 2-bit strategy. This work will have importantpractical applications. For example, in the task of near neighbor search, acrucial step (often called re-ranking) is to compute or estimate datasimilarities once a set of candidate data points have been identified by hashtable techniques. This re-ranking step can take advantage of the proposedcoding scheme and estimator. As a related task, in this paper, we also study a simple uniform quantizationscheme for the purpose of building hash tables with projected data. Ouranalysis shows that typically only a small number of bits are needed. Forexample, when the target similarity level is high, 2 or 3 bits might besufficient. When the target similarity level is not so high, it is preferableto use only 1 or 2 bits. Therefore, a 2-bit scheme appears to be overall a goodchoice for the task of sublinear time approximate near neighbor search via hashtables. Combining these results, we conclude that 2-bit random projections should berecommended for approximate near neighbor search and similarity estimation.Extensive experimental results are provided.
arxiv-1602-06586 | Recovering Structured Probability Matrices |  http://arxiv.org/abs/1602.06586  | author:Qingqing Huang, Sham M. Kakade, Weihao Kong, Gregory Valiant category:cs.LG published:2016-02-21 summary:We consider the problem of accurately recovering a matrix B of size M by M ,which represents a probability distribution over M2 outcomes, given access toan observed matrix of "counts" generated by taking independent samples from thedistribution B. How can structural properties of the underlying matrix B beleveraged to yield computationally efficient and information theoreticallyoptimal reconstruction algorithms? When can accurate reconstruction beaccomplished in the sparse data regime? This basic problem lies at the core ofa number of questions that are currently being considered by differentcommunities, including building recommendation systems and collaborativefiltering in the sparse data regime, community detection in sparse randomgraphs, learning structured models such as topic models or hidden Markovmodels, and the efforts from the natural language processing community tocompute "word embeddings". Our results apply to the setting where B has a low rank structure. For thissetting, we propose an efficient algorithm that accurately recovers theunderlying M by M matrix using Theta(M) samples. This result easily translatesto Theta(M) sample algorithms for learning topic models and learning hiddenMarkov Models. These linear sample complexities are optimal, up to constantfactors, in an extremely strong sense: even testing basic properties of theunderlying matrix (such as whether it has rank 1 or 2) requires Omega(M)samples. We provide an even stronger lower bound where distinguishing whether asequence of observations were drawn from the uniform distribution over Mobservations versus being generated by an HMM with two hidden states requiresOmega(M) observations. This precludes sublinear-sample hypothesis tests forbasic properties, such as identity or uniformity, as well as sublinear sampleestimators for quantities such as the entropy rate of HMMs.
arxiv-1602-06349 | The Segmented iHMM: A Simple, Efficient Hierarchical Infinite HMM |  http://arxiv.org/abs/1602.06349  | author:Ardavan Saeedi, Matthew Hoffman, Matthew Johnson, Ryan Adams category:stat.ML published:2016-02-20 summary:We propose the segmented iHMM (siHMM), a hierarchical infinite hidden Markovmodel (iHMM) that supports a simple, efficient inference scheme. The siHMM iswell suited to segmentation problems, where the goal is to identify points atwhich a time series transitions from one relatively stable regime to a newregime. Conventional iHMMs often struggle with such problems, since they haveno mechanism for distinguishing between high- and low-level dynamics.Hierarchical HMMs (HHMMs) can do better, but they require much more complex andexpensive inference algorithms. The siHMM retains the simplicity and efficiencyof the iHMM, but outperforms it on a variety of segmentation problems,achieving performance that matches or exceeds that of a more complicated HHMM.
arxiv-1602-06359 | Text Matching as Image Recognition |  http://arxiv.org/abs/1602.06359  | author:Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Shengxian Wan, Xueqi Cheng category:cs.CL cs.AI published:2016-02-20 summary:Matching two texts is a fundamental problem in many natural languageprocessing tasks. An effective way is to extract meaningful matching patternsfrom words, phrases, and sentences to produce the matching score. Inspired bythe success of convolutional neural network in image recognition, where neuronscan capture many complicated patterns based on the extracted elementary visualpatterns such as oriented edges and corners, we propose to model text matchingas the problem of image recognition. Firstly, a matching matrix whose entriesrepresent the similarities between words is constructed and viewed as an image.Then a convolutional neural network is utilized to capture rich matchingpatterns in a layer-by-layer way. We show that by resembling the compositionalhierarchies of patterns in image recognition, our model can successfullyidentify salient signals such as n-gram and n-term matchings. Experimentalresults demonstrate its superiority against the baselines.
arxiv-1602-06431 | Burstiness Scale: a highly parsimonious model for characterizing random series of events |  http://arxiv.org/abs/1602.06431  | author:Rodrigo A S Alves, Renato Assunção, Pedro O S Vaz de Melo category:stat.ML cs.SI H.2.8; G.3 published:2016-02-20 summary:The problem to accurately and parsimoniously characterize random series ofevents (RSEs) present in the Web, such as e-mail conversations or Twitterhashtags, is not trivial. Reports found in the literature reveal two apparentconflicting visions of how RSEs should be modeled. From one side, thePoissonian processes, of which consecutive events follow each other at arelatively regular time and should not be correlated. On the other side, theself-exciting processes, which are able to generate bursts of correlated eventsand periods of inactivities. The existence of many and sometimes conflictingapproaches to model RSEs is a consequence of the unpredictability of theaggregated dynamics of our individual and routine activities, which sometimesshow simple patterns, but sometimes results in irregular rising and fallingtrends. In this paper we propose a highly parsimonious way to characterizegeneral RSEs, namely the Burstiness Scale (BuSca) model. BuSca views each RSEas a mix of two independent process: a Poissonian and a self-exciting one. Herewe describe a fast method to extract the two parameters of BuSca that,together, gives the burstyness scale, which represents how much of the RSE isdue to bursty and viral effects. We validated our method in eight diverse andlarge datasets containing real random series of events seen in Twitter, Yelp,e-mail conversations, Digg, and online forums. Results showed that, even usingonly two parameters, BuSca is able to accurately describe RSEs seen in thesediverse systems, what can leverage many applications.
arxiv-1602-06468 | FLASH: Fast Bayesian Optimization for Data Analytic Pipelines |  http://arxiv.org/abs/1602.06468  | author:Yuyu Zhang, Mohammad Taha Bahadori, Hang Su, Jimeng Sun category:cs.LG published:2016-02-20 summary:Modern data science relies on data analytic pipelines to organizeinterdependent computational steps. Such analytic pipelines often involvedifferent algorithms across multiple steps, each with its own hyperparameters.To get the best performance, it is often critical to select optimal algorithmsand set appropriate hyperparameters, which requires large computationalefforts. Bayesian optimization provides a principled way for searching optimalhyperparameters for a single algorithm. However, many challenges remain insolving pipeline optimization problems with high-dimensional and highlyconditional search space. In this work, we propose Fast LineAr SearcH (FLASH),an efficient method for tuning analytic pipelines. FLASH is a two-layerBayesian optimization framework, which firstly uses a parametric model toselect promising algorithms, then computes a nonparametric model to fine-tunehyperparameters of the promising algorithms. FLASH also includes an effectivecaching algorithm which can further accelerate the search process. Extensiveexperiments on a number of benchmark datasets have demonstrated that FLASHsignificantly outperforms previous state-of-the-art methods in both searchspeed and accuracy. Using 50% of the time budget, FLASH achieves up to 20%improvement on test error rate compared to the baselines. Our method alsoyields state-of-the-art performance on a real-world application for healthcarepredictive modeling.
arxiv-1602-06410 | Semidefinite Programs for Exact Recovery of a Hidden Community |  http://arxiv.org/abs/1602.06410  | author:Bruce Hajek, Yihong Wu, Jiaming Xu category:stat.ML cs.IT cs.SI math.IT math.ST stat.TH published:2016-02-20 summary:We study a semidefinite programming (SDP) relaxation of the maximumlikelihood estimation for exactly recovering a hidden community of cardinality$K$ from an $n \times n$ symmetric data matrix $A$, where for distinct indices$i,j$, $A_{ij} \sim P$ if $i, j$ are both in the community and $A_{ij} \sim Q$otherwise, for two known probability distributions $P$ and $Q$. We identify asufficient condition and a necessary condition for the success of SDP for thegeneral model. For both the Bernoulli case ($P={\rm Bern}(p)$ and $Q={\rmBern}(q)$ with $p>q$) and the Gaussian case ($P=\mathcal{N}(\mu,1)$ and$Q=\mathcal{N}(0,1)$ with $\mu>0$), which correspond to the problem of planteddense subgraph recovery and submatrix localization respectively, the generalresults lead to the following findings: (1) If $K=\omega( n /\log n)$, SDPattains the information-theoretic recovery limits with sharp constants; (2) If$K=\Theta(n/\log n)$, SDP is order-wise optimal, but strictly suboptimal by aconstant factor; (3) If $K=o(n/\log n)$ and $K \to \infty$, SDP is order-wisesuboptimal. A key ingredient in the proof of the necessary condition is aconstruction of a primal feasible solution based on random perturbation of thetrue cluster matrix.
arxiv-1602-06429 | Generalized Statistical Tests for mRNA and Protein Subcellular Spatial Patterning against Complete Spatial Randomness |  http://arxiv.org/abs/1602.06429  | author:Jonathan H. Warrell, Anca F. Savulescu, Robyn Brackin, Musa M. Mhlanga category:stat.ML q-bio.QM stat.AP published:2016-02-20 summary:We derive generalized estimators for a number of spatial statistics that havebeen used in the analysis of spatially resolved omics data, such as Ripley's K,H and L functions, clustering index, and degree of clustering, which allowthese statistics to be calculated on data modelled by arbitrary random measures(RMs). Our estimators generalize those typically used to calculate thesestatistics on point process data, allowing them to be calculated on RMs whichassign continuous values to spatial regions, for instance to model proteinintensity. The clustering index (H*) compares Ripley's H function calculatedempirically to its distribution under complete spatial randomness (CSR),leading us to consider CSR null hypotheses for RMs which are notpoint-processes when generalizing this statistic. We thus consider restrictedclasses of completely random measures which can be simulated directly (Gammaprocesses and Marked Poisson Processes), as well as the general class of allCSR RMs, for which we derive an exact permutation-based H* estimator. Weestablish several properties of the estimators, including bounds on theaccuracy of our general Ripley K estimator, its relationship to a previousestimator for the cross-correlation measure, and the relationship of ourgeneralized H* estimator to previous statistics. To test the ability of ourapproach to identify spatial patterning, we use Fluorescent In SituHybridization (FISH) and Immunofluorescence (IF) data to probe for mRNA andprotein subcellular localization patterns respectively in polarizing mousefibroblasts on micropattened cells. We observe correlated patterns ofclustering over time for corresponding mRNAs and proteins, suggesting adeterministic effect of mRNA localization on protein localization for severalpairs tested, including one case in which spatial patterning at the mRNA levelhas not been previously demonstrated.
arxiv-1602-06439 | Context-guided diffusion for label propagation on graphs |  http://arxiv.org/abs/1602.06439  | author:Kwang In Kim, James Tompkin, Hanspeter Pfister, Christian Theobalt category:cs.CV published:2016-02-20 summary:Existing approaches for diffusion on graphs, e.g., for label propagation, aremainly focused on isotropic diffusion, which is induced by the commonly-usedgraph Laplacian regularizer. Inspired by the success of diffusivity tensors foranisotropic diffusion in image processing, we presents anisotropic diffusion ongraphs and the corresponding label propagation algorithm. We develop positivedefinite diffusivity operators on the vector bundles of Riemannian manifolds,and discretize them to diffusivity operators on graphs. This enables us toeasily define new robust diffusivity operators which significantly improvesemi-supervised learning performance over existing diffusion algorithms.
arxiv-1602-06225 | GAP Safe Screening Rules for Sparse-Group-Lasso |  http://arxiv.org/abs/1602.06225  | author:Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort, Joseph Salmon category:stat.ML cs.LG math.OC stat.CO published:2016-02-19 summary:In high dimensional settings, sparse structures are crucial for efficiency,either in term of memory, computation or performance. In some contexts, it isnatural to handle more refined structures than pure sparsity, such as forinstance group sparsity. Sparse-Group Lasso has recently been introduced in thecontext of linear regression to enforce sparsity both at the feature level andat the group level. We adapt to the case of Sparse-Group Lasso recent safescreening rules that discard early in the solver irrelevant features/groups.Such rules have led to important speed-ups for a wide range of iterativemethods. Thanks to dual gap computations, we provide new safe screening rulesfor Sparse-Group Lasso and show significant gains in term of computing time fora coordinate descent implementation.
arxiv-1602-06049 | Scaling up Dynamic Topic Models |  http://arxiv.org/abs/1602.06049  | author:Arnab Bhadury, Jianfei Chen, Jun Zhu, Shixia Liu category:stat.ML H.4; G.3 published:2016-02-19 summary:Dynamic topic models (DTMs) are very effective in discovering topics andcapturing their evolution trends in time series data. To do posterior inferenceof DTMs, existing methods are all batch algorithms that scan the full datasetbefore each update of the model and make inexact variational approximationswith mean-field assumptions. Due to a lack of a more scalable inferencealgorithm, despite the usefulness, DTMs have not captured large topic dynamics. This paper fills this research void, and presents a fast and parallelizableinference algorithm using Gibbs Sampling with Stochastic Gradient LangevinDynamics that does not make any unwarranted assumptions. We also present aMetropolis-Hastings based $O(1)$ sampler for topic assignments for each wordtoken. In a distributed environment, our algorithm requires very littlecommunication between workers during sampling (almost embarrassingly parallel)and scales up to large-scale applications. We are able to learn the largestDynamic Topic Model to our knowledge, and learned the dynamics of 1,000 topicsfrom 2.6 million documents in less than half an hour, and our empirical resultsshow that our algorithm is not only orders of magnitude faster than thebaselines but also achieves lower perplexity.
arxiv-1602-06149 | Large age-gap face verification by feature injection in deep networks |  http://arxiv.org/abs/1602.06149  | author:Simone Bianco category:cs.CV published:2016-02-19 summary:This paper introduces a new method for face verification across large agegaps and also a dataset containing variations of age in the wild, the LargeAge-Gap (LAG) dataset, with images ranging from child/young to adult/old. Theproposed method exploits a deep convolutional neural network (DCNN) pre-trainedfor the face recognition task on a large dataset and then fine-tuned for thelarge age-gap face verification task. Finetuning is performed in a Siamesearchitecture using a contrastive loss function. A feature injection layer isintroduced to boost verification accuracy, showing the ability of the DCNN tolearn a similarity metric leveraging external features. Experimental results onthe LAG dataset show that our method is able to outperform the faceverification solutions in the state of the art considered.
arxiv-1602-06157 | Depth-Based Object Tracking Using a Robust Gaussian Filter |  http://arxiv.org/abs/1602.06157  | author:Jan Issac, Manuel Wüthrich, Cristina Garcia Cifuentes, Jeannette Bohg, Sebastian Trimpe, Stefan Schaal category:cs.RO cs.CV published:2016-02-19 summary:We consider the problem of model-based 3D-tracking of objects given densedepth images as input. Two difficulties preclude the application of a standardGaussian filter to this problem. First of all, depth sensors are characterizedby fat-tailed measurement noise. To address this issue, we show how a recentlypublished robustification method for Gaussian filters can be applied to theproblem at hand. Thereby, we avoid using heuristic outlier detection methodsthat simply reject measurements if they do not match the model. Secondly, thecomputational cost of the standard Gaussian filter is prohibitive due to thehigh-dimensional measurement, i.e. the depth image. To address this problem, wepropose an approximation to reduce the computational complexity of the filter.In quantitative experiments on real data we show how our method clearlyoutperforms the standard Gaussian filter. Furthermore, we compare itsperformance to a particle-filter-based tracking method, and observe comparablecomputational efficiency and improved accuracy and smoothness of the estimates.
arxiv-1602-06183 | Node-By-Node Greedy Deep Learning for Interpretable Features |  http://arxiv.org/abs/1602.06183  | author:Ke Wu, Malik Magdon-Ismail category:cs.LG published:2016-02-19 summary:Multilayer networks have seen a resurgence under the umbrella of deeplearning. Current deep learning algorithms train the layers of the networksequentially, improving algorithmic performance as well as providing someregularization. We present a new training algorithm for deep networks whichtrains \emph{each node in the network} sequentially. Our algorithm is orders ofmagnitude faster, creates more interpretable internal representations at thenode level, while not sacrificing on the ultimate out-of-sample performance.
arxiv-1602-06057 | Uniresolution representations of white-matter data from CoCoMac |  http://arxiv.org/abs/1602.06057  | author:Raghavendra Singh category:cs.NE q-bio.NC published:2016-02-19 summary:Tracing data as collated by CoCoMac, a seminal neuroinformatics database, isat multiple resolutions -- white matter tracts were studied for areas and theirsubdivisions by different reports. Network theoretic analysis of thismulti-resolution data often assumes that the data at various resolutions isequivalent, which may not be correct. In this paper we propose three methods toresolve the multi-resolution issue such that the resultant networks haveconnectivity data at only one resolution. The different resultant networks arecompared in terms of their network analysis metrics and degree distributions.
arxiv-1602-06042 | Structured Sparse Regression via Greedy Hard-Thresholding |  http://arxiv.org/abs/1602.06042  | author:Prateek Jain, Nikhil Rao, Inderjit Dhillon category:stat.ML cs.LG published:2016-02-19 summary:Several learning applications require solving high-dimensional regressionproblems where the relevant features belong to a small number of (overlapping)groups. For very large datasets, hard thresholding methods have proven to beextremely efficient under standard sparsity assumptions, but such methodsrequire NP hard projections when dealing with overlapping groups. In thispaper, we propose a simple and efficient method that avoids NP-hard projectionsby using greedy approaches. Our proposed methods come with strong theoreticalguarantees even in the presence of poorly conditioned data, exhibit aninteresting computation-accuracy trade-off and can be extended to significantlyharder problems such as sparse overlapping groups. Experiments on both real andsynthetic data validate our claims and demonstrate that the proposed methodsare significantly faster than the best known greedy and convex relaxationtechniques for learning with structured sparsity.
arxiv-1602-06025 | Spectral Learning for Supervised Topic Models |  http://arxiv.org/abs/1602.06025  | author:Yong Ren, Yining Wang, Jun Zhu category:cs.LG cs.CL cs.IR stat.ML published:2016-02-19 summary:Supervised topic models simultaneously model the latent topic structure oflarge collections of documents and a response variable associated with eachdocument. Existing inference methods are based on variational approximation orMonte Carlo sampling, which often suffers from the local minimum defect.Spectral methods have been applied to learn unsupervised topic models, such aslatent Dirichlet allocation (LDA), with provable guarantees. This paperinvestigates the possibility of applying spectral methods to recover theparameters of supervised LDA (sLDA). We first present a two-stage spectralmethod, which recovers the parameters of LDA followed by a power update methodto recover the regression model parameters. Then, we further present asingle-phase spectral algorithm to jointly recover the topic distributionmatrix as well as the regression weights. Our spectral algorithms are provablycorrect and computationally efficient. We prove a sample complexity bound foreach algorithm and subsequently derive a sufficient condition for theidentifiability of sLDA. Thorough experiments on synthetic and real-worlddatasets verify the theory and demonstrate the practical effectiveness of thespectral algorithms. In fact, our results on a large-scale review ratingdataset demonstrate that our single-phase spectral algorithm alone getscomparable or even better performance than state-of-the-art methods, whileprevious work on spectral methods has rarely reported such promisingperformance.
arxiv-1602-06023 | Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond |  http://arxiv.org/abs/1602.06023  | author:Ramesh Nallapati, Bowen Zhou, Cicero Nogueira dos santos, Caglar Gulcehre, Bing Xiang category:cs.CL published:2016-02-19 summary:In this work, we cast abstractive text summarization as asequence-to-sequence problem and employ the framework of AttentionalEncoder-Decoder Recurrent Neural Networks to this problem, outperformingstate-of-the art model of Rush et. al. (2015) on two different corpora. We alsomove beyond the basic architecture, and propose several novel models to addressimportant problems in summarization including modeling key-words, capturing thehierarchy of sentence-to-word structure and addressing the problem of wordsthat are key to a document, but rare elsewhere. Our work shows that many of ourproposed solutions contribute to further improvement in performance. Inaddition, we propose a new dataset consisting of multi-sentence summaries, andestablish performance benchmarks for further research.
arxiv-1602-06904 | Structured illumination microscopy image reconstruction algorithm |  http://arxiv.org/abs/1602.06904  | author:Amit Lal, Chunyan Shan, Peng Xi category:cs.CV published:2016-02-19 summary:Structured illumination microscopy (SIM) is a very important super-resolutionmicroscopy technique, which provides high speed super-resolution with abouttwo-fold spatial resolution enhancement. Several attempts aimed at improvingthe performance of SIM reconstruction algorithm have been reported. However,most of these highlight only one specific aspect of the SIM reconstruction --such as the determination of the illumination pattern phase shift accurately --whereas other key elements -- such as determination of modulation factor,estimation of object power spectrum, Wiener filtering frequency components withinclusion of object power spectrum information, translocating and the mergingof the overlapping frequency components -- are usually glossed oversuperficially. In addition, most of the work reported lie scattered throughoutthe literature and a comprehensive review of the theoretical background isfound lacking. The purpose of the present work is two-fold: 1) to collect theessential theoretical details of SIM algorithm at one place, thereby makingthem readily accessible to readers for the first time; and 2) to provide anopen source SIM reconstruction code (named OpenSIM), which enables users tointeractively vary the code parameters and study it's effect on reconstructedSIM image.
arxiv-1602-06291 | Contextual LSTM (CLSTM) models for Large scale NLP tasks |  http://arxiv.org/abs/1602.06291  | author:Shalini Ghosh, Oriol Vinyals, Brian Strope, Scott Roy, Tom Dean, Larry Heck category:cs.CL published:2016-02-19 summary:Documents exhibit sequential structure at multiple levels of abstraction(e.g., sentences, paragraphs, sections). These abstractions constitute anatural hierarchy for representing the context in which to infer the meaning ofwords and larger fragments of text. In this paper, we present CLSTM (ContextualLSTM), an extension of the recurrent neural network LSTM (Long-Short TermMemory) model, where we incorporate contextual features (e.g., topics) into themodel. We evaluate CLSTM on three specific NLP tasks: word prediction, nextsentence selection, and sentence topic prediction. Results from experiments runon two corpora, English documents in Wikipedia and a subset of articles from arecent snapshot of English Google News, indicate that using both words andtopics as features improves performance of the CLSTM models over baseline LSTMmodels for these tasks. For example on the next sentence selection task, we getrelative accuracy improvements of 21% for the Wikipedia dataset and 18% for theGoogle News dataset. This clearly demonstrates the significant benefit of usingcontext appropriately in natural language (NL) tasks. This has implications fora wide variety of NL applications like question answering, sentence completion,paraphrase generation, and next utterance prediction in dialog systems.
arxiv-1602-06235 | A Mutual Contamination Analysis of Mixed Membership and Partial Label Models |  http://arxiv.org/abs/1602.06235  | author:Julian Katz-Samuels, Clayton Scott category:stat.ML published:2016-02-19 summary:Many machine learning problems can be characterized by mutual contaminationmodels. In these problems, one observes several random samples from differentconvex combinations of a set of unknown base distributions. It is of interestto decontaminate mutual contamination models, i.e., to recover the basedistributions either exactly or up to a permutation. This paper considers thegeneral setting where the base distributions are defined on arbitraryprobability spaces. We examine the decontamination problem in two mutualcontamination models that describe popular machine learning tasks: recoveringthe base distributions up to a permutation in a mixed membership model, andrecovering the base distributions exactly in a partial label model forclassification. We give necessary and sufficient conditions for identifiabilityof both mutual contamination models, algorithms for both problems in theinfinite and finite sample cases, and introduce novel proof techniques based onaffine geometry.
arxiv-1602-06346 | Policy Error Bounds for Model-Based Reinforcement Learning with Factored Linear Models |  http://arxiv.org/abs/1602.06346  | author:Bernardo Ávila Pires, Csaba Szepesvári category:stat.ML cs.LG published:2016-02-19 summary:In this paper we study a model-based approach to calculating approximatelyoptimal policies in Markovian Decision Processes. In particular, we derivenovel bounds on the loss of using a policy derived from a factored linearmodel, a class of models which generalize virtually all previous models thatcome with strong computational guarantees. For the first time in theliterature, we derive performance bounds for model-based techniques where themodel inaccuracy is measured in weighted norms. Moreover, our bounds show adecreased sensitivity to the discount factor and, unlike similar bounds derivedfor other approaches, they are insensitive to measure mismatch. Similarly toprevious works, our proofs are also based on contraction arguments, but withthe main differences that we use carefully constructed norms building on Banachlattices, and the contraction property is only assumed for operators acting on"compressed" spaces, thus weakening previous assumptions, while strengtheningprevious results.
arxiv-1602-06276 | Semi-parametric Order-based Generalized Multivariate Regression |  http://arxiv.org/abs/1602.06276  | author:Milad Kharratzadeh, Mark Coates category:stat.ML math.ST stat.TH published:2016-02-19 summary:In this paper, we consider a generalized multivariate regression problemwhere the responses are monotonic functions of linear transformations ofpredictors. We propose a semi-parametric algorithm based on the ordering of theresponses which is invariant to the functional form of the transformationfunction. We prove that our algorithm, which maximizes the rank correlation ofresponses and linear transformations of predictors, is a consistent estimatorof the true coefficient matrix. We also identify the rate of convergence andshow that the squared estimation error decays with a rate of $o(1/\sqrt{n})$.We then propose a greedy algorithm to maximize the highly non-smooth objectivefunction of our model and examine its performance through extensivesimulations. Finally, we compare our algorithm with traditional multivariateregression algorithms over synthetic and real data.
arxiv-1602-06294 | Stacking for machine learning redshifts applied to SDSS galaxies |  http://arxiv.org/abs/1602.06294  | author:Roman Zitlau, Ben Hoyle, Kerstin Paech, Jochen Weller, Markus Michael Rau, Stella Seitz category:astro-ph.IM astro-ph.CO cs.LG published:2016-02-19 summary:We present an analysis of a general machine learning technique called'stacking' for the estimation of photometric redshifts. Stacking techniques canfeed the photometric redshift estimate, as output by a base algorithm, backinto the same algorithm as an additional input feature in a subsequent learninground. We shown how all tested base algorithms benefit from at least oneadditional stacking round (or layer). To demonstrate the benefit of stacking,we apply the method to both unsupervised machine learning techniques based onself-organising maps (SOMs), and supervised machine learning methods based ondecision trees. We explore a range of stacking architectures, such as thenumber of layers and the number of base learners per layer. Finally we explorethe effectiveness of stacking even when using a successful algorithm such asAdaBoost. We observe a significant improvement of between 1.9% and 21% on allcomputed metrics when stacking is applied to weak learners (such as SOMs anddecision trees). When applied to strong learning algorithms (such as AdaBoost)the ratio of improvement shrinks, but still remains positive and is between0.4% and 2.5% for the explored metrics and comes at almost no additionalcomputational cost.
arxiv-1602-06289 | Learning to SMILE(S) |  http://arxiv.org/abs/1602.06289  | author:Stanisław Jastrzębski, Damian Leśniak, Wojciech Marian Czarnecki category:cs.CL published:2016-02-19 summary:This paper shows how one can directly apply natural language processing (NLP)methods to classification problems in cheminformatics. Connection between theseseemingly separate fields is shown by considering standard textualrepresentation of compound, SMILES. The problem of activity prediction againsta target protein is considered, which is a crucial part of computer aided drugdesign process. Conducted experiments show that this way one can not onlyoutrank state of the art results of hand crafted representations but also getsdirect structural insights into the way decisions are made.
arxiv-1602-06064 | On Training Bi-directional Neural Network Language Model with Noise Contrastive Estimation |  http://arxiv.org/abs/1602.06064  | author:Tianxing He, Yu Zhang, Jasha Droppo, Kai Yu category:cs.CL published:2016-02-19 summary:We propose to train bi-directional neural network language model(NNLM) withnoise contrastive estimation(NCE). Experiments are conducted on a rescore taskon the PTB data set. It is shown that NCE-trained bi-directional NNLMoutperformed the one trained by conventional maximum likelihood training. Butstill(regretfully), it did not out-perform the baseline uni-directional NNLM.
arxiv-1602-06053 | First-order Methods for Geodesically Convex Optimization |  http://arxiv.org/abs/1602.06053  | author:Hongyi Zhang, Suvrit Sra category:math.OC cs.LG stat.ML published:2016-02-19 summary:Geodesic convexity generalizes the notion of (vector space) convexity tononlinear metric spaces. But unlike convex optimization, geodesically convex(g-convex) optimization is much less developed. In this paper we contribute tothe understanding of g-convex optimization by developing iteration complexityanalysis for several first-order algorithms on Hadamard manifolds.Specifically, we prove upper bounds for the global complexity of deterministicand stochastic (sub)gradient methods for optimizing smooth and nonsmoothg-convex functions, both with and without strong g-convexity. Our analysis alsoreveals how the manifold geometry, especially \emph{sectional curvature},impacts convergence rates. To the best of our knowledge, our work is the firstto provide global complexity analysis for first-order algorithms for generalg-convex optimization.
arxiv-1602-05822 | What is the distribution of the number of unique original items in a bootstrap sample? |  http://arxiv.org/abs/1602.05822  | author:Alex F. Mendelson, Maria A. Zuluaga, Brian F. Hutton, Sébastien Ourselin category:stat.ML 62G09 published:2016-02-18 summary:Sampling with replacement occurs in many settings in machine learning,notably in the bagging ensemble technique and the .632+ validation scheme. Thenumber of unique original items in a bootstrap sample can have an importantrole in the behaviour of prediction models learned on it. Indeed, there areuncontrived examples where duplicate items have no effect. The purpose of thisreport is to present the distribution of the number of unique original items ina bootstrap sample clearly and concisely, with a view to enabling other machinelearning researchers to understand and control this quantity in existing andfuture resampling techniques. We describe the key characteristics of thisdistribution along with the generalisation for the case where items come fromdistinct categories, as in classification. In both cases we discuss the normallimit, and conduct an empirical investigation to derive a heuristic for when anormal approximation is permissible.
arxiv-1602-05980 | Revise Saturated Activation Functions |  http://arxiv.org/abs/1602.05980  | author:Bing Xu, Ruitong Huang, Mu Li category:cs.LG published:2016-02-18 summary:In this paper, we revise two commonly used saturated functions, the logisticsigmoid and the hyperbolic tangent (tanh). We point out that, besides the well-known non-zero centered property, slopeof the activation function near the origin is another possible reason makingtraining deep networks with the logistic function difficult to train. Wedemonstrate that, with proper rescaling, the logistic sigmoid achievescomparable results with tanh. Then following the same argument, we improve tahn by penalizing in thenegative part. We show that "penalized tanh" is comparable and even outperformsthe state-of-the-art non-saturated functions including ReLU and leaky ReLU ondeep convolution neural networks. Our results contradict to the conclusion of previous works that thesaturation property causes the slow convergence. It suggests furtherinvestigation is necessary to better understand activation functions in deeparchitectures.
arxiv-1602-05897 | Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity |  http://arxiv.org/abs/1602.05897  | author:Amit Daniely, Roy Frostig, Yoram Singer category:cs.LG cs.AI cs.CC cs.DS stat.ML published:2016-02-18 summary:We develop a general duality between neural networks and compositionalkernels, striving towards a better understanding of deep learning. We show thatinitial representations generated by common random initializations aresufficiently rich to express all functions in the dual kernel space. Hence,though the training objective is hard to optimize in the worst case, theinitial weights form a good starting point for optimization. Our dual view alsoreveals a pragmatic and aesthetic perspective of neural networks andunderscores their expressive power.
arxiv-1602-05908 | Efficient approaches for escaping higher order saddle points in non-convex optimization |  http://arxiv.org/abs/1602.05908  | author:Anima Anandkumar, Rong Ge category:cs.LG stat.ML published:2016-02-18 summary:Local search heuristics for non-convex optimizations are popular in appliedmachine learning. However, in general it is hard to guarantee that suchalgorithms even converge to a local minimum, due to the existence ofcomplicated saddle point structures in high dimensions. Many functions havedegenerate saddle points such that the first and second order derivativescannot distinguish them with local optima. In this paper we use higher orderderivatives to escape these saddle points: we design the first efficientalgorithm guaranteed to converge to a third order local optimum (while existingtechniques are at most second order). We also show that it is NP-hard to extendthis further to finding fourth order local optima.
arxiv-1602-05772 | Corpus analysis without prior linguistic knowledge - unsupervised mining of phrases and subphrase structure |  http://arxiv.org/abs/1602.05772  | author:Stefan Gerdjikov, Klaus U. Schulz category:cs.CL published:2016-02-18 summary:When looking at the structure of natural language, "phrases" and "words" arecentral notions. We consider the problem of identifying such "meaningfulsubparts" of language of any length and underlying composition principles in acompletely corpus-based and language-independent way without using any kind ofprior linguistic knowledge. Unsupervised methods for identifying "phrases",mining subphrase structure and finding words in a fully automated way aredescribed. This can be considered as a step towards automatically computing a"general dictionary and grammar of the corpus". We hope that in the long runvariants of our approach turn out to be useful for other kind of sequence dataas well, such as, e.g., speech, genom sequences, or music annotation. Even ifwe are not primarily interested in immediate applications, results obtained fora variety of languages show that our methods are interesting for many practicaltasks in text mining, terminology extraction and lexicography, search enginetechnology, and related fields.
arxiv-1602-05916 | Local Rademacher Complexity-based Learning Guarantees for Multi-Task Learning |  http://arxiv.org/abs/1602.05916  | author:Niloofar Yousefi, Yunwen Lei, Marius Kloft, Mansooreh Mollaghasemi, Georgios Anagnastapolous category:cs.LG published:2016-02-18 summary:We show a Talagrand-type of concentration inequality for Multi-Task Learning(MTL), using which we establish sharp excess risk bounds for MTL in terms ofdistribution- and data-dependent versions of the Local Rademacher Complexity(LRC). We also give a new bound on the LRC for strongly convex hypothesisclasses, which applies not only to MTL but also to the standard i.i.d. setting.Combining both results, one can now easily derive fast-rate bounds on theexcess risk for many prominent MTL methods, including---as wedemonstrate---Schatten-norm, group-norm, and graph-regularized MTL. The derivedbounds reflect a relationship akeen to a conservation law of asymptoticconvergence rates. This very relationship allows for trading o? slower ratesw.r.t. the number of tasks for faster rates with respect to the number ofavailable samples per task, when compared to the rates obtained via atraditional, global Rademacher analysis.
arxiv-1602-05765 | Entity Embeddings with Conceptual Subspaces as a Basis for Plausible Reasoning |  http://arxiv.org/abs/1602.05765  | author:Shoaib Jameel, Steven Schockaert category:cs.AI cs.CL published:2016-02-18 summary:Conceptual spaces are geometric representations of conceptual knowledge, inwhich entities correspond to points, natural properties correspond to convexregions, and the dimensions of the space correspond to salient features. Whileconceptual spaces enable elegant models of various cognitive phenomena, thelack of automated methods for constructing such representations have so farlimited their application in artificial intelligence. To address this issue, wepropose a method which learns a vector-space embedding of entities fromWikipedia and constrains this embedding such that entities of the same semantictype are located in some lower-dimensional subspace. We experimentallydemonstrate the usefulness of these subspaces as (approximate) conceptual spacerepresentations by showing, among others, that important features can bemodelled as directions and that natural properties tend to correspond to convexregions.
arxiv-1602-05920 | Weighted Unsupervised Learning for 3D Object Detection |  http://arxiv.org/abs/1602.05920  | author:Kamran Kowsari, Manal H. Alassaf category:cs.CV published:2016-02-18 summary:This paper introduces a novel weighted unsupervised learning for objectdetection using an RGB-D camera. This technique is feasible for detecting themoving objects in the noisy environments that are captured by an RGB-D camera.The main contribution of this paper is a real-time algorithm for detecting eachobject using weighted clustering as a separate cluster. In a preprocessingstep, the algorithm calculates the pose 3D position X, Y, Z and RGB color ofeach data point and then it calculates each data point's normal vector usingthe point's neighbor. After preprocessing, our algorithm calculates k-weightsfor each data point; each weight indicates membership. Resulting in clusteredobjects of the scene.
arxiv-1602-05925 | Encoding Data for HTM Systems |  http://arxiv.org/abs/1602.05925  | author:Scott Purdy category:cs.NE q-bio.NC published:2016-02-18 summary:Hierarchical Temporal Memory (HTM) is a biologically inspired machineintelligence technology that mimics the architecture and processes of theneocortex. In this white paper we describe how to encode data as SparseDistributed Representations (SDRs) for use in HTM systems. We explain severalexisting encoders, which are available through the open source project calledNuPIC, and we discuss requirements for creating encoders for new types of data.
arxiv-1602-05753 | Overview of Annotation Creation: Processes & Tools |  http://arxiv.org/abs/1602.05753  | author:Mark A. Finlayson, Tomaž Erjavec category:cs.CL cs.HC published:2016-02-18 summary:Creating linguistic annotations requires more than just a reliable annotationscheme. Annotation can be a complex endeavour potentially involving manypeople, stages, and tools. This chapter outlines the process of creatingend-to-end linguistic annotations, identifying specific tasks that researchersoften perform. Because tool support is so central to achieving high quality,reusable annotations with low cost, the focus is on identifying capabilitiesthat are necessary or useful for annotation tools, as well as common problemsthese tools present that reduce their utility. Although examples of specifictools are provided in many cases, this chapter concentrates more on abstractcapabilities and problems because new tools appear continuously, while oldtools disappear into disuse or disrepair. The two core capabilities tools musthave are support for the chosen annotation scheme and the ability to work onthe language under study. Additional capabilities are organized into threecategories: those that are widely provided; those that often useful but foundin only a few tools; and those that have as yet little or no available toolsupport.
arxiv-1602-05719 | An improved analysis of the ER-SpUD dictionary learning algorithm |  http://arxiv.org/abs/1602.05719  | author:Jarosław Błasiok, Jelani Nelson category:cs.LG cs.DS cs.IT math.IT math.PR I.2.6; F.2.0 published:2016-02-18 summary:In "dictionary learning" we observe $Y = AX + E$ for some$Y\in\mathbb{R}^{n\times p}$, $A \in\mathbb{R}^{m\times n}$, and$X\in\mathbb{R}^{m\times p}$. The matrix $Y$ is observed, and $A, X, E$ areunknown. Here $E$ is "noise" of small norm, and $X$ is column-wise sparse. Thematrix $A$ is referred to as a {\em dictionary}, and its columns as {\ematoms}. Then, given some small number $p$ of samples, i.e.\ columns of $Y$, thegoal is to learn the dictionary $A$ up to small error, as well as $X$. Themotivation is that in many applications data is expected to sparse whenrepresented by atoms in the "right" dictionary $A$ (e.g.\ images in the Haarwavelet basis), and the goal is to learn $A$ from the data to then use it forother applications. Recently, [SWW12] proposed the dictionary learning algorithm ER-SpUD withprovable guarantees when $E = 0$ and $m = n$. They showed if $X$ hasindependent entries with an expected $s$ non-zeroes per column for $1 \lesssims \lesssim \sqrt{n}$, and with non-zero entries being subgaussian, then for$p\gtrsim n^2\log^2 n$ with high probability ER-SpUD outputs matrices $A', X'$which equal $A, X$ up to permuting and scaling columns (resp.\ rows) of $A$(resp.\ $X$). They conjectured $p\gtrsim n\log n$ suffices, which they showedwas information theoretically necessary for {\em any} algorithm to succeed when$s \simeq 1$. Significant progress was later obtained in [LV15]. We show that for a slight variant of ER-SpUD, $p\gtrsim n\log(n/\delta)$samples suffice for successful recovery with probability $1-\delta$. We alsoshow that for the unmodified ER-SpUD, $p\gtrsim n^{1.99}$ samples are requiredeven to learn $A, X$ with polynomially small success probability. This resolvesthe main conjecture of [SWW12], and contradicts the main result of [LV15],which claimed that $p\gtrsim n\log^4 n$ guarantees success whp.
arxiv-1602-05682 | Audio Recording Device Identification Based on Deep Learning |  http://arxiv.org/abs/1602.05682  | author:Simeng Qi, Zheng Huang, Yan Li, Shaopei Shi category:cs.SD cs.LG published:2016-02-18 summary:In this paper we present a research on identification of audio recordingdevices from background noise, thus providing a method for forensics. The audiosignal is the sum of speech signal and noise signal. Usually, people pay moreattention to speech signal, because it carries the information to deliver. So agreat amount of researches have been dedicated to getting higherSignal-Noise-Ratio (SNR). There are many speech enhancement algorithms toimprove the quality of the speech, which can be seen as reducing the noise.However, noises can be regarded as the intrinsic fingerprint traces of an audiorecording device. These digital traces can be characterized and identified bynew machine learning techniques. Therefore, in our research, we use the noiseas the intrinsic features. As for the identification, multiple classifiers ofdeep learning methods are used and compared. The identification result showsthat the method of getting feature vector from the noise of each device andidentifying them with deep learning techniques is viable, and well-preformed.
arxiv-1602-05941 | Multi-resolution Compressive Sensing Reconstruction |  http://arxiv.org/abs/1602.05941  | author:Adriana Gonzalez, Hong Jiang, Gang Huang, Laurent Jacques category:cs.CV published:2016-02-18 summary:We consider the problem of reconstructing an image from compressivemeasurements using a multi-resolution grid. In this context, the reconstructedimage is divided into multiple regions, each one with a different resolution.This problem arises in situations where the image to reconstruct contains acertain region of interest (RoI) that is more important than the rest. Througha theoretical analysis and simulation experiments we show that themulti-resolution reconstruction provides a higher quality of the RoI comparedto the traditional single-resolution approach.
arxiv-1602-05659 | Boost Picking: A Universal Method on Converting Supervised Classification to Semi-supervised Classification |  http://arxiv.org/abs/1602.05659  | author:Fuqiang Liu, Fukun Bi, Yiding Yang, Liang Chen category:cs.CV cs.LG published:2016-02-18 summary:This paper proposes a universal method, Boost Picking, to train supervisedclassification models mainly by un-labeled data. Boost Picking only adopts twoweak classifiers to estimate and correct the error. It is theoretically provedthat Boost Picking could train a supervised model mainly by un-labeled data aseffectively as the same model trained by 100% labeled data, only if recalls ofthe two weak classifiers are all greater than zero and the sum of precisions isgreater than one. Based on Boost Picking, we present "Test along with Training(TawT)" to improve the generalization of supervised models. Both Boost Pickingand TawT are successfully tested in varied little data sets.
arxiv-1602-05660 | Feature-Area Optimization: A Novel SAR Image Registration Method |  http://arxiv.org/abs/1602.05660  | author:Fuqiang Liu, Fukun Bi, Liang Chen, Hao Shi, Wei Liu category:cs.CV published:2016-02-18 summary:This letter proposes a synthetic aperture radar (SAR) image registrationmethod named Feature-Area Optimization (FAO). First, the traditional area-basedoptimization model is reconstructed and decomposed into three key but uncertainfactors: initialization, slice set and regularization. Next, structuralfeatures are extracted by scale invariant feature transform (SIFT) indual-resolution space (SIFT-DRS), a novel SIFT-Like method dedicated to FAO.Then, the three key factors are determined based on these features. Finally,solving the factor-determined optimization model can get the registrationresult. A series of experiments demonstrate that the proposed method canregister multi-temporal SAR images accurately and efficiently.
arxiv-1602-05931 | RandomOut: Using a convolutional gradient norm to win The Filter Lottery |  http://arxiv.org/abs/1602.05931  | author:Joseph Paul Cohen, Henry Z. Lo, Wei Ding category:cs.CV published:2016-02-18 summary:Convolutional neural networks are sensitive to the random initialization offilters. We call this The Filter Lottery (TFL) because the random numbers usedto initialize the network determine if you will "win" and converge to asatisfactory local minimum. This issue forces networks to contain more filters(be wider) to achieve higher accuracy because they have better odds of beingtransformed into highly discriminative features at the risk of introducingredundant features. To deal with this, we propose to evaluate and replacespecific convolutional filters that have little impact on the prediction. Weuse the gradient norm to evaluate the impact of a filter on error, andre-initialize filters when the gradient norm of its weights falls below aspecific threshold. This consistently improves accuracy across two datasets byup to 1.8%. Our scheme RandomOut allows us to increase the number of filtersexplored without increasing the size of the network. This yields more compactnetworks which can train and predict with less computation, thus allowing morepowerful CNNs to run on mobile devices.
arxiv-1602-05703 | Least Mean Squares Estimation of Graph Signals |  http://arxiv.org/abs/1602.05703  | author:Paolo Di Lorenzo, Sergio Barbarossa, Paolo Banelli, Stefania Sardellitti category:cs.LG cs.SY published:2016-02-18 summary:In many applications spanning from sensor to social networks, transportationsystems, gene regulatory networks or big data, the signals of interest aredefined over the vertices of a graph. The aim of this paper is to propose aleast mean square (LMS) strategy for adaptive estimation of signals definedover graphs. Assuming the graph signal to be band-limited, over a knownbandwidth, the method enables reconstruction, with guaranteed performance interms of mean-square error, and tracking from a limited number of observationsover a subset of vertices. A detailed mean square analysis provides theperformance of the proposed method, and leads to several insights for designinguseful sampling strategies for graph signals. Numerical results validate ourtheoretical findings, and illustrate the performance of the proposed method.Furthermore, to cope with the case where the bandwidth is not known beforehand,we propose a method that performs a sparse online estimation of the signalsupport in the (graph) frequency domain, which enables online adaptation of thegraph sampling strategy. Finally, we apply the proposed method to build thepower spatial density cartography of a given operational region in a cognitivenetwork environment.
arxiv-1602-05996 | A Nonparametric Framework for Quantifying Generative Inference on Neuromorphic Systems |  http://arxiv.org/abs/1602.05996  | author:Ojash Neopane, Srinjoy Das, Ery Arias-Castro, Kenneth Kreutz-Delgado category:cs.NE published:2016-02-18 summary:Restricted Boltzmann Machines and Deep Belief Networks have been successfullyused in probabilistic generative model applications such as image occlusionremoval, pattern completion and motion synthesis. Generative inference in suchalgorithms can be performed very efficiently on hardware using a Markov ChainMonte Carlo procedure called Gibbs sampling, where stochastic samples are drawnfrom noisy integrate and fire neurons implemented on neuromorphic substrates.Currently, no satisfactory metrics exist for evaluating the generativeperformance of such algorithms implemented on high-dimensional data forneuromorphic platforms. This paper demonstrates the application ofnonparametric goodness-of-fit testing to both quantify the generativeperformance as well as provide decision-directed criteria for choosing theparameters of the neuromorphic Gibbs sampler and optimizing usage of hardwareresources used during sampling.
arxiv-1602-05702 | EEG-informed attended speaker extraction from recorded speech mixtures with application in neuro-steered hearing prostheses |  http://arxiv.org/abs/1602.05702  | author:Simon Van Eyndhoven, Tom Francart, Alexander Bertrand category:cs.SD cs.SY stat.ML published:2016-02-18 summary:OBJECTIVE: We aim to extract and denoise the attended speaker in a noisy,two-speaker acoustic scenario, relying on microphone array recordings from abinaural hearing aid, which are complemented with electroencephalography (EEG)recordings to infer the speaker of interest. METHODS: In this study, we proposea modular processing flow that first extracts the two speech envelopes from themicrophone recordings, then selects the attended speech envelope based on theEEG, and finally uses this envelope to inform a multi-channel speech separationand denoising algorithm. RESULTS: Strong suppression of interfering(unattended) speech and background noise is achieved, while the attended speechis preserved. Furthermore, EEG-based auditory attention detection (AAD) isshown to be robust to the use of noisy speech signals. CONCLUSIONS: Our resultsshow that AAD-based speaker extraction from microphone array recordings isfeasible and robust, even in noisy acoustic environments, and without access tothe clean speech signals to perform EEG-based AAD. SIGNIFICANCE: Currentresearch on AAD always assumes the availability of the clean speech signals,which limits the applicability in real settings. We have extended this researchto detect the attended speaker even when only microphone recordings with noisyspeech mixtures are available. This is an enabling ingredient for newbrain-computer interfaces and effective filtering schemes in neuro-steeredhearing prostheses. Here, we provide a first proof of concept for EEG-informedattended speaker extraction and denoising.
arxiv-1602-05990 | Plücker Correction Problem: Analysis and Improvements in Efficiency |  http://arxiv.org/abs/1602.05990  | author:João R. Cardoso, Pedro Miraldo, Helder Araujo category:cs.CV cs.RO published:2016-02-18 summary:A given six dimensional vector represents a 3D straight line in Pluckercoordinates if its coordinates satisfy the Klein quadric constraint. In manyproblems aiming to find the Plucker coordinates of lines, noise in the dataand other type of errors contribute for obtaining 6D vectors that do notcorrespond to lines, because of that constraint. A common procedure to overcomethis drawback is to find the Plucker coordinates of the lines that are closestto those vectors. This is known as the Plucker correction problem. In thisarticle we propose a simple, closed-form, and global solution for this problem.When compared with the state-of-the-art method, one can conclude that ouralgorithm is easier and requires much less operations than previous techniques(it does not require Singular Value Decomposition techniques).
arxiv-1602-05944 | The Interaction of Memory and Attention in Novel Word Generalization: A Computational Investigation |  http://arxiv.org/abs/1602.05944  | author:Erin Grant, Aida Nematzadeh, Suzanne Stevenson category:cs.CL published:2016-02-18 summary:People exhibit a tendency to generalize a novel noun to the basic-level in ahierarchical taxonomy -- a cognitively salient category such as "dog" -- withthe degree of generalization depending on the number and type of exemplars.Recently, a change in the presentation timing of exemplars has also been shownto have an effect, surprisingly reversing the prior observed pattern ofbasic-level generalization. We explore the precise mechanisms that could leadto such behavior by extending a computational model of word learning and wordgeneralization to integrate cognitive processes of memory and attention. Ourresults show that the interaction of forgetting and attention to novelty, aswell as sensitivity to both type and token frequencies of exemplars, enablesthe model to replicate the empirical results from different presentationtimings. Our results reinforce the need to incorporate general cognitiveprocesses within word learning models to better understand the range ofobserved behaviors in vocabulary acquisition.
arxiv-1602-05875 | Convolutional RNN: an Enhanced Model for Extracting Features from Sequential Data |  http://arxiv.org/abs/1602.05875  | author:Gil Keren, Björn Schuller category:stat.ML cs.CL published:2016-02-18 summary:Traditional convolutional layers extract features from patches of data byapplying a non-linearity on an affine function of the input. We propose a modelthat enhances this feature extraction process for the case of sequential data,by feeding patches of the data into a recurrent neural network and using theoutputs or hidden states of the recurrent units to compute the extractedfeatures. By doing so, we exploit the fact that a window containing a fewframes of the sequential data is a sequence itself and this additionalstructure might encapsulate valuable information. In addition, we allow formore steps of computation in the feature extraction process, which ispotentially beneficial as an affine function followed by a non-linearity canresult in too simple features. Using our convolutional recurrent layers weobtain an improvement in performance in two audio classification tasks,compared to traditional convolutional layers.
arxiv-1602-05285 | Choice by Elimination via Deep Neural Networks |  http://arxiv.org/abs/1602.05285  | author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.IR cs.LG published:2016-02-17 summary:We introduce Neural Choice by Elimination, a new framework that integratesdeep neural networks into probabilistic sequential choice models for learningto rank. Given a set of items to chose from, the elimination strategy startswith the whole item set and iteratively eliminates the least worthy item in theremaining subset. We prove that the choice by elimination is equivalent tomarginalizing out the random Gompertz latent utilities. Coupled with the choicemodel is the recently introduced Neural Highway Networks for approximatingarbitrarily complex rank functions. We evaluate the proposed framework on alarge-scale public dataset with over 425K items, drawn from the Yahoo! learningto rank challenge. It is demonstrated that the proposed method is competitiveagainst state-of-the-art learning to rank methods.
arxiv-1602-05572 | A landmark-based algorithm for automatic pattern recognition and abnormality detection |  http://arxiv.org/abs/1602.05572  | author:S. Huzurbazar, Long Lee, Dongyang Kuang category:cs.CV published:2016-02-17 summary:We study a class of mathematical and statistical algorithms with the aim ofestablishing a computer-based framework for fast and reliable automatic patternrecognition and abnormality detection. Under this framework, we propose anumerical algorithm for finding group averages where an average of a group isan estimator that is said to best represent the properties of interest of thatgroup. A novelty of the proposed landmark-based algorithm is that the algorithmtracks information of the momentum field through the geodesic shooting process.The momentum field provides a local template-based coordinate system and islinear in nature. It is also a dual of the velocity field with respect to anassigned base template, yielding advantages for statistical analyses. We applythis framework to a small brain image database for detecting structureabnormality. The brain structure changes identified by our framework are highlyconsistent with studies in the literature.
arxiv-1602-05292 | Authorship Attribution Using a Neural Network Language Model |  http://arxiv.org/abs/1602.05292  | author:Zhenhao Ge, Yufang Sun, Mark J. T. Smith category:cs.CL cs.AI published:2016-02-17 summary:In practice, training language models for individual authors is oftenexpensive because of limited data resources. In such cases, Neural NetworkLanguage Models (NNLMs), generally outperform the traditional non-parametricN-gram models. Here we investigate the performance of a feed-forward NNLM on anauthorship attribution problem, with moderate author set size and relativelylimited data. We also consider how the text topics impact performance. Comparedwith a well-constructed N-gram baseline method with Kneser-Ney smoothing, theproposed method achieves nearly 2:5% reduction in perplexity and increasesauthor classification accuracy by 3:43% on average, given as few as 5 testsentences. The performance is very competitive with the state of the art interms of accuracy and demand on test data. The source code, preprocesseddatasets, a detailed description of the methodology and results are availableat https://github.com/zge/authorship-attribution.
arxiv-1602-05419 | Harder, Better, Faster, Stronger Convergence Rates for Least-Squares Regression |  http://arxiv.org/abs/1602.05419  | author:Aymeric Dieuleveut, Nicolas Flammarion, Francis Bach category:math.OC cs.LG stat.ML published:2016-02-17 summary:We consider the optimization of a quadratic objective function whosegradients are only accessible through a stochastic oracle that returns thegradient at any given point plus a zero-mean finite variance random error. Wepresent the first algorithm that achieves jointly the optimal prediction errorrates for least-squares regression, both in terms of forgetting of initialconditions in O(1/n 2), and in terms of dependence on the noise and dimension dof the problem, as O(d/n). Our new algorithm is based on averaged acceleratedregularized gradient descent, and may also be analyzed through finerassumptions on initial conditions and the Hessian matrix, leading todimension-free quantities that may still be small while the " optimal " termsabove are large. In order to characterize the tightness of these new bounds, weconsider an application to non-parametric regression and use the known lowerbounds on the statistical performance (without computational limits), whichhappen to match our bounds obtained from a single pass on the data and thusshow optimality of our algorithm in a wide variety of particular trade-offsbetween bias and variance.
arxiv-1602-05257 | Peak Criterion for Choosing Gaussian Kernel Bandwidth in Support Vector Data Description |  http://arxiv.org/abs/1602.05257  | author:Deovrat Kakde, Arin Chaudhuri, Seunghyun Kong, Maria Jahja, Hansi Jiang, Jorge Silva category:cs.LG published:2016-02-17 summary:Support Vector Data Description (SVDD) is a machine-learning technique usedfor single class classification and outlier detection. SVDD formulation withkernel function provides a flexible boundary around data. The value of kernelfunction parameters affects the nature of the data boundary. For example, it isobserved that with a Gaussian kernel, as the value of kernel bandwidth islowered, the data boundary changes from spherical to wiggly. The spherical databoundary leads to underfitting, and an extremely wiggly data boundary leads tooverfitting. In this paper, we propose empirical criterion to obtain goodvalues of the Gaussian kernel bandwidth parameter. This criterion provides asmooth boundary that captures the essential geometric features of the data.
arxiv-1602-05531 | On the Use of Deep Learning for Blind Image Quality Assessment |  http://arxiv.org/abs/1602.05531  | author:Simone Bianco, Luigi Celona, Paolo Napoletano, Raimondo Schettini category:cs.CV published:2016-02-17 summary:In this work we investigate the use of deep learning for distortion-genericblind image quality assessment. We report on different design choices, rangingfrom the use of features extracted from pre-trained Convolutional NeuralNetworks (CNNs) as a generic image description, to the use of featuresextracted from a CNN fine-tuned for the image quality task. Our best proposal,named DeepBIQ, estimates the image quality by aver- age pooling the scorespredicted on multiple sub-regions of the original image. The score of eachsub-region is computed using a Support Vector Regression (SVR) machine takingas input features extracted using a CNN fine-tuned for image qualityassessment. Experimental results on the LIVE In the Wild Image QualityChallenge Database show that DeepBIQ outperforms the state-of-the-art methodscompared, having a Linear Correlation Coefficient (LCC) with human subjectivescores of almost 0.91. Furthermore, in many cases, the quality scorepredictions of DeepBIQ are closer to the average observer than those of ageneric human observer.
arxiv-1602-05307 | Label Noise Reduction in Entity Typing by Heterogeneous Partial-Label Embedding |  http://arxiv.org/abs/1602.05307  | author:Xiang Ren, Wenqi He, Meng Qu, Clare R. Voss, Heng Ji, Jiawei Han category:cs.CL cs.LG published:2016-02-17 summary:Current systems of fine-grained entity typing use distant supervision inconjunction with existing knowledge bases to assign categories (type labels) toentity mentions. However, the type labels so obtained from knowledge bases areoften noisy (i.e., incorrect for the entity mention's local context). We definea new task, Label Noise Reduction in Entity Typing (LNR), to be the automaticidentification of correct type labels (type-paths) for training examples, giventhe set of candidate type labels obtained by distant supervision with a giventype hierarchy. The unknown type labels for individual entity mentions and thesemantic similarity between entity types pose unique challenges for solving theLNR task. We propose a general framework, called PLE, to jointly embed entitymentions, text features and entity types into the same low-dimensional spacewhere, in that space, objects whose types are semantically close have similarrepresentations. Then we estimate the type-path for each training example in atop-down manner using the learned embeddings. We formulate a global objectivefor learning the embeddings from text corpora and knowledge bases, which adoptsa novel margin-based loss that is robust to noisy labels and faithfully modelstype correlation derived from knowledge bases. Our experiments on three publictyping datasets demonstrate the effectiveness and robustness of PLE, with anaverage of 25% improvement in accuracy compared to next best method.
arxiv-1602-05310 | Large Scale Kernel Learning using Block Coordinate Descent |  http://arxiv.org/abs/1602.05310  | author:Stephen Tu, Rebecca Roelofs, Shivaram Venkataraman, Benjamin Recht category:cs.LG math.OC stat.ML published:2016-02-17 summary:We demonstrate that distributed block coordinate descent can quickly solvekernel regression and classification problems with millions of data points.Armed with this capability, we conduct a thorough comparison between the fullkernel, the Nystr\"om method, and random features on three large classificationtasks from various domains. Our results suggest that the Nystr\"om methodgenerally achieves better statistical accuracy than random features, but canrequire significantly more iterations of optimization. Lastly, we derive newrates for block coordinate descent which support our experimental findings whenspecialized to kernel methods.
arxiv-1602-05629 | Federated Learning of Deep Networks using Model Averaging |  http://arxiv.org/abs/1602.05629  | author:H. Brendan McMahan, Eider Moore, Daniel Ramage, Blaise Agüera y Arcas category:cs.LG published:2016-02-17 summary:Modern mobile devices have access to a wealth of data suitable for learningmodels, which in turn can greatly improve the user experience on the device.For example, language models can improve speech recognition and text entry, andimage models can automatically select good photos. However, this rich data isoften privacy sensitive, large in quantity, or both, which may preclude loggingto the data-center and training there using conventional approaches. Weadvocate an alternative that leaves the training data distributed on the mobiledevices, and learns a shared model by aggregating locally-computed updates. Weterm this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networksthat proves robust to the unbalanced and non-IID data distributions thatnaturally arise. This method allows high-quality models to be trained inrelatively few rounds of communication, the principal constraint for federatedlearning. The key insight is that despite the non-convex loss functions weoptimize, parameter averaging over updates from multiple clients producessurprisingly good results, for example decreasing the communication needed totrain an LSTM language model by two orders of magnitude.
arxiv-1602-05568 | Multi-layer Representation Learning for Medical Concepts |  http://arxiv.org/abs/1602.05568  | author:Edward Choi, Mohammad Taha Bahadori, Elizabeth Searles, Catherine Coffey, Jimeng Sun category:cs.LG published:2016-02-17 summary:Learning efficient representations for concepts has been proven to be animportant basis for many applications such as machine translation or documentclassification. Proper representations of medical concepts such as diagnosis,medication, procedure codes and visits will have broad applications inhealthcare analytics. However, in Electronic Health Records (EHR) the visitsequences of patients include multiple concepts (diagnosis, procedure, andmedication codes) per visit. This structure provides two types of relationalinformation, namely sequential order of visits and co-occurrence of the codeswithin each visit. In this work, we propose Med2Vec, which not only learnsdistributed representations for both medical codes and visits from a large EHRdataset with over 3 million visits, but also allows us to interpret the learnedrepresentations confirmed positively by clinical experts. In the experiments,Med2Vec displays significant improvement in key medical applications comparedto popular baselines such as Skip-gram, GloVe and stacked autoencoder, whileproviding clinically meaningful interpretation.
arxiv-1602-05563 | Robust Kernel (Cross-) Covariance Operators in Reproducing Kernel Hilbert Space toward Kernel Methods |  http://arxiv.org/abs/1602.05563  | author:Md. Ashad Alam, Kenji Fukumizu, Yu-Ping Wang category:stat.ML published:2016-02-17 summary:To the best of our knowledge, there are no general well-founded robustmethods for statistical unsupervised learning. Most of the unsupervised methodsexplicitly or implicitly depend on the kernel covariance operator (kernel CO)or kernel cross-covariance operator (kernel CCO). They are sensitive tocontaminated data, even when using bounded positive definite kernels. First, wepropose robust kernel covariance operator (robust kernel CO) and robust kernelcrosscovariance operator (robust kernel CCO) based on a generalized lossfunction instead of the quadratic loss function. Second, we propose influencefunction of classical kernel canonical correlation analysis (classical kernelCCA). Third, using this influence function, we propose a visualization methodto detect influential observations from two sets of data. Finally, we propose amethod based on robust kernel CO and robust kernel CCO, called robust kernelCCA, which is designed for contaminated data and less sensitive to noise thanclassical kernel CCA. The principles we describe also apply to many kernelmethods which must deal with the issue of kernel CO or kernel CCO. Experimentson synthesized and imaging genetics analysis demonstrate that the proposedvisualization and robust kernel CCA can be applied effectively to both idealdata and contaminated data. The robust methods show the superior performanceover the state-of-the-art methods.
arxiv-1602-05473 | Auxiliary Deep Generative Models |  http://arxiv.org/abs/1602.05473  | author:Lars Maaløe, Casper Kaae Sønderby, Søren Kaae Sønderby, Ole Winther category:stat.ML cs.AI cs.LG published:2016-02-17 summary:Deep generative models parameterized by neural networks have recentlyachieved state-of-the-art performance in unsupervised and semi-supervisedlearning. We extend deep generative models with auxiliary variables whichimproves the variational approximation. The auxiliary variables leave thegenerative model unchanged but make the variational distribution moreexpressive. Inspired by the structure of the auxiliary variable we also proposea model with two stochastic layers and skip connections. Our findings suggestthat more expressive and properly specified deep generative models convergefaster with better results. We show state-of-the-art performance withinsemi-supervised learning on MNIST (0.96%), SVHN (16.61%) and NORB (9.40%)datasets.
arxiv-1602-05312 | Density-based Denoising of Point Cloud |  http://arxiv.org/abs/1602.05312  | author:Faisal Zaman, Ya Ping Wong, Boon Yian Ng category:cs.CV published:2016-02-17 summary:Point cloud source data for surface reconstruction is usually contaminatedwith noise and outliers. To overcome this deficiency, a density-based pointcloud denoising method is presented to remove outliers and noisy points. First,particle-swam optimization technique is employed for automaticallyapproximating optimal bandwidth of multivariate kernel density estimation toensure the robust performance of density estimation. Then, mean-shift basedclustering technique is used to remove outliers through a thresholding scheme.After removing outliers from the point cloud, bilateral mesh filtering isapplied to smooth the remaining points. The experimental results show that thisapproach, comparably, is robust and efficient.
arxiv-1602-05314 | PlaNet - Photo Geolocation with Convolutional Neural Networks |  http://arxiv.org/abs/1602.05314  | author:Tobias Weyand, Ilya Kostrikov, James Philbin category:cs.CV published:2016-02-17 summary:Is it possible to build a system to determine the location where a photo wastaken using just its pixels? In general, the problem seems exceptionallydifficult: it is trivial to construct situations where no location can beinferred. Yet images often contain informative cues such as landmarks, weatherpatterns, vegetation, road markings, and architectural details, which incombination may allow one to determine an approximate location and occasionallyan exact location. Websites such as GeoGuessr and View from your Window suggestthat humans are relatively good at integrating these cues to geolocate images,especially en-masse. In computer vision, the photo geolocation problem isusually approached using image retrieval methods. In contrast, we pose theproblem as one of classification by subdividing the surface of the earth intothousands of multi-scale geographic cells, and train a deep network usingmillions of geotagged images. While previous approaches only recognizelandmarks or perform approximate matching using global image descriptors, ourmodel is able to use and integrate multiple visible cues. We show that theresulting model, called PlaNet, outperforms previous approaches and evenattains superhuman levels of accuracy in some cases. Moreover, we extend ourmodel to photo albums by combining it with a long short-term memory (LSTM)architecture. By learning to exploit temporal coherence to geolocate uncertainphotos, we demonstrate that this model achieves a 50% performance improvementover the single-image model.
arxiv-1602-05450 | Inverse Reinforcement Learning in Swarm Systems |  http://arxiv.org/abs/1602.05450  | author:Adrian Šošić, Wasiur R. KhudaBukhsh, Abdelhak M. Zoubir, Heinz Koeppl category:stat.ML cs.AI cs.MA cs.SY published:2016-02-17 summary:Inverse reinforcement learning (IRL) is the problem of recovering a system'slatent reward function from observed system behavior. In this paper, weconcentrate on IRL in homogeneous large-scale systems, which we refer to asswarms. We show that, by exploiting the inherent homogeneity of a swarm, theIRL objective can be reduced to an equivalent single-agent formulation ofconstant complexity, which allows us to decompose a global system objectiveinto local subgoals at the agent-level. Based on this finding, we reformulatethe corresponding optimal control problem as a fix-point problem pointingtowards a symmetric Nash equilibrium, which we solve using a novelheterogeneous learning scheme particularly tailored to the swarm setting.Results on the Vicsek model and the Ising model demonstrate that the proposedframework is able to produce meaningful reward models from which we can learnnear-optimal local controllers that replicate the observed system dynamics.
arxiv-1602-05264 | Anomaly Detection in Clutter using Spectrally Enhanced Ladar |  http://arxiv.org/abs/1602.05264  | author:Puneet S Chhabra, Andrew M Wallace, James R Hopgood category:physics.optics cs.LG stat.AP stat.ML published:2016-02-17 summary:Discrete return (DR) Laser Detection and Ranging (Ladar) systems provide aseries of echoes that reflect from objects in a scene. These can be first, lastor multi-echo returns. In contrast, Full-Waveform (FW)-Ladar systems measurethe intensity of light reflected from objects continuously over a period oftime. In a camouflaged scenario, e.g., objects hidden behind dense foliage, aFW-Ladar penetrates such foliage and returns a sequence of echoes includingburied faint echoes. The aim of this paper is to learn local-patterns ofco-occurring echoes characterised by their measured spectra. A deviation fromsuch patterns defines an abnormal event in a forest/tree depth profile. As faras the authors know, neither DR or FW-Ladar, along with several spectralmeasurements, has not been applied to anomaly detection. This work presents analgorithm that allows detection of spectral and temporal anomalies in FW-MultiSpectral Ladar (FW-MSL) data samples. An anomaly is defined as a full waveformtemporal and spectral signature that does not conform to a prior expectation,represented using a learnt subspace (dictionary) and set of coefficients thatcapture co-occurring local-patterns using an overlapping temporal window. Amodified optimization scheme is proposed for subspace learning based onstochastic approximations. The objective function is augmented with adiscriminative term that represents the subspace's separability properties andsupports anomaly characterisation. The algorithm detects several man-madeobjects and anomalous spectra hidden in a dense clutter of vegetation and alsoallows tree species classification.
arxiv-1602-05256 | 2D SEM images turn into 3D object models |  http://arxiv.org/abs/1602.05256  | author:Wichai Shanklin category:cs.CV cs.GR published:2016-02-17 summary:The scanning electron microscopy (SEM) is probably one the most fascinatingexamination approach that has been used since more than two decades to detailedinspection of micro scale objects. Most of the scanning electron microscopescould only produce 2D images that could not assist operational analysis ofmicroscopic surface properties. Computer vision algorithms combined with veryadvanced geometry and mathematical approaches turn any SEM into a full 3Dmeasurement device. This work focuses on a methodical literature review forautomatic 3D surface reconstruction of scanning electron microscope images.
arxiv-1602-05332 | Image Restoration: A General Wavelet Frame Based Model and Its Asymptotic Analysis |  http://arxiv.org/abs/1602.05332  | author:Bin Dong, Zuowei Shen, Peichu Xie category:math.FA cs.CV published:2016-02-17 summary:Image restoration is one of the most important areas in imaging science.Mathematical tools have been widely used in image restoration, where waveletframe based approach is one of the successful examples. In this paper, weintroduce a generic wavelet frame based image restoration model, called the"general model", which includes most of the existing wavelet frame based modelsas special cases. Moreover, the general model also includes examples that arenew to the literature. Motivated by our earlier studies [1-3], We provide anasymptotic analysis of the general model as image resolution goes to infinity,which establishes a connection between the general model in discrete settingand a new variatonal model in continuum setting. The variational model alsoincludes some of the existing variational models as special cases, such as thetotal generalized variational model proposed by [4]. In the end, we introducean algorithm solving the general model and present one numerical simulation asan example.
arxiv-1602-05350 | Relative Error Embeddings for the Gaussian Kernel Distance |  http://arxiv.org/abs/1602.05350  | author:Di Chen, Jeff M. Phillips category:cs.LG published:2016-02-17 summary:A reproducing kernel can define an embedding of a data point into an infinitedimensional reproducing kernel Hilbert space (RKHS). The norm in this spacedescribes a distance, which we call the kernel distance. The random Fourierfeatures (of Rahimi and Recht) describe an oblivious approximate mapping intofinite dimensional Euclidean space that behaves similar to the RKHS. We show inthis paper that for the Gaussian kernel the Euclidean norm between these mappedto features has $(1+\epsilon)$-relative error with respect to the kerneldistance. When there are $n$ data points, we show that $O((1/\epsilon^2)\log(n))$ dimensions of the approximate feature space are sufficient andnecessary. Without a bound on $n$, but when the original points lie in $\mathbb{R}^d$and have diameter bounded by $\mathcal{M}$, then we show that $O((d/\epsilon^2)\log(\mathcal{M}))$ dimensions are sufficient, and that this many are required,up to $\log(1/\epsilon)$ factors.
arxiv-1602-05388 | Cross-Language Domain Adaptation for Classifying Crisis-Related Short Messages |  http://arxiv.org/abs/1602.05388  | author:Muhammad Imran, Prasenjit Mitra, Jaideep Srivastava category:cs.CL published:2016-02-17 summary:Rapid crisis response requires real-time analysis of messages. After adisaster happens, volunteers attempt to classify tweets to determine needs,e.g., supplies, infrastructure damage, etc. Given labeled data, supervisedmachine learning can help classify these messages. Scarcity of labeled datacauses poor performance in machine training. Can we reuse old tweets to trainclassifiers? How can we choose labeled tweets for training? Specifically, westudy the usefulness of labeled data of past events. Do labeled tweets indifferent language help? We observe the performance of our classifiers trainedusing different combinations of training sets obtained from past disasters. Weperform extensive experimentation on real crisis datasets and show that thepast labels are useful when both source and target events are of the same type(e.g. both earthquakes). For similar languages (e.g., Italian and Spanish),cross-language domain adaptation was useful, however, when for differentlanguages (e.g., Italian and English), the performance decreased.
arxiv-1602-05352 | Recommendations as Treatments: Debiasing Learning and Evaluation |  http://arxiv.org/abs/1602.05352  | author:Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, Thorsten Joachims category:cs.LG cs.AI cs.IR published:2016-02-17 summary:Most data for evaluating and training recommender systems is subject toselection biases, either through self-selection by the users or through theactions of the recommendation system itself. In this paper, we provide aprincipled approach to handling selection biases, adapting models andestimation techniques from causal inference. The approach leads to unbiasedperformance estimators despite biased data, and to a matrix factorizationmethod that provides substantially improved prediction performance onreal-world data. We theoretically and empirically characterize the robustnessof the approach, finding that it is highly practical and scalable.
arxiv-1602-05394 | Online optimization and regret guarantees for non-additive long-term constraints |  http://arxiv.org/abs/1602.05394  | author:Rodolphe Jenatton, Jim Huang, Cedric Archambeau category:stat.ML cs.LG math.OC math.ST stat.TH published:2016-02-17 summary:We consider online optimization in the 1-lookahead setting, where theobjective does not decompose additively over the rounds of the online game. Theresulting formulation enables us to deal with non-stationary and/or long-termconstraints , which arise, for example, in online display advertising problems.We propose an on-line primal-dual algorithm for which we obtain dynamiccumulative regret guarantees. They depend on the convexity and the smoothnessof the non-additive penalty, as well as terms capturing the smoothness withwhich the residuals of the non-stationary and long-term constraints vary overthe rounds. We conduct experiments on synthetic data to illustrate the benefitsof the non-additive penalty and show vanishing regret convergence on livetraffic data collected by a display advertising platform in production.
arxiv-1602-05439 | Cell segmentation with random ferns and graph-cuts |  http://arxiv.org/abs/1602.05439  | author:Arnaud Browet, Christophe De Vleeschouwer, Laurent Jacques, Navrita Mathiah, Bechara Saykali, Isabelle Migeotte category:cs.CV cs.LG published:2016-02-17 summary:The progress in imaging techniques have allowed the study of various aspectof cellular mechanisms. To isolate individual cells in live imaging data, weintroduce an elegant image segmentation framework that effectively extractscell boundaries, even in the presence of poor edge details. Our approach worksin two stages. First, we estimate pixel interior/border/exterior classprobabilities using random ferns. Then, we use an energy minimization frameworkto compute boundaries whose localization is compliant with the pixel classprobabilities. We validate our approach on a manually annotated dataset.
arxiv-1602-05436 | Low-Rank Factorization of Determinantal Point Processes for Recommendation |  http://arxiv.org/abs/1602.05436  | author:Mike Gartrell, Ulrich Paquet, Noam Koenigstein category:stat.ML cs.LG published:2016-02-17 summary:Determinantal point processes (DPPs) have garnered attention as an elegantprobabilistic model of set diversity. They are useful for a number of subsetselection tasks, including product recommendation. DPPs are parametrized by apositive semi-definite kernel matrix. In this work we present a new method forlearning the DPP kernel from observed data using a low-rank factorization ofthis kernel. We show that this low-rank factorization enables a learningalgorithm that is nearly an order of magnitude faster than previous approaches,while also providing for a method for computing product recommendationpredictions that is far faster (up to 20x faster or more for large itemcatalogs) than previous techniques that involve a full-rank DPP kernel.Furthermore, we show that our method provides equivalent or sometimes betterpredictive performance than prior full-rank DPP approaches, and betterperformance than several other competing recommendation methods in many cases.We conduct an extensive experimental evaluation using several real-worlddatasets in the domain of product recommendation to demonstrate the utility ofour method, along with its limitations.
arxiv-1602-05221 | Patterns of Scalable Bayesian Inference |  http://arxiv.org/abs/1602.05221  | author:Elaine Angelino, Matthew James Johnson, Ryan P. Adams category:stat.ML published:2016-02-16 summary:Datasets are growing not just in size but in complexity, creating a demandfor rich models and quantification of uncertainty. Bayesian methods are anexcellent fit for this demand, but scaling Bayesian inference is a challenge.In response to this challenge, there has been considerable recent work based onvarying assumptions about model structure, underlying computational resources,and the importance of asymptotic correctness. As a result, there is a zoo ofideas with few clear overarching principles. In this paper, we seek to identify unifying principles, patterns, andintuitions for scaling Bayesian inference. We review existing work on utilizingmodern computing resources with both MCMC and variational approximationtechniques. From this taxonomy of ideas, we characterize the general principlesthat have proven successful for designing scalable inference procedures andcomment on the path forward.
arxiv-1602-05242 | Monte Carlo Markov Chain Algorithms for Sampling Strongly Rayleigh Distributions and Determinantal Point Processes |  http://arxiv.org/abs/1602.05242  | author:Nima Anari, Shayan Oveis Gharan, Alireza Rezaei category:cs.LG cs.DS math.PR published:2016-02-16 summary:Strongly Rayleigh distributions are natural generalizations of product anddeterminantal probability distributions and satisfy strongest form of negativedependence properties. We show that the "natural" Monte Carlo Markov Chain(MCMC) is rapidly mixing in the support of a {\em homogeneous} stronglyRayleigh distribution. As a byproduct, our proof implies Markov chains can beused to efficiently generate approximate samples of a $k$-determinantal pointprocess. This answers an open question raised by Deshpande and Rademacher.
arxiv-1602-05003 | The Multivariate Generalised von Mises: Inference and applications |  http://arxiv.org/abs/1602.05003  | author:Alexandre K. W. Navarro, Jes Frellsen, Richard E. Turner category:stat.ML published:2016-02-16 summary:Circular variables arise in a multitude of data-modelling contexts rangingfrom robots to the social sciences. To correctly predict and analyse circulardata, the field of circular and directional statistics has developed a range ofMCMC methods for low-dimensional problems and small-to-medium-sized datasets.In this paper, we extend the toolbox of circular statistics to higherdimensions as a first step towards bringing this field and probabilisticmachine learning closer together. To achieve this task, we introduce themultivariate Generalised von Mises (mGvM) distribution, a Gaussian Processanalogue for circular variables, and demonstrate how this model naturallyoccurs as the posterior in regression and latent variable modelling withcircular variables. We also outline how to perform variational inference forthis model and present experimental results where the mGvM out-performsstandard probabilistic machine learning approaches that do not account for thetopological properties of circular variables.
arxiv-1602-05110 | Generating images with recurrent adversarial networks |  http://arxiv.org/abs/1602.05110  | author:Daniel Jiwoong Im, Chris Dongjoo Kim, Hui Jiang, Roland Memisevic category:cs.LG cs.CV published:2016-02-16 summary:Gatys et al. (2015) showed that optimizing pixels to match features in aconvolutional network with respect reference image features is a way to renderimages of high visual quality. We show that unrolling this gradient-basedoptimization yields a recurrent computation that creates images byincrementally adding onto a visual "canvas". We propose a recurrent generativemodel inspired by this view, and show that it can be trained using adversarialtraining to generate very good image samples. We also propose a way toquantitatively compare adversarial networks by having the generators anddiscriminators of these networks compete against each other.
arxiv-1602-04868 | Deep Feature-based Face Detection on Mobile Devices |  http://arxiv.org/abs/1602.04868  | author:Sayantan Sarkar, Vishal M. Patel, Rama Chellappa category:cs.CV published:2016-02-16 summary:We propose a deep feature-based face detector for mobile devices to detectuser's face acquired by the front facing camera. The proposed method is able todetect faces in images containing extreme pose and illumination variations aswell as partial faces. The main challenge in developing deep feature-basedalgorithms for mobile devices is the constrained nature of the mobile platformand the non-availability of CUDA enabled GPUs on such devices. Ourimplementation takes into account the special nature of the images captured bythe front-facing camera of mobile devices and exploits the GPUs present inmobile devices without CUDA-based frameorks, to meet these challenges.
arxiv-1602-04874 | Bi-directional LSTM Recurrent Neural Network for Chinese Word Segmentation |  http://arxiv.org/abs/1602.04874  | author:Yushi Yao, Zheng Huang category:cs.LG cs.CL published:2016-02-16 summary:Recurrent neural network(RNN) has been broadly applied to natural languageprocessing(NLP) problems. This kind of neural network is designed for modelingsequential data and has been testified to be quite efficient in sequentialtagging tasks. In this paper, we propose to use bi-directional RNN with longshort-term memory(LSTM) units for Chinese word segmentation, which is a crucialpreprocess task for modeling Chinese sentences and articles. Classical methodsfocus on designing and combining hand-craft features from context, whereasbi-directional LSTM network(BLSTM) does not need any prior knowledge orpre-designing, and it is expert in keeping the contextual information in bothdirections. Experiment result shows that our approach gets state-of-the-artperformance in word segmentation on both traditional Chinese datasets andsimplified Chinese datasets.
arxiv-1602-04886 | Fast, Robust, Continuous Monocular Egomotion Computation |  http://arxiv.org/abs/1602.04886  | author:Andrew Jaegle, Stephen Phillips, Kostas Daniilidis category:cs.CV cs.RO published:2016-02-16 summary:We propose robust methods for estimating camera egomotion in noisy,real-world monocular image sequences in the general case of unknown observerrotation and translation with two views and a small baseline. This is adifficult problem because of the nonconvex cost function of the perspectivecamera motion equation and because of non-Gaussian noise arising from noisyoptical flow estimates and scene non-rigidity. To address this problem, weintroduce the expected residual likelihood method (ERL), which estimatesconfidence weights for noisy optical flow data using likelihood distributionsof the residuals of the flow field under a range of counterfactual modelparameters. We show that ERL is effective at identifying outliers andrecovering appropriate confidence weights in many settings. We compare ERL to anovel formulation of the perspective camera motion equation using a liftedkernel, a recently proposed optimization framework for joint parameter andconfidence weight estimation with good empirical properties. We incorporatethese strategies into a motion estimation pipeline that avoids falling intolocal minima. We find that ERL outperforms the lifted kernel method andbaseline monocular egomotion estimation strategies on the challenging KITTIdataset, while adding almost no runtime cost over baseline egomotion methods.
arxiv-1602-04906 | Segmentation Rectification for Video Cutout via One-Class Structured Learning |  http://arxiv.org/abs/1602.04906  | author:Junyan Wang, Sai-kit Yeung, Jue Wang, Kun Zhou category:cs.CV cs.GR cs.LG published:2016-02-16 summary:Recent works on interactive video object cutout mainly focus on designingdynamic foreground-background (FB) classifiers for segmentation propagation.However, the research on optimally removing errors from the FB classificationis sparse, and the errors often accumulate rapidly, causing significant errorsin the propagated frames. In this work, we take the initial steps to addressingthis problem, and we call this new task \emph{segmentation rectification}. Ourkey observation is that the possibly asymmetrically distributed false positiveand false negative errors were handled equally in the conventional methods. We,alternatively, propose to optimally remove these two types of errors. To thiseffect, we propose a novel bilayer Markov Random Field (MRF) model for this newtask. We also adopt the well-established structured learning framework to learnthe optimal model from data. Additionally, we propose a novel one-classstructured SVM (OSSVM) which greatly speeds up the structured learning process.Our method naturally extends to RGB-D videos as well. Comprehensive experimentson both RGB and RGB-D data demonstrate that our simple and effective methodsignificantly outperforms the segmentation propagation methods adopted in thestate-of-the-art video cutout systems, and the results also suggest thepotential usefulness of our method in image cutout system.
arxiv-1602-04912 | Uniform {\varepsilon}-Stability of Distributed Nonlinear Filtering over DNAs: Gaussian-Finite HMMs |  http://arxiv.org/abs/1602.04912  | author:Dionysios S. Kalogerias, Athina P. Petropulu category:math.ST math.OC stat.ML stat.TH published:2016-02-16 summary:In this work, we study stability of distributed filtering of Markov chainswith finite state space, partially observed in conditionally Gaussian noise. Weconsider a nonlinear filtering scheme over a Distributed Network of Agents(DNA), which relies on the distributed evaluation of the likelihood part of thecentralized nonlinear filter and is based on a particular specialization of theAlternating Direction Method of Multipliers (ADMM) for fast average consensus.Assuming the same number of consensus steps between any two consecutive noisymeasurements for each sensor in the network, we fully characterize a minimalnumber of such steps, such that the distributed filter remains uniformly stablewith a prescribed accuracy level, {\varepsilon} \in (0,1], within a finiteoperational horizon, T, and across all sensors. Stability is in the sense ofthe of the \ell_1-norm between the centralized and distributed versions of theposterior at each sensor, and at each time within T. Roughly speaking, our mainresult shows that uniform {\varepsilon}-stability of the distributed filteringprocess depends only loglinearly on T and (roughly) the size of the network,and only logarithmically on 1/{\varepsilon}. If this total loglinear bound isfulfilled, any additional consensus iterations will incur a fully quantifiedfurther exponential decay in the consensus error. Our bounds are universal, inthe sense that they are independent of the particular structure of the GaussianHidden Markov Model (HMM) under consideration.
arxiv-1602-04910 | Bayesian generalized fused lasso modeling via NEG distribution |  http://arxiv.org/abs/1602.04910  | author:Kaito Shimamura, Masao Ueki, Shuichi Kawano, Sadanori Konishi category:stat.ME stat.ML published:2016-02-16 summary:The fused lasso penalizes a loss function by the $L_1$ norm for both theregression coefficients and their successive differences to encourage sparsityof both. In this paper, we propose a Bayesian generalized fused lasso modelingbased on a normal-exponential-gamma (NEG) prior distribution. The NEG prior isassumed into the difference of successive regression coefficients. The proposedmethod enables us to construct a more versatile sparse model than the ordinaryfused lasso by using a flexible regularization term. We also propose a sparsefused algorithm to produce exact sparse solutions. Simulation studies and realdata analyses show that the proposed method has superior performance to theordinary fused lasso.
arxiv-1602-04921 | A diffusion and clustering-based approach for finding coherent motions and understanding crowd scenes |  http://arxiv.org/abs/1602.04921  | author:Weiyao Lin, Yang Mi, Weiyue Wang, Jianxin Wu, Jingdong Wang, Tao Mei category:cs.CV cs.AI cs.MM published:2016-02-16 summary:This paper addresses the problem of detecting coherent motions in crowdscenes and presents its two applications in crowd scene understanding: semanticregion detection and recurrent activity mining. It processes input motionfields (e.g., optical flow fields) and produces a coherent motion filed, namedas thermal energy field. The thermal energy field is able to capture bothmotion correlation among particles and the motion trends of individualparticles which are helpful to discover coherency among them. We furtherintroduce a two-step clustering process to construct stable semantic regionsfrom the extracted time-varying coherent motions. These semantic regions can beused to recognize pre-defined activities in crowd scenes. Finally, we introducea cluster-and-merge process which automatically discovers recurrent activitiesin crowd scenes by clustering and merging the extracted coherent motions.Experiments on various videos demonstrate the effectiveness of our approach.
arxiv-1602-05179 | Towards a Biologically Plausible Backprop |  http://arxiv.org/abs/1602.05179  | author:Benjamin Scellier, Yoshua Bengio category:cs.LG published:2016-02-16 summary:This work follows Bengio and Fischer (2015) in which theoretical foundationswere laid to show how iterative inference can backpropagate error signals.Neurons move their activations towards configurations corresponding to lowerenergy and smaller prediction error: a new observation creates a perturbationat visible neurons that propagates into hidden layers, with these propagatedperturbations corresponding to the back-propagated gradient. This avoids theneed for a lengthy relaxation in the positive phase of training (when bothinputs and targets are observed), as was believed with previous work onfixed-point recurrent networks. We show experimentally that energy-based neuralnetworks with several hidden layers can be trained at discriminative tasks byusing iterative inference and an STDP-like learning rule. The main result ofthis paper is that we can train neural networks with 1, 2 and 3 hidden layerson the permutation-invariant MNIST task and get the training error down to0.00%. The results presented here make it more biologically plausible that amechanism similar to back-propagation may take place in brains in order toachieve credit assignment in deep networks. The paper also discusses some ofthe remaining open problems to achieve a biologically plausible implementationof backprop in brains.
arxiv-1602-04889 | Multi-Source Domain Adaptation Using Approximate Label Matching |  http://arxiv.org/abs/1602.04889  | author:Jordan T. Ash, Robert E. Schapire category:cs.LG cs.AI published:2016-02-16 summary:Domain adaptation, and transfer learning more generally, seeks to remedy theproblem created when training and testing datasets are generated by differentdistributions. In this work, we introduce a new unsupervised domain adaptationalgorithm for when there are multiple sources available to a learner. Ourtechnique assigns a rough labeling on the target samples, then uses it to learna transformation that aligns the two datasets before final classification. Inthis article we give a convenient implementation of our method, show severalexperiments using it, and compare it to other methods commonly used in thefield.
arxiv-1602-05149 | Parallel Bayesian Global Optimization of Expensive Functions |  http://arxiv.org/abs/1602.05149  | author:Jialei Wang, Scott C. Clark, Eric Liu, Peter I. Frazier category:stat.ML math.OC published:2016-02-16 summary:We consider parallel global optimization of derivative-freeexpensive-to-evaluate functions, and propose an efficient method based onstochastic approximation for implementing a conceptual Bayesian optimizationalgorithm proposed by Ginsbourger et al. (2007). To accomplish this, we useinfinitessimal perturbation analysis (IPA) to construct a stochastic gradientestimator and show that this estimator is unbiased. We also show that thestochastic gradient ascent algorithm using the constructed gradient estimatorconverges to a stationary point of the q-EI surface, and therefore, as thenumber of multiple starts of the gradient ascent algorithm and the number ofsteps for each start grow large, the one-step Bayes optimal set of points isrecovered. We show in numerical experiments that our method for maximizing theq-EI is faster than methods based on closed-form evaluation usinghigh-dimensional integration, when considering many parallel functionevaluations, and is comparable in speed when considering few. We also show thatthe resulting one-step Bayes optimal algorithm for parallel global optimizationfinds high quality solutions with fewer evaluations that a heuristic based onapproximately maximizing the q-EI. A high quality open source implementation ofthis algorithm is available in the open source Metrics Optimization Engine(MOE).
arxiv-1602-04984 | Deconvolutional Feature Stacking for Weakly-Supervised Semantic Segmentation |  http://arxiv.org/abs/1602.04984  | author:Hyo-Eun Kim, Sangheum Hwang category:cs.CV published:2016-02-16 summary:A weakly-supervised semantic segmentation framework with a tieddeconvolutional neural network is presented. Each deconvolution layer in theframework consists of unpooling and deconvolution operations. 'Unpooling'upsamples the input feature map based on unpooling switches defined bycorresponding convolution layer's pooling operation. 'Deconvolution' convolvesthe input unpooled features by using convolutional weights tied with thecorresponding convolution layer's convolution operation. Theunpooling-deconvolution combination helps to eliminate less discriminativefeatures in a feature extraction stage, since output features of thedeconvolution layer are reconstructed from the most discriminative unpooledfeatures instead of the raw one. This results in reduction of false positivesin a pixel-level inference stage. All the feature maps restored from the entiredeconvolution layers can constitute a rich discriminative feature set accordingto different abstraction levels. Those features are stacked to be selectivelyused for generating class-specific activation maps. Under the weak supervision(image-level labels), the proposed framework shows promising results on lesionsegmentation in medical images (chest X-rays) and achieves state-of-the-artperformance on the PASCAL VOC segmentation dataset in the same experimentalcondition.
arxiv-1602-04924 | Personalized Federated Search at LinkedIn |  http://arxiv.org/abs/1602.04924  | author:Dhruv Arya, Viet Ha-Thuc, Shakti Sinha category:cs.IR cs.LG published:2016-02-16 summary:LinkedIn has grown to become a platform hosting diverse sources ofinformation ranging from member profiles, jobs, professional groups, slideshowsetc. Given the existence of multiple sources, when a member issues a query like"software engineer", the member could look for software engineer profiles, jobsor professional groups. To tackle this problem, we exploit a data-drivenapproach that extracts searcher intents from their profile data and recentactivities at a large scale. The intents such as job seeking, hiring, contentconsuming are used to construct features to personalize federated searchexperience. We tested the approach on the LinkedIn homepage and A/B tests showsignificant improvements in member engagement. As of writing this paper, theapproach powers all of federated search on LinkedIn homepage.
arxiv-1602-05128 | Interacting Particle Markov Chain Monte Carlo |  http://arxiv.org/abs/1602.05128  | author:Tom Rainforth, Christian A. Naesseth, Fredrik Lindsten, Brooks Paige, Jan-Willem van de Meent, Arnaud Doucet, Frank Wood category:stat.CO stat.ML published:2016-02-16 summary:We introduce interacting particle Markov chain Monte Carlo (iPMCMC), a PMCMCmethod that introduces a coupling between multiple standard and conditionalsequential Monte Carlo samplers. Like related methods, iPMCMC is a Markov chainMonte Carlo sampler on an extended space. We present empirical results thatshow significant improvements in mixing rates relative to both non-interactingPMCMC samplers and a single PMCMC sampler with an equivalent totalcomputational budget. An additional advantage of the iPMCMC method is that itis suitable for distributed and multi-core architectures.
arxiv-1602-05161 | Fast Learning Requires Good Memory: A Time-Space Lower Bound for Parity Learning |  http://arxiv.org/abs/1602.05161  | author:Ran Raz category:cs.LG cs.CC cs.CR published:2016-02-16 summary:We prove that any algorithm for learning parities requires either a memory ofquadratic size or an exponential number of samples. This proves a recentconjecture of Steinhardt, Valiant and Wager and shows that for some learningproblems a large storage space is crucial. More formally, in the problem of parity learning, an unknown string $x \in\{0,1\}^n$ was chosen uniformly at random. A learner tries to learn $x$ from astream of samples $(a_1, b_1), (a_2, b_2) \ldots$, where each~$a_t$ isuniformly distributed over $\{0,1\}^n$ and $b_t$ is the inner product of $a_t$and $x$, modulo~2. We show that any algorithm for parity learning, that usesless than $\frac{n^2}{25}$ bits of memory, requires an exponential number ofsamples. Previously, there was no non-trivial lower bound on the number of samplesneeded, for any learning problem, even if the allowed memory size is $O(n)$(where $n$ is the space needed to store one sample). We also give an application of our result in the field of bounded-storagecryptography. We show an encryption scheme that requires a private key oflength $n$, as well as time complexity of $n$ per encryption/decription of eachbit, and is provenly and unconditionally secure as long as the attacker usesless than $\frac{n^2}{25}$ memory bits and the scheme is used at most anexponential number of times. Previous works on bounded-storage cryptographyassumed that the memory size used by the attacker is at most linear in the timeneeded for encryption/decription.
arxiv-1602-05127 | A Harmonic Extension Approach for Collaborative Ranking |  http://arxiv.org/abs/1602.05127  | author:Da Kuang, Zuoqiang Shi, Stanley Osher, Andrea Bertozzi category:cs.LG published:2016-02-16 summary:We present a new perspective on graph-based methods for collaborative rankingfor recommender systems. Unlike user-based or item-based methods that compute aweighted average of ratings given by the nearest neighbors, or low-rankapproximation methods using convex optimization and the nuclear norm, weformulate matrix completion as a series of semi-supervised learning problems,and propagate the known ratings to the missing ones on the user-user oritem-item graph globally. The semi-supervised learning problems are expressedas Laplace-Beltrami equations on a manifold, or namely, harmonic extension, andcan be discretized by a point integral method. We show that our approach doesnot impose a low-rank Euclidean subspace on the data points, but insteadminimizes the dimension of the underlying manifold. Our method, named LDM (lowdimensional manifold), turns out to be particularly effective in generatingrankings of items, showing decent computational efficiency and robust rankingquality compared to state-of-the-art methods.
arxiv-1602-05124 | Practical Introduction to Clustering Data |  http://arxiv.org/abs/1602.05124  | author:Alexander K. Hartmann category:astro-ph.IM cs.LG published:2016-02-16 summary:Data clustering is an approach to seek for structure in sets of complex data,i.e., sets of "objects". The main objective is to identify groups of objectswhich are similar to each other, e.g., for classification. Here, anintroduction to clustering is given and three basic approaches are introduced:the k-means algorithm, neighbour-based clustering, and an agglomerativeclustering method. For all cases, C source code examples are given, allowingfor an easy implementation.
arxiv-1602-05168 | An Approach for Noise Removal on Depth Images |  http://arxiv.org/abs/1602.05168  | author:Rashi Chaudhary, Himanshu Dasgupta category:cs.CV published:2016-02-16 summary:Image based rendering is a fundamental problem in computer vision andgraphics. Modern techniques often rely on depth image for the 3D construction.However for most of the existing depth cameras, the large and unpredictablenoises can be problematic, which can cause noticeable artifacts in the renderedresults. In this paper, we proposed an efficacious method for depth image noiseremoval that can be applied for most RGBD systems. The proposed solution willbenefit many subsequent vision problems such as 3D reconstruction, novel viewrendering, object recognition. Our experimental results demonstrate theefficacy and accuracy.
arxiv-1602-05012 | A Subsequence Interleaving Model for Sequential Pattern Mining |  http://arxiv.org/abs/1602.05012  | author:Jaroslav Fowkes, Charles Sutton category:stat.ML cs.AI cs.LG published:2016-02-16 summary:Recent sequential pattern mining methods have used the minimum descriptionlength (MDL) principle to define an encoding scheme which describes analgorithm for mining the most compressing patterns in a database. We present anovel subsequence interleaving model based on a probabilistic model of thesequence database, which allows us to search for the most compressing set ofpatterns without designing a specific encoding scheme. Our proposed algorithmis able to efficiently mine the most relevant sequential patterns and rank themusing an associated measure of interestingness. The efficient inference in ourmodel is a direct result of our use of a structural expectation-maximizationframework, in which the expectation-step takes the form of a submodularoptimization problem subject to a coverage constraint. We show on bothsynthetic and real world datasets that our model mines a set of sequentialpatterns with low spuriousness and redundancy, high interpretability andusefulness in real-world applications. Furthermore, we demonstrate that thequality of the patterns from our approach is comparable to, if not better than,existing state of the art sequential pattern mining algorithms.
arxiv-1602-04983 | Contextual Media Retrieval Using Natural Language Queries |  http://arxiv.org/abs/1602.04983  | author:Sreyasi Nag Chowdhury, Mateusz Malinowski, Andreas Bulling, Mario Fritz category:cs.IR cs.AI cs.CL cs.CV cs.HC published:2016-02-16 summary:The widespread integration of cameras in hand-held and head-worn devices aswell as the ability to share content online enables a large and diverse visualcapture of the world that millions of users build up collectively every day. Weenvision these images as well as associated meta information, such as GPScoordinates and timestamps, to form a collective visual memory that can bequeried while automatically taking the ever-changing context of mobile usersinto account. As a first step towards this vision, in this work we presentXplore-M-Ego: a novel media retrieval system that allows users to query adynamic database of images and videos using spatio-temporal natural languagequeries. We evaluate our system using a new dataset of real user queries aswell as through a usability study. One key finding is that there is aconsiderable amount of inter-user variability, for example in the resolution ofspatial relations in natural language utterances. We show that our retrievalsystem can cope with this variability using personalisation through an onlinelearning-based retrieval formulation.
arxiv-1602-04981 | Optimizing Gaze Direction in a Visual Navigation Task |  http://arxiv.org/abs/1602.04981  | author:Tuomas Välimäki, Risto Ritala category:cs.RO cs.CV published:2016-02-16 summary:Navigation in an unknown environment consists of multiple separable subtasks,such as collecting information about the surroundings and navigating to thecurrent goal. In the case of pure visual navigation, all these subtasks need toutilize the same vision system, and therefore a way to optimally control thedirection of focus is needed. We present a case study, where we model theactive sensing problem of directing the gaze of a mobile robot with threemachine vision cameras as a partially observable Markov decision process(POMDP) using a mutual information (MI) based reward function. The key aspectof the solution is that the cameras are dynamically used either in monocular orstereo configuration. The benefits of using the proposed active sensingimplementation are demonstrated with simulations and experiments on a realrobot.
arxiv-1602-04976 | Stochastic Process Bandits: Upper Confidence Bounds Algorithms via Generic Chaining |  http://arxiv.org/abs/1602.04976  | author:Emile Contal, Nicolas Vayatis category:stat.ML cs.LG published:2016-02-16 summary:The paper considers the problem of global optimization in the setup ofstochastic process bandits. We introduce an UCB algorithm which builds acascade of discretization trees based on generic chaining in order to renderpossible his operability over a continuous domain. The theoretical frameworkapplies to functions under weak probabilistic smoothness assumptions and alsoextends significantly the spectrum of application of UCB strategies. Moreovergeneric regret bounds are derived which are then specialized to Gaussianprocesses indexed on infinite-dimensional spaces as well as to quadratic formsof Gaussian processes. Lower bounds are also proved in the case of Gaussianprocesses to assess the optimality of the proposed algorithm.
arxiv-1602-04915 | Gradient Descent Converges to Minimizers |  http://arxiv.org/abs/1602.04915  | author:Jason D. Lee, Max Simchowitz, Michael I. Jordan, Benjamin Recht category:stat.ML cs.LG math.OC published:2016-02-16 summary:We show that gradient descent converges to a local minimizer, almost surelywith random initialization. This is proved by applying the Stable ManifoldTheorem from dynamical systems theory.
arxiv-1602-04951 | Q($λ$) with Off-Policy Corrections |  http://arxiv.org/abs/1602.04951  | author:Anna Harutyunyan, Marc G. Bellemare, Tom Stepleton, Remi Munos category:cs.AI cs.LG stat.ML published:2016-02-16 summary:We propose and analyze an alternate approach to off-policy multi-steptemporal difference learning, in which off-policy returns are corrected withthe current Q-function in terms of rewards, rather than with the target policyin terms of transition probabilities. We prove that such approximatecorrections are sufficient for off-policy convergence both in policy evaluationand control, provided certain conditions. These conditions relate the distancebetween the target and behavior policies, the eligibility trace parameter andthe discount factor, and formalize an underlying tradeoff in off-policyTD($\lambda$). We illustrate this theoretical relationship empirically on acontinuous-state control task.
arxiv-1602-04938 | "Why Should I Trust You?": Explaining the Predictions of Any Classifier |  http://arxiv.org/abs/1602.04938  | author:Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin category:cs.LG cs.AI stat.ML published:2016-02-16 summary:Despite widespread adoption, machine learning models remain mostly blackboxes. Understanding the reasons behind predictions is, however, quiteimportant in assessing trust in a model. Trust is fundamental if one plans totake action based on a prediction, or when choosing whether or not to deploy anew model. Such understanding further provides insights into the model, whichcan be used to turn an untrustworthy model or prediction into a trustworthyone. In this work, we propose LIME, a novel explanation technique that explainsthe predictions of any classifier in an interpretable and faithful manner, bylearning an interpretable model locally around the prediction. We furtherpropose a method to explain models by presenting representative individualpredictions and their explanations in a non-redundant way, framing the task asa submodular optimization problem. We demonstrate the flexibility of thesemethods by explaining different models for text (e.g. random forests) and imageclassification (e.g. neural networks). The usefulness of explanations is shownvia novel experiments, both simulated and with human subjects. Our explanationsempower users in various scenarios that require trust: deciding if one shouldtrust a prediction, choosing between models, improving an untrustworthyclassifier, and detecting why a classifier should not be trusted.
arxiv-1602-05205 | Primal-Dual Rates and Certificates |  http://arxiv.org/abs/1602.05205  | author:Celestine Dünner, Simone Forte, Martin Takáč, Martin Jaggi category:cs.LG math.OC published:2016-02-16 summary:We propose an algorithm-independent framework to equip existing optimizationmethods with primal-dual certificates. Such certificates and corresponding rateof convergence guarantees are important for practitioners to diagnose progress,in particular in machine learning applications. We obtain new primal-dualconvergence rates e.g. for the Lasso as well as many L1, Elastic-Net andgroup-lasso-regularized problems. The theory applies to any norm-regularizedgeneralized linear model. Our approach provides efficiently computable dualitygaps which are globally defined, without modifying the original problems in theregion of interest.
arxiv-1602-04933 | Greedy Ants Colony Optimization Strategy for Solving the Curriculum Based University Course Timetabling Problem |  http://arxiv.org/abs/1602.04933  | author:Patrick Kenekayoro, Godswill Zipamone category:cs.NE published:2016-02-16 summary:Timetabling is a problem faced in all higher education institutions. TheInternational Timetabling Competition (ITC) has published a dataset that can beused to test the quality of methods used to solve this problem. A number ofmeta-heuristic approaches have obtained good results when tested on the ITCdataset, however few have used the ant colony optimization technique,particularly on the ITC 2007 curriculum based university course timetablingproblem. This study describes an ant system that solves the curriculum baseduniversity course timetabling problem and the quality of the algorithm istested on the ITC 2007 dataset. The ant system was able to find feasiblesolutions in all instances of the dataset and close to optimal solutions insome instances. The ant system performs better than some published approaches,however results obtained are not as good as those obtained by the bestpublished approaches. This study may be used as a benchmark for ant basedalgorithms that solve the curriculum based university course timetablingproblem.
arxiv-1602-05236 | A Sparse PCA Approach to Clustering |  http://arxiv.org/abs/1602.05236  | author:T. Tony Cai, Linjun Zhang category:stat.ME stat.ML published:2016-02-16 summary:We discuss a clustering method for Gaussian mixture model based on the sparseprincipal component analysis (SPCA) method and compare it with the IF-PCAmethod. We also discuss the dependent case where the covariance matrix $\Sigma$is not necessarily diagonal.
arxiv-1602-04930 | Generalized minimum dominating set and application in automatic text summarization |  http://arxiv.org/abs/1602.04930  | author:Yi-Zhi Xu, Hai-Jun Zhou category:cs.IR cs.CL physics.soc-ph published:2016-02-16 summary:For a graph formed by vertices and weighted edges, a generalized minimumdominating set (MDS) is a vertex set of smallest cardinality such that thesummed weight of edges from each outside vertex to vertices in this set isequal to or larger than certain threshold value. This generalized MDS problemreduces to the conventional MDS problem in the limiting case of all the edgeweights being equal to the threshold value. We treat the generalized MDSproblem in the present paper by a replica-symmetric spin glass theory andderive a set of belief-propagation equations. As a practical application weconsider the problem of extracting a set of sentences that best summarize agiven input text document. We carry out a preliminary test of the statisticalphysics-inspired method to this automatic text summarization problem.
arxiv-1602-04548 | Safe Pattern Pruning: An Efficient Approach for Predictive Pattern Mining |  http://arxiv.org/abs/1602.04548  | author:Kazuya Nakagawa, Shinya Suzumura, Masayuki Karasuyama, Koji Tsuda, Ichiro Takeuchi category:stat.ML published:2016-02-15 summary:In this paper we study predictive pattern mining problems where the goal isto construct a predictive model based on a subset of predictive patterns in thedatabase. Our main contribution is to introduce a novel method called safepattern pruning (SPP) for a class of predictive pattern mining problems. TheSPP method allows us to efficiently find a superset of all the predictivepatterns in the database that are needed for the optimal predictive model. Theadvantage of the SPP method over existing boosting-type method is that theformer can find the superset by a single search over the database, while thelatter requires multiple searches. The SPP method is inspired by recentdevelopment of safe feature screening. In order to extend the idea of safefeature screening into predictive pattern mining, we derive a novel pruningrule called safe pattern pruning (SPP) rule that can be used for searching overthe tree defined among patterns in the database. The SPP rule has a propertythat, if a node corresponding to a pattern in the database is pruned out by theSPP rule, then it is guaranteed that all the patterns corresponding to itsdescendant nodes are never needed for the optimal predictive model. We applythe SPP method to graph mining and item-set mining problems, and demonstrateits computational advantage.
arxiv-1602-04805 | DR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution Regression |  http://arxiv.org/abs/1602.04805  | author:Jovana Mitrovic, Dino Sejdinovic, Yee Whye Teh category:stat.ML cs.LG stat.CO stat.ME published:2016-02-15 summary:Performing exact posterior inference in complex generative models is oftendifficult or impossible due to an expensive to evaluate or intractablelikelihood function. Approximate Bayesian computation (ABC) is an inferenceframework that constructs an approximation to the true likelihood based on thesimilarity between the observed and simulated data as measured by a predefinedset of summary statistics. Although the choice of appropriate problem-specificsummary statistics crucially influences the quality of the likelihoodapproximation and hence also the quality of the posterior sample in ABC, thereare only few principled general-purpose approaches to the selection orconstruction of such summary statistics. In this paper, we develop a novelframework for this task using kernel-based distribution regression. We modelthe functional relationship between data distributions and the optimal choice(with respect to a loss function) of summary statistics using kernel-baseddistribution regression. We show that our approach can be implemented in acomputationally and statistically efficient way using the random Fourierfeatures framework for large-scale kernel learning. In addition to that, ourframework shows superior performance when compared to related methods on toyand real-world problems.
arxiv-1602-04567 | Adversarial Top-$K$ Ranking |  http://arxiv.org/abs/1602.04567  | author:Changho Suh, Vincent Y. F. Tan, Renbo Zhao category:cs.IR cs.IT cs.LG math.IT stat.ML published:2016-02-15 summary:We study the top-$K$ ranking problem where the goal is to recover the set oftop-$K$ ranked items out of a large collection of items based on partiallyrevealed preferences. We consider an adversarial crowdsourced setting wherethere are two population sets, and pairwise comparison samples drawn from oneof the populations follow the standard Bradley-Terry-Luce model (i.e., thechance of item $i$ beating item $j$ is proportional to the relative score ofitem $i$ to item $j$), while in the other population, the corresponding chanceis inversely proportional to the relative score. When the relative size of thetwo populations is known, we characterize the minimax limit on the sample sizerequired (up to a constant) for reliably identifying the top-$K$ items, anddemonstrate how it scales with the relative size. Moreover, by leveraging atensor decomposition method for disambiguating mixture distributions, we extendour result to the more realistic scenario in which the relative population sizeis unknown, thus establishing an upper bound on the fundamental limit of thesample size for recovering the top-$K$ set.
arxiv-1602-04572 | Personalized Expertise Search at LinkedIn |  http://arxiv.org/abs/1602.04572  | author:Viet Ha-Thuc, Ganesh Venkataraman, Mario Rodriguez, Shakti Sinha, Senthil Sundaram, Lin Guo category:cs.IR cs.LG cs.SI published:2016-02-15 summary:LinkedIn is the largest professional network with more than 350 millionmembers. As the member base increases, searching for experts becomes more andmore challenging. In this paper, we propose an approach to address the problemof personalized expertise search on LinkedIn, particularly for exploratorysearch queries containing {\it skills}. In the offline phase, we introduce acollaborative filtering approach based on matrix factorization. Our approachestimates expertise scores for both the skills that members list on theirprofiles as well as the skills they are likely to have but do not explicitlylist. In the online phase (at query time) we use expertise scores on theseskills as a feature in combination with other features to rank the results. Tolearn the personalized ranking function, we propose a heuristic to extracttraining data from search logs while handling position and sample selectionbiases. We tested our models on two products - LinkedIn homepage and LinkedInrecruiter. A/B tests showed significant improvements in click through rates -31% for CTR@1 for recruiter (18% for homepage) as well as downstream messagessent from search - 37% for recruiter (20% for homepage). As of writing thispaper, these models serve nearly all live traffic for skills search on LinkedInhomepage as well as LinkedIn recruiter.
arxiv-1602-04579 | Secure Approximation Guarantee for Cryptographically Private Empirical Risk Minimization |  http://arxiv.org/abs/1602.04579  | author:Toshiyuki Takada, Hiroyuki Hanada, Yoshiji Yamada, Jun Sakuma, Ichiro Takeuchi category:stat.ML cs.CR cs.LG published:2016-02-15 summary:Privacy concern has been increasingly important in many machine learning (ML)problems. We study empirical risk minimization (ERM) problems under securemulti-party computation (MPC) frameworks. Main technical tools for MPC havebeen developed based on cryptography. One of limitations in currentcryptographically private ML is that it is computationally intractable toevaluate non-linear functions such as logarithmic functions or exponentialfunctions. Therefore, for a class of ERM problems such as logistic regressionin which non-linear function evaluations are required, one can only obtainapproximate solutions. In this paper, we introduce a novel cryptographicallyprivate tool called secure approximation guarantee (SAG) method. The keyproperty of SAG method is that, given an arbitrary approximate solution, it canprovide a non-probabilistic assumption-free bound on the approximation qualityunder cryptographically secure computation framework. We demonstrate thebenefit of the SAG method by applying it to several problems including apractical privacy-preserving data analysis task on genomic and clinicalinformation.
arxiv-1602-04589 | Optimal Best Arm Identification with Fixed Confidence |  http://arxiv.org/abs/1602.04589  | author:Aurélien Garivier, Emilie Kaufmann category:math.ST cs.LG stat.ML stat.TH published:2016-02-15 summary:We provide a complete characterization of the complexity of best-armidentification in one-parameter bandit problems. We prove a new, tight lowerbound on the sample complexity. We propose the 'Track-and-Stop' strategy, whichis proved to be asymptotically optimal. It consists in a new sampling rule(which tracks the optimal proportions of arm draws highlighted by the lowerbound) and in a stopping rule named after Chernoff, for which we give a newanalysis.
arxiv-1602-04593 | Edge Detection for Pattern Recognition: A Survey |  http://arxiv.org/abs/1602.04593  | author:Alex Pappachen James category:cs.CV published:2016-02-15 summary:This review provides an overview of the literature on the edge detectionmethods for pattern recognition that inspire from the understanding of humanvision. We note that edge detection is one of the most fundamental processwithin the low level vision and provides the basis for the higher level visualintelligence in primates. The recognition of the patterns within the imagesrelate closely to the spatiotemporal processes of edge formations, and itsimplementation needs a crossdisciplanry approach in neuroscience, computing andpattern recognition. In this review, the edge detectors are grouped in as edgefeatures, gradients and sketch models, and some example applications areprovided for reference. We note a significant increase in the amount ofpublished research in the last decade that utilizes edge features in a widerange of problems in computer vision and image understanding having a directimplication to pattern recognition with images.
arxiv-1602-04621 | Deep Exploration via Bootstrapped DQN |  http://arxiv.org/abs/1602.04621  | author:Ian Osband, Charles Blundell, Alexander Pritzel, Benjamin Van Roy category:cs.LG cs.AI cs.SY stat.ML published:2016-02-15 summary:Efficient exploration in complex environments remains a major challenge forreinforcement learning. We propose bootstrapped DQN, a simple algorithm thatexplores in a computationally and statistically efficient manner through use ofrandomized value functions. Unlike dithering strategies such as epsilon-greedyexploration, bootstrapped DQN carries out temporally-extended (or deep)exploration; this can lead to exponentially faster learning. We demonstratethese benefits in complex stochastic MDPs and in the large-scale ArcadeLearning Environment. Bootstrapped DQN substantially improves learning timesand performance across most Atari games.
arxiv-1602-04676 | Maximin Action Identification: A New Bandit Framework for Games |  http://arxiv.org/abs/1602.04676  | author:Aurélien Garivier, Emilie Kaufmann, Wouter Koolen category:math.ST cs.GT stat.ML stat.TH published:2016-02-15 summary:We study an original problem of pure exploration in a strategic bandit modelmotivated by Monte Carlo Tree Search. It consists in identifying the bestaction in a game, when the player may sample random outcomes of sequentiallychosen pairs of actions. We propose two strategies for the fixed-confidencesetting: Maximin-LUCB, based on lower-and upper-confidence bounds; andMaximin-Racing, which operates by successively eliminating the sub-optimalactions. We discuss the sample complexity of both methods and compare theirperformance empirically. We sketch a lower bound analysis, and possibleconnections to an optimal algorithm.
arxiv-1602-04847 | Black-box optimization with a politician |  http://arxiv.org/abs/1602.04847  | author:Sébastien Bubeck, Yin-Tat Lee category:math.OC cs.DS cs.LG cs.NA published:2016-02-15 summary:We propose a new framework for black-box convex optimization which iswell-suited for situations where gradient computations are expensive. We derivea new method for this framework which leverages several concepts from convexoptimization, from standard first-order methods (e.g. gradient descent orquasi-Newton methods) to analytical centers (i.e. minimizers of self-concordantbarriers). We demonstrate empirically that our new technique compares favorablywith state of the art algorithms (such as BFGS).
arxiv-1602-04601 | Selective Inference Approach for Statistically Sound Predictive Pattern Mining |  http://arxiv.org/abs/1602.04601  | author:Shinya Suzumura, Kazuya Nakagawa, Mahito Sugiyama, Koji Tsuda, Ichiro Takeuchi category:stat.ML published:2016-02-15 summary:Discovering statistically significant patterns from databases is an importantchallenging problem. The main obstacle of this problem is in the difficulty oftaking into account the selection bias, i.e., the bias arising from the factthat patterns are selected from extremely large number of candidates indatabases. In this paper, we introduce a new approach for predictive patternmining problems that can address the selection bias issue. Our approach isbuilt on a recently popularized statistical inference framework calledselective inference. In selective inference, statistical inferences (such asstatistical hypothesis testing) are conducted based on sampling distributionsconditional on a selection event. If the selection event is characterized in atractable way, statistical inferences can be made without minding selectionbias issue. However, in pattern mining problems, it is difficult tocharacterize the entire selection process of mining algorithms. Our maincontribution in this paper is to solve this challenging problem for a class ofpredictive pattern mining problems by introducing a novel algorithmicframework. We demonstrate that our approach is useful for finding statisticallysignificant patterns from databases.
arxiv-1602-04723 | Efficient Representation of Low-Dimensional Manifolds using Deep Networks |  http://arxiv.org/abs/1602.04723  | author:Ronen Basri, David Jacobs category:cs.NE cs.LG stat.ML published:2016-02-15 summary:We consider the ability of deep neural networks to represent data that liesnear a low-dimensional manifold in a high-dimensional space. We show that deepnetworks can efficiently extract the intrinsic, low-dimensional coordinates ofsuch data. We first show that the first two layers of a deep network canexactly embed points lying on a monotonic chain, a special type of piecewiselinear manifold, mapping them to a low-dimensional Euclidean space. Remarkably,the network can do this using an almost optimal number of parameters. We alsoshow that this network projects nearby points onto the manifold and then embedsthem with little error. We then extend these results to more general manifolds.
arxiv-1602-07614 | A Model of Selective Advantage for the Efficient Inference of Cancer Clonal Evolution |  http://arxiv.org/abs/1602.07614  | author:Daniele Ramazzotti category:cs.LG published:2016-02-15 summary:Recently, there has been a resurgence of interest in rigorous algorithms forthe inference of cancer progression from genomic data. The motivations aremanifold: (i) growing NGS and single cell data from cancer patients, (ii) needfor novel Data Science and Machine Learning algorithms to infer models ofcancer progression, and (iii) a desire to understand the temporal andheterogeneous structure of tumor to tame its progression by efficacioustherapeutic intervention. This thesis presents a multi-disciplinary effort tomodel tumor progression involving successive accumulation of geneticalterations, each resulting populations manifesting themselves in a cancerphenotype. The framework presented in this work along with algorithms derivedfrom it, represents a novel approach for inferring cancer progression, whoseaccuracy and convergence rates surpass the existing techniques. The approachderives its power from several fields including algorithms in machine learning,theory of causality and cancer biology. Furthermore, a modular pipeline toextract ensemble-level progression models from sequenced cancer genomes isproposed. The pipeline combines state-of-the-art techniques for samplestratification, driver selection, identification of fitness-equivalentexclusive alterations and progression model inference. Furthermore, the resultsare validated by synthetic data with realistic generative models, andempirically interpreted in the context of real cancer datasets; in the latercase, biologically significant conclusions are also highlighted. Specifically,it demonstrates the pipeline's ability to reproduce much of the knowledge oncolorectal cancer, as well as to suggest novel hypotheses. Lastly, it alsoproves that the proposed framework can be applied to reconstruct theevolutionary history of cancer clones in single patients, as illustrated by anexample from clear cell renal carcinomas.
arxiv-1602-04741 | Delay and Cooperation in Nonstochastic Bandits |  http://arxiv.org/abs/1602.04741  | author:Nicolo' Cesa-Bianchi, Claudio Gentile, Yishay Mansour, Alberto Minora category:cs.LG published:2016-02-15 summary:We study networks of communicating learning agents that cooperate to solve acommon nonstochastic bandit problem. Agents use an underlying communicationnetwork to get messages about actions selected by other agents, and dropmessages that took more than $d$ hops to arrive, where $d$ is a delayparameter. We introduce \textsc{Exp3-Coop}, a cooperative version of the {\scExp3} algorithm and prove that with $K$ actions and $N$ agents the averageper-agent regret after $T$ rounds is at most of order $\sqrt{\bigl(d+1 +\tfrac{K}{N}\alpha_{\le d}\bigr)(T\ln K)}$, where $\alpha_{\le d}$ is theindependence number of the $d$-th power of the connected communication graph$G$. We then show that for any connected graph, for $d=\sqrt{K}$ the regretbound is $K^{1/4}\sqrt{T}$, strictly better than the minimax regret $\sqrt{KT}$for noncooperating agents. More informed choices of $d$ lead to bounds whichare arbitrarily close to the full information minimax regret $\sqrt{T\ln K}$when $G$ is dense. When $G$ has sparse components, we show that a variant of\textsc{Exp3-Coop}, allowing agents to choose their parameters according totheir centrality in $G$, strictly improves the regret. Finally, as a by-productof our analysis, we provide the first characterization of the minimax regretfor bandit learning with delay.
arxiv-1602-04742 | Training of spiking neural networks based on information theoretic costs |  http://arxiv.org/abs/1602.04742  | author:Oleg Y. Sinyavskiy category:cs.NE q-bio.NC published:2016-02-15 summary:Spiking neural network is a type of artificial neural network in whichneurons communicate between each other with spikes. Spikes are identicalBoolean events characterized by the time of their arrival. A spiking neuron hasinternal dynamics and responds to the history of inputs as opposed to thecurrent inputs only. Because of such properties a spiking neural network hasrich intrinsic capabilities to process spatiotemporal data. However, becausethe spikes are discontinuous 'yes or no' events, it is not trivial to applytraditional training procedures such as gradient descend to the spikingneurons. In this thesis we propose to use stochastic spiking neuron models inwhich probability of a spiking output is a continuous function of parameters.We formulate several learning tasks as minimization of certaininformation-theoretic cost functions that use spiking output probabilitydistributions. We develop a generalized description of the stochastic spikingneuron and a new spiking neuron model that allows to flexibly process richspatiotemporal data. We formulate and derive learning rules for the followingtasks: - a supervised learning task of detecting a spatiotemporal pattern as aminimization of the negative log-likelihood (the surprisal) of the neuron'soutput - an unsupervised learning task of increasing the stability of neurons outputas a minimization of the entropy - a reinforcement learning task of controlling an agent as a modulatedoptimization of filtered surprisal of the neuron's output. We test the derived learning rules in several experiments such asspatiotemporal pattern detection, spatiotemporal data storing and recall withautoassociative memory, combination of supervised and unsupervised learning tospeed up the learning process, adaptive control of simple virtual agents inchanging environments.
arxiv-1602-04799 | Quantum Perceptron Models |  http://arxiv.org/abs/1602.04799  | author:Nathan Wiebe, Ashish Kapoor, Krysta M Svore category:quant-ph cs.LG stat.ML published:2016-02-15 summary:We demonstrate how quantum computation can provide non-trivial improvementsin the computational and statistical complexity of the perceptron model. Wedevelop two quantum algorithms for perceptron learning. The first algorithmexploits quantum information processing to determine a separating hyperplaneusing a number of steps sublinear in the number of data points $N$, namely$O(\sqrt{N})$. The second algorithm illustrates how the classical mistake boundof $O(\frac{1}{\gamma^2})$ can be further improved to$O(\frac{1}{\sqrt{\gamma}})$ through quantum means, where $\gamma$ denotes themargin. Such improvements are achieved through the application of quantumamplitude amplification to the version space interpretation of the perceptronmodel.
arxiv-1602-04605 | Distributed Information-Theoretic Biclustering |  http://arxiv.org/abs/1602.04605  | author:Georg Pichler, Pablo Piantanida, Gerald Matz category:cs.IT cs.LG math.IT published:2016-02-15 summary:We study a novel multi-terminal source coding setup motivated by thebiclustering problem. Two separate encoders observe two stationary, memorylesssources $X^n$ and $Z^n$, respectively. The goal is to find rate-limitedencodings $f(x^n)$ and $g(z^n)$ that maximize the mutual information$I(f(X^n);g(Z^n))/n$. We present non-trivial outer and inner bounds on theachievable region for this problem. These bounds are also generalized to anarbitrary collection of stationary, memoryless sources. The considered problemis intimately connected to distributed hypothesis testing against independenceunder communication constraints, and hence our results are expected to apply tothat setting as well.
arxiv-1602-04409 | Convex Optimization For Non-Convex Problems via Column Generation |  http://arxiv.org/abs/1602.04409  | author:Julian Yarkony, Kamalika Chaudhuri category:cs.LG published:2016-02-14 summary:We apply column generation to approximating complex structured objects via aset of primitive structured objects under either the cross entropy or L2 loss.We use L1 regularization to encourage the use of few structured primitiveobjects. We attack approximation using convex optimization over an infinitenumber of variables each corresponding to a primitive structured object thatare generated on demand by easy inference in the Lagrangian dual. We apply ourapproach to producing low rank approximations to large 3-way tensors.
arxiv-1602-04484 | Fundamental differences between Dropout and Weight Decay in Deep Networks |  http://arxiv.org/abs/1602.04484  | author:David P. Helmbold, Philip M. Long category:cs.LG cs.AI cs.NE math.ST stat.ML stat.TH published:2016-02-14 summary:We study dropout and weight decay applied to deep networks with rectifiedlinear units and the quadratic loss. We show how using dropout in this contextcan be viewed as adding a regularization penalty term that grows exponentiallywith the depth of the network when the more traditional weight decay penaltygrows polynomially. We then show how this difference affects the inductive biasof algorithms using one regularizer or the other: we describe a random sourceof data that dropout is unwilling to fit, but that is compatible with theinductive bias of weight decay. We also describe a source that is compatiblewith the inductive bias of dropout, but not weight decay. We also show that, incontrast with the case of generalized linear models, when used with deepnetworks with rectified linear units and the quadratic loss, the regularizationpenalty of dropout (a) is not just a function of the independent variables, butalso depends on the response variables, and (b) can be negative. Finally, thedropout penalty can drive a learning algorithm to use negative weights evenwhen trained with monotone training data.
arxiv-1602-05112 | ICU Patient Flow Prediction via Discriminative Learning of Mutually-Correcting Processes |  http://arxiv.org/abs/1602.05112  | author:Hongteng Xu, Weichang Wu, Shamim Nemati, Hongyuan Zha category:cs.LG published:2016-02-14 summary:Over the past decade the rate of intensive care unit (ICU) use in the UnitedStates has been increasing, with a recent study reporting almost one in threeMedicare beneficiaries experiencing an ICU visit during the last month of theirlives. With an aging population and ever-growing demand for critical care,effective management of patient flow and transition among different carefacilities will prove indispensible for shortening lengths of hospital stays,improving patient outcomes, allocating critical resources, and reducingpreventable re-admissions. In this paper, we focus on a new problem ofpredicting the so-called ICU patient flow from longitudinal electronic healthrecords (EHRs), which is not explored via existing machine learning techniques.By treating a sequence of transition events as a point process, we develop anovel framework for modeling patient flow through various ICU care units andpredict patients' destination ICUs and duration days jointly. Instead oflearning a generative point process model via maximum likelihood estimation, wepropose a novel discriminative learning algorithm aiming at improving theprediction of transition events. By parameterizing the proposed model as amutually-correcting process, we formulate the problem as a generalized linearmodel, i.e., multinomial logistic regression, which yields itself to efficientlearning via alternating direction method of multipliers (ADMM). Furthermore,we achieve simultaneous feature selection and learning by adding a group-lassoregularizer to the ADMM algorithm. Using real-world data of ICU patients, weshow that our method obtains superior performance in terms of accuracy ofpredicting the destination ICU transition and duration of each ICU occupancy.
arxiv-1602-04513 | Validity and reliability of free software for bidimensional gait analysis |  http://arxiv.org/abs/1602.04513  | author:Ana Paula Quixadá, Andrea Naomi Onodera, Norberto Peña, José Garcia Vivas Miranda, Katia Nunes Sá category:q-bio.QM cs.CV physics.med-ph published:2016-02-14 summary:Despite the evaluation systems of human movement that have been advancing inrecent decades, their use are not feasible for clinical practice because it hasa high cost and scarcity of trained operators to interpret their results. Anideal videogrammetry system should be easy to use, low cost, with minimalequipment, and fast realization. The CvMob is a free tool for dynamicevaluation of human movements that express measurements in figures, tables, andgraphics. This paper aims to determine if CvMob is a reliable tool for theevaluation of two dimensional human gait. This is a validity and reliabilitystudy. The sample was composed of 56 healthy individuals who walked on a9-meterlong walkway and were simultaneously filmed by CvMob and Vicon systemcameras. Linear trajectories and angular measurements were compared to validatethe CvMob system, and inter and intrarater findings of the same measurementswere used to determine reliability. A strong correlation (rs mean = 0.988) ofthe linear trajectories between systems and inter and intrarater analysis werefound. According to the Bland-Altman method, the angles that had good agreementbetween systems were maximum flexion and extension (stance and swing) of theknee and dorsiflexion range of motion and stride length. The CvMob is areliable tool for analysis of linear motion and lengths in two-dimensionalevaluations of human gait. The angular measurements demonstrate high agreementfor the knee joint; however, the hip and ankle measurements were limited bydifferences between systems.
arxiv-1602-04511 | Learning Granger Causality for Hawkes Processes |  http://arxiv.org/abs/1602.04511  | author:Hongteng Xu, Mehrdad Farajtabar, Hongyuan Zha category:cs.LG stat.ML published:2016-02-14 summary:Learning Granger causality for general point processes is a very challengingtask. In this paper, we propose an effective method, learning Grangercausality, for a special but significant type of point processes --- Hawkesprocess. We reveal the relationship between Hawkes process's impact functionand its Granger causality graph. Specifically, our model represents impactfunctions using a series of basis functions and recovers the Granger causalitygraph via group sparsity of the impact functions' coefficients. We propose aneffective learning algorithm combining a maximum likelihood estimator (MLE)with a sparse-group-lasso (SGL) regularizer. Additionally, the flexibility ofour model allows to incorporate the clustering structure event types intolearning framework. We analyze our learning algorithm and propose an adaptiveprocedure to select basis functions. Experiments on both synthetic andreal-world data show that our method can learn the Granger causality graph andthe triggering patterns of the Hawkes processes simultaneously.
arxiv-1602-04506 | Embracing Error to Enable Rapid Crowdsourcing |  http://arxiv.org/abs/1602.04506  | author:Ranjay Krishna, Kenji Hata, Stephanie Chen, Joshua Kravitz, David A. Shamma, Li Fei-Fei, Michael S. Bernstein category:cs.HC cs.CV H.5.m published:2016-02-14 summary:Microtask crowdsourcing has enabled dataset advances in social science andmachine learning, but existing crowdsourcing schemes are too expensive to scaleup with the expanding volume of data. To scale and widen the applicability ofcrowdsourcing, we present a technique that produces extremely rapid judgmentsfor binary and categorical labels. Rather than punishing all errors, whichcauses workers to proceed slowly and deliberately, our technique speeds upworkers' judgments to the point where errors are acceptable and even expected.We demonstrate that it is possible to rectify these errors by randomizing taskorder and modeling response latency. We evaluate our technique on a breadth ofcommon labeling tasks such as image verification, word similarity, sentimentanalysis and topic classification. Where prior work typically achieves a 0.25xto 1x speedup over fixed majority vote, our approach often achieves an order ofmagnitude (10x) speedup.
arxiv-1602-04504 | Can we still avoid automatic face detection? |  http://arxiv.org/abs/1602.04504  | author:Michael J. Wilber, Vitaly Shmatikov, Serge Belongie category:cs.CV published:2016-02-14 summary:After decades of study, automatic face detection and recognition systems arenow accurate and widespread. Naturally, this means users who wish to avoidautomatic recognition are becoming less able to do so. Where do we stand inthis cat-and-mouse race? We currently live in a society where everyone carriesa camera in their pocket. Many people willfully upload most or all of thepictures they take to social networks which invest heavily in automatic facerecognition systems. In this setting, is it still possible forprivacy-conscientious users to avoid automatic face detection and recognition?If so, how? Must evasion techniques be obvious to be effective, or are therestill simple measures that users can use to protect themselves? In this work, we find ways to evade face detection on Facebook, arepresentative example of a popular social network that uses automatic facedetection to enhance their service. We challenge widely-held beliefs aboutevading face detection: do our old techniques such as blurring the face regionor wearing "privacy glasses" still work? We show that in general,state-of-the-art detectors can often find faces even if the subject wearsoccluding clothing or even if the uploader damages the photo to prevent facesfrom being detected.
arxiv-1602-04502 | Do We Need Binary Features for 3D Reconstruction? |  http://arxiv.org/abs/1602.04502  | author:Bin Fan, Qingqun Kong, Wei Sui, Zhiheng Wang, Xinchao Wang, Shiming Xiang, Chunhong Pan, Pascal Fua category:cs.CV published:2016-02-14 summary:Binary features have been incrementally popular in the past few years due totheir low memory footprints and the efficient computation of Hamming distancebetween binary descriptors. They have been shown with promising results on somereal time applications, e.g., SLAM, where the matching operations are relativefew. However, in computer vision, there are many applications such as 3Dreconstruction requiring lots of matching operations between local features.Therefore, a natural question is that is the binary feature still a promisingsolution to this kind of applications? To get the answer, this paper conducts acomparative study of binary features and their matching methods on the contextof 3D reconstruction in a recently proposed large scale mutliview stereodataset. Our evaluations reveal that not all binary features are capable ofthis task. Most of them are inferior to the classical SIFT based method interms of reconstruction accuracy and completeness with a not significant bettercomputational performance.
arxiv-1602-04489 | Convolutional Tables Ensemble: classification in microseconds |  http://arxiv.org/abs/1602.04489  | author:Aharon Bar-Hillel, Eyal Krupka, Noam Bloom category:cs.CV cs.LG 68T45 published:2016-02-14 summary:We study classifiers operating under severe classification time constraints,corresponding to 1-1000 CPU microseconds, using Convolutional Tables Ensemble(CTE), an inherently fast architecture for object category recognition. Thearchitecture is based on convolutionally-applied sparse feature extraction,using trees or ferns, and a linear voting layer. Several structure andoptimization variants are considered, including novel decision functions, treelearning algorithm, and distillation from CNN to CTE architecture. Accuracyimprovements of 24-45% over related art of similar speed are demonstrated onstandard object recognition benchmarks. Using Pareto speed-accuracy curves, weshow that CTE can provide better accuracy than Convolutional Neural Networks(CNN) for a certain range of classification time constraints, or alternativelyprovide similar error rates with 5-200X speedup.
arxiv-1602-04485 | Benefits of depth in neural networks |  http://arxiv.org/abs/1602.04485  | author:Matus Telgarsky category:cs.LG cs.NE stat.ML published:2016-02-14 summary:For any positive integer $k$, there exist neural networks with $\Theta(k^3)$layers, $\Theta(1)$ nodes per layer, and $\Theta(1)$ distinct parameters whichcan not be approximated by networks with $\mathcal{O}(k)$ layers unless theyare exponentially large --- they must possess $\Omega(2^k)$ nodes. This resultis proved here for a class of nodes termed "semi-algebraic gates" whichincludes the common choices of ReLU, maximum, indicator, and piecewisepolynomial functions, therefore establishing benefits of depth against not juststandard networks with ReLU gates, but also convolutional networks with ReLUand maximization gates, and boosted decision trees (in this last case with astronger separation: $\Omega(2^{k^3})$ total tree nodes are required).
arxiv-1602-04427 | Exploiting Lists of Names for Named Entity Identification of Financial Institutions from Unstructured Documents |  http://arxiv.org/abs/1602.04427  | author:Zheng Xu, Douglas Burdick, Louiqa Raschid category:cs.CL published:2016-02-14 summary:There is a wealth of information about financial systems that is embedded indocument collections. In this paper, we focus on a specialized text extractiontask for this domain. The objective is to extract mentions of names offinancial institutions, or FI names, from financial prospectus documents, andto identify the corresponding real world entities, e.g., by matching against acorpus of such entities. The tasks are Named Entity Recognition (NER) andEntity Resolution (ER); both are well studied in the literature. Ourcontribution is to develop a rule-based approach that will exploit lists of FInames for both tasks; our solution is labeled Dict-based NER and Rank-based ER.Since the FI names are typically represented by a root, and a suffix thatmodifies the root, we use these lists of FI names to create specialized rootand suffix dictionaries. To evaluate the effectiveness of our specializedsolution for extracting FI names, we compare Dict-based NER with a generalpurpose rule-based NER solution, ORG NER. Our evaluation highlights thebenefits and limitations of specialized versus general purpose approaches, andpresents additional suggestions for tuning and customization for FI nameextraction. To our knowledge, our proposed solutions, Dict-based NER andRank-based ER, and the root and suffix dictionaries, are the first attempt toexploit specialized knowledge, i.e., lists of FI names, for rule-based NER andER.
arxiv-1602-04474 | Generalization Properties of Learning with Random Features |  http://arxiv.org/abs/1602.04474  | author:Alessandro Rudi, Raffaello Camoriano, Lorenzo Rosasco category:stat.ML cs.LG published:2016-02-14 summary:We study the generalization properties of regularized learning with randomfeatures in the statistical learning theory framework. We show that optimallearning errors can be achieved with a number of features smaller than thenumber of examples. As a byproduct, we also show that learning with randomfeatures can be seen as a form of regularization, rather than only a way tospeed up computations.
arxiv-1602-04450 | Bayesian Optimization with Safety Constraints: Safe and Automatic Parameter Tuning in Robotics |  http://arxiv.org/abs/1602.04450  | author:Felix Berkenkamp, Andreas Krause, Angela P. Schoellig category:cs.RO cs.LG cs.SY published:2016-02-14 summary:Robotics algorithms typically depend on various parameters, the choice ofwhich significantly affects the robot's performance. While an initial guess forthe parameters may be obtained from dynamic models of the robot, parameters areusually tuned manually on the real system to achieve the best performance.Optimization algorithms, such as Bayesian optimization, have been used toautomate this process. However, these methods may evaluate parameters duringthe optimization process that lead to safety-critical system failures.Recently, a safe Bayesian optimization algorithm, called SafeOpt, has beendeveloped and applied in robotics, which guarantees that the performance of thesystem never falls below a critical value; that is, safety is defined based onthe performance function. However, coupling performance and safety is notdesirable in most cases. In this paper, we define separate functions forperformance and safety. We present a generalized SafeOpt algorithm that, givenan initial safe guess for the parameters, maximizes performance but onlyevaluates parameters that satisfy all safety constraints with high probability.It achieves this by modeling the underlying and unknown performance andconstraint functions as Gaussian processes. We provide a theoretical analysisand demonstrate in experiments on a quadrotor vehicle that the proposedalgorithm enables fast, automatic, and safe optimization of tuning parameters.Moreover, we show an extension to context- or environment-dependent, safeoptimization in the experiments.
arxiv-1602-04436 | Distributed Time-Varying Graph Filtering |  http://arxiv.org/abs/1602.04436  | author:Elvin Isufi, Andreas Loukas, Andrea Simonetto, Geert Leus category:cs.LG cs.SY stat.ML published:2016-02-14 summary:One of the cornerstones of the field of signal processing on graphs are graphfilters, direct analogues of classical filters, but intended for signalsdefined on graphs. This work brings forth new insights on the distributed graphfiltering problem. We design a family of autoregressive moving average (ARMA)recursions, which (i) are able to approximate any desired graph frequencyresponse, and (ii) give exact solutions for tasks such as graph signaldenoising and interpolation. The design philosophy, which allows us to design the ARMA coefficientsindependently from the underlying graph, renders the ARMA graph filterssuitable in static and, particularly, time-varying settings. The latter occurwhen the graph signal and/or graph are changing over time. We show that in caseof a time-varying graph signal our approach extends naturally to atwo-dimensional filter, operating concurrently in the graph and regular timedomains. We also derive sufficient conditions for filter stability when thegraph and signal are time-varying. The analytical and numerical resultspresented in this paper illustrate that ARMA graph filters are practicallyappealing for static and time-varying settings, accompanied by strongtheoretical guarantees.
arxiv-1602-04422 | Hi Detector, What's Wrong with that Object? Identifying Irregular Object From Images by Modelling the Detection Score Distribution |  http://arxiv.org/abs/1602.04422  | author:Peng Wang, Lingqiao Liu, Chunhua Shen, Anton van den Hengel, Heng Tao Shen category:cs.CV published:2016-02-14 summary:In this work, we study the challenging problem of identifying the irregularstatus of objects from images in an "open world" setting, that is,distinguishing the irregular status of an object category from its regularstatus as well as objects from other categories in the absence of "irregularobject" training data. To address this problem, we propose a novel approach byinspecting the distribution of the detection scores at multiple image regionsbased on the detector trained from the "regular object" and "other objects".The key observation motivating our approach is that for "regular object" imagesas well as "other objects" images, the region-level scores follow their ownessential patterns in terms of both the score values and the spatialdistributions while the detection scores obtained from an "irregular object"image tend to break these patterns. To model this distribution, we propose touse Gaussian Processes (GP) to construct two separate generative models for thecase of the "regular object" and the "other objects". More specifically, wedesign a new covariance function to simultaneously model the detection score ata single region and the score dependencies at multiple regions. We finallydemonstrate the superior performance of our method on a large dataset newlyproposed in this paper.
arxiv-1602-04435 | Random Forest Based Approach for Concept Drift Handling |  http://arxiv.org/abs/1602.04435  | author:A. Zhukov, D. Sidorov, A. Foley category:cs.AI cs.LG math.ST stat.TH published:2016-02-14 summary:Concept drift has potential in smart grid analysis because the socio-economicbehaviour of consumers is not governed by the laws of physics. Likewise thereare also applications in wind power forecasting. In this paper we presentdecision tree ensemble classification method based on the Random Forestalgorithm for concept drift. The weighted majority voting ensemble aggregationrule is employed based on the ideas of Accuracy Weighted Ensemble (AWE) method.Base learner weight in our case is computed for each sample evaluation usingbase learners accuracy and intrinsic proximity measure of Random Forest. Ouralgorithm exploits both temporal weighting of samples and ensemble pruning as aforgetting strategy. We present results of empirical comparison of our methodwith original random forest with incorporated "replace-the-looser" forgettingandother state-of-the-art concept-drfit classifiers like AWE2.
arxiv-1602-04434 | Frequency Analysis of Temporal Graph Signals |  http://arxiv.org/abs/1602.04434  | author:Andreas Loukas, Damien Foucard category:cs.LG cs.SY stat.ML published:2016-02-14 summary:This letter extends the concept of graph-frequency to graph signals thatevolve with time. Our goal is to generalize and, in fact, unify the familiarconcepts from time- and graph-frequency analysis. To this end, we study a jointtemporal and graph Fourier transform (JFT) and demonstrate its attractiveproperties. We build on our results to create filters which act on the joint(temporal and graph) frequency domain, and show how these can be used toperform interference cancellation. The proposed algorithms are distributed,have linear complexity, and can approximate any desired joint filteringobjective.
arxiv-1602-04433 | Unsupervised Domain Adaptation with Residual Transfer Networks |  http://arxiv.org/abs/1602.04433  | author:Mingsheng Long, Jianmin Wang, Michael I. Jordan category:cs.LG published:2016-02-14 summary:The recent success of deep neural networks relies on massive amounts oflabeled data. For a target task where labeled data is unavailable, domainadaptation can transfer a learner from a different source domain. In thispaper, we propose a new approach to domain adaptation in deep networks that cansimultaneously learn adaptive classifiers and transferable features fromlabeled data in the source domain and unlabeled data in the target domain. Werelax a shared-classifier assumption made by previous methods and assume thatthe source classifier and target classifier differ by a residual function. Weenable classifier adaptation by plugging several layers into the deep networkto explicitly learn the residual function with reference to the targetclassifier. We embed features of multiple layers into reproducing kernelHilbert spaces (RKHSs) and match feature distributions for feature adaptation.The adaptation behaviors can be achieved in most feed-forward models byextending them with new residual layers and loss functions, which can betrained efficiently using standard back-propagation. Empirical evidenceexhibits that the approach outperforms state of art methods on standard domainadaptation datasets.
arxiv-1602-04418 | Identifiability assumptions for directed graphical models with feedback |  http://arxiv.org/abs/1602.04418  | author:Gunwoong Park, Garvesh Raskutti category:stat.ML cs.LG published:2016-02-14 summary:Directed graphical models provide a useful framework for modeling causal ordirectional relationships for multivariate data. Prior work has largely focusedon identifiability and search algorithms for directed acyclic graphical (DAG)models. In many applications, feedback naturally arises and directed graphicalmodels that permit cycles arise. However theory and methodology for directedgraphical models with feedback are considerably less developed since graphswith cycles pose a number of additional challenges. In this paper we addressthe issue of identifiability for general directed cyclic graphical (DCG) modelssatisfying only the Markov assumption. In particular, in addition to thefaithfulness assumption which has already been introduced for cyclic models, weintroduce two new identifiability assumptions, one based on selecting the modelwith the fewest edges and the other based on selecting the DCG model thatentails the maximum d-separation rules. We provide theoretical resultscomparing these assumptions which shows that: (1) selecting models with thelargest number of d-separation rules is strictly weaker than the faithfulnessassumption; (2) unlike for DAG models, selecting models with the fewest edgesdo not necessarily result in a milder assumption than the faithfulnessassumption. We also provide connections between our two new principles andminimality assumptions which lead to a ranking of how strong and weak variousidentifiability and minimality assumptions are for both DAG and DCG models. Weuse our identifiability assumptions to develop search algorithms forsmall-scale DCG models. Our simulations results using our search algorithmssupport our theoretical results, showing that our two new principles generallyout-perform the faithfulness assumption in terms of selecting the true skeletonfor DCG models.
arxiv-1602-04375 | Science Question Answering using Instructional Materials |  http://arxiv.org/abs/1602.04375  | author:Mrinmaya Sachan, Avinava Dubey, Eric P. Xing category:cs.CL cs.AI cs.IR cs.LG published:2016-02-13 summary:We provide a solution for elementary science test using instructionalmaterials. We posit that there is a hidden structure that explains thecorrectness of an answer given the question and instructional materials andpresent a unified max-margin framework that learns to find these hiddenstructures (given a corpus of question-answer pairs and instructionalmaterials), and uses what it learns to answer novel elementary sciencequestions. Our evaluation shows that our framework outperforms several strongbaselines.
arxiv-1602-04330 | On the Topology of Projective Shape Spaces |  http://arxiv.org/abs/1602.04330  | author:Florian Kelma, John T. Kent, Thomas Hotz category:math.ST cs.CV math.GT stat.TH I.4.1; I.4.7 published:2016-02-13 summary:The projective shape of a configuration consists of the information that isinvariant under projective transformations. It encodes the information about anobject reconstructable from uncalibrated camera views. The space of projectiveshapes of k points in d-dimensional real projective space is by definition thequotient space of k copies of that projective space modulo the action of theprojective linear group. A detailed examination of the topology of projectiveshape space is given, and it is shown how to derive subsets that are maximalHausdorff manifolds. A special case are Tyler regular shapes for which one canconstruct a Riemannian metric.
arxiv-1602-04335 | Learning Over Long Time Lags |  http://arxiv.org/abs/1602.04335  | author:Hojjat Salehinejad category:cs.NE published:2016-02-13 summary:The advantage of recurrent neural networks (RNNs) in learning dependenciesbetween time-series data has distinguished RNNs from other deep learningmodels. Recently, many advances are proposed in this emerging field. However,there is a lack of comprehensive review on memory models in RNNs in theliterature. This paper provides a fundamental review on RNNs and long shortterm memory (LSTM) model. Then, provides a surveys of recent advances indifferent memory enhancements and learning techniques for capturing long termdependencies in RNNs.
arxiv-1602-04341 | Attention-Based Convolutional Neural Network for Machine Comprehension |  http://arxiv.org/abs/1602.04341  | author:Wenpeng Yin, Sebastian Ebert, Hinrich Schütze category:cs.CL published:2016-02-13 summary:Understanding open-domain text is one of the primary challenges in naturallanguage processing (NLP). Machine comprehension benchmarks evaluate thesystem's ability to understand text based on the text content only. In thiswork, we investigate machine comprehension on MCTest, a question answering (QA)benchmark. Prior work is mainly based on feature engineering approaches. Wecome up with a neural network framework, named hierarchical attention-basedconvolutional neural network (HABCNN), to address this task without anymanually designed features. Specifically, we explore HABCNN for this task bytwo routes, one is through traditional joint modeling of passage, question andanswer, one is through textual entailment. HABCNN employs an attentionmechanism to detect key phrases, key sentences and key snippets that arerelevant to answering the question. Experiments show that HABCNN outperformsprior deep learning approaches by a big margin.
arxiv-1602-04398 | Dimensionality Reduction for Nonlinear Regression with Two Predictor Vectors |  http://arxiv.org/abs/1602.04398  | author:Yanjun Li, Yoram Bresler category:stat.ML cs.IT cs.LG math.IT published:2016-02-13 summary:Many variables that we would like to predict depend nonlinearly on two typesof attributes. For example, prices are influenced by supply and demand. Movieratings are determined by demographic attributes and genre attributes. Thispaper addresses the dimensionality reduction problem in such regressionproblems with two predictor vectors. In particular, we assume a discriminativemodel where low-dimensional linear embeddings of the two predictor vectors aresufficient statistics for predicting a dependent variable. We show that asimple algorithm involving singular value decomposition can accurately estimatethe embeddings provided that certain sample complexities are satisfied,surprisingly, without specifying the nonlinear regression model. Theseembeddings improve the efficiency and robustness of subsequent training, andcan serve as a pre-training algorithm for neural networks. The main resultsestablish sample complexities under multiple settings. Sample complexities fordifferent regression models only differ by constant factors.
arxiv-1602-04287 | A Minimax Theory for Adaptive Data Analysis |  http://arxiv.org/abs/1602.04287  | author:Yu-Xiang Wang, Jing Lei, Stephen E. Fienberg category:stat.ML cs.LG published:2016-02-13 summary:In adaptive data analysis, the user makes a sequence of queries on the data,where at each step the choice of query may depend on the results in previoussteps. The releases are often randomized in order to reduce overfitting forsuch adaptively chosen queries. In this paper, we propose a minimax frameworkfor adaptive data analysis. Assuming Gaussianity of queries, we establish thefirst sharp minimax lower bound on the squared error in the order of$O(\frac{\sqrt{k}\sigma^2}{n})$, where $k$ is the number of queries asked, and$\sigma^2/n$ is the ordinary signal-to-noise ratio for a single query. Ourlower bound is based on the construction of an approximately least favorableadversary who picks a sequence of queries that are most likely to be affectedby overfitting. This approximately least favorable adversary uses only onelevel of adaptivity, suggesting that the minimax risk for 1-step adaptivitywith k-1 initial releases and that for $k$-step adaptivity are on the sameorder. The key technical component of the lower bound proof is a reduction tofinding the convoluting distribution that optimally obfuscates the sign of aGaussian signal. Our lower bound construction also reveals a transparent andelementary proof of the matching upper bound as an alternative approach toRusso and Zou (2015), who used information-theoretic tools to provide the sameupper bound. We believe that the proposed framework opens up opportunities toobtain theoretical insights for many other settings of adaptive data analysis,which would extend the idea to more practical realms.
arxiv-1602-04348 | Character Proposal Network for Robust Text Extraction |  http://arxiv.org/abs/1602.04348  | author:Shuye Zhang, Mude Lin, Tianshui Chen, Lianwen Jin, Liang Lin category:cs.CV published:2016-02-13 summary:Maximally stable extremal regions (MSER), which is a popular method togenerate character proposals/candidates, has shown superior performance inscene text detection. However, the pixel-level operation limits its capabilityfor handling some challenging cases (e.g., multiple connected characters,separated parts of one character and non-uniform illumination). To bettertackle these cases, we design a character proposal network (CPN) by takingadvantage of the high capacity and fast computing of fully convolutionalnetwork (FCN). Specifically, the network simultaneously predicts characternessscores and refines the corresponding locations. The characterness scores can beused for proposal ranking to reject non-character proposals and the refiningprocess aims to obtain the more accurate locations. Furthermore, consideringthe situation that different characters have different aspect ratios, wepropose a multi-template strategy, designing a refiner for each aspect ratio.The extensive experiments indicate our method achieves recall rates of 93.88%,93.60% and 96.46% on ICDAR 2013, SVT and Chinese2k datasets respectively usingless than 1000 proposals, demonstrating promising performance of our characterproposal network.
arxiv-1602-04358 | Machine olfaction using time scattering of sensor multiresolution graphs |  http://arxiv.org/abs/1602.04358  | author:Leonid Gugel, Yoel Shkolnisky, Shai Dekel category:cs.AI cs.DS stat.ML published:2016-02-13 summary:In this paper we construct a learning architecture for high dimensional timeseries sampled by sensor arrangements. Using a redundant wavelet decompositionon a graph constructed over the sensor locations, our algorithm is able toconstruct discriminative features that exploit the mutual information betweenthe sensors. The algorithm then applies scattering networks to the time seriesgraphs to create the feature space. We demonstrate our method on a machineolfaction problem, where one needs to classify the gas type and the locationwhere it originates from data sampled by an array of sensors. Our experimentalresults clearly demonstrate that our method outperforms classical machinelearning techniques used in previous studies.
arxiv-1602-04364 | Look, Listen and Learn - A Multimodal LSTM for Speaker Identification |  http://arxiv.org/abs/1602.04364  | author:Jimmy Ren, Yongtao Hu, Yu-Wing Tai, Chuan Wang, Li Xu, Wenxiu Sun, Qiong Yan category:cs.LG published:2016-02-13 summary:Speaker identification refers to the task of localizing the face of a personwho has the same identity as the ongoing voice in a video. This task not onlyrequires collective perception over both visual and auditory signals, therobustness to handle severe quality degradations and unconstrained contentvariations are also indispensable. In this paper, we describe a novelmultimodal Long Short-Term Memory (LSTM) architecture which seamlessly unifiesboth visual and auditory modalities from the beginning of each sequence input.The key idea is to extend the conventional LSTM by not only sharing weightsacross time steps, but also sharing weights across modalities. We show thatmodeling the temporal dependency across face and voice can significantlyimprove the robustness to content quality degradations and variations. We alsofound that our multimodal LSTM is robustness to distractors, namely thenon-speaking identities. We applied our multimodal LSTM to The Big Bang Theorydataset and showed that our system outperforms the state-of-the-art systems inspeaker identification with lower false alarm rate and higher recognitionaccuracy.
arxiv-1602-04278 | Signer-independent Fingerspelling Recognition with Deep Neural Network Adaptation |  http://arxiv.org/abs/1602.04278  | author:Taehwan Kim, Weiran Wang, Hao Tang, Karen Livescu category:cs.CL cs.CV cs.NE published:2016-02-13 summary:We study the problem of recognition of fingerspelled letter sequences inAmerican Sign Language in a signer-independent setting. Fingerspelled sequencesare both challenging and important to recognize, as they are used for manycontent words such as proper nouns and technical terms. Previous work has shownthat it is possible to achieve almost 90% accuracies on fingerspellingrecognition in a signer-dependent setting. However, the more realisticsigner-independent setting presents challenges due to significant variationsamong signers, coupled with the dearth of available training data. Weinvestigate this problem with approaches inspired by automatic speechrecognition. We start with the best-performing approaches from prior work,based on tandem models and segmental conditional random fields (SCRFs), withfeatures based on deep neural network (DNN) classifiers of letters andphonological features. Using DNN adaptation, we find that it is possible tobridge a large part of the gap between signer-dependent and signer-independentperformance. Using only about 115 transcribed words for adaptation from thetarget signer, we obtain letter accuracies of up to 82.7% with frame-leveladaptation labels and 69.7% with only word labels.
arxiv-1602-04277 | Evaluation of Protein Structural Models Using Random Forests |  http://arxiv.org/abs/1602.04277  | author:Renzhi Cao, Taeho Jo, Jianlin Cheng category:cs.LG q-bio.BM q-bio.QM stat.ML published:2016-02-13 summary:Protein structure prediction has been a grand challenge problem in thestructure biology over the last few decades. Protein quality assessment plays avery important role in protein structure prediction. In the paper, we propose anew protein quality assessment method which can predict both local and globalquality of the protein 3D structural models. Our method uses both multi andsingle model quality assessment method for global quality assessment, and useschemical, physical, geo-metrical features, and global quality score for localquality assessment. CASP9 targets are used to generate the features for localquality assessment. We evaluate the performance of our local quality assessmentmethod on CASP10, which is comparable with two stage-of-art QA methods based onthe average absolute distance between the real and predicted distance. Inaddition, we blindly tested our method on CASP11, and the good performanceshows that combining single and multiple model quality assessment method couldbe a good way to improve the accuracy of model quality assessment, and therandom forest technique could be used to train a good local quality assessmentmodel.
arxiv-1602-04391 | Constrained Multi-Slot Optimization for Ranking Recommendations |  http://arxiv.org/abs/1602.04391  | author:Kinjal Basu, Shaunak Chatterjee, Ankan Saha category:stat.ML math.OC stat.AP 90C20, 11K36 G.1.6 published:2016-02-13 summary:Ranking items to be recommended to users is one of the main problems in largescale social media applications. This problem can be set up as amulti-objective optimization problem to allow for trading off multiple,potentially conflicting objectives (that are driven by those items) againsteach other. Most previous approaches to this problem optimize for a single slotwithout considering the interaction effect of these items on one another. In this paper, we develop a constrained multi-slot optimization formulation,which allows for modeling interactions among the items on the different slots.We characterize the solution in terms of problem parameters and identifyconditions under which an efficient solution is possible. The problemformulation results in a quadratically constrained quadratic program (QCQP). Weprovide an algorithm that gives us an efficient solution by relaxing theconstraints of the QCQP minimally. Through simulated experiments, we show thebenefits of modeling interactions in a multi-slot ranking context, and thespeed and accuracy of our QCQP approximate solver against other state of theart methods.
arxiv-1602-04393 | Semantic Scan: Detecting Subtle, Spatially Localized Events in Text Streams |  http://arxiv.org/abs/1602.04393  | author:Abhinav Maurya, Kenton Murray, Yandong Liu, Chris Dyer, William W. Cohen, Daniel B. Neill category:cs.IR stat.ML published:2016-02-13 summary:Early detection and precise characterization of emerging topics in textstreams can be highly useful in applications such as timely and targeted publichealth interventions and discovering evolving regional business trends. Manymethods have been proposed for detecting emerging events in text streams usingtopic modeling. However, these methods have numerous shortcomings that makethem unsuitable for rapid detection of locally emerging events on massive textstreams. In this paper, we describe Semantic Scan (SS) that has been developedspecifically to overcome these shortcomings in detecting new spatially compactevents in text streams. Semantic Scan integrates novel contrastive topic modeling with onlinedocument assignment and principled likelihood ratio-based spatial scanning toidentify emerging events with unexpected patterns of keywords hidden in textstreams. This enables more timely and accurate detection and characterizationof anomalous, spatially localized emerging events. Semantic Scan does notrequire manual intervention or labeled training data, and is robust to noise inreal-world text data since it identifies anomalous text patterns that occur ina cluster of new documents rather than an anomaly in a single new document. We compare Semantic Scan to alternative state-of-the-art methods such asTopics over Time, Online LDA, and Labeled LDA on two real-world tasks: (i) adisease surveillance task monitoring free-text Emergency Department chiefcomplaints in Allegheny County, and (ii) an emerging business trend detectiontask based on Yelp reviews. On both tasks, we find that Semantic Scan providessignificantly better event detection and characterization accuracy thancompeting approaches, while providing up to an order of magnitude speedup.
arxiv-1602-04282 | Conservative Bandits |  http://arxiv.org/abs/1602.04282  | author:Yifan Wu, Roshan Shariff, Tor Lattimore, Csaba Szepesvári category:stat.ML cs.LG published:2016-02-13 summary:We study a novel multi-armed bandit problem that models the challenge facedby a company wishing to explore new strategies to maximize revenue whilstsimultaneously maintaining their revenue above a fixed baseline, uniformly overtime. While previous work addressed the problem under the weaker requirement ofmaintaining the revenue constraint only at a given fixed time in the future,the algorithms previously proposed are unsuitable due to their design under themore stringent constraints. We consider both the stochastic and the adversarialsettings, where we propose, natural, yet novel strategies and analyze the pricefor maintaining the constraints. Amongst other things, we prove both highprobability and expectation bounds on the regret, while we also consider boththe problem of maintaining the constraints with high probability orexpectation. For the adversarial setting the price of maintaining theconstraint appears to be higher, at least for the algorithm considered. A lowerbound is given showing that the algorithm for the stochastic setting is almostoptimal. Empirical results obtained in synthetic environments complement ourtheoretical findings.
arxiv-1602-04283 | Deep Learning on FPGAs: Past, Present, and Future |  http://arxiv.org/abs/1602.04283  | author:Griffin Lacey, Graham W. Taylor, Shawki Areibi category:cs.DC cs.LG stat.ML published:2016-02-13 summary:The rapid growth of data size and accessibility in recent years hasinstigated a shift of philosophy in algorithm design for artificialintelligence. Instead of engineering algorithms by hand, the ability to learncomposable systems automatically from massive amounts of data has led toground-breaking performance in important domains such as computer vision,speech recognition, and natural language processing. The most popular class oftechniques used in these domains is called deep learning, and is seeingsignificant attention from industry. However, these models require incredibleamounts of data and compute power to train, and are limited by the need forbetter hardware acceleration to accommodate scaling beyond current data andmodel sizes. While the current solution has been to use clusters of graphicsprocessing units (GPU) as general purpose processors (GPGPU), the use of fieldprogrammable gate arrays (FPGA) provide an interesting alternative. Currenttrends in design tools for FPGAs have made them more compatible with thehigh-level software practices typically practiced in the deep learningcommunity, making FPGAs more accessible to those who build and deploy models.Since FPGA architectures are flexible, this could also allow researchers theability to explore model-level optimizations beyond what is possible on fixedarchitectures such as GPUs. As well, FPGAs tend to provide high performance perwatt of power consumption, which is of particular importance for applicationscientists interested in large scale server-based deployment orresource-limited embedded applications. This review takes a look at deeplearning and FPGAs from a hardware acceleration perspective, identifying trendsand innovations that make these technologies a natural fit, and motivates adiscussion on how FPGAs may best serve the needs of the deep learning communitymoving forward.
arxiv-1602-04302 | Convex Optimization for Linear Query Processing under Approximate Differential Privacy |  http://arxiv.org/abs/1602.04302  | author:Ganzhao Yuan, Yin Yang, Zhenjie Zhang, Zhifeng Hao category:cs.DB cs.LG stat.ML published:2016-02-13 summary:Differential privacy enables organizations to collect accurate aggregatesover sensitive data with strong, rigorous guarantees on individuals' privacy.Previous work has found that under differential privacy, computing multiplecorrelated aggregates as a batch, using an appropriate \emph{strategy}, mayyield higher accuracy than computing each of them independently. However,finding the best strategy that maximizes result accuracy is non-trivial, as itinvolves solving a complex constrained optimization program that appears to benon-linear and non-convex. Hence, in the past much effort has been devoted insolving this non-convex optimization program. Existing approaches includevarious sophisticated heuristics and expensive numerical solutions. None ofthem, however, guarantees to find the optimal solution of this optimizationproblem. This paper points out that under ($\epsilon$, $\delta$)-differential privacy,the optimal solution of the above constrained optimization problem in search ofa suitable strategy can be found, rather surprisingly, by solving a simple andelegant convex optimization program. Then, we propose an efficient algorithmbased on Newton's method, which we prove to always converge to the optimalsolution with linear global convergence rate and quadratic local convergencerate. Empirical evaluations demonstrate the accuracy and efficiency of theproposed solution.
arxiv-1602-03943 | Second Order Stochastic Optimization in Linear Time |  http://arxiv.org/abs/1602.03943  | author:Naman Agarwal, Brian Bullins, Elad Hazan category:stat.ML cs.LG published:2016-02-12 summary:Stochastic optimization and, in particular, first-order stochastic methodsare a cornerstone of modern machine learning due to their extremely efficientper-iteration computational cost. Second-order methods, while able to providefaster per-iteration convergence, have been much less explored due to the highcost of computing the second-order information. In this paper we develop asecond-order stochastic method for optimization problems arising in machinelearning based on novel matrix randomization techniques that match theper-iteration cost of gradient descent, yet enjoy the linear-convergenceproperties of second-order optimization. We also consider the special case ofself-concordant functions where we show that a first order method can achievelinear convergence with guarantees independent of the condition number. Wedemonstrate significant speedups for training linear classifiers over severalconvex benchmarks.
arxiv-1602-04265 | Regularized Estimation in High Dimensional Time Series under Mixing Conditions |  http://arxiv.org/abs/1602.04265  | author:Kam Chung Wong, Ambuj Tewari, Zifan Li category:stat.ML cs.LG published:2016-02-12 summary:The Lasso is one of the most popular methods in high dimensional statisticallearning. Most existing theoretical results for the Lasso, however, require thesamples to be iid. Recent work has provided guarantees for the Lasso assumingthat the time series is generated by a sparse Vector Auto-Regressive (VAR)model with Gaussian innovations. Proofs of these results rely critically on thefact that the true data generating mechanism (DGM) is a finite-order GaussianVAR. This assumption is quite brittle: linear transformations, includingselecting a subset of variables, can lead to the violation of this assumption.In order to break free from such assumptions, we derive nonasymptoticinequalities for estimation error and prediction error of the Lasso estimate ofthe best linear predictor without assuming any special parametric form of theDGM. Instead, we rely only on (strict) stationarity and mixing conditions toestablish consistency of the Lasso in the following two scenarios: (a)alpha-mixing Gaussian processes, and (b) beta-mixing sub-Gaussian randomvectors. Our work provides an alternative proof of the consistency of the Lassofor sparse Gaussian VAR models. But the applicability of our results extends tonon-Gaussian and non-linear times series models as the examples we providedemonstrate. In order to prove our results, we derive a novel Hanson-Wrighttype concentration inequality for beta-mixing sub-Gaussian random vectors thatmay be of independent interest.
arxiv-1602-04227 | Scale-free network optimization: foundations and algorithms |  http://arxiv.org/abs/1602.04227  | author:Patrick Rebeschini, Sekhar Tatikonda category:stat.ML math.OC published:2016-02-12 summary:We investigate the fundamental principles that drive the development ofscalable algorithms for network optimization. Despite the significant amount ofwork on parallel and decentralized algorithms in the optimization community,the methods that have been proposed typically rely on strict separabilityassumptions for objective function and constraints. Beside sparsity, thesemethods typically do not exploit the strength of the interaction betweenvariables in the system. We propose a notion of correlation in constrainedoptimization that is based on the sensitivity of the optimal solution uponperturbations of the constraints. We develop a general theory of sensitivity ofoptimizers the extends beyond the infinitesimal setting. We present instancesin network optimization where the correlation decays exponentially fast withrespect to the natural distance in the network, and we design algorithms thatcan exploit this decay to yield dimension-free optimization. Our results arethe first of their kind, and open new possibilities in the theory of localalgorithms.
arxiv-1602-04105 | Convolutional Radio Modulation Recognition Networks |  http://arxiv.org/abs/1602.04105  | author:Timothy J O'Shea, Johnathan Corgan, T. Charles Clancy category:cs.LG cs.CV published:2016-02-12 summary:We study the adaptation of convolutional neural networks to the complextemporal radio signal domain. We compare the efficacy of radio modulationclassification using naively learned features against using expert features,which are currently used widely and well regarded in the field and we showsignificant performance improvements. We show that blind temporal learning onlarge and densely encoded time series using deep convolutional neural networksis viable and a strong candidate approach for this task.
arxiv-1602-04259 | A Minimalistic Approach to Sum-Product Network Learning for Real Applications |  http://arxiv.org/abs/1602.04259  | author:Viktoriya Krakovna, Moshe Looks category:cs.AI cs.LG stat.ML published:2016-02-12 summary:Sum-Product Networks (SPNs) are a class of expressive yet tractablehierarchical graphical models. LearnSPN is a structure learning algorithm forSPNs that uses hierarchical co-clustering to simultaneously identifying similarentities and similar features. The original LearnSPN algorithm assumes that allthe variables are discrete and there is no missing data. We introduce apractical, simplified version of LearnSPN, MiniSPN, that runs faster and canhandle missing data and heterogeneous features common in real applications. Wedemonstrate the performance of MiniSPN on standard benchmark datasets and ontwo datasets from Google's Knowledge Graph exhibiting high missingness ratesand a mix of discrete and continuous features.
arxiv-1602-03930 | Global Deconvolutional Networks for Semantic Segmentation |  http://arxiv.org/abs/1602.03930  | author:Vladimir Nekrasov, Janghoon Ju, Jaesik Choi category:cs.CV published:2016-02-12 summary:Semantic image segmentation is an important low-level computer vision problemaimed to correctly classify each individual pixel of the image. Recentempirical improvements achieved in this area have primarily been motivated bysuccessful exploitation of Convolutional Neural Networks (CNNs) pre-trained forimage classification and object recognition tasks. However, the pixel-wiselabeling with CNNs has its own unique challenges: (1) an accuratedeconvolution, or upsampling, of low-resolution output into a higher-resolutionsegmentation mask and (2) an inclusion of global information, or context,within locally extracted features. To address these issues, we propose a novelarchitecture to conduct the deconvolution operation and acquire densepredictions, and an additional refinement, which allows to incorporate globalinformation into the network. We demonstrate that these alterations lead toimproved performance of state-of-the-art semantic segmentation models on thePASCAL VOC 2012 benchmark.
arxiv-1602-03935 | Face Attribute Prediction Using Off-The-Shelf Deep Learning Networks |  http://arxiv.org/abs/1602.03935  | author:Yang Zhong, Josephine Sullivan, Haibo Li category:cs.CV published:2016-02-12 summary:Attribute prediction from face images in the wild is a challenging problem.To automatically describe face attributes from face containing images,traditionally one needs to cascade three technical blocks --- facelocalization, facial feature extraction, and classification --- in a pipeline.As a typical classification problem, face attribute prediction has beenaddressed by using deep learning networks. Current state-of-the-art performancewas achieved by using two cascaded CNNs, which were specifically trained tolearn face localization and facial attribute prediction. In this paper weexperiment in an alternative way of exploring the power of deep representationsfrom the networks: we employ off-the-shelf CNNs trained for face recognitiontasks to do facial feature extraction, combined with conventional facelocalization techniques. Recognizing that the describable face attributes arediverse, we select representations from different levels of the CNNs andinvestigate their utilities for attribute classification. Experiments on twolarge datasets, LFWA and CeleA, show that the performance is totally comparableto state-of-the-art approach. Our findings suggest two potentially importantquestions in using CNNs for face attribute prediction: 1) How to maximallyleverage the power of CNN representations. 2) How to best combine traditionalcomputer vision techniques with deep learning networks.
arxiv-1602-03950 | General Vector Machine |  http://arxiv.org/abs/1602.03950  | author:Hong Zhao category:stat.ML cs.LG published:2016-02-12 summary:The support vector machine (SVM) is an important class of learning machinesfor function approach, pattern recognition, and time-serious prediction, etc.It maps samples into the feature space by so-called support vectors of selectedsamples, and then feature vectors are separated by maximum margin hyperplane.The present paper presents the general vector machine (GVM) to replace the SVM.The support vectors are replaced by general project vectors selected from theusual vector space, and a Monte Carlo (MC) algorithm is developed to find thegeneral vectors. The general project vectors improves the feature-extractionability, and the MC algorithm can control the width of the separation margin ofthe hyperplane. By controlling the separation margin, we show that the maximummargin hyperplane can usually induce the overlearning, and the best learningmachine is achieved with a proper separation margin. Applications in functionapproach, pattern recognition, and classification indicate that the developedmethod is very successful, particularly for small-set training problems.Additionally, our algorithm may induce some particular applications, such asfor the transductive inference.
arxiv-1602-03960 | TabMCQ: A Dataset of General Knowledge Tables and Multiple-choice Questions |  http://arxiv.org/abs/1602.03960  | author:Sujay Kumar Jauhar, Peter Turney, Eduard Hovy category:cs.CL published:2016-02-12 summary:We describe two new related resources that facilitate modelling of generalknowledge reasoning in 4th grade science exams. The first is a collection ofcurated facts in the form of tables, and the second is a large set ofcrowd-sourced multiple-choice questions covering the facts in the tables.Through the setup of the crowd-sourced annotation task we obtain implicitalignment information between questions and tables. We envisage that theresources will be useful not only to researchers working on question answering,but also to people investigating a diverse range of other applications such asinformation extraction, question parsing, answer type identification, andlexical semantic modelling.
arxiv-1602-03992 | Orthogonal Sparse PCA and Covariance Estimation via Procrustes Reformulation |  http://arxiv.org/abs/1602.03992  | author:Konstantinos Benidis, Ying Sun, Prabhu Babu, Daniel P. Palomar category:stat.ML cs.LG math.OC stat.AP published:2016-02-12 summary:The problem of estimating sparse eigenvectors of a symmetric matrix attractsa lot of attention in many applications, especially those with high dimensionaldata set. While classical eigenvectors can be obtained as the solution of amaximization problem, existing approaches formulate this problem by adding apenalty term into the objective function that encourages a sparse solution.However, the resulting methods achieve sparsity at the expense of sacrificingthe orthogonality property. In this paper, we develop a new method to estimatedominant sparse eigenvectors without trading off their orthogonality. Theproblem is highly non-convex and hard to handle. We apply the MM frameworkwhere we iteratively maximize a tight lower bound (surrogate function) of theobjective function over the Stiefel manifold. The inner maximization problemturns out to be a rectangular Procrustes problem, which has a closed formsolution. In addition, we propose a method to improve the covariance estimationproblem when its underlying eigenvectors are known to be sparse. We use theeigenvalue decomposition of the covariance matrix to formulate an optimizationproblem where we impose sparsity on the corresponding eigenvectors. Numericalexperiments show that the proposed eigenvector extraction algorithm matches oroutperforms existing algorithms in terms of support recovery and explainedvariance, while the covariance estimation algorithms improve significantly thesample covariance estimator.
arxiv-1602-03995 | An automatic method for segmentation of fission tracks in epidote crystal photomicrographs |  http://arxiv.org/abs/1602.03995  | author:Alexandre Fioravante de Siqueira, Wagner Massayuki Nakasuga, Aylton Pagamisse, Carlos Alberto Tello Saenz, Aldo Eloizo Job category:cs.CV 65T60 published:2016-02-12 summary:Manual identification of fission tracks has practical problems, such asvariation due to observer-observation efficiency. An automatic processingmethod that could identify fission tracks in a photomicrograph could solve thisproblem and improve the speed of track counting. However, separation ofnon-trivial images is one of the most difficult tasks in image processing.Several commercial and free softwares are available, but these softwares aremeant to be used in specific images. In this paper, an automatic method basedon starlet wavelets is presented in order to separate fission tracks in mineralphotomicrographs. Automatization is obtained by Matthews correlationcoefficient, and results are evaluated by precision, recall and accuracy. Thistechnique is an improvement of a method aimed at segmentation of scanningelectron microscopy images. This method is applied in photomicrographs ofepidote phenocrystals, in which accuracy higher than 89% was obtained infission track segmentation, even for difficult images. Algorithms correspondingto the proposed method are available for download. Using the method presentedhere, an user could easily determine fission tracks in photomicrographs ofmineral samples.
arxiv-1602-03990 | Efficient functional ANOVA through wavelet-domain Markov groves |  http://arxiv.org/abs/1602.03990  | author:Li Ma, Jacopo Soriano category:stat.ME stat.CO stat.ML published:2016-02-12 summary:We introduce a wavelet-domain functional analysis of variance (fANOVA) methodbased on a Bayesian hierarchical model. The factor effects are modeled througha spike-and-slab mixture at each location-scale combination along with anormal-inverse-Gamma (NIG) conjugate setup for the coefficients and errors. Agraphical model called the Markov grove (MG) is designed to jointly model thespike-and-slab statuses at all location-scale combinations, which incorporatesthe clustering of each factor effect in the wavelet-domain thereby allowingborrowing of strength across location and scale. The posterior of this NIG-MGmodel is analytically available through a pyramid algorithm of the samecomputational complexity as Mallat's pyramid algorithm for discrete wavelettransform, i.e., linear in both the number of observations and the number oflocations. Posterior probabilities of factor contributions can also be computedthrough pyramid recursion, and exact samples from the posterior can be drawnwithout MCMC. We investigate the performance of our method through extensivesimulation and show that it outperforms existing wavelet-domain fANOVA methodsin a variety of common settings. We apply the method to analyzing the orthosisdata.
arxiv-1602-04709 | Identifying Structures in Social Conversations in NSCLC Patients through the Semi-Automatic extraction of Topical Taxonomies |  http://arxiv.org/abs/1602.04709  | author:Giancarlo Crocetti, Amir A. Delay, Fatemeh Seyedmendhi category:cs.IR cs.AI cs.CL H.3.1; H.3.3 published:2016-02-12 summary:The exploration of social conversations for addressing patient's needs is animportant analytical task in which many scholarly publications are contributingto fill the knowledge gap in this area. The main difficulty remains theinability to turn such contributions into pragmatic processes thepharmaceutical industry can leverage in order to generate insight from socialmedia data, which can be considered as one of the most challenging source ofinformation available today due to its sheer volume and noise. This study isbased on the work by Scott Spangler and Jeffrey Kreulen and applies it toidentify structure in social media through the extraction of a topical taxonomyable to capture the latent knowledge in social conversations in health-relatedsites. The mechanism for automatically identifying and generating a taxonomyfrom social conversations is developed and pressured tested using public datafrom media sites focused on the needs of cancer patients and their families.Moreover, a novel method for generating the category's label and thedetermination of an optimal number of categories is presented which extendsScott and Jeffrey's research in a meaningful way. We assume the reader isfamiliar with taxonomies, what they are and how they are used.
arxiv-1602-04186 | An Evolutionary Strategy based on Partial Imitation for Solving Optimization Problems |  http://arxiv.org/abs/1602.04186  | author:Marco Alberto Javarone category:cs.NE math.OC published:2016-02-12 summary:In this work we introduce an evolutionary strategy to solve optimizationtasks. In particular, we focus on the Travel Salesman Problem (TSP), i.e., aNP-hard problem with a discrete search space. The solutions of the TSP can becodified by arrays of cities, and can be evaluated by a fitness computedaccording to a cost function (e.g., the length of a path). Our method is basedon the evolution of an agent population by means of a `partial imitation'mechanism. In particular, agents receive a random solution and then,interacting among themselves, imitate the solutions of agents with a higherfitness. Moreover, as stated above, the imitation is only partial, i.e., agentscopy only one, randomly chosen, entry of better (array) solutions. In doing so,the population converges towards a shared solution, behaving like a spin systemundergoing a cooling process, i.e., driven towards an ordered phase. Wehighlight that the adopted `partial imitation' mechanism allows the populationto generate new solutions over time, before reaching the final equilibrium.Remarkably, results of numerical simulations show that our method is able tofind the optimal solution in all considered search spaces.
arxiv-1602-04208 | Pursuits in Structured Non-Convex Matrix Factorizations |  http://arxiv.org/abs/1602.04208  | author:Rajiv Khanna, Michael Tschannen, Martin Jaggi category:cs.LG stat.ML published:2016-02-12 summary:Efficiently representing real world data in a succinct and parsimoniousmanner is of central importance in many fields. We present a generalized greedypursuit framework, allowing us to efficiently solve structured matrixfactorization problems, where the factors are allowed to be from arbitrary setsof structured vectors. Such structure may include sparsity, non-negativeness,order, or a combination thereof. The algorithm approximates a given matrix by alinear combination of few rank-1 matrices, each factorized into an outerproduct of two vector atoms of the desired structure. For the non-convexsubproblems of obtaining good rank-1 structured matrix atoms, we employ andanalyze a general atomic power method. In addition to the above applications,we prove linear convergence for generalized pursuit variants in Hilbert spaces- for the task of approximation over the linear span of arbitrary dictionaries- which generalizes OMP and is useful beyond matrix problems. Our experimentson real datasets confirm both the efficiency and also the broad applicabilityof our framework in practice.
arxiv-1602-04133 | Deep Gaussian Processes for Regression using Approximate Expectation Propagation |  http://arxiv.org/abs/1602.04133  | author:Thang D. Bui, Daniel Hernández-Lobato, Yingzhen Li, José Miguel Hernández-Lobato, Richard E. Turner category:stat.ML cs.LG published:2016-02-12 summary:Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisationsof Gaussian processes (GPs) and are formally equivalent to neural networks withmultiple, infinitely wide hidden layers. DGPs are nonparametric probabilisticmodels and as such are arguably more flexible, have a greater capacity togeneralise, and provide better calibrated uncertainty estimates thanalternative deep models. This paper develops a new approximate Bayesianlearning scheme that enables DGPs to be applied to a range of medium to largescale regression problems for the first time. The new method uses anapproximate Expectation Propagation procedure and a novel and efficientextension of the probabilistic backpropagation algorithm for learning. Weevaluate the new method for non-linear regression on eleven real-worlddatasets, showing that it always outperforms GP regression and is almost alwaysbetter than state-of-the-art deterministic and sampling-based approximateinference methods for Bayesian neural networks. As a by-product, this workprovides a comprehensive analysis of six approximate Bayesian methods fortraining neural networks.
arxiv-1602-04129 | Learning may need only a few bits of synaptic precision |  http://arxiv.org/abs/1602.04129  | author:Carlo Baldassi, Federica Gerace, Carlo Lucibello, Luca Saglietti, Riccardo Zecchina category:q-bio.NC stat.ML published:2016-02-12 summary:Learning in neural networks poses peculiar challenges when using discretizedrather then continuous synaptic states. The choice of discrete synapses ismotivated by biological reasoning and experiments, and possibly by hardwareimplementation considerations as well. In this paper we extend a previous largedeviations analysis which unveiled the existence of peculiar dense regions inthe space of synaptic states which accounts for the possibility of learningefficiently in networks with binary synapses. We extend the analysis tosynapses with multiple states and generally more plausible biological features.The results clearly indicate that the overall qualitative picture is unchangedwith respect to the binary case, and very robust to variation of the details ofthe model. We also provide quantitative results which suggest that using fewsynaptic states is convenient for practical applications, consistently withrecent biological results.
arxiv-1602-04128 | From Coin Betting to Parameter-Free Online Learning |  http://arxiv.org/abs/1602.04128  | author:Francesco Orabona, Dávid Pál category:cs.LG published:2016-02-12 summary:In the recent years a number of parameter-free algorithms for online linearoptimization over Hilbert spaces and for learning with expert advice have beendeveloped. While these two families of algorithms might seem different to adistract eye, the proof methods are indeed very similar, making the readerwonder if such a connection is only accidental. In this paper, we unify these two families, showing that both can beinstantiated from online coin betting algorithms. We present two new reductionsfrom online coin betting to online linear optimization over Hilbert spaces andto learning with expert advice. We instantiate our framework using a bettingalgorithm based on the Krichevsky-Trofimov estimator. We obtain a simplealgorithm for online linear optimization over any Hilbert space with$O(\norm{u}\sqrt{T \log(1+T \norm{u}}))$ regret with respect to any competitor$u$. For learning with expert advice we obtain an algorithm that has $O(\sqrt{T(1 + \KL{u}{\pi})})$ regret against any competitor $u$ and where $\KL{u}{\pi}$is the Kullback-Leibler divergence between algorithm's prior distribution $\pi$and the competitor. In both cases, no parameters need to be tuned.
arxiv-1602-04124 | Fast and Robust Hand Tracking Using Detection-Guided Optimization |  http://arxiv.org/abs/1602.04124  | author:Srinath Sridhar, Franziska Mueller, Antti Oulasvirta, Christian Theobalt category:cs.CV published:2016-02-12 summary:Markerless tracking of hands and fingers is a promising enabler forhuman-computer interaction. However, adoption has been limited because oftracking inaccuracies, incomplete coverage of motions, low framerate, complexcamera setups, and high computational requirements. In this paper, we present afast method for accurately tracking rapid and complex articulations of the handusing a single depth camera. Our algorithm uses a novel detection-guidedoptimization strategy that increases the robustness and speed of poseestimation. In the detection step, a randomized decision forest classifiespixels into parts of the hand. In the optimization step, a novel objectivefunction combines the detected part labels and a Gaussian mixturerepresentation of the depth to estimate a pose that best fits the depth. Ourapproach needs comparably less computational resources which makes it extremelyfast (50 fps without GPU support). The approach also supports varying static,or moving, camera-to-scene arrangements. We show the benefits of our method byevaluating on public datasets and comparing against previous work.
arxiv-1602-04101 | An Empirical Study on Academic Commentary and Its Implications on Reading and Writing |  http://arxiv.org/abs/1602.04101  | author:Tai Wang, Xiangen Hu, Keith Shubeck, Zhiqiang Cai, Jie Tang category:cs.CY cs.CL published:2016-02-12 summary:The relationship between reading and writing (RRW) is one of the major themesin learning science. One of its obstacles is that it is difficult to define ormeasure the latent background knowledge of the individual. However, in anacademic research setting, scholars are required to explicitly list theirbackground knowledge in the citation sections of their manuscripts. This uniqueopportunity was taken advantage of to observe RRW, especially in the publishedacademic commentary scenario. RRW was visualized under a proposed topic processmodel by using a state of the art version of latent Dirichlet allocation (LDA).The empirical study showed that the academic commentary is modulated both byits target paper and the author's background knowledge. Although thisconclusion was obtained in a unique environment, we suggest its implicationscan also shed light on other similar interesting areas, such as dialog andconversation, group discussion, and social media.
arxiv-1602-04062 | Using Deep Q-Learning to Control Optimization Hyperparameters |  http://arxiv.org/abs/1602.04062  | author:Samantha Hansen category:math.OC cs.LG published:2016-02-12 summary:We present a novel definition of the reinforcement learning state, actionsand reward function that allows a deep Q-network (DQN) to learn to control anoptimization hyperparameter. Using Q-learning with experience replay, we traintwo DQNs to accept a state representation of an objective function as input andoutput the expected discounted return of rewards, or q-values, connected to theactions of either adjusting the learning rate or leaving it unchanged. The twoDQNs learn a policy similar to a line search, but differ in the number ofallowed actions. The trained DQNs in combination with a gradient-based updateroutine form the basis of the Q-gradient descent algorithms. To demonstrate theviability of this framework, we show that the DQN's q-values associated withoptimal action converge and that the Q-gradient descent algorithms outperformgradient descent with an Armijo or nonomonotone line search. Unlike traditionaloptimization methods, Q-gradient descent can incorporate any objectivestatistic and by varying the actions we gain insight into the type of learningrate adjustment strategies that are successful for neural network optimization.
arxiv-1602-04052 | Image Restoration and Reconstruction using Variable Splitting and Class-adapted Image Priors |  http://arxiv.org/abs/1602.04052  | author:Afonso M. Teodoro, José M. Bioucas-Dias, Mário A. T. Figueiredo category:cs.CV I.4.5; I.4.4 published:2016-02-12 summary:This paper proposes using a Gaussian mixture model as a prior, for solvingtwo image inverse problems, namely image deblurring and compressive imaging. Wecapitalize on the fact that variable splitting algorithms, like ADMM, are ableto decouple the handling of the observation operator from that of theregularizer, and plug a state-of-the-art algorithm into the pure denoisingstep. Furthermore, we show that, when applied to a specific type of image, aGaussian mixture model trained from an database of images of the same type isable to outperform current state-of-the-art methods.
arxiv-1602-03861 | Statistical Foundation of Spectral Graph Theory |  http://arxiv.org/abs/1602.03861  | author:Subhadeep Mukhopadhyay category:math.ST stat.ME stat.ML stat.TH published:2016-02-11 summary:Spectral graph theory is undoubtedly the most favored graph data analysistechnique, both in theory and practice. It has emerged as a versatile tool fora wide variety of applications including data mining, web search, quantumcomputing, computer vision, image segmentation, and among others. However, theway in which spectral graph theory is currently taught and practiced is rathermechanical, consisting of a series of matrix calculations that at first glanceseem to have very little to do with statistics, thus posing a seriouslimitation to our understanding of graph problems from a statisticalperspective. Our work is motivated by the following question: How can wedevelop a general statistical foundation of "spectral heuristics" that avoidsthe cookbook mechanical approach? A unified method is proposed that permitsfrequency analysis of graphs from a nonparametric perspective by viewing it asfunction estimation problem. We show that the proposed formalism incorporatesseemingly unrelated spectral modeling tools (e.g., Laplacian, modularity,regularized Laplacian, etc.) under a single general method, thus providingbetter fundamental understanding. It is the purpose of this paper to bridge thegap between two spectral graph modeling cultures: Statistical theory (based onnonparametric function approximation and smoothing methods) and Algorithmiccomputing (based on matrix theory and numerical linear algebra basedtechniques) to provide transparent and complementary insight into graphproblems.
arxiv-1602-03619 | Optimality of Belief Propagation for Crowdsourced Classification |  http://arxiv.org/abs/1602.03619  | author:Jungseul Ok, Sewoong Oh, Jinwoo Shin, Yung Yi category:cs.LG stat.ML published:2016-02-11 summary:Crowdsourcing systems are popular for solving large-scale labelling taskswith low-paid (or even non-paid) workers. We study the problem of recoveringthe true labels from the possibly erroneous crowdsourced labels under thepopular Dawid-Skene model. To address this inference problem, severalalgorithms have recently been proposed, but the best known guarantee is stillsignificantly larger than the fundamental limit. We close this gap under asimple but canonical scenario where each worker is assigned at most two tasks.In particular, we introduce a tighter lower bound on the fundamental limit andprove that Belief Propagation (BP) exactly matches this lower bound. Theguaranteed optimality of BP is the strongest in the sense that it isinformation-theoretically impossible for any other algorithm to correctly labela larger fraction of the tasks. In the general setting, when more than twotasks are assigned to each worker, we establish the dominance result on BP thatit outperforms all existing algorithms with provable guarantees. Experimentalresults suggest that BP is close to optimal for all regimes considered, whileall other algorithms show suboptimal performances in certain regimes.
arxiv-1602-03616 | Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks |  http://arxiv.org/abs/1602.03616  | author:Anh Nguyen, Jason Yosinski, Jeff Clune category:cs.NE cs.CV published:2016-02-11 summary:We can better understand deep neural networks by identifying which featureseach of their neurons have learned to detect. To do so, researchers havecreated Deep Visualization techniques including activation maximization, whichsynthetically generates inputs (e.g. images) that maximally activate eachneuron. A limitation of current techniques is that they assume each neurondetects only one type of feature, but we know that neurons can be multifaceted,in that they fire in response to many different types of features: for example,a grocery store class neuron must activate either for rows of produce or for astorefront. Previous activation maximization techniques constructed imageswithout regard for the multiple different facets of a neuron, creatinginappropriate mixes of colors, parts of objects, scales, orientations, etc.Here, we introduce an algorithm that explicitly uncovers the multiple facets ofeach neuron by producing a synthetic visualization of each of the types ofimages that activate a neuron. We also introduce regularization methods thatproduce state-of-the-art results in terms of the interpretability of imagesobtained by activation maximization. By separately synthesizing each type ofimage a neuron fires in response to, the visualizations have more appropriatecolors and coherent global structure. Multifaceted feature visualization thusprovides a clearer and more comprehensive description of the role of eachneuron.
arxiv-1602-03585 | Generating Discriminative Object Proposals via Submodular Ranking |  http://arxiv.org/abs/1602.03585  | author:Yangmuzi Zhang, Zhuolin Jiang, Xi Chen, Larry S. Davis category:cs.CV published:2016-02-11 summary:A multi-scale greedy-based object proposal generation approach is presented.Based on the multi-scale nature of objects in images, our approach is built ontop of a hierarchical segmentation. We first identify the representative anddiverse exemplar clusters within each scale by using a diversity rankingalgorithm. Object proposals are obtained by selecting a subset from themulti-scale segment pool via maximizing a submodular objective function, whichconsists of a weighted coverage term, a single-scale diversity term and amulti-scale reward term. The weighted coverage term forces the selected set ofobject proposals to be representative and compact; the single-scale diversityterm encourages choosing segments from different exemplar clusters so that theywill cover as many object patterns as possible; the multi-scale reward termencourages the selected proposals to be discriminative and selected frommultiple layers generated by the hierarchical image segmentation. Theexperimental results on the Berkeley Segmentation Dataset and PASCAL VOC2012segmentation dataset demonstrate the accuracy and efficiency of our objectproposal model. Additionally, we validate our object proposals in simultaneoussegmentation and detection and outperform the state-of-art performance.
arxiv-1602-03822 | Neural Network Support Vector Detection via a Soft-Label, Hybrid K-Means Classifier |  http://arxiv.org/abs/1602.03822  | author:Robert A. Murphy category:cs.LG 60D05, 62C99 published:2016-02-11 summary:We use random geometric graphs to describe clusters of higher dimensionaldata points which are bijectively mapped to a (possibly) lower dimensionalspace where an equivalent random cluster model is used to calculate theexpected number of modes to be found when separating the data of a multi-modaldata set into distinct clusters. Furthermore, as a function of the expectednumber of modes and the number of data points in the sample, an upper bound ona given distance measure is found such that data points have the greatestcorrelation if their mutual distances from a common center is less than orequal to the calculated bound. Anomalies are exposed, which lie outside of theunion of all regularized clusters of data points. Similar to finding a hyperplane which can be shifted along its normal toexpose the maximal distance between binary classes, it is shown that the unionof regularized clusters can be used to define a hyperplane which can be shiftedby a certain amount to separate the data into binary classes and that theshifted hyperplane defines the activation function for a two-classdiscriminating neural network. Lastly, this neural network is used to detectthe set of support vectors which determines the maximally-separating regionbetween the binary classes.
arxiv-1602-03600 | Data-Driven Online Decision Making with Costly Information Acquisition |  http://arxiv.org/abs/1602.03600  | author:Onur Atan, William Whoiles, Mihaela van der Schaar category:stat.ML cs.LG published:2016-02-11 summary:Existing work on online learning for decision making takes the informationavailable as a given and focuses solely on choosing best actions given thisinformation. Instead, in this paper, the decision maker needs to simultaneouslylearn both what decisions to make and what source(s) of contextual informationto gather data from in order to inform its decisions such that its reward ismaximized. We propose algorithms that obtain costly source(s) of contextualinformation over time, while simultaneously learning what actions to take basedon the contextual information revealed by the selected source(s). We prove thatour algorithms achieve regret that is logarithmic in time. We demonstrate theperformance of our algorithms using a medical dataset. The proposed algorithmcan be applied in many applications including clinical decision assist systemsfor medical diagnosis, recommender systems, actionable intelligence etc., whereobserving the complete information in every instance or consulting all theavailable sources to gather intelligence before making decisions is costly.
arxiv-1602-03606 | Variations of the Similarity Function of TextRank for Automated Summarization |  http://arxiv.org/abs/1602.03606  | author:Federico Barrios, Federico López, Luis Argerich, Rosa Wachenchauzer category:cs.CL cs.IR I.2.7 published:2016-02-11 summary:This article presents new alternatives to the similarity function for theTextRank algorithm for automatic summarization of texts. We describe thegeneralities of the algorithm and the different functions we propose. Some ofthese variants achieve a significative improvement using the same metrics anddataset as the original publication.
arxiv-1602-03609 | Attentive Pooling Networks |  http://arxiv.org/abs/1602.03609  | author:Cicero dos Santos, Ming Tan, Bing Xiang, Bowen Zhou category:cs.CL cs.LG published:2016-02-11 summary:In this work, we propose Attentive Pooling (AP), a two-way attentionmechanism for discriminative model training. In the context of pair-wiseranking or classification with neural networks, AP enables the pooling layer tobe aware of the current input pair, in a way that information from the twoinput items can directly influence the computation of each other'srepresentations. Along with such representations of the paired inputs, APjointly learns a similarity measure over projected segments (e.g. trigrams) ofthe pair, and subsequently, derives the corresponding attention vector for eachinput to guide the pooling. Our two-way attention mechanism is a generalframework independent of the underlying representation learning, and it hasbeen applied to both convolutional neural networks (CNNs) and recurrent neuralnetworks (RNNs) in our studies. The empirical results, from three verydifferent benchmark tasks of question answering/answer selection, demonstratethat our proposed models outperform a variety of strong baselines and achievestate-of-the-art performance in all the benchmarks.
arxiv-1602-03670 | Online Low-Rank Subspace Learning from Incomplete Data: A Bayesian View |  http://arxiv.org/abs/1602.03670  | author:Paris V. Giampouras, Athanasios A. Rontogiannis, Konstantinos E. Themelis, Konstantinos D. Koutroumbas category:stat.ML published:2016-02-11 summary:Extracting the underlying low-dimensional space where high-dimensionalsignals often reside has long been at the center of numerous algorithms in thesignal processing and machine learning literature during the past few decades.At the same time, working with incomplete (partly observed) large scaledatasets has recently been commonplace for diverse reasons. This so called {\itbig data era} we are currently living calls for devising online subspacelearning algorithms that can suitably handle incomplete data. Their envisagedobjective is to {\it recursively} estimate the unknown subspace by processingstreaming data sequentially, thus reducing computational complexity, whileobviating the need for storing the whole dataset in memory. In this paper, anonline variational Bayes subspace learning algorithm from partial observationsis presented. To account for the unawareness of the true rank of the subspace,commonly met in practice, low-rankness is explicitly imposed on the soughtsubspace data matrix by exploiting sparse Bayesian learning principles.Moreover, sparsity, {\it simultaneously} to low-rankness, is favored on thesubspace matrix by the sophisticated hierarchical Bayesian scheme that isadopted. In doing so, the proposed algorithm becomes adept in dealing withapplications whereby the underlying subspace may be also sparse, as, e.g., insparse dictionary learning problems. As shown, the new subspace tracking schemeoutperforms its state-of-the-art counterparts in terms of estimation accuracy,in a variety of experiments conducted on simulated and real data.
arxiv-1602-03681 | Package equivalence in complex software network |  http://arxiv.org/abs/1602.03681  | author:Tomislav Slijepčević category:cs.SI stat.ML published:2016-02-11 summary:The public package registry npm is one of the biggest software registry. Withits 216 911 software packages, it forms a big network of software dependencies.In this paper we evaluate various methods for finding similar packages in thenpm network, using only the structure of the graph. Namely, we want to find away of categorizing similar packages, which would be useful for recommendationsystems. This size enables us to compute meaningful results, as it softened theparticularities of the graph. Npm is also quite famous as it is the defaultpackage repository of Node.js. We believe that it will make our resultsinteresting for more people than a less used package repository. This makes ita good subject of analysis of software networks.
arxiv-1602-03860 | Real-Time Hand Tracking Using a Sum of Anisotropic Gaussians Model |  http://arxiv.org/abs/1602.03860  | author:Srinath Sridhar, Helge Rhodin, Hans-Peter Seidel, Antti Oulasvirta, Christian Theobalt category:cs.CV published:2016-02-11 summary:Real-time marker-less hand tracking is of increasing importance inhuman-computer interaction. Robust and accurate tracking of arbitrary handmotion is a challenging problem due to the many degrees of freedom, frequentself-occlusions, fast motions, and uniform skin color. In this paper, wepropose a new approach that tracks the full skeleton motion of the hand frommultiple RGB cameras in real-time. The main contributions include a newgenerative tracking method which employs an implicit hand shape representationbased on Sum of Anisotropic Gaussians (SAG), and a pose fitting energy that issmooth and analytically differentiable making fast gradient based poseoptimization possible. This shape representation, together with a fullperspective projection model, enables more accurate hand modeling than arelated baseline method from literature. Our method achieves better accuracythan previous methods and runs at 25 fps. We show these improvements bothqualitatively and quantitatively on publicly available datasets.
arxiv-1602-03828 | Community Recovery in Graphs with Locality |  http://arxiv.org/abs/1602.03828  | author:Yuxin Chen, Govinda Kamath, Changho Suh, David Tse category:cs.IT cs.LG cs.SI math.IT math.ST q-bio.GN stat.TH published:2016-02-11 summary:Motivated by applications in domains such as social networks andcomputational biology, we study the problem of community recovery in graphswith locality. In this problem, pairwise noisy measurements of whether twonodes are in the same community or different communities come mainly orexclusively from nearby nodes rather than uniformly sampled between all nodespairs, as in most existing models. We present an algorithm that runs nearlylinearly in the number of measurements and which achieves the informationtheoretic limit for exact recovery.
arxiv-1602-03647 | On the Difficulty of Selecting Ising Models with Approximate Recovery |  http://arxiv.org/abs/1602.03647  | author:Jonathan Scarlett, Volkan Cevher category:cs.IT cs.LG cs.SI math.IT stat.ML published:2016-02-11 summary:In this paper, we consider the problem of estimating the underlying graphicalmodel of an Ising distribution given a number of independent and identicallydistributed samples. We adopt an \emph{approximate recovery} criterion thatallows for a number of missed edges or incorrectly-included edges, thusdeparting from the extensive literature considering the exact recovery problem.Our main results provide information-theoretic lower bounds on the requirednumber of samples (i.e., the sample complexity) for graph classes imposingconstraints on the number of edges, maximal degree, and sparse separationproperties. We identify a broad range of scenarios where, either up to constantfactors or logarithmic factors, our lower bounds match the best known lowerbounds for the exact recovery criterion, several of which are known to be tightor near-tight. Hence, in these cases, we prove that the approximate recoveryproblem is not much easier than the exact recovery problem. Our bounds are obtained via a modification of Fano's inequality for handlingthe approximate recovery criterion, along with suitably-designed ensembles ofgraphs that can broadly be classed into two categories: (i) Those containinggraphs that contain several isolated edges or cliques and are thus difficult todistinguish from the empty graph; (ii) Those containing graphs for whichcertain groups of nodes are highly correlated, thus making it difficult todetermine precisely which edges connect them. We support our theoreticalresults on these ensembles with numerical experiments.
arxiv-1602-03779 | Network of Bandits |  http://arxiv.org/abs/1602.03779  | author:Raphaël Féraud category:cs.AI cs.DC cs.LG published:2016-02-11 summary:The distribution of the best arm identification task on the user's devicesoffers several advantages for application purposes: scalability, reduction ofdeployment costs and privacy. We propose a distributed version of the algorithmSuccessive Elimination using a simple architecture based on a single serverwhich synchronizes each task executed on the user's devices. We show that thisalgorithm is near optimal both in terms of transmitted number of bits and interms of number of pulls per player. Finally, we propose an extension of thisapproach to distribute the contextual bandit algorithm Bandit Forest, which isable to finely exploit the user's data while guaranteeing the privacy.
arxiv-1602-03661 | On the emergence of syntactic structures: quantifying and modelling duality of patterning |  http://arxiv.org/abs/1602.03661  | author:Vittorio Loreto, Pietro Gravino, Vito D. P. Servedio, Francesca Tria category:physics.soc-ph cs.CL published:2016-02-11 summary:The complex organization of syntax in hierarchical structures is one of thecore design features of human language. Duality of patterning refers forinstance to the organization of the meaningful elements in a language at twodistinct levels: a combinatorial level where meaningless forms are combinedinto meaningful forms and a compositional level where meaningful forms arecomposed into larger lexical units. The question remains wide open regardinghow such a structure could have emerged. Furthermore a clear mathematicalframework to quantify this phenomenon is still lacking. The aim of this paperis that of addressing these two aspects in a self-consistent way. First, weintroduce suitable measures to quantify the level of combinatoriality andcompositionality in a language, and present a framework to estimate theseobservables in human natural languages. Second, we show that the theoreticalpredictions of a multi-agents modeling scheme, namely the Blending Game, are insurprisingly good agreement with empirical data. In the Blending Game apopulation of individuals plays language games aiming at success incommunication. It is remarkable that the two sides of duality of patterningemerge simultaneously as a consequence of a pure cultural dynamics in asimulated environment that contains meaningful relations, provided a simpleconstraint on message transmission fidelity is also considered.
arxiv-1602-03683 | A Universal Approximation Theorem for Mixture of Experts Models |  http://arxiv.org/abs/1602.03683  | author:Hien D Nguyen, Luke R Lloyd-Jones, Geoffrey J McLachlan category:stat.ML published:2016-02-11 summary:The mixture of experts (MoE) model is a popular neural network architecturefor nonlinear regression and classification. The class of MoE mean functions isknown to be uniformly convergent to any unknown target function, assuming thatthe target function is from Sobolev space that is sufficiently differentiableand that the domain of estimation is a compact unit hypercube. We provide analternative result, which shows that the class of MoE mean functions is densein the class of all continuous functions over arbitrary compact domains ofestimation. Our result can be viewed as a universal approximation theorem forMoE models.
arxiv-1602-03903 | Wavelet-Based Semantic Features for Hyperspectral Signature Discrimination |  http://arxiv.org/abs/1602.03903  | author:Siwei Feng, Yuki Itoh, Mario Parente, Marco F. Duarte category:cs.CV cs.LG published:2016-02-11 summary:Hyperspectral signature classification is a quantitative analysis approachfor hyperspectral imagery which performs detection and classification of theconstituent materials at the pixel level in the scene. The classificationprocedure can be operated directly on hyperspectral data or performed by usingsome features extracted from the corresponding hyperspectral signaturescontaining information like the signature's energy or shape. In this paper, wedescribe a technique that applies non-homogeneous hidden Markov chain (NHMC)models to hyperspectral signature classification. The basic idea is to usestatistical models (such as NHMC) to characterize wavelet coefficients whichcapture the spectrum semantics (i.e., structural information) at multiplelevels. Experimental results show that the approach based on NHMC models canoutperform existing approaches relevant in classification tasks.
arxiv-1602-03808 | Semi-supervised Learning with Explicit Relationship Regularization |  http://arxiv.org/abs/1602.03808  | author:Kwang In Kim, James Tompkin, Hanspeter Pfister, Christian Theobalt category:cs.CV cs.LG published:2016-02-11 summary:In many learning tasks, the structure of the target space of a function holdsrich information about the relationships between evaluations of functions ondifferent data points. Existing approaches attempt to exploit this relationshipinformation implicitly by enforcing smoothness on function evaluations only.However, what happens if we explicitly regularize the relationships betweenfunction evaluations? Inspired by homophily, we regularize based on a smoothrelationship function, either defined from the data or with labels. Inexperiments, we demonstrate that this significantly improves the performance ofstate-of-the-art algorithms in semi-supervised classification and in spectraldata embedding for constrained clustering and dimensionality reduction.
arxiv-1602-03807 | Bayesian Sparsity for Intractable Distributions |  http://arxiv.org/abs/1602.03807  | author:John B. Ingraham, Debora S. Marks category:stat.ML q-bio.QM published:2016-02-11 summary:Bayesian approaches for single-variable and group-structured sparsityoutperform L1 regularization, but are challenging to apply to large,potentially intractable models. Here we show how noncentered parameterizations,a common trick for improving the efficiency of exact inference in hierarchicalmodels, can similarly improve the accuracy of variational approximations. Wedevelop this with two contributions: First, we introduce Fadeout, an approachfor variational inference that uses noncentered parameterizations to capture aposteriori correlations between parameters and hyperparameters. Second, weextend stochastic variational inference to undirected models, enablingefficient hierarchical Bayes without approximations of intractable normalizingconstants. We find that this framework substantially improves inferences ofundirected graphical models under both sparse and group-sparse priors.
arxiv-1602-03805 | Local High-order Regularization on Data Manifolds |  http://arxiv.org/abs/1602.03805  | author:Kwang In Kim, James Tompkin, Hanspeter Pfister, Christian Theobalt category:cs.CV published:2016-02-11 summary:The common graph Laplacian regularizer is well-established in semi-supervisedlearning and spectral dimensionality reduction. However, as a first-orderregularizer, it can lead to degenerate functions in high-dimensional manifolds.The iterated graph Laplacian enables high-order regularization, but it has ahigh computational complexity and so cannot be applied to large problems. Weintroduce a new regularizer which is globally high order and so does not sufferfrom the degeneracy of the graph Laplacian regularizer, but is also sparse forefficient computation in semi-supervised learning applications. We reducecomputational complexity by building a local first-order approximation of themanifold as a surrogate geometry, and construct our high-order regularizerbased on local derivative evaluations therein. Experiments on human body shapeand pose analysis demonstrate the effectiveness and efficiency of our method.
arxiv-1602-03742 | HMM and DTW for evaluation of therapeutical gestures using kinect |  http://arxiv.org/abs/1602.03742  | author:Carlos Palma, Augusto Salazar, Francisco Vargas category:cs.HC cs.CV published:2016-02-11 summary:Automatic recognition of the quality of movement in human beings is achallenging task, given the difficulty both in defining the constraints thatmake a movement correct, and the difficulty in using noisy data to determine ifthese constraints were satisfied. This paper presents a method for thedetection of deviations from the correct form in movements from physicaltherapy routines based on Hidden Markov Models, which is compared to DynamicTime Warping. The activities studied include upper an lower limbs movements,the data used comes from a Kinect sensor. Correct repetitions of the activitiesof interest were recorded, as well as deviations from these correct forms. Theability of the proposed approach to detect these deviations was studied.Results show that a system based on HMM is much more likely to determine if acertain movement has deviated from the specification.
arxiv-1602-03725 | A Versatile Scene Model with Differentiable Visibility Applied to Generative Pose Estimation |  http://arxiv.org/abs/1602.03725  | author:Helge Rhodin, Nadia Robertini, Christian Richardt, Hans-Peter Seidel, Christian Theobalt category:cs.CV published:2016-02-11 summary:Generative reconstruction methods compute the 3D configuration (such as poseand/or geometry) of a shape by optimizing the overlap of the projected 3D shapemodel with images. Proper handling of occlusions is a big challenge, since thevisibility function that indicates if a surface point is seen from a camera canoften not be formulated in closed form, and is in general discrete andnon-differentiable at occlusion boundaries. We present a new scenerepresentation that enables an analytically differentiable closed-formformulation of surface visibility. In contrast to previous methods, this yieldssmooth, analytically differentiable, and efficient to optimize pose similarityenergies with rigorous occlusion handling, fewer local minima, andexperimentally verified improved convergence of numerical optimization. Theunderlying idea is a new image formation model that represents opaque objectsby a translucent medium with a smooth Gaussian density distribution which turnsvisibility into a smooth phenomenon. We demonstrate the advantages of ourversatile scene model in several generative pose estimation problems, namelymarker-less multi-object pose estimation, marker-less human motion capture withfew cameras, and image-based 3D geometry estimation.
arxiv-1602-03686 | Medical Concept Representation Learning from Electronic Health Records and its Application on Heart Failure Prediction |  http://arxiv.org/abs/1602.03686  | author:Edward Choi, Andy Schuetz, Walter F. Stewart, Jimeng Sun category:cs.LG cs.NE published:2016-02-11 summary:Objective: To transform heterogeneous clinical data from electronic healthrecords into clinically meaningful constructed features using data drivenmethod that rely, in part, on temporal relations among data. Materials andMethods: The clinically meaningful representations of medical concepts andpatients are the key for health analytic applications. Most of existingapproaches directly construct features mapped to raw data (e.g., ICD or CPTcodes), or utilize some ontology mapping such as SNOMED codes. However, none ofthe existing approaches leverage EHR data directly for learning such conceptrepresentation. We propose a new way to represent heterogeneous medicalconcepts (e.g., diagnoses, medications and procedures) based on co-occurrencepatterns in longitudinal electronic health records. The intuition behind themethod is to map medical concepts that are co-occuring closely in time tosimilar concept vectors so that their distance will be small. We also derive asimple method to construct patient vectors from the related medical conceptvectors. Results: We evaluate similar medical concepts across diagnosis,medication and procedure. The results show xx% relevancy between similar pairsof medical concepts. Our proposed representation significantly improves thepredictive modeling performance for onset of heart failure (HF), whereclassification methods (e.g. logistic regression, neural network, supportvector machine and K-nearest neighbors) achieve up to 23% improvement in areaunder the ROC curve (AUC) using this proposed representation. Conclusion: Weproposed an effective method for patient and medical concept representationlearning. The resulting representation can map relevant concepts together andalso improves predictive modeling performance.
arxiv-1602-03468 | 3D Pictorial Structures on RGB-D Data for Articulated Human Detection in Operating Rooms |  http://arxiv.org/abs/1602.03468  | author:Abdolrahim Kadkhodamohammadi, Afshin Gangi, Michel de Mathelin, Nicolas Padoy category:cs.CV published:2016-02-10 summary:Reliable human pose estimation (HPE) is essential to many clinicalapplications, such as surgical workflow analysis, radiation safety monitoringand human-robot cooperation. Proposed methods for the operating room (OR) relyeither on foreground estimation using a multi-camera system, which is achallenge in real ORs due to color similarities and frequent illuminationchanges, or on wearable sensors or markers, which are invasive and thereforedifficult to introduce in the room. Instead, we propose a novel approach basedon Pictorial Structures (PS) and on RGB-D data, which can be easily deployed inreal ORs. We extend the PS framework in two ways. First, we build robust anddiscriminative part detectors using both color and depth images. We alsopresent a novel descriptor for depth images, called histogram of depthdifferences (HDD). Second, we extend PS to 3D by proposing 3D pairwiseconstraints and a new method for exact and tractable inference. Our approach isevaluated for pose estimation and clinician detection on a challenging RGB-Ddataset recorded in a busy operating room during live surgeries. We conductseries of experiments to study the different part detectors in conjunction withthe various 2D or 3D pairwise constraints. Our comparisons demonstrate that 3DPS with RGB-D part detectors significantly improves the results in a visuallychallenging operating environment.
arxiv-1602-03258 | Interactive Bayesian Hierarchical Clustering |  http://arxiv.org/abs/1602.03258  | author:Sharad Vikram, Sanjoy Dasgupta category:cs.LG published:2016-02-10 summary:Clustering is a powerful tool in data analysis, but it is often difficult tofind a grouping that aligns with a user's needs. To address this, severalmethods incorporate constraints obtained from users into clustering algorithms,but unfortunately do not apply to hierarchical clustering. We design aninteractive Bayesian algorithm that incorporates user interaction intohierarchical clustering while still utilizing the geometry of the data bysampling a constrained posterior distribution over hierarchies. We also suggestseveral ways to intelligently query a user. The algorithm, along with thequerying schemes, shows promising results on real data.
arxiv-1602-03476 | Causal Strength via Shannon Capacity: Axioms, Estimators and Applications |  http://arxiv.org/abs/1602.03476  | author:Weihao Gao, Sreeram Kannan, Sewoong Oh, Pramod Viswanath category:cs.IT cs.LG math.IT stat.ML published:2016-02-10 summary:We conduct an axiomatic study of the problem of estimating the strength of aknown causal relationship between a pair of variables. We propose that anestimate of causal strength should be based on the conditional distribution ofthe effect given the cause (and not on the driving distribution of the cause),and study dependence measures on conditional distributions. Shannon capacity,appropriately regularized, emerges as a natural measure under these axioms. Weexamine the problem of calculating Shannon capacity from the observed samplesand propose a novel fixed-$k$ nearest neighbor estimator, and demonstrate itsconsistency. Finally, we demonstrate an application to single-cellflow-cytometry, where the proposed estimators significantly reduce samplecomplexity.
arxiv-1602-03253 | A Kernelized Stein Discrepancy for Goodness-of-fit Tests and Model Evaluation |  http://arxiv.org/abs/1602.03253  | author:Qiang Liu, Jason D. Lee, Michael I. Jordan category:stat.ML published:2016-02-10 summary:We derive a new discrepancy statistic for measuring differences between twoprobability distributions based on a novel combination of Stein's method andthe reproducing kernel Hilbert space theory. We apply our result to test howwell a probabilistic model fits a set of observations, and derive a new classof powerful goodness-of-fit tests that are widely applicable for complex andhigh dimensional distributions, even for those with computationally intractablenormalization constants. Both theoretical and empirical properties of ourmethods are studied thoroughly.
arxiv-1602-03256 | Improved Eigenfeature Regularization for Face Identification |  http://arxiv.org/abs/1602.03256  | author:Bappaditya Mandal category:cs.CV published:2016-02-10 summary:In this work, we propose to divide each class (a person) into subclassesusing spatial partition trees which helps in better capturing theintra-personal variances arising from the appearances of the same individual.We perform a comprehensive analysis on within-class and within-subclasseigenspectrums of face images and propose a novel method of eigenspectrummodeling which extracts discriminative features of faces from bothwithin-subclass and total or between-subclass scatter matrices. Effectivelow-dimensional face discriminative features are extracted for face recognition(FR) after performing discriminant evaluation in the entire eigenspace.Experimental results on popular face databases (AR, FERET) and the challengingunconstrained YouTube Face database show the superiority of our proposedapproach on all three databases.
arxiv-1602-03264 | A Theory of Generative ConvNet |  http://arxiv.org/abs/1602.03264  | author:Jianwen Xie, Yang Lu, Song-Chun Zhu, Ying Nian Wu category:stat.ML cs.LG published:2016-02-10 summary:The convolutional neural network (ConvNet or CNN) is a powerfuldiscriminative learning machine. In this paper, we show that a generativerandom field model that we call generative ConvNet can be derived from thediscriminative ConvNet. The probability distribution of the generative ConvNetmodel is in the form of exponential tilting of a reference distribution.Assuming re-lu non-linearity and Gaussian white noise reference distribution,we show that the generative ConvNet model contains a representational structurewith multiple layers of binary activation variables. The model is non-Gaussian,or more precisely, piecewise Gaussian, where each piece is determined by aninstantiation of the binary activation variables that reconstruct the mean ofthe Gaussian piece. The Langevin dynamics for synthesis is driven by thereconstruction error, and the corresponding gradient descent dynamics convergesto a local energy minimum that is auto-encoding. As for learning, we show thatthe contrastive divergence learning tends to reconstruct the observed images.Finally, we show that the maximum likelihood learning algorithm can generaterealistic natural images.
arxiv-1602-03308 | Gabor Wavelets in Image Processing |  http://arxiv.org/abs/1602.03308  | author:David Barina category:cs.CV cs.GR cs.MM published:2016-02-10 summary:This work shows the use of a two-dimensional Gabor wavelets in imageprocessing. Convolution with such a two-dimensional wavelet can be separatedinto two series of one-dimensional ones. The key idea of this work is toutilize a Gabor wavelet as a multiscale partial differential operator of agiven order. Gabor wavelets are used here to detect edges, corners and blobs. Aperformance of such an interest point detector is compared to detectorsutilizing a Haar wavelet and a derivative of a Gaussian function. The proposedapproach may be useful when a fast implementation of the Gabor transform isavailable or when the transform is already precomputed.
arxiv-1602-03346 | DAP3D-Net: Where, What and How Actions Occur in Videos? |  http://arxiv.org/abs/1602.03346  | author:Li Liu, Yi Zhou, Ling Shao category:cs.CV published:2016-02-10 summary:Action parsing in videos with complex scenes is an interesting butchallenging task in computer vision. In this paper, we propose a generic 3Dconvolutional neural network in a multi-task learning manner for effective DeepAction Parsing (DAP3D-Net) in videos. Particularly, in the training phase,action localization, classification and attributes learning can be jointlyoptimized on our appearancemotion data via DAP3D-Net. For an upcoming testvideo, we can describe each individual action in the video simultaneously as:Where the action occurs, What the action is and How the action is performed. Towell demonstrate the effectiveness of the proposed DAP3D-Net, we alsocontribute a new Numerous-category Aligned Synthetic Action dataset, i.e.,NASA, which consists of 200; 000 action clips of more than 300 categories andwith 33 pre-defined action attributes in two hierarchical levels (i.e.,low-level attributes of basic body part movements and high-level attributesrelated to action motion). We learn DAP3D-Net using the NASA dataset and thenevaluate it on our collected Human Action Understanding (HAU) dataset.Experimental results show that our approach can accurately localize, categorizeand describe multiple actions in realistic videos.
arxiv-1602-03348 | Iterative Hierarchical Optimization for Misspecified Problems (IHOMP) |  http://arxiv.org/abs/1602.03348  | author:Daniel J. Mankowitz, Timothy A. Mann, Shie Mannor category:cs.LG cs.AI published:2016-02-10 summary:Reinforcement Learning (RL) aims to learn an optimal policy for a MarkovDecision Process (MDP). For complex, high-dimensional MDPs, it may only befeasible to represent the policy with function approximation. If the policyrepresentation used cannot represent good policies, the problem is misspecifiedand the learned policy may be far from optimal. We introduce IHOMP as anapproach for solving misspecified problems. IHOMP iteratively refines a set ofspecialized policies based on a limited representation. We refer to thesepolicies as policy threads. At the same time, IHOMP stitches these policythreads together in a hierarchical fashion to solve a problem that wasotherwise misspecified. We prove that IHOMP enjoys theoretical convergenceguarantees and extend IHOMP to exploit Option Interruption (OI) enabling it tolearn where policy threads can be reused. Our experiments demonstrate thatIHOMP can find near-optimal solutions to otherwise misspecified problems andthat OI can further improve the solutions.
arxiv-1602-03351 | Adaptive Skills, Adaptive Partitions (ASAP) |  http://arxiv.org/abs/1602.03351  | author:Daniel J. Mankowitz, Timothy A. Mann, Shie Mannor category:cs.LG cs.AI stat.ML published:2016-02-10 summary:We introduce the Adaptive Skills, Adaptive Partitions (ASAP) algorithm that(1) learns skills (i.e., temporally extended actions or options) as well as (2)where to apply them to solve a Markov decision process. ASAP is initiallyprovided with a misspecified hierarchical model and is able to correct thismodel and learn a near-optimal set of skills to solve a given task. We believethat (1) and (2) are the core components necessary for a truly general skilllearning framework, which is a key building block needed to scale up tolifelong learning agents. ASAP is also able to solve related new tasks simplyby adapting where it applies its existing learned skills. We prove that ASAPconverges to a local optimum under natural conditions. Finally, our extensiveexperimental results, which include a RoboCup domain, demonstrate the abilityof ASAP to learn where to reuse skills as well as solve multiple tasks withconsiderably less experience than solving each task from scratch.
arxiv-1602-03368 | Fast model selection by limiting SVM training times |  http://arxiv.org/abs/1602.03368  | author:Aydin Demircioglu, Daniel Horn, Tobias Glasmachers, Bernd Bischl, Claus Weihs category:stat.ML cs.LG published:2016-02-10 summary:Kernelized Support Vector Machines (SVMs) are among the best performingsupervised learning methods. But for optimal predictive performance,time-consuming parameter tuning is crucial, which impedes application. Totackle this problem, the classic model selection procedure based on grid-searchand cross-validation was refined, e.g. by data subsampling and direct searchheuristics. Here we focus on a different aspect, the stopping criterion for SVMtraining. We show that by limiting the training time given to the SVM solverduring parameter tuning we can reduce model selection times by an order ofmagnitude.
arxiv-1602-03379 | Comparison of feature extraction and dimensionality reduction methods for single channel extracellular spike sorting |  http://arxiv.org/abs/1602.03379  | author:Anupam Mitra, Anagh Pathak, Kaushik Majumdar category:q-bio.QM cs.CV q-bio.NC published:2016-02-10 summary:Spikes in the membrane electrical potentials of neurons play a major role inthe functioning of nervous systems of animals. Obtaining the spikes fromdifferent neurons has been a challenging problem for decades. Several schemeshave been proposed for spike sorting to isolate the spikes of individualneurons from electrical recordings in extracellular media. However, there ismuch scope for improvement in the accuracies obtained using the prevailingmethods of spike sorting. To determine more effective spike sorting strategiesusing well known methods, we compared different types of signal features andtechniques for dimensionality reduction in feature space. We tried to determinean optimum or near optimum feature extraction and dimensionality reductionmethods and an optimum or near optimum number of features for spike sorting. Weassessed relative performance of well known methods on simulated recordingsspecially designed for development and benchmarking of spike sorting schemes,with varying number of spike classes and the well established method of$k$-means clustering of selected features. We found that almost all well knownmethods performed quite well. Nevertheless, from spike waveforms of 64 samples,sampled at 24 kHz, using principal component analysis (PCA) to select around 46to 55 features led to the better spike sorting performance than most othermethods (Wilcoxon signed rank sum test, $p < 0.001$).
arxiv-1602-03409 | Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning |  http://arxiv.org/abs/1602.03409  | author:Hoo-Chang Shin, Holger R. Roth, Mingchen Gao, Le Lu, Ziyue Xu, Isabella Nogues, Jianhua Yao, Daniel Mollura, Ronald M. Summers category:cs.CV published:2016-02-10 summary:Remarkable progress has been made in image recognition, primarily due to theavailability of large-scale annotated datasets and the revival of deep CNN.CNNs enable learning data-driven, highly representative, layered hierarchicalimage features from sufficient training data. However, obtaining datasets ascomprehensively annotated as ImageNet in the medical imaging domain remains achallenge. There are currently three major techniques that successfully employCNNs to medical image classification: training the CNN from scratch, usingoff-the-shelf pre-trained CNN features, and conducting unsupervised CNNpre-training with supervised fine-tuning. Another effective method is transferlearning, i.e., fine-tuning CNN models pre-trained from natural image datasetto medical image tasks. In this paper, we exploit three important, butpreviously understudied factors of employing deep convolutional neural networksto computer-aided detection problems. We first explore and evaluate differentCNN architectures. The studied models contain 5 thousand to 160 millionparameters, and vary in numbers of layers. We then evaluate the influence ofdataset scale and spatial image context on performance. Finally, we examinewhen and why transfer learning from pre-trained ImageNet (via fine-tuning) canbe useful. We study two specific computer-aided detection (CADe) problems,namely thoraco-abdominal lymph node (LN) detection and interstitial lungdisease (ILD) classification. We achieve the state-of-the-art performance onthe mediastinal LN detection, with 85% sensitivity at 3 false positive perpatient, and report the first five-fold cross-validation classification resultson predicting axial CT slices with ILD categories. Our extensive empiricalevaluation, CNN model analysis and valuable insights can be extended to thedesign of high performance CAD systems for other medical imaging tasks.
arxiv-1602-03426 | Automatic Sarcasm Detection: A Survey |  http://arxiv.org/abs/1602.03426  | author:Aditya Joshi, Pushpak Bhattacharyya, Mark James Carman category:cs.CL published:2016-02-10 summary:Automatic detection of sarcasm has witnessed interest from the sentimentanalysis research community. With diverse approaches, datasets and analysesthat have been reported, there is an essential need to have a collectiveunderstanding of the research in this area. In this survey of automatic sarcasmdetection, we describe datasets, approaches (both supervised and rule-based),and trends in sarcasm detection research. We also present a research matrixthat summarizes past work, and list pointers to future work.
arxiv-1602-03442 | Stochastic Quasi-Newton Langevin Monte Carlo |  http://arxiv.org/abs/1602.03442  | author:Umut Şimşekli, Roland Badeau, A. Taylan Cemgil, Gaël Richard category:stat.ML published:2016-02-10 summary:Recently, Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) methods havebeen proposed for scaling up Monte Carlo computations to large data problems.Whilst these approaches have proven useful in many applications, vanillaSG-MCMC might suffer from poor mixing rates when random variables exhibitstrong couplings under the target densities or big scale differences. In thisstudy, we propose a novel SG-MCMC method that takes the local geometry intoaccount by using ideas from Quasi-Newton optimization methods. These secondorder methods directly approximate the inverse Hessian by using a limitedhistory of samples and their gradients. Our method uses dense approximations ofthe inverse Hessian while keeping the time and memory complexities linear withthe dimension of the problem. We provide a formal theoretical analysis where weshow that the proposed method is asymptotically unbiased and consistent withthe posterior expectations. We illustrate the effectiveness of the approach onboth synthetic and real datasets. Our experiments on two challengingapplications show that our method achieves fast convergence rates similar toRiemannian approaches while at the same time having low computationalrequirements similar to diagonal preconditioning approaches.
arxiv-1602-03458 | Super-Resolved Retinal Image Mosaicing |  http://arxiv.org/abs/1602.03458  | author:Thomas Köhler, Axel Heinrich, Andreas Maier, Joachim Hornegger, Ralf P. Tornow category:cs.CV published:2016-02-10 summary:The acquisition of high-resolution retinal fundus images with a large fieldof view (FOV) is challenging due to technological, physiological and economicreasons. This paper proposes a fully automatic framework to reconstruct retinalimages of high spatial resolution and increased FOV from multiplelow-resolution images captured with non-mydriatic, mobile and video-capable butlow-cost cameras. Within the scope of one examination, we scan differentregions on the retina by exploiting eye motion conducted by a patient guidance.Appropriate views for our mosaicing method are selected based on optic disktracking to trace eye movements. For each view, one super-resolved image isreconstructed by fusion of multiple video frames. Finally, all super-resolvedviews are registered to a common reference using a novel polynomialregistration scheme and combined by means of image mosaicing. We evaluated ourframework for a mobile and low-cost video fundus camera. In our experiments, wereconstructed retinal images of up to 30{\deg} FOV from 10 complementary viewsof 15{\deg} FOV. An evaluation of the mosaics by human experts as well as aquantitative comparison to conventional color fundus images encourage theclinical usability of our framework.
arxiv-1602-03481 | Reliable Crowdsourcing under the Generalized Dawid-Skene Model |  http://arxiv.org/abs/1602.03481  | author:Ashish Khetan, Sewoong Oh category:cs.LG cs.HC cs.SI stat.ML published:2016-02-10 summary:Crowdsourcing systems provide scalable and cost-effective human-poweredsolutions at marginal cost, for classification tasks where humans aresignificantly better than the machines. Although traditional approaches inaggregating crowdsourced labels have relied on the Dawid-Skene model, thisfails to capture how some tasks are inherently more difficult than the others.Several generalizations have been proposed, but inference becomes intractableand typical solutions resort to heuristics. To bridge this gap, we study arecently proposed generalize Dawid-Skene model, and propose a linear-timealgorithm based on spectral methods. We show near-optimality of the proposedapproach, by providing an upper bound on the error and comparing it to afundamental limit. We provide numerical experiments on synthetic data matchingour analyses, and also on real datasets demonstrating that the spectral methodsignificantly improves over simple majority voting and is comparable to othermethods.
arxiv-1602-03265 | Simple Search Algorithms on Semantic Networks Learned from Language Use |  http://arxiv.org/abs/1602.03265  | author:Aida Nematzadeh, Filip Miscevic, Suzanne Stevenson category:cs.CL published:2016-02-10 summary:Recent empirical and modeling research has focused on the semantic fluencytask because it is informative about semantic memory. An interesting interplayarises between the richness of representations in semantic memory and thecomplexity of algorithms required to process it. It has remained an openquestion whether representations of words and their relations learned fromlanguage use can enable a simple search algorithm to mimic the observedbehavior in the fluency task. Here we show that it is plausible to learn richrepresentations from naturalistic data for which a very simple search algorithm(a random walk) can replicate the human patterns. We suggest that explicitlystructuring knowledge about words into a semantic network plays a crucial rolein modeling human behavior in memory search and retrieval; moreover, this isthe case across a range of semantic information sources.
arxiv-1602-03418 | Triplet Similarity Embedding for Face Verification |  http://arxiv.org/abs/1602.03418  | author:Swami Sankaranarayanan, Azadeh Alavi, Rama Chellappa category:cs.CV published:2016-02-10 summary:In this work, we present an unconstrained face verification algorithm andevaluate it on the recently released IJB-A dataset that aims to push theboundaries of face verification methods. The proposed algorithm couples a deepCNN-based approach with a low-dimensional discriminative embedding learnt usingtriplet similarity constraints in a large margin fashion. Aside from yieldingperformance improvement, this embedding provides significant advantages interms of memory and post-processing operations like hashing and visualization.Experiments on the IJB-A dataset show that the proposed algorithm outperformsstate of the art methods in verification and identification metrics, whilerequiring less training time.
arxiv-1602-03570 | Optimized Kernel-based Projection Space of Riemannian Manifolds |  http://arxiv.org/abs/1602.03570  | author:Azadeh Alavi, Vishal M Patel, Rama Chellappa category:cs.CV published:2016-02-10 summary:It is proven that encoding images and videos through Symmetric PositiveDefinite (SPD) matrices, and considering the Riemannian geometry of theresulting space, can lead to increased classification performance. Taking intoaccount manifold geometry is typically done via embedding the manifolds intangent spaces, or Reproducing Kernel Hilbert Spaces (RKHS). Recently, it wasshown that embedding such manifolds into a Random Projection Spaces (RPS),rather than RKHS or tangent space, leads to higher classification andclustering performance. However, based on structure and dimensionality of therandomly generated hyperplanes, the classification performance over RPS mayvary significantly. In addition, fine-tuning RPS is data expensive (as itrequires validation-data), time consuming, and resource demanding. In thispaper, we introduce an approach to learn an optimized kernel-based projection(with fixed dimensionality), by employing the concept of subspace clustering.As such, we encode the association of data points to the underlying subspace ofeach point, to generate meaningful hyperplanes. Further, we adopt the conceptof dictionary learning and sparse coding, and discriminative analysis, for theoptimized kernel-based projection space (OPS) on SPD manifolds. We validate ouralgorithm on several classification tasks. The experiment results alsodemonstrate that the proposed method outperforms state-of-the-art methods onsuch manifolds.
arxiv-1602-03571 | High Dimensional Inference with Random Maximum A-Posteriori Perturbations |  http://arxiv.org/abs/1602.03571  | author:Tamir Hazan, Francesco Orabona, Anand D. Sarwate, Subhransu Maji, Tommi Jaakkola category:cs.LG cs.IT math.IT stat.ML published:2016-02-10 summary:In this work we present a new approach for high-dimensional statisticalinference that is based on optimization and random perturbations. Thisframework injects randomness to maximum a-posteriori (MAP) predictors byrandomly perturbing its potential function. When the perturbations are of lowdimension, sampling the perturb-max prediction is as efficient as MAPoptimization. A classic result from extreme value statistics asserts thatperturb-max operations generate unbiased samples from the Gibbs distributionusing high-dimensional perturbations. Unfortunately, the computational cost ofgenerating so many high-dimensional random variables can be prohibitive. Inthis work we show that the expected value of perturb-max inference with lowdimensional perturbations can be used sequentially to generate unbiased samplesfrom the Gibbs distribution. We also show that the expected value of themaximal perturbations is a natural bound on the entropy of such perturb-maxmodels. Finally we describe the measure concentration properties of perturb-maxvalues while showing that the deviation of their sampled average from itsexpectation decays exponentially in the number of samples.
arxiv-1602-03552 | Learning Privately from Multiparty Data |  http://arxiv.org/abs/1602.03552  | author:Jihun Hamm, Paul Cao, Mikhail Belkin category:cs.LG cs.CR published:2016-02-10 summary:Learning a classifier from private data collected by multiple parties is animportant problem that has many potential applications. How can we build anaccurate and differentially private global classifier by combininglocally-trained classifiers from different parties, without access to anyparty's private data? We propose to transfer the `knowledge' of the localclassifier ensemble by first creating labeled data from auxiliary unlabeleddata, and then train a global $\epsilon$-differentially private classifier. Weshow that majority voting is too sensitive and therefore propose a new riskweighted by class probabilities estimated from the ensemble. Relative to anon-private solution, our private solution has a generalization error boundedby $O(\epsilon^{-2}M^{-2})$ where $M$ is the number of parties. This allowsstrong privacy without performance loss when $M$ is large, such as incrowdsensing applications. We demonstrate the performance of our method withrealistic tasks of activity recognition, network intrusion detection, andmalicious URL detection.
arxiv-1602-03483 | Learning Distributed Representations of Sentences from Unlabelled Data |  http://arxiv.org/abs/1602.03483  | author:Felix Hill, Kyunghyun Cho, Anna Korhonen category:cs.CL cs.LG published:2016-02-10 summary:Unsupervised methods for learning distributed representations of words areubiquitous in today's NLP research, but far less is known about the best waysto learn distributed phrase or sentence representations from unlabelled data.This paper is a systematic comparison of models that learn suchrepresentations. We find that the optimal approach depends critically on theintended application. Deeper, more complex models are preferable forrepresentations to be used in supervised systems, but shallow log-linear modelswork best for building representation spaces that can be decoded with simplespatial distance metrics. We also propose two new unsupervisedrepresentation-learning objectives designed to optimise the trade-off betweentraining time, domain portability and performance.
arxiv-1602-03506 | Research Priorities for Robust and Beneficial Artificial Intelligence |  http://arxiv.org/abs/1602.03506  | author:Stuart Russell, Daniel Dewey, Max Tegmark category:cs.AI stat.ML published:2016-02-10 summary:Success in the quest for artificial intelligence has the potential to bringunprecedented benefits to humanity, and it is therefore worthwhile toinvestigate how to maximize these benefits while avoiding potential pitfalls.This article gives numerous examples (which should by no means be construed asan exhaustive list) of such worthwhile research aimed at ensuring that AIremains robust and beneficial.
arxiv-1602-03551 | Knowledge Transfer with Medical Language Embeddings |  http://arxiv.org/abs/1602.03551  | author:Stephanie L. Hyland, Theofanis Karaletsos, Gunnar Rätsch category:cs.CL stat.AP published:2016-02-10 summary:Identifying relationships between concepts is a key aspect of scientificknowledge synthesis. Finding these links often requires a researcher tolaboriously search through scien- tific papers and databases, as the size ofthese resources grows ever larger. In this paper we describe how distributionalsemantics can be used to unify structured knowledge graphs with unstructuredtext to predict new relationships between medical concepts, using aprobabilistic generative model. Our approach is also designed to amelioratedata sparsity and scarcity issues in the medical domain, which make languagemodelling more challenging. Specifically, we integrate the medical relationaldatabase (SemMedDB) with text from electronic health records (EHRs) to performknowledge graph completion. We further demonstrate the ability of our model topredict relationships between tokens not appearing in the relational database.
arxiv-1602-03534 | Unsupervised Transductive Domain Adaptation |  http://arxiv.org/abs/1602.03534  | author:Ozan Sener, Hyun Oh Song, Ashutosh Saxena, Silvio Savarese category:stat.ML cs.LG published:2016-02-10 summary:Supervised learning with large scale labeled datasets and deep layered modelshas made a paradigm shift in diverse areas in learning and recognition.However, this approach still suffers generalization issues under the presenceof a domain shift between the training and the test data distribution. In thisregard, unsupervised domain adaptation algorithms have been proposed todirectly address the domain shift problem. In this paper, we approach theproblem from a transductive perspective. We incorporate the domain shift andthe transductive target inference into our framework by jointly solving for anasymmetric similarity metric and the optimal transductive target labelassignment. We also show that our model can easily be extended for deep featurelearning in order to learn features which are discriminative in the targetdomain. Our experiments show that the proposed method significantly outperformsstate-of-the-art algorithms in both object recognition and digit classificationexperiments by a large margin.
arxiv-1602-03001 | A Convolutional Attention Network for Extreme Summarization of Source Code |  http://arxiv.org/abs/1602.03001  | author:Miltiadis Allamanis, Hao Peng, Charles Sutton category:cs.LG cs.CL cs.SE published:2016-02-09 summary:Attention mechanisms in neural networks have proved useful for problems inwhich the input and output do not have fixed dimension. Often there existfeatures that are locally translation invariant and would be valuable fordirecting the model's attention, but previous attentional architectures are notconstructed to learn such features specifically. We introduce an attentionalneural network that employs convolution on the input tokens to detect localtime-invariant and long-range topical attention features in a context-dependentway. We apply this architecture to the problem of extreme summarization ofsource code snippets into short, descriptive function name-like summaries.Using those features, the model sequentially generates a summary bymarginalizing over two attention mechanisms: one that predicts the next summarytoken based on the attention weights of the input tokens and another that isable to copy a code token as-is directly into the summary. We demonstrate ourconvolutional attention neural network's performance on 10 popular Javaprojects showing that it achieves better performance compared to previousattentional mechanisms.
arxiv-1602-02999 | Face Recognition: Perspectives from the Real-World |  http://arxiv.org/abs/1602.02999  | author:Bappaditya Mandal category:cs.CV published:2016-02-09 summary:In this paper, we analyze some of our real-world deployment of facerecognition (FR) systems for various applications and discuss the gaps betweenexpectations of the user and what the system can deliver. We evaluate some ofour proposed algorithms with ad-hoc modifications for applications such as FRon wearable devices (like Google Glass), monitoring of elderly people in seniorcitizens centers, FR of children in child care centers and face matchingbetween a scanned IC/passport face image and a few live webcam images forautomatic hotel/resort checkouts. We describe each of these applications, thechallenges involved and proposed solutions. Since FR is intuitive in nature andwe human beings use it for interactions with the outside world, people havehigh expectations of its performance in real-world scenarios. However, weanalyze and discuss here that it is not the case, machine recognition of facesfor each of these applications poses unique challenges and demands specificresearch components so as to adapt in the actual sites.
arxiv-1602-02990 | The world as its own best controller: a case study with anthropomimetic robots |  http://arxiv.org/abs/1602.02990  | author:Ralf Der, Georg Martius category:cs.RO cs.LG cs.SY I.2.9; I.2.6 published:2016-02-09 summary:With the accelerated development of robot technologies, optimal controlbecomes one of the central themes of research. In traditional approaches, thecontroller, by its internal functionality, finds appropriate actions on thebasis of the history of sensor values, guided by the goals, intentions,objectives, learning schemes, and so on planted into it. The idea is that thecontroller controls the world---the body plus its environment---as reliably aspossible. This paper advocates for a new paradigm of control, obtained bymaking the world control its controller in the first place. The paper presentsa solution with a controller that is devoid of any functionalities of its own,given by a fixed, explicit and context-free function of the recent history ofthe sensor values. When applying this controller to a muscle-tendon drivenarm-shoulder system from the Myorobotics toolkit, we observe a vast variety ofself-organized behavior patterns: when left alone, the arm realizespseudo-random sequences of different poses but one can also manipulate thesystem into definite motion patterns. But most interestingly, after attachingan object, the controller gets in a functional resonance with the object'sinternal dynamics: when given a half-filled bottle, the system spontaneouslystarts shaking the bottle so that maximum response from the dynamics of thewater is being generated. After attaching a pendulum to the arm, the controllerdrives the pendulum into a circular mode. In this way, the robot discoversaffordances of objects its body is interacting with. We also discussperspectives for using this controller paradigm for intention driven behaviorgeneration.
arxiv-1602-02950 | Spoofing detection under noisy conditions: a preliminary investigation and an initial database |  http://arxiv.org/abs/1602.02950  | author:Xiaohai Tian, Zhizheng Wu, Xiong Xiao, Eng Siong Chng, Haizhou Li category:cs.LG cs.SD published:2016-02-09 summary:Spoofing detection for automatic speaker verification (ASV), which is todiscriminate between live speech and attacks, has received increasingattentions recently. However, all the previous studies have been done on theclean data without significant additive noise. To simulate the real-lifescenarios, we perform a preliminary investigation of spoofing detection underadditive noisy conditions, and also describe an initial database for this task.The noisy database is based on the ASVspoof challenge 2015 database andgenerated by artificially adding background noises at different signal-to-noiseratios (SNRs). Five different additive noises are included. Our preliminaryresults show that using the model trained from clean data, the systemperformance degrades significantly in noisy conditions. Phase-based feature ismore noise robust than magnitude-based features. And the systems performsignificantly differ under different noise scenarios.
arxiv-1602-02830 | Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 |  http://arxiv.org/abs/1602.02830  | author:Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, Yoshua Bengio category:cs.LG published:2016-02-09 summary:We introduce a method to train Binarized Neural Networks (BNNs) - neuralnetworks with binary weights and activations at run-time. At training-time thebinary weights and activations are used for computing the parameters gradients.During the forward pass, BNNs drastically reduce memory size and accesses, andreplace most arithmetic operations with bit-wise operations, which is expectedto substantially improve power-efficiency. To validate the effectiveness ofBNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. Onboth, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10and SVHN datasets. Last but not least, we wrote a binary matrix multiplicationGPU kernel with which it is possible to run our MNIST BNN 7 times faster thanwith an unoptimized GPU kernel, without suffering any loss in classificationaccuracy. The code for training and running our BNNs is available on-line.
arxiv-1602-02938 | Challenges of Integrating A Priori Information Efficiently in the Discovery of Spatio-Temporal Objects in Large Databases |  http://arxiv.org/abs/1602.02938  | author:Benjamin Schott, Johannes Stegmaier, Masanari Takamiya, Ralf Mikut category:cs.CV published:2016-02-09 summary:Using the knowledge discovery framework, it is possible to explore objectdatabases and extract groups of objects with highly heterogeneous movementbehavior by efficiently integrating a priori knowledge through interacting withthe framework. The whole process is modular expandable and is thereforeadaptive to any problem formulation. Further, the flexible use of differentinformation allocation processes reveal a great potential to efficientlyincorporate the a priori knowledge of different users in different ways.Therefore, the stepwise knowledge discovery process embedded in the knowledgediscovery framework is described in detail to point out the flexibility of sucha system incorporating object databases from different applications. Thedescribed framework can be used to gain knowledge out of object databases inmany different fields. This knowledge can be used to gain further insights andimprove the understanding of underlying phenomena. The functionality of theproposed framework is exemplarily demonstrated using a benchmark database basedon real biological object data.
arxiv-1602-02850 | Toward Optimal Feature Selection in Naive Bayes for Text Categorization |  http://arxiv.org/abs/1602.02850  | author:Bo Tang, Steven Kay, Haibo He category:stat.ML cs.CL cs.IR cs.LG published:2016-02-09 summary:Automated feature selection is important for text categorization to reducethe feature size and to speed up the learning process of classifiers. In thispaper, we present a novel and efficient feature selection framework based onthe Information Theory, which aims to rank the features with theirdiscriminative capacity for classification. We first revisit two informationmeasures: Kullback-Leibler divergence and Jeffreys divergence for binaryhypothesis testing, and analyze their asymptotic properties relating to type Iand type II errors of a Bayesian classifier. We then introduce a new divergencemeasure, called Jeffreys-Multi-Hypothesis (JMH) divergence, to measuremulti-distribution divergence for multi-class classification. Based on theJMH-divergence, we develop two efficient feature selection methods, termedmaximum discrimination ($MD$) and $MD-\chi^2$ methods, for text categorization.The promising results of extensive experiments demonstrate the effectiveness ofthe proposed approaches.
arxiv-1602-02934 | Turbocharging Mini-Batch K-Means |  http://arxiv.org/abs/1602.02934  | author:James Newling, François Fleuret category:stat.ML cs.LG published:2016-02-09 summary:We propose an accelerated Mini-Batch k-means algorithm which combines threekey improvements. The first is a modified center update which results inconvergence to a local minimum in fewer iterations. The second is an adaptiveincrease of batchsize to meet an increasing requirement for centroid accuracy.The third is the inclusion of distance bounds based on the triangle inequality,which are used to eliminate distance calculations along the same lines asElkan's algorithm. The combination of the two latter constitutes a verypowerful scheme to reuse computation already done over samples untilstatistical accuracy requires the use of additional data points.
arxiv-1602-02915 | Calculus of the exponent of Kurdyka-Łojasiewicz inequality and its applications to linear convergence of first-order methods |  http://arxiv.org/abs/1602.02915  | author:Guoyin Li, Ting Kei Pong category:math.OC stat.ML published:2016-02-09 summary:In this paper, we study the Kurdyka-{\L}ojasiewicz (KL) exponent, animportant quantity for analyzing the convergence rate of first-order methods.Specifically, we develop various calculus rules to deduce the KL exponent ofnew (possibly nonconvex and nonsmooth) functions formed from functions withknown KL exponents. In addition, we show that the well-studied Luo-Tseng errorbound together with a mild assumption on the separation of stationary valuesimplies that the KL exponent is $\frac{1}{2}$. The Luo-Tseng error bound isknown to hold for a large class of concrete structured optimization problems,and thus we deduce the KL exponent of a large class of functions whoseexponents were previously unknown. Building upon this and the calculus rules,we are then able to show that for many convex or nonconvex optimization modelsfor applications, such as sparse recovery, their objective function's KLexponent is $\frac{1}{2}$. This includes the least squares problem withsmoothly clipped absolute deviation (SCAD) regularization or minimax concavepenalty (MCP) regularization and the logistic regression problem with $\ell_1$regularization. Since many existing local convergence rate analysis forfirst-order methods in the nonconvex scenario relies on the KL exponent, ourresults enable us to obtain explicit convergence rate for various first-ordermethods when they are applied to a large variety of practical optimizationmodels. Finally, we further illustrate how our results can be applied toanalyzing the local linear convergence rate of the proximal gradient algorithmand the inertial proximal algorithm for some specific models that arise insparse recovery.
arxiv-1602-02852 | Compliance-Aware Bandits |  http://arxiv.org/abs/1602.02852  | author:Nicolás Della Penna, Mark D. Reid, David Balduzzi category:stat.ML cs.LG published:2016-02-09 summary:Motivated by clinical trials, we study bandits with observablenon-compliance. At each step, the learner chooses an arm, after, instead ofobserving only the reward, it also observes the action that took place. We showthat such noncompliance can be helpful or hurtful to the learner in general.Unfortunately, naively incorporating compliance information into banditalgorithms loses guarantees on sublinear regret. We present hybrid algorithmsthat maintain regret bounds up to a multiplicative factor and can incorporatecompliance information. Simulations based on real data from the InternationalStoke Trial show the practical potential of these algorithms.
arxiv-1602-02899 | Secure Multi-Party Computation Based Privacy Preserving Extreme Learning Machine Algorithm Over Vertically Distributed Data |  http://arxiv.org/abs/1602.02899  | author:Ferhat Özgür Çatak category:cs.CR cs.LG published:2016-02-09 summary:Especially in the Big Data era, the usage of different classification methodsis increasing day by day. The success of these classification methods dependson the effectiveness of learning methods. Extreme learning machine (ELM)classification algorithm is a relatively new learning method built onfeed-forward neural-network. ELM classification algorithm is a simple and fastmethod that can create a model from high-dimensional data sets. Traditional ELMlearning algorithm implicitly assumes complete access to whole data set. Thisis a major privacy concern in most of cases. Sharing of private data (i.e.medical records) is prevented because of security concerns. In this research,we propose an efficient and secure privacy-preserving learning algorithm forELM classification over data that is vertically partitioned among severalparties. The new learning method preserves the privacy on numerical attributes,builds a classification model without sharing private data without disclosingthe data of each party to others.
arxiv-1602-02881 | Detection and Visualization of Endoleaks in CT Data for Monitoring of Thoracic and Abdominal Aortic Aneurysm Stents |  http://arxiv.org/abs/1602.02881  | author:Jing Lu, Jan Egger, Andreas Wimmer, Stefan Großkopf, Bernd Freisleben category:cs.CV cs.CG cs.GR published:2016-02-09 summary:In this paper we present an efficient algorithm for the segmentation of theinner and outer boundary of thoratic and abdominal aortic aneurysms (TAA & AAA)in computed tomography angiography (CTA) acquisitions. The aneurysmsegmentation includes two steps: first, the inner boundary is segmented basedon a grey level model with two thresholds; then, an adapted active contourmodel approach is applied to the more complicated outer boundary segmentation,with its initialization based on the available inner boundary segmentation. Anopacity image, which aims at enhancing important features while reducingspurious structures, is calculated from the CTA images and employed to guidethe deformation of the model. In addition, the active contour model is extendedby a constraint force that prevents intersections of the inner and outerboundary and keeps the outer boundary at a distance, given by the thrombusthickness, to the inner boundary. Based upon the segmentation results, we canmeasure the aneurysm size at each centerline point on the centerline orthogonalmultiplanar reformatting (MPR) plane. Furthermore, a 3D TAA or AAA model isreconstructed from the set of segmented contours, and the presence of endoleaksis detected and highlighted. The implemented method has been evaluated on nineclinical CTA data sets with variations in anatomy and location of the pathologyand has shown promising results.
arxiv-1602-02867 | Value Iteration Networks |  http://arxiv.org/abs/1602.02867  | author:Aviv Tamar, Sergey Levine, Pieter Abbeel category:cs.AI cs.LG cs.NE stat.ML published:2016-02-09 summary:We introduce the value iteration network: a fully differentiable neuralnetwork with a `planning module' embedded within. Value iteration networks aresuitable for making predictions about outcomes that involve planning-basedreasoning, such as predicting a desired trajectory from an observation of amap. Key to our approach is a novel differentiable approximation of thevalue-iteration algorithm, which can be represented as a convolutional neuralnetwork, and trained end-to-end using standard backpropagation. We evaluate ourvalue iteration networks on the task of predicting optimal obstacle-avoidingtrajectories from an image of a landscape, both on synthetic data, and onchallenging raw images of the Mars terrain.
arxiv-1602-02865 | The Role of Typicality in Object Classification: Improving The Generalization Capacity of Convolutional Neural Networks |  http://arxiv.org/abs/1602.02865  | author:Babak Saleh, Ahmed Elgammal, Jacob Feldman category:cs.CV cs.LG cs.NE published:2016-02-09 summary:Deep artificial neural networks have made remarkable progress in differenttasks in the field of computer vision. However, the empirical analysis of thesemodels and investigation of their failure cases has received attentionrecently. In this work, we show that deep learning models cannot generalize toatypical images that are substantially different from training images. This isin contrast to the superior generalization ability of the visual system in thehuman brain. We focus on Convolutional Neural Networks (CNN) as thestate-of-the-art models in object recognition and classification; investigatethis problem in more detail, and hypothesize that training CNN models sufferfrom unstructured loss minimization. We propose computational models to improvethe generalization capacity of CNNs by considering how typical a training imagelooks like. By conducting an extensive set of experiments we show thatinvolving a typicality measure can improve the classification results on a newset of images by a large margin. More importantly, this significant improvementis achieved without fine-tuning the CNN model on the target image set.
arxiv-1602-02862 | A Feature-Based Prediction Model of Algorithm Selection for Constrained Continuous Optimisation |  http://arxiv.org/abs/1602.02862  | author:Shayan Poursoltan, Frank Neumann category:cs.NE published:2016-02-09 summary:With this paper, we contribute to the growing research area of feature-basedanalysis of bio-inspired computing. In this research area, problem instancesare classified according to different features of the underlying problem interms of their difficulty of being solved by a particular algorithm. Weinvestigate the impact of different sets of evolved instances for buildingprediction models in the area of algorithm selection. Building on the work ofPoursoltan and Neumann [11,10], we consider how evolved instances can be usedto predict the best performing algorithm for constrained continuousoptimisation from a set of bio-inspired computing methods, namely highperforming variants of differential evolution, particle swarm optimization, andevolution strategies. Our experimental results show that instances evolved witha multi-objective approach in combination with random instances of theunderlying problem allow to build a model that accurately predicts the bestperforming algorithm for a wide range of problem instances.
arxiv-1602-02887 | Classification with Boosting of Extreme Learning Machine Over Arbitrarily Partitioned Data |  http://arxiv.org/abs/1602.02887  | author:Ferhat Özgür Çatak category:cs.LG published:2016-02-09 summary:Machine learning based computational intelligence methods are widely used toanalyze large scale data sets in this age of big data. Extracting usefulpredictive modeling from these types of data sets is a challenging problem dueto their high complexity. Analyzing large amount of streaming data that can beleveraged to derive business value is another complex problem to solve. Withhigh levels of data availability (\textit{i.e. Big Data}) automaticclassification of them has become an important and complex task. Hence, weexplore the power of applying MapReduce based Distributed AdaBoosting ofExtreme Learning Machine (ELM) to build a predictive bag of classificationmodels. Accordingly, (i) data set ensembles are created; (ii) ELM algorithm isused to build weak learners (classifier functions); and (iii) builds a stronglearner from a set of weak learners. We applied this training model to thebenchmark knowledge discovery and data mining data sets.
arxiv-1602-03012 | EndoNet: A Deep Architecture for Recognition Tasks on Laparoscopic Videos |  http://arxiv.org/abs/1602.03012  | author:Andru P. Twinanda, Sherif Shehata, Didier Mutter, Jacques Marescaux, Michel de Mathelin, Nicolas Padoy category:cs.CV published:2016-02-09 summary:Surgical workflow recognition has numerous potential medical applications,such as the automatic indexing of surgical video databases and the optimizationof real-time OR scheduling, among others. As a result, phase recognition hasbeen studied in the context of several kinds of surgeries, such as cataract,neurological, and laparoscopic surgeries. In the literature, two types offeatures are typically used to perform this task: visual features and toolusage signals. However, the visual features used are mostly handcrafted.Furthermore, since additional equipment is needed to obtain the tool usagesignals automatically, they are usually collected via a tedious manualannotation process. In this paper, we propose a novel method for phaserecognition that uses a convolutional neural network (CNN) to automaticallylearn features from cholecystectomy videos and that relies uniquely on visualinformation. In previous studies, it has been shown that the tool signals canprovide valuable information in performing the phase recognition task. Thus, wepresent a novel CNN architecture, called EndoNet, that is designed to carry outthe phase recognition and tool presence detection tasks in a multi-task manner.To the best of our knowledge, this is the first work proposing to use a CNN formultiple recognition tasks on laparoscopic videos. Extensive experimentalcomparisons to other methods show that EndoNet yields state-of-the-art resultsfor both tasks.
arxiv-1602-02842 | Collaborative filtering via sparse Markov random fields |  http://arxiv.org/abs/1602.02842  | author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.IR cs.LG published:2016-02-09 summary:Recommender systems play a central role in providing individualized access toinformation and services. This paper focuses on collaborative filtering, anapproach that exploits the shared structure among mind-liked users and similaritems. In particular, we focus on a formal probabilistic framework known asMarkov random fields (MRF). We address the open problem of structure learningand introduce a sparsity-inducing algorithm to automatically estimate theinteraction structures between users and between items. Item-item and user-usercorrelation networks are obtained as a by-product. Large-scale experiments onmovie recommendation and date matching datasets demonstrate the power of theproposed method.
arxiv-1602-03027 | Minimax Lower Bounds for Realizable Transductive Classification |  http://arxiv.org/abs/1602.03027  | author:Ilya Tolstikhin, David Lopez-Paz category:stat.ML cs.LG published:2016-02-09 summary:Transductive learning considers a training set of $m$ labeled samples and atest set of $u$ unlabeled samples, with the goal of best labeling thatparticular test set. Conversely, inductive learning considers a training set of$m$ labeled samples drawn iid from $P(X,Y)$, with the goal of best labeling anyfuture samples drawn iid from $P(X)$. This comparison suggests thattransduction is a much easier type of inference than induction, but is thisreally the case? This paper provides a negative answer to this question, byproving the first known minimax lower bounds for transductive, realizable,binary classification. Our lower bounds show that $m$ should be at least$\Omega(d/\epsilon + \log(1/\delta)/\epsilon)$ when $\epsilon$-learning aconcept class $\mathcal{H}$ of finite VC-dimension $d<\infty$ with confidence$1-\delta$, for all $m \leq u$. This result draws three important conclusions.First, general transduction is as hard as general induction, since bothproblems have $\Omega(d/m)$ minimax values. Second, the use of unlabeled datadoes not help general transduction, since supervised learning algorithms suchas ERM and (Hanneke, 2015) match our transductive lower bounds while ignoringthe unlabeled test set. Third, our transductive lower bounds imply lower boundsfor semi-supervised learning, which add to the important discussion about therole of unlabeled data in machine learning.
arxiv-1602-03040 | The Structured Weighted Violations Perceptron Algorithm |  http://arxiv.org/abs/1602.03040  | author:Rotem Dror, Roi Reichart category:cs.LG published:2016-02-09 summary:We present the Structured Weighted Violations Perceptron (SWVP) algorithm, anew perceptron algorithm for structured prediction, that generalizes theCollins Structured Perceptron (CSP, (Collins, 2002)). Unlike CSP, the updaterule of SWVP explicitly exploits the internal structure of the predictedlabels. We prove that for linearly separable training sets, SWVP converges to aweight vector that separates the data, under certain conditions on theparameters of the algorithm. We further prove bounds for SWVP on: (a) thenumber of updates in the separable case; (b) mistakes in the non-separablecase; and (c) the probability to misclassify an unseen example(generalization), and show that for most SWVP variants these bounds are tighterthan those of the CSP special case. In synthetic data experiments where data isdrawn from a generative hidden variable model, SWVP provides substantialimprovements over CSP.
arxiv-1602-02995 | Segmental Spatio-Temporal CNNs for Fine-grained Action Segmentation and Classification |  http://arxiv.org/abs/1602.02995  | author:Colin Lea, Austin Reiter, Rene Vidal, Gregory D. Hager category:cs.CV cs.RO published:2016-02-09 summary:Joint segmentation and classification of fine-grained actions is importantfor applications in human-robot interaction, video surveillance, and humanskill evaluation. However, despite substantial recent progress in large scaleaction classification, the performance of state-of-the-art fine-grained actionrecognition approaches remains low. In this paper, we propose a newspatio-temporal CNN model for fine-grained action classification andsegmentation, which combines (1) a spatial CNN to represent objects in thescene and their spatial relationships; (2) a temporal CNN that captures howobject relationships within an action change over time; and (3) a semi-Markovmodel that captures transitions from one action to another. In addition, weintroduce an efficient segmental inference algorithm for joint segmentation andclassification of actions that is orders of magnitude faster thanstate-of-the-art approaches. We highlight the effectiveness of our approach oncooking and surgical action datasets for which we observe substantiallyimproved performance relative to recent baseline methods.
arxiv-1602-02885 | Joint Defogging and Demosaicking |  http://arxiv.org/abs/1602.02885  | author:Y. J. Lee, K. Hirakawa, T. Q. Nguyen category:cs.CV published:2016-02-09 summary:Image defogging is a technique used extensively for enhancing visual qualityof images in bad weather condition. Even though defogging algorithms have beenwell studied, defogging performance is degraded by demosaicking artifacts andsensor noise amplification in distant scenes. In order to improve visualquality of restored images, we propose a novel approach to perform defoggingand demosaicking simultaneously. We conclude that better defogging performancewith fewer artifacts can be achieved when a defogging algorithm is combinedwith a demosaicking algorithm simultaneously. We also demonstrate that theproposed joint algorithm has the benefit of suppressing noise amplification indistant scene. In addition, we validate our theoretical analysis andobservations for both synthesized datasets with ground truth fog-free imagesand natural scene datasets captured in a raw format.
arxiv-1602-03014 | Herding as a Learning System with Edge-of-Chaos Dynamics |  http://arxiv.org/abs/1602.03014  | author:Yutian Chen, Max Welling category:stat.ML cs.LG published:2016-02-09 summary:Herding defines a deterministic dynamical system at the edge of chaos. Itgenerates a sequence of model states and parameters by alternating parameterperturbations with state maximizations, where the sequence of states can beinterpreted as "samples" from an associated MRF model. Herding differs frommaximum likelihood estimation in that the sequence of parameters does notconverge to a fixed point and differs from an MCMC posterior sampling approachin that the sequence of states is generated deterministically. Herding may beinterpreted as a"perturb and map" method where the parameter perturbations aregenerated using a deterministic nonlinear dynamical system rather than randomlyfrom a Gumbel distribution. This chapter studies the distinct statisticalcharacteristics of the herding algorithm and shows that the fast convergencerate of the controlled moments may be attributed to edge of chaos dynamics. Theherding algorithm can also be generalized to models with latent variables andto a discriminative learning setting. The perceptron cycling theorem ensuresthat the fast moment matching property is preserved in the more generalframework.
arxiv-1602-03131 | Large scale multi-objective optimization: Theoretical and practical challenges |  http://arxiv.org/abs/1602.03131  | author:Kinjal Basu, Ankan Saha, Shaunak Chatterjee category:stat.AP math.OC stat.ML published:2016-02-09 summary:Multi-objective optimization (MOO) is a well-studied problem for severalimportant recommendation problems. While multiple approaches have beenproposed, in this work, we focus on using constrained optimization formulations(e.g., quadratic and linear programs) to formulate and solve MOO problems. Thisapproach can be used to pick desired operating points on the trade-off curvebetween multiple objectives. It also works well for internet applications whichserve large volumes of online traffic, by working with Lagrangian dualityformulation to connect dual solutions (computed offline) with the primalsolutions (computed online). We identify some key limitations of this approach -- namely the inability tohandle user and item level constraints, scalability considerations and varianceof dual estimates introduced by sampling processes. We propose solutions foreach of the problems and demonstrate how through these solutions wesignificantly advance the state-of-the-art in this realm. Our proposed methodscan exactly handle user and item (and other such local) constraints, achieve a$100\times$ scalability boost over existing packages in R and reduce varianceof dual estimates by two orders of magnitude.
arxiv-1602-02823 | Poor starting points in machine learning |  http://arxiv.org/abs/1602.02823  | author:Mark Tygert category:cs.LG cs.NE math.OC stat.ML published:2016-02-09 summary:Poor (even random) starting points for learning/training/optimization arecommon in machine learning. In many settings, the method of Robbins and Monro(online stochastic gradient descent) is known to be optimal for good startingpoints, but may not be optimal for poor starting points -- indeed, for poorstarting points Nesterov acceleration can help during the initial iterations,even though Nesterov methods not designed for stochastic approximation couldhurt during later iterations. The common practice of training with nontrivialminibatches enhances the advantage of Nesterov acceleration.
arxiv-1602-02822 | Parameterizing Region Covariance: An Efficient Way To Apply Sparse Codes On Second Order Statistics |  http://arxiv.org/abs/1602.02822  | author:Xiyang Dai, Sameh Khamis, Yangmuzi Zhang, Larry S. Davis category:cs.CV published:2016-02-09 summary:Sparse representations have been successfully applied to signal processing,computer vision and machine learning. Currently there is a trend to learnsparse models directly on structure data, such as region covariance. However,such methods when combined with region covariance often require complexcomputation. We present an approach to transform a structured sparse modellearning problem to a traditional vectorized sparse modeling problem byconstructing a Euclidean space representation for region covariance matrices.Our new representation has multiple advantages. Experiments on several visiontasks demonstrate competitive performance with the state-of-the-art methods.
arxiv-1602-03061 | Minimum Conditional Description Length Estimation for Markov Random Fields |  http://arxiv.org/abs/1602.03061  | author:Matthew G. Reyes, David L. Neuhoff category:cs.IT cs.LG math.IT math.ST stat.TH published:2016-02-09 summary:In this paper we discuss a method, which we call Minimum ConditionalDescription Length (MCDL), for estimating the parameters of a subset of siteswithin a Markov random field. We assume that the edges are known for the entiregraph $G=(V,E)$. Then, for a subset $U\subset V$, we estimate the parametersfor nodes and edges in $U$ as well as for edges incident to a node in $U$, byfinding the exponential parameter for that subset that yields the bestcompression conditioned on the values on the boundary $\partial U$. Ourestimate is derived from a temporally stationary sequence of observations onthe set $U$. We discuss how this method can also be applied to estimate aspatially invariant parameter from a single configuration, and in so doing,derive the Maximum Pseudo-Likelihood (MPL) estimate.
arxiv-1602-03048 | Bayesian nonparametric image segmentation using a generalized Swendsen-Wang algorithm |  http://arxiv.org/abs/1602.03048  | author:Richard Yi Da Xu, Francois Caron, Arnaud Doucet category:stat.ML published:2016-02-09 summary:Unsupervised image segmentation aims at clustering the set of pixels of animage into spatially homogeneous regions. We introduce here a class of Bayesiannonparametric models to address this problem. These models are based on acombination of a Potts-like spatial smoothness component and a prior onpartitions which is used to control both the number and size of clusters. Thisclass of models is flexible enough to include the standard Potts model and themore recent Potts-Dirichlet Process model \cite{Orbanz2008}. More importantly,any prior on partitions can be introduced to control the global clusteringstructure so that it is possible to penalize small or large clusters ifnecessary. Bayesian computation is carried out using an original generalizedSwendsen-Wang algorithm. Experiments demonstrate that our method is competitivein terms of RAND\ index compared to popular image segmentation methods, such asmean-shift, and recent alternative Bayesian nonparametric models.
arxiv-1602-03032 | Associative Long Short-Term Memory |  http://arxiv.org/abs/1602.03032  | author:Ivo Danihelka, Greg Wayne, Benigno Uria, Nal Kalchbrenner, Alex Graves category:cs.NE published:2016-02-09 summary:We investigate a new method to augment recurrent neural networks with extramemory without increasing the number of network parameters. The system has anassociative memory based on complex-valued vectors and is closely related toHolographic Reduced Representations and Long Short-Term Memory networks.Holographic Reduced Representations have limited capacity: as they store moreinformation, each retrieval becomes noisier due to interference. Our system incontrast creates redundant copies of stored information, which enablesretrieval with reduced noise. Experiments demonstrate faster learning onmultiple memorization tasks.
arxiv-1602-02964 | A Kernel Test of Goodness of Fit |  http://arxiv.org/abs/1602.02964  | author:Kacper Chwialkowski, Heiko Strathmann, Arthur Gretton category:stat.ML published:2016-02-09 summary:We propose a nonparametric statistical test for goodness-of-fit: given a setof samples, the test determines how likely it is that these were generated froma target density function. The measure of goodness-of-fit is a divergenceconstructed via Stein's method using functions from a Reproducing KernelHilbert Space. Our test statistic is based on an empirical estimate of thisdivergence, taking the form of a V-statistic in terms of the log gradients ofthe target density and the kernel. We derive a statistical test, both fori.i.d. and non-i.i.d. samples, where we estimate the null distributionquantiles using a wild bootstrap procedure. We apply our test to quantifyingconvergence of approximate Markov Chain Monte Carlo methods, statistical modelcriticism, and evaluating quality of fit vs model complexity in nonparametricdensity estimation.
arxiv-1602-03105 | Graphical Model Sketch |  http://arxiv.org/abs/1602.03105  | author:Branislav Kveton, Hung Bui, Mohammad Ghavamzadeh, Georgios Theocharous, S. Muthukrishnan, Siqi Sun category:cs.DS cs.LG stat.ML published:2016-02-09 summary:Structured high-cardinality data arises in many domains and poses a majorchallenge for both modeling and inference, which is beyond current graphicalmodel frameworks. We view these data as a stream $(x^{(t)})_{t = 1}^n$ of $n$observations from an unknown distribution $P$, where $x^{(t)} \in [M]^K$ is a$K$-dimensional vector and $M$ is the cardinality of its entries, which is verylarge. Suppose that the graphical model $\mathcal{G}$ of $P$ is known, and let$\bar{P}$ be the maximum-likelihood estimate (MLE) of $P$ from $(x^{(t)})_{t =1}^n$ conditioned on $\mathcal{G}$. In this work, we design and analyzealgorithms that approximate $\bar{P}$ with $\hat{P}$, such that $\hat{P}(x)\approx \bar{P}(x)$ for any $x \in [M]^K$ with a high probability, andcrucially in the space independent of $M$. The key idea of our approximationsis to use the structure of $\mathcal{G}$ and approximately estimate its factorsby "sketches". The sketches hash high-cardinality variables using randomprojections. Our approximations are computationally and space efficient, beingindependent of $M$. Our error bounds are multiplicative and provably improveupon those of the count-min (CM) sketch, a state-of-the-art approach toestimating the frequency of values in a stream, in a class of naive Bayesmodels. We evaluate our algorithms on synthetic and real-world problems, andreport an order of magnitude improvements over the CM sketch.
arxiv-1602-03220 | Discriminative Regularization for Generative Models |  http://arxiv.org/abs/1602.03220  | author:Alex Lamb, Vincent Dumoulin, Aaron Courville category:stat.ML cs.LG published:2016-02-09 summary:We explore the question of whether the representations learned by classifierscan be used to enhance the quality of generative models. Our conjecture is thatlabels correspond to characteristics of natural data which are most salient tohumans: identity in faces, objects in images, and utterances in speech. Wepropose to take advantage of this by using the representations fromdiscriminative classifiers to augment the objective function corresponding to agenerative model. In particular we enhance the objective function of thevariational autoencoder, a popular generative model, with a discriminativeregularization term. We show that enhancing the objective function in this wayleads to samples that are clearer and have higher visual quality than thesamples from the standard variational autoencoders.
arxiv-1602-03145 | A New Spatio-Spectral Morphological Segmentation For Multi-Spectral Remote-Sensing Images |  http://arxiv.org/abs/1602.03145  | author:Guillaume Noyel, Jesus Angulo, Dominique Jeulin category:cs.CV published:2016-02-09 summary:A general framework of spatio-spectral segmentation for multi-spectral imagesis introduced in this paper. The method is based on classification-drivenstochastic watershed (WS) by Monte Carlo simulations, and it gives more regularand reliable contours than standard WS. The present approach is decomposed intoseveral sequential steps. First, a dimensionality-reduction stage is performedusing the factor-correspondence analysis method. In this context, a new way toselect the factor axes (eigenvectors) according to their spatial information isintroduced. Then, a spectral classification produces a spectralpre-segmentation of the image. Subsequently, a probability density function(pdf) of contours containing spatial and spectral information is estimated bysimulation using a stochastic WS approach driven by the spectralclassification. The pdf of the contours is finally segmented by a WS controlledby markers from a regularization of the initial classification.
arxiv-1602-03146 | DCM Bandits: Learning to Rank with Multiple Clicks |  http://arxiv.org/abs/1602.03146  | author:Sumeet Katariya, Branislav Kveton, Csaba Szepesvári, Zheng Wen category:cs.LG stat.ML published:2016-02-09 summary:Search engines recommend a list of web pages. The user examines this list,from the first page to the last, and may click on multiple attractive pages.This type of user behavior can be modeled by the \emph{dependent click model(DCM)}. In this work, we propose \emph{DCM bandits}, an online learning variantof the DCM model where the objective is to maximize the probability ofrecommending a satisfactory item. The main challenge of our problem is that thelearning agent does not observe the reward. It only observes the clicks. Thisimbalance between the feedback and rewards makes our setting challenging. Wepropose a computationally-efficient learning algorithm for our problem, whichwe call dcmKL-UCB; derive gap-dependent upper bounds on its regret underreasonable assumptions; and prove a matching lower bound up to logarithmicfactors. We experiment with dcmKL-UCB on both synthetic and real-worldproblems. Our algorithm outperforms a range of baselines and performs well evenwhen our modeling assumptions are violated. To the best of our knowledge, thisis the first regret-optimal online learning algorithm for learning to rank withmultiple clicks in a cascade-like model.
arxiv-1602-03205 | Image encryption with dynamic chaotic Look-Up Table |  http://arxiv.org/abs/1602.03205  | author:Med Karim Abdmouleh, Ali Khalfallah, Med Salim Bouhlel category:cs.CR cs.CV published:2016-02-09 summary:In this paper we propose a novel image encryption scheme. The proposed methodis based on the chaos theory. Our cryptosystem uses the chaos theory to definea dynamic chaotic Look-Up Table (LUT) to compute the new value of the currentpixel to cipher. Applying this process on each pixel of the plain image, wegenerate the encrypted image. The results of different experimental tests, suchas Key space analysis, Information Entropy and Histogram analysis, show thatthe proposed encryption image scheme seems to be protected against variousattacks. A comparison between the plain and encrypted image, in terms ofcorrelation coefficient, proves that the plain image is very different from theencrypted one.
arxiv-1602-02888 | Robust Ensemble Classifier Combination Based on Noise Removal with One-Class SVM |  http://arxiv.org/abs/1602.02888  | author:Ferhat Özgür Çatak category:cs.LG published:2016-02-09 summary:In machine learning area, as the number of labeled input samples becomes verylarge, it is very difficult to build a classification model because of inputdata set is not fit in a memory in training phase of the algorithm, therefore,it is necessary to utilize data partitioning to handle overall data set.Bagging and boosting based data partitioning methods have been broadly used indata mining and pattern recognition area. Both of these methods have shown agreat possibility for improving classification model performance. This study isconcerned with the analysis of data set partitioning with noise removal and itsimpact on the performance of multiple classifier models. In this study, wepropose noise filtering preprocessing at each data set partition to incrementclassifier model performance. We applied Gini impurity approach to find thebest split percentage of noise filter ratio. The filtered sub data set is thenused to train individual ensemble models.
arxiv-1602-03218 | Learning Efficient Algorithms with Hierarchical Attentive Memory |  http://arxiv.org/abs/1602.03218  | author:Marcin Andrychowicz, Karol Kurach category:cs.LG published:2016-02-09 summary:In this paper, we propose and investigate a novel memory architecture forneural networks called Hierarchical Attentive Memory (HAM). It is based on abinary tree with leaves corresponding to memory cells. This allows HAM toperform memory access in O(log n) complexity, which is a significantimprovement over the standard attention mechanism that requires O(n)operations, where n is the size of the memory. We show that an LSTM network augmented with HAM can learn algorithms forproblems like merging, sorting or binary searching from pure input-outputexamples. In particular, it learns to sort n numbers in time O(n log n) andgeneralizes well to input sequences much longer than the ones seen during thetraining. We also show that HAM can be trained to act like classic datastructures: a stack, a FIFO queue and a priority queue.
arxiv-1602-02845 | Online Active Linear Regression via Thresholding |  http://arxiv.org/abs/1602.02845  | author:Carlos Riquelme, Ramesh Johari, Baosen Zhang category:stat.ML cs.LG published:2016-02-09 summary:We consider the problem of online active learning to collect data forregression modeling. Specifically, we consider a decision maker that faces alimited experimentation budget but must efficiently learn an underlying linearpopulation model. Our goal is to develop algorithms that provide substantialgains over passive random sampling of observations. To that end, our maincontribution is a novel threshold-based algorithm for selection ofobservations; we characterize its performance and related lower bounds. We alsoapply our approach successfully to regularized regression. Simulations suggestthe algorithm is remarkably robust: it provides significant benefits overpassive random sampling even in several real-world datasets that exhibit highnonlinearity and high dimensionality --- significantly reducing the mean andvariance of the squared error.
arxiv-1602-02660 | Exploiting Cyclic Symmetry in Convolutional Neural Networks |  http://arxiv.org/abs/1602.02660  | author:Sander Dieleman, Jeffrey De Fauw, Koray Kavukcuoglu category:cs.LG cs.CV cs.NE published:2016-02-08 summary:Many classes of images exhibit rotational symmetry. Convolutional neuralnetworks are sometimes trained using data augmentation to exploit this, butthey are still required to learn the rotation equivariance properties from thedata. Encoding these properties into the network architecture, as we arealready used to doing for translation equivariance by using convolutionallayers, could result in a more efficient use of the parameter budget byrelieving the model from learning them. We introduce four operations which canbe inserted into neural network models as layers, and which can be combined tomake these models partially equivariant to rotations. They also enableparameter sharing across different orientations. We evaluate the effect ofthese architectural modifications on three datasets which exhibit rotationalsymmetry and demonstrate improved performance with smaller models.
arxiv-1602-02499 | The "Sprekend Nederland" project and its application to accent location |  http://arxiv.org/abs/1602.02499  | author:David A. van Leeuwen, Rosemary Orr category:stat.ML cs.CL published:2016-02-08 summary:This paper describes the data collection effort that is part of the projectSprekend Nederland (The Netherlands Talking), and discusses its potential usein Automatic Accent Location. We define Automatic Accent Location as the taskto describe the accent of a speaker in terms of the location of the speaker andits history. We discuss possible ways of describing accent location, theconsequence these have for the task of automatic accent location, and potentialevaluation metrics.
arxiv-1602-02505 | Binarized Neural Networks |  http://arxiv.org/abs/1602.02505  | author:Itay Hubara, Daniel Soudry, Ran El Yaniv category:cs.LG cs.NE published:2016-02-08 summary:We introduce a method to train Binarized Neural Networks (BNNs) - neuralnetworks with binary weights and activations at run-time and when computing theparameters' gradient at train-time. We conduct two sets of experiments, eachbased on a different framework, namely Torch7 and Theano, where we train BNNson MNIST, CIFAR-10 and SVHN, and achieve nearly state-of-the-art results.During the forward pass, BNNs drastically reduce memory size and accesses, andreplace most arithmetic operations with bit-wise operations, which might leadto a great increase in power-efficiency. Last but not least, we wrote a binarymatrix multiplication GPU kernel with which it is possible to run our MNIST BNN7 times faster than with an unoptimized GPU kernel, without suffering any lossin classification accuracy. The code for training and running our BNNs isavailable.
arxiv-1602-02644 | Generating Images with Perceptual Similarity Metrics based on Deep Networks |  http://arxiv.org/abs/1602.02644  | author:Alexey Dosovitskiy, Thomas Brox category:cs.LG cs.CV cs.NE published:2016-02-08 summary:Image-generating machine learning models are typically trained with lossfunctions based on distance in the image space. This often leads toover-smoothed results. We propose a class of loss functions, which we call deepperceptual similarity metrics (DeePSiM), that mitigate this problem. Instead ofcomputing distances in the image space, we compute distances between imagefeatures extracted by deep neural networks. This metric better reflectsperceptually similarity of images and thus leads to better results. We showthree applications: autoencoder training, a modification of a variationalautoencoder, and inversion of deep convolutional networks. In all cases, thegenerated images look sharp and resemble natural images.
arxiv-1602-02658 | Graying the black box: Understanding DQNs |  http://arxiv.org/abs/1602.02658  | author:Tom Zahavy, Nir Ben Zrihem, Shie Mannor category:cs.LG cs.AI cs.NE published:2016-02-08 summary:In recent years there is a growing interest in using deep representations forreinforcement learning. In this paper, we present a methodology and tools toanalyze Deep Q-networks (DQNs) in a non-blind matter. Using our tools we revealthat the features learned by DQNs aggregate the state space in a hierarchicalfashion, explaining its success. Moreover we are able to understand anddescribe the policies learned by DQNs for three different Atari2600 games andsuggest ways to interpret, debug and optimize of deep neural networks inReinforcement Learning.
arxiv-1602-02722 | Contextual-MDPs for PAC-Reinforcement Learning with Rich Observations |  http://arxiv.org/abs/1602.02722  | author:Akshay Krishnamurthy, Alekh Agarwal, John Langford category:cs.LG stat.ML published:2016-02-08 summary:We propose and study a new tractable model for reinforcement learning withhigh-dimensional observation called Contextual-MDPs, generalizing contextualbandits to a sequential decision making setting. These models require an agentto take actions based on high-dimensional observations (features) with the goalof achieving long-term performance competitive with a large set of policies.Since the size of the observation space is a primary obstacle tosample-efficient learning, Contextual-MDPs are assumed to be summarizable by asmall number of hidden states. In this setting, we design a new reinforcementlearning algorithm that engages in global exploration while using a functionclass to approximate future performance. We also establish a sample complexityguarantee for this algorithm, proving that it learns near optimal behaviorafter a number of episodes that is polynomial in all relevant parameters,logarithmic in the number of policies, and independent of the size of theobservation space. This represents an exponential improvement on the samplecomplexity of all existing alternative approaches and provides theoreticaljustification for reinforcement learning with function approximation.
arxiv-1602-02481 | A Large Dataset of Object Scans |  http://arxiv.org/abs/1602.02481  | author:Sungjoon Choi, Qian-Yi Zhou, Stephen Miller, Vladlen Koltun category:cs.CV cs.GR published:2016-02-08 summary:We have created a dataset of more than ten thousand 3D scans of real objects.To create the dataset, we recruited 70 operators, equipped them withconsumer-grade mobile 3D scanning setups, and paid them to scan objects intheir environments. The operators scanned objects of their choosing, outsidethe laboratory and without direct supervision by computer vision professionals.The result is a large and diverse collection of object scans: from shoes, mugs,and toys to grand pianos, construction vehicles, and large outdoor sculptures.We worked with an attorney to ensure that data acquisition did not violateprivacy constraints. The acquired data was irrevocably placed in the publicdomain and is available freely at http://redwood-data.org/3dscan .
arxiv-1602-02726 | Local and Global Convergence of a General Inertial Proximal Splitting Scheme |  http://arxiv.org/abs/1602.02726  | author:Patrick R. Johnstone, Pierre Moulin category:math.OC cs.LG math.NA published:2016-02-08 summary:This paper is concerned with convex composite minimization problems in aHilbert space. In these problems, the objective is the sum of two closed,proper, and convex functions where one is smooth and the other admits acomputationally inexpensive proximal operator. We analyze a general family ofinertial proximal splitting algorithms (GIPSA) for solving such problems. Weestablish finiteness of the sum of squared increments of the iterates andoptimality of the accumulation points. Weak convergence of the entire sequencethen follows if the minimum is attained. Our analysis unifies and extendsseveral previous results. We then focus on $\ell_1$-regularized optimization, which is the ubiquitousspecial case where the nonsmooth term is the $\ell_1$-norm. For certainparameter choices, GIPSA is amenable to a local analysis for this problem. Forthese choices we show that GIPSA achieves finite "active manifoldidentification", i.e. convergence in a finite number of iterations to theoptimal support and sign, after which GIPSA reduces to minimizing a localsmooth function. Local linear convergence then holds under certain conditions.We determine the rate in terms of the inertia, stepsize, and local curvature.Our local analysis is applicable to certain recent variants of the FastIterative Shrinkage-Thresholding Algorithm (FISTA), for which we establishactive manifold identification and local linear convergence. Our analysismotivates the use of a momentum restart scheme in these FISTA variants toobtain the optimal local linear convergence rate.
arxiv-1602-02720 | Multimodal Remote Sensing Image Registration with Accuracy Estimation at Local and Global Scales |  http://arxiv.org/abs/1602.02720  | author:M. L. Uss, B. Vozel, V. V. Lukin, K. Chehdi category:cs.CV published:2016-02-08 summary:This paper investigates and takes advantage of estimation of registrationaccuracy for mono- and multi-modal pairs of remote sensing images, following anintegrated framework from local to global scales. At the local scale, theCramer-Rao lower bound on parameter estimation error is estimated forcharacterizing registration accuracy of local fragment correspondence between acoarsely registered pair of images. Each local correspondence is assigned itsestimated registration accuracy dependent on local image texture and noiseproperties. Opposite to the standard approach, where registration accuracy isfound a posteriori at the output of the registration process, such valuableinformation is used by us as additional a priori information in theregistration process at global scale. It greatly helps detecting and discardingoutliers and refining the estimation of geometrical transformation modelparameters. Based on these ideas, a new area-based registration method calledRAE (Registration with Accuracy Estimation) is proposed. The RAE method is ableto provide registration accuracy at the global scale as error estimationcovariance matrix of geometrical transformation model parameters that can beused to estimate point-wise registration Standard Deviation (SD). This accuracydoes not rely on any ground truth and characterizes each pair of registeredimages individually. The RAE method is proved successful with reaching subpixelaccuracy while registering the three complex multimodal and multitemporal imagepairs: optical to radar, optical to Digital Elevation Model (DEM) images andDEM to radar images. Other methods employed in comparisons fail to provideaccurate results on the same test cases.
arxiv-1602-02706 | Indistinguishable Bandits Dueling with Decoys on a Poset |  http://arxiv.org/abs/1602.02706  | author:Julien Audiffren, Ralaivola Liva category:cs.LG cs.AI published:2016-02-08 summary:We adress the problem of dueling bandits defined on partially ordered sets,or posets. In this setting, arms may not be comparable, and there may beseveral (incomparable) optimal arms. We propose an algorithm, UnchainedBandits,that efficiently finds the set of optimal arms of any poset even when pairs ofcomparable arms cannot be distinguished from pairs of incomparable arms, with aset of minimal assumptions. This algorithm relies on the concept of decoys,which stems from social psychology. For the easier case where theincomparability information may be accessible, we propose a second algorithm,SlicingBandits, which takes advantage of this information and achieves a verysignificant gain of performance compared to UnchainedBandits. We providetheoretical guarantees and experimental evaluation for both algorithms.
arxiv-1602-02701 | Compressed Online Dictionary Learning for Fast fMRI Decomposition |  http://arxiv.org/abs/1602.02701  | author:Arthur Mensch, Gaël Varoquaux, Bertrand Thirion category:stat.ML cs.LG published:2016-02-08 summary:We present a method for fast resting-state fMRI spatial decomposi-tions ofvery large datasets, based on the reduction of the temporal dimension beforeapplying dictionary learning on concatenated individual records from groups ofsubjects. Introducing a measure of correspondence between spatialdecompositions of rest fMRI, we demonstrates that time-reduced dictionarylearning produces result as reliable as non-reduced decompositions. We alsoshow that this reduction significantly improves computational scalability.
arxiv-1602-02685 | Predicting Clinical Events by Combining Static and Dynamic Information Using Recurrent Neural Networks |  http://arxiv.org/abs/1602.02685  | author:Cristóbal Esteban, Oliver Staeck, Yinchong Yang, Volker Tresp category:cs.LG cs.AI cs.NE published:2016-02-08 summary:In clinical data sets we often find static information (e.g. gender of thepatients, blood type, etc.) combined with sequences of data that are recordedduring multiple hospital visits (e.g. medications prescribed, tests performed,etc.). Recurrent Neural Networks (RNNs) have proven to be very successful formodelling sequences of data in many areas of Machine Learning. In this work wepresent an approach based on RNNs that is specifically designed for theclinical domain and that combines static and dynamic information in order topredict future events. We work with a database collected in the Charit\'{e}Hospital in Berlin that contains all the information concerning patients thatunderwent a kidney transplantation. After the transplantation three mainendpoints can occur: rejection of the kidney, loss of the kidney and death ofthe patient. Our goal is to predict, given the Electronic Health Record of eachpatient, whether any of those endpoints will occur within the next six ortwelve months after each visit to the clinic. We compared different types ofRNNs that we developed for this work, a model based on a Feedforward NeuralNetwork and a Logistic Regression model. We found that the RNN that wedeveloped based on Gated Recurrent Units provides the best performance for thistask. We also performed an additional experiment using these models to predictnext actions and found that for such use case the model based on a FeedforwardNeural Network outperformed the other models. Our hypothesis is that long-termdependencies are not as relevant in this task.
arxiv-1602-02672 | Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks |  http://arxiv.org/abs/1602.02672  | author:Jakob N. Foerster, Yannis M. Assael, Nando de Freitas, Shimon Whiteson category:cs.AI cs.LG published:2016-02-08 summary:We propose deep distributed recurrent Q-networks (DDRQN), which enable teamsof agents to learn to solve communication-based coordination tasks. In thesetasks, the agents are not given any pre-designed communication protocol.Therefore, in order to successfully communicate, they must first automaticallydevelop and agree upon their own communication protocol. We present empiricalresults on two multi-agent learning problems based on well-known riddles,demonstrating that DDRQN can successfully solve such tasks and discover elegantcommunication protocols to do so. To our knowledge, this is the first time deepreinforcement learning has succeeded in learning communication protocols. Inaddition, we present ablation experiments that confirm that each of the maincomponents of the DDRQN architecture are critical to its success.
arxiv-1602-02665 | The happiness paradox: your friends are happier than you |  http://arxiv.org/abs/1602.02665  | author:Johan Bollen, Bruno Gonçalves, Ingrid van de Leemput, Guangchen Ruan category:cs.SI cs.CL cs.HC physics.soc-ph published:2016-02-08 summary:Most individuals in social networks experience a so-called FriendshipParadox: they are less popular than their friends on average. This effect mayexplain recent findings that widespread social network media use leads toreduced happiness. However the relation between popularity and happiness ispoorly understood. A Friendship paradox does not necessarily imply a Happinessparadox where most individuals are less happy than their friends. Here wereport the first direct observation of a significant Happiness Paradox in alarge-scale online social network of $39,110$ Twitter users. Our results revealthat popular individuals are indeed happier and that a majority of individualsexperience a significant Happiness paradox. The magnitude of the latter effectis shaped by complex interactions between individual popularity, happiness, andthe fact that users cluster assortatively by level of happiness. Our resultsindicate that the topology of online social networks and the distribution ofhappiness in some populations can cause widespread psycho-social effects thataffect the well-being of billions of individuals.
arxiv-1602-02666 | A Variational Analysis of Stochastic Gradient Algorithms |  http://arxiv.org/abs/1602.02666  | author:Stephan Mandt, Matthew D. Hoffman, David M. Blei category:stat.ML cs.LG published:2016-02-08 summary:Stochastic Gradient Descent (SGD) is an important algorithm in machinelearning. With constant learning rates, it is a stochastic process that, afteran initial phase of convergence, generates samples from a stationarydistribution. We show that SGD with constant rates can be effectively used asan approximate posterior inference algorithm for probabilistic modeling.Specifically, we show how to adjust the tuning parameters of SGD such as tomatch the resulting stationary distribution to the posterior. This analysisrests on interpreting SGD as a continuous-time stochastic process and thenminimizing the Kullback-Leibler divergence between its stationary distributionand the target posterior. (This is in the spirit of variational inference.) Inmore detail, we model SGD as a multivariate Ornstein-Uhlenbeck process and thenuse properties of this process to derive the optimal parameters. Thistheoretical framework also connects SGD to modern scalable inferencealgorithms; we analyze the recently proposed stochastic gradient Fisher scoringunder this perspective. We demonstrate that SGD with properly chosen constantrates gives a new way to optimize hyperparameters in probabilistic models.
arxiv-1602-02697 | Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples |  http://arxiv.org/abs/1602.02697  | author:Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, Ananthram Swami category:cs.CR cs.LG published:2016-02-08 summary:Advances in deep learning have led to the broad adoption of Deep NeuralNetworks (DNNs) to a range of important machine learning problems, e.g.,guiding autonomous vehicles, speech recognition, malware detection. Yet,machine learning models, including DNNs, were shown to be vulnerable toadversarial samples-subtly (and often humanly indistinguishably) modifiedmalicious inputs crafted to compromise the integrity of their outputs.Adversarial examples thus enable adversaries to manipulate system behaviors.Potential attacks include attempts to control the behavior of vehicles, havespam content identified as legitimate content, or have malware identified aslegitimate software. Adversarial examples are known to transfer from one modelto another, even if the second model has a different architecture or wastrained on a different set. We introduce the first practical demonstration thatthis cross-model transfer phenomenon enables attackers to control a remotelyhosted DNN with no access to the model, its parameters, or its training data.In our demonstration, we only assume that the adversary can observe outputsfrom the target DNN given inputs chosen by the adversary. We introduce theattack strategy of fitting a substitute model to the input-output pairs in thismanner, then crafting adversarial examples based on this auxiliary model. Weevaluate the approach on existing DNN datasets and real-world settings. In oneexperiment, we force a DNN supported by MetaMind (one of the online APIs forDNN classifiers) to mis-classify inputs at a rate of 84.24%. We conclude withexperiments exploring why adversarial samples transfer between DNNs, and adiscussion on the applicability of our attack when targeting machine learningalgorithms distinct from DNNs.
arxiv-1602-02442 | A Simple Practical Accelerated Method for Finite Sums |  http://arxiv.org/abs/1602.02442  | author:Aaron Defazio category:stat.ML cs.LG published:2016-02-08 summary:We describe a novel optimization method for finite sums (such as empiricalrisk minimization problems) building on the recently introduced SAGA method.Our method achieves an accelerated convergence rate on strongly convex smoothproblems, matching the conjectured optimal rate. Our method has only oneparameter (a step size), and is radically simpler than other acceleratedmethods for finite sums.
arxiv-1602-02454 | Efficient Algorithms for Adversarial Contextual Learning |  http://arxiv.org/abs/1602.02454  | author:Vasilis Syrgkanis, Akshay Krishnamurthy, Robert E. Schapire category:cs.LG published:2016-02-08 summary:We provide the first oracle efficient sublinear regret algorithms foradversarial versions of the contextual bandit problem. In this problem, thelearner repeatedly makes an action on the basis of a context and receivesreward for the chosen action, with the goal of achieving reward competitivewith a large class of policies. We analyze two settings: i) in the transductivesetting the learner knows the set of contexts a priori, ii) in the smallseparator setting, there exists a small set of contexts such that any twopolicies behave differently in one of the contexts in the set. Our algorithmsfall into the follow the perturbed leader family \cite{Kalai2005} and achieveregret $O(T^{3/4}\sqrt{K\log(N)})$ in the transductive setting and $O(T^{2/3}d^{3/4} K\sqrt{\log(N)})$ in the separator setting, where $K$ is the number ofactions, $N$ is the number of baseline policies, and $d$ is the size of theseparator. We actually solve the more general adversarial contextualsemi-bandit linear optimization problem, whilst in the full information settingwe address the even more general contextual combinatorial optimization. Weprovide several extensions and implications of our algorithms, such asswitching regret and efficient learning with predictable sequences.
arxiv-1602-02575 | DECOrrelated feature space partitioning for distributed sparse regression |  http://arxiv.org/abs/1602.02575  | author:Xiangyu Wang, David Dunson, Chenlei Leng category:stat.ME cs.DC stat.CO stat.ML published:2016-02-08 summary:Fitting statistical models is computationally challenging when the samplesize or the dimension of the dataset is huge. An attractive approach fordown-scaling the problem size is to first partition the dataset into subsetsand then fit using distributed algorithms. The dataset can be partitionedeither horizontally (in the sample space) or vertically (in the feature space).While the majority of the literature focuses on sample space partitioning,feature space partitioning is more effective when $p\gg n$. Existing methodsfor partitioning features, however, are either vulnerable to high correlationsor inefficient in reducing the model dimension. In this paper, we solve theseproblems through a new embarrassingly parallel framework named DECO fordistributed variable selection and parameter estimation. In DECO, variables arefirst partitioned and allocated to $m$ distributed workers. The decorrelatedsubset data within each worker are then fitted via any algorithm designed forhigh-dimensional problems. We show that by incorporating the decorrelationstep, DECO can achieve consistent variable selection and parameter estimationon each subset with (almost) no assumptions. In addition, the convergence rateis nearly minimax optimal for both sparse and weakly sparse models and does NOTdepend on the partition number $m$. Extensive numerical experiments areprovided to illustrate the performance of the new framework.
arxiv-1602-02656 | LSTM Deep Neural Networks Postfiltering for Improving the Quality of Synthetic Voices |  http://arxiv.org/abs/1602.02656  | author:Marvin Coto-Jiménez, John Goddard-Close category:cs.SD cs.NE published:2016-02-08 summary:Recent developments in speech synthesis have produced systems capable ofoutcome intelligible speech, but now researchers strive to create models thatmore accurately mimic human voices. One such development is the incorporationof multiple linguistic styles in various languages and accents. HMM-based Speech Synthesis is of great interest to many researchers, due toits ability to produce sophisticated features with small footprint. Despitesuch progress, its quality has not yet reached the level of the predominantunit-selection approaches that choose and concatenate recordings of realspeech. Recent efforts have been made in the direction of improving thesesystems. In this paper we present the application of Long-Short Term Memory DeepNeural Networks as a Postfiltering step of HMM-based speech synthesis, in orderto obtain closer spectral characteristics to those of natural speech. Theresults show how HMM-voices could be improved using this approach.
arxiv-1602-02651 | Automatic Face Reenactment |  http://arxiv.org/abs/1602.02651  | author:Pablo Garrido, Levi Valgaerts, Ole Rehmsen, Thorsten Thormaehlen, Patrick Perez, Christian Theobalt category:cs.CV cs.GR published:2016-02-08 summary:We propose an image-based, facial reenactment system that replaces the faceof an actor in an existing target video with the face of a user from a sourcevideo, while preserving the original target performance. Our system is fullyautomatic and does not require a database of source expressions. Instead, it isable to produce convincing reenactment results from a short source videocaptured with an off-the-shelf camera, such as a webcam, where the userperforms arbitrary facial gestures. Our reenactment pipeline is conceived aspart image retrieval and part face transfer: The image retrieval is based ontemporal clustering of target frames and a novel image matching metric thatcombines appearance and motion to select candidate frames from the sourcevideo, while the face transfer uses a 2D warping strategy that preserves theuser's identity. Our system excels in simplicity as it does not rely on a 3Dface model, it is robust under head motion and does not require the source andtarget performance to be similar. We show convincing reenactment results forvideos that we recorded ourselves and for low-quality footage taken from theInternet.
arxiv-1602-02514 | Fast k-means with accurate bounds |  http://arxiv.org/abs/1602.02514  | author:James Newling, François Fleuret category:stat.ML cs.LG published:2016-02-08 summary:We propose a novel accelerated exact k-means algorithm, which performs betterthan the current state-of-the-art low-dimensional algorithm in 18 of 22experiments, running up to 3 times faster. We also propose a generalimprovement of existing state-of-the-art accelerated exact k-means algorithmsthrough better estimates of the distance bounds used to reduce the number ofdistance calculations, and get a speedup in 36 of 44 experiments, up to 1.8times faster. We have conducted experiments with our own implementations of existingmethods to ensure homogeneous evaluation of performance, and we show that ourimplementations perform as well or better than existing availableimplementations. Finally, we propose simplified variants of standard approachesand show that they are faster than their fully-fledged counterparts in 59 of 62experiments.
arxiv-1602-02616 | Guarantees in Wasserstein Distance for the Langevin Monte Carlo Algorithm |  http://arxiv.org/abs/1602.02616  | author:Thomas Bonis category:stat.CO math.ST stat.ML stat.TH published:2016-02-08 summary:We study the problem of sampling from a distribution $\target$ using theLangevin Monte Carlo algorithm and provide rate of convergences for thisalgorithm in terms of Wasserstein distance of order $2$. Our result holds aslong as the continuous diffusion process associated with the algorithmconverges exponentially fast to the target distribution along with sometechnical assumptions. While such an exponential convergence holds for examplein the log-concave measure case, it also holds for the more general case ofasymptoticaly log-concave measures. Our results thus extends the known rates ofconvergence in total variation and Wasserstein distances which have only beenobtained in the log-concave case. Moreover, using a sharper approximation boundof the continuous process, we obtain better asymptotic rates than traditionalresults. We also look into variations of the Langevin Monte Carlo algorithmusing other discretization schemes. In a first time, we look into the use ofthe Ozaki's discretization but are unable to obtain any significativeimprovement in terms of convergence rates compared to the Euler's scheme. Wethen provide a (sub-optimal) way to study more general schemes, however ourapproach only holds for the log-concave case.
arxiv-1602-02586 | Tumour ROI Estimation in Ultrasound Images via Radon Barcodes in Patients with Locally Advanced Breast Cancer |  http://arxiv.org/abs/1602.02586  | author:Hamid R. Tizhoosh, Mehrdad J. Gangeh, Hadi Tadayyon, Gregory J. Czarnota category:cs.CV published:2016-02-08 summary:Quantitative ultrasound (QUS) methods provide a promising framework that cannon-invasively and inexpensively be used to predict or assess the tumourresponse to cancer treatment. The first step in using the QUS methods is toselect a region of interest (ROI) inside the tumour in ultrasound images.Manual segmentation, however, is very time consuming and tedious. In thispaper, a semi-automated approach will be proposed to roughly localize an ROIfor a tumour in ultrasound images of patients with locally advanced breastcancer (LABC). Content-based barcodes, a recently introduced binary descriptorbased on Radon transform, were used in order to find similar cases and estimatea bounding box surrounding the tumour. Experiments with 33 B-scan imagesresulted in promising results with an accuracy of $81\%$.
arxiv-1602-02543 | Homogeneity of Cluster Ensembles |  http://arxiv.org/abs/1602.02543  | author:Brijnesh J. Jain category:cs.LG cs.CV published:2016-02-08 summary:The expectation and the mean of partitions generated by a cluster ensembleare not unique in general. This issue poses challenges in statistical inferenceand cluster stability. In this contribution, we state sufficient conditions foruniqueness of expectation and mean. The proposed conditions show that a uniquemean is neither exceptional nor generic. To cope with this issue, we introducehomogeneity as a measure of how likely is a unique mean for a sample ofpartitions. We show that homogeneity is related to cluster stability. Thisresult points to a possible conflict between cluster stability and diversity inconsensus clustering. To assess homogeneity in a practical setting, we proposean efficient way to compute a lower bound of homogeneity. Empirical resultsusing the k-means algorithm suggest that uniqueness of the mean partition isnot exceptional for real-world data. Moreover, for samples of high homogeneity,uniqueness can be enforced by increasing the number of data points or byremoving outlier partitions. In a broader context, this contribution can beplaced as a further step towards a statistical theory of partitions.
arxiv-1602-02523 | Data-Efficient Reinforcement Learning in Continuous-State POMDPs |  http://arxiv.org/abs/1602.02523  | author:Rowan McAllister, Carl Edward Rasmussen category:stat.ML cs.LG cs.SY published:2016-02-08 summary:We present a data-efficient reinforcement learning algorithm resistant toobservation noise. Our method extends the highly data-efficient PILCO algorithm(Deisenroth & Rasmussen, 2011) into partially observed Markov decisionprocesses (POMDPs) by considering the filtering process during policyevaluation. PILCO conducts policy search, evaluating each policy by firstpredicting an analytic distribution of possible system trajectories. Weadditionally predict trajectories w.r.t. a filtering process, achievingsignificantly higher performance than combining a filter with a policyoptimised by the original (unfiltered) framework. Our test setup is thecartpole swing-up task with sensor noise, which involves nonlinear dynamics andrequires nonlinear control.
arxiv-1602-02522 | A Semi-Automated Method for Object Segmentation in Infant's Egocentric Videos to Study Object Perception |  http://arxiv.org/abs/1602.02522  | author:Qazaleh Mirsharif, Sidharth Sadani, Shishir Shah, Hanako Yoshida, Joseph Burling category:cs.CV published:2016-02-08 summary:Object segmentation in infant's egocentric videos is a fundamental step instudying how children perceive objects in early stages of development. From thecomputer vision perspective, object segmentation in such videos pose quite afew challenges because the child's view is unfocused, often with large headmovements, effecting in sudden changes in the child's point of view which leadsto frequent change in object properties such as size, shape and illumination.In this paper, we develop a semi-automated, domain specific, method to addressthese concerns and facilitate the object annotation process for cognitivescientists allowing them to select and monitor the object under segmentation.The method starts with an annotation from the user of the desired object andemploys graph cut segmentation and optical flow computation to predict theobject mask for subsequent video frames automatically. To maintain accuracy, weuse domain specific heuristic rules to re-initialize the program with new userinput whenever object properties change dramatically. The evaluationsdemonstrate the high speed and accuracy of the presented method for objectsegmentation in voluminous egocentric videos. We apply the proposed method toinvestigate potential patterns in object distribution in child's view atprogressive ages.
arxiv-1602-02518 | Multi-view Kernel Completion |  http://arxiv.org/abs/1602.02518  | author:Sahely Bhadra, Samuel Kaski, Juho Rousu category:cs.LG stat.ML published:2016-02-08 summary:In this paper, we introduce the first method that (1) can complete kernelmatrices with completely missing rows and columns as opposed to individualmissing kernel values, (2) does not require any of the kernels to be complete apriori, and (3) can tackle non-linear kernels. These aspects are necessary inpractical applications such as integrating legacy data sets, learning undersensor failures and learning when measurements are costly for some of theviews. The proposed approach predicts missing rows by modelling bothwithin-view and between-view relationships among kernel values. We show, bothon simulated data and real world data, that the proposed method outperformsexisting techniques in the restricted settings where they are available, andextends applicability to new settings.
arxiv-1602-02450 | Loss factorization, weakly supervised learning and label noise robustness |  http://arxiv.org/abs/1602.02450  | author:Giorgio Patrini, Frank Nielsen, Richard Nock, Marcello Carioni category:cs.LG stat.ML published:2016-02-08 summary:We prove that the empirical risk of most well-known loss functions factorsinto a linear term aggregating all labels with a term that is label free, andcan further be expressed by sums of the loss. This holds true even fornon-smooth, non-convex losses and in any RKHS. The first term is a (kernel)mean operator --the focal quantity of this work-- which we characterize as thesufficient statistic for the labels. The result tightens known generalizationbounds and sheds new light on their interpretation. Factorization has a direct application on weakly supervised learning. Inparticular, we demonstrate that algorithms like SGD and proximal methods can beadapted with minimal effort to handle weak supervision, once the mean operatorhas been estimated. We apply this idea to learning with asymmetric noisylabels, connecting and extending prior work. Furthermore, we show that mostlosses enjoy a data-dependent (by the mean operator) form of noise robustness,in contrast with known negative results.
arxiv-1602-02485 | Simultaneous Safe Screening of Features and Samples in Doubly Sparse Modeling |  http://arxiv.org/abs/1602.02485  | author:Atsushi Shibagaki, Masayuki Karasuyama, Kohei Hatano, Ichiro Takeuchi category:stat.ML published:2016-02-08 summary:The problem of learning a sparse model is conceptually interpreted as theprocess of identifying active features/samples and then optimizing the modelover them. Recently introduced safe screening allows us to identify a part ofnon-active features/samples. So far, safe screening has been individuallystudied either for feature screening or for sample screening. In this paper, weintroduce a new approach for safely screening features and samplessimultaneously by alternatively iterating feature and sample screening steps. Asignificant advantage of considering them simultaneously rather thanindividually is that they have a synergy effect in the sense that the resultsof the previous safe feature screening can be exploited for improving the nextsafe sample screening performances, and vice-versa. We first theoreticallyinvestigate the synergy effect, and then illustrate the practical advantagethrough intensive numerical experiments for problems with large numbers offeatures and samples.
arxiv-1602-02350 | Solving Ridge Regression using Sketched Preconditioned SVRG |  http://arxiv.org/abs/1602.02350  | author:Alon Gonen, Francesco Orabona, Shai Shalev-Shwartz category:cs.LG published:2016-02-07 summary:We develop a novel preconditioning method for ridge regression, based onrecent linear sketching methods. By equipping Stochastic Variance ReducedGradient (SVRG) with this preconditioning process, we obtain a significantspeed-up relative to fast stochastic methods such as SVRG, SDCA and SAG.
arxiv-1602-02386 | Network Inference by Learned Node-Specific Degree Prior |  http://arxiv.org/abs/1602.02386  | author:Qingming Tang, Lifu Tu, Weiran Wang, Jinbo Xu category:stat.ML cs.LG published:2016-02-07 summary:We propose a novel method for network inference from partially observed edgesusing a node-specific degree prior. The degree prior is derived from observededges in the network to be inferred, and its hyper-parameters are determined bycross validation. Then we formulate network inference as a matrix completionproblem regularized by our degree prior. Our theoretical analysis indicatesthat this prior favors a network following the learned degree distribution, andmay lead to improved network recovery error bound than previous work.Experimental results on both simulated and real biological networks demonstratethe superior performance of our method in various settings.
arxiv-1602-02383 | Disentangled Representations in Neural Models |  http://arxiv.org/abs/1602.02383  | author:William Whitney category:cs.LG cs.NE published:2016-02-07 summary:Representation learning is the foundation for the recent success of neuralnetwork models. However, the distributed representations generated by neuralnetworks are far from ideal. Due to their highly entangled nature, they are dicult to reuse and interpret, and they do a poor job of capturing the sparsitywhich is present in real- world transformations. In this paper, I describemethods for learning disentangled representations in the two domains ofgraphics and computation. These methods allow neural methods to learnrepresentations which are easy to interpret and reuse, yet they incur little orno penalty to performance. In the Graphics section, I demonstrate the abilityof these methods to infer the generating parameters of images and rerenderthose images under novel conditions. In the Computation section, I describe amodel which is able to factorize a multitask learning problem into subtasks andwhich experiences no catastrophic forgetting. Together these techniques providethe tools to design a wide range of models that learn disentangledrepresentations and better model the factors of variation in the real world.
arxiv-1602-02373 | Supervised and Semi-Supervised Text Categorization using One-Hot LSTM for Region Embeddings |  http://arxiv.org/abs/1602.02373  | author:Rie Johnson, Tong Zhang category:stat.ML cs.CL cs.LG published:2016-02-07 summary:One-hot CNN (convolutional neural network) has been shown to be effective fortext categorization in our previous work. We view it as a special case of ageneral framework which jointly trains a linear model with a non-linear featuregenerator consisting of `text region embedding + pooling'. Under thisframework, we explore a more sophisticated region embedding method using LongShort-Term Memory (LSTM). LSTM can embed text regions of variable (and possiblylarge) sizes, whereas the region size needs to be fixed in a CNN. We seek thebest use of LSTM for the purpose in the supervised and semi-supervisedsettings, starting with the idea of one-hot LSTM, which eliminates thecustomarily used word embedding layer. Our results indicate that on this task,embeddings of text regions, which can convey higher concepts than single wordsin isolation, are more useful than word embeddings. We report performancesexceeding the previous best results on four benchmark datasets.
