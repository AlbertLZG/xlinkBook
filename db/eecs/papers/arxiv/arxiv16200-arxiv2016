arxiv-16200-1 | You-Do, I-Learn: Unsupervised Multi-User egocentric Approach Towards Video-Based Guidance | http://arxiv.org/pdf/1510.04862v2.pdf | author:Dima Damen, Teesid Leelasawassuk, Walterio Mayol-Cuevas category:cs.CV published:2015-10-16 summary:This paper presents an unsupervised approach towards automatically extractingvideo-based guidance on object usage, from egocentric video and wearable gazetracking, collected from multiple users while performing tasks. The approach i)discovers task relevant objects, ii) builds a model for each, iii)distinguishes different ways in which each discovered object has been used andiv) discovers the dependencies between object interactions. The workinvestigates using appearance, position, motion and attention, and presentsresults using each and a combination of relevant features. Moreover, an onlinescalable approach is presented and is compared to offline results. The paperproposes a method for selecting a suitable video guide to be displayed to anovice user indicating how to use an object, purely triggered by the user'sgaze. The potential assistive mode can also recommend an object to be used nextbased on the learnt sequence of object interactions. The approach was tested ona variety of daily tasks such as initialising a printer, preparing a coffee andsetting up a gym machine.
arxiv-16200-2 | Doctor AI: Predicting Clinical Events via Recurrent Neural Networks | http://arxiv.org/pdf/1511.05942v8.pdf | author:Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F. Stewart, Jimeng Sun category:cs.LG published:2015-11-18 summary:Large amount of Electronic Health Record (EHR) data have been collected overmillions of patients over multiple years. The rich longitudinal EHR datadocumented the collective experiences of physicians including diagnosis,medication prescription and procedures. We argue it is possible now to leveragethe EHR data to model how physicians behave, and we call our model Doctor AI.Towards this direction of modeling clinical behavior of physicians, we developa successful application of Recurrent Neural Networks (RNN) to jointly forecastthe future disease diagnosis and medication prescription along with theirtiming. Unlike traditional classification models where a single target is ofinterest, our model can assess the entire history of patients and makecontinuous and multilabel predictions based on patients' historical data. Weevaluate the performance of the proposed method on a large real-world EHR dataover 260K patients over 8 years. We observed Doctor AI can perform differentialdiagnosis with similar accuracy to physicians. In particular, Doctor AIachieves up to 79% recall@30, significantly higher than several baselines.Moreover, we demonstrate great generalizability of Doctor AI by applying theresulting models on data from a completely different medication institutionachieving comparable performance.
arxiv-16200-3 | How Transferable are Neural Networks in NLP Applications? | http://arxiv.org/pdf/1603.06111v1.pdf | author:Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, Zhi Jin category:cs.CL cs.LG cs.NE published:2016-03-19 summary:Transfer learning is aimed to make use of valuable knowledge in a sourcedomain to help the model performance in a target domain. It is particularlyimportant to neural networks because neural models are very likely to beoverfitting. In some fields like image processing, many studies have shown theeffectiveness of neural network-based transfer learning. For neural NLP,however, existing studies have only casually applied transfer learning, andconclusions are inconsistent. In this paper, we conduct a series of empiricalstudies and provide an illuminating picture on the transferability of neuralnetworks in NLP.
arxiv-16200-4 | Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation | http://arxiv.org/pdf/1603.06098v1.pdf | author:Alexander Kolesnikov, Christoph H. Lampert category:cs.CV published:2016-03-19 summary:We introduce a new loss function for the weakly-supervised training ofsemantic image segmentation models based on three guiding principles: to seedwith weak location cues, to expand objects based on the information about whichclasses can occur, and to constrain the segmentations to coincide with imageboundaries. We show experimentally that training a deep convolutional neuralnetwork using the proposed loss function leads to substantially bettersegmentations than previous state-of-the-art methods on the challenging PASCALVOC 2012 dataset. We furthermore give insight into the working mechanism of ourmethod by a detailed experimental study that illustrates how the segmentationquality is affected by each term of the proposed loss function as well as theircombinations.
arxiv-16200-5 | Item2Vec: Neural Item Embedding for Collaborative Filtering | http://arxiv.org/pdf/1603.04259v2.pdf | author:Oren Barkan, Noam Koenigstein category:cs.LG cs.AI cs.IR published:2016-03-14 summary:Many Collaborative Filtering (CF) algorithms are item-based in the sense thatthey analyze item-item relations in order to produce item similarities.Recently, several works in the field of Natural Language Processing suggestedto learn a latent representation of words using neural embedding algorithms.Among them, the Skip-gram with Negative Sampling (SGNS), also known asWord2Vec, was shown to provide state-of-the-art results on various linguisticstasks. In this paper, we show that item-based CF can be cast in the sameframework of neural word embedding. Inspired by SGNS, we describe a method wename Item2Vec for item-based CF that produces embedding for items in a latentspace. The method is capable of inferring item-to-item relations even when userinformation is not available. We present experimental results on large scaledatasets that demonstrate the effectiveness of the Item2Vec method and show itis competitive with SVD.
arxiv-16200-6 | Large scale near-duplicate image retrieval using Triples of Adjacent Ranked Features (TARF) with embedded geometric information | http://arxiv.org/pdf/1603.06093v1.pdf | author:Sergei Fedorov, Olga Kacher category:cs.CV published:2016-03-19 summary:Most approaches to large-scale image retrieval are based on the constructionof the inverted index of local image descriptors or visual words. A search insuch an index usually results in a large number of candidates. This list ofcandidates is then re-ranked with the help of a geometric verification, using aRANSAC algorithm, for example. In this paper we propose a featurerepresentation, which is built as a combination of three local descriptors. Itallows one to significantly decrease the number of false matches and to shortenthe list of candidates after the initial search in the inverted index. Thiscombination of local descriptors is both reproducible and highlydiscriminative, and thus can be efficiently used for large-scale near-duplicateimage retrieval.
arxiv-16200-7 | Deep Shading: Convolutional Neural Networks for Screen-Space Shading | http://arxiv.org/pdf/1603.06078v1.pdf | author:Oliver Nalbach, Elena Arabadzhiyska, Dushyant Mehta, Hans-Peter Seidel, Tobias Ritschel category:cs.GR cs.LG I.3.7; I.2.6 published:2016-03-19 summary:In computer vision, Convolutional Neural Networks (CNNs) have recentlyachieved new levels of performance for several inverse problems where RGB pixelappearance is mapped to attributes such as positions, normals or reflectance.In computer graphics, screen-space shading has recently increased the visualquality in interactive image synthesis, where per-pixel attributes such aspositions, normals or reflectance of a virtual 3D scene are converted into RGBpixel appearance, enabling effects like ambient occlusion, indirect light,scattering, depth-of-field, motion blur, or anti-aliasing. In this paper weconsider the diagonal problem: synthesizing appearance from given per-pixelattributes using a CNN. The resulting Deep Shading simulates all screen-spaceeffects as well as arbitrary combinations thereof at competitive quality andspeed while not being programmed by human experts but learned from exampleimages.
arxiv-16200-8 | Improving Hypernymy Detection with an Integrated Path-based and Distributional Method | http://arxiv.org/pdf/1603.06076v1.pdf | author:Vered Shwartz, Yoav Goldberg, Ido Dagan category:cs.CL published:2016-03-19 summary:Detecting hypernymy relations is a key task in NLP, which is addressed in theliterature using two complementary approaches. Distributional methods, whosesupervised variants are the current best performers, and path-based methods whoreceive less research attention. We suggest an improved path-based algorithm,in which the dependency paths are encoded using a recurrent neural network, andachieve results comparable to distributional methods. We then extend theapproach to integrate both path-based and distributional signals, significantlyimproving the state-of-the-art on this task.
arxiv-16200-9 | DASA: Domain Adaptation in Stacked Autoencoders using Systematic Dropout | http://arxiv.org/pdf/1603.06060v1.pdf | author:Abhijit Guha Roy, Debdoot Sheet category:cs.CV cs.LG published:2016-03-19 summary:Domain adaptation deals with adapting behaviour of machine learning basedsystems trained using samples in source domain to their deployment in targetdomain where the statistics of samples in both domains are dissimilar. The taskof directly training or adapting a learner in the target domain is challengedby lack of abundant labeled samples. In this paper we propose a technique fordomain adaptation in stacked autoencoder (SAE) based deep neural networks (DNN)performed in two stages: (i) unsupervised weight adaptation using systematicdropouts in mini-batch training, (ii) supervised fine-tuning with limitednumber of labeled samples in target domain. We experimentally evaluateperformance in the problem of retinal vessel segmentation where the SAE-DNN istrained using large number of labeled samples in the source domain (DRIVEdataset) and adapted using less number of labeled samples in target domain(STARE dataset). The performance of SAE-DNN measured using $logloss$ in sourcedomain is $0.19$, without and with adaptation are $0.40$ and $0.18$, and $0.39$when trained exclusively with limited samples in target domain. The area underROC curve is observed respectively as $0.90$, $0.86$, $0.92$ and $0.87$. Thehigh efficiency of vessel segmentation with DASA strongly substantiates ourclaim.
arxiv-16200-10 | Fast DPP Sampling for Nyström with Application to Kernel Methods | http://arxiv.org/pdf/1603.06052v1.pdf | author:Chengtao Li, Stefanie Jegelka, Suvrit Sra category:cs.LG published:2016-03-19 summary:The Nystr\"om method has long been popular for scaling up kernel methods.However, successful use of Nystr\"om depends crucially on the selectedlandmarks. We consider landmark selection by using a Determinantal PointProcess (DPP) to tractably select a diverse subset from the columns of an inputkernel matrix. We prove that the landmarks selected using DPP sampling enjoyguaranteed error bounds; subsequently, we illustrate impact of DPP-sampledlandmarks on kernel ridge regression. Moreover, we show how to efficientlysample from a DPP in linear time using a fast mixing (under certainconstraints) Markov chain, which makes the overall procedure practical.Empirical results support our theoretical analysis: DPP-based landmarkselection shows performance superior to existing approaches.
arxiv-16200-11 | Globally Normalized Transition-Based Neural Networks | http://arxiv.org/pdf/1603.06042v1.pdf | author:Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov, Michael Collins category:cs.CL cs.LG cs.NE published:2016-03-19 summary:We introduce a globally normalized transition-based neural network model thatachieves state-of-the-art part-of-speech tagging, dependency parsing andsentence compression results. Our model is a simple feed-forward neural networkthat operates on a task-specific transition system, yet achieves comparable orbetter accuracies than recurrent models. The key insight is based on a novelproof illustrating the label bias problem and showing that globally normalizedmodels can be strictly more expressive than locally normalized models.
arxiv-16200-12 | Tensor Methods and Recommender Systems | http://arxiv.org/pdf/1603.06038v1.pdf | author:Evgeny Frolov, Ivan Oseledets category:cs.LG cs.IR stat.ML published:2016-03-19 summary:A substantial progress in development of new and efficient tensorfactorization techniques has led to an extensive research of theirapplicability in recommender systems field. Tensor-based recommender modelspush the boundaries of traditional collaborative filtering techniques by takinginto account a multifaceted nature of real environments, which allows toproduce more accurate, situational (e.g. context-aware, criteria-driven)recommendations. Despite the promising results, tensor-based methods are poorlycovered in existing recommender systems surveys. This survey aims to complementprevious works and provide a comprehensive overview on the subject. To the bestof our knowledge, this is the first attempt to consolidate studies from variousapplication domains in an easily readable, digestible format, which helps toget a notion of the current state of the field. We also provide a high leveldiscussion of the future perspectives and directions for further improvement oftensor-based recommendation systems.
arxiv-16200-13 | A Fractal-based CNN for Detecting Complicated Curves in AFM Images | http://arxiv.org/pdf/1603.06036v1.pdf | author:Hongteng Xu, Junchi Yan, Nils Persson, Hongyuan Zha category:cs.CV published:2016-03-19 summary:Convolutional neural networks (CNNs) have been widely used in computervision, including low-and middle-level vision problems such as contourdetection and image reconstruction. In this paper, we propose a novelfractal-based CNN model for the problem of complicated curve detection,providing a geometric interpretation of CNN-based image model leveraging localfractal analysis. Utilizing the notion of local self-similarity, we develop alocal fractal model for images. A curve detector is designed based on themodel, consisting of an orientation-adaptive filtering process to enhance thelocal response along a certain orientation. This is followed by apost-processing step to preserve local invariance of the fractal dimension ofimage. We show the iterative framework of such an adaptive filtering processcan be re-instantiated approximately via a CNN model, the nonlinear processinglayer of which preserves fractal dimension approximately and the convolutionlayer achieves orientation enhancement. We demonstrate our fractal-based CNNmodel on the challenging task for detecting complicated curves from thetexture-like images depicting microstructures of materials obtained by atomicforce microscopy (AFM).
arxiv-16200-14 | Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering | http://arxiv.org/pdf/1511.05234v2.pdf | author:Huijuan Xu, Kate Saenko category:cs.CV cs.AI cs.CL cs.NE published:2015-11-17 summary:We address the problem of Visual Question Answering (VQA), which requiresjoint image and language understanding to answer a question about a givenphotograph. Recent approaches have applied deep image captioning methods basedon convolutional-recurrent networks to this problem, but have failed to modelspatial inference. To remedy this, we propose a model we call the SpatialMemory Network and apply it to the VQA task. Memory networks are recurrentneural networks with an explicit attention mechanism that selects certain partsof the information stored in memory. Our Spatial Memory Network stores neuronactivations from different spatial regions of the image in its memory, and usesthe question to choose relevant regions for computing the answer, a process ofwhich constitutes a single "hop" in the network. We propose a novel spatialattention architecture that aligns words with image patches in the first hop,and obtain improved results by adding a second attention hop which considersthe whole question to choose visual evidence based on the results of the firsthop. To better understand the inference process learned by the network, wedesign synthetic questions that specifically require spatial inference andvisualize the attention weights. We evaluate our model on two published visualquestion answering datasets, DAQUAR [1] and VQA [2], and obtain improvedresults compared to a strong deep baseline model (iBOWIMG) which concatenatesimage and question features to predict the answer [3].
arxiv-16200-15 | L0-norm Sparse Graph-regularized SVD for Biclustering | http://arxiv.org/pdf/1603.06035v1.pdf | author:Wenwen Min, Juan Liu, Shihua Zhang category:cs.LG stat.ML published:2016-03-19 summary:Learning the "blocking" structure is a central challenge for high dimensionaldata (e.g., gene expression data). Recently, a sparse singular valuedecomposition (SVD) has been used as a biclustering tool to achieve this goal.However, this model ignores the structural information between variables (e.g.,gene interaction graph). Although typical graph-regularized norm canincorporate such prior graph information to get accurate discovery and betterinterpretability, it fails to consider the opposite effect of variables withdifferent signs. Motivated by the development of sparse coding andgraph-regularized norm, we propose a novel sparse graph-regularized SVD as apowerful biclustering tool for analyzing high-dimensional data. The key of thismethod is to impose two penalties including a novel graph-regularized norm($\pmb{u}\pmb{L}\pmb{u}$) and $L_0$-norm ($\\pmb{u}\_0$) on singularvectors to induce structural sparsity and enhance interpretability. We designan efficient Alternating Iterative Sparse Projection (AISP) algorithm to solveit. Finally, we apply our method and related ones to simulated and real data toshow its efficiency in capturing natural blocking structures.
arxiv-16200-16 | A Multi-scale Multiple Instance Video Description Network | http://arxiv.org/pdf/1505.05914v3.pdf | author:Huijuan Xu, Subhashini Venugopalan, Vasili Ramanishka, Marcus Rohrbach, Kate Saenko category:cs.CV published:2015-05-21 summary:Generating natural language descriptions for in-the-wild videos is achallenging task. Most state-of-the-art methods for solving this problem borrowexisting deep convolutional neural network (CNN) architectures (AlexNet,GoogLeNet) to extract a visual representation of the input video. However,these deep CNN architectures are designed for single-label centered-positionedobject classification. While they generate strong semantic features, they haveno inherent structure allowing them to detect multiple objects of differentsizes and locations in the frame. Our paper tries to solve this problem byintegrating the base CNN into several fully convolutional neural networks(FCNs) to form a multi-scale network that handles multiple receptive fieldsizes in the original image. FCNs, previously applied to image segmentation,can generate class heat-maps efficiently compared to sliding window mechanisms,and can easily handle multiple scales. To further handle the ambiguity overmultiple objects and locations, we incorporate the Multiple Instance Learningmechanism (MIL) to consider objects in different positions and at differentscales simultaneously. We integrate our multi-scale multi-instance architecturewith a sequence-to-sequence recurrent neural network to generate sentencedescriptions based on the visual representation. Ours is the first end-to-endtrainable architecture that is capable of multi-scale region processing.Evaluation on a Youtube video dataset shows the advantage of our approachcompared to the original single-scale whole frame CNN model. Our flexible andefficient architecture can potentially be extended to support other videoprocessing tasks.
arxiv-16200-17 | End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures | http://arxiv.org/pdf/1601.00770v2.pdf | author:Makoto Miwa, Mohit Bansal category:cs.CL cs.LG published:2016-01-05 summary:We present a novel end-to-end neural model to extract entities and relationsbetween them. Our recurrent neural network based model captures both wordsequence and dependency tree substructure information by stacking bidirectionaltree-structured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allowsour model to jointly represent both entities and relations with sharedparameters in a single model. We further encourage detection of entities duringtraining and use of entity information in relation extraction via entitypretraining and scheduled sampling. Our model improves over thestate-of-the-art feature-based model on end-to-end relation extraction,achieving 3.5% and 4.8% relative error reductions in F1-score on ACE2004 andACE2005, respectively. We also show a 2.5% relative error reduction in F1-scoreover the state-of-the-art convolutional neural network based model on nominalrelation classification (SemEval-2010 Task 8).
arxiv-16200-18 | New Optimisation Methods for Machine Learning | http://arxiv.org/pdf/1510.02533v2.pdf | author:Aaron Defazio category:cs.LG stat.ML published:2015-10-09 summary:A thesis submitted for the degree of Doctor of Philosophy of The AustralianNational University. In this work we introduce several new optimisation methods for problems inmachine learning. Our algorithms broadly fall into two categories: optimisationof finite sums and of graph structured objectives. The finite sum problem issimply the minimisation of objective functions that are naturally expressed asa summation over a large number of terms, where each term has a similar oridentical weight. Such objectives most often appear in machine learning in theempirical risk minimisation framework in the non-online learning setting. Thesecond category, that of graph structured objectives, consists of objectivesthat result from applying maximum likelihood to Markov random field models.Unlike the finite sum case, all the non-linearity is contained within apartition function term, which does not readily decompose into a summation. For the finite sum problem, we introduce the Finito and SAGA algorithms, aswell as variants of each. For graph-structured problems, we take three complementary approaches. Welook at learning the parameters for a fixed structure, learning the structureindependently, and learning both simultaneously. Specifically, for the combinedapproach, we introduce a new method for encouraging graph structures with the"scale-free" property. For the structure learning problem, we establishSHORTCUT, a O(n^{2.5}) expected time approximate structure learning method forGaussian graphical models. For problems where the structure is known but theparameters unknown, we introduce an approximate maximum likelihood learningalgorithm that is capable of learning a useful subclass of Gaussian graphicalmodels.
arxiv-16200-19 | A Survey of Stealth Malware: Attacks, Mitigation Measures, and Steps Toward Autonomous Open World Solutions | http://arxiv.org/pdf/1603.06028v1.pdf | author:Ethan Rudd, Andras Rozsa, Manuel Gunther, Terrance Boult category:cs.CR cs.CV published:2016-03-19 summary:Development of generic and autonomous anti-malware solutions is becomingincreasingly vital as the deployment of stealth malware continues to increaseat an alarming rate. In this paper, we survey malicious stealth technologies aswell as existing autonomous countermeasures. Our findings suggest that whilemachine learning offers promising potential for generic and autonomoussolutions, both at the network level and at the host level, several flawedassumptions inherent to most recognition algorithms prevent a direct mappingbetween the stealth malware recognition problem and a machine learningsolution. The most notable of these flawed assumptions is the closed worldassumption: that no sample belonging to a class outside of a static trainingset will appear at query time. We present a formalized adaptive open worldframework for stealth malware recognition, relating it mathematically toresearch from other machine learning domains.
arxiv-16200-20 | A Fast Unified Model for Parsing and Sentence Understanding | http://arxiv.org/pdf/1603.06021v1.pdf | author:Samuel R. Bowman, Jon Gauthier, Abhinav Rastogi, Raghav Gupta, Christopher D. Manning, Christopher Potts category:cs.CL published:2016-03-19 summary:Tree-structured neural networks exploit valuable syntactic parse informationas they interpret the meanings of sentences. However, they suffer from two keytechnical problems that make them slow and unwieldy for large-scale NLP tasks:they can only operate on parsed sentences and they do not directly supportbatched computation. We address these issues by introducing the Stack-augmentedParser-Interpreter Neural Network (SPINN), which combines parsing andinterpretation within a single tree-sequence hybrid model by integratingtree-structured sentence interpretation into the linear sequential structure ofa shift-reduce parser. Our model supports batched computation for a speedup ofup to 25x over other tree-structured models, and its integrated parser allowsit to operate on unparsed data with little loss of accuracy. We evaluate it onthe Stanford NLI entailment task and show that it significantly outperformsother sentence-encoding models.
arxiv-16200-21 | A Comprehensive Performance Evaluation of Deformable Face Tracking "In-the-Wild" | http://arxiv.org/pdf/1603.06015v1.pdf | author:Grigorios G. Chrysos, Epameinondas Antonakos, Patrick Snape, Akshay Asthana, Stefanos Zafeiriou category:cs.CV cs.AI published:2016-03-18 summary:Recently, technologies such as face detection, facial landmark localisationand face recognition and verification have matured enough to provide effectiveand efficient solutions for imagery captured under arbitrary conditions(referred to as "in-the-wild"). This is partially attributed to the fact thatcomprehensive "in-the-wild" benchmarks have been developed for face detection,landmark localisation and recognition/verification. A very important technologythat has not been thoroughly evaluated yet is deformable face tracking"in-the-wild". Until now, the performance has mainly been assessedqualitatively by visually assessing the result of a deformable face trackingtechnology on short videos. In this paper, we perform the first, to the best ofour knowledge, thorough evaluation of state-of-the-art deformable face trackingpipelines using the recently introduced 300VW benchmark. We evaluate manydifferent architectures focusing mainly on the task of on-line deformable facetracking. In particular, we compare the following general strategies: (a)generic face detection plus generic facial landmark localisation, (b) genericmodel free tracking plus generic facial landmark localisation, as well as (c)hybrid approaches using state-of-the-art face detection, model free trackingand facial landmark localisation technologies. Our evaluation reveals futureavenues for further research on the topic.
arxiv-16200-22 | Readability-based Sentence Ranking for Evaluating Text Simplification | http://arxiv.org/pdf/1603.06009v1.pdf | author:Sowmya Vajjala, Detmar Meurers category:cs.CL published:2016-03-18 summary:We propose a new method for evaluating the readability of simplifiedsentences through pair-wise ranking. The validity of the method is establishedthrough in-corpus and cross-corpus evaluation experiments. The approachcorrectly identifies the ranking of simplified and unsimplified sentences interms of their reading level with an accuracy of over 80%, significantlyoutperforming previous results. To gain qualitative insights into the nature ofsimplification at the sentence level, we studied the impact of specificlinguistic features. We empirically confirm that both word-level and syntacticfeatures play a role in comparing the degree of simplification of authenticdata. To carry out this research, we created a new sentence-aligned corpus fromprofessionally simplified news articles. The new corpus resource enriches theempirical basis of sentence-level simplification research, which so far reliedon a single resource. Most importantly, it facilitates cross-corpus evaluationfor simplification, a key step towards generalizable results.
arxiv-16200-23 | A Message Passing Algorithm for the Problem of Path Packing in Graphs | http://arxiv.org/pdf/1603.06002v1.pdf | author:Patrick Eschenfeldt, David Gamarnik category:cs.DS stat.ML published:2016-03-18 summary:We consider the problem of packing node-disjoint directed paths in a directedgraph. We consider a variant of this problem where each path starts within afixed subset of root nodes, subject to a given bound on the length of paths.This problem is motivated by the so-called kidney exchange problem, but haspotential other applications and is interesting in its own right. We propose a new algorithm for this problem based on the messagepassing/belief propagation technique. A priori this problem does not have anassociated graphical model, so in order to apply a belief propagation algorithmwe provide a novel representation of the problem as a graphical model. Standardbelief propagation on this model has poor scaling behavior, so we provide anefficient implementation that significantly decreases the complexity. Weprovide numerical results comparing the performance of our algorithm on bothartificially created graphs and real world networks to several alternativealgorithms, including algorithms based on integer programming (IP) techniques.These comparisons show that our algorithm scales better to large instances thanIP-based algorithms and often finds better solutions than a simple algorithmthat greedily selects the longest path from each root node. In some cases italso finds better solutions than the ones found by IP-based algorithms evenwhen the latter are allowed to run significantly longer than our algorithm.
arxiv-16200-24 | Linear Dimensionality Reduction: Survey, Insights, and Generalizations | http://arxiv.org/pdf/1406.0873v2.pdf | author:John P. Cunningham, Zoubin Ghahramani category:stat.ML published:2014-06-03 summary:Linear dimensionality reduction methods are a cornerstone of analyzing highdimensional data, due to their simple geometric interpretations and typicallyattractive computational properties. These methods capture many data featuresof interest, such as covariance, dynamical structure, correlation between datasets, input-output relationships, and margin between data classes. Methods havebeen developed with a variety of names and motivations in many fields, andperhaps as a result the connections between all these methods have not beenhighlighted. Here we survey methods from this disparate literature asoptimization programs over matrix manifolds. We discuss principal componentanalysis, factor analysis, linear multidimensional scaling, Fisher's lineardiscriminant analysis, canonical correlations analysis, maximum autocorrelationfactors, slow feature analysis, sufficient dimensionality reduction,undercomplete independent component analysis, linear regression, distancemetric learning, and more. This optimization framework gives insight to somerarely discussed shortcomings of well-known methods, such as the suboptimalityof certain eigenvector solutions. Modern techniques for optimization overmatrix manifolds enable a generic linear dimensionality reduction solver, whichaccepts as input data and an objective to be optimized, and returns, as output,an optimal low-dimensional projection of the data. This simple optimizationframework further allows straightforward generalizations and novel variants ofclassical methods, which we demonstrate here by creating anorthogonal-projection canonical correlations analysis. More broadly, thissurvey and generic solver suggest that linear dimensionality reduction can movetoward becoming a blackbox, objective-agnostic numerical technology.
arxiv-16200-25 | Best-of-K Bandits | http://arxiv.org/pdf/1603.02752v2.pdf | author:Max Simchowitz, Kevin Jamieson, Benjamin Recht category:cs.LG stat.ML published:2016-03-09 summary:This paper studies the Best-of-K Bandit game: At each time the player choosesa subset S among all N-choose-K possible options and observes reward max(X(i) :i in S) where X is a random vector drawn from a joint distribution. Theobjective is to identify the subset that achieves the highest expected rewardwith high probability using as few queries as possible. We presentdistribution-dependent lower bounds based on a particular construction whichforce a learner to consider all N-choose-K subsets, and match naive extensionsof known upper bounds in the bandit setting obtained by treating each subset asa separate arm. Nevertheless, we present evidence that exhaustive search may beavoided for certain, favorable distributions because the influence ofhigh-order order correlations may be dominated by lower order statistics.Finally, we present an algorithm and analysis for independent arms, whichmitigates the surprising non-trivial information occlusion that occurs due toonly observing the max in the subset. This may inform strategies for moregeneral dependent measures, and we complement these result with independent-armlower bounds.
arxiv-16200-26 | Document Neural Autoregressive Distribution Estimation | http://arxiv.org/pdf/1603.05962v1.pdf | author:Stanislas Lauly, Yin Zheng, Alexandre Allauzen, Hugo Larochelle category:cs.LG cs.CL published:2016-03-18 summary:We present an approach based on feed-forward neural networks for learning thedistribution of textual documents. This approach is inspired by the NeuralAutoregressive Distribution Estimator(NADE) model, which has been shown to be agood estimator of the distribution of discrete-valued igh-dimensional vectors.In this paper, we present how NADE can successfully be adapted to the case oftextual data, retaining from NADE the property that sampling or computing theprobability of observations can be done exactly and efficiently. The approachcan also be used to learn deep representations of documents that arecompetitive to those learned by the alternative topic modeling approaches.Finally, we describe how the approach can be combined with a regular neuralnetwork N-gram model and substantially improve its performance, by making itslearned representation sensitive to the larger, document-specific context.
arxiv-16200-27 | Transferring Learned Microcalcification Group Detection from 2D Mammography to 3D Digital Breast Tomosynthesis Using a Hierarchical Model and Scope-based Normalization Features | http://arxiv.org/pdf/1603.05955v1.pdf | author:Yin Yin, Sergei V. Fotin, Hrishikesh Haldankar, Jeffrey W. Hoffmeister, Senthil Periaswamy category:cs.CV published:2016-03-18 summary:A novel hierarchical model is introduced to solve a general problem ofdetecting groups of similar objects. Under this model, detection of groups isperformed in hierarchically organized layers while each layer represents ascope for target objects. The processing of these layers involves sequentialextraction of appearance features for an individual object, consistencymeasurement features for nearby objects, and finally the distribution featuresfor all objects within the group. Using the concept of scope-basednormalization, the extracted features not only enhance local contrast of anindividual object, but also provide consistent characterization for all relatedobjects. As an example, a microcalcification group detection system for 2Dmammography was developed, and then the learned model was transferred to 3Ddigital breast tomosynthesis without any retraining or fine-tuning. Thedetection system demonstrated state-of-the-art performance and detected 96% ofcancerous lesions at the rate of 1.2 false positives per volume as measured onan independent tomosynthesis test set.
arxiv-16200-28 | Distributed Iterative Learning Control for a Team of Quadrotors | http://arxiv.org/pdf/1603.05933v1.pdf | author:Andreas Hock, Angela P. Schoellig category:cs.RO cs.LG cs.MA published:2016-03-18 summary:The goal of this work is to enable a team of quadrotors to learn how toaccurately track a desired trajectory while holding a given formation. We solvethis problem in a distributed manner, where each vehicle has only access to theinformation of its neighbors. The desired trajectory is only available to one(or few) vehicles. We present a distributed iterative learning control (ILC)approach where each vehicle learns from the experience of its own and itsneighbors' previous task repetitions and adapts its feedforward input toimprove performance. Existing algorithms are extended in theory to make themmore applicable for real-world experiments. In particular, we prove stabilityfor any causal learning function with gains chosen according to a simple scalarcondition. Previous proofs were restricted to a specific learning function,which only depends on the tracking error derivative (D-type ILC). Thisextension provides more degrees of freedom in the ILC design and, as a result,better performance can be achieved. We also show that stability is not affectedby a linear dynamic coupling between neighbors. This allows us to use anadditional consensus feedback controller to compensate for non-repetitivedisturbances. Experiments with two quadrotors attest the practicalapplicability of the proposed distributed multi-agent ILC approach. This is thefirst work to show distributed ILC in experiment.
arxiv-16200-29 | Geometric Hypergraph Learning for Visual Tracking | http://arxiv.org/pdf/1603.05930v1.pdf | author:Dawei Du, Honggang Qi, Longyin Wen, Qi Tian, Qingming Huang, Siwei Lyu category:cs.CV published:2016-03-18 summary:Graph based representation is widely used in visual tracking field by findingcorrect correspondences between target parts in consecutive frames. However,most graph based trackers consider pairwise geometric relations between localparts. They do not make full use of the target's intrinsic structure, therebymaking the representation easily disturbed by errors in pairwise affinitieswhen large deformation and occlusion occur. In this paper, we propose ageometric hypergraph learning based tracking method, which fully exploitshigh-order geometric relations among multiple correspondences of parts inconsecutive frames. Then visual tracking is formulated as the mode-seekingproblem on the hypergraph in which vertices represent correspondence hypothesesand hyperedges describe high-order geometric relations. Besides, aconfidence-aware sampling method is developed to select representative verticesand hyperedges to construct the geometric hypergraph for more robustness andscalability. The experiments are carried out on two challenging datasets(VOT2014 and Deform-SOT) to demonstrate that the proposed method performsfavorable against other existing trackers.
arxiv-16200-30 | Learning Fair Classifiers | http://arxiv.org/pdf/1507.05259v3.pdf | author:Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, Krishna P. Gummadi category:stat.ML cs.LG published:2015-07-19 summary:Automated data-driven decision systems are ubiquitous across a wide varietyof online services, from online social networking and e-commerce toe-government. These systems rely on complex learning methods and vast amountsof data to optimize the service functionality, satisfaction of the end user andprofitability. However, there is a growing concern that these automateddecisions can lead to user discrimination, even in the absence of intent,leading to a lack of fairness, i.e., their outcomes have a disproportionallylarge adverse impact on particular groups of people sharing one or moresensitive attributes (e.g., race, sex). In this paper, we introduce a flexiblemechanism to design fair classifiers in a principled manner, by leveraging anovel intuitive measure of decision boundary (un)fairness. Then, we instantiatethis mechanism on two well-known classifiers: logistic regression and supportvector machines. Experiments on both synthetic and real-world data show thatour mechanism allows for a fine-grained control of the level of fairness, oftenat a minimal cost in terms of accuracy, and it provides more flexibility thanalternatives.
arxiv-16200-31 | Approximating Likelihood Ratios with Calibrated Discriminative Classifiers | http://arxiv.org/pdf/1506.02169v2.pdf | author:Kyle Cranmer, Juan Pavez, Gilles Louppe category:stat.AP stat.ML published:2015-06-06 summary:In many fields of science, generalized likelihood ratio tests are establishedtools for statistical inference. At the same time, it has become increasinglycommon that a simulator (or generative model) is used to describe complexprocesses that tie parameters $\theta$ of an underlying theory and measurementapparatus to high-dimensional observations $\mathbf{x}\in \mathbb{R}^p$.However, simulator often do not provide a way to evaluate the likelihoodfunction for a given observation $\mathbf{x}$, which motivates a new class oflikelihood-free inference algorithms. In this paper, we show that likelihoodratios are invariant under a specific class of dimensionality reduction maps$\mathbb{R}^p \mapsto \mathbb{R}$. As a direct consequence, we show thatdiscriminative classifiers can be used to approximate the generalizedlikelihood ratio statistic when only a generative model for the data isavailable. This leads to a new machine learning-based approach tolikelihood-free inference that is complementary to Approximate BayesianComputation, and which does not require a prior on the model parameters.Experimental results on artificial problems with known exact likelihoodsillustrate the potential of the proposed method.
arxiv-16200-32 | Predicting Grades | http://arxiv.org/pdf/1508.03865v2.pdf | author:Yannick Meier, Jie Xu, Onur Atan, Mihaela van der Schaar category:cs.LG published:2015-08-16 summary:To increase efficacy in traditional classroom courses as well as in MassiveOpen Online Courses (MOOCs), automated systems supporting the instructor areneeded. One important problem is to automatically detect students that aregoing to do poorly in a course early enough to be able to take remedialactions. Existing grade prediction systems focus on maximizing the accuracy ofthe prediction while overseeing the importance of issuing timely andpersonalized predictions. This paper proposes an algorithm that predicts thefinal grade of each student in a class. It issues a prediction for each studentindividually, when the expected accuracy of the prediction is sufficient. Thealgorithm learns online what is the optimal prediction and time to issue aprediction based on past history of students' performance in a course. Wederive a confidence estimate for the prediction accuracy and demonstrate theperformance of our algorithm on a dataset obtained based on the performance ofapproximately 700 UCLA undergraduate students who have taken an introductorydigital signal processing over the past 7 years. We demonstrate that for 85% ofthe students we can predict with 76% accuracy whether they are going do well orpoorly in the class after the 4th course week. Using data obtained from a pilotcourse, our methodology suggests that it is effective to perform early in-classassessments such as quizzes, which result in timely performance prediction foreach student, thereby enabling timely interventions by the instructor (at thestudent or class level) when necessary.
arxiv-16200-33 | Multivariate Median Filters and Partial Differential Equations | http://arxiv.org/pdf/1509.08082v2.pdf | author:Martin Welk category:cs.CV I.4.3; G.1.8 published:2015-09-27 summary:Multivariate median filters have been proposed as generalisations of thewell-established median filter for grey-value images to multi-channel images.As multivariate median, most of the recent approaches use the $L^1$ median,i.e.\ the minimiser of an objective function that is the sum of distances toall input points. Many properties of univariate median filters generalise tosuch a filter. However, the famous result by Guichard and Morel aboutapproximation of the mean curvature motion PDE by median filtering does nothave a comparably simple counterpart for $L^1$ multivariate median filtering.We discuss the affine equivariant Oja median and the affine equivarianttransformation--retransformation $L^1$ median as alternatives to $L^1$ medianfiltering. We analyse multivariate median filters in a space-continuoussetting, including the formulation of a space-continuous version of thetransformation--retransformation $L^1$ median, and derive PDEs approximated bythese filters in the cases of bivariate planar images, three-channel volumeimages and three-channel planar images. The PDEs for the affine equivariantfilters can be interpreted geometrically as combinations of a diffusion and aprincipal-component-wise curvature motion contribution with a cross-effect termbased on torsions of principal components. Numerical experiments are presentedthat demonstrate the validity of the approximation results.
arxiv-16200-34 | An Infinite Restricted Boltzmann Machine | http://arxiv.org/pdf/1502.02476v4.pdf | author:Marc-Alexandre Côté, Hugo Larochelle category:cs.LG published:2015-02-09 summary:We present a mathematical construction for the restricted Boltzmann machine(RBM) that doesn't require specifying the number of hidden units. In fact, thehidden layer size is adaptive and can grow during training. This is obtained byfirst extending the RBM to be sensitive to the ordering of its hidden units.Then, thanks to a carefully chosen definition of the energy function, we showthat the limit of infinitely many hidden units is well defined. As with RBM,approximate maximum likelihood training can be performed, resulting in analgorithm that naturally and adaptively adds trained hidden units duringlearning. We empirically study the behaviour of this infinite RBM, showing thatits performance is competitive to that of the RBM, while not requiring thetuning of a hidden layer size.
arxiv-16200-35 | Generalized support vector regression: duality and tensor-kernel representation | http://arxiv.org/pdf/1603.05876v1.pdf | author:Saverio Salzo, Johan A. K. Suykens category:math.OC math.FA stat.ML published:2016-03-18 summary:In this paper we study the variational problem associated to support vectorregression in Banach function spaces. Using the Fenchel-Rockafellar dualitytheory, we give explicit formulation of the dual problem as well as of therelated optimality conditions. Moreover, we provide a new computationalframework for solving the problem which relies on a tensor-kernelrepresentation. This analysis overcomes the typical difficulties connected tolearning in Banach spaces. We finally present a large class of tensor-kernelsto which our theory fully applies: power series tensor kernels. This type ofkernels describe Banach spaces of analytic functions and includegeneralizations of the exponential and polynomial kernels as well as, in thecomplex case, generalizations of the Szeg\"o and Bergman kernels.
arxiv-16200-36 | Approximated Robust Principal Component Analysis for Improved General Scene Background Subtraction | http://arxiv.org/pdf/1603.05875v1.pdf | author:Salehe Erfanian Ebadi, Valia Guerra Ones, Ebroul Izquierdo category:cs.CV stat.AP published:2016-03-18 summary:The research reported in this paper addresses the fundamental task ofseparation of locally moving or deforming image areas from a static or globallymoving background. It builds on the latest developments in the field of robustprincipal component analysis, specifically, the recently reported practicalsolutions for the long-standing problem of recovering the low-rank and sparseparts of a large matrix made up of the sum of these two components. Thisarticle addresses a few critical issues including: embedding global motionparameters in the matrix decomposition model, i.e., estimation of global motionparameters simultaneously with the foreground/background separation task,considering matrix block-sparsity rather than generic matrix sparsity asnatural feature in video processing applications, attenuating backgroundghosting effects when foreground is subtracted, and more critically providingan extremely efficient algorithm to solve the low-rank/sparse matrixdecomposition task. The first aspect is important for background/foregroundseparation in generic video sequences where the background usually obeys globaldisplacements originated by the camera motion in the capturing process. Thesecond aspect exploits the fact that in video processing applications thesparse matrix has a very particular structure, where the non-zero matrixentries are not randomly distributed but they build small blocks within thesparse matrix. The next feature of the proposed approach addresses removal ofghosting effects originated from foreground silhouettes and the lack ofinformation in the occluded background regions of the image. Finally, theproposed model also tackles algorithmic complexity by introducing an extremelyefficient "SVD-free" technique that can be applied in mostbackground/foreground separation tasks for conventional video processing.
arxiv-16200-37 | N-ary Error Correcting Coding Scheme | http://arxiv.org/pdf/1603.05850v1.pdf | author:Joey Tianyi Zhou, Ivor W. Tsang, Shen-Shyang Ho, Klaus-Robert Muller category:cs.LG published:2016-03-18 summary:The coding matrix design plays a fundamental role in the predictionperformance of the error correcting output codes (ECOC)-based multi-class task.{In many-class classification problems, e.g., fine-grained categorization, itis difficult to distinguish subtle between-class differences under existingcoding schemes due to a limited choices of coding values.} In this paper, weinvestigate whether one can relax existing binary and ternary code design to$N$-ary code design to achieve better classification performance. {Inparticular, we present a novel $N$-ary coding scheme that decomposes theoriginal multi-class problem into simpler multi-class subproblems, which issimilar to applying a divide-and-conquer method.} The two main advantages ofsuch a coding scheme are as follows: (i) the ability to construct morediscriminative codes and (ii) the flexibility for the user to select the best$N$ for ECOC-based classification. We show empirically that the optimal $N$(based on classification performance) lies in $[3, 10]$ with some trade-off incomputational cost. Moreover, we provide theoretical insights on the dependencyof the generalization error bound of an $N$-ary ECOC on the average baseclassifier generalization error and the minimum distance between any two codesconstructed. Extensive experimental results on benchmark multi-class datasetsshow that the proposed coding scheme achieves superior prediction performanceover the state-of-the-art coding methods.
arxiv-16200-38 | A Flexible Primal-Dual Toolbox | http://arxiv.org/pdf/1603.05835v1.pdf | author:Hendrik Dirks category:math.OC cs.CV cs.MS published:2016-03-18 summary:\textbf{FlexBox} is a flexible MATLAB toolbox for finite dimensional convexvariational problems in image processing and beyond. Such problems oftenconsist of non-differentiable parts and involve linear operators. The toolboxuses a primal-dual scheme to avoid (computationally) inefficient operatorinversion and to get reliable error estimates. From the user-side,\textbf{FlexBox} expects the primal formulation of the problem, automaticallydecouples operators and dualizes the problem. For large-scale problems,\textbf{FlexBox} also comes with a \cpp-module, which can be used stand-aloneor together with MATLAB via MEX-interfaces. Besides various pre-implementeddata-fidelities and regularization-terms, \textbf{FlexBox} is able to handlearbitrary operators while being easily extendable, due to its object-orienteddesign. The toolbox is available at\href{http://www.flexbox.im}{http://www.flexbox.im}
arxiv-16200-39 | Comparing Time and Frequency Domain for Audio Event Recognition Using Deep Learning | http://arxiv.org/pdf/1603.05824v1.pdf | author:Lars Hertel, Huy Phan, Alfred Mertins category:cs.NE cs.LG cs.SD published:2016-03-18 summary:Recognizing acoustic events is an intricate problem for a machine and anemerging field of research. Deep neural networks achieve convincing results andare currently the state-of-the-art approach for many tasks. One advantage istheir implicit feature learning, opposite to an explicit feature extraction ofthe input signal. In this work, we analyzed whether more discriminativefeatures can be learned from either the time-domain or the frequency-domainrepresentation of the audio signal. For this purpose, we trained multiple deepnetworks with different architectures on the Freiburg-106 and ESC-10 datasets.Our results show that feature learning from the frequency domain is superior tothe time domain. Moreover, additionally using convolution and pooling layers,to explore local structures of the audio signal, significantly improves therecognition performance and achieves state-of-the-art results.
arxiv-16200-40 | Compression of Deep Neural Networks on the Fly | http://arxiv.org/pdf/1509.08745v5.pdf | author:Guillaume Soulié, Vincent Gripon, Maëlys Robert category:cs.LG cs.CV cs.NE published:2015-09-29 summary:Thanks to their state-of-the-art performance, deep neural networks areincreasingly used for object recognition. To achieve these results, they usemillions of parameters to be trained. However, when targeting embeddedapplications the size of these models becomes problematic. As a consequence,their usage on smartphones or other resource limited devices is prohibited. Inthis paper we introduce a novel compression method for deep neural networksthat is performed during the learning phase. It consists in adding an extraregularization term to the cost function of fully-connected layers. We combinethis method with Product Quantization (PQ) of the trained weights for highersavings in storage consumption. We evaluate our method on two data sets (MNISTand CIFAR10), on which we achieve significantly larger compression rates thanstate-of-the-art methods.
arxiv-16200-41 | Postprocessing of Compressed Images via Sequential Denoising | http://arxiv.org/pdf/1510.09041v2.pdf | author:Yehuda Dar, Alfred M. Bruckstein, Michael Elad, Raja Giryes category:cs.CV published:2015-10-30 summary:In this work we propose a novel postprocessing technique forcompression-artifact reduction. Our approach is based on posing this task as aninverse problem, with a regularization that leverages on existingstate-of-the-art image denoising algorithms. We rely on the recently proposedPlug-and-Play Prior framework, suggesting the solution of general inverseproblems via Alternating Direction Method of Multipliers (ADMM), leading to asequence of Gaussian denoising steps. A key feature in our scheme is alinearization of the compression-decompression process, so as to get aformulation that can be optimized. In addition, we supply a thorough analysisof this linear approximation for several basic compression procedures. Theproposed method is suitable for diverse compression techniques that rely ontransform coding. Specifically, we demonstrate impressive gains in imagequality for several leading compression methods - JPEG, JPEG2000, and HEVC.
arxiv-16200-42 | A Comparison between Deep Neural Nets and Kernel Acoustic Models for Speech Recognition | http://arxiv.org/pdf/1603.05800v1.pdf | author:Zhiyun Lu, Dong Guo, Alireza Bagheri Garakani, Kuan Liu, Avner May, Aurelien Bellet, Linxi Fan, Michael Collins, Brian Kingsbury, Michael Picheny, Fei Sha category:cs.LG stat.ML published:2016-03-18 summary:We study large-scale kernel methods for acoustic modeling and compare to DNNson performance metrics related to both acoustic modeling and recognition.Measuring perplexity and frame-level classification accuracy, kernel-basedacoustic models are as effective as their DNN counterparts. However, ontoken-error-rates DNN models can be significantly better. We have discoveredthat this might be attributed to DNN's unique strength in reducing both theperplexity and the entropy of the predicted posterior probabilities. Motivatedby our findings, we propose a new technique, entropy regularized perplexity,for model selection. This technique can noticeably improve the recognitionperformance of both types of models, and reduces the gap between them. Whileeffective on Broadcast News, this technique could be also applicable to othertasks.
arxiv-16200-43 | Unsupervised Cross-Media Hashing with Structure Preservation | http://arxiv.org/pdf/1603.05782v1.pdf | author:Xiangyu Wang, Alex Yong-Sang Chia category:cs.CV cs.IR H.3.3 published:2016-03-18 summary:Recent years have seen the exponential growth of heterogeneous multimediadata. The need for effective and accurate data retrieval from heterogeneousdata sources has attracted much research interest in cross-media retrieval.Here, given a query of any media type, cross-media retrieval seeks to findrelevant results of different media types from heterogeneous data sources. Tofacilitate large-scale cross-media retrieval, we propose a novel unsupervisedcross-media hashing method. Our method incorporates local affinity and distancerepulsion constraints into a matrix factorization framework. Correspondingly,the proposed method learns hash functions that generates unified hash codesfrom different media types, while ensuring intrinsic geometric structure of thedata distribution is preserved. These hash codes empower the similarity betweendata of different media types to be evaluated directly. Experimental results ontwo large-scale multimedia datasets demonstrate the effectiveness of theproposed method, where we outperform the state-of-the-art methods.
arxiv-16200-44 | Learning to Navigate the Energy Landscape | http://arxiv.org/pdf/1603.05772v1.pdf | author:Julien Valentin, Angela Dai, Matthias Nießner, Pushmeet Kohli, Philip Torr, Shahram Izadi, Cem Keskin category:cs.CV published:2016-03-18 summary:In this paper, we present a novel and efficient architecture for addressingcomputer vision problems that use `Analysis by Synthesis'. Analysis bysynthesis involves the minimization of the reconstruction error which istypically a non-convex function of the latent target variables.State-of-the-art methods adopt a hybrid scheme where discriminatively trainedpredictors like Random Forests or Convolutional Neural Networks are used toinitialize local search algorithms. While these methods have been shown toproduce promising results, they often get stuck in local optima. Our methodgoes beyond the conventional hybrid architecture by not only proposing multipleaccurate initial solutions but by also defining a navigational structure overthe solution space that can be used for extremely efficient gradient-free localsearch. We demonstrate the efficacy of our approach on the challenging problemof RGB Camera Relocalization. To make the RGB camera relocalization problemparticularly challenging, we introduce a new dataset of 3D environments whichare significantly larger than those found in other publicly-available datasets.Our experiments reveal that the proposed method is able to achievestate-of-the-art camera relocalization results. We also demonstrate thegeneralizability of our approach on Hand Pose Estimation and Image Retrievaltasks.
arxiv-16200-45 | Accelerating Deep Neural Network Training with Inconsistent Stochastic Gradient Descent | http://arxiv.org/pdf/1603.05544v2.pdf | author:Linnan Wang, Yi Yang, Martin Renqiang Min, Srimat Chakradhar category:cs.LG cs.DC published:2016-03-17 summary:SGD is the widely adopted method to train CNN. Conceptually it approximatesthe population with a randomly sampled batch; then it evenly trains batches byconducting a gradient update on every batch in an epoch. In this paper, wedemonstrate Sampling Bias, Intrinsic Image Difference and Fixed Cycle PseudoRandom Sampling differentiate batches in training, which then affect learningspeeds on them. Because of this, the unbiased treatment of batches involved inSGD creates improper load balancing. To address this issue, we presentInconsistent Stochastic Gradient Descent (ISGD) to dynamically vary trainingeffort according to learning statuses on batches. Specifically ISGD leveragestechniques in Statistical Process Control to identify a undertrained batch.Once a batch is undertrained, ISGD solves a new subproblem, a chasing logicplus a conservative constraint, to accelerate the training on the batch whileavoid drastic parameter changes. Extensive experiments on a variety of datasetsdemonstrate ISGD converges faster than SGD. In training AlexNet, ISGD is21.05\% faster than SGD to reach 56\% top1 accuracy under the exactly sameexperiment setup. We also extend ISGD to work on multiGPU or heterogeneousdistributed system based on data parallelism, enabling the batch size to be thekey to scalability. Then we present the study of ISGD batch size to thelearning rate, parallelism, synchronization cost, system saturation andscalability. We conclude the optimal ISGD batch size is machine dependent.Various experiments on a multiGPU system validate our claim. In particular,ISGD trains AlexNet to 56.3% top1 and 80.1% top5 accuracy in 11.5 hours with 4NVIDIA TITAN X at the batch size of 1536.
arxiv-16200-46 | A Probabilistic Machine Learning Approach to Detect Industrial Plant Faults | http://arxiv.org/pdf/1603.05770v1.pdf | author:Wei Xiao category:stat.ML stat.AP published:2016-03-18 summary:Fault detection in industrial plants is a hot research area as more and moresensor data are being collected throughout the industrial process. Automaticdata-driven approaches are widely needed and seen as a promising area ofinvestment. This paper proposes an effective machine learning algorithm topredict industrial plant faults based on classification methods such aspenalized logistic regression, random forest and gradient boosted tree. Afault's start time and end time are predicted sequentially in two steps byformulating the original prediction problems as classification problems. Thealgorithms described in this paper won first place in the Prognostics andHealth Management Society 2015 Data Challenge.
arxiv-16200-47 | From line segments to more organized Gestalts | http://arxiv.org/pdf/1603.05763v1.pdf | author:Boshra Rajaei, Rafael Grompone von Gioi, Jean-Michel Morel category:cs.CV published:2016-03-18 summary:In this paper, we reconsider the early computer vision bottom-up program,according to which higher level features (geometric structures) in an imagecould be built up recursively from elementary features by simple groupingprinciples coming from Gestalt theory. Taking advantage of the (recent)advances in reliable line segment detectors, we propose three feature detectorsthat constitute one step up in this bottom up pyramid. For any digital image,our unsupervised algorithm computes three classic Gestalts from the set ofpredetected line segments: good continuations, nonlocal alignments, and bars.The methodology is based on a common stochastic {\it a contrario model}yielding three simple detection formulas, characterized by their number offalse alarms. This detection algorithm is illustrated on several digitalimages.
arxiv-16200-48 | Grounding of Textual Phrases in Images by Reconstruction | http://arxiv.org/pdf/1511.03745v3.pdf | author:Anna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor Darrell, Bernt Schiele category:cs.CV cs.CL cs.LG published:2015-11-12 summary:Grounding (i.e. localizing) arbitrary, free-form textual phrases in visualcontent is a challenging problem with many applications for human-computerinteraction and image-text reference resolution. Few datasets provide theground truth spatial localization of phrases, thus it is desirable to learnfrom data with no or little grounding supervision. We propose a novel approachwhich learns grounding by reconstructing a given phrase using an attentionmechanism, which can be either latent or optimized directly. During trainingour model encodes the phrase using a recurrent network language model and thenlearns to attend to the relevant image region in order to reconstruct the inputphrase. At test time, the correct attention, i.e., the grounding, is evaluated.If grounding supervision is available it can be directly applied via a lossover the attention mechanism. We demonstrate the effectiveness of our approachon the Flickr 30k Entities and ReferItGame datasets with different levels ofsupervision, ranging from no supervision over partial supervision to fullsupervision. Our supervised variant improves by a large margin over thestate-of-the-art on both datasets.
arxiv-16200-49 | Mean-Field Inference in Gaussian Restricted Boltzmann Machine | http://arxiv.org/pdf/1512.00927v2.pdf | author:Chako Takahashi, Muneki Yasuda category:stat.ML published:2015-12-03 summary:A Gaussian restricted Boltzmann machine (GRBM) is a Boltzmann machine definedon a bipartite graph and is an extension of usual restricted Boltzmannmachines. A GRBM consists of two different layers: a visible layer composed ofcontinuous visible variables and a hidden layer composed of discrete hiddenvariables. In this paper, we derive two different inference algorithms forGRBMs based on the naive mean-field approximation (NMFA). One is an inferencealgorithm for whole variables in a GRBM, and the other is an inferencealgorithm for partial variables in a GBRBM. We compare the two methodsanalytically and numerically and show that the latter method is better.
arxiv-16200-50 | Program Evaluation and Causal Inference with High-Dimensional Data | http://arxiv.org/pdf/1311.2645v6.pdf | author:Alexandre Belloni, Victor Chernozhukov, Ivan Fernández-Val, Chris Hansen category:math.ST stat.ME stat.ML stat.TH published:2013-11-11 summary:In this paper, we provide efficient estimators and honest confidence bandsfor a variety of treatment effects including local average (LATE) and localquantile treatment effects (LQTE) in data-rich environments. We can handle verymany control variables, endogenous receipt of treatment, heterogeneoustreatment effects, and function-valued outcomes. Our framework covers thespecial case of exogenous receipt of treatment, either conditional on controlsor unconditionally as in randomized control trials. In the latter case, ourapproach produces efficient estimators and honest bands for (functional)average treatment effects (ATE) and quantile treatment effects (QTE). To makeinformative inference possible, we assume that key reduced form predictiverelationships are approximately sparse. This assumption allows the use ofregularization and selection methods to estimate those relations, and weprovide methods for post-regularization and post-selection inference that areuniformly valid (honest) across a wide-range of models. We show that a keyingredient enabling honest inference is the use of orthogonal or doubly robustmoment conditions in estimating certain reduced form functional parameters. Weillustrate the use of the proposed methods with an application to estimatingthe effect of 401(k) eligibility and participation on accumulated assets.
arxiv-16200-51 | A Readability Analysis of Campaign Speeches from the 2016 US Presidential Campaign | http://arxiv.org/pdf/1603.05739v1.pdf | author:Elliot Schumacher, Maxine Eskenazi category:cs.CL published:2016-03-18 summary:Readability is defined as the reading level of the speech from grade 1 tograde 12. It results from the use of the REAP readability analysis (vocabulary- Collins-Thompson and Callan, 2004; syntax - Heilman et al ,2006, 2007), whichuse the lexical contents and grammatical structure of the sentences in adocument to predict the reading level. After analysis, results were groupedinto the average readability of each candidate, the evolution of thecandidate's speeches' readability over time and the standard deviation, or howmuch each candidate varied their speech from one venue to another. Forcomparison, one speech from four past presidents and the Gettysburg Addresswere also analyzed.
arxiv-16200-52 | Do Deep Convolutional Nets Really Need to be Deep (Or Even Convolutional)? | http://arxiv.org/pdf/1603.05691v1.pdf | author:Gregor Urban, Krzysztof J. Geras, Samira Ebrahimi Kahou, Ozlem Aslan, Shengjie Wang, Rich Caruana, Abdelrahman Mohamed, Matthai Philipose, Matt Richardson category:stat.ML cs.LG published:2016-03-17 summary:Yes, apparently they do. Previous research demonstrated that shallow feed-forward nets sometimes canlearn the complex functions previously learned by deep nets while using asimilar number of parameters as the deep models they mimic. In this paper weinvestigate if shallow models can learn to mimic the functions learned by deepconvolutional models. We experiment with shallow models and models with avarying number of convolutional layers, all trained to mimic a state-of-the-artensemble of CIFAR- 10 models. We demonstrate that we are unable to trainshallow models to be of comparable accuracy to deep convolutional models.Although the student models do not have to be as deep as the teacher modelsthey mimic, the student models apparently need multiple convolutional layers tolearn functions of comparable accuracy.
arxiv-16200-53 | Predicting health inspection results from online restaurant reviews | http://arxiv.org/pdf/1603.05673v1.pdf | author:Samantha Wong, Hamidreza Chinaei, Frank Rudzicz category:cs.CL cs.LG published:2016-03-17 summary:Informatics around public health are increasingly shifting from theprofessional to the public spheres. In this work, we apply linguistic analyticsto restaurant reviews, from Yelp, in order to automatically predict officialhealth inspection reports. We consider two types of feature sets, i.e., keyworddetection and topic model features, and use these in several classificationmethods. Our empirical analysis shows that these extracted features can predictpublic health inspection reports with over 90% accuracy using simple supportvector machines.
arxiv-16200-54 | Bank distress in the news: Describing events through deep learning | http://arxiv.org/pdf/1603.05670v1.pdf | author:Samuel Rönnqvist, Peter Sarlin category:cs.CL cs.AI cs.IR cs.NE q-fin.CP published:2016-03-17 summary:While many models are purposed for detecting the occurrence of events incomplex systems, the task of providing qualitative detail on the developmentsis not usually as well automated. We present a deep learning approach fordetecting relevant discussion in text and extracting natural languagedescriptions of events. Supervised by only a small set of event information,the model is leveraged by unsupervised learning of semantic vectorrepresentations on extensive text data. We demonstrate applicability to thestudy of financial risk based on news (6.6M articles), particularly bankdistress and government interventions (243 events), where indices can signalthe level of bank-stress-related reporting at the entity level, or aggregatedat country or European level, while being coupled with explanations. Thus, weexemplify how text, as timely and widely available data, can serve as a usefulcomplementary source of information for financial risk analytics.
arxiv-16200-55 | Variance Reduction for Faster Non-Convex Optimization | http://arxiv.org/pdf/1603.05643v1.pdf | author:Zeyuan Allen-Zhu, Elad Hazan category:math.OC cs.DS cs.LG cs.NE stat.ML published:2016-03-17 summary:We consider the fundamental problem in non-convex optimization of efficientlyreaching a stationary point. In contrast to the convex case, in the longhistory of this basic problem, the only known theoretical results onfirst-order non-convex optimization remain to be full gradient descent thatconverges in $O(1/\varepsilon)$ iterations for smooth objectives, andstochastic gradient descent that converges in $O(1/\varepsilon^2)$ iterationsfor objectives that are sum of smooth functions. We provide the first improvement in this line of research. Our result isbased on the variance reduction trick recently introduced to convexoptimization, as well as a brand new analysis of variance reduction that issuitable for non-convex optimization. For objectives that are sum of smoothfunctions, our first-order minibatch stochastic method converges with an$O(1/\varepsilon)$ rate, and is faster than full gradient descent by$\Omega(n^{1/3})$. We demonstrate the effectiveness of our methods on empirical riskminimizations with non-convex loss functions and training neural nets.
arxiv-16200-56 | Generative Image Modeling using Style and Structure Adversarial Networks | http://arxiv.org/pdf/1603.05631v1.pdf | author:Xiaolong Wang, Abhinav Gupta category:cs.CV published:2016-03-17 summary:Current generative frameworks use end-to-end learning and generate images bysampling from uniform noise distribution. However, these approaches ignore themost basic principle of image formation: images are product of: (a) Structure:the underlying 3D model; (b) Style: the texture mapped onto structure. In thispaper, we factorize the image generation process and propose Style andStructure Generative Adversarial Network (S^2-GAN). Our S^2-GAN has twocomponents: the Structure-GAN generates a surface normal map; the Style-GANtakes the surface normal map as input and generates the 2D image. Apart from areal vs. generated loss function, we use an additional loss with computedsurface normals from generated images. The two GANs are first trainedindependently, and then merged together via joint learning. We show our S^2-GANmodel is interpretable, generates more realistic images and can be used tolearn unsupervised RGBD representations.
arxiv-16200-57 | Streaming Algorithms for News and Scientific Literature Recommendation: Submodular Maximization with a $d$-Knapsack Constraint | http://arxiv.org/pdf/1603.05614v1.pdf | author:Qilian Yu, Easton Li Xu, Shuguang Cui category:cs.LG cs.DS published:2016-03-17 summary:Submodular maximization problems belong to the family of combinatorialoptimization problems and enjoy wide applications. In this paper, we focus onthe problem of maximizing a monotone submodular function subject to a$d$-knapsack constraint, for which we propose a streaming algorithm thatachieves a $\left(\frac{1}{1+d}-\epsilon\right)$-approximation of the optimalvalue, while it only needs one single pass through the dataset without storingall the data in the memory. In our experiments, we extensively evaluate theeffectiveness of our proposed algorithm via two applications: newsrecommendation and scientific literature recommendation. It is observed thatthe proposed streaming algorithm achieves both execution speedup and memorysaving by several orders of magnitude, compared with existing approaches.
arxiv-16200-58 | "What happens if..." Learning to Predict the Effect of Forces in Images | http://arxiv.org/pdf/1603.05600v1.pdf | author:Roozbeh Mottaghi, Mohammad Rastegari, Abhinav Gupta, Ali Farhadi category:cs.CV published:2016-03-17 summary:What happens if one pushes a cup sitting on a table toward the edge of thetable? How about pushing a desk against a wall? In this paper, we study theproblem of understanding the movements of objects as a result of applyingexternal forces to them. For a given force vector applied to a specificlocation in an image, our goal is to predict long-term sequential movementscaused by that force. Doing so entails reasoning about scene geometry, objects,their attributes, and the physical rules that govern the movements of objects.We design a deep neural network model that learns long-term sequentialdependencies of object movements while taking into account the geometry andappearance of the scene by combining Convolutional and Recurrent NeuralNetworks. Training our model requires a large-scale dataset of object movementscaused by external forces. To build a dataset of forces in scenes, wereconstructed all images in SUN RGB-D dataset in a physics simulator toestimate the physical movements of objects caused by external forces applied tothem. Our Forces in Scenes (ForScene) dataset contains 10,335 images in which avariety of external forces are applied to different types of objects resultingin more than 65,000 object movements represented in 3D. Our experimentalevaluations show that the challenging task of predicting long-term movements ofobjects as their reaction to external forces is possible from a single image.
arxiv-16200-59 | Mapping Temporal Variables into the NeuCube for Improved Pattern Recognition, Predictive Modelling and Understanding of Stream Data | http://arxiv.org/pdf/1603.05594v1.pdf | author:Enmei Tu, Nikola Kasabov, Jie Yang category:cs.NE cs.AI stat.ML published:2016-03-17 summary:This paper proposes a new method for an optimized mapping of temporalvariables, describing a temporal stream data, into the recently proposedNeuCube spiking neural network architecture. This optimized mapping extends theuse of the NeuCube, which was initially designed for spatiotemporal brain data,to work on arbitrary stream data and to achieve a better accuracy of temporalpattern recognition, a better and earlier event prediction and a betterunderstanding of complex temporal stream data through visualization of theNeuCube connectivity. The effect of the new mapping is demonstrated on threebench mark problems. The first one is early prediction of patient sleep stageevent from temporal physiological data. The second one is pattern recognitionof dynamic temporal patterns of traffic in the Bay Area of California and thelast one is the Challenge 2012 contest data set. In all cases the use of theproposed mapping leads to an improved accuracy of pattern recognition and eventprediction and a better understanding of the data when compared to traditionalmachine learning techniques or spiking neural network reservoirs with arbitrarymapping of the variables.
arxiv-16200-60 | Deep Manifold Traversal: Changing Labels with Convolutional Features | http://arxiv.org/pdf/1511.06421v3.pdf | author:Jacob R. Gardner, Paul Upchurch, Matt J. Kusner, Yixuan Li, Kilian Q. Weinberger, Kavita Bala, John E. Hopcroft category:cs.LG cs.CV stat.ML published:2015-11-19 summary:Many tasks in computer vision can be cast as a "label changing" problem,where the goal is to make a semantic change to the appearance of an image orsome subject in an image in order to alter the class membership. Althoughsuccessful task-specific methods have been developed for some label changingapplications, to date no general purpose method exists. Motivated by this wepropose deep manifold traversal, a method that addresses the problem in itsmost general form: it first approximates the manifold of natural images thenmorphs a test image along a traversal path away from a source class and towardsa target class while staying near the manifold throughout. The resultingalgorithm is surprisingly effective and versatile. It is completely datadriven, requiring only an example set of images from the desired source andtarget domains. We demonstrate deep manifold traversal on highly diverse labelchanging tasks: changing an individual's appearance (age and hair color),changing the season of an outdoor image, and transforming a city skylinetowards nighttime.
arxiv-16200-61 | Learning to Generate Images with Perceptual Similarity Metrics | http://arxiv.org/pdf/1511.06409v2.pdf | author:Karl Ridgeway, Jake Snell, Brett D. Roads, Richard S. Zemel, Michael C. Mozer category:cs.LG cs.CV published:2015-11-19 summary:Deep networks are increasingly being applied to problems involving imagesynthesis, e.g., generating images from textual descriptions and reconstructingan input image from a compact representation. Supervised training ofimage-synthesis networks typically uses a pixel-wise loss (PL) to indicate themismatch between a generated image and its corresponding target image. Wepropose instead to use a loss function that is better calibrated to humanperceptual judgments of image quality: the multiscale structural-similarityscore (MS-SSIM). Because MS-SSIM is differentiable, it is easily incorporatedinto gradient-descent learning. We compare the consequences of using MS-SSIMversus PL loss on training deterministic and stochastic autoencoders. For threedifferent architectures, we collected human judgments of the quality of imagereconstructions. Observers reliably prefer images synthesized byMS-SSIM-optimized models over those synthesized by PL-optimized models, for twodistinct PL measures ($\ell_1$ and $\ell_2$ distances). We also explore theeffect of training objective on image encoding and analyze conditions underwhich perceptually-optimized representations yield better performance on imageclassification. Just as computer vision has advanced through the use ofconvolutional architectures that mimic the structure of the mammalian visualsystem, we argue that significant additional advances can be made in modelingimages through the use of training objectives that are well aligned tocharacteristics of human perception.
arxiv-16200-62 | Predicate Gradual Logic and Linguistics | http://arxiv.org/pdf/1603.05570v1.pdf | author:Ryuta Arisaka category:cs.CL published:2016-03-17 summary:There are several major proposals for treating donkey anaphora such asdiscourse representation theory and the likes, or E-Type theories and thelikes. Every one of them works well for a set of specific examples that theyuse to demonstrate validity of their approaches. As I show in this paper,however, they are not very generalisable and do not account for essentially thesame problem that they remedy when it manifests in other examples. I proposeanother logical approach. I develoop logic that extends a recent, propositionalgradual logic, and show that it can treat donkey anaphora generally. I alsoidentify and address a problem around the modern convention on existentialimport. Furthermore, I show that Aristotle's syllogisms and conversion arerealisable in this logic.
arxiv-16200-63 | Less is More: Nyström Computational Regularization | http://arxiv.org/pdf/1507.04717v6.pdf | author:Alessandro Rudi, Raffaello Camoriano, Lorenzo Rosasco category:stat.ML cs.LG published:2015-07-16 summary:We study Nystr\"om type subsampling approaches to large scale kernel methods,and prove learning bounds in the statistical learning setting, where randomsampling and high probability estimates are considered. In particular, we provethat these approaches can achieve optimal learning bounds, provided thesubsampling level is suitably chosen. These results suggest a simpleincremental variant of Nystr\"om Kernel Regularized Least Squares, where thesubsampling level implements a form of computational regularization, in thesense that it controls at the same time regularization and computations.Extensive experimental analysis shows that the considered approach achievesstate of the art performances on benchmark large scale datasets.
arxiv-16200-64 | Tracking multiple moving objects in images using Markov Chain Monte Carlo | http://arxiv.org/pdf/1603.05522v1.pdf | author:Lan Jiang, Sumeetpal S. Singh category:stat.AP cs.CV stat.CO published:2016-03-17 summary:A new Bayesian state and parameter learning algorithm for multiple targettracking (MTT) models with image observations is proposed. Specifically, aMarkov chain Monte Carlo algorithm is designed to sample from the posteriordistribution of the unknown number of targets, their birth and death times,states and model parameters, which constitutes the complete solution to thetracking problem. The conventional approach is to pre-process the images toextract point observations and then perform tracking. We model the imagegeneration process directly to avoid potential loss of information whenextracting point observations. Numerical examples show that our algorithm hasimproved tracking performance over commonly used techniques, for both syntheticexamples and real florescent microscopy data, especially in the case of dimtargets with overlapping illuminated regions.
arxiv-16200-65 | Neural Machine Translation of Rare Words with Subword Units | http://arxiv.org/pdf/1508.07909v3.pdf | author:Rico Sennrich, Barry Haddow, Alexandra Birch category:cs.CL published:2015-08-31 summary:Neural machine translation (NMT) models typically operate with a fixedvocabulary, but translation is an open-vocabulary problem. Previous workaddresses the translation of out-of-vocabulary words by backing off to adictionary. In this paper, we introduce a simpler and more effective approach,making the NMT model capable of open-vocabulary translation by encoding rareand unknown words as sequences of subword units. This is based on the intuitionthat various word classes are translatable via smaller units than words, forinstance names (via character copying or transliteration), compounds (viacompositional translation), and cognates and loanwords (via phonological andmorphological transformations). We discuss the suitability of different wordsegmentation techniques, including simple character n-gram models and asegmentation based on the byte pair encoding compression algorithm, andempirically show that subword models improve over a back-off dictionarybaseline for the WMT 15 translation tasks English-German and English-Russian by1.1 and 1.3 BLEU, respectively.
arxiv-16200-66 | Feature Selection with Annealing for Computer Vision and Big Data Learning | http://arxiv.org/pdf/1310.2880v7.pdf | author:Adrian Barbu, Yiyuan She, Liangjing Ding, Gary Gramajo category:stat.ML cs.CV cs.LG math.ST stat.TH published:2013-10-10 summary:Many computer vision and medical imaging problems are faced with learningfrom large-scale datasets, with millions of observations and features. In thispaper we propose a novel efficient learning scheme that tightens a sparsityconstraint by gradually removing variables based on a criterion and a schedule.The attractive fact that the problem size keeps dropping throughout theiterations makes it particularly suitable for big data learning. Our approachapplies generically to the optimization of any differentiable loss function,and finds applications in regression, classification and ranking. The resultantalgorithms build variable screening into estimation and are extremely simple toimplement. We provide theoretical guarantees of convergence and selectionconsistency. In addition, one dimensional piecewise linear response functionsare used to account for nonlinearity and a second order prior is imposed onthese functions to avoid overfitting. Experiments on real and synthetic datashow that the proposed method compares very well with other state of the artmethods in regression, classification and ranking while being computationallyvery efficient and scalable.
arxiv-16200-67 | Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 | http://arxiv.org/pdf/1602.02830v3.pdf | author:Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, Yoshua Bengio category:cs.LG published:2016-02-09 summary:We introduce a method to train Binarized Neural Networks (BNNs) - neuralnetworks with binary weights and activations at run-time. At training-time thebinary weights and activations are used for computing the parameters gradients.During the forward pass, BNNs drastically reduce memory size and accesses, andreplace most arithmetic operations with bit-wise operations, which is expectedto substantially improve power-efficiency. To validate the effectiveness ofBNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. Onboth, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10and SVHN datasets. Last but not least, we wrote a binary matrix multiplicationGPU kernel with which it is possible to run our MNIST BNN 7 times faster thanwith an unoptimized GPU kernel, without suffering any loss in classificationaccuracy. The code for training and running our BNNs is available on-line.
arxiv-16200-68 | A flexible state space model for learning nonlinear dynamical systems | http://arxiv.org/pdf/1603.05486v1.pdf | author:Andreas Svensson, Thomas B. Schön category:stat.CO cs.SY stat.ML published:2016-03-17 summary:We consider a nonlinear state space model with the state transition andobservation functions expressed as basis function expansions. We learn thecoefficients in the basis function expansions from data, and with a connectionto Gaussian processes we also develop priors on them for tuning the modelflexibility and to prevent overfitting to data, akin to a Gaussian processstate space model. The priors can alternatively be seen as a regularization,and helps the model in generalizing the data without sacrificing the richnessoffered by the basis function expansion. To learn the coefficients and otherunknown parameters efficiently, we tailor an algorithm for this model usingstate-of-the-art sequential Monte Carlo methods, which comes with theoreticalguarantees on the learning. Our approach indicates promising results whenevaluated on a classical benchmark as well as real data.
arxiv-16200-69 | Classification and Reconstruction of High-Dimensional Signals from Low-Dimensional Features in the Presence of Side Information | http://arxiv.org/pdf/1412.0614v2.pdf | author:Francesco Renna, Liming Wang, Xin Yuan, Jianbo Yang, Galen Reeves, Robert Calderbank, Lawrence Carin, Miguel R. D. Rodrigues category:cs.IT cs.CV math.IT math.ST stat.ML stat.TH published:2014-12-01 summary:This paper offers a characterization of fundamental limits on theclassification and reconstruction of high-dimensional signals fromlow-dimensional features, in the presence of side information. We consider ascenario where a decoder has access both to linear features of the signal ofinterest and to linear features of the side information signal; while the sideinformation may be in a compressed form, the objective is recovery orclassification of the primary signal, not the side information. The signal ofinterest and the side information are each assumed to have (distinct) latentdiscrete labels; conditioned on these two labels, the signal of interest andside information are drawn from a multivariate Gaussian distribution. Withjoint probabilities on the latent labels, the overall signal-(side information)representation is defined by a Gaussian mixture model. We then provide sharpsufficient and/or necessary conditions for these quantities to approach zerowhen the covariance matrices of the Gaussians are nearly low-rank. Theseconditions, which are reminiscent of the well-known Slepian-Wolf and Wyner-Zivconditions, are a function of the number of linear features extracted from thesignal of interest, the number of linear features extracted from the sideinformation signal, and the geometry of these signals and their interplay.Moreover, on assuming that the signal of interest and the side information obeysuch an approximately low-rank model, we derive expansions of thereconstruction error as a function of the deviation from an exactly low-rankmodel; such expansions also allow identification of operational regimes wherethe impact of side information on signal reconstruction is most relevant. Ourframework, which offers a principled mechanism to integrate side information inhigh-dimensional data problems, is also tested in the context of imagingapplications.
arxiv-16200-70 | Neural Aggregation Network for Video Face Recognition | http://arxiv.org/pdf/1603.05474v1.pdf | author:Jiaolong Yang, Peiran Ren, Dong Chen, Fang Wen, Hongdong Li, Gang Hua category:cs.CV cs.AI published:2016-03-17 summary:In this paper, we present a Neural Aggregation Network (NAN) for video facerecognition. The network takes a face video or face image set of a person withvariable number of face frames as its input, and produces a compact andfixed-dimension visual representation of that person. The whole network iscomposed of two modules. The feature embedding module is a CNN which maps eachface frame into a feature representation. The neural aggregation module iscomposed of two content based attention blocks which is driven by a memorystoring all the features extracted from the face video through the featureembedding module. The output of the first attention block adapts the second,whose output is adopted as the aggregated representation of the video faces.Due to the attention mechanism, this representation is invariant to the orderof the face frames. The experiments show that the proposed NAN consistentlyoutperforms hand-crafted aggregations such as average pooling, and achievesstate-of-the-art accuracy on three video face recognition datasets: the YouTubeFace, IJB-A and Celebrity-1000 datasets.
arxiv-16200-71 | Variable-Length Hashing | http://arxiv.org/pdf/1603.05414v1.pdf | author:Honghai Yu, Pierre Moulin, Hong Wei Ng, Xiaoli Li category:cs.CV cs.IR published:2016-03-17 summary:Hashing has emerged as a popular technique for large-scale similarity search.Most learning-based hashing methods generate compact yet correlated hash codes.However, this redundancy is storage-inefficient. Hence we propose a losslessvariable-length hashing (VLH) method that is both storage- andsearch-efficient. Storage efficiency is achieved by converting the fixed-lengthhash code into a variable-length code. Search efficiency is obtained by using amultiple hash table structure. With VLH, we are able to deliberately addredundancy into hash codes to improve retrieval performance with littlesacrifice in storage efficiency or search complexity. In particular, we proposea block K-means hashing (B-KMH) method to obtain significantly improvedretrieval performance with no increase in storage and marginal increase incomputational cost.
arxiv-16200-72 | Online semi-parametric learning for inverse dynamics modeling | http://arxiv.org/pdf/1603.05412v1.pdf | author:Diego Romeres, Mattia Zorzi, Alessandro Chiuso category:math.OC cs.LG stat.ML published:2016-03-17 summary:This paper presents a semi-parametric algorithm for online learning of arobot inverse dynamics model. It combines the strength of the parametric andnon-parametric modeling. The former exploits the rigid body dynamics equation,while the latter exploits a suitable kernel function. We provide an extensivecomparison with other methods from the literature using real data from the iCubhumanoid robot. In doing so we also compare two different techniques, namelycross validation and marginal likelihood optimization, for estimating thehyperparameters of the kernel function.
arxiv-16200-73 | Cooking in the kitchen: Recognizing and Segmenting Human Activities in Videos | http://arxiv.org/pdf/1508.06073v2.pdf | author:Hilde Kuehne, Juergen Gall, Thomas Serre category:cs.CV published:2015-08-25 summary:As research on action recognition matures, the focus is shifting away fromcategorizing basic task-oriented actions using hand-segmented video datasets tounderstanding complex goal-oriented daily human activities in real-worldsettings. Temporally structured models would seem obvious to tackle this set ofproblems, but so far, cases where these models have outperformed simplerunstructured bag-of-word types of models are scarce. With the increasingavailability of large human activity datasets, combined with the development ofnovel feature coding techniques that yield more compact representations, it istime to revisit structured generative approaches. Here, we describe an end-to-end generative approach from the encoding offeatures to the structural modeling of complex human activities by applyingFisher vectors and temporal models for the analysis of video sequences. We systematically evaluate the proposed approach on several availabledatasets (ADL, MPIICooking, and Breakfast datasets) using a variety ofperformance metrics. Through extensive system evaluations, we demonstrate thatcombining compact video representations based on Fisher Vectors with HMM-basedmodeling yields very significant gains in accuracy and when properly trainedwith sufficient training samples, structured temporal models outperformunstructured bag-of-word types of models by a large margin on the testedperformance metric.
arxiv-16200-74 | An end-to-end generative framework for video segmentation and recognition | http://arxiv.org/pdf/1509.01947v2.pdf | author:Hilde Kuehne, Juergen Gall, Thomas Serre category:cs.CV published:2015-09-07 summary:We describe an end-to-end generative approach for the segmentation andrecognition of human activities. In this approach, a visual representationbased on reduced Fisher Vectors is combined with a structured temporal modelfor recognition. We show that the statistical properties of Fisher Vectors makethem an especially suitable front-end for generative models such as Gaussianmixtures. The system is evaluated for both the recognition of complexactivities as well as their parsing into action units. Using a variety of videodatasets ranging from human cooking activities to animal behaviors, ourexperiments demonstrate that the resulting architecture outperformsstate-of-the-art approaches for larger datasets, i.e. when sufficient amount ofdata is available for training structured generative models.
arxiv-16200-75 | Audio Word2Vec: Unsupervised Learning of Audio Segment Representations using Sequence-to-sequence Autoencoder | http://arxiv.org/pdf/1603.00982v3.pdf | author:Yu-An Chung, Chao-Chung Wu, Chia-Hao Shen, Hung-Yi Lee, Lin-Shan Lee category:cs.SD cs.LG published:2016-03-03 summary:The vector representations of fixed dimensionality for words (in text)offered by Word2Vec have been shown to be very useful in many applicationscenarios, in particular due to the semantic information they carry. This paperproposes a parallel version, the Audio Word2Vec. It offers the vectorrepresentations of fixed dimensionality for variable-length audio segments.These vector representations are shown to describe the sequential phoneticstructures of the audio segments to a good degree, with very attractive realworld applications such as query-by-example Spoken Term Detection (STD). Inthis STD application, the proposed approach significantly outperformed theconventional Dynamic Time Warping (DTW) based approaches at significantly lowercomputation requirements. We propose unsupervised learning of Audio Word2Vecfrom audio data without human annotation using Sequence-to-sequence Audoencoder(SA). SA consists of two RNNs equipped with Long Short-Term Memory (LSTM)units: the first RNN (encoder) maps the input audio sequence into a vectorrepresentation of fixed dimensionality, and the second RNN (decoder) maps therepresentation back to the input audio sequence. The two RNNs are jointlytrained by minimizing the reconstruction error. Denoising Sequence-to-sequenceAutoencoder (DSA) is furthered proposed offering more robust learning.
arxiv-16200-76 | Cascading Bandits for Large-Scale Recommendation Problems | http://arxiv.org/pdf/1603.05359v1.pdf | author:Shi Zong, Hao Ni, Kenny Sung, Nan Rosemary Ke, Zheng Wen, Branislav Kveton category:cs.LG stat.ML published:2016-03-17 summary:Most recommender systems recommend a list of items. The user examines thelist, from the first item to the last, and often chooses the first attractiveitem and does not examine the rest. This type of user behavior can be modeledby the cascade model. In this work, we study cascading bandits, an onlinelearning variant of the cascade model where the goal is to recommend $K$ mostattractive items from a large set of $L$ candidate items. We propose twoalgorithms for solving this problem, which are based on the idea of lineargeneralization. The key idea in our solutions is that we learn a predictor ofthe attraction probabilities of items from their features, as opposing tolearning the attraction probability of each item independently as in theexisting work. This results in practical learning algorithms whose regret doesnot depend on the number of items $L$. We bound the regret of one algorithm andcomprehensively evaluate the other on a range of recommendation problems. Thealgorithm performs well and outperforms all baselines.
arxiv-16200-77 | Validation of k-Nearest Neighbor Classifiers Using Inclusion and Exclusion | http://arxiv.org/pdf/1410.2500v3.pdf | author:Eric Bax, Lingjie Weng, Xu Tian category:cs.LG cs.IT math.IT stat.ML published:2014-10-09 summary:This paper presents a series of PAC exponential error bounds for $k$-nearestneighbors classifiers, with O($n^{-\frac{r}{2r+1}}\sqrt{k \ln n}$) error boundrange for each integer $r>0$, where $n$ is the number of in-sample examples.This shows that $k$-nn classifiers, in spite of their famously fractureddecision boundaries, come close to having Gaussian-style exponential errorbounds with O($n^{-\frac{1}{2}}$) bound ranges.
arxiv-16200-78 | Convolutional Neural Networks using Logarithmic Data Representation | http://arxiv.org/pdf/1603.01025v2.pdf | author:Daisuke Miyashita, Edward H. Lee, Boris Murmann category:cs.NE cs.LG published:2016-03-03 summary:Recent advances in convolutional neural networks have considered modelcomplexity and hardware efficiency to enable deployment onto embedded systemsand mobile devices. For example, it is now well-known that the arithmeticoperations of deep networks can be encoded down to 8-bit fixed-point withoutsignificant deterioration in performance. However, further reduction inprecision down to as low as 3-bit fixed-point results in significant losses inperformance. In this paper we propose a new data representation that enablesstate-of-the-art networks to be encoded to 3 bits with negligible loss inclassification performance. To perform this, we take advantage of the fact thatthe weights and activations in a trained network naturally have non-uniformdistributions. Using non-uniform, base-2 logarithmic representation to encodeweights, communicate activations, and perform dot-products enables networks to1) achieve higher classification accuracies than fixed-point at the sameresolution and 2) eliminate bulky digital multipliers. Finally, we propose anend-to-end training procedure that uses log representation at 5-bits, whichachieves higher final test accuracy than linear at 5-bits.
arxiv-16200-79 | Feature Selection for Classification under Anonymity Constraint | http://arxiv.org/pdf/1512.07158v4.pdf | author:Baichuan Zhang, Vachik Dave, Noman Mohammed, Mohammad Al Hasan category:cs.LG cs.CR published:2015-12-22 summary:Over the last decade, proliferation of various online platforms and theirincreasing adoption by billions of users have heightened the privacy risk of auser enormously. In fact, security researchers have shown that sparse microdatacontaining information about online activities of a user although anonymous,can still be used to disclose the identity of the user by cross-referencing thedata with other data sources. To preserve the privacy of a user, in existingworks several methods (k-anonymity, l-diversity, differential privacy) areproposed that ensure a dataset which is meant to share or publish bears smallidentity disclosure risk. However, the majority of these methods modify thedata in isolation, without considering their utility in subsequent knowledgediscovery tasks, which makes these datasets less informative. In this work, weconsider labeled data that are generally used for classification, and proposetwo methods for feature selection considering two goals: first, on the reducedfeature set the data has small disclosure risk, and second, the utility of thedata is preserved for performing a classification task. Experimental results onvarious real-world datasets show that the method is effective and useful inpractice.
arxiv-16200-80 | Saliency Detection with Spaces of Background-based Distribution | http://arxiv.org/pdf/1603.05335v1.pdf | author:Tong Zhao, Lin Li, Xinghao Ding, Yue Huang, Delu Zeng category:cs.CV published:2016-03-17 summary:In this letter, an effective image saliency detection method is proposed byconstructing some novel spaces to model the background and redefine thedistance of the salient patches away from the background. Concretely, given thebackgroundness prior, eigendecomposition is utilized to create four spaces ofbackground-based distribution (SBD) to model the background, in which a moreappropriate metric (Mahalanobis distance) is quoted to delicately measure thesaliency of every image patch away from the background. After that, a coarsesaliency map is obtained by integrating the four adjusted Mahalanobis distancemaps, each of which is formed by the distances between all the patches andbackground in the corresponding SBD. To be more discriminative, the coarsesaliency map is further enhanced into the posterior probability map withinBayesian perspective. Finally, the final saliency map is generated by properlyrefining the posterior probability map with geodesic distance. Experimentalresults on two usual datasets show that the proposed method is effectivecompared with the state-of-the-art algorithms.
arxiv-16200-81 | Detecting events and key actors in multi-person videos | http://arxiv.org/pdf/1511.02917v2.pdf | author:Vignesh Ramanathan, Jonathan Huang, Sami Abu-El-Haija, Alexander Gorban, Kevin Murphy, Li Fei-Fei category:cs.CV cs.AI published:2015-11-09 summary:Multi-person event recognition is a challenging task, often with many peopleactive in the scene but only a small subset contributing to an actual event. Inthis paper, we propose a model which learns to detect events in such videoswhile automatically "attending" to the people responsible for the event. Ourmodel does not use explicit annotations regarding who or where those people areduring training and testing. In particular, we track people in videos and use arecurrent neural network (RNN) to represent the track features. We learntime-varying attention weights to combine these features at each time-instant.The attended features are then processed using another RNN for eventdetection/classification. Since most video datasets with multiple people arerestricted to a small number of videos, we also collected a new basketballdataset comprising 257 basketball games with 14K event annotationscorresponding to 11 event classes. Our model outperforms state-of-the-artmethods for both event classification and detection on this new dataset.Additionally, we show that the attention mechanism is able to consistentlylocalize the relevant players.
arxiv-16200-82 | On the Covariance of ICP-based Scan-matching Techniques | http://arxiv.org/pdf/1410.7632v3.pdf | author:Silvère Bonnabel, Martin Barczyk, François Goulette category:cs.CV cs.RO cs.SY published:2014-10-16 summary:This paper considers the problem of estimating the covariance ofroto-translations computed by the Iterative Closest Point (ICP) algorithm. Theproblem is relevant for localization of mobile robots and vehicles equippedwith depth-sensing cameras (e.g., Kinect) or Lidar (e.g., Velodyne). Theclosed-form formulas for covariance proposed in previous literature generallybuild upon the fact that the solution to ICP is obtained by minimizing a linearleast-squares problem. In this paper, we show this approach needs cautionbecause the rematching step of the algorithm is not explicitly accounted for,and applying it to the point-to-point version of ICP leads to completelyerroneous covariances. We then provide a formal mathematical proof why theapproach is valid in the point-to-plane version of ICP, which validates theintuition and experimental results of practitioners.
arxiv-16200-83 | Persistent Homology of Attractors For Action Recognition | http://arxiv.org/pdf/1603.05310v1.pdf | author:Vinay Venkataraman, Karthikeyan Natesan Ramamurthy, Pavan Turaga category:cs.CG cs.CV published:2016-03-16 summary:In this paper, we propose a novel framework for dynamical analysis of humanactions from 3D motion capture data using topological data analysis. We modelhuman actions using the topological features of the attractor of the dynamicalsystem. We reconstruct the phase-space of time series corresponding to actionsusing time-delay embedding, and compute the persistent homology of thephase-space reconstruction. In order to better represent the topologicalproperties of the phase-space, we incorporate the temporal adjacencyinformation when computing the homology groups. The persistence of thesehomology groups encoded using persistence diagrams are used as features for theactions. Our experiments with action recognition using these featuresdemonstrate that the proposed approach outperforms other baseline methods.
arxiv-16200-84 | Near-Optimal Stochastic Approximation for Online Principal Component Estimation | http://arxiv.org/pdf/1603.05305v1.pdf | author:Chris J. Li, Mengdi Wang, Han Liu, Tong Zhang category:math.OC stat.ML published:2016-03-16 summary:Principal component analysis (PCA) has been a prominent tool forhigh-dimensional data analysis. Online algorithms that estimate the principalcomponent by processing streaming data are of tremendous practical andtheoretical interests. Despite its rich applications, theoretical convergenceanalysis remains largely open. In this paper, we cast online PCA into astochastic nonconvex optimization problem, and we analyze the online PCAalgorithm as a stochastic approximation iteration. The stochastic approximationiteration processes data points incrementally and maintains a running estimateof the principal component. We prove for the first time a nearly optimalconvergence rate result for the online PCA algorithm. We show that thefinite-sample error closely matches the minimax information lower bound. Inaddition, we characterize the convergence process using ordinary and stochasticdifferential equation approximations.
arxiv-16200-85 | Semantic Folding Theory And its Application in Semantic Fingerprinting | http://arxiv.org/pdf/1511.08855v2.pdf | author:Francisco De Sousa Webber category:cs.AI cs.CL q-bio.NC published:2015-11-28 summary:Human language is recognized as a very complex domain since decades. Nocomputer system has been able to reach human levels of performance so far. Theonly known computational system capable of proper language processing is thehuman brain. While we gather more and more data about the brain, itsfundamental computational processes still remain obscure. The lack of a soundcomputational brain theory also prevents the fundamental understanding ofNatural Language Processing. As always when science lacks a theoreticalfoundation, statistical modeling is applied to accommodate as many sampledreal-world data as possible. An unsolved fundamental issue is the actualrepresentation of language (data) within the brain, denoted as theRepresentational Problem. Starting with Jeff Hawkins' Hierarchical TemporalMemory (HTM) theory, a consistent computational theory of the human cortex, wehave developed a corresponding theory of language data representation: TheSemantic Folding Theory. The process of encoding words, by using a topographicsemantic space as distributional reference frame into a sparse binaryrepresentational vector is called Semantic Folding and is the central topic ofthis document. Semantic Folding describes a method of converting language fromits symbolic representation (text) into an explicit, semantically groundedrepresentation that can be generically processed by Hawkins' HTM networks. Asit turned out, this change in representation, by itself, can solve many complexNLP problems by applying Boolean operators and a generic similarity functionlike the Euclidian Distance. Many practical problems of statistical NLPsystems, like the high cost of computation, the fundamental incongruity ofprecision and recall , the complex tuning procedures etc., can be elegantlyovercome by applying Semantic Folding.
arxiv-16200-86 | Image Labeling by Assignment | http://arxiv.org/pdf/1603.05285v1.pdf | author:Freddie Åström, Stefania Petra, Bernhard Schmitzer, Christoph Schnörr category:cs.CV math.OC published:2016-03-16 summary:We introduce a novel geometric approach to the image labeling problem.Abstracting from specific labeling applications, a general objective functionis defined on a manifold of stochastic matrices, whose elements assign priordata that are given in any metric space, to observed image measurements. Thecorresponding Riemannian gradient flow entails a set of replicator equations,one for each data point, that are spatially coupled by geometric averaging onthe manifold. Starting from uniform assignments at the barycenter as naturalinitialization, the flow terminates at some global maximum, each of whichcorresponds to an image labeling that uniquely assigns the prior data. Ourgeometric variational approach constitutes a smooth non-convex innerapproximation of the general image labeling problem, implemented with sparseinterior-point numerics in terms of parallel multiplicative updates thatconverge efficiently.
arxiv-16200-87 | PERCH: Perception via Search for Multi-Object Recognition and Localization | http://arxiv.org/pdf/1510.05613v2.pdf | author:Venkatraman Narayanan, Maxim Likhachev category:cs.CV cs.AI cs.RO published:2015-10-19 summary:In many robotic domains such as flexible automated manufacturing or personalassistance, a fundamental perception task is that of identifying and localizingobjects whose 3D models are known. Canonical approaches to this problem includediscriminative methods that find correspondences between feature descriptorscomputed over the model and observed data. While these methods have beenemployed successfully, they can be unreliable when the feature descriptors failto capture variations in observed data; a classic cause being occlusion. As astep towards deliberative reasoning, we present PERCH: PErception via SeaRCH,an algorithm that seeks to find the best explanation of the observed sensordata by hypothesizing possible scenes in a generative fashion. Ourcontributions are: i) formulating the multi-object recognition and localizationtask as an optimization problem over the space of hypothesized scenes, ii)exploiting structure in the optimization to cast it as a combinatorial searchproblem on what we call the Monotone Scene Generation Tree, and iii) leveragingparallelization and recent advances in multi-heuristic search in makingcombinatorial search tractable. We prove that our system can guaranteedlyproduce the best explanation of the scene under the chosen cost function, andvalidate our claims on real world RGB-D test data. Our experimental resultsshow that we can identify and localize objects under heavy occlusion--caseswhere state-of-the-art methods struggle.
arxiv-16200-88 | Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units | http://arxiv.org/pdf/1603.05201v1.pdf | author:Wenling Shang, Kihyuk Sohn, Diogo Almeida, Honglak Lee category:cs.LG cs.CV published:2016-03-16 summary:Recently, convolutional neural networks (CNNs) have been used as a powerfultool to solve many problems of machine learning and computer vision. In thispaper, we aim to provide insight on the property of convolutional neuralnetworks, as well as a generic method to improve the performance of many CNNarchitectures. Specifically, we first examine existing CNN models and observean intriguing property that the filters in the lower layers form pairs (i.e.,filters with opposite phase). Inspired by our observation, we propose a novel,simple yet effective activation scheme called concatenated ReLU (CRelu) andtheoretically analyze its reconstruction property in CNNs. We integrate CReluinto several state-of-the-art CNN architectures and demonstrate improvement intheir recognition performance on CIFAR-10/100 and ImageNet datasets with fewertrainable parameters. Our results suggest that better understanding of theproperties of CNNs can lead to significant performance improvement with asimple modification.
arxiv-16200-89 | Distributed Inexact Damped Newton Method: Data Partitioning and Load-Balancing | http://arxiv.org/pdf/1603.05191v1.pdf | author:Chenxin Ma, Martin Takáč category:cs.LG math.OC published:2016-03-16 summary:In this paper we study inexact dumped Newton method implemented in adistributed environment. We start with an original DiSCO algorithm[Communication-Efficient Distributed Optimization of Self-Concordant EmpiricalLoss, Yuchen Zhang and Lin Xiao, 2015]. We will show that this algorithm maynot scale well and propose an algorithmic modifications which will lead to lesscommunications, better load-balancing and more efficient computation. Weperform numerical experiments with an regularized empirical loss minimizationinstance described by a 273GB dataset.
arxiv-16200-90 | Applying Artifical Neural Networks To Predict Nominal Vehicle Performance | http://arxiv.org/pdf/1603.05189v1.pdf | author:Adam J. Last category:cs.NE cs.SY published:2016-03-16 summary:This paper investigates the use of artificial neural networks (ANNs) toreplace traditional algorithms and manual review for identifying anomalies invehicle run data. The specific data used for this study is from underseavehicle qualification tests. Such data is highly non-linear, thereforetraditional algorithms are not adequate and manual review is time consuming. Byusing ANNs to predict nominal vehicle performance based solely on informationavailable pre-run, vehicle deviation from expected performance can beautomatically identified in the post-run data. Such capability is only nowbecoming available due to the rapid increase in understanding of ANN frameworkand available computing power in the past decade. The ANN trained for thepurpose of this investigation is relatively simple, to keep the computingrequirements within the parameters of a modern desktop PC. This ANN showedpotential in predicting vehicle performance, particularly during transientevents within the run data. However, there were also several performance cases,such as steady state operation and cases which did not have sufficient trainingdata, where the ANN showed deficiencies. It is expected that as computationalpower becomes more readily available, ANN understanding matures, and moretraining data is acquired from real world tests, the performance predictions ofthe ANN will surpass traditional algorithms and manual human review.
arxiv-16200-91 | TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems | http://arxiv.org/pdf/1603.04467v2.pdf | author:Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, Xiaoqiang Zheng category:cs.DC cs.LG published:2016-03-14 summary:TensorFlow is an interface for expressing machine learning algorithms, and animplementation for executing such algorithms. A computation expressed usingTensorFlow can be executed with little or no change on a wide variety ofheterogeneous systems, ranging from mobile devices such as phones and tabletsup to large-scale distributed systems of hundreds of machines and thousands ofcomputational devices such as GPU cards. The system is flexible and can be usedto express a wide variety of algorithms, including training and inferencealgorithms for deep neural network models, and it has been used for conductingresearch and for deploying machine learning systems into production across morethan a dozen areas of computer science and other fields, including speechrecognition, computer vision, robotics, information retrieval, natural languageprocessing, geographic information extraction, and computational drugdiscovery. This paper describes the TensorFlow interface and an implementationof that interface that we have built at Google. The TensorFlow API and areference implementation were released as an open-source package under theApache 2.0 license in November, 2015 and are available at www.tensorflow.org.
arxiv-16200-92 | 2D Discrete Fourier Transform with Simultaneous Edge Artifact Removal for Real-Time Applications | http://arxiv.org/pdf/1603.05154v1.pdf | author:Faisal Mahmood, Märt Toots, Lars-Göran Öfverstedt, Ulf Skoglund category:cs.CV cs.AR published:2016-03-16 summary:Two-Dimensional (2D) Discrete Fourier Transform (DFT) is a basic andcomputationally intensive algorithm, with a vast variety of applications. 2Dimages are, in general, non-periodic, but are assumed to be periodic whilecalculating their DFTs. This leads to cross-shaped artifacts in the frequencydomain due to spectral leakage. These artifacts can have critical consequencesif the DFTs are being used for further processing. In this paper we present anovel FPGA-based design to calculate high-throughput 2D DFTs with simultaneousedge artifact removal. Standard approaches for removing these artifacts usingapodization functions or mirroring, either involve removing criticalfrequencies or a surge in computation by increasing image size. We use aperiodic-plus-smooth decomposition based artifact removal algorithm optimizedfor FPGA implementation, while still achieving real-time ($\ge$23 frames persecond) performance for a 512$\times$512 size image stream. Our optimizationapproach leads to a significant decrease in external memory utilization therebyavoiding memory conflicts and simplifies the design. We have tested our designon a PXIe based Xilinx Kintex 7 FPGA system communicating with a host PC whichgives us the advantage to further expand the design for industrialapplications.
arxiv-16200-93 | Feature Selection as a Multiagent Coordination Problem | http://arxiv.org/pdf/1603.05152v1.pdf | author:Kleanthis Malialis, Jun Wang, Gary Brooks, George Frangou category:cs.LG stat.ML published:2016-03-16 summary:Datasets with hundreds to tens of thousands features is the new norm. Featureselection constitutes a central problem in machine learning, where the aim isto derive a representative set of features from which to construct aclassification (or prediction) model for a specific task. Our experimentalstudy involves microarray gene expression datasets, these are high-dimensionaland noisy datasets that contain genetic data typically used for distinguishingbetween benign or malicious tissues or classifying different types of cancer.In this paper, we formulate feature selection as a multiagent coordinationproblem and propose a novel feature selection method using multiagentreinforcement learning. The central idea of the proposed approach is to"assign" a reinforcement learning agent to each feature where each agent learnsto control a single feature, we refer to this approach as MARL. Applying thisto microarray datasets creates an enormous multiagent coordination problembetween thousands of learning agents. To address the scalability challenge weapply a form of reward shaping called CLEAN rewards. We compare in total ninefeature selection methods, including state-of-the-art methods, and show thatthe proposed method using CLEAN rewards can significantly scale-up, thusoutperforming the rest of learning-based methods. We further show that a hybridvariant of MARL achieves the best overall performance.
arxiv-16200-94 | Suppressing the Unusual: towards Robust CNNs using Symmetric Activation Functions | http://arxiv.org/pdf/1603.05145v1.pdf | author:Qiyang Zhao, Lewis D Griffin category:cs.CV cs.AI cs.LG published:2016-03-16 summary:Many deep Convolutional Neural Networks (CNN) make incorrect predictions onadversarial samples obtained by imperceptible perturbations of clean samples.We hypothesize that this is caused by a failure to suppress unusual signalswithin network layers. As remedy we propose the use of Symmetric ActivationFunctions (SAF) in non-linear signal transducer units. These units suppresssignals of exceptional magnitude. We prove that SAF networks can performclassification tasks to arbitrary precision in a simplified situation. Inpractice, rather than use SAFs alone, we add them into CNNs to improve theirrobustness. The modified CNNs can be easily trained using popular strategieswith the moderate training load. Our experiments on MNIST and CIFAR-10 showthat the modified CNNs perform similarly to plain ones on clean samples, andare remarkably more robust against adversarial and nonsense samples.
arxiv-16200-95 | Recurrent Dropout without Memory Loss | http://arxiv.org/pdf/1603.05118v1.pdf | author:Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth category:cs.CL published:2016-03-16 summary:This paper presents a novel approach to recurrent neural network (RNN)regularization. Differently from the widely adopted dropout method, which isapplied to forward connections of feed-forward architectures or RNNs, wepropose to drop neurons directly in recurrent connections in a way that doesnot cause loss of long-term memory. Our approach is as easy to implement andapply as the regular feed-forward dropout and we demonstrate its effectivenessfor the most popular recurrent networks: vanilla RNNs, Long Short-Term Memory(LSTM) and Gated Recurrent Unit (GRU) networks. Our experiments on three NLPbenchmarks show consistent improvements even when combined with conventionalfeed-forward dropout.
arxiv-16200-96 | On semidefinite relaxations for the block model | http://arxiv.org/pdf/1406.5647v3.pdf | author:Arash A. Amini, Elizaveta Levina category:cs.LG cs.SI stat.ML published:2014-06-21 summary:The stochastic block model (SBM) is a popular tool for community detection innetworks, but fitting it by maximum likelihood (MLE) involves a computationallyinfeasible optimization problem. We propose a new semidefinite programming(SDP) solution to the problem of fitting the SBM, derived as a relaxation ofthe MLE. We put ours and previously proposed SDPs in a unified framework, asrelaxations of the MLE over various sub-classes of the SBM, revealing aconnection to sparse PCA. Our main relaxation, which we call SDP-1, is tighterthan other recently proposed SDP relaxations, and thus previously establishedtheoretical guarantees carry over. However, we show that SDP-1 exactly recoverstrue communities over a wider class of SBMs than those covered by currentresults. In particular, the assumption of strong assortativity of the SBM,implicit in consistency conditions for previously proposed SDPs, can be relaxedto weak assortativity for our approach, thus significantly broadening the classof SBMs covered by the consistency results. We also show that strongassortativity is indeed a necessary condition for exact recovery for previouslyproposed SDP approaches and not an artifact of the proofs. Our analysis of SDPsis based on primal-dual witness constructions, which provides some insight intothe nature of the solutions of various SDPs. We show how to combine featuresfrom SDP-1 and already available SDPs to achieve the most flexibility in termsof both assortativity and block-size constraints, as our relaxation has thetendency to produce communities of similar sizes. This tendency makes it theideal tool for fitting network histograms, a method gaining popularity in thegraphon estimation literature, as we illustrate on an example of a socialnetworks of dolphins. We also provide empirical evidence that SDPs outperformspectral methods for fitting SBMs with a large number of blocks.
arxiv-16200-97 | One-Shot Generalization in Deep Generative Models | http://arxiv.org/pdf/1603.05106v1.pdf | author:Danilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka, Karol Gregor, Daan Wierstra category:stat.ML cs.AI cs.LG published:2016-03-16 summary:Humans have an impressive ability to reason about new concepts andexperiences from just a single example. In particular, humans have an abilityfor one-shot generalization: an ability to encounter a new concept, understandits structure, and then be able to generate compelling alternative variationsof the concept. We develop machine learning systems with this importantcapacity by developing new deep generative models, models that combine therepresentational power of deep learning with the inferential power of Bayesianreasoning. We develop a class of sequential generative models that are built onthe principles of feedback and attention. These two characteristics lead togenerative models that are among the state-of-the art in density estimation andimage generation. We demonstrate the one-shot generalization ability of ourmodels using three tasks: unconditional sampling, generating new exemplars of agiven concept, and generating new exemplars of a family of concepts. In allcases our models are able to generate compelling and diverse samples---havingseen new examples just once---providing an important class of general-purposemodels for one-shot machine learning.
arxiv-16200-98 | Short-term time series prediction using Hilbert space embeddings of autoregressive processes | http://arxiv.org/pdf/1603.05060v1.pdf | author:Edgar A. Valencia, Mauricio A. Álvarez category:stat.ML published:2016-03-16 summary:Linear autoregressive models serve as basic representations of discrete timestochastic processes. Different attempts have been made to provide non-linearversions of the basic autoregressive process, including different versionsbased on kernel methods. Motivated by the powerful framework of Hilbert spaceembeddings of distributions, in this paper we apply this methodology for thekernel embedding of an autoregressive process of order $p$. By doing so, weprovide a non-linear version of an autoregressive process, that shows increasedperformance over the linear model in highly complex time series. We use themethod proposed for one-step ahead forecasting of different time-series, andcompare its performance against other non-linear methods.
arxiv-16200-99 | Non-linear Dimensionality Regularizer for Solving Inverse Problems | http://arxiv.org/pdf/1603.05015v1.pdf | author:Ravi Garg, Anders Eriksson, Ian Reid category:cs.CV published:2016-03-16 summary:Consider an ill-posed inverse problem of estimating causal factors fromobservations, one of which is known to lie near some (un- known)low-dimensional, non-linear manifold expressed by a predefined Mercer-kernel.Solving this problem requires simultaneous estimation of these factors andlearning the low-dimensional representation for them. In this work, weintroduce a novel non-linear dimensionality regulariza- tion technique forsolving such problems without pre-training. We re-formulate Kernel-PCA as anenergy minimization problem in which low dimensionality constraints areintroduced as regularization terms in the energy. To the best of our knowledge,ours is the first at- tempt to create a dimensionality regularizer in the KPCAframework. Our approach relies on robustly penalizing the rank of the recoveredfac- tors directly in the implicit feature space to create theirlow-dimensional approximations in closed form. Our approach performs robustKPCA in the presence of missing data and noise. We demonstrate state-of-the-artresults on predicting missing entries in the standard oil flow dataset.Additionally, we evaluate our method on the challenging problem of Non-RigidStructure from Motion and our approach delivers promising results on CMU mocapdataset despite the presence of significant occlusions and noise.
arxiv-16200-100 | Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue | http://arxiv.org/pdf/1603.04992v1.pdf | author:Ravi Garg, Vijay Kumar BG, Ian Reid category:cs.CV published:2016-03-16 summary:A significant weakness of most current deep Convolutional Neural Networks isthe need to train them using vast amounts of manu- ally labelled data. In thiswork we propose a unsupervised framework to learn a deep convolutional neuralnetwork for single view depth predic- tion, without requiring a pre-trainingstage or annotated ground truth depths. We achieve this by training the networkin a manner analogous to an autoencoder. At training time we consider a pair ofimages, source and target, with small, known camera motion between the two suchas a stereo pair. We train the convolutional encoder for the task of predictingthe depth map for the source image. To do so, we explicitly generate an inversewarp of the target image using the predicted depth and known inter-viewdisplacement, to reconstruct the source image; the photomet- ric error in thereconstruction is the reconstruction loss for the encoder. The acquisition ofthis training data is considerably simpler than for equivalent systems,requiring no manual annotation, nor calibration of depth sensor to camera. Weshow that our network trained on less than half of the KITTI dataset (withoutany further augmentation) gives com- parable performance to that of the stateof art supervised methods for single view depth estimation.
arxiv-16200-101 | Scaled stochastic gradient descent for low-rank matrix completion | http://arxiv.org/pdf/1603.04989v1.pdf | author:Bamdev Mishra, Rodolphe Sepulchre category:cs.LG math.OC published:2016-03-16 summary:The paper looks at a scaled variant of the stochastic gradient descentalgorithm for the matrix completion problem. Specifically, we propose a novelmatrix-scaling of the partial derivatives that acts as an efficientpreconditioning for the standard stochastic gradient descent algorithm. Thisproposed matrix-scaling provides a trade-off between local and global secondorder information. It also resolves the issue of scale invariance that existsin matrix factorization models. The overall computational complexity is linearwith the number of known entries, thereby extending to a large-scale setup.Numerical comparisons show that the proposed algorithm competes favorably withstate-of-the-art algorithms on various different benchmarks.
arxiv-16200-102 | Spherical Conformal Parameterization of Genus-0 Point Clouds for Meshing | http://arxiv.org/pdf/1508.07569v3.pdf | author:Gary Pui-Tung Choi, Kin Tat Ho, Lok Ming Lui category:cs.CG cs.CV cs.GR math.DG published:2015-08-30 summary:Point cloud is the most fundamental representation of 3D geometric objects.Analyzing and processing point cloud surfaces is important in computer graphicsand computer vision. However, most of the existing algorithms for surfaceanalysis require connectivity information. Therefore, it is desirable todevelop a mesh structure on point clouds. This task can be simplified with theaid of a parameterization. In particular, conformal parameterizations areadvantageous in preserving the geometric information of the point cloud data.In this paper, we extend a state-of-the-art spherical conformalparameterization algorithm for genus-0 closed meshes to the case of pointclouds, using an improved approximation of the Laplace-Beltrami operator ondata points. Then, we propose an iterative scheme called the North-Southreiteration for achieving a spherical conformal parameterization. A balancingscheme is introduced to enhance the distribution of the sphericalparameterization. High quality triangulations and quadrangulations can then bebuilt on the point clouds with the aid of the parameterizations. Also, themeshes generated are guaranteed to be genus-0 closed meshes. Moreover, usingour proposed spherical conformal parameterization, multilevel representationsof point clouds can be easily constructed. Experimental results demonstrate theeffectiveness of our proposed framework.
arxiv-16200-103 | Regret-optimal Strategies for Playing Repeated Games with Discounted Losses | http://arxiv.org/pdf/1603.04981v1.pdf | author:Vijay Kamble, Patrick Loiseau, Jean Walrand category:cs.GT cs.DS cs.LG stat.ML published:2016-03-16 summary:The regret-minimization paradigm has emerged as a powerful technique fordesigning algorithms for online decision-making in adversarial environments.But so far, designing exact minmax-optimal algorithms for minimizing theworst-case regret has proven to be a difficult task in general, with only a fewknown results in specific settings. In this paper, we present a novelset-valued dynamic programming approach for designing such exact regret-optimalpolicies for playing repeated games with discounted losses. Our approach first draws the connection between regret minimization, anddetermining minimal achievable guarantees in repeated games with vector-valuedlosses. We then characterize the set of these minimal guarantees as the fixedpoint of a dynamic programming operator defined on the space of Paretofrontiers of convex and compact sets. This approach simultaneously results inthe characterization of the optimal strategies that achieve these minimalguarantees, and hence of regret-optimal strategies in the original repeatedgame. As an illustration of our approach, we design a simple near-optimalstrategy for prediction using expert advice for the case of 2 experts.
arxiv-16200-104 | Online Optimization in Dynamic Environments: Improved Regret Rates for Strongly Convex Problems | http://arxiv.org/pdf/1603.04954v1.pdf | author:Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, Alejandro Ribeiro category:cs.LG math.OC published:2016-03-16 summary:In this paper, we address tracking of a time-varying parameter with unknowndynamics. We formalize the problem as an instance of online optimization in adynamic setting. Using online gradient descent, we propose a method thatsequentially predicts the value of the parameter and in turn suffers a loss.The objective is to minimize the accumulation of losses over the time horizon,a notion that is termed dynamic regret. While existing methods focus on convexloss functions, we consider strongly convex functions so as to provide betterguarantees of performance. We derive a regret bound that captures thepath-length of the time-varying parameter, defined in terms of the distancebetween its consecutive values. In other words, the bound represents thenatural connection of tracking quality to the rate of change of the parameter.We provide numerical experiments to complement our theoretical findings.
arxiv-16200-105 | Revisiting Batch Normalization For Practical Domain Adaptation | http://arxiv.org/pdf/1603.04779v2.pdf | author:Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, Xiaodi Hou category:cs.CV cs.LG published:2016-03-15 summary:Deep neural networks (DNN) have shown unprecedented success in variouscomputer vision applications such as image classification and object detection.However, it is still a common (yet inconvenient) practice to prepare at leasttens of thousands of labeled image to fine-tune a network on every task beforethe model is ready to use. Recent study shows that a DNN has strong dependencytowards the training dataset, and the learned features cannot be easilytransferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive BatchNormalization(AdaBN), to increase the generalization ability of a DNN. Ourapproach is based on the well-known Batch Normalization technique which hasbecome a standard component in modern deep learning. In contrary to other deeplearning domain adaptation methods, our method does not require additionalcomponents, and is parameter-free. It archives state-of-the-art performancedespite its surprising simplicity. Furthermore, we demonstrate that our methodis complementary with other existing methods. Combining AdaBN with existingdomain adaptation treatments may further improve model performance.
arxiv-16200-106 | Audio Classical Composer Identification by Deep Neural Network | http://arxiv.org/pdf/1301.3195v7.pdf | author:Zhen Hu, Kun Fu, Changshui Zhang category:cs.NE cs.IR published:2013-01-15 summary:Audio Classical Composer Identification (ACC) is an important problem inMusic Information Retrieval (MIR) which aims at identifying the composer foraudio classical music clips. The famous annual competition, Music InformationRetrieval Evaluation eXchange (MIREX), also takes it as one of the fourtraining&testing tasks. We built a hybrid model based on Deep Belief Network(DBN) and Stacked Denoising Autoencoder (SDA) to identify the composer fromaudio signal. As a matter of copyright, sponsors of MIREX cannot publish theirdata set. We built a comparable data set to test our model. We got an accuracyof 76.26% in our data set which is better than some pure models and shallowmodels. We think our method is promising even though we test it in a differentdata set, since our data set is comparable to that in MIREX by size. We alsofound that samples from different classes become farther away from each otherwhen transformed by more layers in our model.
arxiv-16200-107 | On the Complexity of One-class SVM for Multiple Instance Learning | http://arxiv.org/pdf/1603.04947v1.pdf | author:Zhen Hu, Zhuyin Xue category:cs.LG published:2016-03-16 summary:In traditional multiple instance learning (MIL), both positive and negativebags are required to learn a prediction function. However, a high human cost isneeded to know the label of each bag---positive or negative. Only positive bagscontain our focus (positive instances) while negative bags consist of noise orbackground (negative instances). So we do not expect to spend too much to labelthe negative bags. Contrary to our expectation, nearly all existing MIL methodsrequire enough negative bags besides positive ones. In this paper we propose analgorithm called "Positive Multiple Instance" (PMI), which learns a classifiergiven only a set of positive bags. So the annotation of negative bags becomesunnecessary in our method. PMI is constructed based on the assumption that theunknown positive instances in positive bags be similar each other andconstitute one compact cluster in feature space and the negative instanceslocate outside this cluster. The experimental results demonstrate that PMIachieves the performances close to or a little worse than those of thetraditional MIL algorithms on benchmark and real data sets. However, the numberof training bags in PMI is reduced significantly compared with traditional MILalgorithms.
arxiv-16200-108 | Deep Fully-Connected Networks for Video Compressive Sensing | http://arxiv.org/pdf/1603.04930v1.pdf | author:Michael Iliadis, Leonidas Spinoulas, Aggelos K. Katsaggelos category:cs.CV cs.LG cs.MM published:2016-03-16 summary:In this work we present a deep learning framework for video compressivesensing. The proposed formulation enables recovery of video frames in a fewseconds at significantly improved reconstruction quality compared to previousapproaches. Our investigation starts by learning a linear mapping between videosequences and corresponding measured frames which turns out to providepromising results. We then extend the linear formulation to deepfully-connected networks and explore the performance gains using deeperarchitectures. Our analysis is always driven by the applicability of theproposed framework on existing compressive video architectures. Extensivesimulations on several video sequences document the superiority of our approachboth quantitatively and qualitatively. Finally, our analysis offers insightsinto understanding how dataset sizes and number of layers affect reconstructionperformance while raising a few points for future investigation.
arxiv-16200-109 | Data Clustering and Graph Partitioning via Simulated Mixing | http://arxiv.org/pdf/1603.04918v1.pdf | author:Shahzad Bhatti, Carolyn Beck, Angelia Nedic category:cs.LG stat.ML published:2016-03-15 summary:Spectral clustering approaches have led to well-accepted algorithms forfinding accurate clusters in a given dataset. However, their application tolarge-scale datasets has been hindered by computational complexity ofeigenvalue decompositions. Several algorithms have been proposed in the recentpast to accelerate spectral clustering, however they compromise on the accuracyof the spectral clustering to achieve faster speed. In this paper, we propose anovel spectral clustering algorithm based on a mixing process on a graph.Unlike the existing spectral clustering algorithms, our algorithm does notrequire computing eigenvectors. Specifically, it finds the equivalent of alinear combination of eigenvectors of the normalized similarity matrix weightedwith corresponding eigenvalues. This linear combination is then used topartition the dataset into meaningful clusters. Simulations on real datasetsshow that partitioning datasets based on such linear combinations ofeigenvectors achieves better accuracy than standard spectral clustering methodsas the number of clusters increase. Our algorithm can easily be implemented ina distributed setting.
arxiv-16200-110 | First Person Action-Object Detection with EgoNet | http://arxiv.org/pdf/1603.04908v1.pdf | author:Gedas Bertasius, Hyun Soo Park, Stella X. Yu, Jianbo Shi category:cs.CV published:2016-03-15 summary:Objects afford visual sensation and motor actions. A first person camera,placed at the person's head, captures unscripted moments of our visualsensorimotor object interactions. Can a single first person image tell us aboutour momentary visual attention and motor action with objects, without a gazetracking device or tactile sensors? To study the holistic correlation of visualattention with motor action, we introduce the concept ofaction-objects---objects associated with seeing and touching actions, whichexhibit characteristic 3D spatial distance and orientation with respect to theperson. A predictive action-object model is designed to re-organize the spaceof interactions in terms of visual and tactile sensations, which is realized byour proposed EgoNet network. EgoNet is composed of two convolutional neuralnetworks: 1) Semantic Gaze Pathway that learns 2D appearance cues with firstperson coordinate embedding, and 2) 3D Spatial Pathway that focuses on 3D depthand height measurements relative to the person with brightness reflectanceattached. Retaining two distinct pathways enables effective learning from alimited number of examples, diversified prediction from complementary visualsignals, and flexible architecture that is functional with RGB image withoutdepth information. We show that our model correctly predicts action-objects ina first person image where we outperform the existing approaches acrossdifferent datasets.
arxiv-16200-111 | Turing learning: a metric-free approach to inferring behavior and its application to swarms | http://arxiv.org/pdf/1603.04904v1.pdf | author:Wei Li, Melvin Gauci, Roderich Gross category:stat.ML cs.LG cs.NE published:2016-03-15 summary:We propose Turing Learning, a novel system identification method forinferring behavior. Turing Learning simultaneously optimizes models andclassifiers. The classifiers are provided with data samples from both an agentand models under observation, and are rewarded for discriminating between them.Conversely, the models are rewarded for 'tricking' the classifiers intocategorizing them as the agent. Unlike other methods for system identification,Turing Learning does not require predefined metrics to quantify the differencebetween the agent and models. We present two case studies with swarms ofsimulated robots that show that Turing Learning outperforms a metric-basedsystem identification method in terms of model accuracy. The classifiersperform well collectively and could be used to detect abnormal behavior in theswarm. Moreover, we show that Turing Learning also successfully infers thebehavior of physical robot swarms. The results show that collective behaviorscan be directly inferred from motion trajectories of a single agent in theswarm, which may have significant implications for the study of animalcollectives.
arxiv-16200-112 | Bias Correction for Regularized Regression and its Application in Learning with Streaming Data | http://arxiv.org/pdf/1603.04882v1.pdf | author:Qiang Wu category:stat.ML cs.LG published:2016-03-15 summary:We propose an approach to reduce the bias of ridge regression andregularization kernel network. When applied to a single data set the newalgorithms have comparable learning performance with the original ones. Whenapplied to incremental learning with block wise streaming data the newalgorithms are more efficient due to bias reduction. Both theoreticalcharacterizations and simulation studies are used to verify the effectivenessof these new algorithms.
arxiv-16200-113 | Combining the Best of Convolutional Layers and Recurrent Layers: A Hybrid Network for Semantic Segmentation | http://arxiv.org/pdf/1603.04871v1.pdf | author:Zhicheng Yan, Hao Zhang, Yangqing Jia, Thomas Breuel, Yizhou Yu category:cs.CV published:2016-03-15 summary:State-of-the-art results of semantic segmentation are established by FullyConvolutional neural Networks (FCNs). FCNs rely on cascaded convolutional andpooling layers to gradually enlarge the receptive fields of neurons, resultingin an indirect way of modeling the distant contextual dependence. In this work,we advocate the use of spatially recurrent layers (i.e. ReNet layers) whichdirectly capture global contexts and lead to improved feature representations.We demonstrate the effectiveness of ReNet layers by building a Naive deep ReNet(N-ReNet), which achieves competitive performance on Stanford Backgrounddataset. Furthermore, we integrate ReNet layers with FCNs, and develop a novelHybrid deep ReNet (H-ReNet). It enjoys a few remarkable properties, includingfull-image receptive fields, end-to-end training, and efficient networkexecution. On the PASCAL VOC 2012 benchmark, the H-ReNet improves the resultsof state-of-the-art approaches Piecewise, CRFasRNN and DeepParsing by 3.6%,2.3% and 0.2%, respectively, and achieves the highest IoUs for 13 out of the 20object classes.
arxiv-16200-114 | Efficient Globally Optimal Point Cloud Alignment using Bayesian Nonparametric Mixtures | http://arxiv.org/pdf/1603.04868v1.pdf | author:Julian Straub, Trevor Campbell, Jonathan P. How, John W. Fisher III category:cs.CV published:2016-03-15 summary:Point cloud alignment is a common problem in computer vision and robotics,with applications ranging from object recognition to reconstruction. We proposea novel approach to the alignment problem that utilizes Bayesian nonparametricsto describe the point cloud and surface normal densities, and the branch andbound (BB) paradigm to recover the optimal relative transformation. BB relieson a novel, refinable, approximately-uniform tessellation of the rotation spaceusing 4D tetrahedra which leads to more efficient BB operation in comparison tothe common axis-angle tessellation. For this novel tessellation, we provideupper and lower objective function bounds, and prove convergence and optimalityof the BB approach under mild assumptions. Finally, we empirically demonstratethe efficiency of the proposed approach as well as its robustness to suboptimalreal-world conditions such as missing data and partial overlap.
arxiv-16200-115 | Ensemble of Deep Convolutional Neural Networks for Learning to Detect Retinal Vessels in Fundus Images | http://arxiv.org/pdf/1603.04833v1.pdf | author:Debapriya Maji, Anirban Santara, Pabitra Mitra, Debdoot Sheet category:cs.LG cs.CV stat.ML published:2016-03-15 summary:Vision impairment due to pathological damage of the retina can largely beprevented through periodic screening using fundus color imaging. However thechallenge with large scale screening is the inability to exhaustively detectfine blood vessels crucial to disease diagnosis. In this work we present acomputational imaging framework using deep and ensemble learning for reliabledetection of blood vessels in fundus color images. An ensemble of deepconvolutional neural networks is trained to segment vessel and non-vessel areasof a color fundus image. During inference, the responses of the individualConvNets of the ensemble are averaged to form the final segmentation. Inexperimental evaluation with the DRIVE database, we achieve the objective ofvessel detection with maximum average accuracy of 94.7\% and area under ROCcurve of 0.9283.
arxiv-16200-116 | Second Order Stochastic Optimization in Linear Time | http://arxiv.org/pdf/1602.03943v3.pdf | author:Naman Agarwal, Brian Bullins, Elad Hazan category:stat.ML cs.LG published:2016-02-12 summary:Stochastic optimization and, in particular, first-order stochastic methodsare a cornerstone of modern machine learning due to their extremely efficientper-iteration computational cost. Second-order methods, while able to providefaster per-iteration convergence, have been much less explored due to the highcost of computing the second-order information. In this paper we develop asecond-order stochastic method for optimization problems arising in machinelearning based on novel matrix randomization techniques that match theper-iteration cost of gradient descent, yet enjoy the linear-convergenceproperties of second-order optimization. We also consider the special case ofself-concordant functions where we show that a first order method can achievelinear convergence with guarantees independent of the condition number. Wedemonstrate significant speedups for training linear classifiers over severalconvex benchmarks.
arxiv-16200-117 | An optimal algorithm for bandit convex optimization | http://arxiv.org/pdf/1603.04350v2.pdf | author:Elad Hazan, Yuanzhi Li category:cs.LG cs.DS G.1.6 published:2016-03-14 summary:We consider the problem of online convex optimization against an arbitraryadversary with bandit feedback, known as bandit convex optimization. We givethe first $\tilde{O}(\sqrt{T})$-regret algorithm for this setting based on anovel application of the ellipsoid method to online learning. This bound isknown to be tight up to logarithmic factors. Our analysis introduces new toolsin discrete convex geometry.
arxiv-16200-118 | A Neural Approach to Blind Motion Deblurring | http://arxiv.org/pdf/1603.04771v1.pdf | author:Ayan Chakrabarti category:cs.CV published:2016-03-15 summary:We present a new method for blind motion deblurring that uses a neuralnetwork trained to compute estimates of sharp image patches from observationsthat are blurred by an unknown motion kernel. Instead of regressing directly topatch intensities, this network learns to predict the complex Fouriercoefficients of a deconvolution filter to be applied to the input patch forrestoration. For inference, we apply the network independently to alloverlapping patches in the observed image, and average its outputs to form aninitial estimate of the sharp image. We then explicitly estimate a singleglobal blur kernel by relating this estimate to the observed image, and finallyperform non-blind deconvolution with this kernel. Our method exhibits accuracyand robustness close to state-of-the-art iterative methods, while being muchfaster when parallelized on GPU hardware.
arxiv-16200-119 | Evaluating the word-expert approach for Named-Entity Disambiguation | http://arxiv.org/pdf/1603.04767v1.pdf | author:Angel X. Chang, Valentin I. Spitkovsky, Christopher D. Manning, Eneko Agirre category:cs.CL published:2016-03-15 summary:Named Entity Disambiguation (NED) is the task of linking a named-entitymention to an instance in a knowledge-base, typically Wikipedia. This task isclosely related to word-sense disambiguation (WSD), where the supervisedword-expert approach has prevailed. In this work we present the results of theword-expert approach to NED, where one classifier is built for each targetentity mention string. The resources necessary to build the system, adictionary and a set of training instances, have been automatically derivedfrom Wikipedia. We provide empirical evidence of the value of this approach, aswell as a study of the differences between WSD and NED, including ambiguity andsynonymy statistics.
arxiv-16200-120 | Topic Modeling Using Distributed Word Embeddings | http://arxiv.org/pdf/1603.04747v1.pdf | author:Ramandeep S Randhawa, Parag Jain, Gagan Madan category:cs.CL published:2016-03-15 summary:We propose a new algorithm for topic modeling, Vec2Topic, that identifies themain topics in a corpus using semantic information captured viahigh-dimensional distributed word embeddings. Our technique is unsupervised andgenerates a list of topics ranked with respect to importance. We find that itworks better than existing topic modeling techniques such as Latent DirichletAllocation for identifying key topics in user-generated content, such asemails, chats, etc., where topics are diffused across the corpus. We also findthat Vec2Topic works equally well for non-user generated content, such aspapers, reports, etc., and for small corpora such as a single-document.
arxiv-16200-121 | Online Learning to Sample | http://arxiv.org/pdf/1506.09016v2.pdf | author:Guillaume Bouchard, Théo Trouillon, Julien Perez, Adrien Gaidon category:cs.LG cs.CV cs.NA math.OC stat.ML published:2015-06-30 summary:Stochastic Gradient Descent (SGD) is one of the most widely used techniquesfor online optimization in machine learning. In this work, we accelerate SGD byadaptively learning how to sample the most useful training examples at eachtime step. First, we show that SGD can be used to learn the best possiblesampling distribution of an importance sampling estimator. Second, we show thatthe sampling distribution of an SGD algorithm can be estimated online byincrementally minimizing the variance of the gradient. The resulting algorithm- called Adaptive Weighted SGD (AW-SGD) - maintains a set of parameters tooptimize, as well as a set of parameters to sample learning examples. We showthat AWSGD yields faster convergence in three different applications: (i) imageclassification with deep features, where the sampling of images depends ontheir labels, (ii) matrix factorization, where rows and columns are not sampleduniformly, and (iii) reinforcement learning, where the optimized andexploration policies are estimated at the same time, where our approachcorresponds to an off-policy gradient algorithm.
arxiv-16200-122 | Modeling Time Series Similarity with Siamese Recurrent Networks | http://arxiv.org/pdf/1603.04713v1.pdf | author:Wenjie Pei, David M. J. Tax, Laurens van der Maaten category:cs.CV published:2016-03-15 summary:Traditional techniques for measuring similarities between time series arebased on handcrafted similarity measures, whereas more recent learning-basedapproaches cannot exploit external supervision. We combine ideas fromtime-series modeling and metric learning, and study siamese recurrent networks(SRNs) that minimize a classification loss to learn a good similarity measurebetween time series. Specifically, our approach learns a vectorialrepresentation for each time series in such a way that similar time series aremodeled by similar representations, and dissimilar time series by dissimilarrepresentations. Because it is a similarity prediction models, SRNs areparticularly well-suited to challenging scenarios such as signaturerecognition, in which each person is a separate class and very few examples perclass are available. We demonstrate the potential merits of SRNs inwithin-domain and out-of-domain classification experiments and in one-shotlearning experiments on tasks such as signature, voice, and sign languagerecognition.
arxiv-16200-123 | Evolving Boolean Regulatory Networks with Variable Gene Expression Times | http://arxiv.org/pdf/1603.01185v2.pdf | author:Larry Bull category:q-bio.BM cs.NE q-bio.MN published:2016-03-02 summary:The time taken for gene expression varies not least because proteins vary inlength considerably. This paper uses an abstract, tuneable Boolean regulatorynetwork model to explore gene expression time variation. In particular, it isshown how non-uniform expression times can emerge under certain conditionsthrough simulated evolution. That is, gene expression time variance appearsbeneficial in the shaping of the dynamical behaviour of the regulatory networkwithout explicit consideration of protein function.
arxiv-16200-124 | Zero-sum repeated games: Counterexamples to the existence of the asymptotic value and the conjecture $\operatorname{maxmin}=\operatorname{lim}v_n$ | http://arxiv.org/pdf/1305.4778v4.pdf | author:Bruno Ziliotto category:math.OC cs.LG published:2013-05-21 summary:Mertens [In Proceedings of the International Congress of Mathematicians(Berkeley, Calif., 1986) (1987) 1528-1577 Amer. Math. Soc.] proposed twogeneral conjectures about repeated games: the first one is that, in anytwo-person zero-sum repeated game, the asymptotic value exists, and the secondone is that, when Player 1 is more informed than Player 2, in the long runPlayer 1 is able to guarantee the asymptotic value. We disprove these twolong-standing conjectures by providing an example of a zero-sum repeated gamewith public signals and perfect observation of the actions, where the value ofthe $\lambda$-discounted game does not converge when $\lambda$ goes to 0. Theaforementioned example involves seven states, two actions and two signals foreach player. Remarkably, players observe the payoffs, and play in turn.
arxiv-16200-125 | Accelerating a hybrid continuum-atomistic fluidic model with on-the-fly machine learning | http://arxiv.org/pdf/1603.04628v1.pdf | author:David Stephenson, James R Kermode, Duncan A Lockerby category:stat.ML published:2016-03-15 summary:We present a hybrid continuum-atomistic scheme which combines moleculardynamics (MD) simulations with on-the-fly machine learning techniques for theaccurate and efficient prediction of multiscale fluidic systems. By using aGaussian process as a surrogate model for the computationally expensive MDsimulations, we use Bayesian inference to predict the system behaviour at theatomistic scale, purely by consideration of the macroscopic inputs and outputs.Whenever the uncertainty of this prediction is greater than a predeterminedacceptable threshold, a new MD simulation is performed to continually augmentthe database, which is never required to be complete. This provides asubstantial enhancement to the current generation of hybrid methods, whichoften require many similar atomistic simulations to be performed, discardinginformation after it is used once. We apply our hybrid scheme to nano-confined unsteady flow through ahigh-aspect-ratio converging-diverging channel, and make comparisons betweenthe new scheme and full MD simulations for a range of uncertainty thresholdsand initial databases. For low thresholds, our hybrid solution is highlyaccurate\,---\,within the thermal noise of a full MD simulation. As theuncertainty threshold is raised, the accuracy of our scheme decreases and thecomputational speed-up increases (relative to a full MD simulation), enablingthe compromise between precision and efficiency to be tuned. The speed-up ofour hybrid solution ranges from an order of magnitude, with no initialdatabase, to cases where an extensive initial database ensures no new MDsimulations are required.
arxiv-16200-126 | Image Co-localization by Mimicking a Good Detector's Confidence Score Distribution | http://arxiv.org/pdf/1603.04619v1.pdf | author:Yao Li, Linqiao Liu, Chunhua Shen, Anton van den Hengel category:cs.CV published:2016-03-15 summary:Given a set of images containing objects from the same category, the task ofimage co-localization is to identify and localize each instance. This papershows that this problem can be solved by a simple but intriguing idea, that is,a common object detector can be learnt by making its detection confidencescores distributed like those of a strongly supervised detector. Morespecifically, we observe that given a set of object proposals extracted from animage that contains the object of interest, an accurate strongly supervisedobject detector should give high scores to only a small minority of proposals,and low scores to most of them. Thus, we devise an entropy-based objectivefunction to enforce the above property when learning the common objectdetector. Once the detector is learnt, we resort to a segmentation approach torefine the localization. We show that despite its simplicity, our approachoutperforms state-of-the-art methods.
arxiv-16200-127 | Scalable Image Retrieval by Sparse Product Quantization | http://arxiv.org/pdf/1603.04614v1.pdf | author:Qingqun Ning, Jianke Zhu, Zhiyuan Zhong, Steven C. H. Hoi, Chun Chen category:cs.CV published:2016-03-15 summary:Fast Approximate Nearest Neighbor (ANN) search technique for high-dimensionalfeature indexing and retrieval is the crux of large-scale image retrieval. Arecent promising technique is Product Quantization, which attempts to indexhigh-dimensional image features by decomposing the feature space into aCartesian product of low dimensional subspaces and quantizing each of themseparately. Despite the promising results reported, their quantization approachfollows the typical hard assignment of traditional quantization methods, whichmay result in large quantization errors and thus inferior search performance.Unlike the existing approaches, in this paper, we propose a novel approachcalled Sparse Product Quantization (SPQ) to encoding the high-dimensionalfeature vectors into sparse representation. We optimize the sparserepresentations of the feature vectors by minimizing their quantization errors,making the resulting representation is essentially close to the original datain practice. Experiments show that the proposed SPQ technique is not only ableto compress data, but also an effective encoding technique. We obtainstate-of-the-art results for ANN search on four public image datasets and thepromising results of content-based image retrieval further validate theefficacy of our proposed method.
arxiv-16200-128 | Classification with Repulsion Tensors: A Case Study on Face Recognition | http://arxiv.org/pdf/1603.04588v1.pdf | author:Hawren Fang category:cs.CV I.5.2; I.4.10 published:2016-03-15 summary:We consider dimensionality reduction methods for face recognition in asupervised setting, using an image-as-matrix representation. A common procedureis to project image matrices into a smaller space in which the recognition isperformed. These methods are often called "two-dimensional" in the literatureand there exist counterparts that use an image-as-vector representation. Whentwo face images are close to each other in the input space they may remainclose after projection - but this is not desirable in the situation when thesetwo images are from different classes, and this often affects the recognitionperformance. We extend a previously developed `repulsion Laplacean' techniquebased on adding terms to the objective function with the goal or creation arepulsion energy between such images in the projected space. This scheme, whichrelies on a repulsion graph, is generic and can be incorporated into varioustwo-dimensional methods. It can be regarded as a multilinear generalization ofthe repulsion strategy by Kokiopoulou and Saad [Pattern Recog., 42 (2009), pp.2392--2402]. Experimental results demonstrate that the proposed methodologyoffers significant recognition improvement relative to the underlyingtwo-dimensional methods.
arxiv-16200-129 | Model-based Reinforcement Learning with Parametrized Physical Models and Optimism-Driven Exploration | http://arxiv.org/pdf/1509.06824v2.pdf | author:Christopher Xie, Sachin Patil, Teodor Moldovan, Sergey Levine, Pieter Abbeel category:cs.LG cs.RO published:2015-09-23 summary:In this paper, we present a robotic model-based reinforcement learning methodthat combines ideas from model identification and model predictive control. Weuse a feature-based representation of the dynamics that allows the dynamicsmodel to be fitted with a simple least squares procedure, and the features areidentified from a high-level specification of the robot's morphology,consisting of the number and connectivity structure of its links. Modelpredictive control is then used to choose the actions under an optimistic modelof the dynamics, which produces an efficient and goal-directed explorationstrategy. We present real time experimental results on standard benchmarkproblems involving the pendulum, cartpole, and double pendulum systems.Experiments indicate that our method is able to learn a range of benchmarktasks substantially faster than the previous best methods. To evaluate ourapproach on a realistic robotic control task, we also demonstrate real timecontrol of a simulated 7 degree of freedom arm.
arxiv-16200-130 | On the exact recovery of sparse signals via conic relaxations | http://arxiv.org/pdf/1603.04572v1.pdf | author:Hongbo Dong category:stat.ML math.OC published:2016-03-15 summary:In this note we compare two recently proposed semidefinite relaxations forthe sparse linear regression problem by Pilanci, Wainwright and El Ghaoui(Sparse learning via boolean relaxations, 2015) and Dong, Chen and Linderoth(Relaxation vs. Regularization A conic optimization perspective of statisticalvariable selection, 2015). We focus on the cardinality constrained formulation,and prove that the relaxation proposed by Dong, etc. is theoretically no weakerthan the one proposed by Pilanci, etc. Therefore any sufficient condition ofexact recovery derived by Pilanci can be readily applied to the otherrelaxation, including their results on high probability recovery for Gaussianensemble. Finally we provide empirical evidence that the relaxation by Dong,etc. requires much fewer observations to guarantee the recovery of truesupport.
arxiv-16200-131 | One-Shot Learning of Manipulation Skills with Online Dynamics Adaptation and Neural Network Priors | http://arxiv.org/pdf/1509.06841v2.pdf | author:Justin Fu, Sergey Levine, Pieter Abbeel category:cs.LG cs.RO published:2015-09-23 summary:One of the key challenges in applying reinforcement learning to complexrobotic control tasks is the need to gather large amounts of experience inorder to find an effective policy for the task at hand. Model-basedreinforcement learning can achieve good sample efficiency, but requires theability to learn a model of the dynamics that is good enough to learn aneffective policy. In this work, we develop a modelbased reinforcement learningalgorithm that combines prior knowledge from previous tasks with onlineadaptation of the dynamics model. These two ingredients enable highly sampleefficient learning even in regimes where estimating the true dynamics is verydifficult, since the online model adaptation allows the method to locallycompensate for unmodeled variation in the dynamics. We encode the priorexperience into a neural network dynamics model, and adapt it online byprogressively refitting a local linear model of the dynamics. Our experimentalresults show that this approach can be used to solve a variety of complexrobotic manipulation tasks in just a single attempt, using prior data fromother manipulation behaviors.
arxiv-16200-132 | Optimized Kernel-based Projection Space of Riemannian Manifolds | http://arxiv.org/pdf/1602.03570v3.pdf | author:Azadeh Alavi, Vishal M Patel, Rama Chellappa category:cs.CV published:2016-02-10 summary:It is proven that encoding images and videos through Symmetric PositiveDefinite (SPD) matrices, and considering the Riemannian geometry of theresulting space, can lead to increased classification performance. Taking intoaccount manifold geometry is typically done via embedding the manifolds intangent spaces, or Reproducing Kernel Hilbert Spaces (RKHS). Recently, it wasshown that embedding such manifolds into a Random Projection Spaces (RPS),rather than RKHS or tangent space, leads to higher classification andclustering performance. However, based on structure and dimensionality of therandomly generated hyperplanes, the classification performance over RPS mayvary significantly. In addition, fine-tuning RPS is data expensive (as itrequires validation-data), time consuming, and resource demanding. In thispaper, we introduce an approach to learn an optimized kernel-based projection(with fixed dimensionality), by employing the concept of subspace clustering.As such, we encode the association of data points to the underlying subspace ofeach point, to generate meaningful hyperplanes. Further, we adopt the conceptof dictionary learning and sparse coding, and discriminative analysis, for theoptimized kernel-based projection space (OPS) on SPD manifolds. We validate ouralgorithm on several classification tasks. The experiment results alsodemonstrate that the proposed method outperforms state-of-the-art methods onsuch manifolds.
arxiv-16200-133 | Unsupervised Ranking Model for Entity Coreference Resolution | http://arxiv.org/pdf/1603.04553v1.pdf | author:Xuezhe Ma, Zhengzhong Liu, Eduard Hovy category:cs.CL cs.LG published:2016-03-15 summary:Coreference resolution is one of the first stages in deep languageunderstanding and its importance has been well recognized in the naturallanguage processing community. In this paper, we propose a generative,unsupervised ranking model for entity coreference resolution by introducingresolution mode variables. Our unsupervised system achieves 58.44% F1 score ofthe CoNLL metric on the English data from the CoNLL-2012 shared task (Pradhanet al., 2012), outperforming the Stanford deterministic system (Lee et al.,2013) by 3.01%.
arxiv-16200-134 | Effective Computer Model For Recognizing Nationality From Frontal Image | http://arxiv.org/pdf/1603.04550v1.pdf | author:Bat-Erdene Batsukh, Ganbat Tsend category:cs.CV published:2016-03-15 summary:We are introducing new effective computer model for extracting nationalityfrom frontal image candidate using face part color, size and distances based ondeep research. Determining face part size, color, and distances is depending ona variety of factors including image quality, lighting condition, rotationangle, occlusion and facial emotion. Therefore, first we need to detect a faceon the image then convert an image into the real input. After that, we candetermine image candidate gender, face shape, key points and face parts.Finally, we will return the result, based on the comparison of sizes anddistances with the sample measurement table database. While we were measuringsamples, there were big differences between images by their gender and faceshapes. Input images must be the frontal face image that has smooth lightingand does not have any rotation angle. The model can be used in military,police, defense, healthcare, and technology sectors. Finally, Computer candistinguish nationality from the face image.
arxiv-16200-135 | Know Your Customer: Multi-armed Bandits with Capacity Constraints | http://arxiv.org/pdf/1603.04549v1.pdf | author:Ramesh Johari, Vijay Kamble, Yash Kanoria category:cs.LG cs.DS stat.ME stat.ML published:2016-03-15 summary:A wide range of resource allocation and platform operation settings exhibitthe following two simultaneous challenges: (1) service resources are capacityconstrained; and (2) clients' preferences are not perfectly known. To studythis pair of challenges, we consider a service system with heterogeneousservers and clients. Server types are known and there is fixed capacity ofservers of each type. Clients arrive over time, with types initially unknownand drawn from some distribution. Each client sequentially brings $N$ jobsbefore leaving. The system operator assigns each job to some server type,resulting in a payoff whose distribution depends on the client and servertypes. Our main contribution is a complete characterization of the structure of theoptimal policy for maximization of the rate of payoff accumulation. Such apolicy must balance three goals: (i) earning immediate payoffs; (ii) learningclient types to increase future payoffs; and (iii) satisfying the capacityconstraints. We construct a policy that has provably optimal regret (to leadingorder as $N$ grows large). Our policy has an appealingly simple three-phasestructure: a short type-"guessing" phase, a type-"confirmation" phase thatbalances payoffs with learning, and finally an "exploitation" phase thatfocuses on payoffs. Crucially, our approach employs the shadow prices of thecapacity constraints in the assignment problem with known types as "externalityprices" on the servers' capacity.
arxiv-16200-136 | Brain-Inspired Deep Networks for Image Aesthetics Assessment | http://arxiv.org/pdf/1601.04155v2.pdf | author:Zhangyang Wang, Shiyu Chang, Florin Dolcos, Diane Beck, Ding Liu, Thomas S. Huang category:cs.CV cs.LG cs.NE published:2016-01-16 summary:Image aesthetics assessment has been challenging due to its subjectivenature. Inspired by the scientific advances in the human visual perception andneuroaesthetics, we design Brain-Inspired Deep Networks (BDN) for this task.BDN first learns attributes through the parallel supervised pathways, on avariety of selected feature dimensions. A high-level synthesis network istrained to associate and transform those attributes into the overall aestheticsrating. We then extend BDN to predicting the distribution of human ratings,since aesthetics ratings are often subjective. Another highlight is ourfirst-of-its-kind study of label-preserving transformations in the context ofaesthetics assessment, which leads to an effective data augmentation approach.Experimental results on the AVA dataset show that our biological inspired andtask-specific BDN model gains significantly performance improvement, comparedto other state-of-the-art models with the same or higher parameter capacity.
arxiv-16200-137 | Estimation of Large Covariance and Precision Matrices from Temporally Dependent Observations | http://arxiv.org/pdf/1412.5059v4.pdf | author:Hai Shu, Bin Nan category:math.ST stat.ML stat.TH published:2014-12-16 summary:We consider the estimation of large covariance and precision matrices fromhigh-dimensional sub-Gaussian observations with slowly decaying temporaldependence that is bounded by certain polynomial decay rate. The temporaldependence is allowed to be long-range so with longer memory than thoseconsidered in the current literature. The rates of convergence are obtained forthe generalized thresholding estimation of covariance and correlation matrices,and for the constrained $\ell_1$ minimization and the $\ell_1$ penalizedlikelihood estimation of precision matrix. Properties of sparsistency andsign-consistency are also established. A gap-block cross-validation method isproposed for the tuning parameter selection, which performs well insimulations. As our motivating example, we study the brain functionalconnectivity using resting-state fMRI time series data with long-range temporaldependence.
arxiv-16200-138 | Robust Multi-body Feature Tracker: A Segmentation-free Approach | http://arxiv.org/pdf/1603.00110v2.pdf | author:Pan Ji, Hongdong Li, Mathieu Salzmann, Yiran Zhong category:cs.CV published:2016-03-01 summary:Feature tracking is a fundamental problem in computer vision, withapplications in many computer vision tasks, such as visual SLAM and actionrecognition. This paper introduces a novel multi-body feature tracker thatexploits a multi-body rigidity assumption to improve tracking robustness undera general perspective camera model. A conventional approach to addressing thisproblem would consist of alternating between solving two subtasks: motionsegmentation and feature tracking under rigidity constraints for each segment.This approach, however, requires knowing the number of motions, as well asassigning points to motion groups, which is typically sensitive to the motionestimates. By contrast, here, we introduce a segmentation-free solution tomulti-body feature tracking that bypasses the motion assignment step andreduces to solving a series of subproblems with closed-form solutions. Ourexperiments demonstrate the benefits of our approach in terms of trackingaccuracy and robustness to noise.
arxiv-16200-139 | Domain Adaptation via Maximum Independence of Domain Features | http://arxiv.org/pdf/1603.04535v1.pdf | author:Ke Yan, Lu Kou, David Zhang category:cs.CV cs.AI cs.LG published:2016-03-15 summary:When the distributions of the source and the target domains are different,domain adaptation techniques are needed. For example, in the field of sensorsand measurement, discrete and continuous distributional change often exist indata because of instrumental variation and time-varying sensor drift. In thispaper, we propose maximum independence domain adaptation (MIDA) to address thisproblem. Domain features are first defined to describe the backgroundinformation of a sample, such as the device label and acquisition time. Then,MIDA learns features which have maximal independence with the domain features,so as to reduce the inter-domain discrepancy in distributions. A featureaugmentation strategy is designed so that the learned projection isbackground-specific. Semi-supervised MIDA (SMIDA) extends MIDA by exploitingthe label information. The proposed methods can handle not only discretedomains in traditional domain adaptation problems but also continuousdistributional change such as the time-varying drift. In addition, they arenaturally applicable in supervised/semi-supervised/unsupervised classificationor regression problems with multiple domains. This flexibility brings potentialfor a wide range of applications. The effectiveness of our approaches isverified by experiments on synthetic datasets and four real-world ones onsensors, measurement, and computer vision.
arxiv-16200-140 | A Novel Method for Extrinsic Calibration of a 2-D Laser-Rangefinder and a Camera | http://arxiv.org/pdf/1603.04132v2.pdf | author:Wenbo Dong, Volkan Isler category:cs.CV cs.RO published:2016-03-14 summary:We present a novel solution for extrinsically calibrating a camera and aLaser Rangefinder (LRF) by computing the transformation between the cameraframe and the LRF frame. Our method is applicable for LRFs which measure only asingle plane. It does not rely on observing the laser plane in the cameraimage. Instead, we show that point-to-plane constraints from a singleobservation of a V-shaped calibration pattern composed of two non-coplanartriangles suffice to uniquely constrain the transformation. Next, we present amethod to obtain a solution using point-to-plane constraints from single ormultiple observations. Along the way, we also show that previous solutions, incontrast to our method, have inherent ambiguities and therefore must rely on agood initial estimate. Real and synthetic experiments validate our method andshow that it achieves better accuracy than previous methods.
arxiv-16200-141 | Object Contour Detection with a Fully Convolutional Encoder-Decoder Network | http://arxiv.org/pdf/1603.04530v1.pdf | author:Jimei Yang, Brian Price, Scott Cohen, Honglak Lee, Ming-Hsuan Yang category:cs.CV cs.LG published:2016-03-15 summary:We develop a deep learning algorithm for contour detection with a fullyconvolutional encoder-decoder network. Different from previous low-level edgedetection, our algorithm focuses on detecting higher-level object contours. Ournetwork is trained end-to-end on PASCAL VOC with refined ground truth frominaccurate polygon annotations, yielding much higher precision in objectcontour detection than previous methods. We find that the learned modelgeneralizes well to unseen object classes from the same super-categories on MSCOCO and can match state-of-the-art edge detection on BSDS500 with fine-tuning.By combining with the multiscale combinatorial grouping algorithm, our methodcan generate high-quality segmented object proposals, which significantlyadvance the state-of-the-art on PASCAL VOC (improving average recall from 0.62to 0.67) with a relatively small amount of candidates ($\sim$1660 per image).
arxiv-16200-142 | Saliency Detection for Improving Object Proposals | http://arxiv.org/pdf/1603.04146v2.pdf | author:Shuhan Chen, Jindong Li, Xuelong Hu, Ping Zhou category:cs.CV published:2016-03-14 summary:Object proposals greatly benefit object detection task in recentstate-of-the-art works, such as R-CNN [2]. However, the existing objectproposals usually have low localization accuracy at high intersection overunion threshold. To address it, we apply saliency detection to each boundingbox to improve their quality in this paper. We first present a geodesicsaliency detection method in contour, which is designed to find closedcontours. Then, we apply it to each candidate box with multi-sizes, and refinedboxes can be easily produced in the obtained saliency maps which are furtherused to calculate saliency scores for proposal ranking. Experiments on PASCALVOC 2007 test dataset demonstrate the proposed refinement approach can greatlyimprove existing models.
arxiv-16200-143 | Pushing the Limits of Deep CNNs for Pedestrian Detection | http://arxiv.org/pdf/1603.04525v1.pdf | author:Qichang Hu, Peng Wang, Chunhua Shen, Anton van den Hengel, Fatih Porikli category:cs.CV published:2016-03-15 summary:Compared to other applications in computer vision, convolutional neuralnetworks have under-performed on pedestrian detection. A breakthrough was madevery recently by using sophisticated deep CNN models, with a number ofhand-crafted features, or explicit occlusion handling mechanism. In this work,we show that by re-using the convolutional feature maps (CFMs) of a deepconvolutional neural network (DCNN) model as image features to train anensemble of boosted decision models, we are able to achieve the best reportedaccuracy without using specially designed learning algorithms. We empiricallyidentify and disclose important implementation details. We also show that pixellabelling may be simply combined with a detector to boost the detectionperformance. By adding complementary hand-crafted features such as opticalflow, the DCNN based detector can be further improved. We set a new record onthe Caltech pedestrian dataset, lowering the log-average miss rate from$11.7\%$ to $8.9\%$, a relative improvement of $24\%$. We also achieve acomparable result to the state-of-the-art approaches on the KITTI dataset.
arxiv-16200-144 | Sample and Filter: Nonparametric Scene Parsing via Efficient Filtering | http://arxiv.org/pdf/1511.04960v2.pdf | author:Mohammad Najafi, Sarah Taghavi Namin, Mathieu Salzmann, Lars Petersson category:cs.CV published:2015-11-16 summary:Scene parsing has attracted a lot of attention in computer vision. Whileparametric models have proven effective for this task, they cannot easilyincorporate new training data. By contrast, nonparametric approaches, whichbypass any learning phase and directly transfer the labels from the trainingdata to the query images, can readily exploit new labeled samples as theybecome available. Unfortunately, because of the computational cost of theirlabel transfer procedures, state-of-the-art nonparametric methods typicallyfilter out most training images to only keep a few relevant ones to label thequery. As such, these methods throw away many images that still containvaluable information and generally obtain an unbalanced set of labeled samples.In this paper, we introduce a nonparametric approach to scene parsing thatfollows a sample-and-filter strategy. More specifically, we propose to samplelabeled superpixels according to an image similarity score, which allows us toobtain a balanced set of samples. We then formulate label transfer as anefficient filtering procedure, which lets us exploit more labeled samples thanexisting techniques. Our experiments evidence the benefits of our approach overstate-of-the-art nonparametric methods on two benchmark datasets.
arxiv-16200-145 | Multichannel Variable-Size Convolution for Sentence Classification | http://arxiv.org/pdf/1603.04513v1.pdf | author:Wenpeng Yin, Hinrich Schütze category:cs.CL published:2016-03-15 summary:We propose MVCNN, a convolution neural network (CNN) architecture forsentence classification. It (i) combines diverse versions of pretrained wordembeddings and (ii) extracts features of multigranular phrases withvariable-size convolution filters. We also show that pretraining MVCNN iscritical for good performance. MVCNN achieves state-of-the-art performance onfour tasks: on small-scale binary, small-scale multi-class and largescaleTwitter sentiment prediction and on subjectivity classification.
arxiv-16200-146 | Neural GPUs Learn Algorithms | http://arxiv.org/pdf/1511.08228v3.pdf | author:Łukasz Kaiser, Ilya Sutskever category:cs.LG cs.NE published:2015-11-25 summary:Learning an algorithm from examples is a fundamental problem that has beenwidely studied. Recently it has been addressed using neural networks, inparticular by Neural Turing Machines (NTMs). These are fully differentiablecomputers that use backpropagation to learn their own programming. Despitetheir appeal NTMs have a weakness that is caused by their sequential nature:they are not parallel and are are hard to train due to their large depth whenunfolded. We present a neural network architecture to address this problem: the NeuralGPU. It is based on a type of convolutional gated recurrent unit and, like theNTM, is computationally universal. Unlike the NTM, the Neural GPU is highlyparallel which makes it easier to train and efficient to run. An essential property of algorithms is their ability to handle inputs ofarbitrary size. We show that the Neural GPU can be trained on short instancesof an algorithmic task and successfully generalize to long instances. Weverified it on a number of tasks including long addition and longmultiplication of numbers represented in binary. We train the Neural GPU onnumbers with upto 20 bits and observe no errors whatsoever while testing it,even on much longer numbers. To achieve these results we introduce a technique for training deep recurrentnetworks: parameter sharing relaxation. We also found a small amount of dropoutand gradient noise to have a large positive effect on learning andgeneralization.
arxiv-16200-147 | Conformal Predictors for Compound Activity Prediction | http://arxiv.org/pdf/1603.04506v1.pdf | author:Paolo Toccacheli, Ilia Nouretdinov, Alexander Gammerman category:cs.LG published:2016-03-14 summary:The paper presents an application of Conformal Predictors to achemoinformatics problem of identifying activities of chemical compounds. Thepaper addresses some specific challenges of this domain: a large number ofcompounds (training examples), high-dimensionality of feature space, sparsenessand a strong class imbalance. A variant of conformal predictors calledInductive Mondrian Conformal Predictor is applied to deal with thesechallenges. Results are presented for several non-conformity measures (NCM)extracted from underlying algorithms and different kernels. A number ofperformance measures are used in order to demonstrate the flexibility ofInductive Mondrian Conformal Predictors in dealing with such a complex set ofdata. Keywords: Conformal Prediction, Confidence Estimation, Chemoinformatics,Non-Conformity Measure.
arxiv-16200-148 | End-to-End Attention-based Large Vocabulary Speech Recognition | http://arxiv.org/pdf/1508.04395v2.pdf | author:Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, Yoshua Bengio category:cs.CL cs.AI cs.LG cs.NE published:2015-08-18 summary:Many of the current state-of-the-art Large Vocabulary Continuous SpeechRecognition Systems (LVCSR) are hybrids of neural networks and Hidden MarkovModels (HMMs). Most of these systems contain separate components that deal withthe acoustic modelling, language modelling and sequence decoding. Weinvestigate a more direct approach in which the HMM is replaced with aRecurrent Neural Network (RNN) that performs sequence prediction directly atthe character level. Alignment between the input features and the desiredcharacter sequence is learned automatically by an attention mechanism builtinto the RNN. For each predicted character, the attention mechanism scans theinput sequence and chooses relevant frames. We propose two methods to speed upthis operation: limiting the scan to a subset of most promising frames andpooling over time the information contained in neighboring frames, therebyreducing source sequence length. Integrating an n-gram language model into thedecoding process yields recognition accuracies similar to other HMM-freeRNN-based approaches.
arxiv-16200-149 | A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena | http://arxiv.org/pdf/1502.04938v2.pdf | author:Arianna Bisazza, Marcello Federico category:cs.CL published:2015-02-17 summary:Word reordering is one of the most difficult aspects of statistical machinetranslation (SMT), and an important factor of its quality and efficiency.Despite the vast amount of research published to date, the interest of thecommunity in this problem has not decreased, and no single method appears to bestrongly dominant across language pairs. Instead, the choice of the optimalapproach for a new translation task still seems to be mostly driven byempirical trials. To orientate the reader in this vast and complex researcharea, we present a comprehensive survey of word reordering viewed as astatistical modeling challenge and as a natural language phenomenon. The surveydescribes in detail how word reordering is modeled within differentstring-based and tree-based SMT frameworks and as a stand-alone task, includingsystematic overviews of the literature in advanced reordering modeling. We thenquestion why some approaches are more successful than others in differentlanguage pairs. We argue that, besides measuring the amount of reordering, itis important to understand which kinds of reordering occur in a given languagepair. To this end, we conduct a qualitative analysis of word reorderingphenomena in a diverse sample of language pairs, based on a large collection oflinguistic knowledge. Empirical results in the SMT literature are shown tosupport the hypothesis that a few linguistic facts can be very useful toanticipate the reordering characteristics of a language pair and to select theSMT framework that best suits them.
arxiv-16200-150 | End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF | http://arxiv.org/pdf/1603.01354v4.pdf | author:Xuezhe Ma, Eduard Hovy category:cs.LG cs.CL stat.ML published:2016-03-04 summary:State-of-the-art sequence labeling systems traditionally require largeamounts of task-specific knowledge in the form of hand-crafted features anddata pre-processing. In this paper, we introduce a novel neutral networkarchitecture that benefits from both word- and character-level representationsautomatically, by using combination of bidirectional LSTM, CNN and CRF. Oursystem is truly end-to-end, requiring no feature engineering or datapre-processing, thus making it applicable to a wide range of sequence labelingtasks on different languages. We evaluate our system on two data sets for twosequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS)tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtainstate-of-the-art performance on both the two data --- 97.55\% accuracy for POStagging and 91.21\% F1 for NER.
arxiv-16200-151 | Online but Accurate Inference for Latent Variable Models with Local Gibbs Sampling | http://arxiv.org/pdf/1603.02644v2.pdf | author:Christophe Dupuy, Francis Bach category:cs.LG stat.ML published:2016-03-08 summary:We study parameter inference in large-scale latent variable models. We firstpropose an unified treatment of online inference for latent variable modelsfrom a non-canonical exponential family, and draw explicit links betweenseveral previously proposed frequentist or Bayesian methods. We then propose anovel inference method for the frequentist estimation of parameters, thatadapts MCMC methods to online inference of latent variable models with theproper use of local Gibbs sampling. Then, for latent Dirich-let allocation,weprovide an extensive set of experiments and comparisons with existing work,where our new approach outperforms all previously proposed methods. Inparticular, using Gibbs sampling for latent variable inference is superior tovariational inference in terms of test log-likelihoods. Moreover, Bayesianinference through variational methods perform poorly, sometimes leading toworse fits with latent variables of higher dimensionality.
arxiv-16200-152 | Criteria of efficiency for conformal prediction | http://arxiv.org/pdf/1603.04416v1.pdf | author:Vladimir Vovk, Valentina Fedorova, Ilia Nouretdinov, Alex Gammerman category:cs.LG 68T05 I.2.6 published:2016-03-14 summary:We study optimal conformity measures for various criteria of efficiency in anidealized setting. This leads to an important class of criteria of efficiencythat we call probabilistic; it turns out that the most standard criteria ofefficiency used in literature on conformal prediction are not probabilistic.
arxiv-16200-153 | A New PAC-Bayesian Perspective on Domain Adaptation | http://arxiv.org/pdf/1506.04573v3.pdf | author:Pascal Germain, Amaury Habrard, François Laviolette, Emilie Morvant category:stat.ML cs.LG published:2015-06-15 summary:We study the issue of PAC-Bayesian domain adaptation: We want to learn, froma source domain, a majority vote model dedicated to a target one. Ourtheoretical contribution brings a new perspective by deriving an upper-bound onthe target risk where the distributions' divergence---expressed as aratio---controls the trade-off between a source error measure and the targetvoters' disagreement. Our bound suggests that one has to focus on regions wherethe source data is informative.From this result, we derive a PAC-Bayesiangeneralization bound, and specialize it to linear classifiers. Then, we infer alearning algorithmand perform experiments on real data.
arxiv-16200-154 | Sparsity in Multivariate Extremes with Applications to Anomaly Detection | http://arxiv.org/pdf/1507.05899v2.pdf | author:Nicolas Goix, Anne Sabourin, Stéphan Clémençon category:stat.ML published:2015-07-21 summary:Capturing the dependence structure of multivariate extreme events is a majorconcern in many fields involving the management of risks stemming from multiplesources, e.g. portfolio monitoring, insurance, environmental risk managementand anomaly detection. One convenient (non-parametric) characterization ofextremal dependence in the framework of multivariate Extreme Value Theory (EVT)is the angular measure, which provides direct information about the probable'directions' of extremes, that is, the relative contribution of eachfeature/coordinate of the 'largest' observations. Modeling the angular measurein high dimensional problems is a major challenge for the multivariate analysisof rare events. The present paper proposes a novel methodology aiming atexhibiting a sparsity pattern within the dependence structure of extremes. Thisis done by estimating the amount of mass spread by the angular measure onrepresentative sets of directions, corresponding to specific sub-cones of$R^d\_+$. This dimension reduction technique paves the way towards scaling upexisting multivariate EVT methods. Beyond a non-asymptotic study providing atheoretical validity framework for our method, we propose as a directapplication a --first-- anomaly detection algorithm based on multivariate EVT.This algorithm builds a sparse 'normal profile' of extreme behaviours, to beconfronted with new (possibly abnormal) extreme observations. Illustrativeexperimental results provide strong empirical evidence of the relevance of ourapproach.
arxiv-16200-155 | Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy? | http://arxiv.org/pdf/1504.08291v5.pdf | author:Raja Giryes, Guillermo Sapiro, Alex M. Bronstein category:cs.NE cs.LG stat.ML 62M45 I.5.1 published:2015-04-30 summary:Three important properties of a classification machinery are: (i) the systempreserves the core information of the input data; (ii) the training examplesconvey information about unseen data; and (iii) the system is able to treatdifferently points from different classes. In this work we show that thesefundamental properties are satisfied by the architecture of deep neuralnetworks. We formally prove that these networks with random Gaussian weightsperform a distance-preserving embedding of the data, with a special treatmentfor in-class and out-of-class data. Similar points at the input of the networkare likely to have a similar output. The theoretical analysis of deep networkshere presented exploits tools used in the compressed sensing and dictionarylearning literature, thereby making a formal connection between these importanttopics. The derived results allow drawing conclusions on the metric learningproperties of the network and their relation to its structure, as well asproviding bounds on the required size of the training set such that thetraining examples would represent faithfully the unseen data. The results arevalidated with state-of-the-art trained networks.
arxiv-16200-156 | Rapid building detection using machine learning | http://arxiv.org/pdf/1603.04392v1.pdf | author:Joseph Paul Cohen, Wei Ding, Caitlin Kuhlman, Aijun Chen, Liping Di category:cs.CV published:2016-03-14 summary:This work describes algorithms for performing discrete object detection,specifically in the case of buildings, where usually only low quality RGB-onlygeospatial reflective imagery is available. We utilize new candidate search andfeature extraction techniques to reduce the problem to a machine learning (ML)classification task. Here we can harness the complex patterns of contrastfeatures contained in training data to establish a model of buildings. We avoidcostly sliding windows to generate candidates; instead we innovatively stitchtogether well known image processing techniques to produce candidates forbuilding detection that cover 80-85% of buildings. Reducing the number ofpossible candidates is important due to the scale of the problem. Eachcandidate is subjected to classification which, although linear, costs time andprohibits large scale evaluation. We propose a candidate alignment algorithm toboost classification performance to 80-90% precision with a linear timealgorithm and show it has negligible cost. Also, we propose a new conceptcalled a Permutable Haar Mesh (PHM) which we use to form and traverse a searchspace to recover candidate buildings which were lost in the initialpreprocessing phase.
arxiv-16200-157 | A Ranking Approach to Global Optimization | http://arxiv.org/pdf/1603.04381v1.pdf | author:Cédric Malherbe, Emile Contal, Nicolas Vayatis category:stat.ML published:2016-03-14 summary:In this paper, we consider the problem of maximizing an unknown function fover a compact and convex set using as few observations f(x) as possible. Weobserve that the optimization of the function f essentially relies on learningthe induced bipartite ranking rule of f. Based on this idea, we relate globaloptimization to bipartite ranking which allows to address problems with highdimensional input space, as well as cases of functions with weak regularityproperties. The paper introduces novel meta-algorithms for global optimizationwhich rely on the choice of any bipartite ranking method. Theoreticalproperties are provided as well as convergence guarantees and equivalencesbetween various optimization methods are obtained as a by-product. Eventually,numerical evidence is given to show that the main algorithm of the paper whichadapts empirically to the underlying ranking structure essentially outperformsexisting state-of-the-art global optimization algorithms in typical benchmarks.
arxiv-16200-158 | Sparse Coding with Earth Mover's Distance for Multi-Instance Histogram Representation | http://arxiv.org/pdf/1502.02377v2.pdf | author:Mohua Zhang, Jianhua Peng, Xuejie Liu, Jim Jing-Yan Wang category:cs.LG stat.ML published:2015-02-09 summary:Sparse coding (Sc) has been studied very well as a powerful datarepresentation method. It attempts to represent the feature vector of a datasample by reconstructing it as the sparse linear combination of some basicelements, and a $L_2$ norm distance function is usually used as the lossfunction for the reconstruction error. In this paper, we investigate using Scas the representation method within multi-instance learning framework, where asample is given as a bag of instances, and further represented as a histogramof the quantized instances. We argue that for the data type of histogram, using$L_2$ norm distance is not suitable, and propose to use the earth mover'sdistance (EMD) instead of $L_2$ norm distance as a measure of thereconstruction error. By minimizing the EMD between the histogram of a sampleand the its reconstruction from some basic histograms, a novel sparse codingmethod is developed, which is refereed as SC-EMD. We evaluate its performancesas a histogram representation method in tow multi-instance learning problems--- abnormal image detection in wireless capsule endoscopy videos, and proteinbinding site retrieval. The encouraging results demonstrate the advantages ofthe new method over the traditional method using $L_2$ norm distance.
arxiv-16200-159 | Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations | http://arxiv.org/pdf/1603.04351v1.pdf | author:Eliyahu Kiperwasser, Yoav Goldberg category:cs.CL published:2016-03-14 summary:We present a simple and effective scheme for dependency parsing which isbased on bidirectional-LSTMs (BiLSTMs). Each sentence token is associated witha BiLSTM vector representing the token in its sentential context, and featurevectors are constructed by concatenating a few BiLSTM vectors. The BiLSTM istrained jointly with the parser objective, resulting in very effective featureextractors for parsing. We demonstrate the effectiveness of the approach byapplying it to a greedy transition based parser as well as to a globallyoptimized graph-based parser. The resulting parsers have very simplearchitectures, and match or surpass the state-of-the-art accuracies on Englishand Chinese.
arxiv-16200-160 | Iterative Refinement of Approximate Posterior for Training Directed Belief Networks | http://arxiv.org/pdf/1511.06382v4.pdf | author:R Devon Hjelm, Kyunghyun Cho, Junyoung Chung, Russ Salakhutdinov, Vince Calhoun, Nebojsa Jojic category:cs.LG stat.ML published:2015-11-19 summary:Recent advances in variational inference that make use of an inference orrecognition network for training and evaluating deep directed graphical modelshave advanced well beyond traditional variational inference and Markov chainMonte Carlo methods. These techniques offer higher flexibility with simpler andfaster inference; yet training and evaluation still remains a challenge. Wepropose a method for improving the per-example approximate posterior byiterative refinement, which can provide notable gains in maximizing thevariational lower bound of the log likelihood and works with both continuousand discrete latent variables. We evaluate our approach as a method of trainingand evaluating directed graphical models. We show that, when used for training,iterative refinement improves the variational lower bound and can also improvethe log-likelihood over related methods. We also show that iterative refinementcan be used to get a better estimate of the log-likelihood in any directedmodel trained with mean-field inference.
arxiv-16200-161 | Neuroprosthetic decoder training as imitation learning | http://arxiv.org/pdf/1511.04156v2.pdf | author:Josh Merel, David Carlson, Liam Paninski, John P. Cunningham category:stat.ML cs.LG q-bio.NC published:2015-11-13 summary:Neuroprosthetic brain-computer interfaces function via an algorithm whichdecodes neural activity of the user into movements of an end effector, such asa cursor or robotic arm. In practice, the decoder is often learned by updatingits parameters while the user performs a task. When the user's intention is notdirectly observable, recent methods have demonstrated value in training thedecoder against a surrogate for the user's intended movement. We describe howtraining a decoder in this way is a novel variant of an imitation learningproblem, where an oracle or expert is employed for supervised training in lieuof direct observations, which are not available. Specifically, we describe howa generic imitation learning meta-algorithm, dataset aggregation (DAgger, [1]),can be adapted to train a generic brain-computer interface. By derivingexisting learning algorithms for brain-computer interfaces in this framework,we provide a novel analysis of regret (an important metric of learningefficacy) for brain-computer interfaces. This analysis allows us tocharacterize the space of algorithmic variants and bounds on their regretrates. Existing approaches for decoder learning have been performed in thecursor control setting, but the available design principles for these decodersare such that it has been impossible to scale them to naturalistic settings.Leveraging our findings, we then offer an algorithm that combines imitationlearning with optimal control, which should allow for training of arbitraryeffectors for which optimal control can generate goal-oriented control. Wedemonstrate this novel and general BCI algorithm with simulated neuroprostheticcontrol of a 26 degree-of-freedom model of an arm, a sophisticated andrealistic end effector.
arxiv-16200-162 | Automatic Discrimination of Color Retinal Images using the Bag of Words Approach | http://arxiv.org/pdf/1603.04327v1.pdf | author:Ibrahim Sadek category:cs.CV published:2016-03-14 summary:Diabetic retinopathy (DR) and age related macular degeneration (ARMD) areamong the major causes of visual impairment worldwide. DR is mainlycharacterized by red spots, namely microaneurysms and bright lesions,specifically exudates whereas ARMD is mainly identified by tiny yellow or whitedeposits called drusen. Since exudates might be the only manifestation of theearly diabetic retinopathy, there is an increase demand for automaticretinopathy diagnosis. Exudates and drusen may share similar appearances, thusdiscriminating between them is of interest to enhance screening performance. Inthis research, we investigative the role of bag of words approach in theautomatic diagnosis of retinopathy diabetes. We proposed to use a single basedand multiple based methods for the construction of the visual dictionary bycombining the histogram of word occurrences from each dictionary and building asingle histogram. The introduced approach is evaluated for automatic diagnosisof normal and abnormal color fundus images with bright lesions. This approachhas been implemented on 430 fundus images, including six publicly availabledatasets, in addition to one local dataset. The mean accuracies reported are97.2% and 99.77% for single based and multiple based dictionaries respectively.
arxiv-16200-163 | Learning Network of Multivariate Hawkes Processes: A Time Series Approach | http://arxiv.org/pdf/1603.04319v1.pdf | author:Jalal Etesami, Negar Kiyavash, Kun Zhang, Kushagra Singhal category:cs.LG cs.AI stat.ML published:2016-03-14 summary:Learning the influence structure of multiple time series data is of greatinterest to many disciplines. This paper studies the problem of recovering thecausal structure in network of multivariate linear Hawkes processes. In suchprocesses, the occurrence of an event in one process affects the probability ofoccurrence of new events in some other processes. Thus, a natural notion ofcausality exists between such processes captured by the support of theexcitation matrix. We show that the resulting causal influence network isequivalent to the Directed Information graph (DIG) of the processes, whichencodes the causal factorization of the joint distribution of the processes.Furthermore, we present an algorithm for learning the support of excitationmatrix (or equivalently the DIG). The performance of the algorithm is evaluatedon synthesized multivariate Hawkes networks as well as a stock market andMemeTracker real-world dataset.
arxiv-16200-164 | Diversity in Object Proposals | http://arxiv.org/pdf/1603.04308v1.pdf | author:Anton Winschel, Rainer Lienhart, Christian Eggert category:cs.CV published:2016-03-14 summary:Current top performing object recognition systems build on object proposalsas a preprocessing step. Object proposal algorithms are designed to generatecandidate regions for generic objects, yet current approaches are limited incapturing the vast variety of object characteristics. In this paper we analyzethe error modes of the state-of-the-art Selective Search object proposalalgorithm and suggest extensions to broaden its feature diversity in order tomitigate its error modes. We devise an edge grouping algorithm for handlingobjects without clear boundaries. To further enhance diversity, we incorporatethe Edge Boxes proposal algorithm, which is based on fundamentally differentprinciples than Selective Search. The combination of segmentations and edgesprovides rich image information and feature diversity which is essential forobtaining high quality object proposals for generic objects. For a presetamount of object proposals we achieve considerably better results by using ourcombination of different strategies than using any single strategy alone.
arxiv-16200-165 | Universal probability-free conformal prediction | http://arxiv.org/pdf/1603.04283v1.pdf | author:Vladimir Vovk, Dusko Pavlovic category:cs.LG 68T05 I.2.6 published:2016-03-14 summary:We construct a universal prediction system in the spirit of Popper'sfalsifiability and Kolmogorov complexity. This prediction system does notdepend on any statistical assumptions, but under the IID assumption itdominates, although in a rather weak sense, conformal prediction.
arxiv-16200-166 | Dynamic Scene Deblurring using a Locally Adaptive Linear Blur Model | http://arxiv.org/pdf/1603.04265v1.pdf | author:Tae Hyun Kim, Seungjun Nah, Kyoung Mu Lee category:cs.CV published:2016-03-14 summary:State-of-the-art video deblurring methods cannot handle blurry videosrecorded in dynamic scenes, since they are built under a strong assumption thatthe captured scenes are static. Contrary to the existing methods, we propose avideo deblurring algorithm that can deal with general blurs inherent in dynamicscenes. To handle general and locally varying blurs caused by various sources,such as moving objects, camera shake, depth variation, and defocus, we estimatepixel-wise non-uniform blur kernels. We infer bidirectional optical flows tohandle motion blurs, and also estimate Gaussian blur maps to remove opticalblur from defocus in our new blur model. Therefore, we propose a single energymodel that jointly estimates optical flows, defocus blur maps and latentframes. We also provide a framework and efficient solvers to minimize theproposed energy model. By optimizing the energy model, we achieve significantimprovements in removing general blurs, estimating optical flows, and extendingdepth-of-field in blurry frames. Moreover, in this work, to evaluate theperformance of non-uniform deblurring methods objectively, we have constructeda new realistic dataset with ground truths. In addition, extensive experimentalon publicly available challenging video data demonstrate that the proposedmethod produces qualitatively superior performance than the state-of-the-artmethods which often fail in either deblurring or optical flow estimation.
arxiv-16200-167 | Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks | http://arxiv.org/pdf/1511.04508v2.pdf | author:Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, Ananthram Swami category:cs.CR cs.LG cs.NE stat.ML published:2015-11-14 summary:Deep learning algorithms have been shown to perform extremely well on manyclassical machine learning problems. However, recent studies have shown thatdeep learning, like other machine learning techniques, is vulnerable toadversarial samples: inputs crafted to force a deep neural network (DNN) toprovide adversary-selected outputs. Such attacks can seriously undermine thesecurity of the system supported by the DNN, sometimes with devastatingconsequences. For example, autonomous vehicles can be crashed, illicit orillegal content can bypass content filters, or biometric authentication systemscan be manipulated to allow improper access. In this work, we introduce adefensive mechanism called defensive distillation to reduce the effectivenessof adversarial samples on DNNs. We analytically investigate thegeneralizability and robustness properties granted by the use of defensivedistillation when training DNNs. We also empirically study the effectiveness ofour defense mechanisms on two DNNs placed in adversarial settings. The studyshows that defensive distillation can reduce effectiveness of sample creationfrom 95% to less than 0.5% on a studied DNN. Such dramatic gains can beexplained by the fact that distillation leads gradients used in adversarialsample creation to be reduced by a factor of 10^30. We also find thatdistillation increases the average minimum number of features that need to bemodified to create adversarial samples by about 800% on one of the DNNs wetested.
arxiv-16200-168 | A Variational Perspective on Accelerated Methods in Optimization | http://arxiv.org/pdf/1603.04245v1.pdf | author:Andre Wibisono, Ashia C. Wilson, Michael I. Jordan category:math.OC cs.LG stat.ML published:2016-03-14 summary:Accelerated gradient methods play a central role in optimization, achievingoptimal rates in many settings. While many generalizations and extensions ofNesterov's original acceleration method have been proposed, it is not yet clearwhat is the natural scope of the acceleration concept. In this paper, we studyaccelerated methods from a continuous-time perspective. We show that there is aLagrangian functional that we call the \emph{Bregman Lagrangian} whichgenerates a large class of accelerated methods in continuous time, including(but not limited to) accelerated gradient descent, its non-Euclidean extension,and accelerated higher-order gradient methods. We show that the continuous-timelimit of all of these methods correspond to traveling the same curve inspacetime at different speeds. From this perspective, Nesterov's technique andmany of its generalizations can be viewed as a systematic way to go from thecontinuous-time curves generated by the Bregman Lagrangian to a family ofdiscrete-time accelerated algorithms.
arxiv-16200-169 | Interactive Tools and Tasks for the Hebrew Bible | http://arxiv.org/pdf/1603.04236v1.pdf | author:Nicolai Winther-Nielsen category:cs.CL published:2016-03-14 summary:Ancient texts can support intertextuality in different ways through digitaltools for databases and for tasks that scholars and students do, when theyinterac twith the texts in new ways. This contribution explores how the corpusof the Hebrew Bible created and maintained by the Eep Talstra Center for Bibleand Computer has potential for redefining the way we learn from our ancienttexts as modern knowledge workers. It first describes how the corpus was usedfor development of Bible Online Learner as a persuasive technology to enhancelanguage learning with, in and around a database that drives interactive tasksfor learners. The achievements obtained through so far are very promising, andit can help us explore textual criticism as another target for interactivestudy of the Hebrew Bible through corpus-technology. Because textual criticismis an increasingly specialized area of research which depends on digitalresources. The commercial solution from Logos Bible Software offers advancedscholarly resources from the German Bible Society as a model for how affluentWestern scholars can use technology for the Hebrew corpus. The achievements incorpus-driven learning and the potential of commercial resources can help ussuggest new tasks in textual criticism based on online applications which usecorpora for a new kind of textual corpus criticism. Some promising tools fortext categorization, analysis of translation shifts and interpretation arerecommended as potential models for the future. The main goal in the futuremust be more open global access for the new tools.
arxiv-16200-170 | Graph Based Sinogram Denoising for Tomographic Reconstructions | http://arxiv.org/pdf/1603.04203v1.pdf | author:Faisal Mahmood, Nauman Shahid, Pierre Vandergheynst, Ulf Skoglund category:cs.CV published:2016-03-14 summary:Limited data and low dose constraints are common problems in a variety oftomographic reconstruction paradigms which lead to noisy and incomplete data.Over the past few years sinogram denoising has become an essentialpre-processing step for low dose Computed Tomographic (CT) reconstructions. Wepropose a novel sinogram denoising algorithm inspired by the modern field ofsignal processing on graphs. Graph based methods often perform better thanstandard filtering operations since they can exploit the signal structure. Thismakes the sinogram an ideal candidate for graph based denoising since itgenerally has a piecewise smooth structure. We test our method with a varietyof phantoms and different reconstruction methods. Our numerical study showsthat the proposed algorithm improves the performance of analytical filteredback-projection (FBP) and iterative methods ART (Kaczmarz) and SIRT(Cimmino).We observed that graph denoised sinogram always minimizes the errormeasure and improves the accuracy of the solution as compared to regularreconstructions.
arxiv-16200-171 | Online Isotonic Regression | http://arxiv.org/pdf/1603.04190v1.pdf | author:Wojciech Kotłowski, Wouter M. Koolen, Alan Malek category:cs.LG stat.ML published:2016-03-14 summary:We consider the online version of the isotonic regression problem. Given aset of linearly ordered points (e.g., on the real line), the learner mustpredict labels sequentially at adversarially chosen positions and is evaluatedby her total squared loss compared against the best isotonic (non-decreasing)function in hindsight. We survey several standard online learning algorithmsand show that none of them achieve the optimal regret exponent; in fact, mostof them (including Online Gradient Descent, Follow the Leader and ExponentialWeights) incur linear regret. We then prove that the Exponential Weightsalgorithm played over a covering net of isotonic functions has regret isbounded by $O(T^{1/3} \log^{2/3}(T))$ and present a matching $\Omega(T^{1/3})$lower bound on regret. We also provide a computationally efficient version ofthis algorithm. We also analyze the noise-free case, in which the revealedlabels are isotonic, and show that the bound can be improved to $O(\log T)$ oreven to $O(1)$ (when the labels are revealed in the isotonic order). Finally,we extend the analysis beyond squared loss and give bounds for log-loss andabsolute loss.
arxiv-16200-172 | Visual Concept Recognition and Localization via Iterative Introspection | http://arxiv.org/pdf/1603.04186v1.pdf | author:Amir Rosenfeld, Shimon Ullman category:cs.CV cs.LG published:2016-03-14 summary:Convolutional neural networks have been shown to develop internalrepresentations, which correspond closely to semantically meaningful objectsand parts, although trained solely on class labels. Class Activation Mapping(CAM) is a recent method that makes it possible to easily highlight the imageregions contributing to a network's classification decision. We build uponthese two developments to enable a network to re-examine informative imageregions, which we term introspection. We propose a weakly-supervised iterativescheme, which shifts its center of attention to increasingly discriminativeregions as it progresses, by alternating stages of classification andintrospection. We evaluate our method and show its effectiveness over a rangeof several datasets, obtaining a top-1 accuracy 84.48% CUB-200-2011, which isthe highest to-date without using external data or stronger supervision. OnStanford-40 Actions, we set a new state-of the art of 87.89%, and onFGVC-Aircraft and the Stanford Dogs dataset, we show consistent improvementsover baselines, some of which include significantly more supervision.
arxiv-16200-173 | Extended Object Tracking: Introduction, Overview and Applications | http://arxiv.org/pdf/1604.00970v1.pdf | author:Karl Granstrom, Marcus Baum category:cs.CV cs.SY published:2016-03-14 summary:This article provides an elaborate overview of current research in extendedobject tracking. We provide a clear definition of an extended object anddiscuss its delimitation to other object types and sensor models. Next,different shape models and possibilities to model the number of measurementsare extensively discussed. Subsequently, we give a tutorial introduction to twobasic and well used extended object tracking methods -- the random matrixapproach and random hypersurface approach. The next part treats approaches fortracking multiple extended objects and elaborates how the large number offeasible association hypotheses can be tackled using both Random Finite Set(RFS) and Non-RFS multi-object trackers. The article concludes with a summaryof current applications, where three example applications involving Lidar, RGB,and RGB-D sensors are highlighted.
arxiv-16200-174 | Correlations between the Hurst exponent and the maximal Lyapunov exponent for some low-dimensional discrete conservative dynamical systems | http://arxiv.org/pdf/1501.03766v3.pdf | author:Mariusz Tarnopolski category:nlin.CD math.DS stat.ML published:2015-01-15 summary:The Chirikov standard map and the 2D Froeschl\'e map are investigated. A fewthousand values of the Hurst exponent (HE) and the maximal Lyapunov exponent(mLE) are plotted in a mixed space of the nonlinear parameter versus theinitial condition. Both characteristic exponents reveal remarkably similarstructures in this mixed space. Moreover, a tight correlation between the HEsand mLEs for the two maps was found: $\rho=0.83$ and $\rho=0.75$ for theChirikov and Froeschl\'e maps, respectively, where $\rho$ is the Spearman rank.Based on this relation, a machine learning (ML) procedure, using the nearestneighbour algorithm, was performed to reproduce the HE distributions based onthe mLE distributions. A few thousand HE and mLE values from the mixed spaceswere used for training, and then using $2-2.4\times 10^5$ mLEs, the HEs wereretrieved. The ML procedure allowed to reproduce the structure of the mixedspaces in great detail. The HE is proposed as an informative parameter in thearea of chaotic control, as it provides expectations about the general trend ina time series.
arxiv-16200-175 | Graphical Representation for Heterogeneous Face Recognition | http://arxiv.org/pdf/1503.00488v3.pdf | author:Chunlei Peng, Xinbo Gao, Nannan Wang, Jie Li category:cs.CV published:2015-03-02 summary:Heterogeneous face recognition (HFR) refers to matching face images acquiredfrom different sources (i.e., different sensors or different wavelengths) foridentification. HFR plays an important role in both biometrics research andindustry. In spite of promising progresses achieved in recent years, HFR isstill a challenging problem due to the difficulty to represent twoheterogeneous images in a homogeneous manner. Existing HFR methods eitherrepresent an image ignoring the spatial information, or rely on atransformation procedure which complicates the recognition task. Consideringthese problems, we propose a novel graphical representation based HFR method(G-HFR) in this paper. Markov networks are employed to represent heterogeneousimage patches separately, which takes the spatial compatibility betweenneighboring image patches into consideration. A coupled representationsimilarity metric (CRSM) is designed to measure the similarity between obtainedgraphical representations. Extensive experiments conducted on multiple HFRscenarios (viewed sketch, forensic sketch, near infrared image, and thermalinfrared image) show that the proposed method outperforms state-of-the-artmethods.
arxiv-16200-176 | Top-$K$ Ranking from Pairwise Comparisons: When Spectral Ranking is Optimal | http://arxiv.org/pdf/1603.04153v1.pdf | author:Minje Jang, Sunghyun Kim, Changho Suh, Sewoong Oh category:cs.LG cs.IT cs.SI math.IT stat.ML published:2016-03-14 summary:We explore the top-$K$ rank aggregation problem. Suppose a collection ofitems is compared in pairs repeatedly, and we aim to recover a consistentordering that focuses on the top-$K$ ranked items based on partially revealedpreference information. We investigate the Bradley-Terry-Luce model in whichone ranks items according to their perceived utilities modeled as noisyobservations of their underlying true utilities. Our main contributions aretwo-fold. First, in a general comparison model where item pairs to compare aregiven a priori, we attain an upper and lower bound on the sample size forreliable recovery of the top-$K$ ranked items. Second, more importantly,extending the result to a random comparison model where item pairs to compareare chosen independently with some probability, we show that in slightlyrestricted regimes, the gap between the derived bounds reduces to a constantfactor, hence reveals that a spectral method can achieve the minimax optimalityon the (order-wise) sample size required for top-$K$ ranking. That is to say,we demonstrate a spectral method alone to be sufficient to achieve theoptimality and advantageous in terms of computational complexity, as it doesnot require an additional stage of maximum likelihood estimation that astate-of-the-art scheme employs to achieve the optimality. We corroborate ourmain results by numerical experiments.
arxiv-16200-177 | Regression-based Hypergraph Learning for Image Clustering and Classification | http://arxiv.org/pdf/1603.04150v1.pdf | author:Sheng Huang, Dan Yang, Bo Liu, Xiaohong Zhang category:cs.CV published:2016-03-14 summary:Inspired by the recently remarkable successes of Sparse Representation (SR),Collaborative Representation (CR) and sparse graph, we present a novelhypergraph model named Regression-based Hypergraph (RH) which utilizes theregression models to construct the high quality hypergraphs. Moreover, we plugRH into two conventional hypergraph learning frameworks, namely hypergraphspectral clustering and hypergraph transduction, to present Regression-basedHypergraph Spectral Clustering (RHSC) and Regression-based HypergraphTransduction (RHT) models for addressing the image clustering andclassification issues. Sparse Representation and Collaborative Representationare employed to instantiate two RH instances and their RHSC and RHT algorithms.The experimental results on six popular image databases demonstrate that theproposed RH learning algorithms achieve promising image clustering andclassification performances, and also validate that RH can inherit thedesirable properties from both hypergraph models and regression models.
arxiv-16200-178 | SSSC-AM: A Unified Framework for Video Co-Segmentation by Structured Sparse Subspace Clustering with Appearance and Motion Features | http://arxiv.org/pdf/1603.04139v1.pdf | author:Junlin Yao, Frank Nielsen category:cs.CV published:2016-03-14 summary:Video co-segmentation refers to the task of jointly segmenting common objectsappearing in a given group of videos. In practice, high-dimensional data suchas videos can be conceptually thought as being drawn from a union of subspacescorresponding to categories rather than from a smooth manifold. Therefore,segmenting data into respective subspaces --- subspace clustering --- findswidespread applications in computer vision, including co-segmentation.State-of-the-art methods via subspace clustering seek to solve the problem intwo steps: First, an affinity matrix is built from data, with appearance features ormotion patterns. Second, the data are segmented by applying spectral clusteringto the affinity matrix. However, this process is insufficient to obtain anoptimal solution since it does not take into account the {\em interdependence}of the affinity matrix with the segmentation. In this work, we present a novelunified video co-segmentation framework inspired by the recent StructuredSparse Subspace Clustering ($\mathrm{S^{3}C}$) based on the {\emself-expressiveness} model. Our method yields more consistent segmentationresults. In order to improve the detectability of motion features with missingtrajectories due to occlusion or tracked points moving out of frames, we add anextra-dimensional signature to the motion trajectories. Moreover, wereformulate the $\mathrm{S^{3}C}$ algorithm by adding the affine subspaceconstraint in order to make it more suitable to segment rigid motions lying inaffine subspaces of dimension at most $3$. Experiments on MOViCS datasetdemonstrate the effectiveness of our approaches and its robustness to heavynoise.
arxiv-16200-179 | RISAS: A Novel Rotation, Illumination, Scale Invariant Appearance and Shape Feature | http://arxiv.org/pdf/1603.04134v1.pdf | author:Xiaoyang Li, Kanzhi Wu, Yong Liu, Ravindra Ranasinghe, Gamini Dissanayake, Rong Xiong category:cs.RO cs.CV published:2016-03-14 summary:In this paper, we present a novel RGB-D feature, RISAS, which is robust toRotation, Illumination and Scale variations through fusing Appearance and Shapeinformation. We propose a keypoint detector which is able to extractinformation rich regions in both appearance and shape using a novel 3Dinformation representation method in combination with grayscale information. Weextend our recent work on Local Ordinal Intensity and Normal Descriptor(LOIND),to further significantly improve its illumination, scale and rotationinvariance using 1) a precise neighbourhood region selection method and 2) amore robust dominant orientation estimation. We also present a dataset forevaluation of RGB-D features, together with comprehensive experiments toillustrate the effectiveness of the proposed RGB-D feature when compared toSIFT, C-SHOT and LOIND. We also show the use of RISAS for point cloud alignmentassociated with many robotics applications and demonstrate its effectiveness ina poorly illuminated environment when compared with SIFT and ORB.
arxiv-16200-180 | How deep is knowledge tracing? | http://arxiv.org/pdf/1604.02416v1.pdf | author:Mohammad Khajah, Robert V. Lindsey, Michael C. Mozer category:cs.AI cs.NE published:2016-03-14 summary:In theoretical cognitive science, there is a tension between highlystructured models whose parameters have a direct psychological interpretationand highly complex, general-purpose models whose parameters and representationsare difficult to interpret. The former typically provide more insight intocognition but the latter often perform better. This tension has recentlysurfaced in the realm of educational data mining, where a deep learningapproach to predicting students' performance as they work through a series ofexercises---termed deep knowledge tracing or DKT---has demonstrated a stunningperformance advantage over the mainstay of the field, Bayesian knowledgetracing or BKT. In this article, we attempt to understand the basis for DKT'sadvantage by considering the sources of statistical regularity in the data thatDKT can leverage but which BKT cannot. We hypothesize four forms of regularitythat BKT fails to exploit: recency effects, the contextualized trial sequence,inter-skill similarity, and individual variation in ability. We demonstratethat when BKT is extended to allow it more flexibility in modeling statisticalregularities---using extensions previously proposed in the literature---BKTachieves a level of performance indistinguishable from that of DKT. We arguethat while DKT is a powerful, useful, general-purpose framework for modelingstudent learning, its gains do not come from the discovery of novelrepresentations---the fundamental advantage of deep learning. To answer thequestion posed in our title, knowledge tracing may be a domain that does notrequire `depth'; shallow models like BKT can perform just as well and offer usgreater interpretability and explanatory power.
arxiv-16200-181 | Fast Gaussian Process Regression for Big Data | http://arxiv.org/pdf/1509.05142v4.pdf | author:Sourish Das, Sasanka Roy, Rajiv Sambasivan category:cs.LG stat.ML published:2015-09-17 summary:Gaussian Processes are widely used for regression tasks. A known limitationin the application of Gaussian Processes to regression tasks is that thecomputation of the solution requires performing a matrix inversion. Thesolution also requires the storage of a large matrix in memory. These factorsrestrict the application of Gaussian Process regression to small and moderatesize data sets. We present an algorithm based on empirically determined subsetselection that works well on both real world and synthetic datasets. We alsocompare the performance of this algorithm with two other methods that are usedto apply Gaussian Processes Regression on large datasets. On the synthetic andreal world datasets used in this study, the algorithm demonstrated sub-lineartime and space complexity. The accuracy obtained with this algorithm on thedatasets used for this study is comparable to what is achieved with the twoother methods commonly used to apply Gaussian Processes to large datasets.
arxiv-16200-182 | Exploratory Gradient Boosting for Reinforcement Learning in Complex Domains | http://arxiv.org/pdf/1603.04119v1.pdf | author:David Abel, Alekh Agarwal, Fernando Diaz, Akshay Krishnamurthy, Robert E. Schapire category:cs.AI cs.LG stat.ML published:2016-03-14 summary:High-dimensional observations and complex real-world dynamics present majorchallenges in reinforcement learning for both function approximation andexploration. We address both of these challenges with two complementarytechniques: First, we develop a gradient-boosting style, non-parametricfunction approximator for learning on $Q$-function residuals. And second, wepropose an exploration strategy inspired by the principles of state abstractionand information acquisition under uncertainty. We demonstrate the empiricaleffectiveness of these techniques, first, as a preliminary check, on twostandard tasks (Blackjack and $n$-Chain), and then on two much larger and morerealistic tasks with high-dimensional observation spaces. Specifically, weintroduce two benchmarks built within the game Minecraft where the observationsare pixel arrays of the agent's visual field. A combination of our twoalgorithmic techniques performs competitively on the standardreinforcement-learning tasks while consistently and substantially outperformingbaselines on the two tasks with high-dimensional observation spaces. The newfunction approximator, exploration strategy, and evaluation benchmarks are eachof independent interest in the pursuit of reinforcement-learning methods thatscale to real-world domains.
arxiv-16200-183 | Bandit Approaches to Preference Learning Problems with Multiple Populations | http://arxiv.org/pdf/1603.04118v1.pdf | author:Aniruddha Bhargava, Ravi Ganti, Robert Nowak category:stat.ML cs.AI cs.LG published:2016-03-14 summary:In this paper we study an extension of the stochastic multi-armed bandit(MAB) framework, where in each round a player can play multiple actions andreceive a stochastic reward which depends on the actions played. This problemis motivated by applications in recommendation problems where there aremultiple populations of users and hence no single choice might be good for theentire population. We specifically look at bandit problems where we are allowedto make two choices in each round. We provide algorithms for this problem inboth the noiseless and noisy case. Our algorithms are computationally efficientand have provable sample complexity guarantees. In the process of establishingsample complexity guarantees for our algorithms, we establish new resultsregarding the Nystr{\"o}m method which can be of independent interest. Wesupplement our theoretical results with experimental comparisons.
arxiv-16200-184 | Multi-modal Tracking for Object based SLAM | http://arxiv.org/pdf/1603.04117v1.pdf | author:Prateek Singhal, Ruffin White, Henrik Christensen category:cs.CV published:2016-03-14 summary:We present an on-line 3D visual object tracking framework for monocularcameras by incorporating spatial knowledge and uncertainty from semanticmapping along with high frequency measurements from visual odometry. Using acombination of vision and odometry that are tightly integrated we can increasethe overall performance of object based tracking for semantic mapping. Wepresent a framework for integration of the two data-sources into a coherentframework through information based fusion/arbitration. We demonstrate theframework in the context of OmniMapper[1] and present results on 6 challengingsequences over multiple objects compared to data obtained from a motion capturesystems. We are able to achieve a mean error of 0.23m for per frame trackingshowing 9% relative error less than state of the art tracker.
arxiv-16200-185 | Learning Binary Codes and Binary Weights for Efficient Classification | http://arxiv.org/pdf/1603.04116v1.pdf | author:Fumin Shen, Yadong Mu, Wei Liu, Yang Yang, Heng Tao Shen category:cs.CV published:2016-03-14 summary:This paper proposes a generic formulation that significantly expedites thetraining and deployment of image classification models, particularly under thescenarios of many image categories and high feature dimensions. As a definingproperty, our method represents both the images and learned classifiers usingbinary hash codes, which are simultaneously learned from the training data.Classifying an image thereby reduces to computing the Hamming distance betweenthe binary codes of the image and classifiers and selecting the class withminimal Hamming distance. Conventionally, compact hash codes are primarily usedfor accelerating image search. Our work is first of its kind to representclassifiers using binary codes. Specifically, we formulate multi-class imageclassification as an optimization problem over binary variables. Theoptimization alternatively proceeds over the binary classifiers and image hashcodes. Profiting from the special property of binary codes, we show that thesub-problems can be efficiently solved through either a binary quadraticprogram (BQP) or linear program. In particular, for attacking the BQP problem,we propose a novel bit-flipping procedure which enjoys high efficacy and localoptimality guarantee. Our formulation supports a large family of empirical lossfunctions and is here instantiated by exponential / hinge losses. Comprehensiveevaluations are conducted on several representative image benchmarks. Theexperiments consistently observe reduced complexities of model training anddeployment, without sacrifice of accuracies.
arxiv-16200-186 | A Stochastic Approach to STDP | http://arxiv.org/pdf/1603.04080v1.pdf | author:Runchun Wang, Chetan Singh Thakur, Tara Julia Hamilton, Jonathan Tapson, André van Schaik category:cs.NE published:2016-03-13 summary:We present a digital implementation of the Spike Timing Dependent Plasticity(STDP) learning rule. The proposed digital implementation consists of anexponential decay generator array and a STDP adaptor array. On the arrival of apre- and post-synaptic spike, the STDP adaptor will send a digital spike to thedecay generator. The decay generator will then generate an exponential decay,which will be used by the STDP adaptor to perform the weight adaption. Theexponential decay, which is computational expensive, is efficiently implementedby using a novel stochastic approach, which we analyse and characterise here.We use a time multiplexing approach to achieve 8192 (8k) virtual STDP adaptorsand decay generators with only one physical implementation of each. We havevalidated our stochastic STDP approach with measurement results of a balancedexcitation/inhibition experiment. Our stochastic approach is ideal forimplementing the STDP learning rule in large-scale spiking neural networksrunning in real time.
arxiv-16200-187 | Exploring the Neural Algorithm of Artistic Style | http://arxiv.org/pdf/1602.07188v2.pdf | author:Yaroslav Nikulin, Roman Novak category:cs.CV published:2016-02-23 summary:We explore the method of style transfer presented in the article "A NeuralAlgorithm of Artistic Style" by Leon A. Gatys, Alexander S. Ecker and MatthiasBethge (arXiv:1508.06576). We first demonstrate the power of the suggested style space on a fewexamples. We then vary different hyper-parameters and program properties thatwere not discussed in the original paper, among which are the recognitionnetwork used, starting point of the gradient descent and different ways topartition style and content layers. We also give a brief comparison of some ofthe existing algorithm implementations and deep learning frameworks used. To study the style space further we attempt to generate synthetic images bymaximizing a single entry in one of the Gram matrices $\mathcal{G}_l$ and someinteresting results are observed. Next, we try to mimic the sparsity andintensity distribution of Gram matrices obtained from a real painting andgenerate more complex textures. Finally, we propose two new style representations built on top of network'sfeatures and discuss how one could be used to achieve local and potentiallycontent-aware style transfer.
arxiv-16200-188 | A Grothendieck-type inequality for local maxima | http://arxiv.org/pdf/1603.04064v1.pdf | author:Andrea Montanari category:math.OC stat.ML published:2016-03-13 summary:A large number of problems in optimization, machine learning, signalprocessing can be effectively addressed by suitable semidefinite programming(SDP) relaxations. Unfortunately, generic SDP solvers hardly scale beyondinstances with a few hundreds variables (in the underlying combinatorialproblem). On the other hand, it has been observed empirically that an effectivestrategy amounts to introducing a (non-convex) rank constraint, and solving theresulting smooth optimization problem by ascent methods. This non-convexproblem has --generically-- a large number of local maxima, and the reason forthis success is therefore unclear. This paper provides rigorous support for this approach. For the problem ofmaximizing a linear functional over the elliptope, we prove that all localmaxima are within a small gap from the SDP optimum. In several problems ofinterest, arbitrarily small relative error can be achieved by taking the rankconstraint $k$ to be of order one, independently of the problem size.
arxiv-16200-189 | Revisiting Active Perception | http://arxiv.org/pdf/1603.02729v2.pdf | author:Ruzena Bajcsy, Yiannis Aloimonos, John K. Tsotsos category:cs.CV cs.RO published:2016-03-08 summary:Despite the recent successes in robotics, artificial intelligence andcomputer vision, a complete artificial agent necessarily must include activeperception. A multitude of ideas and methods for how to accomplish this havealready appeared in the past, their broader utility perhaps impeded byinsufficient computational power or costly hardware. The history of theseideas, perhaps selective due to our perspectives, is presented with the goal oforganizing the past literature and highlighting the seminal contributions. Weargue that those contributions are as relevant today as they were decades agoand, with the state of modern computational tools, are poised to find new lifein the robotic perception systems of the next decade.
arxiv-16200-190 | Triplet Similarity Embedding for Face Verification | http://arxiv.org/pdf/1602.03418v2.pdf | author:Swami Sankaranarayanan, Azadeh Alavi, Rama Chellappa category:cs.CV published:2016-02-10 summary:In this work, we present an unconstrained face verification algorithm andevaluate it on the recently released IJB-A dataset that aims to push theboundaries of face verification methods. The proposed algorithm couples a deepCNN-based approach with a low-dimensional discriminative embedding learnt usingtriplet similarity constraints in a large margin fashion. Aside from yieldingperformance improvement, this embedding provides significant advantages interms of memory and post-processing operations like hashing and visualization.Experiments on the IJB-A dataset show that the proposed algorithm outperformsstate of the art methods in verification and identification metrics, whilerequiring less training time.
arxiv-16200-191 | Divide and Conquer Local Average Regression | http://arxiv.org/pdf/1601.06239v2.pdf | author:Xiangyu Chang, Shaobo Lin, Yao Wang category:cs.LG math.ST stat.TH published:2016-01-23 summary:The divide and conquer strategy, which breaks a massive data set into a se-ries of manageable data blocks, and then combines the independent results ofdata blocks to obtain a final decision, has been recognized as astate-of-the-art method to overcome challenges of massive data analysis. Inthis paper, we merge the divide and conquer strategy with local averageregression methods to infer the regressive relationship of input-output pairsfrom a massive data set. After theoretically analyzing the pros and cons, wefind that although the divide and conquer local average regression can reachthe optimal learning rate, the restric- tion to the number of data blocks is abit strong, which makes it only feasible for small number of data blocks. Wethen propose two variants to lessen (or remove) this restriction. Our resultsshow that these variants can achieve the optimal learning rate with much milderrestriction (or without such restriction). Extensive experimental studies arecarried out to verify our theoretical assertions.
arxiv-16200-192 | Image and Depth from a Single Defocused Image Using Coded Aperture Photography | http://arxiv.org/pdf/1603.04046v1.pdf | author:Mina Masoudifar, Hamid Reza Pourreza category:cs.CV published:2016-03-13 summary:Depth from defocus and defocus deblurring from a single image are twochallenging problems that are derived from the finite depth of field inconventional cameras. Coded aperture imaging is one of the techniques that isused for improving the results of these two problems. Up to now, differentmethods have been proposed for improving the results of either defocusdeblurring or depth estimation. In this paper, a multi-objective function isproposed for evaluating and designing aperture patterns with the aim ofimproving the results of both depth from defocus and defocus deblurring.Pattern evaluation is performed by considering the scene illumination conditionand camera system specification. Based on the proposed criteria, a singleasymmetric pattern is designed that is used for restoring a sharp image and adepth map from a single input. Since the designed pattern is asymmetric,defocus objects on the two sides of the focal plane can be distinguished. Depthestimation is performed by using a new algorithm, which is based on imagequality assessment criteria and can distinguish between blurred objects lyingin front or behind the focal plane. Extensive simulations as well asexperiments on a variety of real scenes are conducted to compare our aperturewith previously proposed ones.
arxiv-16200-193 | Deep Interactive Object Selection | http://arxiv.org/pdf/1603.04042v1.pdf | author:Ning Xu, Brian Price, Scott Cohen, Jimei Yang, Thomas Huang category:cs.CV published:2016-03-13 summary:Interactive object selection is a very important research problem and hasmany applications. Previous algorithms require substantial user interactions toestimate the foreground and background distributions. In this paper, we presenta novel deep learning based algorithm which has a much better understanding ofobjectness and thus can reduce user interactions to just a few clicks. Ouralgorithm transforms user provided positive and negative clicks into twoEuclidean distance maps which are then concatenated with the RGB channels ofimages to compose (image, user interactions) pairs. We generate many of suchpairs by combining several random sampling strategies to model user clickpatterns and use them to fine tune deep Fully Convolutional Networks (FCNs).Finally the output probability maps of our FCN 8s model is integrated withgraph cut optimization to refine the boundary segments. Our model is trained onthe PASCAL segmentation dataset and evaluated on other datasets with differentobject classes. Experimental results on both seen and unseen objects clearlydemonstrate that our algorithm has a good generalization ability and issuperior to all existing interactive object selection approaches.
arxiv-16200-194 | Pose for Action - Action for Pose | http://arxiv.org/pdf/1603.04037v1.pdf | author:Umar Iqbal, Martin Garbade, Juergen Gall category:cs.CV published:2016-03-13 summary:In this work we propose to utilize information about human actions to improvepose estimation in monocular videos. To this end, we present a pictorialstructure model that exploits high-level information about activities toincorporate higher-order part dependencies by modeling action specificappearance models and pose priors. However, instead of using an additionalexpensive action recognition framework, the action priors are efficientlyestimated by our pose estimation framework. This is achieved by starting with auniform action prior and updating the action prior during pose estimation. Wealso show that learning the right amount of appearance sharing among actionclasses improves the pose estimation. Our proposed model achievesstate-of-the-art performance on two challenging datasets for pose estimationand action recognition with over 80,000 test images.
arxiv-16200-195 | A comprehensive study of sparse codes on abnormality detection | http://arxiv.org/pdf/1603.04026v1.pdf | author:Huamin Ren, Hong Pan, Søren Ingvor Olsen, Thomas B. Moeslund category:cs.CV published:2016-03-13 summary:Sparse representation has been applied successfully in abnormal eventdetection, in which the baseline is to learn a dictionary accompanied by sparsecodes. While much emphasis is put on discriminative dictionary construction,there are no comparative studies of sparse codes regarding abnormalitydetection. We comprehensively study two types of sparse codes solutions -greedy algorithms and convex L1-norm solutions - and their impact onabnormality detection performance. We also propose our framework of combiningsparse codes with different detection methods. Our comparative experiments arecarried out from various angles to better understand the applicability ofsparse codes, including computation time, reconstruction error, sparsity,detection accuracy, and their performance combining various detection methods.Experiments show that combining OMP codes with maximum coordinate detectioncould achieve state-of-the-art performance on the UCSD dataset [14].
arxiv-16200-196 | Conditional Risk Minimization for Stochastic Processes | http://arxiv.org/pdf/1510.02706v2.pdf | author:Alexander Zimin, Christoph H. Lampert category:stat.ML cs.LG published:2015-10-09 summary:We study the task of learning from non-i.i.d. data. In particular, we aim atlearning predictors that minimize the conditional risk for a stochasticprocess, i.e. the expected loss of the predictor on the next point conditionedon the set of training samples observed so far. For non-i.i.d. data, thetraining set contains information about the upcoming samples, so learning withrespect to the conditional distribution can be expected to yield betterpredictors than one obtains from the classical setting of minimizing themarginal risk. Our main contribution is a practical estimator for theconditional risk based on the theory of non-parametric time-series prediction,and a finite sample concentration bound that establishes uniform convergence ofthe estimator to the true conditional risk under certain regularity assumptionson the process.
arxiv-16200-197 | Learning zeroth class dictionary for human action recognition | http://arxiv.org/pdf/1603.04015v1.pdf | author:Jia-xin Cai, Xin Tang, Lifang Zhang, Guocan Feng category:cs.CV published:2016-03-13 summary:In this paper, a discriminative two-layer dictionary learning framework isproposed for classifying human action by sparse shape representations, in whichthe first-layer dictionary is learned on the selected discriminative frames andthe second-layer dictionary is built for recognition using reconstructionerrors of the first-layer dictionary as input features. We propose a "zerothclass" trick for detecting undiscriminating frames of the test video andeliminating them before voting on the action categories. Experimental resultson benchmarks demonstrate the effectiveness of our method.
arxiv-16200-198 | Complexity Theoretic Limitations on Learning Halfspaces | http://arxiv.org/pdf/1505.05800v2.pdf | author:Amit Daniely category:cs.CC cs.LG published:2015-05-21 summary:We study the problem of agnostically learning halfspaces which is defined bya fixed but unknown distribution $\mathcal{D}$ on $\mathbb{Q}^n\times \{\pm1\}$. We define $\mathrm{Err}_{\mathrm{HALF}}(\mathcal{D})$ as the least errorof a halfspace classifier for $\mathcal{D}$. A learner who can access$\mathcal{D}$ has to return a hypothesis whose error is small compared to$\mathrm{Err}_{\mathrm{HALF}}(\mathcal{D})$. Using the recently developed method of the author, Linial and Shalev-Shwartzwe prove hardness of learning results under a natural assumption on thecomplexity of refuting random $K$-$\mathrm{XOR}$ formulas. We show that noefficient learning algorithm has non-trivial worst-case performance even underthe guarantees that $\mathrm{Err}_{\mathrm{HALF}}(\mathcal{D}) \le \eta$ forarbitrarily small constant $\eta>0$, and that $\mathcal{D}$ is supported in$\{\pm 1\}^n\times \{\pm 1\}$. Namely, even under these favorable conditionsits error must be $\ge \frac{1}{2}-\frac{1}{n^c}$ for every $c>0$. Inparticular, no efficient algorithm can achieve a constant approximation ratio.Under a stronger version of the assumption (where $K$ can be poly-logarithmicin $n$), we can take $\eta = 2^{-\log^{1-\nu}(n)}$ for arbitrarily small$\nu>0$. Interestingly, this is even stronger than the best known lower bounds(Arora et. al. 1993, Feldamn et. al. 2006, Guruswami and Raghavendra 2006) forthe case that the learner is restricted to return a halfspace classifier (i.e.proper learning).
arxiv-16200-199 | Fast Learning from Distributed Datasets without Entity Matching | http://arxiv.org/pdf/1603.04002v1.pdf | author:Giorgio Patrini, Richard Nock, Stephen Hardy, Tiberio Caetano category:cs.LG cs.DC I.2.6 published:2016-03-13 summary:Consider the following data fusion scenario: two datasets/peers contain thesame real-world entities described using partially shared features, e.g.banking and insurance company records of the same customer base. Our goal is tolearn a classifier in the cross product space of the two domains, in the hardcase in which no shared ID is available -- e.g. due to anonymization.Traditionally, the problem is approached by first addressing entity matchingand subsequently learning the classifier in a standard manner. We present anend-to-end solution which bypasses matching entities, based on the recentlyintroduced concept of Rademacher observations (rados). Informally, we replacethe minimisation of a loss over examples, which requires to solve entityresolution, by the equivalent minimisation of a (different) loss over rados.Among others, key properties we show are (i) a potentially huge subset of theserados does not require to perform entity matching, and (ii) the algorithm thatprovably minimizes the rado loss over these rados has time and spacecomplexities smaller than the algorithm minimizing the equivalent example loss.Last, we relax a key assumption of the model, that the data is verticallypartitioned among peers --- in this case, we would not even know the existenceof a solution to entity resolution. In this more general setting, experimentsvalidate the possibility of significantly beating even the optimal peer inhindsight.
arxiv-16200-200 | Learning Typographic Style | http://arxiv.org/pdf/1603.04000v1.pdf | author:Shumeet Baluja category:cs.CV cs.LG cs.NE published:2016-03-13 summary:Typography is a ubiquitous art form that affects our understanding,perception, and trust in what we read. Thousands of different font-faces havebeen created with enormous variations in the characters. In this paper, welearn the style of a font by analyzing a small subset of only four letters.From these four letters, we learn two tasks. The first is a discriminationtask: given the four letters and a new candidate letter, does the new letterbelong to the same font? Second, given the four basis letters, can we generateall of the other letters with the same characteristics as those in the basisset? We use deep neural networks to address both tasks, quantitatively andqualitatively measure the results in a variety of novel manners, and present athorough investigation of the weaknesses and strengths of the approach.
arxiv-16200-201 | An efficient Exact-PGA algorithm for constant curvature manifolds | http://arxiv.org/pdf/1603.03984v1.pdf | author:Rudrasis Chakraborty, Dohyung Seo, Baba C. Vemuri category:cs.CV published:2016-03-13 summary:Manifold-valued datasets are widely encountered in many computer visiontasks. A non-linear analog of the PCA, called the Principal Geodesic Analysis(PGA) suited for data lying on Riemannian manifolds was reported in literaturea decade ago. Since the objective function in PGA is highly non-linear and hardto solve efficiently in general, researchers have proposed a linearapproximation. Though this linear approximation is easy to compute, it lacksaccuracy especially when the data exhibits a large variance. Recently, analternative called exact PGA was proposed which tries to solve the optimizationwithout any linearization. For general Riemannian manifolds, though it givesbetter accuracy than the original (linearized) PGA, for data that exhibit largevariance, the optimization is not computationally efficient. In this paper, wepropose an efficient exact PGA for constant curvature Riemannian manifolds(CCM-EPGA). CCM-EPGA differs significantly from existing PGA algorithms in twoaspects, (i) the distance between a given manifold-valued data point and theprincipal submanifold is computed analytically and thus no optimization isrequired as in existing methods. (ii) Unlike the existing PGA algorithms, thedescent into codimension-1 submanifolds does not require any optimization butis accomplished through the use of the Rimeannian inverse Exponential map andthe parallel transport operations. We present theoretical and experimentalresults for constant curvature Riemannian manifolds depicting favorableperformance of CCM-EPGA compared to existing PGA algorithms. We also presentdata reconstruction from principal components and directions which has not beenpresented in literature in this setting.
arxiv-16200-202 | On Learning High Dimensional Structured Single Index Models | http://arxiv.org/pdf/1603.03980v1.pdf | author:Nikhil Rao, Ravi Ganti, Laura Balzano, Rebecca Willett, Robert Nowak category:stat.ML cs.AI cs.LG published:2016-03-13 summary:Single Index Models (SIMs) are simple yet flexible semi-parametric models forclassification and regression, where response variables are modeled as anonlinear, monotonic function of a linear combination of features. Estimationin this context requires learning both the feature weights and the nonlinearfunction that relates features to observations. While methods have beendescribed to learn SIMs in the low dimensional regime, a method that canefficiently learn SIMs in high dimensions, and under general structuralassumptions, has not been forthcoming. In this paper, we proposecomputationally efficient algorithms for SIM inference in high dimensions usingatomic norm regularization. This general approach to imposing structure inhigh-dimensional modeling specializes to sparsity, group sparsity, and low-rankassumptions among others. We also provide a scalable, stochastic version of themethod. Experiments show that the method we propose enjoys superior predictiveperformance when compared to generalized linear models such as logisticregression, on several real-world datasets.
arxiv-16200-203 | Privacy-preserving Analysis of Correlated Data | http://arxiv.org/pdf/1603.03977v1.pdf | author:Yizhen Wang, Shuang Song, Kamalika Chaudhuri category:cs.LG cs.CR stat.ML published:2016-03-13 summary:Many modern machine learning applications involve sensitive correlated data,such private information on users connected together in a social network, andmeasurements of physical activity of a single user across time. However, thecurrent standard of privacy in machine learning, differential privacy, cannotadequately address privacy issues in this kind of data. This work looks at a recent generalization of differential privacy, calledPufferfish, that can be used to address privacy in correlated data. The mainchallenge in applying Pufferfish to correlated data problems is the lack ofsuitable mechanisms. In this paper, we provide a general mechanism, called theWasserstein Mechanism, which applies to any Pufferfish framework. Since theWasserstein Mechanism may be computationally inefficient, we provide anadditional mechanism, called Markov Quilt Mechanism, that applies to somepractical cases such as physical activity measurements across time, and iscomputationally efficient.
arxiv-16200-204 | Laplacian Eigenmaps from Sparse, Noisy Similarity Measurements | http://arxiv.org/pdf/1603.03972v1.pdf | author:Keith Levin, Vince Lyzinski category:stat.ML published:2016-03-12 summary:Manifold learning and dimensionality reduction techniques are ubiquitous inscience and engineering, but can be computationally expensive procedures whenapplied to large data sets or when similarities are expensive to compute. Todate, little work has been done to investigate the tradeoff betweencomputational resources and the quality of learned representations. We presentboth theoretical and experimental explorations of this question. In particular,we consider Laplacian eigenmaps embeddings based on a kernel matrix, andexplore how the embeddings behave when this kernel matrix is corrupted byocclusion and noise. Our main theoretical result shows that under modest noiseand occlusion assumptions, we can (with high probability) recover a goodapproximation to the Laplacian eigenmaps embedding based on the uncorruptedkernel matrix. Our results also show how regularization can aid thisapproximation. Experimentally, we explore the effects of noise and occlusion onLaplacian eigenmaps embeddings of two real-world data sets, one from speechprocessing and one from neuroscience, as well as a synthetic data set.
arxiv-16200-205 | Temporally Robust Global Motion Compensation by Keypoint-based Congealing | http://arxiv.org/pdf/1603.03968v1.pdf | author:S. Morteza Safdarnejad, Yousef Atoum, Xiaoming Liu category:cs.CV published:2016-03-12 summary:Global motion compensation (GMC) removes the impact of camera motion andcreates a video in which the background appears static over the progression oftime. Various vision problems, such as human activity recognition, backgroundreconstruction, and multi-object tracking can benefit from GMC. Existing GMCalgorithms rely on sequentially processing consecutive frames, by estimatingthe transformation mapping the two frames, and obtaining a compositetransformation to a global motion compensated coordinate. Sequential GMCsuffers from temporal drift of frames from the accurate global coordinate, dueto either error accumulation or sporadic failures of motion estimation at a fewframes. We propose a temporally robust global motion compensation (TRGMC)algorithm which performs accurate and stable GMC, despite complicated andlong-term camera motion. TRGMC densely connects pairs of frames, by matchinglocal keypoints of each frame. A joint alignment of these frames is formulatedas a novel keypoint-based congealing problem, where the transformation of eachframe is updated iteratively, such that the spatial coordinates for the startand end points of matched keypoints are identical. Experimental resultsdemonstrate that TRGMC has superior performance in a wide range of scenarios.
arxiv-16200-206 | Image Captioning with Semantic Attention | http://arxiv.org/pdf/1603.03925v1.pdf | author:Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, Jiebo Luo category:cs.CV published:2016-03-12 summary:Automatically generating a natural language description of an image hasattracted interests recently both because of its importance in practicalapplications and because it connects two major artificial intelligence fields:computer vision and natural language processing. Existing approaches are eithertop-down, which start from a gist of an image and convert it into words, orbottom-up, which come up with words describing various aspects of an image andthen combine them. In this paper, we propose a new algorithm that combines bothapproaches through a model of semantic attention. Our algorithm learns toselectively attend to semantic concept proposals and fuse them into hiddenstates and outputs of recurrent neural networks. The selection and fusion forma feedback connecting the top-down and bottom-up computation. We evaluate ouralgorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimentalresults show that our algorithm significantly outperforms the state-of-the-artapproaches consistently across different evaluation metrics.
arxiv-16200-207 | Variational Neural Discourse Relation Recognizer | http://arxiv.org/pdf/1603.03876v1.pdf | author:Biao Zhang, Deyi Xiong, Jinsong Su category:cs.CL published:2016-03-12 summary:Implicit discourse relation recognition is a crucial component for automaticdiscourse-level analysis and nature language understanding. Previous studiesexploit discriminative models that are built on either powerful manual featuresor deep discourse representations. In this paper, instead, we exploregenerative models and propose a variational neural discourse relationrecognizer. We refer to this model as VIRILE. VIRILE establishes a directedprobabilistic model with a latent continuous variable that generates both adiscourse and the relation between the two arguments of the discourse. In orderto perform efficient inference and learning, we introduce a neural discourserelation model to approximate the posterior of the latent variable, and employthis approximated posterior to optimize a reparameterized variational lowerbound. This allows VIRILE to be trained with standard stochastic gradientmethods. Experiments on the benchmark data set show that VIRILE can achievecompetitive results against state-of-the-art baselines.
arxiv-16200-208 | Two New Approaches to Compressed Sensing Exhibiting Both Robust Sparse Recovery and the Grouping Effect | http://arxiv.org/pdf/1410.8229v2.pdf | author:Mehmet Eren Ahsen, Mathukumalli Vidyasagar category:stat.ML 90C25 published:2014-10-30 summary:In this paper we introduce a new optimization formulation for sparseregression and compressed sensing, called CLOT (Combined L-One and Two),wherein the regularizer is a convex combination of the $\ell_1$- and$\ell_2$-norms. This formulation differs from the Elastic Net (EN) formulation,in which the regularizer is a convex combination of the $\ell_1$- and$\ell_2$-norm squared. This seemingly simple modification has fairlysignificant consequences. In particular, it is shown in this paper that the ENformulation \textit{does not achieve} robust recovery of sparse vectors in thecontext of compressed sensing, whereas the new CLOT formulation does so. Also,like EN but unlike LASSO, the CLOT formulation achieves the grouping effect,wherein coefficients of highly correlated columns of the measurement (ordesign) matrix are assigned roughly comparable values. It is noteworthy thatLASSO does not have the grouping effect and EN (as shown here) does not achieverobust sparse recovery. Therefore the CLOT formulation combines the bestfeatures of both LASSO (robust sparse recovery) and EN (grouping effect). The CLOT formulation is a special case of another one called SGL (SparseGroup LASSO) which was introduced into the literature previously, but withoutany analysis of either the grouping effect or robust sparse recovery. It isshown here that SGL achieves robust sparse recovery, and also achieves aversion of the grouping effect in that coefficients of highly correlatedcolumns of the measurement (or design) matrix are assigned roughly comparablevalues, \textit{if the columns belong to the same group}.
arxiv-16200-209 | Towards Building an RGBD-M Scanner | http://arxiv.org/pdf/1603.03875v1.pdf | author:Zhe Wu, Sai-Kit Yeung, Ping Tan category:cs.CV published:2016-03-12 summary:We present a portable device to capture both shape and reflectance of anindoor scene. Consisting of a Kinect, an IR camera and several IR LEDs, ourdevice allows the user to acquire data in a similar way as he/she scans with asingle Kinect. Scene geometry is reconstructed by KinectFusion. To estimatereflectance from incomplete and noisy observations, 3D vertices of the samematerial are identified by our material segmentation propagation algorithm.Then BRDF observations at these vertices are merged into a more complete andaccurate BRDF for the material. Effectiveness of our device is demonstrated byquality results on real-world scenes.
arxiv-16200-210 | Neural Discourse Relation Recognition with Semantic Memory | http://arxiv.org/pdf/1603.03873v1.pdf | author:Biao Zhang, Deyi Xiong, Jinsong Su category:cs.CL published:2016-03-12 summary:Humans comprehend the meanings and relations of discourses heavily relying ontheir semantic memory that encodes general knowledge about concepts and facts.Inspired by this, we propose a neural recognizer for implicit discourserelation analysis, which builds upon a semantic memory that stores knowledge ina distributed fashion. We refer to this recognizer as SeMDER. Starting fromword embeddings of discourse arguments, SeMDER employs a shallow encoder togenerate a distributed surface representation for a discourse. A semanticencoder with attention to the semantic memory matrix is further establishedover surface representations. It is able to retrieve a deep semantic meaningrepresentation for the discourse from the memory. Using the surface andsemantic representations as input, SeMDER finally predicts implicit discourserelations via a neural recognizer. Experiments on the benchmark data set showthat SeMDER benefits from the semantic memory and achieves substantialimprovements of 2.56\% on average over current state-of-the-art baselines interms of F1-score.
arxiv-16200-211 | Deconvolutional Feature Stacking for Weakly-Supervised Semantic Segmentation | http://arxiv.org/pdf/1602.04984v3.pdf | author:Hyo-Eun Kim, Sangheum Hwang category:cs.CV published:2016-02-16 summary:A weakly-supervised semantic segmentation framework with a tieddeconvolutional neural network is presented. Each deconvolution layer in theframework consists of unpooling and deconvolution operations. 'Unpooling'upsamples the input feature map based on unpooling switches defined bycorresponding convolution layer's pooling operation. 'Deconvolution' convolvesthe input unpooled features by using convolutional weights tied with thecorresponding convolution layer's convolution operation. Theunpooling-deconvolution combination helps to eliminate less discriminativefeatures in a feature extraction stage, since output features of thedeconvolution layer are reconstructed from the most discriminative unpooledfeatures instead of the raw one. This results in reduction of false positivesin a pixel-level inference stage. All the feature maps restored from the entiredeconvolution layers can constitute a rich discriminative feature set accordingto different abstraction levels. Those features are stacked to be selectivelyused for generating class-specific activation maps. Under the weak supervision(image-level labels), the proposed framework shows promising results on lesionsegmentation in medical images (chest X-rays) and achieves state-of-the-artperformance on the PASCAL VOC segmentation dataset in the same experimentalcondition.
arxiv-16200-212 | Real-time 3D scene description using Spheres, Cones and Cylinders | http://arxiv.org/pdf/1603.03856v1.pdf | author:Kristiyan Georgiev, Motaz Al-Hami, Rolf Lakaemper category:cs.CV published:2016-03-12 summary:The paper describes a novel real-time algorithm for finding 3D geometricprimitives (cylinders, cones and spheres) from 3D range data. In its core, itperforms a fast model fitting with a model update in constant time (O(1)) foreach new data point added to the model. We use a three stage approach.The firststep inspects 1.5D sub spaces, to find ellipses. The next stage uses theseellipses as input by examining their neighborhood structure to form sets ofcandidates for the 3D geometric primitives. Finally, candidate ellipses arefitted to the geometric primitives. The complexity for point processing isO(n); additional time of lower order is needed for working on significantlysmaller amount of mid-level objects. This allows the approach to process 30frames per second on Kinect depth data, which suggests this approach as apre-processing step for 3D real-time higher level tasks in robotics, liketracking or feature based mapping.
arxiv-16200-213 | Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks | http://arxiv.org/pdf/1603.03827v1.pdf | author:Ji Young Lee, Franck Dernoncourt category:cs.CL cs.AI cs.LG cs.NE stat.ML published:2016-03-12 summary:Recent approaches based on artificial neural networks (ANNs) have shownpromising results for short-text classification. However, many short textsoccur in sequences (e.g., sentences in a document or utterances in a dialog),and most existing ANN-based systems do not leverage the preceding short textswhen classifying a subsequent one. In this work, we present a model based onrecurrent neural networks and convolutional neural networks that incorporatesthe preceding short texts. Our model achieves state-of-the-art results on threedifferent datasets for dialog act prediction.
arxiv-16200-214 | Personalized Speech recognition on mobile devices | http://arxiv.org/pdf/1603.03185v2.pdf | author:Ian McGraw, Rohit Prabhavalkar, Raziel Alvarez, Montse Gonzalez Arenas, Kanishka Rao, David Rybach, Ouais Alsharif, Hasim Sak, Alexander Gruenstein, Francoise Beaufays, Carolina Parada category:cs.CL cs.LG cs.SD published:2016-03-10 summary:We describe a large vocabulary speech recognition system that is accurate,has low latency, and yet has a small enough memory and computational footprintto run faster than real-time on a Nexus 5 Android smartphone. We employ aquantized Long Short-Term Memory (LSTM) acoustic model trained withconnectionist temporal classification (CTC) to directly predict phonemetargets, and further reduce its memory footprint using an SVD-based compressionscheme. Additionally, we minimize our memory footprint by using a singlelanguage model for both dictation and voice command domains, constructed usingBayesian interpolation. Finally, in order to properly handle device-specificinformation, such as proper names and other context-dependent information, weinject vocabulary items into the decoder graph and bias the language modelon-the-fly. Our system achieves 13.5% word error rate on an open-endeddictation task, running with a median speed that is seven times faster thanreal-time.
arxiv-16200-215 | Provable Non-convex Phase Retrieval with Outliers: Median Truncated Wirtinger Flow | http://arxiv.org/pdf/1603.03805v1.pdf | author:Huishuai Zhang, Yuejie Chi, Yingbin Liang category:stat.ML published:2016-03-11 summary:Solving systems of quadratic equations is a central problem in machinelearning and signal processing. One important example is phase retrieval, whichaims to recover a signal from only magnitudes of its linear measurements. Thispaper focuses on the situation when the measurements are corrupted by arbitraryoutliers, for which the recently developed non-convex gradient descentWirtinger flow (WF) and truncated Wirtinger flow (TWF) algorithms likely fail.We develop a novel median-TWF algorithm that exploits robustness of samplemedian to resist arbitrary outliers in the initialization and the gradientupdate in each iteration. We show that such a non-convex algorithm provablyrecovers the signal from a near-optimal number of measurements composed ofi.i.d. Gaussian entries, up to a logarithmic factor, even when a constantportion of the measurements are corrupted by arbitrary outliers. We furthershow that median-TWF is also robust when measurements are corrupted by botharbitrary outliers and bounded noise. Our analysis of performance guarantee isaccomplished by development of non-trivial concentration measures ofmedian-related quantities, which may be of independent interest. We furtherprovide numerical experiments to demonstrate the effectiveness of the approach.
arxiv-16200-216 | Demonstrating the Feasibility of Automatic Game Balancing | http://arxiv.org/pdf/1603.03795v1.pdf | author:Vanessa Volz, Günter Rudolph, Boris Naujoks category:cs.HC cs.AI cs.NE published:2016-03-11 summary:Game balancing is an important part of the (computer) game design process, inwhich designers adapt a game prototype so that the resulting gameplay is asentertaining as possible. In industry, the evaluation of a game is often basedon costly playtests with human players. It suggests itself to automate thisprocess using surrogate models for the prediction of gameplay and outcome. Inthis paper, the feasibility of automatic balancing using simulation- anddeck-based objectives is investigated for the card game top trumps.Additionally, the necessity of a multi-objective approach is asserted by acomparison with the only known (single-objective) method. We apply amulti-objective evolutionary algorithm to obtain decks that optimiseobjectives, e.g. win rate and average number of tricks, developed to expressthe fairness and the excitement of a game of top trumps. The results arecompared with decks from published top trumps decks using simulation-basedobjectives. The possibility to generate decks better or at least as good asdecks from published top trumps decks in terms of these objectives isdemonstrated. Our results indicate that automatic balancing with the presentedapproach is feasible even for more complex games such as real-time strategygames.
arxiv-16200-217 | Training with Exploration Improves a Greedy Stack-LSTM Parser | http://arxiv.org/pdf/1603.03793v1.pdf | author:Miguel Ballesteros, Yoav Goldberg, Chris Dyer, Noah A. Smith category:cs.CL published:2016-03-11 summary:We adapt the greedy Stack-LSTM dependency parser of Dyer et al. (2015) tosupport a training-with-exploration procedure using dynamic oracles(Goldbergand Nivre, 2013) instead of cross-entropy minimization. This form of training,which accounts for model predictions at training time rather than assuming anerror-free action history, improves parsing accuracies for both English andChinese, obtaining very strong results for both languages. We discuss somemodifications needed in order to get training with exploration to work well fora probabilistic neural-network.
arxiv-16200-218 | A Primer on the Signature Method in Machine Learning | http://arxiv.org/pdf/1603.03788v1.pdf | author:Ilya Chevyrev, Andrey Kormilitzin category:stat.ML cs.LG stat.ME published:2016-03-11 summary:In these notes, we wish to provide an introduction to the signature method,focusing on its basic theoretical properties and recent numerical applications. The notes are split into two parts. The first part focuses on the definitionand fundamental properties of the signature of a path, or the path signature.We have aimed for a minimalistic approach, assuming only familiarity withclassical real analysis and integration theory, and supplementing theory withstraightforward examples. We have chosen to focus in detail on the principleproperties of the signature which we believe are fundamental to understandingits role in applications. We also present an informal discussion on some of itsdeeper properties and briefly mention the role of the signature in rough pathstheory, which we hope could serve as a light introduction to rough paths forthe interested reader. The second part of these notes discusses practical applications of the pathsignature to the area of machine learning. The signature approach represents anon-parametric way for extraction of characteristic features from data. Thedata are converted into a multi-dimensional path by means of various embeddingalgorithms and then processed for computation of individual terms of thesignature which summarise certain information contained in the data. Thesignature thus transforms raw data into a set of features which are used inmachine learning tasks. We will review current progress in applications ofsignatures to machine learning problems.
arxiv-16200-219 | Towards using social media to identify individuals at risk for preventable chronic illness | http://arxiv.org/pdf/1603.03784v1.pdf | author:Dane Bell, Daniel Fried, Luwen Huangfu, Mihai Surdeanu, Stephen Kobourov category:cs.CL cs.CY cs.SI published:2016-03-11 summary:We describe a strategy for the acquisition of training data necessary tobuild a social-media-driven early detection system for individuals at risk for(preventable) type 2 diabetes mellitus (T2DM). The strategy uses a game-likequiz with data and questions acquired semi-automatically from Twitter. Thequestions are designed to inspire participant engagement and collect relevantdata to train a public-health model applied to individuals. Prior systemsdesigned to use social media such as Twitter to predict obesity (a risk factorfor T2DM) operate on entire communities such as states, counties, or cities,based on statistics gathered by government agencies. Because there isconsiderable variation among individuals within these groups, training data onthe individual level would be more effective, but this data is difficult toacquire. The approach proposed here aims to address this issue. Our strategyhas two steps. First, we trained a random forest classifier on data gatheredfrom (public) Twitter statuses and state-level statistics with state-of-the-artaccuracy. We then converted this classifier into a 20-questions-style quiz andmade it available online. In doing so, we achieved high engagement withindividuals that took the quiz, while also building a training set ofvoluntarily supplied individual-level data for future classification.
arxiv-16200-220 | Region Graph Based Method for Multi-Object Detection and Tracking using Depth Cameras | http://arxiv.org/pdf/1603.03783v1.pdf | author:Sachin Mehta, Balakrishnan Prabhakaran category:cs.CV published:2016-03-11 summary:In this paper, we propose a multi-object detection and tracking method usingdepth cameras. Depth maps are very noisy and obscure in object detection. Wefirst propose a region-based method to suppress high magnitude noise whichcannot be filtered using spatial filters. Second, the proposed method detectRegion of Interests by temporal learning which are then tracked using weightedgraph-based approach. We demonstrate the performance of the proposed method onstandard depth camera datasets with and without object occlusions. Experimentalresults show that the proposed method is able to suppress high magnitude noisein depth maps and detect/track the objects (with and without occlusion).
arxiv-16200-221 | An investigation of coreference phenomena in the biomedical domain | http://arxiv.org/pdf/1603.03758v1.pdf | author:Dane Bell, Gus Hahn-Powell, Marco A. Valenzuela-Escárcega, Mihai Surdeanu category:cs.CL published:2016-03-11 summary:We describe challenges and advantages unique to coreference resolution in thebiomedical domain, and a sieve-based architecture that leverages domainknowledge for both entity and event coreference resolution. Domain-generalcoreference resolution algorithms perform poorly on biomedical documents,because the cues they rely on such as gender are largely absent in this domain,and because they do not encode domain-specific knowledge such as the number andtype of participants required in chemical reactions. Moreover, it is difficultto directly encode this knowledge into most coreference resolution algorithmsbecause they are not rule-based. Our rule-based architecture uses sequentiallyapplied hand-designed "sieves", with the output of each sieve informing andconstraining subsequent sieves. This architecture provides a 3.2% increase inthroughput to our Reach event extraction system with precision parallel to thatof the stricter system that relies solely on syntactic patterns for extraction.
arxiv-16200-222 | Efficient Clustering of Correlated Variables and Variable Selection in High-Dimensional Linear Models | http://arxiv.org/pdf/1603.03724v1.pdf | author:Niharika Gauraha, Swapan K. Parui category:stat.ML cs.LG published:2016-03-11 summary:In this paper, we introduce Adaptive Cluster Lasso(ACL) method for variableselection in high dimensional sparse regression models with strongly correlatedvariables. To handle correlated variables, the concept of clustering orgrouping variables and then pursuing model fitting is widely accepted. When thedimension is very high, finding an appropriate group structure is as difficultas the original problem. The ACL is a three-stage procedure where, at the firststage, we use the Lasso(or its adaptive or thresholded version) to do initialselection, then we also include those variables which are not selected by theLasso but are strongly correlated with the variables selected by the Lasso. Atthe second stage we cluster the variables based on the reduced set ofpredictors and in the third stage we perform sparse estimation such as Lasso oncluster representatives or the group Lasso based on the structures generated byclustering procedure. We show that our procedure is consistent and efficient infinding true underlying population group structure(under assumption ofirrepresentable and beta-min conditions). We also study the group selectionconsistency of our method and we support the theory using simulated andpseudo-real dataset examples.
arxiv-16200-223 | Group Equivariant Convolutional Networks | http://arxiv.org/pdf/1602.07576v2.pdf | author:Taco S. Cohen, Max Welling category:cs.LG stat.ML published:2016-02-24 summary:We introduce Group equivariant Convolutional Neural Networks (G-CNNs), anatural generalization of convolutional neural networks that reduces samplecomplexity by exploiting symmetries. By convolving over groups larger than thetranslation group, G-CNNs build representations that are equivariant to thesegroups, which makes it possible to greatly increase the degree of parametersharing. We show how G-CNNs can be implemented with negligible computationaloverhead for discrete groups such as the group of translations, reflections androtations by multiples of 90 degrees. G-CNNs achieve state of the art resultson rotated MNIST and significantly improve over a competitive baseline onaugmented and non-augmented CIFAR-10.
arxiv-16200-224 | Distribution Free Learning with Local Queries | http://arxiv.org/pdf/1603.03714v1.pdf | author:Galit Bary-Weisberg, Amit Daniely, Shai Shalev-Shwartz category:cs.LG published:2016-03-11 summary:The model of learning with \emph{local membership queries} interpolatesbetween the PAC model and the membership queries model by allowing the learnerto query the label of any example that is similar to an example in the trainingset. This model, recently proposed and studied by Awasthi, Feldman and Kanade,aims to facilitate practical use of membership queries. We continue this line of work, proving both positive and negative results inthe {\em distribution free} setting. We restrict to the boolean cube $\{-1,1\}^n$, and say that a query is $q$-local if it is of a hamming distance $\leq$ from some training example. On the positive side, we show that $1$-localqueries already give an additional strength, and allow to learn a certain typeof DNF formulas. On the negative side, we show that even$\left(n^{0.99}\right)$-local queries cannot help to learn various classesincluding Automata, DNFs and more. Likewise, $q$-local queries for any constant$q$ cannot help to learn Juntas, Decision Trees, Sparse Polynomials and more.Moreover, for these classes, an algorithm that uses$\left(\log^{0.99}(n)\right)$-local queries would lead to a breakthrough in thebest known running times.
arxiv-16200-225 | Cost-sensitive Learning for Bidding in Online Advertising Auctions | http://arxiv.org/pdf/1603.03713v1.pdf | author:Flavian Vasile, Damien Lefortier category:cs.LG published:2016-03-11 summary:One of the most challenging problems in computational advertising is theprediction of ad click and conversion rates for bidding in online advertisingauctions. State-of- the-art prediction methods include using the maximumentropy framework (also known as logistic regression) and log linear models.However, one unaddressed problem in the previous approaches is the existence ofhighly non-uniform misprediction costs. In this paper, we present our approachfor making cost-sensitive predictions for bidding in online advertisingauctions. We show that one can get significant lifts in offline and onlineperformance by using a simple modification of the logistic loss function.
arxiv-16200-226 | Searching for Topological Symmetry in Data Haystack | http://arxiv.org/pdf/1603.03703v1.pdf | author:Kallol Roy, Anh Tong, Jaesik Choi category:cs.LG published:2016-03-11 summary:Finding interesting symmetrical topological structures in high-dimensionalsystems is an important problem in statistical machine learning. Limited amountof available high-dimensional data and its sensitivity to noise posecomputational challenges to find symmetry. Our paper presents a new method tofind local symmetries in a low-dimensional 2-D grid structure which is embeddedin high-dimensional structure. To compute the symmetry in a grid structure, weintroduce three legal grid moves (i) Commutation (ii) Cyclic Permutation (iii)Stabilization on sets of local grid squares, grid blocks. The three grid movesare legal transformations as they preserve the statistical distribution ofhamming distances in each grid block. We propose and coin the term of gridsymmetry of data on the 2-D data grid as the invariance of statisticaldistributions of hamming distance are preserved after a sequence of grid moves.We have computed and analyzed the grid symmetry of data on multivariateGaussian distributions and Gamma distributions with noise.
arxiv-16200-227 | Determination of the edge of criticality in echo state networks through Fisher information maximization | http://arxiv.org/pdf/1603.03685v1.pdf | author:Lorenzo Livi, Filippo Maria Bianchi, Cesare Alippi category:cs.LG cs.NE published:2016-03-11 summary:It is a widely accepted fact that the computational capability of recurrentneural networks is maximized on the so-called "edge of criticality". Once inthis configuration, the network performs efficiently on a specific applicationboth in terms of (i) low prediction error and (ii) high short-term memorycapacity. Since the behavior of recurrent networks is strongly influenced bythe particular input signal driving the dynamics, a universal,application-independent method for determining the edge of criticality is stillmissing. In this paper, we propose a theoretically motivated method based onFisher information for determining the edge of criticality in recurrent neuralnetworks. It is proven that Fisher information is maximized for (finite-size)systems operating in such critical regions. However, Fisher information isnotoriously difficult to compute and either requires the probability densityfunction or the conditional dependence of the system states with respect to themodel parameters. The paper exploits a recently-developed non-parametricestimator of the Fisher information matrix and provides a method to determinethe critical region of echo state networks, a particular class of recurrentnetworks. The considered control parameters, which indirectly affect the echostate network performance, are suitably controlled to identify a collection ofnetwork configurations lying on the edge of criticality and, as such,maximizing Fisher information and computational performance.
arxiv-16200-228 | Distance Metric Tracking | http://arxiv.org/pdf/1603.03678v1.pdf | author:Kristjan Greenewald, Stephen Kelley, Alfred Hero category:stat.ML cs.LG published:2016-03-11 summary:Recent work in distance metric learning focused on learning transformationsof data that best align with provided sets of pairwise similarity anddissimilarity constraints. The learned transformations lead to improvedretrieval, classification, and clustering algorithms due to the more accuratedistance or similarity measures. Here, we introduce the problem of learningthese transformations when the underlying constraint generation process isdynamic. These dynamics can be due to changes in either the ground-truth labelsused to generate constraints or changes to the feature subspaces in which theclass structure is apparent. We propose and evaluate an adaptive, onlinealgorithm for learning and tracking metrics as they change over time. Wedemonstrate the proposed algorithm on both real and synthetic data sets andshow significant performance improvements relative to previously proposed batchand online distance metric learning algorithms.
arxiv-16200-229 | Learning Gaze Transitions from Depth to Improve Video Saliency Estimation | http://arxiv.org/pdf/1603.03669v1.pdf | author:G. Leifman, D. Rudoy, T. Swedish, E. Bayro-Corrochano, R. Raskar category:cs.CV published:2016-03-11 summary:In this paper we introduce a novel Depth-Aware Video Saliency approach topredict human focus of attention when viewing RGBD videos on regular 2Dscreens. We train a generative convolutional neural network which predicts asaliency map for a frame, given the fixation map of the previous frame.Saliency estimation in this scenario is highly important since in the nearfuture 3D video content will be easily acquired and yet hard to display. Thiscan be explained, on the one hand, by the dramatic improvement of 3D-capableacquisition equipment. On the other hand, despite the considerable progress in3D display technologies, most of the 3D displays are still expensive andrequire wearing special glasses. To evaluate the performance of our approach,we present a new comprehensive database of eye-fixation ground-truth for RGBDvideos. Our experiments indicate that integrating depth into video saliencycalculation is beneficial. We demonstrate that our approach outperformsstate-of-the-art methods for video saliency, achieving 15% relativeimprovement.
arxiv-16200-230 | Efficient forward propagation of time-sequences in convolutional neural networks using Deep Shifting | http://arxiv.org/pdf/1603.03657v1.pdf | author:Koen Groenland, Sander Bohte category:cs.LG cs.CV cs.NE published:2016-03-11 summary:When a Convolutional Neural Network is used for on-the-fly evaluation ofcontinuously updating time-sequences, many redundant convolution operations areperformed. We propose the method of Deep Shifting, which remembers previouslycalculated results of convolution operations in order to minimize the number ofcalculations. The reduction in complexity is at least a constant and in thebest case quadratic. We demonstrate that this method does indeed savesignificant computation time in a practical implementation, especially when thenetworks receives a large number of time-frames.
arxiv-16200-231 | Square Root Graphical Models: Multivariate Generalizations of Univariate Exponential Families that Permit Positive Dependencies | http://arxiv.org/pdf/1603.03629v1.pdf | author:David I. Inouye, Pradeep Ravikumar, Inderjit S. Dhillon category:stat.ML published:2016-03-11 summary:We develop a novel class of parametric graphical models, called Square RootGraphical Models (SQR), that provides multivariate generalizations ofunivariate exponential family distributions---e.g. discrete, Gaussian,exponential and Poisson distributions. Previous multivariate graphical modelsdid not allow positive dependencies for the exponential and Poissongeneralizations. However, in many real-world datasets, variables clearly havepositive dependencies. For example, the airport delay time in NewYork---modeled as an exponential distribution---is positively related to thedelay time in Boston. With this motivation, we give an example of our modelclass derived from the univariate exponential distribution that allows foralmost arbitrary positive and negative dependencies with only a mild conditionon the parameter matrix---a condition akin to the positive definiteness of theGaussian covariance matrix. Our Poisson generalization allows for both positiveand negative dependencies without any constraints on the parameter values. Wealso develop parameter estimation methods using node-wise regressions with$\ell_1$ regularization and likelihood approximation methods using sampling.Finally, we demonstrate our exponential generalization on a dataset of airportdelay times.
arxiv-16200-232 | Learning from Imbalanced Multiclass Sequential Data Streams Using Dynamically Weighted Conditional Random Fields | http://arxiv.org/pdf/1603.03627v1.pdf | author:Roberto L. Shinmoto Torres, Damith C. Ranasinghe, Qinfeng Shi, Anton van den Hengel category:cs.LG published:2016-03-11 summary:The present study introduces a method for improving the classificationperformance of imbalanced multiclass data streams from wireless body wornsensors. Data imbalance is an inherent problem in activity recognition causedby the irregular time distribution of activities, which are sequential anddependent on previous movements. We use conditional random fields (CRF), agraphical model for structured classification, to take advantage ofdependencies between activities in a sequence. However, CRFs do not considerthe negative effects of class imbalance during training. We propose aclass-wise dynamically weighted CRF (dWCRF) where weights are automaticallydetermined during training by maximizing the expected overall F-score. Ourresults based on three case studies from a healthcare application using abatteryless body worn sensor, demonstrate that our method, in general, improvesoverall and minority class F-score when compared to other CRF based classifiersand achieves similar or better overall and class-wise performance when comparedto SVM based classifiers under conditions of limited training data. We alsoconfirm the performance of our approach using an additional battery poweredbody worn sensor dataset, achieving similar results in cases of high classimbalance.
arxiv-16200-233 | Designing labeled graph classifiers by exploiting the Rényi entropy of the dissimilarity representation | http://arxiv.org/pdf/1408.5286v4.pdf | author:Lorenzo Livi category:cs.CV cs.IT math.IT stat.ML published:2014-08-22 summary:Representing patterns by complex relational structures, such as labeledgraphs, is becoming an increasingly common practice in the broad field ofcomputational intelligence. Accordingly, a wide repertoire of patternrecognition tools, such as classifiers and knowledge discovery procedures, arenowadays available and tested for various labeled graph data types. However,the design of effective learning and mining procedures operating in the spaceof labeled graphs is still a challenging problem, especially from thecomputational complexity viewpoint. In this paper, we present a majorimprovement of a general-purpose graph classification system, which isconceived on an interplay among dissimilarity representation, clustering,information-theoretic techniques, and evolutionary optimization. Theimprovement focuses on a specific key subroutine of the system that performsthe compression of the input data. We prove different theorems which arefundamental to the setting of such a compression operation. We demonstrate theeffectiveness of the resulting classifier by benchmarking the developedvariants on well-known datasets of labeled graphs, considering as distinctperformance indicators the classification accuracy, the computing time, and theparsimony in terms of structural complexity of the synthesized classificationmodel. Overall, the results show state-of-the-art standards in terms of testset accuracy, while achieving considerable reductions for what concerns boththe effective computing time and model complexity.
arxiv-16200-234 | Relief R-CNN : Utilizing Convolutional Feature Interrelationship for Object Detection | http://arxiv.org/pdf/1601.06719v2.pdf | author:Guiying Li, Junlong Liu, Chunhui Jiang, Liangpeng Zhang, Ke Tang, Yufeng Liu category:cs.CV published:2016-01-25 summary:The state-of-the-art object detection pipeline needs a set of object locationhypotheses followed by a deep CNN classifier. Previous research on thisparadigm, usually extracts features from the same image for the two stepsseparately, which is time consuming and is hard to optimize. This work showsthat the high-level patterns of feature values in deep convolutional featuremap contain plenty of useful spatial information and proposes a new deeplearning approach to object detection, namely Relief R-CNN ($R^2$-CNN). Byextracting positions of objects from these high-level patterns, $R^2$-CNNgenerates region proposals and performs deep classification simultaneouslyusing the same forward CNN features, unifying the formerly separated objectdetection process. In this way, $R^2$-CNN does not involve additionalinformation extraction process for region proposal generation, considerablyreducing the total computation costs. In addition, a recursive fine-tunetechnique is also developed for refining coarse proposals. Empirical resultsshowed that our $R^2$-CNN had a very high speed and a reasonable detection rateeven in the presence of limit on the proposal number, indicating that theregion proposals generated by our $R^2$-CNN are in excellent quality.
arxiv-16200-235 | A Recursive Born Approach to Nonlinear Inverse Scattering | http://arxiv.org/pdf/1603.03768v1.pdf | author:Ulugbek S. Kamilov, Dehong Liu, Hassan Mansour, Petros T. Boufounos category:cs.LG physics.optics published:2016-03-11 summary:The Iterative Born Approximation (IBA) is a well-known method for describingwaves scattered by semi-transparent objects. In this paper, we present a novelnonlinear inverse scattering method that combines IBA with an edge-preservingtotal variation (TV) regularizer. The proposed method is obtained by relatingiterations of IBA to layers of a feedforward neural network and developing acorresponding error backpropagation algorithm for efficiently estimating thepermittivity of the object. Simulations illustrate that, by accounting formultiple scattering, the method successfully recovers the permittivitydistribution where the traditional linear inverse scattering fails.
arxiv-16200-236 | A short proof that $O_2$ is an MCFL | http://arxiv.org/pdf/1603.03610v1.pdf | author:Mark-Jan Nederhof category:cs.FL cs.CL 68T50 I.2.7; F.4.2 published:2016-03-11 summary:We present a new proof that $O_2$ is a multiple context-free language. Itcontrasts with a recent proof by Salvati (2015) in its avoidance of conceptsthat seem specific to two-dimensional geometry, such as the complex exponentialfunction. Our simple proof creates realistic prospects of widening the resultsto higher dimensions. This finding is of central importance to the relationbetween extreme free word order and classes of grammars used to describe thesyntax of natural language.
arxiv-16200-237 | Optimized Polynomial Evaluation with Semantic Annotations | http://arxiv.org/pdf/1603.01520v3.pdf | author:Daniel Rubio Bonilla, Colin W. Glass, Jan Kuper category:cs.PL cs.CL B.1.4 published:2016-03-04 summary:In this paper we discuss how semantic annotations can be used to introducemathematical algorithmic information of the underlying imperative code toenable compilers to produce code transformations that will enable betterperformance. By using this approaches not only good performance is achieved,but also better programmability, maintainability and portability acrossdifferent hardware architectures. To exemplify this we will use polynomialequations of different degrees.
arxiv-16200-238 | Maximum Entropy Deep Inverse Reinforcement Learning | http://arxiv.org/pdf/1507.04888v3.pdf | author:Markus Wulfmeier, Peter Ondruska, Ingmar Posner category:cs.LG published:2015-07-17 summary:This paper presents a general framework for exploiting the representationalcapacity of neural networks to approximate complex, nonlinear reward functionsin the context of solving the inverse reinforcement learning (IRL) problem. Weshow in this context that the Maximum Entropy paradigm for IRL lends itselfnaturally to the efficient training of deep architectures. At test time, theapproach leads to a computational complexity independent of the number ofdemonstrations, which makes it especially well-suited for applications inlife-long learning scenarios. Our approach achieves performance commensurate tothe state-of-the-art on existing benchmarks while exceeding on an alternativebenchmark based on highly varying reward structures. Finally, we extend thebasic architecture - which is equivalent to a simplified subclass of FullyConvolutional Neural Networks (FCNNs) with width one - to include largerconvolutions in order to eliminate dependency on precomputed spatial featuresand work on raw input representations.
arxiv-16200-239 | Fast Optical Flow using Dense Inverse Search | http://arxiv.org/pdf/1603.03590v1.pdf | author:Till Kroeger, Radu Timofte, Dengxin Dai, Luc Van Gool category:cs.CV cs.RO published:2016-03-11 summary:Most recent works in optical flow extraction focus on the accuracy andneglect the time complexity. However, in real-life visual applications, such astracking, activity detection and recognition, the time complexity is critical. We propose a solution with very low time complexity and competitive accuracyfor the computation of dense optical flow. It consists of three parts: 1)inverse search for patch correspondences; 2) dense displacement field creationthrough patch aggregation along multiple scales; 3) variational refinement. Atthe core of our Dense Inverse Search-based method (DIS) is the efficient searchof correspondences inspired by the inverse compositional image alignmentproposed by Baker and Matthews in 2001. DIS is competitive on standard optical flow benchmarks with largedisplacements. DIS runs at 300Hz up to 600Hz on a single CPU core, reaching thetemporal resolution of human's biological vision system. It is order(s) ofmagnitude faster than state-of-the-art methods in the same range of accuracy,making DIS ideal for visual applications.
arxiv-16200-240 | Learning with Clustering Penalties | http://arxiv.org/pdf/1506.04908v2.pdf | author:Vincent Roulet, Fajwel Fogel, Alexandre d'Aspremont, Francis Bach category:cs.LG 68T05, 91C20 published:2015-06-16 summary:We study supervised learning problems using clustering penalties to imposestructure on either features, tasks or samples, seeking to help both predictionand interpretation. This arises naturally in problems involving dimensionalityreduction, transfer learning or regression clustering. We derive a unifiedoptimization formulation handling these three settings and produce algorithmswhose core iteration complexity amounts to a k-means clustering step, which canbe approximated efficiently. We test the robustness of our methods onartificial data sets as well as real data extracted from movie reviews and acorpus of text documents.
arxiv-16200-241 | Template Matching on the Roto-Translation Group | http://arxiv.org/pdf/1603.03304v2.pdf | author:Erik J. Bekkers, Marco Loog, Bart M. ter Haar Romeny, Remco Duits category:cs.CV math.GR published:2016-03-10 summary:We propose a template matching method for the detection of 2D image objectsthat are characterized by orientation patterns. Our method is based on datarepresentations via orientation scores, which are functions on the space ofpositions and orientations, and which are obtained via a wavelet-typetransform. This new representation allows us to detect orientation patterns inan intuitive and direct way, namely via cross-correlations. Additionally, wepropose a generalized linear regression framework for the construction ofsuitable templates using smoothing splines. Here, it is important to recognizea curved geometry on the position-orientation domain, which we identify withthe Lie group SE(2): the roto-translation group. Templates are then optimizedin a B-spline basis, and smoothness is defined with respect to the curvedgeometry. We achieve state-of-the-art results on two important detectionproblems in retinal imaging: detection of the optic nerve head (99.83\% successrate on 1737 images) and detection of the fovea (99.32\% success rate on 1616images). The high performance is due to inclusion of both intensity andorientation features with effective geometric priors in the template matching.Moreover, our method is fast due to a cross-correlation based matchingapproach.
arxiv-16200-242 | Watch-n-Patch: Unsupervised Learning of Actions and Relations | http://arxiv.org/pdf/1603.03541v1.pdf | author:Chenxia Wu, Jiemi Zhang, Ozan Sener, Bart Selman, Silvio Savarese, Ashutosh Saxena category:cs.CV cs.LG cs.RO published:2016-03-11 summary:There is a large variation in the activities that humans perform in theireveryday lives. We consider modeling these composite human activities whichcomprises multiple basic level actions in a completely unsupervised setting.Our model learns high-level co-occurrence and temporal relations between theactions. We consider the video as a sequence of short-term action clips, whichcontains human-words and object-words. An activity is about a set ofaction-topics and object-topics indicating which actions are present and whichobjects are interacting with. We then propose a new probabilistic modelrelating the words and the topics. It allows us to model long-range actionrelations that commonly exist in the composite activities, which is challengingin previous works. We apply our model to the unsupervised action segmentationand clustering, and to a novel application that detects forgotten actions,which we call action patching. For evaluation, we contribute a new challengingRGB-D activity video dataset recorded by the new Kinect v2, which containsseveral human daily activities as compositions of multiple actions interactingwith different objects. Moreover, we develop a robotic system that watchespeople and reminds people by applying our action patching algorithm. Ourrobotic setup can be easily deployed on any assistive robot.
arxiv-16200-243 | Dimension Coupling: Optimal Active Learning of Halfspaces via Query Synthesis | http://arxiv.org/pdf/1603.03515v1.pdf | author:Lin Chen, Hamed Hassani, Amin Karbasi category:cs.AI cs.IT cs.LG math.IT published:2016-03-11 summary:In this paper, we consider the problem of actively learning a linearclassifier through query synthesis where the learner can construct artificialqueries in order to estimate the true decision boundaries. This problem hasrecently gained a lot of interest in automated science and adversarial reverseengineering for which only heuristic algorithms are known. In suchapplications, queries can be constructed de novo to elicit information (e.g.,automated science) or to evade detection with minimal cost (e.g., adversarialreverse engineering). We develop a general framework, called dimension coupling (DC), that 1)reduces a d-dimensional learning problem to d-1 low-dimensional sub-problems,2) solves each sub-problem efficiently, and 3) appropriately aggregates theresults and outputs a linear classifier. We consider the three most commonscenarios in the literature: idealized noise-free, independent noiserealizations, and agnostic settings. We show that the DC framework avoids thecurse of dimensionality: its computational complexity in all three cases scaleslinearly with the dimension. Moreover, in the noiseless and noisy cases, weshow that the query complexity of DC is near optimal (within a constant factorof the optimum algorithm). We also develop an agnostic variant of DC for whichwe provide strong theoretical guarantees. To further support our theoreticalanalysis, we compare the performance of DC with the existing work in all threesettings. We observe that DC consistently outperforms the prior arts in termsof query complexity while often running orders of magnitude faster.
arxiv-16200-244 | Embedding Label Structures for Fine-Grained Feature Representation | http://arxiv.org/pdf/1512.02895v2.pdf | author:Xiaofan Zhang, Feng Zhou, Yuanqing Lin, Shaoting Zhang category:cs.CV published:2015-12-09 summary:Recent algorithms in convolutional neural networks (CNN) considerably advancethe fine-grained image classification, which aims to differentiate subtledifferences among subordinate classes. However, previous studies have rarelyfocused on learning a fined-grained and structured feature representation thatis able to locate similar images at different levels of relevance, e.g.,discovering cars from the same make or the same model, both of which requirehigh precision. In this paper, we propose two main contributions to tackle thisproblem. 1) A multi-task learning framework is designed to effectively learnfine-grained feature representations by jointly optimizing both classificationand similarity constraints. 2) To model the multi-level relevance, labelstructures such as hierarchy or shared attributes are seamlessly embedded intothe framework by generalizing the triplet loss. Extensive and thoroughexperiments have been conducted on three fine-grained datasets, i.e., theStanford car, the car-333, and the food datasets, which contain eitherhierarchical labels or shared attributes. Our proposed method has achieved verycompetitive performance, i.e., among state-of-the-art classification accuracy.More importantly, it significantly outperforms previous fine-grained featurerepresentations for image retrieval at different levels of relevance.
arxiv-16200-245 | Temporal Convolutional Neural Networks for Diagnosis from Lab Tests | http://arxiv.org/pdf/1511.07938v4.pdf | author:Narges Razavian, David Sontag category:cs.LG published:2015-11-25 summary:Early diagnosis of treatable diseases is essential for improving healthcare,and many diseases' onsets are predictable from annual lab tests and theirtemporal trends. We introduce a multi-resolution convolutional neural networkfor early detection of multiple diseases from irregularly measured sparse labvalues. Our novel architecture takes as input both an imputed version of thedata and a binary observation matrix. For imputing the temporal sparseobservations, we develop a flexible, fast to train method for differentiablemultivariate kernel regression. Our experiments on data from 298K individualsover 8 years, 18 common lab measurements, and 171 diseases show that thetemporal signatures learned via convolution are significantly more predictivethan baselines commonly used for early disease diagnosis.
arxiv-16200-246 | Texture Networks: Feed-forward Synthesis of Textures and Stylized Images | http://arxiv.org/pdf/1603.03417v1.pdf | author:Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, Victor Lempitsky category:cs.CV published:2016-03-10 summary:Gatys et al. recently demonstrated that deep networks can generate beautifultextures and stylized images from a single texture example. However, theirmethods requires a slow and memory-consuming optimization process. We proposehere an alternative approach that moves the computational burden to a learningstage. Given a single example of a texture, our approach trains compactfeed-forward convolutional networks to generate multiple samples of the sametexture of arbitrary size and to transfer artistic style from a given image toany other image. The resulting networks are remarkably light-weight and cangenerate textures of quality comparable to Gatys~et~al., but hundreds of timesfaster. More generally, our approach highlights the power and flexibility ofgenerative feed-forward models trained with complex and expressive lossfunctions.
arxiv-16200-247 | Lexical bundles in computational linguistics academic literature | http://arxiv.org/pdf/1603.02905v2.pdf | author:Adel Rahimi category:cs.CL published:2016-03-09 summary:In this study we analyzed a corpus of 8 million words academic literaturefrom Computational lingustics' academic literature. the lexical bundles fromthis corpus are categorized based on structures and functions.
arxiv-16200-248 | Efficient Bayesian experimentation using an expected information gain lower bound | http://arxiv.org/pdf/1506.00053v2.pdf | author:Panagiotis Tsilifis, Roger G. Ghanem, Paris Hajali category:stat.ML physics.geo-ph stat.CO stat.ME published:2015-05-30 summary:Experimental design is crucial for inference where limitations in the datacollection procedure are present due to cost or other restrictions. Optimalexperimental designs determine parameters that in some appropriate sense makethe data the most informative possible. In a Bayesian setting this istranslated to updating to the best possible posterior. Information theoreticarguments have led to the formation of the expected information gain as adesign criterion. This can be evaluated mainly by Monte Carlo sampling andmaximized by using stochastic approximation methods, both known for beingcomputationally expensive tasks. We propose a framework where a lower bound ofthe expected information gain is used as an alternative design criterion. Inaddition to alleviating the computational burden, this also addresses issuesconcerning estimation bias. The problem of permeability inference in a largecontaminated area is used to demonstrate the validity of our approach where weemploy the massively parallel version of the multiphase multicomponentsimulator TOUGH2 to simulate contaminant transport and a Polynomial Chaosapproximation of the forward model that further accelerates the objectivefunction evaluations. The proposed methodology is demonstrated to a settingwhere field measurements are available.
arxiv-16200-249 | Spectral Ranking using Seriation | http://arxiv.org/pdf/1406.5370v4.pdf | author:Fajwel Fogel, Alexandre d'Aspremont, Milan Vojnovic category:cs.LG cs.AI stat.ML published:2014-06-20 summary:We describe a seriation algorithm for ranking a set of items given pairwisecomparisons between these items. Intuitively, the algorithm assigns similarrankings to items that compare similarly with all others. It does so byconstructing a similarity matrix from pairwise comparisons, using seriationmethods to reorder this matrix and construct a ranking. We first show that thisspectral seriation algorithm recovers the true ranking when all pairwisecomparisons are observed and consistent with a total order. We then show thatranking reconstruction is still exact when some pairwise comparisons arecorrupted or missing, and that seriation based spectral ranking is more robustto noise than classical scoring methods. Finally, we bound the ranking errorwhen only a random subset of the comparions are observed. An additional benefitof the seriation formulation is that it allows us to solve semi-supervisedranking problems. Experiments on both synthetic and real datasets demonstratethat seriation based spectral ranking achieves competitive and in some casessuperior performance compared to classical ranking methods.
arxiv-16200-250 | Scalable Linear Causal Inference for Irregularly Sampled Time Series with Long Range Dependencies | http://arxiv.org/pdf/1603.03336v1.pdf | author:Francois W. Belletti, Evan R. Sparks, Michael J. Franklin, Alexandre M. Bayen, Joseph E. Gonzalez category:cs.LG stat.ME published:2016-03-10 summary:Linear causal analysis is central to a wide range of important applicationspanning finance, the physical sciences, and engineering. Much of the existingliterature in linear causal analysis operates in the time domain.Unfortunately, the direct application of time domain linear causal analysis tomany real-world time series presents three critical challenges: irregulartemporal sampling, long range dependencies, and scale. Moreover, real-worlddata is often collected at irregular time intervals across vast arrays ofdecentralized sensors and with long range dependencies which make naive timedomain correlation estimators spurious. In this paper we present a frequencydomain based estimation framework which naturally handles irregularly sampleddata and long range dependencies while enabled memory and communicationefficient distributed processing of time series data. By operating in thefrequency domain we eliminate the need to interpolate and help mitigate theeffects of long range dependencies. We implement and evaluate our new work-flowin the distributed setting using Apache Spark and demonstrate on both MonteCarlo simulations and high-frequency financial trading that we can accuratelyrecover causal structure at scale.
arxiv-16200-251 | Some like it hot - visual guidance for preference prediction | http://arxiv.org/pdf/1510.07867v2.pdf | author:Rasmus Rothe, Radu Timofte, Luc Van Gool category:cs.CV published:2015-10-27 summary:For people first impressions of someone are of determining importance. Theyare hard to alter through further information. This begs the question if acomputer can reach the same judgement. Earlier research has already pointed outthat age, gender, and average attractiveness can be estimated with reasonableprecision. We improve the state-of-the-art, but also predict - based onsomeone's known preferences - how much that particular person is attracted to anovel face. Our computational pipeline comprises a face detector, convolutionalneural networks for the extraction of deep features, standard support vectorregression for gender, age and facial beauty, and - as the main novelties -visual regularized collaborative filtering to infer inter-person preferences aswell as a novel regression technique for handling visual queries without ratinghistory. We validate the method using a very large dataset from a dating siteas well as images from celebrities. Our experiments yield convincing results,i.e. we predict 76% of the ratings correctly solely based on an image, andreveal some sociologically relevant conclusions. We also validate ourcollaborative filtering solution on the standard MovieLens rating dataset,augmented with movie posters, to predict an individual's movie rating. Wedemonstrate our algorithms on howhot.io which went viral around the Internetwith more than 50 million pictures evaluated in the first month.
arxiv-16200-252 | Sequential Monte Carlo Methods for System Identification | http://arxiv.org/pdf/1503.06058v3.pdf | author:Thomas B. Schön, Fredrik Lindsten, Johan Dahlin, Johan Wågberg, Christian A. Naesseth, Andreas Svensson, Liang Dai category:stat.CO math.OC stat.ML published:2015-03-20 summary:One of the key challenges in identifying nonlinear and possibly non-Gaussianstate space models (SSMs) is the intractability of estimating the system state.Sequential Monte Carlo (SMC) methods, such as the particle filter (introducedmore than two decades ago), provide numerical solutions to the nonlinear stateestimation problems arising in SSMs. When combined with additionalidentification techniques, these algorithms provide solid solutions to thenonlinear system identification problem. We describe two general strategies forcreating such combinations and discuss why SMC is a natural tool forimplementing these strategies.
arxiv-16200-253 | An Innovative Imputation and Classification Approach for Accurate Disease Prediction | http://arxiv.org/pdf/1603.03281v1.pdf | author:Yelipe UshaRani, P. Sammulal category:cs.DB cs.IR cs.LG published:2016-03-10 summary:Imputation of missing attribute values in medical datasets for extractinghidden knowledge from medical datasets is an interesting research topic ofinterest which is very challenging. One cannot eliminate missing values inmedical records. The reason may be because some tests may not been conducted asthey are cost effective, values missed when conducting clinical trials, valuesmay not have been recorded to name some of the reasons. Data mining researchershave been proposing various approaches to find and impute missing values toincrease classification accuracies so that disease may be predicted accurately.In this paper, we propose a novel imputation approach for imputation of missingvalues and performing classification after fixing missing values. The approachis based on clustering concept and aims at dimensionality reduction of therecords. The case study discussed shows that missing values can be fixed andimputed efficiently by achieving dimensionality reduction. The importance ofproposed approach for classification is visible in the case study which assignssingle class label in contrary to multi-label assignment if dimensionalityreduction is not performed.
arxiv-16200-254 | Binarized Neural Networks | http://arxiv.org/pdf/1602.02505v3.pdf | author:Itay Hubara, Daniel Soudry, Ran El Yaniv category:cs.LG cs.NE published:2016-02-08 summary:We introduce a method to train Binarized Neural Networks (BNNs) - neuralnetworks with binary weights and activations at run-time and when computing theparameters' gradient at train-time. We conduct two sets of experiments, eachbased on a different framework, namely Torch7 and Theano, where we train BNNson MNIST, CIFAR-10 and SVHN, and achieve nearly state-of-the-art results.During the forward pass, BNNs drastically reduce memory size and accesses, andreplace most arithmetic operations with bit-wise operations, which might leadto a great increase in power-efficiency. Last but not least, we wrote a binarymatrix multiplication GPU kernel with which it is possible to run our MNIST BNN7 times faster than with an unoptimized GPU kernel, without suffering any lossin classification accuracy. The code for training and running our BNNs isavailable.
arxiv-16200-255 | Instance-Aware Hashing for Multi-Label Image Retrieval | http://arxiv.org/pdf/1603.03234v1.pdf | author:Hanjiang Lai, Pan Yan, Xiangbo Shu, Yunchao Wei, Shuicheng Yan category:cs.CV published:2016-03-10 summary:Similarity-preserving hashing is a commonly used method for nearest neighboursearch in large-scale image retrieval. For image retrieval, deep-networks-basedhashing methods are appealing since they can simultaneously learn effectiveimage representations and compact hash codes. This paper focuses ondeep-networks-based hashing for multi-label images, each of which may containobjects of multiple categories. In most existing hashing methods, each image isrepresented by one piece of hash code, which is referred to as semantichashing. This setting may be suboptimal for multi-label image retrieval. Tosolve this problem, we propose a deep architecture that learns\textbf{instance-aware} image representations for multi-label image data, whichare organized in multiple groups, with each group containing the features forone category. The instance-aware representations not only bring advantages tosemantic hashing, but also can be used in category-aware hashing, in which animage is represented by multiple pieces of hash codes and each piece of codecorresponds to a category. Extensive evaluations conducted on several benchmarkdatasets demonstrate that, for both semantic hashing and category-awarehashing, the proposed method shows substantial improvement over thestate-of-the-art supervised and unsupervised hashing methods.
arxiv-16200-256 | Optimality of Belief Propagation for Crowdsourced Classification | http://arxiv.org/pdf/1602.03619v2.pdf | author:Jungseul Ok, Sewoong Oh, Jinwoo Shin, Yung Yi category:cs.LG stat.ML published:2016-02-11 summary:Crowdsourcing systems are popular for solving large-scale labelling taskswith low-paid (or even non-paid) workers. We study the problem of recoveringthe true labels from the possibly erroneous crowdsourced labels under thepopular Dawid-Skene model. To address this inference problem, severalalgorithms have recently been proposed, but the best known guarantee is stillsignificantly larger than the fundamental limit. We close this gap under asimple but canonical scenario where each worker is assigned at most two tasks.In particular, we introduce a tighter lower bound on the fundamental limit andprove that Belief Propagation (BP) exactly matches this lower bound. Theguaranteed optimality of BP is the strongest in the sense that it isinformation-theoretically impossible for any other algorithm to correctly labela larger fraction of the tasks. In the general setting, when more than twotasks are assigned to each worker, we establish the dominance result on BP thatit outperforms all existing algorithms with provable guarantees. Experimentalresults suggest that BP is close to optimal for all regimes considered, whileall other algorithms show suboptimal performances in certain regimes.
arxiv-16200-257 | Estimating Renyi Entropy of Discrete Distributions | http://arxiv.org/pdf/1408.1000v3.pdf | author:Jayadev Acharya, Alon Orlitsky, Ananda Theertha Suresh, Himanshu Tyagi category:cs.IT cs.DS cs.LG math.IT published:2014-08-02 summary:It was recently shown that estimating the Shannon entropy $H({\rm p})$ of adiscrete $k$-symbol distribution ${\rm p}$ requires $\Theta(k/\log k)$ samples,a number that grows near-linearly in the support size. In many applications$H({\rm p})$ can be replaced by the more general R\'enyi entropy of order$\alpha$, $H_\alpha({\rm p})$. We determine the number of samples needed toestimate $H_\alpha({\rm p})$ for all $\alpha$, showing that $\alpha < 1$requires a super-linear, roughly $k^{1/\alpha}$ samples, noninteger $\alpha>1$requires a near-linear $k$ samples, but, perhaps surprisingly, integer$\alpha>1$ requires only $\Theta(k^{1-1/\alpha})$ samples. Furthermore,developing on a recently established connection between polynomialapproximation and estimation of additive functions of the form $\sum_{x} f({\rmp}_x)$, we reduce the sample complexity for noninteger values of $\alpha$ by afactor of $\log k$ compared to the empirical estimator. The estimatorsachieving these bounds are simple and run in time linear in the number ofsamples. Our lower bounds provide explicit constructions of distributions withdifferent R\'enyi entropies that are hard to distinguish.
arxiv-16200-258 | Scenario Submodular Cover | http://arxiv.org/pdf/1603.03158v1.pdf | author:Nathaniel Grammel, Lisa Hellerstein, Devorah Kletenik, Patrick Lin category:cs.DS cs.LG published:2016-03-10 summary:Many problems in Machine Learning can be modeled as submodular optimizationproblems. Recent work has focused on stochastic or adaptive versions of theseproblems. We consider the Scenario Submodular Cover problem, which is acounterpart to the Stochastic Submodular Cover problem studied by Golovin andKrause. In Scenario Submodular Cover, the goal is to produce a cover withminimum expected cost, where the expectation is with respect to an empiricaljoint distribution, given as input by a weighted sample of realizations. Incontrast, in Stochastic Submodular Cover, the variables of the inputdistribution are assumed to be independent, and the distribution of eachvariable is given as input. Building on algorithms developed by Cicalese et al.and Golovin and Krause for related problems, we give two approximationalgorithms for Scenario Submodular Cover over discrete distributions. The firstachieves an approximation factor of O(log Qm), where m is the size of thesample and Q is the goal utility. The second, simpler algorithm achieves anapproximation bound of O(log QW), where Q is the goal utility and W is the sumof the integer weights. (Both bounds assume an integer-valued utilityfunction.) Our results yield approximation bounds for other problems involvingnon-independent distributions that are explicitly specified by their support.
arxiv-16200-259 | Real time error detection in metal arc welding process using Artificial Neural Netwroks | http://arxiv.org/pdf/1603.03149v1.pdf | author:Prashant Sharma, Shaju K. Albert, S. Rajeswari category:cs.NE published:2016-03-10 summary:Quality assurance in production line demands reliable weld joints. Human madeerrors is a major cause of faulty production. Promptly Identifying errors inthe weld while welding is in progress will decrease the post inspection costspent on the welding process. Electrical parameters generated during welding,could able to characterize the process efficiently. Parameter values arecollected using high speed data acquisition system. Time series analysis taskssuch as filtering, pattern recognition etc. are performed over the collecteddata. Filtering removes the unwanted noisy signal components and patternrecognition task segregate error patterns in the time series based uponsimilarity, which is performed by Self Organized mapping clustering algorithm.Welder quality is thus compared by detecting and counting number of errorpatterns appeared in his parametric time series. Moreover, Self Organizedmapping algorithm provides the database in which patterns are segregated intotwo classes either desirable or undesirable. Database thus generated is used totrain the classification algorithms, and thereby automating the real time errordetection task. Multi Layer Perceptron and Radial basis function are the twoclassification algorithms used, and their performance has been compared basedon metrics such as specificity, sensitivity, accuracy and time required intraining.
arxiv-16200-260 | Is language evolution grinding to a halt?: Exploring the life and death of words in English fiction | http://arxiv.org/pdf/1503.03512v3.pdf | author:Eitan Adam Pechenick, Christopher M. Danforth, Peter Sheridan Dodds category:cs.CL cs.IT math.IT physics.soc-ph stat.AP published:2015-03-11 summary:The Google Books corpus, derived from millions of books in a range of majorlanguages, would seem to offer many possibilities for research into cultural,social, and linguistic evolution. In a previous work, we found that the 2009and 2012 versions of the unfiltered English data set as well as the 2009version of the English Fiction data set are all heavily saturated withscientific and medical literature, rendering them unsuitable for rigorousanalysis [Pechenick, Danforth and Dodds, PLoS ONE, 10, e0137041, 2015]. Bycontrast, the 2012 version of English Fiction appeared to be uncompromised, andwe use this data set to explore language dynamics for English from 1820--2000.We critique an earlier method for measuring birth and death rates of words, andprovide a robust, principled approach to examining the volume of word fluxacross various relative frequency usage thresholds. We use the contributions tothe Jensen-Shannon divergence of words crossing thresholds between consecutivedecades to illuminate the major driving factors behind the flux. We find thatwhile individual word usage may vary greatly, the overall statistical structureof the language appears to remain fairly stable. We also find indications thatscholarly works about fiction are strongly represented in the 2012 EnglishFiction corpus, and suggest that a future revision of the corpus should attemptto separate critical works from fiction itself.
arxiv-16200-261 | Multi-centrality Graph Spectral Decompositions and their Application to Cyber Intrusion Detection | http://arxiv.org/pdf/1512.07372v2.pdf | author:Pin-Yu Chen, Sutanay Choudhury, Alfred O. Hero category:cs.SI cs.CR stat.ML published:2015-12-23 summary:Many modern datasets can be represented as graphs and hence spectraldecompositions such as graph principal component analysis (PCA) can be useful.Distinct from previous graph decomposition approaches based on subspaceprojection of a single topological feature, e.g., the Fiedler vector ofcentered graph adjacency matrix (graph Laplacian), we propose spectraldecomposition approaches to graph PCA and graph dictionary learning thatintegrate multiple features, including graph walk statistics, centralitymeasures and graph distances to reference nodes. In this paper we propose a newPCA method for single graph analysis, called multi-centrality graph PCA(MC-GPCA), and a new dictionary learning method for ensembles of graphs, calledmulti-centrality graph dictionary learning (MC-GDL), both based on spectraldecomposition of multi-centrality matrices. As an application to cyberintrusion detection, MC-GPCA can be an effective indicator of anomalousconnectivity pattern and MC-GDL can provide discriminative basis for attackclassification.
arxiv-16200-262 | Theoretical Comparisons of Learning from Positive-Negative, Positive-Unlabeled, and Negative-Unlabeled Data | http://arxiv.org/pdf/1603.03130v1.pdf | author:Gang Niu, Marthinus Christoffel du Plessis, Tomoya Sakai, Masashi Sugiyama category:cs.LG stat.ML published:2016-03-10 summary:In PU learning, a binary classifier is trained only from positive (P) andunlabeled (U) data without negative (N) data. Although N data is missing, itsometimes outperforms PN learning (i.e., supervised learning) in experiments.In this paper, we theoretically compare PU (and the opposite NU) learningagainst PN learning, and prove that, one of PU and NU learning given infinite Udata will almost always improve on PN learning. Our theoretical finding is alsovalidated experimentally.
arxiv-16200-263 | Global and Local Uncertainty Principles for Signals on Graphs | http://arxiv.org/pdf/1603.03030v1.pdf | author:Nathanael Perraudin, Benjamin Ricaud, David Shuman, Pierre Vandergheynst category:stat.ML math-ph math.MP published:2016-03-10 summary:Uncertainty principles such as Heisenberg's provide limits on thetime-frequency concentration of a signal, and constitute an importanttheoretical tool for designing and evaluating linear signal transforms.Generalizations of such principles to the graph setting can inform dictionarydesign for graph signals, lead to algorithms for reconstructing missinginformation from graph signals via sparse representations, and yield new graphanalysis tools. While previous work has focused on generalizing notions ofspreads of a graph signal in the vertex and graph spectral domains, ourapproach is to generalize the methods of Lieb in order to develop uncertaintyprinciples that provide limits on the concentration of the analysiscoefficients of any graph signal under a dictionary transform whose atoms arejointly localized in the vertex and graph spectral domains. One challenge wehighlight is that due to the inhomogeneity of the underlying graph data domain,the local structure in a single small region of the graph can drasticallyaffect the uncertainty bounds for signals concentrated in different regions ofthe graph, limiting the information provided by global uncertainty principles.Accordingly, we suggest a new way to incorporate a notion of locality, anddevelop local uncertainty principles that bound the concentration of theanalysis coefficients of each atom of a localized graph spectral filter framein terms of quantities that depend on the local structure of the graph aroundthe center vertex of the given atom. Finally, we demonstrate how our proposedlocal uncertainty measures can improve the random sampling of graph signals.
arxiv-16200-264 | Building a Fine-Grained Entity Typing System Overnight for a New X (X = Language, Domain, Genre) | http://arxiv.org/pdf/1603.03112v1.pdf | author:Lifu Huang, Jonathan May, Xiaoman Pan, Heng Ji category:cs.CL cs.AI published:2016-03-10 summary:Recent research has shown great progress on fine-grained entity typing. Mostexisting methods require pre-defining a set of types and training a multi-classclassifier from a large labeled data set based on multi-level linguisticfeatures. They are thus limited to certain domains, genres and languages. Inthis paper, we propose a novel unsupervised entity typing framework bycombining symbolic and distributional semantics. We start from learning generalembeddings for each entity mention, compose the embeddings of specific contextsusing linguistic structures, link the mention to knowledge bases and learn itsrelated knowledge representations. Then we develop a novel joint hierarchicalclustering and linking algorithm to type all mentions using theserepresentations. This framework doesn't rely on any annotated data, predefinedtyping schema, or hand-crafted features, therefore it can be quickly adapted toa new domain, genre and language. Furthermore, it has great flexibility atincorporating linguistic structures (e.g., Abstract Meaning Representation(AMR), dependency relations) to improve specific context representation.Experiments on genres (news and discussion forum) show comparable performancewith state-of-the-art supervised typing systems trained from a large amount oflabeled data. Results on various languages (English, Chinese, Japanese, Hausa,and Yoruba) and domains (general and biomedical) demonstrate the portability ofour framework.
arxiv-16200-265 | Recursive Recurrent Nets with Attention Modeling for OCR in the Wild | http://arxiv.org/pdf/1603.03101v1.pdf | author:Chen-Yu Lee, Simon Osindero category:cs.CV published:2016-03-09 summary:We present recursive recurrent neural networks with attention modeling(R$^2$AM) for lexicon-free optical character recognition in natural sceneimages. The primary advantages of the proposed method are: (1) use of recursiveconvolutional neural networks (CNNs), which allow for parametrically efficientand effective image feature extraction; (2) an implicitly learnedcharacter-level language model, embodied in a recurrent neural network whichavoids the need to use N-grams; and (3) the use of a soft-attention mechanism,allowing the model to selectively exploit image features in a coordinated way,and allowing for end-to-end training within a standard backpropagationframework. We validate our method with state-of-the-art performance onchallenging benchmark datasets: Street View Text, IIIT5k, ICDAR and Synth90k.
arxiv-16200-266 | Blind Source Separation: Fundamentals and Recent Advances (A Tutorial Overview Presented at SBrT-2001) | http://arxiv.org/pdf/1603.03089v1.pdf | author:Eleftherios Kofidis category:stat.ML cs.IT math.IT published:2016-03-09 summary:Blind source separation (BSS), i.e., the decoupling of unknown signals thathave been mixed in an unknown way, has been a topic of great interest in thesignal processing community for the last decade, covering a wide range ofapplications in such diverse fields as digital communications, patternrecognition, biomedical engineering, and financial data analysis, among others.This course aims at an introduction to the BSS problem via an exposition ofwell-known and established as well as some more recent approaches to itssolution. A unified way is followed in presenting the various results so as tomore easily bring out their similarities/differences and emphasize theirrelative advantages/disadvantages. Only a representative sample of the existingknowledge on BSS will be included in this course. The interested readers areencouraged to consult the list of bibliographical references for more detailson this exciting and always active research topic.
arxiv-16200-267 | Dynamic Privacy For Distributed Machine Learning Over Network | http://arxiv.org/pdf/1601.03466v3.pdf | author:Tao Zhang, Quanyan Zhu category:cs.LG published:2016-01-14 summary:Privacy-preserving distributed machine learning becomes increasinglyimportant due to the recent rapid growth of data. This paper focuses on a classof regularized empirical risk minimization (ERM) machine learning problems, anddevelops two methods to provide differential privacy to distributed learningalgorithms over a network. We first decentralize the learning algorithm usingthe alternating direction method of multipliers (ADMM), and propose the methodsof dual variable perturbation and primal variable perturbation to providedynamic differential privacy. The two mechanisms lead to algorithms that canprovide privacy guarantees under mild conditions of the convexity anddifferentiability of the loss function and the regularizer. We study theperformance of the algorithms, and show that the dual variable perturbationoutperforms its primal counterpart. To design an optimal privacy mechanisms, weanalyze the fundamental tradeoff between privacy and accuracy, and provideguidelines to choose privacy parameters. Numerical experiments using customerinformation database are performed to corroborate the results on privacy andutility tradeoffs and design.
arxiv-16200-268 | Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images | http://arxiv.org/pdf/1511.02300v2.pdf | author:Shuran Song, Jianxiong Xiao category:cs.CV published:2015-11-07 summary:We focus on the task of amodal 3D object detection in RGB-D images, whichaims to produce a 3D bounding box of an object in metric form at its fullextent. We introduce Deep Sliding Shapes, a 3D ConvNet formulation that takes a3D volumetric scene from a RGB-D image as input and outputs 3D object boundingboxes. In our approach, we propose the first 3D Region Proposal Network (RPN)to learn objectness from geometric shapes and the first joint ObjectRecognition Network (ORN) to extract geometric features in 3D and colorfeatures in 2D. In particular, we handle objects of various sizes by trainingan amodal RPN at two different scales and an ORN to regress 3D bounding boxes.Experiments show that our algorithm outperforms the state-of-the-art by 13.8 inmAP and is 200x faster than the original Sliding Shapes. All source code andpre-trained models will be available at GitHub.
arxiv-16200-269 | Hybrid Collaborative Filtering with Neural Networks | http://arxiv.org/pdf/1603.00806v2.pdf | author:Florian Strub, Jeremie Mary, Romaric Gaudel category:cs.IR cs.AI cs.NE published:2016-03-02 summary:Collaborative Filtering aims at exploiting the feedback of users to providepersonalised recommendations. Such algorithms look for latent variables in alarge sparse matrix of ratings. They can be enhanced by adding side informationto tackle the well-known cold start problem. While Neu-ral Networks havetremendous success in image and speech recognition, they have received lessattention in Collaborative Filtering. This is all the more surprising thatNeural Networks are able to discover latent variables in large andheterogeneous datasets. In this paper, we introduce a Collaborative FilteringNeural network architecture aka CFN which computes a non-linear MatrixFactorization from sparse rating inputs and side information. We showexperimentally on the MovieLens and Douban dataset that CFN outper-forms thestate of the art and benefits from side information. We provide animplementation of the algorithm as a reusable plugin for Torch, a popularNeural Network framework.
arxiv-16200-270 | Summarization of Films and Documentaries Based on Subtitles and Scripts | http://arxiv.org/pdf/1506.01273v3.pdf | author:Marta Aparício, Paulo Figueiredo, Francisco Raposo, David Martins de Matos, Ricardo Ribeiro, Luís Marujo category:cs.CL cs.AI cs.IR I.2.7 published:2015-06-03 summary:We assess the performance of generic text summarization algorithms applied tofilms and documentaries, using the well-known behavior of summarization of newsarticles as reference. We use three datasets: (i) news articles, (ii) filmscripts and subtitles, and (iii) documentary subtitles. Standard ROUGE metricsare used for comparing generated summaries against news abstracts, plotsummaries, and synopses. We show that the best performing algorithms are LSA,for news articles and documentaries, and LexRank and Support Sets, for films.Despite the different nature of films and documentaries, their relativebehavior is in accordance with that obtained for news articles.
arxiv-16200-271 | Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks | http://arxiv.org/pdf/1603.01431v2.pdf | author:Devansh Arpit, Yingbo Zhou, Bhargava U. Kota, Venu Govindaraju category:stat.ML cs.LG published:2016-03-04 summary:While the authors of Batch Normalization (BN) identify and address animportant problem involved in training deep networks-- \textit{InternalCovariate Shift}-- the current solution has multiple drawbacks. For instance,BN depends on batch statistics for layerwise input normalization duringtraining which makes the estimates of mean and standard deviation of input(distribution) to hidden layers inaccurate due to shifting parameter values(specially during initial training epochs). Another fundamental problem with BNis that it cannot be used with batch-size $ 1 $ during training. We addressthese (and other) drawbacks of BN by proposing a non-adaptive normalizationtechnique for removing covariate shift, that we call \textit{NormalizationPropagation}. Our approach does not depend on batch statistics, but rather usesa data-independent parametric estimate of mean and standard-deviation in everylayer thus being faster compared with BN. We exploit the observation that thepre-activation before Rectified Linear Units follow a Gaussian distribution indeep networks, and that once the first and second order statistics of any givendataset are normalized, we can forward propagate this normalization without theneed for recalculating the approximate statistics (using data) for any of thehidden layers.
arxiv-16200-272 | Using Generic Summarization to Improve Music Information Retrieval Tasks | http://arxiv.org/pdf/1503.06666v3.pdf | author:Francisco Raposo, Ricardo Ribeiro, David Martins de Matos category:cs.IR cs.LG cs.SD H.5.5 published:2015-03-23 summary:In order to satisfy processing time constraints, many MIR tasks process onlya segment of the whole music signal. This practice may lead to decreasingperformance, since the most important information for the tasks may not be inthose processed segments. In this paper, we leverage generic summarizationalgorithms, previously applied to text and speech summarization, to summarizeitems in music datasets. These algorithms build summaries, that are bothconcise and diverse, by selecting appropriate segments from the input signalwhich makes them good candidates to summarize music as well. We evaluate thesummarization process on binary and multiclass music genre classificationtasks, by comparing the performance obtained using summarized datasets againstthe performances obtained using continuous segments (which is the traditionalmethod used for addressing the previously mentioned time constraints) and fullsongs of the same original dataset. We show that GRASSHOPPER, LexRank, LSA,MMR, and a Support Sets-based Centrality model improve classificationperformance when compared to selected 30-second baselines. We also show thatsummarized datasets lead to a classification performance whose difference isnot statistically significant from using full songs. Furthermore, we make anargument stating the advantages of sharing summarized datasets for future MIRresearch.
arxiv-16200-273 | Nonparametric Bayesian Double Articulation Analyzer for Direct Language Acquisition from Continuous Speech Signals | http://arxiv.org/pdf/1506.06646v2.pdf | author:Tadahiro Taniguchi, Ryo Nakashima, Shogo Nagasaka category:cs.AI cs.CL cs.LG stat.ML published:2015-06-22 summary:Human infants can discover words directly from unsegmented speech signalswithout any explicitly labeled data. In this paper, we develop a novel machinelearning method called nonparametric Bayesian double articulation analyzer(NPB-DAA) that can directly acquire language and acoustic models from observedcontinuous speech signals. For this purpose, we propose an integrativegenerative model that combines a language model and an acoustic model into asingle generative model called the "hierarchical Dirichlet process hiddenlanguage model" (HDP-HLM). The HDP-HLM is obtained by extending thehierarchical Dirichlet process hidden semi-Markov model (HDP-HSMM) proposed byJohnson et al. An inference procedure for the HDP-HLM is derived using theblocked Gibbs sampler originally proposed for the HDP-HSMM. This procedureenables the simultaneous and direct inference of language and acoustic modelsfrom continuous speech signals. Based on the HDP-HLM and its inferenceprocedure, we developed a novel double articulation analyzer. By assumingHDP-HLM as a generative model of observed time series data, and by inferringlatent variables of the model, the method can analyze latent doublearticulation structure, i.e., hierarchically organized latent words andphonemes, of the data in an unsupervised manner. The novel unsupervised doublearticulation analyzer is called NPB-DAA. The NPB-DAA can automatically estimate double articulation structure embeddedin speech signals. We also carried out two evaluation experiments usingsynthetic data and actual human continuous speech signals representing Japanesevowel sequences. In the word acquisition and phoneme categorization tasks, theNPB-DAA outperformed a conventional double articulation analyzer (DAA) andbaseline automatic speech recognition system whose acoustic model was trainedin a supervised manner.
arxiv-16200-274 | Patch-based Convolutional Neural Network for Whole Slide Tissue Image Classification | http://arxiv.org/pdf/1504.07947v5.pdf | author:Le Hou, Dimitris Samaras, Tahsin M. Kurc, Yi Gao, James E. Davis, Joel H. Saltz category:cs.CV J.3; I.4; I.5 published:2015-04-29 summary:Convolutional Neural Networks (CNN) are state-of-the-art models for manyimage classification tasks. However, to recognize cancer subtypesautomatically, training a CNN on gigapixel resolution Whole Slide Tissue Images(WSI) is currently computationally impossible. The differentiation of cancersubtypes is based on cellular-level visual features observed on image patchscale. Therefore, we argue that in this situation, training a patch-levelclassifier on image patches will perform better than or similar to animage-level classifier. The challenge becomes how to intelligently combinepatch-level classification results and model the fact that not all patches willbe discriminative. We propose to train a decision fusion model to aggregatepatch-level predictions given by patch-level CNNs, which to the best of ourknowledge has not been shown before. Furthermore, we formulate a novelExpectation-Maximization (EM) based method that automatically locatesdiscriminative patches robustly by utilizing the spatial relationships ofpatches. We apply our method to the classification of glioma and non-small-celllung carcinoma cases into subtypes. The classification accuracy of our methodis similar to the inter-observer agreement between pathologists. Although it isimpossible to train CNNs on WSIs, we experimentally demonstrate using acomparable non-cancer dataset of smaller images that a patch-based CNN canoutperform an image-based CNN.
arxiv-16200-275 | Minimum Density Hyperplanes | http://arxiv.org/pdf/1507.04201v2.pdf | author:Nicos Pavlidis, David Hofmeyr, Sotiris Tasoulis category:stat.ML cs.LG 62H30, 68T10 published:2015-07-15 summary:Associating distinct groups of objects (clusters) with contiguous regions ofhigh probability density (high-density clusters), is central to manystatistical and machine learning approaches to the classification of unlabelleddata. We propose a novel hyperplane classifier for clustering andsemi-supervised classification which is motivated by this objective. Theproposed minimum density hyperplane minimises the integral of the empiricalprobability density function along it, thereby avoiding intersection with highdensity clusters. We show that the minimum density and the maximum marginhyperplanes are asymptotically equivalent, thus linking this approach tomaximum margin clustering and semi-supervised support vector classifiers. Wepropose a projection pursuit formulation of the associated optimisation problemwhich allows us to find minimum density hyperplanes efficiently in practice,and evaluate its performance on a range of benchmark datasets. The proposedapproach is found to be very competitive with state of the art methods forclustering and semi-supervised classification.
arxiv-16200-276 | Unsupervised word segmentation and lexicon discovery using acoustic word embeddings | http://arxiv.org/pdf/1603.02845v1.pdf | author:Herman Kamper, Aren Jansen, Sharon Goldwater category:cs.CL published:2016-03-09 summary:In settings where only unlabelled speech data is available, speech technologyneeds to be developed without transcriptions, pronunciation dictionaries, orlanguage modelling text. A similar problem is faced when modelling infantlanguage acquisition. In these cases, categorical linguistic structure needs tobe discovered directly from speech audio. We present a novel unsupervisedBayesian model that segments unlabelled speech and clusters the segments intohypothesized word groupings. The result is a complete unsupervised tokenizationof the input speech in terms of discovered word types. In our approach, apotential word segment (of arbitrary length) is embedded in a fixed-dimensionalacoustic vector space. The model, implemented as a Gibbs sampler, then builds awhole-word acoustic model in this space while jointly performing segmentation.We report word error rates in a small-vocabulary connected digit recognitiontask by mapping the unsupervised decoded output to ground truth transcriptions.The model achieves around 20% error rate, outperforming a previous HMM-basedsystem by about 10% absolute. Moreover, in contrast to the baseline, our modeldoes not require a pre-specified vocabulary size.
arxiv-16200-277 | Fast Training of Triplet-based Deep Binary Embedding Networks | http://arxiv.org/pdf/1603.02844v1.pdf | author:Bohan Zhuang, Guosheng Lin, Chunhua Shen, Ian Reid category:cs.CV published:2016-03-09 summary:In this paper, we aim to learn a mapping (or embedding) from images to acompact binary space in which Hamming distances correspond to a ranking measurefor the image retrieval task. We make use of a triplet loss because this has been shown to be mosteffective for ranking problems. However, training in previous works can be prohibitively expensive due to thefact that optimization is directly performed on the triplet space, where thenumber of possible triplets for training is cubic in the number of trainingexamples. To address this issue, we propose to formulate high-order binary codeslearning as a multi-label classification problem by explicitly separatinglearning into two interleaved stages. To solve the first stage, we design a large-scale high-order binary codesinference algorithm to reduce the high-order objective to a standard binaryquadratic problem such that graph cuts can be used to efficiently infer thebinary code which serve as the label of each training datum. In the second stage we propose to map the original image to compact binarycodes via carefully designed deep convolutional neural networks (CNNs) and thehashing function fitting can be solved by training binary CNN classifiers. An incremental/interleaved optimization strategy is proffered to ensure thatthese two steps are interactive with each other during training for betteraccuracy. We conduct experiments on several benchmark datasets, which demonstrate bothimproved training time (by as much as two orders of magnitude) as well asproducing state-of-the-art hashing for various retrieval tasks.
arxiv-16200-278 | Note on the equivalence of hierarchical variational models and auxiliary deep generative models | http://arxiv.org/pdf/1603.02443v2.pdf | author:Niko Brümmer category:stat.ML published:2016-03-08 summary:This note compares two recently published machine learning methods forconstructing flexible, but tractable families of variational hidden-variableposteriors. The first method, called "hierarchical variational models" enrichesthe inference model with an extra variable, while the other, called "auxiliarydeep generative models", enriches the generative model instead. We concludethat the two methods are mathematically equivalent.
arxiv-16200-279 | Starting Small -- Learning with Adaptive Sample Sizes | http://arxiv.org/pdf/1603.02839v1.pdf | author:Hadi Daneshmand, Aurelien Lucchi, Thomas Hofmann category:cs.LG published:2016-03-09 summary:For many machine learning problems, data is abundant and it may beprohibitive to make multiple passes through the full training set. In thiscontext, we investigate strategies for dynamically increasing the effectivesample size, when using iterative methods such as stochastic gradient descent.Our interest is motivated by the rise of variance-reduced methods, whichachieve linear convergence rates that scale favorably for smaller sample sizes.Exploiting this feature, we show -- theoretically and empirically -- how toobtain significant speed-ups with a novel algorithm that reaches statisticalaccuracy on an $n$-sample in $2n$, instead of $n \log n$ steps.
arxiv-16200-280 | Faster learning of deep stacked autoencoders on multi-core systems using synchronized layer-wise pre-training | http://arxiv.org/pdf/1603.02836v1.pdf | author:Anirban Santara, Debapriya Maji, DP Tejas, Pabitra Mitra, Arobinda Gupta category:cs.LG published:2016-03-09 summary:Deep neural networks are capable of modelling highly non-linear functions bycapturing different levels of abstraction of data hierarchically. Whiletraining deep networks, first the system is initialized near a good optimum bygreedy layer-wise unsupervised pre-training. However, with burgeoning data andincreasing dimensions of the architecture, the time complexity of this approachbecomes enormous. Also, greedy pre-training of the layers often turnsdetrimental by over-training a layer causing it to lose harmony with the restof the network. In this paper a synchronized parallel algorithm forpre-training deep networks on multi-core machines has been proposed. Differentlayers are trained by parallel threads running on different cores with regularsynchronization. Thus the pre-training process becomes faster and chances ofover-training are reduced. This is experimentally validated using a stackedautoencoder for dimensionality reduction of MNIST handwritten digit database.The proposed algorithm achieved 26\% speed-up compared to greedy layer-wisepre-training for achieving the same reconstruction accuracy substantiating itspotential as an alternative.
arxiv-16200-281 | Image Captioning and Visual Question Answering Based on Attributes and Their Related External Knowledge | http://arxiv.org/pdf/1603.02814v1.pdf | author:Qi Wu, Chunhua Shen, Anton van den Hengel, Peng Wang, Anthony Dick category:cs.CV published:2016-03-09 summary:Much recent progress in Vision-to-Language problems has been achieved througha combination of Convolutional Neural Networks (CNNs) and Recurrent NeuralNetworks (RNNs). This approach does not explicitly represent high-levelsemantic concepts, but rather seeks to progress directly from image features totext. In this paper we first propose a method of incorporating high-levelconcepts into the successful CNN-RNN approach, and show that it achieves asignificant improvement on the state-of-the-art in both image captioning andvisual question answering. We further show that the same mechanism can be usedto incorporate external knowledge, which is critically important for answeringhigh level visual questions. Specifically, we design a visual questionanswering model that combines an internal representation of the content of animage with information extracted from a general knowledge base to answer abroad range of image-based questions. It particularly allows questions to beasked about the contents of an image, even when the image itself does notcontain a complete answer. Our final model achieves the best reported resultson both image captioning and visual question answering on several benchmarkdatasets.
arxiv-16200-282 | Optimized Kernel Entropy Components | http://arxiv.org/pdf/1603.02806v1.pdf | author:Emma Izquierdo-Verdiguier, Valero Laparra, Robert Jenssen, Luis Gómez-Chova, Gustau Camps-Valls category:stat.ML cs.LG published:2016-03-09 summary:This work addresses two main issues of the standard Kernel Entropy ComponentAnalysis (KECA) algorithm: the optimization of the kernel decomposition and theoptimization of the Gaussian kernel parameter. KECA roughly reduces to asorting of the importance of kernel eigenvectors by entropy instead of byvariance as in Kernel Principal Components Analysis. In this work, we proposean extension of the KECA method, named Optimized KECA (OKECA), that directlyextracts the optimal features retaining most of the data entropy by means ofcompacting the information in very few features (often in just one or two). Theproposed method produces features which have higher expressive power. Inparticular, it is based on the Independent Component Analysis (ICA) framework,and introduces an extra rotation to the eigen-decomposition, which is optimizedvia gradient ascent search. This maximum entropy preservation suggests thatOKECA features are more efficient than KECA features for density estimation. Inaddition, a critical issue in both methods is the selection of the kernelparameter since it critically affects the resulting performance. Here weanalyze the most common kernel length-scale selection criteria. Results of bothmethods are illustrated in different synthetic and real problems. Results showthat 1) OKECA returns projections with more expressive power than KECA, 2) themost successful rule for estimating the kernel parameter is based on maximumlikelihood, and 3) OKECA is more robust to the selection of the length-scaleparameter in kernel density estimation.
arxiv-16200-283 | Selective Inference Approach for Statistically Sound Predictive Pattern Mining | http://arxiv.org/pdf/1602.04601v2.pdf | author:Shinya Suzumura, Kazuya Nakagawa, Mahito Sugiyama, Koji Tsuda, Ichiro Takeuchi category:stat.ML published:2016-02-15 summary:Discovering statistically significant patterns from databases is an importantchallenging problem. The main obstacle of this problem is in the difficulty oftaking into account the selection bias, i.e., the bias arising from the factthat patterns are selected from extremely large number of candidates indatabases. In this paper, we introduce a new approach for predictive patternmining problems that can address the selection bias issue. Our approach isbuilt on a recently popularized statistical inference framework calledselective inference. In selective inference, statistical inferences (such asstatistical hypothesis testing) are conducted based on sampling distributionsconditional on a selection event. If the selection event is characterized in atractable way, statistical inferences can be made without minding selectionbias issue. However, in pattern mining problems, it is difficult tocharacterize the entire selection process of mining algorithms. Our maincontribution in this paper is to solve this challenging problem for a class ofpredictive pattern mining problems by introducing a novel algorithmicframework. We demonstrate that our approach is useful for finding statisticallysignificant patterns from databases.
arxiv-16200-284 | Bipartite Correlation Clustering -- Maximizing Agreements | http://arxiv.org/pdf/1603.02782v1.pdf | author:Megasthenis Asteris, Anastasios Kyrillidis, Dimitris Papailiopoulos, Alexandros G. Dimakis category:cs.DS stat.ML published:2016-03-09 summary:In Bipartite Correlation Clustering (BCC) we are given a complete bipartitegraph $G$ with `+' and `-' edges, and we seek a vertex clustering thatmaximizes the number of agreements: the number of all `+' edges within clustersplus all `-' edges cut across clusters. BCC is known to be NP-hard. We present a novel approximation algorithm for $k$-BCC, a variant of BCC withan upper bound $k$ on the number of clusters. Our algorithm outputs a$k$-clustering that provably achieves a number of agreements within amultiplicative ${(1-\delta)}$-factor from the optimal, for any desired accuracy$\delta$. It relies on solving a combinatorially constrained bilinearmaximization on the bi-adjacency matrix of $G$. It runs in time exponential in$k$ and $\delta^{-1}$, but linear in the size of the input. Further, we show that, in the (unconstrained) BCC setting, an${(1-\delta)}$-approximation can be achieved by $O(\delta^{-1})$ clustersregardless of the size of the graph. In turn, our $k$-BCC algorithm implies anEfficient PTAS for the BCC objective of maximizing agreements.
arxiv-16200-285 | Implicit Discourse Relation Classification via Multi-Task Neural Networks | http://arxiv.org/pdf/1603.02776v1.pdf | author:Yang Liu, Sujian Li, Xiaodong Zhang, Zhifang Sui category:cs.CL cs.AI cs.NE published:2016-03-09 summary:Without discourse connectives, classifying implicit discourse relations is achallenging task and a bottleneck for building a practical discourse parser.Previous research usually makes use of one kind of discourse framework such asPDTB or RST to improve the classification performance on discourse relations.Actually, under different discourse annotation frameworks, there exist multiplecorpora which have internal connections. To exploit the combination ofdifferent discourse corpora, we design related discourse classification tasksspecific to a corpus, and propose a novel Convolutional Neural Network embeddedmulti-task learning system to synthesize these tasks by learning both uniqueand shared representations for each task. The experimental results on the PDTBimplicit discourse relation classification task demonstrate that our modelachieves significant gains over baseline systems.
arxiv-16200-286 | Efficient piecewise training of deep structured models for semantic segmentation | http://arxiv.org/pdf/1504.01013v3.pdf | author:Guosheng Lin, Chunhua Shen, Anton van dan Hengel, Ian Reid category:cs.CV published:2015-04-04 summary:Recent advances in semantic image segmentation have mostly been achieved bytraining deep convolutional neural networks (CNNs) for the task. We show how toimprove semantic segmentation through the use of contextual information.Specifically, we explore `patch-patch' context and `patch-background' contextwith deep CNNs. For learning the patch-patch context between image regions, weformulate Conditional Random Fields (CRFs) with CNN-based pairwise potentialfunctions to capture semantic correlations between neighboring patches.Efficient piecewise training of the proposed deep structured model is thenapplied to avoid repeated expensive CRF inference for back propagation. Inorder to capture the patch-background context, we show that a network designwith traditional multi-scale image input and sliding pyramid pooling iseffective for improving performance. Our experiment results set newstate-of-the-art performance on a number of popular semantic segmentationdatasets, including NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow.Particularly, we achieve an intersection-over-union score of $77.8$ on thechallenging PASCAL VOC 2012 dataset.
arxiv-16200-287 | Rectified Gaussian Scale Mixtures and the Sparse Non-Negative Least Squares Problem | http://arxiv.org/pdf/1601.06207v3.pdf | author:Alican Nalci, Igor Fedorov, Bhaskar D. Rao category:cs.LG stat.ML published:2016-01-22 summary:In this paper we introduce a hierarchical Bayesian framework to obtain sparseand non-negative solutions to the sparse non-negative least squares problem(S-NNLS). We introduce a new family of scale mixtures, the Rectified GaussianScale Mixture (R-GSM), to model the sparsity enforcing prior distribution forthe signal of interest. One advantage of the R-GSM prior is that through properchoice of the mixing density it encompasses a wide variety of heavy taileddistributions, such as the rectified Laplacian and rectified Student's tdistributions. Similar to the Gaussian Scale Mixture (GSM) approach, a Type IIExpectation-Maximization framework is developed to estimate thehyper-parameters and obtain a point estimate of the parameter of interest. Inthe proposed method, called rectified Sparse Bayesian Learning (R-SBL), weprovide two ways to perform the Expectation step; Markov-Chain Monte-Carlo(MCMC) simulations and a simple yet effective diagonal approximation approach(DA). Through numerical experiments we show that R-SBL outperforms existingS-NNLS solvers in terms of both signal and support recovery and that theproposed DA approach admits both computational efficiency and numericalaccuracy.
arxiv-16200-288 | megaman: Manifold Learning with Millions of points | http://arxiv.org/pdf/1603.02763v1.pdf | author:James McQueen, Marina Meila, Jacob VanderPlas, Zhongyue Zhang category:cs.LG cs.CG stat.ML published:2016-03-09 summary:Manifold Learning is a class of algorithms seeking a low-dimensionalnon-linear representation of high-dimensional data. Thus manifold learningalgorithms are, at least in theory, most applicable to high-dimensional dataand sample sizes to enable accurate estimation of the manifold. Despite this,most existing manifold learning implementations are not particularly scalable.Here we present a Python package that implements a variety of manifold learningalgorithms in a modular and scalable fashion, using fast approximate neighborssearches and fast sparse eigendecompositions. The package incorporatestheoretical advances in manifold learning, such as the unbiased Laplacianestimator and the estimation of the embedding distortion by the Riemannianmetric method. In benchmarks, even on a single-core desktop computer, our codeembeds millions of data points in minutes, and takes just 200 minutes to embedthe main sample of galaxy spectra from the Sloan Digital Sky Survey ---consisting of 0.6 million samples in 3750-dimensions --- a task which has notpreviously been possible.
arxiv-16200-289 | XGBoost: A Scalable Tree Boosting System | http://arxiv.org/pdf/1603.02754v1.pdf | author:Tianqi Chen, Carlos Guestrin category:cs.LG published:2016-03-09 summary:Tree boosting is a highly effective and widely used machine learning method.In this paper, we describe a scalable end-to-end tree boosting system calledXGBoost, which is used widely by data scientists to achieve state-of-the-artresults on many machine learning challenges. We propose a novel sparsity-awarealgorithm for sparse data and weighted quantile sketch for approximate treelearning. More importantly, we provide insights on cache access patterns, datacompression and sharding to build a scalable tree boosting system. By combiningthese insights, XGBoost scales beyond billions of examples using far fewerresources than existing systems.
arxiv-16200-290 | Computing AIC for black-box models using Generalised Degrees of Freedom: a comparison with cross-validation | http://arxiv.org/pdf/1603.02743v1.pdf | author:Severin Hauenstein, Carsten F. Dormann, Simon N Wood category:stat.ML published:2016-03-09 summary:Generalised Degrees of Freedom (GDF), as defined by Ye (1998 JASA93:120-131), represent the sensitivity of model fits to perturbations of thedata. As such they can be computed for any statistical model, making itpossible, in principle, to derive the number of parameters in machine-learningapproaches. Defined originally for normally distributed data only, we hereinvestigate the potential of this approach for Bernoulli-data. GDF-values formodels of simulated and real data are compared to model complexity-estimatesfrom cross-validation. Similarly, we computed GDF-based AICc for randomForest,neural networks and boosted regression trees and demonstrated its similarity tocross-validation. GDF-estimates for binary data were unstable andinconsistently sensitive to the number of data points perturbed simultaneously,while at the same time being extremely computer-intensive in their calculation.Repeated 10-fold cross-validation was more robust, based on fewer assumptionsand faster to compute. Our findings suggest that the GDF-approach does notreadily transfer to Bernoulli data and a wider range of regression approaches.
arxiv-16200-291 | Pairwise Choice Markov Chains | http://arxiv.org/pdf/1603.02740v1.pdf | author:Stephen Ragain, Johan Ugander category:stat.ML cs.AI published:2016-03-08 summary:As datasets capturing human choices grow in richness and scale, particularlyin online domains, there is an increasing need for choice models that explainand predict complex empirical choices that violate traditional choice-theoreticassumptions such as regularity, stochastic transitivity, or Luce's choiceaxiom. In this work we introduce a Pairwise Choice Markov Chain (PCMC) model ofdiscrete choice that is free of all those assumptions while still satisfyingthe attractive foundational axiom of uniform expansion. Uniform expansion isknown to imply Luce's choice axiom in the context of independent random utilitymodels (RUMs), but the PCMC model is not a RUM (let alone an independent RUM).Inference for the PCMC model is straight-forward, and we thus introduce it asthe first inferentially tractable model of discrete choice known to satisfyuniform expansion without the choice axiom, regularity, or strict stochastictransitivity. It is thus more flexible than even Tversky's Elimination ByAspects model, which assumes regularity and is also known to be inferentiallyintractable. We show that our model learns and predicts syntheticnon-transitive data well. Our analysis also synthesizes several recentobservations connecting the Multinomial Logit (MNL) model and Markov chains;the PCMC model retains the Multinomial Logit model as a special case.
arxiv-16200-292 | A Kernel Test for Three-Variable Interactions with Random Processes | http://arxiv.org/pdf/1603.00929v2.pdf | author:Paul K. Rubenstein, Kacper P. Chwialkowski, Arthur Gretton category:stat.ML 62G10 published:2016-03-02 summary:We apply a wild bootstrap method to the Lancaster three-variable interactionmeasure in order to detect factorisation of the joint distribution on threevariables forming a stationary random process, for which the existingpermutation bootstrap method fails. As in the i.i.d. case, the Lancaster testis found to outperform existing tests in cases for which two independentvariables individually have a weak influence on a third, but that whenconsidered jointly the influence is strong. The main contributions of thispaper are twofold: first, we prove that the Lancaster statistic satisfies theconditions required to estimate the quantiles of the null distribution usingthe wild bootstrap; second, the manner in which this is proved is novel,simpler than existing methods, and can further be applied to other statistics.
arxiv-16200-293 | Discriminative models for robust image classification | http://arxiv.org/pdf/1603.02736v1.pdf | author:Umamahesh Srinivas category:stat.ML cs.CV published:2016-03-08 summary:A variety of real-world tasks involve the classification of images intopre-determined categories. Designing image classification algorithms thatexhibit robustness to acquisition noise and image distortions, particularlywhen the available training data are insufficient to learn accurate models, isa significant challenge. This dissertation explores the development ofdiscriminative models for robust image classification that exploit underlyingsignal structure, via probabilistic graphical models and sparse signalrepresentations. Probabilistic graphical models are widely used in many applications toapproximate high-dimensional data in a reduced complexity set-up. Learninggraphical structures to approximate probability distributions is an area ofactive research. Recent work has focused on learning graphs in a discriminativemanner with the goal of minimizing classification error. In the first part ofthe dissertation, we develop a discriminative learning framework that exploitsthe complementary yet correlated information offered by multiplerepresentations (or projections) of a given signal/image. Specifically, wepropose a discriminative tree-based scheme for feature fusion by explicitlylearning the conditional correlations among such multiple projections in aniterative manner. Experiments reveal the robustness of the resulting graphicalmodel classifier to training insufficiency.
arxiv-16200-294 | Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks | http://arxiv.org/pdf/1602.00991v2.pdf | author:Peter Ondruska, Ingmar Posner category:cs.LG cs.AI cs.CV cs.NE cs.RO published:2016-02-02 summary:This paper presents to the best of our knowledge the first end-to-end objecttracking approach which directly maps from raw sensor input to object tracks insensor space without requiring any feature engineering or system identificationin the form of plant or sensor models. Specifically, our system accepts astream of raw sensor data at one end and, in real-time, produces an estimate ofthe entire environment state at the output including even occluded objects. Weachieve this by framing the problem as a deep learning task and exploitsequence models in the form of recurrent neural networks to learn a mappingfrom sensor measurements to object tracks. In particular, we propose a learningmethod based on a form of input dropout which allows learning in anunsupervised manner, only based on raw, occluded sensor data without access toground-truth annotations. We demonstrate our approach using a synthetic datasetdesigned to mimic the task of tracking objects in 2D laser data -- as commonlyencountered in robotics applications -- and show that it learns to track manydynamic objects despite occlusions and the presence of sensor noise.
arxiv-16200-295 | Super Mario as a String: Platformer Level Generation Via LSTMs | http://arxiv.org/pdf/1603.00930v2.pdf | author:Adam Summerville, Michael Mateas category:cs.NE cs.LG published:2016-03-02 summary:The procedural generation of video game levels has existed for at least 30years, but only recently have machine learning approaches been used to generatelevels without specifying the rules for generation. A number of these havelooked at platformer levels as a sequence of characters and performedgeneration using Markov chains. In this paper we examine the use of LongShort-Term Memory recurrent neural networks (LSTMs) for the purpose ofgenerating levels trained from a corpus of Super Mario Brothers levels. Weanalyze a number of different data representations and how the generated levelsfit into the space of human authored Super Mario Brothers levels.
arxiv-16200-296 | Parsing Linear Context-Free Rewriting Systems with Fast Matrix Multiplication | http://arxiv.org/pdf/1504.08342v3.pdf | author:Shay B. Cohen, Daniel Gildea category:cs.CL cs.FL published:2015-04-30 summary:We describe a matrix multiplication recognition algorithm for a subset ofbinary linear context-free rewriting systems (LCFRS) with running time$O(n^{\omega d})$ where $M(m) = O(m^{\omega})$ is the running time for $m\times m$ matrix multiplication and $d$ is the "contact rank" of the LCFRS --the maximal number of combination and non-combination points that appear in thegrammar rules. We also show that this algorithm can be used as a subroutine toget a recognition algorithm for general binary LCFRS with running time$O(n^{\omega d + 1})$. The currently best known $\omega$ is smaller than$2.38$. Our result provides another proof for the best known result for parsingmildly context sensitive formalisms such as combinatory categorial grammars,head grammars, linear indexed grammars, and tree adjoining grammars, which canbe parsed in time $O(n^{4.76})$. It also shows that inversion transductiongrammars can be parsed in time $O(n^{5.76})$. In addition, binary LCFRSsubsumes many other formalisms and types of grammars, for some of which we alsoimprove the asymptotic complexity of parsing.
arxiv-16200-297 | A regularization-based approach for unsupervised image segmentation | http://arxiv.org/pdf/1603.02649v1.pdf | author:Aleksandar Dimitriev, Matej Kristan category:cs.CV published:2016-03-08 summary:We propose a novel unsupervised image segmentation algorithm, which aims tosegment an image into several coherent parts. It requires no user input, nosupervised learning phase and assumes an unknown number of segments. Itachieves this by first over-segmenting the image into several hundredsuperpixels. These are iteratively joined on the basis of a discriminativeclassifier trained on color and texture information obtained from eachsuperpixel. The output of the classifier is regularized by a Markov randomfield that lends more influence to neighbouring superpixels that are moresimilar. In each iteration, similar superpixels fall under the same label,until only a few coherent regions remain in the image. The algorithm was testedon a standard evaluation data set, where it performs on par withstate-of-the-art algorithms in term of precision and greatly outperforms thestate of the art by reducing the oversegmentation of the object of interest.
arxiv-16200-298 | Small ensembles of kriging models for optimization | http://arxiv.org/pdf/1603.02638v1.pdf | author:Hossein Mohammadi, Rodolphe Le Riche, Eric Touboul category:math.OC cs.LG stat.ML published:2016-03-08 summary:The Efficient Global Optimization (EGO) algorithm uses a conditionalGaus-sian Process (GP) to approximate an objective function known at a finitenumber of observation points and sequentially adds new points which maximizethe Expected Improvement criterion according to the GP. The important factorthat controls the efficiency of EGO is the GP covariance function (or kernel)which should be chosen according to the objective function. Traditionally, apa-rameterized family of covariance functions is considered whose parametersare learned through statistical procedures such as maximum likelihood orcross-validation. However, it may be questioned whether statistical proceduresfor learning covariance functions are the most efficient for optimization asthey target a global agreement between the GP and the observations which is notthe ultimate goal of optimization. Furthermore, statistical learning proceduresare computationally expensive. The main alternative to the statistical learningof the GP is self-adaptation, where the algorithm tunes the kernel parametersbased on their contribution to objective function improvement. Afterquestioning the possibility of self-adaptation for kriging based optimizers,this paper proposes a novel approach for tuning the length-scale of the GP inEGO: At each iteration, a small ensemble of kriging models structured by theirlength-scales is created. All of the models contribute to an iterate in anEGO-like fashion. Then, the set of models is densified around the model whoselength-scale yielded the best iterate and further points are produced.Numerical experiments are provided which motivate the use of manylength-scales. The tested implementation does not perform better than theclassical EGO algorithm in a sequential context but show the potential of theapproach for parallel implementations.
arxiv-16200-299 | DROW: Real-Time Deep Learning based Wheelchair Detection in 2D Range Data | http://arxiv.org/pdf/1603.02636v1.pdf | author:Lucas Beyer, Alexander Hermans, Bastian Leibe category:cs.RO cs.CV cs.LG cs.NE published:2016-03-08 summary:We introduce the DROW detector, a deep learning based detector for 2D rangedata. Laser scanners are lighting invariant, provide accurate range data, andtypically cover a large field of view, making them interesting sensors forrobotics applications. So far, research on detection in laser range data hasbeen dominated by handcrafted features and boosted classifiers, potentiallylosing performance due to suboptimal design choices. We propose a ConvolutionalNeural Network (CNN) based detector for this task. We show how to effectivelyapply CNNs for detection in 2D range data, and propose a depth preprocessingstep and voting scheme that significantly improve CNN performance. Wedemonstrate our approach on wheelchairs and walkers, obtaining state of the artdetection results. Apart from the training data, none of our design choiceslimits the detector to these two classes, though. We provide a ROS node for ourdetector and release our dataset containing 464k laser scans, out of which 24kwere annotated for training.
arxiv-16200-300 | The red one!: On learning to refer to things based on their discriminative properties | http://arxiv.org/pdf/1603.02618v1.pdf | author:Angeliki Lazaridou, Nghia The Pham, Marco Baroni category:cs.CL cs.CV published:2016-03-08 summary:As a first step towards agents learning to communicate about their visualenvironment, we propose a system that, given visual representations of areferent (cat) and a context (sofa), identifies their discriminativeattributes, i.e., properties that distinguish them (has_tail). Moreover,despite the lack of direct supervision at the attribute level, the model learnsto assign plausible attributes to objects (sofa-has_cushion). Finally, wepresent a preliminary experiment confirming the referential success of thepredicted discriminative attributes.
