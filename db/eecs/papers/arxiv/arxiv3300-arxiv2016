arxiv-3300-1 | Learning Heteroscedastic Models by Convex Programming under Group Sparsity | http://arxiv.org/pdf/1304.4549v1.pdf | author:Arnak S. Dalalyan, Mohamed Hebiri, Katia Méziani, Joseph Salmon category:stat.ML published:2013-04-16 summary:Popular sparse estimation methods based on $\ell_1$-relaxation, such as theLasso and the Dantzig selector, require the knowledge of the variance of thenoise in order to properly tune the regularization parameter. This constitutesa major obstacle in applying these methods in several frameworks---such as timeseries, random fields, inverse problems---for which the noise is rarelyhomoscedastic and its level is hard to know in advance. In this paper, wepropose a new approach to the joint estimation of the conditional mean and theconditional variance in a high-dimensional (auto-) regression setting. Anattractive feature of the proposed estimator is that it is efficientlycomputable even for very large scale problems by solving a second-order coneprogram (SOCP). We present theoretical analysis and numerical results assessingthe performance of the proposed procedure.
arxiv-3300-2 | Easy and hard functions for the Boolean hidden shift problem | http://arxiv.org/pdf/1304.4642v1.pdf | author:Andrew M. Childs, Robin Kothari, Maris Ozols, Martin Roetteler category:quant-ph cs.CC cs.LG published:2013-04-16 summary:We study the quantum query complexity of the Boolean hidden shift problem.Given oracle access to f(x+s) for a known Boolean function f, the task is todetermine the n-bit string s. The quantum query complexity of this problemdepends strongly on f. We demonstrate that the easiest instances of thisproblem correspond to bent functions, in the sense that an exact one-queryalgorithm exists if and only if the function is bent. We partially characterizethe hardest instances, which include delta functions. Moreover, we show thatthe problem is easy for random functions, since two queries suffice. Ouralgorithm for random functions is based on performing the pretty goodmeasurement on several copies of a certain state; its analysis relies on theFourier transform. We also use this approach to improve the quantum rejectionsampling approach to the Boolean hidden shift problem.
arxiv-3300-3 | Heterogeneous patterns enhancing static and dynamic texture classification | http://arxiv.org/pdf/1304.4535v1.pdf | author:Núbia Rosa da Silva, Odemir Martinez Bruno category:cs.CV published:2013-04-16 summary:Some mixtures, such as colloids like milk, blood, and gelatin, havehomogeneous appearance when viewed with the naked eye, however, to observe themat the nanoscale is possible to understand the heterogeneity of its components.The same phenomenon can occur in pattern recognition in which it is possible tosee heterogeneous patterns in texture images. However, current methods oftexture analysis can not adequately describe such heterogeneous patterns.Common methods used by researchers analyse the image information in a globalway, taking all its features in an integrated manner. Furthermore, multi-scaleanalysis verifies the patterns at different scales, but still preserving thehomogeneous analysis. On the other hand various methods use textons torepresent the texture, breaking texture down into its smallest unit. To tacklethis problem, we propose a method to identify texture patterns not small astextons at distinct scales enhancing the separability among different types oftexture. We find sub patterns of texture according to the scale and then groupsimilar patterns for a more refined analysis. Tests were performed in fourstatic texture databases and one dynamic one. Results show that our methodprovides better classification rate compared with conventional approaches bothin static and in dynamic texture.
arxiv-3300-4 | PAC Quasi-automatizability of Resolution over Restricted Distributions | http://arxiv.org/pdf/1304.4633v1.pdf | author:Brendan Juba category:cs.DS cs.LG cs.LO published:2013-04-16 summary:We consider principled alternatives to unsupervised learning in data miningby situating the learning task in the context of the subsequent analysis task.Specifically, we consider a query-answering (hypothesis-testing) task: In thecombined task, we decide whether an input query formula is satisfied over abackground distribution by using input examples directly, rather than invokinga two-stage process in which (i) rules over the distribution are learned by anunsupervised learning algorithm and (ii) a reasoning algorithm decides whetheror not the query formula follows from the learned rules. In a previous work(2013), we observed that the learning task could satisfy numerous desirablecriteria in this combined context -- effectively matching what could beachieved by agnostic learning of CNFs from partial information -- that are notknown to be achievable directly. In this work, we show that likewise, there arereasoning tasks that are achievable in such a combined context that are notknown to be achievable directly (and indeed, have been seriously conjectured tobe impossible, cf. (Alekhnovich and Razborov, 2008)). Namely, we test for aresolution proof of the query formula of a given size in quasipolynomial time(that is, "quasi-automatizing" resolution). The learning setting we consider isa partial-information, restricted-distribution setting that generalizeslearning parities over the uniform distribution from partial information,another task that is known not to be achievable directly in various models (cf.(Ben-David and Dichterman, 1998) and (Michael, 2010)).
arxiv-3300-5 | Speckle Reduction in Polarimetric SAR Imagery with Stochastic Distances and Nonlocal Means | http://arxiv.org/pdf/1304.4634v1.pdf | author:Leonardo Torres, Sidnei J. S. Sant'Anna, Corina C. Freitas, Alejandro C. Frery category:cs.IT cs.CV cs.GR math.IT stat.AP stat.ML published:2013-04-16 summary:This paper presents a technique for reducing speckle in PolarimetricSynthetic Aperture Radar (PolSAR) imagery using Nonlocal Means and astatistical test based on stochastic divergences. The main objective is toselect homogeneous pixels in the filtering area through statistical testsbetween distributions. This proposal uses the complex Wishart model to describePolSAR data, but the technique can be extended to other models. The weights ofthe location-variant linear filter are function of the p-values of tests whichverify the hypothesis that two samples come from the same distribution and,therefore, can be used to compute a local mean. The test stems from the familyof (h-phi) divergences which originated in Information Theory. This noveltechnique was compared with the Boxcar, Refined Lee and IDAN filters. Imagequality assessment methods on simulated and real data are employed to validatethe performance of this approach. We show that the proposed filter alsoenhances the polarimetric entropy and preserves the scattering information ofthe targets.
arxiv-3300-6 | GPU Acclerated Automated Feature Extraction from Satellite Images | http://arxiv.org/pdf/1304.3992v1.pdf | author:K. Phani Tejaswi, D. Shanmukha Rao, Thara Nair, A. V. V. Prasad category:cs.DC cs.CV published:2013-04-15 summary:The availability of large volumes of remote sensing data insists on higherdegree of automation in feature extraction, making it a need of the hour.Thehuge quantum of data that needs to be processed entails accelerated processingto be enabled.GPUs, which were originally designed to provide efficientvisualization, are being massively employed for computation intensive parallelprocessing environments. Image processing in general and hence automatedfeature extraction, is highly computation intensive, where performanceimprovements have a direct impact on societal needs. In this context, analgorithm has been formulated for automated feature extraction from apanchromatic or multispectral image based on image processing techniques. TwoLaplacian of Guassian (LoG) masks were applied on the image individuallyfollowed by detection of zero crossing points and extracting the pixels basedon their standard deviation with the surrounding pixels. The two extractedimages with different LoG masks were combined together which resulted in animage with the extracted features and edges. Finally the user is at liberty toapply the image smoothing step depending on the noise content in the extractedimage. The image is passed through a hybrid median filter to remove the saltand pepper noise from the image. This paper discusses the aforesaid algorithmfor automated feature extraction, necessity of deployment of GPUs for the same;system-level challenges and quantifies the benefits of integrating GPUs in suchenvironment. The results demonstrate that substantial enhancement inperformance margin can be achieved with the best utilization of GPU resourcesand an efficient parallelization strategy. Performance results in comparisonwith the conventional computing scenario have provided a speedup of 20x, onrealization of this parallelizing strategy.
arxiv-3300-7 | Multiobjective optimization in Gene Expression Programming for Dew Point | http://arxiv.org/pdf/1304.4055v2.pdf | author:Siddharth Shroff, Vipul Dabhi category:cs.NE published:2013-04-15 summary:The processes occurring in climatic change evolution and their variationsplay a major role in environmental engineering. Different techniques are usedto model the relationship between temperatures, dew point and relativehumidity. Gene expression programming is capable of modelling complex realitieswith great accuracy, allowing, at the same time, the extraction of knowledgefrom the evolved models compared to other learning algorithms. This researchaims to use Gene Expression Programming for modelling of dew point. Generally,accuracy of the model is the only objective used by selection mechanism of GEP.This will evolve large size models with low training error. To avoid thissituation, use of multiple objectives, like accuracy and size of the model arepreferred by Genetic Programming practitioners. Multi-objective problem finds aset of solutions satisfying the objectives given by decision maker.Multiobjective based GEP will be used to evolve simple models. Variousalgorithms widely used for multi objective optimization like NSGA II and SPEA 2are tested for different test cases. The results obtained thereafter gives ideathat SPEA 2 is better algorithm compared to NSGA II based on the features likeexecution time, number of solutions obtained and convergence rate. Thuscompared to models obtained by GEP, multi-objective algorithms fetch bettersolutions considering the dual objectives of fitness and size of the equation.These simple models can be used to predict dew point.
arxiv-3300-8 | Hubiness, length, crossings and their relationships in dependency trees | http://arxiv.org/pdf/1304.4086v5.pdf | author:Ramon Ferrer-i-Cancho category:cs.CL cs.DM cs.SI physics.soc-ph published:2013-04-15 summary:Here tree dependency structures are studied from three differentperspectives: their degree variance (hubiness), the mean dependency length andthe number of dependency crossings. Bounds that reveal pairwise dependenciesamong these three metrics are derived. Hubiness (the variance of degrees) playsa central role: the mean dependency length is bounded below by hubiness whilethe number of crossings is bounded above by hubiness. Our findings suggest thatthe online memory cost of a sentence might be determined not just by theordering of words but also by the hubiness of the underlying structure. The 2ndmoment of degree plays a crucial role that is reminiscent of its role in largecomplex networks.
arxiv-3300-9 | A new Bayesian ensemble of trees classifier for identifying multi-class labels in satellite images | http://arxiv.org/pdf/1304.4077v2.pdf | author:Reshu Agarwal, Pritam Ranjan, Hugh Chipman category:stat.ME cs.CV cs.LG published:2013-04-15 summary:Classification of satellite images is a key component of many remote sensingapplications. One of the most important products of a raw satellite image isthe classified map which labels the image pixels into meaningful classes.Though several parametric and non-parametric classifiers have been developedthus far, accurate labeling of the pixels still remains a challenge. In thispaper, we propose a new reliable multiclass-classifier for identifying classlabels of a satellite image in remote sensing applications. The proposedmulticlass-classifier is a generalization of a binary classifier based on theflexible ensemble of regression trees model called Bayesian Additive RegressionTrees (BART). We used three small areas from the LANDSAT 5 TM image, acquiredon August 15, 2009 (path/row: 08/29, L1T product, UTM map projection) overKings County, Nova Scotia, Canada to classify the land-use. Several predictionaccuracy and uncertainty measures have been used to compare the reliability ofthe proposed classifier with the state-of-the-art classifiers in remotesensing.
arxiv-3300-10 | Link Prediction with Social Vector Clocks | http://arxiv.org/pdf/1304.4058v1.pdf | author:Conrad Lee, Bobo Nick, Ulrik Brandes, Pádraig Cunningham category:cs.SI physics.soc-ph stat.ML published:2013-04-15 summary:State-of-the-art link prediction utilizes combinations of complex featuresderived from network panel data. We here show that computationally lessexpensive features can achieve the same performance in the common scenario inwhich the data is available as a sequence of interactions. Our features arebased on social vector clocks, an adaptation of the vector-clock conceptintroduced in distributed computing to social interaction networks. In fact,our experiments suggest that by taking into account the order and spacing ofinteractions, social vector clocks exploit different aspects of link formationso that their combination with previous approaches yields the most accuratepredictor to date.
arxiv-3300-11 | Shadow Estimation Method for "The Episolar Constraint: Monocular Shape from Shadow Correspondence" | http://arxiv.org/pdf/1304.4112v1.pdf | author:Austin Abrams, Chris Hawley, Kylia Miskell, Adina Stoica, Nathan Jacobs, Robert Pless category:cs.CV published:2013-04-15 summary:Recovering shadows is an important step for many vision algorithms. Currentapproaches that work with time-lapse sequences are limited to simplethresholding heuristics. We show these approaches only work with very carefultuning of parameters, and do not work well for long-term time-lapse sequencestaken over the span of many months. We introduce a parameter-free expectationmaximization approach which simultaneously estimates shadows, albedo, surfacenormals, and skylight. This approach is more accurate than previous methods,works over both very short and very long sequences, and is robust to theeffects of nonlinear camera response. Finally, we demonstrate that the shadowmasks derived through this algorithm substantially improve the performance ofsun-based photometric stereo compared to earlier shadow mask estimation.
arxiv-3300-12 | Coordinating metaheuristic agents with swarm intelligence | http://arxiv.org/pdf/1304.4051v1.pdf | author:Mehmet Emin Aydin category:cs.MA cs.NE published:2013-04-15 summary:Coordination of multi agent systems remains as a problem since there is noprominent method to completely solve this problem. Metaheuristic agents arespecific implementations of multi-agent systems, which imposes working togetherto solve optimisation problems with metaheuristic algorithms. The idea borrowedfrom swarm intelligence seems working much better than those implementationssuggested before. This paper reports the performance of swarms of simulatedannealing agents collaborating with particle swarm optimization algorithm. Theproposed approach is implemented for multidimensional knapsack problem and hasresulted much better than some other works published before.
arxiv-3300-13 | Multispectral Spatial Characterization: Application to Mitosis Detection in Breast Cancer Histopathology | http://arxiv.org/pdf/1304.4041v1.pdf | author:H. Irshad, A. Gouaillard, L. Roux, D. Racoceanu category:cs.CV published:2013-04-15 summary:Accurate detection of mitosis plays a critical role in breast cancerhistopathology. Manual detection and counting of mitosis is tedious and subjectto considerable inter- and intra-reader variations. Multispectral imaging is arecent medical imaging technology, proven successful in increasing thesegmentation accuracy in other fields. This study aims at improving theaccuracy of mitosis detection by developing a specific solution usingmultispectral and multifocal imaging of breast cancer histopathological data.We propose to enable clinical routine-compliant quality of mitosisdiscrimination from other objects. The proposed framework includescomprehensive analysis of spectral bands and z-stack focus planes, detection ofexpected mitotic regions (candidates) in selected focus planes and spectralbands, computation of multispectral spatial features for each candidate,selection of multispectral spatial features and a study of differentstate-of-the-art classification methods for candidates classification asmitotic or non mitotic figures. This framework has been evaluated on MITOSmultispectral medical dataset and achieved 60% detection rate and 57%F-Measure. Our results indicate that multispectral spatial features have moreinformation for mitosis classification in comparison with white spectral bandfeatures, being therefore a very promising exploration area to improve thequality of the diagnosis assistance in histopathology.
arxiv-3300-14 | Automatic case acquisition from texts for process-oriented case-based reasoning | http://arxiv.org/pdf/1304.3879v1.pdf | author:Valmi Dufour-Lussier, Florence Le Ber, Jean Lieber, Emmanuel Nauer category:cs.AI cs.CL published:2013-04-14 summary:This paper introduces a method for the automatic acquisition of a rich caserepresentation from free text for process-oriented case-based reasoning. Caseengineering is among the most complicated and costly tasks in implementing acase-based reasoning system. This is especially so for process-orientedcase-based reasoning, where more expressive case representations are generallyused and, in our opinion, actually required for satisfactory case adaptation.In this context, the ability to acquire cases automatically from proceduraltexts is a major step forward in order to reason on processes. We thereforedetail a methodology that makes case acquisition from processes described asfree text possible, with special attention given to assembly instruction texts.This methodology extends the techniques we used to extract actions from cookingrecipes. We argue that techniques taken from natural language processing arerequired for this task, and that they give satisfactory results. An evaluationbased on our implemented prototype extracting workflows from recipe texts isprovided.
arxiv-3300-15 | An accelerated CLPSO algorithm | http://arxiv.org/pdf/1304.3892v1.pdf | author:Muhammad Omer Bin Saeed, Muhammad Saqib Sohail, Syed Zeeshan Rizvi, Mobien Shoaib, Asrar Ul Haq Sheikh category:cs.NE published:2013-04-14 summary:The particle swarm approach provides a low complexity solution to theoptimization problem among various existing heuristic algorithms. Recentadvances in the algorithm resulted in improved performance at the cost ofincreased computational complexity, which is undesirable. Literature shows thatthe particle swarm optimization algorithm based on comprehensive learningprovides the best complexity-performance trade-off. We show how to reduce thecomplexity of this algorithm further, with a slight but acceptable performanceloss. This enhancement allows the application of the algorithm in time criticalapplications, such as, real-time tracking, equalization etc.
arxiv-3300-16 | Single View Depth Estimation from Examples | http://arxiv.org/pdf/1304.3915v1.pdf | author:Tal Hassner, Ronen Basri category:cs.CV 68T45 published:2013-04-14 summary:We describe a non-parametric, "example-based" method for estimating the depthof an object, viewed in a single photo. Our method consults a database ofexample 3D geometries, searching for those which look similar to the object inthe photo. The known depths of the selected database objects act as shapepriors which constrain the process of estimating the object's depth. We showhow this process can be performed by optimizing a well defined targetlikelihood function, via a hard-EM procedure. We address the problem ofrepresenting the (possibly infinite) variability of viewing conditions with afinite (and often very small) example set, by proposing an on-the-fly exampleupdate scheme. We further demonstrate the importance of non-stationarity inavoiding misleading examples when estimating structured shapes. We evaluate ourmethod and present both qualitative as well as quantitative results forchallenging object classes. Finally, we show how this same technique may bereadily applied to a number of related problems. These include the novel taskof estimating the occluded depth of an object's backside and the task oftailoring custom fitting image-maps for input depths.
arxiv-3300-17 | Identification of biologically relevant subtypes via preweighted sparse clustering | http://arxiv.org/pdf/1304.3760v2.pdf | author:Sheila Gaynor, Eric Bair category:stat.ME cs.LG q-bio.QM stat.AP stat.ML published:2013-04-13 summary:Cluster analysis methods are used to identify homogeneous subgroups in a dataset. Frequently one applies cluster analysis in order to identify biologicallyinteresting subgroups. In particular, one may wish to identify subgroups thatare associated with a particular outcome of interest. Conventional clusteringmethods often fail to identify such subgroups, particularly when there are alarge number of high-variance features in the data set. Conventional methodsmay identify clusters associated with these high-variance features when onewishes to obtain secondary clusters that are more interesting biologically ormore strongly associated with a particular outcome of interest. We describe amodification of the sparse clustering method of Witten and Tibshirani (2010)that can be used to identify such secondary clusters or clusters associatedwith an outcome of interest. We show that this method can correctly identifysuch clusters of interest in several simulation scenarios. The method is alsoapplied to a large case-control study of TMD and a leukemia microarray dataset.
arxiv-3300-18 | An Improved ACS Algorithm for the Solutions of Larger TSP Problems | http://arxiv.org/pdf/1304.3763v1.pdf | author:Md. Rakib Hassan, Md. Kamrul Hasan, M. M. A. Hashem category:cs.AI cs.DS cs.NE published:2013-04-13 summary:Solving large traveling salesman problem (TSP) in an efficient way is achallenging area for the researchers of computer science. This paper presents amodified version of the ant colony system (ACS) algorithm called Red-Black AntColony System (RB-ACS) for the solutions of TSP which is the most prominentmember of the combinatorial optimization problem. RB-ACS uses the concept ofant colony system together with the parallel search of genetic algorithm forobtaining the optimal solutions quickly. In this paper, it is shown that theproposed RB-ACS algorithm yields significantly better performance than theexisting best-known algorithms.
arxiv-3300-19 | A New Homogeneity Inter-Clusters Measure in SemiSupervised Clustering | http://arxiv.org/pdf/1304.3840v1.pdf | author:Badreddine Meftahi, Ourida Ben Boubaker Saidi category:cs.LG published:2013-04-13 summary:Many studies in data mining have proposed a new learning calledsemi-Supervised. Such type of learning combines unlabeled and labeled datawhich are hard to obtain. However, in unsupervised methods, the only unlabeleddata are used. The problem of significance and the effectiveness ofsemi-supervised clustering results is becoming of main importance. This paperpursues the thesis that muchgreater accuracy can be achieved in such clusteringby improving the similarity computing. Hence, we introduce a new approach ofsemisupervised clustering using an innovative new homogeneity measure ofgenerated clusters. Our experimental results demonstrate significantly improvedaccuracy as a result.
arxiv-3300-20 | Solving Linear Equations Using a Jacobi Based Time-Variant Adaptive Hybrid Evolutionary Algorithm | http://arxiv.org/pdf/1304.3792v1.pdf | author:A. R. M. Jalal Uddin Jamali, M. M. A. Hashem, Md. Bazlar Rahman category:cs.NE published:2013-04-13 summary:Large set of linear equations, especially for sparse and structuredcoefficient (matrix) equations, solutions using classical methods becomearduous. And evolutionary algorithms have mostly been used to solve variousoptimization and learning problems. Recently, hybridization of classicalmethods (Jacobi method and Gauss-Seidel method) with evolutionary computationtechniques have successfully been applied in linear equation solving. In theboth above hybrid evolutionary methods, uniform adaptation (UA) techniques areused to adapt relaxation factor. In this paper, a new Jacobi Based Time-VariantAdaptive (JBTVA) hybrid evolutionary algorithm is proposed. In this algorithm,a Time-Variant Adaptive (TVA) technique of relaxation factor is introducedaiming at both improving the fine local tuning and reducing the disadvantage ofuniform adaptation of relaxation factors. This algorithm integrates the Jacobibased SR method with time variant adaptive evolutionary algorithm. Theconvergence theorems of the proposed algorithm are proved theoretically. Andthe performance of the proposed algorithm is compared with JBUA hybridevolutionary algorithm and classical methods in the experimental domain. Theproposed algorithm outperforms both the JBUA hybrid algorithm and classicalmethods in terms of convergence speed and effectiveness.
arxiv-3300-21 | Improving Generalization Ability of Genetic Programming: Comparative Study | http://arxiv.org/pdf/1304.3779v1.pdf | author:Tejashvi R. Naik, Vipul K. Dabhi category:cs.NE published:2013-04-13 summary:In the field of empirical modeling using Genetic Programming (GP), it isimportant to evolve solution with good generalization ability. Generalizationability of GP solutions get affected by two important issues: bloat andover-fitting. Bloat is uncontrolled growth of code without any gain in fitnessand important issue in GP. We surveyed and classified existing literaturerelated to different techniques used by GP research community to deal with theissue of bloat. Moreover, the classifications of different bloat controlapproaches and measures for bloat are discussed. Next, we tested four bloatcontrol methods: Tarpeian, double tournament, lexicographic parsimony pressurewith direct bucketing and ratio bucketing on six different problems andidentified where each bloat control method performs well on per problem basis.Based on the analysis of each method, we combined two methods: doubletournament (selection method) and Tarpeian method (works before evaluation) toavoid bloated solutions and compared with the results obtained from individualperformance of double tournament method. It was found that the results wereimproved with this combination of two methods.
arxiv-3300-22 | The risks of mixing dependency lengths from sequences of different length | http://arxiv.org/pdf/1304.3841v2.pdf | author:Ramon Ferrer-i-Cancho, Haitao Liu category:cs.CL published:2013-04-13 summary:Mixing dependency lengths from sequences of different length is a commonpractice in language research. However, the empirical distribution ofdependency lengths of sentences of the same length differs from that ofsentences of varying length and the distribution of dependency lengths dependson sentence length for real sentences and also under the null hypothesis thatdependencies connect vertices located in random positions of the sequence. Thissuggests that certain results, such as the distribution of syntactic dependencylengths mixing dependencies from sentences of varying length, could be a mereconsequence of that mixing. Furthermore, differences in the global averages ofdependency length (mixing lengths from sentences of varying length) for twodifferent languages do not simply imply a priori that one language optimizesdependency lengths better than the other because those differences could be dueto differences in the distribution of sentence lengths and other factors.
arxiv-3300-23 | A Novel Metaheuristics To Solve Mixed Shop Scheduling Problems | http://arxiv.org/pdf/1304.3612v1.pdf | author:V. Ravibabu category:cs.NE published:2013-04-12 summary:This paper represents the metaheuristics proposed for solving a class of ShopScheduling problem. The Bacterial Foraging Optimization algorithm is featuredwith Ant Colony Optimization algorithm and proposed as a natural inspiredcomputing approach to solve the Mixed Shop Scheduling problem. The Mixed Shopis the combination of Job Shop, Flow Shop and Open Shop scheduling problems.The sample instances for all mentioned Shop problems are used as test data andMixed Shop survive its computational complexity to minimize the makespan. Thecomputational results show that the proposed algorithm is gentler to solve andperforms better than the existing algorithms.
arxiv-3300-24 | Modified Soft Brood Crossover in Genetic Programming | http://arxiv.org/pdf/1304.3610v1.pdf | author:Hardik M. Parekh, Vipul K. Dabhi category:cs.NE published:2013-04-12 summary:Premature convergence is one of the important issues while using GeneticProgramming for data modeling. It can be avoided by improving populationdiversity. Intelligent genetic operators can help to improve the populationdiversity. Crossover is an important operator in Genetic Programming. So, wehave analyzed number of intelligent crossover operators and proposed analgorithm with the modification of soft brood crossover operator. It will helpto improve the population diversity and reduce the premature convergence. Wehave performed experiments on three different symbolic regression problems.Then we made the performance comparison of our proposed crossover (ModifiedSoft Brood Crossover) with the existing soft brood crossover and subtreecrossover operators.
arxiv-3300-25 | Identifying cancer subtypes in glioblastoma by combining genomic, transcriptomic and epigenomic data | http://arxiv.org/pdf/1304.3577v2.pdf | author:Richard S. Savage, Zoubin Ghahramani, Jim E. Griffin, Paul Kirk, David L. Wild category:q-bio.GN stat.ML published:2013-04-12 summary:We present a nonparametric Bayesian method for disease subtype discovery inmulti-dimensional cancer data. Our method can simultaneously analyse a widerange of data types, allowing for both agreement and disagreement between theirunderlying clustering structure. It includes feature selection and infers themost likely number of disease subtypes, given the data. We apply the method to 277 glioblastoma samples from The Cancer Genome Atlas,for which there are gene expression, copy number variation, methylation andmicroRNA data. We identify 8 distinct consensus subtypes and study theirprognostic value for death, new tumour events, progression and recurrence. Theconsensus subtypes are prognostic of tumour recurrence (log-rank p-value of$3.6 \times 10^{-4}$ after correction for multiple hypothesis tests). This isdriven principally by the methylation data (log-rank p-value of $2.0 \times10^{-3}$) but the effect is strengthened by the other 3 data types,demonstrating the value of integrating multiple data types. Of particular note is a subtype of 47 patients characterised by very lowlevels of methylation. This subtype has very low rates of tumour recurrence andno new events in 10 years of follow up. We also identify a small geneexpression subtype of 6 patients that shows particularly poor survivaloutcomes. Additionally, we note a consensus subtype that showly a highlydistinctive data signature and suggest that it is therefore a biologicallydistinct subtype of glioblastoma. The code is available from https://sites.google.com/site/multipledatafusion/
arxiv-3300-26 | Advice-Efficient Prediction with Expert Advice | http://arxiv.org/pdf/1304.3708v1.pdf | author:Yevgeny Seldin, Peter Bartlett, Koby Crammer category:cs.LG stat.ML published:2013-04-12 summary:Advice-efficient prediction with expert advice (in analogy to label-efficientprediction) is a variant of prediction with expert advice game, where on eachround of the game we are allowed to ask for advice of a limited number $M$ outof $N$ experts. This setting is especially interesting when asking for adviceof every expert on every round is expensive. We present an algorithm foradvice-efficient prediction with expert advice that achieves$O(\sqrt{\frac{N}{M}T\ln N})$ regret on $T$ rounds of the game.
arxiv-3300-27 | Astronomical Image Denoising Using Dictionary Learning | http://arxiv.org/pdf/1304.3573v1.pdf | author:Simon Beckouche, Jean-Luc Starck, Jalal Fadili category:astro-ph.IM cs.CV published:2013-04-12 summary:Astronomical images suffer a constant presence of multiple defects that areconsequences of the intrinsic properties of the acquisition equipments, andatmospheric conditions. One of the most frequent defects in astronomicalimaging is the presence of additive noise which makes a denoising stepmandatory before processing data. During the last decade, a particular modelingscheme, based on sparse representations, has drawn the attention of an evergrowing community of researchers. Sparse representations offer a promisingframework to many image and signal processing tasks, especially denoising andrestoration applications. At first, the harmonics, wavelets, and similar basesand overcomplete representations have been considered as candidate domains toseek the sparsest representation. A new generation of algorithms, based ondata-driven dictionaries, evolved rapidly and compete now with theoff-the-shelf fixed dictionaries. While designing a dictionary beforehand leanson a guess of the most appropriate representative elementary forms andfunctions, the dictionary learning framework offers to construct the dictionaryupon the data themselves, which provides us with a more flexible setup tosparse modeling and allows to build more sophisticated dictionaries. In thispaper, we introduce the Centered Dictionary Learning (CDL) method and we studyits performances for astronomical image denoising. We show how CDL outperformswavelet or classic dictionary learning denoising techniques on astronomicalimages, and we give a comparison of the effect of these different algorithms onthe photometry of the denoised images.
arxiv-3300-28 | Towards more accurate clustering method by using dynamic time warping | http://arxiv.org/pdf/1304.3745v1.pdf | author:Khadoudja Ghanem category:cs.LG stat.ML published:2013-04-12 summary:An intrinsic problem of classifiers based on machine learning (ML) methods isthat their learning time grows as the size and complexity of the trainingdataset increases. For this reason, it is important to have efficientcomputational methods and algorithms that can be applied on large datasets,such that it is still possible to complete the machine learning tasks inreasonable time. In this context, we present in this paper a more accuratesimple process to speed up ML methods. An unsupervised clustering algorithm iscombined with Expectation, Maximization (EM) algorithm to develop an efficientHidden Markov Model (HMM) training. The idea of the proposed process consistsof two steps. In the first step, training instances with similar inputs areclustered and a weight factor which represents the frequency of these instancesis assigned to each representative cluster. Dynamic Time Warping technique isused as a dissimilarity function to cluster similar examples. In the secondstep, all formulas in the classical HMM training algorithm (EM) associated withthe number of training instances are modified to include the weight factor inappropriate terms. This process significantly accelerates HMM training whilemaintaining the same initial, transition and emission probabilities matrixes asthose obtained with the classical HMM training algorithm. Accordingly, theclassification accuracy is preserved. Depending on the size of the trainingset, speedups of up to 2200 times is possible when the size is about 100.000instances. The proposed approach is not limited to training HMMs, but it can beemployed for a large variety of MLs methods.
arxiv-3300-29 | Distributed dictionary learning over a sensor network | http://arxiv.org/pdf/1304.3568v1.pdf | author:Pierre Chainais, Cédric Richard category:stat.ML cs.LG stat.AP published:2013-04-12 summary:We consider the problem of distributed dictionary learning, where a set ofnodes is required to collectively learn a common dictionary from noisymeasurements. This approach may be useful in several contexts including sensornetworks. Diffusion cooperation schemes have been proposed to solve thedistributed linear regression problem. In this work we focus on adiffusion-based adaptive dictionary learning strategy: each node recordsobservations and cooperates with its neighbors by sharing its local dictionary.The resulting algorithm corresponds to a distributed block coordinate descent(alternate optimization). Beyond dictionary learning, this strategy could beadapted to many matrix factorization problems and generalized to varioussettings. This article presents our approach and illustrates its efficiency onsome numerical examples.
arxiv-3300-30 | Improvement studies on neutron-gamma separation in HPGe detectors by using neural networks | http://arxiv.org/pdf/1304.3209v1.pdf | author:Serkan Akkoyun, Tuncay Bayram, S. Okan Kara category:cs.NE nucl-ex published:2013-04-11 summary:The neutrons emitted in heavy-ion fusion-evaporation (HIFE) reactionstogether with the gamma-rays cause unwanted backgrounds in gamma-ray spectra.Especially in the nuclear reactions, where relativistic ion beams (RIBs) areused, these neutrons are serious problem. They have to be rejected in order toobtain clearer gamma-ray peaks. In this study, the radiation energy and threecriteria which were previously determined for separation between neutron andgamma-rays in the HPGe detectors have been used in artificial neural network(ANN) for improving of the decomposition power. According to the preliminaryresults obtained from ANN method, the ratio of neutron rejection has beenimproved by a factor of 1.27 and the ratio of the lost in gamma-rays has beendecreased by a factor of 0.50.
arxiv-3300-31 | Scaling the Indian Buffet Process via Submodular Maximization | http://arxiv.org/pdf/1304.3285v4.pdf | author:Colorado Reed, Zoubin Ghahramani category:stat.ML cs.LG published:2013-04-11 summary:Inference for latent feature models is inherently difficult as the inferencespace grows exponentially with the size of the input data and number of latentfeatures. In this work, we use Kurihara & Welling (2008)'smaximization-expectation framework to perform approximate MAP inference forlinear-Gaussian latent feature models with an Indian Buffet Process (IBP)prior. This formulation yields a submodular function of the features thatcorresponds to a lower bound on the model evidence. By adding a constant tothis function, we obtain a nonnegative submodular function that can bemaximized via a greedy algorithm that obtains at least a one-thirdapproximation to the optimal solution. Our inference method scales linearlywith the size of the input data, and we show the efficacy of our method on thelargest datasets currently analyzed using an IBP model.
arxiv-3300-32 | Rotational Projection Statistics for 3D Local Surface Description and Object Recognition | http://arxiv.org/pdf/1304.3192v1.pdf | author:Yulan Guo, Ferdous Sohel, Mohammed Bennamoun, Min Lu, Jianwei Wan category:cs.CV I.4; I.5.4 published:2013-04-11 summary:Recognizing 3D objects in the presence of noise, varying mesh resolution,occlusion and clutter is a very challenging task. This paper presents a novelmethod named Rotational Projection Statistics (RoPS). It has three majormodules: Local Reference Frame (LRF) definition, RoPS feature description and3D object recognition. We propose a novel technique to define the LRF bycalculating the scatter matrix of all points lying on the local surface. RoPSfeature descriptors are obtained by rotationally projecting the neighboringpoints of a feature point onto 2D planes and calculating a set of statistics(including low-order central moments and entropy) of the distribution of theseprojected points. Using the proposed LRF and RoPS descriptor, we present ahierarchical 3D object recognition algorithm. The performance of the proposedLRF, RoPS descriptor and object recognition algorithm was rigorously tested ona number of popular and publicly available datasets. Our proposed techniquesexhibited superior performance compared to existing techniques. We also showedthat our method is robust with respect to noise and varying mesh resolution.Our RoPS based algorithm achieved recognition rates of 100%, 98.9%, 95.4% and96.0% respectively when tested on the Bologna, UWA, Queen's and Ca' FoscariVenezia Datasets.
arxiv-3300-33 | An Approach to Solve Linear Equations Using a Time-Variant Adaptation Based Hybrid Evolutionary Algorithm | http://arxiv.org/pdf/1304.3200v1.pdf | author:A. R. M. Jalal Uddin Jamali, M. M. A. Hashem, Md. Bazlar Rahman category:cs.NE cs.NA published:2013-04-11 summary:For small number of equations, systems of linear (and sometimes nonlinear)equations can be solved by simple classical techniques. However, for largenumber of systems of linear (or nonlinear) equations, solutions using classicalmethod become arduous. On the other hand evolutionary algorithms have mostlybeen used to solve various optimization and learning problems. Recently,hybridization of evolutionary algorithm with classical Gauss-Seidel basedSuccessive Over Relaxation (SOR) method has successfully been used to solvelarge number of linear equations; where a uniform adaptation (UA) technique ofrelaxation factor is used. In this paper, a new hybrid algorithm is proposed inwhich a time-variant adaptation (TVA) technique of relaxation factor is usedinstead of uniform adaptation technique to solve large number of linearequations. The convergence theorems of the proposed algorithms are provedtheoretically. And the performance of the proposed TVA-based algorithm iscompared with the UA-based hybrid algorithm in the experimental domain. Theproposed algorithm outperforms the hybrid one in terms of efficiency.
arxiv-3300-34 | Extension of hidden markov model for recognizing large vocabulary of sign language | http://arxiv.org/pdf/1304.3265v1.pdf | author:Maher Jebali, Patrice Dalle, Mohamed Jemni category:cs.CL published:2013-04-11 summary:Computers still have a long way to go before they can interact with users ina truly natural fashion. From a users perspective, the most natural way tointeract with a computer would be through a speech and gesture interface.Although speech recognition has made significant advances in the past tenyears, gesture recognition has been lagging behind. Sign Languages (SL) are themost accomplished forms of gestural communication. Therefore, their automaticanalysis is a real challenge, which is interestingly implied to their lexicaland syntactic organization levels. Statements dealing with sign language occupya significant interest in the Automatic Natural Language Processing (ANLP)domain. In this work, we are dealing with sign language recognition, inparticular of French Sign Language (FSL). FSL has its own specificities, suchas the simultaneity of several parameters, the important role of the facialexpression or movement and the use of space for the proper utteranceorganization. Unlike speech recognition, Frensh sign language (FSL) eventsoccur both sequentially and simultaneously. Thus, the computational processingof FSL is too complex than the spoken languages. We present a novel approachbased on HMM to reduce the recognition complexity.
arxiv-3300-35 | Probabilistic Classification using Fuzzy Support Vector Machines | http://arxiv.org/pdf/1304.3345v1.pdf | author:Marzieh Parandehgheibi category:cs.LG math.ST stat.TH published:2013-04-11 summary:In medical applications such as recognizing the type of a tumor as Malignantor Benign, a wrong diagnosis can be devastating. Methods like Fuzzy SupportVector Machines (FSVM) try to reduce the effect of misplaced training points byassigning a lower weight to the outliers. However, there are still uncertainpoints which are similar to both classes and assigning a class by the giveninformation will cause errors. In this paper, we propose a two-phaseclassification method which probabilistically assigns the uncertain points toeach of the classes. The proposed method is applied to the Breast CancerWisconsin (Diagnostic) Dataset which consists of 569 instances in 2 classes ofMalignant and Benign. This method assigns certain instances to theirappropriate classes with probability of one, and the uncertain instances toeach of the classes with associated probabilities. Therefore, based on thedegree of uncertainty, doctors can suggest further examinations before makingthe final diagnosis.
arxiv-3300-36 | Evolution of Swarm Robotics Systems with Novelty Search | http://arxiv.org/pdf/1304.3362v1.pdf | author:Jorge Gomes, Paulo Urbano, Anders Lyhne Christensen category:cs.NE published:2013-04-11 summary:Novelty search is a recent artificial evolution technique that challengestraditional evolutionary approaches. In novelty search, solutions are rewardedbased on their novelty, rather than their quality with respect to a predefinedobjective. The lack of a predefined objective precludes premature convergencecaused by a deceptive fitness function. In this paper, we apply novelty searchcombined with NEAT to the evolution of neural controllers for homogeneousswarms of robots. Our empirical study is conducted in simulation, and we use acommon swarm robotics task - aggregation, and a more challenging task - sharingof an energy recharging station. Our results show that novelty search isunaffected by deception, is notably effective in bootstrapping the evolution,can find solutions with lower complexity than fitness-based evolution, and canfind a broad diversity of solutions for the same task. Even in non-deceptivesetups, novelty search achieves solution qualities similar to those obtained intraditional fitness-based evolution. Our study also encompasses variants ofnovelty search that work in concert with fitness-based evolution to combine theexploratory character of novelty search with the exploitatory character ofobjective-based evolution. We show that these variants can further improve theperformance of novelty search. Overall, our study shows that novelty search isa promising alternative for the evolution of controllers for robotic swarms.
arxiv-3300-37 | Generic Behaviour Similarity Measures for Evolutionary Swarm Robotics | http://arxiv.org/pdf/1304.3393v1.pdf | author:Jorge Gomes, Anders Lyhne Christensen category:cs.NE published:2013-04-11 summary:Novelty search has shown to be a promising approach for the evolution ofcontrollers for swarm robotics. In existing studies, however, the experimenterhad to craft a domain dependent behaviour similarity measure to use noveltysearch in swarm robotics applications. The reliance on hand-crafted similaritymeasures places an additional burden to the experimenter and introduces a biasin the evolutionary process. In this paper, we propose and compare twotask-independent, generic behaviour similarity measures: combined state countand sampled average state. The proposed measures use the values of sensors andeffectors recorded for each individual robot of the swarm. The characterisationof the group-level behaviour is then obtained by combining the sensor-effectorvalues from all the robots. We evaluate the proposed measures in an aggregationtask and in a resource sharing task. We show that the generic measures matchthe performance of domain dependent measures in terms of solution quality. Ourresults indicate that the proposed generic measures operate as effectivebehaviour similarity measures, and that it is possible to leverage the benefitsof novelty search without having to craft domain specific similarity measures.
arxiv-3300-38 | Merging Satellite Measurements of Rainfall Using Multi-scale Imagery Technique | http://arxiv.org/pdf/1304.3406v1.pdf | author:Seyed Hamed Alemohammad, Dara Entekhabi category:cs.CV cs.IR published:2013-04-11 summary:Several passive microwave satellites orbit the Earth and measure rainfall.These measurements have the advantage of almost full global coverage whencompared to surface rain gauges. However, these satellites have low temporalrevisit and missing data over some regions. Image fusion is a useful techniqueto fill in the gaps of one image (one satellite measurement) using another one.The proposed algorithm uses an iterative fusion scheme to integrate informationfrom two satellite measurements. The algorithm is implemented on two datasetsfor 7 years of half-hourly data. The results show significant improvements inrain detection and rain intensity in the merged measurements.
arxiv-3300-39 | A Generalized Online Mirror Descent with Applications to Classification and Regression | http://arxiv.org/pdf/1304.2994v3.pdf | author:Francesco Orabona, Koby Crammer, Nicolò Cesa-Bianchi category:cs.LG published:2013-04-10 summary:Online learning algorithms are fast, memory-efficient, easy to implement, andapplicable to many prediction problems, including classification, regression,and ranking. Several online algorithms were proposed in the past few decades,some based on additive updates, like the Perceptron, and some on multiplicativeupdates, like Winnow. A unifying perspective on the design and the analysis ofonline algorithms is provided by online mirror descent, a general predictionstrategy from which most first-order algorithms can be obtained as specialcases. We generalize online mirror descent to time-varying regularizers withgeneric updates. Unlike standard mirror descent, our more general formulationalso captures second order algorithms, algorithms for composite losses andalgorithms for adaptive filtering. Moreover, we recover, and sometimes improve,known regret bounds as special cases of our analysis using specificregularizers. Finally, we show the power of our approach by deriving a newsecond order algorithm with a regret bound invariant with respect to arbitraryrescalings of individual features.
arxiv-3300-40 | Sustainable Cooperative Coevolution with a Multi-Armed Bandit | http://arxiv.org/pdf/1304.3138v1.pdf | author:François-Michel De Rainville, Michèle Sebag, Christian Gagné, Marc Schoenauer, Denis Laurendeau category:cs.NE I.2.8 published:2013-04-10 summary:This paper proposes a self-adaptation mechanism to manage the resourcesallocated to the different species comprising a cooperative coevolutionaryalgorithm. The proposed approach relies on a dynamic extension to thewell-known multi-armed bandit framework. At each iteration, the dynamicmulti-armed bandit makes a decision on which species to evolve for ageneration, using the history of progress made by the different species toguide the decisions. We show experimentally, on a benchmark and a real-worldproblem, that evolving the different populations at different paces allows notonly to identify solutions more rapidly, but also improves the capacity ofcooperative coevolution to solve more complex problems.
arxiv-3300-41 | The BOSARIS Toolkit: Theory, Algorithms and Code for Surviving the New DCF | http://arxiv.org/pdf/1304.2865v1.pdf | author:Niko Brümmer, Edward de Villiers category:stat.AP cs.LG stat.ML published:2013-04-10 summary:The change of two orders of magnitude in the 'new DCF' of NIST's SRE'10,relative to the 'old DCF' evaluation criterion, posed a difficult challenge forparticipants and evaluator alike. Initially, participants were at a loss as tohow to calibrate their systems, while the evaluator underestimated the requirednumber of evaluation trials. After the fact, it is now obvious that bothcalibration and evaluation require very large sets of trials. This poses thechallenges of (i) how to decide what number of trials is enough, and (ii) howto process such large data sets with reasonable memory and CPU requirements.After SRE'10, at the BOSARIS Workshop, we built solutions to these problemsinto the freely available BOSARIS Toolkit. This paper explains the principlesand algorithms behind this toolkit. The main contributions of the toolkit are:1. The Normalized Bayes Error-Rate Plot, which analyses likelihood- ratiocalibration over a wide range of DCF operating points. These plots also help injudging the adequacy of the sizes of calibration and evaluation databases. 2.Efficient algorithms to compute DCF and minDCF for large score files, over therange of operating points required by these plots. 3. A new score file format,which facilitates working with very large trial lists. 4. A faster logisticregression optimizer for fusion and calibration. 5. A principled way to defineEER (equal error rate), which is of practical interest when the absolute errorcount is small.
arxiv-3300-42 | A New Approach To Two-View Motion Segmentation Using Global Dimension Minimization | http://arxiv.org/pdf/1304.2999v2.pdf | author:Bryan Poling, Gilad Lerman category:cs.CV published:2013-04-10 summary:We present a new approach to rigid-body motion segmentation from two views.We use a previously developed nonlinear embedding of two-view pointcorrespondences into a 9-dimensional space and identify the different motionsby segmenting lower-dimensional subspaces. In order to overcome nonuniformdistributions along the subspaces, whose dimensions are unknown, we suggest thenovel concept of global dimension and its minimization for clustering subspaceswith some theoretical motivation. We propose a fast projected gradientalgorithm for minimizing global dimension and thus segmenting motions from2-views. We develop an outlier detection framework around the proposed method,and we present state-of-the-art results on outlier-free and outlier-corruptedtwo-view data for segmenting motion.
arxiv-3300-43 | Entropy landscape of solutions in the binary perceptron problem | http://arxiv.org/pdf/1304.2850v2.pdf | author:Haiping Huang, K. Y. Michael Wong, Yoshiyuki Kabashima category:cs.LG published:2013-04-10 summary:The statistical picture of the solution space for a binary perceptron isstudied. The binary perceptron learns a random classification of input randompatterns by a set of binary synaptic weights. The learning of this network isdifficult especially when the pattern (constraint) density is close to thecapacity, which is supposed to be intimately related to the structure of thesolution space. The geometrical organization is elucidated by the entropylandscape from a reference configuration and of solution-pairs separated by agiven Hamming distance in the solution space. We evaluate the entropy at theannealed level as well as replica symmetric level and the mean field result isconfirmed by the numerical simulations on single instances using the proposedmessage passing algorithms. From the first landscape (a random configuration asa reference), we see clearly how the solution space shrinks as more constraintsare added. From the second landscape of solution-pairs, we deduce thecoexistence of clustering and freezing in the solution space.
arxiv-3300-44 | Adaptive piecewise polynomial estimation via trend filtering | http://arxiv.org/pdf/1304.2986v2.pdf | author:Ryan J. Tibshirani category:math.ST stat.ME stat.ML stat.TH published:2013-04-10 summary:We study trend filtering, a recently proposed tool of Kim et al. [SIAM Rev.51 (2009) 339-360] for nonparametric regression. The trend filtering estimateis defined as the minimizer of a penalized least squares criterion, in whichthe penalty term sums the absolute $k$th order discrete derivatives over theinput points. Perhaps not surprisingly, trend filtering estimates appear tohave the structure of $k$th degree spline functions, with adaptively chosenknot points (we say ``appear'' here as trend filtering estimates are not reallyfunctions over continuous domains, and are only defined over the discrete setof inputs). This brings to mind comparisons to other nonparametric regressiontools that also produce adaptive splines; in particular, we compare trendfiltering to smoothing splines, which penalize the sum of squared derivativesacross input points, and to locally adaptive regression splines [Ann. Statist.25 (1997) 387-413], which penalize the total variation of the $k$th derivative.Empirically, we discover that trend filtering estimates adapt to the locallevel of smoothness much better than smoothing splines, and further, theyexhibit a remarkable similarity to locally adaptive regression splines. We alsoprovide theoretical support for these empirical findings; most notably, weprove that (with the right choice of tuning parameter) the trend filteringestimate converges to the true underlying function at the minimax rate forfunctions whose $k$th derivative is of bounded variation. This is done via anasymptotic pairing of trend filtering and locally adaptive regression splines,which have already been shown to converge at the minimax rate [Ann. Statist. 25(1997) 387-413]. At the core of this argument is a new result tying togetherthe fitted values of two lasso problems that share the same outcome vector, buthave different predictor matrices.
arxiv-3300-45 | Detecting Directionality in Random Fields Using the Monogenic Signal | http://arxiv.org/pdf/1304.2998v3.pdf | author:Sofia Olhede, David Ramírez, Peter J. Schreier category:cs.IT cs.CV math.IT published:2013-04-10 summary:Detecting and analyzing directional structures in images is important in manyapplications since one-dimensional patterns often correspond to importantfeatures such as object contours or trajectories. Classifying a structure asdirectional or non-directional requires a measure to quantify the degree ofdirectionality and a threshold, which needs to be chosen based on thestatistics of the image. In order to do this, we model the image as a randomfield. So far, little research has been performed on analyzing directionalityin random fields. In this paper, we propose a measure to quantify the degree ofdirectionality based on the random monogenic signal, which enables a uniquedecomposition of a 2D signal into local amplitude, local orientation, and localphase. We investigate the second-order statistical properties of the monogenicsignal for isotropic, anisotropic, and unidirectional random fields. We analyzeour measure of directionality for finite-size sample images, and determine athreshold to distinguish between unidirectional and non-unidirectional randomfields, which allows the automatic classification of images.
arxiv-3300-46 | Roborobo! a Fast Robot Simulator for Swarm and Collective Robotics | http://arxiv.org/pdf/1304.2888v1.pdf | author:Nicolas Bredeche, Jean-Marc Montanier, Berend Weel, Evert Haasdijk category:cs.RO cs.AI cs.NE published:2013-04-10 summary:Roborobo! is a multi-platform, highly portable, robot simulator forlarge-scale collective robotics experiments. Roborobo! is coded in C++, andfollows the KISS guideline ("Keep it simple"). Therefore, its externaldependency is solely limited to the widely available SDL library for fast 2DGraphics. Roborobo! is based on a Khepera/ePuck model. It is targeted for fastsingle and multi-robots simulation, and has already been used in more than adozen published research mainly concerned with evolutionary swarm robotics,including environment-driven self-adaptation and distributed evolutionaryoptimization, as well as online onboard embodied evolution and embodiedmorphogenesis.
arxiv-3300-47 | Corpus-based Web Document Summarization using Statistical and Linguistic Approach | http://arxiv.org/pdf/1304.2476v1.pdf | author:Rushdi Shams, M. M. A. Hashem, Afrina Hossain, Suraiya Rumana Akter, Monika Gope category:cs.IR cs.CL published:2013-04-09 summary:Single document summarization generates summary by extracting therepresentative sentences from the document. In this paper, we presented a noveltechnique for summarization of domain-specific text from a single web documentthat uses statistical and linguistic analysis on the text in a reference corpusand the web document. The proposed summarizer uses the combinational functionof Sentence Weight (SW) and Subject Weight (SuW) to determine the rank of asentence, where SW is the function of number of terms (t_n) and number of words(w_n) in a sentence, and term frequency (t_f) in the corpus and SuW is thefunction of t_n and w_n in a subject, and t_f in the corpus. 30 percent of theranked sentences are considered to be the summary of the web document. Wegenerated three web document summaries using our technique and compared each ofthem with the summaries developed manually from 16 different human subjects.Results showed that 68 percent of the summaries produced by our approachsatisfy the manual summaries.
arxiv-3300-48 | Evolutionary Design of Digital Circuits Using Genetic Programming | http://arxiv.org/pdf/1304.2467v1.pdf | author:S. M. Ashik Eftekhar, Sk. Mahbub Habib, M. M. A. Hashem category:cs.NE published:2013-04-09 summary:For simple digital circuits, conventional method of designing circuits caneasily be applied. But for complex digital circuits, the conventional method ofdesigning circuits is not fruitfully applicable because it is time-consuming.On the contrary, Genetic Programming is used mostly for automatic programgeneration. The modern approach for designing Arithmetic circuits, commonlydigital circuits, is based on Graphs. This graph-based evolutionary design ofarithmetic circuits is a method of optimized designing of arithmetic circuits.In this paper, a new technique for evolutionary design of digital circuits isproposed using Genetic Programming (GP) with Subtree Mutation in place ofGraph-based design. The results obtained using this technique demonstrates thepotential capability of genetic programming in digital circuit design withlimited computer algorithms. The proposed technique, helps to simplify andspeed up the process of designing digital circuits, discovers a variation inthe field of digital circuit design where optimized digital circuits can besuccessfully and effectively designed.
arxiv-3300-49 | High-dimensional Mixed Graphical Models | http://arxiv.org/pdf/1304.2810v2.pdf | author:Jie Cheng, Tianxi Li, Elizaveta Levina, Ji Zhu category:stat.ML stat.ME published:2013-04-09 summary:While graphical models for continuous data (Gaussian graphical models) anddiscrete data (Ising models) have been extensively studied, there is littlework on graphical models linking both continuous and discrete variables (mixeddata), which are common in many scientific applications. We propose a novelgraphical model for mixed data, which is simple enough to be suitable forhigh-dimensional data, yet flexible enough to represent all possible graphstructures. We develop a computationally efficient regression-based algorithmfor fitting the model by focusing on the conditional log-likelihood of eachvariable given the rest. The parameters have a natural group structure, andsparsity in the fitted graph is attained by incorporating a group lassopenalty, approximated by a weighted $\ell_1$ penalty for computationalefficiency. We demonstrate the effectiveness of our method through an extensivesimulation study and apply it to a music annotation data set (CAL500),obtaining a sparse and interpretable graphical model relating the continuousfeatures of the audio signal to categorical variables such as genre, emotions,and usage associated with particular songs. While we focus on binary discretevariables, we also show that the proposed methodology can be easily extended togeneral discrete variables.
arxiv-3300-50 | Image Classification by Feature Dimension Reduction and Graph based Ranking | http://arxiv.org/pdf/1304.2683v1.pdf | author:Yao Nan, Qian Feng, Sun Zuolei category:cs.CV published:2013-04-09 summary:Dimensionality reduction (DR) of image features plays an important role inimage retrieval and classification tasks. Recently, two types of methods havebeen proposed to improve the both the accuracy and efficiency for thedimensionality reduction problem. One uses Non-negative matrix factorization(NMF) to describe the image distribution on the space of base matrix. Anotherone for dimension reduction trains a subspace projection matrix to projectoriginal data space into some low-dimensional subspaces which have deeparchitecture, so that the low-dimensional codes would be learned. At the sametime, the graph based similarity learning algorithm which tries to exploitcontextual information for improving the effectiveness of image rankings isalso proposed for image class and retrieval problem. In this paper, after abovetwo methods mentioned are utilized to reduce the high-dimensional features ofimages respectively, we learn the graph based similarity for the imageclassification problem. This paper compares the proposed approach with otherapproaches on an image database.
arxiv-3300-51 | For Solving Linear Equations Recombination is a Needless Operation in Time-Variant Adaptive Hybrid Algorithms | http://arxiv.org/pdf/1304.2545v1.pdf | author:A. R. M. Jalal Uddin Jamali, Mohammad Arif Hossain, G. M. Moniruzzaman, M. M. A. Hashem category:cs.NE cs.NA published:2013-04-09 summary:Recently hybrid evolutionary computation (EC) techniques are successfullyimplemented for solving large sets of linear equations. All the recentlydeveloped hybrid evolutionary algorithms, for solving linear equations, containboth the recombination and the mutation operations. In this paper, two modifiedhybrid evolutionary algorithms contained time-variant adaptive evolutionarytechnique are proposed for solving linear equations in which recombinationoperation is absent. The effectiveness of the recombination operator has beenstudied for the time-variant adaptive hybrid algorithms for solving large setof linear equations. Several experiments have been carried out using both theproposed modified hybrid evolutionary algorithms (in which the recombinationoperation is absent) and corresponding existing hybrid algorithms (in which therecombination operation is present) to solve large set of linear equations. Itis found that the number of generations required by the existing hybridalgorithms (i.e. the Gauss-Seidel-SR based time variant adaptive (GSBTVA)hybrid algorithm and the Jacobi-SR based time variant adaptive (JBTVA) hybridalgorithm) and modified hybrid algorithms (i.e. the modified Gauss-Seidel-SRbased time variant adaptive (MGSBTVA) hybrid algorithm and the modifiedJacobi-SR based time variant adaptive (MJBTVA) hybrid algorithm) arecomparable. Also the proposed modified algorithms require less amount ofcomputational time in comparison to the corresponding existing hybridalgorithms. As the proposed modified hybrid algorithms do not containrecombination operation, so they require less computational effort, and alsothey are more efficient, effective and easy to implement.
arxiv-3300-52 | Kernel Reconstruction ICA for Sparse Representation | http://arxiv.org/pdf/1304.2490v1.pdf | author:Yanhui Xiao, Zhenfeng Zhu, Yao Zhao category:cs.CV cs.LG published:2013-04-09 summary:Independent Component Analysis (ICA) is an effective unsupervised tool tolearn statistically independent representation. However, ICA is not onlysensitive to whitening but also difficult to learn an over-complete basis.Consequently, ICA with soft Reconstruction cost(RICA) was presented to learnsparse representations with over-complete basis even on unwhitened data.Whereas RICA is infeasible to represent the data with nonlinear structure dueto its intrinsic linearity. In addition, RICA is essentially an unsupervisedmethod and can not utilize the class information. In this paper, we propose akernel ICA model with reconstruction constraint (kRICA) to capture thenonlinear features. To bring in the class information, we further extend theunsupervised kRICA to a supervised one by introducing a discriminationconstraint, namely d-kRICA. This constraint leads to learn a structured basisconsisted of basis vectors from different basis subsets corresponding todifferent class labels. Then each subset will sparsely represent well for itsown class but not for the others. Furthermore, data samples belonging to thesame class will have similar representations, and thereby the learned sparserepresentations can take more discriminative power. Experimental resultsvalidate the effectiveness of kRICA and d-kRICA for image classification.
arxiv-3300-53 | A New Distributed Evolutionary Computation Technique for Multi-Objective Optimization | http://arxiv.org/pdf/1304.2543v1.pdf | author:Md. Asadul Islam, G. M. Mashrur-E-Elahi, M. M. A. Hashem category:cs.NE published:2013-04-09 summary:Now-a-days, it is important to find out solutions of Multi-ObjectiveOptimization Problems (MOPs). Evolutionary Strategy helps to solve such realworld problems efficiently and quickly. But sequential Evolutionary Algorithms(EAs) require an enormous computation power to solve such problems and it takesmuch time to solve large problems. To enhance the performance for solving thistype of problems, this paper presents a new Distributed Novel EvolutionaryStrategy Algorithm (DNESA) for Multi-Objective Optimization. The proposed DNESAapplies the divide-and-conquer approach to decompose population into smallersub-population and involves multiple solutions in the form of cooperativesub-populations. In DNESA, the server distributes the total computation load toall associate clients and simulation results show that the time for solvinglarge problems is much less than sequential EAs. Also DNESA shows betterperformance in convergence test when compared with other three well-known EAs.
arxiv-3300-54 | Solving Linear Equations by Classical Jacobi-SR Based Hybrid Evolutionary Algorithm with Uniform Adaptation Technique | http://arxiv.org/pdf/1304.2097v1.pdf | author:R. M. Jalal Uddin Jamali, M. M. A. Hashem, M. Mahfuz Hasan, Md. Bazlar Rahman category:cs.NE cs.NA published:2013-04-08 summary:Solving a set of simultaneous linear equations is probably the most importanttopic in numerical methods. For solving linear equations, iterative methods arepreferred over the direct methods especially when the coefficient matrix issparse. The rate of convergence of iteration method is increased by usingSuccessive Relaxation (SR) technique. But SR technique is very much sensitiveto relaxation factor, {\omega}. Recently, hybridization of classicalGauss-Seidel based successive relaxation technique with evolutionarycomputation techniques have successfully been used to solve large set of linearequations in which relaxation factors are self-adapted. In this paper, a newhybrid algorithm is proposed in which uniform adaptive evolutionary computationtechniques and classical Jacobi based SR technique are used instead ofclassical Gauss-Seidel based SR technique. The proposed Jacobi-SR based uniformadaptive hybrid algorithm, inherently, can be implemented in parallelprocessing environment efficiently. Whereas Gauss-Seidel-SR based hybridalgorithms cannot be implemented in parallel computing environment efficiently.The convergence theorem and adaptation theorem of the proposed algorithm areproved theoretically. And the performance of the proposed Jacobi-SR baseduniform adaptive hybrid evolutionary algorithm is compared with Gauss-Seidel-SRbased uniform adaptive hybrid evolutionary algorithm as well as with bothclassical Jacobi-SR method and Gauss-Seidel-SR method in the experimentaldomain. The proposed Jacobi-SR based hybrid algorithm outperforms theGauss-Seidel-SR based hybrid algorithm as well as both classical Jacobi-SRmethod and Gauss-Seidel-SR method in terms of convergence speed andeffectiveness.
arxiv-3300-55 | Learning Coverage Functions and Private Release of Marginals | http://arxiv.org/pdf/1304.2079v3.pdf | author:Vitaly Feldman, Pravesh Kothari category:cs.LG cs.CC cs.DS published:2013-04-08 summary:We study the problem of approximating and learning coverage functions. Afunction $c: 2^{[n]} \rightarrow \mathbf{R}^{+}$ is a coverage function, ifthere exists a universe $U$ with non-negative weights $w(u)$ for each $u \in U$and subsets $A_1, A_2, \ldots, A_n$ of $U$ such that $c(S) = \sum_{u \in\cup_{i \in S} A_i} w(u)$. Alternatively, coverage functions can be describedas non-negative linear combinations of monotone disjunctions. They are anatural subclass of submodular functions and arise in a number of applications. We give an algorithm that for any $\gamma,\delta>0$, given random and uniformexamples of an unknown coverage function $c$, finds a function $h$ thatapproximates $c$ within factor $1+\gamma$ on all but $\delta$-fraction of thepoints in time $poly(n,1/\gamma,1/\delta)$. This is the first fully-polynomialalgorithm for learning an interesting class of functions in the demanding PMACmodel of Balcan and Harvey (2011). Our algorithms are based on several newstructural properties of coverage functions. Using the results in (Feldman andKothari, 2014), we also show that coverage functions are learnable agnosticallywith excess $\ell_1$-error $\epsilon$ over all product and symmetricdistributions in time $n^{\log(1/\epsilon)}$. In contrast, we show that,without assumptions on the distribution, learning coverage functions is atleast as hard as learning polynomial-size disjoint DNF formulas, a class offunctions for which the best known algorithm runs in time$2^{\tilde{O}(n^{1/3})}$ (Klivans and Servedio, 2004). As an application of our learning results, we give simpledifferentially-private algorithms for releasing monotone conjunction countingqueries with low average error. In particular, for any $k \leq n$, we obtainprivate release of $k$-way marginals with average error $\bar{\alpha}$ in time$n^{O(\log(1/\bar{\alpha}))}$.
arxiv-3300-56 | Automatic Fingerprint Recognition Using Minutiae Matching Technique for the Large Fingerprint Database | http://arxiv.org/pdf/1304.2109v1.pdf | author:S. M. Mohsen, S. M. Zamshed Farhan, M. M. A. Hashem category:cs.CV published:2013-04-08 summary:Extracting minutiae from fingerprint images is one of the most importantsteps in automatic fingerprint identification system. Because minutiae matchingare certainly the most well-known and widely used method for fingerprintmatching, minutiae are local discontinuities in the fingerprint pattern. Inthis paper a fingerprint matching algorithm is proposed using some specificfeature of the minutiae points, also the acquired fingerprint image isconsidered by minimizing its size by generating a corresponding fingerprinttemplate for a large fingerprint database. The results achieved are comparedwith those obtained through some other methods also shows some improvement inthe minutiae detection process in terms of memory and time required.
arxiv-3300-57 | Dynamic Amelioration of Resolution Mismatches for Local Feature Based Identity Inference | http://arxiv.org/pdf/1304.2133v1.pdf | author:Yongkang Wong, Conrad Sanderson, Sandra Mau, Brian C. Lovell category:cs.CV cs.IR I.5.4; I.4 published:2013-04-08 summary:While existing face recognition systems based on local features are robust toissues such as misalignment, they can exhibit accuracy degradation whencomparing images of differing resolutions. This is common in surveillanceenvironments where a gallery of high resolution mugshots is compared to lowresolution CCTV probe images, or where the size of a given image is not areliable indicator of the underlying resolution (eg. poor optics). To alleviatethis degradation, we propose a compensation framework which dynamically choosesthe most appropriate face recognition system for a given pair of imageresolutions. This framework applies a novel resolution detection method whichdoes not rely on the size of the input images, but instead exploits thesensitivity of local features to resolution using a probabilistic multi-regionhistogram approach. Experiments on a resolution-modified version of the"Labeled Faces in the Wild" dataset show that the proposed resolution detectorfrontend obtains a 99% average accuracy in selecting the most appropriate facerecognition system, resulting in higher overall face discrimination accuracy(across several resolutions) compared to the individual baseline facerecognition systems.
arxiv-3300-58 | The PAV algorithm optimizes binary proper scoring rules | http://arxiv.org/pdf/1304.2331v1.pdf | author:Niko Brummer, Johan du Preez category:stat.AP cs.LG stat.ML published:2013-04-08 summary:There has been much recent interest in application of thepool-adjacent-violators (PAV) algorithm for the purpose of calibrating theprobabilistic outputs of automatic pattern recognition and machine learningalgorithms. Special cost functions, known as proper scoring rules form naturalobjective functions to judge the goodness of such calibration. We show that forbinary pattern classifiers, the non-parametric optimization of calibration,subject to a monotonicity constraint, can be solved by PAV and that thissolution is optimal for all regular binary proper scoring rules. This extendsprevious results which were limited to convex binary proper scoring rules. Wefurther show that this result holds not only for calibration of probabilities,but also for calibration of log-likelihood-ratios, in which case optimalityholds independently of the prior probabilities of the pattern classes.
arxiv-3300-59 | Synaptic Scaling Balances Learning in a Spiking Model of Neocortex | http://arxiv.org/pdf/1304.2266v1.pdf | author:Mark Rowan, Samuel Neymotin category:q-bio.NC cs.NE published:2013-04-08 summary:Learning in the brain requires complementary mechanisms: potentiation andactivity-dependent homeostatic scaling. We introduce synaptic scaling to abiologically-realistic spiking model of neocortex which can learn changes inoscillatory rhythms using STDP, and show that scaling is necessary to balanceboth positive and negative changes in input from potentiation and atrophy. Wediscuss some of the issues that arise when considering synaptic scaling in sucha model, and show that scaling regulates activity whilst allowing learning toremain unaltered.
arxiv-3300-60 | ClusterCluster: Parallel Markov Chain Monte Carlo for Dirichlet Process Mixtures | http://arxiv.org/pdf/1304.2302v1.pdf | author:Dan Lovell, Jonathan Malmaud, Ryan P. Adams, Vikash K. Mansinghka category:stat.ML cs.DC cs.LG published:2013-04-08 summary:The Dirichlet process (DP) is a fundamental mathematical tool for Bayesiannonparametric modeling, and is widely used in tasks such as density estimation,natural language processing, and time series modeling. Although MCMC inferencemethods for the DP often provide a gold standard in terms asymptotic accuracy,they can be computationally expensive and are not obviously parallelizable. Wepropose a reparameterization of the Dirichlet process that induces conditionalindependencies between the atoms that form the random measure. This conditionalindependence enables many of the Markov chain transition operators for DPinference to be simulated in parallel across multiple cores. Applied to mixturemodeling, our approach enables the Dirichlet process to simultaneously learnclusters that describe the data and superclusters that define the granularityof parallelization. Unlike previous approaches, our technique does not requirealteration of the model and leaves the true posterior distribution invariant.It also naturally lends itself to a distributed software implementation interms of Map-Reduce, which we test in cluster configurations of over 50machines and 100 cores. We present experiments exploring the parallelefficiency and convergence properties of our approach on both synthetic andreal-world data, including runs on 1MM data vectors in 256 dimensions.
arxiv-3300-61 | Facial transformations of ancient portraits: the face of Caesar | http://arxiv.org/pdf/1304.1972v1.pdf | author:Amelia Carolina Sparavigna category:cs.CV published:2013-04-07 summary:Some software solutions used to obtain the facial transformations can helpinvestigating the artistic metamorphosis of the ancient portraits of the sameperson. An analysis with a freely available software of portraitures of JuliusCaesar is proposed, showing his several "morphs". The software helps enhancingthe mood the artist added to a portrait.
arxiv-3300-62 | A General Framework for Interacting Bayes-Optimally with Self-Interested Agents using Arbitrary Parametric Model and Model Prior | http://arxiv.org/pdf/1304.2024v3.pdf | author:Trong Nghia Hoang, Kian Hsiang Low category:cs.LG cs.AI cs.MA stat.ML published:2013-04-07 summary:Recent advances in Bayesian reinforcement learning (BRL) have shown thatBayes-optimality is theoretically achievable by modeling the environment'slatent dynamics using Flat-Dirichlet-Multinomial (FDM) prior. Inself-interested multi-agent environments, the transition dynamics are mainlycontrolled by the other agent's stochastic behavior for which FDM'sindependence and modeling assumptions do not hold. As a result, FDM does notallow the other agent's behavior to be generalized across different states norspecified using prior domain knowledge. To overcome these practical limitationsof FDM, we propose a generalization of BRL to integrate the general class ofparametric models and model priors, thus allowing practitioners' domainknowledge to be exploited to produce a fine-grained and compact representationof the other agent's behavior. Empirical evaluation shows that our approachoutperforms existing multi-agent reinforcement learning algorithms.
arxiv-3300-63 | Image Compression predicated on Recurrent Iterated Function Systems | http://arxiv.org/pdf/1304.2014v1.pdf | author:Chol-Hui Yun, W. Metzler, M. Barski category:math.DS cs.CV math.GT published:2013-04-07 summary:Recurrent iterated function systems (RIFSs) are improvements of iteratedfunction systems (IFSs) using elements of the theory of Marcovian stochasticprocesses which can produce more natural looking images. We construct new RIFSsconsisting substantially of a vertical contraction factor function andnonlinear transformations. These RIFSs are applied to image compression.
arxiv-3300-64 | Constructing Low Star Discrepancy Point Sets with Genetic Algorithms | http://arxiv.org/pdf/1304.1978v2.pdf | author:Carola Doerr, Francois-Michel De Rainville category:cs.NE cs.NA F.2.1; I.2.8 published:2013-04-07 summary:Geometric discrepancies are standard measures to quantify the irregularity ofdistributions. They are an important notion in numerical integration. One ofthe most important discrepancy notions is the so-called \emph{stardiscrepancy}. Roughly speaking, a point set of low star discrepancy valueallows for a small approximation error in quasi-Monte Carlo integration. It isthus the most studied discrepancy notion. In this work we present a new algorithm to compute point sets of low stardiscrepancy. The two components of the algorithm (for the optimization and theevaluation, respectively) are based on evolutionary principles. Our algorithmclearly outperforms existing approaches. To the best of our knowledge, it isalso the first algorithm which can be adapted easily to optimize inverse stardiscrepancies.
arxiv-3300-65 | Image Retrieval using Histogram Factorization and Contextual Similarity Learning | http://arxiv.org/pdf/1304.1995v2.pdf | author:Liu Liang category:cs.CV cs.DB cs.LG published:2013-04-07 summary:Image retrieval has been a top topic in the field of both computer vision andmachine learning for a long time. Content based image retrieval, which tries toretrieve images from a database visually similar to a query image, hasattracted much attention. Two most important issues of image retrieval are therepresentation and ranking of the images. Recently, bag-of-words based methodhas shown its power as a representation method. Moreover, nonnegative matrixfactorization is also a popular way to represent the data samples. In addition,contextual similarity learning has also been studied and proven to be aneffective method for the ranking problem. However, these technologies havenever been used together. In this paper, we developed an effective imageretrieval system by representing each image using the bag-of-words method ashistograms, and then apply the nonnegative matrix factorization to factorizethe histograms, and finally learn the ranking score using the contextualsimilarity learning method. The proposed novel system is evaluated on a largescale image database and the effectiveness is shown.
arxiv-3300-66 | Client-Driven Content Extraction Associated with Table | http://arxiv.org/pdf/1304.1930v1.pdf | author:K. C. Santosh, Abdel Belaïd category:cs.CV cs.IR published:2013-04-06 summary:The goal of the project is to extract content within table in document imagesbased on learnt patterns. Real-world users i.e., clients first provide a set ofkey fields within the table which they think are important. These are firstused to represent the graph where nodes are labelled with semantics includingother features and edges are attributed with relations. Attributed relationalgraph (ARG) is then employed to mine similar graphs from a document image. Eachmined graph will represent an item within the table, and hence a set of suchgraphs will compose a table. We have validated the concept by using areal-world industrial problem.
arxiv-3300-67 | Nonlinear unmixing of hyperspectral images: models and algorithms | http://arxiv.org/pdf/1304.1875v2.pdf | author:Nicolas Dobigeon, Jean-Yves Tourneret, Cédric Richard, José C. M. Bermudez, Stephen McLaughlin, Alfred O. Hero category:stat.AP stat.ME stat.ML published:2013-04-06 summary:When considering the problem of unmixing hyperspectral images, most of theliterature in the geoscience and image processing areas relies on the widelyused linear mixing model (LMM). However, the LMM may be not valid and othernonlinear models need to be considered, for instance, when there aremulti-scattering effects or intimate interactions. Consequently, over the lastfew years, several significant contributions have been proposed to overcome thelimitations inherent in the LMM. In this paper, we present an overview ofrecent advances in nonlinear unmixing modeling.
arxiv-3300-68 | Proceedings of the 37th Annual Workshop of the Austrian Association for Pattern Recognition (ÖAGM/AAPR), 2013 | http://arxiv.org/pdf/1304.1876v3.pdf | author:Justus Piater, Antonio Rodríguez-Sánchez category:cs.CV published:2013-04-06 summary:This volume represents the proceedings of the 37th Annual Workshop of theAustrian Association for Pattern Recognition (\"OAGM/AAPR), held May 23-24,2013, in Innsbruck, Austria.
arxiv-3300-69 | Bug Classification: Feature Extraction and Comparison of Event Model using Naïve Bayes Approach | http://arxiv.org/pdf/1304.1677v1.pdf | author:Sunil Joy Dommati, Ruchi Agrawal, Ram Mohana Reddy G., S. Sowmya Kamath category:cs.SE cs.IR cs.LG published:2013-04-05 summary:In software industries, individuals at different levels from customer to anengineer apply diverse mechanisms to detect to which class a particular bugshould be allocated. Sometimes while a simple search in Internet might help, inmany other cases a lot of effort is spent in analyzing the bug report toclassify the bug. So there is a great need of a structured mining algorithm -where given a crash log, the existing bug database could be mined to find outthe class to which the bug should be allocated. This would involve Miningpatterns and applying different classification algorithms. This paper focuseson the feature extraction, noise reduction in data and classification ofnetwork bugs using probabilistic Na\"ive Bayes approach. Different event modelslike Bernoulli and Multinomial are applied on the extracted features. When new,unseen bugs are given as input to the algorithms, the performance comparison ofdifferent algorithms is done on the basis of accuracy and recall parameters.
arxiv-3300-70 | Fast SVM training using approximate extreme points | http://arxiv.org/pdf/1304.1391v1.pdf | author:Manu Nandan, Pramod P. Khargonekar, Sachin S. Talathi category:cs.LG published:2013-04-04 summary:Applications of non-linear kernel Support Vector Machines (SVMs) to largedatasets is seriously hampered by its excessive training time. We propose amodification, called the approximate extreme points support vector machine(AESVM), that is aimed at overcoming this burden. Our approach relies onconducting the SVM optimization over a carefully selected subset, called therepresentative set, of the training dataset. We present analytical results thatindicate the similarity of AESVM and SVM solutions. A linear time algorithmbased on convex hulls and extreme points is used to compute the representativeset in kernel space. Extensive computational experiments on nine datasetscompared AESVM to LIBSVM \citep{LIBSVM}, CVM \citep{Tsang05}, BVM\citep{Tsang07}, LASVM \citep{Bordes05}, $\text{SVM}^{\text{perf}}$\citep{Joachims09}, and the random features method \citep{rahimi07}. Our AESVMimplementation was found to train much faster than the other methods, while itsclassification accuracy was similar to that of LIBSVM in all cases. Inparticular, for a seizure detection dataset, AESVM training was almost $10^3$times faster than LIBSVM and LASVM and more than forty times faster than CVMand BVM. Additionally, AESVM also gave competitively fast classification times.
arxiv-3300-71 | Classification of Human Epithelial Type 2 Cell Indirect Immunofluoresence Images via Codebook Based Descriptors | http://arxiv.org/pdf/1304.1262v1.pdf | author:Arnold Wiliem, Yongkang Wong, Conrad Sanderson, Peter Hobson, Shaokang Chen, Brian C. Lovell category:q-bio.CB cs.CV q-bio.QM published:2013-04-04 summary:The Anti-Nuclear Antibody (ANA) clinical pathology test is commonly used toidentify the existence of various diseases. A hallmark method for identifyingthe presence of ANAs is the Indirect Immunofluorescence method on HumanEpithelial (HEp-2) cells, due to its high sensitivity and the large range ofantigens that can be detected. However, the method suffers from numerousshortcomings, such as being subjective as well as time and labour intensive.Computer Aided Diagnostic (CAD) systems have been developed to address theseproblems, which automatically classify a HEp-2 cell image into one of its knownpatterns (eg., speckled, homogeneous). Most of the existing CAD systems usehandpicked features to represent a HEp-2 cell image, which may only work inlimited scenarios. In this paper, we propose a cell classification systemcomprised of a dual-region codebook-based descriptor, combined with the NearestConvex Hull Classifier. We evaluate the performance of several variants of thedescriptor on two publicly available datasets: ICPR HEp-2 cell classificationcontest dataset and the new SNPHEp-2 dataset. To our knowledge, this is thefirst time codebook-based descriptors are applied and studied in this domain.Experiments show that the proposed system has consistent high performance andis more robust than two recent CAD systems.
arxiv-3300-72 | Integration of spatio-temporal contrast sensitivity with a multi-slice channelized Hotelling observer | http://arxiv.org/pdf/1304.1419v1.pdf | author:Ali N. Avanaki, Kathryn S. Espig, Cedric Marchessoux, Elizabeth A. Krupinski, Predrag R. Bakic, Tom R. L. Kimpe, Andrew D. A. Maidment category:cs.CV published:2013-04-04 summary:Barten's model of spatio-temporal contrast sensitivity function of humanvisual system is embedded in a multi-slice channelized Hotelling observer. Thisis done by 3D filtering of the stack of images with the spatio-temporalcontrast sensitivity function and feeding the result (i.e., the perceived imagestack) to the multi-slice channelized Hotelling observer. The proposedprocedure of considering spatio-temporal contrast sensitivity function isgeneric in the sense that it can be used with observers other than multi-slicechannelized Hotelling observer. Detection performance of the new observer indigital breast tomosynthesis is measured in a variety of browsing speeds, attwo spatial sampling rates, using computer simulations. Our results show a peakin detection performance in mid browsing speeds. We compare our results tothose of a human observer study reported earlier (I. Diaz et al. SPIE MI 2011).The effects of display luminance, contrast and spatial sampling rate, with andwithout considering foveal vision, are also studied. Reported simulations areconducted with real digital breast tomosynthesis image stacks, as well asstacks from an anthropomorphic software breast phantom (P. Bakic et al. MedPhys. 2011). Lesion cases are simulated by inserting singlemicro-calcifications or masses. Limitations of our methods and ways to improvethem are discussed.
arxiv-3300-73 | Fast Approximate L_infty Minimization: Speeding Up Robust Regression | http://arxiv.org/pdf/1304.1250v1.pdf | author:Fumin Shen, Chunhua Shen, Rhys Hill, Anton van den Hengel, Zhenmin Tang category:cs.CV stat.CO published:2013-04-04 summary:Minimization of the $L_\infty$ norm, which can be viewed as approximatelysolving the non-convex least median estimation problem, is a powerful methodfor outlier removal and hence robust regression. However, current techniquesfor solving the problem at the heart of $L_\infty$ norm minimization are slow,and therefore cannot scale to large problems. A new method for the minimizationof the $L_\infty$ norm is presented here, which provides a speedup of multipleorders of magnitude for data with high dimension. This method, termed Fast$L_\infty$ Minimization, allows robust regression to be applied to a class ofproblems which were previously inaccessible. It is shown how the $L_\infty$norm minimization problem can be broken up into smaller sub-problems, which canthen be solved extremely efficiently. Experimental results demonstrate theradical reduction in computation time, along with robustness against largenumbers of outliers in a few model-fitting problems.
arxiv-3300-74 | Shadow Detection: A Survey and Comparative Evaluation of Recent Methods | http://arxiv.org/pdf/1304.1233v1.pdf | author:Andres Sanin, Conrad Sanderson, Brian C. Lovell category:cs.CV cs.RO published:2013-04-04 summary:This paper presents a survey and a comparative evaluation of recenttechniques for moving cast shadow detection. We identify shadow removal as acritical step for improving object detection and tracking. The survey coversmethods published during the last decade, and places them in a feature-basedtaxonomy comprised of four categories: chromacity, physical, geometry andtextures. A selection of prominent methods across the categories is compared interms of quantitative performance measures (shadow detection and discriminationrates, colour desaturation) as well as qualitative observations. Furthermore,we propose the use of tracking performance as an unbiased approach fordetermining the practical usefulness of shadow detection methods. Theevaluation indicates that all shadow detection approaches make differentcontributions and all have individual strength and weaknesses. Out of theselected methods, the geometry-based technique has strict assumptions and isnot generalisable to various environments, but it is a straightforward choicewhen the objects of interest are easy to model and their shadows have differentorientation. The chromacity based method is the fastest to implement and run,but it is sensitive to noise and less effective in low saturated scenes. Thephysical method improves upon the accuracy of the chromacity method by adaptingto local shadow models, but fails when the spectral properties of the objectsare similar to that of the background. The small-region texture based method isespecially robust for pixels whose neighbourhood is textured, but may takelonger to implement and is the most computationally expensive. The large-regiontexture based method produces the most accurate results, but has a significantcomputational load due to its multiple processing steps.
arxiv-3300-75 | Hiding Image in Image by Five Modulus Method for Image Steganography | http://arxiv.org/pdf/1304.1571v1.pdf | author:Firas A. Jassim category:cs.MM cs.CV published:2013-04-04 summary:This paper is to create a practical steganographic implementation to hidecolor image (stego) inside another color image (cover). The proposed techniqueuses Five Modulus Method to convert the whole pixels within both the cover andthe stego images into multiples of five. Since each pixels inside the stegoimage is divisible by five then the whole stego image could be divided by fiveto get new range of pixels 0..51. Basically, the reminder of each number thatis not divisible by five is either 1,2,3 or 4 when divided by 5. Subsequently,then a 4-by-4 window size has been implemented to accommodate the proposedtechnique. For each 4-by-4 window inside the cover image, a number from 1 to 4could be embedded secretly from the stego image. The previous discussion mustbe applied separately for each of the R, G, and B arrays. Moreover, a stego-keycould be combined with the proposed algorithm to make it difficult for anyadversary to extract the secret image from the cover image. Based on the PSNRvalue, the extracted stego image has high PSNR value. Hence this newsteganography algorithm is very efficient to hide color images.
arxiv-3300-76 | Spectral Descriptors for Graph Matching | http://arxiv.org/pdf/1304.1572v4.pdf | author:Nan Hu, Leonidas Guibas category:cs.CV published:2013-04-04 summary:In this paper, we consider the weighted graph matching problem. Recently,approaches to this problem based on spectral methods have gained significantattention. We propose two graph spectral descriptors based on the graphLaplacian, namely a Laplacian family signature (LFS) on nodes, and a pairwiseheat kernel distance on edges. We show the stability of both our descriptorsunder small perturbation of edges and nodes. In addition, we show that ourpairwise heat kernel distance is a noise-tolerant approximation of theclassical adjacency matrix-based second order compatibility function. Thesenice properties suggest a descriptor-based matching scheme, for which we set upan integer quadratic problem (IQP) and apply an approximate solver to find anear optimal solution. We have tested our matching method on a set of randomlygenerated graphs, the widely-used CMU house sequence and a set of real images.These experiments show the superior performance of our selected node signaturesand edge descriptors for graph matching, as compared with other existingsignature-based matchings and adjacency matrix-based matchings.
arxiv-3300-77 | Generalization Bounds for Domain Adaptation | http://arxiv.org/pdf/1304.1574v1.pdf | author:Chao Zhang, Lei Zhang, Jieping Ye category:cs.LG math.PR published:2013-04-04 summary:In this paper, we provide a new framework to obtain the generalization boundsof the learning process for domain adaptation, and then apply the derivedbounds to analyze the asymptotical convergence of the learning process. Withoutloss of generality, we consider two kinds of representative domain adaptation:one is with multiple sources and the other is combining source and target data. In particular, we use the integral probability metric to measure thedifference between two domains. For either kind of domain adaptation, wedevelop a related Hoeffding-type deviation inequality and a symmetrizationinequality to achieve the corresponding generalization bound based on theuniform entropy number. We also generalized the classical McDiarmid'sinequality to a more general setting where independent random variables cantake values from different domains. By using this inequality, we then obtaingeneralization bounds based on the Rademacher complexity. Afterwards, weanalyze the asymptotic convergence and the rate of convergence of the learningprocess for such kind of domain adaptation. Meanwhile, we discuss the factorsthat affect the asymptotic behavior of the learning process and the numericalexperiments support our theoretical findings as well.
arxiv-3300-78 | Restoration of Images Corrupted by Impulse Noise and Mixed Gaussian Impulse Noise using Blind Inpainting | http://arxiv.org/pdf/1304.1408v1.pdf | author:Ming Yan category:math.OC cs.CV math.NA published:2013-04-04 summary:This article studies the problem of image restoration of observed imagescorrupted by impulse noise and mixed Gaussian impulse noise. Since the pixelsdamaged by impulse noise contain no information about the true image, how tofind this set correctly is a very important problem. We propose two methodsbased on blind inpainting and $\ell_0$ minimization that can simultaneouslyfind the damaged pixels and restore the image. By iteratively restoring theimage and updating the set of damaged pixels, these methods have betterperformance than other methods, as shown in the experiments. In addition, weprovide convergence analysis for these methods, these algorithms will convergeto coordinatewise minimum points. In addition, they will converge to localminimum points (or with probability one) with some modifications in thealgorithms.
arxiv-3300-79 | Multiscale Fractal Descriptors Applied to Texture Classification | http://arxiv.org/pdf/1304.1568v1.pdf | author:João Batista Florindo, Odemir Martinez Bruno category:cs.CV published:2013-04-04 summary:This work proposes the combination of multiscale transform with fractaldescriptors employed in the classification of gray-level texture images. Weapply the space-scale transform (derivative + Gaussian filter) over theBouligand-Minkowski fractal descriptors, followed by a threshold over thefilter response, aiming at attenuating noise effects caused by the final partof this response. The method is tested in the classification of a well-knowndata set (Brodatz) and compared with other classical texture descriptortechniques. The results demonstrate the advantage of the proposed approach,achieving a higher success rate with a reduced amount of descriptors.
arxiv-3300-80 | Estimating Phoneme Class Conditional Probabilities from Raw Speech Signal using Convolutional Neural Networks | http://arxiv.org/pdf/1304.1018v2.pdf | author:Dimitri Palaz, Ronan Collobert, Mathew Magimai. -Doss category:cs.LG cs.CL cs.NE published:2013-04-03 summary:In hybrid hidden Markov model/artificial neural networks (HMM/ANN) automaticspeech recognition (ASR) system, the phoneme class conditional probabilitiesare estimated by first extracting acoustic features from the speech signalbased on prior knowledge such as, speech perception or/and speech productionknowledge, and, then modeling the acoustic features with an ANN. Recentadvances in machine learning techniques, more specifically in the field ofimage processing and text processing, have shown that such divide and conquerstrategy (i.e., separating feature extraction and modeling steps) may not benecessary. Motivated from these studies, in the framework of convolutionalneural networks (CNNs), this paper investigates a novel approach, where theinput to the ANN is raw speech signal and the output is phoneme classconditional probability estimates. On TIMIT phoneme recognition task, we studydifferent ANN architectures to show the benefit of CNNs and compare theproposed approach against conventional approach where, spectral-based featureMFCC is extracted and modeled by a multilayer perceptron. Our studies show thatthe proposed approach can yield comparable or better phoneme recognitionperformance when compared to the conventional approach. It indicates that CNNscan learn features relevant for phoneme classification automatically from theraw speech signal.
arxiv-3300-81 | Patch-based Probabilistic Image Quality Assessment for Face Selection and Improved Video-based Face Recognition | http://arxiv.org/pdf/1304.0869v2.pdf | author:Yongkang Wong, Shaokang Chen, Sandra Mau, Conrad Sanderson, Brian C. Lovell category:cs.CV stat.AP published:2013-04-03 summary:In video based face recognition, face images are typically captured overmultiple frames in uncontrolled conditions, where head pose, illumination,shadowing, motion blur and focus change over the sequence. Additionally,inaccuracies in face localisation can also introduce scale and alignmentvariations. Using all face images, including images of poor quality, canactually degrade face recognition performance. While one solution it to useonly the "best" subset of images, current face selection techniques areincapable of simultaneously handling all of the abovementioned issues. Wepropose an efficient patch-based face image quality assessment algorithm whichquantifies the similarity of a face image to a probabilistic face model,representing an "ideal" face. Image characteristics that affect recognition aretaken into account, including variations in geometric alignment (shift,rotation and scale), sharpness, head pose and cast shadows. Experiments onFERET and PIE datasets show that the proposed algorithm is able to identifyimages which are simultaneously the most frontal, aligned, sharp and wellilluminated. Further experiments on a new video surveillance dataset (termedChokePoint) show that the proposed method provides better face subsets thanexisting face selection techniques, leading to significant improvements inrecognition accuracy.
arxiv-3300-82 | Improved Anomaly Detection in Crowded Scenes via Cell-based Analysis of Foreground Speed, Size and Texture | http://arxiv.org/pdf/1304.0886v1.pdf | author:Vikas Reddy, Conrad Sanderson, Brian C. Lovell category:cs.CV published:2013-04-03 summary:A robust and efficient anomaly detection technique is proposed, capable ofdealing with crowded scenes where traditional tracking based approaches tend tofail. Initial foreground segmentation of the input frames confines the analysisto foreground objects and effectively ignores irrelevant background dynamics.Input frames are split into non-overlapping cells, followed by extractingfeatures based on motion, size and texture from each cell. Each feature type isindependently analysed for the presence of an anomaly. Unlike most methods, arefined estimate of object motion is achieved by computing the optical flow ofonly the foreground pixels. The motion and size features are modelled by anapproximated version of kernel density estimation, which is computationallyefficient even for large training datasets. Texture features are modelled by anadaptively grown codebook, with the number of entries in the codebook selectedin an online fashion. Experiments on the recently published UCSD AnomalyDetection dataset show that the proposed method obtains considerably betterresults than three recent approaches: MPPCA, social force, and mixture ofdynamic textures (MDT). The proposed method is also several orders of magnitudefaster than MDT, the next best performing method.
arxiv-3300-83 | A software for aging faces applied to ancient marble busts | http://arxiv.org/pdf/1304.1022v1.pdf | author:Amelia Carolina Sparavigna category:cs.CV published:2013-04-03 summary:The study and development of software able to show the effect of aging offaces is one of the tasks of face recognition technologies. Some softwaresolutions are used for investigations, some others to show the effects of drugson healthy appearance, however some other applications can be proposed for theanalysis of visual arts. Here we use a freely available software, which isproviding interesting results, for the comparison of ancient marble busts. Ananalysis of Augustus busts is proposed.
arxiv-3300-84 | Highly comparative time-series analysis: The empirical structure of time series and their methods | http://arxiv.org/pdf/1304.1209v1.pdf | author:Ben D. Fulcher, Max A. Little, Nick S. Jones category:cs.CV physics.bio-ph q-bio.QM stat.ML published:2013-04-03 summary:The process of collecting and organizing sets of observations represents acommon theme throughout the history of science. However, despite the ubiquityof scientists measuring, recording, and analyzing the dynamics of differentprocesses, an extensive organization of scientific time-series data andanalysis methods has never been performed. Addressing this, annotatedcollections of over 35 000 real-world and model-generated time series and over9000 time-series analysis algorithms are analyzed in this work. We introducereduced representations of both time series, in terms of their propertiesmeasured by diverse scientific methods, and of time-series analysis methods, interms of their behaviour on empirical time series, and use them to organizethese interdisciplinary resources. This new approach to comparing acrossdiverse scientific data and methods allows us to organize time-series datasetsautomatically according to their properties, retrieve alternatives toparticular analysis methods developed in other scientific disciplines, andautomate the selection of useful methods for time-series classification andregression tasks. The broad scientific utility of these tools is demonstratedon datasets of electroencephalograms, self-affine time series, heart beatintervals, speech signals, and others, in each case contributing novel analysistechniques to the existing literature. Highly comparative techniques thatcompare across an interdisciplinary literature can thus be used to guide morefocused research in time-series analysis for applications across the scientificdisciplines.
arxiv-3300-85 | A Fast Semidefinite Approach to Solving Binary Quadratic Problems | http://arxiv.org/pdf/1304.0840v1.pdf | author:Peng Wang, Chunhua Shen, Anton van den Hengel category:cs.CV cs.LG published:2013-04-03 summary:Many computer vision problems can be formulated as binary quadratic programs(BQPs). Two classic relaxation methods are widely used for solving BQPs,namely, spectral methods and semidefinite programming (SDP), each with theirown advantages and disadvantages. Spectral relaxation is simple and easy toimplement, but its bound is loose. Semidefinite relaxation has a tighter bound,but its computational complexity is high for large scale problems. We present anew SDP formulation for BQPs, with two desirable properties. First, it has asimilar relaxation bound to conventional SDP formulations. Second, comparedwith conventional SDP methods, the new SDP formulation leads to a significantlymore efficient and scalable dual optimization approach, which has the samedegree of complexity as spectral methods. Extensive experiments on variousapplications including clustering, image segmentation, co-segmentation andregistration demonstrate the usefulness of our SDP formulation for solvinglarge-scale BQPs.
arxiv-3300-86 | Computational Lower Bounds for Sparse PCA | http://arxiv.org/pdf/1304.0828v2.pdf | author:Quentin Berthet, Philippe Rigollet category:math.ST cs.CC stat.ML stat.TH 62C20 published:2013-04-03 summary:In the context of sparse principal component detection, we bring evidencetowards the existence of a statistical price to pay for computationalefficiency. We measure the performance of a test by the smallest signalstrength that it can detect and we propose a computationally efficient methodbased on semidefinite programming. We also prove that the statisticalperformance of this test cannot be strictly improved by any computationallyefficient method. Our results can be viewed as complexity theoretic lowerbounds conditionally on the assumptions that some instances of the plantedclique problem cannot be solved in randomized polynomial time.
arxiv-3300-87 | Efficient Distance Metric Learning by Adaptive Sampling and Mini-Batch Stochastic Gradient Descent (SGD) | http://arxiv.org/pdf/1304.1192v1.pdf | author:Qi Qian, Rong Jin, Jinfeng Yi, Lijun Zhang, Shenghuo Zhu category:cs.LG published:2013-04-03 summary:Distance metric learning (DML) is an important task that has foundapplications in many domains. The high computational cost of DML arises fromthe large number of variables to be determined and the constraint that adistance metric has to be a positive semi-definite (PSD) matrix. Althoughstochastic gradient descent (SGD) has been successfully applied to improve theefficiency of DML, it can still be computationally expensive because in orderto ensure that the solution is a PSD matrix, it has to, at every iteration,project the updated distance metric onto the PSD cone, an expensive operation.We address this challenge by developing two strategies within SGD, i.e.mini-batch and adaptive sampling, to effectively reduce the number of updates(i.e., projections onto the PSD cone) in SGD. We also develop hybrid approachesthat combine the strength of adaptive sampling with that of mini-batch onlinelearning techniques to further improve the computational efficiency of SGD forDML. We prove the theoretical guarantees for both adaptive sampling andmini-batch based approaches for DML. We also conduct an extensive empiricalstudy to verify the effectiveness of the proposed algorithms for DML.
arxiv-3300-88 | Multiscale Hybrid Non-local Means Filtering Using Modified Similarity Measure | http://arxiv.org/pdf/1304.0839v1.pdf | author:Zahid Hussain Shamsi, Dai-Gyoung Kim category:cs.CV published:2013-04-03 summary:A new multiscale implementation of non-local means filtering for imagedenoising is proposed. The proposed algorithm also introduces a modification ofsimilarity measure for patch comparison. The standard Euclidean norm isreplaced by weighted Euclidean norm for patch based comparison. Assuming thepatch as an oriented surface, notion of normal vector patch is being associatedwith each patch. The inner product of these normal vector patches is then usedin weighted Euclidean distance of photometric patches as the weight factor. Thealgorithm involves two steps: The first step is multiscale implementation of anaccelerated non-local means filtering in the stationary wavelet domain toobtain a refined version of the noisy patches for later comparison. This stepis inspired by a preselection phase of finding similar patches in variousnon-local means approaches. The next step is to apply the modified non-localmeans filtering to the noisy image using the reference patches obtained in thefirst step. These refined patches contain less noise, and consequently thecomputation of normal vectors and partial derivatives is more accurate.Experimental results indicate equivalent or better performance of proposedalgorithm as compared to various state of the art algorithms.
arxiv-3300-89 | Lie Algebrized Gaussians for Image Representation | http://arxiv.org/pdf/1304.0823v1.pdf | author:Liyu Gong, Meng Chen, Chunlong Hu category:cs.CV published:2013-04-03 summary:We present an image representation method which is derived from analyzingGaussian probability density function (\emph{pdf}) space using Lie grouptheory. In our proposed method, images are modeled by Gaussian mixture models(GMMs) which are adapted from a globally trained GMM called universalbackground model (UBM). Then we vectorize the GMMs based on two facts: (1)components of image-specific GMMs are closely grouped together around theircorresponding component of the UBM due to the characteristic of the UBMadaption procedure; (2) Gaussian \emph{pdf}s form a Lie group, which is adifferentiable manifold rather than a vector space. We map each Gaussiancomponent to the tangent vector space (named Lie algebra) of Lie group at themanifold position of UBM. The final feature vector, named Lie algebrizedGaussians (LAG) is then constructed by combining the Lie algebrized Gaussiancomponents with mixture weights. We apply LAG features to scene categoryrecognition problem and observe state-of-the-art performance on 15Scenesbenchmark.
arxiv-3300-90 | A Novel Frank-Wolfe Algorithm. Analysis and Applications to Large-Scale SVM Training | http://arxiv.org/pdf/1304.1014v2.pdf | author:Hector Allende, Emanuele Frandi, Ricardo Nanculef, Claudio Sartori category:cs.CV cs.AI cs.LG math.OC stat.ML published:2013-04-03 summary:Recently, there has been a renewed interest in the machine learning communityfor variants of a sparse greedy approximation procedure for concaveoptimization known as {the Frank-Wolfe (FW) method}. In particular, thisprocedure has been successfully applied to train large-scale instances ofnon-linear Support Vector Machines (SVMs). Specializing FW to SVM training hasallowed to obtain efficient algorithms but also important theoretical results,including convergence analysis of training algorithms and new characterizationsof model sparsity. In this paper, we present and analyze a novel variant of the FW method basedon a new way to perform away steps, a classic strategy used to accelerate theconvergence of the basic FW procedure. Our formulation and analysis is focusedon a general concave maximization problem on the simplex. However, thespecialization of our algorithm to quadratic forms is strongly related to someclassic methods in computational geometry, namely the Gilbert and MDMalgorithms. On the theoretical side, we demonstrate that the method matches theguarantees in terms of convergence rate and number of iterations obtained byusing classic away steps. In particular, the method enjoys a linear rate ofconvergence, a result that has been recently proved for MDM on quadratic forms. On the practical side, we provide experiments on several classificationdatasets, and evaluate the results using statistical tests. Experiments showthat our method is faster than the FW method with classic away steps, and workswell even in the cases in which classic away steps slow down the algorithm.Furthermore, these improvements are obtained without sacrificing the predictiveaccuracy of the obtained SVM model.
arxiv-3300-91 | A Semiparametric Bayesian Extreme Value Model Using a Dirichlet Process Mixture of Gamma Densities | http://arxiv.org/pdf/1304.0596v1.pdf | author:Jairo Fuquene category:stat.ML stat.AP stat.ME published:2013-04-02 summary:In this paper we propose a model with a Dirichlet process mixture of gammadensities in the bulk part below threshold and a generalized Pareto density inthe tail for extreme value estimation. The proposed model is simple andflexible allowing us posterior density estimation and posterior inference forhigh quantiles. The model works well even for small sample sizes and in theabsence of prior information. We evaluate the performance of the proposed modelthrough a simulation study. Finally, the proposed model is applied to a realenvironmental data.
arxiv-3300-92 | Event management for large scale event-driven digital hardware spiking neural networks | http://arxiv.org/pdf/1304.0640v1.pdf | author:Louis-Charles Caron, \and Michiel D'Haene, \and Frédéric Mailhot, \and Benjamin Schrauwen, \and Jean Rouat category:cs.NE cs.AI cs.DC published:2013-04-02 summary:The interest in brain-like computation has led to the design of a plethora ofinnovative neuromorphic systems. Individually, spiking neural networks (SNNs),event-driven simulation and digital hardware neuromorphic systems get a lot ofattention. Despite the popularity of event-driven SNNs in software, very fewdigital hardware architectures are found. This is because existing hardwaresolutions for event management scale badly with the number of events. Thispaper introduces the structured heap queue, a pipelined digital hardware datastructure, and demonstrates its suitability for event management. Thestructured heap queue scales gracefully with the number of events, allowing theefficient implementation of large scale digital hardware event-driven SNNs. Thescaling is linear for memory, logarithmic for logic resources and constant forprocessing time. The use of the structured heap queue is demonstrated onfield-programmable gate array (FPGA) with an image segmentation experiment anda SNN of 65~536 neurons and 513~184 synapses. Events can be processed at therate of 1 every 7 clock cycles and a 406$\times$158 pixel image is segmented in200 ms.
arxiv-3300-93 | Sparse Signal Processing with Linear and Non-Linear Observations: A Unified Shannon Theoretic Approach | http://arxiv.org/pdf/1304.0682v7.pdf | author:Cem Aksoylar, George Atia, Venkatesh Saligrama category:cs.IT cs.LG math.IT math.ST stat.ML stat.TH published:2013-04-02 summary:We derive fundamental sample complexity bounds for recovering sparse andstructured signals for linear and nonlinear observation models including sparseregression, group testing, multivariate regression and problems with missingfeatures. In general, sparse signal processing problems can be characterized interms of the following Markovian property. We are given a set of $N$ variables$X_1,X_2,\ldots,X_N$, and there is an unknown subset of variables $S \subset[N]$ that are \emph{relevant} for predicting outcomes $Y$. More specifically,when $Y$ is conditioned on $\{X_n\}_{n\in S}$ it is conditionally independentof the other variables, $\{X_n\}_{n \not \in S}$. Our goal is to identify theset $S$ from samples of the variables $X$ and the associated outcomes $Y$. Wecharacterize this problem as a version of the noisy channel coding problem.Using asymptotic information theoretic analyses, we establish mutualinformation formulas that provide sufficient and necessary conditions on thenumber of samples required to successfully recover the salient variables. Thesemutual information expressions unify conditions for both linear and nonlinearobservations. We then compute sample complexity bounds for the aforementionedmodels, based on the mutual information expressions in order to demonstrate theapplicability and flexibility of our results in general sparse signalprocessing models.
arxiv-3300-94 | Representation, Approximation and Learning of Submodular Functions Using Low-rank Decision Trees | http://arxiv.org/pdf/1304.0730v1.pdf | author:Vitaly Feldman, Pravesh Kothari, Jan Vondrak category:cs.LG cs.CC cs.DS published:2013-04-02 summary:We study the complexity of approximate representation and learning ofsubmodular functions over the uniform distribution on the Boolean hypercube$\{0,1\}^n$. Our main result is the following structural theorem: anysubmodular function is $\epsilon$-close in $\ell_2$ to a real-valued decisiontree (DT) of depth $O(1/\epsilon^2)$. This immediately implies that anysubmodular function is $\epsilon$-close to a function of at most$2^{O(1/\epsilon^2)}$ variables and has a spectral $\ell_1$ norm of$2^{O(1/\epsilon^2)}$. It also implies the closest previous result that statesthat submodular functions can be approximated by polynomials of degree$O(1/\epsilon^2)$ (Cheraghchi et al., 2012). Our result is proved byconstructing an approximation of a submodular function by a DT of rank$4/\epsilon^2$ and a proof that any rank-$r$ DT can be $\epsilon$-approximatedby a DT of depth $\frac{5}{2}(r+\log(1/\epsilon))$. We show that these structural results can be exploited to give anattribute-efficient PAC learning algorithm for submodular functions running intime $\tilde{O}(n^2) \cdot 2^{O(1/\epsilon^{4})}$. The best previous algorithmfor the problem requires $n^{O(1/\epsilon^{2})}$ time and examples (Cheraghchiet al., 2012) but works also in the agnostic setting. In addition, we giveimproved learning algorithms for a number of related settings. We also prove that our PAC and agnostic learning algorithms are essentiallyoptimal via two lower bounds: (1) an information-theoretic lower bound of$2^{\Omega(1/\epsilon^{2/3})}$ on the complexity of learning monotonesubmodular functions in any reasonable model; (2) computational lower bound of$n^{\Omega(1/\epsilon^{2/3})}$ based on a reduction to learning of sparseparities with noise, widely-believed to be intractable. These are the firstlower bounds for learning of submodular functions over the uniformdistribution.
arxiv-3300-95 | O(logT) Projections for Stochastic Optimization of Smooth and Strongly Convex Functions | http://arxiv.org/pdf/1304.0740v1.pdf | author:Lijun Zhang, Tianbao Yang, Rong Jin, Xiaofei He category:cs.LG published:2013-04-02 summary:Traditional algorithms for stochastic optimization require projecting thesolution at each iteration into a given domain to ensure its feasibility. Whenfacing complex domains, such as positive semi-definite cones, the projectionoperation can be expensive, leading to a high computational cost per iteration.In this paper, we present a novel algorithm that aims to reduce the number ofprojections for stochastic optimization. The proposed algorithm combines thestrength of several recent developments in stochastic optimization, includingmini-batch, extra-gradient, and epoch gradient descent, in order to effectivelyexplore the smoothness and strong convexity. We show, both in expectation andwith a high probability, that when the objective function is both smooth andstrongly convex, the proposed algorithm achieves the optimal $O(1/T)$ rate ofconvergence with only $O(\log T)$ projections. Our empirical study verifies thetheoretical result.
arxiv-3300-96 | An improved quasar detection method in EROS-2 and MACHO LMC datasets | http://arxiv.org/pdf/1304.0401v1.pdf | author:Karim Pichara, Pavlos Protopapas, Dae-Won Kim, Jean-Baptiste Marquette, Patrick Tisserand category:astro-ph.IM stat.ML published:2013-04-01 summary:We present a new classification method for quasar identification in theEROS-2 and MACHO datasets based on a boosted version of Random Forestclassifier. We use a set of variability features including parameters of acontinuous auto regressive model. We prove that continuous auto regressiveparameters are very important discriminators in the classification process. Wecreate two training sets (one for EROS-2 and one for MACHO datasets) usingknown quasars found in the LMC. Our model's accuracy in both EROS-2 and MACHOtraining sets is about 90% precision and 86% recall, improving the state of theart models accuracy in quasar detection. We apply the model on the complete,including 28 million objects, EROS-2 and MACHO LMC datasets, finding 1160 and2551 candidates respectively. To further validate our list of candidates, wecrossmatched our list with a previous 663 known strong candidates, getting 74%of matches for MACHO and 40% in EROS-2. The main difference on matching levelis because EROS-2 is a slightly shallower survey which translates tosignificantly lower signal-to-noise ratio lightcurves.
arxiv-3300-97 | Splitting Methods for Convex Clustering | http://arxiv.org/pdf/1304.0499v2.pdf | author:Eric C. Chi, Kenneth Lange category:stat.ML math.NA math.OC stat.CO published:2013-04-01 summary:Clustering is a fundamental problem in many scientific applications. Standardmethods such as $k$-means, Gaussian mixture models, and hierarchicalclustering, however, are beset by local minima, which are sometimes drasticallysuboptimal. Recently introduced convex relaxations of $k$-means andhierarchical clustering shrink cluster centroids toward one another and ensurea unique global minimizer. In this work we present two splitting methods forsolving the convex clustering problem. The first is an instance of thealternating direction method of multipliers (ADMM); the second is an instanceof the alternating minimization algorithm (AMA). In contrast to previouslyconsidered algorithms, our ADMM and AMA formulations provide simple and unifiedframeworks for solving the convex clustering problem under the previouslystudied norms and open the door to potentially novel norms. We demonstrate theperformance of our algorithm on both simulated and real data examples. Whilethe differences between the two algorithms appear to be minor on the surface,complexity analysis and numerical experiments show AMA to be significantly moreefficient.
arxiv-3300-98 | Stroke-Based Cursive Character Recognition | http://arxiv.org/pdf/1304.0421v1.pdf | author:K. C. Santosh, E. Iwata category:cs.CV published:2013-04-01 summary:Human eye can see and read what is written or displayed either in naturalhandwriting or in printed format. The same work in case the machine does iscalled handwriting recognition. Handwriting recognition can be broken down intotwo categories: off-line and on-line. ...
arxiv-3300-99 | Fast Feature Reduction in intrusion detection datasets | http://arxiv.org/pdf/1305.2388v1.pdf | author:Shafigh Parsazad, Ehsan Saboori, Amin Allahyar category:cs.CR cs.LG published:2013-04-01 summary:In the most intrusion detection systems (IDS), a system tries to learncharacteristics of different type of attacks by analyzing packets that sent orreceived in network. These packets have a lot of features. But not all of themis required to be analyzed to detect that specific type of attack. Detectionspeed and computational cost is another vital matter here, because in thesetypes of problems, datasets are very huge regularly. In this paper we tried topropose a very simple and fast feature selection method to eliminate featureswith no helpful information on them. Result faster learning in process ofredundant feature omission. We compared our proposed method with three mostsuccessful similarity based feature selection algorithm including CorrelationCoefficient, Least Square Regression Error and Maximal Information CompressionIndex. After that we used recommended features by each of these algorithms intwo popular classifiers including: Bayes and KNN classifier to measure thequality of the recommendations. Experimental result shows that although theproposed method can't outperform evaluated algorithms with high differences inaccuracy, but in computational cost it has huge superiority over them.
arxiv-3300-100 | Parallel Computation Is ESS | http://arxiv.org/pdf/1304.0160v8.pdf | author:Nabarun Mondal, Partha P. Ghosh category:cs.LG cs.AI cs.GT published:2013-03-31 summary:There are enormous amount of examples of Computation in nature, exemplifiedacross multiple species in biology. One crucial aim for these computationsacross all life forms their ability to learn and thereby increase the chance oftheir survival. In the current paper a formal definition of autonomous learningis proposed. From that definition we establish a Turing Machine model forlearning, where rule tables can be added or deleted, but can not be modified.Sequential and parallel implementations of this model are discussed. It isfound that for general purpose learning based on this model, theimplementations capable of parallel execution would be evolutionarily stable.This is proposed to be of the reasons why in Nature parallelism in computationis found in abundance.
arxiv-3300-101 | A cookbook of translating English to Xapi | http://arxiv.org/pdf/1304.0715v1.pdf | author:Ladislau Bölöni category:cs.AI cs.CL published:2013-03-31 summary:The Xapagy cognitive architecture had been designed to perform narrativereasoning: to model and mimic the activities performed by humans whenwitnessing, reading, recalling, narrating and talking about stories. Xapagycommunicates with the outside world using Xapi, a simplified, "pidgin" languagewhich is strongly tied to the internal representation model (instances, scenesand verb instances) and reasoning techniques (shadows and headless shadows).While not fully a semantic equivalent of natural language, Xapi can represent awide range of complex stories. We illustrate the representation technique usedin Xapi through examples taken from folk physics, folk psychology as well assome more unusual literary examples. We argue that while the Xapi modelrepresents a conceptual shift from the English representation, the mapping islogical and consistent, and a trained knowledge engineer can translate betweenEnglish and Xapi at near-native speed.
arxiv-3300-102 | Compressive adaptive computational ghost imaging | http://arxiv.org/pdf/1304.0243v1.pdf | author:Marc Aßmann, Manfred Bayer category:physics.optics cs.CV published:2013-03-31 summary:Compressive sensing is considered a huge breakthrough in signal acquisition.It allows recording an image consisting of $N^2$ pixels using much fewer than$N^2$ measurements if it can be transformed to a basis where most pixels takeon negligibly small values. Standard compressive sensing techniques suffer fromthe computational overhead needed to reconstruct an image with typicalcomputation times between hours and days and are thus not optimal forapplications in physics and spectroscopy. We demonstrate an adaptivecompressive sampling technique that performs measurements directly in a sparsebasis. It needs much fewer than $N^2$ measurements without any computationaloverhead, so the result is available instantly.
arxiv-3300-103 | A Neuromorphic VLSI Design for Spike Timing and Rate Based Synaptic Plasticity | http://arxiv.org/pdf/1304.0090v1.pdf | author:Mostafa Rahimi Azghadi, Said Al-Sarawi, Derek Abbott, Nicolangelo Iannella category:cs.NE published:2013-03-30 summary:Triplet-based Spike Timing Dependent Plasticity (TSTDP) is a powerfulsynaptic plasticity rule that acts beyond conventional pair-based STDP (PSTDP).Here, the TSTDP is capable of reproducing the outcomes from a variety ofbiological experiments, while the PSTDP rule fails to reproduce them.Additionally, it has been shown that the behaviour inherent to the spikerate-based Bienenstock-Cooper-Munro (BCM) synaptic plasticity rule can alsoemerge from the TSTDP rule. This paper proposes an analog implementation of theTSTDP rule. The proposed VLSI circuit has been designed using the AMS 0.35 umCMOS process and has been simulated using design kits for Synopsys and Cadencetools. Simulation results demonstrate how well the proposed circuit can altersynaptic weights according to the timing difference amongst a set of differentpatterns of spikes. Furthermore, the circuit is shown to give rise to aBCM-like learning rule, which is a rate-based rule. To mimic implementationenvironment, a 1000 run Monte Carlo (MC) analysis was conducted on the proposedcircuit. The presented MC simulation analysis and the simulation result fromfine-tuned circuits show that, it is possible to mitigate the effect of processvariations in the proof of concept circuit, however, a practical variationaware design technique is required to promise a high circuit performance in alarge scale neural network. We believe that the proposed design can play asignificant role in future VLSI implementations of both spike timing and ratebased neuromorphic learning systems.
arxiv-3300-104 | Meaning-focused and Quantum-inspired Information Retrieval | http://arxiv.org/pdf/1304.0104v1.pdf | author:Diederik Aerts, Jan Broekaert, Sandro Sozzo, Tomas Veloz category:cs.IR cs.CL quant-ph published:2013-03-30 summary:In recent years, quantum-based methods have promisingly integrated thetraditional procedures in information retrieval (IR) and natural languageprocessing (NLP). Inspired by our research on the identification andapplication of quantum structures in cognition, more specifically our work onthe representation of concepts and their combinations, we put forward a'quantum meaning based' framework for structured query retrieval in textcorpora and standardized testing corpora. This scheme for IR rests onconsidering as basic notions, (i) 'entities of meaning', e.g., concepts andtheir combinations and (ii) traces of such entities of meaning, which is howdocuments are considered in this approach. The meaning content of these'entities of meaning' is reconstructed by solving an 'inverse problem' in thequantum formalism, consisting of reconstructing the full states of the entitiesof meaning from their collapsed states identified as traces in relevantdocuments. The advantages with respect to traditional approaches, such asLatent Semantic Analysis (LSA), are discussed by means of concrete examples.
arxiv-3300-105 | On the symmetrical Kullback-Leibler Jeffreys centroids | http://arxiv.org/pdf/1303.7286v3.pdf | author:Frank Nielsen category:cs.IT cs.LG math.IT stat.ML published:2013-03-29 summary:Due to the success of the bag-of-word modeling paradigm, clusteringhistograms has become an important ingredient of modern information processing.Clustering histograms can be performed using the celebrated $k$-meanscentroid-based algorithm. From the viewpoint of applications, it is usuallyrequired to deal with symmetric distances. In this letter, we consider theJeffreys divergence that symmetrizes the Kullback-Leibler divergence, andinvestigate the computation of Jeffreys centroids. We first prove that theJeffreys centroid can be expressed analytically using the Lambert $W$ functionfor positive histograms. We then show how to obtain a fast guaranteedapproximation when dealing with frequency histograms. Finally, we conclude withsome remarks on the $k$-means histogram clustering.
arxiv-3300-106 | Universal Approximation Depth and Errors of Narrow Belief Networks with Discrete Units | http://arxiv.org/pdf/1303.7461v2.pdf | author:Guido F. Montúfar category:stat.ML cs.LG math.PR published:2013-03-29 summary:We generalize recent theoretical work on the minimal number of layers ofnarrow deep belief networks that can approximate any probability distributionon the states of their visible units arbitrarily well. We relax the setting ofbinary units (Sutskever and Hinton, 2008; Le Roux and Bengio, 2008, 2010;Mont\'ufar and Ay, 2011) to units with arbitrary finite state spaces, and thevanishing approximation error to an arbitrary approximation error tolerance.For example, we show that a $q$-ary deep belief network with $L\geq2+\frac{q^{\lceil m-\delta \rceil}-1}{q-1}$ layers of width $n \leq m +\log_q(m) + 1$ for some $m\in \mathbb{N}$ can approximate any probabilitydistribution on $\{0,1,\ldots,q-1\}^n$ without exceeding a Kullback-Leiblerdivergence of $\delta$. Our analysis covers discrete restricted Boltzmannmachines and na\"ive Bayes models as special cases.
arxiv-3300-107 | ParceLiNGAM: A causal ordering method robust against latent confounders | http://arxiv.org/pdf/1303.7410v2.pdf | author:Tatsuya Tashiro, Shohei Shimizu, Aapo Hyvarinen, Takashi Washio category:stat.ML published:2013-03-29 summary:We consider learning a causal ordering of variables in a linear non-Gaussianacyclic model called LiNGAM. Several existing methods have been shown toconsistently estimate a causal ordering assuming that all the model assumptionsare correct. But, the estimation results could be distorted if some assumptionsactually are violated. In this paper, we propose a new algorithm for learningcausal orders that is robust against one typical violation of the modelassumptions: latent confounders. The key idea is to detect latent confoundersby testing independence between estimated external influences and find subsets(parcels) that include variables that are not affected by latent confounders.We demonstrate the effectiveness of our method using artificial data andsimulated brain imaging data.
arxiv-3300-108 | The two-dimensional Gabor function adapted to natural image statistics: An analytical model of simple-cell responses in the early visual system | http://arxiv.org/pdf/1304.0023v3.pdf | author:Peter Loxley category:cs.CV published:2013-03-29 summary:The two-dimensional Gabor function is adapted to natural image statistics bylearning the joint distribution of the Gabor function parameters. The jointdistribution is then approximated to yield an analytical model of simple-cellreceptive fields. Adapting a basis of Gabor functions is found to take an orderof magnitude less computation than learning an equivalent non-parameterizedbasis. Derived learning rules are shown to be capable of adapting Gaborparameters to the statistics of images of man-made and natural environments.Learning is found to be most pronounced in three Gabor parameters thatrepresent the size, aspect-ratio, and spatial frequency of the two-dimensionalGabor function. These three parameters are characterized by non-uniformmarginal distributions with heavy tails -- most likely due to scale invariancein natural images -- and all three parameters are strongly correlated:resulting in a basis of multiscale Gabor functions with similar aspect-ratios,and size-dependent spatial frequencies. The Gabor orientation and phaseparameters do not appear to gain anything from learning over natural images.Different tuning strategies are found by controlling learning through the Gaborparameter learning rates. Two opposing strategies include well-resolvedorientation and well-resolved spatial frequency. On image reconstruction, abasis of Gabor functions with fitted marginal distributions is shown tosignificantly outperform a basis of Gabor functions generated from uniformlysampled parameters. An additional increase in performance results when thestrong correlations are included. However, the best analytical model does notyet achieve the performance of the learned model. A comparison with estimatesfor biological simple cells shows that the Gabor function adapted to naturalimage statistics correctly predicts some key receptive field properties.
arxiv-3300-109 | Infinitely imbalanced binomial regression and deformed exponential families | http://arxiv.org/pdf/1303.7297v2.pdf | author:Tomonari Sei category:math.ST stat.ML stat.TH published:2013-03-29 summary:The logistic regression model is known to converge to a Poisson point processmodel if the binary response tends to infinitely imbalanced. In this paper, itis shown that this phenomenon is universal in a wide class of link functions onbinomial regression. The proof relies on the extreme value theory. For thelogit, probit and complementary log-log link functions, the intensity measureof the point process becomes an exponential family. For some other linkfunctions, deformed exponential families appear. A penalized maximum likelihoodestimator for the Poisson point process model is suggested.
arxiv-3300-110 | Geometric tree kernels: Classification of COPD from airway tree geometry | http://arxiv.org/pdf/1303.7390v2.pdf | author:Aasa Feragen, Jens Petersen, Dominik Grimm, Asger Dirksen, Jesper Holst Pedersen, Karsten Borgwardt, Marleen de Bruijne category:cs.CV 68T10 published:2013-03-29 summary:Methodological contributions: This paper introduces a family of kernels foranalyzing (anatomical) trees endowed with vector valued measurements made alongthe tree. While state-of-the-art graph and tree kernels use combinatorialtree/graph structure with discrete node and edge labels, the kernels presentedin this paper can include geometric information such as branch shape, branchradius or other vector valued properties. In addition to being flexible intheir ability to model different types of attributes, the presented kernels arecomputationally efficient and some of them can easily be computed for largedatasets (N of the order 10.000) of trees with 30-600 branches. Combining thekernels with standard machine learning tools enables us to analyze the relationbetween disease and anatomical tree structure and geometry. Experimentalresults: The kernels are used to compare airway trees segmented from low-doseCT, endowed with branch shape descriptors and airway wall area percentagemeasurements made along the tree. Using kernelized hypothesis testing we showthat the geometric airway trees are significantly differently distributed inpatients with Chronic Obstructive Pulmonary Disease (COPD) than in healthyindividuals. The geometric tree kernels also give a significant increase in theclassification accuracy of COPD from geometric tree structure endowed withairway wall thickness measurements in comparison with state-of-the-art methods,giving further insight into the relationship between airway wall thickness andCOPD. Software: Software for computing kernels and statistical tests isavailable at http://image.diku.dk/aasa/software.php.
arxiv-3300-111 | Registration of Images with Outliers Using Joint Saliency Map | http://arxiv.org/pdf/1304.8052v1.pdf | author:Binjie Qin, Zhijun Gu, Xianjun Sun, Yisong Lv category:cs.CV published:2013-03-29 summary:Mutual information (MI) is a popular similarity measure for imageregistration, whereby good registration can be achieved by maximizing thecompactness of the clusters in the joint histogram. However, MI is sensitive tothe "outlier" objects that appear in one image but not the other, and alsosuffers from local and biased maxima. We propose a novel joint saliency map(JSM) to highlight the corresponding salient structures in the two images, andemphatically group those salient structures into the smoothed compact clustersin the weighted joint histogram. This strategy could solve both the outlier andthe local maxima problems. Experimental results show that the JSM-MI basedalgorithm is not only accurate but also robust for registration of challengingimage pairs with outliers.
arxiv-3300-112 | A problem dependent analysis of SOCP algorithms in noisy compressed sensing | http://arxiv.org/pdf/1304.0480v1.pdf | author:Mihailo Stojnic category:cs.IT math.IT stat.ML published:2013-03-29 summary:Under-determined systems of linear equations with sparse solutions have beenthe subject of an extensive research in last several years above all due toresults of \cite{CRT,CanRomTao06,DonohoPol}. In this paper we will consider\emph{noisy} under-determined linear systems. In a breakthrough\cite{CanRomTao06} it was established that in \emph{noisy} systems for anylinear level of under-determinedness there is a linear sparsity that can be\emph{approximately} recovered through an SOCP (second order cone programming)optimization algorithm so that the approximate solution vector is (in an$\ell_2$-norm sense) guaranteed to be no further from the sparse unknown vectorthan a constant times the noise. In our recent work \cite{StojnicGenSocp10} weestablished an alternative framework that can be used for statisticalperformance analysis of the SOCP algorithms. To demonstrate how the frameworkworks we then showed in \cite{StojnicGenSocp10} how one can use it to preciselycharacterize the \emph{generic} (worst-case) performance of the SOCP. In thispaper we present a different set of results that can be obtained through theframework of \cite{StojnicGenSocp10}. The results will relate to \emph{problemdependent} performance analysis of SOCP's. We will consider specific types ofunknown sparse vectors and characterize the SOCP performance when used forrecovery of such vectors. We will also show that our theoretical predictionsare in a solid agreement with the results one can get through numericalsimulations.
arxiv-3300-113 | Translation-Invariant Shrinkage/Thresholding of Group Sparse Signals | http://arxiv.org/pdf/1304.0035v1.pdf | author:Po-Yu Chen, Ivan W. Selesnick category:cs.CV cs.LG cs.SD published:2013-03-29 summary:This paper addresses signal denoising when large-amplitude coefficients formclusters (groups). The L1-norm and other separable sparsity models do notcapture the tendency of coefficients to cluster (group sparsity). This workdevelops an algorithm, called 'overlapping group shrinkage' (OGS), based on theminimization of a convex cost function involving a group-sparsity promotingpenalty function. The groups are fully overlapping so the denoising method istranslation-invariant and blocking artifacts are avoided. Based on theprinciple of majorization-minimization (MM), we derive a simple iterativeminimization algorithm that reduces the cost function monotonically. Aprocedure for setting the regularization parameter, based on attenuating thenoise to a specified level, is also described. The proposed approach isillustrated on speech enhancement, wherein the OGS approach is applied in theshort-time Fourier transform (STFT) domain. The denoised speech produced by OGSdoes not suffer from musical noise.
arxiv-3300-114 | Exploring the Role of Logically Related Non-Question Phrases for Answering Why-Questions | http://arxiv.org/pdf/1303.7310v1.pdf | author:Niraj Kumar, Rashmi Gangadharaiah, Kannan Srinathan, Vasudeva Varma category:cs.CL cs.IR H.3.m published:2013-03-29 summary:In this paper, we show that certain phrases although not present in a givenquestion/query, play a very important role in answering the question. Exploringthe role of such phrases in answering questions not only reduces the dependencyon matching question phrases for extracting answers, but also improves thequality of the extracted answers. Here matching question phrases means phraseswhich co-occur in given question and candidate answers. To achieve the abovediscussed goal, we introduce a bigram-based word graph model populated withsemantic and topical relatedness of terms in the given document. Next, we applyan improved version of ranking with a prior-based approach, which ranks allwords in the candidate document with respect to a set of root words (i.e.non-stopwords present in the question and in the candidate document). As aresult, terms logically related to the root words are scored higher than termsthat are not related to the root words. Experimental results show that ourdevised system performs better than state-of-the-art for the task of answeringWhy-questions.
arxiv-3300-115 | Age group and gender recognition from human facial images | http://arxiv.org/pdf/1304.0019v1.pdf | author:Tizita Nesibu Shewaye category:cs.CV published:2013-03-29 summary:This work presents an automatic human gender and age group recognition systembased on human facial images. It makes an extensive experiment with row pixelintensity valued features and Discrete Cosine Transform (DCT) coefficientfeatures with Principal Component Analysis and k-Nearest Neighborclassification to identify the best recognition approach. The final resultsshow approaches using DCT coefficient outperform their counter parts resultingin a 99% correct gender recognition rate and 68% correct age group recognitionrate (considering four distinct age groups) in unseen test images. Detailedexperimental settings and obtained results are clearly presented and explainedin this report.
arxiv-3300-116 | Independent Vector Analysis: Identification Conditions and Performance Bounds | http://arxiv.org/pdf/1303.7474v1.pdf | author:Matthew Anderson, Geng-Shen Fu, Ronald Phlypo, Tülay Adalı category:cs.LG cs.IT math.IT stat.ML published:2013-03-29 summary:Recently, an extension of independent component analysis (ICA) from one tomultiple datasets, termed independent vector analysis (IVA), has been thesubject of significant research interest. IVA has also been shown to be ageneralization of Hotelling's canonical correlation analysis. In this paper, weprovide the identification conditions for a general IVA formulation, whichaccounts for linear, nonlinear, and sample-to-sample dependencies. Theidentification conditions are a generalization of previous results for ICA andfor IVA when samples are independently and identically distributed.Furthermore, a principal aim of IVA is the identification of dependent sourcesbetween datasets. Thus, we provide the additional conditions for when thearbitrary ordering of the sources within each dataset is common. Performancebounds in terms of the Cramer-Rao lower bound are also provided for thedemixing matrices and interference to source ratio. The performance of two IVAalgorithms are compared to the theoretical bounds.
arxiv-3300-117 | Relevance As a Metric for Evaluating Machine Learning Algorithms | http://arxiv.org/pdf/1303.7093v3.pdf | author:Aravind Kota Gopalakrishna, Tanir Ozcelebi, Antonio Liotta, Johan J. Lukkien category:stat.ML cs.LG published:2013-03-28 summary:In machine learning, the choice of a learning algorithm that is suitable forthe application domain is critical. The performance metric used to comparedifferent algorithms must also reflect the concerns of users in the applicationdomain under consideration. In this work, we propose a novel probability-basedperformance metric called Relevance Score for evaluating supervised learningalgorithms. We evaluate the proposed metric through empirical analysis on adataset gathered from an intelligent lighting pilot installation. In comparisonto the commonly used Classification Accuracy metric, the Relevance Score provesto be more appropriate for a certain class of applications.
arxiv-3300-118 | Confidence sets for persistence diagrams | http://arxiv.org/pdf/1303.7117v3.pdf | author:Brittany Terese Fasy, Fabrizio Lecci, Alessandro Rinaldo, Larry Wasserman, Sivaraman Balakrishnan, Aarti Singh category:math.ST cs.CG cs.LG stat.TH published:2013-03-28 summary:Persistent homology is a method for probing topological properties of pointclouds and functions. The method involves tracking the birth and death oftopological features (2000) as one varies a tuning parameter. Features withshort lifetimes are informally considered to be "topological noise," and thosewith a long lifetime are considered to be "topological signal." In this paper,we bring some statistical ideas to persistent homology. In particular, wederive confidence sets that allow us to separate topological signal fromtopological noise.
arxiv-3300-119 | A Massively Parallel Associative Memory Based on Sparse Neural Networks | http://arxiv.org/pdf/1303.7032v2.pdf | author:Zhe Yao, Vincent Gripon, Michael G. Rabbat category:cs.AI cs.DC cs.NE published:2013-03-28 summary:Associative memories store content in such a way that the content can belater retrieved by presenting the memory with a small portion of the content,rather than presenting the memory with an address as in more traditionalmemories. Associative memories are used as building blocks for algorithmswithin database engines, anomaly detection systems, compression algorithms, andface recognition systems. A classical example of an associative memory is theHopfield neural network. Recently, Gripon and Berrou have introduced analternative construction which builds on ideas from the theory of errorcorrecting codes and which greatly outperforms the Hopfield network incapacity, diversity, and efficiency. In this paper we implement a variation ofthe Gripon-Berrou associative memory on a general purpose graphical processingunit (GPU). The work of Gripon and Berrou proposes two retrieval rules,sum-of-sum and sum-of-max. The sum-of-sum rule uses only matrix-vectormultiplication and is easily implemented on the GPU. The sum-of-max rule ismuch less straightforward to implement because it involves non-linearoperations. However, the sum-of-max rule gives significantly better retrievalerror rates. We propose a hybrid rule tailored for implementation on a GPUwhich achieves a 880-fold speedup without sacrificing any accuracy.
arxiv-3300-120 | Inductive Hashing on Manifolds | http://arxiv.org/pdf/1303.7043v1.pdf | author:Fumin Shen, Chunhua Shen, Qinfeng Shi, Anton van den Hengel, Zhenmin Tang category:cs.LG published:2013-03-28 summary:Learning based hashing methods have attracted considerable attention due totheir ability to greatly increase the scale at which existing algorithms mayoperate. Most of these methods are designed to generate binary codes thatpreserve the Euclidean distance in the original space. Manifold learningtechniques, in contrast, are better able to model the intrinsic structureembedded in the original high-dimensional data. The complexity of these models,and the problems with out-of-sample data, have previously rendered themunsuitable for application to large-scale embedding, however. In this work, weconsider how to learn compact binary embeddings on their intrinsic manifolds.In order to address the above-mentioned difficulties, we describe an efficient,inductive solution to the out-of-sample data problem, and a process by whichnon-parametric manifold learning may be used as the basis of a hashing method.Our proposed approach thus allows the development of a range of new hashingtechniques exploiting the flexibility of the wide variety of manifold learningapproaches available. We particularly show that hashing on the basis of t-SNE .
arxiv-3300-121 | Large-Scale Automatic Reconstruction of Neuronal Processes from Electron Microscopy Images | http://arxiv.org/pdf/1303.7186v1.pdf | author:Verena Kaynig, Amelio Vazquez-Reina, Seymour Knowles-Barley, Mike Roberts, Thouis R. Jones, Narayanan Kasthuri, Eric Miller, Jeff Lichtman, Hanspeter Pfister category:q-bio.NC cs.CV published:2013-03-28 summary:Automated sample preparation and electron microscopy enables acquisition ofvery large image data sets. These technical advances are of special importanceto the field of neuroanatomy, as 3D reconstructions of neuronal processes atthe nm scale can provide new insight into the fine grained structure of thebrain. Segmentation of large-scale electron microscopy data is the mainbottleneck in the analysis of these data sets. In this paper we present apipeline that provides state-of-the art reconstruction performance whilescaling to data sets in the GB-TB range. First, we train a random forestclassifier on interactive sparse user annotations. The classifier output iscombined with an anisotropic smoothing prior in a Conditional Random Fieldframework to generate multiple segmentation hypotheses per image. Thesesegmentations are then combined into geometrically consistent 3D objects bysegmentation fusion. We provide qualitative and quantitative evaluation of theautomatic segmentation and demonstrate large-scale 3D reconstructions ofneuronal processes from a $\mathbf{27,000}$ $\mathbf{\mu m^3}$ volume of braintissue over a cube of $\mathbf{30 \; \mu m}$ in each dimension corresponding to1000 consecutive image sections. We also introduce Mojo, a proofreading toolincluding semi-automated correction of merge errors based on sparse userscribbles.
arxiv-3300-122 | Scalable Text and Link Analysis with Mixed-Topic Link Models | http://arxiv.org/pdf/1303.7264v1.pdf | author:Yaojia Zhu, Xiaoran Yan, Lise Getoor, Cristopher Moore category:cs.LG cs.IR cs.SI stat.ML published:2013-03-28 summary:Many data sets contain rich information about objects, as well as pairwiserelations between them. For instance, in networks of websites, scientificpapers, and other documents, each node has content consisting of a collectionof words, as well as hyperlinks or citations to other nodes. In order toperform inference on such data sets, and make predictions and recommendations,it is useful to have models that are able to capture the processes whichgenerate the text at each node and the links between them. In this paper, wecombine classic ideas in topic modeling with a variant of the mixed-membershipblock model recently developed in the statistical physics community. Theresulting model has the advantage that its parameters, including the mixture oftopics of each document and the resulting overlapping communities, can beinferred with a simple and scalable expectation-maximization algorithm. We testour model on three data sets, performing unsupervised topic classification andlink prediction. For both tasks, our model outperforms several existingstate-of-the-art methods, achieving higher accuracy with significantly lesscomputation, analyzing a data set with 1.3 million words and 44 thousand linksin a few minutes.
arxiv-3300-123 | Detecting Overlapping Temporal Community Structure in Time-Evolving Networks | http://arxiv.org/pdf/1303.7226v1.pdf | author:Yudong Chen, Vikas Kawadia, Rahul Urgaonkar category:cs.SI cs.LG physics.soc-ph stat.ML published:2013-03-28 summary:We present a principled approach for detecting overlapping temporal communitystructure in dynamic networks. Our method is based on the following framework:find the overlapping temporal community structure that maximizes a qualityfunction associated with each snapshot of the network subject to a temporalsmoothness constraint. A novel quality function and a smoothness constraint areproposed to handle overlaps, and a new convex relaxation is used to solve theresulting combinatorial optimization problem. We provide theoretical guaranteesas well as experimental results that reveal community structure in real andsynthetic networks. Our main insight is that certain structures can beidentified only when temporal correlation is considered and when communitiesare allowed to overlap. In general, discovering such overlapping temporalcommunity structure can enhance our understanding of real-world complexnetworks by revealing the underlying stability behind their seemingly chaoticevolution.
arxiv-3300-124 | Exploiting correlation and budget constraints in Bayesian multi-armed bandit optimization | http://arxiv.org/pdf/1303.6746v4.pdf | author:Matthew W. Hoffman, Bobak Shahriari, Nando de Freitas category:stat.ML cs.LG published:2013-03-27 summary:We address the problem of finding the maximizer of a nonlinear smoothfunction, that can only be evaluated point-wise, subject to constraints on thenumber of permitted function evaluations. This problem is also known asfixed-budget best arm identification in the multi-armed bandit literature. Weintroduce a Bayesian approach for this problem and show that it empiricallyoutperforms both the existing frequentist counterpart and other Bayesianoptimization methods. The Bayesian approach places emphasis on detailedmodelling, including the modelling of correlations among the arms. As a result,it can perform well in situations where the number of arms is much larger thanthe number of allowed function evaluation, whereas the frequentist counterpartis inapplicable. This feature enables us to develop and deploy practicalapplications, such as automatic machine learning toolboxes. The paper presentscomprehensive comparisons of the proposed approach, Thompson sampling,classical Bayesian optimization techniques, more recent Bayesian banditapproaches, and state-of-the-art best arm identification methods. This is thefirst comparison of many of these methods in the literature and allows us toexamine the relative merits of their different features.
arxiv-3300-125 | Imprecise Meanings as a Cause of Uncertainty in Medical Knowledge-Based Systems | http://arxiv.org/pdf/1304.3092v1.pdf | author:Steven J. Henkind category:cs.AI cs.CL published:2013-03-27 summary:There has been a considerable amount of work on uncertainty inknowledge-based systems. This work has generally been concerned withuncertainty arising from the strength of inferences and the weight of evidence.In this paper we discuss another type of uncertainty: that which is due toimprecision in the underlying primitives used to represent the knowledge of thesystem. In particular, a given word may denote many similar but not identicalentities. Such words are said to be lexically imprecise. Lexical imprecisionhas caused widespread problems in many areas. Unless this phenomenon isrecognized and appropriately handled, it can degrade the performance ofknowledge-based systems. In particular, it can lead to difficulties with theuser interface, and with the inferencing processes of these systems. Sometechniques are suggested for coping with this phenomenon.
arxiv-3300-126 | Evidential Reasoning in Image Understanding | http://arxiv.org/pdf/1304.2749v1.pdf | author:Minchuan Zhang, Su-shing Chen category:cs.CV cs.AI published:2013-03-27 summary:In this paper, we present some results of evidential reasoning inunderstanding multispectral images of remote sensing systems. TheDempster-Shafer approach of combination of evidences is pursued to yieldcontextual classification results, which are compared with previous results ofthe Bayesian context free classification, contextual classifications of dynamicprogramming and stochastic relaxation approaches.
arxiv-3300-127 | Evidential Reasoning in Parallel Hierarchical Vision Programs | http://arxiv.org/pdf/1304.3098v1.pdf | author:Ze-Nian Li, Leonard Uhr category:cs.AI cs.CV published:2013-03-27 summary:This paper presents an efficient adaptation and application of theDempster-Shafer theory of evidence, one that can be used effectively in amassively parallel hierarchical system for visual pattern perception. Itdescribes the techniques used, and shows in an extended example how they serveto improve the system's performance as it applies a multiple-level set ofprocesses.
arxiv-3300-128 | Comparisons of Reasoning Mechanisms for Computer Vision | http://arxiv.org/pdf/1304.2743v1.pdf | author:Ze-Nian Li category:cs.CV cs.AI published:2013-03-27 summary:An evidential reasoning mechanism based on the Dempster-Shafer theory ofevidence is introduced. Its performance in real-world image analysis iscompared with other mechanisms based on the Bayesian formalism and a simpleweight combination method.
arxiv-3300-129 | Utility-Based Control for Computer Vision | http://arxiv.org/pdf/1304.2367v1.pdf | author:Tod S. Levitt, Thomas O. Binford, Gil J. Ettinger, Patrice Gelband category:cs.CV cs.AI cs.SY published:2013-03-27 summary:Several key issues arise in implementing computer vision recognition of worldobjects in terms of Bayesian networks. Computational efficiency is a drivingforce. Perceptual networks are very deep, typically fifteen levels ofstructure. Images are wide, e.g., an unspecified-number of edges may appearanywhere in an image 512 x 512 pixels or larger. For efficiency, we dynamicallyinstantiate hypotheses of observed objects. The network is not fixed, but iscreated incrementally at runtime. Generation of hypotheses of world objects andindexing of models for recognition are important, but they are not consideredhere [4,11]. This work is aimed at near-term implementation with parallelcomputation in a radar surveillance system, ADRIES [5, 15], and a system forindustrial part recognition, SUCCESSOR [2]. For many applications, vision mustbe faster to be practical and so efficiently controlling the machine visionprocess is critical. Perceptual operators may scan megapixels and may requireminutes of computation time. It is necessary to avoid unnecessary sensoractions and computation. Parallel computation is available at several levels ofprocessor capability. The potential for parallel, distributed computation forhigh-level vision means distributing non-homogeneous computations. This paperaddresses the problem of task control in machine vision systems based onBayesian probability models. We separate control and inference to extend theprevious work [3] to maximize utility instead of probability. Maximizingutility allows adopting perceptual strategies for efficient informationgathering with sensors and analysis of sensor data. Results of controllingmachine vision via utility to recognize military situations are presented inthis paper. Future work extends this to industrial part recognition forSUCCESSOR.
arxiv-3300-130 | Multiple decision trees | http://arxiv.org/pdf/1304.2363v1.pdf | author:Suk Wah Kwok, Chris Carter category:cs.LG cs.AI stat.ML published:2013-03-27 summary:This paper describes experiments, on two domains, to investigate the effectof averaging over predictions of multiple decision trees, instead of using asingle tree. Other authors have pointed out theoretical and commonsense reasonsfor preferring the multiple tree approach. Ideally, we would like to considerpredictions from all trees, weighted by their probability. However, there is avast number of different trees, and it is difficult to estimate the probabilityof each tree. We sidestep the estimation problem by using a modified version ofthe ID3 algorithm to build good trees, and average over only these trees. Ourresults are encouraging. For each domain, we managed to produce a small numberof good trees. We find that it is best to average across sets of trees withdifferent structure; this usually gives better performance than any of theconstituent trees, including the ID3 tree.
arxiv-3300-131 | Machine Learning, Clustering, and Polymorphy | http://arxiv.org/pdf/1304.3432v1.pdf | author:Stephen Jose Hanson, Malcolm Bauer category:cs.AI cs.CL cs.LG published:2013-03-27 summary:This paper describes a machine induction program (WITT) that attempts tomodel human categorization. Properties of categories to which human subjectsare sensitive includes best or prototypical members, relative contrasts betweenputative categories, and polymorphy (neither necessary or sufficient features).This approach represents an alternative to usual Artificial Intelligenceapproaches to generalization and conceptual clustering which tend to focus onnecessary and sufficient feature rules, equivalence classes, and simple searchand match schemes. WITT is shown to be more consistent with humancategorization while potentially including results produced by more traditionalclustering schemes. Applications of this approach in the domains of expertsystems and information retrieval are also discussed.
arxiv-3300-132 | Expectation Propagation for Neural Networks with Sparsity-promoting Priors | http://arxiv.org/pdf/1303.6938v1.pdf | author:Pasi Jylänki, Aapo Nummenmaa, Aki Vehtari category:stat.ML published:2013-03-27 summary:We propose a novel approach for nonlinear regression using a two-layer neuralnetwork (NN) model structure with sparsity-favoring hierarchical priors on thenetwork weights. We present an expectation propagation (EP) approach forapproximate integration over the posterior distribution of the weights, thehierarchical scale parameters of the priors, and the residual scale. Using afactorized posterior approximation we derive a computationally efficientalgorithm, whose complexity scales similarly to an ensemble of independentsparse linear models. The approach enables flexible definition of weight priorswith different sparseness properties such as independent Laplace priors with acommon scale parameter or Gaussian automatic relevance determination (ARD)priors with different relevance parameters for all inputs. The approach can beextended beyond standard activation functions and NN model structures to formflexible nonlinear predictors from multiple sparse linear models. The effectsof the hierarchical priors and the predictive performance of the algorithm areassessed using both simulated and real-world data. Comparisons are made to twoalternative models with ARD priors: a Gaussian process with a NN covariancefunction and marginal maximum a posteriori estimates of the relevanceparameters, and a NN with Markov chain Monte Carlo integration over all theunknown model parameters.
arxiv-3300-133 | Model-based Influence Diagrams for Machine Vision | http://arxiv.org/pdf/1304.1517v1.pdf | author:Tod S. Levitt, John Mark Agosta, Thomas O. Binford category:cs.CV cs.AI published:2013-03-27 summary:We show an approach to automated control of machine vision systems based onincremental creation and evaluation of a particular family of influencediagrams that represent hypotheses of imagery interpretation and possiblesubsequent processing decisions. In our approach, model-based machine visiontechniques are integrated with hierarchical Bayesian inference to provide aframework for representing and matching instances of objects and relationshipsin imagery and for accruing probabilities to rank order conflicting sceneinterpretations. We extend a result of Tatman and Shachter to show that thesequence of processing decisions derived from evaluating the diagrams at eachstage is the same as the sequence that would have been derived by evaluatingthe final influence diagram that contains all random variables created duringthe run of the vision system.
arxiv-3300-134 | Efficiently Using Second Order Information in Large l1 Regularization Problems | http://arxiv.org/pdf/1303.6935v1.pdf | author:Xiaocheng Tang, Katya Scheinberg category:stat.ML cs.LG published:2013-03-27 summary:We propose a novel general algorithm LHAC that efficiently uses second-orderinformation to train a class of large-scale l1-regularized problems. Our methodexecutes cheap iterations while achieving fast local convergence rate byexploiting the special structure of a low-rank matrix, constructed viaquasi-Newton approximation of the Hessian of the smooth loss function. A greedyactive-set strategy, based on the largest violations in the dual constraints,is employed to maintain a working set that iteratively estimates the complementof the optimal active set. This allows for smaller size of subproblems andeventually identifies the optimal active set. Empirical comparisons confirmthat LHAC is highly competitive with several recently proposed state-of-the-artspecialized solvers for sparse logistic regression and sparse inversecovariance matrix selection.
arxiv-3300-135 | An investigation towards wavelet based optimization of automatic image registration techniques | http://arxiv.org/pdf/1303.6927v1.pdf | author:Arun P. V., Dr. S. K. Katiyar category:cs.CV published:2013-03-27 summary:Image registration is the process of transforming different sets of data intoone coordinate system and is required for various remote sensing applicationslike change detection, image fusion, and other related areas. The effect ofincreased relief displacement, requirement of more control points, andincreased data volume are the challenges associated with the registration ofhigh resolution image data. The objective of this research work is to study themost efficient techniques and to investigate the extent of improvementachievable by enhancing them with Wavelet transform. The SIFT feature basedmethod uses the Eigen value for extracting thousands of key points based onscale invariant features and these feature points when further enhanced by thewavelet transform yields the best results.
arxiv-3300-136 | A Comparative Analysis on the Applicability of Entropy in remote sensing | http://arxiv.org/pdf/1303.6926v1.pdf | author:Dr. S. K. Katiyar, Arun P. V. category:cs.CV published:2013-03-27 summary:Entropy is the measure of uncertainty in any data and is adopted formaximisation of mutual information in many remote sensing operations. Theavailability of wide entropy variations motivated us for an investigation overthe suitability preference of these versions to specific operations.Methodologies were implemented in Matlab and were enhanced with entropyvariations. Evaluation of various implementations was based on differentstatistical parameters with reference to the study area The popular availableversions like Tsalli's, Shanon's, and Renyi's entropies were analysed incontext of various remote sensing operations namely thresholding, clusteringand registration.
arxiv-3300-137 | Developing and Analyzing Boundary Detection Operators Using Probabilistic Models | http://arxiv.org/pdf/1304.3447v1.pdf | author:David Sher category:cs.CV published:2013-03-27 summary:Most feature detectors such as edge detectors or circle finders arestatistical, in the sense that they decide at each point in an image about thepresence of a feature, this paper describes the use of Bayesian featuredetectors.
arxiv-3300-138 | Sparse approximation and recovery by greedy algorithms in Banach spaces | http://arxiv.org/pdf/1303.6811v1.pdf | author:Vladimir Temlyakov category:stat.ML math.FA 41A65 published:2013-03-27 summary:We study sparse approximation by greedy algorithms. We prove theLebesgue-type inequalities for the Weak Chebyshev Greedy Algorithm (WCGA), ageneralization of the Weak Orthogonal Matching Pursuit to the case of a Banachspace. The main novelty of these results is a Banach space setting instead of aHilbert space setting. The results are proved for redundant dictionariessatisfying certain conditions. Then we apply these general results to the caseof bases. In particular, we prove that the WCGA provides almost optimal sparseapproximation for the trigonometric system in $L_p$, $2\le p<\infty$.
arxiv-3300-139 | Sequential testing over multiple stages and performance analysis of data fusion | http://arxiv.org/pdf/1303.6750v1.pdf | author:Gaurav Thakur category:stat.ML cs.LG published:2013-03-27 summary:We describe a methodology for modeling the performance of decision-level datafusion between different sensor configurations, implemented as part of theJIEDDO Analytic Decision Engine (JADE). We first discuss a Bayesian networkformulation of classical probabilistic data fusion, which allows elementaryfusion structures to be stacked and analyzed efficiently. We then present anextension of the Wald sequential test for combining the outputs of the Bayesiannetwork over time. We discuss an algorithm to compute its performancestatistics and illustrate the approach on some examples. This variant of thesequential test involves multiple, distinct stages, where the evidenceaccumulated from each stage is carried over into the next one, and is motivatedby a need to keep certain sensors in the network inactive unless triggered byother sensors.
arxiv-3300-140 | An intelligent approach towards automatic shape modeling and object extraction from satellite images using cellular automata based algorithm | http://arxiv.org/pdf/1303.6711v1.pdf | author:P. V. Arun, S. K. Katiyar category:cs.CV published:2013-03-27 summary:Automatic feature extraction domain has witnessed the application of manyintelligent methodologies over past decade; however detection accuracy of theseapproaches were limited as object geometry and contextual knowledge were notgiven enough consideration. In this paper, we propose a frame work for accuratedetection of features along with automatic interpolation, and interpretation bymodeling feature shape as well as contextual knowledge using advancedtechniques such as SVRF, Cellular Neural Network, Core set, and MACA. Developedmethodology has been compared with contemporary methods using differentstatistical measures. Investigations over various satellite images revealedthat considerable success was achieved with the CNN approach. CNN has beeneffective in modeling different complex features effectively and complexity ofthe approach has been considerably reduced using corset optimization. Thesystem has dynamically used spectral and spatial information for representingcontextual knowledge using CNN-prolog approach. System has been also proved tobe effective in providing intelligent interpolation and interpretation ofrandom features.
arxiv-3300-141 | ABC Reinforcement Learning | http://arxiv.org/pdf/1303.6977v4.pdf | author:Christos Dimitrakakis, Nikolaos Tziortziotis category:stat.ML cs.LG published:2013-03-27 summary:This paper introduces a simple, general framework for likelihood-freeBayesian reinforcement learning, through Approximate Bayesian Computation(ABC). The main advantage is that we only require a prior distribution on aclass of simulators (generative models). This is useful in domains where ananalytical probabilistic model of the underlying process is too complex toformulate, but where detailed simulation models are available. ABC-RL allowsthe use of any Bayesian reinforcement learning technique, even in this case. Inaddition, it can be seen as an extension of rollout algorithms to the casewhere we do not know what the correct model to draw rollouts from is. Weexperimentally demonstrate the potential of this approach in a comparison withLSPI. Finally, we introduce a theorem showing that ABC is a sound methodologyin principle, even when non-sufficient statistics are used.
arxiv-3300-142 | Performance Evaluation of Edge-Directed Interpolation Methods for Images | http://arxiv.org/pdf/1303.6455v1.pdf | author:Shaode Yu, Qingsong Zhu, Shibin Wu, Yaoqin Xie category:cs.CV I.4.1 published:2013-03-26 summary:Many interpolation methods have been developed for high visual quality, butfail for inability to preserve image structures. Edges carry heavy structuralinformation for detection, determination and classification. Edge-adaptiveinterpolation approaches become a center of focus. In this paper, performanceof four edge-directed interpolation methods comparing with two traditionalmethods is evaluated on two groups of images. These methods include newedge-directed interpolation (NEDI), edge-guided image interpolation (EGII),iterative curvature-based interpolation (ICBI), directional cubic convolutioninterpolation (DCCI) and two traditional approaches, bi-linear and bi-cubic.Meanwhile, no parameters are mentioned to measure edge-preserving ability ofedge-adaptive interpolation approaches and we proposed two. One evaluatesaccuracy and the other measures robustness of edge-preservation ability.Performance evaluation is based on six parameters. Objective assessment andvisual analysis are illustrated and conclusions are drawn from theoreticalbackgrounds and practical results.
arxiv-3300-143 | Simulation of Fractional Brownian Surfaces via Spectral Synthesis on Manifolds | http://arxiv.org/pdf/1303.6377v1.pdf | author:Zachary Gelbaum, Mathew Titus category:cs.CG cs.CV math.PR published:2013-03-26 summary:Using the spectral decomposition of the Laplace-Beltrami operator we simulatefractal surfaces as random series of eigenfunctions. This approach allows us togenerate random fields over smooth manifolds of arbitrary dimension,generalizing previous work with fractional Brownian motion withmulti-dimensional parameter. We give examples of surfaces with and withoutboundary and discuss implementation.
arxiv-3300-144 | An N-dimensional approach towards object based classification of remotely sensed imagery | http://arxiv.org/pdf/1303.6619v1.pdf | author:Arun p V, S. K. Katiyar category:cs.CV published:2013-03-26 summary:Remote sensing techniques are widely used for land cover classification andurban analysis. The availability of high resolution remote sensing imagerylimits the level of classification accuracy attainable from pixel-basedapproach. In this paper object-based classification scheme based on ahierarchical support vector machine is introduced. By combining spatial andspectral information, the amount of overlap between classes can be decreased;thereby yielding higher classification accuracy and more accurate land covermaps. We have adopted certain automatic approaches based on the advancedtechniques as Cellular automata and Genetic Algorithm for kernel and tuningparameter selection. Performance evaluation of the proposed methodology incomparison with the existing approaches is performed with reference to theBhopal city study area.
arxiv-3300-145 | Convex Tensor Decomposition via Structured Schatten Norm Regularization | http://arxiv.org/pdf/1303.6370v1.pdf | author:Ryota Tomioka, Taiji Suzuki category:stat.ML cs.LG cs.NA published:2013-03-26 summary:We discuss structured Schatten norms for tensor decomposition that includestwo recently proposed norms ("overlapped" and "latent") forconvex-optimization-based tensor decomposition, and connect tensordecomposition with wider literature on structured sparsity. Based on theproperties of the structured Schatten norms, we mathematically analyze theperformance of "latent" approach for tensor decomposition, which wasempirically found to perform better than the "overlapped" approach in somesettings. We show theoretically that this is indeed the case. In particular,when the unknown true tensor is low-rank in a specific mode, this approachperforms as good as knowing the mode with the smallest rank. Along the way, weshow a novel duality result for structures Schatten norms, establish theconsistency, and discuss the identifiability of this approach. We confirmthrough numerical simulations that our theoretical prediction can preciselypredict the scaling behavior of the mean squared error.
arxiv-3300-146 | Video Face Matching using Subset Selection and Clustering of Probabilistic Multi-Region Histograms | http://arxiv.org/pdf/1303.6361v1.pdf | author:Sandra Mau, Shaokang Chen, Conrad Sanderson, Brian C. Lovell category:cs.CV cs.IR published:2013-03-26 summary:Balancing computational efficiency with recognition accuracy is one of themajor challenges in real-world video-based face recognition. A significantdesign decision for any such system is whether to process and use all possiblefaces detected over the video frames, or whether to select only a few "best"faces. This paper presents a video face recognition system based onprobabilistic Multi-Region Histograms to characterise performance trade-offsin: (i) selecting a subset of faces compared to using all faces, and (ii)combining information from all faces via clustering. Three face selectionmetrics are evaluated for choosing a subset: face detection confidence, randomsubset, and sequential selection. Experiments on the recently introduced MOBIOdataset indicate that the usage of all faces through clustering alwaysoutperformed selecting only a subset of faces. The experiments also show thatthe face selection metric based on face detection confidence generally providesbetter recognition performance than random or sequential sampling. Moreover,the optimal number of faces varies drastically across selection metric andsubsets of MOBIO. Given the trade-offs between computational effort,recognition accuracy and robustness, it is recommended that face featureclustering would be most advantageous in batch processing (particularly forvideo-based watchlists), whereas face selection methods should be limited toapplications with significant computational restrictions.
arxiv-3300-147 | A Note on k-support Norm Regularized Risk Minimization | http://arxiv.org/pdf/1303.6390v2.pdf | author:Matthew Blaschko category:cs.LG published:2013-03-26 summary:The k-support norm has been recently introduced to perform correlatedsparsity regularization. Although Argyriou et al. only reported experimentsusing squared loss, here we apply it to several other commonly used settingsresulting in novel machine learning algorithms with interesting and familiarlimit cases. Source code for the algorithms described here is available.
arxiv-3300-148 | Random Intersection Trees | http://arxiv.org/pdf/1303.6223v1.pdf | author:Rajen Dinesh Shah, Nicolai Meinshausen category:stat.ML stat.CO stat.ME published:2013-03-25 summary:Finding interactions between variables in large and high-dimensional datasetsis often a serious computational challenge. Most approaches build upinteraction sets incrementally, adding variables in a greedy fashion. Thedrawback is that potentially informative high-order interactions may beoverlooked. Here, we propose at an alternative approach for classificationproblems with binary predictor variables, called Random Intersection Trees. Itworks by starting with a maximal interaction that includes all variables, andthen gradually removing variables if they fail to appear in randomly chosenobservations of a class of interest. We show that informative interactions areretained with high probability, and the computational complexity of ourprocedure is of order $p^\kappa$ for a value of $\kappa$ that can reach valuesas low as 1 for very sparse data; in many more general settings, it will stillbeat the exponent $s$ obtained when using a brute force search constrained toorder $s$ interactions. In addition, by using some new ideas based on min-wisehash schemes, we are able to further reduce the computational cost.Interactions found by our algorithm can be used for predictive modelling invarious forms, but they are also often of interest in their own right as usefulcharacterisations of what distinguishes a certain class from others.
arxiv-3300-149 | Particles Prefer Walking Along the Axes: Experimental Insights into the Behavior of a Particle Swarm | http://arxiv.org/pdf/1303.6145v2.pdf | author:Manuel Schmitt, Rolf Wanka category:cs.NE cs.AI I.2.8 published:2013-03-25 summary:Particle swarm optimization (PSO) is a widely used nature-inspiredmeta-heuristic for solving continuous optimization problems. However, whenrunning the PSO algorithm, one encounters the phenomenon of so-calledstagnation, that means in our context, the whole swarm starts to converge to asolution that is not (even a local) optimum. The goal of this work is to pointout possible reasons why the swarm stagnates at these non-optimal points. Toachieve our results, we use the newly defined potential of a swarm. The totalpotential has a portion for every dimension of the search space, and it dropswhen the swarm approaches the point of convergence. As it turns outexperimentally, the swarm is very likely to come sometimes into "unbalanced"states, i. e., almost all potential belongs to one axis. Therefore, the swarmbecomes blind for improvements still possible in any other direction. Finally,we show how in the light of the potential and these observations, a slightlyadapted PSO rebalances the potential and therefore increases the quality of thesolution.
arxiv-3300-150 | Compression as a universal principle of animal behavior | http://arxiv.org/pdf/1303.6175v1.pdf | author:R. Ferrer-i-Cancho, A. Hernández-Fernández, D. Lusseau, G. Agoramoorthy, M. J. Hsu, S. Semple category:q-bio.NC cs.CL cs.IT math.IT q-bio.QM published:2013-03-25 summary:A key aim in biology and psychology is to identify fundamental principlesunderpinning the behavior of animals, including humans. Analyses of humanlanguage and the behavior of a range of non-human animal species have providedevidence for a common pattern underlying diverse behavioral phenomena: wordsfollow Zipf's law of brevity (the tendency of more frequently used words to beshorter), and conformity to this general pattern has been seen in the behaviorof a number of other animals. It has been argued that the presence of this lawis a sign of efficient coding in the information theoretic sense. However, nostrong direct connection has been demonstrated between the law and compression,the information theoretic principle of minimizing the expected length of acode. Here we show that minimizing the expected code length implies that thelength of a word cannot increase as its frequency increases. Furthermore, weshow that the mean code length or duration is significantly small in humanlanguage, and also in the behavior of other species in all cases whereagreement with the law of brevity has been found. We argue that compression isa general principle of animal behavior, that reflects selection for efficiencyof coding.
arxiv-3300-151 | On Sparsity Inducing Regularization Methods for Machine Learning | http://arxiv.org/pdf/1303.6086v1.pdf | author:Andreas Argyriou, Luca Baldassarre, Charles A. Micchelli, Massimiliano Pontil category:cs.LG stat.ML published:2013-03-25 summary:During the past years there has been an explosion of interest in learningmethods based on sparsity regularization. In this paper, we discuss a generalclass of such methods, in which the regularizer can be expressed as thecomposition of a convex function $\omega$ with a linear function. This settingincludes several methods such the group Lasso, the Fused Lasso, multi-tasklearning and many more. We present a general approach for solvingregularization problems of this kind, under the assumption that the proximityoperator of the function $\omega$ is available. Furthermore, we comment on theapplication of this approach to support vector machines, a technique pioneeredby the groundbreaking work of Vladimir Vapnik.
arxiv-3300-152 | Spatio-Temporal Covariance Descriptors for Action and Gesture Recognition | http://arxiv.org/pdf/1303.6021v1.pdf | author:Andres Sanin, Conrad Sanderson, Mehrtash T. Harandi, Brian C. Lovell category:cs.CV cs.HC published:2013-03-25 summary:We propose a new action and gesture recognition method based onspatio-temporal covariance descriptors and a weighted Riemannian localitypreserving projection approach that takes into account the curved space formedby the descriptors. The weighted projection is then exploited during boostingto create a final multiclass classification algorithm that employs the mostuseful spatio-temporal regions. We also show how the descriptors can becomputed quickly through the use of integral video representations. Experimentson the UCF sport, CK+ facial expression and Cambridge hand gesture datasetsindicate superior performance of the proposed method compared to several recentstate-of-the-art techniques. The proposed method is robust and does not requireadditional processing of the videos, such as foreground detection,interest-point detection or tracking.
arxiv-3300-153 | Asymmetric Pruning for Learning Cascade Detectors | http://arxiv.org/pdf/1303.6066v2.pdf | author:Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel category:cs.CV published:2013-03-25 summary:Cascade classifiers are one of the most important contributions to real-timeobject detection. Nonetheless, there are many challenging problems arising intraining cascade detectors. One common issue is that the node classifier istrained with a symmetric classifier. Having a low misclassification error ratedoes not guarantee an optimal node learning goal in cascade classifiers, i.e.,an extremely high detection rate with a moderate false positive rate. In thiswork, we present a new approach to train an effective node classifier in acascade detector. The algorithm is based on two key observations: 1) Redundantweak classifiers can be safely discarded; 2) The final detector should satisfythe asymmetric learning objective of the cascade architecture. To achieve this,we separate the classifier training into two steps: finding a pool ofdiscriminative weak classifiers/features and training the final classifier bypruning weak classifiers which contribute little to the asymmetric learningcriterion (asymmetric classifier construction). Our model reduction approachhelps accelerate the learning time while achieving the pre-determined learningobjective. Experimental results on both face and car data sets verify theeffectiveness of the proposed algorithm. On the FDDB face data sets, ourapproach achieves the state-of-the-art performance, which demonstrates theadvantage of our approach.
arxiv-3300-154 | A hybrid bat algorithm | http://arxiv.org/pdf/1303.6310v3.pdf | author:Iztok Fister Jr., Dušan Fister, Xin-She Yang category:cs.NE published:2013-03-25 summary:Swarm intelligence is a very powerful technique to be used for optimizationpurposes. In this paper we present a new swarm intelligence algorithm, based onthe bat algorithm. The Bat algorithm is hybridized with differential evolutionstrategies. Besides showing very promising results of the standard benchmarkfunctions, this hybridization also significantly improves the original batalgorithm.
arxiv-3300-155 | Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression | http://arxiv.org/pdf/1303.6149v3.pdf | author:Francis Bach category:math.ST cs.LG math.OC stat.TH published:2013-03-25 summary:In this paper, we consider supervised learning problems such as logisticregression and study the stochastic gradient method with averaging, in theusual stochastic approximation setting where observations are used only once.We show that after $N$ iterations, with a constant step-size proportional to$1/R^2 \sqrt{N}$ where $N$ is the number of observations and $R$ is the maximumnorm of the observations, the convergence rate is always of order$O(1/\sqrt{N})$, and improves to $O(R^2 / \mu N)$ where $\mu$ is the lowesteigenvalue of the Hessian at the global optimum (when this eigenvalue isgreater than $R^2/\sqrt{N}$). Since $\mu$ does not need to be known in advance,this shows that averaged stochastic gradient is adaptive to \emph{unknownlocal} strong convexity of the objective function. Our proof relies on thegeneralized self-concordance properties of the logistic loss and thus extendsto all generalized linear models with uniformly bounded features.
arxiv-3300-156 | Machine learning of hierarchical clustering to segment 2D and 3D images | http://arxiv.org/pdf/1303.6163v3.pdf | author:Juan Nunez-Iglesias, Ryan Kennedy, Toufiq Parag, Jianbo Shi, Dmitri B. Chklovskii category:cs.CV cs.LG published:2013-03-25 summary:We aim to improve segmentation through the use of machine learning toolsduring region agglomeration. We propose an active learning approach forperforming hierarchical agglomerative segmentation from superpixels. Our methodcombines multiple features at all scales of the agglomerative process, worksfor data with an arbitrary number of dimensions, and scales to very largedatasets. We advocate the use of variation of information to measuresegmentation accuracy, particularly in 3D electron microscopy (EM) images ofneural tissue, and using this metric demonstrate an improvement over competingalgorithms in EM and natural images.
arxiv-3300-157 | A Diffusion Process on Riemannian Manifold for Visual Tracking | http://arxiv.org/pdf/1303.5913v1.pdf | author:Marcus Chen, Cham Tat Jen, Pang Sze Kim, Alvina Goh category:cs.CV cs.LG cs.RO stat.ML published:2013-03-24 summary:Robust visual tracking for long video sequences is a research area that hasmany important applications. The main challenges include how the target imagecan be modeled and how this model can be updated. In this paper, we model thetarget using a covariance descriptor, as this descriptor is robust to problemssuch as pixel-pixel misalignment, pose and illumination changes, that commonlyoccur in visual tracking. We model the changes in the template using agenerative process. We introduce a new dynamical model for the template updateusing a random walk on the Riemannian manifold where the covariance descriptorslie in. This is done using log-transformed space of the manifold to free theconstraints imposed inherently by positive semidefinite matrices. Modelingtemplate variations and poses kinetics together in the state space enables usto jointly quantify the uncertainties relating to the kinematic states and thetemplate in a principled way. Finally, the sequential inference of theposterior distribution of the kinematic states and the template is done using aparticle filter. Our results shows that this principled approach can be robustto changes in illumination, poses and spatial affine transformation. In theexperiments, our method outperformed the current state-of-the-art algorithm -the incremental Principal Component Analysis method, particularly when a targetunderwent fast poses changes and also maintained a comparable performance instable target tracking cases.
arxiv-3300-158 | On Learnability, Complexity and Stability | http://arxiv.org/pdf/1303.5976v1.pdf | author:Silvia Villa, Lorenzo Rosasco, Tomaso Poggio category:stat.ML cs.LG published:2013-03-24 summary:We consider the fundamental question of learnability of a hypotheses class inthe supervised learning setting and in the general learning setting introducedby Vladimir Vapnik. We survey classic results characterizing learnability interm of suitable notions of complexity, as well as more recent results thatestablish the connection between learnability and stability of a learningalgorithm.
arxiv-3300-159 | Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems | http://arxiv.org/pdf/1303.5984v1.pdf | author:Morteza Ibrahimi, Adel Javanmard, Benjamin Van Roy category:stat.ML cs.LG math.OC published:2013-03-24 summary:We study the problem of adaptive control of a high dimensional linearquadratic (LQ) system. Previous work established the asymptotic convergence toan optimal controller for various adaptive control schemes. More recently, forthe average cost LQ problem, a regret bound of ${O}(\sqrt{T})$ was shown, apartform logarithmic factors. However, this bound scales exponentially with $p$,the dimension of the state space. In this work we consider the case where thematrices describing the dynamic of the LQ system are sparse and theirdimensions are large. We present an adaptive control scheme that achieves aregret bound of ${O}(p \sqrt{T})$, apart from logarithmic factors. Inparticular, our algorithm has an average cost of $(1+\eps)$ times the optimumcost after $T = \polylog(p) O(1/\eps^2)$. This is in comparison to previouswork on the dense dynamics where the algorithm requires time that scalesexponentially with dimension in order to achieve regret of $\eps$ times theoptimal cost. We believe that our result has prominent applications in the emerging area ofcomputational advertising, in particular targeted online advertising andadvertising in social networks.
arxiv-3300-160 | Generalizing k-means for an arbitrary distance matrix | http://arxiv.org/pdf/1303.6001v1.pdf | author:Balázs Szalkai category:cs.LG cs.CV stat.ML published:2013-03-24 summary:The original k-means clustering method works only if the exact vectorsrepresenting the data points are known. Therefore calculating the distancesfrom the centroids needs vector operations, since the average of abstract datapoints is undefined. Existing algorithms can be extended for those cases whenthe sole input is the distance matrix, and the exact representing vectors areunknown. This extension may be named relational k-means after a notation for asimilar algorithm invented for fuzzy clustering. A method is then proposed forgeneralizing k-means for scenarios when the data points have absolutely noconnection with a Euclidean space.
arxiv-3300-161 | SYNTAGMA. A Linguistic Approach to Parsing | http://arxiv.org/pdf/1303.5960v3.pdf | author:Daniel Christen category:cs.CL published:2013-03-24 summary:SYNTAGMA is a rule-based parsing system, structured on two levels: a generalparsing engine and a language specific grammar. The parsing engine is alanguage independent program, while grammar and language specific rules andresources are given as text files, consisting in a list of constituentstructuresand a lexical database with word sense related features andconstraints. Since its theoretical background is principally Tesniere'sElements de syntaxe, SYNTAGMA's grammar emphasizes the role of argumentstructure (valency) in constraint satisfaction, and allows also horizontalbounds, for instance treating coordination. Notions such as Pro, traces, emptycategories are derived from Generative Grammar and some solutions are close toGovernment&Binding Theory, although they are the result of an autonomousresearch. These properties allow SYNTAGMA to manage complex syntacticconfigurations and well known weak points in parsing engineering. An importantresource is the semantic network, which is used in disambiguation tasks.Parsing process follows a bottom-up, rule driven strategy. Its behavior can becontrolled and fine-tuned.
arxiv-3300-162 | Sparse Projections of Medical Images onto Manifolds | http://arxiv.org/pdf/1303.5508v2.pdf | author:George H. Chen, Christian Wachinger, Polina Golland category:cs.CV cs.LG stat.ML published:2013-03-22 summary:Manifold learning has been successfully applied to a variety of medicalimaging problems. Its use in real-time applications requires fast projectiononto the low-dimensional space. To this end, out-of-sample extensions areapplied by constructing an interpolation function that maps from the inputspace to the low-dimensional manifold. Commonly used approaches such as theNystr\"{o}m extension and kernel ridge regression require using all trainingpoints. We propose an interpolation function that only depends on a smallsubset of the input training data. Consequently, in the testing phase each newpoint only needs to be compared against a small number of input training datain order to project the point onto the low-dimensional space. We interpret ourmethod as an out-of-sample extension that approximates kernel ridge regression.Our method involves solving a simple convex optimization problem and has theattractive property of guaranteeing an upper bound on the approximation error,which is crucial for medical applications. Tuning this error bound controls thesparsity of the resulting interpolation function. We illustrate our method intwo clinical applications that require fast mapping of input images onto alow-dimensional space.
arxiv-3300-163 | Cortical Surface Co-Registration based on MRI Images and Photos | http://arxiv.org/pdf/1303.5691v1.pdf | author:Benjamin Berkels, Ivan Cabrilo, Sven Haller, Martin Rumpf, Carlo Schaller category:cs.CV published:2013-03-22 summary:Brain shift, i.e. the change in configuration of the brain after opening thedura mater, is a key problem in neuronavigation. We present an approach toco-register intra-operative microscope images with pre-operative MRI to adaptand optimize intra-operative neuronavigation. The tools are a robustclassification of sulci on MRI extracted cortical surfaces, guided user markingof most prominent sulci on a microscope image, and the actual variationalregistration method with a fidelity energy for 3D deformations of the corticalsurface combined with a higher order, linear elastica type prior energy.Furthermore, the actual registration is validated on an artificial testbed withknown ground truth deformation and on real data of a neuro clinical patient.
arxiv-3300-164 | Network Detection Theory and Performance | http://arxiv.org/pdf/1303.5613v1.pdf | author:Steven T. Smith, Kenneth D. Senne, Scott Philips, Edward K. Kao, Garrett Bernstein category:cs.SI cs.LG math.ST physics.soc-ph stat.ML stat.TH published:2013-03-22 summary:Network detection is an important capability in many areas of appliedresearch in which data can be represented as a graph of entities andrelationships. Oftentimes the object of interest is a relatively small subgraphin an enormous, potentially uninteresting background. This aspect characterizesnetwork detection as a "big data" problem. Graph partitioning and networkdiscovery have been major research areas over the last ten years, driven byinterest in internet search, cyber security, social networks, and criminal orterrorist activities. The specific problem of network discovery is addressed asa special case of graph partitioning in which membership in a small subgraph ofinterest must be determined. Algebraic graph theory is used as the basis toanalyze and compare different network detection methods. A new Bayesian networkdetection framework is introduced that partitions the graph based on priorinformation and direct observations. The new approach, called space-time threatpropagation, is proved to maximize the probability of detection and istherefore optimum in the Neyman-Pearson sense. This optimality criterion iscompared to spectral community detection approaches which divide the globalgraph into subsets or communities with optimal connectivity properties. We alsoexplore a new generative stochastic model for covert networks and analyze usingreceiver operating characteristics the detection performance of both classes ofoptimal detection techniques.
arxiv-3300-165 | Sparse Factor Analysis for Learning and Content Analytics | http://arxiv.org/pdf/1303.5685v2.pdf | author:Andrew S. Lan, Andrew E. Waters, Christoph Studer, Richard G. Baraniuk category:stat.ML cs.LG math.OC stat.AP published:2013-03-22 summary:We develop a new model and algorithms for machine learning-based learninganalytics, which estimate a learner's knowledge of the concepts underlying adomain, and content analytics, which estimate the relationships among acollection of questions and those concepts. Our model represents theprobability that a learner provides the correct response to a question in termsof three factors: their understanding of a set of underlying concepts, theconcepts involved in each question, and each question's intrinsic difficulty.We estimate these factors given the graded responses to a collection ofquestions. The underlying estimation problem is ill-posed in general,especially when only a subset of the questions are answered. The keyobservation that enables a well-posed solution is the fact that typicaleducational domains of interest involve only a small number of key concepts.Leveraging this observation, we develop both a bi-convex maximum-likelihood anda Bayesian solution to the resulting SPARse Factor Analysis (SPARFA) problem.We also incorporate user-defined tags on questions to facilitate theinterpretability of the estimated factors. Experiments with synthetic andreal-world data demonstrate the efficacy of our approach. Finally, we make aconnection between SPARFA and noisy, binary-valued (1-bit) dictionary learningthat is of independent interest.
arxiv-3300-166 | Robust and Trend Following Student's t Kalman Smoothers | http://arxiv.org/pdf/1303.5588v1.pdf | author:Aleksandr Y. Aravkin, James V. Burke, Gianluigi Pillonetto category:math.OC math.NA stat.AP stat.ML 62F35, 65K10 published:2013-03-22 summary:We present a Kalman smoothing framework based on modeling errors using theheavy tailed Student's t distribution, along with algorithms, convergencetheory, open-source general implementation, and several important applications.The computational effort per iteration grows linearly with the length of thetime series, and all smoothers allow nonlinear process and measurement models. Robust smoothers form an important subclass of smoothers within thisframework. These smoothers work in situations where measurements are highlycontaminated by noise or include data unexplained by the forward model. Highlyrobust smoothers are developed by modeling measurement errors using theStudent's t distribution, and outperform the recently proposed L1-Laplacesmoother in extreme situations with data containing 20% or more outliers. A second special application we consider in detail allows tracking suddenchanges in the state. It is developed by modeling process noise using theStudent's t distribution, and the resulting smoother can track sudden changesin the state. These features can be used separately or in tandem, and we present a generalsmoother algorithm and open source implementation, together with convergenceanalysis that covers a wide range of smoothers. A key ingredient of ourapproach is a technique to deal with the non-convexity of the Student's t lossfunction. Numerical results for linear and nonlinear models illustrate theperformance of the new smoothers for robust and tracking applications, as wellas for mixed problems that have both types of features.
arxiv-3300-167 | Adverse Conditions and ASR Techniques for Robust Speech User Interface | http://arxiv.org/pdf/1303.5515v1.pdf | author:Urmila Shrawankar, VM Thakare category:cs.CL cs.SD published:2013-03-22 summary:The main motivation for Automatic Speech Recognition (ASR) is efficientinterfaces to computers, and for the interfaces to be natural and truly useful,it should provide coverage for a large group of users. The purpose of thesetasks is to further improve man-machine communication. ASR systems exhibitunacceptable degradations in performance when the acoustical environments usedfor training and testing the system are not the same. The goal of this researchis to increase the robustness of the speech recognition systems with respect tochanges in the environment. A system can be labeled as environment-independentif the recognition accuracy for a new environment is the same or higher thanthat obtained when the system is retrained for that environment. Attaining suchperformance is the dream of the researchers. This paper elaborates some of thedifficulties with Automatic Speech Recognition (ASR). These difficulties areclassified into Speakers characteristics and environmental conditions, andtried to suggest some techniques to compensate variations in speech signal.This paper focuses on the robustness with respect to speakers variations andchanges in the acoustical environment. We discussed several different externalfactors that change the environment and physiological differences that affectthe performance of a speech recognition system followed by techniques that arehelpful to design a robust ASR system.
arxiv-3300-168 | Speech Recognition with Deep Recurrent Neural Networks | http://arxiv.org/pdf/1303.5778v1.pdf | author:Alex Graves, Abdel-rahman Mohamed, Geoffrey Hinton category:cs.NE cs.CL published:2013-03-22 summary:Recurrent neural networks (RNNs) are a powerful model for sequential data.End-to-end training methods such as Connectionist Temporal Classification makeit possible to train RNNs for sequence labelling problems where theinput-output alignment is unknown. The combination of these methods with theLong Short-term Memory RNN architecture has proved particularly fruitful,delivering state-of-the-art results in cursive handwriting recognition. HoweverRNN performance in speech recognition has so far been disappointing, withbetter results returned by deep feedforward networks. This paper investigates\emph{deep recurrent neural networks}, which combine the multiple levels ofrepresentation that have proved so effective in deep networks with the flexibleuse of long range context that empowers RNNs. When trained end-to-end withsuitable regularisation, we find that deep Long Short-term Memory RNNs achievea test set error of 17.7% on the TIMIT phoneme recognition benchmark, which toour knowledge is the best recorded score.
arxiv-3300-169 | Parameters Optimization for Improving ASR Performance in Adverse Real World Noisy Environmental Conditions | http://arxiv.org/pdf/1303.5513v1.pdf | author:Urmila Shrawankar, Vilas Thakare category:cs.CL cs.SD published:2013-03-22 summary:From the existing research it has been observed that many techniques andmethodologies are available for performing every step of Automatic SpeechRecognition (ASR) system, but the performance (Minimization of Word ErrorRecognition-WER and Maximization of Word Accuracy Rate- WAR) of the methodologyis not dependent on the only technique applied in that method. The researchwork indicates that, performance mainly depends on the category of the noise,the level of the noise and the variable size of the window, frame, frameoverlap etc is considered in the existing methods. The main aim of the workpresented in this paper is to use variable size of parameters like window size,frame size and frame overlap percentage to observe the performance ofalgorithms for various categories of noise with different levels and also trainthe system for all size of parameters and category of real world noisyenvironment to improve the performance of the speech recognition system. Thispaper presents the results of Signal-to-Noise Ratio (SNR) and Accuracy test byapplying variable size of parameters. It is observed that, it is really veryhard to evaluate test results and decide parameter size for ASR performanceimprovement for its resultant optimization. Hence, this study further suggeststhe feasible and optimum parameter size using Fuzzy Inference System (FIS) forenhancing resultant accuracy in adverse real world noisy environmentalconditions. This work will be helpful to give discriminative training ofubiquitous ASR system for better Human Computer Interaction (HCI).
arxiv-3300-170 | Sample Distortion for Compressed Imaging | http://arxiv.org/pdf/1303.5492v2.pdf | author:Chunli Guo, Mike E. Davies category:cs.CV cs.IT math.IT published:2013-03-22 summary:We propose the notion of a sample distortion (SD) function for independentand identically distributed (i.i.d) compressive distributions to fundamentallyquantify the achievable reconstruction performance of compressed sensing forcertain encoder-decoder pairs at a given sampling ratio. Two lower bounds onthe achievable performance and the intrinsic convexity property is derived. Azeroing procedure is then introduced to improve non convex SD functions. The SDframework is then applied to analyse compressed imaging with a multi-resolutionstatistical image model using both the generalized Gaussian distribution andthe two-state Gaussian mixture distribution. We subsequently focus on theGaussian encoder-Bayesian optimal approximate message passing (AMP) decoderpair, whose theoretical SD function is provided by the rigorous analysis of theAMP algorithm. Given the image statistics, analytic bandwise sample allocationfor bandwise independent model is derived as a reverse water-filling scheme.Som and Schniter's turbo message passing approach is further deployed tointegrate the bandwise sampling with the exploitation of the hidden Markov treestructure of wavelet coefficients. Natural image simulations confirm that withoracle image statistics, the SD function associated with the optimized sampleallocation can accurately predict the possible compressed sensing gains.Finally, a general sample allocation profile based on average image statisticsnot only illustrates preferable performance but also makes the schemepractical.
arxiv-3300-171 | Separable Dictionary Learning | http://arxiv.org/pdf/1303.5244v1.pdf | author:Simon Hawe, Matthias Seibert, Martin Kleinsteuber category:cs.CV cs.LG stat.ML published:2013-03-21 summary:Many techniques in computer vision, machine learning, and statistics rely onthe fact that a signal of interest admits a sparse representation over somedictionary. Dictionaries are either available analytically, or can be learnedfrom a suitable training set. While analytic dictionaries permit to capture theglobal structure of a signal and allow a fast implementation, learneddictionaries often perform better in applications as they are more adapted tothe considered class of signals. In imagery, unfortunately, the numericalburden for (i) learning a dictionary and for (ii) employing the dictionary forreconstruction tasks only allows to deal with relatively small image patchesthat only capture local image information. The approach presented in this paperaims at overcoming these drawbacks by allowing a separable structure on thedictionary throughout the learning process. On the one hand, this permitslarger patch-sizes for the learning phase, on the other hand, the dictionary isapplied efficiently in reconstruction tasks. The learning procedure is based onoptimizing over a product of spheres which updates the dictionary as a whole,thus enforces basic dictionary properties such as mutual coherence explicitlyduring the learning procedure. In the special case where no separable structureis enforced, our method competes with state-of-the-art dictionary learningmethods like K-SVD.
arxiv-3300-172 | Methods Of Measurement The Three-Dimensional Wind Waves Spectra, Based On The Processing Of Video Images Of The Sea Surface | http://arxiv.org/pdf/1303.5248v2.pdf | author:Boris M. Salin, Mikhail B. Salin category:physics.ao-ph cs.CV published:2013-03-21 summary:Optical instruments for measuring surface-wave characteristics provide abetter spatial and temporal resolution than other methods, but they facedifficulties while converting the results of indirect measurements intoabsolute levels of the waves. We have solved this problem to some extent. Inthis paper, we propose an optical method for measuring the 3D power spectraldensity of the surface waves and spatio-temporal samples of the wave profiles.The method involves, first, synchronous recording of the brightness field overa patch of a rough surface and measurement of surface oscillations at one ormore points and, second, filtering of the spatial image spectrum. Filterparameters are chosen to maximize the correlation of the surface oscillationsrecovered and measured at one or two points. In addition to the measurementprocedure, the paper provides experimental results of measuringmultidimensional spectra of roughness, which generally agree with theoreticalexpectations and the results of other authors.
arxiv-3300-173 | Multi-dimensional sparse structured signal approximation using split Bregman iterations | http://arxiv.org/pdf/1303.5197v3.pdf | author:Yoann Isaac, Quentin Barthélemy, Jamal Atif, Cédric Gouy-Pailler, Michèle Sebag category:cs.DS stat.ML published:2013-03-21 summary:The paper focuses on the sparse approximation of signals using overcompleterepresentations, such that it preserves the (prior) structure ofmulti-dimensional signals. The underlying optimization problem is tackled usinga multi-dimensional split Bregman optimization approach. An extensive empiricalevaluation shows how the proposed approach compares to the state of the artdepending on the signal features.
arxiv-3300-174 | Estimating Confusions in the ASR Channel for Improved Topic-based Language Model Adaptation | http://arxiv.org/pdf/1303.5148v1.pdf | author:Damianos Karakos, Mark Dredze, Sanjeev Khudanpur category:cs.CL cs.LG published:2013-03-21 summary:Human language is a combination of elemental languages/domains/styles thatchange across and sometimes within discourses. Language models, which play acrucial role in speech recognizers and machine translation systems, areparticularly sensitive to such changes, unless some form of adaptation takesplace. One approach to speech language model adaptation is self-training, inwhich a language model's parameters are tuned based on automaticallytranscribed audio. However, transcription errors can misguide self-training,particularly in challenging settings such as conversational speech. In thiswork, we propose a model that considers the confusions (errors) of the ASRchannel. By modeling the likely confusions in the ASR output instead of usingjust the 1-best, we improve self-training efficacy by obtaining a more reliablereference transcription estimate. We demonstrate improved topic-based languagemodeling adaptation results over both 1-best and lattice self-training usingour ASR channel confusion estimates on telephone conversations.
arxiv-3300-175 | Node-Based Learning of Multiple Gaussian Graphical Models | http://arxiv.org/pdf/1303.5145v4.pdf | author:Karthik Mohan, Palma London, Maryam Fazel, Daniela Witten, Su-In Lee category:stat.ML cs.LG math.OC published:2013-03-21 summary:We consider the problem of estimating high-dimensional Gaussian graphicalmodels corresponding to a single set of variables under several distinctconditions. This problem is motivated by the task of recovering transcriptionalregulatory networks on the basis of gene expression data {containingheterogeneous samples, such as different disease states, multiple species, ordifferent developmental stages}. We assume that most aspects of the conditionaldependence networks are shared, but that there are some structured differencesbetween them. Rather than assuming that similarities and differences betweennetworks are driven by individual edges, we take a node-based approach, whichin many cases provides a more intuitive interpretation of the networkdifferences. We consider estimation under two distinct assumptions: (1)differences between the K networks are due to individual nodes that areperturbed across conditions, or (2) similarities among the K networks are dueto the presence of common hub nodes that are shared across all K networks.Using a row-column overlap norm penalty function, we formulate two convexoptimization problems that correspond to these two assumptions. We solve theseproblems using an alternating direction method of multipliers algorithm, and wederive a set of necessary and sufficient conditions that allows us to decomposethe problem into independent subproblems so that our algorithm can be scaled tohigh-dimensional settings. Our proposal is illustrated on synthetic data, awebpage data set, and a brain cancer gene expression data set.
arxiv-3300-176 | Analytic solution of a model of language competition with bilingualism and interlinguistic similarity | http://arxiv.org/pdf/1303.4959v1.pdf | author:Victoria Otero-Espinar, Luís F. Seoane, Juan J. Nieto, Jorge Mira category:physics.soc-ph cs.CL published:2013-03-20 summary:An in-depth analytic study of a model of language dynamics is presented: amodel which tackles the problem of the coexistence of two languages within aclosed community of speakers taking into account bilingualism and incorporatinga parameter to measure the distance between languages. After previous numericalsimulations, the model yielded that coexistence might lead to survival of bothlanguages within monolingual speakers along with a bilingual community or toextinction of the weakest tongue depending on different parameters. In thispaper, such study is closed with thorough analytical calculations to settle theresults in a robust way and previous results are refined with somemodifications. From the present analysis it is possible to almost completelyassay the number and nature of the equilibrium points of the model, whichdepend on its parameters, as well as to build a phase space based on them.Also, we obtain conclusions on the way the languages evolve with time. Ourrigorous considerations also suggest ways to further improve the model andfacilitate the comparison of its consequences with those from other approachesor with real data.
arxiv-3300-177 | A Robust Rapid Approach to Image Segmentation with Optimal Thresholding and Watershed Transform | http://arxiv.org/pdf/1303.4866v1.pdf | author:Ankit R. Chadha, Neha S. Satam category:cs.CV published:2013-03-20 summary:This paper describes a novel method for partitioning image into meaningfulsegments. The proposed method employs watershed transform, a well-known imagesegmentation technique. Along with that, it uses various auxiliary schemes suchas Binary Gradient Masking, dilation which segment the image in proper way. Thealgorithm proposed in this paper considers all these methods in effective wayand takes little time. It is organized in such a manner so that it operates oninput image adaptively. Its robustness and efficiency makes it more convenientand suitable for all types of images.
arxiv-3300-178 | Asynchronous Cellular Operations on Gray Images Extracting Topographic Shape Features and Their Relations | http://arxiv.org/pdf/1303.4840v1.pdf | author:Igor Polkovnikov category:cs.CV published:2013-03-20 summary:A variety of operations of cellular automata on gray images is presented. Alloperations are of a wave-front nature finishing in a stable state. They areused to extract shape descripting gray objects robust to a variety of patterndistortions. Topographic terms are used: "lakes", "dales", "dales of dales". Itis shown how mutual object relations like "above" can be presented in terms ofgray image analysis and how it can be used for character classification and forgray pattern decomposition. Algorithms can be realized with a parallelasynchronous architecture. Keywords: Pattern Recognition, MathematicalMorphology, Cellular Automata, Wave-front Algorithms, Gray Image Analysis,Topographical Shape Descriptors, Asynchronous Parallel Processors, Holes,Cavities, Concavities, Graphs.
arxiv-3300-179 | Ensembling classification models based on phalanxes of variables with applications in drug discovery | http://arxiv.org/pdf/1303.4805v4.pdf | author:Jabed H. Tomal, William J. Welch, Ruben H. Zamar category:stat.ML stat.CO published:2013-03-20 summary:Statistical detection of a rare class of objects in a two-classclassification problem can pose several challenges. Because the class ofinterest is rare in the training data, there is relatively little informationin the known class response labels for model building. At the same time theavailable explanatory variables are often moderately high dimensional. In thefour assays of our drug-discovery application, compounds are active or notagainst a specific biological target, such as lung cancer tumor cells, andactive compounds are rare. Several sets of chemical descriptor variables fromcomputational chemistry are available to classify the active versus inactiveclass; each can have up to thousands of variables characterizing molecularstructure of the compounds. The statistical challenge is to make use of therichness of the explanatory variables in the presence of scant responseinformation. Our algorithm divides the explanatory variables into subsetsadaptively and passes each subset to a base classifier. The various baseclassifiers are then ensembled to produce one model to rank new objects bytheir estimated probabilities of belonging to the rare class of interest. Theessence of the algorithm is to choose the subsets such that variables in thesame group work well together; we call such groups phalanxes.
arxiv-3300-180 | Compressive Shift Retrieval | http://arxiv.org/pdf/1303.4996v2.pdf | author:Henrik Ohlsson, Yonina C. Eldar, Allen Y. Yang, S. Shankar Sastry category:cs.SY cs.IT math.IT stat.ML published:2013-03-20 summary:The classical shift retrieval problem considers two signals in vector formthat are related by a shift. The problem is of great importance in manyapplications and is typically solved by maximizing the cross-correlationbetween the two signals. Inspired by compressive sensing, in this paper, weseek to estimate the shift directly from compressed signals. We show that undercertain conditions, the shift can be recovered using fewer samples and lesscomputation compared to the classical setup. Of particular interest is shiftestimation from Fourier coefficients. We show that under rather mild conditionsonly one Fourier coefficient suffices to recover the true shift.
arxiv-3300-181 | The State of the Art Recognize in Arabic Script through Combination of Online and Offline | http://arxiv.org/pdf/1303.4839v1.pdf | author:Dr. Firoj Parwej category:cs.CV published:2013-03-20 summary:Handwriting recognition refers to the identification of written characters.Handwriting recognition has become an acute research area in recent years forthe ease of access of computer science. In this paper primarily discussedOn-line and Off-line handwriting recognition methods for Arabic words which areoften used among then across the Middle East and North Africa People. Arabicword online handwriting recognition is a very challenging task due to itscursive nature. Because of the characteristic of the whole body of the Arabicscript, namely connectivity between the characters, thereby the segmentation ofAn Arabic script is very difficult. In this paper we introduced an Arabicscript multiple classifier system for recognizing notes written on a Starboard.This Arabic script multiple classifier system combines one off-line and on-linehandwriting recognition systems. The Arabic script recognizers are all based onHidden Markov Models but vary in the way of preprocessing and normalization. Tocombine the Arabic script output sequences of the recognizers, we incrementallyalign the word sequences using a norm string matching algorithm. The Arabicscript combination we could increase the system performance over the excellentcharacter recognizer by about 3%. The proposed technique is also the necessarystep towards character recognition, person identification, personalitydetermination where input data is processed from all perspectives.
arxiv-3300-182 | A Survey of Appearance Models in Visual Object Tracking | http://arxiv.org/pdf/1303.4803v1.pdf | author:Xi Li, Weiming Hu, Chunhua Shen, Zhongfei Zhang, Anthony Dick, Anton van den Hengel category:cs.CV published:2013-03-20 summary:Visual object tracking is a significant computer vision task which can beapplied to many domains such as visual surveillance, human computerinteraction, and video compression. In the literature, researchers haveproposed a variety of 2D appearance models. To help readers swiftly learn therecent advances in 2D appearance models for visual object tracking, wecontribute this survey, which provides a detailed review of the existing 2Dappearance models. In particular, this survey takes a module-based architecturethat enables readers to easily grasp the key points of visual object tracking.In this survey, we first decompose the problem of appearance modeling into twodifferent processing stages: visual representation and statistical modeling.Then, different 2D appearance models are categorized and discussed with respectto their composition modules. Finally, we address several issues of interest aswell as the remaining challenges for future research on this topic. Thecontributions of this survey are four-fold. First, we review the literature ofvisual representations according to their feature-construction mechanisms(i.e., local and global). Second, the existing statistical modeling schemes fortracking-by-detection are reviewed according to their model-constructionmechanisms: generative, discriminative, and hybrid generative-discriminative.Third, each type of visual representations or statistical modeling techniquesis analyzed and discussed from a theoretical or practical viewpoint. Fourth,the existing benchmark resources (e.g., source code and video datasets) areexamined in this survey.
arxiv-3300-183 | On Constructing the Value Function for Optimal Trajectory Problem and its Application to Image Processing | http://arxiv.org/pdf/1303.4845v2.pdf | author:Myong-Song Ho, Gwang-Hui Ju, Yong-Bom O, Gwang-Ho Jong category:cs.CV published:2013-03-20 summary:We proposed an algorithm for solving Hamilton-Jacobi equation associated toan optimal trajectory problem for a vehicle moving inside the pre-specifieddomain with the speed depending upon the direction of the motion and currentposition of the vehicle. The dynamics of the vehicle is defined by an ordinarydifferential equation, the right hand of which is given by product of control(atime dependent fuction) and a function dependent on trajectory and control. Atsome unspecified terminal time, the vehicle reaches the boundary of thepre-specified domain and incurs a terminal cost. We also associate thetraveling cost with a type of integral to the trajectory followed by vehicle.We are interested in a numerical method for finding a trajectory that minimizesthe sum of the traveling cost and terminal cost. We developed an algorithmsolving the value function for general trajectory optimization problem. Ouralgorithm is closely related to the Tsitsiklis's Fast Marching Method and J. A.Sethian's OUM and SLF-LLL[1-4] and is a generalization of them. On the basis ofthese results, We applied our algorithm to the image processing such asfingerprint verification.
arxiv-3300-184 | Greedy Feature Selection for Subspace Clustering | http://arxiv.org/pdf/1303.4778v2.pdf | author:Eva L. Dyer, Aswin C. Sankaranarayanan, Richard G. Baraniuk category:cs.LG math.NA stat.ML published:2013-03-19 summary:Unions of subspaces provide a powerful generalization to linear subspacemodels for collections of high-dimensional data. To learn a union of subspacesfrom a collection of data, sets of signals in the collection that belong to thesame subspace must be identified in order to obtain accurate estimates of thesubspace structures present in the data. Recently, sparse recovery methods havebeen shown to provide a provable and robust strategy for exact featureselection (EFS)--recovering subsets of points from the ensemble that live inthe same subspace. In parallel with recent studies of EFS with L1-minimization,in this paper, we develop sufficient conditions for EFS with a greedy methodfor sparse signal recovery known as orthogonal matching pursuit (OMP).Following our analysis, we provide an empirical study of feature selectionstrategies for signals living on unions of subspaces and characterize the gapbetween sparse recovery methods and nearest neighbor (NN)-based approaches. Inparticular, we demonstrate that sparse recovery methods provide significantadvantages over NN methods and the gap between the two approaches isparticularly pronounced when the sampling of subspaces in the dataset issparse. Our results suggest that OMP may be employed to reliably recover exactfeature sets in a number of regimes where NN approaches fail to reveal thesubspace membership of points in the ensemble.
arxiv-3300-185 | Large-Scale Learning with Less RAM via Randomization | http://arxiv.org/pdf/1303.4664v1.pdf | author:Daniel Golovin, D. Sculley, H. Brendan McMahan, Michael Young category:cs.LG published:2013-03-19 summary:We reduce the memory footprint of popular large-scale online learning methodsby projecting our weight vector onto a coarse discrete set using randomizedrounding. Compared to standard 32-bit float encodings, this reduces RAM usageby more than 50% during training and by up to 95% when making predictions froma fixed model, with almost no loss in accuracy. We also show that randomizedcounting can be used to implement per-coordinate learning rates, improvingmodel quality with little additional RAM. We prove these memory-saving methodsachieve regret guarantees similar to their exact variants. Empirical evaluationconfirms excellent performance, dominating standard approaches across memoryversus accuracy tradeoffs.
arxiv-3300-186 | Handwritten and Printed Text Separation in Real Document | http://arxiv.org/pdf/1303.4614v1.pdf | author:Abdel Belaïd, K. C. Santosh, Vincent Poulain D'Andecy category:cs.CV published:2013-03-19 summary:The aim of the paper is to separate handwritten and printed text from a realdocument embedded with noise, graphics including annotations. Relying onrun-length smoothing algorithm (RLSA), the extracted pseudo-lines andpseudo-words are used as basic blocks for classification. To handle this, amulti-class support vector machine (SVM) with Gaussian kernel performs a firstlabelling of each pseudo-word including the study of local neighbourhood. Itthen propagates the context between neighbours so that we can correct possiblelabelling errors. Considering running time complexity issue, we propose linearcomplexity methods where we use k-NN with constraint. When using a kd-tree, itis almost linearly proportional to the number of pseudo-words. The performanceof our system is close to 90%, even when very small learning dataset wheresamples are basically composed of complex administrative documents.
arxiv-3300-187 | Inferring Fitness in Finite Populations with Moran-like dynamics | http://arxiv.org/pdf/1303.4566v3.pdf | author:Marc Harper category:math.DS cs.NE q-bio.PE 91A22 published:2013-03-19 summary:Biological fitness is not an observable quantity and must be inferred frompopulation dynamics. Bayesian inference applied to the Moran process andvariants yields a robust inference method that can infer fitness in populationsevolving via a Moran dynamic and generalizations. Information about fitness isderived solely from birth-events in birth-death and death-birth processes inwhich selection acts proportionally to fitness, which allows the method to beapplied to populations on a network where the network itself may be changing intime. Populations may also be allowed to change size while still allowingestimates for fitness to be inferred.
arxiv-3300-188 | Marginal Likelihoods for Distributed Parameter Estimation of Gaussian Graphical Models | http://arxiv.org/pdf/1303.4756v6.pdf | author:Zhaoshi Meng, Dennis Wei, Ami Wiesel, Alfred O. Hero III category:stat.ML cs.LG published:2013-03-19 summary:We consider distributed estimation of the inverse covariance matrix, alsocalled the concentration or precision matrix, in Gaussian graphical models.Traditional centralized estimation often requires global inference of thecovariance matrix, which can be computationally intensive in large dimensions.Approximate inference based on message-passing algorithms, on the other hand,can lead to unstable and biased estimation in loopy graphical models. In thispaper, we propose a general framework for distributed estimation based on amaximum marginal likelihood (MML) approach. This approach computes localparameter estimates by maximizing marginal likelihoods defined with respect todata collected from local neighborhoods. Due to the non-convexity of the MMLproblem, we introduce and solve a convex relaxation. The local estimates arethen combined into a global estimate without the need for iterativemessage-passing between neighborhoods. The proposed algorithm is naturallyparallelizable and computationally efficient, thereby making it suitable forhigh-dimensional problems. In the classical regime where the number ofvariables $p$ is fixed and the number of samples $T$ increases to infinity, theproposed estimator is shown to be asymptotically consistent and to improvemonotonically as the local neighborhood size increases. In the high-dimensionalscaling regime where both $p$ and $T$ increase to infinity, the convergencerate to the true parameters is derived and is seen to be comparable tocentralized maximum likelihood estimation. Extensive numerical experimentsdemonstrate the improved performance of the two-hop version of the proposedestimator, which suffices to almost close the gap to the centralized maximumlikelihood estimator at a reduced computational cost.
arxiv-3300-189 | Genetic algorithms for finding the weight enumerator of binary linear block codes | http://arxiv.org/pdf/1303.4227v1.pdf | author:Said Nouh, Mostafa Belkasmi category:cs.IT cs.NE math.IT published:2013-03-18 summary:In this paper we present a new method for finding the weight enumerator ofbinary linear block codes by using genetic algorithms. This method consists infinding the binary weight enumerator of the code and its dual and to createfrom the famous MacWilliams identity a linear system (S) of integer variablesfor which we add all known information obtained from the structure of the code.The knowledge of some subgroups of the automorphism group, under which the coderemains invariant, permits to give powerful restrictions on the solutions of(S) and to approximate the weight enumerator. By applying this method and byusing the stability of the Extended Quadratic Residue codes (ERQ) by theProjective Special Linear group PSL2, we find a list of all possible values ofthe weight enumerators for the two ERQ codes of lengths 192 and 200. We alsomade a good approximation of the true value for these two enumerators.
arxiv-3300-190 | Margins, Shrinkage, and Boosting | http://arxiv.org/pdf/1303.4172v1.pdf | author:Matus Telgarsky category:cs.LG stat.ML published:2013-03-18 summary:This manuscript shows that AdaBoost and its immediate variants can produceapproximate maximum margin classifiers simply by scaling step size choices witha fixed small constant. In this way, when the unscaled step size is an optimalchoice, these results provide guarantees for Friedman's empirically successful"shrinkage" procedure for gradient boosting (Friedman, 2000). Guarantees arealso provided for a variety of other step sizes, affirming the intuition thatincreasingly regularized line searches provide improved margin guarantees. Theresults hold for the exponential loss and similar losses, most notably thelogistic loss.
arxiv-3300-191 | Markov Chain Monte Carlo for Arrangement of Hyperplanes in Locality-Sensitive Hashing | http://arxiv.org/pdf/1303.4169v1.pdf | author:Yui Noma, Makiko Konoshima category:cs.LG published:2013-03-18 summary:Since Hamming distances can be calculated by bitwise computations, they canbe calculated with less computational load than L2 distances. Similaritysearches can therefore be performed faster in Hamming distance space. Theelements of Hamming distance space are bit strings. On the other hand, thearrangement of hyperplanes induce the transformation from the feature vectorsinto feature bit strings. This transformation method is a type oflocality-sensitive hashing that has been attracting attention as a way ofperforming approximate similarity searches at high speed. Supervised learningof hyperplane arrangements allows us to obtain a method that transforms theminto feature bit strings reflecting the information of labels applied tohigher-dimensional feature vectors. In this p aper, we propose a supervisedlearning method for hyperplane arrangements in feature space that uses a Markovchain Monte Carlo (MCMC) method. We consider the probability density functionsused during learning, and evaluate their performance. We also consider thesampling method for learning data pairs needed in learning, and we evaluate itsperformance. We confirm that the accuracy of this learning method when using asuitable probability density function and sampling method is greater than theaccuracy of existing learning methods.
arxiv-3300-192 | Generalized Thompson Sampling for Sequential Decision-Making and Causal Inference | http://arxiv.org/pdf/1303.4431v1.pdf | author:Pedro A. Ortega, Daniel A. Braun category:cs.AI stat.ML published:2013-03-18 summary:Recently, it has been shown how sampling actions from the predictivedistribution over the optimal action-sometimes called Thompson sampling-can beapplied to solve sequential adaptive control problems, when the optimal policyis known for each possible environment. The predictive distribution can then beconstructed by a Bayesian superposition of the optimal policies weighted bytheir posterior probability that is updated by Bayesian inference and causalcalculus. Here we discuss three important features of this approach. First, wediscuss in how far such Thompson sampling can be regarded as a naturalconsequence of the Bayesian modeling of policy uncertainty. Second, we show howThompson sampling can be used to study interactions between multiple adaptiveagents, thus, opening up an avenue of game-theoretic analysis. Third, we showhow Thompson sampling can be applied to infer causal relationships wheninteracting with an environment in a sequential fashion. In summary, ourresults suggest that Thompson sampling might not merely be a useful heuristic,but a principled method to address problems of adaptive sequentialdecision-making and causal inference.
arxiv-3300-193 | A General Iterative Shrinkage and Thresholding Algorithm for Non-convex Regularized Optimization Problems | http://arxiv.org/pdf/1303.4434v1.pdf | author:Pinghua Gong, Changshui Zhang, Zhaosong Lu, Jianhua Huang, Jieping Ye category:cs.LG cs.NA stat.CO stat.ML published:2013-03-18 summary:Non-convex sparsity-inducing penalties have recently received considerableattentions in sparse learning. Recent theoretical investigations havedemonstrated their superiority over the convex counterparts in several sparselearning settings. However, solving the non-convex optimization problemsassociated with non-convex penalties remains a big challenge. A commonly usedapproach is the Multi-Stage (MS) convex relaxation (or DC programming), whichrelaxes the original non-convex problem to a sequence of convex problems. Thisapproach is usually not very practical for large-scale problems because itscomputational cost is a multiple of solving a single convex problem. In thispaper, we propose a General Iterative Shrinkage and Thresholding (GIST)algorithm to solve the nonconvex optimization problem for a large class ofnon-convex penalties. The GIST algorithm iteratively solves a proximal operatorproblem, which in turn has a closed-form solution for many commonly usedpenalties. At each outer iteration of the algorithm, we use a line searchinitialized by the Barzilai-Borwein (BB) rule that allows finding anappropriate step size quickly. The paper also presents a detailed convergenceanalysis of the GIST algorithm. The efficiency of the proposed algorithm isdemonstrated by extensive experiments on large-scale data sets.
arxiv-3300-194 | Neurally Implementable Semantic Networks | http://arxiv.org/pdf/1303.4164v1.pdf | author:Garrett N. Evans, John C. Collins category:q-bio.NC cs.NE I.2.4; I.2.6 published:2013-03-18 summary:We propose general principles for semantic networks allowing them to beimplemented as dynamical neural networks. Major features of our scheme include:(a) the interpretation that each node in a network stands for a boundintegration of the meanings of all nodes and external events the node linkswith; (b) the systematic use of nodes that stand for categories or types, withseparate nodes for instances of these types; (c) an implementation ofrelationships that does not use intrinsically typed links between nodes.
arxiv-3300-195 | Improved Foreground Detection via Block-based Classifier Cascade with Probabilistic Decision Integration | http://arxiv.org/pdf/1303.4160v1.pdf | author:Vikas Reddy, Conrad Sanderson, Brian C. Lovell category:cs.CV published:2013-03-18 summary:Background subtraction is a fundamental low-level processing task in numerouscomputer vision applications. The vast majority of algorithms process images ona pixel-by-pixel basis, where an independent decision is made for each pixel. Ageneral limitation of such processing is that rich contextual information isnot taken into account. We propose a block-based method capable of dealing withnoise, illumination variations and dynamic backgrounds, while still obtainingsmooth contours of foreground objects. Specifically, image sequences areanalysed on an overlapping block-by-block basis. A low-dimensional texturedescriptor obtained from each block is passed through an adaptive classifiercascade, where each stage handles a distinct problem. A probabilisticforeground mask generation approach then exploits block overlaps to integrateinterim block-level decisions into final pixel-level foreground segmentation.Unlike many pixel-based methods, ad-hoc post-processing of foreground masks isnot required. Experiments on the difficult Wallflower and I2R datasets showthat the proposed approach obtains on average better results (bothqualitatively and quantitatively) than several prominent methods. Wefurthermore propose the use of tracking performance as an unbiased approach forassessing the practical usefulness of foreground segmentation methods, and showthat the proposed approach leads to considerable improvements in trackingaccuracy on the CAVIAR dataset.
arxiv-3300-196 | Modeling a Sensor to Improve its Efficacy | http://arxiv.org/pdf/1303.4385v1.pdf | author:N. K. Malakar, D. Gladkov, K. H. Knuth category:astro-ph.IM stat.ML published:2013-03-18 summary:Robots rely on sensors to provide them with information about theirsurroundings. However, high-quality sensors can be extremely expensive andcost-prohibitive. Thus many robotic systems must make due with lower-qualitysensors. Here we demonstrate via a case study how modeling a sensor can improveits efficacy when employed within a Bayesian inferential framework. As a testbed we employ a robotic arm that is designed to autonomously take its ownmeasurements using an inexpensive LEGO light sensor to estimate the positionand radius of a white circle on a black field. The light sensor integrates thelight arriving from a spatially distributed region within its field of viewweighted by its Spatial Sensitivity Function (SSF). We demonstrate that byincorporating an accurate model of the light sensor SSF into the likelihoodfunction of a Bayesian inference engine, an autonomous system can make improvedinferences about its surroundings. The method presented here is data-based,fairly general, and made with plug-and play in mind so that it could beimplemented in similar problems.
arxiv-3300-197 | Improving CUR Matrix Decomposition and the Nyström Approximation via Adaptive Sampling | http://arxiv.org/pdf/1303.4207v7.pdf | author:Shusen Wang, Zhihua Zhang category:cs.LG cs.NA published:2013-03-18 summary:The CUR matrix decomposition and the Nystr\"{o}m approximation are twoimportant low-rank matrix approximation techniques. The Nystr\"{o}m methodapproximates a symmetric positive semidefinite matrix in terms of a smallnumber of its columns, while CUR approximates an arbitrary data matrix by asmall number of its columns and rows. Thus, CUR decomposition can be regardedas an extension of the Nystr\"{o}m approximation. In this paper we establish a more general error bound for the adaptivecolumn/row sampling algorithm, based on which we propose more accurate CUR andNystr\"{o}m algorithms with expected relative-error bounds. The proposed CURand Nystr\"{o}m algorithms also have low time complexity and can avoidmaintaining the whole data matrix in RAM. In addition, we give theoreticalanalysis for the lower error bounds of the standard Nystr\"{o}m method and theensemble Nystr\"{o}m method. The main theoretical results established in thispaper are novel, and our analysis makes no special assumption on the datamatrices.
arxiv-3300-198 | Using evolutionary design to interactively sketch car silhouettes and stimulate designer's creativity | http://arxiv.org/pdf/1303.5050v2.pdf | author:François Cluzel, Bernard Yannou, Markus Dihlmann category:cs.NE cs.HC physics.med-ph published:2013-03-17 summary:An Interactive Genetic Algorithm is proposed to progressively sketch thedesired side-view of a car profile. It adopts a Fourier decomposition of a 2Dprofile as the genotype, and proposes a cross-over mechanism. In addition, aformula function of two genes' discrepancies is fitted to the perceiveddissimilarity between two car profiles. This similarity index is intensivelyused, throughout a series of user tests, to highlight the added value of theIGA compared to a systematic car shape exploration, to prove its ability tocreate superior satisfactory designs and to stimulate designer's creativity.These tests have involved six designers with a design goal defined by asemantic attribute. The results reveal that if "friendly" is diverselyinterpreted in terms of car shapes, "sportive" denotes a very conventionalrepresentation which may be a limitation for shape renewal.
arxiv-3300-199 | A Quorum Sensing Inspired Algorithm for Dynamic Clustering | http://arxiv.org/pdf/1303.3934v2.pdf | author:Feng Tan, Jean-Jacques Slotine category:cs.LG published:2013-03-16 summary:Quorum sensing is a decentralized biological process, through which acommunity of cells with no global awareness coordinate their functionalbehaviors based solely on cell-medium interactions and local decisions. Thispaper draws inspirations from quorum sensing and colony competition to derive anew algorithm for data clustering. The algorithm treats each data as a singlecell, and uses knowledge of local connectivity to cluster cells into multiplecolonies simultaneously. It simulates auto-inducers secretion in quorum sensingto tune the influence radius for each cell. At the same time, sparselydistributed core cells spread their influences to form colonies, andinteractions between colonies eventually determine each cell's identity. Thealgorithm has the flexibility to analyze not only static but also time-varyingdata, which surpasses the capacity of many existing algorithms. Its stabilityand convergence properties are established. The algorithm is tested on severalapplications, including both synthetic and real benchmarks data sets, allelesclustering, community detection, image segmentation. In particular, thealgorithm's distinctive capability to deal with time-varying data allows us toexperiment it on novel applications such as robotic swarms grouping andswitching model identification. We believe that the algorithm's promisingperformance would stimulate many more exciting applications.
arxiv-3300-200 | On multi-class learning through the minimization of the confusion matrix norm | http://arxiv.org/pdf/1303.4015v2.pdf | author:Sokol Koço, Cécile Capponi category:cs.LG published:2013-03-16 summary:In imbalanced multi-class classification problems, the misclassification rateas an error measure may not be a relevant choice. Several methods have beendeveloped where the performance measure retained richer information than themere misclassification rate: misclassification costs, ROC-based information,etc. Following this idea of dealing with alternate measures of performance, wepropose to address imbalanced classification problems by using a new measure tobe optimized: the norm of the confusion matrix. Indeed, recent results showthat using the norm of the confusion matrix as an error measure can be quiteinteresting due to the fine-grain informations contained in the matrix,especially in the case of imbalanced classes. Our first contribution thenconsists in showing that optimizing criterion based on the confusion matrixgives rise to a common background for cost-sensitive methods aimed at dealingwith imbalanced classes learning problems. As our second contribution, wepropose an extension of a recent multi-class boosting method --- namelyAdaBoost.MM --- to the imbalanced class problem, by greedily minimizing theempirical norm of the confusion matrix. A theoretical analysis of theproperties of the proposed method is presented, while experimental resultsillustrate the behavior of the algorithm and show the relevancy of the approachcompared to other methods.
arxiv-3300-201 | $l_{2,p}$ Matrix Norm and Its Application in Feature Selection | http://arxiv.org/pdf/1303.3987v1.pdf | author:Liping Wang, Songcan Chen category:cs.LG cs.CV stat.ML published:2013-03-16 summary:Recently, $l_{2,1}$ matrix norm has been widely applied to many areas such ascomputer vision, pattern recognition, biological study and etc. As an extensionof $l_1$ vector norm, the mixed $l_{2,1}$ matrix norm is often used to findjointly sparse solutions. Moreover, an efficient iterative algorithm has beendesigned to solve $l_{2,1}$-norm involved minimizations. Actually,computational studies have showed that $l_p$-regularization ($0<p<1$) issparser than $l_1$-regularization, but the extension to matrix norm has beenseldom considered. This paper presents a definition of mixed $l_{2,p}$ $(p\in(0, 1])$ matrix pseudo norm which is thought as both generalizations of $l_p$vector norm to matrix and $l_{2,1}$-norm to nonconvex cases $(0<p<1)$.Fortunately, an efficient unified algorithm is proposed to solve the induced$l_{2,p}$-norm $(p\in (0, 1])$ optimization problems. The convergence can alsobe uniformly demonstrated for all $p\in (0, 1]$. Typical $p\in (0,1]$ areapplied to select features in computational biology and the experimentalresults show that some choices of $0<p<1$ do improve the sparse pattern ofusing $p=1$.
arxiv-3300-202 | An Adaptive Methodology for Ubiquitous ASR System | http://arxiv.org/pdf/1303.3948v1.pdf | author:Urmila Shrawankar, Vilas Thakare category:cs.CL cs.HC cs.SD published:2013-03-16 summary:Achieving and maintaining the performance of ubiquitous (Automatic SpeechRecognition) ASR system is a real challenge. The main objective of this work isto develop a method that will improve and show the consistency in performanceof ubiquitous ASR system for real world noisy environment. An adaptivemethodology has been developed to achieve an objective with the help ofimplementing followings, -Cleaning speech signal as much as possible whilepreserving originality / intangibility using various modified filters andenhancement techniques. -Extracting features from speech signals using varioussizes of parameter. -Train the system for ubiquitous environment usingmulti-environmental adaptation training methods. -Optimize the word recognitionrate with appropriate variable size of parameters using fuzzy technique. Theconsistency in performance is tested using standard noise databases as well asin real world environment. A good improvement is noticed. This work will behelpful to give discriminative training of ubiquitous ASR system for betterHuman Computer Interaction (HCI) using Speech User Interface (SUI).
arxiv-3300-203 | A Last-Step Regression Algorithm for Non-Stationary Online Learning | http://arxiv.org/pdf/1303.3754v1.pdf | author:Edward Moroshko, Koby Crammer category:cs.LG published:2013-03-15 summary:The goal of a learner in standard online learning is to maintain an averageloss close to the loss of the best-performing single function in some class. Inmany real-world problems, such as rating or ranking items, there is no singlebest target function during the runtime of the algorithm, instead the best(local) target function is drifting over time. We develop a novel last-stepminmax optimal algorithm in context of a drift. We analyze the algorithm in theworst-case regret framework and show that it maintains an average loss close tothat of the best slowly changing sequence of linear functions, as long as thetotal of drift is sublinear. In some situations, our bound improves overexisting bounds, and additionally the algorithm suffers logarithmic regret whenthere is no drift. We also build on the H_infinity filter and its bound, anddevelop and analyze a second algorithm for drifting setting. Syntheticsimulations demonstrate the advantages of our algorithms in a worst-caseconstant drift setting.
arxiv-3300-204 | Variational Semi-blind Sparse Deconvolution with Orthogonal Kernel Bases and its Application to MRFM | http://arxiv.org/pdf/1303.3866v1.pdf | author:Se Un Park, Nicolas Dobigeon, Alfred O. Hero category:stat.ML published:2013-03-15 summary:We present a variational Bayesian method of joint image reconstruction andpoint spread function (PSF) estimation when the PSF of the imaging device isonly partially known. To solve this semi-blind deconvolution problem, priordistributions are specified for the PSF and the 3D image. Joint imagereconstruction and PSF estimation is then performed within a Bayesianframework, using a variational algorithm to estimate the posteriordistribution. The image prior distribution imposes an explicit atomic measurethat corresponds to image sparsity. Importantly, the proposed Bayesiandeconvolution algorithm does not require hand tuning. Simulation resultsclearly demonstrate that the semi-blind deconvolution algorithm comparesfavorably with previous Markov chain Monte Carlo (MCMC) version of myopicsparse reconstruction. It significantly outperforms mismatched non-blindalgorithms that rely on the assumption of the perfect knowledge of the PSF. Thealgorithm is illustrated on real data from magnetic resonance force microscopy(MRFM).
arxiv-3300-205 | Efficient Evolutionary Algorithm for Single-Objective Bilevel Optimization | http://arxiv.org/pdf/1303.3901v2.pdf | author:Ankur Sinha, Pekka Malo, Kalyanmoy Deb category:cs.NE published:2013-03-15 summary:Bilevel optimization problems are a class of challenging optimizationproblems, which contain two levels of optimization tasks. In these problems,the optimal solutions to the lower level problem become possible feasiblecandidates to the upper level problem. Such a requirement makes theoptimization problem difficult to solve, and has kept the researchers busytowards devising methodologies, which can efficiently handle the problem.Despite the efforts, there hardly exists any effective methodology, which iscapable of handling a complex bilevel problem. In this paper, we introducebilevel evolutionary algorithm based on quadratic approximations (BLEAQ) ofoptimal lower level variables with respect to the upper level variables. Theapproach is capable of handling bilevel problems with different kinds ofcomplexities in relatively smaller number of function evaluations. Ideas fromclassical optimization have been hybridized with evolutionary methods togenerate an efficient optimization algorithm for generic bilevel problems. Theefficacy of the algorithm has been shown on two sets of test problems. Thefirst set is a recently proposed SMD test set, which contains problems withcontrollable complexities, and the second set contains standard test problemscollected from the literature. The proposed method has been evaluated againsttwo benchmarks, and the performance gain is observed to be significant.
arxiv-3300-206 | Subspace Clustering via Thresholding and Spectral Clustering | http://arxiv.org/pdf/1303.3716v1.pdf | author:Reinhard Heckel, Helmut Bölcskei category:cs.IT cs.LG math.IT math.ST stat.ML stat.TH published:2013-03-15 summary:We consider the problem of clustering a set of high-dimensional data pointsinto sets of low-dimensional linear subspaces. The number of subspaces, theirdimensions, and their orientations are unknown. We propose a simple andlow-complexity clustering algorithm based on thresholding the correlationsbetween the data points followed by spectral clustering. A probabilisticperformance analysis shows that this algorithm succeeds even when the subspacesintersect, and when the dimensions of the subspaces scale (up to a log-factor)linearly in the ambient dimension. Moreover, we prove that the algorithm alsosucceeds for data points that are subject to erasures with the number oferasures scaling (up to a log-factor) linearly in the ambient dimension.Finally, we propose a simple scheme that provably detects outliers.
arxiv-3300-207 | Topic Discovery through Data Dependent and Random Projections | http://arxiv.org/pdf/1303.3664v2.pdf | author:Weicong Ding, Mohammad H. Rohban, Prakash Ishwar, Venkatesh Saligrama category:stat.ML cs.LG published:2013-03-15 summary:We present algorithms for topic modeling based on the geometry ofcross-document word-frequency patterns. This perspective gains significanceunder the so called separability condition. This is a condition on existence ofnovel-words that are unique to each topic. We present a suite of highlyefficient algorithms based on data-dependent and random projections ofword-frequency patterns to identify novel words and associated topics. We willalso discuss the statistical guarantees of the data-dependent projectionsmethod based on two mild assumptions on the prior density of topic documentmatrix. Our key insight here is that the maximum and minimum values ofcross-document frequency patterns projected along any direction are associatedwith novel words. While our sample complexity bounds for topic recovery aresimilar to the state-of-art, the computational complexity of our randomprojection scheme scales linearly with the number of documents and the numberof words per document. We present several experiments on synthetic andreal-world datasets to demonstrate qualitative and quantitative merits of ourscheme.
arxiv-3300-208 | A survey on sensing methods and feature extraction algorithms for SLAM problem | http://arxiv.org/pdf/1303.3605v1.pdf | author:Adheen Ajay, D. Venkataraman category:cs.RO cs.CV cs.LG published:2013-03-14 summary:This paper is a survey work for a bigger project for designing a Visual SLAMrobot to generate 3D dense map of an unknown unstructured environment. A lot offactors have to be considered while designing a SLAM robot. Sensing method ofthe SLAM robot should be determined by considering the kind of environment tobe modeled. Similarly the type of environment determines the suitable featureextraction method. This paper goes through the sensing methods used in somerecently published papers. The main objective of this survey is to conduct acomparative study among the current sensing methods and feature extractionalgorithms and to extract out the best for our work.
arxiv-3300-209 | Statistical Regression to Predict Total Cumulative CPU Usage of MapReduce Jobs | http://arxiv.org/pdf/1303.3632v1.pdf | author:Nikzad Babaii Rizvandi, Javid Taheri, Reza Moraveji, Albert Y. Zomaya category:cs.DC cs.LG cs.PF published:2013-03-14 summary:Recently, businesses have started using MapReduce as a popular computationframework for processing large amount of data, such as spam detection, anddifferent data mining tasks, in both public and private clouds. Two of thechallenging questions in such environments are (1) choosing suitable values forMapReduce configuration parameters e.g., number of mappers, number of reducers,and DFS block size, and (2) predicting the amount of resources that a usershould lease from the service provider. Currently, the tasks of both choosingconfiguration parameters and estimating required resources are solely the usersresponsibilities. In this paper, we present an approach to provision the totalCPU usage in clock cycles of jobs in MapReduce environment. For a MapReducejob, a profile of total CPU usage in clock cycles is built from the job pastexecutions with different values of two configuration parameters e.g., numberof mappers, and number of reducers. Then, a polynomial regression is used tomodel the relation between these configuration parameters and total CPU usagein clock cycles of the job. We also briefly study the influence of input datascaling on measured total CPU usage in clock cycles. This derived model alongwith the scaling result can then be used to provision the total CPU usage inclock cycles of the same jobs with different input data size. We validate theaccuracy of our models using three realistic applications (WordCount, EximMainLog parsing, and TeraSort). Results show that the predicted total CPU usagein clock cycles of generated resource provisioning options are less than 8% ofthe measured total CPU usage in clock cycles in our 20-node virtual Hadoopcluster.
arxiv-3300-210 | Expressing Ethnicity through Behaviors of a Robot Character | http://arxiv.org/pdf/1303.3592v1.pdf | author:Maxim Makatchev, Reid Simmons, Majd Sakr, Micheline Ziadee category:cs.CL cs.CY cs.RO published:2013-03-14 summary:Achieving homophily, or association based on similarity, between a human userand a robot holds a promise of improved perception and task performance.However, no previous studies that address homophily via ethnic similarity withrobots exist. In this paper, we discuss the difficulties of evoking ethnic cuesin a robot, as opposed to a virtual agent, and an approach to overcome thosedifficulties based on using ethnically salient behaviors. We outline ourmethodology for selecting and evaluating such behaviors, and culminate with astudy that evaluates our hypotheses of the possibility of ethnic attribution ofa robot character through verbal and nonverbal behaviors and of achieving thehomophily effect.
arxiv-3300-211 | Hybrid Evolutionary Computation for Continuous Optimization | http://arxiv.org/pdf/1303.3469v1.pdf | author:Hassan A. Bashir, Richard S. Neville category:cs.NE published:2013-03-14 summary:Hybrid optimization algorithms have gained popularity as it has becomeapparent there cannot be a universal optimization strategy which is globallymore beneficial than any other. Despite their popularity, hybridizationframeworks require more detailed categorization regarding: the nature of theproblem domain, the constituent algorithms, the coupling schema and theintended area of application. This report proposes a hybrid algorithm forsolving small to large-scale continuous global optimization problems. Itcomprises evolutionary computation (EC) algorithms and a sequential quadraticprogramming (SQP) algorithm; combined in a collaborative portfolio. The SQP isa gradient based local search method. To optimize the individual contributionsof the EC and SQP algorithms for the overall success of the proposed hybridsystem, improvements were made in key features of these algorithms. The reportproposes enhancements in: i) the evolutionary algorithm, ii) a new convergencedetection mechanism was proposed; and iii) in the methods for evaluating thesearch directions and step sizes for the SQP local search algorithm. Theproposed hybrid design aim was to ensure that the two algorithms complementeach other by exploring and exploiting the problem search space. Preliminaryresults justify that an adept hybridization of evolutionary algorithms with asuitable local search method, could yield a robust and efficient means ofsolving wide range of global optimization problems. Finally, a discussion ofthe outcomes of the initial investigation and a review of the associatedchallenges and inherent limitations of the proposed method is presented tocomplete the investigation. The report highlights extensive research,particularly, some potential case studies and application areas.
arxiv-3300-212 | Convex Hull-Based Multi-objective Genetic Programming for Maximizing ROC Performance | http://arxiv.org/pdf/1303.3145v2.pdf | author:Pu Wang, Michael Emmerich, Rui Li, Ke Tang, Thomas Baeck, Xin Yao category:cs.NE published:2013-03-13 summary:ROC is usually used to analyze the performance of classifiers in data mining.ROC convex hull (ROCCH) is the least convex major-ant (LCM) of the empiricalROC curve, and covers potential optima for the given set of classifiers.Generally, ROC performance maximization could be considered to maximize theROCCH, which also means to maximize the true positive rate (tpr) and minimizethe false positive rate (fpr) for each classifier in the ROC space. However,tpr and fpr are conflicting with each other in the ROCCH optimization process.Though ROCCH maximization problem seems like a multi-objective optimizationproblem (MOP), the special characters make it different from traditional MOP.In this work, we will discuss the difference between them and propose convexhull-based multi-objective genetic programming (CH-MOGP) to solve ROCCHmaximization problems. Convex hull-based sort is an indicator based selectionscheme that aims to maximize the area under convex hull, which serves as aunary indicator for the performance of a set of points. A selection procedureis described that can be efficiently implemented and follows similar designprinciples than classical hyper-volume based optimization algorithms. It ishypothesized that by using a tailored indicator-based selection scheme CH-MOGPgets more efficient for ROC convex hull approximation than algorithms whichcompute all Pareto optimal points. To test our hypothesis we compare the newCH-MOGP to MOGP with classical selection schemes, including NSGA-II, MOEA/D)and SMS-EMOA. Meanwhile, CH-MOGP is also compared with traditional machinelearning algorithms such as C4.5, Naive Bayes and Prie. Experimental resultsbased on 22 well-known UCI data sets show that CH-MOGP outperformssignificantly traditional EMOAs.
arxiv-3300-213 | A dependent partition-valued process for multitask clustering and time evolving network modelling | http://arxiv.org/pdf/1303.3265v2.pdf | author:Konstantina Palla, David A. Knowles, Zoubin Ghahramani category:stat.ML published:2013-03-13 summary:The fundamental aim of clustering algorithms is to partition data points. Weconsider tasks where the discovered partition is allowed to vary with somecovariate such as space or time. One approach would be to usefragmentation-coagulation processes, but these, being Markov processes, arerestricted to linear or tree structured covariate spaces. We define apartition-valued process on an arbitrary covariate space using Gaussianprocesses. We use the process to construct a multitask clustering model whichpartitions datapoints in a similar way across multiple data sources, and a timeseries model of network data which allows cluster assignments to vary overtime. We describe sampling algorithms for inference and apply our method todefining cancer subtypes based on different types of cellular characteristics,finding regulatory modules from gene expression data from multiple humanpopulations, and discovering time varying community structure in a socialnetwork.
arxiv-3300-214 | A Unified Framework for Probabilistic Component Analysis | http://arxiv.org/pdf/1303.3240v2.pdf | author:Mihalis A. Nicolaou, Stefanos Zafeiriou, Maja Pantic category:cs.LG cs.CV stat.ML published:2013-03-13 summary:We present a unifying framework which reduces the construction ofprobabilistic component analysis techniques to a mere selection of the latentneighbourhood, thus providing an elegant and principled framework for creatingnovel component analysis models as well as constructing probabilisticequivalents of deterministic component analysis methods. Under our framework,we unify many very popular and well-studied component analysis algorithms, suchas Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA),Locality Preserving Projections (LPP) and Slow Feature Analysis (SFA), some ofwhich have no probabilistic equivalents in literature thus far. We firstlydefine the Markov Random Fields (MRFs) which encapsulate the latentconnectivity of the aforementioned component analysis techniques; subsequently,we show that the projection directions produced by all PCA, LDA, LPP and SFAare also produced by the Maximum Likelihood (ML) solution of a single jointprobability density function, composed by selecting one of the defined MRFpriors while utilising a simple observation model. Furthermore, we proposenovel Expectation Maximization (EM) algorithms, exploiting the proposed jointPDF, while we generalize the proposed methodologies to arbitrary connectivitiesvia parameterizable MRF products. Theoretical analysis and experiments on bothsimulated and real world data show the usefulness of the proposed framework, byderiving methods which well outperform state-of-the-art equivalents.
arxiv-3300-215 | Ranking and combining multiple predictors without labeled data | http://arxiv.org/pdf/1303.3257v3.pdf | author:Fabio Parisi, Francesco Strino, Boaz Nadler, Yuval Kluger category:stat.ML cs.LG published:2013-03-13 summary:In a broad range of classification and decision making problems, one is giventhe advice or predictions of several classifiers, of unknown reliability, overmultiple questions or queries. This scenario is different from the standardsupervised setting, where each classifier accuracy can be assessed usingavailable labeled data, and raises two questions: given only the predictions ofseveral classifiers over a large set of unlabeled test data, is it possible toa) reliably rank them; and b) construct a meta-classifier more accurate thanmost classifiers in the ensemble? Here we present a novel spectral approach toaddress these questions. First, assuming conditional independence betweenclassifiers, we show that the off-diagonal entries of their covariance matrixcorrespond to a rank-one matrix. Moreover, the classifiers can be ranked usingthe leading eigenvector of this covariance matrix, as its entries areproportional to their balanced accuracies. Second, via a linear approximationto the maximum likelihood estimator, we derive the Spectral Meta-Learner (SML),a novel ensemble classifier whose weights are equal to this eigenvectorentries. On both simulated and real data, SML typically achieves a higheraccuracy than most classifiers in the ensemble and can provide a betterstarting point than majority voting, for estimating the maximum likelihoodsolution. Furthermore, SML is robust to the presence of small malicious groupsof classifiers designed to veer the ensemble prediction away from the (unknown)ground truth.
arxiv-3300-216 | Estimation Stability with Cross Validation (ESCV) | http://arxiv.org/pdf/1303.3128v2.pdf | author:Chinghway Lim, Bin Yu category:stat.ME stat.ML published:2013-03-13 summary:Cross-validation (CV) is often used to select the regularization parameter inhigh dimensional problems. However, when applied to the sparse modeling methodLasso, CV leads to models that are unstable in high-dimensions, andconsequently not suited for reliable interpretation. In this paper, we proposea model-free criterion ESCV based on a new estimation stability (ES) metric andCV. Our proposed ESCV finds a locally ES-optimal model smaller than the CVchoice so that the it fits the data and also enjoys estimation stabilityproperty. We demonstrate that ESCV is an effective alternative to CV at asimilar easily parallelizable computational cost. In particular, we compare thetwo approaches with respect to several performance measures when applied to theLasso on both simulated and real data sets. For dependent predictors common inpractice, our main finding is that, ESCV cuts down false positive rates oftenby a large margin, while sacrificing little of true positive rates. ESCVusually outperforms CV in terms of parameter estimation while giving similarperformance as CV in terms of prediction. For the two real data sets fromneuroscience and cell biology, the models found by ESCV are less than half ofthe model sizes by CV. Judged based on subject knowledge, they are moreplausible than those by CV as well. We also discuss some regularizationparameter alignment issues that come up in both approaches.
arxiv-3300-217 | A Greedy Approximation of Bayesian Reinforcement Learning with Probably Optimistic Transition Model | http://arxiv.org/pdf/1303.3163v3.pdf | author:Kenji Kawaguchi, Mauricio Araya category:cs.AI cs.LG stat.ML published:2013-03-13 summary:Bayesian Reinforcement Learning (RL) is capable of not only incorporatingdomain knowledge, but also solving the exploration-exploitation dilemma in anatural way. As Bayesian RL is intractable except for special cases, previouswork has proposed several approximation methods. However, these methods areusually too sensitive to parameter values, and finding an acceptable parametersetting is practically impossible in many applications. In this paper, wepropose a new algorithm that greedily approximates Bayesian RL to achieverobustness in parameter space. We show that for a desired learning behavior,our proposed algorithm has a polynomial sample complexity that is lower thanthose of existing algorithms. We also demonstrate that the proposed algorithmnaturally outperforms other existing algorithms when the prior distributionsare not significantly misleading. On the other hand, the proposed algorithmcannot handle greatly misspecified priors as well as the other algorithms can.This is a natural consequence of the fact that the proposed algorithm isgreedier than the other algorithms. Accordingly, we discuss a way to select anappropriate algorithm for different tasks based on the algorithms' greediness.We also introduce a new way of simplifying Bayesian planning, based on whichfuture work would be able to derive new algorithms.
arxiv-3300-218 | Types and forgetfulness in categorical linguistics and quantum mechanics | http://arxiv.org/pdf/1303.3170v1.pdf | author:Peter Hines category:cs.CL math.CT quant-ph published:2013-03-13 summary:The role of types in categorical models of meaning is investigated. A generalscheme for how typed models of meaning may be used to compare sentences,regardless of their grammatical structure is described, and a toy example isused as an illustration. Taking as a starting point the question of whether theevaluation of such a type system 'loses information', we consider theparametrized typing associated with connectives from this viewpoint. The answer to this question implies that, within full categorical models ofmeaning, the objects associated with types must exhibit a simple but subtlecategorical property known as self-similarity. We investigate the categorytheory behind this, with explicit reference to typed systems, and theirmonoidal closed structure. We then demonstrate close connections between suchself-similar structures and dagger Frobenius algebras. In particular, wedemonstrate that the categorical structures implied by the polymorphicallytyped connectives give rise to a (lax unitless) form of the special forms ofFrobenius algebras known as classical structures, used heavily in abstractcategorical approaches to quantum mechanics.
arxiv-3300-219 | Mixed Strategy May Outperform Pure Strategy: An Initial Study | http://arxiv.org/pdf/1303.3154v3.pdf | author:Jun He, Wei Hou, Hongbin Dong, Feidun He category:cs.NE cs.GT published:2013-03-13 summary:In pure strategy meta-heuristics, only one search strategy is applied for alltime. In mixed strategy meta-heuristics, each time one search strategy ischosen from a strategy pool with a probability and then is applied. An exampleis classical genetic algorithms, where either a mutation or crossover operatoris chosen with a probability each time. The aim of this paper is to compare theperformance between mixed strategy and pure strategy meta-heuristic algorithms.First an experimental study is implemented and results demonstrate that mixedstrategy evolutionary algorithms may outperform pure strategy evolutionaryalgorithms on the 0-1 knapsack problem in up to 77.8% instances. ThenComplementary Strategy Theorem is rigorously proven for applying mixed strategyat the population level. The theorem asserts that given two meta-heuristicalgorithms where one uses pure strategy 1 and another uses pure strategy 2, thecondition of pure strategy 2 being complementary to pure strategy 1 issufficient and necessary if there exists a mixed strategy meta-heuristicsderived from these two pure strategies and its expected number of generationsto find an optimal solution is no more than that of using pure strategy 1 forany initial population, and less than that of using pure strategy 1 for someinitial population.
arxiv-3300-220 | Group-Sparse Model Selection: Hardness and Relaxations | http://arxiv.org/pdf/1303.3207v4.pdf | author:Luca Baldassarre, Nirav Bhan, Volkan Cevher, Anastasios Kyrillidis, Siddhartha Satpathi category:cs.LG cs.IT math.IT stat.ML published:2013-03-13 summary:Group-based sparsity models are proven instrumental in linear regressionproblems for recovering signals from much fewer measurements than standardcompressive sensing. The main promise of these models is the recovery of"interpretable" signals through the identification of their constituent groups.In this paper, we establish a combinatorial framework for group-model selectionproblems and highlight the underlying tractability issues. In particular, weshow that the group-model selection problem is equivalent to the well-knownNP-hard weighted maximum coverage problem (WMC). Leveraging a graph-basedunderstanding of group models, we describe group structures which enablecorrect model selection in polynomial time via dynamic programming.Furthermore, group structures that lead to totally unimodular constraints havetractable discrete as well as convex relaxations. We also present ageneralization of the group-model that allows for within group sparsity, whichcan be used to model hierarchical sparsity. Finally, we study the Paretofrontier of group-sparse approximations for two tractable models, among whichthe tree sparsity model, and illustrate selection and computation trade-offsbetween our framework and the existing convex relaxations.
arxiv-3300-221 | Material quality assessment of silk nanofibers based on swarm intelligence | http://arxiv.org/pdf/1303.3152v1.pdf | author:Bruno Brandoli Machado, Wesley Nunes Gonçalves, Odemir Martinez Bruno category:cs.CV published:2013-03-13 summary:In this paper, we propose a novel approach for texture analysis based onartificial crawler model. Our method assumes that each agent can interact withthe environment and each other. The evolution process converges to anequilibrium state according to the set of rules. For each textured image, thefeature vector is composed by signatures of the live agents curve at each time.Experimental results revealed that combining the minimum and maximum signaturesinto one increase the classification rate. In addition, we pioneer the use ofautonomous agents for characterizing silk fibroin scaffolds. The resultsstrongly suggest that our approach can be successfully employed for textureanalysis.
arxiv-3300-222 | Computing Motion with 3D Memristive Grid | http://arxiv.org/pdf/1303.3067v1.pdf | author:Chuan Kai Kenneth. Lim, T. Prodromakis category:cs.CV q-bio.NC published:2013-03-13 summary:Computing the relative motion of objects is an important navigation task thatwe routinely perform by relying on inherently unreliable biological cells inthe retina. The non-linear and adaptive response of memristive devices makethem excellent building blocks for realizing complex synaptic-likearchitectures that are common in the human retina. Here, we introduce a novelmemristive thresholding scheme that facilitates the detection of moving edges.In addition, a double-layered 3-D memristive network is employed for modelingthe motion computations that take place in both the Outer Plexiform Layer (OPL)and Inner Plexiform Layer (IPL) that enables the detection of on-center andoff-center transient responses. Applying the transient detection results, it isshown that it is possible to generate an estimation of the speed and directiona moving object.
arxiv-3300-223 | Iterative MapReduce for Large Scale Machine Learning | http://arxiv.org/pdf/1303.3517v1.pdf | author:Joshua Rosen, Neoklis Polyzotis, Vinayak Borkar, Yingyi Bu, Michael J. Carey, Markus Weimer, Tyson Condie, Raghu Ramakrishnan category:cs.DC cs.DB cs.LG published:2013-03-13 summary:Large datasets ("Big Data") are becoming ubiquitous because the potentialvalue in deriving insights from data, across a wide range of business andscientific applications, is increasingly recognized. In particular, machinelearning - one of the foundational disciplines for data analysis, summarizationand inference - on Big Data has become routine at most organizations thatoperate large clouds, usually based on systems such as Hadoop that support theMapReduce programming paradigm. It is now widely recognized that whileMapReduce is highly scalable, it suffers from a critical weakness for machinelearning: it does not support iteration. Consequently, one has to programaround this limitation, leading to fragile, inefficient code. Further, relianceon the programmer is inherently flawed in a multi-tenanted cloud environment,since the programmer does not have visibility into the state of the system whenhis or her program executes. Prior work has sought to address this problem byeither developing specialized systems aimed at stylized applications, or byaugmenting MapReduce with ad hoc support for saving state across iterations(driven by an external loop). In this paper, we advocate support for looping asa first-class construct, and propose an extension of the MapReduce programmingparadigm called {\em Iterative MapReduce}. We then develop an optimizer for aclass of Iterative MapReduce programs that cover most machine learningtechniques, provide theoretical justifications for the key optimization steps,and empirically demonstrate that system-optimized programs for significantmachine learning tasks are competitive with state-of-the-art specializedsolutions.
arxiv-3300-224 | An Entropy-based Learning Algorithm of Bayesian Conditional Trees | http://arxiv.org/pdf/1303.5403v1.pdf | author:Dan Geiger category:cs.LG cs.AI cs.CV published:2013-03-13 summary:This article offers a modification of Chow and Liu's learning algorithm inthe context of handwritten digit recognition. The modified algorithm directsthe user to group digits into several classes consisting of digits that arehard to distinguish and then constructing an optimal conditional treerepresentation for each class of digits instead of for each single digit asdone by Chow and Liu (1968). Advantages and extensions of the new method arediscussed. Related works of Wong and Wang (1977) and Wong and Poon (1989) whichoffer a different entropy-based learning algorithm are shown to rest oninappropriate assumptions.
arxiv-3300-225 | Statistical Texture Features based Handwritten and Printed Text Classification in South Indian Documents | http://arxiv.org/pdf/1303.3087v1.pdf | author:Mallikarjun Hangarge, K. C. Santosh, Srikanth Doddamani, Rajmohan Pardeshi category:cs.CV published:2013-03-13 summary:In this paper, we use statistical texture features for handwritten andprinted text classification. We primarily aim for word level classification insouth Indian scripts. Words are first extracted from the scanned document. Foreach extracted word, statistical texture features are computed such as mean,standard deviation, smoothness, moment, uniformity, entropy and local rangeincluding local entropy. These feature vectors are then used to classify wordsvia k-NN classifier. We have validated the approach over several differentdatasets. Scripts like Kannada, Telugu, Malayalam and Hindi i.e., Devanagariare primarily employed where an average classification rate of 99.26% isachieved. In addition, to provide an extensibility of the approach, we addressRoman script by using publicly available dataset and interesting results arereported.
arxiv-3300-226 | On Improving Energy Efficiency within Green Femtocell Networks: A Hierarchical Reinforcement Learning Approach | http://arxiv.org/pdf/1303.4638v1.pdf | author:Xianfu Chen, Honggang Zhang, Tao Chen, Mika Lasanen, Jacques Palicot category:cs.LG cs.GT published:2013-03-13 summary:One of the efficient solutions of improving coverage and increasing capacityin cellular networks is the deployment of femtocells. As the cellular networksare becoming more complex, energy consumption of whole network infrastructureis becoming important in terms of both operational costs and environmentalimpacts. This paper investigates energy efficiency of two-tier femtocellnetworks through combining game theory and stochastic learning. With theStackelberg game formulation, a hierarchical reinforcement learning frameworkis applied for studying the joint expected utility maximization of macrocellsand femtocells subject to the minimum signal-to-interference-plus-noise-ratiorequirements. In the learning procedure, the macrocells act as leaders and thefemtocells are followers. At each time step, the leaders commit to dynamicstrategies based on the best responses of the followers, while the followerscompete against each other with no further information but the leaders'transmission parameters. In this paper, we propose two reinforcement learningbased intelligent algorithms to schedule each cell's stochastic power levels.Numerical experiments are presented to validate the investigations. The resultsshow that the two learning algorithms substantially improve the energyefficiency of the femtocell networks.
arxiv-3300-227 | Egocentric vision IT technologies for Alzheimer disease assessment and studies | http://arxiv.org/pdf/1303.3134v1.pdf | author:Hugo Boujut, Vincent Buso, Guillaume Bourmaud, Jenny Benois-Pineau, Rémi Mégret, Jean-Philippe Domenger, Yann Gaëstel, Jean-François Dartigues category:cs.HC cs.CV published:2013-03-13 summary:Egocentric vision technology consists in capturing the actions of personsfrom their own visual point of view using wearable camera sensors. We applythis new paradigm to instrumental activities monitoring with the objective ofproviding new tools for the clinical evaluation of the impact of the disease onpersons with dementia. In this paper, we introduce the current state of thedevelopment of this technology and focus on two technology modules: automaticlocation estimation and visual saliency estimation for content interpretation.
arxiv-3300-228 | Machine Learning for Bioclimatic Modelling | http://arxiv.org/pdf/1303.2739v1.pdf | author:Maumita Bhattacharya category:cs.LG stat.AP 97R30 published:2013-03-12 summary:Many machine learning (ML) approaches are widely used to generate bioclimaticmodels for prediction of geographic range of organism as a function of climate.Applications such as prediction of range shift in organism, range of invasivespecies influenced by climate change are important parameters in understandingthe impact of climate change. However, success of machine learning-basedapproaches depends on a number of factors. While it can be safely said that noparticular ML technique can be effective in all applications and success of atechnique is predominantly dependent on the application or the type of theproblem, it is useful to understand their behavior to ensure informed choice oftechniques. This paper presents a comprehensive review of machinelearning-based bioclimatic model generation and analyses the factorsinfluencing success of such models. Considering the wide use of statisticaltechniques, in our discussion we also include conventional statisticaltechniques used in bioclimatic modelling.
arxiv-3300-229 | Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions | http://arxiv.org/pdf/1303.3055v1.pdf | author:Yasin Abbasi-Yadkori, Peter L. Bartlett, Csaba Szepesvari category:cs.LG stat.ML published:2013-03-12 summary:We study the problem of learning Markov decision processes with finite stateand action spaces when the transition probability distributions and lossfunctions are chosen adversarially and are allowed to change with time. Weintroduce an algorithm whose regret with respect to any policy in a comparisonclass grows as the square root of the number of rounds of the game, providedthe transition probabilities satisfy a uniform mixing condition. Our approachis efficient as long as the comparison class is polynomial and we can computeexpectations over sample paths for each policy. Designing an efficientalgorithm with small regret for the general case remains an open problem.
arxiv-3300-230 | Evolutionary Approaches to Expensive Optimisation | http://arxiv.org/pdf/1303.2745v1.pdf | author:Maumita Bhattacharya category:cs.NE 97R40 published:2013-03-12 summary:Surrogate assisted evolutionary algorithms (EA) are rapidly gainingpopularity where applications of EA in complex real world problem domains areconcerned. Although EAs are powerful global optimizers, finding optimalsolution to complex high dimensional, multimodal problems often require veryexpensive fitness function evaluations. Needless to say, this could brand anypopulation-based iterative optimization technique to be the most cripplingchoice to handle such problems. Use of approximate model or surrogates providesa much cheaper option. However, naturally this cheaper option comes with itsown price. This paper discusses some of the key issues involved with use ofapproximation in evolutionary algorithm, possible best practices and solutions.Answers to the following questions have been sought: what type of fitnessapproximation to be used; which approximation model to use; how to integratethe approximation model in EA; how much approximation to use; and how to ensurereliable approximation.
arxiv-3300-231 | Type-theoretical natural language semantics: on the system F for meaning assembly | http://arxiv.org/pdf/1303.3036v1.pdf | author:Christian Retoré category:cs.LO cs.CL math.LO published:2013-03-12 summary:This paper presents and extends our type theoretical framework for acompositional treatment of natural language semantics with some lexicalfeatures like coercions (e.g. of a town into a football club) and copredication(e.g. on a town as a set of people and as a location). The second order typedlambda calculus was shown to be a good framework, and here we discuss how tointroduced predefined types and coercive subtyping which are much more naturalthan internally coded similar constructs. Linguistic applications of these newfeatures are also exemplified.
arxiv-3300-232 | Toward Optimal Stratification for Stratified Monte-Carlo Integration | http://arxiv.org/pdf/1303.2892v1.pdf | author:Alexandra Carpentier, Remi Munos category:stat.ML published:2013-03-12 summary:We consider the problem of adaptive stratified sampling for Monte Carlointegration of a noisy function, given a finite budget n of noisy evaluationsto the function. We tackle in this paper the problem of adapting to thefunction at the same time the number of samples into each stratum and thepartition itself. More precisely, it is interesting to refine the partition ofthe domain in area where the noise to the function, or where the variations ofthe function, are very heterogeneous. On the other hand, having a (too) refinedstratification is not optimal. Indeed, the more refined the stratification, themore difficult it is to adjust the allocation of the samples to thestratification, i.e. sample more points where the noise or variations of thefunction are larger. We provide in this paper an algorithm that selects online,among a large class of partitions, the partition that provides the optimaltrade-off, and allocates the samples almost optimally on this partition.
arxiv-3300-233 | A Stochastic Grammar for Natural Shapes | http://arxiv.org/pdf/1303.2844v1.pdf | author:Pedro F. Felzenszwalb category:cs.CV published:2013-03-12 summary:We consider object detection using a generic model for natural shapes. Acommon approach for object recognition involves matching object models directlyto images. Another approach involves building intermediate representations viaa generic grouping processes. We argue that these two processes (model-basedrecognition and grouping) may use similar computational mechanisms. By defininga generic model for shapes we can use model-based techniques to implement amid-level vision grouping process.
arxiv-3300-234 | Probabilistic Topic and Syntax Modeling with Part-of-Speech LDA | http://arxiv.org/pdf/1303.2826v1.pdf | author:William M. Darling, Fei Song category:cs.CL published:2013-03-12 summary:This article presents a probabilistic generative model for text based onsemantic topics and syntactic classes called Part-of-Speech LDA (POSLDA).POSLDA simultaneously uncovers short-range syntactic patterns (syntax) andlong-range semantic patterns (topics) that exist in document collections. Thisresults in word distributions that are specific to both topics (sports,education, ...) and parts-of-speech (nouns, verbs, ...). For example,multinomial distributions over words are uncovered that can be understood as"nouns about weather" or "verbs about law". We describe the model and anapproximate inference algorithm and then demonstrate the quality of the learnedtopics both qualitatively and quantitatively. Then, we discuss an NLPapplication where the output of POSLDA can lead to strong improvements inquality: unsupervised part-of-speech tagging. We describe algorithms for thistask that make use of POSLDA-learned distributions that result in improvedperformance beyond the state of the art.
arxiv-3300-235 | Linear system identification using stable spline kernels and PLQ penalties | http://arxiv.org/pdf/1303.2827v1.pdf | author:Aleksandr Y. Aravkin, James V. Burke, Gianluigi Pillonetto category:stat.ML math.OC stat.CO 47N30, 65K10 published:2013-03-12 summary:The classical approach to linear system identification is given by parametricPrediction Error Methods (PEM). In this context, model complexity is oftenunknown so that a model order selection step is needed to suitably trade-offbias and variance. Recently, a different approach to linear systemidentification has been introduced, where model order determination is avoidedby using a regularized least squares framework. In particular, the penalty termon the impulse response is defined by so called stable spline kernels. Theyembed information on regularity and BIBO stability, and depend on a smallnumber of parameters which can be estimated from data. In this paper, weprovide new nonsmooth formulations of the stable spline estimator. Inparticular, we consider linear system identification problems in a very broadcontext, where regularization functionals and data misfits can come from a richset of piecewise linear quadratic functions. Moreover, our anal- ysis includespolyhedral inequality constraints on the unknown impulse response. For anyformulation in this class, we show that interior point methods can be used tosolve the system identification problem, with complexity O(n3)+O(mn2) in eachiteration, where n and m are the number of impulse response coefficients andmeasurements, respectively. The usefulness of the framework is illustrated viaa numerical experiment where output measurements are contaminated by outliers.
arxiv-3300-236 | A Cooperative Q-learning Approach for Real-time Power Allocation in Femtocell Networks | http://arxiv.org/pdf/1303.2789v1.pdf | author:Hussein Saad, Amr Mohamed, Tamer ElBatt category:cs.MA cs.LG published:2013-03-12 summary:In this paper, we address the problem of distributed interference managementof cognitive femtocells that share the same frequency range with macrocells(primary user) using distributed multi-agent Q-learning. We formulate and solvethree problems representing three different Q-learning algorithms: namely,centralized, distributed and partially distributed power control usingQ-learning (CPC-Q, DPC-Q and PDPC-Q). CPCQ, although not of practical interest,characterizes the global optimum. Each of DPC-Q and PDPC-Q works in twodifferent learning paradigms: Independent (IL) and Cooperative (CL). The formeris considered the simplest form for applying Qlearning in multi-agentscenarios, where all the femtocells learn independently. The latter is theproposed scheme in which femtocells share partial information during thelearning process in order to strike a balance between practical relevance andperformance. In terms of performance, the simulation results showed that the CLparadigm outperforms the IL paradigm and achieves an aggregate femtocellscapacity that is very close to the optimal one. For the practical relevanceissue, we evaluate the robustness and scalability of DPC-Q, in real time, bydeploying new femtocells in the system during the learning process, where weshowed that DPC-Q in the CL paradigm is scalable to large number of femtocellsand more robust to the network dynamics compared to the IL paradigm
arxiv-3300-237 | Combined Learning of Salient Local Descriptors and Distance Metrics for Image Set Face Verification | http://arxiv.org/pdf/1303.2783v1.pdf | author:Conrad Sanderson, Mehrtash T. Harandi, Yongkang Wong, Brian C. Lovell category:cs.CV published:2013-03-12 summary:In contrast to comparing faces via single exemplars, matching sets of faceimages increases robustness and discrimination performance. Recent image setmatching approaches typically measure similarities between subspaces ormanifolds, while representing faces in a rigid and holistic manner. Suchrepresentations are easily affected by variations in terms of alignment,illumination, pose and expression. While local feature based representationsare considerably more robust to such variations, they have received littleattention within the image set matching area. We propose a novel image setmatching technique, comprised of three aspects: (i) robust descriptors of faceregions based on local features, partly inspired by the hierarchy in the humanvisual system, (ii) use of several subspace and exemplar metrics to comparecorresponding face regions, (iii) jointly learning which regions are the mostdiscriminative while finding the optimal mixing weights for combining metrics.Face recognition experiments on LFW, PIE and MOBIO face datasets show that theproposed algorithm obtains considerably better performance than several recentstate-of-the-art techniques, such as Local Principal Angle and the KernelAffine Hull Method.
arxiv-3300-238 | Gaussian Mixture Model for Handwritten Script Identification | http://arxiv.org/pdf/1303.2751v1.pdf | author:Mallikarjun Hangarge category:cs.CV published:2013-03-12 summary:This paper presents a Gaussian Mixture Model (GMM) to identify the script ofhandwritten words of Roman, Devanagari, Kannada and Telugu scripts. Itemphasizes the significance of directional energies for identification ofscript of the word. It is robust to varied image sizes and different styles ofwriting. A GMM is modeled using a set of six novel features derived fromdirectional energy distributions of the underlying image. The standarddeviation of directional energy distributions are computed by decomposing animage matrix into right and left diagonals. Furthermore, deviation ofhorizontal and vertical distributions of energies is also built-in to GMM. Adataset of 400 images out of 800 (200 of each script) are used for training GMMand the remaining is for testing. An exhaustive experimentation is carried outat bi-script, tri-script and multi-script level and achieved scriptidentification accuracies in percentage as 98.7, 98.16 and 96.91 respectively.
arxiv-3300-239 | Gaussian Processes for Nonlinear Signal Processing | http://arxiv.org/pdf/1303.2823v2.pdf | author:Fernando Pérez-Cruz, Steven Van Vaerenbergh, Juan José Murillo-Fuentes, Miguel Lázaro-Gredilla, Ignacio Santamaria category:cs.LG cs.IT math.IT stat.ML published:2013-03-12 summary:Gaussian processes (GPs) are versatile tools that have been successfullyemployed to solve nonlinear estimation problems in machine learning, but thatare rarely used in signal processing. In this tutorial, we present GPs forregression as a natural nonlinear extension to optimal Wiener filtering. Afterestablishing their basic formulation, we discuss several important aspects andextensions, including recursive and adaptive algorithms for dealing withnon-stationarity, low-complexity solutions, non-Gaussian noise models andclassification scenarios. Furthermore, we provide a selection of relevantapplications to wireless digital communications.
arxiv-3300-240 | Integrated Pre-Processing for Bayesian Nonlinear System Identification with Gaussian Processes | http://arxiv.org/pdf/1303.2912v3.pdf | author:Roger Frigola, Carl Edward Rasmussen category:cs.AI cs.RO cs.SY stat.ML published:2013-03-12 summary:We introduce GP-FNARX: a new model for nonlinear system identification basedon a nonlinear autoregressive exogenous model (NARX) with filtered regressors(F) where the nonlinear regression problem is tackled using sparse Gaussianprocesses (GP). We integrate data pre-processing with system identificationinto a fully automated procedure that goes from raw data to an identifiedmodel. Both pre-processing parameters and GP hyper-parameters are tuned bymaximizing the marginal likelihood of the probabilistic model. We obtain aBayesian model of the system's dynamics which is able to report its uncertaintyin regions where the data is scarce. The automated approach, the modeling ofuncertainty and its relatively low computational cost make of GP-FNARX a goodcandidate for applications in robotics and adaptive control.
arxiv-3300-241 | Toggling a Genetic Switch Using Reinforcement Learning | http://arxiv.org/pdf/1303.3183v2.pdf | author:Aivar Sootla, Natalja Strelkowa, Damien Ernst, Mauricio Barahona, Guy-Bart Stan category:cs.SY cs.CE cs.LG q-bio.MN published:2013-03-12 summary:In this paper, we consider the problem of optimal exogenous control of generegulatory networks. Our approach consists in adapting an establishedreinforcement learning algorithm called the fitted Q iteration. This algorithminfers the control law directly from the measurements of the system's responseto external control inputs without the use of a mathematical model of thesystem. The measurement data set can either be collected from wet-labexperiments or artificially created by computer simulations of dynamical modelsof the system. The algorithm is applicable to a wide range of biologicalsystems due to its ability to deal with nonlinear and stochastic systemdynamics. To illustrate the application of the algorithm to a gene regulatorynetwork, the regulation of the toggle switch system is considered. The controlobjective of this problem is to drive the concentrations of two specificproteins to a target region in the state space.
arxiv-3300-242 | Recovering Non-negative and Combined Sparse Representations | http://arxiv.org/pdf/1303.4694v2.pdf | author:Karthikeyan Natesan Ramamurthy, Jayaraman J. Thiagarajan, Andreas Spanias category:math.NA cs.LG stat.ML published:2013-03-12 summary:The non-negative solution to an underdetermined linear system can be uniquelyrecovered sometimes, even without imposing any additional sparsity constraints.In this paper, we derive conditions under which a unique non-negative solutionfor such a system can exist, based on the theory of polytopes. Furthermore, wedevelop the paradigm of combined sparse representations, where only a part ofthe coefficient vector is constrained to be non-negative, and the rest isunconstrained (general). We analyze the recovery of the unique, sparsestsolution, for combined representations, under three different cases ofcoefficient support knowledge: (a) the non-zero supports of non-negative andgeneral coefficients are known, (b) the non-zero support of generalcoefficients alone is known, and (c) both the non-zero supports are unknown.For case (c), we propose the combined orthogonal matching pursuit algorithm forcoefficient recovery and derive the deterministic sparsity threshold underwhich recovery of the unique, sparsest coefficient vector is possible. Wequantify the order complexity of the algorithms, and examine their performancein exact and approximate recovery of coefficients under various conditions ofnoise. Furthermore, we also obtain their empirical phase transitioncharacteristics. We show that the basis pursuit algorithm, with partialnon-negative constraints, and the proposed greedy algorithm perform better inrecovering the unique sparse representation when compared to theirunconstrained counterparts. Finally, we demonstrate the utility of the proposedmethods in recovering images corrupted by saturation noise.
arxiv-3300-243 | Classification of Segments in PolSAR Imagery by Minimum Stochastic Distances Between Wishart Distributions | http://arxiv.org/pdf/1303.2108v1.pdf | author:Wagner Barreto da Silva, Corina da Costa Freitas, Sidnei João Siqueira Sant'Anna, Alejandro C. Frery category:cs.CV stat.AP published:2013-03-11 summary:A new classifier for Polarimetric SAR (PolSAR) images is proposed andassessed in this paper. Its input consists of segments, and each one isassigned the class which minimizes a stochastic distance. Assuming the complexWishart model, several stochastic distances are obtained from the h-phi familyof divergences, and they are employed to derive hypothesis test statistics thatare also used in the classification process. This article also presents, as anovelty, analytic expressions for the test statistics based on the followingstochastic distances between complex Wishart models: Kullback-Leibler,Bhattacharyya, Hellinger, R\'enyi, and Chi-Square; also, the test statisticbased on the Bhattacharyya distance between multivariate Gaussian distributionsis presented. The classifier performance is evaluated using simulated and realPolSAR data. The simulated data are based on the complex Wishart model, aimingat the analysis of the proposal well controlled data. The real data refer tothe complex L-band image, acquired during the 1994 SIR-C mission. The resultsof the proposed classifier are compared with those obtained by a Wishartper-pixel/contextual classifier, and we show the better performance of theregion-based classification. The influence of the statistical modeling isassessed by comparing the results using the Bhattacharyya distance betweenmultivariate Gaussian distributions for amplitude data. The results withsimulated data indicate that the proposed classification method has a very goodperformance when the data follow the Wishart model. The proposed classifieralso performs better than the per-pixel/contextual classifier and theBhattacharyya Gaussian distance using SIR-C PolSAR data.
arxiv-3300-244 | Visualizing and Interacting with Concept Hierarchies | http://arxiv.org/pdf/1303.2488v1.pdf | author:Michel Crampes, Michel Plantié category:stat.ML published:2013-03-11 summary:Concept Hierarchies and Formal Concept Analysis are theoretically wellgrounded and largely experimented methods. They rely on line diagrams calledGalois lattices for visualizing and analysing object-attribute sets. Galoislattices are visually seducing and conceptually rich for experts. However theypresent important drawbacks due to their concept oriented overall structure:analysing what they show is difficult for non experts, navigation iscumbersome, interaction is poor, and scalability is a deep bottleneck forvisual interpretation even for experts. In this paper we introduce semanticprobes as a means to overcome many of these problems and extend usability andapplication possibilities of traditional FCA visualization methods. Semanticprobes are visual user centred objects which extract and organize reducedGalois sub-hierarchies. They are simpler, clearer, and they provide a betternavigation support through a rich set of interaction possibilities. Since probedriven sub-hierarchies are limited to users focus, scalability is under controland interpretation is facilitated. After some successful experiments, severalapplications are being developed with the remaining problem of finding acompromise between simplicity and conceptual expressivity.
arxiv-3300-245 | Spectral Clustering with Epidemic Diffusion | http://arxiv.org/pdf/1303.2663v2.pdf | author:Laura M. Smith, Kristina Lerman, Cristina Garcia-Cardona, Allon G. Percus, Rumi Ghosh category:cs.SI cs.LG physics.soc-ph stat.ML I.5.3 published:2013-03-11 summary:Spectral clustering is widely used to partition graphs into distinct modulesor communities. Existing methods for spectral clustering use the eigenvaluesand eigenvectors of the graph Laplacian, an operator that is closely associatedwith random walks on graphs. We propose a new spectral partitioning method thatexploits the properties of epidemic diffusion. An epidemic is a dynamic processthat, unlike the random walk, simultaneously transitions to all the neighborsof a given node. We show that the replicator, an operator describing epidemicdiffusion, is equivalent to the symmetric normalized Laplacian of a reweightedgraph with edges reweighted by the eigenvector centralities of their incidentnodes. Thus, more weight is given to edges connecting more central nodes. Wedescribe a method that partitions the nodes based on the componentwise ratio ofthe replicator's second eigenvector to the first, and compare its performanceto traditional spectral clustering techniques on synthetic graphs with knowncommunity structure. We demonstrate that the replicator gives preference todense, clique-like structures, enabling it to more effectively discovercommunities that may be obscured by dense intercommunity linking.
arxiv-3300-246 | Linear NDCG and Pair-wise Loss | http://arxiv.org/pdf/1303.2417v1.pdf | author:Xiao-Bo Jin, Guang-Gang Geng category:cs.LG stat.ML published:2013-03-11 summary:Linear NDCG is used for measuring the performance of the Web content qualityassessment in ECML/PKDD Discovery Challenge 2010. In this paper, we will provethat the DCG error equals a new pair-wise loss.
arxiv-3300-247 | Kernel Sparse Models for Automated Tumor Segmentation | http://arxiv.org/pdf/1303.2610v1.pdf | author:Jayaraman J. Thiagarajan, Karthikeyan Natesan Ramamurthy, Deepta Rajan, Anup Puri, David Frakes, Andreas Spanias category:cs.CV published:2013-03-11 summary:In this paper, we propose sparse coding-based approaches for segmentation oftumor regions from MR images. Sparse coding with data-adapted dictionaries hasbeen successfully employed in several image recovery and vision problems. Theproposed approaches obtain sparse codes for each pixel in brain magneticresonance images considering their intensity values and location information.Since it is trivial to obtain pixel-wise sparse codes, and combining multiplefeatures in the sparse coding setup is not straightforward, we propose toperform sparse coding in a high-dimensional feature space where non-linearsimilarities can be effectively modeled. We use the training data fromexpert-segmented images to obtain kernel dictionaries with the kernel K-linesclustering procedure. For a test image, sparse codes are computed with thesekernel dictionaries, and they are used to identify the tumor regions. Thisapproach is completely automated, and does not require user intervention toinitialize the tumor regions in a test image. Furthermore, a low complexitysegmentation approach based on kernel sparse codes, which allows the user toinitialize the tumor region, is also presented. Results obtained with both theproposed approaches are validated against manual segmentation by an expertradiologist, and the proposed methods lead to accurate tumor identification.
arxiv-3300-248 | Bilateral Filter: Graph Spectral Interpretation and Extensions | http://arxiv.org/pdf/1303.2685v1.pdf | author:Akshay Gadde, Sunil K Narang, Antonio Ortega category:cs.CV published:2013-03-11 summary:In this paper we study the bilateral filter proposed by Tomasi and Manduchi,as a spectral domain transform defined on a weighted graph. The nodes of thisgraph represent the pixels in the image and a graph signal defined on the nodesrepresents the intensity values. Edge weights in the graph correspond to thebilateral filter coefficients and hence are data adaptive. Spectrum of a graphis defined in terms of the eigenvalues and eigenvectors of the graph Laplacianmatrix. We use this spectral interpretation to generalize the bilateral filterand propose more flexible and application specific spectral designs ofbilateral-like filters. We show that these spectral filters can be implementedwith k-iterative bilateral filtering operations and do not require expensivediagonalization of the Laplacian matrix.
arxiv-3300-249 | Joint optimization of fitting & matching in multi-view reconstruction | http://arxiv.org/pdf/1303.2607v2.pdf | author:Hossam Isack, Yuri Boykov category:cs.CV published:2013-03-11 summary:Many standard approaches for geometric model fitting are based on pre-matchedimage features. Typically, such pre-matching uses only feature appearances(e.g. SIFT) and a large number of non-unique features must be discarded inorder to control the false positive rate. In contrast, we solve featurematching and multi-model fitting problems in a joint optimization framework.This paper proposes several fit-&-match energy formulations based on ageneralization of the assignment problem. We developed an efficient solverbased on min-cost-max-flow algorithm that finds near optimal solutions. Ourapproach significantly increases the number of detected matches. In practice,energy-based joint fitting & matching allows to increase the distance betweenview-points previously restricted by robustness of local SIFT-matching and toimprove the model fitting accuracy when compared to state-of-the-artmulti-model fitting techniques.
arxiv-3300-250 | A Low-Complexity Algorithm for Static Background Estimation from Cluttered Image Sequences in Surveillance Contexts | http://arxiv.org/pdf/1303.2465v1.pdf | author:Vikas Reddy, Conrad Sanderson, Brian C. Lovell category:cs.CV published:2013-03-11 summary:For the purposes of foreground estimation, the true background model isunavailable in many practical circumstances and needs to be estimated fromcluttered image sequences. We propose a sequential technique for staticbackground estimation in such conditions, with low computational and memoryrequirements. Image sequences are analysed on a block-by-block basis. For eachblock location a representative set is maintained which contains distinctblocks obtained along its temporal line. The background estimation is carriedout in a Markov Random Field framework, where the optimal labelling solution iscomputed using iterated conditional modes. The clique potentials are computedbased on the combined frequency response of the candidate block and itsneighbourhood. It is assumed that the most appropriate block results in thesmoothest response, indirectly enforcing the spatial continuity of structureswithin a scene. Experiments on real-life surveillance videos demonstrate thatthe proposed method obtains considerably better background estimates (bothqualitatively and quantitatively) than median filtering and the recentlyproposed "intervals of stable intensity" method. Further experiments on theWallflower dataset suggest that the combination of the proposed method with aforeground segmentation algorithm results in improved foreground segmentation.
arxiv-3300-251 | Quantum and Concept Combination, Entangled Measurements and Prototype Theory | http://arxiv.org/pdf/1303.2430v2.pdf | author:Diederik Aerts category:cs.AI cs.CL quant-ph published:2013-03-11 summary:We analyze the meaning of the violation of the marginal probability law forsituations of correlation measurements where entanglement is identified. Weshow that for quantum theory applied to the cognitive realm such a violationdoes not lead to the type of problems commonly believed to occur in situationsof quantum theory applied to the physical realm. We briefly situate our quantumapproach for modeling concepts and their combinations with respect to thenotions of 'extension' and 'intension' in theories of meaning, and in existingconcept theories.
arxiv-3300-252 | Using qualia information to identify lexical semantic classes in an unsupervised clustering task | http://arxiv.org/pdf/1303.2449v1.pdf | author:Lauren Romeo, Sara Mendes, Núria Bel category:cs.CL published:2013-03-11 summary:Acquiring lexical information is a complex problem, typically approached byrelying on a number of contexts to contribute information for classification.One of the first issues to address in this domain is the determination of suchcontexts. The work presented here proposes the use of automatically obtainedFORMAL role descriptors as features used to draw nouns from the same lexicalsemantic class together in an unsupervised clustering task. We have dealt withthree lexical semantic classes (HUMAN, LOCATION and EVENT) in English. Theresults obtained show that it is possible to discriminate between elements fromdifferent lexical semantic classes using only FORMAL role information, hencevalidating our initial hypothesis. Also, iterating our method accuratelyaccounts for fine-grained distinctions within lexical classes, namelydistinctions involving ambiguous expressions. Moreover, a filtering andbootstrapping strategy employed in extracting FORMAL role descriptors proved tominimize effects of sparse data and noise in our task.
arxiv-3300-253 | Automatic Detection of Non-deverbal Event Nouns for Quick Lexicon Production | http://arxiv.org/pdf/1303.2448v1.pdf | author:Núria Bel, Maria Coll, Gabriela Resnik category:cs.CL published:2013-03-11 summary:In this work we present the results of our experimental work on thedevelop-ment of lexical class-based lexica by automatic means. The objective isto as-sess the use of linguistic lexical-class based information as a featureselection methodology for the use of classifiers in quick lexical development.The results show that the approach can help in re-ducing the human effortrequired in the development of language resources sig-nificantly.
arxiv-3300-254 | A Multilingual Semantic Wiki Based on Attempto Controlled English and Grammatical Framework | http://arxiv.org/pdf/1303.4293v1.pdf | author:Kaarel Kaljurand, Tobias Kuhn category:cs.CL cs.HC published:2013-03-11 summary:We describe a semantic wiki system with an underlying controlled naturallanguage grammar implemented in Grammatical Framework (GF). The grammarrestricts the wiki content to a well-defined subset of Attempto ControlledEnglish (ACE), and facilitates a precise bidirectional automatic translationbetween ACE and language fragments of a number of other natural languages,making the wiki content accessible multilingually. Additionally, our approachallows for automatic translation into the Web Ontology Language (OWL), whichenables automatic reasoning over the wiki content. The developed wikienvironment thus allows users to build, query and view OWL knowledge bases viaa user-friendly multilingual natural language interface. As a further feature,the underlying multilingual grammar is integrated into the wiki and can becollaboratively edited to extend the vocabulary of the wiki or even customizeits sentence structures. This work demonstrates the combination of the existingtechnologies of Attempto Controlled English and Grammatical Framework, and isimplemented as an extension of the existing semantic wiki engine AceWiki.
arxiv-3300-255 | Improved Performance of Unsupervised Method by Renovated K-Means | http://arxiv.org/pdf/1304.0725v1.pdf | author:P. Ashok, G. M Kadhar Nawaz, E. Elayaraja, V. Vadivel category:cs.LG cs.CV stat.ML published:2013-03-11 summary:Clustering is a separation of data into groups of similar objects. Everygroup called cluster consists of objects that are similar to one another anddissimilar to objects of other groups. In this paper, the K-Means algorithm isimplemented by three distance functions and to identify the optimal distancefunction for clustering methods. The proposed K-Means algorithm is comparedwith K-Means, Static Weighted K-Means (SWK-Means) and Dynamic Weighted K-Means(DWK-Means) algorithm by using Davis Bouldin index, Execution Time andIteration count methods. Experimental results show that the proposed K-Meansalgorithm performed better on Iris and Wine dataset when compared with otherthree clustering methods.
arxiv-3300-256 | Least-Squares FIR Models of Low-Resolution MR data for Efficient Phase-Error Compensation with Simultaneous Artefact Removal | http://arxiv.org/pdf/1303.2437v1.pdf | author:Joseph Suresh Paul, Uma Krishna Swamy Pillai, Nyjin Thomas category:cs.CV published:2013-03-11 summary:Signal space models in both phase-encode, and frequency-encode directions arepresented for extrapolation of 2D partial kspace. Using the boxcarrepresentation of low-resolution spatial data, and a geometrical representationof signal space vectors in both positive and negative phase-encode directions,a robust predictor is constructed using a series of signal space projections.Compared to some of the existing phase-correction methods that requireacquisition of a pre-determined set of fractional kspace lines, the proposedpredictor is found to be more efficient, due to its capability of exhibiting anequivalent degree of performance using only half the number of fractionallines. Robust filtering of noisy data is achieved using a second signal spacemodel in the frequency-encode direction, bypassing the requirement of a priorhighpass filtering operation. The signal space is constructed from FourierTransformed samples of each row in the low-resolution image. A set of FIRfilters are estimated by fitting a least squares model to this signal space.Partial kspace extrapolation using the FIR filters is shown to result inartifact-free reconstruction, particularly in respect of Gibbs ringing andstreaking type artifacts.
arxiv-3300-257 | Revealing Cluster Structure of Graph by Path Following Replicator Dynamic | http://arxiv.org/pdf/1303.2643v1.pdf | author:Hairong Liu, Longin Jan Latecki, Shuicheng Yan category:cs.LG cs.GT published:2013-03-11 summary:In this paper, we propose a path following replicator dynamic, andinvestigate its potentials in uncovering the underlying cluster structure of agraph. The proposed dynamic is a generalization of the discrete replicatordynamic. The replicator dynamic has been successfully used to extract denseclusters of graphs; however, it is often sensitive to the degree distributionof a graph, and usually biased by vertices with large degrees, thus may fail todetect the densest cluster. To overcome this problem, we introduce a dynamicparameter, called path parameter, into the evolution process. The pathparameter can be interpreted as the maximal possible probability of a currentcluster containing a vertex, and it monotonically increases as evolutionprocess proceeds. By limiting the maximal probability, the phenomenon of somevertices dominating the early stage of evolution process is suppressed, thusmaking evolution process more robust. To solve the optimization problem with afixed path parameter, we propose an efficient fixed point algorithm. The timecomplexity of the path following replicator dynamic is only linear in thenumber of edges of a graph, thus it can analyze graphs with millions ofvertices and tens of millions of edges on a common PC in a few minutes.Besides, it can be naturally generalized to hypergraph and graph with edges ofdifferent orders. We apply it to four important problems: maximum cliqueproblem, densest k-subgraph problem, structure fitting, and discovery ofhigh-density regions. The extensive experimental results clearly demonstrateits advantages, in terms of robustness, scalability and flexility.
arxiv-3300-258 | Voxel-wise Weighted MR Image Enhancement using an Extended Neighborhood Filter | http://arxiv.org/pdf/1303.2439v1.pdf | author:Joseph Suresh Paul, Joshin John Mathew, Souparnika Kandoth Naroth, Chandrasekar Kesavadas category:cs.CV published:2013-03-11 summary:We present an edge preserving and denoising filter for enhancing the featuresin images, which contain an ROI having a narrow spatial extent. Typicalexamples include angiograms, or ROI spatially distributed in multiple locationsand contained within an outlying region, such as in multiple-sclerosis. Thefiltering involves determination of multiplicative weights in the spatialdomain using an extended set of neighborhood directions. Equivalently, thefiltering operation may be interpreted as a combination of directional filtersin the frequency domain, with selective weighting for spatial frequenciescontained within each direction. The advantages of the proposed filter incomparison to specialized non-linear filters, which operate on diffusionprinciple, are illustrated using numerical phantom data. The performanceevaluation is carried out on simulated images from BrainWeb database formultiple-sclerosis, acute ischemic stroke using clinically acquired FLAIRimages and MR angiograms.
arxiv-3300-259 | Monte-Carlo utility estimates for Bayesian reinforcement learning | http://arxiv.org/pdf/1303.2506v1.pdf | author:Christos Dimitrakakis category:cs.LG stat.ML published:2013-03-11 summary:This paper introduces a set of algorithms for Monte-Carlo Bayesianreinforcement learning. Firstly, Monte-Carlo estimation of upper bounds on theBayes-optimal value function is employed to construct an optimistic policy.Secondly, gradient-based algorithms for approximate upper and lower bounds areintroduced. Finally, we introduce a new class of gradient algorithms forBayesian Bellman error minimisation. We theoretically show that the gradientmethods are sound. Experimentally, we demonstrate the superiority of the upperbound method in terms of reward obtained. However, we also show that theBayesian Bellman error method is a close second, despite its significantcomputational simplicity.
arxiv-3300-260 | Refinement revisited with connections to Bayes error, conditional entropy and calibrated classifiers | http://arxiv.org/pdf/1303.2517v1.pdf | author:Hamed Masnadi-Shirazi category:stat.ML published:2013-03-11 summary:The concept of refinement from probability elicitation is considered forproper scoring rules. Taking directions from the axioms of probability,refinement is further clarified using a Hilbert space interpretation andreformulated into the underlying data distribution setting where connections tomaximal marginal diversity and conditional entropy are considered and used toderive measures that provide arbitrarily tight bounds on the Bayes error.Refinement is also reformulated into the classifier output setting and itsconnections to calibrated classifiers and proper margin losses are established.
arxiv-3300-261 | State estimation under non-Gaussian Levy noise: A modified Kalman filtering method | http://arxiv.org/pdf/1303.2395v1.pdf | author:Xu Sun, Jinqiao Duan, Xiaofan Li, Xiangjun Wang category:math.DS cs.IT cs.LG math.IT math.PR stat.ML published:2013-03-10 summary:The Kalman filter is extensively used for state estimation for linear systemsunder Gaussian noise. When non-Gaussian L\'evy noise is present, theconventional Kalman filter may fail to be effective due to the fact that thenon-Gaussian L\'evy noise may have infinite variance. A modified Kalman filterfor linear systems with non-Gaussian L\'evy noise is devised. It workseffectively with reasonable computational cost. Simulation results arepresented to illustrate this non-Gaussian filtering method.
arxiv-3300-262 | Image compression using anti-forensics method | http://arxiv.org/pdf/1303.2330v1.pdf | author:M. S. Sreelakshmi, D. Venkataraman category:cs.MM cs.CV published:2013-03-10 summary:A large number of image forensics methods are available which are capable ofidentifying image tampering. But these techniques are not capable of addressingthe anti-forensics method which is able to hide the trace of image tampering.In this paper anti-forensics method for digital image compression has beenproposed. This anti-forensics method is capable of removing the traces of imagecompression. Additionally, technique is also able to remove the traces ofblocking artifact that are left by image compression algorithms that divide animage into segments during compression process. This method is targeted toremove the compression fingerprints of JPEG compression.
arxiv-3300-263 | Intelligent Approaches to interact with Machines using Hand Gesture Recognition in Natural way: A Survey | http://arxiv.org/pdf/1303.2292v1.pdf | author:Ankit Chaudhary, J. L. Raheja, Karen Das, Sonia Raheja category:cs.HC cs.CV published:2013-03-10 summary:Hand gestures recognition (HGR) is one of the main areas of research for theengineers, scientists and bioinformatics. HGR is the natural way of HumanMachine interaction and today many researchers in the academia and industry areworking on different application to make interactions more easy, natural andconvenient without wearing any extra device. HGR can be applied from gamescontrol to vision enabled robot control, from virtual reality to smart homesystems. In this paper we are discussing work done in the area of hand gesturerecognition where focus is on the intelligent approaches including softcomputing based methods like artificial neural network, fuzzy logic, geneticalgorithms etc. The methods in the preprocessing of image for segmentation andhand image construction also taken into study. Most researchers used fingertipsfor hand detection in appearance based modeling. Finally the comparison ofresults given by different researchers is also presented.
arxiv-3300-264 | Mini-Batch Primal and Dual Methods for SVMs | http://arxiv.org/pdf/1303.2314v1.pdf | author:Martin Takáč, Avleen Bijral, Peter Richtárik, Nathan Srebro category:cs.LG math.OC published:2013-03-10 summary:We address the issue of using mini-batches in stochastic optimization ofSVMs. We show that the same quantity, the spectral norm of the data, controlsthe parallelization speedup obtained for both primal stochastic subgradientdescent (SGD) and stochastic dual coordinate ascent (SCDA) methods and use itto derive novel variants of mini-batched SDCA. Our guarantees for both methodsare expressed in terms of the original nonsmooth primal problem based on thehinge-loss.
arxiv-3300-265 | Predictive Correlation Screening: Application to Two-stage Predictor Design in High Dimension | http://arxiv.org/pdf/1303.2378v2.pdf | author:Hamed Firouzi, Bala Rajaratnam, Alfred Hero category:stat.ML published:2013-03-10 summary:We introduce a new approach to variable selection, called PredictiveCorrelation Screening, for predictor design. Predictive Correlation Screening(PCS) implements false positive control on the selected variables, is wellsuited to small sample sizes, and is scalable to high dimensions. We establishasymptotic bounds for Familywise Error Rate (FWER), and resultant mean squareerror of a linear predictor on the selected variables. We apply PredictiveCorrelation Screening to the following two-stage predictor design problem. Anexperimenter wants to learn a multivariate predictor of gene expressions basedon successive biological samples assayed on mRNA arrays. She assays the wholegenome on a few samples and from these assays she selects a small number ofvariables using Predictive Correlation Screening. To reduce assay cost, shesubsequently assays only the selected variables on the remaining samples, tolearn the predictor coefficients. We show superiority of Predictive CorrelationScreening relative to LASSO and correlation learning (sometimes popularlyreferred to in the literature as marginal regression or simple thresholding) interms of performance and computational complexity.
arxiv-3300-266 | Hybrid Q-Learning Applied to Ubiquitous recommender system | http://arxiv.org/pdf/1303.2651v2.pdf | author:Djallel Bouneffouf category:cs.LG cs.IR I.2 published:2013-03-10 summary:Ubiquitous information access becomes more and more important nowadays andresearch is aimed at making it adapted to users. Our work consists in applyingmachine learning techniques in order to bring a solution to some of theproblems concerning the acceptance of the system by users. To achieve this, wepropose a fundamental shift in terms of how we model the learning ofrecommender system: inspired by models of human reasoning developed in robotic,we combine reinforcement learning and case-base reasoning to define arecommendation process that uses these two approaches for generatingrecommendations on different context dimensions (social, temporal, geographic).We describe an implementation of the recommender system based on thisframework. We also present preliminary results from experiments with the systemand show how our approach increases the recommendation quality.
arxiv-3300-267 | Complex Support Vector Machines for Regression and Quaternary Classification | http://arxiv.org/pdf/1303.2184v3.pdf | author:Pantelis Bouboulis, Sergios Theodoridis, Charalampos Mavroforakis, Leoni Dalla category:cs.LG stat.ML published:2013-03-09 summary:The paper presents a new framework for complex Support Vector Regression aswell as Support Vector Machines for quaternary classification. The methodexploits the notion of widely linear estimation to model the input-out relationfor complex-valued data and considers two cases: a) the complex data are splitinto their real and imaginary parts and a typical real kernel is employed tomap the complex data to a complexified feature space and b) a pure complexkernel is used to directly map the data to the induced complex feature space.The recently developed Wirtinger's calculus on complex reproducing kernelHilbert spaces (RKHS) is employed in order to compute the Lagrangian and derivethe dual optimization problem. As one of our major results, we prove that anycomplex SVM/SVR task is equivalent with solving two real SVM/SVR tasksexploiting a specific real kernel which is generated by the chosen complexkernel. In particular, the case of pure complex kernels leads to the generationof new kernels, which have not been considered before. In the classificationcase, the proposed framework inherently splits the complex space into fourparts. This leads naturally in solving the four class-task (quaternaryclassification), instead of the typical two classes of the real SVM. In turn,this rationale can be used in a multiclass problem as a split-class scenariobased on four classes, as opposed to the one-versus-all method; this can leadto significant computational savings. Experiments demonstrate the effectivenessof the proposed framework for regression and classification tasks that involvecomplex data.
arxiv-3300-268 | Clustering on Multi-Layer Graphs via Subspace Analysis on Grassmann Manifolds | http://arxiv.org/pdf/1303.2221v1.pdf | author:Xiaowen Dong, Pascal Frossard, Pierre Vandergheynst, Nikolai Nefedov category:cs.LG cs.CV cs.SI stat.ML published:2013-03-09 summary:Relationships between entities in datasets are often of multiple nature, likegeographical distance, social relationships, or common interests among peoplein a social network, for example. This information can naturally be modeled bya set of weighted and undirected graphs that form a global multilayer graph,where the common vertex set represents the entities and the edges on differentlayers capture the similarities of the entities in term of the differentmodalities. In this paper, we address the problem of analyzing multi-layergraphs and propose methods for clustering the vertices by efficiently mergingthe information provided by the multiple modalities. To this end, we propose tocombine the characteristics of individual graph layers using tools fromsubspace analysis on a Grassmann manifold. The resulting combination can thenbe viewed as a low dimensional representation of the original data whichpreserves the most important information from diverse relationships betweenentities. We use this information in new clustering methods and test ouralgorithm on several synthetic and real world datasets where we demonstratesuperior or competitive performances compared to baseline and state-of-the-arttechniques. Our generic framework further extends to numerous analysis andlearning problems that involve different types of information on graphs.
arxiv-3300-269 | Penalty-regulated dynamics and robust learning procedures in games | http://arxiv.org/pdf/1303.2270v2.pdf | author:Pierre Coucheney, Bruno Gaujal, Panayotis Mertikopoulos category:math.OC cs.GT cs.LG published:2013-03-09 summary:Starting from a heuristic learning scheme for N-person games, we derive a newclass of continuous-time learning dynamics consisting of a replicator-likedrift adjusted by a penalty term that renders the boundary of the game'sstrategy space repelling. These penalty-regulated dynamics are equivalent toplayers keeping an exponentially discounted aggregate of their on-going payoffsand then using a smooth best response to pick an action based on theseperformance scores. Owing to this inherent duality, the proposed dynamicssatisfy a variant of the folk theorem of evolutionary game theory and theyconverge to (arbitrarily precise) approximations of Nash equilibria inpotential games. Motivated by applications to traffic engineering, we exploitthis duality further to design a discrete-time, payoff-based learning algorithmwhich retains these convergence properties and only requires players to observetheir in-game payoffs: moreover, the algorithm remains robust in the presenceof stochastic perturbations and observation errors, and it does not require anysynchronization between players.
arxiv-3300-270 | Embedding of Blink Frequency in Electrooculography Signal using Difference Expansion based Reversible Watermarking Technique | http://arxiv.org/pdf/1304.2310v1.pdf | author:Nilanjan Dey, Prasenjit Maji, Poulami Das, Shouvik Biswas, Achintya Das, Sheli Sinha Chaudhuri category:cs.CV cs.IR published:2013-03-09 summary:In the past few years, like other fields, rapid expansion of digitization andglobalization has influenced the medical field as well. For progress ofdiagnostic results most of the reputed hospitals and diagnostic centres allover the world have started exchanging medical information. In this proposedmethod, the calculated diagnostic parametric values of the originalElectrooculography (EOG) signal are embedded as a watermark by using DifferenceExpansion (DE) algorithm based reversible watermarking technique. The extractedwatermark provides the required parametric values at the recipient end withoutany post computation of the recovered EOG signal. By computing the parametricvalues from the recovered signal, the integrity of the extracted watermark canbe validated. The time domain features of EOG signal are calculated for thegeneration of watermark. In the current work, various features are studied andtwo major features related to blink frequency are used to generate thewatermark. The high Signal to Noise Ratio (SNR) and the Bit Error Rate (BER)claim the robustness of the proposed method.
arxiv-3300-271 | Expensive Optimisation: A Metaheuristics Perspective | http://arxiv.org/pdf/1303.2215v1.pdf | author:Maumita Bhattacharya category:cs.NE 97R40 published:2013-03-09 summary:Stochastic, iterative search methods such as Evolutionary Algorithms (EAs)are proven to be efficient optimizers. However, they require evaluation of thecandidate solutions which may be prohibitively expensive in many real worldoptimization problems. Use of approximate models or surrogates is beingexplored as a way to reduce the number of such evaluations. In this paper weinvestigated three such methods. The first method (DAFHEA) partially replacesan expensive function evaluation by its approximate model. The approximation isrealized with support vector machine (SVM) regression models. The second method(DAFHEA II) is an enhancement on DAFHEA to accommodate for uncertainenvironments. The third one uses surrogate ranking with preference learning orordinal regression. The fitness of the candidates is estimated by modelingtheir rank. The techniques' performances on some of the benchmark numericaloptimization problems have been reported. The comparative benefits andshortcomings of both techniques have been identified.
arxiv-3300-272 | Medical Information Embedding in Compressed Watermarked Intravascular Ultrasound Video | http://arxiv.org/pdf/1303.2211v1.pdf | author:Nilanjan Dey, Suvojit Acharjee, Debalina Biswas, Achintya Das, Sheli Sinha Chaudhuri category:cs.MM cs.CV published:2013-03-09 summary:In medical field, intravascular ultrasound (IVUS) is a tomographic imagingmodality, which can identify the boundaries of different layers of bloodvessels. IVUS can detect myocardial infarction (heart attack) that remainsignored and unattended when only angioplasty is done. During the past decade,it became easier for some individuals or groups to copy and transmits digitalinformation without the permission of the owner. For increasing authenticationand security of copyrights, digital watermarking, an information hidingtechnique, was introduced. Achieving watermarking technique with lesser amountof distortion in biomedical data is a challenging task. Watermark can beembedded into an image or in a video. As video data is a huge amount ofinformation, therefore a large storage area is needed which is not feasible. Inthis case motion vector based video compression is done to reduce size. In thispresent paper, an Electronic Patient Record (EPR) is embedded as watermarkwithin an IVUS video and then motion vector is calculated. This proposed methodproves robustness as the extracted watermark has good PSNR value and less MSE.
arxiv-3300-273 | Design and Development of Artificial Neural Networking (ANN) system using sigmoid activation function to predict annual rice production in Tamilnadu | http://arxiv.org/pdf/1303.1913v1.pdf | author:S. Arun Balaji, K. Baskaran category:cs.NE F.2.2; I.2.7 published:2013-03-08 summary:Prediction of annual rice production in all the 31 districts of Tamilnadu isan important decision for the Government of Tamilnadu. Rice production is acomplex process and non linear problem involving soil, crop, weather, pest,disease, capital, labour and management parameters. ANN software was designedand developed with Feed Forward Back Propagation (FFBP) network to predict riceproduction. The input layer has six independent variables like area ofcultivation and rice production in three seasons like Kuruvai, Samba and Kodai.The popular sigmoid activation function was adopted to convert input data intosigmoid values. The hidden layer computes the summation of six sigmoid valueswith six sets of weightages. The final output was converted into sigmoid valuesusing a sigmoid transfer function. ANN outputs are the predicted results. Theerror between original data and ANN output values were computed. A thresholdvalue of 10-9 was used to test whether the error is greater than the thresholdlevel. If the error is greater than threshold then updating of weights was doneall summations were done by back propagation. This process was repeated untilerror equal to zero. The predicted results were printed and it was found to beexactly matching with the expected values. It shows that the ANN prediction was100% accurate.
arxiv-3300-274 | Mining and Exploiting Domain-Specific Corpora in the PANACEA Platform | http://arxiv.org/pdf/1303.1932v1.pdf | author:Núria Bel, Vassilis Papavasiliou, Prokopis Prokopidis, Antonio Toral, Victoria Arranz category:cs.CL published:2013-03-08 summary:The objective of the PANACEA ICT-2007.2.2 EU project is to build a platformthat automates the stages involved in the acquisition, production, updating andmaintenance of the large language resources required by, among others, MTsystems. The development of a Corpus Acquisition Component (CAC) for extractingmonolingual and bilingual data from the web is one of the most innovativebuilding blocks of PANACEA. The CAC, which is the first stage in the PANACEApipeline for building Language Resources, adopts an efficient and distributedmethodology to crawl for web documents with rich textual content in specificlanguages and predefined domains. The CAC includes modules that can acquireparallel data from sites with in-domain content available in more than onelanguage. In order to extrinsically evaluate the CAC methodology, we haveconducted several experiments that used crawled parallel corpora for theidentification and extraction of parallel sentences using sentence alignment.The corpora were then successfully used for domain adaptation of MachineTranslation Systems.
arxiv-3300-275 | A Classification of Adjectives for Polarity Lexicons Enhancement | http://arxiv.org/pdf/1303.1931v1.pdf | author:Silvia Vázquez, Núria Bel category:cs.CL published:2013-03-08 summary:Subjective language detection is one of the most important challenges inSentiment Analysis. Because of the weight and frequency in opinionated texts,adjectives are considered a key piece in the opinion extraction process. Thesesubjective units are more and more frequently collected in polarity lexicons inwhich they appear annotated with their prior polarity. However, at the moment,any polarity lexicon takes into account prior polarity variations acrossdomains. This paper proves that a majority of adjectives change their priorpolarity value depending on the domain. We propose a distinction between domaindependent and domain independent adjectives. Moreover, our analysis led us topropose a further classification related to subjectivity degree: constant,mixed and highly subjective adjectives. Following this classification, polarityvalues will be a better support for Sentiment Analysis.
arxiv-3300-276 | Heuristic Ternary Error-Correcting Output Codes Via Weight Optimization and Layered Clustering-Based Approach | http://arxiv.org/pdf/1303.2132v2.pdf | author:Xiao-Lei Zhang category:cs.LG published:2013-03-08 summary:One important classifier ensemble for multiclass classification problems isError-Correcting Output Codes (ECOCs). It bridges multiclass problems andbinary-class classifiers by decomposing multiclass problems to a serialbinary-class problems. In this paper, we present a heuristic ternary code,named Weight Optimization and Layered Clustering-based ECOC (WOLC-ECOC). Itstarts with an arbitrary valid ECOC and iterates the following two steps untilthe training risk converges. The first step, named Layered Clustering basedECOC (LC-ECOC), constructs multiple strong classifiers on the most confusingbinary-class problem. The second step adds the new classifiers to ECOC by anovel Optimized Weighted (OW) decoding algorithm, where the optimizationproblem of the decoding is solved by the cutting plane algorithm. Technically,LC-ECOC makes the heuristic training process not blocked by some difficultbinary-class problem. OW decoding guarantees the non-increase of the trainingrisk for ensuring a small code length. Results on 14 UCI datasets and a musicgenre classification problem demonstrate the effectiveness of WOLC-ECOC.
arxiv-3300-277 | Towards the Fully Automatic Merging of Lexical Resources: A Step Forward | http://arxiv.org/pdf/1303.1929v1.pdf | author:Muntsa Padró, Núria Bel, Silvia Necsulescu category:cs.CL published:2013-03-08 summary:This article reports on the results of the research done towards the fullyautomatically merging of lexical resources. Our main goal is to show thegenerality of the proposed approach, which have been previously applied tomerge Spanish Subcategorization Frames lexica. In this work we extend and applythe same technique to perform the merging of morphosyntactic lexica encoded inLMF. The experiments showed that the technique is general enough to obtain goodresults in these two different tasks which is an important step towardsperforming the merging of lexical resources fully automatically.
arxiv-3300-278 | Automatic lexical semantic classification of nouns | http://arxiv.org/pdf/1303.1930v1.pdf | author:Núria Bel, Lauren Romeo, Muntsa Padró category:cs.CL published:2013-03-08 summary:The work we present here addresses cue-based noun classification in Englishand Spanish. Its main objective is to automatically acquire lexical semanticinformation by classifying nouns into previously known noun lexical classes.This is achieved by using particular aspects of linguistic contexts as cuesthat identify a specific lexical class. Here we concentrate on the task ofidentifying such cues and the theoretical background that allows for anassessment of the complexity of the task. The results show that, despite of thea-priori complexity of the task, cue-based classification is a useful tool inthe automatic acquisition of lexical semantic classes.
arxiv-3300-279 | Estimation of soil moisture in paddy field using Artificial Neural Networks | http://arxiv.org/pdf/1303.1868v1.pdf | author:Chusnul Arif, Masaru Mizoguchi, Budi Indra Setiawan, Ryoichi Doi category:cs.NE physics.ao-ph published:2013-03-08 summary:In paddy field, monitoring soil moisture is required for irrigationscheduling and water resource allocation, management and planning. The currentstudy proposes an Artificial Neural Networks (ANN) model to estimate soilmoisture in paddy field with limited meteorological data. Dynamic of ANN modelwas adopted to estimate soil moisture with the inputs of referenceevapotranspiration (ETo) and precipitation. ETo was firstly estimated using themaximum, average and minimum values of air temperature as the inputs of model.The models were performed under different weather conditions between the twopaddy cultivation periods. Training process of model was carried out using theobservation data in the first period, while validation process was conductedbased on the observation data in the second period. Dynamic of ANN modelestimated soil moisture with R2 values of 0.80 and 0.73 for training andvalidation processes, respectively, indicated that tight linear correlationsbetween observed and estimated values of soil moisture were observed. Thus, theANN model reliably estimates soil moisture with limited meteorological data.
arxiv-3300-280 | Application of the SP theory of intelligence to the understanding of natural vision and the development of computer vision | http://arxiv.org/pdf/1303.2071v2.pdf | author:J. Gerard Wolff category:cs.CV cs.AI published:2013-03-08 summary:The SP theory of intelligence aims to simplify and integrate concepts incomputing and cognition, with information compression as a unifying theme. Thisarticle discusses how it may be applied to the understanding of natural visionand the development of computer vision. The theory, which is described quitefully elsewhere, is described here in outline but with enough detail to ensurethat the rest of the article makes sense. Low level perceptual features such as edges or corners may be identified bythe extraction of redundancy in uniform areas in a manner that is comparablewith the run-length encoding technique for information compression. The concept of multiple alignment in the SP theory may be applied to therecognition of objects, and to scene analysis, with a hierarchy of parts andsub-parts, and at multiple levels of abstraction. The theory has potential for the unsupervised learning of visual objects andclasses of objects, and suggests how coherent concepts may be derived fromfragments. As in natural vision, both recognition and learning in the SP system isrobust in the face of errors of omission, commission and substitution. The theory suggests how, via vision, we may piece together a knowledge of thethree-dimensional structure of objects and of our environment, it provides anaccount of how we may see things that are not objectively present in an image,and how we recognise something despite variations in the size of its retinalimage. And it has things to say about the phenomena of lightness constancy andcolour constancy, the role of context in recognition, and ambiguities in visualperception. A strength of the SP theory is that it provides for the integration of visionwith other sensory modalities and with other aspects of intelligence.
arxiv-3300-281 | Security Assessment of Software Design using Neural Network | http://arxiv.org/pdf/1303.2017v1.pdf | author:A. Adebiyi, Johnnes Arreymbi, Chris Imafidon category:cs.CR cs.NE published:2013-03-08 summary:Security flaws in software applications today has been attributed mostly todesign flaws. With limited budget and time to release software into the market,many developers often consider security as an afterthought. Previous researchshows that integrating security into software applications at a later stage ofsoftware development lifecycle (SDLC) has been found to be more costly thanwhen it is integrated during the early stages. To assist in the integration ofsecurity early in the SDLC stages, a new approach for assessing security duringthe design phase by neural network is investigated in this paper. Our findingsshow that by training a back propagation neural network to identify attackpatterns, possible attacks can be identified from design scenarios presented toit. The result of performance of the neural network is presented in this paper.
arxiv-3300-282 | Transfer Learning for Voice Activity Detection: A Denoising Deep Neural Network Perspective | http://arxiv.org/pdf/1303.2104v1.pdf | author:Xiao-Lei Zhang, Ji Wu category:cs.LG published:2013-03-08 summary:Mismatching problem between the source and target noisy corpora severelyhinder the practical use of the machine-learning-based voice activity detection(VAD). In this paper, we try to address this problem in the transfer learningprospective. Transfer learning tries to find a common learning machine or acommon feature subspace that is shared by both the source corpus and the targetcorpus. The denoising deep neural network is used as the learning machine.Three transfer techniques, which aim to learn common feature representations,are used for analysis. Experimental results demonstrate the effectiveness ofthe transfer learning schemes on the mismatch problem.
arxiv-3300-283 | Convex Discriminative Multitask Clustering | http://arxiv.org/pdf/1303.2130v2.pdf | author:Xiao-Lei Zhang category:cs.LG published:2013-03-08 summary:Multitask clustering tries to improve the clustering performance of multipletasks simultaneously by taking their relationship into account. Most existingmultitask clustering algorithms fall into the type of generative clustering,and none are formulated as convex optimization problems. In this paper, wepropose two convex Discriminative Multitask Clustering (DMTC) algorithms toaddress the problems. Specifically, we first propose a Bayesian DMTC framework.Then, we propose two convex DMTC objectives within the framework. The firstone, which can be seen as a technical combination of the convex multitaskfeature learning and the convex Multiclass Maximum Margin Clustering (M3C),aims to learn a shared feature representation. The second one, which can beseen as a combination of the convex multitask relationship learning and M3C,aims to learn the task relationship. The two objectives are solved in a uniformprocedure by the efficient cutting-plane algorithm. Experimental results on atoy problem and two benchmark datasets demonstrate the effectiveness of theproposed algorithms.
arxiv-3300-284 | Mining Representative Unsubstituted Graph Patterns Using Prior Similarity Matrix | http://arxiv.org/pdf/1303.2054v1.pdf | author:Wajdi Dhifli, Rabie Saidi, Engelbert Mephu Nguifo category:cs.CE cs.LG published:2013-03-08 summary:One of the most powerful techniques to study protein structures is to lookfor recurrent fragments (also called substructures or spatial motifs), then usethem as patterns to characterize the proteins under study. An emergent trendconsists in parsing proteins three-dimensional (3D) structures into graphs ofamino acids. Hence, the search of recurrent spatial motifs is formulated as aprocess of frequent subgraph discovery where each subgraph represents a spatialmotif. In this scope, several efficient approaches for frequent subgraphdiscovery have been proposed in the literature. However, the set of discoveredfrequent subgraphs is too large to be efficiently analyzed and explored in anyfurther process. In this paper, we propose a novel pattern selection approachthat shrinks the large number of discovered frequent subgraphs by selecting therepresentative ones. Existing pattern selection approaches do not exploit thedomain knowledge. Yet, in our approach we incorporate the evolutionaryinformation of amino acids defined in the substitution matrices in order toselect the representative subgraphs. We show the effectiveness of our approachon a number of real datasets. The results issued from our experiments show thatour approach is able to considerably decrease the number of motifs whileenhancing their interestingness.
arxiv-3300-285 | Gene-Machine, a new search heuristic algorithm | http://arxiv.org/pdf/1303.2096v1.pdf | author:Alfredo Garcia Woods category:cs.NE cs.AI published:2013-03-08 summary:This paper introduces Gene-Machine, an efficient and new search heuristicalgorithm, based in the building-block hypothesis. It is inspired by naturalevolution, but does not use some of the concepts present in genetic algorithmslike population, mutation and generation. This heuristic exhibits goodperformance in comparison with genetic algorithms, and can be used to generateuseful solutions to optimization and search problems.
arxiv-3300-286 | Optimization viewpoint on Kalman smoothing, with applications to robust and sparse estimation | http://arxiv.org/pdf/1303.1993v2.pdf | author:Aleksandr Y. Aravkin, James V. Burke, Gianluigi Pillonetto category:math.OC stat.CO stat.ML 62F35, 65K10 published:2013-03-08 summary:In this paper, we present the optimization formulation of the Kalmanfiltering and smoothing problems, and use this perspective to develop a varietyof extensions and applications. We first formulate classic Kalman smoothing asa least squares problem, highlight special structure, and show that the classicfiltering and smoothing algorithms are equivalent to a particular algorithm forsolving this problem. Once this equivalence is established, we presentextensions of Kalman smoothing to systems with nonlinear process andmeasurement models, systems with linear and nonlinear inequality constraints,systems with outliers in the measurements or sudden changes in the state, andsystems where the sparsity of the state sequence must be accounted for. Allextensions preserve the computational efficiency of the classic algorithms, andmost of the extensions are illustrated with numerical examples, which are partof an open source Kalman smoothing Matlab/Octave package.
arxiv-3300-287 | ALPRS - A New Approach for License Plate Recognition using the Sift Algorithm | http://arxiv.org/pdf/1303.1667v1.pdf | author:Francisco Assis da Silva, Almir Olivette Artero, Maria Stela Veludo de Paiva, Ricardo Luis Barbosa category:cs.CV published:2013-03-07 summary:This paper presents a new approach for the automatic license platerecognition, which includes the SIFT algorithm in step to locate the plate inthe input image. In this new approach, besides the comparison of the featuresobtained with the SIFT algorithm, the correspondence between the spatialorientations and the positioning associated with the keypoints is alsoobserved. Afterwards, an algorithm is used for the character recognition of theplates, very fast, which makes it possible its application in real time. Theresults obtained with the proposed approach presented very good success rates,so much for locating the characters in the input image, as for theirrecognition.
arxiv-3300-288 | Efficient learning strategy of Chinese characters based on network approach | http://arxiv.org/pdf/1303.1599v1.pdf | author:Xiao-Yong Yan, Ying Fan, Zengru Di, Shlomo Havlin, Jinshan Wu category:physics.soc-ph cs.CL cs.SI published:2013-03-07 summary:Based on network analysis of hierarchical structural relations among Chinesecharacters, we develop an efficient learning strategy of Chinese characters. Weregard a more efficient learning method if one learns the same number of usefulChinese characters in less effort or time. We construct a node-weighted networkof Chinese characters, where character usage frequencies are used as nodeweights. Using this hierarchical node-weighted network, we propose a newlearning method, the distributed node weight (DNW) strategy, which is based ona new measure of nodes' importance that takes into account both the weight ofthe nodes and the hierarchical structure of the network. Chinese characterlearning strategies, particularly their learning order, are analyzed asdynamical processes over the network. We compare the efficiency of threetheoretical learning methods and two commonly used methods from mainstreamChinese textbooks, one for Chinese elementary school students and the other forstudents learning Chinese as a second language. We find that the DNW methodsignificantly outperforms the others, implying that the efficiency of currentlearning methods of major textbooks can be greatly improved.
arxiv-3300-289 | On Robust Face Recognition via Sparse Encoding: the Good, the Bad, and the Ugly | http://arxiv.org/pdf/1303.1624v1.pdf | author:Yongkang Wong, Mehrtash T. Harandi, Conrad Sanderson category:cs.CV published:2013-03-07 summary:In the field of face recognition, Sparse Representation (SR) has receivedconsiderable attention during the past few years. Most of the relevantliterature focuses on holistic descriptors in closed-set identificationapplications. The underlying assumption in SR-based methods is that each classin the gallery has sufficient samples and the query lies on the subspacespanned by the gallery of the same class. Unfortunately, such assumption iseasily violated in the more challenging face verification scenario, where analgorithm is required to determine if two faces (where one or both have notbeen seen before) belong to the same person. In this paper, we first discusswhy previous attempts with SR might not be applicable to verification problems.We then propose an alternative approach to face verification via SR.Specifically, we propose to use explicit SR encoding on local image patchesrather than the entire face. The obtained sparse signals are pooled viaaveraging to form multiple region descriptors, which are then concatenated toform an overall face descriptor. Due to the deliberate loss spatial relationswithin each region (caused by averaging), the resulting descriptor is robust tomisalignment & various image deformations. Within the proposed framework, weevaluate several SR encoding techniques: l1-minimisation, Sparse AutoencoderNeural Network (SANN), and an implicit probabilistic technique based onGaussian Mixture Models. Thorough experiments on AR, FERET, exYaleB, BANCA andChokePoint datasets show that the proposed local SR approach obtainsconsiderably better and more robust performance than several previousstate-of-the-art holistic SR methods, in both verification and closed-setidentification problems. The experiments also show that l1-minimisation basedencoding has a considerably higher computational than the other techniques, butleads to higher recognition rates.
arxiv-3300-290 | K-Nearest Neighbour algorithm coupled with logistic regression in medical case-based reasoning systems. Application to prediction of access to the renal transplant waiting list in Brittany | http://arxiv.org/pdf/1303.1700v1.pdf | author:Boris Campillo-Gimenez, Wassim Jouini, Sahar Bayat, Marc Cuggia category:cs.AI stat.ML published:2013-03-07 summary:Introduction. Case Based Reasoning (CBR) is an emerg- ing decision makingparadigm in medical research where new cases are solved relying on previouslysolved similar cases. Usually, a database of solved cases is provided, andevery case is described through a set of attributes (inputs) and a label(output). Extracting useful information from this database can help the CBRsystem providing more reliable results on the yet to be solved cases.Objective. For that purpose we suggest a general frame- work where a CBRsystem, viz. K-Nearest Neighbor (K-NN) algorithm, is combined with variousinformation obtained from a Logistic Regression (LR) model. Methods. LR isapplied, on the case database, to assign weights to the attributes as well asthe solved cases. Thus, five possible decision making systems based on K-NNand/or LR were identified: a standalone K-NN, a standalone LR and three softK-NN algorithms that rely on the weights based on the results of the LR. Theevaluation of the described approaches is performed in the field of renaltransplant access waiting list. Results and conclusion. The results show thatour suggested approach, where the K-NN algorithm relies on both weightedattributes and cases, can efficiently deal with non relevant attributes,whereas the four other approaches suffer from this kind of noisy setups. Therobustness of this approach suggests interesting perspectives for medicalproblem solving tools using CBR methodology.
arxiv-3300-291 | Concept-based indexing in text information retrieval | http://arxiv.org/pdf/1303.1703v1.pdf | author:Fatiha Boubekeur, Wassila Azzoug category:cs.IR cs.CL published:2013-03-07 summary:Traditional information retrieval systems rely on keywords to index documentsand queries. In such systems, documents are retrieved based on the number ofshared keywords with the query. This lexical-focused retrieval leads toinaccurate and incomplete results when different keywords are used to describethe documents and queries. Semantic-focused retrieval approaches attempt toovercome this problem by relying on concepts rather than on keywords toindexing and retrieval. The goal is to retrieve documents that are semanticallyrelevant to a given user query. This paper addresses this issue by proposing asolution at the indexing level. More precisely, we propose a novel approach forsemantic indexing based on concepts identified from a linguistic resource. Inparticular, our approach relies on the joint use of WordNet and WordNetDomainslexical databases for concept identification. Furthermore, we propose asemantic-based concept weighting scheme that relies on a novel definition ofconcept centrality. The resulting system is evaluated on the TIME testcollection. Experimental results show the effectiveness of our proposition overtraditional IR approaches.
arxiv-3300-292 | Multi-relational Learning Using Weighted Tensor Decomposition with Modular Loss | http://arxiv.org/pdf/1303.1733v2.pdf | author:Ben London, Theodoros Rekatsinas, Bert Huang, Lise Getoor category:cs.LG published:2013-03-07 summary:We propose a modular framework for multi-relational learning via tensordecomposition. In our learning setting, the training data contains multipletypes of relationships among a set of objects, which we represent by a sparsethree-mode tensor. The goal is to predict the values of the missing entries. Todo so, we model each relationship as a function of a linear combination oflatent factors. We learn this latent representation by computing a low-ranktensor decomposition, using quasi-Newton optimization of a weighted objectivefunction. Sparsity in the observed data is captured by the weighted objective,leading to improved accuracy when training data is limited. Exploiting sparsityalso improves efficiency, potentially up to an order of magnitude overunweighted approaches. In addition, our framework accommodates arbitrarycombinations of smooth, task-specific loss functions, making it better suitedfor learning different types of relations. For the typical cases of real-valuedfunctions and binary relations, we propose several loss functions and derivethe associated parameter gradients. We evaluate our method on synthetic andreal data, showing significant improvements in both accuracy and scalabilityover related factorization techniques.
arxiv-3300-293 | Simplifying Energy Optimization using Partial Enumeration | http://arxiv.org/pdf/1303.1749v2.pdf | author:Carl Olsson, Johannes Ulen, Yuri Boykov, Vladimir Kolmogorov category:cs.CV published:2013-03-07 summary:Energies with high-order non-submodular interactions have been shown to bevery useful in vision due to their high modeling power. Optimization of suchenergies, however, is generally NP-hard. A naive approach that works for smallproblem instances is exhaustive search, that is, enumeration of all possiblelabelings of the underlying graph. We propose a general minimization approachfor large graphs based on enumeration of labelings of certain small patches.This partial enumeration technique reduces complex high-order energyformulations to pairwise Constraint Satisfaction Problems with unary costs(uCSP), which can be efficiently solved using standard methods like TRW-S. Ourapproach outperforms a number of existing state-of-the-art algorithms on wellknown difficult problems (e.g. curvature regularization, stereo,deconvolution); it gives near global minimum and better speed. Our main application of interest is curvature regularization. In the contextof segmentation, our partial enumeration technique allows to evaluate curvaturedirectly on small patches using a novel integral geometry approach.
arxiv-3300-294 | Revisiting the Nystrom Method for Improved Large-Scale Machine Learning | http://arxiv.org/pdf/1303.1849v2.pdf | author:Alex Gittens, Michael W. Mahoney category:cs.LG cs.DS cs.NA published:2013-03-07 summary:We reconsider randomized algorithms for the low-rank approximation ofsymmetric positive semi-definite (SPSD) matrices such as Laplacian and kernelmatrices that arise in data analysis and machine learning applications. Ourmain results consist of an empirical evaluation of the performance quality andrunning time of sampling and projection methods on a diverse suite of SPSDmatrices. Our results highlight complementary aspects of sampling versusprojection methods; they characterize the effects of common data preprocessingsteps on the performance of these algorithms; and they point to importantdifferences between uniform sampling and nonuniform sampling methods based onleverage scores. In addition, our empirical results illustrate that existingtheory is so weak that it does not provide even a qualitative guide topractice. Thus, we complement our empirical results with a suite of worst-casetheoretical bounds for both random sampling and random projection methods.These bounds are qualitatively superior to existing bounds---e.g. improvedadditive-error bounds for spectral and Frobenius norm error and relative-errorbounds for trace norm error---and they point to future directions to make thesealgorithms useful in even larger-scale machine learning applications.
arxiv-3300-295 | Improving Automatic Emotion Recognition from speech using Rhythm and Temporal feature | http://arxiv.org/pdf/1303.1761v1.pdf | author:Mayank Bhargava, Tim Polzehl category:cs.CV published:2013-03-07 summary:This paper is devoted to improve automatic emotion recognition from speech byincorporating rhythm and temporal features. Research on automatic emotionrecognition so far has mostly been based on applying features like MFCCs, pitchand energy or intensity. The idea focuses on borrowing rhythm features fromlinguistic and phonetic analysis and applying them to the speech signal on thebasis of acoustic knowledge only. In addition to this we exploit a set oftemporal and loudness features. A segmentation unit is employed in starting toseparate the voiced/unvoiced and silence parts and features are explored ondifferent segments. Thereafter different classifiers are used forclassification. After selecting the top features using an IGR filter we areable to achieve a recognition rate of 80.60 % on the Berlin Emotion Databasefor the speaker dependent framework.
arxiv-3300-296 | Watersheds on edge or node weighted graphs "par l'exemple" | http://arxiv.org/pdf/1303.1829v1.pdf | author:Fernand Meyer category:cs.CV published:2013-03-07 summary:Watersheds have been defined both for node and edge weighted graphs. We showthat they are identical: for each edge (resp.\ node) weighted graph exists anode (resp. edge) weighted graph with the same minima and catchment basin.
arxiv-3300-297 | Discovery of factors in matrices with grades | http://arxiv.org/pdf/1303.1264v1.pdf | author:Radim Belohlavek, Vilem Vychodil category:cs.LG cs.NA published:2013-03-06 summary:We present an approach to decomposition and factor analysis of matrices withordinal data. The matrix entries are grades to which objects represented byrows satisfy attributes represented by columns, e.g. grades to which an imageis red, a product has a given feature, or a person performs well in a test. Weassume that the grades form a bounded scale equipped with certain aggregationoperators and conforms to the structure of a complete residuated lattice. Wepresent a greedy approximation algorithm for the problem of decomposition ofsuch matrix in a product of two matrices with grades under the restriction thatthe number of factors be small. Our algorithm is based on a geometric insightprovided by a theorem identifying particular rectangular-shaped submatrices asoptimal factors for the decompositions. These factors correspond to formalconcepts of the input data and allow an easy interpretation of thedecomposition. We present illustrative examples and experimental evaluation.
arxiv-3300-298 | A Generalized Hybrid Real-Coded Quantum Evolutionary Algorithm Based on Particle Swarm Theory with Arithmetic Crossover | http://arxiv.org/pdf/1303.1243v1.pdf | author:Md. Amjad Hossain, Md. Kawser Hossain, M. M. A. Hashem category:cs.NE published:2013-03-06 summary:This paper proposes a generalized Hybrid Real-coded Quantum EvolutionaryAlgorithm (HRCQEA) for optimizing complex functions as well as combinatorialoptimization. The main idea of HRCQEA is to devise a new technique for mutationand crossover operators. Using the evolutionary equation of PSO aSingle-Multiple gene Mutation (SMM) is designed and the concept of ArithmeticCrossover (AC) is used in the new Crossover operator. In HRCQEA, each triploidchromosome represents a particle and the position of the particle is updatedusing SMM and Quantum Rotation Gate (QRG), which can make the balance betweenexploration and exploitation. Crossover is employed to expand the search space,Hill Climbing Selection (HCS) and elitism help to accelerate the convergencespeed. Simulation results on Knapsack Problem and five benchmark complexfunctions with high dimension show that HRCQEA performs better in terms ofability to discover the global optimum and convergence speed.
arxiv-3300-299 | A Fast Iterative Bayesian Inference Algorithm for Sparse Channel Estimation | http://arxiv.org/pdf/1303.1312v1.pdf | author:Niels Lovmand Pedersen, Carles Navarro Manch category:stat.ML cs.IT math.IT published:2013-03-06 summary:In this paper, we present a Bayesian channel estimation algorithm formulticarrier receivers based on pilot symbol observations. The inherent sparsenature of wireless multipath channels is exploited by modeling the priordistribution of multipath components' gains with a hierarchical representationof the Bessel K probability density function; a highly efficient, fastiterative Bayesian inference method is then applied to the proposed model. Theresulting estimator outperforms other state-of-the-art Bayesian andnon-Bayesian estimators, either by yielding lower mean squared estimation erroror by attaining the same accuracy with improved convergence rate, as shown inour numerical evaluation.
arxiv-3300-300 | Large-Margin Metric Learning for Partitioning Problems | http://arxiv.org/pdf/1303.1280v1.pdf | author:Rémi Lajugie, Sylvain Arlot, Francis Bach category:cs.LG stat.ML published:2013-03-06 summary:In this paper, we consider unsupervised partitioning problems, such asclustering, image segmentation, video segmentation and other change-pointdetection problems. We focus on partitioning problems based explicitly orimplicitly on the minimization of Euclidean distortions, which includemean-based change-point detection, K-means, spectral clustering and normalizedcuts. Our main goal is to learn a Mahalanobis metric for these unsupervisedproblems, leading to feature weighting and/or selection. This is done in asupervised way by assuming the availability of several potentially partiallylabelled datasets that share the same metric. We cast the metric learningproblem as a large-margin structured prediction problem, with proper definitionof regularizers and losses, leading to a convex optimization problem which canbe solved efficiently with iterative techniques. We provide experiments wherewe show how learning the metric may significantly improve the partitioningperformance in synthetic examples, bioinformatics, video segmentation and imagesegmentation problems.
