arxiv-3300-1 | Bridging Information Criteria and Parameter Shrinkage for Model Selection | http://arxiv.org/pdf/1307.2307v1.pdf | author:Kun Zhang, Heng Peng, Laiwan Chan, Aapo Hyvarinen category:stat.ML cs.LG published:2013-07-08 summary:Model selection based on classical information criteria, such as BIC, isgenerally computationally demanding, but its properties are well studied. Onthe other hand, model selection based on parameter shrinkage by $\ell_1$-typepenalties is computationally efficient. In this paper we make an attempt tocombine their strengths, and propose a simple approach that penalizes thelikelihood with data-dependent $\ell_1$ penalties as in adaptive Lasso andexploits a fixed penalization parameter. Even for finite samples, its modelselection results approximately coincide with those based on informationcriteria; in particular, we show that in some special cases, this approach andthe corresponding information criterion produce exactly the same model. One canalso consider this approach as a way to directly determine the penalizationparameter in adaptive Lasso to achieve information criteria-like modelselection. As extensions, we apply this idea to complex models includingGaussian mixture model and mixture of factor analyzers, whose model selectionis traditionally difficult to do; by adopting suitable penalties, we providecontinuous approximators to the corresponding information criteria, which areeasy to optimize and enable efficient model selection.
arxiv-3300-2 | Multiarmed Bandits With Limited Expert Advice | http://arxiv.org/pdf/1306.4653v4.pdf | author:Satyen Kale category:cs.LG published:2013-06-19 summary:We solve the COLT 2013 open problem of \citet{SCB} on minimizing regret inthe setting of advice-efficient multiarmed bandits with expert advice. We givean algorithm for the setting of K arms and N experts out of which we areallowed to query and use only M experts' advices in each round, which has aregret bound of \tilde{O}\bigP{\sqrt{\frac{\min\{K, M\} N}{M} T}} after Trounds. We also prove that any algorithm for this problem must have expectedregret at least \tilde{\Omega}\bigP{\sqrt{\frac{\min\{K, M\} N}{M}T}}, thusshowing that our upper bound is nearly tight.
arxiv-3300-3 | Transmodal Analysis of Neural Signals | http://arxiv.org/pdf/1307.2150v1.pdf | author:Yaroslav O. Halchenko, Michael Hanke, James V. Haxby, Stephen Jose Hanson, Christoph S. Herrmann category:q-bio.NC cs.LG q-bio.QM published:2013-07-08 summary:Localizing neuronal activity in the brain, both in time and in space, is acentral challenge to advance the understanding of brain function. Because ofthe inability of any single neuroimaging techniques to cover all aspects atonce, there is a growing interest to combine signals from multiple modalitiesin order to benefit from the advantages of each acquisition method. Due to thecomplexity and unknown parameterization of any suggested complete model of BOLDresponse in functional magnetic resonance imaging (fMRI), the development of areliable ultimate fusion approach remains difficult. But besides the primarygoal of superior temporal and spatial resolution, conjoint analysis of datafrom multiple imaging modalities can alternatively be used to segregate neuralinformation from physiological and acquisition noise. In this paper we suggesta novel methodology which relies on constructing a quantifiable mapping of datafrom one modality (electroencephalography; EEG) into another (fMRI), calledtransmodal analysis of neural signals (TRANSfusion). TRANSfusion attempts tomap neural data embedded within the EEG signal into its reflection in fMRIdata. Assessing the mapping performance on unseen data allows to localize brainareas where a significant portion of the signal could be reliablyreconstructed, hence the areas neural activity of which is reflected in bothEEG and fMRI data. Consecutive analysis of the learnt model allows to localizeareas associated with specific frequency bands of EEG, or areas functionallyrelated (connected or coherent) to any given EEG sensor. We demonstrate theperformance of TRANSfusion on artificial and real data from an auditoryexperiment. We further speculate on possible alternative uses: cross-modal datafiltering and EEG-driven interpolation of fMRI signals to obtain arbitrarilyhigh temporal sampling of BOLD.
arxiv-3300-4 | Differential Contrastive Divergence | http://arxiv.org/pdf/0903.2299v3.pdf | author:David McAllester category:cs.LG published:2009-03-13 summary:This paper has been retracted.
arxiv-3300-5 | A PAC-Bayesian Tutorial with A Dropout Bound | http://arxiv.org/pdf/1307.2118v1.pdf | author:David McAllester category:cs.LG published:2013-07-08 summary:This tutorial gives a concise overview of existing PAC-Bayesian theoryfocusing on three generalization bounds. The first is an Occam bound whichhandles rules with finite precision parameters and which states thatgeneralization loss is near training loss when the number of bits needed towrite the rule is small compared to the sample size. The second is aPAC-Bayesian bound providing a generalization guarantee for posteriordistributions rather than for individual rules. The PAC-Bayesian boundnaturally handles infinite precision rule parameters, $L_2$ regularization,{\em provides a bound for dropout training}, and defines a natural notion of asingle distinguished PAC-Bayesian posterior distribution. The third bound is atraining-variance bound --- a kind of bias-variance analysis but with biasreplaced by expected training loss. The training-variance bound dominates theother bounds but is more difficult to interpret. It seems to suggest variancereduction methods such as bagging and may ultimately provide a more meaningfulanalysis of dropouts.
arxiv-3300-6 | Finding the creatures of habit; Clustering households based on their flexibility in using electricity | http://arxiv.org/pdf/1307.2111v1.pdf | author:Ian Dent, Tony Craig, Uwe Aickelin, Tom Rodden category:cs.LG cs.CE published:2013-07-08 summary:Changes in the UK electricity market, particularly with the roll out of smartmeters, will provide greatly increased opportunities for initiatives intendedto change households' electricity usage patterns for the benefit of the overallsystem. Users show differences in their regular behaviours and clusteringhouseholds into similar groupings based on this variability provides forefficient targeting of initiatives. Those people who are stuck into a regularpattern of activity may be the least receptive to an initiative to changebehaviour. A sample of 180 households from the UK are clustered into fourgroups as an initial test of the concept and useful, actionable groupings arefound.
arxiv-3300-7 | The most controversial topics in Wikipedia: A multilingual and geographical analysis | http://arxiv.org/pdf/1305.5566v2.pdf | author:Taha Yasseri, Anselm Spoerri, Mark Graham, János Kertész category:physics.soc-ph cs.CL cs.DL cs.SI published:2013-05-23 summary:We present, visualize and analyse the similarities and differences betweenthe controversial topics related to "edit wars" identified in 10 differentlanguage versions of Wikipedia. After a brief review of the related work wedescribe the methods developed to locate, measure, and categorize thecontroversial topics in the different languages. Visualizations of the degreeof overlap between the top 100 lists of most controversial articles indifferent languages and the content related to geographical locations will bepresented. We discuss what the presented analysis and visualizations can tellus about the multicultural aspects of Wikipedia and practices ofpeer-production. Our results indicate that Wikipedia is more than just anencyclopaedia; it is also a window into convergent and divergent social-spatialpriorities, interests and preferences.
arxiv-3300-8 | Using Clustering to extract Personality Information from socio economic data | http://arxiv.org/pdf/1307.1998v1.pdf | author:Alexandros Ladas, Uwe Aickelin, Jon Garibaldi, Eamonn Ferguson category:cs.LG cs.CE published:2013-07-08 summary:It has become apparent that models that have been applied widely ineconomics, including Machine Learning techniques and Data Mining methods,should take into consideration principles that derive from the theories ofPersonality Psychology in order to discover more comprehensive knowledgeregarding complicated economic behaviours. In this work, we present a method toextract Behavioural Groups by using simple clustering techniques that canpotentially reveal aspects of the Personalities for their members. We believethat this is very important because the psychological information regarding thePersonalities of individuals is limited in real world applications and becauseit can become a useful tool in improving the traditional models of KnowledgeEconomy.
arxiv-3300-9 | Machine Learning in Proof General: Interfacing Interfaces | http://arxiv.org/pdf/1212.3618v2.pdf | author:Ekaterina Komendantskaya, Jónathan Heras, Gudmund Grov category:cs.AI cs.LG cs.LO I.2.6; F.4.1 published:2012-12-14 summary:We present ML4PG - a machine learning extension for Proof General. It allowsusers to gather proof statistics related to shapes of goals, sequences ofapplied tactics, and proof tree structures from the libraries of interactivehigher-order proofs written in Coq and SSReflect. The gathered data isclustered using the state-of-the-art machine learning algorithms available inMATLAB and Weka. ML4PG provides automated interfacing between Proof General andMATLAB/Weka. The results of clustering are used by ML4PG to provide proof hintsin the process of interactive proof development.
arxiv-3300-10 | Intelligent Hybrid Man-Machine Translation Quality Estimation | http://arxiv.org/pdf/1307.1872v1.pdf | author:Ibrahim Sabek, Noha A. Yousri, Nagwa Elmakky, Mona Habib category:cs.CL published:2013-07-07 summary:Inferring evaluation scores based on human judgments is invaluable comparedto using current evaluation metrics which are not suitable for real-timeapplications e.g. post-editing. However, these judgments are much moreexpensive to collect especially from expert translators, compared to evaluationbased on indicators contrasting source and translation texts. This workintroduces a novel approach for quality estimation by combining learntconfidence scores from a probabilistic inference model based on humanjudgments, with selective linguistic features-based scores, where the proposedinference model infers the credibility of given human ranks to solve thescarcity and inconsistency issues of human judgments. Experimental results,using challenging language-pairs, demonstrate improvement in correlation withhuman judgments over traditional evaluation metrics.
arxiv-3300-11 | Ensemble Methods for Multi-label Classification | http://arxiv.org/pdf/1307.1769v1.pdf | author:Lior Rokach, Alon Schclar, Ehud Itach category:stat.ML cs.LG 68T05, 68Q32 published:2013-07-06 summary:Ensemble methods have been shown to be an effective tool for solvingmulti-label classification tasks. In the RAndom k-labELsets (RAKEL) algorithm,each member of the ensemble is associated with a small randomly-selected subsetof k labels. Then, a single label classifier is trained according to eachcombination of elements in the subset. In this paper we adopt a similarapproach, however, instead of randomly choosing subsets, we select the minimumrequired subsets of k labels that cover all labels and meet additionalconstraints such as coverage of inter-label correlations. Construction of thecover is achieved by formulating the subset selection as a minimum set coveringproblem (SCP) and solving it by using approximation algorithms. Every coverneeds only to be prepared once by offline algorithms. Once prepared, a covermay be applied to the classification of any given multi-label dataset whoseproperties conform with those of the cover. The contribution of this paper istwo-fold. First, we introduce SCP as a general framework for constructing labelcovers while allowing the user to incorporate cover construction constraints.We demonstrate the effectiveness of this framework by proposing twoconstruction constraints whose enforcement produces covers that improve theprediction performance of random selection. Second, we provide theoreticalbounds that quantify the probabilities of random selection to produce coversthat meet the proposed construction criteria. The experimental results indicatethat the proposed methods improve multi-label classification accuracy andstability compared with the RAKEL algorithm and to other state-of-the-artalgorithms.
arxiv-3300-12 | Indexing Medical Images based on Collaborative Experts Reports | http://arxiv.org/pdf/1305.4077v2.pdf | author:Abir Messaoudi, Riadh Bouslimi, Jalel Akaichi category:cs.CV cs.IR published:2013-05-17 summary:A patient is often willing to quickly get, from his physician, reliableanalysis and concise explanation according to provided linked medical images.The fact of making choices individually by the patient's physician may lead tomalpractices and consequently generates unforeseeable damages. The Institute ofMedicine of the National Sciences Academy(IMNAS) in USA published a studyestimating that up to 98,000 hospital deathseach year can be attributed tomedical malpractice [1]. Moreover, physician, in charge of medical imageanalysis, might be unavailable at the right time, which may complicate thepatient's state. The goal of this paper is to provide to physicians andpatients, a social network that permits to foster cooperation and to overcomethe problem of unavailability of doctors on site any time. Therefore, patientscan submit their medical images to be diagnosed and commented by severalexperts instantly. Consequently, the need to process opinions and to extractinformation automatically from the proposed social network became a necessitydue to the huge number of comments expressing specialist's reviews. For thisreason, we propose a kind of comments' summary keywords-based method whichextracts the major current terms and relevant words existing on physicians'annotations. The extracted keywords will present a new and robust method forimage indexation. In fact, significant extracted terms will be used later toindex images in order to facilitate their discovery for any appropriate use. Toovercome this challenge, we propose our Terminology Extraction of Annotation(TEA) mixed approach which focuses on algorithms mainly based on statisticalmethods and on external semantic resources.
arxiv-3300-13 | Stochastic Optimization of PCA with Capped MSG | http://arxiv.org/pdf/1307.1674v1.pdf | author:Raman Arora, Andrew Cotter, Nathan Srebro category:stat.ML cs.LG published:2013-07-05 summary:We study PCA as a stochastic optimization problem and propose a novelstochastic approximation algorithm which we refer to as "Matrix StochasticGradient" (MSG), as well as a practical variant, Capped MSG. We study themethod both theoretically and empirically.
arxiv-3300-14 | Biomarker Clustering of Colorectal Cancer Data to Complement Clinical Classification | http://arxiv.org/pdf/1307.1601v1.pdf | author:Chris Roadknight, Uwe Aickelin, Alex Ladas, Daniele Soria, John Scholefield, Lindy Durrant category:cs.LG cs.CE published:2013-07-05 summary:In this paper, we describe a dataset relating to cellular and physicalconditions of patients who are operated upon to remove colorectal tumours. Thisdata provides a unique insight into immunological status at the point of tumourremoval, tumour classification and post-operative survival. Attempts are madeto cluster this dataset and important subsets of it in an effort tocharacterize the data and validate existing standards for tumourclassification. It is apparent from optimal clustering that existing tumourclassification is largely unrelated to immunological factors within a patientand that there may be scope for re-evaluating treatment options and survivalestimates based on a combination of tumour physiology and patienthistochemistry.
arxiv-3300-15 | Supervised Learning and Anti-learning of Colorectal Cancer Classes and Survival Rates from Cellular Biology Parameters | http://arxiv.org/pdf/1307.1599v1.pdf | author:Chris Roadknight, Uwe Aickelin, Guoping Qiu, John Scholefield, Lindy Durrant category:cs.LG cs.CE stat.ML published:2013-07-05 summary:In this paper, we describe a dataset relating to cellular and physicalconditions of patients who are operated upon to remove colorectal tumours. Thisdata provides a unique insight into immunological status at the point of tumourremoval, tumour classification and post-operative survival. Attempts are madeto learn relationships between attributes (physical and immunological) and theresulting tumour stage and survival. Results for conventional machine learningapproaches can be considered poor, especially for predicting tumour stages forthe most important types of cancer. This poor performance is furtherinvestigated and compared with a synthetic, dataset based on the logicalexclusive-OR function and it is shown that there is a significant level of'anti-learning' present in all supervised methods used and this can beexplained by the highly dimensional, complex and sparsely representativedataset. For predicting the stage of cancer from the immunological attributes,anti-learning approaches outperform a range of popular algorithms.
arxiv-3300-16 | Comparing Data-mining Algorithms Developed for Longitudinal Observational Databases | http://arxiv.org/pdf/1307.1584v1.pdf | author:Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack E. Gibson, Richard B. Hubbard category:cs.LG cs.CE cs.DB published:2013-07-05 summary:Longitudinal observational databases have become a recent interest in thepost marketing drug surveillance community due to their ability of presenting anew perspective for detecting negative side effects. Algorithms mininglongitudinal observation databases are not restricted by many of thelimitations associated with the more conventional methods that have beendeveloped for spontaneous reporting system databases. In this paper weinvestigate the robustness of four recently developed algorithms that minelongitudinal observational databases by applying them to The Health ImprovementNetwork (THIN) for six drugs with well document known negative side effects.Our results show that none of the existing algorithms was able to consistentlyidentify known adverse drug reactions above events related to the cause of thedrug and no algorithm was superior.
arxiv-3300-17 | A Sub-block Based Image Retrieval Using Modified Integrated Region Matching | http://arxiv.org/pdf/1307.1561v1.pdf | author:E. R. Vimina, K. Poulose Jacob category:cs.IR cs.CV published:2013-07-05 summary:This paper proposes a content based image retrieval (CBIR) system using thelocal colour and texture features of selected image sub-blocks and globalcolour and shape features of the image. The image sub-blocks are roughlyidentified by segmenting the image into partitions of different configuration,finding the edge density in each partition using edge thresholding followed bymorphological dilation. The colour and texture features of the identifiedregions are computed from the histograms of the quantized HSV colour space andGray Level Co- occurrence Matrix (GLCM) respectively. The colour and texturefeature vectors is computed for each region. The shape features are computedfrom the Edge Histogram Descriptor (EHD). A modified Integrated Region Matching(IRM) algorithm is used for finding the minimum distance between the sub-blocksof the query and target image. Experimental results show that the proposedmethod provides better retrieving result than retrieval using some of theexisting methods.
arxiv-3300-18 | Toward Guaranteed Illumination Models for Non-Convex Objects | http://arxiv.org/pdf/1307.1437v1.pdf | author:Yuqian Zhang, Cun Mu, Han-wen Kuo, John Wright category:cs.CV published:2013-07-04 summary:Illumination variation remains a central challenge in object detection andrecognition. Existing analyses of illumination variation typically pertain toconvex, Lambertian objects, and guarantee quality of approximation in anaverage case sense. We show that it is possible to build V(vertex)-descriptionconvex cone models with worst-case performance guarantees, for non-convexLambertian objects. Namely, a natural verification test based on the angle tothe constructed cone guarantees to accept any image which is sufficientlywell-approximated by an image of the object under some admissible lightingcondition, and guarantees to reject any image that does not have a sufficientlygood approximation. The cone models are generated by sampling pointilluminations with sufficient density, which follows from a new perturbationbound for point images in the Lambertian model. As the number of point imagesrequired for guaranteed verification may be large, we introduce a newformulation for cone preserving dimensionality reduction, which leverages toolsfrom sparse and low-rank decomposition to reduce the complexity, whilecontrolling the approximation error with respect to the original cone.
arxiv-3300-19 | Discovering Sequential Patterns in a UK General Practice Database | http://arxiv.org/pdf/1307.1411v1.pdf | author:Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack E. Gibson, Richard B. Hubbard category:cs.LG cs.CE stat.AP published:2013-07-04 summary:The wealth of computerised medical information becoming readily availablepresents the opportunity to examine patterns of illnesses, therapies andresponses. These patterns may be able to predict illnesses that a patient islikely to develop, allowing the implementation of preventative actions. In thispaper sequential rule mining is applied to a General Practice database to findrules involving a patients age, gender and medical history. By incorporatingthese rules into current health-care a patient can be highlighted assusceptible to a future illness based on past or current illnesses, gender andyear of birth. This knowledge has the ability to greatly improve health-careand reduce health-care costs.
arxiv-3300-20 | Detect adverse drug reactions for drug Alendronate | http://arxiv.org/pdf/1307.1394v1.pdf | author:Yihui Liu, Uwe Aickelin category:cs.CE cs.LG published:2013-07-04 summary:Adverse drug reaction (ADR) is widely concerned for public health issue. Inthis study we propose an original approach to detect the ADRs using featurematrix and feature selection. The experiments are carried out on the drugSimvastatin. Major side effects for the drug are detected and betterperformance is achieved compared to other computerized methods. The detectedADRs are based on the computerized method, further investigation is needed.
arxiv-3300-21 | Quiet in Class: Classification, Noise and the Dendritic Cell Algorithm | http://arxiv.org/pdf/1307.1391v1.pdf | author:Feng Gu, Jan Feyereisl, Robert Oates, Jenna Reps, Julie Greensmith, Uwe Aickelin category:cs.LG cs.CR published:2013-07-04 summary:Theoretical analyses of the Dendritic Cell Algorithm (DCA) have yieldedseveral criticisms about its underlying structure and operation. As a result,several alterations and fixes have been suggested in the literature to correctfor these findings. A contribution of this work is to investigate the effectsof replacing the classification stage of the DCA (which is known to be flawed)with a traditional machine learning technique. This work goes on to questionthe merits of those unique properties of the DCA that are yet to be thoroughlyanalysed. If none of these properties can be found to have a benefit overtraditional approaches, then "fixing" the DCA is arguably less efficient thansimply creating a new algorithm. This work examines the dynamic filteringproperty of the DCA and questions the utility of this unique feature for theanomaly detection problem. It is found that this feature, while advantageousfor noisy, time-ordered classification, is not as useful as a traditionalstatic filter for processing a synthetic dataset. It is concluded that thereare still unique features of the DCA left to investigate. Areas that may be ofbenefit to the Artificial Immune Systems community are suggested.
arxiv-3300-22 | Examining the Classification Accuracy of TSVMs with ?Feature Selection in Comparison with the GLAD Algorithm | http://arxiv.org/pdf/1307.1387v1.pdf | author:Hala Helmi, Jon M. Garibaldi, Uwe Aickelin category:cs.LG cs.CE published:2013-07-04 summary:Gene expression data sets are used to classify and predict patient diagnosticcategories. As we know, it is extremely difficult and expensive to obtain geneexpression labelled examples. Moreover, conventional supervised approachescannot function properly when labelled data (training examples) areinsufficient using Support Vector Machines (SVM) algorithms. Therefore, in thispaper, we suggest Transductive Support Vector Machines (TSVMs) assemi-supervised learning algorithms, learning with both labelled samples dataand unlabelled samples to perform the classification of microarray data. Toprune the superfluous genes and samples we used a feature selection methodcalled Recursive Feature Elimination (RFE), which is supposed to enhance theoutput of classification and avoid the local optimization problem. We examinedthe classification prediction accuracy of the TSVM-RFE algorithm in comparisonwith the Genetic Learning Across Datasets (GLAD) algorithm, as both aresemi-supervised learning methods. Comparing these two methods, we found thatthe TSVM-RFE surpassed both a SVM using RFE and GLAD.
arxiv-3300-23 | Creating Personalised Energy Plans. From Groups to Individuals using Fuzzy C Means Clustering | http://arxiv.org/pdf/1307.1385v1.pdf | author:Ian Dent, Christian Wagner, Uwe Aickelin, Tom Rodden category:cs.CE cs.LG published:2013-07-04 summary:Changes in the UK electricity market mean that domestic users will berequired to modify their usage behaviour in order that supplies can bemaintained. Clustering allows usage profiles collected at the household levelto be clustered into groups and assigned a stereotypical profile which can beused to target marketing campaigns. Fuzzy C Means clustering extends this byallowing each household to be a member of many groups and hence provides theopportunity to make personalised offers to the household dependent on theirdegree of membership of each group. In addition, feedback can be provided onhow user's changing behaviour is moving them towards more "green" or costeffective stereotypical usage.
arxiv-3300-24 | The Application of a Data Mining Framework to Energy Usage Profiling in Domestic Residences using UK data | http://arxiv.org/pdf/1307.1380v1.pdf | author:Ian Dent, Uwe Aickelin, Tom Rodden category:cs.CE cs.LG stat.AP published:2013-07-04 summary:This paper describes a method for defining representative load profiles fordomestic electricity users in the UK. It considers bottom up and clusteringmethods and then details the research plans for implementing and improvingexisting framework approaches based on the overall usage profile. The workfocuses on adapting and applying analysis framework approaches to UK energydata in order to determine the effectiveness of creating a few (single figures)archetypical users with the intention of improving on the current methods ofdetermining usage profiles. The work is currently in progress and the paperdetails initial results using data collected in Milton Keynes around 1990.Various possible enhancements to the work are considered including a splitbased on temperature to reflect the varying UK weather conditions.
arxiv-3300-25 | Image registration with sparse approximations in parametric dictionaries | http://arxiv.org/pdf/1301.6646v2.pdf | author:Alhussein Fawzi, Pascal Frossard category:cs.CV published:2013-01-28 summary:We examine in this paper the problem of image registration from the newperspective where images are given by sparse approximations in parametricdictionaries of geometric functions. We propose a registration algorithm thatlooks for an estimate of the global transformation between sparse images byexamining the set of relative geometrical transformations between therespective features. We propose a theoretical analysis of our registrationalgorithm and we derive performance guarantees based on two novel importantproperties of redundant dictionaries, namely the robust linear independence andthe transformation inconsistency. We propose several illustrations and insightsabout the importance of these dictionary properties and show that commonproperties such as coherence or restricted isometry property fail to providesufficient information in registration problems. We finally show withillustrative experiments on simple visual objects and handwritten digits imagesthat our algorithm outperforms baseline competitor methods in terms oftransformation-invariant distance computation and classification.
arxiv-3300-26 | Further results on dissimilarity spaces for hyperspectral images RF-CBIR | http://arxiv.org/pdf/1307.1289v1.pdf | author:Miguel Angel Veganzones, Mihai Datcu, Manuel Graña category:cs.IR cs.CV published:2013-07-04 summary:Content-Based Image Retrieval (CBIR) systems are powerful search tools inimage databases that have been little applied to hyperspectral images.Relevance feedback (RF) is an iterative process that uses machine learningtechniques and user's feedback to improve the CBIR systems performance. Wepursued to expand previous research in hyperspectral CBIR systems built ondissimilarity functions defined either on spectral and spatial featuresextracted by spectral unmixing techniques, or on dictionaries extracted bydictionary-based compressors. These dissimilarity functions were not suitablefor direct application in common machine learning techniques. We propose to usea RF general approach based on dissimilarity spaces which is more appropriatefor the application of machine learning algorithms to the hyperspectralRF-CBIR. We validate the proposed RF method for hyperspectral CBIR systems overa real hyperspectral dataset.
arxiv-3300-27 | Constructing Hierarchical Image-tags Bimodal Representations for Word Tags Alternative Choice | http://arxiv.org/pdf/1307.1275v1.pdf | author:Fangxiang Feng, Ruifan Li, Xiaojie Wang category:cs.LG cs.NE published:2013-07-04 summary:This paper describes our solution to the multi-modal learning challenge ofICML. This solution comprises constructing three-level representations in threeconsecutive stages and choosing correct tag words with a data-specificstrategy. Firstly, we use typical methods to obtain level-1 representations.Each image is represented using MPEG-7 and gist descriptors with additionalfeatures released by the contest organizers. And the corresponding word tagsare represented by bag-of-words model with a dictionary of 4000 words.Secondly, we learn the level-2 representations using two stacked RBMs for eachmodality. Thirdly, we propose a bimodal auto-encoder to learn thesimilarities/dissimilarities between the pairwise image-tags as level-3representations. Finally, during the test phase, based on one observation ofthe dataset, we come up with a data-specific strategy to choose the correct tagwords leading to a leap of an improved overall performance. Our final averageaccuracy on the private test set is 100%, which ranks the first place in thischallenge.
arxiv-3300-28 | AdaBoost and Forward Stagewise Regression are First-Order Convex Optimization Methods | http://arxiv.org/pdf/1307.1192v1.pdf | author:Robert M. Freund, Paul Grigas, Rahul Mazumder category:stat.ML cs.LG math.OC published:2013-07-04 summary:Boosting methods are highly popular and effective supervised learning methodswhich combine weak learners into a single accurate model with good statisticalperformance. In this paper, we analyze two well-known boosting methods,AdaBoost and Incremental Forward Stagewise Regression (FS$_\varepsilon$), byestablishing their precise connections to the Mirror Descent algorithm, whichis a first-order method in convex optimization. As a consequence of theseconnections we obtain novel computational guarantees for these boostingmethods. In particular, we characterize convergence bounds of AdaBoost, relatedto both the margin and log-exponential loss function, for any step-sizesequence. Furthermore, this paper presents, for the first time, precisecomputational complexity results for FS$_\varepsilon$.
arxiv-3300-29 | Demosaicing and Superresolution for Color Filter Array via Residual Image Reconstruction and Sparse Representation | http://arxiv.org/pdf/1210.0115v2.pdf | author:Guangling Sun category:cs.CV published:2012-09-29 summary:A framework of demosaicing and superresolution for color filter array (CFA)via residual image reconstruction and sparse representation is presented.Giventhe intermediate image produced by certain demosaicing and interpolationtechnique, a residual image between the final reconstruction image and theintermediate image is reconstructed using sparse representation.The finalreconstruction image has richer edges and details than that of the intermediateimage. Specifically, a generic dictionary is learned from a large set ofcomposite training data composed of intermediate data and residual data. Thelearned dictionary implies a mapping between the two data. A specificdictionary adaptive to the input CFA is learned thereafter. Using the adaptivedictionary, the sparse coefficients of intermediate data are computed andtransformed to predict residual image. The residual image is added back intothe intermediate image to obtain the final reconstruction image. Experimentalresults demonstrate the state-of-the-art performance in terms of PSNR andsubjective visual perception.
arxiv-3300-30 | Learning Mixed Graphical Models | http://arxiv.org/pdf/1205.5012v3.pdf | author:Jason D. Lee, Trevor J. Hastie category:stat.ML cs.CV cs.LG math.OC published:2012-05-22 summary:We consider the problem of learning the structure of a pairwise graphicalmodel over continuous and discrete variables. We present a new pairwise modelfor graphical models with both continuous and discrete variables that isamenable to structure learning. In previous work, authors have consideredstructure learning of Gaussian graphical models and structure learning ofdiscrete models. Our approach is a natural generalization of these two lines ofwork to the mixed case. The penalization scheme involves a novel symmetric useof the group-lasso norm and follows naturally from a particular parametrizationof the model.
arxiv-3300-31 | A Novel Robust Method to Add Watermarks to Bitmap Images by Fading Technique | http://arxiv.org/pdf/1307.1166v1.pdf | author:Firas A. Jassim category:cs.CV cs.MM published:2013-07-03 summary:Digital water marking is one of the essential fields in image security andcopyright protection. The proposed technique in this paper was based on theprinciple of protecting images by hide an invisible watermark in the image. Thetechnique starts with merging the cover image and the watermark image withsuitable ratios, i.e., 99% from the cover image will be merged with 1% from thewatermark image. Technically, the fading process is irreversible but with theproposed technique, the probability to reconstruct the original watermark imageis great. There is no perceptible difference between the original andwatermarked image by human eye. The experimental results show that the proposedtechnique proven its ability to hide images that have the same size of thecover image. Three performance measures were implemented to support theproposed techniques which are MSE, PSNR, and SSIM. Fortunately, all the threemeasures have excellent values.
arxiv-3300-32 | Greedy Feature Selection for Subspace Clustering | http://arxiv.org/pdf/1303.4778v2.pdf | author:Eva L. Dyer, Aswin C. Sankaranarayanan, Richard G. Baraniuk category:cs.LG math.NA stat.ML published:2013-03-19 summary:Unions of subspaces provide a powerful generalization to linear subspacemodels for collections of high-dimensional data. To learn a union of subspacesfrom a collection of data, sets of signals in the collection that belong to thesame subspace must be identified in order to obtain accurate estimates of thesubspace structures present in the data. Recently, sparse recovery methods havebeen shown to provide a provable and robust strategy for exact featureselection (EFS)--recovering subsets of points from the ensemble that live inthe same subspace. In parallel with recent studies of EFS with L1-minimization,in this paper, we develop sufficient conditions for EFS with a greedy methodfor sparse signal recovery known as orthogonal matching pursuit (OMP).Following our analysis, we provide an empirical study of feature selectionstrategies for signals living on unions of subspaces and characterize the gapbetween sparse recovery methods and nearest neighbor (NN)-based approaches. Inparticular, we demonstrate that sparse recovery methods provide significantadvantages over NN methods and the gap between the two approaches isparticularly pronounced when the sampling of subspaces in the dataset issparse. Our results suggest that OMP may be employed to reliably recover exactfeature sets in a number of regimes where NN approaches fail to reveal thesubspace membership of points in the ensemble.
arxiv-3300-33 | Application of a clustering framework to UK domestic electricity data | http://arxiv.org/pdf/1307.1079v1.pdf | author:Ian Dent, Uwe Aickelin, Tom Rodden category:cs.CE cs.LG published:2013-07-03 summary:This paper takes an approach to clustering domestic electricity load profilesthat has been successfully used with data from Portugal and applies it to UKdata. Clustering techniques are applied and it is found that the preferredtechnique in the Portuguese work (a two stage process combining Self OrganisedMaps and Kmeans) is not appropriate for the UK data. The work shows that up tonine clusters of households can be identified with the differences in usageprofiles being visually striking. This demonstrates the appropriateness ofbreaking the electricity usage patterns down to more detail than the two loadprofiles currently published by the electricity industry. The paper detailsinitial results using data collected in Milton Keynes around 1990. Further workis described and will concentrate on building accurate and meaningful clustersof similar electricity users in order to better direct demand side managementinitiatives to the most relevant target customers.
arxiv-3300-34 | Investigating the Detection of Adverse Drug Events in a UK General Practice Electronic Health-Care Database | http://arxiv.org/pdf/1307.1078v1.pdf | author:Jenna Reps, Jan Feyereisl, Jonathan M. Garibaldi, Uwe Aickelin, Jack E. Gibson, Richard B. Hubbard category:cs.CE cs.LG published:2013-07-03 summary:Data-mining techniques have frequently been developed for Spontaneousreporting databases. These techniques aim to find adverse drug eventsaccurately and efficiently. Spontaneous reporting databases are prone tomissing information, under reporting and incorrect entries. This often resultsin a detection lag or prevents the detection of some adverse drug events. Theselimitations do not occur in electronic health-care databases. In this paper,existing methods developed for spontaneous reporting databases are implementedon both a spontaneous reporting database and a general practice electronichealth-care database and compared. The results suggests that the application ofexisting methods to the general practice database may help find signals thathave gone undetected when using the spontaneous reporting system database. Inaddition the general practice database provides far more supplementaryinformation, that if incorporated in analysis could provide a wealth ofinformation for identifying adverse events more accurately.
arxiv-3300-35 | A Comparison of Non-stationary, Type-2 and Dual Surface Fuzzy Control | http://arxiv.org/pdf/1307.1070v1.pdf | author:Naisan Benatar, Uwe Aickelin, Jonathan M. Garibaldi category:cs.AI cs.NE published:2013-07-03 summary:Type-1 fuzzy logic has frequently been used in control systems. However thismethod is sometimes shown to be too restrictive and unable to adapt in thepresence of uncertainty. In this paper we compare type-1 fuzzy control withseveral other fuzzy approaches under a range of uncertain conditions. Intervaltype-2 and non-stationary fuzzy controllers are compared, along with 'dualsurface' type-2 control, named due to utilising both the lower and upper valuesproduced from standard interval type-2 systems. We tune a type-1 controller,then derive the membership functions and footprints of uncertainty from thetype-1 system and evaluate them using a simulated autonomous sailing problemwith varying amounts of environmental uncertainty. We show that while thesemore sophisticated controllers can produce better performance than the type-1controller, this is not guaranteed and that selection of Footprint ofUncertainty (FOU) size has a large effect on this relative performance.
arxiv-3300-36 | An Efficient Model Selection for Gaussian Mixture Model in a Bayesian Framework | http://arxiv.org/pdf/1307.0995v1.pdf | author:Ji Won Yoon category:cs.LG stat.ML published:2013-07-03 summary:In order to cluster or partition data, we often useExpectation-and-Maximization (EM) or Variational approximation with a GaussianMixture Model (GMM), which is a parametric probability density functionrepresented as a weighted sum of $\hat{K}$ Gaussian component densities.However, model selection to find underlying $\hat{K}$ is one of the keyconcerns in GMM clustering, since we can obtain the desired clusters only when$\hat{K}$ is known. In this paper, we propose a new model selection algorithmto explore $\hat{K}$ in a Bayesian framework. The proposed algorithm builds thedensity of the model order which any information criterions such as AIC and BICbasically fail to reconstruct. In addition, this algorithm reconstructs thedensity quickly as compared to the time-consuming Monte Carlo simulation.
arxiv-3300-37 | Extending UML for Conceptual Modeling of Annotation of Medical Images | http://arxiv.org/pdf/1307.0937v1.pdf | author:Mouhamed Gaith Ayadi, Riadh Bouslimi, Jalel Akaichi category:cs.CV published:2013-07-03 summary:Imaging has occupied a huge role in the management of patients, whetherhospitalized or not. Depending on the patients clinical problem, a variety ofimaging modalities were available for use. This gave birth of the annotation ofmedical image process. The annotation is intended to image analysis and solvethe problem of semantic gap. The reason for image annotation is due to increasein acquisition of images. Physicians and radiologists feel better while usingannotation techniques for faster remedy in surgery and medicine due to thefollowing reasons: giving details to the patients, searching the present andpast records from the larger databases, and giving solutions to them in afaster and more accurate way. However, classical conceptual modeling does notincorporate the specificity of medical domain specially the annotation ofmedical image. The design phase is the most important activity in thesuccessful building of annotation process. For this reason, we focus in thispaper on presenting the conceptual modeling of the annotation of medical imageby defining a new profile using the StarUML extensibility mechanism.
arxiv-3300-38 | Separation of cardiac and respiratory components from the electrical bio-impedance signal using PCA and fast ICA | http://arxiv.org/pdf/1307.0915v1.pdf | author:Yar M. Mughal, A. Krivoshei, P. Annus category:stat.AP stat.ML published:2013-07-03 summary:This paper is an attempt to separate cardiac and respiratory signals from anelectrical bio-impedance (EBI) dataset. For this two well-known algorithms,namely Principal Component Analysis (PCA) and Independent Component Analysis(ICA), were used to accomplish the task. The ability of the PCA and the ICAmethods first reduces the dimension and attempt to separate the usefulcomponents of the EBI, the cardiac and respiratory ones accordingly. It wasinvestigated with an assumption, that no motion artefacts are present. To carryout this procedure the two channel complex EBI measurements were provided usingclassical Kelvin type four electrode configurations for the each complexchannel. Thus four real signals were used as inputs for the PCA and fast ICA.The results showed, that neither PCA nor ICA nor combination of them can notaccurately separate the components at least are used only two complex (fourreal valued) input components.
arxiv-3300-39 | Semi-supervised Ranking Pursuit | http://arxiv.org/pdf/1307.0846v1.pdf | author:Evgeni Tsivtsivadze, Tom Heskes category:stat.ML cs.IR cs.LG published:2013-07-02 summary:We propose a novel sparse preference learning/ranking algorithm. Ouralgorithm approximates the true utility function by a weighted sum of basisfunctions using the squared loss on pairs of data points, and is ageneralization of the kernel matching pursuit method. It can operate both in asupervised and a semi-supervised setting and allows efficient search formultiple, near-optimal solutions. Furthermore, we describe the extension of thealgorithm suitable for combined ranking and regression tasks. In ourexperiments we demonstrate that the proposed algorithm outperforms severalstate-of-the-art learning methods when taking into account unlabeled data andperforms comparably in a supervised learning scenario, while providing sparsersolutions.
arxiv-3300-40 | Comparing various regression methods on ensemble strategies in differential evolution | http://arxiv.org/pdf/1307.0841v1.pdf | author:Iztok Fister Jr., Iztok Fister, Janez Brest category:cs.NE published:2013-07-02 summary:Differential evolution possesses a multitude of various strategies forgenerating new trial solutions. Unfortunately, the best strategy is not knownin advance. Moreover, this strategy usually depends on the problem to besolved. This paper suggests using various regression methods (like randomforest, extremely randomized trees, gradient boosting, decision trees, and ageneralized linear model) on ensemble strategies in differential evolutionalgorithm by predicting the best differential evolution strategy during therun. Comparing the preliminary results of this algorithm by optimizing a suiteof five well-known functions from literature, it was shown that using therandom forest regression method substantially outperformed the results of theother regression methods.
arxiv-3300-41 | Approximate Counting of Graphical Models Via MCMC Revisited | http://arxiv.org/pdf/1301.7189v2.pdf | author:Jose M. Peña category:stat.ML cs.AI published:2013-01-30 summary:In Pe\~na (2007), MCMC sampling is applied to approximately calculate theratio of essential graphs (EGs) to directed acyclic graphs (DAGs) for up to 20nodes. In the present paper, we extend that work from 20 to 31 nodes. We alsoextend that work by computing the approximate ratio of connected EGs toconnected DAGs, of connected EGs to EGs, and of connected DAGs to DAGs.Furthermore, we prove that the latter ratio is asymptotically 1. We alsodiscuss the implications of these results for learning DAGs from data.
arxiv-3300-42 | Distributed Online Big Data Classification Using Context Information | http://arxiv.org/pdf/1307.0781v1.pdf | author:Cem Tekin, Mihaela van der Schaar category:cs.LG stat.ML published:2013-07-02 summary:Distributed, online data mining systems have emerged as a result ofapplications requiring analysis of large amounts of correlated andhigh-dimensional data produced by multiple distributed data sources. We proposea distributed online data classification framework where data is gathered bydistributed data sources and processed by a heterogeneous set of distributedlearners which learn online, at run-time, how to classify the different datastreams either by using their locally available classification functions or byhelping each other by classifying each other's data. Importantly, since thedata is gathered at different locations, sending the data to another learner toprocess incurs additional costs such as delays, and hence this will be onlybeneficial if the benefits obtained from a better classification will exceedthe costs. We model the problem of joint classification by the distributed andheterogeneous learners from multiple data sources as a distributed contextualbandit problem where each data is characterized by a specific context. Wedevelop a distributed online learning algorithm for which we can provesublinear regret. Compared to prior work in distributed online data mining, ourwork is the first to provide analytic regret results characterizing theperformance of the proposed algorithm.
arxiv-3300-43 | Regularized Spherical Polar Fourier Diffusion MRI with Optimal Dictionary Learning | http://arxiv.org/pdf/1307.0776v1.pdf | author:Jian Cheng, Tianzi Jiang, Rachid Deriche, Dinggang Shen, Pew-Thian Yap category:cs.CV published:2013-07-02 summary:Compressed Sensing (CS) takes advantage of signal sparsity or compressibilityand allows superb signal reconstruction from relatively few measurements. Basedon CS theory, a suitable dictionary for sparse representation of the signal isrequired. In diffusion MRI (dMRI), CS methods were proposed to reconstructdiffusion-weighted signal and the Ensemble Average Propagator (EAP), and thereare two kinds of Dictionary Learning (DL) methods: 1) Discrete RepresentationDL (DR-DL), and 2) Continuous Representation DL (CR-DL). DR-DL is susceptibleto numerical inaccuracy owing to interpolation and regridding errors in adiscretized q-space. In this paper, we propose a novel CR-DL approach, calledDictionary Learning - Spherical Polar Fourier Imaging (DL-SPFI) for effectivecompressed-sensing reconstruction of the q-space diffusion-weighted signal andthe EAP. In DL-SPFI, an dictionary that sparsifies the signal is learned fromthe space of continuous Gaussian diffusion signals. The learned dictionary isthen adaptively applied to different voxels using a weighted LASSO frameworkfor robust signal reconstruction. The adaptive dictionary is proved to beoptimal. Compared with the start-of-the-art CR-DL and DR-DL methods proposed byMerlet et al. and Bilgic et al., espectively, our work offers the followingadvantages. First, the learned dictionary is proved to be optimal for Gaussiandiffusion signals. Second, to our knowledge, this is the first work to learn avoxel-adaptive dictionary. The importance of the adaptive dictionary in EAPreconstruction will be demonstrated theoretically and empirically. Third,optimization in DL-SPFI is only performed in a small subspace resided by theSPF coefficients, as opposed to the q-space approach utilized by Merlet et al.The experiment results demonstrate the advantages of DL-SPFI over the originalSPF basis and Bilgic et al.'s method.
arxiv-3300-44 | Submodularity of a Set Label Disagreement Function | http://arxiv.org/pdf/1307.1303v1.pdf | author:Toufiq Parag category:cs.CV published:2013-07-02 summary:A set label disagreement function is defined over the number of variablesthat deviates from the dominant label. The dominant label is the value assumedby the largest number of variables within a set of binary variables. Thesubmodularity of a certain family of set label disagreement function isdiscussed in this manuscript. Such disagreement function could be utilized as acost function in combinatorial optimization approaches for problems definedover hypergraphs.
arxiv-3300-45 | Learning from networked examples in a k-partite graph | http://arxiv.org/pdf/1306.0393v2.pdf | author:Yuyi Wang, Jan Ramon, Zheng-Chu Guo category:cs.LG stat.ML published:2013-06-03 summary:Many machine learning algorithms are based on the assumption that trainingexamples are drawn independently. However, this assumption does not holdanymore when learning from a networked sample where two or more trainingexamples may share common features. We propose an efficient weighting methodfor learning from networked examples and show the sample error bound which isbetter than previous work.
arxiv-3300-46 | Discovering the Markov network structure | http://arxiv.org/pdf/1307.0643v1.pdf | author:Edith Kovács, Tamás Szántai category:cs.IT cs.LG math.IT published:2013-07-02 summary:In this paper a new proof is given for the supermodularity of informationcontent. Using the decomposability of the information content an algorithm isgiven for discovering the Markov network graph structure endowed by thepairwise Markov property of a given probability distribution. A discreteprobability distribution is given for which the equivalence ofHammersley-Clifford theorem is fulfilled although some of the possible vectorrealizations are taken on with zero probability. Our algorithm for discoveringthe pairwise Markov network is illustrated on this example, too.
arxiv-3300-47 | Improving Pointwise Mutual Information (PMI) by Incorporating Significant Co-occurrence | http://arxiv.org/pdf/1307.0596v1.pdf | author:Om P. Damani category:cs.CL published:2013-07-02 summary:We design a new co-occurrence based word association measure by incorporatingthe concept of significant cooccurrence in the popular word association measurePointwise Mutual Information (PMI). By extensive experiments with a largenumber of publicly available datasets we show that the newly introduced measureperforms better than other co-occurrence based measures and despite beingresource-light, compares well with the best known resource-heavy distributionalsimilarity and knowledge based word association measures. We investigate thesource of this performance improvement and find that of the two types ofsignificant co-occurrence - corpus-level and document-level, the concept ofcorpus level significance combined with the use of document counts in place ofword counts is responsible for all the performance gains observed. The conceptof document level significance is not helpful for PMI adaptation.
arxiv-3300-48 | The Orchive : Data mining a massive bioacoustic archive | http://arxiv.org/pdf/1307.0589v1.pdf | author:Steven Ness, Helena Symonds, Paul Spong, George Tzanetakis category:cs.LG cs.DB cs.SD published:2013-07-02 summary:The Orchive is a large collection of over 20,000 hours of audio recordingsfrom the OrcaLab research facility located off the northern tip of VancouverIsland. It contains recorded orca vocalizations from the 1980 to the presenttime and is one of the largest resources of bioacoustic data in the world. Wehave developed a web-based interface that allows researchers to listen to theserecordings, view waveform and spectral representations of the audio, labelclips with annotations, and view the results of machine learning classifiersbased on automatic audio features extraction. In this paper we describe suchclassifiers that discriminate between background noise, orca calls, and thevoice notes that are present in most of the tapes. Furthermore we showclassification results for individual calls based on a previously existing orcacall catalog. We have also experimentally investigated the scalability ofclassifiers over the entire Orchive.
arxiv-3300-49 | A non-parametric conditional factor regression model for high-dimensional input and response | http://arxiv.org/pdf/1307.0578v1.pdf | author:Ava Bargi, Richard Yi Da Xu, Massimo Piccardi category:stat.ML cs.LG published:2013-07-02 summary:In this paper, we propose a non-parametric conditional factor regression(NCFR)model for domains with high-dimensional input and response. NCFR enhanceslinear regression in two ways: a) introducing low-dimensional latent factorsleading to dimensionality reduction and b) integrating an Indian Buffet Processas a prior for the latent factors to derive unlimited sparse dimensions.Experimental results comparing NCRF to several alternatives give evidence toremarkable prediction performance.
arxiv-3300-50 | Evaluation Measures for Hierarchical Classification: a unified view and novel approaches | http://arxiv.org/pdf/1306.6802v2.pdf | author:Aris Kosmopoulos, Ioannis Partalas, Eric Gaussier, Georgios Paliouras, Ion Androutsopoulos category:cs.AI cs.LG published:2013-06-28 summary:Hierarchical classification addresses the problem of classifying items into ahierarchy of classes. An important issue in hierarchical classification is theevaluation of different classification algorithms, which is complicated by thehierarchical relations among the classes. Several evaluation measures have beenproposed for hierarchical classification using the hierarchy in different ways.This paper studies the problem of evaluation in hierarchical classification byanalyzing and abstracting the key components of the existing performancemeasures. It also proposes two alternative generic views of hierarchicalevaluation and introduces two corresponding novel measures. The proposedmeasures, along with the state-of-the art ones, are empirically tested on threelarge datasets from the domain of text classification. The empirical resultsillustrate the undesirable behavior of existing approaches and how the proposedmethods overcome most of these methods across a range of cases.
arxiv-3300-51 | Challenges in Representation Learning: A report on three machine learning contests | http://arxiv.org/pdf/1307.0414v1.pdf | author:Ian J. Goodfellow, Dumitru Erhan, Pierre Luc Carrier, Aaron Courville, Mehdi Mirza, Ben Hamner, Will Cukierski, Yichuan Tang, David Thaler, Dong-Hyun Lee, Yingbo Zhou, Chetan Ramaiah, Fangxiang Feng, Ruifan Li, Xiaojie Wang, Dimitris Athanasakis, John Shawe-Taylor, Maxim Milakov, John Park, Radu Ionescu, Marius Popescu, Cristian Grozea, James Bergstra, Jingjing Xie, Lukasz Romaszko, Bing Xu, Zhang Chuang, Yoshua Bengio category:stat.ML cs.LG published:2013-07-01 summary:The ICML 2013 Workshop on Challenges in Representation Learning focused onthree challenges: the black box learning challenge, the facial expressionrecognition challenge, and the multimodal learning challenge. We describe thedatasets created for these challenges and summarize the results of thecompetitions. We provide suggestions for organizers of future challenges andsome comments on what kind of knowledge can be gained from machine learningcompetitions.
arxiv-3300-52 | Gaussian Process Conditional Copulas with Applications to Financial Time Series | http://arxiv.org/pdf/1307.0373v1.pdf | author:José Miguel Hernández-Lobato, James Robert Lloyd, Daniel Hernández-Lobato category:stat.ML published:2013-07-01 summary:The estimation of dependencies between multiple variables is a centralproblem in the analysis of financial time series. A common approach is toexpress these dependencies in terms of a copula function. Typically the copulafunction is assumed to be constant but this may be inaccurate when there arecovariates that could have a large influence on the dependence structure of thedata. To account for this, a Bayesian framework for the estimation ofconditional copulas is proposed. In this framework the parameters of a copulaare non-linearly related to some arbitrary conditioning variables. We evaluatethe ability of our method to predict time-varying dependencies on severalequities and currencies and observe consistent performance gains compared tostatic copula models and other time-varying copula methods.
arxiv-3300-53 | Quantifying the Impact of Parameter Tuning on Nature-Inspired Algorithms | http://arxiv.org/pdf/1305.0763v2.pdf | author:Matthew Crossley, Andy Nisbet, Martyn Amos category:cs.NE published:2013-05-03 summary:The problem of parameterization is often central to the effective deploymentof nature-inspired algorithms. However, finding the optimal set of parametervalues for a combination of problem instance and solution method is highlychallenging, and few concrete guidelines exist on how and when such tuning maybe performed. Previous work tends to either focus on a specific algorithm oruse benchmark problems, and both of these restrictions limit the applicabilityof any findings. Here, we examine a number of different algorithms, and studythem in a "problem agnostic" fashion (i.e., one that is not tied to specificinstances) by considering their performance on fitness landscapes with varyingcharacteristics. Using this approach, we make a number of observations on whichalgorithms may (or may not) benefit from tuning, and in which specificcircumstances.
arxiv-3300-54 | Dimensionality Detection and Integration of Multiple Data Sources via the GP-LVM | http://arxiv.org/pdf/1307.0323v1.pdf | author:James Barrett, Anthony C. C. Coolen category:stat.ML published:2013-07-01 summary:The Gaussian Process Latent Variable Model (GP-LVM) is a non-linearprobabilistic method of embedding a high dimensional dataset in terms lowdimensional `latent' variables. In this paper we illustrate that maximum aposteriori (MAP) estimation of the latent variables and hyperparameters can beused for model selection and hence we can determine the optimal number orlatent variables and the most appropriate model. This is an alternative to thevariational approaches developed recently and may be useful when we want to usea non-Gaussian prior or kernel functions that don't have automatic relevancedetermination (ARD) parameters. Using a second order expansion of the latentvariable posterior we can marginalise the latent variables and obtain anestimate for the hyperparameter posterior. Secondly, we use the GP-LVM tointegrate multiple data sources by simultaneously embedding them in terms ofcommon latent variables. We present results from synthetic data to illustratethe successful detection and retrieval of low dimensional structure from highdimensional data. We demonstrate that the integration of multiple data sourcesleads to more robust performance. Finally, we show that when the data are usedfor binary classification tasks we can attain a significant gain in predictionaccuracy when the low dimensional representation is used.
arxiv-3300-55 | Algorithms of the LDA model [REPORT] | http://arxiv.org/pdf/1307.0317v1.pdf | author:Jaka Špeh, Andrej Muhič, Jan Rupnik category:cs.LG cs.IR stat.ML published:2013-07-01 summary:We review three algorithms for Latent Dirichlet Allocation (LDA). Two of themare variational inference algorithms: Variational Bayesian inference and OnlineVariational Bayesian inference and one is Markov Chain Monte Carlo (MCMC)algorithm -- Collapsed Gibbs sampling. We compare their time complexity andperformance. We find that online variational Bayesian inference is the fastestalgorithm and still returns reasonably good results.
arxiv-3300-56 | A Counterexample for the Validity of Using Nuclear Norm as a Convex Surrogate of Rank | http://arxiv.org/pdf/1304.6233v2.pdf | author:Hongyang Zhang, Zhouchen Lin, Chao Zhang category:stat.ML math.OC published:2013-04-23 summary:Rank minimization has attracted a lot of attention due to its robustness indata recovery. To overcome the computational difficulty, rank is often replacedwith nuclear norm. For several rank minimization problems, such a replacementhas been theoretically proven to be valid, i.e., the solution to nuclear normminimization problem is also the solution to rank minimization problem.Although it is easy to believe that such a replacement may not always be valid,no concrete example has ever been found. We argue that such a validity checkingcannot be done by numerical computation and show, by analyzing the noiselesslatent low rank representation (LatLRR) model, that even for very simple rankminimization problems the validity may still break down. As a by-product, wefind that the solution to the nuclear norm minimization formulation of LatLRRis non-unique. Hence the results of LatLRR reported in the literature may bequestionable.
arxiv-3300-57 | Multilevel Threshold Based Gray Scale Image Segmentation using Cuckoo Search | http://arxiv.org/pdf/1307.0277v1.pdf | author:Sourav Samantaa, Nilanjan Dey, Poulami Das, Suvojit Acharjee, Sheli Sinha Chaudhuri category:cs.CV published:2013-07-01 summary:Image Segmentation is a technique of partitioning the original image intosome distinct classes. Many possible solutions may be available for segmentingan image into a certain number of classes, each one having different quality ofsegmentation. In our proposed method, multilevel thresholding technique hasbeen used for image segmentation. A new approach of Cuckoo Search (CS) is usedfor selection of optimal threshold value. In other words, the algorithm is usedto achieve the best solution from the initial random threshold values orsolutions and to evaluate the quality of a solution correlation function isused. Finally, MSE and PSNR are measured to understand the segmentationquality.
arxiv-3300-58 | WebSets: Extracting Sets of Entities from the Web Using Unsupervised Information Extraction | http://arxiv.org/pdf/1307.0261v1.pdf | author:Bhavana Dalvi, William W. Cohen, Jamie Callan category:cs.LG cs.CL cs.IR published:2013-07-01 summary:We describe a open-domain information extraction method for extractingconcept-instance pairs from an HTML corpus. Most earlier approaches to thisproblem rely on combining clusters of distributionally similar terms andconcept-instance pairs obtained with Hearst patterns. In contrast, our methodrelies on a novel approach for clustering terms found in HTML tables, and thenassigning concept names to these clusters using Hearst patterns. The method canbe efficiently applied to a large corpus, and experimental results on severaldatasets show that our method can accurately extract large numbers ofconcept-instance pairs.
arxiv-3300-59 | Exploratory Learning | http://arxiv.org/pdf/1307.0253v1.pdf | author:Bhavana Dalvi, William W. Cohen, Jamie Callan category:cs.LG published:2013-07-01 summary:In multiclass semi-supervised learning (SSL), it is sometimes the case thatthe number of classes present in the data is not known, and hence no labeledexamples are provided for some classes. In this paper we present variants ofwell-known semi-supervised multiclass learning methods that are robust when thedata contains an unknown number of classes. In particular, we present an"exploratory" extension of expectation-maximization (EM) that exploresdifferent numbers of classes while learning. "Exploratory" SSL greatly improvesperformance on three datasets in terms of F1 on the classes with seed examplesi.e., the classes which are expected to be in the data. Our Exploratory EMalgorithm also outperforms a SSL method based non-parametric Bayesianclustering.
arxiv-3300-60 | Semi-supervised clustering methods | http://arxiv.org/pdf/1307.0252v1.pdf | author:Eric Bair category:stat.ME cs.LG stat.ML published:2013-07-01 summary:Cluster analysis methods seek to partition a data set into homogeneoussubgroups. It is useful in a wide variety of applications, including documentprocessing and modern genetics. Conventional clustering methods areunsupervised, meaning that there is no outcome variable nor is anything knownabout the relationship between the observations in the data set. In manysituations, however, information about the clusters is available in addition tothe values of the features. For example, the cluster labels of someobservations may be known, or certain observations may be known to belong tothe same cluster. In other cases, one may wish to identify clusters that areassociated with a particular outcome variable. This review describes severalclustering algorithms (known as "semi-supervised clustering" methods) that canbe applied in these situations. The majority of these methods are modificationsof the popular k-means clustering method, and several of them will be describedin detail. A brief description of some other semi-supervised clusteringalgorithms is also provided.
arxiv-3300-61 | Sparse Principal Component Analysis for High Dimensional Vector Autoregressive Models | http://arxiv.org/pdf/1307.0164v1.pdf | author:Zhaoran Wang, Fang Han, Han Liu category:stat.ML published:2013-06-30 summary:We study sparse principal component analysis for high dimensional vectorautoregressive time series under a doubly asymptotic framework, which allowsthe dimension $d$ to scale with the series length $T$. We treat the transitionmatrix of time series as a nuisance parameter and directly apply sparseprincipal component analysis on multivariate time series as if the data areindependent. We provide explicit non-asymptotic rates of convergence forleading eigenvector estimation and extend this result to principal subspaceestimation. Our analysis illustrates that the spectral norm of the transitionmatrix plays an essential role in determining the final rates. We alsocharacterize sufficient conditions under which sparse principal componentanalysis attains the optimal parametric rate. Our theoretical results arebacked up by thorough numerical studies.
arxiv-3300-62 | Hyperspectral Data Unmixing Using GNMF Method and Sparseness Constraint | http://arxiv.org/pdf/1307.0129v1.pdf | author:Roozbeh Rajabi, Hassan Ghassemian category:cs.CV published:2013-06-29 summary:Hyperspectral images contain mixed pixels due to low spatial resolution ofhyperspectral sensors. Mixed pixels are pixels containing more than onedistinct material called endmembers. The presence percentages of endmembers inmixed pixels are called abundance fractions. Spectral unmixing problem refersto decomposing these pixels into a set of endmembers and abundance fractions.Due to nonnegativity constraint on abundance fractions, nonnegative matrixfactorization methods (NMF) have been widely used for solving spectral unmixingproblem. In this paper we have used graph regularized (GNMF) method withsparseness constraint to unmix hyperspectral data. This method applied onsimulated data using AVIRIS Indian Pines dataset and USGS library and resultsare quantified based on AAD and SAD measures. Results in comparison with othermethods show that the proposed method can unmix data more effectively.
arxiv-3300-63 | Concentration and Confidence for Discrete Bayesian Sequence Predictors | http://arxiv.org/pdf/1307.0127v1.pdf | author:Tor Lattimore, Marcus Hutter, Peter Sunehag category:cs.LG stat.ML published:2013-06-29 summary:Bayesian sequence prediction is a simple technique for predicting futuresymbols sampled from an unknown measure on infinite sequences over a countablealphabet. While strong bounds on the expected cumulative error are known, thereare only limited results on the distribution of this error. We prove tighthigh-probability bounds on the cumulative error, which is measured in terms ofthe Kullback-Leibler (KL) divergence. We also consider the problem ofconstructing upper confidence bounds on the KL and Hellinger errors similar tothose constructed from Hoeffding-like bounds in the i.i.d. case. The newresults are applied to show that Bayesian sequence prediction can be used inthe Knows What It Knows (KWIK) framework with bounds that match thestate-of-the-art.
arxiv-3300-64 | Semantics and pragmatics in actual software applications and in web search engines: exploring innovations | http://arxiv.org/pdf/1307.0087v1.pdf | author:Fabrizio M. A. Lolli category:cs.IR cs.CL cs.HC published:2013-06-29 summary:While new ways to use the Semantic Web are developed every week, which allowthe user to find information on web more accurately - for example in searchengines - some sophisticated pragmatic tools are becoming more important - forexample in web interfaces known as Social Intelligence, or in the most famousSiri by Apple. The work aims to analyze whether and where we can identify theboundary between semantics and pragmatics in the software used by analyzedsystems. examining how the linguistic disciplines are fundamental in theirprogress. Is it possible to assume that the tools of social intelligence have apragmatic approach to the questions of the user, or it is just a use of a veryrich vocabulary, with the use of semantic tools?
arxiv-3300-65 | Approximate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs | http://arxiv.org/pdf/1307.0060v1.pdf | author:Vikash K. Mansinghka, Tejas D. Kulkarni, Yura N. Perov, Joshua B. Tenenbaum category:cs.AI cs.CV stat.ML published:2013-06-29 summary:The idea of computer vision as the Bayesian inverse problem to computergraphics has a long history and an appealing elegance, but it has proveddifficult to directly implement. Instead, most vision tasks are approached viacomplex bottom-up processing pipelines. Here we show that it is possible towrite short, simple probabilistic graphics programs that define flexiblegenerative models and to automatically invert them to interpret real-worldimages. Generative probabilistic graphics programs consist of a stochasticscene generator, a renderer based on graphics software, a stochastic likelihoodmodel linking the renderer's output and the data, and latent variables thatadjust the fidelity of the renderer and the tolerance of the likelihood model.Representations and algorithms from computer graphics, originally designed toproduce high-quality images, are instead used as the deterministic backbone forhighly approximate and stochastic generative models. This formulation combinesprobabilistic programming, computer graphics, and approximate Bayesiancomputation, and depends only on general-purpose, automatic inferencetechniques. We describe two applications: reading sequences of degraded andadversarially obscured alphanumeric characters, and inferring 3D road modelsfrom vehicle-mounted camera images. Each of the probabilistic graphics programswe present relies on under 20 lines of probabilistic code, and supportsaccurate, approximately Bayesian inferences about ambiguous real-world images.
arxiv-3300-66 | P-HGRMS: A Parallel Hypergraph Based Root Mean Square Algorithm for Image Denoising | http://arxiv.org/pdf/1306.5390v2.pdf | author:Tejaswi Agarwal, Saurabh Jha, B. Rajesh Kanna category:cs.DC cs.CV I.3 published:2013-06-23 summary:This paper presents a parallel Salt and Pepper (SP) noise removal algorithmin a grey level digital image based on the Hypergraph Based Root Mean Square(HGRMS) approach. HGRMS is generic algorithm for identifying noisy pixels inany digital image using a two level hierarchical serial approach. However, forSP noise removal, we reduce this algorithm to a parallel model by introducing acardinality matrix and an iteration factor, k, which helps us reduce thedependencies in the existing approach. We also observe that the performance ofthe serial implementation is better on smaller images, but once the thresholdis achieved in terms of image resolution, its computational complexityincreases drastically. We test P-HGRMS using standard images from the BerkeleySegmentation dataset on NVIDIAs Compute Unified Device Architecture (CUDA) fornoise identification and attenuation. We also compare the noise removalefficiency of the proposed algorithm using Peak Signal to Noise Ratio (PSNR) tothe existing approach. P-HGRMS maintains the noise removal efficiency andoutperforms its sequential counterpart by 6 to 18 times (6x - 18x) incomputational efficiency.
arxiv-3300-67 | Increasing Compression Ratio in PNG Images by k-Modulus Method for Image Transformation | http://arxiv.org/pdf/1307.0036v1.pdf | author:Firas A. Jassim category:cs.CV cs.MM published:2013-06-28 summary:Image compression is an important filed in image processing. The sciencewelcomes any tinny contribution that may increase the compression ratio bywhichever insignificant percentage. Therefore, the essential contribution inthis paper is to increase the compression ratio for the well known PortableNetwork Graphics (PNG) image file format. The contribution starts withconverting the original PNG image into k-Modulus Method (k-MM). Practically,taking k equals to ten, and then the pixels in the constructed image will beintegers divisible by ten. Since PNG uses Lempel-Ziv compression algorithm,then the ability to reduce file size will increase according to the repetitionin pixels in each k-by-k window according to the transformation done by k-MM.Experimental results show that the proposed technique (k-PNG) produces highcompression ratio with smaller file size in comparison to the original PNGfile.
arxiv-3300-68 | Memory Limited, Streaming PCA | http://arxiv.org/pdf/1307.0032v1.pdf | author:Ioannis Mitliagkas, Constantine Caramanis, Prateek Jain category:stat.ML cs.IT cs.LG math.IT published:2013-06-28 summary:We consider streaming, one-pass principal component analysis (PCA), in thehigh-dimensional regime, with limited memory. Here, $p$-dimensional samples arepresented sequentially, and the goal is to produce the $k$-dimensional subspacethat best approximates these points. Standard algorithms require $O(p^2)$memory; meanwhile no algorithm can do better than $O(kp)$ memory, since this iswhat the output itself requires. Memory (or storage) complexity is mostmeaningful when understood in the context of computational and samplecomplexity. Sample complexity for high-dimensional PCA is typically studied inthe setting of the {\em spiked covariance model}, where $p$-dimensional pointsare generated from a population covariance equal to the identity (white noise)plus a low-dimensional perturbation (the spike) which is the signal to berecovered. It is now well-understood that the spike can be recovered when thenumber of samples, $n$, scales proportionally with the dimension, $p$. Yet, allalgorithms that provably achieve this, have memory complexity $O(p^2)$.Meanwhile, algorithms with memory-complexity $O(kp)$ do not have provablebounds on sample complexity comparable to $p$. We present an algorithm thatachieves both: it uses $O(kp)$ memory (meaning storage of any kind) and is ableto compute the $k$-dimensional spike with $O(p \log p)$ sample-complexity --the first algorithm of its kind. While our theoretical analysis focuses on thespiked covariance model, our simulations show that our algorithm is successfulon much more general models for the data.
arxiv-3300-69 | Bioacoustical Periodic Pulse Train Signal Detection and Classification using Spectrogram Intensity Binarization and Energy Projection | http://arxiv.org/pdf/1305.3250v3.pdf | author:Marian Popescu, Peter J. Dugan, Mohammad Pourhomayoun, Denise Risch, Harold W. Lewis III, Christopher W. Clark category:cs.CV published:2013-05-14 summary:The following work outlines an approach for automatic detection andrecognition of periodic pulse train signals using a multi-stage process basedon spectrogram edge detection, energy projection and classification. The methodhas been implemented to automatically detect and recognize pulse train songs ofminke whales. While the long term goal of this work is to properly identify anddetect minke songs from large multi-year datasets, this effort was developedusing sounds off the coast of Massachusetts, in the Stellwagen Bank NationalMarine Sanctuary. The detection methodology is presented and evaluated on 232continuous hours of acoustic recordings and a qualitative analysis of machinelearning classifiers and their performance is described. The trained automaticdetection and classification system is applied to 120 continuous hours,comprised of various challenges such as broadband and narrowband noises, lowSNR, and other pulse train signatures. This automatic system achieves a TPR of63% for FPR of 0.6% (or 0.87 FP/h), at a Precision (PPV) of 84% and an F1 scoreof 71%.
arxiv-3300-70 | New Mathematical and Algorithmic Schemes for Pattern Classification with Application to the Identification of Writers of Important Ancient Documents | http://arxiv.org/pdf/1306.6842v1.pdf | author:Dimitris Arabadjis, Fotios Giannopoulos, Constantin Papaodysseus, Solomon Zannos, Panayiotis Rousopoulos, Michail Panagopoulos, Christopher Blackwell category:cs.CV published:2013-06-28 summary:In this paper, a novel approach is introduced for classifying curves intoproper families, according to their similarity. First, a mathematical quantitywe call plane curvature is introduced and a number of propositions are statedand proved. Proper similarity measures of two curves are introduced and asubsequent statistical analysis is applied. First, the efficiency of the curvefitting process has been tested on 2 shapes datasets of reference. Next, themethodology has been applied to the very important problem of classifying 23Byzantine codices and 46 Ancient inscriptions to their writers, thus achievingcorrect dating of their content. The inscriptions have been attributed to tenindividual hands and the Byzantine codices to four writers.
arxiv-3300-71 | ABC Reinforcement Learning | http://arxiv.org/pdf/1303.6977v4.pdf | author:Christos Dimitrakakis, Nikolaos Tziortziotis category:stat.ML cs.LG published:2013-03-27 summary:This paper introduces a simple, general framework for likelihood-freeBayesian reinforcement learning, through Approximate Bayesian Computation(ABC). The main advantage is that we only require a prior distribution on aclass of simulators (generative models). This is useful in domains where ananalytical probabilistic model of the underlying process is too complex toformulate, but where detailed simulation models are available. ABC-RL allowsthe use of any Bayesian reinforcement learning technique, even in this case. Inaddition, it can be seen as an extension of rollout algorithms to the casewhere we do not know what the correct model to draw rollouts from is. Weexperimentally demonstrate the potential of this approach in a comparison withLSPI. Finally, we introduce a theorem showing that ABC is a sound methodologyin principle, even when non-sufficient statistics are used.
arxiv-3300-72 | An open diachronic corpus of historical Spanish: annotation criteria and automatic modernisation of spelling | http://arxiv.org/pdf/1306.3692v2.pdf | author:Felipe Sánchez-Martínez, Isabel Martínez-Sempere, Xavier Ivars-Ribes, Rafael C. Carrasco category:cs.CL cs.DL published:2013-06-16 summary:The IMPACT-es diachronic corpus of historical Spanish compiles over onehundred books --containing approximately 8 million words-- in addition to acomplementary lexicon which links more than 10 thousand lemmas withattestations of the different variants found in the documents. This textualcorpus and the accompanying lexicon have been released under an open license(Creative Commons by-nc-sa) in order to permit their intensive exploitation inlinguistic research. Approximately 7% of the words in the corpus (a selectionaimed at enhancing the coverage of the most frequent word forms) have beenannotated with their lemma, part of speech, and modern equivalent. This paperdescribes the annotation criteria followed and the standards, based on the TextEncoding Initiative recommendations, used to the represent the texts in digitalform. As an illustration of the possible synergies between diachronic textualresources and linguistic research, we describe the application of statisticalmachine translation techniques to infer probabilistic context-sensitive rulesfor the automatic modernisation of spelling. The automatic modernisation withthis type of statistical methods leads to very low character error rates whenthe output is compared with the supervised modern version of the text.
arxiv-3300-73 | Arabizi Detection and Conversion to Arabic | http://arxiv.org/pdf/1306.6755v1.pdf | author:Kareem Darwish category:cs.CL cs.IR I.2.7 published:2013-06-28 summary:Arabizi is Arabic text that is written using Latin characters. Arabizi isused to present both Modern Standard Arabic (MSA) or Arabic dialects. It iscommonly used in informal settings such as social networking sites and is oftenwith mixed with English. In this paper we address the problems of: identifyingArabizi in text and converting it to Arabic characters. We used word andsequence-level features to identify Arabizi that is mixed with English. Weachieved an identification accuracy of 98.5%. As for conversion, we usedtransliteration mining with language modeling to generate equivalent Arabictext. We achieved 88.7% conversion accuracy, with roughly a third of errorsbeing spelling and morphological variants of the forms in ground truth.
arxiv-3300-74 | Digital Image Tamper Detection Techniques - A Comprehensive Study | http://arxiv.org/pdf/1306.6737v1.pdf | author:Minati Mishra, Flt. Lt. Dr. M. C. Adhikary category:cs.CR cs.CV published:2013-06-28 summary:Photographs are considered to be the most powerful and trustworthy media ofexpression. For a long time, those were accepted as proves of evidences invaried fields such as journalism, forensic investigations, militaryintelligence, scientific research and publications, crime detection and legalproceedings, investigation of insurance claims, medical imaging etc. Today,digital images have completely replaced the conventional photographs from everysphere of life but unfortunately, they seldom enjoy the credibility of theirconventional counterparts, thanks to the rapid advancements in the field ofdigital image processing. The increasing availability of low cost and sometimesfree of cost image editing software such as Photoshop, Corel Paint Shop,Photoscape, PhotoPlus, GIMP and Pixelmator have made the tampering of digitalimages even more easier and a common practice. Now it has become quiteimpossible to say whether a photograph is a genuine camera output or amanipulated version of it just by looking at it. As a result, photographs havealmost lost their reliability and place as proves of evidences in all fields.This is why digital image tamper detection has emerged as an important researcharea to establish the authenticity of digital photographs by separating thetampered lots from the original ones. This paper gives a brief history of imagetampering and a state-of-the-art review of the tamper detection techniques.
arxiv-3300-75 | A Novel Active Contour Model for Texture Segmentation | http://arxiv.org/pdf/1306.6726v1.pdf | author:Aditya Tatu, Sumukh Bansal category:cs.CV published:2013-06-28 summary:Texture is intuitively defined as a repeated arrangement of a basic patternor object in an image. There is no mathematical definition of a texture though.The human visual system is able to identify and segment different textures in agiven image. Automating this task for a computer is far from trivial. There arethree major components of any texture segmentation algorithm: (a) The featuresused to represent a texture, (b) the metric induced on this representationspace and (c) the clustering algorithm that runs over these features in orderto segment a given image into different textures. In this paper, we propose anactive contour based novel unsupervised algorithm for texture segmentation. Weuse intensity covariance matrices of regions as the defining feature oftextures and find regions that have the most inter-region dissimilar covariancematrices using active contours. Since covariance matrices are symmetricpositive definite, we use geodesic distance defined on the manifold ofsymmetric positive definite matrices PD(n) as a measure of dissimlarity betweensuch matrices. We demonstrate performance of our algorithm on both artificialand real texture images.
arxiv-3300-76 | Optimal Feature Selection in High-Dimensional Discriminant Analysis | http://arxiv.org/pdf/1306.6557v1.pdf | author:Mladen Kolar, Han Liu category:stat.ML math.ST stat.TH published:2013-06-27 summary:We consider the high-dimensional discriminant analysis problem. For thisproblem, different methods have been proposed and justified by establishingexact convergence rates for the classification risk, as well as the l2convergence results to the discriminative rule. However, sharp theoreticalanalysis for the variable selection performance of these procedures have notbeen established, even though model interpretation is of fundamental importancein scientific data analysis. This paper bridges the gap by providing sharpsufficient conditions for consistent variable selection using the sparsediscriminant analysis (Mai et al., 2012). Through careful analysis, weestablish rates of convergence that are significantly faster than the bestknown results and admit an optimal scaling of the sample size n, dimensionalityp, and sparsity level s in the high-dimensional setting. Sufficient conditionsare complemented by the necessary information theoretic limits on the variableselection problem in the context of high-dimensional discriminant analysis.Exploiting a numerical equivalence result, our method also establish theoptimal results for the ROAD estimator (Fan et al., 2012) and the sparseoptimal scaling estimator (Clemmensen et al., 2011). Furthermore, we analyze anexhaustive search procedure, whose performance serves as a benchmark, and showthat it is variable selection consistent under weaker conditions. Extensivesimulations demonstrating the sharpness of the bounds are also provided.
arxiv-3300-77 | Solving Relational MDPs with Exogenous Events and Additive Rewards | http://arxiv.org/pdf/1306.6302v2.pdf | author:S. Joshi, R. Khardon, P. Tadepalli, A. Raghavan, A. Fern category:cs.AI cs.LG published:2013-06-26 summary:We formalize a simple but natural subclass of service domains for relationalplanning problems with object-centered, independent exogenous events andadditive rewards capturing, for example, problems in inventory control.Focusing on this subclass, we present a new symbolic planning algorithm whichis the first algorithm that has explicit performance guarantees for relationalMDPs with exogenous events. In particular, under some technical conditions, ourplanning algorithm provides a monotonic lower bound on the optimal valuefunction. To support this algorithm we present novel evaluation and reductiontechniques for generalized first order decision diagrams, a knowledgerepresentation for real-valued functions over relational world states. Ourplanning algorithm uses a set of focus states, which serves as a training set,to simplify and approximate the symbolic solution, and can thus be seen toperform learning for planning. A preliminary experimental evaluationdemonstrates the validity of our approach.
arxiv-3300-78 | Traffic data reconstruction based on Markov random field modeling | http://arxiv.org/pdf/1306.6482v1.pdf | author:Shun Kataoka, Muneki Yasuda, Cyril Furtlehner, Kazuyuki Tanaka category:stat.ML cs.LG published:2013-06-27 summary:We consider the traffic data reconstruction problem. Suppose we have thetraffic data of an entire city that are incomplete because some road data areunobserved. The problem is to reconstruct the unobserved parts of the data. Inthis paper, we propose a new method to reconstruct incomplete traffic datacollected from various traffic sensors. Our approach is based on Markov randomfield modeling of road traffic. The reconstruction is achieved by usingmean-field method and a machine learning method. We numerically verify theperformance of our method using realistic simulated traffic data for the realroad network of Sendai, Japan.
arxiv-3300-79 | Close the Gaps: A Learning-while-Doing Algorithm for a Class of Single-Product Revenue Management Problems | http://arxiv.org/pdf/1101.4681v6.pdf | author:Zizhuo Wang, Shiming Deng, Yinyu Ye category:cs.LG 93E35 published:2011-01-24 summary:We consider a retailer selling a single product with limited on-handinventory over a finite selling season. Customer demand arrives according to aPoisson process, the rate of which is influenced by a single action taken bythe retailer (such as price adjustment, sales commission, advertisementintensity, etc.). The relationship between the action and the demand rate isnot known in advance. However, the retailer is able to learn the optimal action"on the fly" as she maximizes her total expected revenue based on the observeddemand reactions. Using the pricing problem as an example, we propose a dynamic"learning-while-doing" algorithm that only involves function value estimationto achieve a near-optimal performance. Our algorithm employs a series ofshrinking price intervals and iteratively tests prices within that intervalusing a set of carefully chosen parameters. We prove that the convergence rateof our algorithm is among the fastest of all possible algorithms in terms ofasymptotic "regret" (the relative loss comparing to the full informationoptimal solution). Our result closes the performance gaps between parametricand non-parametric learning and between a post-price mechanism and acustomer-bidding mechanism. Important managerial insight from this research isthat the values of information on both the parametric form of the demandfunction as well as each customer's exact reservation price are less importantthan prior literature suggests. Our results also suggest that firms would bebetter off to perform dynamic learning and action concurrently rather thansequentially.
arxiv-3300-80 | A Data Mining Approach to Solve the Goal Scoring Problem | http://arxiv.org/pdf/1305.4955v2.pdf | author:Renato Oliveira, Paulo Adeodato, Arthur Carvalho, Icamaan Viegas, Christian Diego, Tsang Ing-Ren category:cs.AI cs.LG published:2013-05-21 summary:In soccer, scoring goals is a fundamental objective which depends on manyconditions and constraints. Considering the RoboCup soccer 2D-simulator, thispaper presents a data mining-based decision system to identify the best timeand direction to kick the ball towards the goal to maximize the overall chancesof scoring during a simulated soccer match. Following the CRISP-DM methodology,data for modeling were extracted from matches of major internationaltournaments (10691 kicks), knowledge about soccer was embedded viatransformation of variables and a Multilayer Perceptron was used to estimatethe scoring chance. Experimental performance assessment to compare thisapproach against previous LDA-based approach was conducted from 100 matches.Several statistical metrics were used to analyze the performance of the systemand the results showed an increase of 7.7% in the number of kicks, producing anoverall increase of 78% in the number of goals scored.
arxiv-3300-81 | Compressive Acquisition of Dynamic Scenes | http://arxiv.org/pdf/1201.4895v2.pdf | author:Aswin C Sankaranarayanan, Pavan K Turaga, Rama Chellappa, Richard G Baraniuk category:cs.CV published:2012-01-23 summary:Compressive sensing (CS) is a new approach for the acquisition and recoveryof sparse signals and images that enables sampling rates significantly belowthe classical Nyquist rate. Despite significant progress in the theory andmethods of CS, little headway has been made in compressive video acquisitionand recovery. Video CS is complicated by the ephemeral nature of dynamicevents, which makes direct extensions of standard CS imaging architectures andsignal models difficult. In this paper, we develop a new framework for video CSfor dynamic textured scenes that models the evolution of the scene as a lineardynamical system (LDS). This reduces the video recovery problem to firstestimating the model parameters of the LDS from compressive measurements, andthen reconstructing the image frames. We exploit the low-dimensional dynamicparameters (the state sequence) and high-dimensional static parameters (theobservation matrix) of the LDS to devise a novel compressive measurementstrategy that measures only the dynamic part of the scene at each instant andaccumulates measurements over time to estimate the static parameters. Thisenables us to lower the compressive measurement rate considerably. We validateour approach with a range of experiments involving both video recovery, sensinghyper-spectral data, and classification of dynamic scenes from compressivedata. Together, these applications demonstrate the effectiveness of theapproach.
arxiv-3300-82 | Compressive Coded Aperture Keyed Exposure Imaging with Optical Flow Reconstruction | http://arxiv.org/pdf/1306.6281v1.pdf | author:Zachary T. Harmany, Roummel F. Marcia, Rebecca M. Willett category:cs.IT cs.CV math.IT stat.AP published:2013-06-26 summary:This paper describes a coded aperture and keyed exposure approach tocompressive video measurement which admits a small physical platform, highphoton efficiency, high temporal resolution, and fast reconstructionalgorithms. The proposed projections satisfy the Restricted Isometry Property(RIP), and hence compressed sensing theory provides theoretical guarantees onthe video reconstruction quality. Moreover, the projections can be easilyimplemented using existing optical elements such as spatial light modulators(SLMs). We extend these coded mask designs to novel dual-scale masks (DSMs)which enable the recovery of a coarse-resolution estimate of the scene withnegligible computational cost. We develop fast numerical algorithms whichutilize both temporal correlations and optical flow in the video sequence aswell as the innovative structure of the projections. Our numerical experimentsdemonstrate the efficacy of the proposed approach on short-wave infrared data.
arxiv-3300-83 | How to find real-world applications for compressive sensing | http://arxiv.org/pdf/1305.1199v4.pdf | author:Leslie N. Smith category:cs.CV published:2013-05-06 summary:The potential of compressive sensing (CS) has spurred great interest in theresearch community and is a fast growing area of research. However, researchtranslating CS theory into practical hardware and demonstrating clear andsignificant benefits with this hardware over current, conventional imagingtechniques has been limited. This article helps researchers to find those nicheapplications where the CS approach provides substantial gain over conventionalapproaches by articulating lessons learned in finding one such application; seaskimming missile detection. As a proof of concept, it is demonstrated that asimplified CS missile detection architecture and algorithm provides comparableresults to the conventional imaging approach but using a smaller FPA. Theprimary message is that all of the excitement surrounding CS is necessary andappropriate for encouraging our creativity but we all must also take off our"rose colored glasses" and critically judge our ideas, methods and resultsrelative to conventional imaging approaches.
arxiv-3300-84 | A maximal-information color to gray conversion method for document images: Toward an optimal grayscale representation for document image binarization | http://arxiv.org/pdf/1306.6058v2.pdf | author:Reza Farrahi Moghaddam, Shaohua Chen, Rachid Hedjam, Mohamed Cheriet category:cs.CV published:2013-06-25 summary:A novel method to convert color/multi-spectral images to gray-level images isintroduced to increase the performance of document binarization methods. Themethod uses the distribution of the pixel data of the input document image in acolor space to find a transformation, called the dual transform, which balancesthe amount of information on all color channels. Furthermore, in order toreduce the intensity variations on the gray output, a color reductionpreprocessing step is applied. Then, a channel is selected as the gray valuerepresentation of the document image based on the homogeneity criterion on thetext regions. In this way, the proposed method can provide aluminance-independent contrast enhancement. The performance of the method isevaluated against various images from two databases, the ICDAR'03 RobustReading, the KAIST and the DIBCO'09 datasets, subjectively and objectively withpromising results. The ground truth images for the images from the ICDAR'03Robust Reading dataset have been created manually by the authors.
arxiv-3300-85 | Scaling Up Robust MDPs by Reinforcement Learning | http://arxiv.org/pdf/1306.6189v1.pdf | author:Aviv Tamar, Huan Xu, Shie Mannor category:cs.LG stat.ML published:2013-06-26 summary:We consider large-scale Markov decision processes (MDPs) with parameteruncertainty, under the robust MDP paradigm. Previous studies showed that robustMDPs, based on a minimax approach to handle uncertainty, can be solved usingdynamic programming for small to medium sized problems. However, due to the"curse of dimensionality", MDPs that model real-life problems are typicallyprohibitively large for such approaches. In this work we employ a reinforcementlearning approach to tackle this planning problem: we develop a robustapproximate dynamic programming method based on a projected fixed pointequation to approximately solve large scale robust MDPs. We show that theproposed method provably succeeds under certain technical conditions, anddemonstrate its effectiveness through simulation of an option pricing problem.To the best of our knowledge, this is the first attempt to scale up the robustMDPs paradigm.
arxiv-3300-86 | Competency Tracking for English as a Second or Foreign Language Learners | http://arxiv.org/pdf/1306.6130v1.pdf | author:Robert Bishop Jr category:cs.CL published:2013-06-26 summary:My system utilizes the outcomes feature found in Moodle and other learningcontent management systems (LCMSs) to keep track of where students are in termsof what language competencies they have mastered and the competencies they needto get where they want to go. These competencies are based on the CommonEuropean Framework for (English) Language Learning. This data can be availablefor everyone involved with a given student's progress (e.g. educators, parents,supervisors and the students themselves). A given student's record of pastaccomplishments can also be meshed with those of his classmates. Not only are astudent's competencies easily seen and tracked, educators can view competenciesof a group of students that were achieved prior to enrollment in the class.This should make curriculum decision making easier and more efficient foreducators.
arxiv-3300-87 | DISCOMAX: A Proximity-Preserving Distance Correlation Maximization Algorithm | http://arxiv.org/pdf/1306.2533v2.pdf | author:Praneeth Vepakomma, Ahmed Elgammal category:cs.LG stat.ML published:2013-06-11 summary:In a regression setting we propose algorithms that reduce the dimensionalityof the features while simultaneously maximizing a statistical measure ofdependence known as distance correlation between the low-dimensional featuresand a response variable. This helps in solving the prediction problem with alow-dimensional set of features. Our setting is different from subset-selectionalgorithms where the problem is to choose the best subset of features forregression. Instead, we attempt to generate a new set of low-dimensionalfeatures as in a feature-learning setting. We attempt to keep our proposedapproach as model-free and our algorithm does not assume the application of anyspecific regression model in conjunction with the low-dimensional features thatit learns. The algorithm is iterative and is fomulated as a combination of themajorization-minimization and concave-convex optimization procedures. We alsopresent spectral radius based convergence results for the proposed iterations.
arxiv-3300-88 | A Computational Approach to Politeness with Application to Social Factors | http://arxiv.org/pdf/1306.6078v1.pdf | author:Cristian Danescu-Niculescu-Mizil, Moritz Sudhof, Dan Jurafsky, Jure Leskovec, Christopher Potts category:cs.CL cs.SI physics.soc-ph I.2.7 published:2013-06-25 summary:We propose a computational framework for identifying linguistic aspects ofpoliteness. Our starting point is a new corpus of requests annotated forpoliteness, which we use to evaluate aspects of politeness theory and touncover new interactions between politeness markers and context. These findingsguide our construction of a classifier with domain-independent lexical andsyntactic features operationalizing key components of politeness theory, suchas indirection, deference, impersonalization and modality. Our classifierachieves close to human performance and is effective across domains. We use ourframework to study the relationship between politeness and social power,showing that polite Wikipedia editors are more likely to achieve high statusthrough elections, but, once elevated, they become less polite. We see asimilar negative correlation between politeness and power on Stack Exchange,where users at the top of the reputation scale are less polite than those atthe bottom. Finally, we apply our classifier to a preliminary analysis ofpoliteness variation by gender and community.
arxiv-3300-89 | Learning, Generalization, and Functional Entropy in Random Automata Networks | http://arxiv.org/pdf/1306.6041v1.pdf | author:Alireza Goudarzi, Christof Teuscher, Natali Gulbahce, Thimo Rohlf category:cs.NE nlin.AO nlin.CD physics.bio-ph published:2013-06-25 summary:It has been shown \citep{broeck90:physicalreview,patarnello87:europhys} thatfeedforward Boolean networks can learn to perform specific simple tasks andgeneralize well if only a subset of the learning examples is provided forlearning. Here, we extend this body of work and show experimentally that randomBoolean networks (RBNs), where both the interconnections and the Booleantransfer functions are chosen at random initially, can be evolved by using astate-topology evolution to solve simple tasks. We measure the learning andgeneralization performance, investigate the influence of the average nodeconnectivity $K$, the system size $N$, and introduce a new measure that allowsto better describe the network's learning and generalization behavior. We showthat the connectivity of the maximum entropy networks scales as a power-law ofthe system size $N$. Our results show that networks with higher averageconnectivity $K$ (supercritical) achieve higher memorization and partialgeneralization. However, near critical connectivity, the networks show a higherperfect generalization on the even-odd task.
arxiv-3300-90 | The Geometry of Generalized Binary Search | http://arxiv.org/pdf/0910.4397v5.pdf | author:Robert D. Nowak category:stat.ML cs.IT math.IT math.ST stat.TH published:2009-10-22 summary:This paper investigates the problem of determining a binary-valued functionthrough a sequence of strategically selected queries. The focus is an algorithmcalled Generalized Binary Search (GBS). GBS is a well-known greedy algorithmfor determining a binary-valued function through a sequence of strategicallyselected queries. At each step, a query is selected that most evenly splits thehypotheses under consideration into two disjoint subsets, a naturalgeneralization of the idea underlying classic binary search. This paperdevelops novel incoherence and geometric conditions under which GBS achievesthe information-theoretically optimal query complexity; i.e., given acollection of N hypotheses, GBS terminates with the correct function after nomore than a constant times log N queries. Furthermore, a noise-tolerant versionof GBS is developed that also achieves the optimal query complexity. Theseresults are applied to learning halfspaces, a problem arising routinely inimage processing and machine learning.
arxiv-3300-91 | DNA Reservoir Computing: A Novel Molecular Computing Approach | http://arxiv.org/pdf/1306.5998v1.pdf | author:Alireza Goudarzi, Matthew R. Lakin, Darko Stefanovic category:cs.NE cs.ET nlin.AO nlin.CD physics.bio-ph published:2013-06-25 summary:We propose a novel molecular computing approach based on reservoir computing.In reservoir computing, a dynamical core, called a reservoir, is perturbed withan external input signal while a readout layer maps the reservoir dynamics to atarget output. Computation takes place as a transformation from the input spaceto a high-dimensional spatiotemporal feature space created by the transientdynamics of the reservoir. The readout layer then combines these features toproduce the target output. We show that coupled deoxyribozyme oscillators canact as the reservoir. We show that despite using only three coupledoscillators, a molecular reservoir computer could achieve 90% accuracy on abenchmark temporal problem.
arxiv-3300-92 | Supersparse Linear Integer Models for Predictive Scoring Systems | http://arxiv.org/pdf/1306.5860v1.pdf | author:Berk Ustun, Stefano Traca, Cynthia Rudin category:stat.ML published:2013-06-25 summary:We introduce Supersparse Linear Integer Models (SLIM) as a tool to createscoring systems for binary classification. We derive theoretical bounds on thetrue risk of SLIM scoring systems, and present experimental results to showthat SLIM scoring systems are accurate, sparse, and interpretableclassification models.
arxiv-3300-93 | Cellular Tree Classifiers | http://arxiv.org/pdf/1301.4679v2.pdf | author:Gérard Biau, Luc Devroye category:stat.ML cs.LG math.ST stat.TH published:2013-01-20 summary:The cellular tree classifier model addresses a fundamental problem in thedesign of classifiers for a parallel or distributed computing world: Given adata set, is it sufficient to apply a majority rule for classification, orshall one split the data into two or more parts and send each part to apotentially different computer (or cell) for further processing? At firstsight, it seems impossible to define with this paradigm a consistent classifieras no cell knows the "original data size", $n$. However, we show that this isnot so by exhibiting two different consistent classifiers. The consistency isuniversal but is only shown for distributions with nonatomic marginals.
arxiv-3300-94 | Accelerated Canonical Polyadic Decomposition by Using Mode Reduction | http://arxiv.org/pdf/1211.3500v2.pdf | author:Guoxu Zhou, Andrzej Cichocki, Shengli Xie category:cs.NA cs.LG math.NA published:2012-11-15 summary:Canonical Polyadic (or CANDECOMP/PARAFAC, CP) decompositions (CPD) are widelyapplied to analyze high order tensors. Existing CPD methods use alternatingleast square (ALS) iterations and hence need to unfold tensors to each of the$N$ modes frequently, which is one major bottleneck of efficiency forlarge-scale data and especially when $N$ is large. To overcome this problem, inthis paper we proposed a new CPD method which converts the original $N$th($N>3$) order tensor to a 3rd-order tensor first. Then the full CPD is realizedby decomposing this mode reduced tensor followed by a Khatri-Rao productprojection procedure. This way is quite efficient as unfolding to each of the$N$ modes are avoided, and dimensionality reduction can also be easilyincorporated to further improve the efficiency. We show that, under mildconditions, any $N$th-order CPD can be converted into a 3rd-order case butwithout destroying the essential uniqueness, and theoretically gives the sameresults as direct $N$-way CPD methods. Simulations show that, compared withstate-of-the-art CPD methods, the proposed method is more efficient and escapefrom local solutions more easily.
arxiv-3300-95 | Constrained Optimization for a Subset of the Gaussian Parsimonious Clustering Models | http://arxiv.org/pdf/1306.5824v1.pdf | author:Ryan P. Browne, Sanjeena Subedi, Paul McNicholas category:stat.CO math.ST stat.ML stat.TH published:2013-06-25 summary:The expectation-maximization (EM) algorithm is an iterative method forfinding maximum likelihood estimates when data are incomplete or are treated asbeing incomplete. The EM algorithm and its variants are commonly used forparameter estimation in applications of mixture models for clustering andclassification. This despite the fact that even the Gaussian mixture modellikelihood surface contains many local maxima and is singularity riddled.Previous work has focused on circumventing this problem by constraining thesmallest eigenvalue of the component covariance matrices. In this paper, weconsider constraining the smallest eigenvalue, the largest eigenvalue, and boththe smallest and largest within the family setting. Specifically, a subset ofthe GPCM family is considered for model-based clustering, where we use are-parameterized version of the famous eigenvalue decomposition of thecomponent covariance matrices. Our approach is illustrated using variousexperiments with simulated and real data.
arxiv-3300-96 | The AdaBoost Flow | http://arxiv.org/pdf/1110.6228v4.pdf | author:A. Lykov, S. Muzychka, K. Vaninsky category:stat.ML math-ph math.MP 62-07 published:2011-10-28 summary:We introduce a dynamical system which we call the AdaBoost flow. The flow isdefined by a system of ODEs with control. We show that three algorithms of theAdaBoost family (i) the AdaBoost algorithm of Schapire and Freund (ii) thearc-gv algorithm of Breiman (iii) the confidence rated prediction of Schapireand Singer can be can be embedded in the AdaBoost flow. The nontrivial part of the AdaBoost flow equations coincides with theequations of dynamics of nonperiodic Toda system written in terms of spectralvariables. We provide a novel invariant geometrical description of the AdaBoostalgorithm as a gradient flow on a foliation defined by level sets of thepotential function. We propose a new approach for constructing boosting algorithms as acontinuous time gradient flow on measures defined by various metrics andpotential functions. Finally we explain similarity of the AdaBoost algorithmwith the Perelman's construction for the Ricci flow.
arxiv-3300-97 | Recovering Block-structured Activations Using Compressive Measurements | http://arxiv.org/pdf/1209.3431v2.pdf | author:Sivaraman Balakrishnan, Mladen Kolar, Alessandro Rinaldo, Aarti Singh category:stat.ML published:2012-09-15 summary:We consider the problems of detection and localization of a contiguous blockof weak activation in a large matrix, from a small number of noisy, possiblyadaptive, compressive (linear) measurements. This is closely related to theproblem of compressed sensing, where the task is to estimate a sparse vectorusing a small number of linear measurements. Contrary to results in compressedsensing, where it has been shown that neither adaptivity nor contiguousstructure help much, we show that for reliable localization the magnitude ofthe weakest signals is strongly influenced by both structure and the ability tochoose measurements adaptively while for detection neither adaptivity norstructure reduce the requirement on the magnitude of the signal. Wecharacterize the precise tradeoffs between the various problem parameters, thesignal strength and the number of measurements required to reliably detect andlocalize the block of activation. The sufficient conditions are complementedwith information theoretic lower bounds.
arxiv-3300-98 | A State-Space Approach for Optimal Traffic Monitoring via Network Flow Sampling | http://arxiv.org/pdf/1306.5793v1.pdf | author:Michael Kallitsis, Stilian Stoev, George Michailidis category:cs.SY cs.NI stat.AP stat.ML published:2013-06-24 summary:The robustness and integrity of IP networks require efficient tools fortraffic monitoring and analysis, which scale well with traffic volume andnetwork size. We address the problem of optimal large-scale flow monitoring ofcomputer networks under resource constraints. We propose a stochasticoptimization framework where traffic measurements are done by exploiting thespatial (across network links) and temporal relationship of traffic flows.Specifically, given the network topology, the state-space characterization ofnetwork flows and sampling constraints at each monitoring station, we seek anoptimal packet sampling strategy that yields the best traffic volume estimationfor all flows of the network. The optimal sampling design is the result of aconcave minimization problem; then, Kalman filtering is employed to yield asequence of traffic estimates for each network flow. We evaluate our algorithmusing real-world Internet2 data.
arxiv-3300-99 | Modeling The Stable Operating Envelope For Partially Stable Combustion Engines Using Class Imbalance Learning | http://arxiv.org/pdf/1306.5702v1.pdf | author:Vijay Manikandan Janakiraman, XuanLong Nguyen, Jeff Sterniak, Dennis Assanis category:cs.NE published:2013-06-24 summary:Advanced combustion technologies such as homogeneous charge compressionignition (HCCI) engines have a narrow stable operating region defined bycomplex control strategies such as exhaust gas recirculation (EGR) and variablevalve timing among others. For such systems, it is important to identify theoperating envelope or the boundary of stable operation for diagnostics andcontrol purposes. Obtaining a good model of the operating envelope usingphysics becomes intractable owing to engine transient effects. In this paper, amachine learning based approach is employed to identify the stable operatingboundary of HCCI combustion directly from experimental data. Owing to imbalancein class proportions in the data, two approaches are considered. A re-sampling(under-sampling, over-sampling) based approach is used to develop models usingexisting algorithms while a cost-sensitive approach is used to modify thelearning algorithm without modifying the data set. Support vector machines andrecently developed extreme learning machines are used for model development andresults compared against linear classification methods show that cost-sensitiveversions of ELM and SVM algorithms are well suited to model the HCCI operatingenvelope. The prediction results indicate that the models have the potential tobe used for predicting HCCI instability based on sensor measurement history.
arxiv-3300-100 | Using Genetic Programming to Model Software | http://arxiv.org/pdf/1306.5667v1.pdf | author:W. B. Langdon, M. Harman category:cs.NE cs.AI published:2013-06-24 summary:We study a generic program to investigate the scope for automaticallycustomising it for a vital current task, which was not considered when it wasfirst written. In detail, we show genetic programming (GP) can evolve models ofaspects of BLAST's output when it is used to map Solexa Next-Gen DNA sequencesto the human genome.
arxiv-3300-101 | Jitter-Adaptive Dictionary Learning - Application to Multi-Trial Neuroelectric Signals | http://arxiv.org/pdf/1301.3611v4.pdf | author:Sebastian Hitziger, Maureen Clerc, Alexandre Gramfort, Sandrine Saillet, Christian Bénar, Théodore Papadopoulo category:stat.ML 62-07 published:2013-01-16 summary:Dictionary Learning has proven to be a powerful tool for many imageprocessing tasks, where atoms are typically defined on small image patches. Asa drawback, the dictionary only encodes basic structures. In addition, thisapproach treats patches of different locations in one single set, which means aloss of information when features are well-aligned across signals. This is thecase, for instance, in multi-trial magneto- or electroencephalography (M/EEG).Learning the dictionary on the entire signals could make use of the alignementand reveal higher-level features. In this case, however, small missalignementsor phase variations of features would not be compensated for. In this paper, wepropose an extension to the common dictionary learning framework to overcomethese limitations by allowing atoms to adapt their position across signals. Themethod is validated on simulated and real neuroelectric data.
arxiv-3300-102 | Model Reframing by Feature Context Change | http://arxiv.org/pdf/1306.5487v1.pdf | author:Celestine-Periale Ma category:cs.LG published:2013-06-23 summary:The feature space (including both input and output variables) characterises adata mining problem. In predictive (supervised) problems, the quality andavailability of features determines the predictability of the dependentvariable, and the performance of data mining models in terms ofmisclassification or regression error. Good features, however, are usuallydifficult to obtain. It is usual that many instances come with missing values,either because the actual value for a given attribute was not available orbecause it was too expensive. This is usually interpreted as a utility orcost-sensitive learning dilemma, in this case between misclassification (orregression error) costs and attribute tests costs. Both misclassification cost(MC) and test cost (TC) can be integrated into a single measure, known as jointcost (JC). We introduce methods and plots (such as the so-called JROC plots)that can work with any of-the-shelf predictive technique, including ensembles,such that we re-frame the model to use the appropriate subset of attributes(the feature configuration) during deployment time. In other words, models aretrained with the available attributes (once and for all) and then deployed bysetting missing values on the attributes that are deemed ineffective forreducing the joint cost. As the number of feature configuration combinationsgrows exponentially with the number of features we introduce quadratic methodsthat are able to approximate the optimal configuration and model choices, asshown by the experimental results.
arxiv-3300-103 | Exploiting Data Parallelism in the yConvex Hypergraph Algorithm for Image Representation using GPGPUs | http://arxiv.org/pdf/1307.2560v1.pdf | author:Saurabh Jha, Tejaswi Agarwal, B. Rajesh Kanna category:cs.DC cs.CV I.3 published:2013-06-23 summary:To define and identify a region-of-interest (ROI) in a digital image, theshape descriptor of the ROI has to be described in terms of its boundarycharacteristics. To address the generic issues of contour tracking, the yConvexHypergraph (yCHG) model was proposed by Kanna et al [1]. In this work, wepropose a parallel approach to implement the yCHG model by exploiting massivelyparallel cores of NVIDIA's Compute Unified Device Architecture (CUDA). Weperform our experiments on the MODIS satellite image database by NASA, andbased on our analysis we observe that the performance of the serialimplementation is better on smaller images, but once the threshold is achievedin terms of image resolution, the parallel implementation outperforms itssequential counterpart by 2 to 10 times (2x-10x). We also conclude that anincrease in the number of hyperedges in the ROI of a given size does not impactthe performance of the overall algorithm.
arxiv-3300-104 | Characterizing Ambiguity in Light Source Invariant Shape from Shading | http://arxiv.org/pdf/1306.5480v1.pdf | author:Benjamin Kunsberg, Steven W. Zucker category:cs.CV q-bio.NC published:2013-06-23 summary:Shape from shading is a classical inverse problem in computer vision. Thisshape reconstruction problem is inherently ill-defined; it depends on theassumed light source direction. We introduce a novel mathematical formulationfor calculating local surface shape based on covariant derivatives of theshading flow field, rather than the customary integral minimization or P.D.Eapproaches. On smooth surfaces, we show second derivatives of brightness areindependent of the light sources and can be directly related to surfaceproperties. We use these measurements to define the matching local family ofsurfaces that can result from any given shading patch, changing the emphasis tocharacterizing ambiguity in the problem. We give an example of how these localsurface ambiguities collapse along certain image contours and how this can beused for the reconstruction problem.
arxiv-3300-105 | Loss-Proportional Subsampling for Subsequent ERM | http://arxiv.org/pdf/1306.1840v2.pdf | author:Paul Mineiro, Nikos Karampatziakis category:cs.LG stat.ML published:2013-06-07 summary:We propose a sampling scheme suitable for reducing a data set prior toselecting a hypothesis with minimum empirical risk. The sampling only considersa subset of the ultimate (unknown) hypothesis set, but can nonethelessguarantee that the final excess risk will compare favorably with utilizing theentire original data set. We demonstrate the practical benefits of our approachon a large dataset which we subsample and subsequently fit with boosted trees.
arxiv-3300-106 | A Variational Approximations-DIC Rubric for Parameter Estimation and Mixture Model Selection Within a Family Setting | http://arxiv.org/pdf/1306.5368v1.pdf | author:Sanjeena Subedi, Paul McNicholas category:stat.ME stat.CO stat.ML published:2013-06-23 summary:Mixture model-based clustering has become an increasingly popular dataanalysis technique since its introduction fifty years ago, and is now commonlyutilized within the family setting. Families of mixture models arise when thecomponent parameters, usually the component covariance matrices, are decomposedand a number of constraints are imposed. Within the family setting, we need tochoose the member of the family, i.e., the appropriate covariance structure, inaddition to the number of mixture components. To date, the Bayesian informationcriterion (BIC) has proven most effective for model selection, and theexpectation-maximization (EM) algorithm is usually used for parameterestimation. To date, this EM-BIC rubric has monopolized the literature onfamilies of mixture models. We deviate from this rubric, using variationalBayes approximations for parameter estimation and the deviance informationcriterion for model selection. The variational Bayes approach alleviates someof the computational complexities associated with the EM algorithm byconstructing a tight lower bound on the complex marginal likelihood andmaximizing this lower bound by minimizing the associated Kullback-Leiblerdivergence. We use this approach on the most famous family of Gaussian mixturemodels within the literature, and real and simulated data are used to compareour approach to the EM-BIC rubric.
arxiv-3300-107 | A Statistical Perspective on Algorithmic Leveraging | http://arxiv.org/pdf/1306.5362v1.pdf | author:Ping Ma, Michael W. Mahoney, Bin Yu category:stat.ME cs.LG stat.ML published:2013-06-23 summary:One popular method for dealing with large-scale data sets is sampling. Forexample, by using the empirical statistical leverage scores as an importancesampling distribution, the method of algorithmic leveraging samples andrescales rows/columns of data matrices to reduce the data size beforeperforming computations on the subproblem. This method has been successful inimproving computational efficiency of algorithms for matrix problems such asleast-squares approximation, least absolute deviations approximation, andlow-rank matrix approximation. Existing work has focused on algorithmic issuessuch as worst-case running times and numerical issues associated with providinghigh-quality implementations, but none of it addresses statistical aspects ofthis method. In this paper, we provide a simple yet effective framework to evaluate thestatistical properties of algorithmic leveraging in the context of estimatingparameters in a linear regression model with a fixed number of predictors. Weshow that from the statistical perspective of bias and variance, neitherleverage-based sampling nor uniform sampling dominates the other. This resultis particularly striking, given the well-known result that, from thealgorithmic perspective of worst-case analysis, leverage-based samplingprovides uniformly superior worst-case algorithmic results, when compared withuniform sampling. Based on these theoretical results, we propose and analyzetwo new leveraging algorithms. A detailed empirical evaluation of existingleverage-based methods as well as these two new methods is carried out on bothsynthetic and real data sets. The empirical results indicate that our theory isa good predictor of practical performance of existing and new leverage-basedalgorithms and that the new algorithms achieve improved performance.
arxiv-3300-108 | Song-based Classification techniques for Endangered Bird Conservation | http://arxiv.org/pdf/1306.5349v1.pdf | author:Erick Stattner, Wilfried Segretier, Martine Collard, Philippe Hunel, Nicolas Vidot category:cs.LG published:2013-06-22 summary:The work presented in this paper is part of a global framework which longterm goal is to design a wireless sensor network able to support theobservation of a population of endangered birds. We present the first stage forwhich we have conducted a knowledge discovery approach on a sample ofacoustical data. We use MFCC features extracted from bird songs and we exploittwo knowledge discovery techniques. One that relies on clustering-basedapproaches, that highlights the homogeneity in the songs of the species. Theother, based on predictive modeling, that demonstrates the good performances ofvarious machine learning techniques for the identification process. Theknowledge elicited provides promising results to consider a widespread studyand to elicit guidelines for designing a first version of the automaticapproach for data collection based on acoustic sensors.
arxiv-3300-109 | Online dictionary learning for kernel LMS. Analysis and forward-backward splitting algorithm | http://arxiv.org/pdf/1306.5310v1.pdf | author:Wei Gao, Jie Chen, Cédric Richard, Jianguo Huang category:stat.ML published:2013-06-22 summary:Adaptive filtering algorithms operating in reproducing kernel Hilbert spaceshave demonstrated superiority over their linear counterpart for nonlinearsystem identification. Unfortunately, an undesirable characteristic of thesemethods is that the order of the filters grows linearly with the number ofinput data. This dramatically increases the computational burden and memoryrequirement. A variety of strategies based on dictionary learning have beenproposed to overcome this severe drawback. Few, if any, of these works analyzethe problem of updating the dictionary in a time-varying environment. In thispaper, we present an analytical study of the convergence behavior of theGaussian least-mean-square algorithm in the case where the statistics of thedictionary elements only partially match the statistics of the input data. Thisallows us to emphasize the need for updating the dictionary in an online way,by discarding the obsolete elements and adding appropriate ones. We introduce akernel least-mean-square algorithm with L1-norm regularization to automaticallyperform this task. The stability in the mean of this method is analyzed, andits performance is tested with experiments.
arxiv-3300-110 | Cognitive Interpretation of Everyday Activities: Toward Perceptual Narrative Based Visuo-Spatial Scene Interpretation | http://arxiv.org/pdf/1306.5308v1.pdf | author:Mehul Bhatt, Jakob Suchan, Carl Schultz category:cs.AI cs.CV cs.HC cs.RO published:2013-06-22 summary:We position a narrative-centred computational model for high-level knowledgerepresentation and reasoning in the context of a range of assistivetechnologies concerned with "visuo-spatial perception and cognition" tasks. Ourproposed narrative model encompasses aspects such as \emph{space, events,actions, change, and interaction} from the viewpoint of commonsense reasoningand learning in large-scale cognitive systems. The broad focus of this paper ison the domain of "human-activity interpretation" in smart environments, ambientintelligence etc. In the backdrop of a "smart meeting cinematography" domain,we position the proposed narrative model, preliminary work on perceptualnarrativisation, and the immediate outlook on constructing general-purposeopen-source tools for perceptual narrativisation. ACM Classification: I.2 Artificial Intelligence: I.2.0 General -- CognitiveSimulation, I.2.4 Knowledge Representation Formalisms and Methods, I.2.10Vision and Scene Understanding: Architecture and control structures, Motion,Perceptual reasoning, Shape, Video analysis General keywords: cognitive systems; human-computer interaction; spatialcognition and computation; commonsense reasoning; spatial and temporalreasoning; assistive technologies
arxiv-3300-111 | Segmenting DNA sequence into `words' | http://arxiv.org/pdf/1202.2518v4.pdf | author:Wang Liang category:q-bio.GN cs.CL published:2012-02-12 summary:This paper presents a novel method to segment/decode DNA sequences based onn-grams statistical language model. Firstly, we find the length of most DNA'words' is 12 to 15 bps by analyzing the genomes of 12 model species. Then wedesign an unsupervised probability based approach to segment the DNA sequences.The benchmark of segmenting method is also proposed.
arxiv-3300-112 | New Approach of Estimating PSNR-B For De-blocked Images | http://arxiv.org/pdf/1306.5293v1.pdf | author:S. Aruna Mastani, K. Shilpa category:cs.CV published:2013-06-22 summary:Measurement of image quality is very crucial to many image processingapplications. Quality metrics are used to measure the quality of improvement inthe images after they are processed and compared with the original images.Compression is one of the applications where it is required to monitor thequality of decompressed or decoded image. JPEG compression is the lossycompression which is most prevalent technique for image codecs. But it suffersfrom blocking artifacts. Various deblocking filters are used to reduce blockingartifacts. The efficiency of deblocking filters which improves visual signalsdegraded by blocking artifacts from compression will also be studied. Objectivequality metrics like PSNR, SSIM, and PSNRB for analyzing the quality ofdeblocked images will be studied. We introduce a new approach of PSNR-B foranalyzing quality of deblocked images. Simulation results show that newapproach of PSNR-B called modified PSNR-B. it gives even better resultscompared to existing well known blockiness specific indices
arxiv-3300-113 | Discriminative Training: Learning to Describe Video with Sentences, from Video Described with Sentences | http://arxiv.org/pdf/1306.5263v1.pdf | author:Haonan Yu, Jeffrey Mark Siskind category:cs.CV cs.CL published:2013-06-21 summary:We present a method for learning word meanings from complex and realisticvideo clips by discriminatively training (DT) positive sentential labelsagainst negative ones, and then use the trained word models to generatesentential descriptions for new video. This new work is inspired by recent workwhich adopts a maximum likelihood (ML) framework to address the same problemusing only positive sentential labels. The new method, like the ML-based one,is able to automatically determine which words in the sentence correspond towhich concepts in the video (i.e., ground words to meanings) in a weaklysupervised fashion. While both DT and ML yield comparable results withsufficient training data, DT outperforms ML significantly with smaller trainingsets because it can exploit negative training labels to better constrain thelearning problem.
arxiv-3300-114 | Near-optimal Coresets For Least-Squares Regression | http://arxiv.org/pdf/1202.3505v2.pdf | author:Christos Boutsidis, Petros Drineas, Malik Magdon-Ismail category:cs.DS cs.LG published:2012-02-16 summary:We study (constrained) least-squares regression as well as multiple responseleast-squares regression and ask the question of whether a subset of the data,a coreset, suffices to compute a good approximate solution to the regression.We give deterministic, low order polynomial-time algorithms to construct suchcoresets with approximation guarantees, together with lower bounds indicatingthat there is not much room for improvement upon our results.
arxiv-3300-115 | Deterministic Feature Selection for $k$-means Clustering | http://arxiv.org/pdf/1109.5664v4.pdf | author:Christos Boutsidis, Malik Magdon-Ismail category:cs.LG cs.DS published:2011-09-26 summary:We study feature selection for $k$-means clustering. Although the literaturecontains many methods with good empirical performance, algorithms with provabletheoretical behavior have only recently been developed. Unfortunately, thesealgorithms are randomized and fail with, say, a constant probability. Weaddress this issue by presenting a deterministic feature selection algorithmfor k-means with theoretical guarantees. At the heart of our algorithm lies adeterministic method for decompositions of the identity.
arxiv-3300-116 | Clinical Relationships Extraction Techniques from Patient Narratives | http://arxiv.org/pdf/1306.5170v1.pdf | author:Wafaa Tawfik Abdel-moneim, Mohamed Hashem Abdel-Aziz, Mohamed Monier Hassan category:cs.IR cs.CL published:2013-06-21 summary:The Clinical E-Science Framework (CLEF) project was used to extract importantinformation from medical texts by building a system for the purpose of clinicalresearch, evidence-based healthcare and genotype-meets-phenotype informatics.The system is divided into two parts, one part concerns with the identificationof relationships between clinically important entities in the text. The fullparses and domain-specific grammars had been used to apply many approaches toextract the relationship. In the second part of the system, statistical machinelearning (ML) approaches are applied to extract relationship. A corpus ofoncology narratives that hand annotated with clinical relationships can be usedto train and test a system that has been designed and implemented by supervisedmachine learning (ML) approaches. Many features can be extracted from thesetexts that are used to build a model by the classifier. Multiple supervisedmachine learning algorithms can be applied for relationship extraction. Effectsof adding the features, changing the size of the corpus, and changing the typeof the algorithm on relationship extraction are examined. Keywords: Textmining; information extraction; NLP; entities; and relations.
arxiv-3300-117 | Fine-Grained Visual Classification of Aircraft | http://arxiv.org/pdf/1306.5151v1.pdf | author:Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, Andrea Vedaldi category:cs.CV published:2013-06-21 summary:This paper introduces FGVC-Aircraft, a new dataset containing 10,000 imagesof aircraft spanning 100 aircraft models, organised in a three-level hierarchy.At the finer level, differences between models are often subtle but alwaysvisually measurable, making visual recognition challenging but possible. Abenchmark is obtained by defining corresponding classification tasks andevaluation protocols, and baseline results are presented. The construction ofthis dataset was made possible by the work of aircraft enthusiasts, a strategythat can extend to the study of number of other object classes. Compared to thedomains usually considered in fine-grained visual classification (FGVC), forexample animals, aircraft are rigid and hence less deformable. They, however,present other interesting modes of variation, including purpose, size,designation, structure, historical style, and branding.
arxiv-3300-118 | Computer Aided ECG Analysis - State of the Art and Upcoming Challenges | http://arxiv.org/pdf/1306.5096v1.pdf | author:Marko Velic, Ivan Padavic, Sinisa Car category:cs.CV published:2013-06-21 summary:In this paper we present current achievements in computer aided ECG analysisand their applicability in real world medical diagnosis process. Most of thecurrent work is covering problems of removing noise, detecting heartbeats andrhythm-based analysis. There are some advancements in particular ECG segmentsdetection and beat classifications but with limited evaluations and withoutclinical approvals. This paper presents state of the art advancements in thoseareas till present day. Besides this short computer science and signalprocessing literature review, paper covers future challenges regarding the ECGsignal morphology analysis deriving from the medical literature review. Paperis concluded with identified gaps in current advancements and testing, upcomingchallenges for future research and a bullseye test is suggested for morphologyanalysis evaluation.
arxiv-3300-119 | 3-SAT Problem A New Memetic-PSO Algorithm | http://arxiv.org/pdf/1306.5070v1.pdf | author:Nasser Lotfi, Jamshid Tamouk, Mina Farmanbar category:cs.AI cs.NE published:2013-06-21 summary:3-SAT problem is of great importance to many technical and scientificapplications. This paper presents a new hybrid evolutionary algorithm forsolving this satisfiability problem. 3-SAT problem has the huge search spaceand hence it is known as a NP-hard problem. So, deterministic approaches arenot applicable in this context. Thereof, application of evolutionary processingapproaches and especially PSO will be very effective for solving these kinds ofproblems. In this paper, we introduce a new evolutionary optimization techniquebased on PSO, Memetic algorithm and local search approaches. When someheuristics are mixed, their advantages are collected as well and we can reachto the better outcomes. Finally, we test our proposed algorithm over somebenchmarks used by some another available algorithms. Obtained results showthat our new method leads to the suitable results by the appropriate time.Thereby, it achieves a better result in compared with the existent approachessuch as pure genetic algorithm and some verified types
arxiv-3300-120 | Locally adaptive factor processes for multivariate time series | http://arxiv.org/pdf/1210.2022v2.pdf | author:Daniele Durante, Bruno Scarpa, David B. Dunson category:stat.AP stat.ML published:2012-10-07 summary:In modeling multivariate time series, it is important to allow time-varyingsmoothness in the mean and covariance process. In particular, there may becertain time intervals exhibiting rapid changes and others in which changes areslow. If such time-varying smoothness is not accounted for, one can obtainmisleading inferences and predictions, with over-smoothing across erratic timeintervals and under-smoothing across times exhibiting slow variation. This canlead to mis-calibration of predictive intervals, which can be substantially toonarrow or wide depending on the time. We propose a locally adaptive factorprocess for characterizing multivariate mean-covariance changes in continuoustime, allowing locally varying smoothness in both the mean and covariancematrix. This process is constructed utilizing latent dictionary functionsevolving in time through nested Gaussian processes and linearly related to theobserved data with a sparse mapping. Using a differential equationrepresentation, we bypass usual computational bottlenecks in obtaining MCMC andonline algorithms for approximate Bayesian inference. The performance isassessed in simulations and illustrated in a financial application.
arxiv-3300-121 | Determining Points on Handwritten Mathematical Symbols | http://arxiv.org/pdf/1306.4966v1.pdf | author:Rui Hu, Stephen M. Watt category:cs.CV cs.CY published:2013-06-20 summary:In a variety of applications, such as handwritten mathematics and diagramlabelling, it is common to have symbols of many different sizes in use and forthe writing not to follow simple baselines. In order to understand the scaleand relative positioning of individual characters, it is necessary to identifythe location of certain expected features. These are typically identified byparticular points in the symbols, for example, the baseline of a lower case "p"would be identified by the lowest part of the bowl, ignoring the descender. Weinvestigate how to find these special points automatically so they may be usedin a number of problems, such as improving two-dimensional mathematicalrecognition and in handwriting neatening, while preserving the original style.
arxiv-3300-122 | Failure of Calibration is Typical | http://arxiv.org/pdf/1306.4943v1.pdf | author:Gordon Belot category:math.ST stat.ML stat.TH published:2013-06-20 summary:Schervish (1985b) showed that every forecasting system is noncalibrated foruncountably many data sequences that it might see. This result is strengthenedhere: from a topological point of view, failure of calibration is typical andcalibration rare. Meanwhile, Bayesian forecasters are certain that they arecalibrated---this invites worries about the connection between Bayesianism andrationality.
arxiv-3300-123 | Recognition of Named-Event Passages in News Articles | http://arxiv.org/pdf/1306.4908v1.pdf | author:Luis Marujo, Wang Ling, Anatole Gershman, Jaime Carbonell, João P. Neto, David Matos category:cs.CL cs.IR published:2013-06-20 summary:We extend the concept of Named Entities to Named Events - commonly occurringevents such as battles and earthquakes. We propose a method for findingspecific passages in news articles that contain information about such eventsand report our preliminary evaluation results. Collecting "Gold Standard" datapresents many problems, both practical and conceptual. We present a method forobtaining such data using the Amazon Mechanical Turk service.
arxiv-3300-124 | From-Below Approximations in Boolean Matrix Factorization: Geometry and New Algorithm | http://arxiv.org/pdf/1306.4905v1.pdf | author:Radim Belohlavek, Martin Trnecka category:cs.NA cs.LG published:2013-06-20 summary:We present new results on Boolean matrix factorization and a new algorithmbased on these results. The results emphasize the significance offactorizations that provide from-below approximations of the input matrix.While the previously proposed algorithms do not consider the possibly differentsignificance of different matrix entries, our results help measure suchsignificance and suggest where to focus when computing factors. An experimentalevaluation of the new algorithm on both synthetic and real data demonstratesits good performance in terms of good coverage by the first k factors as wellas a small number of factors needed for exact decomposition and indicates thatthe algorithm outperforms the available ones in these terms. We also proposefuture research topics.
arxiv-3300-125 | Key Phrase Extraction of Lightly Filtered Broadcast News | http://arxiv.org/pdf/1306.4890v1.pdf | author:Luis Marujo, Ricardo Ribeiro, David Martins de Matos, João P. Neto, Anatole Gershman, Jaime Carbonell category:cs.CL cs.IR published:2013-06-20 summary:This paper explores the impact of light filtering on automatic key phraseextraction (AKE) applied to Broadcast News (BN). Key phrases are words andexpressions that best characterize the content of a document. Key phrases areoften used to index the document or as features in further processing. Thismakes improvements in AKE accuracy particularly important. We hypothesized thatfiltering out marginally relevant sentences from a document would improve AKEaccuracy. Our experiments confirmed this hypothesis. Elimination of as littleas 10% of the document sentences lead to a 2% improvement in AKE precision andrecall. AKE is built over MAUI toolkit that follows a supervised learningapproach. We trained and tested our AKE method on a gold standard made of 8 BNprograms containing 110 manually annotated news stories. The experiments wereconducted within a Multimedia Monitoring Solution (MMS) system for TV and radionews/programs, running daily, and monitoring 12 TV and 4 radio channels.
arxiv-3300-126 | Regularization and nonlinearities for neural language models: when are they needed? | http://arxiv.org/pdf/1301.5650v2.pdf | author:Marius Pachitariu, Maneesh Sahani category:stat.ML cs.LG published:2013-01-23 summary:Neural language models (LMs) based on recurrent neural networks (RNN) aresome of the most successful word and character-level LMs. Why do they work sowell, in particular better than linear neural LMs? Possible explanations arethat RNNs have an implicitly better regularization or that RNNs have a highercapacity for storing patterns due to their nonlinearities or both. Here weargue for the first explanation in the limit of little training data and thesecond explanation for large amounts of text data. We show state-of-the-artperformance on the popular and small Penn dataset when RNN LMs are regularizedwith random dropout. Nonetheless, we show even better performance from asimplified, much less expressive linear RNN model without off-diagonal entriesin the recurrent matrix. We call this model an impulse-response LM (IRLM).Using random dropout, column normalization and annealed learning rates, IRLMsdevelop neurons that keep a memory of up to 50 words in the past and achieve aperplexity of 102.5 on the Penn dataset. On two large datasets however, thesame regularization methods are unsuccessful for both models and the RNN'sexpressivity allows it to overtake the IRLM by 10 and 20 percent perplexity,respectively. Despite the perplexity gap, IRLMs still outperform RNNs on theMicrosoft Research Sentence Completion (MRSC) task. We develop a slightlymodified IRLM that separates long-context units (LCUs) from short-context unitsand show that the LCUs alone achieve a state-of-the-art performance on the MRSCtask of 60.8%. Our analysis indicates that a fruitful direction of research forneural LMs lies in developing more accessible internal representations, andsuggests an optimization regime of very high momentum terms for effectivelytraining such models.
arxiv-3300-127 | Supervised Topical Key Phrase Extraction of News Stories using Crowdsourcing, Light Filtering and Co-reference Normalization | http://arxiv.org/pdf/1306.4886v1.pdf | author:Luis Marujo, Anatole Gershman, Jaime Carbonell, Robert Frederking, João P. Neto category:cs.CL cs.IR published:2013-06-20 summary:Fast and effective automated indexing is critical for search and personalizedservices. Key phrases that consist of one or more words and represent the mainconcepts of the document are often used for the purpose of indexing. In thispaper, we investigate the use of additional semantic features andpre-processing steps to improve automatic key phrase extraction. These featuresinclude the use of signal words and freebase categories. Some of these featureslead to significant improvements in the accuracy of the results. We alsoexperimented with 2 forms of document pre-processing that we call lightfiltering and co-reference normalization. Light filtering removes sentencesfrom the document, which are judged peripheral to its main content.Co-reference normalization unifies several written forms of the same namedentity into a unique form. We also needed a "Gold Standard" - a set of labeleddocuments for training and evaluation. While the subjective nature of keyphrase selection precludes a true "Gold Standard", we used Amazon's MechanicalTurk service to obtain a useful approximation. Our data indicates that thebiggest improvements in performance were due to shallow semantic features, newscategories, and rhetorical signals (nDCG 78.47% vs. 68.93%). The inclusion ofdeeper semantic features such as Freebase sub-categories was not beneficial byitself, but in combination with pre-processing, did cause slight improvementsin the nDCG scores.
arxiv-3300-128 | Iterative Grassmannian Optimization for Robust Image Alignment | http://arxiv.org/pdf/1306.0404v2.pdf | author:Jun He, Dejiao Zhang, Laura Balzano, Tao Tao category:cs.CV math.OC stat.ML published:2013-06-03 summary:Robust high-dimensional data processing has witnessed an exciting developmentin recent years, as theoretical results have shown that it is possible usingconvex programming to optimize data fit to a low-rank component plus a sparseoutlier component. This problem is also known as Robust PCA, and it has foundapplication in many areas of computer vision. In image and video processing andface recognition, the opportunity to process massive image databases isemerging as people upload photo and video data online in unprecedented volumes.However, data quality and consistency is not controlled in any way, and themassiveness of the data poses a serious computational challenge. In this paperwe present t-GRASTA, or "Transformed GRASTA (Grassmannian Robust AdaptiveSubspace Tracking Algorithm)". t-GRASTA iteratively performs incrementalgradient descent constrained to the Grassmann manifold of subspaces in order tosimultaneously estimate a decomposition of a collection of images into alow-rank subspace, a sparse part of occlusions and foreground objects, and atransformation such as rotation or translation of the image. We show thatt-GRASTA is 4 $\times$ faster than state-of-the-art algorithms, has half thememory requirement, and can achieve alignment for face images as well asjittered camera surveillance images.
arxiv-3300-129 | Off-Policy Actor-Critic | http://arxiv.org/pdf/1205.4839v5.pdf | author:Thomas Degris, Martha White, Richard S. Sutton category:cs.LG published:2012-05-22 summary:This paper presents the first actor-critic algorithm for off-policyreinforcement learning. Our algorithm is online and incremental, and itsper-time-step complexity scales linearly with the number of learned weights.Previous work on actor-critic algorithms is limited to the on-policy settingand does not take advantage of the recent advances in off-policy gradienttemporal-difference learning. Off-policy techniques, such as Greedy-GQ, enablea target policy to be learned while following and obtaining data from another(behavior) policy. For many problems, however, actor-critic methods are morepractical than action value methods (like Greedy-GQ) because they explicitlyrepresent the policy; consequently, the policy can be stochastic and utilize alarge action space. In this paper, we illustrate how to practically combine thegenerality and learning potential of off-policy learning with the flexibilityin action selection given by actor-critic methods. We derive an incremental,linear time and space complexity algorithm that includes eligibility traces,prove convergence under assumptions similar to previous off-policy algorithms,and empirically show better or comparable performance to existing algorithms onstandard reinforcement-learning benchmark problems.
arxiv-3300-130 | Evolving Boolean Regulatory Networks with Epigenetic Control | http://arxiv.org/pdf/1306.4793v1.pdf | author:Larry Bull category:cs.NE q-bio.MN published:2013-06-20 summary:The significant role of epigenetic mechanisms within natural systems hasbecome increasingly clear. This paper uses a recently presented abstract,tunable Boolean genetic regulatory network model to explore aspects ofepigenetics. It is shown how dynamically controlling transcription via a DNAmethylation-inspired mechanism can be selected for by simulated evolution undervarious single and multiple cell scenarios. Further, it is shown that theeffects of such control can be inherited without detriment to fitness.
arxiv-3300-131 | Analysing Word Importance for Image Annotation | http://arxiv.org/pdf/1306.4758v1.pdf | author:Payal Gulati, A. K. Sharma category:cs.IR cs.CV published:2013-06-20 summary:Image annotation provides several keywords automatically for a given imagebased on various tags to describe its contents which is useful in Imageretrieval. Various researchers are working on text based and content basedimage annotations [7,9]. It is seen, in traditional Image annotationapproaches, annotation words are treated equally without considering theimportance of each word in real world. In context of this, in this work, imagesare annotated with keywords based on their frequency count and wordcorrelation. Moreover this work proposes an approach to compute importancescore of candidate keywords, having same frequency count.
arxiv-3300-132 | Gene selection with guided regularized random forest | http://arxiv.org/pdf/1209.6425v3.pdf | author:Houtao Deng, George Runger category:cs.LG cs.CE published:2012-09-28 summary:The regularized random forest (RRF) was recently proposed for featureselection by building only one ensemble. In RRF the features are evaluated on apart of the training data at each tree node. We derive an upper bound for thenumber of distinct Gini information gain values in a node, and show that manyfeatures can share the same information gain at a node with a small number ofinstances and a large number of features. Therefore, in a node with a smallnumber of instances, RRF is likely to select a feature not strongly relevant.Here an enhanced RRF, referred to as the guided RRF (GRRF), is proposed. InGRRF, the importance scores from an ordinary random forest (RF) are used toguide the feature selection process in RRF. Experiments on 10 gene data setsshow that the accuracy performance of GRRF is, in general, more robust than RRFwhen their parameters change. GRRF is computationally efficient, can selectcompact feature subsets, and has competitive accuracy performance, compared toRRF, varSelRF and LASSO logistic regression (with evaluations from an RFclassifier). Also, RF applied to the features selected by RRF with the minimalregularization outperforms RF applied to all the features for most of the datasets considered here. Therefore, if accuracy is considered more important thanthe size of the feature subset, RRF with the minimal regularization may beconsidered. We use the accuracy performance of RF, a strong classifier, toevaluate feature selection methods, and illustrate that weak classifiers areless capable of capturing the information contained in a feature subset. BothRRF and GRRF were implemented in the "RRF" R package available at CRAN, theofficial R package archive.
arxiv-3300-133 | Galerkin Methods for Complementarity Problems and Variational Inequalities | http://arxiv.org/pdf/1306.4753v1.pdf | author:Geoffrey J. Gordon category:cs.LG cs.AI math.OC published:2013-06-20 summary:Complementarity problems and variational inequalities arise in a wide varietyof areas, including machine learning, planning, game theory, and physicalsimulation. In all of these areas, to handle large-scale problem instances, weneed fast approximate solution methods. One promising idea is Galerkinapproximation, in which we search for the best answer within the span of agiven set of basis functions. Bertsekas proposed one possible Galerkin methodfor variational inequalities. However, this method can exhibit two problems inpractice: its approximation error is worse than might be expected based on theability of the basis to represent the desired solution, and each iterationrequires a projection step that is not always easy to implement efficiently.So, in this paper, we present a new Galerkin method with improved behavior: ournew error bounds depend directly on the distance from the true solution to thesubspace spanned by our basis, and the only projections we require are onto thefeasible region or onto the span of our basis.
arxiv-3300-134 | Felzenszwalb-Baum-Welch: Event Detection by Changing Appearance | http://arxiv.org/pdf/1306.4746v1.pdf | author:Daniel Paul Barrett, Jeffrey Mark Siskind category:cs.CV published:2013-06-20 summary:We propose a method which can detect events in videos by modeling the changein appearance of the event participants over time. This method makes itpossible to detect events which are characterized not by motion, but by thechanging state of the people or objects involved. This is accomplished by usingobject detectors as output models for the states of a hidden Markov model(HMM). The method allows an HMM to model the sequence of poses of the eventparticipants over time, and is effective for poses of humans and inanimateobjects. The ability to use existing object-detection methods as part of anevent model makes it possible to leverage ongoing work in the object-detectioncommunity. A novel training method uses an EM loop to simultaneously learn thetemporal structure and object models automatically, without the need to specifyeither the individual poses to be modeled or the frames in which they occur.The E-step estimates the latent assignment of video frames to HMM states, whilethe M-step estimates both the HMM transition probabilities and state outputmodels, including the object detectors, which are trained on the weightedsubset of frames assigned to their state. A new dataset was gathered becauselittle work has been done on events characterized by changing object pose, andsuitable datasets are not available. Our method produced results superior tothat of comparison systems on this dataset.
arxiv-3300-135 | Computer simulation based parameter selection for resistance exercise | http://arxiv.org/pdf/1306.4724v1.pdf | author:Ognjen Arandjelovic category:cs.CV cs.HC published:2013-06-20 summary:In contrast to most scientific disciplines, sports science research has beencharacterized by comparatively little effort investment in the development ofrelevant phenomenological models. Scarcer yet is the application of said modelsin practice. We present a framework which allows resistance trainingpractitioners to employ a recently proposed neuromuscular model in actualtraining program design. The first novelty concerns the monitoring aspect ofcoaching. A method for extracting training performance characteristics fromloosely constrained video sequences, effortlessly and with minimal human input,using computer vision is described. The extracted data is subsequently used tofit the underlying neuromuscular model. This is achieved by solving an inversedynamics problem corresponding to a particular exercise. Lastly, a computersimulation of hypothetical training bouts, using athlete-specific capabilityparameters, is used to predict the effected adaptation and changes inperformance. The software described here allows the practitioner to manipulatehypothetical training parameters and immediately see their effect on predictedadaptation for a specific athlete. Thus, this work presents a holistic view ofthe monitoring-assessment-adjustment loop.
arxiv-3300-136 | Using Arabic Wordnet for semantic indexation in information retrieval system | http://arxiv.org/pdf/1306.2499v2.pdf | author:Mohammed Alaeddine Abderrahim, Mohammed El Amine Abderrahim, Mohammed Amine Chikh category:cs.IR cs.CL published:2013-06-11 summary:In the context of arabic Information Retrieval Systems (IRS) guided by arabicontology and to enable those systems to better respond to user requirements,this paper aims to representing documents and queries by the best conceptsextracted from Arabic Wordnet. Identified concepts belonging to Arabic WordNetsynsets are extracted from documents and queries, and those having a singlesense are expanded. The expanded query is then used by the IRS to retrieve therelevant documents searched. Our experiments are based primarily on a mediumsize corpus of arabic text. The results obtained shown us that there are aglobal improvement in the performance of the arabic IRS.
arxiv-3300-137 | Non-Correlated Character Recognition using Artificial Neural Network | http://arxiv.org/pdf/1306.4629v1.pdf | author:Tirtharaj Dash, Tanistha Nayak category:cs.NE cs.CV published:2013-06-19 summary:This paper investigates a method of Handwritten English Character Recognitionusing Artificial Neural Network (ANN). This work has been done in offlineEnvironment for non correlated characters, which do not possess any linearrelationships among them. We test that whether the particular tested characterbelongs to a cluster or not. The implementation is carried out in Matlabenvironment and successfully tested. Fifty-two sets of English alphabets areused to train the ANN and test the network. The algorithms are tested with 26capital letters and 26 small letters. The testing result showed that theproposed ANN based algorithm showed a maximum recognition rate of 85%.
arxiv-3300-138 | Solution to Quadratic Equation Using Genetic Algorithm | http://arxiv.org/pdf/1306.4622v1.pdf | author:Tanistha Nayak, Tirtharaj Dash category:cs.NE published:2013-06-19 summary:Solving Quadratic equation is one of the intrinsic interests as it is thesimplest nonlinear equations. A novel approach for solving Quadratic Equationbased on Genetic Algorithms (GAs) is presented. Genetic Algorithms (GAs) are atechnique to solve problems which need optimization. Generation of trialsolutions have been formed by this method. Many examples have been worked out,and in most cases we find out the exact solution. We have discussed the effectof different parameters on the performance of the developed algorithm. Theresults are concluded after rigorous testing on different equations.
arxiv-3300-139 | English Character Recognition using Artificial Neural Network | http://arxiv.org/pdf/1306.4621v1.pdf | author:Tirtharaj Dash, Tanistha Nayak category:cs.NE published:2013-06-19 summary:This work focuses on development of a Offline Hand Written English CharacterRecognition algorithm based on Artificial Neural Network (ANN). The ANNimplemented in this work has single output neuron which shows whether thetested character belongs to a particular cluster or not. The implementation iscarried out completely in 'C' language. Ten sets of English alphabets(small-26, capital-26) were used to train the ANN and 5 sets of Englishalphabets were used to test the network. The characters were collected fromdifferent persons over duration of about 25 days. The algorithm was tested with5 capital letters and 5 small letter sets. However, the result showed that thealgorithm recognized English alphabet patterns with maximum accuracy of 92.59%and False Rejection Rate (FRR) of 0%.
arxiv-3300-140 | Time Efficient Approach To Offline Hand Written Character Recognition Using Associative Memory Net | http://arxiv.org/pdf/1306.4592v1.pdf | author:Tirtharaj Dash category:cs.NE cs.CV published:2013-06-19 summary:In this paper, an efficient Offline Hand Written Character Recognitionalgorithm is proposed based on Associative Memory Net (AMN). The AMN used inthis work is basically auto associative. The implementation is carried outcompletely in 'C' language. To make the system perform to its best with minimalcomputation time, a Parallel algorithm is also developed using an API packageOpenMP. Characters are mainly English alphabets (Small (26), Capital (26))collected from system (52) and from different persons (52). The characterscollected from system are used to train the AMN and characters collected fromdifferent persons are used for testing the recognition ability of the net. Thedetailed analysis showed that the network recognizes the hand writtencharacters with recognition rate of 72.20% in average case. However, in bestcase, it recognizes the collected hand written characters with 88.5%. Thedeveloped network consumes 3.57 sec (average) in Serial implementation and 1.16sec (average) in Parallel implementation using OpenMP.
arxiv-3300-141 | A lasso for hierarchical interactions | http://arxiv.org/pdf/1205.5050v3.pdf | author:Jacob Bien, Jonathan Taylor, Robert Tibshirani category:stat.ME math.ST stat.ML stat.TH published:2012-05-22 summary:We add a set of convex constraints to the lasso to produce sparse interactionmodels that honor the hierarchy restriction that an interaction only beincluded in a model if one or both variables are marginally important. We givea precise characterization of the effect of this hierarchy constraint, provethat hierarchy holds with probability one and derive an unbiased estimate forthe degrees of freedom of our estimator. A bound on this estimate reveals theamount of fitting "saved" by the hierarchy constraint. We distinguish betweenparameter sparsity - the number of nonzero coefficients - and practicalsparsity - the number of raw variables one must measure to make a newprediction. Hierarchy focuses on the latter, which is more closely tied toimportant data collection concerns such as cost, time and effort. We develop analgorithm, available in the R package hierNet, and perform an empirical studyof our method.
arxiv-3300-142 | Hacking Smart Machines with Smarter Ones: How to Extract Meaningful Data from Machine Learning Classifiers | http://arxiv.org/pdf/1306.4447v1.pdf | author:Giuseppe Ateniese, Giovanni Felici, Luigi V. Mancini, Angelo Spognardi, Antonio Villani, Domenico Vitali category:cs.CR cs.LG stat.ML published:2013-06-19 summary:Machine Learning (ML) algorithms are used to train computers to perform avariety of complex tasks and improve with experience. Computers learn how torecognize patterns, make unintended decisions, or react to a dynamicenvironment. Certain trained machines may be more effective than others becausethey are based on more suitable ML algorithms or because they were trainedthrough superior training sets. Although ML algorithms are known and publiclyreleased, training sets may not be reasonably ascertainable and, indeed, may beguarded as trade secrets. While much research has been performed about theprivacy of the elements of training sets, in this paper we focus our attentionon ML classifiers and on the statistical information that can be unconsciouslyor maliciously revealed from them. We show that it is possible to inferunexpected but useful information from ML classifiers. In particular, we builda novel meta-classifier and train it to hack other classifiers, obtainingmeaningful information about their training sets. This kind of informationleakage can be exploited, for example, by a vendor to build more effectiveclassifiers or to simply acquire trade secrets from a competitor's apparatus,potentially violating its intellectual property rights.
arxiv-3300-143 | Machine Learning with Operational Costs | http://arxiv.org/pdf/1112.0698v4.pdf | author:Theja Tulabandhula, Cynthia Rudin category:stat.ML cs.AI math.OC published:2011-12-03 summary:This work proposes a way to align statistical modeling with decision making.We provide a method that propagates the uncertainty in predictive modeling tothe uncertainty in operational cost, where operational cost is the amount spentby the practitioner in solving the problem. The method allows us to explore therange of operational costs associated with the set of reasonable statisticalmodels, so as to provide a useful way for practitioners to understanduncertainty. To do this, the operational cost is cast as a regularization termin a learning algorithm's objective function, allowing either an optimistic orpessimistic view of possible costs, depending on the regularization parameter.From another perspective, if we have prior knowledge about the operationalcost, for instance that it should be low, this knowledge can help to restrictthe hypothesis space, and can help with generalization. We provide atheoretical generalization bound for this scenario. We also show that learningwith operational costs is related to robust optimization.
arxiv-3300-144 | Joint estimation of sparse multivariate regression and conditional graphical models | http://arxiv.org/pdf/1306.4410v1.pdf | author:Junhui Wang category:stat.ML cs.LG published:2013-06-19 summary:Multivariate regression model is a natural generalization of the classicalunivari- ate regression model for ?tting multiple responses. In this paper, wepropose a high- dimensional multivariate conditional regression model forconstructing sparse estimates of the multivariate regression coe?cient matrixthat accounts for the dependency struc- ture among the multiple responses. Theproposed method decomposes the multivariate regression problem into a series ofpenalized conditional log-likelihood of each response conditioned on thecovariates and other responses. It allows simultaneous estimation of the sparseregression coe?cient matrix and the sparse inverse covariance matrix. Theasymptotic selection consistency and normality are established for thediverging dimension of the covariates and number of responses. The e?ectivenessof the pro- posed method is also demonstrated in a variety of simulatedexamples as well as an application to the Glioblastoma multiforme cancer data.
arxiv-3300-145 | An Overview of the Research on Texture Based Plant Leaf Classification | http://arxiv.org/pdf/1306.4345v1.pdf | author:Vishakha Metre, Jayshree Ghorpade category:cs.CV published:2013-06-18 summary:Plant classification has a broad application prospective in agriculture andmedicine, and is especially significant to the biology diversity research. Asplants are vitally important for environmental protection, it is more importantto identify and classify them accurately. Plant leaf classification is atechnique where leaf is classified based on its different morphologicalfeatures. The goal of this paper is to provide an overview of different aspectsof texture based plant leaf classification and related things. At last we willbe concluding about the efficient method i.e. the method that gives betterperformance compared to the other methods.
arxiv-3300-146 | The evolutionary origins of modularity | http://arxiv.org/pdf/1207.2743v2.pdf | author:Jeff Clune, Jean-Baptiste Mouret, Hod Lipson category:q-bio.PE cs.NE q-bio.MN q-bio.NC published:2012-07-11 summary:A central biological question is how natural organisms are so evolvable(capable of quickly adapting to new environments). A key driver of evolvabilityis the widespread modularity of biological networks--their organization asfunctional, sparsely connected subunits--but there is no consensus regardingwhy modularity itself evolved. While most hypotheses assume indirect selectionfor evolvability, here we demonstrate that the ubiquitous, direct selectionpressure to reduce the cost of connections between network nodes causes theemergence of modular networks. Experiments with selection pressures to maximizenetwork performance and minimize connection costs yield networks that aresignificantly more modular and more evolvable than control experiments thatonly select for performance. These results will catalyze research in numerousdisciplines, including neuroscience, genetics and harnessing evolution forengineering purposes.
arxiv-3300-147 | Multiarmed Bandit Problems with Delayed Feedback | http://arxiv.org/pdf/1011.1161v3.pdf | author:Sudipto Guha, Kamesh Munagala, Martin Pal category:cs.DS cs.LG published:2010-11-04 summary:In this paper we initiate the study of optimization of bandit type problemsin scenarios where the feedback of a play is not immediately known. This arisesnaturally in allocation problems which have been studied extensively in theliterature, albeit in the absence of delays in the feedback. We study thisproblem in the Bayesian setting. In presence of delays, no solution withprovable guarantees is known to exist with sub-exponential running time. We show that bandit problems with delayed feedback that arise in allocationsettings can be forced to have significant structure, with a slight loss inoptimality. This structure gives us the ability to reason about therelationship of single arm policies to the entangled optimum policy, andeventually leads to a O(1) approximation for a significantly general class ofpriors. The structural insights we develop are of key interest and carry overto the setting where the feedback of an action is available instantaneously,and we improve all previous results in this setting as well.
arxiv-3300-148 | A class of random fields on complete graphs with tractable partition function | http://arxiv.org/pdf/1212.2136v2.pdf | author:Boris Flach category:cs.LG stat.ML published:2012-12-10 summary:The aim of this short note is to draw attention to a method by which thepartition function and marginal probabilities for a certain class of randomfields on complete graphs can be computed in polynomial time. This classincludes Ising models with homogeneous pairwise potentials but arbitrary(inhomogeneous) unary potentials. Similarly, the partition function andmarginal probabilities can be computed in polynomial time for random fields oncomplete bipartite graphs, provided they have homogeneous pairwise potentials.We expect that these tractable classes of large scale random fields can be veryuseful for the evaluation of approximation algorithms by providing exact errorestimates.
arxiv-3300-149 | Generalized Beta Divergence | http://arxiv.org/pdf/1306.3530v2.pdf | author:Y. Kenan Yilmaz category:stat.ML published:2013-06-14 summary:This paper generalizes beta divergence beyond its classical form associatedwith power variance functions of Tweedie models. Generalized form isrepresented by a compact definite integral as a function of variance functionof the exponential dispersion model. This compact integral form simplifiesderivations of many properties such as scaling, translation and expectation ofthe beta divergence. Further, we show that beta divergence and (half of) thestatistical deviance are equivalent measures.
arxiv-3300-150 | Bioclimating Modelling: A Machine Learning Perspective | http://arxiv.org/pdf/1306.4152v1.pdf | author:Maumita Bhattacharya category:cs.LG stat.ML 68T05 published:2013-06-18 summary:Many machine learning (ML) approaches are widely used to generate bioclimaticmodels for prediction of geographic range of organism as a function of climate.Applications such as prediction of range shift in organism, range of invasivespecies influenced by climate change are important parameters in understandingthe impact of climate change. However, success of machine learning-basedapproaches depends on a number of factors. While it can be safely said that noparticular ML technique can be effective in all applications and success of atechnique is predominantly dependent on the application or the type of theproblem, it is useful to understand their behaviour to ensure informed choiceof techniques. This paper presents a comprehensive review of machinelearning-based bioclimatic model generation and analyses the factorsinfluencing success of such models. Considering the wide use of statisticaltechniques, in our discussion we also include conventional statisticaltechniques used in bioclimatic modelling.
arxiv-3300-151 | Punjabi Language Interface to Database: a brief review | http://arxiv.org/pdf/1306.4139v1.pdf | author:Preeti Verma, Suket Arora, Kamaljit Batra category:cs.CL cs.HC published:2013-06-18 summary:Unlike most user-computer interfaces, a natural language interface allowsusers to communicate fluently with a computer system with very littlepreparation. Databases are often hard to use in cooperating with the usersbecause of their rigid interface. A good NLIDB allows a user to enter commandsand ask questions in native language and then after interpreting respond to theuser in native language. For a large number of applications requiringinteraction between humans and the computer systems, it would be convenient toprovide the end-user friendly interface. Punjabi language interface to databasewould proof fruitful to native people of Punjab, as it provides ease to them touse various e-governance applications like Punjab Sewa, Suwidha, Online PublicUtility Forms, Online Grievance Cell, Land Records Management System,legacymatters, e-District, agriculture, etc. Punjabi is the mother tongue of morethan 110 million people all around the world. According to availableinformation, Punjabi ranks 10th from top out of a total of 6,900 languagesrecognized internationally by the United Nations. This paper covers a briefoverview of the Natural language interface to database, its differentcomponents, its advantages, disadvantages, approaches and techniques used. Thepaper ends with the work done on Punjabi language interface to database andfuture enhancements that can be done.
arxiv-3300-152 | Dialogue System: A Brief Review | http://arxiv.org/pdf/1306.4134v1.pdf | author:Suket Arora, Kamaljeet Batra, Sarabjit Singh category:cs.CL published:2013-06-18 summary:A Dialogue System is a system which interacts with human in natural language.At present many universities are developing the dialogue system in theirregional language. This paper will discuss about dialogue system, itscomponents, challenges and its evaluation. This paper helps the researchers forgetting info regarding dialogues system.
arxiv-3300-153 | Group Symmetry and non-Gaussian Covariance Estimation | http://arxiv.org/pdf/1306.4103v1.pdf | author:Ilya Soloveychik, Ami Wiesel category:stat.ML published:2013-06-18 summary:We consider robust covariance estimation with group symmetry constraints.Non-Gaussian covariance estimation, e.g., Tyler scatter estimator andMultivariate Generalized Gaussian distribution methods, usually involvenon-convex minimization problems. Recently, it was shown that the underlyingprinciple behind their success is an extended form of convexity over thegeodesics in the manifold of positive definite matrices. A modern approach toimprove estimation accuracy is to exploit prior knowledge via additionalconstraints, e.g., restricting the attention to specific classes of covarianceswhich adhere to prior symmetry structures. In this paper, we prove that suchgroup symmetry constraints are also geodesically convex and can therefore beincorporated into various non-Gaussian covariance estimators. Practicalexamples of such sets include: circulant, persymmetric and complex/quaternionproper structures. We provide a simple numerical technique for finding maximumlikelihood estimates under such constraints, and demonstrate their performanceadvantage using synthetic experiments.
arxiv-3300-154 | A Novel Block-DCT and PCA Based Image Perceptual Hashing Algorithm | http://arxiv.org/pdf/1306.4079v1.pdf | author:Zeng Jie category:cs.CV published:2013-06-18 summary:Image perceptual hashing finds applications in content indexing, large-scaleimage database management, certification and authentication and digitalwatermarking. We propose a Block-DCT and PCA based image perceptual hash inthis article and explore the algorithm in the application of tamper detection.The main idea of the algorithm is to integrate color histogram and DCTcoefficients of image blocks as perceptual feature, then to compress perceptualfeatures as inter-feature with PCA, and to threshold to create a robust hash.The robustness and discrimination properties of the proposed algorithm areevaluated in detail. Our algorithms first construct a secondary image, derivedfrom input image by pseudo-randomly extracting features that approximatelycapture semi-global geometric characteristics. From the secondary image (whichdoes not perceptually resemble the input), we further extract the finalfeatures which can be used as a hash value (and can be further suitablyquantized). In this paper, we use spectral matrix invariants as embodied bySingular Value Decomposition. Surprisingly, formation of the secondary imageturns out be quite important since it not only introduces further robustness,but also enhances the security properties. Indeed, our experiments reveal thatour hashing algorithms extract most of the geometric information from theimages and hence are robust to severe perturbations (e.g. up to %50 cropping byarea with 20 degree rotations) on images while avoiding misclassification.Experimental results show that the proposed image perceptual hash algorithm caneffectively address the tamper detection problem with advantageous robustnessand discrimination.
arxiv-3300-155 | Classification for Big Dataset of Bioacoustic Signals Based on Human Scoring System and Artificial Neural Network | http://arxiv.org/pdf/1305.3633v2.pdf | author:Mohammad Pourhomayoun, Peter Dugan, Marian Popescu, Denise Risch, Hal Lewis, Christopher Clark category:cs.CV published:2013-05-15 summary:In this paper, we propose a method to improve sound classificationperformance by combining signal features, derived from the time-frequencyspectrogram, with human perception. The method presented herein exploits anartificial neural network (ANN) and learns the signal features based on thehuman perception knowledge. The proposed method is applied to a large acousticdataset containing 24 months of nearly continuous recordings. The results showa significant improvement in performance of the detection-classificationsystem; yielding as much as 20% improvement in true positive rate for a givenfalse positive rate.
arxiv-3300-156 | Bioacoustic Signal Classification Based on Continuous Region Processing, Grid Masking and Artificial Neural Network | http://arxiv.org/pdf/1305.3635v2.pdf | author:Mohammad Pourhomayoun, Peter Dugan, Marian Popescu, Christopher Clark category:cs.CV published:2013-05-15 summary:In this paper, we develop a novel method based on machine-learning and imageprocessing to identify North Atlantic right whale (NARW) up-calls in thepresence of high levels of ambient and interfering noise. We apply a continuousregion algorithm on the spectrogram to extract the regions of interest, andthen use grid masking techniques to generate a small feature set that is thenused in an artificial neural network classifier to identify the NARW up-calls.It is shown that the proposed technique is effective in detecting and capturingeven very faint up-calls, in the presence of ambient and interfering noises.The method is evaluated on a dataset recorded in Massachusetts Bay, UnitedStates. The dataset includes 20000 sound clips for training, and 10000 soundclips for testing. The results show that the proposed technique can achieve anerror rate of less than FPR = 4.5% for a 90% true positive rate.
arxiv-3300-157 | Discriminating word senses with tourist walks in complex networks | http://arxiv.org/pdf/1306.3920v1.pdf | author:Thiago C. Silva, Diego R. Amancio category:cs.CL cs.SI physics.soc-ph published:2013-06-17 summary:Patterns of topological arrangement are widely used for both animal and humanbrains in the learning process. Nevertheless, automatic learning techniquesfrequently overlook these patterns. In this paper, we apply a learningtechnique based on the structural organization of the data in the attributespace to the problem of discriminating the senses of 10 polysemous words. Usingtwo types of characterization of meanings, namely semantical and topologicalapproaches, we have observed significative accuracy rates in identifying thesuitable meanings in both techniques. Most importantly, we have found that thecharacterization based on the deterministic tourist walk improves thedisambiguation process when one compares with the discrimination achieved withtraditional complex networks measurements such as assortativity and clusteringcoefficient. To our knowledge, this is the first time that such deterministicwalk has been applied to such a kind of problem. Therefore, our findingsuggests that the tourist walk characterization may be useful in other relatedapplications.
arxiv-3300-158 | On Finding the Largest Mean Among Many | http://arxiv.org/pdf/1306.3917v1.pdf | author:Kevin Jamieson, Matthew Malloy, Robert Nowak, Sebastien Bubeck category:stat.ML cs.LG published:2013-06-17 summary:Sampling from distributions to find the one with the largest mean arises in abroad range of applications, and it can be mathematically modeled as amulti-armed bandit problem in which each distribution is associated with anarm. This paper studies the sample complexity of identifying the best arm(largest mean) in a multi-armed bandit problem. Motivated by large-scaleapplications, we are especially interested in identifying situations where thetotal number of samples that are necessary and sufficient to find the best armscale linearly with the number of arms. We present a single-parametermulti-armed bandit model that spans the range from linear to superlinear samplecomplexity. We also give a new algorithm for best arm identification, calledPRISM, with linear sample complexity for a wide range of mean distributions.The algorithm, like most exploration procedures for multi-armed bandits, isadaptive in the sense that the next arms to sample are selected based onprevious samples. We compare the sample complexity of adaptive procedures withsimpler non-adaptive procedures using new lower bounds. For many probleminstances, the increased sample complexity required by non-adaptive proceduresis a polynomial factor of the number of arms.
arxiv-3300-159 | Stability of Multi-Task Kernel Regression Algorithms | http://arxiv.org/pdf/1306.3905v1.pdf | author:Julien Audiffren, Hachem Kadri category:cs.LG stat.ML published:2013-06-17 summary:We study the stability properties of nonlinear multi-task regression inreproducing Hilbert spaces with operator-valued kernels. Such kernels, a.k.a.multi-task kernels, are appropriate for learning prob- lems with nonscalaroutputs like multi-task learning and structured out- put prediction. We showthat multi-task kernel regression algorithms are uniformly stable in thegeneral case of infinite-dimensional output spaces. We then derive under mildassumption on the kernel generaliza- tion bounds of such algorithms, and weshow their consistency even with non Hilbert-Schmidt operator-valued kernels .We demonstrate how to apply the results to various multi-task kernel regressionmethods such as vector-valued SVR and functional ridge regression.
arxiv-3300-160 | Bayesian methods for low-rank matrix estimation: short survey and theoretical study | http://arxiv.org/pdf/1306.3862v1.pdf | author:Pierre Alquier category:stat.ML published:2013-06-17 summary:The problem of low-rank matrix estimation recently received a lot ofattention due to challenging applications. A lot of work has been done onrank-penalized methods and convex relaxation, both on the theoretical andapplied sides. However, only a few papers considered Bayesian estimation. Inthis paper, we review the different type of priors considered on matrices tofavour low-rank. We also prove that the obtained Bayesian estimators, undersuitable assumptions, enjoys the same optimality properties as the ones basedon penalization.
arxiv-3300-161 | Cluster coloring of the Self-Organizing Map: An information visualization perspective | http://arxiv.org/pdf/1306.3860v1.pdf | author:Peter Sarlin, Samuel Rönnqvist category:cs.LG cs.HC published:2013-06-17 summary:This paper takes an information visualization perspective to visualrepresentations in the general SOM paradigm. This involves viewing SOM-basedvisualizations through the eyes of Bertin's and Tufte's theories on datagraphics. The regular grid shape of the Self-Organizing Map (SOM), while beinga virtue for linking visualizations to it, restricts representation of clusterstructures. From the viewpoint of information visualization, this paperprovides a general, yet simple, solution to projection-based coloring of theSOM that reveals structures. First, the proposed color space is easy toconstruct and customize to the purpose of use, while aiming at beingperceptually correct and informative through two separable dimensions. Second,the coloring method is not dependent on any specific method of projection, butis rather modular to fit any objective function suitable for the task at hand.The cluster coloring is illustrated on two datasets: the iris data, and welfareand poverty indicators.
arxiv-3300-162 | Non-Uniform Blind Deblurring with a Spatially-Adaptive Sparse Prior | http://arxiv.org/pdf/1306.3828v1.pdf | author:Haichao Zhang, David Wipf category:cs.CV published:2013-06-17 summary:Typical blur from camera shake often deviates from the standard uniformconvolutional script, in part because of problematic rotations which creategreater blurring away from some unknown center point. Consequently, successfulblind deconvolution requires the estimation of a spatially-varying ornon-uniform blur operator. Using ideas from Bayesian inference and convexanalysis, this paper derives a non-uniform blind deblurring algorithm withseveral desirable, yet previously-unexplored attributes. The underlyingobjective function includes a spatially adaptive penalty which couples thelatent sharp image, non-uniform blur operator, and noise level together. Thiscoupling allows the penalty to automatically adjust its shape based on theestimated degree of local blur and image structure such that regions with largeblur or few prominent edges are discounted. Remaining regions with modest blurand revealing edges therefore dominate the overall estimation process withoutexplicitly incorporating structure-selection heuristics. The algorithm can beimplemented using a majorization-minimization strategy that is virtuallyparameter free. Detailed theoretical analysis and empirical validation on realimages serve to validate the proposed method.
arxiv-3300-163 | Discrete perceptrons | http://arxiv.org/pdf/1306.4375v1.pdf | author:Mihailo Stojnic category:math.PR math-ph math.MP stat.ML published:2013-06-17 summary:Perceptrons have been known for a long time as a promising tool within theneural networks theory. The analytical treatment for a special class ofperceptrons started in seminal work of Gardner \cite{Gar88}. Techniquesinitially employed to characterize perceptrons relied on a statisticalmechanics approach. Many of such predictions obtained in \cite{Gar88} (and in afollow-up \cite{GarDer88}) were later on established rigorously as mathematicalfacts (see, e.g.\cite{SchTir02,SchTir03,TalBook,StojnicGardGen13,StojnicGardSphNeg13,StojnicGardSphErr13}).These typically related to spherical perceptrons. A lot of work has been donerelated to various other types of perceptrons. Among the most challenging onesare what we will refer to as the discrete perceptrons. An introductorystatistical mechanics treatment of such perceptrons was given in\cite{GutSte90}. Relying on results of \cite{Gar88}, \cite{GutSte90}characterized many of the features of several types of discrete perceptrons. Wein this paper, consider a similar subclass of discrete perceptrons and providea mathematically rigorous set of results related to their performance. As itwill turn out, many of the statistical mechanics predictions obtained fordiscrete predictions will in fact appear as mathematically provable bounds.This will in a way emulate a similar type of behavior we observed in\cite{StojnicGardGen13,StojnicGardSphNeg13,StojnicGardSphErr13} when studyingspherical perceptrons.
arxiv-3300-164 | Spherical perceptron as a storage memory with limited errors | http://arxiv.org/pdf/1306.3809v1.pdf | author:Mihailo Stojnic category:math.PR math-ph math.MP stat.ML published:2013-06-17 summary:It has been known for a long time that the classical spherical perceptronscan be used as storage memories. Seminal work of Gardner, \cite{Gar88}, startedan analytical study of perceptrons storage abilities. Many of the Gardner'spredictions obtained through statistical mechanics tools have been rigorouslyjustified. Among the most important ones are of course the storage capacities.The first rigorous confirmations were obtained in \cite{SchTir02,SchTir03} forthe storage capacity of the so-called positive spherical perceptron. These werelater reestablished in \cite{TalBook} and a bit more recently in\cite{StojnicGardGen13}. In this paper we consider a variant of the sphericalperceptron that operates as a storage memory but allows for a certain fractionof errors. In Gardner's original work the statistical mechanics predictions inthis directions were presented sa well. Here, through a mathematically rigorousanalysis, we confirm that the Gardner's predictions in this direction are infact provable upper bounds on the true values of the storage capacity.Moreover, we then present a mechanism that can be used to lower these bounds.Numerical results that we present indicate that the Garnder's storage capacitypredictions may, in a fairly wide range of parameters, be not that far awayfrom the true values.
arxiv-3300-165 | Navigation domain representation for interactive multiview imaging | http://arxiv.org/pdf/1210.5041v2.pdf | author:Thomas Maugey, Ismael Daribo, Gene Cheung, Pascal Frossard category:cs.MM cs.CV published:2012-10-18 summary:Enabling users to interactively navigate through different viewpoints of astatic scene is a new interesting functionality in 3D streaming systems. Whileit opens exciting perspectives towards rich multimedia applications, itrequires the design of novel representations and coding techniques in order tosolve the new challenges imposed by interactive navigation. Interactivityclearly brings new design constraints: the encoder is unaware of the exactdecoding process, while the decoder has to reconstruct information fromincomplete subsets of data since the server can generally not transmit imagesfor all possible viewpoints due to resource constrains. In this paper, wepropose a novel multiview data representation that permits to satisfy bandwidthand storage constraints in an interactive multiview streaming system. Inparticular, we partition the multiview navigation domain into segments, each ofwhich is described by a reference image and some auxiliary information. Theauxiliary information enables the client to recreate any viewpoint in thenavigation segment via view synthesis. The decoder is then able to navigatefreely in the segment without further data request to the server; it requestsadditional data only when it moves to a different segment. We discuss thebenefits of this novel representation in interactive navigation systems andfurther propose a method to optimize the partitioning of the navigation domaininto independent segments, under bandwidth and storage constraints.Experimental results confirm the potential of the proposed representation;namely, our system leads to similar compression performance as classicalinter-view coding, while it provides the high level of flexibility that isrequired for interactive streaming. Hence, our new framework represents apromising solution for 3D data representation in novel interactive multimediaservices.
arxiv-3300-166 | Spectral Experts for Estimating Mixtures of Linear Regressions | http://arxiv.org/pdf/1306.3729v1.pdf | author:Arun Tejasvi Chaganty, Percy Liang category:cs.LG stat.ML published:2013-06-17 summary:Discriminative latent-variable models are typically learned using EM orgradient-based optimization, which suffer from local optima. In this paper, wedevelop a new computationally efficient and provably consistent estimator for amixture of linear regressions, a simple instance of a discriminativelatent-variable model. Our approach relies on a low-rank linear regression torecover a symmetric tensor, which can be factorized into the parameters using atensor power method. We prove rates of convergence for our estimator andprovide an empirical evaluation illustrating its strengths relative to localoptimization (EM).
arxiv-3300-167 | Bayesian test of significance for conditional independence: The multinomial model | http://arxiv.org/pdf/1306.3627v1.pdf | author:Pablo de Morais Andrade, Julio Michael Stern, Carlos Alberto de Bragança Pereira category:stat.CO stat.ML 47N30 G.3 published:2013-06-16 summary:Conditional independence tests (CI tests) have received special attentionlately in Machine Learning and Computational Intelligence related literature asan important indicator of the relationship among the variables used by theirmodels. In the field of Probabilistic Graphical Models (PGM)--which includesBayesian Networks (BN) models--CI tests are especially important for the taskof learning the PGM structure from data. In this paper, we propose the FullBayesian Significance Test (FBST) for tests of conditional independence fordiscrete datasets. FBST is a powerful Bayesian test for precise hypothesis, asan alternative to frequentist's significance tests (characterized by thecalculation of the \emph{p-value}).
arxiv-3300-168 | Recurrent Convolutional Neural Networks for Discourse Compositionality | http://arxiv.org/pdf/1306.3584v1.pdf | author:Nal Kalchbrenner, Phil Blunsom category:cs.CL published:2013-06-15 summary:The compositionality of meaning extends beyond the single sentence. Just aswords combine to form the meaning of sentences, so do sentences combine to formthe meaning of paragraphs, dialogues and general discourse. We introduce both asentence model and a discourse model corresponding to the two levels ofcompositionality. The sentence model adopts convolution as the centraloperation for composing semantic vectors and is based on a novel hierarchicalconvolutional neural network. The discourse model extends the sentence modeland is based on a recurrent neural network that is conditioned in a novel wayboth on the current sentence and on the current speaker. The discourse model isable to capture both the sequentiality of sentences and the interaction betweendifferent speakers. Without feature engineering or pretraining and with simplegreedy decoding, the discourse model coupled to the sentence model obtainsstate of the art performance on a dialogue act classification experiment.
arxiv-3300-169 | Early stopping and non-parametric regression: An optimal data-dependent stopping rule | http://arxiv.org/pdf/1306.3574v1.pdf | author:Garvesh Raskutti, Martin J. Wainwright, Bin Yu category:stat.ML published:2013-06-15 summary:The strategy of early stopping is a regularization technique based onchoosing a stopping time for an iterative algorithm. Focusing on non-parametricregression in a reproducing kernel Hilbert space, we analyze the early stoppingstrategy for a form of gradient-descent applied to the least-squares lossfunction. We propose a data-dependent stopping rule that does not involvehold-out or cross-validation data, and we prove upper bounds on the squarederror of the resulting function estimate, measured in either the $L^2(P)$ and$L^2(P_n)$ norm. These upper bounds lead to minimax-optimal rates for variouskernel classes, including Sobolev smoothness classes and other forms ofreproducing kernel Hilbert spaces. We show through simulation that our stoppingrule compares favorably to two other stopping rules, one based on hold-out dataand the other based on Stein's unbiased risk estimate. We also establish atight connection between our early stopping strategy and the solution path of akernel ridge regression estimator.
arxiv-3300-170 | iCub World: Friendly Robots Help Building Good Vision Data-Sets | http://arxiv.org/pdf/1306.3560v1.pdf | author:Sean Ryan Fanello, Carlo Ciliberto, Matteo Santoro, Lorenzo Natale, Giorgio Metta, Lorenzo Rosasco, Francesca Odone category:cs.CV published:2013-06-15 summary:In this paper we present and start analyzing the iCub World data-set, anobject recognition data-set, we acquired using a Human-Robot Interaction (HRI)scheme and the iCub humanoid robot platform. Our set up allows for rapidacquisition and annotation of data with corresponding ground truth. While moreconstrained in its scopes -- the iCub world is essentially a robotics researchlab -- we demonstrate how the proposed data-set poses challenges to currentrecognition systems. The iCubWorld data-set is publicly available. The data-setcan be downloaded from: http://www.iit.it/en/projects/data-sets.html.
arxiv-3300-171 | Outlying Property Detection with Numerical Attributes | http://arxiv.org/pdf/1306.3558v1.pdf | author:Fabrizio Angiulli, Fabio Fassetti, Luigi Palopoli, Giuseppe Manco category:cs.LG cs.DB stat.ML published:2013-06-15 summary:The outlying property detection problem is the problem of discovering theproperties distinguishing a given object, known in advance to be an outlier ina database, from the other database objects. In this paper, we analyze theproblem within a context where numerical attributes are taken into account,which represents a relevant case left open in the literature. We introduce ameasure to quantify the degree the outlierness of an object, which isassociated with the relative likelihood of the value, compared to the to therelative likelihood of other objects in the database. As a major contribution,we present an efficient algorithm to compute the outlierness relative tosignificant subsets of the data. The latter subsets are characterized in a"rule-based" fashion, and hence the basis for the underlying explanation of theoutlierness.
arxiv-3300-172 | Constructive Setting of the Density Ratio Estimation Problem and its Rigorous Solution | http://arxiv.org/pdf/1306.0407v2.pdf | author:Vladimir Vapnik, Igor Braga, Rauf Izmailov category:stat.ML published:2013-06-03 summary:We introduce a general constructive setting of the density ratio estimationproblem as a solution of a (multidimensional) integral equation. In thisequation, not only its right hand side is known approximately, but also theintegral operator is defined approximately. We show that this ill-posed problemhas a rigorous solution and obtain the solution in a closed form. The keyelement of this solution is the novel V-matrix, which captures the geometry ofthe observed samples. We compare our method with three well-known previouslyproposed ones. Our experimental results demonstrate the good potential of thenew approach.
arxiv-3300-173 | A framework for (under)specifying dependency syntax without overloading annotators | http://arxiv.org/pdf/1306.2091v2.pdf | author:Nathan Schneider, Brendan O'Connor, Naomi Saphra, David Bamman, Manaal Faruqui, Noah A. Smith, Chris Dyer, Jason Baldridge category:cs.CL published:2013-06-10 summary:We introduce a framework for lightweight dependency syntax annotation. Ourformalism builds upon the typical representation for unlabeled dependencies,permitting a simple notation and annotation workflow. Moreover, the formalismencourages annotators to underspecify parts of the syntax if doing so wouldstreamline the annotation process. We demonstrate the efficacy of thisannotation on three languages and develop algorithms to evaluate and compareunderspecified annotations.
arxiv-3300-174 | Hyperparameter Optimization and Boosting for Classifying Facial Expressions: How good can a "Null" Model be? | http://arxiv.org/pdf/1306.3476v1.pdf | author:James Bergstra, David D. Cox category:cs.CV cs.LG stat.ML published:2013-06-14 summary:One of the goals of the ICML workshop on representation and learning is toestablish benchmark scores for a new data set of labeled facial expressions.This paper presents the performance of a "Null" model consisting ofconvolutions with random weights, PCA, pooling, normalization, and a linearreadout. Our approach focused on hyperparameter optimization rather than novelmodel components. On the Facial Expression Recognition Challenge held by theKaggle website, our hyperparameter optimization approach achieved a score of60% accuracy on the test data. This paper also introduces a new ensembleconstruction variant that combines hyperparameter optimization with theconstruction of ensembles. This algorithm constructed an ensemble of fourmodels that scored 65.5% accuracy. These scores rank 12th and 5th respectivelyamong the 56 challenge participants. It is worth noting that our approach wasdeveloped prior to the release of the data set, and applied withoutmodification; our strong competition performance suggests that the TPEhyperparameter optimization algorithm and domain expertise encoded in our Nullmodel can generalize to new image classification data sets.
arxiv-3300-175 | Classifying Single-Trial EEG during Motor Imagery with a Small Training Set | http://arxiv.org/pdf/1306.3474v1.pdf | author:Yijun Wang category:cs.LG cs.HC stat.ML published:2013-06-14 summary:Before the operation of a motor imagery based brain-computer interface (BCI)adopting machine learning techniques, a cumbersome training procedure isunavoidable. The development of a practical BCI posed the challenge ofclassifying single-trial EEG with a small training set. In this letter, weaddressed this problem by employing a series of signal processing and machinelearning approaches to alleviate overfitting and obtained test accuracy similarto training accuracy on the datasets from BCI Competition III and our ownexperiments.
arxiv-3300-176 | Random crossings in dependency trees | http://arxiv.org/pdf/1305.4561v2.pdf | author:Ramon Ferrer-i-Cancho category:cs.CL cs.DM cs.SI physics.soc-ph published:2013-05-20 summary:It has been hypothesized that the rather small number of crossings in realsyntactic dependency trees is a side-effect of pressure for dependency lengthminimization. Here we answer a related important research question: what wouldbe the expected number of crossings if the natural order of a sentence waslost? We show that this number depends only on the number of vertices of thedependency tree (the sentence length) and the second moment of vertex degrees.The expected number of crossings is minimum for a star tree (crossings areimpossible) and maximum for a linear tree (the number of crossings is of theorder of the square of the sequence length).
arxiv-3300-177 | Live-wire 3D medical images segmentation | http://arxiv.org/pdf/1306.3415v1.pdf | author:Ognjen Arandjelovic category:cs.CV published:2013-06-14 summary:This report describes the design, implementation, evaluation and originalenhancements to the Live-Wire method for 2D and 3D image segmentation.Live-Wire 2D employs a semi-automatic paradigm; the user is asked to select afew boundary points of the object to segment, to steer the process in the rightdirection, while the result is displayed in real time. In our implementationsegmentation is extended to three dimensions by performing this process on aslice-by-slice basis. User's time and involvement is further reduced byallowing him to specify object contours in planes orthogonal to the slices. Ifthese planes are chosen strategically, Live-Wire 3D can perform 2D segmentationin the plane of each slice automatically. This report also proposes twoimprovements to the original method, path heating and a new graph edge featurefunction based on variance of path properties along the boundary. We show thatthese improvements lead up to a 33% reduction in interaction with the user, andimproved delineation in presence of strong interfering edges.
arxiv-3300-178 | Constrained fractional set programs and their application in local clustering and community detection | http://arxiv.org/pdf/1306.3409v1.pdf | author:Thomas Bühler, Syama Sundar Rangapuram, Simon Setzer, Matthias Hein category:stat.ML cs.LG math.OC published:2013-06-14 summary:The (constrained) minimization of a ratio of set functions is a problemfrequently occurring in clustering and community detection. As theseoptimization problems are typically NP-hard, one uses convex or spectralrelaxations in practice. While these relaxations can be solved globallyoptimally, they are often too loose and thus lead to results far away from theoptimum. In this paper we show that every constrained minimization problem of aratio of non-negative set functions allows a tight relaxation into anunconstrained continuous optimization problem. This result leads to a flexibleframework for solving constrained problems in network analysis. While aglobally optimal solution for the resulting non-convex problem cannot beguaranteed, we outperform the loose convex or spectral relaxations by a largemargin on constrained local clustering problems.
arxiv-3300-179 | Sparse Recovery of Streaming Signals Using L1-Homotopy | http://arxiv.org/pdf/1306.3331v1.pdf | author:M. Salman Asif, Justin Romberg category:cs.IT math.IT math.OC stat.ML published:2013-06-14 summary:Most of the existing methods for sparse signal recovery assume a staticsystem: the unknown signal is a finite-length vector for which a fixed set oflinear measurements and a sparse representation basis are available and anL1-norm minimization program is solved for the reconstruction. However, thesame representation and reconstruction framework is not readily applicable in astreaming system: the unknown signal changes over time, and it is measured andreconstructed sequentially over small time intervals. In this paper, we discuss two such streaming systems and a homotopy-basedalgorithm for quickly solving the associated L1-norm minimization programs: 1)Recovery of a smooth, time-varying signal for which, instead of using blocktransforms, we use lapped orthogonal transforms for sparse representation. 2)Recovery of a sparse, time-varying signal that follows a linear dynamic model.For both the systems, we iteratively process measurements over a slidinginterval and estimate sparse coefficients by solving a weighted L1-normminimization program. Instead of solving a new L1 program from scratch at everyiteration, we use an available signal estimate as a starting point in ahomotopy formulation. Starting with a warm-start vector, our homotopy algorithmupdates the solution in a small number of computationally inexpensive steps asthe system changes. The homotopy algorithm presented in this paper is highlyversatile as it can update the solution for the L1 problem in a number ofdynamical settings. We demonstrate with numerical experiments that our proposedstreaming recovery framework outperforms the methods that represent andreconstruct a signal as independent, disjoint blocks, in terms of quality ofreconstruction, and that our proposed homotopy-based updating schemeoutperforms current state-of-the-art solvers in terms of the computation timeand complexity.
arxiv-3300-180 | Matching objects across the textured-smooth continuum | http://arxiv.org/pdf/1306.3297v1.pdf | author:Ognjen Arandjelovic category:cs.CV published:2013-06-14 summary:The problem of 3D object recognition is of immense practical importance, withthe last decade witnessing a number of breakthroughs in the state of the art.Most of the previous work has focused on the matching of textured objects usinglocal appearance descriptors extracted around salient image points. Therecently proposed bag of boundaries method was the first to address directlythe problem of matching smooth objects using boundary features. However, noprevious work has attempted to achieve a holistic treatment of the problem byjointly using textural and shape features which is what we describe herein. Dueto the complementarity of the two modalities, we fuse the correspondingmatching scores and learn their relative weighting in a data specific manner byoptimizing discriminative performance on synthetically distorted data. For thetextural description of an object we adopt a representation in the form of ahistogram of SIFT based visual words. Similarly the apparent shape of an objectis represented by a histogram of discretized features capturing local shape. Ona large public database of a diverse set of objects, the proposed method isshown to outperform significantly both purely textural and purely shape basedapproaches for matching across viewpoint variation.
arxiv-3300-181 | Feature Learning by Multidimensional Scaling and its Applications in Object Recognition | http://arxiv.org/pdf/1306.3294v1.pdf | author:Quan Wang, Kim L. Boyer category:cs.CV published:2013-06-14 summary:We present the MDS feature learning framework, in which multidimensionalscaling (MDS) is applied on high-level pairwise image distances to learnfixed-length vector representations of images. The aspects of the images thatare captured by the learned features, which we call MDS features, completelydepend on what kind of image distance measurement is employed. With properlyselected semantics-sensitive image distances, the MDS features provide richsemantic information about the images that is not captured by other featureextraction techniques. In our work, we introduce the iteratedLevenberg-Marquardt algorithm for solving MDS, and study the MDS featurelearning with IMage Euclidean Distance (IMED) and Spatial Pyramid Matching(SPM) distance. We present experiments on both synthetic data and real images--- the publicly accessible UIUC car image dataset. The MDS features based onSPM distance achieve exceptional performance for the car recognition task.
arxiv-3300-182 | Sparse Inverse Covariance Matrix Estimation Using Quadratic Approximation | http://arxiv.org/pdf/1306.3212v1.pdf | author:Cho-Jui Hsieh, Matyas A. Sustik, Inderjit S. Dhillon, Pradeep Ravikumar category:cs.LG stat.ML published:2013-06-13 summary:The L1-regularized Gaussian maximum likelihood estimator (MLE) has been shownto have strong statistical guarantees in recovering a sparse inverse covariancematrix, or alternatively the underlying graph structure of a Gaussian MarkovRandom Field, from very limited samples. We propose a novel algorithm forsolving the resulting optimization problem which is a regularizedlog-determinant program. In contrast to recent state-of-the-art methods thatlargely use first order gradient information, our algorithm is based onNewton's method and employs a quadratic approximation, but with somemodifications that leverage the structure of the sparse Gaussian MLE problem.We show that our method is superlinearly convergent, and present experimentalresults using synthetic and real-world application data that demonstrate theconsiderable improvements in performance of our method when compared to otherstate-of-the-art methods.
arxiv-3300-183 | Segmentation et Interprétation de Nuages de Points pour la Modélisation d'Environnements Urbains | http://arxiv.org/pdf/1306.3084v1.pdf | author:Jorge Hernandez, Beatriz Marcotegui category:cs.CV published:2013-06-13 summary:Dans cet article, nous pr\'esentons une m\'ethode pour la d\'etection et laclassification d'artefacts au niveau du sol, comme phase de filtragepr\'ealable \`a la mod\'elisation d'environnements urbains. La m\'ethode ded\'etection est r\'ealis\'ee sur l'image profondeur, une projection de nuage depoints sur un plan image o\`u la valeur du pixel correspond \`a la distance dupoint au plan. En faisant l'hypoth\`ese que les artefacts sont situ\'es au sol,ils sont d\'etect\'es par une transformation de chapeau haut de forme parremplissage de trous sur l'image de profondeur. Les composantes connexes ainsiobtenues, sont ensuite caract\'eris\'ees et une analyse des variables estutilis\'ee pour la s\'election des caract\'eristiques les plus discriminantes.Les composantes connexes sont donc classifi\'ees en quatre cat\'egories(lampadaires, pi\'etons, voitures et "Reste") \`a l'aide d'un algorithmed'apprentissage supervis\'e. La m\'ethode a \'et\'e test\'ee sur des nuages depoints de la ville de Paris, en montrant de bons r\'esultats de d\'etection etde classification dans l'ensemble de donn\'ees.---In this article, we present amethod for detection and classification of artifacts at the street level, inorder to filter cloud point, facilitating the urban modeling process. Ourapproach exploits 3D information by using range image, a projection of 3Dpoints onto an image plane where the pixel intensity is a function of themeasured distance between 3D points and the plane. By assuming that theartifacts are on the ground, they are detected using a Top-Hat of the holefilling algorithm of range images. Then, several features are extracted fromthe detected connected components and a stepwise forward variable/modelselection by using the Wilk's Lambda criterion is performed. Afterward, CCs areclassified in four categories (lampposts, pedestrians, cars and others) byusing a supervised machine learning method. The proposed method was tested oncloud points of Paris, and have shown satisfactory results on the wholedataset.
arxiv-3300-184 | Physeter catodon localization by sparse coding | http://arxiv.org/pdf/1306.3058v1.pdf | author:Sébastien Paris, Yann Doh, Hervé Glotin, Xanadu Halkias, Joseph Razik category:cs.LG cs.CE stat.ML published:2013-06-13 summary:This paper presents a spermwhale' localization architecture using jointly abag-of-features (BoF) approach and machine learning framework. BoF methods areknown, especially in computer vision, to produce from a collection of localfeatures a global representation invariant to principal signal transformations.Our idea is to regress supervisely from these local features two roughestimates of the distance and azimuth thanks to some datasets where bothacoustic events and ground-truth position are now available. Furthermore, theseestimates can feed a particle filter system in order to obtain a precisespermwhale' position even in mono-hydrophone configuration. Anti-collisionsystem and whale watching are considered applications of this work.
arxiv-3300-185 | The Ripple Pond: Enabling Spiking Networks to See | http://arxiv.org/pdf/1306.3036v1.pdf | author:Saeed Afshar, Gregory Cohen, Runchun Wang, Andre van Schaik, Jonathan Tapson, Torsten Lehmann, Tara Julia Hamilton category:cs.NE q-bio.NC published:2013-06-13 summary:In this paper we present the biologically inspired Ripple Pond Network (RPN),a simply connected spiking neural network that, operating together withrecently proposed PolyChronous Networks (PCN), enables rapid, unsupervised,scale and rotation invariant object recognition using efficient spatio-temporalspike coding. The RPN has been developed as a hardware solution linkingpreviously implemented neuromorphic vision and memory structures capable ofdelivering end-to-end high-speed, low-power and low-resolution recognition formobile and autonomous applications where slow, highly sophisticated and powerhungry signal processing solutions are ineffective. Key aspects in the proposedapproach include utilising the spatial properties of physically embedded neuralnetworks and propagating waves of activity therein for information processing,using dimensional collapse of imagery information into amenable temporalpatterns and the use of asynchronous frames for information binding.
arxiv-3300-186 | A Face-like Structure Detection on Planet and Satellite Surfaces using Image Processing | http://arxiv.org/pdf/1306.3032v1.pdf | author:Kazutaka Kurihara, Masakazu Takasu, Kazuhiro Sasao, Hal Seki, Takayuki Narabu, Mitsuo Yamamoto, Satoshi Iida, Hiroyuki Yamamoto category:cs.CV published:2013-06-13 summary:This paper demonstrates that face-like structures are everywhere, and can bede-tected automatically even with computers. Huge amount of satellite images ofthe Earth, the Moon, the Mars are explored and many interesting face-likestructure are detected. Throughout this fact, we believe that science andtechnologies can alert people not to easily become an occultist.
arxiv-3300-187 | Second Order Swarm Intelligence | http://arxiv.org/pdf/1306.3018v1.pdf | author:Vitorino Ramos, David M. S. Rodrigues, Jorge Louçã category:cs.NE 68T05 published:2013-06-13 summary:An artificial Ant Colony System (ACS) algorithm to solve general-purposecombinatorial Optimization Problems (COP) that extends previous AC models [21]by the inclusion of a negative pheromone, is here described. Several TravellingSalesman Problem (TSP) were used as benchmark. We show that by using twodifferent sets of pheromones, a second-order co-evolved compromise betweenpositive and negative feedbacks achieves better results than single positivefeedback systems. The algorithm was tested against known NP-completecombinatorial Optimization Problems, running on symmetrical TSP's. We show thatthe new algorithm compares favourably against these benchmarks, accordingly torecent biological findings by Robinson [26,27], and Gruter [28] where "Noentry" signals and negative feedback allows a colony to quickly reallocate themajority of its foragers to superior food patches. This is the first time anextended ACS algorithm is implemented with these successful characteristics.
arxiv-3300-188 | Non-parametric Power-law Data Clustering | http://arxiv.org/pdf/1306.3003v1.pdf | author:Xuhui Fan, Yiling Zeng, Longbing Cao category:cs.LG cs.CV stat.ML published:2013-06-13 summary:It has always been a great challenge for clustering algorithms toautomatically determine the cluster numbers according to the distribution ofdatasets. Several approaches have been proposed to address this issue,including the recent promising work which incorporate Bayesian Nonparametricsinto the $k$-means clustering procedure. This approach shows simplicity inimplementation and solidity in theory, while it also provides a feasible way toinference in large scale datasets. However, several problems remains unsolvedin this pioneering work, including the power-law data applicability, mechanismto merge centers to avoid the over-fitting problem, clustering order problem,e.t.c.. To address these issues, the Pitman-Yor Process based k-means (namely\emph{pyp-means}) is proposed in this paper. Taking advantage of the Pitman-YorProcess, \emph{pyp-means} treats clusters differently by dynamically andadaptively changing the threshold to guarantee the generation of power-lawclustering results. Also, one center agglomeration procedure is integrated intothe implementation to be able to merge small but close clusters and thenadaptively determine the cluster number. With more discussion on the clusteringorder, the convergence proof, complexity analysis and extension to spectralclustering, our approach is compared with traditional clustering algorithm andvariational inference methods. The advantages and properties of pyp-means arevalidated by experiments on both synthetic datasets and real world datasets.
arxiv-3300-189 | A Greedy Approximation of Bayesian Reinforcement Learning with Probably Optimistic Transition Model | http://arxiv.org/pdf/1303.3163v3.pdf | author:Kenji Kawaguchi, Mauricio Araya category:cs.AI cs.LG stat.ML published:2013-03-13 summary:Bayesian Reinforcement Learning (RL) is capable of not only incorporatingdomain knowledge, but also solving the exploration-exploitation dilemma in anatural way. As Bayesian RL is intractable except for special cases, previouswork has proposed several approximation methods. However, these methods areusually too sensitive to parameter values, and finding an acceptable parametersetting is practically impossible in many applications. In this paper, wepropose a new algorithm that greedily approximates Bayesian RL to achieverobustness in parameter space. We show that for a desired learning behavior,our proposed algorithm has a polynomial sample complexity that is lower thanthose of existing algorithms. We also demonstrate that the proposed algorithmnaturally outperforms other existing algorithms when the prior distributionsare not significantly misleading. On the other hand, the proposed algorithmcannot handle greatly misspecified priors as well as the other algorithms can.This is a natural consequence of the fact that the proposed algorithm isgreedier than the other algorithms. Accordingly, we discuss a way to select anappropriate algorithm for different tasks based on the algorithms' greediness.We also introduce a new way of simplifying Bayesian planning, based on whichfuture work would be able to derive new algorithms.
arxiv-3300-190 | A Convergence Theorem for the Graph Shift-type Algorithms | http://arxiv.org/pdf/1306.3002v1.pdf | author:Xuhui Fan, Longbing Cao category:stat.ML cs.LG published:2013-06-13 summary:Graph Shift (GS) algorithms are recently focused as a promising approach fordiscovering dense subgraphs in noisy data. However, there are no theoreticalfoundations for proving the convergence of the GS Algorithm. In this paper, wepropose a generic theoretical framework consisting of three key GS components:simplex of generated sequence set, monotonic and continuous objective functionand closed mapping. We prove that GS algorithms with such components can betransformed to fit the Zangwill's convergence theorem, and the sequence setgenerated by the GS procedures always terminates at a local maximum, or atworst, contains a subsequence which converges to a local maximum of thesimilarity measure function. The framework is verified by expanding it to otherGS-type algorithms and experimental results.
arxiv-3300-191 | Dynamic Infinite Mixed-Membership Stochastic Blockmodel | http://arxiv.org/pdf/1306.2999v1.pdf | author:Xuhui Fan, Longbing Cao, Richard Yi Da Xu category:cs.SI cs.LG stat.ML published:2013-06-13 summary:Directional and pairwise measurements are often used to modelinter-relationships in a social network setting. The Mixed-MembershipStochastic Blockmodel (MMSB) was a seminal work in this area, and many of itscapabilities were extended since then. In this paper, we propose the\emph{Dynamic Infinite Mixed-Membership stochastic blockModel (DIM3)}, ageneralised framework that extends the existing work to a potentially infinitenumber of communities and mixture memberships for each of the network's nodes.This model is in a dynamic setting, where additional model parameters areintroduced to reflect the degree of persistence between one's memberships atconsecutive times. Accordingly, two effective posterior sampling strategies andtheir results are presented using both synthetic and real data.
arxiv-3300-192 | A Linear Approximation to the chi^2 Kernel with Geometric Convergence | http://arxiv.org/pdf/1206.4074v3.pdf | author:Fuxin Li, Guy Lebanon, Cristian Sminchisescu category:cs.LG cs.CV stat.ML published:2012-06-18 summary:We propose a new analytical approximation to the $\chi^2$ kernel thatconverges geometrically. The analytical approximation is derived withelementary methods and adapts to the input distribution for optimal convergencerate. Experiments show the new approximation leads to improved performance inimage classification and semantic segmentation tasks using a random Fourierfeature approximation of the $\exp-\chi^2$ kernel. Besides, out-of-coreprincipal component analysis (PCA) methods are introduced to reduce thedimensionality of the approximation and achieve better performance at theexpense of only an additional constant factor to the time complexity. Moreover,when PCA is performed jointly on the training and unlabeled testing data,further performance improvements can be obtained. Experiments conducted on thePASCAL VOC 2010 segmentation and the ImageNet ILSVRC 2010 datasets showstatistically significant improvements over alternative approximation methods.
arxiv-3300-193 | Reinforcement learning with restrictions on the action set | http://arxiv.org/pdf/1306.2918v1.pdf | author:Mario Bravo, Mathieu Faure category:cs.GT cs.LG math.PR published:2013-06-12 summary:Consider a 2-player normal-form game repeated over time. We introduce anadaptive learning procedure, where the players only observe their own realizedpayoff at each stage. We assume that agents do not know their own payofffunction, and have no information on the other player. Furthermore, we assumethat they have restrictions on their own action set such that, at each stage,their choice is limited to a subset of their action set. We prove that theempirical distributions of play converge to the set of Nash equilibria forzero-sum and potential games, and games where one player has two actions.
arxiv-3300-194 | Robust Support Vector Machines for Speaker Verification Task | http://arxiv.org/pdf/1306.2906v1.pdf | author:Kawthar Yasmine Zergat, Abderrahmane Amrouche category:cs.LG cs.SD stat.ML published:2013-06-12 summary:An important step in speaker verification is extracting features that bestcharacterize the speaker voice. This paper investigates a front-end processingthat aims at improving the performance of speaker verification based on theSVMs classifier, in text independent mode. This approach combines featuresbased on conventional Mel-cepstral Coefficients (MFCCs) and Line SpectralFrequencies (LSFs) to constitute robust multivariate feature vectors. To reducethe high dimensionality required for training these feature vectors, we use adimension reduction method called principal component analysis (PCA). In orderto evaluate the robustness of these systems, different noisy environments havebeen used. The obtained results using TIMIT database showed that, using theparadigm that combines these spectral cues leads to a significant improvementin verification accuracy, especially with PCA reduction for low signal-to-noiseratio noisy environment.
arxiv-3300-195 | Random Drift Particle Swarm Optimization | http://arxiv.org/pdf/1306.2863v1.pdf | author:Jun Sun, Xiaojun Wu, Vasile Palade, Wei Fang, Yuhui Shi category:cs.AI cs.NE math.OC 68T20 published:2013-06-12 summary:The random drift particle swarm optimization (RDPSO) algorithm, inspired bythe free electron model in metal conductors placed in an external electricfield, is presented, systematically analyzed and empirically studied in thispaper. The free electron model considers that electrons have both a thermal anda drift motion in a conductor that is placed in an external electric field. Themotivation of the RDPSO algorithm is described first, and the velocity equationof the particle is designed by simulating the thermal motion as well as thedrift motion of the electrons, both of which lead the electrons to a locationwith minimum potential energy in the external electric field. Then, acomprehensive analysis of the algorithm is made, in order to provide a deepinsight into how the RDPSO algorithm works. It involves a theoretical analysisand the simulation of the stochastic dynamical behavior of a single particle inthe RDPSO algorithm. The search behavior of the algorithm itself is alsoinvestigated in detail, by analyzing the interaction between the particles.Some variants of the RDPSO algorithm are proposed by incorporating differentrandom velocity components with different neighborhood topologies. Finally,empirical studies on the RDPSO algorithm are performed by using a set ofbenchmark functions from the CEC2005 benchmark suite. Based on the theoreticalanalysis of the particle's behavior, two methods of controlling the algorithmicparameters are employed, followed by an experimental analysis on how to selectthe parameter values, in order to obtain a good overall performance of theRDPSO algorithm and its variants in real-world applications. A furtherperformance comparison between the RDPSO algorithms and other variants of PSOis made to prove the efficiency of the RDPSO algorithms.
arxiv-3300-196 | The Quantum Challenge in Concept Theory and Natural Language Processing | http://arxiv.org/pdf/1306.2838v1.pdf | author:Diederik Aerts, Jan Broekaert, Sandro Sozzo, Tomas Veloz category:cs.CL cs.IR quant-ph published:2013-06-12 summary:The mathematical formalism of quantum theory has been successfully used inhuman cognition to model decision processes and to deliver representations ofhuman knowledge. As such, quantum cognition inspired tools have improvedtechnologies for Natural Language Processing and Information Retrieval. In thispaper, we overview the quantum cognition approach developed in our Brusselsteam during the last two decades, specifically our identification of quantumstructures in human concepts and language, and the modeling of data frompsychological and corpus-text-based experiments. We discuss ourquantum-theoretic framework for concepts and their conjunctions/disjunctions ina Fock-Hilbert space structure, adequately modeling a large amount of datacollected on concept combinations. Inspired by this modeling, we put forwardelements for a quantum contextual and meaning-based approach to informationtechnologies in which 'entities of meaning' are inversely reconstructed fromtexts, which are considered as traces of these entities' states.
arxiv-3300-197 | Recurrent Convolutional Neural Networks for Scene Parsing | http://arxiv.org/pdf/1306.2795v1.pdf | author:Pedro H. O. Pinheiro, Ronan Collobert category:cs.CV published:2013-06-12 summary:Scene parsing is a technique that consist on giving a label to all pixels inan image according to the class they belong to. To ensure a good visualcoherence and a high class accuracy, it is essential for a scene parser tocapture image long range dependencies. In a feed-forward architecture, this canbe simply achieved by considering a sufficiently large input context patch,around each pixel to be labeled. We propose an approach consisting of arecurrent convolutional neural network which allows us to consider a largeinput context, while limiting the capacity of the model. Contrary to moststandard approaches, our method does not rely on any segmentation methods, norany task-specific features. The system is trained in an end-to-end manner overraw pixels, and models complex spatial dependencies with low inference cost. Asthe context size increases with the built-in recurrence, the system identifiesand corrects its own errors. Our approach yields state-of-the-art performanceon both the Stanford Background Dataset and the SIFT Flow Dataset, whileremaining very fast at test time.
arxiv-3300-198 | Estimating Phoneme Class Conditional Probabilities from Raw Speech Signal using Convolutional Neural Networks | http://arxiv.org/pdf/1304.1018v2.pdf | author:Dimitri Palaz, Ronan Collobert, Mathew Magimai. -Doss category:cs.LG cs.CL cs.NE published:2013-04-03 summary:In hybrid hidden Markov model/artificial neural networks (HMM/ANN) automaticspeech recognition (ASR) system, the phoneme class conditional probabilitiesare estimated by first extracting acoustic features from the speech signalbased on prior knowledge such as, speech perception or/and speech productionknowledge, and, then modeling the acoustic features with an ANN. Recentadvances in machine learning techniques, more specifically in the field ofimage processing and text processing, have shown that such divide and conquerstrategy (i.e., separating feature extraction and modeling steps) may not benecessary. Motivated from these studies, in the framework of convolutionalneural networks (CNNs), this paper investigates a novel approach, where theinput to the ANN is raw speech signal and the output is phoneme classconditional probability estimates. On TIMIT phoneme recognition task, we studydifferent ANN architectures to show the benefit of CNNs and compare theproposed approach against conventional approach where, spectral-based featureMFCC is extracted and modeled by a multilayer perceptron. Our studies show thatthe proposed approach can yield comparable or better phoneme recognitionperformance when compared to the conventional approach. It indicates that CNNscan learn features relevant for phoneme classification automatically from theraw speech signal.
arxiv-3300-199 | Horizontal and Vertical Ensemble with Deep Representation for Classification | http://arxiv.org/pdf/1306.2759v1.pdf | author:Jingjing Xie, Bing Xu, Zhang Chuang category:cs.LG stat.ML published:2013-06-12 summary:Representation learning, especially which by using deep learning, has beenwidely applied in classification. However, how to use limited size of labeleddata to achieve good classification performance with deep neural network, andhow can the learned features further improve classification remain indefinite.In this paper, we propose Horizontal Voting Vertical Voting and HorizontalStacked Ensemble methods to improve the classification performance of deepneural networks. In the ICML 2013 Black Box Challenge, via using these methodsindependently, Bing Xu achieved 3rd in public leaderboard, and 7th in privateleaderboard; Jingjing Xie achieved 4th in public leaderboard, and 5th inprivate leaderboard.
arxiv-3300-200 | Sparse Representation-based Image Quality Assessment | http://arxiv.org/pdf/1306.2727v1.pdf | author:Tanaya Guha, Ehsan Nezhadarya, Rabab K Ward category:cs.CV cs.MM published:2013-06-12 summary:A successful approach to image quality assessment involves comparing thestructural information between a distorted and its reference image. However,extracting structural information that is perceptually important to our visualsystem is a challenging task. This paper addresses this issue by employing asparse representation-based approach and proposes a new metric called the\emph{sparse representation-based quality} (SPARQ) \emph{index}. The proposedmethod learns the inherent structures of the reference image as a set of basisvectors, such that any structure in the image can be represented by a linearcombination of only a few of those basis vectors. This sparse strategy isemployed because it is known to generate basis vectors that are qualitativelysimilar to the receptive field of the simple cells present in the mammalianprimary visual cortex. The visual quality of the distorted image is estimatedby comparing the structures of the reference and the distorted images in termsof the learnt basis vectors resembling cortical cells. Our approach isevaluated on six publicly available subject-rated image quality assessmentdatasets. The proposed SPARQ index consistently exhibits high correlation withthe subjective ratings on all datasets and performs better or at par with thestate-of-the-art.
arxiv-3300-201 | Distance Majorization and Its Applications | http://arxiv.org/pdf/1211.3907v5.pdf | author:Eric C. Chi, Hua Zhou, Kenneth Lange category:math.OC stat.CO stat.ML published:2012-11-16 summary:The problem of minimizing a continuously differentiable convex function overan intersection of closed convex sets is ubiquitous in applied mathematics. Itis particularly interesting when it is easy to project onto each separate set,but nontrivial to project onto their intersection. Algorithms based on Newton'smethod such as the interior point method are viable for small to medium-scaleproblems. However, modern applications in statistics, engineering, and machinelearning are posing problems with potentially tens of thousands of parametersor more. We revisit this convex programming problem and propose an algorithmthat scales well with dimensionality. Our proposal is an instance of asequential unconstrained minimization technique and revolves around threeideas: the majorization-minimization (MM) principle, the classical penaltymethod for constrained optimization, and quasi-Newton acceleration offixed-point algorithms. The performance of our distance majorization algorithmsis illustrated in several applications.
arxiv-3300-202 | Large Margin Low Rank Tensor Analysis | http://arxiv.org/pdf/1306.2663v1.pdf | author:Guoqiang Zhong, Mohamed Cheriet category:cs.LG cs.NA 57-04 published:2013-06-11 summary:Other than vector representations, the direct objects of human cognition aregenerally high-order tensors, such as 2D images and 3D textures. From thisfact, two interesting questions naturally arise: How does the human brainrepresent these tensor perceptions in a "manifold" way, and how can they berecognized on the "manifold"? In this paper, we present a supervised model tolearn the intrinsic structure of the tensors embedded in a high dimensionalEuclidean space. With the fixed point continuation procedures, our modelautomatically and jointly discovers the optimal dimensionality and therepresentations of the low dimensional embeddings. This makes it an effectivesimulation of the cognitive process of human brain. Furthermore, thegeneralization of our model based on similarity between the learned lowdimensional embeddings can be viewed as counterpart of recognition of humanbrain. Experiments on applications for object recognition and face recognitiondemonstrate the superiority of our proposed model over state-of-the-artapproaches.
arxiv-3300-203 | Stopping Criterion for the Mean Shift Iterative Algorithm | http://arxiv.org/pdf/1306.2624v1.pdf | author:Yasel Garcés Suárez, Esley Torres, Osvaldo Pereira, Claudia Pérez, Roberto Rogríguez category:cs.CV math.RA published:2013-06-11 summary:Image segmentation is a critical step in computer vision tasks constitutingan essential issue for pattern recognition and visual interpretation. In thispaper, we propose a new stopping criterion for the mean shift iterativealgorithm by using images defined in Zn ring, with the goal of reaching abetter segmentation. We carried out also a study on the weak and strong ofequivalence classes between two images. An analysis on the convergence withthis new stopping criterion is carried out too.
arxiv-3300-204 | Hand Gesture Recognition Based on Karhunen-Loeve Transform | http://arxiv.org/pdf/1306.2599v1.pdf | author:Joyeeta Singha, Karen Das category:cs.CV published:2013-06-11 summary:In this paper, we have proposed a system based on K-L Transform to recognizedifferent hand gestures. The system consists of five steps: skin filtering,palm cropping, edge detection, feature extraction, and classification. Firstlythe hand is detected using skin filtering and palm cropping was performed toextract out only the palm portion of the hand. The extracted image was thenprocessed using the Canny Edge Detection technique to extract the outlineimages of palm. After palm extraction, the features of hand were extractedusing K-L Transform technique and finally the input gesture was recognizedusing proper classifier. In our system, we have tested for 10 different handgestures, and recognizing rate obtained was 96%. Hence we propose an easyapproach to recognize different hand gestures.
arxiv-3300-205 | A 10-dimensional Phonetic-prosodic Space and its Stochastic Structure (A framework for probabilistic modeling of spoken languages and their phonology) | http://arxiv.org/pdf/1306.2593v1.pdf | author:Elaine Tsiang category:cs.SD cs.CL I.2.7 published:2013-06-11 summary:We formulate a phonetic-prosodic space based on attributes as perceptualobservables, rather than articulatory specifications. We propose an alphabet asmarkers in the phonetic subspace, aiming for a resolution sufficient to supportrecognition of all spoken languages. The prosodic subspace is made up ofdirectly measurable physical variables. With the proposed alphabet, traditionaldiphthongs naturally generalize to a broader class of language-neutralphonotactic constraints, indicating a correlation structure similar to that ofthe traditional sonority-based syllable. We define a stochastic structure onthe phone strings based on this diphthongal constraint, and show how a specificspoken language can be defined as a specific set of probability distributionsof this stochastic structure. Furthermore, phonological variations within aspoken language can be modeled as varying probability distributions restrictedto the phonetic subspace, conditioned on different values in the prosodicsubspace.
arxiv-3300-206 | A biological gradient descent for prediction through a combination of STDP and homeostatic plasticity | http://arxiv.org/pdf/1206.4812v2.pdf | author:Mathieu Galtier, Gilles Wainrib category:q-bio.NC cs.NE math.DS published:2012-06-21 summary:Identifying, formalizing and combining biological mechanisms which implementknown brain functions, such as prediction, is a main aspect of current researchin theoretical neuroscience. In this letter, the mechanisms of Spike TimingDependent Plasticity (STDP) and homeostatic plasticity, combined in an originalmathematical formalism, are shown to shape recurrent neural networks intopredictors. Following a rigorous mathematical treatment, we prove that theyimplement the online gradient descent of a distance between the networkactivity and its stimuli. The convergence to an equilibrium, where the networkcan spontaneously reproduce or predict its stimuli, does not suffer frombifurcation issues usually encountered in learning in recurrent neuralnetworks.
arxiv-3300-207 | The association problem in wireless networks: a Policy Gradient Reinforcement Learning approach | http://arxiv.org/pdf/1306.2554v1.pdf | author:Richard Combes, Ilham El Bouloumi, Stephane Senecal, Zwi Altman category:cs.NI cs.IT cs.LG math.IT published:2013-06-11 summary:The purpose of this paper is to develop a self-optimized associationalgorithm based on PGRL (Policy Gradient Reinforcement Learning), which is bothscalable, stable and robust. The term robust means that performance degradationin the learning phase should be forbidden or limited to predefined thresholds.The algorithm is model-free (as opposed to Value Iteration) and robust (asopposed to Q-Learning). The association problem is modeled as a Markov DecisionProcess (MDP). The policy space is parameterized. The parameterized family ofpolicies is then used as expert knowledge for the PGRL. The PGRL convergestowards a local optimum and the average cost decreases monotonically during thelearning process. The properties of the solution make it a good candidate forpractical implementation. Furthermore, the robustness property allows to usethe PGRL algorithm in an "always-on" learning mode.
arxiv-3300-208 | Hybrid Maximum Likelihood Modulation Classification Using Multiple Radios | http://arxiv.org/pdf/1303.0775v2.pdf | author:Onur Ozdemir, Ruoyu Li, Pramod K. Varshney category:cs.IT math.IT stat.ML published:2013-03-04 summary:The performance of a modulation classifier is highly sensitive to channelsignal-to-noise ratio (SNR). In this paper, we focus on amplitude-phasemodulations and propose a modulation classification framework based oncentralized data fusion using multiple radios and the hybrid maximum likelihood(ML) approach. In order to alleviate the computational complexity associatedwith ML estimation, we adopt the Expectation Maximization (EM) algorithm. Dueto SNR diversity, the proposed multi-radio framework provides robustness tochannel SNR. Numerical results show the superiority of the proposed approachwith respect to single radio approaches as well as to modulation classifiersusing moments based estimators.
arxiv-3300-209 | Markov random fields factorization with context-specific independences | http://arxiv.org/pdf/1306.2295v1.pdf | author:Alejandro Edera, Facundo Bromberg, Federico Schlüter category:cs.AI cs.LG published:2013-06-10 summary:Markov random fields provide a compact representation of joint probabilitydistributions by representing its independence properties in an undirectedgraph. The well-known Hammersley-Clifford theorem uses these conditionalindependences to factorize a Gibbs distribution into a set of factors. However,an important issue of using a graph to represent independences is that itcannot encode some types of independence relations, such as thecontext-specific independences (CSIs). They are a particular case ofconditional independences that is true only for a certain assignment of itsconditioning set; in contrast to conditional independences that must hold forall its assignments. This work presents a method for factorizing a Markovrandom field according to CSIs present in a distribution, and formallyguarantees that this factorization is correct. This is presented in our maincontribution, the context-specific Hammersley-Clifford theorem, ageneralization to CSIs of the Hammersley-Clifford theorem that applies forconditional independences.
arxiv-3300-210 | Asymptotically Optimal Sequential Estimation of the Mean Based on Inclusion Principle | http://arxiv.org/pdf/1306.2290v1.pdf | author:Xinjia Chen category:math.ST cs.LG math.PR stat.TH published:2013-06-10 summary:A large class of problems in sciences and engineering can be formulated asthe general problem of constructing random intervals with pre-specifiedcoverage probabilities for the mean. Wee propose a general approach forstatistical inference of mean values based on accumulated observational data.We show that the construction of such random intervals can be accomplished bycomparing the endpoints of random intervals with confidence sequences for themean. Asymptotic results are obtained for such sequential methods.
arxiv-3300-211 | A Kernel Test for Three-Variable Interactions | http://arxiv.org/pdf/1306.2281v1.pdf | author:Dino Sejdinovic, Arthur Gretton, Wicher Bergsma category:stat.ME stat.ML published:2013-06-10 summary:We introduce kernel nonparametric tests for Lancaster three-variableinteraction and for total independence, using embeddings of signed measuresinto a reproducing kernel Hilbert space. The resulting test statistics arestraightforward to compute, and are used in powerful interaction tests, whichare consistent against all alternatives for a large family of reproducingkernels. We show the Lancaster test to be sensitive to cases where twoindependent causes individually have weak influence on a third dependentvariable, but their combined effect has a strong influence. This makes theLancaster test especially suited to finding structure in directed graphicalmodels, where it outperforms competing nonparametric tests in detecting suchV-structures.
arxiv-3300-212 | Using the quaternion's representation of individuals in swarm intelligence and evolutionary computation | http://arxiv.org/pdf/1306.2257v1.pdf | author:Iztok Fister, Iztok Fister Jr category:cs.NE published:2013-06-10 summary:This paper introduces a novel idea for representation of individuals usingquaternions in swarm intelligence and evolutionary algorithms. Quaternions area number system, which extends complex numbers. They are successfully appliedto problems of theoretical physics and to those areas needing fast rotationcalculations. We propose the application of quaternions in optimization, moreprecisely, we have been using quaternions for representation of individuals inBat algorithm. The preliminary results of our experiments when optimizing atest-suite consisting of ten standard functions showed that this new algorithmsignificantly improved the results of the original Bat algorithm. Moreover, theobtained results are comparable with other swarm intelligence and evolutionaryalgorithms, like the artificial bees colony, and differential evolution. Webelieve that this representation could also be successfully applied to otherswarm intelligence and evolutionary algorithms.
arxiv-3300-213 | Detection of Outer Rotations on 3D-Vector Fields with Iterative Geometric Correlation and its Efficiency | http://arxiv.org/pdf/1307.2457v1.pdf | author:Roxana Bujack, Gerik Scheuermann, Eckhard Hitzer category:cs.CV cs.GR published:2013-06-10 summary:Correlation is a common technique for the detection of shifts. Itsgeneralization to the multidimensional geometric correlation in Cliffordalgebras has been proven a useful tool for color image processing, because itadditionally contains information about a rotational misalignment. But so farthe exact correction of a three-dimensional outer rotation could only beachieved in certain special cases. In this paper we prove that applying thegeometric correlation iteratively has the potential to detect the outerrotational misalignment for arbitrary three-dimensional vector fields. Wefurther present the explicit iterative algorithm, analyze its efficiencydetecting the rotational misalignment in the color space of a color image. Theexperiments suggest a method for the acceleration of the algorithm, which ispractically tested with great success.
arxiv-3300-214 | Combinatorial clustering and the beta negative binomial process | http://arxiv.org/pdf/1111.1802v5.pdf | author:Tamara Broderick, Lester Mackey, John Paisley, Michael I. Jordan category:stat.ME stat.ML published:2011-11-08 summary:We develop a Bayesian nonparametric approach to a general family of latentclass problems in which individuals can belong simultaneously to multipleclasses and where each class can be exhibited multiple times by an individual.We introduce a combinatorial stochastic process known as the negative binomialprocess (NBP) as an infinite-dimensional prior appropriate for such problems.We show that the NBP is conjugate to the beta process, and we characterize theposterior distribution under the beta-negative binomial process (BNBP) andhierarchical models based on the BNBP (the HBNBP). We study the asymptoticproperties of the BNBP and develop a three-parameter extension of the BNBP thatexhibits power-law behavior. We derive MCMC algorithms for posterior inferenceunder the HBNBP, and we present experiments using these algorithms in thedomains of image segmentation, object recognition, and document analysis.
arxiv-3300-215 | Adaptive Noisy Clustering | http://arxiv.org/pdf/1306.2194v1.pdf | author:Michael Chichignoud, Sébastien Loustau category:math.ST stat.ML stat.TH published:2013-06-10 summary:The problem of adaptive noisy clustering is investigated. Given a set ofnoisy observations $Z_i=X_i+\epsilon_i$, $i=1,...,n$, the goal is to designclusters associated with the law of $X_i$'s, with unknown density $f$ withrespect to the Lebesgue measure. Since we observe a corrupted sample, a directapproach as the popular {\it $k$-means} is not suitable in this case. In thispaper, we propose a noisy $k$-means minimization, which is based on the$k$-means loss function and a deconvolution estimator of the density $f$. Inparticular, this approach suffers from the dependence on a bandwidth involvedin the deconvolution kernel. Fast rates of convergence for the excess risk areproposed for a particular choice of the bandwidth, which depends on thesmoothness of the density $f$. Then, we turn out into the main issue of the paper: the data-driven choice ofthe bandwidth. We state an adaptive upper bound for a new selection rule,called ERC (Empirical Risk Comparison). This selection rule is based on theLepski's principle, where empirical risks associated with different bandwidthsare compared. Finally, we illustrate that this adaptive rule can be used inmany statistical problems of $M$-estimation where the empirical risk depends ona nuisance parameter.
arxiv-3300-216 | Image segmentation by optimal and hierarchical piecewise constant approximations | http://arxiv.org/pdf/1306.2159v1.pdf | author:M. Kharinov category:cs.CV published:2013-06-10 summary:Piecewise constant image approximations of sequential number of segments orclusters of disconnected pixels are treated. The method of majorizing ofoptimal approximation sequence by hierarchical sequence of image approximationsis proposed. A generalization for multidimensional case of color andmultispectral images is foreseen.
arxiv-3300-217 | "Not not bad" is not "bad": A distributional account of negation | http://arxiv.org/pdf/1306.2158v1.pdf | author:Karl Moritz Hermann, Edward Grefenstette, Phil Blunsom category:cs.CL 68T50 I.2.7 published:2013-06-10 summary:With the increasing empirical success of distributional models ofcompositional semantics, it is timely to consider the types of textual logicthat such models are capable of capturing. In this paper, we addressshortcomings in the ability of current models to capture logical operationssuch as negation. As a solution we propose a tripartite formulation for acontinuous vector space representation of semantics and subsequently use thisrepresentation to develop a formal compositional notion of negation within suchmodels.
arxiv-3300-218 | Dictionary Subselection Using an Overcomplete Joint Sparsity Model | http://arxiv.org/pdf/1212.2834v2.pdf | author:Mehrdad Yaghoobi, Laurent Daudet, Michael E. Davies category:cs.LG math.OC stat.ML published:2012-12-12 summary:Many natural signals exhibit a sparse representation, whenever a suitabledescribing model is given. Here, a linear generative model is considered, wheremany sparsity-based signal processing techniques rely on such a simplifiedmodel. As this model is often unknown for many classes of the signals, we needto select such a model based on the domain knowledge or using some exemplarsignals. This paper presents a new exemplar based approach for the linear model(called the dictionary) selection, for such sparse inverse problems. Theproblem of dictionary selection, which has also been called the dictionarylearning in this setting, is first reformulated as a joint sparsity model. Thejoint sparsity model here differs from the standard joint sparsity model as itconsiders an overcompleteness in the representation of each signal, within therange of selected subspaces. The new dictionary selection paradigm is examinedwith some synthetic and realistic simulations.
arxiv-3300-219 | Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n) | http://arxiv.org/pdf/1306.2119v1.pdf | author:Francis Bach, Eric Moulines category:cs.LG math.OC stat.ML published:2013-06-10 summary:We consider the stochastic approximation problem where a convex function hasto be minimized, given only the knowledge of unbiased estimates of itsgradients at certain points, a framework which includes machine learningmethods based on the minimization of the empirical risk. We focus on problemswithout strong convexity, for which all previously known algorithms achieve aconvergence rate for function values of O(1/n^{1/2}). We consider and analyzetwo algorithms that achieve a rate of O(1/n) for classical supervised learningproblems. For least-squares regression, we show that averaged stochasticgradient descent with constant step-size achieves the desired rate. Forlogistic regression, this is achieved by a simple novel stochastic gradientalgorithm that (a) constructs successive local quadratic approximations of theloss functions, while (b) preserving the same running time complexity asstochastic gradient descent. For these algorithms, we provide a non-asymptoticanalysis of the generalization error (in expectation, and also in highprobability for least-squares), and run extensive experiments on standardmachine learning benchmarks showing that they often outperform existingapproaches.
arxiv-3300-220 | A Novel Approach for Single Gene Selection Using Clustering and Dimensionality Reduction | http://arxiv.org/pdf/1306.2118v1.pdf | author:E. N. Sathishkumar, K. Thangavel, T. Chandrasekhar category:cs.CE cs.LG published:2013-06-10 summary:We extend the standard rough set-based approach to deal with huge amounts ofnumeric attributes versus small amount of available objects. Here, a novelapproach of clustering along with dimensionality reduction; Hybrid Fuzzy CMeans-Quick Reduct (FCMQR) algorithm is proposed for single gene selection.Gene selection is a process to select genes which are more informative. It isone of the important steps in knowledge discovery. The problem is that allgenes are not important in gene expression data. Some of the genes may beredundant, and others may be irrelevant and noisy. In this study, the entiredataset is divided in proper grouping of similar genes by applying Fuzzy CMeans (FCM) algorithm. A high class discriminated genes has been selected basedon their degree of dependence by applying Quick Reduct algorithm based on RoughSet Theory to all the resultant clusters. Average Correlation Value (ACV) iscalculated for the high class discriminated genes. The clusters which have theACV value a s 1 is determined as significant clusters, whose classificationaccuracy will be equal or high when comparing to the accuracy of the entiredataset. The proposed algorithm is evaluated using WEKA classifiers andcompared. Finally, experimental results related to the leukemia cancer dataconfirm that our approach is quite promising, though it surely requires furtherresearch.
arxiv-3300-221 | Discriminative k-means clustering | http://arxiv.org/pdf/1306.2102v1.pdf | author:Ognjen Arandjelovic category:cs.CV published:2013-06-10 summary:The k-means algorithm is a partitional clustering method. Over 60 years old,it has been successfully used for a variety of problems. The popularity ofk-means is in large part a consequence of its simplicity and efficiency. Inthis paper we are inspired by these appealing properties of k-means in thedevelopment of a clustering algorithm which accepts the notion of "positively"and "negatively" labelled data. The goal is to discover the cluster structureof both positive and negative data in a manner which allows for thediscrimination between the two sets. The usefulness of this idea isdemonstrated practically on the problem of face recognition, where the task oflearning the scope of a person's appearance should be done in a manner whichallows this face to be differentiated from others.
arxiv-3300-222 | Discriminative extended canonical correlation analysis for pattern set matching | http://arxiv.org/pdf/1306.2100v1.pdf | author:Ognjen Arandjelovic category:cs.CV published:2013-06-10 summary:In this paper we address the problem of matching sets of vectors embedded inthe same input space. We propose an approach which is motivated by canonicalcorrelation analysis (CCA), a statistical technique which has proven successfulin a wide variety of pattern recognition problems. Like CCA when applied to thematching of sets, our extended canonical correlation analysis (E-CCA) aims toextract the most similar modes of variability within two sets. Our first majorcontribution is the formulation of a principled framework for robust inferenceof such modes from data in the presence of uncertainty associated with noiseand sampling randomness. E-CCA retains the efficiency and closed formcomputability of CCA, but unlike it, does not possess free parameters whichcannot be inferred directly from data (inherent data dimensionality, and thenumber of canonical correlations used for set similarity computation). Oursecond major contribution is to show that in contrast to CCA, E-CCA is readilyadapted to match sets in a discriminative learning scheme which we calldiscriminative extended canonical correlation analysis (DE-CCA). Theoreticalcontributions of this paper are followed by an empirical evaluation of itspremises on the task of face recognition from sets of rasterized appearanceimages. The results demonstrate that our approach, E-CCA, already outperformsboth CCA and its quasi-discriminative counterpart constrained CCA (C-CCA), forall values of their free parameters. An even greater improvement is achievedwith the discriminative variant, DE-CCA.
arxiv-3300-223 | Predicting Risk-of-Readmission for Congestive Heart Failure Patients: A Multi-Layer Approach | http://arxiv.org/pdf/1306.2094v1.pdf | author:Kiyana Zolfaghar, Nele Verbiest, Jayshree Agarwal, Naren Meadem, Si-Chi Chin, Senjuti Basu Roy, Ankur Teredesai, David Hazel, Paul Amoroso, Lester Reed category:cs.LG stat.AP published:2013-06-10 summary:Mitigating risk-of-readmission of Congestive Heart Failure (CHF) patientswithin 30 days of discharge is important because such readmissions are not onlyexpensive but also critical indicator of provider care and quality oftreatment. Accurately predicting the risk-of-readmission may allow hospitals toidentify high-risk patients and eventually improve quality of care byidentifying factors that contribute to such readmissions in many scenarios. Inthis paper, we investigate the problem of predicting risk-of-readmission as asupervised learning problem, using a multi-layer classification approach.Earlier contributions inadequately attempted to assess a risk value for 30 dayreadmission by building a direct predictive model as opposed to our approach.We first split the problem into various stages, (a) at risk in general (b) riskwithin 60 days (c) risk within 30 days, and then build suitable classifiers foreach stage, thereby increasing the ability to accurately predict the risk usingmultiple layers of decision. The advantage of our approach is that we can usedifferent classification models for the subtasks that are more suited for therespective problems. Moreover, each of the subtasks can be solved usingdifferent features and training data leading to a highly confident diagnosis orrisk compared to a one-shot single layer approach. An experimental evaluationon actual hospital patient record data from Multicare Health Systems shows thatour model is significantly better at predicting risk-of-readmission of CHFpatients within 30 days after discharge compared to prior attempts.
arxiv-3300-224 | Logistic Tensor Factorization for Multi-Relational Data | http://arxiv.org/pdf/1306.2084v1.pdf | author:Maximilian Nickel, Volker Tresp category:stat.ML cs.LG published:2013-06-10 summary:Tensor factorizations have become increasingly popular approaches for variouslearning tasks on structured data. In this work, we extend the RESCAL tensorfactorization, which has shown state-of-the-art results for multi-relationallearning, to account for the binary nature of adjacency tensors. We study theimprovements that can be gained via this approach on various benchmark datasetsand show that the logistic extension can improve the prediction resultssignificantly.
arxiv-3300-225 | 3D model retrieval using global and local radial distances | http://arxiv.org/pdf/1306.2081v1.pdf | author:Bo Li, Henry Johan category:cs.GR cs.CV cs.IR published:2013-06-10 summary:3D model retrieval techniques can be classified as histogram-based,view-based and graph-based approaches. We propose a hybrid shape descriptorwhich combines the global and local radial distance features by utilizing thehistogram-based and view-based approaches respectively. We define anarea-weighted global radial distance with respect to the center of the boundingsphere of the model and encode its distribution into a 2D histogram as theglobal radial distance shape descriptor. We then uniformly divide the boundingcube of a 3D model into a set of small cubes and define their centers as localcenters. Then, we compute the local radial distance of a point based on thenearest local center. By sparsely sampling a set of views and encoding thelocal radial distance feature on the rendered views by color coding, we extractthe local radial distance shape descriptor. Based on these two shapedescriptors, we develop a hybrid radial distance shape descriptor for 3D modelretrieval. Experiment results show that our hybrid shape descriptor outperformsseveral typical histogram-based and view-based approaches.
arxiv-3300-226 | Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation | http://arxiv.org/pdf/1306.2035v1.pdf | author:Martin Azizyan, Aarti Singh, Larry Wasserman category:stat.ML cs.LG math.ST stat.TH published:2013-06-09 summary:While several papers have investigated computationally and statisticallyefficient methods for learning Gaussian mixtures, precise minimax bounds fortheir statistical performance as well as fundamental limits in high-dimensionalsettings are not well-understood. In this paper, we provide precise informationtheoretic bounds on the clustering accuracy and sample complexity of learning amixture of two isotropic Gaussians in high dimensions under small meanseparation. If there is a sparse subset of relevant dimensions that determinethe mean separation, then the sample complexity only depends on the number ofrelevant dimensions and mean separation, and can be achieved by a simplecomputationally efficient procedure. Our results provide the first step of atheoretical basis for recent methods that combine feature selection andclustering.
arxiv-3300-227 | Comparing Edge Detection Methods based on Stochastic Entropies and Distances for PolSAR Imagery | http://arxiv.org/pdf/1306.2003v1.pdf | author:Abraão D. C. Nascimento, Michelle M. Horta, Alejandro C. Frery, Renato J. Cintra category:math.ST cs.CV stat.TH published:2013-06-09 summary:Polarimetric synthetic aperture radar (PolSAR) has achieved a prominentposition as a remote imaging method. However, PolSAR images are contaminated byspeckle noise due to the coherent illumination employed during the dataacquisition. This noise provides a granular aspect to the image, making itsprocessing and analysis (such as in edge detection) hard tasks. This paperdiscusses seven methods for edge detection in multilook PolSAR images. In allmethods, the basic idea consists in detecting transition points in the finestpossible strip of data which spans two regions. The edge is contoured using thetransitions points and a B-spline curve. Four stochastic distances, twodifferences of entropies, and the maximum likelihood criterion were used underthe scaled complex Wishart distribution; the first six stem from the h-phiclass of measures. The performance of the discussed detection methods wasquantified and analyzed by the computational time and probability of correctedge detection, with respect to the number of looks, the backscatter matrix asa whole, the SPAN, the covariance an the spatial resolution. The detectionprocedures were applied to three real PolSAR images. Results provide evidencethat the methods based on the Bhattacharyya distance and the difference ofShannon entropies outperform the other techniques.
arxiv-3300-228 | Blind Signal Separation in the Presence of Gaussian Noise | http://arxiv.org/pdf/1211.1716v2.pdf | author:Mikhail Belkin, Luis Rademacher, James Voss category:cs.LG cs.DS stat.ML published:2012-11-07 summary:A prototypical blind signal separation problem is the so-called cocktailparty problem, with n people talking simultaneously and n different microphoneswithin a room. The goal is to recover each speech signal from the microphoneinputs. Mathematically this can be modeled by assuming that we are givensamples from an n-dimensional random variable X=AS, where S is a vector whosecoordinates are independent random variables corresponding to each speaker. Theobjective is to recover the matrix A^{-1} given random samples from X. A rangeof techniques collectively known as Independent Component Analysis (ICA) havebeen proposed to address this problem in the signal processing and machinelearning literature. Many of these techniques are based on using the kurtosisor other cumulants to recover the components. In this paper we propose a new algorithm for solving the blind signalseparation problem in the presence of additive Gaussian noise, when we aregiven samples from X=AS+\eta, where \eta is drawn from an unknown, notnecessarily spherical n-dimensional Gaussian distribution. Our approach isbased on a method for decorrelating a sample with additive Gaussian noise underthe assumption that the underlying distribution is a linear transformation of adistribution with independent components. Our decorrelation routine is based onthe properties of cumulant tensors and can be combined with any standardcumulant-based method for ICA to get an algorithm that is provably robust inthe presence of Gaussian noise. We derive polynomial bounds for the samplecomplexity and error propagation of our method.
arxiv-3300-229 | Learning About Meetings | http://arxiv.org/pdf/1306.1927v1.pdf | author:Been Kim, Cynthia Rudin category:stat.AP cs.CL published:2013-06-08 summary:Most people participate in meetings almost every day, multiple times a day.The study of meetings is important, but also challenging, as it requires anunderstanding of social signals and complex interpersonal dynamics. Our aimthis work is to use a data-driven approach to the science of meetings. Weprovide tentative evidence that: i) it is possible to automatically detect whenduring the meeting a key decision is taking place, from analyzing only thelocal dialogue acts, ii) there are common patterns in the way social dialogueacts are interspersed throughout a meeting, iii) at the time key decisions aremade, the amount of time left in the meeting can be predicted from the amountof time that has passed, iv) it is often possible to predict whether a proposalduring a meeting will be accepted or rejected based entirely on the language(the set of persuasive words) used by the speaker.
arxiv-3300-230 | Efficient Regularized Least-Squares Algorithms for Conditional Ranking on Relational Data | http://arxiv.org/pdf/1209.4825v2.pdf | author:Tapio Pahikkala, Antti Airola, Michiel Stock, Bernard De Baets, Willem Waegeman category:cs.LG stat.ML published:2012-09-21 summary:In domains like bioinformatics, information retrieval and social networkanalysis, one can find learning tasks where the goal consists of inferring aranking of objects, conditioned on a particular target object. We present ageneral kernel framework for learning conditional rankings from various typesof relational data, where rankings can be conditioned on unseen data objects.We propose efficient algorithms for conditional ranking by optimizing squaredregression and ranking loss functions. We show theoretically, that learningwith the ranking loss is likely to generalize better than with the regressionloss. Further, we prove that symmetry or reciprocity properties of relationscan be efficiently enforced in the learned models. Experiments on synthetic andreal-world data illustrate that the proposed methods deliver state-of-the-artperformance in terms of predictive power and computational efficiency.Moreover, we also show empirically that incorporating symmetry or reciprocityproperties can improve the generalization performance.
arxiv-3300-231 | Emotional Expression Classification using Time-Series Kernels | http://arxiv.org/pdf/1306.1913v1.pdf | author:Andras Lorincz, Laszlo Jeni, Zoltan Szabo, Jeffrey Cohn, Takeo Kanade category:cs.CV cs.LG stat.ML published:2013-06-08 summary:Estimation of facial expressions, as spatio-temporal processes, can takeadvantage of kernel methods if one considers facial landmark positions andtheir motion in 3D space. We applied support vector classification with kernelsderived from dynamic time-warping similarity measures. We achieved over 99%accuracy - measured by area under ROC curve - using only the 'motion pattern'of the PCA compressed representation of the marker point vector, the so-calledshape parameters. Beyond the classification of full motion patterns, severalexpressions were recognized with over 90% accuracy in as few as 5-6 frames fromtheir onset, about 200 milliseconds.
arxiv-3300-232 | Speckle Reduction with Adaptive Stack Filters | http://arxiv.org/pdf/1306.1894v1.pdf | author:María Elena Buemi, Alejandro C. Frery, Heitor S. Ramos category:cs.CV published:2013-06-08 summary:Stack filters are a special case of non-linear filters. They have a goodperformance for filtering images with different types of noise while preservingedges and details. A stack filter decomposes an input image into stacks ofbinary images according to a set of thresholds. Each binary image is thenfiltered by a Boolean function, which characterizes the filter. Adaptive stackfilters can be computed by training using a prototype (ideal) image and itscorrupted version, leading to optimized filters with respect to a lossfunction. In this work we propose the use of training with selected samples forthe estimation of the optimal Boolean function. We study the performance ofadaptive stack filters when they are applied to speckled imagery, in particularto Synthetic Aperture Radar (SAR) images. This is done by evaluating thequality of the filtered images through the use of suitable image qualityindexes and by measuring the classification accuracy of the resulting images.We used SAR images as input, since they are affected by speckle noise thatmakes classification a difficult task.
arxiv-3300-233 | A Factor Graph Approach to Joint OFDM Channel Estimation and Decoding in Impulsive Noise Environments | http://arxiv.org/pdf/1306.1851v1.pdf | author:Marcel Nassar, Philip Schniter, Brian L. Evans category:cs.IT math.IT stat.ML published:2013-06-07 summary:We propose a novel receiver for orthogonal frequency division multiplexing(OFDM) transmissions in impulsive noise environments. Impulsive noise arises inmany modern wireless and wireline communication systems, such as Wi-Fi andpowerline communications, due to uncoordinated interference that is muchstronger than thermal noise. We first show that the bit-error-rate optimalreceiver jointly estimates the propagation channel coefficients, the noiseimpulses, the finite-alphabet symbols, and the unknown bits. We then propose anear-optimal yet computationally tractable approach to this joint estimationproblem using loopy belief propagation. In particular, we merge the recentlyproposed "generalized approximate message passing" (GAMP) algorithm with theforward-backward algorithm and soft-input soft-output decoding using a "turbo"approach. Numerical results indicate that the proposed receiver drasticallyoutperforms existing receivers under impulsive noise and comes within 1 dB ofthe matched-filter bound. Meanwhile, with N tones, the proposedfactor-graph-based receiver has only O(N log N) complexity, and it can beparallelized.
arxiv-3300-234 | Orbital-free Bond Breaking via Machine Learning | http://arxiv.org/pdf/1306.1812v1.pdf | author:John C. Snyder, Matthias Rupp, Katja Hansen, Leo Blooston, Klaus-Robert Müller, Kieron Burke category:stat.ML published:2013-06-07 summary:Machine learning is used to approximate the kinetic energy of one dimensionaldiatomics as a functional of the electron density. The functional canaccurately dissociate a diatomic, and can be systematically improved withtraining. Highly accurate self-consistent densities and molecular forces arefound, indicating the possibility for ab-initio molecular dynamics simulations.
arxiv-3300-235 | The DeLiVerMATH project - Text analysis in mathematics | http://arxiv.org/pdf/1306.6944v1.pdf | author:Ulf Schöneberg, Wolfram Sperber category:cs.CL cs.DL cs.IR published:2013-06-07 summary:A high-quality content analysis is essential for retrieval functionalitiesbut the manual extraction of key phrases and classification is expensive.Natural language processing provides a framework to automatize the process.Here, a machine-based approach for the content analysis of mathematical textsis described. A prototype for key phrase extraction and classification ofmathematical texts is presented.
arxiv-3300-236 | Fast greedy algorithm for subspace clustering from corrupted and incomplete data | http://arxiv.org/pdf/1306.1716v1.pdf | author:Alexander Petukhov, Inna Kozlov category:cs.LG cs.DS math.NA stat.ML published:2013-06-07 summary:We describe the Fast Greedy Sparse Subspace Clustering (FGSSC) algorithmproviding an efficient method for clustering data belonging to a fewlow-dimensional linear or affine subspaces. The main difference of ouralgorithm from predecessors is its ability to work with noisy data having ahigh rate of erasures (missed entries with the known coordinates) and errors(corrupted entries with unknown coordinates). We discuss here how to implementthe fast version of the greedy algorithm with the maximum efficiency whosegreedy strategy is incorporated into iterations of the basic algorithm. We provide numerical evidences that, in the subspace clustering capability,the fast greedy algorithm outperforms not only the existing state-of-the artSSC algorithm taken by the authors as a basic algorithm but also the recentGSSC algorithm. At the same time, its computational cost is only slightlyhigher than the cost of SSC. The numerical evidence of the algorithm significant advantage is presentedfor a few synthetic models as well as for the Extended Yale B dataset of facialimages. In particular, the face recognition misclassification rate turned outto be 6-20 times lower than for the SSC algorithm. We provide also thenumerical evidence that the FGSSC algorithm is able to perform clustering ofcorrupted data efficiently even when the sum of subspace dimensionssignificantly exceeds the dimension of the ambient space.
arxiv-3300-237 | Clifford Fourier-Mellin transform with two real square roots of -1 in Cl(p,q), p+q=2 | http://arxiv.org/pdf/1306.1679v1.pdf | author:Eckhard Hitzer category:math.RA cs.CV published:2013-06-07 summary:We describe a non-commutative generalization of the complex Fourier-Mellintransform to Clifford algebra valued signal functions over the domain$\R^{p,q}$ taking values in Cl(p,q), p+q=2. Keywords: algebra, Fourier transforms; Logic, set theory, and algebra,Fourier analysis, Integral transforms
arxiv-3300-238 | Algebraic foundations of split hypercomplex nonlinear adaptive filtering | http://arxiv.org/pdf/1306.1676v1.pdf | author:Eckhard Hitzer category:cs.CV math.RA 60G35, 15A66 published:2013-06-07 summary:A split hypercomplex learning algorithm for the training of nonlinear finiteimpulse response adaptive filters for the processing of hypercomplex signals ofany dimension is proposed. The derivation strictly takes into account the lawsof hypercomplex algebra and hypercomplex calculus, some of which have beenneglected in existing learning approaches (e.g. for quaternions). Already inthe case of quaternions we can predict improvements in performance ofhypercomplex processes. The convergence of the proposed algorithms isrigorously analyzed. Keywords: Quaternionic adaptive filtering, Hypercomplex adaptive filtering,Nonlinear adaptive filtering, Hypercomplex Multilayer Perceptron, Cliffordgeometric algebra
arxiv-3300-239 | Reducing statistical time-series problems to binary classification | http://arxiv.org/pdf/1210.6001v3.pdf | author:Daniil Ryabko, Jérémie Mary category:cs.LG stat.ML published:2012-10-22 summary:We show how binary classification methods developed to work on i.i.d. datacan be used for solving statistical problems that are seemingly unrelated toclassification and concern highly-dependent time series. Specifically, theproblems of time-series clustering, homogeneity testing and the three-sampleproblem are addressed. The algorithms that we construct for solving theseproblems are based on a new metric between time-series distributions, which canbe evaluated using binary classification methods. Universal consistency of theproposed algorithms is proven under most general assumptions. The theoreticalresults are illustrated with experiments on synthetic and real-world data.
arxiv-3300-240 | Quaternionic Fourier-Mellin Transform | http://arxiv.org/pdf/1306.1669v1.pdf | author:Eckhard Hitzer category:math.RA cs.CV published:2013-06-07 summary:In this contribution we generalize the classical Fourier Mellin transform [S.Dorrode and F. Ghorbel, Robust and efficient Fourier-Mellin transformapproximations for gray-level image reconstruction and complete invariantdescription, Computer Vision and Image Understanding, 83(1) (2001), 57-78, DOI10.1006/cviu.2001.0922.], which transforms functions $f$ representing, e.g., agray level image defined over a compact set of $\mathbb{R}^2$. The quaternionicFourier Mellin transform (QFMT) applies to functions $f: \mathbb{R}^2\rightarrow \mathbb{H}$, for which $f$ is summable over $\mathbb{R}_+^*\times \mathbb{S}^1$ under the measure $d\theta \frac{dr}{r}$. $\mathbb{R}_+^*$is the multiplicative group of positive and non-zero real numbers. Weinvestigate the properties of the QFMT similar to the investigation of thequaternionic Fourier Transform (QFT) in [E. Hitzer, Quaternion FourierTransform on Quaternion Fields and Generalizations, Advances in AppliedClifford Algebras, 17(3) (2007), 497-517.; E. Hitzer, Directional UncertaintyPrinciple for Quaternion Fourier Transforms, Advances in Applied CliffordAlgebras, 20(2) (2010), 271-284, online since 08 July 2009.].
arxiv-3300-241 | Non-constant bounded holomorphic functions of hyperbolic numbers - Candidates for hyperbolic activation functions | http://arxiv.org/pdf/1306.1653v1.pdf | author:Eckhard Hitzer category:cs.NE cs.CV math.RA published:2013-06-07 summary:The Liouville theorem states that bounded holomorphic complex functions arenecessarily constant. Holomorphic functions fulfill the socalled Cauchy-Riemann(CR) conditions. The CR conditions mean that a complex $z$-derivative isindependent of the direction. Holomorphic functions are ideal for activationfunctions of complex neural networks, but the Liouville theorem makes themuseless. Yet recently the use of hyperbolic numbers, lead to the constructionof hyperbolic number neural networks. We will describe the Cauchy-Riemannconditions for hyperbolic numbers and show that there exists a new interestingtype of bounded holomorphic functions of hyperbolic numbers, which are notconstant. We give examples of such functions. They therefore substantiallyexpand the available candidates for holomorphic activation functions forhyperbolic number neural networks. Keywords: Hyperbolic numbers, Liouville theorem, Cauchy-Riemann conditions,bounded holomorphic functions
arxiv-3300-242 | OPS-QFTs: A new type of quaternion Fourier transforms based on the orthogonal planes split with one or two general pure quaternions | http://arxiv.org/pdf/1306.1650v1.pdf | author:Eckhard Hitzer category:math.RA cs.CV 15A66, 42A38 published:2013-06-07 summary:We explain the orthogonal planes split (OPS) of quaternions based on thearbitrary choice of one or two linearly independent pure unit quaternions$f,g$. Next we systematically generalize the quaternionic Fourier transform(QFT) applied to quaternion fields to conform with the OPS determined by $f,g$,or by only one pure unit quaternion $f$, comment on their geometric meaning,and establish inverse transformations. Keywords: Clifford geometric algebra, quaternion geometry, quaternion Fouriertransform, inverse Fourier transform, orthogonal planes split
arxiv-3300-243 | Statistical Denoising for single molecule fluorescence microscopic images | http://arxiv.org/pdf/1306.1619v1.pdf | author:Ji Won Yoon category:cs.CV published:2013-06-07 summary:Single molecule fluorescence microscopy is a powerful technique foruncovering detailed information about biological systems, both in vitro and invivo. In such experiments, the inherently low signal to noise ratios mean thataccurate algorithms to separate true signal and background noise are essentialto generate meaningful results. To this end, we have developed a new and robustmethod to reduce noise in single molecule fluorescence images by using aGaussian Markov Random Field (GMRF) prior in a Bayesian framework. Twodifferent strategies are proposed to build the prior - an intrinsic GMRF, witha stationary relationship between pixels and a heterogeneous intrinsic GMRF,with a differently weighted relationship between pixels classified as moleculesand background. Testing with synthetic and real experimental fluorescenceimages demonstrates that the heterogeneous intrinsic GMRF is superior to otherconventional de-noising approaches.
arxiv-3300-244 | Vesselness features and the inverse compositional AAM for robust face recognition using thermal IR | http://arxiv.org/pdf/1306.1609v1.pdf | author:Reza Shoja Ghiass, Ognjen Arandjelovic, Hakim Bendada, Xavier Maldague category:cs.CV published:2013-06-07 summary:Over the course of the last decade, infrared (IR) and particularly thermal IRimaging based face recognition has emerged as a promising complement toconventional, visible spectrum based approaches which continue to struggle whenapplied in the real world. While inherently insensitive to visible spectrumillumination changes, IR images introduce specific challenges of their own,most notably sensitivity to factors which affect facial heat emission patterns,e.g. emotional state, ambient temperature, and alcohol intake. In addition,facial expression and pose changes are more difficult to correct in IR imagesbecause they are less rich in high frequency detail which is an important cuefor fitting any deformable model. We describe a novel method which addressesthese challenges. To normalize for pose and facial expression changes wegenerate a synthetic frontal image of a face in a canonical, neutral facialexpression from an image of the face in an arbitrary pose and facialexpression. This is achieved by piecewise affine warping which follows activeappearance model (AAM) fitting. This is the first publication which exploresthe use of an AAM on thermal IR images; we propose a pre-processing step whichenhances detail in thermal images, making AAM convergence faster and moreaccurate. To overcome the problem of thermal IR image sensitivity to thepattern of facial temperature emissions we describe a representation based onreliable anatomical features. In contrast to previous approaches, ourrepresentation is not binary; rather, our method accounts for the reliabilityof the extracted features. This makes the proposed representation much morerobust both to pose and scale changes. The effectiveness of the proposedapproach is demonstrated on the largest public database of thermal IR images offaces on which it achieved 100% identification, significantly outperformingprevious methods.
arxiv-3300-245 | Accomplishable Tasks in Knowledge Representation | http://arxiv.org/pdf/1306.2268v1.pdf | author:Keehang Kwon, Mi-Young Park category:cs.AI cs.CL published:2013-06-07 summary:Knowledge Representation (KR) is traditionally based on the logic of facts,expressed in boolean logic. However, facts about an agent can also be seen as aset of accomplished tasks by the agent. This paper proposes a new approach toKR: the notion of task logical KR based on Computability Logic. This notionallows the user to represent both accomplished tasks and accomplishable tasksby the agent. This notion allows us to build sophisticated KRs about manyinteresting agents, which have not been supported by previous logicallanguages.
arxiv-3300-246 | Illumination-invariant face recognition from a single image across extreme pose using a dual dimension AAM ensemble in the thermal infrared spectrum | http://arxiv.org/pdf/1306.1822v1.pdf | author:Reza Shoja Ghiass, Ognjen Arandjelovic, Hakim Bendada, Xavier Maldague category:cs.CV published:2013-06-07 summary:Over the course of the last decade, infrared (IR) and particularly thermal IRimaging based face recognition has emerged as a promising complement toconventional, visible spectrum based approaches which continue to struggle whenapplied in practice. While inherently insensitive to visible spectrumillumination changes, IR data introduces specific challenges of its own, mostnotably sensitivity to factors which affect facial heat emission patterns, e.g.emotional state, ambient temperature, and alcohol intake. In addition, facialexpression and pose changes are more difficult to correct in IR images becausethey are less rich in high frequency detail which is an important cue forfitting any deformable model. In this paper we describe a novel method whichaddresses these major challenges. Specifically, when comparing two thermal IRimages of faces, we mutually normalize their poses and facial expressions byusing an active appearance model (AAM) to generate synthetic images of the twofaces with a neutral facial expression and in the same view (the average of thetwo input views). This is achieved by piecewise affine warping which followsAAM fitting. A major contribution of our work is the use of an AAM ensemble inwhich each AAM is specialized to a particular range of poses and a particularregion of the thermal IR face space. Combined with the contributions from ourprevious work which addressed the problem of reliable AAM fitting in thethermal IR spectrum, and the development of a person-specific representationrobust to transient changes in the pattern of facial temperature emissions, theproposed ensemble framework accurately matches faces across the full range ofyaw from frontal to profile, even in the presence of scale variation (e.g. dueto the varying distance of a subject from the camera).
arxiv-3300-247 | Infrared face recognition: a literature review | http://arxiv.org/pdf/1306.1603v1.pdf | author:Reza Shoja Ghiass, Ognjen Arandjelovic, Hakim Bendada, Xavier Maldague category:cs.CV published:2013-06-07 summary:Automatic face recognition (AFR) is an area with immense practical potentialwhich includes a wide range of commercial and law enforcement applications, andit continues to be one of the most active research areas of computer vision.Even after over three decades of intense research, the state-of-the-art in AFRcontinues to improve, benefiting from advances in a range of different fieldsincluding image processing, pattern recognition, computer graphics andphysiology. However, systems based on visible spectrum images continue to facechallenges in the presence of illumination, pose and expression changes, aswell as facial disguises, all of which can significantly decrease theiraccuracy. Amongst various approaches which have been proposed in an attempt toovercome these limitations, the use of infrared (IR) imaging has emerged as aparticularly promising research direction. This paper presents a comprehensiveand timely review of the literature on this subject.
arxiv-3300-248 | Deep Learning of Representations: Looking Forward | http://arxiv.org/pdf/1305.0445v2.pdf | author:Yoshua Bengio category:cs.LG published:2013-05-02 summary:Deep learning research aims at discovering learning algorithms that discovermultiple levels of distributed representations, with higher levels representingmore abstract concepts. Although the study of deep learning has already led toimpressive theoretical results, learning algorithms and breakthroughexperiments, several challenges lie ahead. This paper proposes to examine someof these challenges, centering on the questions of scaling deep learningalgorithms to much larger models and datasets, reducing optimizationdifficulties due to ill-conditioning or local minima, designing more efficientand powerful inference and sampling procedures, and learning to disentangle thefactors of variation underlying the observed data. It also proposes a fewforward-looking research directions aimed at overcoming these challenges.
arxiv-3300-249 | North Atlantic Right Whale Contact Call Detection | http://arxiv.org/pdf/1304.7851v2.pdf | author:Rami Abousleiman, Guangzhi Qu, Osamah Rawashdeh category:cs.LG cs.SD published:2013-04-30 summary:The North Atlantic right whale (Eubalaena glacialis) is an endangeredspecies. These whales continuously suffer from deadly vessel impacts alongsidethe eastern coast of North America. There have been countless efforts to savethe remaining 350 - 400 of them. One of the most prominent works is done byMarinexplore and Cornell University. A system of hydrophones linked tosatellite connected-buoys has been deployed in the whales habitat. Thesehydrophones record and transmit live sounds to a base station. These recordingmight contain the right whale contact call as well as many other noises. Thenoise rate increases rapidly in vessel-busy areas such as by the Boston harbor.This paper presents and studies the problem of detecting the North Atlanticright whale contact call with the presence of noise and other marine lifesounds. A novel algorithm was developed to preprocess the sound waves before atree based hierarchical classifier is used to classify the data and provide ascore. The developed model was trained with 30,000 data points made availablethrough the Cornell University Whale Detection Challenge program. Resultsshowed that the developed algorithm had close to 85% success rate in detectingthe presence of the North Atlantic right whale.
arxiv-3300-250 | Policy Search: Any Local Optimum Enjoys a Global Performance Guarantee | http://arxiv.org/pdf/1306.1520v1.pdf | author:Bruno Scherrer, Matthieu Geist category:cs.LG cs.AI cs.RO math.OC published:2013-06-06 summary:Local Policy Search is a popular reinforcement learning approach for handlinglarge state spaces. Formally, it searches locally in a paramet erized policyspace in order to maximize the associated value function averaged over somepredefined distribution. It is probably commonly b elieved that the best onecan hope in general from such an approach is to get a local optimum of thiscriterion. In this article, we show th e following surprising result:\emph{any} (approximate) \emph{local optimum} enjoys a \emph{global performanceguarantee}. We compare this g uarantee with the one that is satisfied by DirectPolicy Iteration, an approximate dynamic programming algorithm that does someform of Poli cy Search: if the approximation error of Local Policy Search maygenerally be bigger (because local search requires to consider a space of stochastic policies), we argue that the concentrability coefficient that appearsin the performance bound is much nicer. Finally, we discuss several practicaland theoretical consequences of our analysis.
arxiv-3300-251 | Kernel Mean Estimation and Stein's Effect | http://arxiv.org/pdf/1306.0842v2.pdf | author:Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Arthur Gretton, Bernhard Schölkopf category:stat.ML cs.LG math.ST stat.TH published:2013-06-04 summary:A mean function in reproducing kernel Hilbert space, or a kernel mean, is animportant part of many applications ranging from kernel principal componentanalysis to Hilbert-space embedding of distributions. Given finite samples, anempirical average is the standard estimate for the true kernel mean. We showthat this estimator can be improved via a well-known phenomenon in statisticscalled Stein's phenomenon. After consideration, our theoretical analysisreveals the existence of a wide class of estimators that are better than thestandard. Focusing on a subset of this class, we propose efficient shrinkageestimators for the kernel mean. Empirical evaluations on several benchmarkapplications clearly demonstrate that the proposed estimators outperform thestandard kernel mean estimator.
arxiv-3300-252 | Highly Scalable, Parallel and Distributed AdaBoost Algorithm using Light Weight Threads and Web Services on a Network of Multi-Core Machines | http://arxiv.org/pdf/1306.1467v1.pdf | author:Munther Abualkibash, Ahmed ElSayed, Ausif Mahmood category:cs.DC cs.LG published:2013-06-06 summary:AdaBoost is an important algorithm in machine learning and is being widelyused in object detection. AdaBoost works by iteratively selecting the bestamongst weak classifiers, and then combines several weak classifiers to obtaina strong classifier. Even though AdaBoost has proven to be very effective, itslearning execution time can be quite large depending upon the application e.g.,in face detection, the learning time can be several days. Due to its increasinguse in computer vision applications, the learning time needs to be drasticallyreduced so that an adaptive near real time object detection system can beincorporated. In this paper, we develop a hybrid parallel and distributedAdaBoost algorithm that exploits the multiple cores in a CPU via light weightthreads, and also uses multiple machines via a web service softwarearchitecture to achieve high scalability. We present a novel hierarchical webservices based distributed architecture and achieve nearly linear speedup up tothe number of processors available to us. In comparison with the previouslypublished work, which used a single level master-slave parallel and distributedimplementation [1] and only achieved a speedup of 2.66 on four nodes, weachieve a speedup of 95.1 on 31 workstations each having a quad-core processor,resulting in a learning time of only 4.8 seconds per feature.
arxiv-3300-253 | K-Algorithm A Modified Technique for Noise Removal in Handwritten Documents | http://arxiv.org/pdf/1306.1462v1.pdf | author:Kanika Bansal, Rajiv Kumar category:cs.CV published:2013-06-06 summary:OCR has been an active research area since last few decades. OCR performs therecognition of the text in the scanned document image and converts it intoeditable form. The OCR process can have several stages like pre-processing,segmentation, recognition and post processing. The pre-processing stage is acrucial stage for the success of OCR, which mainly deals with noise removal. Inthe present paper, a modified technique for noise removal named as K-Algorithmhas been proposed, which has two stages as filtering and binarization. Theproposed technique shows improvised results in comparison to median filteringtechnique.
arxiv-3300-254 | PyHST2: an hybrid distributed code for high speed tomographic reconstruction with iterative reconstruction and a priori knowledge capabilities | http://arxiv.org/pdf/1306.1392v1.pdf | author:Alessandro Mirone, Emmanuelle Gouillart, Emmanuel Brun, Paul Tafforeau, Jerome Kieffer category:math.NA cs.CV published:2013-06-06 summary:We present the PyHST2 code which is in service at ESRF for phase-contrast andabsorption tomography. This code has been engineered to sustain the high dataflow typical of the third generation synchrotron facilities (10 terabytes perexperiment) by adopting a distributed and pipelined architecture. The codeimplements, beside a default filtered backprojection reconstruction, iterativereconstruction techniques with a-priori knowledge. These latter are used toimprove the reconstruction quality or in order to reduce the required datavolume and reach a given quality goal. The implemented a-priori knowledgetechniques are based on the total variation penalisation and a new recentlyfound convex functional which is based on overlapping patches. We give details of the different methods and their implementations while thecode is distributed under free license. We provide methods for estimating, in the absence of ground-truth data, theoptimal parameters values for a-priori techniques.
arxiv-3300-255 | Geometric operations implemented by conformal geometric algebra neural nodes | http://arxiv.org/pdf/1306.1358v1.pdf | author:Eckhard Hitzer category:cs.CV cs.NE math.RA published:2013-06-06 summary:Geometric algebra is an optimal frame work for calculating with vectors. Thegeometric algebra of a space includes elements that represent all the itssubspaces (lines, planes, volumes, ...). Conformal geometric algebra expandsthis approach to elementary representations of arbitrary points, point pairs,lines, circles, planes and spheres. Apart from including curved objects,conformal geometric algebra has an elegant unified quaternion likerepresentation for all proper and improper Euclidean transformations, includingreflections at spheres, general screw transformations and scaling. Expandingthe concepts of real and complex neurons we arrive at the new powerful conceptof conformal geometric algebra neurons. These neurons can easily take the abovementioned geometric objects or sets of these objects as inputs and apply a widerange of geometric transformations via the geometric algebra valued weights.
arxiv-3300-256 | The User Feedback on SentiWordNet | http://arxiv.org/pdf/1306.1343v1.pdf | author:Andrea Esuli category:cs.CL cs.IR I.2.7 published:2013-06-06 summary:With the release of SentiWordNet 3.0 the related Web interface has beenrestyled and improved in order to allow users to submit feedback on theSentiWordNet entries, in the form of the suggestion of alternative triplets ofvalues for an entry. This paper reports on the release of the user feedbackcollected so far and on the plans for the future.
arxiv-3300-257 | Table of Content detection using Machine Learning | http://arxiv.org/pdf/1306.4631v1.pdf | author:Rachana Parikh, Avani R. Vasant category:cs.LG cs.DL cs.IR published:2013-06-06 summary:Table of content (TOC) detection has drawn attention now a day because itplays an important role in digitization of multipage document. Generally bookdocument is multipage document. So it becomes necessary to detect Table ofContent page for easy navigation of multipage document and also to makeinformation retrieval faster for desirable data from the multipage document.All the Table of content pages follow the different layout, different way ofpresenting the contents of the document like chapter, section, subsection etc.This paper introduces a new method to detect Table of content using machinelearning technique with different features. With the main aim to detect Tableof Content pages is to structure the document according to their contents.
arxiv-3300-258 | Performance analysis of unsupervised feature selection methods | http://arxiv.org/pdf/1306.1326v1.pdf | author:A. Nisthana Parveen, H. Hannah Inbarani, E. N. Sathishkumar category:cs.LG published:2013-06-06 summary:Feature selection (FS) is a process which attempts to select more informativefeatures. In some cases, too many redundant or irrelevant features mayoverpower main features for classification. Feature selection can remedy thisproblem and therefore improve the prediction accuracy and reduce thecomputational overhead of classification algorithms. The main aim of featureselection is to determine a minimal feature subset from a problem domain whileretaining a suitably high accuracy in representing the original features. Inthis paper, Principal Component Analysis (PCA), Rough PCA, Unsupervised QuickReduct (USQR) algorithm and Empirical Distribution Ranking (EDR) approaches areapplied to discover discriminative features that will be the most adequate onesfor classification. Efficiency of the approaches is evaluated using standardclassification metrics.
arxiv-3300-259 | A Fuzzy Based Approach to Text Mining and Document Clustering | http://arxiv.org/pdf/1306.4633v1.pdf | author:Sumit Goswami, Mayank Singh Shishodia category:cs.LG cs.IR published:2013-06-06 summary:Fuzzy logic deals with degrees of truth. In this paper, we have shown how toapply fuzzy logic in text mining in order to perform document clustering. Wetook an example of document clustering where the documents had to be clusteredinto two categories. The method involved cleaning up the text and stemming ofwords. Then, we chose m number of features which differ significantly in theirword frequencies (WF), normalized by document length, between documentsbelonging to these two clusters. The documents to be clustered were representedas a collection of m normalized WF values. Fuzzy c-means (FCM) algorithm wasused to cluster these documents into two clusters. After the FCM executionfinished, the documents in the two clusters were analysed for the values oftheir respective m features. It was known that documents belonging to adocument type, say X, tend to have higher WF values for some particularfeatures. If the documents belonging to a cluster had higher WF values forthose same features, then that cluster was said to represent X. By fuzzy logic,we not only get the cluster name, but also the degree to which a documentbelongs to a cluster.
arxiv-3300-260 | Verdict Accuracy of Quick Reduct Algorithm using Clustering and Classification Techniques for Gene Expression Data | http://arxiv.org/pdf/1306.1323v1.pdf | author:T. Chandrasekhar, K. Thangavel, E. N. Sathishkumar category:cs.LG cs.CE stat.ML published:2013-06-06 summary:In most gene expression data, the number of training samples is very smallcompared to the large number of genes involved in the experiments. However,among the large amount of genes, only a small fraction is effective forperforming a certain task. Furthermore, a small subset of genes is desirable indeveloping gene expression based diagnostic tools for delivering reliable andunderstandable results. With the gene selection results, the cost of biologicalexperiment and decision can be greatly reduced by analyzing only the markergenes. An important application of gene expression data in functional genomicsis to classify samples according to their gene expression profiles. Featureselection (FS) is a process which attempts to select more informative features.It is one of the important steps in knowledge discovery. Conventionalsupervised FS methods evaluate various feature subsets using an evaluationfunction or metric to select only those features which are related to thedecision classes of the data under consideration. This paper studies a featureselection method based on rough set theory. Further K-Means, Fuzzy C-Means(FCM) algorithm have implemented for the reduced feature set withoutconsidering class labels. Then the obtained results are compared with theoriginal class labels. Back Propagation Network (BPN) has also been used forclassification. Then the performance of K-Means, FCM, and BPN are analyzedthrough the confusion matrix. It is found that the BPN is performing wellcomparatively.
arxiv-3300-261 | Recognition of Indian Sign Language in Live Video | http://arxiv.org/pdf/1306.1301v1.pdf | author:Joyeeta Singha, Karen Das category:cs.CV published:2013-06-06 summary:Sign Language Recognition has emerged as one of the important area ofresearch in Computer Vision. The difficulty faced by the researchers is thatthe instances of signs vary with both motion and appearance. Thus, in thispaper a novel approach for recognizing various alphabets of Indian SignLanguage is proposed where continuous video sequences of the signs have beenconsidered. The proposed system comprises of three stages: Preprocessing stage,Feature Extraction and Classification. Preprocessing stage includes skinfiltering, histogram matching. Eigen values and Eigen Vectors were consideredfor feature extraction stage and finally Eigen value weighted Euclideandistance is used to recognize the sign. It deals with bare hands, thus allowingthe user to interact with the system in natural way. We have considered 24different alphabets in the video sequences and attained a success rate of96.25%.
arxiv-3300-262 | Multi-scale Discriminant Saliency with Wavelet-based Hidden Markov Tree Modelling | http://arxiv.org/pdf/1301.7641v2.pdf | author:Anh Cat Le Ngo, Kenneth Li-Minn Ang, Guoping Qiu, Jasmine Kah-Phooi Seng category:cs.CV published:2013-01-31 summary:The bottom-up saliency, an early stage of humans' visual attention, can beconsidered as a binary classification problem between centre and surroundclasses. Discriminant power of features for the classification is measured asmutual information between distributions of image features and correspondingclasses . As the estimated discrepancy very much depends on considered scalelevel, multi-scale structure and discriminant power are integrated by employingdiscrete wavelet features and Hidden Markov Tree (HMT). With waveletcoefficients and Hidden Markov Tree parameters, quad-tree like label structuresare constructed and utilized in maximum a posterior probability (MAP) of hiddenclass variables at corresponding dyadic sub-squares. Then, a saliency value foreach square block at each scale level is computed with discriminant powerprinciple. Finally, across multiple scales is integrated the final saliency mapby an information maximization rule. Both standard quantitative tools such asNSS, LCC, AUC and qualitative assessments are used for evaluating the proposedmulti-scale discriminant saliency (MDIS) method against the well-knowinformation based approach AIM on its released image collection witheye-tracking data. Simulation results are presented and analysed to verify thevalidity of MDIS as well as point out its limitation for further researchdirection.
arxiv-3300-263 | Multiclass Semi-Supervised Learning on Graphs using Ginzburg-Landau Functional Minimization | http://arxiv.org/pdf/1306.1298v1.pdf | author:Cristina Garcia-Cardona, Arjuna Flenner, Allon G. Percus category:stat.ML cs.LG math.ST stat.TH I.5.3 published:2013-06-06 summary:We present a graph-based variational algorithm for classification ofhigh-dimensional data, generalizing the binary diffuse interface model to thecase of multiple classes. Motivated by total variation techniques, the methodinvolves minimizing an energy functional made up of three terms. The first twoterms promote a stepwise continuous classification function with sharptransitions between classes, while preserving symmetry among the class labels.The third term is a data fidelity term, allowing us to incorporate priorinformation into the model in a semi-supervised framework. The performance ofthe algorithm on synthetic data, as well as on the COIL and MNIST benchmarkdatasets, is competitive with state-of-the-art graph-based multiclasssegmentation methods.
arxiv-3300-264 | Efficient learning of simplices | http://arxiv.org/pdf/1211.2227v3.pdf | author:Joseph Anderson, Navin Goyal, Luis Rademacher category:cs.LG cs.DS stat.ML published:2012-11-09 summary:We show an efficient algorithm for the following problem: Given uniformlyrandom points from an arbitrary n-dimensional simplex, estimate the simplex.The size of the sample and the number of arithmetic operations of our algorithmare polynomial in n. This answers a question of Frieze, Jerrum and Kannan[FJK]. Our result can also be interpreted as efficiently learning theintersection of n+1 half-spaces in R^n in the model where the intersection isbounded and we are given polynomially many uniform samples from it. Our proofuses the local search technique from Independent Component Analysis (ICA), alsoused by [FJK]. Unlike these previous algorithms, which were based on analyzingthe fourth moment, ours is based on the third moment. We also show a direct connection between the problem of learning a simplexand ICA: a simple randomized reduction to ICA from the problem of learning asimplex. The connection is based on a known representation of the uniformmeasure on a simplex. Similar representations lead to a reduction from theproblem of learning an affine transformation of an n-dimensional l_p ball toICA.
arxiv-3300-265 | Multiclass Total Variation Clustering | http://arxiv.org/pdf/1306.1185v1.pdf | author:Xavier Bresson, Thomas Laurent, David Uminsky, James H. von Brecht category:stat.ML cs.LG math.OC published:2013-06-05 summary:Ideas from the image processing literature have recently motivated a new setof clustering algorithms that rely on the concept of total variation. Whilethese algorithms perform well for bi-partitioning tasks, their recursiveextensions yield unimpressive results for multiclass clustering tasks. Thispaper presents a general framework for multiclass total variation clusteringthat does not rely on recursion. The results greatly outperform previous totalvariation algorithms and compare well with state-of-the-art NMF approaches.
arxiv-3300-266 | A hybrid bat algorithm | http://arxiv.org/pdf/1303.6310v3.pdf | author:Iztok Fister Jr., Dušan Fister, Xin-She Yang category:cs.NE published:2013-03-25 summary:Swarm intelligence is a very powerful technique to be used for optimizationpurposes. In this paper we present a new swarm intelligence algorithm, based onthe bat algorithm. The Bat algorithm is hybridized with differential evolutionstrategies. Besides showing very promising results of the standard benchmarkfunctions, this hybridization also significantly improves the original batalgorithm.
arxiv-3300-267 | Securing Your Transactions: Detecting Anomalous Patterns In XML Documents | http://arxiv.org/pdf/1209.1797v3.pdf | author:Eitan Menahem, Alon Schclar, Lior Rokach, Yuval Elovici category:cs.CR cs.LG published:2012-09-09 summary:XML transactions are used in many information systems to store data andinteract with other systems. Abnormal transactions, the result of either anon-going cyber attack or the actions of a benign user, can potentially harm theinteracting systems and therefore they are regarded as a threat. In this paperwe address the problem of anomaly detection and localization in XMLtransactions using machine learning techniques. We present a new XML anomalydetection framework, XML-AD. Within this framework, an automatic method forextracting features from XML transactions was developed as well as a practicalmethod for transforming XML features into vectors of fixed dimensionality. Withthese two methods in place, the XML-AD framework makes it possible to utilizegeneral learning algorithms for anomaly detection. Central to the functioningof the framework is a novel multi-univariate anomaly detection algorithm,ADIFA. The framework was evaluated on four XML transactions datasets, capturedfrom real information systems, in which it achieved over 89% true positivedetection rate with less than a 0.2% false positive rate.
arxiv-3300-268 | Discriminative Parameter Estimation for Random Walks Segmentation: Technical Report | http://arxiv.org/pdf/1306.1083v1.pdf | author:Pierre-Yves Baudin, Danny Goodman, Puneet Kumar, Noura Azzabou, Pierre G. Carlier, Nikos Paragios, M. Pawan Kumar category:cs.CV cs.LG published:2013-06-05 summary:The Random Walks (RW) algorithm is one of the most e - cient and easy-to-useprobabilistic segmentation methods. By combining contrast terms with priorterms, it provides accurate segmentations of medical images in a fullyautomated manner. However, one of the main drawbacks of using the RW algorithmis that its parameters have to be hand-tuned. we propose a novel discriminativelearning framework that estimates the parameters using a training dataset. Themain challenge we face is that the training samples are not fully supervised.Speci cally, they provide a hard segmentation of the images, instead of aproba-bilistic segmentation. We overcome this challenge by treating the optimalprobabilistic segmentation that is compatible with the given hard segmentationas a latent variable. This allows us to employ the latent support vectormachine formulation for parameter estimation. We show that our approach signicantly outperforms the baseline methods on a challenging dataset consisting ofreal clinical 3D MRI volumes of skeletal muscles.
arxiv-3300-269 | Fast Dual Variational Inference for Non-Conjugate LGMs | http://arxiv.org/pdf/1306.1052v1.pdf | author:Mohammad Emtiyaz Khan, Aleksandr Y. Aravkin, Michael P. Friedlander, Matthias Seeger category:stat.ML math.OC stat.CO published:2013-06-05 summary:Latent Gaussian models (LGMs) are widely used in statistics and machinelearning. Bayesian inference in non-conjugate LGMs is difficult due tointractable integrals involving the Gaussian prior and non-conjugatelikelihoods. Algorithms based on variational Gaussian (VG) approximations arewidely employed since they strike a favorable balance between accuracy,generality, speed, and ease of use. However, the structure of the optimizationproblems associated with these approximations remains poorly understood, andstandard solvers take too long to converge. We derive a novel dual variationalinference approach that exploits the convexity property of the VGapproximations. We obtain an algorithm that solves a convex optimizationproblem, reduces the number of variational parameters, and converges muchfaster than previous methods. Using real-world data, we demonstrate theseadvantages on a variety of LGMs, including Gaussian process classification, andlatent Gaussian Markov random fields.
arxiv-3300-270 | ROTUNDE - A Smart Meeting Cinematography Initiative: Tools, Datasets, and Benchmarks for Cognitive Interpretation and Control | http://arxiv.org/pdf/1306.1034v1.pdf | author:Mehul Bhatt, Jakob Suchan, Christian Freksa category:cs.AI cs.CV cs.HC published:2013-06-05 summary:We construe smart meeting cinematography with a focus on professionalsituations such as meetings and seminars, possibly conducted in a distributedmanner across socio-spatially separated groups. The basic objective in smartmeeting cinematography is to interpret professional interactions involvingpeople, and automatically produce dynamic recordings of discussions, debates,presentations etc in the presence of multiple communication modalities. Typicalmodalities include gestures (e.g., raising one's hand for a question,applause), voice and interruption, electronic apparatus (e.g., pressing abutton), movement (e.g., standing-up, moving around) etc. ROTUNDE, an instanceof smart meeting cinematography concept, aims to: (a) developfunctionality-driven benchmarks with respect to the interpretation and controlcapabilities of human-cinematographers, real-time video editors, surveillancepersonnel, and typical human performance in everyday situations; (b) Developgeneral tools for the commonsense cognitive interpretation of dynamic scenesfrom the viewpoint of visuo-spatial cognition centred perceptualnarrativisation. Particular emphasis is placed on declarative representationsand interfacing mechanisms that seamlessly integrate within large-scalecognitive (interaction) systems and companion technologies consisting ofdiverse AI sub-components. For instance, the envisaged tools would providegeneral capabilities for high-level commonsense reasoning about space, events,actions, change, and interaction.
arxiv-3300-271 | Quaternion Fourier Transform on Quaternion Fields and Generalizations | http://arxiv.org/pdf/1306.1023v1.pdf | author:Eckhard Hitzer category:math.RA cs.CV math-ph math.MP published:2013-06-05 summary:We treat the quaternionic Fourier transform (QFT) applied to quaternionfields and investigate QFT properties useful for applications. Different formsof the QFT lead us to different Plancherel theorems. We relate the QFTcomputation for quaternion fields to the QFT of real signals. We research thegeneral linear ($GL$) transformation behavior of the QFT with matrices,Clifford geometric algebra and with examples. We finally arrive at wide-rangingnon-commutative multivector FT generalizations of the QFT. Examples given arenew volume-time and spacetime algebra Fourier transformations.
arxiv-3300-272 | Alternating Decision trees for early diagnosis of dengue fever | http://arxiv.org/pdf/1305.7331v2.pdf | author:M. Naresh Kumar category:cs.LG q-bio.QM stat.AP published:2013-05-31 summary:Dengue fever is a flu-like illness spread by the bite of an infected mosquitowhich is fast emerging as a major health problem. Timely and cost effectivediagnosis using clinical and laboratory features would reduce the mortalityrates besides providing better grounds for clinical management and diseasesurveillance. We wish to develop a robust and effective decision tree basedapproach for predicting dengue disease. Our analysis is based on the clinicalcharacteristics and laboratory measurements of the diseased individuals. Wehave developed and trained an alternating decision tree with boosting andcompared its performance with C4.5 algorithm for dengue disease diagnosis. Ofthe 65 patient records a diagnosis establishes that 53 individuals have beenconfirmed to have dengue fever. An alternating decision tree based algorithmwas able to differentiate the dengue fever using the clinical and laboratorydata with number of correctly classified instances as 89%, F-measure of 0.86and receiver operator characteristics (ROC) of 0.826 as compared to C4.5 havingcorrectly classified instances as 78%,h F-measure of 0.738 and ROC of 0.617respectively. Alternating decision tree based approach with boosting has beenable to predict dengue fever with a higher degree of accuracy than C4.5 baseddecision tree using simple clinical and laboratory features. Further analysison larger data sets is required to improve the sensitivity and specificity ofthe alternating decision trees.
arxiv-3300-273 | Distributed Bayesian inference for consistent labeling of tracked objects in non-overlapping camera networks | http://arxiv.org/pdf/1306.0974v1.pdf | author:Jiuqing Wan, Li Liu category:cs.CV published:2013-06-05 summary:One of the fundamental requirements for visual surveillance usingnon-overlapping camera networks is the correct labeling of tracked objects oneach camera in a consistent way,in the sense that the captured tracklets, orobservations in this paper, of the same object at different cameras should beassigned with the same label. In this paper, we formulate this task as aBayesian inference problem and propose a distributed inference framework inwhich the posterior distribution of labeling variable corresponding to eachobservation, conditioned on all history appearance and spatio-temporal evidencemade in the whole networks, is calculated based solely on local informationprocessing on each camera and mutual information exchanging between neighboringcameras. In our framework, the number of objects presenting in the monitoredregion, i.e. the sampling space of labeling variables, does not need to bespecified beforehand. Instead, it can be determined automatically on the fly.In addition, we make no assumption about the appearance distribution of asingle object, but use similarity scores between appearance pairs, given byadvanced object re-identification algorithm, as appearance likelihood forinference. This feature makes our method very flexible and competitive whenobserving condition undergoes large changes across camera views. To cope withthe problem of missing detection, which is critical for distributed inference,we consider an enlarged neighborhood of each camera during inference and use amixture model to describe the higher order spatio-temporal constraints. Therobustness of the algorithm against missing detection is improved at the costof slightly increased computation and communication burden at each camera node.Finally, we demonstrate the effectiveness of our method through experiments onan indoor Office Building dataset and an outdoor Campus Garden dataset.
arxiv-3300-274 | Inferring Robot Task Plans from Human Team Meetings: A Generative Modeling Approach with Logic-Based Prior | http://arxiv.org/pdf/1306.0963v1.pdf | author:Been Kim, Caleb M. Chacha, Julie Shah category:cs.AI cs.CL cs.RO stat.ML published:2013-06-05 summary:We aim to reduce the burden of programming and deploying autonomous systemsto work in concert with people in time-critical domains, such as military fieldoperations and disaster response. Deployment plans for these operations arefrequently negotiated on-the-fly by teams of human planners. A human operatorthen translates the agreed upon plan into machine instructions for the robots.We present an algorithm that reduces this translation burden by inferring thefinal plan from a processed form of the human team's planning conversation. Ourapproach combines probabilistic generative modeling with logical planvalidation used to compute a highly structured prior over possible plans. Thishybrid approach enables us to overcome the challenge of performing inferenceover the large solution space with only a small amount of noisy data from theteam planning session. We validate the algorithm through human subjectexperimentation and show we are able to infer a human team's final plan with83% accuracy on average. We also describe a robot demonstration in which twopeople plan and execute a first-response collaborative task with a PR2 robot.To the best of our knowledge, this is the first work that integrates a logicalplanning technique within a generative model to perform plan inference.
arxiv-3300-275 | Fast Reinforcement Learning for Energy-Efficient Wireless Communications | http://arxiv.org/pdf/1009.5773v4.pdf | author:Nicholas Mastronarde, Mihaela van der Schaar category:cs.LG published:2010-09-29 summary:We consider the problem of energy-efficient point-to-point transmission ofdelay-sensitive data (e.g. multimedia data) over a fading channel. Existingresearch on this topic utilizes either physical-layer centric solutions, namelypower-control and adaptive modulation and coding (AMC), or system-levelsolutions based on dynamic power management (DPM); however, there is currentlyno rigorous and unified framework for simultaneously utilizing bothphysical-layer centric and system-level techniques to achieve the minimumpossible energy consumption, under delay constraints, in the presence ofstochastic and a priori unknown traffic and channel conditions. In this report,we propose such a framework. We formulate the stochastic optimization problemas a Markov decision process (MDP) and solve it online using reinforcementlearning. The advantages of the proposed online method are that (i) it does notrequire a priori knowledge of the traffic arrival and channel statistics todetermine the jointly optimal power-control, AMC, and DPM policies; (ii) itexploits partial information about the system so that less information needs tobe learned than when using conventional reinforcement learning algorithms; and(iii) it obviates the need for action exploration, which severely limits theadaptation speed and run-time performance of conventional reinforcementlearning algorithms. Our results show that the proposed learning algorithms canconverge up to two orders of magnitude faster than a state-of-the-art learningalgorithm for physical layer power-control and up to three orders of magnitudefaster than conventional reinforcement learning algorithms.
arxiv-3300-276 | Online Learning under Delayed Feedback | http://arxiv.org/pdf/1306.0686v2.pdf | author:Pooria Joulani, András György, Csaba Szepesvári category:cs.LG cs.AI stat.ML published:2013-06-04 summary:Online learning with delayed feedback has received increasing attentionrecently due to its several applications in distributed, web-based learningproblems. In this paper we provide a systematic study of the topic, and analyzethe effect of delay on the regret of online learning algorithms. Somewhatsurprisingly, it turns out that delay increases the regret in a multiplicativeway in adversarial problems, and in an additive way in stochastic problems. Wegive meta-algorithms that transform, in a black-box fashion, algorithmsdeveloped for the non-delayed case into ones that can handle the presence ofdelays in the feedback loop. Modifications of the well-known UCB algorithm arealso developed for the bandit problem with delayed feedback, with the advantageover the meta-algorithms that they can be implemented with lower complexity.
arxiv-3300-277 | $\propto$SVM for learning with label proportions | http://arxiv.org/pdf/1306.0886v1.pdf | author:Felix X. Yu, Dong Liu, Sanjiv Kumar, Tony Jebara, Shih-Fu Chang category:cs.LG stat.ML published:2013-06-04 summary:We study the problem of learning with label proportions in which the trainingdata is provided in groups and only the proportion of each class in each groupis known. We propose a new method called proportion-SVM, or $\propto$SVM, whichexplicitly models the latent unknown instance labels together with the knowngroup label proportions in a large-margin framework. Unlike the existing works,our approach avoids making restrictive assumptions about the data. The$\propto$SVM model leads to a non-convex integer programming problem. In orderto solve it efficiently, we propose two algorithms: one based on simplealternating optimization and the other based on a convex relaxation. Extensiveexperiments on standard datasets show that $\propto$SVM outperforms thestate-of-the-art, especially for larger group sizes.
arxiv-3300-278 | Urban ozone concentration forecasting with artificial neural network in Corsica | http://arxiv.org/pdf/1306.0897v1.pdf | author:Wani W. Tamas, Gilles Notton, Christophe Paoli, Cyril Voyant, Marie Laure Nivet, Aurélia Balu category:cs.NE published:2013-06-04 summary:Atmospheric pollutants concentration forecasting is an important issue in airquality monitoring. Qualitair Corse, the organization responsible formonitoring air quality in Corsica (France) region, needs to develop ashort-term prediction model to lead its mission of information towards thepublic. Various deterministic models exist for meso-scale or local forecasting,but need powerful large variable sets, a good knowledge of atmosphericprocesses, and can be inaccurate because of local climatical or geographicalparticularities, as observed in Corsica, a mountainous island located in aMediterranean Sea. As a result, we focus in this study on statistical models,and particularly Artificial Neural Networks (ANN) that have shown good resultsin the prediction of ozone concentration at horizon h+1 with data measuredlocally. The purpose of this study is to build a predictor to realizepredictions of ozone and PM10 at horizon d+1 in Corsica in order to be able toanticipate pollution peak formation and to take appropriated preventionmeasures. Specific meteorological conditions are known to lead to particularpollution event in Corsica (e.g. Saharan dust event). Therefore, several ANNmodels will be used, for meteorological conditions clustering and foroperational forecasting.
arxiv-3300-279 | Finding Numerical Solutions of Diophantine Equations using Ant Colony Optimization | http://arxiv.org/pdf/1306.0896v1.pdf | author:Siby Abraham, Sugata Sanyal, Mukund Sanglikar category:cs.NE cs.ET published:2013-06-04 summary:The paper attempts to find numerical solutions of Diophantine equations, achallenging problem as there are no general methods to find solutions of suchequations. It uses the metaphor of foraging habits of real ants. The ant colonyoptimization based procedure starts with randomly assigned locations to a fixednumber of artificial ants. Depending upon the quality of these positions, antsdeposit pheromone at the nodes. A successor node is selected from thetopological neighborhood of each of the nodes based on this stochasticpheromone deposit. If an ant bumps into an already encountered node, thepheromone is updated correspondingly. A suitably defined pheromone evaporationstrategy guarantees that premature convergence does not take place. Theexperimental results, which compares with those of other machine intelligencetechniques, validate the effectiveness of the proposed method.
arxiv-3300-280 | Evolutionary optimization of an experimental apparatus | http://arxiv.org/pdf/1305.4094v2.pdf | author:I. Geisel, K. Cordes, J. Mahnke, S. Jöllenbeck, J. Ostermann, J. Arlt, W. Ertmer, C. Klempt category:quant-ph cs.NE published:2013-05-17 summary:In recent decades, cold atom experiments have become increasingly complex.While computers control most parameters, optimization is mostly done manually.This is a time-consuming task for a high-dimensional parameter space withunknown correlations. Here we automate this process using a genetic algorithmbased on Differential Evolution. We demonstrate that this algorithm optimizes21 correlated parameters and that it is robust against local maxima andexperimental noise. The algorithm is flexible and easy to implement. Thus, thepresented scheme can be applied to a wide range of experimental optimizationtasks.
arxiv-3300-281 | Sinkhorn Distances: Lightspeed Computation of Optimal Transportation Distances | http://arxiv.org/pdf/1306.0895v1.pdf | author:Marco Cuturi category:stat.ML published:2013-06-04 summary:Optimal transportation distances are a fundamental family of parameterizeddistances for histograms. Despite their appealing theoretical properties,excellent performance in retrieval tasks and intuitive formulation, theircomputation involves the resolution of a linear program whose cost isprohibitive whenever the histograms' dimension exceeds a few hundreds. Wepropose in this work a new family of optimal transportation distances that lookat transportation problems from a maximum-entropy perspective. We smooth theclassical optimal transportation problem with an entropic regularization term,and show that the resulting optimum is also a distance which can be computedthrough Sinkhorn-Knopp's matrix scaling algorithm at a speed that is severalorders of magnitude faster than that of transportation solvers. We also reportimproved performance over classical optimal transportation distances on theMNIST benchmark problem.
arxiv-3300-282 | Fast Gradient-Based Inference with Continuous Latent Variable Models in Auxiliary Form | http://arxiv.org/pdf/1306.0733v1.pdf | author:Diederik P Kingma category:cs.LG stat.ML published:2013-06-04 summary:We propose a technique for increasing the efficiency of gradient-basedinference and learning in Bayesian networks with multiple layers of continuouslatent vari- ables. We show that, in many cases, it is possible to express suchmodels in an auxiliary form, where continuous latent variables areconditionally deterministic given their parents and a set of independentauxiliary variables. Variables of mod- els in this auxiliary form have muchlarger Markov blankets, leading to significant speedups in gradient-basedinference, e.g. rapid mixing Hybrid Monte Carlo and efficient gradient-basedoptimization. The relative efficiency is confirmed in ex- periments.
arxiv-3300-283 | Fault detection system for Arabic language | http://arxiv.org/pdf/1203.2498v2.pdf | author:Riadh Bouslimi, Houda Amraoui category:cs.CL published:2012-03-08 summary:The study of natural language, especially Arabic, and mechanisms for theimplementation of automatic processing is a fascinating field of study, withvarious potential applications. The importance of tools for natural languageprocessing is materialized by the need to have applications that caneffectively treat the vast mass of information available nowadays on electronicforms. Among these tools, mainly driven by the necessity of a fast writing inalignment to the actual daily life speed, our interest is on the writingauditors. The morphological and syntactic properties of Arabic make it adifficult language to master, and explain the lack in the processing tools forthat language. Among these properties, we can mention: the complex structure ofthe Arabic word, the agglutinative nature, lack of vocalization, thesegmentation of the text, the linguistic richness, etc.
arxiv-3300-284 | Using a bag of Words for Automatic Medical Image Annotation with a Latent Semantic | http://arxiv.org/pdf/1306.0178v2.pdf | author:Riadh Bouslimi, Abir Messaoudi, Jalel Akaichi category:cs.IR cs.CV published:2013-06-02 summary:We present in this paper a new approach for the automatic annotation ofmedical images, using the approach of "bag-of-words" to represent the visualcontent of the medical image combined with text descriptors based approachtf.idf and reduced by latent semantic to extract the co-occurrence betweenterms and visual terms. A medical report is composed of a text describing amedical image. First, we are interested to index the text and extract allrelevant terms using a thesaurus containing MeSH medical concepts. In a secondphase, the medical image is indexed while recovering areas of interest whichare invariant to change in scale, light and tilt. To annotate a new medicalimage, we use the approach of "bagof-words" to recover the feature vector.Indeed, we use the vector space model to retrieve similar medical image fromthe database training. The calculation of the relevance value of an image tothe query image is based on the cosine function. We conclude with an experimentcarried out on five types of radiological imaging to evaluate the performanceof our system of medical annotation. The results showed that our approach worksbetter with more images from the radiology of the skull.
arxiv-3300-285 | Information Theoretic Learning with Infinitely Divisible Kernels | http://arxiv.org/pdf/1301.3551v6.pdf | author:Luis G. Sanchez Giraldo, Jose C. Principe category:cs.LG cs.CV published:2013-01-16 summary:In this paper, we develop a framework for information theoretic learningbased on infinitely divisible matrices. We formulate an entropy-like functionalon positive definite matrices based on Renyi's axiomatic definition of entropyand examine some key properties of this functional that lead to the concept ofinfinite divisibility. The proposed formulation avoids the plug in estimationof density and brings along the representation power of reproducing kernelHilbert spaces. As an application example, we derive a supervised metriclearning algorithm using a matrix based analogue to conditional entropyachieving results comparable with the state of the art.
arxiv-3300-286 | Spectral Descriptors for Graph Matching | http://arxiv.org/pdf/1304.1572v4.pdf | author:Nan Hu, Leonidas Guibas category:cs.CV published:2013-04-04 summary:In this paper, we consider the weighted graph matching problem. Recently,approaches to this problem based on spectral methods have gained significantattention. We propose two graph spectral descriptors based on the graphLaplacian, namely a Laplacian family signature (LFS) on nodes, and a pairwiseheat kernel distance on edges. We show the stability of both our descriptorsunder small perturbation of edges and nodes. In addition, we show that ourpairwise heat kernel distance is a noise-tolerant approximation of theclassical adjacency matrix-based second order compatibility function. Thesenice properties suggest a descriptor-based matching scheme, for which we set upan integer quadratic problem (IQP) and apply an approximate solver to find anear optimal solution. We have tested our matching method on a set of randomlygenerated graphs, the widely-used CMU house sequence and a set of real images.These experiments show the superior performance of our selected node signaturesand edge descriptors for graph matching, as compared with other existingsignature-based matchings and adjacency matrix-based matchings.
arxiv-3300-287 | Provable Inductive Matrix Completion | http://arxiv.org/pdf/1306.0626v1.pdf | author:Prateek Jain, Inderjit S. Dhillon category:cs.LG cs.IT math.IT stat.ML published:2013-06-04 summary:Consider a movie recommendation system where apart from the ratingsinformation, side information such as user's age or movie's genre is alsoavailable. Unlike standard matrix completion, in this setting one should beable to predict inductively on new users/movies. In this paper, we study theproblem of inductive matrix completion in the exact recovery setting. That is,we assume that the ratings matrix is generated by applying feature vectors to alow-rank matrix and the goal is to recover back the underlying matrix.Furthermore, we generalize the problem to that of low-rank matrix estimationusing rank-1 measurements. We study this generic problem and provide conditionsthat the set of measurements should satisfy so that the alternatingminimization method (which otherwise is a non-convex method with no convergenceguarantees) is able to recover back the {\em exact} underlying low-rank matrix. In addition to inductive matrix completion, we show that two other low-rankestimation problems can be studied in our framework: a) general low-rank matrixsensing using rank-1 measurements, and b) multi-label regression with missinglabels. For both the problems, we provide novel and interesting bounds on thenumber of measurements required by alternating minimization to provablyconverges to the {\em exact} low-rank matrix. In particular, our analysis forthe general low rank matrix sensing problem significantly improves the requiredstorage and computational cost than that required by the RIP-based matrixsensing methods \cite{RechtFP2007}. Finally, we provide empirical validation ofour approach and demonstrate that alternating minimization is able to recoverthe true matrix for the above mentioned problems using a small number ofmeasurements.
arxiv-3300-288 | Characterizing Truthful Multi-Armed Bandit Mechanisms | http://arxiv.org/pdf/0812.2291v7.pdf | author:Moshe Babaioff, Yogeshwer Sharma, Aleksandrs Slivkins category:cs.DS cs.GT cs.LG published:2008-12-12 summary:We consider a multi-round auction setting motivated by pay-per-click auctionsfor Internet advertising. In each round the auctioneer selects an advertiserand shows her ad, which is then either clicked or not. An advertiser derivesvalue from clicks; the value of a click is her private information. Initially,neither the auctioneer nor the advertisers have any information about thelikelihood of clicks on the advertisements. The auctioneer's goal is to designa (dominant strategies) truthful mechanism that (approximately) maximizes thesocial welfare. If the advertisers bid their true private values, our problem is equivalentto the "multi-armed bandit problem", and thus can be viewed as a strategicversion of the latter. In particular, for both problems the quality of analgorithm can be characterized by "regret", the difference in social welfarebetween the algorithm and the benchmark which always selects the same "best"advertisement. We investigate how the design of multi-armed bandit algorithmsis affected by the restriction that the resulting mechanism must be truthful.We find that truthful mechanisms have certain strong structural properties --essentially, they must separate exploration from exploitation -- and they incurmuch higher regret than the optimal multi-armed bandit algorithms. Moreover, weprovide a truthful mechanism which (essentially) matches our lower bound onregret.
arxiv-3300-289 | Revisiting the Nystrom Method for Improved Large-Scale Machine Learning | http://arxiv.org/pdf/1303.1849v2.pdf | author:Alex Gittens, Michael W. Mahoney category:cs.LG cs.DS cs.NA published:2013-03-07 summary:We reconsider randomized algorithms for the low-rank approximation ofsymmetric positive semi-definite (SPSD) matrices such as Laplacian and kernelmatrices that arise in data analysis and machine learning applications. Ourmain results consist of an empirical evaluation of the performance quality andrunning time of sampling and projection methods on a diverse suite of SPSDmatrices. Our results highlight complementary aspects of sampling versusprojection methods; they characterize the effects of common data preprocessingsteps on the performance of these algorithms; and they point to importantdifferences between uniform sampling and nonuniform sampling methods based onleverage scores. In addition, our empirical results illustrate that existingtheory is so weak that it does not provide even a qualitative guide topractice. Thus, we complement our empirical results with a suite of worst-casetheoretical bounds for both random sampling and random projection methods.These bounds are qualitatively superior to existing bounds---e.g. improvedadditive-error bounds for spectral and Frobenius norm error and relative-errorbounds for trace norm error---and they point to future directions to make thesealgorithms useful in even larger-scale machine learning applications.
arxiv-3300-290 | On the Performance Bounds of some Policy Search Dynamic Programming Algorithms | http://arxiv.org/pdf/1306.0539v1.pdf | author:Bruno Scherrer category:cs.AI cs.LG published:2013-06-03 summary:We consider the infinite-horizon discounted optimal control problemformalized by Markov Decision Processes. We focus on Policy Search algorithms,that compute an approximately optimal policy by following the standard PolicyIteration (PI) scheme via an -approximate greedy operator (Kakade and Langford,2002; Lazaric et al., 2010). We describe existing and a few new performancebounds for Direct Policy Iteration (DPI) (Lagoudakis and Parr, 2003; Fern etal., 2006; Lazaric et al., 2010) and Conservative Policy Iteration (CPI)(Kakade and Langford, 2002). By paying a particular attention to theconcentrability constants involved in such guarantees, we notably argue thatthe guarantee of CPI is much better than that of DPI, but this comes at thecost of a relative--exponential in $\frac{1}{\epsilon}$-- increase of timecomplexity. We then describe an algorithm, Non-Stationary Direct PolicyIteration (NSDPI), that can either be seen as 1) a variation of Policy Searchby Dynamic Programming by Bagnell et al. (2003) to the infinite horizonsituation or 2) a simplified version of the Non-Stationary PI with growingperiod of Scherrer and Lesner (2012). We provide an analysis of this algorithm,that shows in particular that it enjoys the best of both worlds: itsperformance guarantee is similar to that of CPI, but within a time complexitysimilar to that of DPI.
arxiv-3300-291 | Evolutionary Approach for the Containers Bin-Packing Problem | http://arxiv.org/pdf/1306.0442v1.pdf | author:R. Kammarti, I. Ayachi, M. Ksouri, P. Borne category:cs.NE published:2013-06-03 summary:This paper deals with the resolution of combinatorial optimization problems,particularly those concerning the maritime transport scheduling. We areinterested in the management platforms in a river port and more specifically incontainer organisation operations with a view to minimizing the number ofcontainer rehandlings. Subsequently, we rmeet customers delivery deadlines andwe reduce ship stoppage time In this paper, we propose a genetic algorithm tosolve this problem and we present some experiments and results.
arxiv-3300-292 | The Randomized Dependence Coefficient | http://arxiv.org/pdf/1304.7717v2.pdf | author:David Lopez-Paz, Philipp Hennig, Bernhard Schölkopf category:stat.ML published:2013-04-29 summary:We introduce the Randomized Dependence Coefficient (RDC), a measure ofnon-linear dependence between random variables of arbitrary dimension based onthe Hirschfeld-Gebelein-R\'enyi Maximum Correlation Coefficient. RDC is definedin terms of correlation of random non-linear copula projections; it isinvariant with respect to marginal distribution transformations, has lowcomputational cost and is easy to implement: just five lines of R code,included at the end of the paper.
arxiv-3300-293 | Low-rank optimization with trace norm penalty | http://arxiv.org/pdf/1112.2318v2.pdf | author:B. Mishra, G. Meyer, F. Bach, R. Sepulchre category:math.OC cs.LG published:2011-12-11 summary:The paper addresses the problem of low-rank trace norm minimization. Wepropose an algorithm that alternates between fixed-rank optimization andrank-one updates. The fixed-rank optimization is characterized by an efficientfactorization that makes the trace norm differentiable in the search space andthe computation of duality gap numerically tractable. The search space isnonlinear but is equipped with a particular Riemannian structure that leads toefficient computations. We present a second-order trust-region algorithm with aguaranteed quadratic rate of convergence. Overall, the proposed optimizationscheme converges super-linearly to the global solution while maintainingcomplexity that is linear in the number of rows and columns of the matrix. Tocompute a set of solutions efficiently for a grid of regularization parameterswe propose a predictor-corrector approach that outperforms the naivewarm-restart approach on the fixed-rank quotient manifold. The performance ofthe proposed algorithm is illustrated on problems of low-rank matrix completionand multivariate linear regression.
arxiv-3300-294 | Minimal cost feature selection of data with normal distribution measurement errors | http://arxiv.org/pdf/1211.2512v2.pdf | author:Hong Zhao, Fan Min, William Zhu category:cs.AI cs.LG published:2012-11-12 summary:Minimal cost feature selection is devoted to obtain a trade-off between testcosts and misclassification costs. This issue has been addressed recently onnominal data. In this paper, we consider numerical data with measurement errorsand study minimal cost feature selection in this model. First, we build a datamodel with normal distribution measurement errors. Second, the neighborhood ofeach data item is constructed through the confidence interval. Comparing withdiscretized intervals, neighborhoods are more reasonable to maintain theinformation of data. Third, we define a new minimal total cost featureselection problem through considering the trade-off between test costs andmisclassification costs. Fourth, we proposed a backtracking algorithm withthree effective pruning techniques to deal with this problem. The algorithm istested on four UCI data sets. Experimental results indicate that the pruningtechniques are effective, and the algorithm is efficient for data sets withnearly one thousand objects.
arxiv-3300-295 | Cost-Sensitive Feature Selection of Data with Errors | http://arxiv.org/pdf/1212.3185v3.pdf | author:Hong Zhao, Fan Min, William Zhu category:cs.LG published:2012-12-13 summary:In data mining applications, feature selection is an essential process sinceit reduces a model's complexity. The cost of obtaining the feature values mustbe taken into consideration in many domains. In this paper, we study thecost-sensitive feature selection problem on numerical data with measurementerrors, test costs and misclassification costs. The major contributions of thispaper are four-fold. First, a new data model is built to address test costs andmisclassification costs as well as error boundaries. Second, a covering-basedrough set with measurement errors is constructed. Given a confidence interval,the neighborhood is an ellipse in a two-dimension space, or an ellipsoidal in athree-dimension space, etc. Third, a new cost-sensitive feature selectionproblem is defined on this covering-based rough set. Fourth, both backtrackingand heuristic algorithms are proposed to deal with this new problem. Thealgorithms are tested on six UCI (University of California - Irvine) data sets.Experimental results show that (1) the pruning techniques of the backtrackingalgorithm help reducing the number of operations significantly, and (2) theheuristic algorithm usually obtains optimal results. This study is a steptoward realistic applications of cost-sensitive learning.
arxiv-3300-296 | KERT: Automatic Extraction and Ranking of Topical Keyphrases from Content-Representative Document Titles | http://arxiv.org/pdf/1306.0271v1.pdf | author:Marina Danilevsky, Chi Wang, Nihit Desai, Jingyi Guo, Jiawei Han category:cs.LG cs.IR published:2013-06-03 summary:We introduce KERT (Keyphrase Extraction and Ranking by Topic), a frameworkfor topical keyphrase generation and ranking. By shifting from theunigram-centric traditional methods of unsupervised keyphrase extraction to aphrase-centric approach, we are able to directly compare and rank phrases ofdifferent lengths. We construct a topical keyphrase ranking function whichimplements the four criteria that represent high quality topical keyphrases(coverage, purity, phraseness, and completeness). The effectiveness of ourapproach is demonstrated on two collections of content-representative titles inthe domains of Computer Science and Physics.
arxiv-3300-297 | Convolutional Neural Networks learn compact local image descriptors | http://arxiv.org/pdf/1304.7948v2.pdf | author:Christian Osendorfer, Justin Bayer, Patrick van der Smagt category:cs.CV published:2013-04-30 summary:A standard deep convolutional neural network paired with a suitable lossfunction learns compact local image descriptors that perform comparably tostate-of-the art approaches.
arxiv-3300-298 | Dynamic Covariance Models for Multivariate Financial Time Series | http://arxiv.org/pdf/1305.4268v2.pdf | author:Yue Wu, José Miguel Hernández-Lobato, Zoubin Ghahramani category:stat.ME stat.ML published:2013-05-18 summary:The accurate prediction of time-changing covariances is an important problemin the modeling of multivariate financial data. However, some of the mostpopular models suffer from a) overfitting problems and multiple local optima,b) failure to capture shifts in market conditions and c) large computationalcosts. To address these problems we introduce a novel dynamic model fortime-changing covariances. Over-fitting and local optima are avoided byfollowing a Bayesian approach instead of computing point estimates. Changes inmarket conditions are captured by assuming a diffusion process in parametervalues, and finally computationally efficient and scalable inference isperformed using particle filters. Experiments with financial data showexcellent performance of the proposed method with respect to current standardmodels.
arxiv-3300-299 | Robust Text Detection in Natural Scene Images | http://arxiv.org/pdf/1301.2628v3.pdf | author:Xu-Cheng Yin, Xuwang Yin, Kaizhu Huang, Hong-Wei Hao category:cs.CV cs.IR cs.LG I.5.4 published:2013-01-11 summary:Text detection in natural scene images is an important prerequisite for manycontent-based image analysis tasks. In this paper, we propose an accurate androbust method for detecting texts in natural scene images. A fast and effectivepruning algorithm is designed to extract Maximally Stable Extremal Regions(MSERs) as character candidates using the strategy of minimizing regularizedvariations. Character candidates are grouped into text candidates by theingle-link clustering algorithm, where distance weights and threshold of theclustering algorithm are learned automatically by a novel self-trainingdistance metric learning algorithm. The posterior probabilities of textcandidates corresponding to non-text are estimated with an characterclassifier; text candidates with high probabilities are then eliminated andfinally texts are identified with a text classifier. The proposed system isevaluated on the ICDAR 2011 Robust Reading Competition dataset; the f measureis over 76% and is significantly better than the state-of-the-art performanceof 71%. Experimental results on a publicly available multilingual dataset alsoshow that our proposed method can outperform the other competitive method withthe f measure increase of over 9 percent. Finally, we have setup an online demoof our proposed scene text detection system athttp://kems.ustb.edu.cn/learning/yin/dtext.
arxiv-3300-300 | Gaussian Process-Based Decentralized Data Fusion and Active Sensing for Mobility-on-Demand System | http://arxiv.org/pdf/1306.1491v1.pdf | author:Jie Chen, Kian Hsiang Low, Colin Keng-Yan Tan category:cs.RO cs.DC cs.LG cs.MA published:2013-06-02 summary:Mobility-on-demand (MoD) systems have recently emerged as a promisingparadigm of one-way vehicle sharing for sustainable personal urban mobility indensely populated cities. In this paper, we enhance the capability of a MoDsystem by deploying robotic shared vehicles that can autonomously cruise thestreets to be hailed by users. A key challenge to managing the MoD systemeffectively is that of real-time, fine-grained mobility demand sensing andprediction. This paper presents a novel decentralized data fusion and activesensing algorithm for real-time, fine-grained mobility demand sensing andprediction with a fleet of autonomous robotic vehicles in a MoD system. OurGaussian process (GP)-based decentralized data fusion algorithm can achieve afine balance between predictive power and time efficiency. We theoreticallyguarantee its predictive performance to be equivalent to that of asophisticated centralized sparse approximation for the GP model: Thecomputation of such a sparse approximate GP model can thus be distributed amongthe MoD vehicles, hence achieving efficient and scalable demand prediction.Though our decentralized active sensing strategy is devised to gather the mostinformative demand data for demand prediction, it can achieve a dual effect offleet rebalancing to service the mobility demands. Empirical evaluation onreal-world mobility demand data shows that our proposed algorithm can achieve abetter balance between predictive accuracy and time efficiency thanstate-of-the-art algorithms.
