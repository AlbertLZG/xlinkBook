arxiv-11400-1 | Analysis of the South Slavic Scripts by Run-Length Features of the Image Texture | http://arxiv.org/pdf/1507.04908v1.pdf | author:Darko Brodic, Zoran N. Milivojevic, Alessia Amelio category:cs.CV cs.CL published:2015-07-17 summary:The paper proposes an algorithm for the script recognition based on thetexture characteristics. The image texture is achieved by coding each letterwith the equivalent script type (number code) according to its position in thetext line. Each code is transformed into equivalent gray level pixel creatingan 1-D image. Then, the image texture is subjected to the run-length analysis.This analysis extracts the run-length features, which are classified to make adistinction between the scripts under consideration. In the experiment, acustom oriented database is subject to the proposed algorithm. The databaseconsists of some text documents written in Cyrillic, Latin and Glagoliticscripts. Furthermore, it is divided into training and test parts. The resultsof the experiment show that 3 out of 5 run-length features can be used foreffective differentiation between the analyzed South Slavic scripts.
arxiv-11400-2 | Learning Robust Deep Face Representation | http://arxiv.org/pdf/1507.04844v1.pdf | author:Xiang Wu category:cs.CV published:2015-07-17 summary:With the development of convolution neural network, more and more researchersfocus their attention on the advantage of CNN for face recognition task. Inthis paper, we propose a deep convolution network for learning a robust facerepresentation. The deep convolution net is constructed by 4 convolutionlayers, 4 max pooling layers and 2 fully connected layers, which totallycontains about 4M parameters. The Max-Feature-Map activation function is usedinstead of ReLU because the ReLU might lead to the loss of information due tothe sparsity while the Max-Feature-Map can get the compact and discriminativefeature vectors. The model is trained on CASIA-WebFace dataset and evaluated onLFW dataset. The result on LFW achieves 97.77% on unsupervised setting forsingle net.
arxiv-11400-3 | Parallel Magnetic Resonance Imaging | http://arxiv.org/pdf/1501.06209v2.pdf | author:Martin Uecker category:cs.NA cs.CV math.NA math.OC physics.med-ph published:2015-01-25 summary:The main disadvantage of Magnetic Resonance Imaging (MRI) are its long scantimes and, in consequence, its sensitivity to motion. Exploiting thecomplementary information from multiple receive coils, parallel imaging is ableto recover images from under-sampled k-space data and to accelerate themeasurement. Because parallel magnetic resonance imaging can be used toaccelerate basically any imaging sequence it has many important applications.Parallel imaging brought a fundamental shift in image reconstruction: Imagereconstruction changed from a simple direct Fourier transform to the solutionof an ill-conditioned inverse problem. This work gives an overview of imagereconstruction from the perspective of inverse problems. After introducingbasic concepts such as regularization, discretization, and iterativereconstruction, advanced topics are discussed including algorithms forauto-calibration, the connection to approximation theory, and the combinationwith compressed sensing.
arxiv-11400-4 | Multiscale Adaptive Representation of Signals: I. The Basic Framework | http://arxiv.org/pdf/1507.04835v1.pdf | author:Cheng Tai, Weinan E category:cs.CV published:2015-07-17 summary:We introduce a framework for designing multi-scale, adaptive, shift-invariantframes and bi-frames for representing signals. The new framework, calledAdaFrame, improves over dictionary learning-based techniques in terms ofcomputational efficiency at inference time. It improves classical multi-scalebasis such as wavelet frames in terms of coding efficiency. It provides anattractive alternative to dictionary learning-based techniques for low levelsignal processing tasks, such as compression and denoising, as well as highlevel tasks, such as feature extraction for object recognition. Connectionswith deep convolutional networks are also discussed. In particular, theproposed framework reveals a drawback in the commonly used approach forvisualizing the activations of the intermediate layers in convolutionalnetworks, and suggests a natural alternative.
arxiv-11400-5 | Deep Multimodal Speaker Naming | http://arxiv.org/pdf/1507.04831v1.pdf | author:Yongtao Hu, Jimmy Ren, Jingwen Dai, Chang Yuan, Li Xu, Wenping Wang category:cs.CV cs.LG cs.MM cs.SD H.3 published:2015-07-17 summary:Automatic speaker naming is the problem of localizing as well as identifyingeach speaking character in a TV/movie/live show video. This is a challengingproblem mainly attributes to its multimodal nature, namely face cue alone isinsufficient to achieve good performance. Previous multimodal approaches tothis problem usually process the data of different modalities individually andmerge them using handcrafted heuristics. Such approaches work well for simplescenes, but fail to achieve high performance for speakers with large appearancevariations. In this paper, we propose a novel convolutional neural networks(CNN) based learning framework to automatically learn the fusion function ofboth face and audio cues. We show that without using face tracking, faciallandmark localization or subtitle/transcript, our system with robust multimodalfeature extraction is able to achieve state-of-the-art speaker namingperformance evaluated on two diverse TV series. The dataset and implementationof our algorithm are publicly available online.
arxiv-11400-6 | RBIR Based on Signature Graph | http://arxiv.org/pdf/1507.04816v1.pdf | author:Thanh The Van, Thanh Manh Le category:cs.CV published:2015-07-17 summary:This paper approaches the image retrieval system on the base of visualfeatures local region RBIR (region-based image retrieval). First of all, thepaper presents a method for extracting the interest points based onHarris-Laplace to create the feature region of the image. Next, in order toreduce the storage space and speed up query image, the paper builds the binarysignature structure to describe the visual content of image. Based on theimage's binary signature, the paper builds the SG (signature graph) to classifyand store image's binary signatures. Since then, the paper builds the imageretrieval algorithm on SG through the similar measure EMD (earth mover'sdistance) between the image's binary signatures. Last but not least, the papergives an image retrieval model RBIR, experiments and assesses the imageretrieval method on Corel image database over 10,000 images.
arxiv-11400-7 | Lens Factory: Automatic Lens Generation Using Off-the-shelf Components | http://arxiv.org/pdf/1506.08956v2.pdf | author:Libin Sun, Brian Guenter, Neel Joshi, Patrick Therien, James Hays category:cs.GR cs.CV published:2015-06-30 summary:Custom optics is a necessity for many imaging applications. Unfortunately,custom lens design is costly (thousands to tens of thousands of dollars), timeconsuming (10-12 weeks typical lead time), and requires specialized opticsdesign expertise. By using only inexpensive, off-the-shelf lens components theLens Factory automatic design system greatly reduces cost and time. Design,ordering of parts, delivery, and assembly can be completed in a few days, at acost in the low hundreds of dollars. Lens design constraints, such as focallength and field of view, are specified in terms familiar to the graphicscommunity so no optics expertise is necessary. Unlike conventional lens designsystems, which only use continuous optimization methods, Lens Factory adds adiscrete optimization stage. This stage searches the combinatorial space ofpossible combinations of lens elements to find novel designs, evolving simplecanonical lens designs into more complex, better designs. Intelligent pruningrules make the combinatorial search feasible. We have designed and builtseveral high performance optical systems which demonstrate the practicality ofthe system.
arxiv-11400-8 | Exploratory topic modeling with distributional semantics | http://arxiv.org/pdf/1507.04798v1.pdf | author:Samuel RÃ¶nnqvist category:cs.IR cs.CL cs.LG published:2015-07-16 summary:As we continue to collect and store textual data in a multitude of domains,we are regularly confronted with material whose largely unknown thematicstructure we want to uncover. With unsupervised, exploratory analysis, no priorknowledge about the content is required and highly open-ended tasks can besupported. In the past few years, probabilistic topic modeling has emerged as apopular approach to this problem. Nevertheless, the representation of thelatent topics as aggregations of semi-coherent terms limits theirinterpretability and level of detail. This paper presents an alternative approach to topic modeling that mapstopics as a network for exploration, based on distributional semantics usinglearned word vectors. From the granular level of terms and their semanticsimilarity relations global topic structures emerge as clustered regions andgradients of concepts. Moreover, the paper discusses the visual interactiverepresentation of the topic map, which plays an important role in supportingits exploration.
arxiv-11400-9 | Deep Learning and Music Adversaries | http://arxiv.org/pdf/1507.04761v1.pdf | author:Corey Kereliuk, Bob L. Sturm, Jan Larsen category:cs.LG cs.NE cs.SD published:2015-07-16 summary:An adversary is essentially an algorithm intent on making a classificationsystem perform in some particular way given an input, e.g., increase theprobability of a false negative. Recent work builds adversaries for deeplearning systems applied to image object recognition, which exploits theparameters of the system to find the minimal perturbation of the input imagesuch that the network misclassifies it with high confidence. We adapt thisapproach to construct and deploy an adversary of deep learning systems appliedto music content analysis. In our case, however, the input to the systems ismagnitude spectral frames, which requires special care in order to producevalid input audio signals from network-derived perturbations. For two differenttrain-test partitionings of two benchmark datasets, and two different deeparchitectures, we find that this adversary is very effective in defeating theresulting systems. We find the convolutional networks are more robust, however,compared with systems based on a majority vote over individually classifiedaudio frames. Furthermore, we integrate the adversary into the training of newdeep systems, but do not find that this improves their resilience against thesame adversary.
arxiv-11400-10 | Variational Gram Functions: Convex Analysis and Optimization | http://arxiv.org/pdf/1507.04734v1.pdf | author:Amin Jalali, Lin Xiao, Maryam Fazel category:math.OC cs.LG stat.ML published:2015-07-16 summary:We propose a new class of convex penalty functions, called variational Gramfunctions (VGFs), that can promote pairwise relations, such as orthogonality,among a set of vectors in a vector space. When used as regularizers in convexoptimization problems, these functions find application in hierarchicalclassification, multitask learning, and estimation of vectors with disjointsupports, among other applications. We describe a general condition forconvexity, which is then used to prove the convexity of a few known functionsas well as some new ones. We give a characterization of the associatedsubdifferential and the proximal operator, and discuss efficient optimizationalgorithms for some structured regularized loss-minimization problems usingVGFs. Numerical experiments on a hierarchical classification problem are alsopresented that demonstrate the effectiveness of VGFs and the associatedoptimization algorithms in practice.
arxiv-11400-11 | Recursive Sparse Point Process Regression with Application to Spectrotemporal Receptive Field Plasticity Analysis | http://arxiv.org/pdf/1507.04727v1.pdf | author:Alireza Sheikhattar, Jonathan B. Fritz, Shihab A. Shamma, Behtash Babadi category:cs.NE cs.SY math.OC stat.AP stat.CO published:2015-07-16 summary:We consider the problem of estimating the sparse time-varying parametervectors of a point process model in an online fashion, where the observationsand inputs respectively consist of binary and continuous time series. Weconstruct a novel objective function by incorporating a forgetting factormechanism into the point process log-likelihood to enforce adaptivity andemploy $\ell_1$-regularization to capture the sparsity. We provide a rigorousanalysis of the maximizers of the objective function, which extends theguarantees of compressed sensing to our setting. We construct two recursivefilters for online estimation of the parameter vectors based on proximaloptimization techniques, as well as a novel filter for recursive computation ofstatistical confidence regions. Simulation studies reveal that our algorithmsoutperform several existing point process filters in terms of trackability,goodness-of-fit and mean square error. We finally apply our filteringalgorithms to experimentally recorded spiking data from the ferret primaryauditory cortex during attentive behavior in a click rate discrimination task.Our analysis provides new insights into the time-course of the spectrotemporalreceptive field plasticity of the auditory neurons.
arxiv-11400-12 | Iterative Subsampling in Solution Path Clustering of Noisy Big Data | http://arxiv.org/pdf/1412.1559v2.pdf | author:Yuliya Marchetti, Qing Zhou category:stat.ME stat.ML published:2014-12-04 summary:We develop an iterative subsampling approach to improve the computationalefficiency of our previous work on solution path clustering (SPC). The SPCmethod achieves clustering by concave regularization on the pairwise distancesbetween cluster centers. This clustering method has the important capability torecognize noise and to provide a short path of clustering solutions; however,it is not sufficiently fast for big datasets. Thus, we propose a method thatiterates between clustering a small subsample of the full data and sequentiallyassigning the other data points to attain orders of magnitude of computationalsavings. The new method preserves the ability to isolate noise, includes asolution selection mechanism that ultimately provides one clustering solutionwith an estimated number of clusters, and is shown to be able to extract smalltight clusters from noisy data. The method's relatively minor losses inaccuracy are demonstrated through simulation studies, and its ability to handlelarge datasets is illustrated through applications to gene expression datasets.An R package, SPClustering, for the SPC method with iterative subsampling isavailable at http://www.stat.ucla.edu/~zhou/Software.html.
arxiv-11400-13 | A Dependency-Based Neural Network for Relation Classification | http://arxiv.org/pdf/1507.04646v1.pdf | author:Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou, Houfeng Wang category:cs.CL cs.LG cs.NE published:2015-07-16 summary:Previous research on relation classification has verified the effectivenessof using dependency shortest paths or subtrees. In this paper, we furtherexplore how to make full use of the combination of these dependencyinformation. We first propose a new structure, termed augmented dependency path(ADP), which is composed of the shortest dependency path between two entitiesand the subtrees attached to the shortest path. To exploit the semanticrepresentation behind the ADP structure, we develop dependency-based neuralnetworks (DepNN): a recursive neural network designed to model the subtrees,and a convolutional neural network to capture the most important features onthe shortest path. Experiments on the SemEval-2010 dataset show that ourproposed method achieves state-of-art results.
arxiv-11400-14 | Tight Risk Bounds for Multi-Class Margin Classifiers | http://arxiv.org/pdf/1507.03040v2.pdf | author:Yury Maximov, Daria Reshetova category:stat.ML cs.LG published:2015-07-10 summary:We consider a problem of risk estimation for large-margin multi-classclassifiers. We propose a novel risk bound for the multi-class classificationproblem. The bound involves the marginal distribution of the classifier and theRademacher complexity of the hypothesis class. We prove that our bound is tightin the number of classes. Finally, we compare our bound with the related onesand provide a simplified version of the bound for the multi-classclassification with kernel based hypotheses.
arxiv-11400-15 | Constructing Binary Descriptors with a Stochastic Hill Climbing Search | http://arxiv.org/pdf/1501.04782v2.pdf | author:Nenad MarkuÅ¡, Igor S. PandÅ¾iÄ, JÃ¶rgen Ahlberg category:cs.CV published:2015-01-20 summary:Binary descriptors of image patches provide processing speed advantages andrequire less storage than methods that encode the patch appearance with avector of real numbers. We provide evidence that, despite its simplicity, astochastic hill climbing bit selection procedure for descriptor constructiondefeats recently proposed alternatives on a standard discriminative powerbenchmark. The method is easy to implement and understand, has no freeparameters that need fine tuning, and runs fast.
arxiv-11400-16 | Ordinal optimization - empirical large deviations rate estimators, and stochastic multi-armed bandits | http://arxiv.org/pdf/1507.04564v1.pdf | author:Peter Glynn, Sandeep Juneja category:math.PR stat.ML 65C05, 60-08 published:2015-07-16 summary:Consider the ordinal optimization problem of finding a population amongstmany with the smallest mean when these means are unknown but population samplescan be generated via simulation. Typically, by selecting a population with thesmallest sample mean, it can be shown that the false selection probabilitydecays at an exponential rate. Lately researchers have sought algorithms thatguarantee that this probability is restricted to a small $\delta$ in order$\log(1/\delta)$ computational time by estimating the associated largedeviations rate function via simulation. We show that such guarantees aremisleading. Enroute, we identify the large deviations principle followed by theempirically estimated large deviations rate function that may also be ofindependent interest. Further, we show a negative result that when populationshave unbounded support, any policy that asymptotically identifies the correctpopulation with probability at least $1-\delta$ for each problem instancerequires more than $O(\log(1/\delta))$ samples in making such a determinationin any problem instance. This suggests that some restrictions are essential onpopulations to devise $O(\log(1/\delta))$ algorithms with $1 - \delta$correctness guarantees. We note that under restriction on population moments,such methods are easily designed. We also observe that sequential methods fromstochastic multi-armed bandit literature can be adapted to devise suchalgorithms.
arxiv-11400-17 | Upper-Confidence-Bound Algorithms for Active Learning in Multi-Armed Bandits | http://arxiv.org/pdf/1507.04523v1.pdf | author:Alexandra Carpentier, Alessandro Lazaric, Mohammad Ghavamzadeh, RÃ©mi Munos, Peter Auer, AndrÃ¡s Antos category:cs.LG G.3 published:2015-07-16 summary:In this paper, we study the problem of estimating uniformly well the meanvalues of several distributions given a finite budget of samples. If thevariance of the distributions were known, one could design an optimal samplingstrategy by collecting a number of independent samples per distribution that isproportional to their variance. However, in the more realistic case where thedistributions are not known in advance, one needs to design adaptive samplingstrategies in order to select which distribution to sample from according tothe previously observed samples. We describe two strategies based on pullingthe distributions a number of times that is proportional to a high-probabilityupper-confidence-bound on their variance (built from previous observed samples)and report a finite-sample performance analysis on the excess estimation errorcompared to the optimal allocation. We show that the performance of theseallocation strategies depends not only on the variances but also on the fullshape of the distributions.
arxiv-11400-18 | Scalable Gaussian Process Classification via Expectation Propagation | http://arxiv.org/pdf/1507.04513v1.pdf | author:Daniel HernÃ¡ndez-Lobato, JosÃ© Miguel HernÃ¡ndez-Lobato category:stat.ML published:2015-07-16 summary:Variational methods have been recently considered for scaling the trainingprocess of Gaussian process classifiers to large datasets. As an alternative,we describe here how to train these classifiers efficiently using expectationpropagation. The proposed method allows for handling datasets with millions ofdata instances. More precisely, it can be used for (i) training in adistributed fashion where the data instances are sent to different nodes inwhich the required computations are carried out, and for (ii) maximizing anestimate of the marginal likelihood using a stochastic approximation of thegradient. Several experiments indicate that the method described is competitivewith the variational approach.
arxiv-11400-19 | Diagnosing State-Of-The-Art Object Proposal Methods | http://arxiv.org/pdf/1507.04512v1.pdf | author:Hongyuan Zhu, Shijian Lu, Jianfei Cai, Quangqing Lee category:cs.CV published:2015-07-16 summary:Object proposal has become a popular paradigm to replace exhaustive slidingwindow search in current top-performing methods in PASCAL VOC and ImageNet.Recently, Hosang et al. conduct the first unified study of existing methods' interms of various image-level degradations. On the other hand, the vitalquestion "what object-level characteristics really affect existing methods'performance?" is not yet answered. Inspired by Hoiem et al.'s work incategorical object detection, this paper conducts the first meta-analysis ofvarious object-level characteristics' impact on state-of-the-art objectproposal methods. Specifically, we examine the effects of object size, aspectratio, iconic view, color contrast, shape regularity and texture. We alsoanalyse existing methods' localization accuracy and latency for various PASCALVOC object classes. Our study reveals the limitations of existing methods interms of non-iconic view, small object size, low color contrast, shaperegularity etc. Based on our observations, lessons are also learned and sharedwith respect to the selection of existing object proposal technologies as wellas the design of the future ones.
arxiv-11400-20 | On the Convergence of Stochastic Variational Inference in Bayesian Networks | http://arxiv.org/pdf/1507.04505v1.pdf | author:Ulrich Paquet category:stat.ML published:2015-07-16 summary:We highlight a pitfall when applying stochastic variational inference togeneral Bayesian networks. For global random variables approximated by anexponential family distribution, natural gradient steps, commonly starting froma unit length step size, are averaged to convergence. This useful insight intothe scaling of initial step sizes is lost when the approximation factorizesacross a general Bayesian network, and care must be taken to ensure practicalconvergence. We experimentally investigate how much of the baby (well-scaledsteps) is thrown out with the bath water (exact gradients).
arxiv-11400-21 | Towards Predicting First Daily Departure Times: a Gaussian Modeling Approach for Load Shift Forecasting | http://arxiv.org/pdf/1507.04502v1.pdf | author:Nicholas H. Kirk, Ilya Dianov category:cs.LG published:2015-07-16 summary:This work provides two statistical Gaussian forecasting methods forpredicting First Daily Departure Times (FDDTs) of everyday use electricvehicles. This is important in smart grid applications to understanddisconnection times of such mobile storage units, for instance to forecaststorage of non dispatchable loads (e.g. wind and solar power). We provide areview of the relevant state-of-the-art driving behavior features towards FDDTprediction, to then propose an approximated Gaussian method which qualitativelyforecasts how many vehicles will depart within a given time frame, by assumingthat departure times follow a normal distribution. This method considerssampling sessions as Poisson distributions which are superimposed to obtain asingle approximated Gaussian model. Given the Gaussian distribution assumptionof the departure times, we also model the problem with Gaussian Mixture Models(GMM), in which the priorly set number of clusters represents the desired timegranularity. Evaluation has proven that for the dataset tested, low error andhigh confidence ($\approx 95\%$) is possible for 15 and 10 minute intervals,and that GMM outperforms traditional modeling but is less generalizable acrossdatasets, as it is a closer fit to the sampling data. Conclusively we discussfuture possibilities and practical applications of the discussed model.
arxiv-11400-22 | Massively Parallel Methods for Deep Reinforcement Learning | http://arxiv.org/pdf/1507.04296v2.pdf | author:Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro De Maria, Vedavyas Panneershelvam, Mustafa Suleyman, Charles Beattie, Stig Petersen, Shane Legg, Volodymyr Mnih, Koray Kavukcuoglu, David Silver category:cs.LG cs.AI cs.DC cs.NE published:2015-07-15 summary:We present the first massively distributed architecture for deepreinforcement learning. This architecture uses four main components: parallelactors that generate new behaviour; parallel learners that are trained fromstored experience; a distributed neural network to represent the value functionor behaviour policy; and a distributed store of experience. We used ourarchitecture to implement the Deep Q-Network algorithm (DQN). Our distributedalgorithm was applied to 49 games from Atari 2600 games from the ArcadeLearning Environment, using identical hyperparameters. Our performancesurpassed non-distributed DQN in 41 of the 49 games and also reduced thewall-time required to achieve these results by an order of magnitude on mostgames.
arxiv-11400-23 | How to Center Binary Deep Boltzmann Machines | http://arxiv.org/pdf/1311.1354v3.pdf | author:Jan Melchior, Asja Fischer, Laurenz Wiskott category:stat.ML cs.LG published:2013-11-06 summary:This work analyzes centered binary Restricted Boltzmann Machines (RBMs) andbinary Deep Boltzmann Machines (DBMs), where centering is done by subtractingoffset values from visible and hidden variables. We show analytically that (i)centering results in a different but equivalent parameterization for artificialneural networks in general, (ii) the expected performance of centered binaryRBMs/DBMs is invariant under simultaneous flip of data and offsets, for anyoffset value in the range of zero to one, (iii) centering can be reformulatedas a different update rule for normal binary RBMs/DBMs, and (iv) using theenhanced gradient is equivalent to setting the offset values to the averageover model and data mean. Furthermore, numerical simulations suggest that (i)optimal generative performance is achieved by subtracting mean values fromvisible as well as hidden variables, (ii) centered RBMs/DBMs reachsignificantly higher log-likelihood values than normal binary RBMs/DBMs, (iii)centering variants whose offsets depend on the model mean, like the enhancedgradient, suffer from severe divergence problems, (iv) learning is stabilizedif an exponentially moving average over the batch means is used for the offsetvalues instead of the current batch mean, which also prevents the enhancedgradient from diverging, (v) centered RBMs/DBMs reach higher LL values thannormal RBMs/DBMs while having a smaller norm of the weight matrix, (vi)centering leads to an update direction that is closer to the natural gradientand that the natural gradient is extremly efficient for training RBMs, (vii)centering dispense the need for greedy layer-wise pre-training of DBMs, (viii)furthermore we show that pre-training often even worsen the resultsindependently whether centering is used or not, and (ix) centering is alsobeneficial for auto encoders.
arxiv-11400-24 | Preference Completion: Large-scale Collaborative Ranking from Pairwise Comparisons | http://arxiv.org/pdf/1507.04457v1.pdf | author:Dohyung Park, Joe Neeman, Jin Zhang, Sujay Sanghavi, Inderjit S. Dhillon category:stat.ML cs.LG published:2015-07-16 summary:In this paper we consider the collaborative ranking setting: a pool of userseach provides a small number of pairwise preferences between $d$ possibleitems; from these we need to predict preferences of the users for items theyhave not yet seen. We do so by fitting a rank $r$ score matrix to the pairwisedata, and provide two main contributions: (a) we show that an algorithm basedon convex optimization provides good generalization guarantees once each userprovides as few as $O(r\log^2 d)$ pairwise comparisons -- essentially matchingthe sample complexity required in the related matrix completion setting (whichuses actual numerical as opposed to pairwise information), and (b) we develop alarge-scale non-convex implementation, which we call AltSVM, that trains afactored form of the matrix via alternating minimization (which we show reducesto alternating SVM problems), and scales and parallelizes very well to largeproblem settings. It also outperforms common baselines on many moderately largepopular collaborative filtering datasets in both NDCG and in other measures ofranking performance.
arxiv-11400-25 | A Deep Hashing Learning Network | http://arxiv.org/pdf/1507.04437v1.pdf | author:Guoqiang Zhong, Pan Yang, Sijiang Wang, Junyu Dong category:cs.CV published:2015-07-16 summary:Hashing-based methods seek compact and efficient binary codes that preservethe neighborhood structure in the original data space. For most existinghashing methods, an image is first encoded as a vector of hand-crafted visualfeature, followed by a hash projection and quantization step to get the compactbinary vector. Most of the hand-crafted features just encode the low-levelinformation of the input, the feature may not preserve the semanticsimilarities of images pairs. Meanwhile, the hashing function learning processis independent with the feature representation, so the feature may not beoptimal for the hashing projection. In this paper, we propose a supervisedhashing method based on a well designed deep convolutional neural network,which tries to learn hashing code and compact representations of datasimultaneously. The proposed model learn the binary codes by adding a compactsigmoid layer before the loss layer. Experiments on several image data setsshow that the proposed model outperforms other state-of-the-art methods.
arxiv-11400-26 | Joint Tensor Factorization and Outlying Slab Suppression with Applications | http://arxiv.org/pdf/1507.04436v1.pdf | author:Xiao Fu, Kejun Huang, Wing-Kin Ma, Nicholas D. Sidiropoulos, Rasmus Bro category:stat.ML published:2015-07-16 summary:We consider factoring low-rank tensors in the presence of outlying slabs.This problem is important in practice, because data collected in manyreal-world applications, such as speech, fluorescence, and some social networkdata, fit this paradigm. Prior work tackles this problem by iterativelyselecting a fixed number of slabs and fitting, a procedure which may notconverge. We formulate this problem from a group-sparsity promoting point ofview, and propose an alternating optimization framework to handle thecorresponding $\ell_p$ ($0<p\leq 1$) minimization-based low-rank tensorfactorization problem. The proposed algorithm features a similar per-iterationcomplexity as the plain trilinear alternating least squares (TALS) algorithm.Convergence of the proposed algorithm is also easy to analyze under theframework of alternating optimization and its variants. In addition,regularization and constraints can be easily incorporated to make use of\emph{a priori} information on the latent loading factors. Simulations and realdata experiments on blind speech separation, fluorescence data analysis, andsocial network mining are used to showcase the effectiveness of the proposedalgorithm.
arxiv-11400-27 | Certifying and removing disparate impact | http://arxiv.org/pdf/1412.3756v3.pdf | author:Michael Feldman, Sorelle Friedler, John Moeller, Carlos Scheidegger, Suresh Venkatasubramanian category:stat.ML cs.CY published:2014-12-11 summary:What does it mean for an algorithm to be biased? In U.S. law, unintentionalbias is encoded via disparate impact, which occurs when a selection process haswidely different outcomes for different groups, even as it appears to beneutral. This legal determination hinges on a definition of a protected class(ethnicity, gender, religious practice) and an explicit description of theprocess. When the process is implemented using computers, determining disparate impact(and hence bias) is harder. It might not be possible to disclose the process.In addition, even if the process is open, it might be hard to elucidate in alegal setting how the algorithm makes its decisions. Instead of requiringaccess to the algorithm, we propose making inferences based on the data thealgorithm uses. We make four contributions to this problem. First, we link the legal notionof disparate impact to a measure of classification accuracy that while known,has received relatively little attention. Second, we propose a test fordisparate impact based on analyzing the information leakage of the protectedclass from the other data attributes. Third, we describe methods by which datamight be made unbiased. Finally, we present empirical evidence supporting theeffectiveness of our test for disparate impact and our approach for bothmasking bias and preserving relevant information in the data. Interestingly,our approach resembles some actual selection practices that have recentlyreceived legal scrutiny.
arxiv-11400-28 | Bias and population structure in the actuation of sound change | http://arxiv.org/pdf/1507.04420v1.pdf | author:James Kirby, Morgan Sonderegger category:cs.CL physics.soc-ph published:2015-07-16 summary:Why do human languages change at some times, and not others? We address thislongstanding question from a computational perspective, focusing on the case ofsound change. Sound change arises from the pronunciation variability ubiquitousin every speech community, but most such variability does not lead to change.Hence, an adequate model must allow for stability as well as change. Existingtheories of sound change tend to emphasize factors at the level of individuallearners promoting one outcome or the other, such as channel bias (which favorschange) or inductive bias (which favors stability). Here, we consider how theinteraction of these biases can lead to both stability and change in apopulation setting. We find that population structure itself can act as asource of stability, but that both stability and change are possible only whenboth types of bias are active, suggesting that it is possible to understand whysound change occurs at some times and not others as the population-level resultof the interplay between forces promoting each outcome in individual speakers.In addition, if it is assumed that learners learn from two or more teachers,the transition from stability to change is marked by a phase transition,consistent with the abrupt transitions seen in many empirical cases of soundchange. The predictions of multiple-teacher models thus match empirical casesof sound change better than the predictions of single-teacher models,underscoring the importance of modeling language change in a populationsetting.
arxiv-11400-29 | Parallel MMF: a Multiresolution Approach to Matrix Computation | http://arxiv.org/pdf/1507.04396v1.pdf | author:Risi Kondor, Nedelina Teneva, Pramod K. Mudrakarta category:cs.NA cs.LG stat.ML published:2015-07-15 summary:Multiresolution Matrix Factorization (MMF) was recently introduced as amethod for finding multiscale structure and defining wavelets ongraphs/matrices. In this paper we derive pMMF, a parallel algorithm forcomputing the MMF factorization. Empirically, the running time of pMMF scaleslinearly in the dimension for sparse matrices. We argue that this makes pMMF avaluable new computational primitive in its own right, and present experimentson using pMMF for two distinct purposes: compressing matrices andpreconditioning large sparse linear systems.
arxiv-11400-30 | A Generalized Kernel Approach to Structured Output Learning | http://arxiv.org/pdf/1205.2171v2.pdf | author:Hachem Kadri, Mohammad Ghavamzadeh, Philippe Preux category:stat.ML cs.LG published:2012-05-10 summary:We study the problem of structured output learning from a regressionperspective. We first provide a general formulation of the kernel dependencyestimation (KDE) problem using operator-valued kernels. We show that some ofthe existing formulations of this problem are special cases of our framework.We then propose a covariance-based operator-valued kernel that allows us totake into account the structure of the kernel feature space. This kerneloperates on the output space and encodes the interactions between the outputswithout any reference to the input space. To address this issue, we introduce avariant of our KDE method based on the conditional covariance operator that inaddition to the correlation between the outputs takes into account the effectsof the input variables. Finally, we evaluate the performance of our KDEapproach using both covariance and conditional covariance kernels on twostructured output problems, and compare it to the state-of-the-art kernel-basedstructured output regression methods.
arxiv-11400-31 | Learning Boolean functions with concentrated spectra | http://arxiv.org/pdf/1507.04319v1.pdf | author:Dustin G. Mixon, Jesse Peterson category:cs.LG cs.IT math.FA math.IT published:2015-07-15 summary:This paper discusses the theory and application of learning Boolean functionsthat are concentrated in the Fourier domain. We first estimate the VC dimensionof this function class in order to establish a small sample complexity oflearning in this case. Next, we propose a computationally efficient method ofempirical risk minimization, and we apply this method to the MNIST database ofhandwritten digits. These results demonstrate the effectiveness of our modelfor modern classification tasks. We conclude with a list of open problems forfuture investigation.
arxiv-11400-32 | Learning Action Models: Qualitative Approach | http://arxiv.org/pdf/1507.04285v1.pdf | author:Thomas Bolander, Nina Gierasimczuk category:cs.LG cs.AI cs.LO published:2015-07-15 summary:In dynamic epistemic logic, actions are described using action models. Inthis paper we introduce a framework for studying learnability of action modelsfrom observations. We present first results concerning propositional actionmodels. First we check two basic learnability criteria: finite identifiability(conclusively inferring the appropriate action model in finite time) andidentifiability in the limit (inconclusive convergence to the right actionmodel). We show that deterministic actions are finitely identifiable, whilenon-deterministic actions require more learning power-they are identifiable inthe limit. We then move on to a particular learning method, which proceeds viarestriction of a space of events within a learning-specific action model. Thisway of learning closely resembles the well-known update method from dynamicepistemic logic. We introduce several different learning methods suited forfinite identifiability of particular types of deterministic actions.
arxiv-11400-33 | The Role of Principal Angles in Subspace Classification | http://arxiv.org/pdf/1507.04230v1.pdf | author:Jiaji Huang, Qiang Qiu, Robert Calderbank category:stat.ML cs.LG published:2015-07-15 summary:Subspace models play an important role in a wide range of signal processingtasks, and this paper explores how the pairwise geometry of subspacesinfluences the probability of misclassification. When the mismatch between thesignal and the model is vanishingly small, the probability of misclassificationis determined by the product of the sines of the principal angles betweensubspaces. When the mismatch is more significant, the probability ofmisclassification is determined by the sum of the squares of the sines of theprincipal angles. Reliability of classification is derived in terms of thedistribution of signal energy across principal vectors. Larger principal angleslead to smaller classification error, motivating a linear transform thatoptimizes principal angles. The transform presented here (TRAIT) preserves somespecific characteristic of each individual class, and this approach is shown tobe complementary to a previously developed transform (LRT) that enlargesinter-class distance while suppressing intra-class dispersion. Theoreticalresults are supported by demonstration of superior classification accuracy onsynthetic and measured data even in the presence of significant model mismatch.
arxiv-11400-34 | Associative Measures and Multi-word Unit Extraction in Turkish | http://arxiv.org/pdf/1507.04214v1.pdf | author:Umit Mersinli category:cs.CL published:2015-07-15 summary:Associative measures are "mathematical formulas determining the strength ofassociation between two or more words based on their occurrences andcooccurrences in a text corpus" (Pecina, 2010, p. 138). The purpose of thispaper is to test the 12 associative measures that Text-NSP (Banerjee &Pedersen, 2003) contains on a 10-million-word subcorpus of Turkish NationalCorpus (TNC) (Aksan et.al., 2012). A statistical comparison of those measuresis out of the scope of the study, and the measures will be evaluated accordingto the linguistic relevance of the rankings they provide. The focus of thestudy is basically on optimizing the corpus data, before applying the measuresand then, evaluating the rankings produced by these measures as a whole, not onthe linguistic relevance of individual n-grams. The findings includeintra-linguistically relevant associative measures for a comma delimited,sentence splitted, lower-cased, well-balanced, representative, 10-million-wordcorpus of Turkish.
arxiv-11400-35 | Bayesian Modeling with Gaussian Processes using the GPstuff Toolbox | http://arxiv.org/pdf/1206.5754v6.pdf | author:Jarno Vanhatalo, Jaakko RiihimÃ¤ki, Jouni Hartikainen, Pasi JylÃ¤nki, Ville Tolvanen, Aki Vehtari category:stat.ML cs.AI cs.MS published:2012-06-25 summary:Gaussian processes (GP) are powerful tools for probabilistic modelingpurposes. They can be used to define prior distributions over latent functionsin hierarchical Bayesian models. The prior over functions is defined implicitlyby the mean and covariance function, which determine the smoothness andvariability of the function. The inference can then be conducted directly inthe function space by evaluating or approximating the posterior process.Despite their attractive theoretical properties GPs provide practicalchallenges in their implementation. GPstuff is a versatile collection ofcomputational tools for GP models compatible with Linux and Windows MATLAB andOctave. It includes, among others, various inference methods, sparseapproximations and tools for model assessment. In this work, we review thesetools and demonstrate the use of GPstuff in several models.
arxiv-11400-36 | Predictive Entropy Search for Bayesian Optimization with Unknown Constraints | http://arxiv.org/pdf/1502.05312v2.pdf | author:JosÃ© Miguel HernÃ¡ndez-Lobato, Michael A. Gelbart, Matthew W. Hoffman, Ryan P. Adams, Zoubin Ghahramani category:stat.ML published:2015-02-18 summary:Unknown constraints arise in many types of expensive black-box optimizationproblems. Several methods have been proposed recently for performing Bayesianoptimization with constraints, based on the expected improvement (EI)heuristic. However, EI can lead to pathologies when used with constraints. Forexample, in the case of decoupled constraints---i.e., when one canindependently evaluate the objective or the constraints---EI can encounter apathology that prevents exploration. Additionally, computing EI requires acurrent best solution, which may not exist if none of the data collected so farsatisfy the constraints. By contrast, information-based approaches do notsuffer from these failure modes. In this paper, we present a newinformation-based method called Predictive Entropy Search with Constraints(PESC). We analyze the performance of PESC and show that it compares favorablyto EI-based approaches on synthetic and benchmark problems, as well as severalreal-world examples. We demonstrate that PESC is an effective algorithm thatprovides a promising direction towards a unified solution for constrainedBayesian optimization.
arxiv-11400-37 | ALEVS: Active Learning by Statistical Leverage Sampling | http://arxiv.org/pdf/1507.04155v1.pdf | author:Cem Orhan, Ãznur TaÅtan category:cs.LG stat.ML published:2015-07-15 summary:Active learning aims to obtain a classifier of high accuracy by using fewerlabel requests in comparison to passive learning by selecting effectivequeries. Many active learning methods have been developed in the past twodecades, which sample queries based on informativeness or representativeness ofunlabeled data points. In this work, we explore a novel querying criterionbased on statistical leverage scores. The statistical leverage scores of a rowin a matrix are the squared row-norms of the matrix containing its (top) leftsingular vectors and is a measure of influence of the row on the matrix.Leverage scores have been used for detecting high influential points inregression diagnostics and have been recently shown to be useful for dataanalysis and randomized low-rank matrix approximation algorithms. We explorehow sampling data instances with high statistical leverage scores perform inactive learning. Our empirical comparison on several binary classificationdatasets indicate that querying high leverage points is an effective strategy.
arxiv-11400-38 | Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks | http://arxiv.org/pdf/1502.05336v2.pdf | author:JosÃ© Miguel HernÃ¡ndez-Lobato, Ryan P. Adams category:stat.ML published:2015-02-18 summary:Large multilayer neural networks trained with backpropagation have recentlyachieved state-of-the-art results in a wide range of problems. However, usingbackprop for neural net learning still has some disadvantages, e.g., having totune a large number of hyperparameters to the data, lack of calibratedprobabilistic predictions, and a tendency to overfit the training data. Inprinciple, the Bayesian approach to learning neural networks does not havethese problems. However, existing Bayesian techniques lack scalability to largedataset and network sizes. In this work we present a novel scalable method forlearning Bayesian neural networks, called probabilistic backpropagation (PBP).Similar to classical backpropagation, PBP works by computing a forwardpropagation of probabilities through the network and then doing a backwardcomputation of gradients. A series of experiments on ten real-world datasetsshow that PBP is significantly faster than other techniques, while offeringcompetitive predictive abilities. Our experiments also show that PBP providesaccurate estimates of the posterior variance on the network weights.
arxiv-11400-39 | Factors of Transferability for a Generic ConvNet Representation | http://arxiv.org/pdf/1406.5774v3.pdf | author:Hossein Azizpour, Ali Sharif Razavian, Josephine Sullivan, Atsuto Maki, Stefan Carlsson category:cs.CV published:2014-06-22 summary:Evidence is mounting that Convolutional Networks (ConvNets) are the mosteffective representation learning method for visual recognition tasks. In thecommon scenario, a ConvNet is trained on a large labeled dataset (source) andthe feed-forward units activation of the trained network, at a certain layer ofthe network, is used as a generic representation of an input image for a taskwith relatively smaller training set (target). Recent studies have shown thisform of representation transfer to be suitable for a wide range of targetvisual recognition tasks. This paper introduces and investigates severalfactors affecting the transferability of such representations. It includesparameters for training of the source ConvNet such as its architecture,distribution of the training data, etc. and also the parameters of featureextraction such as layer of the trained ConvNet, dimensionality reduction, etc.Then, by optimizing these factors, we show that significant improvements can beachieved on various (17) visual recognition tasks. We further show that thesevisual recognition tasks can be categorically ordered based on their distancefrom the source task such that a correlation between the performance of tasksand their distance from the source task w.r.t. the proposed factors isobserved.
arxiv-11400-40 | Revisiting AdaBoost for Cost-Sensitive Classification. Part II: Empirical Analysis | http://arxiv.org/pdf/1507.04126v1.pdf | author:Iago Landesa-VÃ¡zquez, JosÃ© Luis Alba-Castro category:cs.CV cs.AI cs.LG published:2015-07-15 summary:A lot of approaches, each following a different strategy, have been proposedin the literature to provide AdaBoost with cost-sensitive properties. In thefirst part of this series of two papers, we have presented these algorithms ina homogeneous notational framework, proposed a clustering scheme for them andperformed a thorough theoretical analysis of those approaches with a fullytheoretical foundation. The present paper, in order to complete our analysis,is focused on the empirical study of all the algorithms previously presentedover a wide range of heterogeneous classification problems. The results of ourexperiments, confirming the theoretical conclusions, seem to reveal that thesimplest approach, just based on cost-sensitive weight initialization, is theone showing the best and soundest results, despite having been recurrentlyoverlooked in the literature.
arxiv-11400-41 | Revisiting AdaBoost for Cost-Sensitive Classification. Part I: Theoretical Perspective | http://arxiv.org/pdf/1507.04125v1.pdf | author:Iago Landesa-VÃ¡zquez, JosÃ© Luis Alba-Castro category:cs.CV cs.AI cs.LG published:2015-07-15 summary:Boosting algorithms have been widely used to tackle a plethora of problems.In the last few years, a lot of approaches have been proposed to providestandard AdaBoost with cost-sensitive capabilities, each with a differentfocus. However, for the researcher, these algorithms shape a tangled set withdiffuse differences and properties, lacking a unifying analysis to jointlycompare, classify, evaluate and discuss those approaches on a common basis. Inthis series of two papers we aim to revisit the various proposals, both fromtheoretical (Part I) and practical (Part II) perspectives, in order to analyzetheir specific properties and behavior, with the final goal of identifying thealgorithm providing the best and soundest results.
arxiv-11400-42 | On the Computability of Solomonoff Induction and Knowledge-Seeking | http://arxiv.org/pdf/1507.04124v1.pdf | author:Jan Leike, Marcus Hutter category:cs.AI cs.LG published:2015-07-15 summary:Solomonoff induction is held as a gold standard for learning, but it is knownto be incomputable. We quantify its incomputability by placing various flavorsof Solomonoff's prior M in the arithmetical hierarchy. We also derivecomputability bounds for knowledge-seeking agents, and give a limit-computableweakly asymptotically optimal reinforcement learning agent.
arxiv-11400-43 | Solomonoff Induction Violates Nicod's Criterion | http://arxiv.org/pdf/1507.04121v1.pdf | author:Jan Leike, Marcus Hutter category:cs.LG cs.AI math.ST stat.TH published:2015-07-15 summary:Nicod's criterion states that observing a black raven is evidence for thehypothesis H that all ravens are black. We show that Solomonoff induction doesnot satisfy Nicod's criterion: there are time steps in which observing blackravens decreases the belief in H. Moreover, while observing any computableinfinite string compatible with H, the belief in H decreases infinitely oftenwhen using the unnormalized Solomonoff prior, but only finitely often whenusing the normalized Solomonoff prior. We argue that the fault is not withSolomonoff induction; instead we should reject Nicod's criterion.
arxiv-11400-44 | Language discrimination and clustering via a neural network approach | http://arxiv.org/pdf/1507.04116v1.pdf | author:Angelo Mariano, Giorgio Parisi, Saverio Pascazio category:cs.CL cs.NE physics.soc-ph published:2015-07-15 summary:We classify twenty-one Indo-European languages starting from written text. Weuse neural networks in order to define a distance among different languages,construct a dendrogram and analyze the ultrametric structure that emerges. Fouror five subgroups of languages are identified, according to the "cut" of thedendrogram, drawn with an entropic criterion. The results and the method arediscussed.
arxiv-11400-45 | Unsupervised Decision Forest for Data Clustering and Density Estimation | http://arxiv.org/pdf/1507.04060v1.pdf | author:Hayder Albehadili, Naz Islam category:cs.CV published:2015-07-15 summary:An algorithm to improve performance parameter for unsupervised decisionforest clustering and density estimation is presented. Specifically, a dualassignment parameter is introduced as a density estimator by combining RandomForest and Gaussian Mixture Model. The Random Forest method has beenspecifically applied to construct a robust affinity graph that providesinformation on the underlying structure of data objects used in clustering. Theproposed algorithm differs from the commonly used spectral clustering methodswhere the computed distance metric is used to find similarities between datapoints. Experiments were conducted using five datasets. A comparison with sixother state-of-the-art methods shows that our model is superior to existingapproaches. Efficiency of the proposed model is in capturing the underlyingstructure for a given set of data points. The proposed method is also robust,and can discriminate between the complex features of data points amongdifferent clusters.
arxiv-11400-46 | Online Transfer Learning in Reinforcement Learning Domains | http://arxiv.org/pdf/1507.00436v2.pdf | author:Yusen Zhan, Matthew E. Taylor category:cs.AI cs.LG I.2.11; I.2.6 published:2015-07-02 summary:This paper proposes an online transfer framework to capture the interactionamong agents and shows that current transfer learning in reinforcement learningis a special case of online transfer. Furthermore, this paper re-characterizesexisting agents-teaching-agents methods as online transfer and analyze one suchteaching method in three ways. First, the convergence of Q-learning and Sarsawith tabular representation with a finite budget is proven. Second, theconvergence of Q-learning and Sarsa with linear function approximation isestablished. Third, the we show the asymptotic performance cannot be hurtthrough teaching. Additionally, all theoretical results are empiricallyvalidated.
arxiv-11400-47 | Training artificial neural networks to learn a nondeterministic game | http://arxiv.org/pdf/1507.04029v1.pdf | author:Thomas E. Portegys category:cs.LG published:2015-07-14 summary:It is well known that artificial neural networks (ANNs) can learndeterministic automata. Learning nondeterministic automata is another matter.This is important because much of the world is nondeterministic, taking theform of unpredictable or probabilistic events that must be acted upon. If ANNsare to engage such phenomena, then they must be able to learn how to deal withnondeterminism. In this project the game of Pong poses a nondeterministicenvironment. The learner is given an incomplete view of the game state andunderlying deterministic physics, resulting in a nondeterministic game. Threemodels were trained and tested on the game: Mona, Elman, and Numenta's NuPIC.
arxiv-11400-48 | Feature Normalisation for Robust Speech Recognition | http://arxiv.org/pdf/1507.04019v1.pdf | author:D. S. Pavan Kumar category:cs.CL cs.SD published:2015-07-14 summary:Speech recognition system performance degrades in noisy environments. If theacoustic models are built using features of clean utterances, the features of anoisy test utterance would be acoustically mismatched with the trained model.This gives poor likelihoods and poor recognition accuracy. Model adaptation andfeature normalisation are two broad areas that address this problem. While theformer often gives better performance, the latter involves estimation of lessernumber of parameters, making the system feasible for practical implementations. This research focuses on the efficacies of various subspace, statistical andstereo based feature normalisation techniques. A subspace projection basedmethod has been investigated as a standalone and adjunct technique involvingreconstruction of noisy speech features from a precomputed set of clean speechbuilding-blocks. The building blocks are learned using non-negative matrixfactorisation (NMF) on log-Mel filter bank coefficients, which form a basis forthe clean speech subspace. The work provides a detailed study on how the methodcan be incorporated into the extraction process of Mel-frequency cepstralcoefficients. Experimental results show that the new features are robust tonoise, and achieve better results when combined with the existing techniques. The work also proposes a modification to the training process of SPLICEalgorithm for noise robust speech recognition. It is based on featurecorrelations, and enables this stereo-based algorithm to improve theperformance in all noise conditions, especially in unseen cases. Further, themodified framework is extended to work for non-stereo datasets where clean andnoisy training utterances, but not stereo counterparts, are required. AnMLLR-based computationally efficient run-time noise adaptation method in SPLICEframework has been proposed.
arxiv-11400-49 | Structure and inference in annotated networks | http://arxiv.org/pdf/1507.04001v1.pdf | author:M. E. J. Newman, Aaron Clauset category:cs.SI physics.soc-ph stat.ML published:2015-07-14 summary:For many networks of scientific interest we know both the connections of thenetwork and information about the network nodes, such as the age or gender ofindividuals in a social network, geographic location of nodes in the Internet,or cellular function of nodes in a gene regulatory network. Here we demonstratehow this "metadata" can be used to improve our analysis and understanding ofnetwork structure. We focus in particular on the problem of community detectionin networks and develop a mathematically principled approach that combines anetwork and its metadata to detect communities more accurately than can be donewith either alone. Crucially, the method does not assume that the metadata arecorrelated with the communities we are trying to find. Instead the methodlearns whether a correlation exists and correctly uses or ignores the metadatadepending on whether they contain useful information. The learned correlationsare also of interest in their own right, allowing us to make predictions aboutthe community membership of nodes whose network connections are unknown. Wedemonstrate our method on synthetic networks with known structure and onreal-world networks, large and small, drawn from social, biological, andtechnological domains.
arxiv-11400-50 | Compressive Imaging and Characterization of Sparse Light Deflection Maps | http://arxiv.org/pdf/1406.6425v2.pdf | author:Prasad Sudhakar, Laurent Jacques, Xavier Dubois, Philippe Antoine, Luc Joannes category:cs.CV published:2014-06-25 summary:Light rays incident on a transparent object of uniform refractive indexundergo deflections, which uniquely characterize the surface geometry of theobject. Associated with each point on the surface is a deflection map (orspectrum) which describes the pattern of deflections in various directions.This article presents a novel method to efficiently acquire and reconstructsparse deflection spectra induced by smooth object surfaces. To this end, weleverage the framework of Compressed Sensing (CS) in a particularimplementation of a schlieren deflectometer, i.e., an optical system providinglinear measurements of deflection spectra with programmable spatial lightmodulation patterns. We design those modulation patterns on the principle ofspread spectrum CS for reducing the number of observations. The ability of ourdevice to simultaneously observe the deflection spectra on a densediscretization of the object surface is related to a Multiple MeasurementVector (MMV) model. This scheme allows us to estimate both the noise power andthe instrumental point spread function. We formulate the spectrum reconstruction task as the solving of a linearinverse problem regularized by an analysis sparsity prior using a translationinvariant wavelet frame. Our results demonstrate the capability and advantagesof using a CS based approach for deflectometric imaging both on simulated dataand experimental deflectometric data. Finally, the paper presents an extension of our method showing how we canextract the main deflection direction in each point of the object surface froma few compressive measurements, without needing any costly reconstructionprocedures. This compressive characterization is then confirmed withexperimental results on simple plano-convex and multifocal intra-ocular lensesstudying the evolution of the main deflection as a function of the object pointlocation.
arxiv-11400-51 | Efficient Regression in Metric Spaces via Approximate Lipschitz Extension | http://arxiv.org/pdf/1111.4470v2.pdf | author:Lee-Ad Gottlieb, Aryeh Kontorovich, Robert Krauthgamer category:cs.LG published:2011-11-18 summary:We present a framework for performing efficient regression in general metricspaces. Roughly speaking, our regressor predicts the value at a new point bycomputing a Lipschitz extension --- the smoothest function consistent with theobserved data --- after performing structural risk minimization to avoidoverfitting. We obtain finite-sample risk bounds with minimal structural andnoise assumptions, and a natural speed-precision tradeoff. The offline(learning) and online (prediction) stages can be solved by convex programming,but this naive approach has runtime complexity $O(n^3)$, which is prohibitivefor large datasets. We design instead a regression algorithm whose speed andgeneralization performance depend on the intrinsic dimension of the data, towhich the algorithm adapts. While our main innovation is algorithmic, thestatistical results may also be of independent interest.
arxiv-11400-52 | Practical Inexact Proximal Quasi-Newton Method with Global Complexity Analysis | http://arxiv.org/pdf/1311.6547v4.pdf | author:Katya Scheinberg, Xiaocheng Tang category:cs.LG math.OC stat.ML published:2013-11-26 summary:Recently several methods were proposed for sparse optimization which makecareful use of second-order information [10, 28, 16, 3] to improve localconvergence rates. These methods construct a composite quadratic approximationusing Hessian information, optimize this approximation using a first-ordermethod, such as coordinate descent and employ a line search to ensuresufficient descent. Here we propose a general framework, which includesslightly modified versions of existing algorithms and also a new algorithm,which uses limited memory BFGS Hessian approximations, and provide a novelglobal convergence rate analysis, which covers methods that solve subproblemsvia coordinate descent.
arxiv-11400-53 | An SVM-like Approach for Expectile Regression | http://arxiv.org/pdf/1507.03887v1.pdf | author:Muhammad Farooq, Ingo Steinwart category:stat.CO stat.ML published:2015-07-14 summary:Expectile regression is a nice tool for investigating conditionaldistributions beyond the conditional mean. It is well-known that expectiles canbe described with the help of the asymmetric least square loss function, andthis link makes it possible to estimate expectiles in a non-parametricframework by a support vector machine like approach. In this work we develop anefficient sequential-minimal-optimization-based solver for the underlyingoptimization problem. The behavior of the solver is investigated by conductingvarious experiments and the results are compared with the recent R-packageER-Boost.
arxiv-11400-54 | Rich Component Analysis | http://arxiv.org/pdf/1507.03867v1.pdf | author:Rong Ge, James Zou category:cs.LG stat.ML published:2015-07-14 summary:In many settings, we have multiple data sets (also called views) that capturedifferent and overlapping aspects of the same phenomenon. We are ofteninterested in finding patterns that are unique to one or to a subset of theviews. For example, we might have one set of molecular observations and one setof physiological observations on the same group of individuals, and we want toquantify molecular patterns that are uncorrelated with physiology. Despitebeing a common problem, this is highly challenging when the correlations comefrom complex distributions. In this paper, we develop the general framework ofRich Component Analysis (RCA) to model settings where the observations fromdifferent views are driven by different sets of latent components, and eachcomponent can be a complex, high-dimensional distribution. We introducealgorithms based on cumulant extraction that provably learn each of thecomponents without having to model the other components. We show how tointegrate RCA with stochastic gradient descent into a meta-algorithm forlearning general models, and demonstrate substantial improvement in accuracy onseveral synthetic and real datasets in both supervised and unsupervised tasks.Our method makes it possible to learn latent variable models when we don't havesamples from the true model but only samples after complex perturbations.
arxiv-11400-55 | Ensemble of Hankel Matrices for Face Emotion Recognition | http://arxiv.org/pdf/1507.03811v1.pdf | author:Liliana Lo Presti, Marco La Cascia category:cs.CV cs.HC cs.RO published:2015-07-14 summary:In this paper, a face emotion is considered as the result of the compositionof multiple concurrent signals, each corresponding to the movements of aspecific facial muscle. These concurrent signals are represented by means of aset of multi-scale appearance features that might be correlated with one ormore concurrent signals. The extraction of these appearance features from asequence of face images yields to a set of time series. This paper proposes touse the dynamics regulating each appearance feature time series to recognizeamong different face emotions. To this purpose, an ensemble of Hankel matricescorresponding to the extracted time series is used for emotion classificationwithin a framework that combines nearest neighbor and a majority vote schema.Experimental results on a public available dataset shows that the adoptedrepresentation is promising and yields state-of-the-art accuracy in emotionclassification.
arxiv-11400-56 | The Filament Sensor for Near Real-Time Detection of Cytoskeletal Fiber Structures | http://arxiv.org/pdf/1408.4002v3.pdf | author:Benjamin Eltzner, Carina Wollnik, Carsten Gottschlich, Stephan Huckemann, Florian Rehfeldt category:cs.CV I.4.3; I.4.6 published:2014-08-18 summary:A reliable extraction of filament data from microscopic images is of highinterest in the analysis of acto-myosin structures as early morphologicalmarkers in mechanically guided differentiation of human mesenchymal stem cellsand the understanding of the underlying fiber arrangement processes. In thispaper, we propose the filament sensor (FS), a fast and robust processingsequence which detects and records location, orientation, length and width foreach single filament of an image, and thus allows for the above describedanalysis. The extraction of these features has previously not been possiblewith existing methods. We evaluate the performance of the proposed FS in termsof accuracy and speed in comparison to three existing methods with respect totheir limited output. Further, we provide a benchmark dataset of real cellimages along with filaments manually marked by a human expert as well assimulated benchmark images. The FS clearly outperforms existing methods interms of computational runtime and filament extraction accuracy. Theimplementation of the FS and the benchmark database are available as opensource.
arxiv-11400-57 | Closed Curves and Elementary Visual Object Identification | http://arxiv.org/pdf/1507.03751v1.pdf | author:Manfred Harringer category:cs.CV cs.LG q-bio.NC published:2015-07-14 summary:For two closed curves on a plane (discrete version) and local criteria forsimilarity of points on the curves one gets a potential, which describes thesimilarity between curve points. This is the base for a global similaritymeasure of closed curves (Fr\'echet distance). I use borderlines of handwrittendigits to demonstrate an area of application. I imagine, measuring thesimilarity of closed curves is an essential and elementary task performed by avisual system. This approach to similarity measures may be used by visualsystems.
arxiv-11400-58 | Splitting the Smoothed Primal-Dual Gap: Optimal Alternating Direction Methods | http://arxiv.org/pdf/1507.03734v1.pdf | author:Quoc Tran-Dinh, Volkan Cevher category:math.OC stat.ML published:2015-07-14 summary:We develop rigorous alternating direction optimization methods for aprototype constrained convex optimization template, which has broadapplications in computational sciences. We build upon our earlier work on themodel-based gap reduction (MGR) technique, which revolves around a smoothedestimate of the primal-dual gap. MGR allows us to simultaneously update asequence of primal and dual variables as well as primal and dual smoothnessparameters so that the smoothed gap function converges to the true gap, whichin turn converges to zero -- both at optimal rates. In contrast, this paperintroduces a new split-gap reduction (SGR) technique as a natural counterpartof MGR in order to take advantage of additional splitting structures present inthe prototype template. We illustrate SGR technique using the forward-backwardand Douglas-Rachford splittings on the smoothed gap function and derive newalternating direction methods. The new methods obtain optimal convergence rateswithout heuristics and eliminate the infamous penalty parameter tuning issue inthe existing alternating direction methods. Finally, we verify the performanceof our methods in comparison to the existing state-of-the-art and the newtheoretical performance bounds via numerical examples.
arxiv-11400-59 | A New Framework for Distributed Submodular Maximization | http://arxiv.org/pdf/1507.03719v1.pdf | author:Rafael da Ponte Barbosa, Alina Ene, Huy L. Nguyen, Justin Ward category:cs.DS cs.AI cs.DC cs.LG published:2015-07-14 summary:A wide variety of problems in machine learning, including exemplarclustering, document summarization, and sensor placement, can be cast asconstrained submodular maximization problems. A lot of recent effort has beendevoted to developing distributed algorithms for these problems. However, theseresults suffer from high number of rounds, suboptimal approximation ratios, orboth. We develop a framework for bringing existing algorithms in the sequentialsetting to the distributed setting, achieving near optimal approximation ratiosfor many settings in only a constant number of MapReduce rounds. Our techniquesalso give a fast sequential algorithm for non-monotone maximization subject toa matroid constraint.
arxiv-11400-60 | Projected Wirtinger Gradient Descent for Low-Rank Hankel Matrix Completion in Spectral Compressed Sensing | http://arxiv.org/pdf/1507.03707v1.pdf | author:Jian-Feng Cai, Suhui Liu, Weiyu Xu category:cs.IT cs.LG math.IT math.OC published:2015-07-14 summary:This paper considers reconstructing a spectrally sparse signal from a smallnumber of randomly observed time-domain samples. The signal of interest is alinear combination of complex sinusoids at $R$ distinct frequencies. Thefrequencies can assume any continuous values in the normalized frequency domain$[0,1)$. After converting the spectrally sparse signal recovery into a low rankstructured matrix completion problem, we propose an efficient feasible pointapproach, named projected Wirtinger gradient descent (PWGD) algorithm, toefficiently solve this structured matrix completion problem. We furtheraccelerate our proposed algorithm by a scheme inspired by FISTA. We give theconvergence analysis of our proposed algorithms. Extensive numericalexperiments are provided to illustrate the efficiency of our proposedalgorithm. Different from earlier approaches, our algorithm can solve problemsof very large dimensions very efficiently.
arxiv-11400-61 | Towards Understanding Egyptian Arabic Dialogues | http://arxiv.org/pdf/1509.03208v1.pdf | author:Abdelrahim A Elmadany, Sherif M Abdou, Mervat Gheith category:cs.CL published:2015-07-14 summary:Labelling of user's utterances to understanding his attends which calledDialogue Act (DA) classification, it is considered the key player for dialoguelanguage understanding layer in automatic dialogue systems. In this paper, weproposed a novel approach to user's utterances labeling for Egyptianspontaneous dialogues and Instant Messages using Machine Learning (ML) approachwithout relying on any special lexicons, cues, or rules. Due to the lack ofEgyptian dialect dialogue corpus, the system evaluated by multi-genre corpusincludes 4725 utterances for three domains, which are collected and annotatedmanually from Egyptian call-centers. The system achieves F1 scores of 70. 36%overall domains.
arxiv-11400-62 | Efficient Exact Gradient Update for training Deep Networks with Very Large Sparse Targets | http://arxiv.org/pdf/1412.7091v3.pdf | author:Pascal Vincent, Alexandre de BrÃ©bisson, Xavier Bouthillier category:cs.NE cs.CL cs.LG published:2014-12-22 summary:An important class of problems involves training deep neural networks withsparse prediction targets of very high dimension D. These occur naturally ine.g. neural language models or the learning of word-embeddings, often posed aspredicting the probability of next words among a vocabulary of size D (e.g. 200000). Computing the equally large, but typically non-sparse D-dimensionaloutput vector from a last hidden layer of reasonable dimension d (e.g. 500)incurs a prohibitive O(Dd) computational cost for each example, as doesupdating the D x d output weight matrix and computing the gradient needed forbackpropagation to previous layers. While efficient handling of large sparsenetwork inputs is trivial, the case of large sparse targets is not, and hasthus so far been sidestepped with approximate alternatives such as hierarchicalsoftmax or sampling-based approximations during training. In this work wedevelop an original algorithmic approach which, for a family of loss functionsthat includes squared error and spherical softmax, can compute the exact loss,gradient update for the output weights, and gradient for backpropagation, allin O(d^2) per example instead of O(Dd), remarkably without ever computing theD-dimensional output. The proposed algorithm yields a speedup of D/4d , i.e.two orders of magnitude for typical sizes, for that critical part of thecomputations that often dominates the training time in this kind of networkarchitecture.
arxiv-11400-63 | Neural CRF Parsing | http://arxiv.org/pdf/1507.03641v1.pdf | author:Greg Durrett, Dan Klein category:cs.CL cs.NE published:2015-07-13 summary:This paper describes a parsing model that combines the exact dynamicprogramming of CRF parsing with the rich nonlinear featurization of neural netapproaches. Our model is structurally a CRF that factors over anchored ruleproductions, but instead of linear potential functions based on sparsefeatures, we use nonlinear potentials computed via a feedforward neuralnetwork. Because potentials are still local to anchored rules, structuredinference (CKY) is unchanged from the sparse case. Computing gradients duringlearning involves backpropagating an error signal formed from standard CRFsufficient statistics (expected rule counts). Using only dense features, ourneural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). Incombination with sparse features, our system achieves 91.1 F1 on section 23 ofthe Penn Treebank, and more generally outperforms the best prior single parserresults on a range of languages.
arxiv-11400-64 | Classifying X-ray Binaries: A Probabilistic Approach | http://arxiv.org/pdf/1507.03538v1.pdf | author:Giri Gopalan, Saeqa Dil Vrtilek, Luke Bornn category:astro-ph.HE stat.AP stat.ML published:2015-07-13 summary:In X-ray binary star systems consisting of a compact object that accretesmaterial from an orbiting secondary star, there is no straightforward means todecide if the compact object is a black hole or a neutron star. To assist thisprocess we develop a Bayesian statistical model which makes use of the factthat X-ray binary systems appear to cluster based on their compact object typewhen viewed from a 3-dimensional coordinate system derived from X-ray spectraldata, where the first coordinate is the ratio of counts in mid to low energyband (color 1), the second coordinate is the ratio of counts in high to lowenergy band (color 2), and the third coordinate is the sum of counts in allthree bands. Precisely, we use this model to estimate the probabilities that anX-ray binary system contains a black hole, non-pulsing neutron star or pulsingneutron star. In particular we utilize a latent variable model in which thelatent variables follow a Gaussian process prior distribution, and hence we areable to induce the spatial correlation we believe exists between systems of thesame type. The utility of this approach is evidenced by the accurate predictionof system types using Rossi X-ray Timing Explorer All Sky Monitor data, but itis not flawless. In particular, non-pulsing neutron systems containing"bursters" which are close to the boundary demarcating systems containing blackholes tend to be classified as black hole systems. As a byproduct of ouranalyses, we provide the astronomer with public R code that can be used topredict the compact object type of X-ray binaries given training data.
arxiv-11400-65 | Beyond Hartigan Consistency: Merge Distortion Metric for Hierarchical Clustering | http://arxiv.org/pdf/1506.06422v2.pdf | author:Justin Eldridge, Mikhail Belkin, Yusu Wang category:stat.ML math.ST stat.TH published:2015-06-21 summary:Hierarchical clustering is a popular method for analyzing data whichassociates a tree to a dataset. Hartigan consistency has been used extensivelyas a framework to analyze such clustering algorithms from a statistical pointof view. Still, as we show in the paper, a tree which is Hartigan consistentwith a given density can look very different than the correct limit tree.Specifically, Hartigan consistency permits two types of undesirableconfigurations which we term over-segmentation and improper nesting. Moreover,Hartigan consistency is a limit property and does not directly quantifydifference between trees. In this paper we identify two limit properties, separation and minimality,which address both over-segmentation and improper nesting and together imply(but are not implied by) Hartigan consistency. We proceed to introduce a mergedistortion metric between hierarchical clusterings and show that convergence inour distance implies both separation and minimality. We also prove that uniformseparation and minimality imply convergence in the merge distortion metric.Furthermore, we show that our merge distortion metric is stable underperturbations of the density. Finally, we demonstrate applicability of these concepts by provingconvergence results for two clustering algorithms. First, we show convergence(and hence separation and minimality) of the recent robust single linkagealgorithm of Chaudhuri and Dasgupta (2010). Second, we provide convergenceresults on manifolds for topological split tree clustering.
arxiv-11400-66 | Priors for Random Count Matrices Derived from a Family of Negative Binomial Processes | http://arxiv.org/pdf/1404.3331v3.pdf | author:Mingyuan Zhou, Oscar Hernan Madrid Padilla, James G. Scott category:stat.ME stat.ML published:2014-04-12 summary:We define a family of probability distributions for random count matriceswith a potentially unbounded number of rows and columns. The threedistributions we consider are derived from the gamma-Poisson, gamma-negativebinomial, and beta-negative binomial processes. Because the models lead toclosed-form Gibbs sampling update equations, they are natural candidates fornonparametric Bayesian priors over count matrices. A key aspect of our analysisis the recognition that, although the random count matrices within the familyare defined by a row-wise construction, their columns can be shown to be i.i.d.This fact is used to derive explicit formulas for drawing all the columns atonce. Moreover, by analyzing these matrices' combinatorial structure, wedescribe how to sequentially construct a column-i.i.d. random count matrix onerow at a time, and derive the predictive distribution of a new row count vectorwith previously unseen features. We describe the similarities and differencesbetween the three priors, and argue that the greater flexibility of the gamma-and beta- negative binomial processes, especially their ability to modelover-dispersed, heavy-tailed count data, makes these well suited to a widevariety of real-world applications. As an example of our framework, weconstruct a naive-Bayes text classifier to categorize a count vector to one ofseveral existing random count matrices of different categories. The classifiersupports an unbounded number of features, and unlike most existing methods, itdoes not require a predefined finite vocabulary to be shared by all thecategories, and needs neither feature selection nor parameter tuning. Both thegamma- and beta- negative binomial processes are shown to significantlyoutperform the gamma-Poisson process for document categorization, withcomparable performance to other state-of-the-art supervised text classificationalgorithms.
arxiv-11400-67 | Scalable Bayesian Optimization Using Deep Neural Networks | http://arxiv.org/pdf/1502.05700v2.pdf | author:Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Md. Mostofa Ali Patwary, Prabhat, Ryan P. Adams category:stat.ML published:2015-02-19 summary:Bayesian optimization is an effective methodology for the global optimizationof functions with expensive evaluations. It relies on querying a distributionover functions defined by a relatively cheap surrogate model. An accurate modelfor this distribution over functions is critical to the effectiveness of theapproach, and is typically fit using Gaussian processes (GPs). However, sinceGPs scale cubically with the number of observations, it has been challenging tohandle objectives whose optimization requires many evaluations, and as such,massively parallelizing the optimization. In this work, we explore the use of neural networks as an alternative to GPsto model distributions over functions. We show that performing adaptive basisfunction regression with a neural network as the parametric form performscompetitively with state-of-the-art GP-based approaches, but scales linearlywith the number of data rather than cubically. This allows us to achieve apreviously intractable degree of parallelism, which we apply to large scalehyperparameter optimization, rapidly finding competitive models on benchmarkobject recognition tasks using convolutional networks, and image captiongeneration using neural language models.
arxiv-11400-68 | The mRMR variable selection method: a comparative study for functional data | http://arxiv.org/pdf/1507.03496v1.pdf | author:JosÃ© R. Berrendero, Antonio Cuevas, JosÃ© L. Torrecilla category:stat.ME stat.ML published:2015-07-13 summary:The use of variable selection methods is particularly appealing instatistical problems with functional data. The obvious general criterion forvariable selection is to choose the `most representative' or `most relevant'variables. However, it is also clear that a purely relevance-oriented criterioncould lead to select many redundant variables. The mRMR (minimum RedundanceMaximum Relevance) procedure, proposed by Ding and Peng (2005) and Peng et al.(2005) is an algorithm to systematically perform variable selection, achievinga reasonable trade-off between relevance and redundancy. In its original form,this procedure is based on the use of the so-called mutual informationcriterion to assess relevance and redundancy. Keeping the focus on functionaldata problems, we propose here a modified version of the mRMR method, obtainedby replacing the mutual information by the new association measure (calleddistance correlation) suggested by Sz\'ekely et al. (2007). We have alsoperformed an extensive simulation study, including 1600 functional experiments(100 functional models $\times$ 4 sample sizes $\times$ 4 classifiers) andthree real-data examples aimed at comparing the different versions of the mRMRmethodology. The results are quite conclusive in favor of the new proposedalternative.
arxiv-11400-69 | Individual performance calibration using physiological stress signals | http://arxiv.org/pdf/1507.03482v1.pdf | author:Francisco Hernando-Gallego, Antonio ArtÃ©s-RodrÃ­guez category:cs.HC cs.CY stat.ML published:2015-07-13 summary:The relation between performance and stress is described by the Yerkes-DodsonLaw but varies significantly between individuals. This paper describes a methodfor determining the individual optimal performance as a function ofphysiological signals. The method is based on attention and reasoning tests ofincreasing complexity under monitoring of three physiological signals: GalvanicSkin Response (GSR), Heart Rate (HR), and Electromyogram (EMG). Based on thetest results with 15 different individuals, we first show that two of thesignals, GSR and HR, have enough discriminative power to distinguish betweenrelax and stress periods. We then show a positive correlation between thecomplexity level of the tests and the GSR and HR signals, and we finallydetermine the optimal performance point as the signal level just before aperformance decrease. We also discuss the differences among signals dependingon the type of test.
arxiv-11400-70 | Incremental LSTM-based Dialog State Tracker | http://arxiv.org/pdf/1507.03471v1.pdf | author:Lukas Zilka, Filip Jurcicek category:cs.CL published:2015-07-13 summary:A dialog state tracker is an important component in modern spoken dialogsystems. We present an incremental dialog state tracker, based on LSTMnetworks. It directly uses automatic speech recognition hypotheses to track thestate. We also present the key non-standard aspects of the model that bring itsperformance close to the state-of-the-art and experimentally analyze theircontribution: including the ASR confidence scores, abstracting scarcelyrepresented values, including transcriptions in the training data, and modelaveraging.
arxiv-11400-71 | Supervised Hierarchical Classification for Student Answer Scoring | http://arxiv.org/pdf/1507.03462v1.pdf | author:Itziar Aldabe, Oier Lopez de Lacalle, IÃ±igo Lopez-Gazpio, Montse Maritxalar category:cs.CL published:2015-07-13 summary:This paper describes a hierarchical system that predicts one label at a timefor automated student response analysis. For the task, we build aclassification binary tree that delays more easily confused labels to laterstages using hierarchical processes. In particular, the paper describes how thehierarchical classifier has been built and how the classification task has beenbroken down into binary subtasks. It finally discusses the motivations andfundamentals of such an approach.
arxiv-11400-72 | Zipf's law for word frequencies: word forms versus lemmas in long texts | http://arxiv.org/pdf/1407.8322v2.pdf | author:Alvaro Corral, Gemma Boleda, Ramon Ferrer-i-Cancho category:physics.soc-ph cs.CL published:2014-07-31 summary:Zipf's law is a fundamental paradigm in the statistics of written and spokennatural language as well as in other communication systems. We raise thequestion of the elementary units for which Zipf's law should hold in the mostnatural way, studying its validity for plain word forms and for thecorresponding lemma forms. In order to have as homogeneous sources as possible,we analyze some of the longest literary texts ever written, comprising fourdifferent languages, with different levels of morphological complexity. In allcases Zipf's law is fulfilled, in the sense that a power-law distribution ofword or lemma frequencies is valid for several orders of magnitude. Weinvestigate the extent to which the word-lemma transformation preserves twoparameters of Zipf's law: the exponent and the low-frequency cut-off. We arenot able to demonstrate a strict invariance of the tail, as for a few textsboth exponents deviate significantly, but we conclude that the exponents arevery similar, despite the remarkable transformation that going from words tolemmas represents, considerably affecting all ranges of frequencies. Incontrast, the low-frequency cut-offs are less stable.
arxiv-11400-73 | Direct Variational Perspective Shape from Shading with Cartesian Depth Parametrisation | http://arxiv.org/pdf/1505.06163v2.pdf | author:Yong Chul Ju, Daniel Maurer, Michael BreuÃ, AndrÃ©s Bruhn category:cs.CV published:2015-05-22 summary:Most of today's state-of-the-art methods for perspective shape from shadingare modelled in terms of partial differential equations (PDEs) ofHamilton-Jacobi type. To improve the robustness of such methods w.r.t. noiseand missing data, first approaches have recently been proposed that seek toembed the underlying PDE into a variational framework with data and smoothnessterm. So far, however, such methods either make use of a radial depthparametrisation that makes the regularisation hard to interpret from ageometrical viewpoint or they consider indirect smoothness terms that requireadditional consistency constraints to provide valid solutions. Moreover theminimisation of such frameworks is an intricate task, since the underlyingenergy is typically non-convex. In our paper we address all three of theaforementioned issues. First, we propose a novel variational model thatoperates directly on the Cartesian depth. In this context, we also point out acommon mistake in the derivation of the surface normal. Moreover, we employ adirect second-order regulariser with edge-preservation property. This directregulariser yields by construction valid solutions without requiring additionalconsistency constraints. Finally, we also propose a novel coarse-to-fineminimisation framework based on an alternating explicit scheme. This frameworkallows us to avoid local minima during the minimisation and thus to improve theaccuracy of the reconstruction. Experiments show the good quality of our modelas well as the usefulness of the proposed numerical scheme.
arxiv-11400-74 | Sparsity assisted solution to the twin image problem in phase retrieval | http://arxiv.org/pdf/1507.03360v1.pdf | author:Charu Gaur, Baranidharan Mohan, Kedar Khare category:cs.CV physics.optics published:2015-07-13 summary:The iterative phase retrieval problem for complex-valued objects from Fouriertransform magnitude data is known to suffer from the twin image problem. Inparticular, when the object support is centro-symmetric, the iterative solutionoften stagnates such that the resultant complex image contains the features ofboth the desired solution and its inverted and complex-conjugated replica. Theconventional approach to address the twin image problem is to modify the objectsupport during initial iterations which can possibly lead to elimination of oneof the twin images. However, at present there seems to be no deterministicprocedure to make sure that the twin image will always be very weak or absent.In this work we make an important observation that the ideal solution withoutthe twin image is typically more sparse (in some suitable transform domain) ascompared to the stagnated solution containing the twin image. We further showthat introducing a sparsity enhancing step in the iterative algorithm canaddress the twin image problem without the need to change the object supportthroughout the iterative process even when the object support iscentro-symmetric. In a simulation study, we use binary and gray-scale purephase objects and illustrate the effectiveness of the sparsity assisted phaserecovery in the context of the twin image problem. The results have importantimplications for a wide range of topics in Physics where the phase retrievalproblem plays a central role.
arxiv-11400-75 | Score-based Causal Learning in Additive Noise Models | http://arxiv.org/pdf/1311.6359v3.pdf | author:Christopher Nowzohour, Peter BÃ¼hlmann category:stat.ML published:2013-11-25 summary:Given data sampled from a number of variables, one is often interested in theunderlying causal relationships in the form of a directed acyclic graph. In thegeneral case, without interventions on some of the variables it is onlypossible to identify the graph up to its Markov equivalence class. However, insome situations one can find the true causal graph just from observationaldata, for example in structural equation models with additive noise andnonlinear edge functions. Most current methods for achieving this rely onnonparametric independence tests. One of the problems there is that the nullhypothesis is independence, which is what one would like to get evidence for.We take a different approach in our work by using a penalized likelihood as ascore for model selection. This is practically feasible in many settings andhas the advantage of yielding a natural ranking of the candidate models. Whenmaking smoothness assumptions on the probability density space, we proveconsistency of the penalized maximum likelihood estimator. We also presentempirical results for simulated scenarios and real two-dimensional data sets(cause-effect pairs) where we obtain similar results as other state-of-the-artmethods.
arxiv-11400-76 | Quantitative Evaluation of Performance and Validity Indices for Clustering the Web Navigational Sessions | http://arxiv.org/pdf/1507.03340v1.pdf | author:Zahid Ansari, M. F. Azeem, Waseem Ahmed, A. Vinaya Babu category:cs.LG cs.SI published:2015-07-13 summary:Clustering techniques are widely used in Web Usage Mining to capture similarinterests and trends among users accessing a Web site. For this purpose, webaccess logs generated at a particular web site are preprocessed to discover theuser navigational sessions. Clustering techniques are then applied to group theuser session data into user session clusters, where intercluster similaritiesare minimized while the intra cluster similarities are maximized. Since theapplication of different clustering algorithms generally results in differentsets of cluster formation, it is important to evaluate the performance of thesemethods in terms of accuracy and validity of the clusters, and also the timerequired to generate them, using appropriate performance measures. This paperdescribes various validity and accuracy measures including Dunn's Index, DaviesBouldin Index, C Index, Rand Index, Jaccard Index, Silhouette Index, FowlkesMallows and Sum of the Squared Error (SSE). We conducted the performanceevaluation of the following clustering techniques: k-Means, k-Medoids, Leader,Single Link Agglomerative Hierarchical and DBSCAN. These techniques areimplemented and tested against the Web user navigational data. Finally theirperformance results are presented and compared.
arxiv-11400-77 | EigenGP: Gaussian Process Models with Adaptive Eigenfunctions | http://arxiv.org/pdf/1401.0362v3.pdf | author:Hao Peng, Yuan Qi category:cs.LG published:2014-01-02 summary:Gaussian processes (GPs) provide a nonparametric representation of functions.However, classical GP inference suffers from high computational cost for bigdata. In this paper, we propose a new Bayesian approach, EigenGP, that learnsboth basis dictionary elements--eigenfunctions of a GP prior--and priorprecisions in a sparse finite model. It is well known that, among allorthogonal basis functions, eigenfunctions can provide the most compactrepresentation. Unlike other sparse Bayesian finite models where the basisfunction has a fixed form, our eigenfunctions live in a reproducing kernelHilbert space as a finite linear combination of kernel functions. We learn thedictionary elements--eigenfunctions--and the prior precisions over theseelements as well as all the other hyperparameters from data by maximizing themodel marginal likelihood. We explore computational linear algebra to simplifythe gradient computation significantly. Our experimental results demonstrateimproved predictive performance of EigenGP over alternative sparse GP methodsas well as relevance vector machine.
arxiv-11400-78 | Learning to Mine Chinese Coordinate Terms Using the Web | http://arxiv.org/pdf/1507.02145v2.pdf | author:Xiaojiang Huang, Xiaojun Wan, Jianguo Xiao category:cs.CL published:2015-07-08 summary:Coordinate relation refers to the relation between instances of a concept andthe relation between the directly hyponyms of a concept. In this paper, wefocus on the task of extracting terms which are coordinate with a user givenseed term in Chinese, and grouping the terms which belong to different conceptsif the seed term has several meanings. We propose a semi-supervised method thatintegrates manually defined linguistic patterns and automatically learnedsemi-structural patterns to extract coordinate terms in Chinese from web searchresults. In addition, terms are grouped into different concepts based on theirco-occurring terms and contexts. We further calculate the saliency scores ofextracted terms and rank them accordingly. Experimental results demonstratethat our proposed method generates results with high quality and wide coverage.
arxiv-11400-79 | Scatter Matrix Concordance: A Diagnostic for Regressions on Subsets of Data | http://arxiv.org/pdf/1507.03285v1.pdf | author:Michael J. Kane, Bryan Lewis, Sekhar Tatikonda, Simon Urbanek category:stat.ML published:2015-07-12 summary:Linear regression models depend directly on the design matrix and itsproperties. Techniques that efficiently estimate model coefficients bypartitioning rows of the design matrix are increasingly popular for large-scaleproblems because they fit well with modern parallel computing architectures. Wepropose a simple measure of {\em concordance} between a design matrix and asubset of its rows that estimates how well a subset captures thevariance-covariance structure of a larger data set. We illustrate the use ofthis measure in a heuristic method for selecting row partition sizes thatbalance statistical and computational efficiency goals in real-world problems.
arxiv-11400-80 | Tensor principal component analysis via sum-of-squares proofs | http://arxiv.org/pdf/1507.03269v1.pdf | author:Samuel B. Hopkins, Jonathan Shi, David Steurer category:cs.LG cs.CC cs.DS stat.ML published:2015-07-12 summary:We study a statistical model for the tensor principal component analysisproblem introduced by Montanari and Richard: Given a order-$3$ tensor $T$ ofthe form $T = \tau \cdot v_0^{\otimes 3} + A$, where $\tau \geq 0$ is asignal-to-noise ratio, $v_0$ is a unit vector, and $A$ is a random noisetensor, the goal is to recover the planted vector $v_0$. For the case that $A$has iid standard Gaussian entries, we give an efficient algorithm to recover$v_0$ whenever $\tau \geq \omega(n^{3/4} \log(n)^{1/4})$, and certify that therecovered vector is close to a maximum likelihood estimator, all with highprobability over the random choice of $A$. The previous best algorithms withprovable guarantees required $\tau \geq \Omega(n)$. In the regime $\tau \leq o(n)$, natural tensor-unfolding-based spectralrelaxations for the underlying optimization problem break down (in the sensethat their integrality gap is large). To go beyond this barrier, we use convexrelaxations based on the sum-of-squares method. Our recovery algorithm proceedsby rounding a degree-$4$ sum-of-squares relaxations of themaximum-likelihood-estimation problem for the statistical model. To complementour algorithmic results, we show that degree-$4$ sum-of-squares relaxationsbreak down for $\tau \leq O(n^{3/4}/\log(n)^{1/4})$, which demonstrates thatimproving our current guarantees (by more than logarithmic factors) wouldrequire new techniques or might even be intractable. Finally, we show how to exploit additional problem structure in order tosolve our sum-of-squares relaxations, up to some approximation, veryefficiently. Our fastest algorithm runs in nearly-linear time using shifted(matrix) power iteration and has similar guarantees as above. The analysis ofthis algorithm also confirms a variant of a conjecture of Montanari and Richardabout singular vectors of tensor unfoldings.
arxiv-11400-81 | Homotopy Continuation Approaches for Robust SV Classification and Regression | http://arxiv.org/pdf/1507.03229v1.pdf | author:Shinya Suzumura, Kohei Ogawa, Masashi Sugiyama, Masayuki Karasuyama, Ichiro Takeuchi category:stat.ML cs.LG published:2015-07-12 summary:In support vector machine (SVM) applications with unreliable data thatcontains a portion of outliers, non-robustness of SVMs often causesconsiderable performance deterioration. Although many approaches for improvingthe robustness of SVMs have been studied, two major challenges remain in robustSVM learning. First, robust learning algorithms are essentially formulated asnon-convex optimization problems. It is thus important to develop a non-convexoptimization method for robust SVM that can find a good local optimal solution.The second practical issue is how one can tune the hyperparameter that controlsthe balance between robustness and efficiency. Unfortunately, due to thenon-convexity, robust SVM solutions with slightly different hyper-parametervalues can be significantly different, which makes model selection highlyunstable. In this paper, we address these two issues simultaneously byintroducing a novel homotopy approach to non-convex robust SVM learning. Ourbasic idea is to introduce parametrized formulations of robust SVM which bridgethe standard SVM and fully robust SVM via the parameter that represents theinfluence of outliers. We characterize the necessary and sufficient conditionsof the local optimal solutions of robust SVM, and develop an algorithm that cantrace a path of local optimal solutions when the influence of outliers isgradually decreased. An advantage of our homotopy approach is that it can beinterpreted as simulated annealing, a common approach for finding a good localoptimal solution in non-convex optimization problems. In addition, our homotopymethod allows stable and efficient model selection based on the path of localoptimal solutions. Empirical performances of the proposed approach aredemonstrated through intensive numerical experiments both on robustclassification and regression problems.
arxiv-11400-82 | Scalable Bayesian Inference for Excitatory Point Process Networks | http://arxiv.org/pdf/1507.03228v1.pdf | author:Scott W. Linderman, Ryan P. Adams category:stat.ML published:2015-07-12 summary:Networks capture our intuition about relationships in the world. Theydescribe the friendships between Facebook users, interactions in financialmarkets, and synapses connecting neurons in the brain. These networks arerichly structured with cliques of friends, sectors of stocks, and a smorgasbordof cell types that govern how neurons connect. Some networks, like socialnetwork friendships, can be directly observed, but in many cases we only havean indirect view of the network through the actions of its constituents and anunderstanding of how the network mediates that activity. In this work, we focuson the problem of latent network discovery in the case where the observableactivity takes the form of a mutually-excitatory point process known as aHawkes process. We build on previous work that has taken a Bayesian approach tothis problem, specifying prior distributions over the latent network structureand a likelihood of observed activity given this network. We extend this workby proposing a discrete-time formulation and developing a computationallyefficient stochastic variational inference (SVI) algorithm that allows us toscale the approach to long sequences of observations. We demonstrate ouralgorithm on the calcium imaging data used in the Chalearn neural connectomicschallenge.
arxiv-11400-83 | Classifier-Based Text Simplification for Improved Machine Translation | http://arxiv.org/pdf/1507.03223v1.pdf | author:Shruti Tyagi, Deepti Chopra, Iti Mathur, Nisheeth Joshi category:cs.CL published:2015-07-12 summary:Machine Translation is one of the research fields of ComputationalLinguistics. The objective of many MT Researchers is to develop an MT Systemthat produce good quality and high accuracy output translations and which alsocovers maximum language pairs. As internet and Globalization is increasing dayby day, we need a way that improves the quality of translation. For thisreason, we have developed a Classifier based Text Simplification Model forEnglish-Hindi Machine Translation Systems. We have used support vector machinesand Na\"ive Bayes Classifier to develop this model. We have also evaluated theperformance of these classifiers.
arxiv-11400-84 | Auditing: Active Learning with Outcome-Dependent Query Costs | http://arxiv.org/pdf/1306.2347v4.pdf | author:Sivan Sabato, Anand D. Sarwate, Nathan Srebro category:cs.LG published:2013-06-10 summary:We propose a learning setting in which unlabeled data is free, and the costof a label depends on its value, which is not known in advance. We study binaryclassification in an extreme case, where the algorithm only pays for negativelabels. Our motivation are applications such as fraud detection, in whichinvestigating an honest transaction should be avoided if possible. We term thesetting auditing, and consider the auditing complexity of an algorithm: thenumber of negative labels the algorithm requires in order to learn a hypothesiswith low relative error. We design auditing algorithms for simple hypothesisclasses (thresholds and rectangles), and show that with these algorithms, theauditing complexity can be significantly lower than the active labelcomplexity. We also discuss a general competitive approach for auditing andpossible modifications to the framework.
arxiv-11400-85 | DeepFont: Identify Your Font from An Image | http://arxiv.org/pdf/1507.03196v1.pdf | author:Zhangyang Wang, Jianchao Yang, Hailin Jin, Eli Shechtman, Aseem Agarwala, Jonathan Brandt, Thomas S. Huang category:cs.CV published:2015-07-12 summary:As font is one of the core design concepts, automatic font identification andsimilar font suggestion from an image or photo has been on the wish list ofmany designers. We study the Visual Font Recognition (VFR) problem, and advancethe state-of-the-art remarkably by developing the DeepFont system. First ofall, we build up the first available large-scale VFR dataset, named AdobeVFR,consisting of both labeled synthetic data and partially labeled real-worlddata. Next, to combat the domain mismatch between available training andtesting data, we introduce a Convolutional Neural Network (CNN) decompositionapproach, using a domain adaptation technique based on a Stacked ConvolutionalAuto-Encoder (SCAE) that exploits a large corpus of unlabeled real-world textimages combined with synthetic data preprocessed in a specific way. Moreover,we study a novel learning-based model compression approach, in order to reducethe DeepFont model size without sacrificing its performance. The DeepFontsystem achieves an accuracy of higher than 80% (top-5) on our collecteddataset, and also produces a good font similarity measure for font selectionand suggestion. We also achieve around 6 times compression of the model withoutany visible loss of recognition accuracy.
arxiv-11400-86 | Differential Privacy in a Bayesian setting through posterior sampling | http://arxiv.org/pdf/1306.1066v4.pdf | author:Christos Dimitrakakis, Blaine Nelson, and Zuhe Zhang, Aikaterini Mitrokotsa, Benjamin Rubinstein category:stat.ML cs.LG published:2013-06-05 summary:We examine the robustness and privacy properties of Bayesian inference, underassumptions on the prior. With no modifications to the Bayesian framework, weshow that a simple posterior sampling algorithm results in uniform utility andprivacy guarantees. In more detail, we generalise the concept of differentialprivacy to arbitrary dataset distances, outcome spaces and distributionfamilies. We then prove bounds on the robustness of the posterior, introduce aposterior sampling mechanism, show that it is differentially private andprovide finite sample bounds for distinguishability-based privacy under astrong adversarial model. Finally, we give examples satisfying our assumptions.
arxiv-11400-87 | Dependent Indian Buffet Process-based Sparse Nonparametric Nonnegative Matrix Factorization | http://arxiv.org/pdf/1507.03176v1.pdf | author:Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yi Da Xu, Xiangfeng Luo category:stat.ML published:2015-07-12 summary:Nonnegative Matrix Factorization (NMF) aims to factorize a matrix into twooptimized nonnegative matrices appropriate for the intended applications. Themethod has been widely used for unsupervised learning tasks, includingrecommender systems (rating matrix of users by items) and document clustering(weighting matrix of papers by keywords). However, traditional NMF methodstypically assume the number of latent factors (i.e., dimensionality of theloading matrices) to be fixed. This assumption makes them inflexible for manyapplications. In this paper, we propose a nonparametric NMF framework tomitigate this issue by using dependent Indian Buffet Processes (dIBP). In anutshell, we apply a correlation function for the generation of two stickweights associated with each pair of columns of loading matrices, while stillmaintaining their respective marginal distribution specified by IBP. As aconsequence, the generation of two loading matrices will be column-wise(indirectly) correlated. Under this same framework, two classes of correlationfunction are proposed (1) using Bivariate beta distribution and (2) usingCopula function. Both methods allow us to adopt our work for variousapplications by flexibly choosing an appropriate parameter settings. Comparedwith the other state-of-the art approaches in this area, such as using GaussianProcess (GP)-based dIBP, our work is seen to be much more flexible in terms ofallowing the two corresponding binary matrix columns to have greater variationsin their non-zero entries. Our experiments on the real-world and syntheticdatasets show that three proposed models perform well on the documentclustering task comparing standard NMF without predefining the dimension forthe factor matrices, and the Bivariate beta distribution-based and Copula-basedmodels have better flexibility than the GP-based model.
arxiv-11400-88 | Best Subset Selection via a Modern Optimization Lens | http://arxiv.org/pdf/1507.03133v1.pdf | author:Dimitris Bertsimas, Angela King, Rahul Mazumder category:stat.ME math.OC stat.CO stat.ML published:2015-07-11 summary:In the last twenty-five years (1990-2014), algorithmic advances in integeroptimization combined with hardware improvements have resulted in anastonishing 200 billion factor speedup in solving Mixed Integer Optimization(MIO) problems. We present a MIO approach for solving the classical best subsetselection problem of choosing $k$ out of $p$ features in linear regressiongiven $n$ observations. We develop a discrete extension of modern first ordercontinuous optimization methods to find high quality feasible solutions that weuse as warm starts to a MIO solver that finds provably optimal solutions. Theresulting algorithm (a) provides a solution with a guarantee on itssuboptimality even if we terminate the algorithm early, (b) can accommodateside constraints on the coefficients of the linear regression and (c) extendsto finding best subset solutions for the least absolute deviation lossfunction. Using a wide variety of synthetic and real datasets, we demonstratethat our approach solves problems with $n$ in the 1000s and $p$ in the 100s inminutes to provable optimality, and finds near optimal solutions for $n$ in the100s and $p$ in the 1000s in minutes. We also establish via numericalexperiments that the MIO approach performs better than {\texttt {Lasso}} andother popularly used sparse learning procedures, in terms of achieving sparsesolutions with good predictive power.
arxiv-11400-89 | Joint estimation of quantile planes over arbitrary predictor spaces | http://arxiv.org/pdf/1507.03130v1.pdf | author:Yun Yang, Surya Tokdar category:stat.ME stat.CO stat.ML published:2015-07-11 summary:In spite of the recent surge of interest in quantile regression, jointestimation of linear quantile planes remains a great challenge in statisticsand econometrics. We propose a novel parametrization that characterizes anycollection of non-crossing quantile planes over arbitrarily shaped convexpredictor domains in any dimension by means of unconstrained scalar, vector andfunction valued parameters. Statistical models based on this parametrizationinherit a fast computation of the likelihood function, enabling penalizedlikelihood or Bayesian approaches to model fitting. We introduce a completeBayesian methodology by using Gaussian process prior distributions on thefunction valued parameters and develop a robust and efficient Markov chainMonte Carlo parameter estimation. The resulting method is shown to offerposterior consistency under mild tail and regularity conditions. We presentseveral illustrative examples where the new method is compared against existingapproaches and is found to offer better accuracy, coverage and model fit.
arxiv-11400-90 | A new boosting algorithm based on dual averaging scheme | http://arxiv.org/pdf/1507.03125v1.pdf | author:Nan Wang category:cs.LG published:2015-07-11 summary:The fields of machine learning and mathematical optimization increasinglyintertwined. The special topic on supervised learning and convex optimizationexamines this interplay. The training part of most supervised learningalgorithms can usually be reduced to an optimization problem that minimizes aloss between model predictions and training data. While most optimizationtechniques focus on accuracy and speed of convergence, the qualities of goodoptimization algorithm from the machine learning perspective can be quitedifferent since machine learning is more than fitting the data. Betteroptimization algorithms that minimize the training loss can possibly give verypoor generalization performance. In this paper, we examine a particular kind ofmachine learning algorithm, boosting, whose training process can be viewed asfunctional coordinate descent on the exponential loss. We study the relationbetween optimization techniques and machine learning by implementing a newboosting algorithm. DABoost, based on dual-averaging scheme and study itsgeneralization performance. We show that DABoost, although slower in reducingthe training error, in general enjoys a better generalization error thanAdaBoost.
arxiv-11400-91 | On the Use of Harrell's C for Node Splitting in Random Survival Forests | http://arxiv.org/pdf/1507.03092v1.pdf | author:Matthias Schmid, Marvin Wright, Andreas Ziegler category:stat.ML published:2015-07-11 summary:Random forests are one of the most successful methods for statisticallearning and prediction. Here we consider random survival forests (RSF), whichare an extension of the original random forest method to right-censored outcomevariables. RSF use the log-rank split criterion to form an ensemble of survivaltrees, the prediction accuracy of the ensemble estimate is subsequentlyevaluated by the concordance index for survival data ("Harrell's C").Conceptually, this strategy means that the split criterion in RSF is differentfrom the evaluation criterion of interest. In view of this discrepancy, weanalyze the theoretical relationship between the two criteria and investigatewhether a unified strategy that uses Harrell's C for both node splitting andevaluation is able to improve the performance of RSF. Based on simulationstudies and the analysis of real-world data, we show that substantialperformance gains are possible if the log-rank statistic is replaced byHarrell's C for node splitting in RSF. Our results also show that C-basedsplitting is not superior to log-rank splitting if the percentage of noisevariables is high, a result which can be attributed to the more unbalancedsplits that are generated by the log-rank statistic.
arxiv-11400-92 | Markov Logic Networks for Natural Language Question Answering | http://arxiv.org/pdf/1507.03045v1.pdf | author:Tushar Khot, Niranjan Balasubramanian, Eric Gribkoff, Ashish Sabharwal, Peter Clark, Oren Etzioni category:cs.AI cs.CL published:2015-07-10 summary:Our goal is to answer elementary-level science questions using knowledgeextracted automatically from science textbooks, expressed in a subset offirst-order logic. Given the incomplete and noisy nature of these automaticallyextracted rules, Markov Logic Networks (MLNs) seem a natural model to use, butthe exact way of leveraging MLNs is by no means obvious. We investigate threeways of applying MLNs to our task. In the first, we simply use the extractedscience rules directly as MLN clauses. Unlike typical MLN applications, ourdomain has long and complex rules, leading to an unmanageable number ofgroundings. We exploit the structure present in hard constraints to improvetractability, but the formulation remains ineffective. In the second approach,we instead interpret science rules as describing prototypical entities, thusmapping rules directly to grounded MLN assertions, whose constants are thenclustered using existing entity resolution methods. This drastically simplifiesthe network, but still suffers from brittleness. Finally, our third approach,called Praline, uses MLNs to align the lexical elements as well as define andcontrol how inference should be performed in this task. Our experiments,demonstrating a 15\% accuracy boost and a 10x reduction in runtime, suggestthat the flexibility and different inference semantics of Praline are a betterfit for the natural language question answering task.
arxiv-11400-93 | Linear Convergence of Variance-Reduced Stochastic Gradient without Strong Convexity | http://arxiv.org/pdf/1406.1102v2.pdf | author:Pinghua Gong, Jieping Ye category:cs.NA cs.LG stat.CO stat.ML published:2014-06-04 summary:Stochastic gradient algorithms estimate the gradient based on only one or afew samples and enjoy low computational cost per iteration. They have beenwidely used in large-scale optimization problems. However, stochastic gradientalgorithms are usually slow to converge and achieve sub-linear convergencerates, due to the inherent variance in the gradient computation. To acceleratethe convergence, some variance-reduced stochastic gradient algorithms, e.g.,proximal stochastic variance-reduced gradient (Prox-SVRG) algorithm, haverecently been proposed to solve strongly convex problems. Under the stronglyconvex condition, these variance-reduced stochastic gradient algorithms achievea linear convergence rate. However, many machine learning problems are convexbut not strongly convex. In this paper, we introduce Prox-SVRG and itsprojected variant called Variance-Reduced Projected Stochastic Gradient (VRPSG)to solve a class of non-strongly convex optimization problems widely used inmachine learning. As the main technical contribution of this paper, we showthat both VRPSG and Prox-SVRG achieve a linear convergence rate without strongconvexity. A key ingredient in our proof is a Semi-Strongly Convex (SSC)inequality which is the first to be rigorously proved for a class ofnon-strongly convex problems in both constrained and regularized settings.Moreover, the SSC inequality is independent of algorithms and may be applied toanalyze other stochastic gradient algorithms besides VRPSG and Prox-SVRG, whichmay be of independent interest. To the best of our knowledge, this is the firstwork that establishes the linear convergence rate for the variance-reducedstochastic gradient algorithms on solving both constrained and regularizedproblems without strong convexity.
arxiv-11400-94 | Extending a Single-Document Summarizer to Multi-Document: a Hierarchical Approach | http://arxiv.org/pdf/1507.02907v1.pdf | author:LuÃ­s Marujo, Ricardo Ribeiro, David Martins de Matos, JoÃ£o P. Neto, Anatole Gershman, Jaime Carbonell category:cs.IR cs.CL published:2015-07-10 summary:The increasing amount of online content motivated the development ofmulti-document summarization methods. In this work, we explore straightforwardapproaches to extend single-document summarization methods to multi-documentsummarization. The proposed methods are based on the hierarchical combinationof single-document summaries, and achieves state of the art results.
arxiv-11400-95 | Deep Perceptual Mapping for Thermal to Visible Face Recognition | http://arxiv.org/pdf/1507.02879v1.pdf | author:M. Saquib Sarfraz, Rainer Stiefelhagen category:cs.CV published:2015-07-10 summary:Cross modal face matching between the thermal and visible spectrum is a muchde- sired capability for night-time surveillance and security applications. Dueto a very large modality gap, thermal-to-visible face recognition is one of themost challenging face matching problem. In this paper, we present an approachto bridge this modality gap by a significant margin. Our approach captures thehighly non-linear relationship be- tween the two modalities by using a deepneural network. Our model attempts to learn a non-linear mapping from visibleto thermal spectrum while preserving the identity in- formation. We showsubstantive performance improvement on a difficult thermal-visible facedataset. The presented approach improves the state-of-the-art by more than 10%in terms of Rank-1 identification and bridge the drop in performance due to themodality gap by more than 40%.
arxiv-11400-96 | A Trainable Neuromorphic Integrated Circuit that Exploits Device Mismatch | http://arxiv.org/pdf/1507.02835v1.pdf | author:Chetan Singh Thakur, Runchun Wang, Tara Julia Hamilton, Jonathan Tapson, Andre van Schaik category:cs.NE published:2015-07-10 summary:Random device mismatch that arises as a result of scaling of the CMOS(complementary metal-oxide semi-conductor) technology into the deep submicronregime degrades the accuracy of analogue circuits. Methods to combat thisincrease the complexity of design. We have developed a novel neuromorphicsystem called a Trainable Analogue Block (TAB), which exploits device mismatchas a means for random projections of the input to a higher dimensional space.The TAB framework is inspired by the principles of neural population codingoperating in the biological nervous system. Three neuronal layers, namelyinput, hidden, and output, constitute the TAB framework, with the number ofhidden layer neurons far exceeding the input layer neurons. Here, we presentmeasurement results of the first prototype TAB chip built using a 65nm processtechnology and show its learning capability for various regression tasks. OurTAB chip exploits inherent randomness and variability arising due to thefabrication process to perform various learning tasks. Additionally, wecharacterise each neuron and discuss the statistical variability of its tuningcurve that arises due to random device mismatch, a desirable property for thelearning capability of the TAB. We also discuss the effect of the number ofhidden neurons and the resolution of output weights on the accuracy of thelearning capability of the TAB.
arxiv-11400-97 | Robust Performance-driven 3D Face Tracking in Long Range Depth Scenes | http://arxiv.org/pdf/1507.02779v1.pdf | author:Hai X. Pham, Chongyu Chen, Luc N. Dao, Vladimir Pavlovic, Jianfei Cai, Tat-jen Cham category:cs.CV published:2015-07-10 summary:We introduce a novel robust hybrid 3D face tracking framework from RGBD videostreams, which is capable of tracking head pose and facial actions withoutpre-calibration or intervention from a user. In particular, we emphasize onimproving the tracking performance in instances where the tracked subject is ata large distance from the cameras, and the quality of point cloud deterioratesseverely. This is accomplished by the combination of a flexible 3D shaperegressor and the joint 2D+3D optimization on shape parameters. Our approachfits facial blendshapes to the point cloud of the human head, while beingdriven by an efficient and rapid 3D shape regressor trained on generic RGBdatasets. As an on-line tracking system, the identity of the unknown user isadapted on-the-fly resulting in improved 3D model reconstruction andconsequently better tracking performance. The result is a robust RGBD facetracker, capable of handling a wide range of target scene depths, beyond thosethat can be afforded by traditional depth or RGB face trackers. Lastly, sincethe blendshape is not able to accurately recover the real facial shape, we usethe tracked 3D face model as a prior in a novel filtering process to furtherrefine the depth map for use in other tasks, such as 3D reconstruction.
arxiv-11400-98 | Locally Non-linear Embeddings for Extreme Multi-label Learning | http://arxiv.org/pdf/1507.02743v1.pdf | author:Kush Bhatia, Himanshu Jain, Purushottam Kar, Prateek Jain, Manik Varma category:cs.LG cs.IR math.OC stat.ML published:2015-07-09 summary:The objective in extreme multi-label learning is to train a classifier thatcan automatically tag a novel data point with the most relevant subset oflabels from an extremely large label set. Embedding based approaches maketraining and prediction tractable by assuming that the training label matrix islow-rank and hence the effective number of labels can be reduced by projectingthe high dimensional label vectors onto a low dimensional linear subspace.Still, leading embedding approaches have been unable to deliver high predictionaccuracies or scale to large problems as the low rank assumption is violated inmost real world applications. This paper develops the X-One classifier to address both limitations. Themain technical contribution in X-One is a formulation for learning a smallensemble of local distance preserving embeddings which can accurately predictinfrequently occurring (tail) labels. This allows X-One to break free of thetraditional low-rank assumption and boost classification accuracy by learningembeddings which preserve pairwise distances between only the nearest labelvectors. We conducted extensive experiments on several real-world as well as benchmarkdata sets and compared our method against state-of-the-art methods for extrememulti-label classification. Experiments reveal that X-One can makesignificantly more accurate predictions then the state-of-the-art methodsincluding both embeddings (by as much as 35%) as well as trees (by as much as6%). X-One can also scale efficiently to data sets with a million labels whichare beyond the pale of leading embedding methods.
arxiv-11400-99 | Robot In a Room: Toward Perfect Object Recognition in Closed Environments | http://arxiv.org/pdf/1507.02703v1.pdf | author:Shuran Song, Linguang Zhang, Jianxiong Xiao category:cs.CV published:2015-07-09 summary:While general object recognition is still far from being solved, this paperproposes a way for a robot to recognize every object at an almost human-levelaccuracy. Our key observation is that many robots will stay in a relativelyclosed environment (e.g. a house or an office). By constraining a robot to stayin a limited territory, we can ensure that the robot has seen most objectsbefore and the speed of introducing a new object is slow. Furthermore, we canbuild a 3D map of the environment to reliably subtract the background to makerecognition easier. We propose extremely robust algorithms to obtain a 3D mapand enable humans to collectively annotate objects. During testing time, ouralgorithm can recognize all objects very reliably, and query humans from crowdsourcing platform if confidence is low or new objects are identified. Thispaper explains design decisions in building such a system, and constructs abenchmark for extensive evaluation. Experiments suggest that making robotvision appear to be working from an end user's perspective is a reachable goaltoday, as long as the robot stays in a closed environment. By formulating thistask, we hope to lay the foundation of a new direction in vision for robotics.Code and data will be available upon acceptance.
arxiv-11400-100 | Quantum Inspired Training for Boltzmann Machines | http://arxiv.org/pdf/1507.02642v1.pdf | author:Nathan Wiebe, Ashish Kapoor, Christopher Granade, Krysta M Svore category:cs.LG quant-ph published:2015-07-09 summary:We present an efficient classical algorithm for training deep Boltzmannmachines (DBMs) that uses rejection sampling in concert with variationalapproximations to estimate the gradients of the training objective function.Our algorithm is inspired by a recent quantum algorithm for training DBMs. Weobtain rigorous bounds on the errors in the approximate gradients; in turn, wefind that choosing the instrumental distribution to minimize the alpha=2divergence with the Gibbs state minimizes the asymptotic algorithmiccomplexity. Our rejection sampling approach can yield more accurate gradientsthan low-order contrastive divergence training and the costs incurred infinding increasingly accurate gradients can be easily parallelized. Finally ouralgorithm can train full Boltzmann machines and scales more favorably with thenumber of layers in a DBM than greedy contrastive divergence training.
arxiv-11400-101 | Deep convolutional filter banks for texture recognition and segmentation | http://arxiv.org/pdf/1411.6836v2.pdf | author:Mircea Cimpoi, Subhransu Maji, Andrea Vedaldi category:cs.CV published:2014-11-25 summary:Research in texture recognition often concentrates on the problem of materialrecognition in uncluttered conditions, an assumption rarely met byapplications. In this work we conduct a first study of material and describabletexture at- tributes recognition in clutter, using a new dataset derived fromthe OpenSurface texture repository. Motivated by the challenge posed by thisproblem, we propose a new texture descriptor, D-CNN, obtained by Fisher Vectorpooling of a Convolutional Neural Network (CNN) filter bank. D-CNNsubstantially improves the state-of-the-art in texture, mate- rial and scenerecognition. Our approach achieves 82.3% accuracy on Flickr material datasetand 81.1% accuracy on MIT indoor scenes, providing absolute gains of more than10% over existing approaches. D-CNN easily trans- fers across domains withoutrequiring feature adaptation as for methods that build on the fully-connectedlayers of CNNs. Furthermore, D-CNN can seamlessly incorporate multi-scaleinformation and describe regions of arbitrary shapes and sizes. Our approach isparticularly suited at lo- calizing stuff categories and obtainsstate-of-the-art re- sults on MSRC segmentation dataset, as well as promisingresults on recognizing materials and surface attributes in clutter on theOpenSurfaces dataset.
arxiv-11400-102 | FAQ-based Question Answering via Word Alignment | http://arxiv.org/pdf/1507.02628v1.pdf | author:Zhiguo Wang, Abraham Ittycheriah category:cs.CL published:2015-07-09 summary:In this paper, we propose a novel word-alignment-based method to solve theFAQ-based question answering task. First, we employ a neural network model tocalculate question similarity, where the word alignment between two questionsis used for extracting features. Second, we design a bootstrap-based featureextraction method to extract a small set of effective lexical features. Third,we propose a learning-to-rank algorithm to train parameters more suitable forthe ranking tasks. Experimental results, conducted on three languages (English,Spanish and Japanese), demonstrate that the question similarity model is moreeffective than baseline systems, the sparse features bring 5% improvements ontop-1 accuracy, and the learning-to-rank algorithm works significantly betterthan the traditional method. We further evaluate our method on the answersentence selection task. Our method outperforms all the previous systems on thestandard TREC data set.
arxiv-11400-103 | Sampling from a log-concave distribution with Projected Langevin Monte Carlo | http://arxiv.org/pdf/1507.02564v1.pdf | author:SÃ©bastien Bubeck, Ronen Eldan, Joseph Lehec category:math.PR cs.DS cs.LG published:2015-07-09 summary:We extend the Langevin Monte Carlo (LMC) algorithm to compactly supportedmeasures via a projection step, akin to projected Stochastic Gradient Descent(SGD). We show that (projected) LMC allows to sample in polynomial time from alog-concave distribution with smooth potential. This gives a new Markov chainto sample from a log-concave distribution. Our main result shows in particularthat when the target distribution is uniform, LMC mixes in $\tilde{O}(n^7)$steps (where $n$ is the dimension). We also provide preliminary experimentalevidence that LMC performs at least as well as hit-and-run, for which a bettermixing time of $\tilde{O}(n^4)$ was proved by Lov{\'a}sz and Vempala.
arxiv-11400-104 | Adaptive Chemical Reaction Optimization for Global Numerical Optimization | http://arxiv.org/pdf/1507.02492v1.pdf | author:James J. Q. Yu, Albert Y. S. Lam, Victor O. K. Li category:cs.NE published:2015-07-09 summary:A newly proposed chemical-reaction-inspired metaheurisic, Chemical ReactionOptimization (CRO), has been applied to many optimization problems in bothdiscrete and continuous domains. To alleviate the effort in tuning parameters,this paper reduces the number of optimization parameters in canonical CRO anddevelops an adaptive scheme to evolve them. Our proposed Adaptive CRO (ACRO)adapts better to different optimization problems. We perform simulations withACRO on a widely-used benchmark of continuous problems. The simulation resultsshow that ACRO has superior performance over canonical CRO.
arxiv-11400-105 | Parameter Sensitivity Analysis of Social Spider Algorithm | http://arxiv.org/pdf/1507.02491v1.pdf | author:James J. Q. Yu, Victor O. K. Li category:cs.NE published:2015-07-09 summary:Social Spider Algorithm (SSA) is a recently proposed general-purposereal-parameter metaheuristic designed to solve global numerical optimizationproblems. This work systematically benchmarks SSA on a suite of 11 functionswith different control parameters. We conduct parameter sensitivity analysis ofSSA using advanced non-parametric statistical tests to generate statisticallysignificant conclusion on the best performing parameter settings. Theconclusion can be adopted in future work to reduce the effort in parametertuning. In addition, we perform a success rate test to reveal the impact of thecontrol parameters on the convergence speed of the algorithm.
arxiv-11400-106 | A New Approach to Probabilistic Programming Inference | http://arxiv.org/pdf/1507.00996v2.pdf | author:Frank Wood, Jan Willem van de Meent, Vikash Mansinghka category:stat.ML cs.AI cs.PL published:2015-07-03 summary:We introduce and demonstrate a new approach to inference in expressiveprobabilistic programming languages based on particle Markov chain Monte Carlo.Our approach is simple to implement and easy to parallelize. It applies toTuring-complete probabilistic programming languages and supports accurateinference in models that make use of complex control flow, including stochasticrecursion. It also includes primitives from Bayesian nonparametric statistics.Our experiments show that this approach can be more efficient than previouslyintroduced single-site Metropolis-Hastings methods.
arxiv-11400-107 | Data Mining of Causal Relations from Text: Analysing Maritime Accident Investigation Reports | http://arxiv.org/pdf/1507.02447v1.pdf | author:Santosh Tirunagari category:cs.IR cs.CL published:2015-07-09 summary:Text mining is a process of extracting information of interest from text.Such a method includes techniques from various areas such as InformationRetrieval (IR), Natural Language Processing (NLP), and Information Extraction(IE). In this study, text mining methods are applied to extract causalrelations from maritime accident investigation reports collected from theMarine Accident Investigation Branch (MAIB). These causal relations provideinformation on various mechanisms behind accidents, including human andorganizational factors relating to the accident. The objective of this study isto facilitate the analysis of the maritime accident investigation reports, bymeans of extracting contributory causes with more feasibility. A carefulinvestigation of contributory causes from the reports provide opportunity toimprove safety in future. Two methods have been employed in this study to extract the causal relations.They are 1) Pattern classification method and 2) Connectives method. Theearlier one uses naive Bayes and Support Vector Machines (SVM) as classifiers.The latter simply searches for the words connecting cause and effect insentences. The causal patterns extracted using these two methods are compared to themanual (human expert) extraction. The pattern classification method showed afair and sensible performance with F-measure(average) = 65% when compared toconnectives method with F-measure(average) = 58%. This study is an evidence,that text mining methods could be employed in extracting causal relations frommarine accident investigation reports.
arxiv-11400-108 | Generalized Video Deblurring for Dynamic Scenes | http://arxiv.org/pdf/1507.02438v1.pdf | author:Tae Hyun Kim, Kyoung Mu Lee category:cs.CV published:2015-07-09 summary:Several state-of-the-art video deblurring methods are based on a strongassumption that the captured scenes are static. These methods fail to deblurblurry videos in dynamic scenes. We propose a video deblurring method to dealwith general blurs inherent in dynamic scenes, contrary to other methods. Tohandle locally varying and general blurs caused by various sources, such ascamera shake, moving objects, and depth variation in a scene, we approximatepixel-wise kernel with bidirectional optical flows. Therefore, we propose asingle energy model that simultaneously estimates optical flows and latentframes to solve our deblurring problem. We also provide a framework andefficient solvers to optimize the energy model. By minimizing the proposedenergy function, we achieve significant improvements in removing blurs andestimating accurate optical flows in blurry frames. Extensive experimentalresults demonstrate the superiority of the proposed method in real andchallenging videos that state-of-the-art methods fail in either deblurring oroptical flow estimation.
arxiv-11400-109 | Spaced seeds improve k-mer-based metagenomic classification | http://arxiv.org/pdf/1502.06256v3.pdf | author:Karel Brinda, Maciej Sykulski, Gregory Kucherov category:q-bio.GN cs.CE cs.LG published:2015-02-22 summary:Metagenomics is a powerful approach to study genetic content of environmentalsamples that has been strongly promoted by NGS technologies. To cope withmassive data involved in modern metagenomic projects, recent tools [4, 39] relyon the analysis of k-mers shared between the read to be classified and sampledreference genomes. Within this general framework, we show in this work thatspaced seeds provide a significant improvement of classification accuracy asopposed to traditional contiguous k-mers. We support this thesis through aseries a different computational experiments, including simulations oflarge-scale metagenomic projects. Scripts and programs used in this study, aswell as supplementary material, are available fromhttp://github.com/gregorykucherov/spaced-seeds-for-metagenomics.
arxiv-11400-110 | Learning Structured Ordinal Measures for Video based Face Recognition | http://arxiv.org/pdf/1507.02380v1.pdf | author:Ran He, Tieniu Tan, Larry Davis, Zhenan Sun category:cs.CV published:2015-07-09 summary:This paper presents a structured ordinal measure method for video-based facerecognition that simultaneously learns ordinal filters and structured ordinalfeatures. The problem is posed as a non-convex integer program problem thatincludes two parts. The first part learns stable ordinal filters to projectvideo data into a large-margin ordinal space. The second seeks self-correctingand discrete codes by balancing the projected data and a rank-one ordinalmatrix in a structured low-rank way. Unsupervised and supervised structures areconsidered for the ordinal matrix. In addition, as a complement to hierarchicalstructures, deep feature representations are integrated into our method toenhance coding stability. An alternating minimization method is employed tohandle the discrete and low-rank constraints, yielding high-quality codes thatcapture prior structures well. Experimental results on three commonly used facevideo databases show that our method with a simple voting classifier canachieve state-of-the-art recognition rates using fewer features and samples.
arxiv-11400-111 | Intrinsic Non-stationary Covariance Function for Climate Modeling | http://arxiv.org/pdf/1507.02356v1.pdf | author:Chintan A. Dalal, Vladimir Pavlovic, Robert E. Kopp category:stat.ML cs.LG published:2015-07-09 summary:Designing a covariance function that represents the underlying correlation isa crucial step in modeling complex natural systems, such as climate models.Geospatial datasets at a global scale usually suffer from non-stationarity andnon-uniformly smooth spatial boundaries. A Gaussian process regression using anon-stationary covariance function has shown promise for this task, as thiscovariance function adapts to the variable correlation structure of theunderlying distribution. In this paper, we generalize the non-stationarycovariance function to address the aforementioned global scale geospatialissues. We define this generalized covariance function as an intrinsicnon-stationary covariance function, because it uses intrinsic statistics of thesymmetric positive definite matrices to represent the characteristic lengthscale and, thereby, models the local stochastic process. Experiments on asynthetic and real dataset of relative sea level changes across the worlddemonstrate improvements in the error metrics for the regression estimatesusing our newly proposed approach.
arxiv-11400-112 | The Shadows of a Cycle Cannot All Be Paths | http://arxiv.org/pdf/1507.02355v1.pdf | author:Prosenjit Bose, Jean-Lou De Carufel, Michael G. Dobbins, Heuna Kim, Giovanni Viglietta category:cs.CG cs.CV math.MG published:2015-07-09 summary:A "shadow" of a subset $S$ of Euclidean space is an orthogonal projection of$S$ into one of the coordinate hyperplanes. In this paper we show that it isnot possible for all three shadows of a cycle (i.e., a simple closed curve) in$\mathbb R^3$ to be paths (i.e., simple open curves). We also show two contrasting results: the three shadows of a path in $\mathbbR^3$ can all be cycles (although not all convex) and, for every $d\geq 1$,there exists a $d$-sphere embedded in $\mathbb R^{d+2}$ whose $d+2$ shadowshave no holes (i.e., they deformation-retract onto a point).
arxiv-11400-113 | Achieving Synergy in Cognitive Behavior of Humanoids via Deep Learning of Dynamic Visuo-Motor-Attentional Coordination | http://arxiv.org/pdf/1507.02347v1.pdf | author:Jungsik Hwang, Minju Jung, Naveen Madapana, Jinhyung Kim, Minkyu Choi, Jun Tani category:cs.AI cs.LG cs.RO published:2015-07-09 summary:The current study examines how adequate coordination among differentcognitive processes including visual recognition, attention switching, actionpreparation and generation can be developed via learning of robots byintroducing a novel model, the Visuo-Motor Deep Dynamic Neural Network (VMDNN).The proposed model is built on coupling of a dynamic vision network, a motorgeneration network, and a higher level network allocated on top of these two.The simulation experiments using the iCub simulator were conducted forcognitive tasks including visual object manipulation responding to humangestures. The results showed that synergetic coordination can be developed viaiterative learning through the whole network when spatio-temporal hierarchy andtemporal one can be self-organized in the visual pathway and in the motorpathway, respectively, such that the higher level can manipulate them withabstraction.
arxiv-11400-114 | Neural Network Classifiers for Natural Food Products | http://arxiv.org/pdf/1507.02346v1.pdf | author:Jaderick P. Pabico, Alona V. De Grano, Alan L. Zarsuela category:cs.CV published:2015-07-09 summary:Two cheap, off-the-shelf machine vision systems (MVS), each using anartificial neural network (ANN) as classifier, were developed, improved andevaluated to automate the classification of tomato ripeness and acceptabilityof eggs, respectively. Six thousand color images of human-graded tomatoes and750 images of human-graded eggs were used to train, test, and validate severalmulti-layered ANNs. The ANNs output the corresponding grade of the produce byaccepting as inputs the spectral patterns of the background-less image. In bothMVS, the ANN with the highest validation rate was automatically chosen by aheuristic and its performance compared to that of the human graders'. Using thevalidation set, the MVS correctly graded 97.00\% and 86.00\% of the tomato andegg data, respectively. The human grader's, however, were measured to performat a daily average of 92.65\% and 72.67\% for tomato and egg grading,respectively. This results show that an ANN-based MVS is a potentialalternative to manual grading.
arxiv-11400-115 | An Extragradient-Based Alternating Direction Method for Convex Minimization | http://arxiv.org/pdf/1301.6308v3.pdf | author:Tianyi Lin, Shiqian Ma, Shuzhong Zhang category:math.OC stat.ML published:2013-01-27 summary:In this paper, we consider the problem of minimizing the sum of two convexfunctions subject to linear linking constraints. The classical alternatingdirection type methods usually assume that the two convex functions haverelatively easy proximal mappings. However, many problems arising fromstatistics, image processing and other fields have the structure that while oneof the two functions has easy proximal mapping, the other function is smoothlyconvex but does not have an easy proximal mapping. Therefore, the classicalalternating direction methods cannot be applied. To deal with the difficulty,we propose in this paper an alternating direction method based onextragradients. Under the assumption that the smooth function has a Lipschitzcontinuous gradient, we prove that the proposed method returns an$\epsilon$-optimal solution within $O(1/\epsilon)$ iterations. We apply theproposed method to solve a new statistical model called fused logisticregression. Our numerical experiments show that the proposed method performsvery well when solving the test problems. We also test the performance of theproposed method through solving the lasso problem arising from statistics andcompare the result with several existing efficient solvers for this problem;the results are very encouraging indeed.
arxiv-11400-116 | Multisection in the Stochastic Block Model using Semidefinite Programming | http://arxiv.org/pdf/1507.02323v1.pdf | author:Naman Agarwal, Afonso S. Bandeira, Konstantinos Koiliaris, Alexandra Kolla category:cs.DS math.PR stat.ML published:2015-07-08 summary:We consider the problem of identifying underlying community-like structuresin graphs. Towards this end we study the Stochastic Block Model (SBM) on$k$-clusters: a random model on $n=km$ vertices, partitioned in $k$ equal sizedclusters, with edges sampled independently across clusters with probability $q$and within clusters with probability $p$, $p>q$. The goal is to recover theinitial "hidden" partition of $[n]$. We study semidefinite programming (SDP)based algorithms in this context. In the regime $p = \frac{\alpha \log(m)}{m}$and $q = \frac{\beta \log(m)}{m}$ we show that a certain natural SDP basedalgorithm solves the problem of {\em exact recovery} in the $k$-community SBM,with high probability, whenever $\sqrt{\alpha} - \sqrt{\beta} > \sqrt{1}$, aslong as $k=o(\log n)$. This threshold is known to be the informationtheoretically optimal. We also study the case when $k=\theta(\log(n))$. In thiscase however we achieve recovery guarantees that no longer match the optimalcondition $\sqrt{\alpha} - \sqrt{\beta} > \sqrt{1}$, thus leaving achievingoptimality for this range an open question.
arxiv-11400-117 | End-to-end people detection in crowded scenes | http://arxiv.org/pdf/1506.04878v3.pdf | author:Russell Stewart, Mykhaylo Andriluka category:cs.CV published:2015-06-16 summary:Current people detectors operate either by scanning an image in a slidingwindow fashion or by classifying a discrete set of proposals. We propose amodel that is based on decoding an image into a set of people detections. Oursystem takes an image as input and directly outputs a set of distinct detectionhypotheses. Because we generate predictions jointly, common post-processingsteps such as non-maximum suppression are unnecessary. We use a recurrent LSTMlayer for sequence generation and train our model end-to-end with a new lossfunction that operates on sets of detections. We demonstrate the effectivenessof our approach on the challenging task of detecting people in crowded scenes.
arxiv-11400-118 | Feature Representation in Convolutional Neural Networks | http://arxiv.org/pdf/1507.02313v1.pdf | author:Ben Athiwaratkun, Keegan Kang category:cs.CV published:2015-07-08 summary:Convolutional Neural Networks (CNNs) are powerful models that achieveimpressive results for image classification. In addition, pre-trained CNNs arealso useful for other computer vision tasks as generic feature extractors. Thispaper aims to gain insight into the feature aspect of CNN and demonstrate otheruses of CNN features. Our results show that CNN feature maps can be used withRandom Forests and SVM to yield classification results that outperforms theoriginal CNN. A CNN that is less than optimal (e.g. not fully trained oroverfitting) can also extract features for Random Forest/SVM that yieldcompetitive classification accuracy. In contrast to the literature which usesthe top-layer activations as feature representation of images for other tasks,using lower-layer features can yield better results for classification.
arxiv-11400-119 | A Hierarchical Recurrent Encoder-Decoder For Generative Context-Aware Query Suggestion | http://arxiv.org/pdf/1507.02221v1.pdf | author:Alessandro Sordoni, Yoshua Bengio, Hossein Vahabi, Christina Lioma, Jakob G. Simonsen, Jian-Yun Nie category:cs.NE cs.IR published:2015-07-08 summary:Users may strive to formulate an adequate textual query for their informationneed. Search engines assist the users by presenting query suggestions. Topreserve the original search intent, suggestions should be context-aware andaccount for the previous queries issued by the user. Achieving contextawareness is challenging due to data sparsity. We present a probabilisticsuggestion model that is able to account for sequences of previous queries ofarbitrary lengths. Our novel hierarchical recurrent encoder-decoderarchitecture allows the model to be sensitive to the order of queries in thecontext while avoiding data sparsity. Additionally, our model can suggest forrare, or long-tail, queries. The produced suggestions are synthetic and aresampled one word at a time, using computationally cheap decoding techniques.This is in contrast to current synthetic suggestion models relying upon machinelearning pipelines and hand-engineered feature sets. Results show that itoutperforms existing context-aware approaches in a next query predictionsetting. In addition to query suggestion, our model is general enough to beused in a variety of other applications.
arxiv-11400-120 | Intersecting Faces: Non-negative Matrix Factorization With New Guarantees | http://arxiv.org/pdf/1507.02189v1.pdf | author:Rong Ge, James Zou category:cs.LG stat.ML published:2015-07-08 summary:Non-negative matrix factorization (NMF) is a natural model of admixture andis widely used in science and engineering. A plethora of algorithms have beendeveloped to tackle NMF, but due to the non-convex nature of the problem, thereis little guarantee on how well these methods work. Recently a surge ofresearch have focused on a very restricted class of NMFs, called separable NMF,where provably correct algorithms have been developed. In this paper, wepropose the notion of subset-separable NMF, which substantially generalizes theproperty of separability. We show that subset-separability is a naturalnecessary condition for the factorization to be unique or to have minimumvolume. We developed the Face-Intersect algorithm which provably andefficiently solves subset-separable NMF under natural conditions, and we provethat our algorithm is robust to small noise. We explored the performance ofFace-Intersect on simulations and discuss settings where it empiricallyoutperformed the state-of-art methods. Our work is a step towards findingprovably correct algorithms that solve large classes of NMF problems.
arxiv-11400-121 | AutoCompete: A Framework for Machine Learning Competition | http://arxiv.org/pdf/1507.02188v1.pdf | author:Abhishek Thakur, Artus Krohn-Grimberghe category:stat.ML cs.LG published:2015-07-08 summary:In this paper, we propose AutoCompete, a highly automated machine learningframework for tackling machine learning competitions. This framework has beenlearned by us, validated and improved over a period of more than two years byparticipating in online machine learning competitions. It aims at minimizinghuman interference required to build a first useful predictive model and toassess the practical difficulty of a given machine learning challenge. Theproposed system helps in identifying data types, choosing a machine learn- ingmodel, tuning hyper-parameters, avoiding over-fitting and optimization for aprovided evaluation metric. We also observe that the proposed system producesbetter (or comparable) results with less runtime as compared to otherapproaches.
arxiv-11400-122 | Iris Recognition Using Scattering Transform and Textural Features | http://arxiv.org/pdf/1507.02177v1.pdf | author:Shervin Minaee, AmirAli Abdolrashidi, Yao Wang category:cs.CV published:2015-07-08 summary:Iris recognition has drawn a lot of attention since the mid-twentiethcentury. Among all biometric features, iris is known to possess a rich set offeatures. Different features have been used to perform iris recognition in thepast. In this paper, two powerful sets of features are introduced to be usedfor iris recognition: scattering transform-based features and texturalfeatures. PCA is also applied on the extracted features to reduce thedimensionality of the feature vector while preserving most of the informationof its initial value. Minimum distance classifier is used to perform templatematching for each new test sample. The proposed scheme is tested on awell-known iris database, and showed promising results with the best accuracyrate of 99.2%.
arxiv-11400-123 | Towards Good Practices for Very Deep Two-Stream ConvNets | http://arxiv.org/pdf/1507.02159v1.pdf | author:Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao category:cs.CV published:2015-07-08 summary:Deep convolutional networks have achieved great success for objectrecognition in still images. However, for action recognition in videos, theimprovement of deep convolutional networks is not so evident. We argue thatthere are two reasons that could probably explain this result. First thecurrent network architectures (e.g. Two-stream ConvNets) are relatively shallowcompared with those very deep models in image domain (e.g. VGGNet, GoogLeNet),and therefore their modeling capacity is constrained by their depth. Second,probably more importantly, the training dataset of action recognition isextremely small compared with the ImageNet dataset, and thus it will be easy toover-fit on the training dataset. To address these issues, this report presents very deep two-stream ConvNetsfor action recognition, by adapting recent very deep architectures into videodomain. However, this extension is not easy as the size of action recognitionis quite small. We design several good practices for the training of very deeptwo-stream ConvNets, namely (i) pre-training for both spatial and temporalnets, (ii) smaller learning rates, (iii) more data augmentation techniques,(iv) high drop out ratio. Meanwhile, we extend the Caffe toolbox into Multi-GPUimplementation with high computational efficiency and low memory consumption.We verify the performance of very deep two-stream ConvNets on the dataset ofUCF101 and it achieves the recognition accuracy of $91.4\%$.
arxiv-11400-124 | An Empirical Study on Budget-Aware Online Kernel Algorithms for Streams of Graphs | http://arxiv.org/pdf/1507.02158v1.pdf | author:Giovanni Da San Martino, NicolÃ² Navarin, Alessandro Sperduti category:cs.LG published:2015-07-08 summary:Kernel methods are considered an effective technique for on-line learning.Many approaches have been developed for compactly representing the dualsolution of a kernel method when the problem imposes memory constraints.However, in literature no work is specifically tailored to streams of graphs.Motivated by the fact that the size of the feature space representation of manystate-of-the-art graph kernels is relatively small and thus it is explicitlycomputable, we study whether executing kernel algorithms in the feature spacecan be more effective than the classical dual approach. We propose threedifferent algorithms and various strategies for managing the budget. Efficiencyand efficacy of the proposed approaches are experimentally assessed onrelatively large graph streams exhibiting concept drift. It turns out that,when strict memory budget constraints have to be enforced, working in featurespace, given the current state of the art on graph kernels, is more than aviable alternative to dual approaches, both in terms of speed andclassification performance.
arxiv-11400-125 | Double-Base Asymmetric AdaBoost | http://arxiv.org/pdf/1507.02154v1.pdf | author:Iago Landesa-VÃ¡zquez, JosÃ© Luis Alba-Castro category:cs.CV cs.AI cs.LG published:2015-07-08 summary:Based on the use of different exponential bases to define class-dependenterror bounds, a new and highly efficient asymmetric boosting scheme, coined asAdaBoostDB (Double-Base), is proposed. Supported by a fully theoreticalderivation procedure, unlike most of the other approaches in the literature,our algorithm preserves all the formal guarantees and properties of original(cost-insensitive) AdaBoost, similarly to the state-of-the-art Cost-SensitiveAdaBoost algorithm. However, the key advantage of AdaBoostDB is that our novelderivation scheme enables an extremely efficient conditional search procedure,dramatically improving and simplifying the training phase of the algorithm.Experiments, both over synthetic and real datasets, reveal that AdaBoostDB isable to save over 99% training time with regard to Cost-Sensitive AdaBoost,providing the same cost-sensitive results. This computational advantage ofAdaBoostDB can make a difference in problems managing huge pools of weakclassifiers in which boosting techniques are commonly used.
arxiv-11400-126 | SAR Imaging of Moving Target based on Knowledge-aided Two-dimensional Autofocus | http://arxiv.org/pdf/1507.02150v1.pdf | author:Xinhua Mao category:cs.IT cs.CV math.IT published:2015-07-08 summary:Due to uncertainty on target's motion, the range cell migration (RCM) andazimuth phase error (APE) of moving targets can't be completely compensated insynthetic aperture radar (SAR) processing. Therefore, moving targets oftenappear two-dimensional (2-D) defocused in SAR images. In this paper, a 2-Dautofocus method for refocusing defocused moving targets in SAR images ispresented. The new method only requires a direct estimate of APE, while theresidual 2-D phase error ( or RCM) is computed from the estimated APE byexploiting the analytical relationship between the 2-D phase error ( or RCM)and APE. Because the parameter estimation is performed in the reduced-dimensionspace by exploiting prior knowledge on phase error structure, the proposedapproach offers clear advantages in both computational efficiency andestimation accuracy.
arxiv-11400-127 | Spotlight the Negatives: A Generalized Discriminative Latent Model | http://arxiv.org/pdf/1507.02144v1.pdf | author:Hossein Azizpour, Mostafa Arefiyan, Sobhan Naderi Parizi, Stefan Carlsson category:cs.CV published:2015-07-08 summary:Discriminative latent variable models (LVM) are frequently applied to variousvisual recognition tasks. In these systems the latent (hidden) variablesprovide a formalism for modeling structured variation of visual features.Conventionally, latent variables are de- fined on the variation of theforeground (positive) class. In this work we augment LVMs to include negativelatent variables corresponding to the background class. We formalize thescoring function of such a generalized LVM (GLVM). Then we discuss a frameworkfor learning a model based on the GLVM scoring function. We theoreticallyshowcase how some of the current visual recognition methods can benefit fromthis generalization. Finally, we experiment on a generalized form of DeformablePart Models with negative latent variables and show significant improvements ontwo different detection tasks.
arxiv-11400-128 | Mining and Analyzing the Future Works in Scientific Articles | http://arxiv.org/pdf/1507.02140v1.pdf | author:Yue Hu, Xiaojun Wan category:cs.DL cs.CL cs.IR published:2015-07-08 summary:Future works in scientific articles are valuable for researchers and they canguide researchers to new research directions or ideas. In this paper, we minethe future works in scientific articles in order to 1) provide an insight forfuture work analysis and 2) facilitate researchers to search and browse futureworks in a research area. First, we study the problem of future work extractionand propose a regular expression based method to address the problem. Second,we define four different categories for the future works by observing the dataand investigate the multi-class future work classification problem. Third, weapply the extraction method and the classification model to a paper dataset inthe computer science field and conduct a further analysis of the future works.Finally, we design a prototype system to search and demonstrate the futureworks mined from the scientific papers. Our evaluation results show that ourextraction method can get high precision and recall values and ourclassification model can also get good results and it outperforms severalbaseline models. Further analysis of the future work sentences also indicatesinteresting results.
arxiv-11400-129 | Application of Deep Neural Network in Estimation of the Weld Bead Parameters | http://arxiv.org/pdf/1502.04187v2.pdf | author:Soheil Keshmiri, Xin Zheng, Chee Meng Chew, Chee Khiang Pang category:cs.LG published:2015-02-14 summary:We present a deep learning approach to estimation of the bead parameters inwelding tasks. Our model is based on a four-hidden-layer neural networkarchitecture. More specifically, the first three hidden layers of thisarchitecture utilize Sigmoid function to produce their respective intermediateoutputs. On the other hand, the last hidden layer uses a linear transformationto generate the final output of this architecture. This transforms our deepnetwork architecture from a classifier to a non-linear regression model. Wecompare the performance of our deep network with a selected number of resultsin the literature to show a considerable improvement in reducing the errors inestimation of these values. Furthermore, we show its scalability on estimatingthe weld bead parameters with same level of accuracy on combination of datasetsthat pertain to different welding techniques. This is a nontrivial result thatis counter-intuitive to the general belief in this field of research.
arxiv-11400-130 | The Role of Pragmatics in Legal Norm Representation | http://arxiv.org/pdf/1507.02086v1.pdf | author:Shashishekar Ramakrishna, Lukasz Gorski, Adrian Paschke category:cs.CL cs.AI 68T30 J.1; I.2.1 published:2015-07-08 summary:Despite the 'apparent clarity' of a given legal provision, its applicationmay result in an outcome that does not exactly conform to the semantic level ofa statute. The vagueness within a legal text is induced intentionally toaccommodate all possible scenarios under which such norms should be applied,thus making the role of pragmatics an important aspect also in therepresentation of a legal norm and reasoning on top of it. The notion ofpragmatics considered in this paper does not focus on the aspects associatedwith judicial decision making. The paper aims to shed light on the aspects ofpragmatics in legal linguistics, mainly focusing on the domain of patent law,only from a knowledge representation perspective. The philosophical discussionspresented in this paper are grounded based on the legal theories from Grice andMarmor.
arxiv-11400-131 | Shedding Light on the Asymmetric Learning Capability of AdaBoost | http://arxiv.org/pdf/1507.02084v1.pdf | author:Iago Landesa-VÃ¡zquez, JosÃ© Luis Alba-Castro category:cs.LG cs.AI cs.CV published:2015-07-08 summary:In this paper, we propose a different insight to analyze AdaBoost. Thisanalysis reveals that, beyond some preconceptions, AdaBoost can be directlyused as an asymmetric learning algorithm, preserving all its theoreticalproperties. A novel class-conditional description of AdaBoost, which models theactual asymmetric behavior of the algorithm, is presented.
arxiv-11400-132 | Ego-Object Discovery | http://arxiv.org/pdf/1504.01639v2.pdf | author:Marc BolaÃ±os, Petia Radeva category:cs.CV cs.AI published:2015-04-07 summary:Lifelogging devices are spreading faster everyday. This growth can representgreat benefits to develop methods for extraction of meaningful informationabout the user wearing the device and his/her environment. In this paper, wepropose a semi-supervised strategy for easily discovering objects relevant tothe person wearing a first-person camera. Given an egocentric video/imagessequence acquired by the camera, our algorithm uses both the appearanceextracted by means of a convolutional neural network and an object refillmethodology that allows to discover objects even in case of small amount ofobject appearance in the collection of images. An SVM filtering strategy isapplied to deal with the great part of the False Positive object candidatesfound by most of the state of the art object detectors. We validate our methodon a new egocentric dataset of 4912 daily images acquired by 4 persons as wellas on both PASCAL 2012 and MSRC datasets. We obtain for all of them resultsthat largely outperform the state of the art approach. We make public both theEDUB dataset and the algorithm code.
arxiv-11400-133 | Multi-Document Summarization via Discriminative Summary Reranking | http://arxiv.org/pdf/1507.02062v1.pdf | author:Xiaojun Wan, Ziqiang Cao, Furu Wei, Sujian Li, Ming Zhou category:cs.CL published:2015-07-08 summary:Existing multi-document summarization systems usually rely on a specificsummarization model (i.e., a summarization method with a specific parametersetting) to extract summaries for different document sets with differenttopics. However, according to our quantitative analysis, none of the existingsummarization models can always produce high-quality summaries for differentdocument sets, and even a summarization model with good overall performance mayproduce low-quality summaries for some document sets. On the contrary, abaseline summarization model may produce high-quality summaries for somedocument sets. Based on the above observations, we treat the summaries producedby different summarization models as candidate summaries, and then explorediscriminative reranking techniques to identify high-quality summaries from thecandidates for difference document sets. We propose to extract a set ofcandidate summaries for each document set based on an ILP framework, and thenleverage Ranking SVM for summary reranking. Various useful features have beendeveloped for the reranking process, including word-level features,sentence-level features and summary-level features. Evaluation results on thebenchmark DUC datasets validate the efficacy and robustness of our proposedapproach.
arxiv-11400-134 | On Graduated Optimization for Stochastic Non-Convex Problems | http://arxiv.org/pdf/1503.03712v2.pdf | author:Elad Hazan, Kfir Y. Levy, Shai Shalev-Shwartz category:cs.LG math.OC 68 published:2015-03-12 summary:The graduated optimization approach, also known as the continuation method,is a popular heuristic to solving non-convex problems that has received renewedinterest over the last decade. Despite its popularity, very little is known interms of theoretical convergence analysis. In this paper we describe a newfirst-order algorithm based on graduated optimiza- tion and analyze itsperformance. We characterize a parameterized family of non- convex functionsfor which this algorithm provably converges to a global optimum. In particular,we prove that the algorithm converges to an {\epsilon}-approximate solutionwithin O(1/\epsilon^2) gradient-based steps. We extend our algorithm andanalysis to the setting of stochastic non-convex optimization with noisygradient feedback, attaining the same convergence rate. Additionally, wediscuss the setting of zero-order optimization, and devise a a variant of ouralgorithm which converges at rate of O(d^2/\epsilon^4).
arxiv-11400-135 | Generating Navigable Semantic Maps from Social Sciences Corpora | http://arxiv.org/pdf/1507.02020v1.pdf | author:Thierry Poibeau, Pablo Ruiz category:cs.CL cs.AI cs.IR published:2015-07-08 summary:It is now commonplace to observe that we are facing a deluge of onlineinformation. Researchers have of course long acknowledged the potential valueof this information since digital traces make it possible to directly observe,describe and analyze social facts, and above all the co-evolution of ideas andcommunities over time. However, most online information is expressed throughtext, which means it is not directly usable by machines, since computersrequire structured, organized and typed information in order to be able tomanipulate it. Our goal is thus twofold: 1. Provide new natural languageprocessing techniques aiming at automatically extracting relevant informationfrom texts, especially in the context of social sciences, and connect thesepieces of information so as to obtain relevant socio-semantic networks; 2.Provide new ways of exploring these socio-semantic networks, thanks to toolsallowing one to dynamically navigate these networks, de-construct andre-construct them interactively, from different points of view following theneeds expressed by domain experts.
arxiv-11400-136 | Hindi to English Transfer Based Machine Translation System | http://arxiv.org/pdf/1507.02012v1.pdf | author:Akanksha Gehlot, Vaishali Sharma, Shashi Pal Singh, Ajai Kumar category:cs.CL published:2015-07-08 summary:In large societies like India there is a huge demand to convert one humanlanguage into another. Lots of work has been done in this area. Many transferbased MTS have developed for English to other languages, as MANTRA CDAC Pune,MATRA CDAC Pune, SHAKTI IISc Bangalore and IIIT Hyderabad. Still there is alittle work done for Hindi to other languages. Currently we are working on it.In this paper we focus on designing a system, that translate the document fromHindi to English by using transfer based approach. This system takes an inputtext check its structure through parsing. Reordering rules are used to generatethe text in target language. It is better than Corpus Based MTS because CorpusBased MTS require large amount of word aligned data for translation that is notavailable for many languages while Transfer Based MTS requires only knowledgeof both the languages(source language and target language) to make transferrules. We get correct translation for simple assertive sentences and almostcorrect for complex and compound sentences.
arxiv-11400-137 | A Bayesian Approach for Online Classifier Ensemble | http://arxiv.org/pdf/1507.02011v1.pdf | author:Qinxun Bai, Henry Lam, Stan Sclaroff category:cs.LG published:2015-07-08 summary:We propose a Bayesian approach for recursively estimating the classifierweights in online learning of a classifier ensemble. In contrast with pastmethods, such as stochastic gradient descent or online boosting, our approachestimates the weights by recursively updating its posterior distribution. For aspecified class of loss functions, we show that it is possible to formulate asuitably defined likelihood function and hence use the posterior distributionas an approximation to the global empirical loss minimizer. If the stream oftraining data is sampled from a stationary process, we can also show that ourapproach admits a superior rate of convergence to the expected loss minimizerthan is possible with standard stochastic gradient descent. In experiments withreal-world datasets, our formulation often performs better thanstate-of-the-art stochastic gradient descent and online boosting algorithms.
arxiv-11400-138 | Multi-modal Sensor Registration for Vehicle Perception via Deep Neural Networks | http://arxiv.org/pdf/1412.7006v2.pdf | author:Michael Giering, Vivek Venugopalan, Kishore Reddy category:cs.CV cs.LG cs.NE published:2014-12-22 summary:The ability to simultaneously leverage multiple modes of sensor informationis critical for perception of an automated vehicle's physical surroundings.Spatio-temporal alignment of registration of the incoming information is oftena prerequisite to analyzing the fused data. The persistence and reliability ofmulti-modal registration is therefore the key to the stability of decisionsupport systems ingesting the fused information. LiDAR-video systems like onthose many driverless cars are a common example of where keeping the LiDAR andvideo channels registered to common physical features is important. We developa deep learning method that takes multiple channels of heterogeneous data, todetect the misalignment of the LiDAR-video inputs. A number of variations weretested on the Ford LiDAR-video driving test data set and will be discussed. Tothe best of our knowledge the use of multi-modal deep convolutional neuralnetworks for dynamic real-time LiDAR-video registration has not been presented.
arxiv-11400-139 | Occlusion Edge Detection in RGB-D Frames using Deep Convolutional Networks | http://arxiv.org/pdf/1412.7007v3.pdf | author:Soumik Sarkar, Vivek Venugopalan, Kishore Reddy, Michael Giering, Julian Ryde, Navdeep Jaitly category:cs.CV cs.LG cs.NE published:2014-12-22 summary:Occlusion edges in images which correspond to range discontinuity in thescene from the point of view of the observer are an important prerequisite formany vision and mobile robot tasks. Although they can be extracted from rangedata however extracting them from images and videos would be extremelybeneficial. We trained a deep convolutional neural network (CNN) to identifyocclusion edges in images and videos with both RGB-D and RGB inputs. The use ofCNN avoids hand-crafting of features for automatically isolating occlusionedges and distinguishing them from appearance edges. Other than quantitativeocclusion edge detection results, qualitative results are provided todemonstrate the trade-off between high resolution analysis and frame-levelcomputation time which is critical for real-time robotics applications.
arxiv-11400-140 | Comparing persistence diagrams through complex vectors | http://arxiv.org/pdf/1505.01335v2.pdf | author:Barbara Di Fabio, Massimo Ferri category:math.AT cs.CV published:2015-05-06 summary:The natural pseudo-distance of spaces endowed with filtering functions isprecious for shape classification and retrieval; its optimal estimate comingfrom persistence diagrams is the bottleneck distance, which unfortunatelysuffers from combinatorial explosion. A possible algebraic representation ofpersistence diagrams is offered by complex polynomials; since far polynomialsrepresent far persistence diagrams, a fast comparison of the coefficientvectors can reduce the size of the database to be classified by the bottleneckdistance. This article explores experimentally three transformations fromdiagrams to polynomials and three distances between the complex vectors ofcoefficients.
arxiv-11400-141 | Memcomputing NP-complete problems in polynomial time using polynomial resources and collective states | http://arxiv.org/pdf/1411.4798v3.pdf | author:Fabio L. Traversa, Chiara Ramella, Fabrizio Bonani, Massimiliano Di Ventra category:cs.ET cs.NE published:2014-11-18 summary:Memcomputing is a novel non-Turing paradigm of computation that usesinteracting memory cells (memprocessors for short) to store and processinformation on the same physical platform. It was recently provedmathematically that memcomputing machines have the same computational power ofnon-deterministic Turing machines. Therefore, they can solve NP-completeproblems in polynomial time and, using the appropriate architecture, withresources that only grow polynomially with the input size. The reason for thiscomputational power stems from properties inspired by the brain and shared byany universal memcomputing machine, in particular intrinsic parallelism andinformation overhead, namely the capability of compressing information in thecollective state of the memprocessor network. Here, we show an experimentaldemonstration of an actual memcomputing architecture that solves theNP-complete version of the subset-sum problem in only one step and is composedof a number of memprocessors that scales linearly with the size of the problem.We have fabricated this architecture using standard microelectronic technologyso that it can be easily realized in any laboratory setting. Even though theparticular machine presented here is eventually limited by noise--and will thusrequire error-correcting codes to scale to an arbitrary number ofmemprocessors--it represents the first proof-of-concept of a machine capable ofworking with the collective state of interacting memory cells, unlike thepresent-day single-state machines built using the von Neumann architecture.
arxiv-11400-142 | Correlated Multiarmed Bandit Problem: Bayesian Algorithms and Regret Analysis | http://arxiv.org/pdf/1507.01160v2.pdf | author:Vaibhav Srivastava, Paul Reverdy, Naomi Ehrich Leonard category:math.OC cs.LG stat.ML published:2015-07-05 summary:We consider the correlated multiarmed bandit (MAB) problem in which therewards associated with each arm are modeled by a multivariate Gaussian randomvariable, and we investigate the influence of the assumptions in the Bayesianprior on the performance of the upper credible limit (UCL) algorithm and a newcorrelated UCL algorithm. We rigorously characterize the influence of accuracy,confidence, and correlation scale in the prior on the decision-makingperformance of the algorithms. Our results show how priors and correlationstructure can be leveraged to improve performance.
arxiv-11400-143 | Memcomputing with membrane memcapacitive systems | http://arxiv.org/pdf/1410.3541v2.pdf | author:Yuriy V. Pershin, Fabio L. Traversa, Massimiliano Di Ventra category:cs.ET cs.NE published:2014-10-14 summary:We show theoretically that networks of membrane memcapacitive systems --capacitors with memory made out of membrane materials -- can be used to performa complete set of logic gates in a massively parallel way by simply changingthe external input amplitudes, but not the topology of the network. Thispolymorphism is an important characteristic of memcomputing (computing withmemories) that closely reproduces one of the main features of the brain. Apractical realization of these membrane memcapacitive systems, using, e.g.,graphene or other 2D materials, would be a step forward towards a solid-staterealization of memcomputing with passive devices.
arxiv-11400-144 | Wasserstein Training of Boltzmann Machines | http://arxiv.org/pdf/1507.01972v1.pdf | author:GrÃ©goire Montavon, Klaus-Robert MÃ¼ller, Marco Cuturi category:stat.ML cs.LG published:2015-07-07 summary:The Boltzmann machine provides a useful framework to learn highly complex,multimodal and multiscale data distributions that occur in the real world. Thedefault method to learn its parameters consists of minimizing theKullback-Leibler (KL) divergence from training samples to the Boltzmann model.We propose in this work a novel approach for Boltzmann training which assumesthat a meaningful metric between observations is given. This metric can berepresented by the Wasserstein distance between distributions, for which wederive a gradient with respect to the model parameters. Minimization of thisnew Wasserstein objective leads to generative models that are better whenconsidering the metric and that have a cluster-like structure. We demonstratethe practical potential of these models for data completion and denoising, forwhich the metric between observations plays a crucial role.
arxiv-11400-145 | Incremental Adaptation Strategies for Neural Network Language Models | http://arxiv.org/pdf/1412.6650v4.pdf | author:Aram Ter-Sarkisov, Holger Schwenk, Loic Barrault, Fethi Bougares category:cs.NE cs.CL cs.LG published:2014-12-20 summary:It is today acknowledged that neural network language models outperformbackoff language models in applications like speech recognition or statisticalmachine translation. However, training these models on large amounts of datacan take several days. We present efficient techniques to adapt a neuralnetwork language model to new data. Instead of training a completely new modelor relying on mixture approaches, we propose two new methods: continuedtraining on resampled data or insertion of adaptation layers. We presentexperimental results in an CAT environment where the post-edits of professionaltranslators are used to improve an SMT system. Both methods are very fast andachieve significant improvements without overfitting the small adaptation data.
arxiv-11400-146 | Top-N recommendations in the presence of sparsity: An NCD-based approach | http://arxiv.org/pdf/1507.00043v2.pdf | author:Athanasios N. Nikolakopoulos, John D. Garofalakis category:cs.IR cs.AI stat.ML published:2015-06-30 summary:Making recommendations in the presence of sparsity is known to present one ofthe most challenging problems faced by collaborative filtering methods. In thiswork we tackle this problem by exploiting the innately hierarchical structureof the item space following an approach inspired by the theory ofDecomposability. We view the itemspace as a Nearly Decomposable system and wedefine blocks of closely related elements and corresponding indirect proximitycomponents. We study the theoretical properties of the decomposition and wederive sufficient conditions that guarantee full item space coverage even incold-start recommendation scenarios. A comprehensive set of experiments on theMovieLens and the Yahoo!R2Music datasets, using several widely appliedperformance metrics, support our model's theoretically predicted properties andverify that NCDREC outperforms several state-of-the-art algorithms, in terms ofrecommendation accuracy, diversity and sparseness insensitivity.
arxiv-11400-147 | Unveiling the Political Agenda of the European Parliament Plenary: A Topical Analysis | http://arxiv.org/pdf/1505.07302v4.pdf | author:Derek Greene, James P. Cross category:cs.CL cs.CY published:2015-05-27 summary:This study analyzes political interactions in the European Parliament (EP) byconsidering how the political agenda of the plenary sessions has evolved overtime and the manner in which Members of the European Parliament (MEPs) havereacted to external and internal stimuli when making Parliamentary speeches. Itdoes so by considering the context in which speeches are made, and the contentof those speeches. To detect latent themes in legislative speeches over time,speech content is analyzed using a new dynamic topic modeling method, based ontwo layers of matrix factorization. This method is applied to a new corpus ofall English language legislative speeches in the EP plenary from the period1999-2014. Our findings suggest that the political agenda of the EP has evolvedsignificantly over time, is impacted upon by the committee structure of theParliament, and reacts to exogenous events such as EU Treaty referenda and theemergence of the Euro-crisis have a significant impact on what is beingdiscussed in Parliament.
arxiv-11400-148 | A Survey and Classification of Controlled Natural Languages | http://arxiv.org/pdf/1507.01701v1.pdf | author:Tobias Kuhn category:cs.CL published:2015-07-07 summary:What is here called controlled natural language (CNL) has traditionally beengiven many different names. Especially during the last four decades, a widevariety of such languages have been designed. They are applied to improvecommunication among humans, to improve translation, or to provide natural andintuitive representations for formal notations. Despite the apparentdifferences, it seems sensible to put all these languages under the sameumbrella. To bring order to the variety of languages, a general classificationscheme is presented here. A comprehensive survey of existing English-based CNLsis given, listing and describing 100 languages from 1930 until today.Classification of these languages reveals that they form a single scatteredcloud filling the conceptual space between natural languages such as English onthe one end and formal languages such as propositional logic on the other. Thegoal of this article is to provide a common terminology and a common model forCNL, to contribute to the understanding of their general nature, to provide astarting point for researchers interested in the area, and to help developersto make design decisions.
arxiv-11400-149 | Learning Tractable Probabilistic Models for Fault Localization | http://arxiv.org/pdf/1507.01698v1.pdf | author:Aniruddh Nath, Pedro Domingos category:cs.SE cs.LG published:2015-07-07 summary:In recent years, several probabilistic techniques have been applied tovarious debugging problems. However, most existing probabilistic debuggingsystems use relatively simple statistical models, and fail to generalize acrossmultiple programs. In this work, we propose Tractable Fault Localization Models(TFLMs) that can be learned from data, and probabilistically infer the locationof the bug. While most previous statistical debugging methods generalize overmany executions of a single program, TFLMs are trained on a corpus ofpreviously seen buggy programs, and learn to identify recurring patterns ofbugs. Widely-used fault localization techniques such as TARANTULA evaluate thesuspiciousness of each line in isolation; in contrast, a TFLM defines a jointprobability distribution over buggy indicator variables for each line. Jointdistributions with rich dependency structure are often computationallyintractable; TFLMs avoid this by exploiting recent developments in tractableprobabilistic models (specifically, Relational SPNs). Further, TFLMs canincorporate additional sources of information, including coverage-basedfeatures such as TARANTULA. We evaluate the fault localization performance ofTFLMs that include TARANTULA scores as features in the probabilistic model. Ourstudy shows that the learned TFLMs isolate bugs more effectively than previousstatistical methods or using TARANTULA directly.
arxiv-11400-150 | Developing Postfix-GP Framework for Symbolic Regression Problems | http://arxiv.org/pdf/1507.01687v1.pdf | author:Vipul K. Dabhi, Sanjay Chaudhary category:cs.NE published:2015-07-07 summary:This paper describes Postfix-GP system, postfix notation based GeneticProgramming (GP), for solving symbolic regression problems. It presents anobject-oriented architecture of Postfix-GP framework. It assists the user inunderstanding of the implementation details of various components ofPostfix-GP. Postfix-GP provides graphical user interface which allows user toconfigure the experiment, to visualize evolved solutions, to analyze GP run,and to perform out-of-sample predictions. The use of Postfix-GP is demonstratedby solving the benchmark symbolic regression problem. Finally, features ofPostfix-GP framework are compared with that of other GP systems.
arxiv-11400-151 | Semiblind Hyperspectral Unmixing in the Presence of Spectral Library Mismatches | http://arxiv.org/pdf/1507.01661v1.pdf | author:Xiao Fu, Wing-Kin Ma, JosÃ© Bioucas-Dias, Tsung-Han Chan category:stat.ML published:2015-07-07 summary:The dictionary-aided sparse regression (SR) approach has recently emerged asa promising alternative to hyperspectral unmixing (HU) in remote sensing. Byusing an available spectral library as a dictionary, the SR approach identifiesthe underlying materials in a given hyperspectral image by selecting a smallsubset of spectral samples in the dictionary to represent the whole image. Adrawback with the current SR developments is that an actual spectral signaturein the scene is often assumed to have zero mismatch with its correspondingdictionary sample, and such an assumption is considered too ideal in practice.In this paper, we tackle the spectral signature mismatch problem by proposing adictionary-adjusted nonconvex sparsity-encouraging regression (DANSER)framework. The main idea is to incorporate dictionary correcting variables inan SR formulation. A simple and low per-iteration complexity algorithm istailor-designed for practical realization of DANSER. Using the same dictionarycorrecting idea, we also propose a robust subspace solution for dictionarypruning. Extensive simulations and real-data experiments show that the proposedmethod is effective in mitigating the undesirable spectral signature mismatcheffects.
arxiv-11400-152 | Jointly Embedding Relations and Mentions for Knowledge Population | http://arxiv.org/pdf/1504.01683v4.pdf | author:Miao Fan, Kai Cao, Yifan He, Ralph Grishman category:cs.CL published:2015-04-07 summary:This paper contributes a joint embedding model for predicting relationsbetween a pair of entities in the scenario of relation inference. It differsfrom most stand-alone approaches which separately operate on either knowledgebases or free texts. The proposed model simultaneously learns low-dimensionalvector representations for both triplets in knowledge repositories and thementions of relations in free texts, so that we can leverage the evidence bothresources to make more accurate predictions. We use NELL to evaluate theperformance of our approach, compared with cutting-edge methods. Results ofextensive experiments show that our model achieves significant improvement onrelation extraction.
arxiv-11400-153 | A model of sensory neural responses in the presence of unknown modulatory inputs | http://arxiv.org/pdf/1507.01497v2.pdf | author:Neil C. Rabinowitz, Robbe L. T. Goris, Johannes BallÃ©, Eero P. Simoncelli category:q-bio.NC stat.ML published:2015-07-06 summary:Neural responses are highly variable, and some portion of this variabilityarises from fluctuations in modulatory factors that alter their gain, such asadaptation, attention, arousal, expected or actual reward, emotion, and localmetabolic resource availability. Regardless of their origin, fluctuations inthese signals can confound or bias the inferences that one derives from spikingresponses. Recent work demonstrates that for sensory neurons, these effects canbe captured by a modulated Poisson model, whose rate is the product of astimulus-driven response function and an unknown modulatory signal. Here, weextend this model, by incorporating explicit modulatory elements that are known(specifically, spike-history dependence, as in previous models), and byconstraining the remaining latent modulatory signals to be smooth in time. Wedevelop inference procedures for fitting the entire model, includinghyperparameters, via evidence optimization, and apply these to simulated data,and to responses of ferret auditory midbrain and cortical neurons to complexsounds. We show that integrating out the latent modulators yields better (ormore readily-interpretable) receptive field estimates than a standard Poissonmodel. Conversely, integrating out the stimulus dependence yields estimates ofthe slowly-varying latent modulators.
arxiv-11400-154 | Multi-Step Stochastic ADMM in High Dimensions: Applications to Sparse Optimization and Noisy Matrix Decomposition | http://arxiv.org/pdf/1402.5131v6.pdf | author:Hanie Sedghi, Anima Anandkumar, Edmond Jonckheere category:cs.LG math.OC stat.ML published:2014-02-20 summary:We propose an efficient ADMM method with guarantees for high-dimensionalproblems. We provide explicit bounds for the sparse optimization problem andthe noisy matrix decomposition problem. For sparse optimization, we establishthat the modified ADMM method has an optimal convergence rate of$\mathcal{O}(s\log d/T)$, where $s$ is the sparsity level, $d$ is the datadimension and $T$ is the number of steps. This matches with the minimax lowerbounds for sparse estimation. For matrix decomposition into sparse and low rankcomponents, we provide the first guarantees for any online method, and prove aconvergence rate of $\tilde{\mathcal{O}}((s+r)\beta^2(p) /T) +\mathcal{O}(1/p)$ for a $p\times p$ matrix, where $s$ is the sparsity level,$r$ is the rank and $\Theta(\sqrt{p})\leq \beta(p)\leq \Theta(p)$. Ourguarantees match the minimax lower bound with respect to $s,r$ and $T$. Inaddition, we match the minimax lower bound with respect to the matrix dimension$p$, i.e. $\beta(p)=\Theta(\sqrt{p})$, for many important statistical modelsincluding the independent noise model, the linear Bayesian network and thelatent Gaussian graphical model under some conditions. Our ADMM method is basedon epoch-based annealing and consists of inexpensive steps which involveprojections on to simple norm balls. Experiments show that for both sparseoptimization and matrix decomposition problems, our algorithm outperforms thestate-of-the-art methods. In particular, we reach higher accuracy with sametime complexity.
arxiv-11400-155 | Reflections on Sentiment/Opinion Analysis | http://arxiv.org/pdf/1507.01636v1.pdf | author:Jiwei Li, Eduard Hovy category:cs.CL published:2015-07-06 summary:In this paper, we described possible directions for deeper understanding,helping bridge the gap between psychology / cognitive science and computationalapproaches in sentiment/opinion analysis literature. We focus on the opinionholder's underlying needs and their resultant goals, which, in a utilitarianmodel of sentiment, provides the basis for explaining the reason a sentimentvalence is held. While these thoughts are still immature, scattered,unstructured, and even imaginary, we believe that these perspectives mightsuggest fruitful avenues for various kinds of future work.
arxiv-11400-156 | Towards Faster Rates and Oracle Property for Low-Rank Matrix Estimation | http://arxiv.org/pdf/1505.04780v2.pdf | author:Huan Gui, Quanquan Gu category:stat.ML published:2015-05-18 summary:We present a unified framework for low-rank matrix estimation with nonconvexpenalties. We first prove that the proposed estimator attains a fasterstatistical rate than the traditional low-rank matrix estimator with nuclearnorm penalty. Moreover, we rigorously show that under a certain condition onthe magnitude of the nonzero singular values, the proposed estimator enjoysoracle property (i.e., exactly recovers the true rank of the matrix), besidesattaining a faster rate. As far as we know, this is the first work thatestablishes the theory of low-rank matrix estimation with nonconvex penalties,confirming the advantages of nonconvex penalties for matrix completion.Numerical experiments on both synthetic and real world datasets corroborate ourtheory.
arxiv-11400-157 | Emphatic Temporal-Difference Learning | http://arxiv.org/pdf/1507.01569v1.pdf | author:A. Rupam Mahmood, Huizhen Yu, Martha White, Richard S. Sutton category:cs.LG cs.AI published:2015-07-06 summary:Emphatic algorithms are temporal-difference learning algorithms that changetheir effective state distribution by selectively emphasizing andde-emphasizing their updates on different time steps. Recent works by Sutton,Mahmood and White (2015), and Yu (2015) show that by varying the emphasis in aparticular way, these algorithms become stable and convergent under off-policytraining with linear function approximation. This paper serves as a unifiedsummary of the available results from both works. In addition, we demonstratethe empirical benefits from the flexibility of emphatic algorithms, includingstate-dependent discounting, state-dependent bootstrapping, and theuser-specified allocation of function approximation resources.
arxiv-11400-158 | A Simple Algorithm for Maximum Margin Classification, Revisited | http://arxiv.org/pdf/1507.01563v1.pdf | author:Sariel Har-Peled category:cs.LG published:2015-07-06 summary:In this note, we revisit the algorithm of Har-Peled et. al. [HRZ07] forcomputing a linear maximum margin classifier. Our presentation is selfcontained, and the algorithm itself is slightly simpler than the originalalgorithm. The algorithm itself is a simple Perceptron like iterativealgorithm. For more details and background, the reader is referred to theoriginal paper.
arxiv-11400-159 | Correspondence Factor Analysis of Big Data Sets: A Case Study of 30 Million Words; and Contrasting Analytics using Apache Solr and Correspondence Analysis in R | http://arxiv.org/pdf/1507.01529v1.pdf | author:Fionn Murtagh category:cs.CL 62H25, 62.07 G.3; H.2.8 published:2015-07-06 summary:We consider a large number of text data sets. These are cooking recipes. Termdistribution and other distributional properties of the data are investigated.Our aim is to look at various analytical approaches which allow for mining ofinformation on both high and low detail scales. Metric space embedding isfundamental to our interest in the semantic properties of this data. Weconsider the projection of all data into analyses of aggregated versions of thedata. We contrast that with projection of aggregated versions of the data intoanalyses of all the data. Analogously for the term set, we look at analysis ofselected terms. We also look at inherent term associations such as betweensingular and plural. In addition to our use of Correspondence Analysis in R,for latent semantic space mapping, we also use Apache Solr. Setting up the Solrserver and carrying out querying is described. A further novelty is thatquerying is supported in Solr based on the principal factor plane mapping ofall the data. This uses a bounding box query, based on factor projections.
arxiv-11400-160 | Sparse Approximate Inference for Spatio-Temporal Point Process Models | http://arxiv.org/pdf/1305.4152v5.pdf | author:Botond Cseke, Andrew Zammit Mangion, Tom Heskes, Guido Sanguinetti category:stat.ML published:2013-05-17 summary:Spatio-temporal point process models play a central role in the analysis ofspatially distributed systems in several disciplines. Yet, scalable inferenceremains computa- tionally challenging both due to the high resolution modellinggenerally required and the analytically intractable likelihood function. Here,we exploit the sparsity structure typical of (spatially) discretisedlog-Gaussian Cox process models by using approximate message-passingalgorithms. The proposed algorithms scale well with the state dimension and thelength of the temporal horizon with moderate loss in distributional accuracy.They hence provide a flexible and faster alternative to both non-linearfiltering-smoothing type algorithms and to approaches that implement theLaplace method or expectation propagation on (block) sparse latent Gaussianmodels. We infer the parameters of the latent Gaussian model using a structuredvariational Bayes approach. We demonstrate the proposed framework on simulationstudies with both Gaussian and point-process observations and use it toreconstruct the conflict intensity and dynamics in Afghanistan from theWikiLeaks Afghan War Diary.
arxiv-11400-161 | Semi-proximal Mirror-Prox for Nonsmooth Composite Minimization | http://arxiv.org/pdf/1507.01476v1.pdf | author:Niao He, Zaid Harchaoui category:math.OC cs.LG published:2015-07-06 summary:We propose a new first-order optimisation algorithm to solve high-dimensionalnon-smooth composite minimisation problems. Typical examples of such problemshave an objective that decomposes into a non-smooth empirical risk part and anon-smooth regularisation penalty. The proposed algorithm, called Semi-ProximalMirror-Prox, leverages the Fenchel-type representation of one part of theobjective while handling the other part of the objective via linearminimization over the domain. The algorithm stands in contrast with moreclassical proximal gradient algorithms with smoothing, which require thecomputation of proximal operators at each iteration and can therefore beimpractical for high-dimensional problems. We establish the theoreticalconvergence rate of Semi-Proximal Mirror-Prox, which exhibits the optimalcomplexity bounds, i.e. $O(1/\epsilon^2)$, for the number of calls to linearminimization oracle. We present promising experimental results showing theinterest of the approach in comparison to competing methods.
arxiv-11400-162 | Revisiting Large Scale Distributed Machine Learning | http://arxiv.org/pdf/1507.01461v1.pdf | author:Radu Cristian Ionescu category:cs.DC cs.LG published:2015-07-06 summary:Nowadays, with the widespread of smartphones and other portable gadgetsequipped with a variety of sensors, data is ubiquitous available and the focusof machine learning has shifted from being able to infer from small trainingsamples to dealing with large scale high-dimensional data. In domains such aspersonal healthcare applications, which motivates this survey, distributedmachine learning is a promising line of research, both for scaling up learningalgorithms, but mostly for dealing with data which is inherently produced atdifferent locations. This report offers a thorough overview of andstate-of-the-art algorithms for distributed machine learning, for bothsupervised and unsupervised learning, ranging from simple linear logisticregression to graphical models and clustering. We propose future directions formost categories, specific to the potential personal healthcare applications.With this in mind, the report focuses on how security and low communicationoverhead can be assured in the specific case of a strictly client-serverarchitectural model. As particular directions we provides an exhaustivepresentation of an empirical clustering algorithm, k-windows, and proposed anasynchronous distributed machine learning algorithm that would scale well andalso would be computationally cheap and easy to implement.
arxiv-11400-163 | Towards Data-Driven Autonomics in Data Centers | http://arxiv.org/pdf/1505.04935v2.pdf | author:Alina SÃ®rbu, Ozalp Babaoglu category:cs.DC cs.AI stat.ML published:2015-05-19 summary:Continued reliance on human operators for managing data centers is a majorimpediment for them from ever reaching extreme dimensions. Large computersystems in general, and data centers in particular, will ultimately be managedusing predictive computational and executable models obtained throughdata-science tools, and at that point, the intervention of humans will belimited to setting high-level goals and policies rather than performinglow-level operations. Data-driven autonomics, where management and control arebased on holistic predictive models that are built and updated using generateddata, opens one possible path towards limiting the role of operators in datacenters. In this paper, we present a data-science study of a public Googledataset collected in a 12K-node cluster with the goal of building andevaluating a predictive model for node failures. We use BigQuery, the big dataSQL platform from the Google Cloud suite, to process massive amounts of dataand generate a rich feature set characterizing machine state over time. Wedescribe how an ensemble classifier can be built out of many Random Forestclassifiers each trained on these features, to predict if machines will fail ina future 24-hour window. Our evaluation reveals that if we limit false positiverates to 5%, we can achieve true positive rates between 27% and 88% withprecision varying between 50% and 72%. We discuss the practicality of includingour predictive model as the central component of a data-driven autonomicmanager and operating it on-line with live data streams (rather than off-lineon data logs). All of the scripts used for BigQuery and classification analysesare publicly available from the authors' website.
arxiv-11400-164 | Learning Better Encoding for Approximate Nearest Neighbor Search with Dictionary Annealing | http://arxiv.org/pdf/1507.01442v1.pdf | author:Shicong Liu, Hongtao Lu category:cs.CV published:2015-07-06 summary:We introduce a novel dictionary optimization method for high-dimensionalvector quantization employed in approximate nearest neighbor (ANN) search.Vector quantization methods first seek a series of dictionaries, thenapproximate each vector by a sum of elements selected from these dictionaries.An optimal series of dictionaries should be mutually independent, and eachdictionary should generate a balanced encoding for the target dataset. Existingmethods did not explicitly consider this. To achieve these goals along withminimizing the quantization error (residue), we propose a novel dictionaryoptimization method called \emph{Dictionary Annealing} that alternatively"heats up" a single dictionary by generating an intermediate dataset withresidual vectors, "cools down" the dictionary by fitting the intermediatedataset, then extracts the new residual vectors for the next iteration. Bettercodes can be learned by DA for the ANN search tasks. DA is easily implementedon GPU to utilize the latest computing technology, and can easily extended toan online dictionary learning scheme. We show by experiments that our optimizeddictionaries substantially reduce the overall quantization error. Jointly usedwith residual vector quantization, our optimized dictionaries lead to a betterapproximate nearest neighbor search performance compared to thestate-of-the-art methods.
arxiv-11400-165 | End-to-end Convolutional Network for Saliency Prediction | http://arxiv.org/pdf/1507.01422v1.pdf | author:Junting Pan, Xavier GirÃ³-i-Nieto category:cs.CV cs.LG cs.NE published:2015-07-06 summary:The prediction of saliency areas in images has been traditionally addressedwith hand crafted features based on neuroscience principles. This paper howeveraddresses the problem with a completely data-driven approach by training aconvolutional network. The learning process is formulated as a minimization ofa loss function that measures the Euclidean distance of the predicted saliencymap with the provided ground truth. The recent publication of large datasets ofsaliency prediction has provided enough data to train a not very deeparchitecture which is both fast and accurate. The convolutional network in thispaper, named JuntingNet, won the LSUN 2015 challenge on saliency predictionwith a superior performance in all considered metrics.
arxiv-11400-166 | A linear approach for sparse coding by a two-layer neural network | http://arxiv.org/pdf/1507.01892v1.pdf | author:Alessandro Montalto, Giovanni Tessitore, Roberto Prevete category:cs.LG published:2015-07-06 summary:Many approaches to transform classification problems from non-linear tolinear by feature transformation have been recently presented in theliterature. These notably include sparse coding methods and deep neuralnetworks. However, many of these approaches require the repeated application ofa learning process upon the presentation of unseen data input vectors, or elseinvolve the use of large numbers of parameters and hyper-parameters, which mustbe chosen through cross-validation, thus increasing running time dramatically.In this paper, we propose and experimentally investigate a new approach for thepurpose of overcoming limitations of both kinds. The proposed approach makesuse of a linear auto-associative network (called SCNN) with just one hiddenlayer. The combination of this architecture with a specific error function tobe minimized enables one to learn a linear encoder computing a sparse codewhich turns out to be as similar as possible to the sparse coding that oneobtains by re-training the neural network. Importantly, the linearity of SCNNand the choice of the error function allow one to achieve reduced running timein the learning phase. The proposed architecture is evaluated on the basis oftwo standard machine learning tasks. Its performances are compared with thoseof recently proposed non-linear auto-associative neural networks. The overallresults suggest that linear encoders can be profitably used to obtain sparsedata representations in the context of machine learning problems, provided thatan appropriate error function is used during the learning phase.
arxiv-11400-167 | Early Recognition of Human Activities from First-Person Videos Using Onset Representations | http://arxiv.org/pdf/1406.5309v2.pdf | author:M. S. Ryoo, Thomas J. Fuchs, Lu Xia, J. K. Aggarwal, Larry Matthies category:cs.CV published:2014-06-20 summary:In this paper, we propose a methodology for early recognition of humanactivities from videos taken with a first-person viewpoint. Early recognition,which is also known as activity prediction, is an ability to infer an ongoingactivity at its early stage. We present an algorithm to perform recognition ofactivities targeted at the camera from streaming videos, making the system topredict intended activities of the interacting person and avoid harmful eventsbefore they actually happen. We introduce the novel concept of 'onset' thatefficiently summarizes pre-activity observations, and design an approach toconsider event history in addition to ongoing video observation for earlyfirst-person recognition of activities. We propose to represent onset usingcascade histograms of time series gradients, and we describe a novelalgorithmic setup to take advantage of onset for early recognition ofactivities. The experimental results clearly illustrate that the proposedconcept of onset enables better/earlier recognition of human activities fromfirst-person videos.
arxiv-11400-168 | Visual Data Deblocking using Structural Layer Priors | http://arxiv.org/pdf/1507.01330v1.pdf | author:Xiaojie Guo category:cs.CV published:2015-07-06 summary:The blocking artifact frequently appears in compressed real-world images orvideo sequences, especially coded at low bit rates, which is visually annoyingand likely hurts the performance of many computer vision algorithms. Acompressed frame can be viewed as the superimposition of an intrinsic layer andan artifact one. Recovering the two layers from such frames seems to be aseverely ill-posed problem since the number of unknowns to recover is twice asmany as the given measurements. In this paper, we propose a simple and robustmethod to separate these two layers, which exploits structural layer priorsincluding the gradient sparsity of the intrinsic layer, and the independence ofthe gradient fields of the two layers. A novel Augmented Lagrangian Multiplierbased algorithm is designed to efficiently and effectively solve the recoveryproblem. Extensive experimental results demonstrate the superior performance ofour method over the state of the arts, in terms of visual quality andsimplicity.
arxiv-11400-169 | Deep Image: Scaling up Image Recognition | http://arxiv.org/pdf/1501.02876v5.pdf | author:Ren Wu, Shengen Yan, Yi Shan, Qingqing Dang, Gang Sun category:cs.CV published:2015-01-13 summary:We present a state-of-the-art image recognition system, Deep Image, developedusing end-to-end deep learning. The key components are a custom-builtsupercomputer dedicated to deep learning, a highly optimized parallel algorithmusing new strategies for data partitioning and communication, larger deepneural network models, novel data augmentation approaches, and usage ofmulti-scale high-resolution images. Our method achieves excellent results onmultiple challenging computer vision benchmarks.
arxiv-11400-170 | Subspace-Sparse Representation | http://arxiv.org/pdf/1507.01307v1.pdf | author:C. You, R. Vidal category:stat.ML cs.IT math.IT published:2015-07-06 summary:Given an overcomplete dictionary $A$ and a signal $b$ that is a linearcombination of a few linearly independent columns of $A$, classical sparserecovery theory deals with the problem of recovering the unique sparserepresentation $x$ such that $b = A x$. It is known that under certainconditions on $A$, $x$ can be recovered by the Basis Pursuit (BP) and theOrthogonal Matching Pursuit (OMP) algorithms. In this work, we consider themore general case where $b$ lies in a low-dimensional subspace spanned by somecolumns of $A$, which are possibly linearly dependent. In this case, thesparsest solution $x$ is generally not unique, and we study the problem thatthe representation $x$ identifies the subspace, i.e. the nonzero entries of $x$correspond to dictionary atoms that are in the subspace. Such a representation$x$ is called subspace-sparse. We present sufficient conditions forguaranteeing subspace-sparse recovery, which have clear geometricinterpretations and explain properties of subspace-sparse recovery. We alsoshow that the sufficient conditions can be satisfied under a randomized model.Our results are applicable to the traditional sparse recovery problem and weget conditions for sparse recovery that are less restrictive than the canonicalmutual coherent condition. We also use the results to analyze the sparserepresentation based classification (SRC) method, for which we get conditionsto show its correctness.
arxiv-11400-171 | Semi-supervised Multi-sensor Classification via Consensus-based Multi-View Maximum Entropy Discrimination | http://arxiv.org/pdf/1507.01269v1.pdf | author:Tianpei Xie, Nasser M. Nasrabadi, Alfred O. Hero III category:cs.IT cs.AI cs.LG math.IT published:2015-07-05 summary:In this paper, we consider multi-sensor classification when there is a largenumber of unlabeled samples. The problem is formulated under the multi-viewlearning framework and a Consensus-based Multi-View Maximum EntropyDiscrimination (CMV-MED) algorithm is proposed. By iteratively maximizing thestochastic agreement between multiple classifiers on the unlabeled dataset, thealgorithm simultaneously learns multiple high accuracy classifiers. Wedemonstrate that our proposed method can yield improved performance overprevious multi-view learning approaches by comparing performance on three realmulti-sensor data sets.
arxiv-11400-172 | Autoencoding the Retrieval Relevance of Medical Images | http://arxiv.org/pdf/1507.01251v1.pdf | author:Zehra Camlica, H. R. Tizhoosh, Farzad Khalvati category:cs.CV published:2015-07-05 summary:Content-based image retrieval (CBIR) of medical images is a crucial task thatcan contribute to a more reliable diagnosis if applied to big data. Recentadvances in feature extraction and classification have enormously improved CBIRresults for digital images. However, considering the increasing accessibilityof big data in medical imaging, we are still in need of reducing both memoryrequirements and computational expenses of image retrieval systems. This workproposes to exclude the features of image blocks that exhibit a low encodingerror when learned by a $n/p/n$ autoencoder ($p\!<\!n$). We examine thehistogram of autoendcoding errors of image blocks for each image class tofacilitate the decision which image regions, or roughly what percentage of animage perhaps, shall be declared relevant for the retrieval task. This leads toreduction of feature dimensionality and speeds up the retrieval process. Tovalidate the proposed scheme, we employ local binary patterns (LBP) and supportvector machines (SVM) which are both well-established approaches in CBIRresearch community. As well, we use IRMA dataset with 14,410 x-ray images astest data. The results show that the dimensionality of annotated featurevectors can be reduced by up to 50% resulting in speedups greater than 27% atexpense of less than 1% decrease in the accuracy of retrieval when validatingthe precision and recall of the top 20 hits.
arxiv-11400-173 | Experiments on Parallel Training of Deep Neural Network using Model Averaging | http://arxiv.org/pdf/1507.01239v1.pdf | author:Hang Su, Haoyu Chen category:cs.LG cs.NE published:2015-07-05 summary:In this work we apply model averaging to parallel training of deep neuralnetwork (DNN). Parallelization is done in a model averaging manner. Data ispartitioned and distributed to different nodes for local model updates, andmodel averaging across nodes is done every few minibatches. We use multipleGPUs for data parallelization, and Message Passing Interface (MPI) forcommunication between nodes, which allows us to perform model averagingfrequently without losing much time on communication. We investigate theeffectiveness of Natural Gradient Stochastic Gradient Descent (NG-SGD) andRestricted Boltzmann Machine (RBM) pretraining for parallel training inmodel-averaging framework, and explore the best setups in term of differentlearning rate schedules, averaging frequencies and minibatch sizes. It is shownthat NG-SGD and RBM pretraining benefits parameter-averaging based modeltraining. On the 300h Switchboard dataset, a 9.3 times speedup is achievedusing 16 GPUs and 17 times speedup using 32 GPUs with limited decoding accuracyloss.
arxiv-11400-174 | Fast Label Embeddings via Randomized Linear Algebra | http://arxiv.org/pdf/1412.6547v7.pdf | author:Paul Mineiro, Nikos Karampatziakis category:cs.LG published:2014-12-19 summary:Many modern multiclass and multilabel problems are characterized byincreasingly large output spaces. For these problems, label embeddings havebeen shown to be a useful primitive that can improve computational andstatistical efficiency. In this work we utilize a correspondence between rankconstrained estimation and low dimensional label embeddings that uncovers afast label embedding algorithm which works in both the multiclass andmultilabel settings. The result is a randomized algorithm whose running time isexponentially faster than naive algorithms. We demonstrate our techniques ontwo large-scale public datasets, from the Large Scale Hierarchical TextChallenge and the Open Directory Project, where we obtain state of the artresults.
arxiv-11400-175 | TV News Commercials Detection using Success based Locally Weighted Kernel Combination | http://arxiv.org/pdf/1507.01209v1.pdf | author:Raghvendra Kannao, Prithwijit Guha category:cs.CV cs.MM published:2015-07-05 summary:Commercial detection in news broadcast videos involves judicious selection ofmeaningful audio-visual feature combinations and efficient classifiers. And,this problem becomes much simpler if these combinations can be learned from thedata. To this end, we propose an Multiple Kernel Learning based method forboosting successful kernel functions while ignoring the irrelevant ones. Weadopt a intermediate fusion approach where, a SVM is trained with a weightedlinear combination of different kernel functions instead of single kernelfunction. Each kernel function is characterized by a feature set and kerneltype. We identify the feature sub-space locations of the prediction success ofa particular classifier trained only with particular kernel function. Wepropose to estimate a weighing function using support vector regression (withRBF kernel) for each kernel function which has high values (near 1.0) where theclassifier learned on kernel function succeeded and lower values (nearly 0.0)otherwise. Second contribution of this work is TV News Commercials Dataset of150 Hours of News videos. Classifier trained with our proposed scheme hasoutperformed the baseline methods on 6 of 8 benchmark dataset and our own TVcommercials dataset.
arxiv-11400-176 | Parsimonious Labeling | http://arxiv.org/pdf/1507.01208v1.pdf | author:Puneet K. Dokania, M. Pawan Kumar category:cs.CV published:2015-07-05 summary:We propose a new family of discrete energy minimization problems, which wecall parsimonious labeling. Specifically, our energy functional consists ofunary potentials and high-order clique potentials. While the unary potentialsare arbitrary, the clique potentials are proportional to the {\em diversity} ofset of the unique labels assigned to the clique. Intuitively, our energyfunctional encourages the labeling to be parsimonious, that is, use as fewlabels as possible. This in turn allows us to capture useful cues for importantcomputer vision applications such as stereo correspondence and image denoising.Furthermore, we propose an efficient graph-cuts based algorithm for theparsimonious labeling problem that provides strong theoretical guarantees onthe quality of the solution. Our algorithm consists of three steps. First, weapproximate a given diversity using a mixture of a novel hierarchical $P^n$Potts model. Second, we use a divide-and-conquer approach for each mixturecomponent, where each subproblem is solved using an effficient$\alpha$-expansion algorithm. This provides us with a small number of putativelabelings, one for each mixture component. Third, we choose the best putativelabeling in terms of the energy value. Using both sythetic and standard realdatasets, we show that our algorithm significantly outperforms other graph-cutsbased approaches.
arxiv-11400-177 | Dependency Recurrent Neural Language Models for Sentence Completion | http://arxiv.org/pdf/1507.01193v1.pdf | author:Piotr Mirowski, Andreas Vlachos category:cs.CL cs.AI cs.LG published:2015-07-05 summary:Recent work on language modelling has shifted focus from count-based modelsto neural models. In these works, the words in each sentence are alwaysconsidered in a left-to-right order. In this paper we show how we can improvethe performance of the recurrent neural network (RNN) language model byincorporating the syntactic dependencies of a sentence, which have the effectof bringing relevant contexts closer to the word being predicted. We evaluateour approach on the Microsoft Research Sentence Completion Challenge and showthat the dependency RNN proposed improves over the RNN by about 10 points inaccuracy. Furthermore, we achieve results comparable with the state-of-the-artmodels on this task.
arxiv-11400-178 | Inference for determinantal point processes without spectral knowledge | http://arxiv.org/pdf/1507.01154v1.pdf | author:RÃ©mi Bardenet, Michalis K. Titsias category:stat.CO stat.ML published:2015-07-04 summary:Determinantal point processes (DPPs) are point process models that naturallyencode diversity between the points of a given realization, through a positivedefinite kernel $K$. DPPs possess desirable properties, such as exact samplingor analyticity of the moments, but learning the parameters of kernel $K$through likelihood-based inference is not straightforward. First, the kernelthat appears in the likelihood is not $K$, but another kernel $L$ related to$K$ through an often intractable spectral decomposition. This issue istypically bypassed in machine learning by directly parametrizing the kernel$L$, at the price of some interpretability of the model parameters. We followthis approach here. Second, the likelihood has an intractable normalizingconstant, which takes the form of a large determinant in the case of a DPP overa finite set of objects, and the form of a Fredholm determinant in the case ofa DPP over a continuous domain. Our main contribution is to derive bounds onthe likelihood of a DPP, both for finite and continuous domains. Unlikeprevious work, our bounds are cheap to evaluate since they do not rely onapproximating the spectrum of a large matrix or an operator. Through usualarguments, these bounds thus yield cheap variational inference and moderatelyexpensive exact Markov chain Monte Carlo inference methods for DPPs.
arxiv-11400-179 | Techniques for effective and efficient fire detection from social media images | http://arxiv.org/pdf/1506.03844v2.pdf | author:Marcos Bedo, Gustavo Blanco, Willian Oliveira, Mirela Cazzolato, Alceu Costa, Jose Rodrigues, Agma Traina, Caetano Traina Jr category:cs.CV published:2015-06-11 summary:Social media could provide valuable information to support decision making incrisis management, such as in accidents, explosions and fires. However, much ofthe data from social media are images, which are uploaded in a rate that makesit impossible for human beings to analyze them. Despite the many works on imageanalysis, there are no fire detection studies on social media. To fill thisgap, we propose the use and evaluation of a broad set of content-based imageretrieval and classification techniques for fire detection. Our maincontributions are: (i) the development of the Fast-Fire Detection method(FFDnR), which combines feature extractor and evaluation functions to supportinstance-based learning, (ii) the construction of an annotated set of imageswith ground-truth depicting fire occurrences -- the FlickrFire dataset, and(iii) the evaluation of 36 efficient image descriptors for fire detection.Using real data from Flickr, our results showed that FFDnR was able to achievea precision for fire detection comparable to that of human annotators.Therefore, our work shall provide a solid basis for further developments onmonitoring images from social media.
arxiv-11400-180 | AutoExtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes | http://arxiv.org/pdf/1507.01127v1.pdf | author:Sascha Rothe, Hinrich SchÃ¼tze category:cs.CL published:2015-07-04 summary:We present \textit{AutoExtend}, a system to learn embeddings for synsets andlexemes. It is flexible in that it can take any word embeddings as input anddoes not need an additional training corpus. The synset/lexeme embeddingsobtained live in the same vector space as the word embeddings. A sparse tensorformalization guarantees efficiency and parallelizability. We use WordNet as alexical resource, but AutoExtend can be easily applied to other resources likeFreebase. AutoExtend achieves state-of-the-art performance on word similarityand word sense disambiguation tasks.
arxiv-11400-181 | Learning in the Presence of Corruption | http://arxiv.org/pdf/1504.00091v2.pdf | author:Brendan van Rooyen, Robert C. Williamson category:stat.ML cs.LG published:2015-04-01 summary:In supervised learning one wishes to identify a pattern present in a jointdistribution $P$, of instances, label pairs, by providing a function $f$ frominstances to labels that has low risk $\mathbb{E}_{P}\ell(y,f(x))$. To do so,the learner is given access to $n$ iid samples drawn from $P$. In many realworld problems clean samples are not available. Rather, the learner is givenaccess to samples from a corrupted distribution $\tilde{P}$ from which tolearn, while the goal of predicting the clean pattern remains. There are manydifferent types of corruption one can consider, and as of yet there is nogeneral means to compare the relative ease of learning under these differentcorruption processes. In this paper we develop a general framework for tacklingsuch problems as well as introducing upper and lower bounds on the risk forlearning in the presence of corruption. Our ultimate goal is to be able to makeinformed economic decisions in regards to the acquisition of data sets. For acertain subclass of corruption processes (those that are\emph{reconstructible}) we achieve this goal in a particular sense. Our lowerbounds are in terms of the coefficient of ergodicity, a simple to calculateproperty of stochastic matrices. Our upper bounds proceed via a generalizationof the method of unbiased estimators appearing in recent work of Natarajan etal and implicit in the earlier work of Kearns.
arxiv-11400-182 | Efficient Rotation-Scaling-Translation Parameters Estimation Based on Fractal Image Model | http://arxiv.org/pdf/1501.02372v2.pdf | author:M. Uss, B. Vozel, V. Lukin, K. Chehdi category:cs.CV published:2015-01-10 summary:This paper deals with area-based subpixel image registration underrotation-isometric scaling-translation transformation hypothesis. Our approachis based on a parametrical modeling of geometrically transformed textural imagefragments and maximum likelihood estimation of transformation vector betweenthem. Due to the parametrical approach based on the fractional Brownian motionmodeling of the local fragments texture, the proposed estimator MLfBm (MLstands for "Maximum Likelihood" and fBm for "Fractal Brownian motion") has theability to better adapt to real image texture content compared to other methodsrelying on universal similarity measures like mutual information or normalizedcorrelation. The main benefits are observed when assumptions underlying the fBmmodel are fully satisfied, e.g. for isotropic normally distributed textureswith stationary increments. Experiments on both simulated and real images andfor high and weak correlation between registered images show that the MLfBmestimator offers significant improvement compared to other state-of-the-artmethods. It reduces translation vector, rotation angle and scaling factorestimation errors by a factor of about 1.75...2 and it decreases probability offalse match by up to 5 times. Besides, an accurate confidence interval forMLfBm estimates can be obtained from the Cramer-Rao lower bound onrotation-scaling-translation parameters estimation error. This bound depends ontexture roughness, noise level in reference and template images, correlationbetween these images and geometrical transformation parameters.
arxiv-11400-183 | ShapeFit: Exact location recovery from corrupted pairwise directions | http://arxiv.org/pdf/1506.01437v2.pdf | author:Paul Hand, Choongbum Lee, Vladislav Voroninski category:cs.CV cs.IT math.CO math.IT math.OC published:2015-06-04 summary:Let $t_1,\ldots,t_n \in \mathbb{R}^d$ and consider the location recoveryproblem: given a subset of pairwise direction observations $\{(t_i - t_j) /\t_i - t_j\_2\}_{i<j \in [n] \times [n]}$, where a constant fraction of theseobservations are arbitrarily corrupted, find $\{t_i\}_{i=1}^n$ up to a globaltranslation and scale. We propose a novel algorithm for the location recoveryproblem, which consists of a simple convex program over $dn$ real variables. Weprove that this program recovers a set of $n$ i.i.d. Gaussian locations exactlyand with high probability if the observations are given by an \erdosrenyigraph, $d$ is large enough, and provided that at most a constant fraction ofobservations involving any particular location are adversarially corrupted. Wealso prove that the program exactly recovers Gaussian locations for $d=3$ ifthe fraction of corrupted observations at each location is, up topoly-logarithmic factors, at most a constant. Both of these recovery theoremsare based on a set of deterministic conditions that we prove are sufficient forexact recovery.
arxiv-11400-184 | Describing Multimedia Content using Attention-based Encoder--Decoder Networks | http://arxiv.org/pdf/1507.01053v1.pdf | author:Kyunghyun Cho, Aaron Courville, Yoshua Bengio category:cs.NE cs.CL cs.CV cs.LG published:2015-07-04 summary:Whereas deep neural networks were first mostly used for classification tasks,they are rapidly expanding in the realm of structured output problems, wherethe observed target is composed of multiple random variables that have a richjoint distribution, given the input. We focus in this paper on the case wherethe input also has a rich structure and the input and output structures aresomehow related. We describe systems that learn to attend to different placesin the input, for each element of the output, for a variety of tasks: machinetranslation, image caption generation, video clip description and speechrecognition. All these systems are based on a shared set of building blocks:gated recurrent neural networks and convolutional neural networks, along withtrained attention mechanisms. We report on experimental results with thesesystems, showing impressively good performance and the advantage of theattention mechanism.
arxiv-11400-185 | Adding vs. Averaging in Distributed Primal-Dual Optimization | http://arxiv.org/pdf/1502.03508v2.pdf | author:Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I. Jordan, Peter RichtÃ¡rik, Martin TakÃ¡Ä category:cs.LG 90C25, 68W15 G.1.6; C.1.4 published:2015-02-12 summary:Distributed optimization methods for large-scale machine learning suffer froma communication bottleneck. It is difficult to reduce this bottleneck whilestill efficiently and accurately aggregating partial work from differentmachines. In this paper, we present a novel generalization of the recentcommunication-efficient primal-dual framework (CoCoA) for distributedoptimization. Our framework, CoCoA+, allows for additive combination of localupdates to the global parameters at each iteration, whereas previous schemeswith convergence guarantees only allow conservative averaging. We give stronger(primal-dual) convergence rate guarantees for both CoCoA as well as our newvariants, and generalize the theory for both methods to cover non-smooth convexloss functions. We provide an extensive experimental comparison that shows themarkedly improved performance of CoCoA+ on several real-world distributeddatasets, especially when scaling up the number of machines.
arxiv-11400-186 | Meta learning of bounds on the Bayes classifier error | http://arxiv.org/pdf/1504.07116v2.pdf | author:Kevin R. Moon, Veronique Delouille, Alfred O. Hero III category:cs.LG astro-ph.SR cs.CV cs.IT math.IT published:2015-04-27 summary:Meta learning uses information from base learners (e.g. classifiers orestimators) as well as information about the learning problem to improve uponthe performance of a single base learner. For example, the Bayes error rate ofa given feature space, if known, can be used to aid in choosing a classifier,as well as in feature selection and model selection for the base classifiersand the meta classifier. Recent work in the field of f-divergence functionalestimation has led to the development of simple and rapidly convergingestimators that can be used to estimate various bounds on the Bayes error. Weestimate multiple bounds on the Bayes error using an estimator that appliesmeta learning to slowly converging plug-in estimators to obtain the parametricconvergence rate. We compare the estimated bounds empirically on simulated dataand then estimate the tighter bounds on features extracted from an image patchanalysis of sunspot continuum and magnetogram images.
arxiv-11400-187 | Fine-grained Recognition Datasets for Biodiversity Analysis | http://arxiv.org/pdf/1507.00913v1.pdf | author:Erik Rodner, Marcel Simon, Gunnar Brehm, Stephanie Pietsch, J. Wolfgang WÃ¤gele, Joachim Denzler category:cs.CV published:2015-07-03 summary:In the following paper, we present and discuss challenging applications forfine-grained visual classification (FGVC): biodiversity and species analysis.We not only give details about two challenging new datasets suitable forcomputer vision research with up to 675 highly similar classes, but alsopresent first results with localized features using convolutional neuralnetworks (CNN). We conclude with a list of challenging new research directionsin the area of visual classification for biodiversity research.
arxiv-11400-188 | LogDet Rank Minimization with Application to Subspace Clustering | http://arxiv.org/pdf/1507.00908v1.pdf | author:Zhao Kang, Chong Peng, Jie Cheng, Qiang Chen category:cs.CV cs.LG stat.ML published:2015-07-03 summary:Low-rank matrix is desired in many machine learning and computer visionproblems. Most of the recent studies use the nuclear norm as a convex surrogateof the rank operator. However, all singular values are simply added together bythe nuclear norm, and thus the rank may not be well approximated in practicalproblems. In this paper, we propose to use a log-determinant (LogDet) functionas a smooth and closer, though non-convex, approximation to rank for obtaininga low-rank representation in subspace clustering. Augmented Lagrangemultipliers strategy is applied to iteratively optimize the LogDet-basednon-convex objective function on potentially large-scale data. By making use ofthe angular information of principal directions of the resultant low-rankrepresentation, an affinity graph matrix is constructed for spectralclustering. Experimental results on motion segmentation and face clusteringdata demonstrate that the proposed method often outperforms state-of-the-artsubspace clustering algorithms.
arxiv-11400-189 | Estimating the number of communities in networks by spectral methods | http://arxiv.org/pdf/1507.00827v1.pdf | author:Can M. Le, Elizaveta Levina category:stat.ML cs.SI math.ST stat.TH 62H30, 62G99 published:2015-07-03 summary:Community detection is a fundamental problem in network analysis with manymethods available to estimate communities. Most of these methods assume thatthe number of communities is known, which is often not the case in practice. Wepropose a simple and very fast method for estimating the number of communitiesbased on the spectral properties of certain graph operators, such as thenon-backtracking matrix and the Bethe Hessian matrix. We show that the methodperforms well under several models and a wide range of parameters, and isguaranteed to be consistent under several asymptotic regimes. We compare thenew method to several existing methods for estimating the number of communitiesand show that it is both more accurate and more computationally efficient.
arxiv-11400-190 | Ridge Regression, Hubness, and Zero-Shot Learning | http://arxiv.org/pdf/1507.00825v1.pdf | author:Yutaro Shigeto, Ikumi Suzuki, Kazuo Hara, Masashi Shimbo, Yuji Matsumoto category:cs.LG stat.ML published:2015-07-03 summary:This paper discusses the effect of hubness in zero-shot learning, when ridgeregression is used to find a mapping between the example space to the labelspace. Contrary to the existing approach, which attempts to find a mapping fromthe example space to the label space, we show that mapping labels into theexample space is desirable to suppress the emergence of hubs in the subsequentnearest neighbor search step. Assuming a simple data model, we prove that theproposed approach indeed reduces hubness. This was verified empirically on thetasks of bilingual lexicon extraction and image labeling: hubness was reducedwith both of these tasks and the accuracy was improved accordingly.
arxiv-11400-191 | A Generalized Labeled Multi-Bernoulli Filter Implementation using Gibbs Sampling | http://arxiv.org/pdf/1506.00821v3.pdf | author:Hung Gia Hoang, Ba-Tuong Vo, Ba-Ngu Vo category:stat.CO cs.LG published:2015-06-02 summary:This paper proposes an efficient implementation of the generalized labeledmulti-Bernoulli (GLMB) filter by combining the prediction and update into asingle step. In contrast to the original approach which involves separatetruncations in the prediction and update steps, the proposed implementationrequires only one single truncation for each iteration, which can be performedusing a standard ranked optimal assignment algorithm. Furthermore, we propose anew truncation technique based on Markov Chain Monte Carlo methods such asGibbs sampling, which drastically reduces the complexity of the filter. Thesuperior performance of the proposed approach is demonstrated through extensivenumerical studies.
arxiv-11400-192 | D-MFVI: Distributed Mean Field Variational Inference using Bregman ADMM | http://arxiv.org/pdf/1507.00824v1.pdf | author:Behnam Babagholami-Mohamadabadi, Sejong Yoon, Vladimir Pavlovic category:cs.LG stat.ML published:2015-07-03 summary:Bayesian models provide a framework for probabilistic modelling of complexdatasets. However, many of such models are computationally demanding especiallyin the presence of large datasets. On the other hand, in sensor networkapplications, statistical (Bayesian) parameter estimation usually needsdistributed algorithms, in which both data and computation are distributedacross the nodes of the network. In this paper we propose a general frameworkfor distributed Bayesian learning using Bregman Alternating Direction Method ofMultipliers (B-ADMM). We demonstrate the utility of our framework, with MeanField Variational Bayes (MFVB) as the primitive for distributed MatrixFactorization (MF) and distributed affine structure from motion (SfM).
arxiv-11400-193 | Optimal design of experiments in the presence of network-correlated outcomes | http://arxiv.org/pdf/1507.00803v1.pdf | author:Guillaume W. Basse, Edoardo M. Airoldi category:stat.ME cs.SI physics.soc-ph stat.ML published:2015-07-03 summary:We consider the problem of how to assign treatment in a randomizedexperiment, when the correlation among the outcomes is informed by a networkavailable pre-intervention. Working within the potential outcome causalframework, we develop a class of models that posit such a correlation structureamong the outcomes, and a strategy for allocating treatment optimally, for thegoal of minimizing the integrated mean squared error of the estimated averagetreatment effect. We provide insights into features of the optimal designs viaan analytical decomposition of the mean squared error used for optimization. Weillustrate how the proposed treatment allocation strategy improves onallocations that ignore the network structure, with extensive simulations.
arxiv-11400-194 | Efficient Elastic Net Regularization for Sparse Linear Models | http://arxiv.org/pdf/1505.06449v3.pdf | author:Zachary C. Lipton, Charles Elkan category:cs.LG published:2015-05-24 summary:This paper presents an algorithm for efficient training of sparse linearmodels with elastic net regularization. Extending previous work on delayedupdates, the new algorithm applies stochastic gradient updates to non-zerofeatures only, bringing weights current as needed with closed-form updates.Closed-form delayed updates for the $\ell_1$, $\ell_{\infty}$, and rarely used$\ell_2$ regularizers have been described previously. This paper providesclosed-form updates for the popular squared norm $\ell^2_2$ and elastic netregularizers. We provide dynamic programming algorithms that perform each delayed update inconstant time. The new $\ell^2_2$ and elastic net methods handle both fixed andvarying learning rates, and both standard {stochastic gradient descent} (SGD)and {forward backward splitting (FoBoS)}. Experimental results show that on abag-of-words dataset with $260,941$ features, but only $88$ nonzero features onaverage per training example, the dynamic programming method trains a logisticregression classifier with elastic net regularization over $2000$ times fasterthan otherwise.
arxiv-11400-195 | Actor-Critic Algorithms for Learning Nash Equilibria in N-player General-Sum Games | http://arxiv.org/pdf/1401.2086v2.pdf | author:H. L Prasad, L. A. Prashanth, Shalabh Bhatnagar category:cs.GT cs.LG stat.ML published:2014-01-08 summary:We consider the problem of finding stationary Nash equilibria (NE) in afinite discounted general-sum stochastic game. We first generalize a non-linearoptimization problem from Filar and Vrieze [2004] to a $N$-player setting andbreak down this problem into simpler sub-problems that ensure there is noBellman error for a given state and an agent. We then provide acharacterization of solution points of these sub-problems that correspond toNash equilibria of the underlying game and for this purpose, we derive a set ofnecessary and sufficient SG-SP (Stochastic Game - Sub-Problem) conditions.Using these conditions, we develop two actor-critic algorithms: OFF-SGSP(model-based) and ON-SGSP (model-free). Both algorithms use a critic thatestimates the value function for a fixed policy and an actor that performsdescent in the policy space using a descent direction that avoids local minima.We establish that both algorithms converge, in self-play, to the equilibria ofa certain ordinary differential equation (ODE), whose stable limit pointscoincide with stationary NE of the underlying general-sum stochastic game. On asingle state non-generic game (see Hart and Mas-Colell [2005]) as well as on asynthetic two-player game setup with $810,000$ states, we establish thatON-SGSP consistently outperforms NashQ ([Hu and Wellman, 2003] and FFQ[Littman, 2001] algorithms.
arxiv-11400-196 | Correlated Random Measures | http://arxiv.org/pdf/1507.00720v1.pdf | author:Rajesh Ranganath, David Blei category:stat.ML stat.ME published:2015-07-02 summary:We develop correlated random measures, random measures where the atom weightscan exhibit a flexible pattern of dependence, and use them to develop powerfulhierarchical Bayesian nonparametric models. Hierarchical Bayesian nonparametricmodels are usually built from completely random measures, a Poisson-processbased construction in which the atom weights are independent. Completely randommeasures imply strong independence assumptions in the correspondinghierarchical model, and these assumptions are often misplaced in real-worldsettings. Correlated random measures address this limitation. They modelcorrelation within the measure by using a Gaussian process in concert with thePoisson process. With correlated random measures, for example, we can develop alatent feature model for which we can infer both the properties of the latentfeatures and their dependency pattern. We develop several other examples aswell. We study a correlated random measure model of pairwise count data. Wederive an efficient variational inference algorithm and show improvedpredictive performance on large data sets of documents, web clicks, andelectronic health records.
arxiv-11400-197 | The Elusive Present: Hidden Past and Future Dependency and Why We Build Models | http://arxiv.org/pdf/1507.00672v1.pdf | author:Pooneh M. Ara, Ryan G. James, James P. Crutchfield category:cs.IT math.DS math.IT nlin.CD stat.ML published:2015-07-02 summary:Modeling a temporal process as if it is Markovian assumes the present encodesall of the process's history. When this occurs, the present captures all of thedependency between past and future. We recently showed that if one randomlysamples in the space of structured processes, this is almost never the case.So, how does the Markov failure come about? That is, how do individualmeasurements fail to encode the past? And, how many are needed to capturedependencies between the past and future? Here, we investigate how muchinformation can be shared between the past and future, but not be reflected inthe present. We quantify this elusive information, give explicit calculationalmethods, and draw out the consequences. The most important of which is thatwhen the present hides past-future dependency we must move beyondsequence-based statistics and build state-based models.
arxiv-11400-198 | SQL for SRL: Structure Learning Inside a Database System | http://arxiv.org/pdf/1507.00646v1.pdf | author:Oliver Schulte, Zhensong Qian category:cs.LG cs.DB H.2.8; H.2.4 published:2015-07-02 summary:The position we advocate in this paper is that relational algebra can providea unified language for both representing and computing withstatistical-relational objects, much as linear algebra does for traditionalsingle-table machine learning. Relational algebra is implemented in theStructured Query Language (SQL), which is the basis of relational databasemanagement systems. To support our position, we have developed the FACTORBASEsystem, which uses SQL as a high-level scripting language forstatistical-relational learning of a graphical model structure. The designphilosophy of FACTORBASE is to manage statistical models as first-classcitizens inside a database. Our implementation shows how our SQL constructs inFACTORBASE facilitate fast, modular, and reliable program development.Empirical evidence from six benchmark databases indicates that leveragingdatabase system capabilities achieves scalable model structure learning.
arxiv-11400-199 | Simple, Fast Semantic Parsing with a Tensor Kernel | http://arxiv.org/pdf/1507.00639v1.pdf | author:Daoud Clarke category:cs.CL published:2015-07-02 summary:We describe a simple approach to semantic parsing based on a tensor productkernel. We extract two feature vectors: one for the query and one for eachcandidate logical form. We then train a classifier using the tensor product ofthe two vectors. Using very simple features for both, our system achieves anaverage F1 score of 40.1% on the WebQuestions dataset. This is comparable tomore complex systems but is simpler to implement and runs faster.
arxiv-11400-200 | Robust Compressed Sensing Under Matrix Uncertainties | http://arxiv.org/pdf/1311.4924v4.pdf | author:Yipeng Liu category:cs.IT cs.CV math.IT math.RT stat.AP stat.ML published:2013-11-20 summary:Compressed sensing (CS) shows that a signal having a sparse or compressiblerepresentation can be recovered from a small set of linear measurements. Inclassical CS theory, the sampling matrix and representation matrix are assumedto be known exactly in advance. However, uncertainties exist due to samplingdistortion, finite grids of the parameter space of dictionary, etc. In thispaper, we take a generalized sparse signal model, which simultaneouslyconsiders the sampling and representation matrix uncertainties. Based on thenew signal model, a new optimization model for robust sparse signalreconstruction is proposed. This optimization model can be deduced withstochastic robust approximation analysis. Both convex relaxation and greedyalgorithms are used to solve the optimization problem. For the convexrelaxation method, a sufficient condition for recovery by convex relaxation isgiven; For the greedy algorithm, it is realized by the introduction of apre-processing of the sensing matrix and the measurements. In numericalexperiments, both simulated data and real-life ECG data based results show thatthe proposed method has a better performance than the current methods.
arxiv-11400-201 | Self-Learning Cloud Controllers: Fuzzy Q-Learning for Knowledge Evolution | http://arxiv.org/pdf/1507.00567v1.pdf | author:Pooyan Jamshidi, Amir Sharifloo, Claus Pahl, Andreas Metzger, Giovani Estrada category:cs.SY cs.AI cs.DC cs.LG cs.SE I.2.6; D.2.11 published:2015-07-02 summary:Cloud controllers aim at responding to application demands by automaticallyscaling the compute resources at runtime to meet performance guarantees andminimize resource costs. Existing cloud controllers often resort to scalingstrategies that are codified as a set of adaptation rules. However, for a cloudprovider, applications running on top of the cloud infrastructure are more orless black-boxes, making it difficult at design time to define optimal orpre-emptive adaptation rules. Thus, the burden of taking adaptation decisionsoften is delegated to the cloud application. Yet, in most cases, applicationdevelopers in turn have limited knowledge of the cloud infrastructure. In thispaper, we propose learning adaptation rules during runtime. To this end, weintroduce FQL4KE, a self-learning fuzzy cloud controller. In particular, FQL4KElearns and modifies fuzzy rules at runtime. The benefit is that for designingcloud controllers, we do not have to rely solely on precise design-timeknowledge, which may be difficult to acquire. FQL4KE empowers users to specifycloud controllers by simply adjusting weights representing priorities in systemgoals instead of specifying complex adaptation rules. The applicability ofFQL4KE has been experimentally assessed as part of the cloud applicationframework ElasticBench. The experimental results indicate that FQL4KEoutperforms our previously developed fuzzy controller without learningmechanisms and the native Azure auto-scaling.
arxiv-11400-202 | Anomaly Detection and Removal Using Non-Stationary Gaussian Processes | http://arxiv.org/pdf/1507.00566v1.pdf | author:Steven Reece, Roman Garnett, Michael Osborne, Stephen Roberts category:stat.ML published:2015-07-02 summary:This paper proposes a novel Gaussian process approach to fault removal intime-series data. Fault removal does not delete the faulty signal data but,instead, massages the fault from the data. We assume that only one fault occursat any one time and model the signal by two separate non-parametric Gaussianprocess models for both the physical phenomenon and the fault. In order tofacilitate fault removal we introduce the Markov Region Link kernel forhandling non-stationary Gaussian processes. This kernel is piece-wisestationary but guarantees that functions generated by it and their derivatives(when required) are everywhere continuous. We apply this kernel to the removalof drift and bias errors in faulty sensor data and also to the recovery of EOGartifact corrupted EEG signals.
arxiv-11400-203 | Regularized linear system identification using atomic, nuclear and kernel-based norms: the role of the stability constraint | http://arxiv.org/pdf/1507.00564v1.pdf | author:Gianluigi Pillonetto, Tianshi Chen, Alessandro Chiuso, Giuseppe De Nicolao, Lennart Ljung category:cs.SY cs.LG published:2015-07-02 summary:Inspired by ideas taken from the machine learning literature, newregularization techniques have been recently introduced in linear systemidentification. In particular, all the adopted estimators solve a regularizedleast squares problem, differing in the nature of the penalty term assigned tothe impulse response. Popular choices include atomic and nuclear norms (appliedto Hankel matrices) as well as norms induced by the so called stable splinekernels. In this paper, a comparative study of estimators based on thesedifferent types of regularizers is reported. Our findings reveal that stablespline kernels outperform approaches based on atomic and nuclear norms sincethey suitably embed information on impulse response stability and smoothness.This point is illustrated using the Bayesian interpretation of regularization.We also design a new class of regularizers defined by "integral" versions ofstable spline/TC kernels. Under quite realistic experimental conditions, thenew estimators outperform classical prediction error methods also when thelatter are equipped with an oracle for model order selection.
arxiv-11400-204 | Classical vs. Bayesian methods for linear system identification: point estimators and confidence sets | http://arxiv.org/pdf/1507.00543v1.pdf | author:D. Romeres, G. Prando, G. Pillonetto, A. Chiuso category:stat.ML published:2015-07-02 summary:This paper compares classical parametric methods with recently developedBayesian methods for system identification. A Full Bayes solution is consideredtogether with one of the standard approximations based on the Empirical Bayesparadigm. Results regarding point estimators for the impulse response as wellas for confidence regions are reported.
arxiv-11400-205 | Learning the intensity of time events with change-points | http://arxiv.org/pdf/1507.00513v1.pdf | author:Mokhtar Zahdi Alaya, StÃ©phane GaÃ¯ffas, Agathe Guilloux category:math.ST stat.ML stat.TH published:2015-07-02 summary:We consider the problem of learning the inhomogeneous intensity of a countingprocess, under a sparse segmentation assumption. We introduce a weightedtotal-variation penalization, using data-driven weights that correctly scalethe penalization along the observation interval. We prove that this leads to asharp tuning of the convex relaxation of the segmentation prior, by statingoracle inequalities with fast rates of convergence, and consistency forchange-points detection. This provides first theoretical guarantees forsegmentation with a convex proxy beyond the standard i.i.d signal + white noisesetting. We introduce a fast algorithm to solve this convex problem. Numericalexperiments illustrate our approach on simulated and on a high-frequencygenomics dataset.
arxiv-11400-206 | Identification of stable models via nonparametric prediction error methods | http://arxiv.org/pdf/1507.00507v1.pdf | author:Diego Romeres, Gianluigi Pillonetto, Alessandro Chiuso category:stat.ML published:2015-07-02 summary:A new Bayesian approach to linear system identification has been proposed ina series of recent papers. The main idea is to frame linear systemidentification as predictor estimation in an infinite dimensional space, withthe aid of regularization/Bayesian techniques. This approach guarantees theidentification of stable predictors based on the prediction error minimization.Unluckily, the stability of the predictors does not guarantee the stability ofthe impulse response of the system. In this paper we propose and comparevarious techniques to address this issue. Simulations results comparing thesetechniques will be provided.
arxiv-11400-207 | Optimal Transport for Domain Adaptation | http://arxiv.org/pdf/1507.00504v1.pdf | author:Nicolas Courty, RÃ©mi Flamary, Devis Tuia, Alain Rakotomamonjy category:cs.LG published:2015-07-02 summary:Domain adaptation from one data space (or domain) to another is one of themost challenging tasks of modern data analytics. If the adaptation is donecorrectly, models built on a specific data space become more robust whenconfronted to data depicting the same semantic concepts (the classes), butobserved by another observation system with its own specificities. Among themany strategies proposed to adapt a domain to another, finding a commonrepresentation has shown excellent properties: by finding a commonrepresentation for both domains, a single classifier can be effective in bothand use labelled samples from the source domain to predict the unlabelledsamples of the target domain. In this paper, we propose a regularizedunsupervised optimal transportation model to perform the alignment of therepresentations in the source and target domains. We learn a transportationplan matching both PDFs, which constrains labelled samples in the source domainto remain close during transport. This way, we exploit at the same time the fewlabeled information in the source and the unlabelled distributions observed inboth domains. Experiments in toy and challenging real visual adaptationexamples show the interest of the method, that consistently outperforms stateof the art approaches.
arxiv-11400-208 | Distributed image reconstruction for very large arrays in radio astronomy | http://arxiv.org/pdf/1507.00501v1.pdf | author:AndrÃ© Ferrari, David Mary, RÃ©mi Flamary, CÃ©dric Richard category:astro-ph.IM cs.CV published:2015-07-02 summary:Current and future radio interferometric arrays such as LOFAR and SKA arecharacterized by a paradox. Their large number of receptors (up to millions)allow theoretically unprecedented high imaging resolution. In the same time,the ultra massive amounts of samples makes the data transfer and computationalloads (correlation and calibration) order of magnitudes too high to allow anycurrently existing image reconstruction algorithm to achieve, or even approach,the theoretical resolution. We investigate here decentralized and distributedimage reconstruction strategies which select, transfer and process only afraction of the total data. The loss in MSE incurred by the proposed approachis evaluated theoretically and numerically on simple test cases.
arxiv-11400-209 | Non-convex Regularizations for Feature Selection in Ranking With Sparse SVM | http://arxiv.org/pdf/1507.00500v1.pdf | author:LÃ©a Laporte, RÃ©mi Flamary, Stephane Canu, SÃ©bastien DÃ©jean, Josiane Mothe category:cs.LG published:2015-07-02 summary:Feature selection in learning to rank has recently emerged as a crucialissue. Whereas several preprocessing approaches have been proposed, only a fewworks have been focused on integrating the feature selection into the learningprocess. In this work, we propose a general framework for feature selection inlearning to rank using SVM with a sparse regularization term. We investigateboth classical convex regularizations such as $\ell\_1$ or weighted $\ell\_1$and non-convex regularization terms such as log penalty, Minimax ConcavePenalty (MCP) or $\ell\_p$ pseudo norm with $p\textless{}1$. Two algorithms areproposed, first an accelerated proximal approach for solving the convexproblems, second a reweighted $\ell\_1$ scheme to address the non-convexregularizations. We conduct intensive experiments on nine datasets from Letor3.0 and Letor 4.0 corpora. Numerical results show that the use of non-convexregularizations we propose leads to more sparsity in the resulting models whileprediction performance is preserved. The number of features is decreased by upto a factor of six compared to the $\ell\_1$ regularization. In addition, thesoftware is publicly available on the web.
arxiv-11400-210 | DC Proximal Newton for Non-Convex Optimization Problems | http://arxiv.org/pdf/1507.00438v1.pdf | author:Alain Rakotomamonjy, Remi Flamary, Gilles Gasso category:cs.LG cs.NA stat.ML published:2015-07-02 summary:We introduce a novel algorithm for solving learning problems where both theloss function and the regularizer are non-convex but belong to the class ofdifference of convex (DC) functions. Our contribution is a new general purposeproximal Newton algorithm that is able to deal with such a situation. Thealgorithm consists in obtaining a descent direction from an approximation ofthe loss function and then in performing a line search to ensure sufficientdescent. A theoretical analysis is provided showing that the iterates of theproposed algorithm {admit} as limit points stationary points of the DCobjective function. Numerical experiments show that our approach is moreefficient than current state of the art for a problem with a convex lossfunctions and non-convex regularizer. We have also illustrated the benefit ofour algorithm in high-dimensional transductive learning problem where both lossfunction and regularizers are non-convex.
arxiv-11400-211 | Gaussian Process for Noisy Inputs with Ordering Constraints | http://arxiv.org/pdf/1507.00052v2.pdf | author:Cuong Tran, Vladimir Pavlovic, Robert Kopp category:stat.ML published:2015-06-30 summary:We study the Gaussian Process regression model in the context of trainingdata with noise in both input and output. The presence of two sources of noisemakes the task of learning accurate predictive models extremely challenging.However, in some instances additional constraints may be available that canreduce the uncertainty in the resulting predictive models. In particular, weconsider the case of monotonically ordered latent input, which occurs in manyapplication domains that deal with temporal data. We present a novel inferenceand learning approach based on non-parametric Gaussian variationalapproximation to learn the GP model while taking into account the newconstraints. The resulting strategy allows one to gain access to posteriorestimates of both the input and the output and results in improved predictiveperformance. We compare our proposed models to state-of-the-art Noisy InputGaussian Process (NIGP) and other competing approaches on synthetic and realsea-level rise data. Experimental results suggest that the proposed approachconsistently outperforms selected methods while, at the same time, reducing thecomputational costs of learning and inference.
arxiv-11400-212 | Categorical Matrix Completion | http://arxiv.org/pdf/1507.00421v1.pdf | author:Yang Cao, Yao Xie category:cs.NA cs.LG math.ST stat.ML stat.TH published:2015-07-02 summary:We consider the problem of completing a matrix with categorical-valuedentries from partial observations. This is achieved by extending theformulation and theory of one-bit matrix completion. We recover a low-rankmatrix $X$ by maximizing the likelihood ratio with a constraint on the nuclearnorm of $X$, and the observations are mapped from entries of $X$ throughmultiple link functions. We establish theoretical upper and lower bounds on therecovery error, which meet up to a constant factor $\mathcal{O}(K^{3/2})$ where$K$ is the fixed number of categories. The upper bound in our case depends onthe number of categories implicitly through a maximization of terms thatinvolve the smoothness of the link functions. In contrast to one-bit matrixcompletion, our bounds for categorical matrix completion are optimal up to afactor on the order of the square root of the number of categories, which isconsistent with an intuition that the problem becomes harder when the number ofcategories increases. By comparing the performance of our method with theconventional matrix completion method on the MovieLens dataset, we demonstratethe advantage of our method.
arxiv-11400-213 | Scale-Free Algorithms for Online Linear Optimization | http://arxiv.org/pdf/1502.05744v2.pdf | author:Francesco Orabona, David Pal category:cs.LG math.OC published:2015-02-19 summary:We design algorithms for online linear optimization that have optimal regretand at the same time do not need to know any upper or lower bounds on the normof the loss vectors. We achieve adaptiveness to norms of loss vectors by scaleinvariance, i.e., our algorithms make exactly the same decisions if thesequence of loss vectors is multiplied by any positive constant. Our algorithmswork for any decision set, bounded or unbounded. For unbounded decisions sets,these are the first truly adaptive algorithms for online linear optimization.
arxiv-11400-214 | Learning Privately with Labeled and Unlabeled Examples | http://arxiv.org/pdf/1407.2662v3.pdf | author:Amos Beimel, Kobbi Nissim, Uri Stemmer category:cs.LG cs.CR published:2014-07-10 summary:A private learner is an algorithm that given a sample of labeled individualexamples outputs a generalizing hypothesis while preserving the privacy of eachindividual. In 2008, Kasiviswanathan et al. (FOCS 2008) gave a genericconstruction of private learners, in which the sample complexity is (generally)higher than what is needed for non-private learners. This gap in the samplecomplexity was then further studied in several followup papers, showing that(at least in some cases) this gap is unavoidable. Moreover, those papersconsidered ways to overcome the gap, by relaxing either the privacy or thelearning guarantees of the learner. We suggest an alternative approach, inspired by the (non-private) models ofsemi-supervised learning and active-learning, where the focus is on the samplecomplexity of labeled examples whereas unlabeled examples are of asignificantly lower cost. We consider private semi-supervised learners thatoperate on a random sample, where only a (hopefully small) portion of thissample is labeled. The learners have no control over which of the sampleelements are labeled. Our main result is that the labeled sample complexity ofprivate learners is characterized by the VC dimension. We present two generic constructions of private semi-supervised learners. Thefirst construction is of learners where the labeled sample complexity isproportional to the VC dimension of the concept class, however, the unlabeledsample complexity of the algorithm is as big as the representation length ofdomain elements. Our second construction presents a new technique fordecreasing the labeled sample complexity of a given private learner, whileroughly maintaining its unlabeled sample complexity. In addition, we show thatin some settings the labeled sample complexity does not depend on the privacyparameters of the learner.
arxiv-11400-215 | An Empirical Evaluation of True Online TD(Î») | http://arxiv.org/pdf/1507.00353v1.pdf | author:Harm van Seijen, A. Rupam Mahmood, Patrick M. Pilarski, Richard S. Sutton category:cs.AI cs.LG stat.ML published:2015-07-01 summary:The true online TD({\lambda}) algorithm has recently been proposed (vanSeijen and Sutton, 2014) as a universal replacement for the popularTD({\lambda}) algorithm, in temporal-difference learning and reinforcementlearning. True online TD({\lambda}) has better theoretical properties thanconventional TD({\lambda}), and the expectation is that it also results infaster learning. In this paper, we put this hypothesis to the test.Specifically, we compare the performance of true online TD({\lambda}) with thatof TD({\lambda}) on challenging examples, random Markov reward processes, and areal-world myoelectric prosthetic arm. We use linear function approximationwith tabular, binary, and non-binary features. We assess the algorithms alongthree dimensions: computational cost, learning speed, and ease of use. Ourresults confirm the strength of true online TD({\lambda}): 1) for sparsefeature vectors, the computational overhead with respect to TD({\lambda}) isminimal; for non-sparse features the computation time is at most twice that ofTD({\lambda}), 2) across all domains/representations the learning speed of trueonline TD({\lambda}) is often better, but never worse than that ofTD({\lambda}), and 3) true online TD({\lambda}) is easier to use, because itdoes not require choosing between trace types, and it is generally more stablewith respect to the step-size. Overall, our results suggest that true onlineTD({\lambda}) should be the first choice when looking for an efficient,general-purpose TD method.
arxiv-11400-216 | Exploring Algorithmic Limits of Matrix Rank Minimization under Affine Constraints | http://arxiv.org/pdf/1406.2504v3.pdf | author:Bo Xin, David Wipf category:cs.LG stat.ML published:2014-06-10 summary:Many applications require recovering a matrix of minimal rank within anaffine constraint set, with matrix completion a notable special case. Becausethe problem is NP-hard in general, it is common to replace the matrix rank withthe nuclear norm, which acts as a convenient convex surrogate. While eleganttheoretical conditions elucidate when this replacement is likely to besuccessful, they are highly restrictive and convex algorithms fail when theambient rank is too high or when the constraint set is poorly structured.Non-convex alternatives fare somewhat better when carefully tuned; however,convergence to locally optimal solutions remains a continuing source offailure. Against this backdrop we derive a deceptively simple andparameter-free probabilistic PCA-like algorithm that is capable, over a widebattery of empirical tests, of successful recovery even at the theoreticallimit where the number of measurements equal the degrees of freedom in theunknown low-rank matrix. Somewhat surprisingly, this is possible even when theaffine constraint set is highly ill-conditioned. While proving general recoveryguarantees remains evasive for non-convex algorithms, Bayesian-inspired orotherwise, we nonetheless show conditions whereby the underlying cost functionhas a unique stationary point located at the global optimum; no existing costfunction we are aware of satisfies this same property. We conclude with asimple computer vision application involving image rectification and a standardcollaborative filtering benchmark.
arxiv-11400-217 | A Chaining Algorithm for Online Nonparametric Regression | http://arxiv.org/pdf/1502.07697v2.pdf | author:Pierre Gaillard, SÃ©bastien Gerchinovitz category:stat.ML cs.LG published:2015-02-26 summary:We consider the problem of online nonparametric regression with arbitrarydeterministic sequences. Using ideas from the chaining technique, we design analgorithm that achieves a Dudley-type regret bound similar to the one obtainedin a non-constructive fashion by Rakhlin and Sridharan (2014). Our regret boundis expressed in terms of the metric entropy in the sup norm, which yieldsoptimal guarantees when the metric and sequential entropies are of the sameorder of magnitude. In particular our algorithm is the first one that achievesoptimal rates for online regression over H{\"o}lder balls. In addition we showfor this example how to adapt our chaining algorithm to get a reasonablecomputational efficiency with similar regret guarantees (up to a log factor).
arxiv-11400-218 | Pose Embeddings: A Deep Architecture for Learning to Match Human Poses | http://arxiv.org/pdf/1507.00302v1.pdf | author:Greg Mori, Caroline Pantofaru, Nisarg Kothari, Thomas Leung, George Toderici, Alexander Toshev, Weilong Yang category:cs.CV published:2015-07-01 summary:We present a method for learning an embedding that places images of humans insimilar poses nearby. This embedding can be used as a direct method ofcomparing images based on human pose, avoiding potential challenges ofestimating body joint positions. Pose embedding learning is formulated under atriplet-based distance criterion. A deep architecture is used to allow learningof a representation capable of making distinctions between different poses.Experiments on human pose matching and retrieval from video data demonstratethe potential of the method.
arxiv-11400-219 | Bootstrapped Thompson Sampling and Deep Exploration | http://arxiv.org/pdf/1507.00300v1.pdf | author:Ian Osband, Benjamin Van Roy category:stat.ML cs.LG published:2015-07-01 summary:This technical note presents a new approach to carrying out the kind ofexploration achieved by Thompson sampling, but without explicitly maintainingor sampling from posterior distributions. The approach is based on a bootstraptechnique that uses a combination of observed and artificially generated data.The latter serves to induce a prior distribution which, as we will demonstrate,is critical to effective exploration. We explain how the approach can beapplied to multi-armed bandit and reinforcement learning problems and how itrelates to Thompson sampling. The approach is particularly well-suited forcontexts in which exploration is coupled with deep learning, since in thesesettings, maintaining or generating samples from a posterior distributionbecomes computationally infeasible.
arxiv-11400-220 | Energy-efficient neuromorphic classifiers | http://arxiv.org/pdf/1507.00235v1.pdf | author:Daniel MartÃ­, Mattia Rigotti, Mingoo Seok, Stefano Fusi category:q-bio.NC cs.NE published:2015-07-01 summary:Neuromorphic engineering combines the architectural and computationalprinciples of systems neuroscience with semiconductor electronics, with the aimof building efficient and compact devices that mimic the synaptic and neuralmachinery of the brain. Neuromorphic engineering promises extremely low energyconsumptions, comparable to those of the nervous system. However, until now theneuromorphic approach has been restricted to relatively simple circuits andspecialized functions, rendering elusive a direct comparison of their energyconsumption to that used by conventional von Neumann digital machines solvingreal-world tasks. Here we show that a recent technology developed by IBM can beleveraged to realize neuromorphic circuits that operate as classifiers ofcomplex real-world stimuli. These circuits emulate enough neurons to competewith state-of-the-art classifiers. We also show that the energy consumption ofthe IBM chip is typically 2 or more orders of magnitude lower than that ofconventional digital machines when implementing classifiers with comparableperformance. Moreover, the spike-based dynamics display a trade-off betweenintegration time and accuracy, which naturally translates into algorithms thatcan be flexibly deployed for either fast and approximate classifications, ormore accurate classifications at the mere expense of longer running times andhigher energy costs. This work finally proves that the neuromorphic approachcan be efficiently used in real-world applications and it has significantadvantages over conventional digital devices when energy consumption isconsidered.
arxiv-11400-221 | Bigeometric Organization of Deep Nets | http://arxiv.org/pdf/1507.00220v1.pdf | author:Alexander Cloninger, Ronald R. Coifman, Nicholas Downing, Harlan M. Krumholz category:stat.ML cs.LG published:2015-07-01 summary:In this paper, we build an organization of high-dimensional datasets thatcannot be cleanly embedded into a low-dimensional representation due to missingentries and a subset of the features being irrelevant to modeling functions ofinterest. Our algorithm begins by defining coarse neighborhoods of the pointsand defining an expected empirical function value on these neighborhoods. Wethen generate new non-linear features with deep net representations tuned tomodel the approximate function, and re-organize the geometry of the points withrespect to the new representation. Finally, the points are locally z-scored tocreate an intrinsic geometric organization which is independent of theparameters of the deep net, a geometry designed to assure smoothness withrespect to the empirical function. We examine this approach on data from theCenter for Medicare and Medicaid Services Hospital Quality Initiative, andgenerate an intrinsic low-dimensional organization of the hospitals that issmooth with respect to an expert driven function of quality.
arxiv-11400-222 | Natural Neural Networks | http://arxiv.org/pdf/1507.00210v1.pdf | author:Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, Koray Kavukcuoglu category:stat.ML cs.LG cs.NE published:2015-07-01 summary:We introduce Natural Neural Networks, a novel family of algorithms that speedup convergence by adapting their internal representation during training toimprove conditioning of the Fisher matrix. In particular, we show a specificexample that employs a simple and efficient reparametrization of the neuralnetwork weights by implicitly whitening the representation obtained at eachlayer, while preserving the feed-forward computation of the network. Suchnetworks can be trained efficiently via the proposed Projected Natural GradientDescent algorithm (PRONG), which amortizes the cost of these reparametrizationsover many parameter updates and is closely related to the Mirror Descent onlinelearning algorithm. We highlight the benefits of our method on bothunsupervised and supervised learning tasks, and showcase its scalability bytraining on the large-scale ImageNet Challenge dataset.
arxiv-11400-223 | Dimensionality on Summarization | http://arxiv.org/pdf/1507.00209v1.pdf | author:Hai Zhuge category:cs.CL cs.IR published:2015-07-01 summary:Summarization is one of the key features of human intelligence. It plays animportant role in understanding and representation. With rapid and continualexpansion of texts, pictures and videos in cyberspace, automatic summarizationbecomes more and more desirable. Text summarization has been studied for overhalf century, but it is still hard to automatically generate a satisfiedsummary. Traditional methods process texts empirically and neglect thefundamental characteristics and principles of language use and understanding.This paper summarizes previous text summarization approaches in amulti-dimensional classification space, introduces a multi-dimensionalmethodology for research and development, unveils the basic characteristics andprinciples of language use and understanding, investigates some fundamentalmechanisms of summarization, studies the dimensions and forms ofrepresentations, and proposes a multi-dimensional evaluation mechanisms.Investigation extends to the incorporation of pictures into summary and to thesummarization of videos, graphs and pictures, and then reaches a generalsummarization framework.
arxiv-11400-224 | Beyond Semantic Image Segmentation : Exploring Efficient Inference in Video | http://arxiv.org/pdf/1507.01578v1.pdf | author:Subarna Tripathi, Serge Belongie, Truong Nguyen category:cs.CV published:2015-07-01 summary:We explore the efficiency of the CRF inference module beyond image levelsemantic segmentation. The key idea is to combine the best of two worlds ofsemantic co-labeling and exploiting more expressive models. Similar to[Alvarez14] our formulation enables us perform inference over ten thousandimages within seconds. On the other hand, it can handle higher-order cliquepotentials similar to [vineet2014] in terms of region-level label consistencyand context in terms of co-occurrences. We follow the mean-field updates forhigher order potentials similar to [vineet2014] and extend the spatialsmoothness and appearance kernels [DenseCRF13] to address video data inspiredby [Alvarez14]; thus making the system amenable to perform video semanticsegmentation most effectively.
arxiv-11400-225 | Prior Polarity Lexical Resources for the Italian Language | http://arxiv.org/pdf/1507.00133v1.pdf | author:Valeria BorzÃ¬, Simone Faro, Arianna Pavone, Sabrina Sansone category:cs.CL published:2015-07-01 summary:In this paper we present SABRINA (Sentiment Analysis: a Broad Resource forItalian Natural language Applications) a manually annotated prior polaritylexical resource for Italian natural language applications in the field ofopinion mining and sentiment induction. The resource consists in two differentsets, an Italian dictionary of more than 277.000 words tagged with their priorpolarity value, and a set of polarity modifiers, containing more than 200words, which can be used in combination with non neutral terms of thedictionary in order to induce the sentiment of Italian compound terms. To thebest of our knowledge this is the first prior polarity manually annotatedresource which has been developed for the Italian natural language.
arxiv-11400-226 | Polarimetric Hierarchical Semantic Model and Scattering Mechanism Based PolSAR Image Classification | http://arxiv.org/pdf/1507.00110v1.pdf | author:Fang Liu, Junfei Shi, Licheng Jiao, Hongying Liu, Shuyuan Yang, Jie Wu, Hongxia Hao, Jialing Yuan category:cs.CV published:2015-07-01 summary:For polarimetric SAR (PolSAR) image classification, it is a challenge toclassify the aggregated terrain types, such as the urban area, into semantichomogenous regions due to sharp bright-dark variations in intensity. Theaggregated terrain type is formulated by the similar ground objects aggregatedtogether. In this paper, a polarimetric hierarchical semantic model (PHSM) isfirstly proposed to overcome this disadvantage based on the constructions of aprimal-level and a middle-level semantic. The primal-level semantic is apolarimetric sketch map which consists of sketch segments as the sparserepresentation of a PolSAR image. The middle-level semantic is a region mapwhich can extract semantic homogenous regions from the sketch map by exploitingthe topological structure of sketch segments. Mapping the region map to thePolSAR image, a complex PolSAR scene is partitioned into aggregated, structuraland homogenous pixel-level subspaces with the characteristics of relativelycoherent terrain types in each subspace. Then, according to the characteristicsof three subspaces above, three specific methods are adopted, and furthermorepolarimetric information is exploited to improve the segmentation result.Experimental results on PolSAR data sets with different bands and sensorsdemonstrate that the proposed method is superior to the state-of-the-artmethods in region homogeneity and edge preservation for terrain classification.
arxiv-11400-227 | Supervised Learning of Semantics-Preserving Hashing via Deep Neural Networks for Large-Scale Image Search | http://arxiv.org/pdf/1507.00101v1.pdf | author:Huei-Fang Yang, Kevin Lin, Chu-Song Chen category:cs.CV published:2015-07-01 summary:This paper presents a supervised deep hashing approach that constructs binaryhash codes from labeled data for large-scale image search. We assume thatsemantic labels are governed by a set of latent attributes in which eachattribute can be on or off, and classification relies on these attributes.Based on this assumption, our approach, dubbed supervised semantics-preservingdeep hashing (SSDH), constructs hash functions as a latent layer in a deepnetwork in which binary codes are learned by the optimization of an objectivefunction defined over classification error and other desirable properties ofhash codes. With this design, SSDH has a nice property that classification andretrieval are unified in a single learning model, and the learned binary codesnot only preserve the semantic similarity between images but also are efficientfor image search. Moreover, SSDH performs joint learning of imagerepresentations, hash codes, and classification in a pointwised manner and thusis naturally scalable to large-scale datasets. SSDH is simple and can be easilyrealized by a slight modification of an existing deep architecture forclassification; yet it is effective and outperforms other unsupervised andsupervised hashing approaches on several benchmarks and one large datasetcomprising more than 1 million images.
arxiv-11400-228 | On the Equivalence of Factorized Information Criterion Regularization and the Chinese Restaurant Process Prior | http://arxiv.org/pdf/1506.09068v2.pdf | author:Shaohua Li category:stat.ML published:2015-06-30 summary:Factorized Information Criterion (FIC) is a recently developed informationcriterion, based on which a novel model selection methodology, namelyFactorized Asymptotic Bayesian (FAB) Inference, has been developed andsuccessfully applied to various hierarchical Bayesian models. The DirichletProcess (DP) prior, and one of its well known representations, the ChineseRestaurant Process (CRP), derive another line of model selection methods. FICcan be viewed as a prior distribution over the latent variable configurations.Under this view, we prove that when the parameter dimensionality $D_{c}=2$, FICis equivalent to CRP. We argue that when $D_{c}>2$, FIC avoids an inherentproblem of DP/CRP, i.e. the data likelihood will dominate the impact of theprior, and thus the model selection capability will weaken as $D_{c}$increases. However, FIC overestimates the data likelihood. As a result, FIC maybe overly biased towards models with less components. We propose a naturalgeneralization of FIC, which finds a middle ground between CRP and FIC, and mayyield more accurate model selection results than FIC.
arxiv-11400-229 | A Study of Gradient Descent Schemes for General-Sum Stochastic Games | http://arxiv.org/pdf/1507.00093v1.pdf | author:H. L. Prasad, Shalabh Bhatnagar category:cs.LG cs.GT published:2015-07-01 summary:Zero-sum stochastic games are easy to solve as they can be cast as simpleMarkov decision processes. This is however not the case with general-sumstochastic games. A fairly general optimization problem formulation isavailable for general-sum stochastic games by Filar and Vrieze [2004]. However,the optimization problem there has a non-linear objective and non-linearconstraints with special structure. Since gradients of both the objective aswell as constraints of this optimization problem are well defined, gradientbased schemes seem to be a natural choice. We discuss a gradient scheme tunedfor two-player stochastic games. We show in simulations that this scheme indeedconverges to a Nash equilibrium, for a simple terrain exploration problemmodelled as a general-sum stochastic game. However, it turns out that onlyglobal minima of the optimization problem correspond to Nash equilibria of theunderlying general-sum stochastic game, while gradient schemes only guaranteeconvergence to local minima. We then provide important necessary conditions forgradient schemes to converge to Nash equilibria in general-sum stochasticgames.
arxiv-11400-230 | Evaluation of Genotypic Diversity Measurements Exploited in Real-Coded Representation | http://arxiv.org/pdf/1507.00088v1.pdf | author:Guillaume Corriveau, Raynald Guilbault, Antoine Tahan, Robert Sabourin category:cs.NE published:2015-07-01 summary:Numerous genotypic diversity measures (GDMs) are available in the literatureto assess the convergence status of an evolutionary algorithm (EA) or describeits search behavior. In a recent study, the authors of this paper drewattention to the need for a GDM validation framework. In response, this studyproposes three requirements (monotonicity in individual varieties, twinning,and monotonicity in distance) that can clearly portray any GDMs. Thesediversity requirements are analysed by means of controlled populationarrangements. In this paper four GDMs are evaluated with the proposedvalidation framework. The results confirm that properly evaluating populationdiversity is a rather difficult task, as none of the analysed GDMs complieswith all the diversity requirements.
arxiv-11400-231 | Discovering Characteristic Landmarks on Ancient Coins using Convolutional Networks | http://arxiv.org/pdf/1506.09174v2.pdf | author:Jongpil Kim, Vladimir Pavlovic category:cs.CV published:2015-06-30 summary:In this paper, we propose a novel method to find characteristic landmarks onancient Roman imperial coins using deep convolutional neural network models(CNNs). We formulate an optimization problem to discover class-specific regionswhile guaranteeing specific controlled loss of accuracy. Analysis onvisualization of the discovered region confirms that not only can the proposedmethod successfully find a set of characteristic regions per class, but alsothe discovered region is consistent with human expert annotations. We alsopropose a new framework to recognize the Roman coins which exploitshierarchical structure of the ancient Roman coins using the state-of-the-artclassification power of the CNNs adopted to a new task of coin classification.Experimental results show that the proposed framework is able to effectivelyrecognize the ancient Roman coins. For this research, we have collected a newRoman coin dataset where all coins are annotated and consist of observe (head)and reverse (tail) images.
arxiv-11400-232 | Fast Cross-Validation for Incremental Learning | http://arxiv.org/pdf/1507.00066v1.pdf | author:Pooria Joulani, AndrÃ¡s GyÃ¶rgy, Csaba SzepesvÃ¡ri category:stat.ML cs.AI cs.LG published:2015-06-30 summary:Cross-validation (CV) is one of the main tools for performance estimation andparameter tuning in machine learning. The general recipe for computing CVestimate is to run a learning algorithm separately for each CV fold, acomputationally expensive process. In this paper, we propose a new approach toreduce the computational burden of CV-based performance estimation. As opposedto all previous attempts, which are specific to a particular learning model orproblem domain, we propose a general method applicable to a large class ofincremental learning algorithms, which are uniquely fitted to big dataproblems. In particular, our method applies to a wide range of supervised andunsupervised learning tasks with different performance criteria, as long as thebase learning algorithm is incremental. We show that the running time of thealgorithm scales logarithmically, rather than linearly, in the number of CVfolds. Furthermore, the algorithm has favorable properties for parallel anddistributed implementation. Experiments with state-of-the-art incrementallearning algorithms confirm the practicality of the proposed method.
arxiv-11400-233 | Selective Inference and Learning Mixed Graphical Models | http://arxiv.org/pdf/1507.00039v1.pdf | author:Jason D. Lee category:stat.ML cs.LG published:2015-06-30 summary:This thesis studies two problems in modern statistics. First, we studyselective inference, or inference for hypothesis that are chosen after lookingat the data. The motiving application is inference for regression coefficientsselected by the lasso. We present the Condition-on-Selection method that allowsfor valid selective inference, and study its application to the lasso, andseveral other selection algorithms. In the second part, we consider the problem of learning the structure of apairwise graphical model over continuous and discrete variables. We present anew pairwise model for graphical models with both continuous and discretevariables that is amenable to structure learning. In previous work, authorshave considered structure learning of Gaussian graphical models and structurelearning of discrete models. Our approach is a natural generalization of thesetwo lines of work to the mixed case. The penalization scheme involves a novelsymmetric use of the group-lasso norm and follows naturally from a particularparametrization of the model. We provide conditions under which our estimatoris model selection consistent in the high-dimensional regime.
arxiv-11400-234 | Local and Global Inference for High Dimensional Nonparanormal Graphical Models | http://arxiv.org/pdf/1502.02347v2.pdf | author:Quanquan Gu, Yuan Cao, Yang Ning, Han Liu category:stat.ML published:2015-02-09 summary:This paper proposes a unified framework to quantify local and globalinferential uncertainty for high dimensional nonparanormal graphical models. Inparticular, we consider the problems of testing the presence of a single edgeand constructing a uniform confidence subgraph. Due to the presence of unknownmarginal transformations, we propose a pseudo likelihood based inferentialapproach. In sharp contrast to the existing high dimensional score test method,our method is free of tuning parameters given an initial estimator, and extendsthe scope of the existing likelihood based inferential framework. Furthermore,we propose a U-statistic multiplier bootstrap method to construct theconfidence subgraph. We show that the constructed subgraph is contained in thetrue graph with probability greater than a given nominal level. Compared withexisting methods for constructing confidence subgraphs, our method does notrely on Gaussian or sub-Gaussian assumptions. The theoretical properties of theproposed inferential methods are verified by thorough numerical experiments andreal data analysis.
arxiv-11400-235 | Overview of the NLPCC 2015 Shared Task: Chinese Word Segmentation and POS Tagging for Micro-blog Texts | http://arxiv.org/pdf/1505.07599v3.pdf | author:Xipeng Qiu, Peng Qian, Liusong Yin, Shiyu Wu, Xuanjing Huang category:cs.CL published:2015-05-28 summary:In this paper, we give an overview for the shared task at the 4th CCFConference on Natural Language Processing \& Chinese Computing (NLPCC 2015):Chinese word segmentation and part-of-speech (POS) tagging for micro-blogtexts. Different with the popular used newswire datasets, the dataset of thisshared task consists of the relatively informal micro-texts. The shared taskhas two sub-tasks: (1) individual Chinese word segmentation and (2) jointChinese word segmentation and POS Tagging. Each subtask has three tracks todistinguish the systems with different resources. We first introduce thedataset and task, then we characterize the different approaches of theparticipating systems, report the test results, and provide a overview analysisof these results. An online system is available for open registration andevaluation at http://nlp.fudan.edu.cn/nlpcc2015.
arxiv-11400-236 | Learning to Detect Blue-white Structures in Dermoscopy Images with Weak Supervision | http://arxiv.org/pdf/1506.09179v1.pdf | author:Ali Madooei, Mark S. Drew, Hossein Hajimirsadeghi category:cs.CV published:2015-06-30 summary:We propose a novel approach to identify one of the most significantdermoscopic criteria in the diagnosis of Cutaneous Melanoma: the Blue-whitishstructure. In this paper, we achieve this goal in a Multiple Instance Learningframework using only image-level labels of whether the feature is present ornot. As the output, we predict the image classification label and as welllocalize the feature in the image. Experiments are conducted on a challengingdataset with results outperforming state-of-the-art. This study provides animprovement on the scope of modelling for computerized image analysis of skinlesions, in particular in that it puts forward a framework for identificationof dermoscopic local features from weakly-labelled data.
arxiv-11400-237 | On anthropomorphic decision making in a model observer | http://arxiv.org/pdf/1506.09169v1.pdf | author:Ali R. N. Avanaki, Kathryn S. Espig, Tom R. L. Kimpe, Andrew D. A. Maidment category:cs.CV cs.HC published:2015-06-30 summary:By analyzing human readers' performance in detecting small round lesions insimulated digital breast tomosynthesis background in a location known exactlyscenario, we have developed a model observer that is a better predictor ofhuman performance with different levels of background complexity (i.e.,anatomical and quantum noise). Our analysis indicates that human observersperform a lesion detection task by combining a number of sub-decisions, each anindicator of the presence of a lesion in the image stack. This is in contrastto a channelized Hotelling observer, where the detection task is conductedholistically by thresholding a single decision variable, made from an optimallyweighted linear combination of channels. However, it seems that the sub-parperformance of human readers compared to the CHO cannot be fully explained bytheir reliance on sub-decisions, or perhaps we do not consider a sufficientnumber of sub-decisions. To bridge the gap between the performances of humanreaders and the model observer based upon sub-decisions, we use an additivenoise model, the power of which is modulated with the level of backgroundcomplexity. The proposed model observer better predicts the fast drop in humandetection performance with background complexity.
arxiv-11400-238 | Aging display's effect on interpretation of digital pathology slides | http://arxiv.org/pdf/1506.09166v1.pdf | author:Ali R. N. Avanaki, Kathryn S. Espig, Sameer Sawhney, Liron Pantanowitz, Anil V. Parwani, Albert Xthona, Tom R. L. Kimpe category:cs.CV cs.GR published:2015-06-30 summary:It is our conjecture that the variability of colors in a pathology imageeffects the interpretation of pathology cases, whether it is diagnosticaccuracy, diagnostic confidence, or workflow efficiency. In this paper, digitalpathology images are analyzed to quantify the perceived difference in colorthat occurs due to display aging, in particular a change in the maximumluminance, white point, and color gamut. The digital pathology images studiedinclude diagnostically important features, such as the conspicuity of nuclei.Three different display aging models are applied to images: aging of luminance& chrominance, aging of chrominance only, and a stabilized luminance &chrominance (i.e., no aging). These display models and images are then used tocompare conspicuity of nuclei using CIE deltaE2000, a perceptual colordifference metric. The effect of display aging using these display models andimages is further analyzed through a human reader study designed to quantifythe effects from a clinical perspective. Results from our reader study indicatesignificant impact of aged displays on workflow as well as diagnosis as follow.As compared to the originals (no-aging), slides with the effect of agingsimulated were significantly more difficult to read (p-value of 0.0005) andtook longer to score (p-value of 0.02). Moreover, luminance+chrominance agingsignificantly reduced inter-session percent agreement of diagnostic scores(p-value of 0.0418).
arxiv-11400-239 | Framework for Multi-task Multiple Kernel Learning and Applications in Genome Analysis | http://arxiv.org/pdf/1506.09153v1.pdf | author:Christian Widmer, Marius Kloft, Vipin T Sreedharan, Gunnar RÃ¤tsch category:stat.ML cs.CE cs.LG published:2015-06-30 summary:We present a general regularization-based framework for Multi-task learning(MTL), in which the similarity between tasks can be learned or refined using$\ell_p$-norm Multiple Kernel learning (MKL). Based on this very generalformulation (including a general loss function), we derive the correspondingdual formulation using Fenchel duality applied to Hermitian matrices. We showthat numerous established MTL methods can be derived as special cases fromboth, the primal and dual of our formulation. Furthermore, we derive a moderndual-coordinate descend optimization strategy for the hinge-loss variant of ourformulation and provide convergence bounds for our algorithm. As a specialcase, we implement in C++ a fast LibLinear-style solver for $\ell_p$-norm MKL.In the experimental section, we analyze various aspects of our algorithm suchas predictive performance and ability to reconstruct task relationships onbiologically inspired synthetic data, where we have full control over theunderlying ground truth. We also experiment on a new dataset from the domain ofcomputational biology that we collected for the purpose of this paper. Itconcerns the prediction of transcription start sites (TSS) over nine organisms,which is a crucial task in gene finding. Our solvers including all discussedspecial cases are made available as open-source software as part of the SHOGUNmachine learning toolbox (available at \url{http://shogun.ml}).
arxiv-11400-240 | Multi-Cue Structure Preserving MRF for Unconstrained Video Segmentation | http://arxiv.org/pdf/1506.09124v1.pdf | author:Saehoon Yi, Vladimir Pavlovic category:cs.CV published:2015-06-30 summary:Video segmentation is a stepping stone to understanding video context. Videosegmentation enables one to represent a video by decomposing it into coherentregions which comprise whole or parts of objects. However, the challengeoriginates from the fact that most of the video segmentation algorithms arebased on unsupervised learning due to expensive cost of pixelwise videoannotation and intra-class variability within similar unconstrained videoclasses. We propose a Markov Random Field model for unconstrained videosegmentation that relies on tight integration of multiple cues: vertices aredefined from contour based superpixels, unary potentials from temporal smoothlabel likelihood and pairwise potentials from global structure of a video.Multi-cue structure is a breakthrough to extracting coherent object regions forunconstrained videos in absence of supervision. Our experiments on VSB100dataset show that the proposed model significantly outperforms competingstate-of-the-art algorithms. Qualitative analysis illustrates that videosegmentation result of the proposed model is consistent with human perceptionof objects.
arxiv-11400-241 | Algorithms for Lipschitz Learning on Graphs | http://arxiv.org/pdf/1505.00290v2.pdf | author:Rasmus Kyng, Anup Rao, Sushant Sachdeva, Daniel A. Spielman category:cs.LG cs.DS math.MG published:2015-05-01 summary:We develop fast algorithms for solving regression problems on graphs whereone is given the value of a function at some vertices, and must find itssmoothest possible extension to all vertices. The extension we compute is theabsolutely minimal Lipschitz extension, and is the limit for large $p$ of$p$-Laplacian regularization. We present an algorithm that computes a minimalLipschitz extension in expected linear time, and an algorithm that computes anabsolutely minimal Lipschitz extension in expected time $\widetilde{O} (m n)$.The latter algorithm has variants that seem to run much faster in practice.These extensions are particularly amenable to regularization: we can perform$l_{0}$-regularization on the given values in polynomial time and$l_{1}$-regularization on the initial function values and on graph edge weightsin time $\widetilde{O} (m^{3/2})$.
arxiv-11400-242 | Forming A Random Field via Stochastic Cliques: From Random Graphs to Fully Connected Random Fields | http://arxiv.org/pdf/1506.09110v1.pdf | author:Mohammad Javad Shafiee, Alexander Wong, Paul Fieguth category:cs.CV published:2015-06-30 summary:Random fields have remained a topic of great interest over past decades forthe purpose of structured inference, especially for problems such as imagesegmentation. The local nodal interactions commonly used in such models oftensuffer the short-boundary bias problem, which are tackled primarily through theincorporation of long-range nodal interactions. However, the issue ofcomputational tractability becomes a significant issue when incorporating suchlong-range nodal interactions, particularly when a large number of long-rangenodal interactions (e.g., fully-connected random fields) are modeled. In this work, we introduce a generalized random field framework based aroundthe concept of stochastic cliques, which addresses the issue of computationaltractability when using fully-connected random fields by stochastically forminga sparse representation of the random field. The proposed framework allows forefficient structured inference using fully-connected random fields without anyrestrictions on the potential functions that can be utilized. Severalrealizations of the proposed framework using graph cuts are presented andevaluated, and experimental results demonstrate that the proposed framework canprovide competitive performance for the purpose of image segmentation whencompared to existing fully-connected and principled deep random fieldframeworks.
arxiv-11400-243 | Divide-and-Conquer with Sequential Monte Carlo | http://arxiv.org/pdf/1406.4993v2.pdf | author:Fredrik Lindsten, Adam M. Johansen, Christian A. Naesseth, Bonnie Kirkpatrick, Thomas B. SchÃ¶n, John Aston, Alexandre Bouchard-CÃ´tÃ© category:stat.CO stat.ML published:2014-06-19 summary:We propose a novel class of Sequential Monte Carlo (SMC) algorithms,appropriate for inference in probabilistic graphical models. This class ofalgorithms adopts a divide-and-conquer approach based upon an auxiliarytree-structured decomposition of the model of interest, turning the overallinferential task into a collection of recursively solved sub-problems. Theproposed method is applicable to a broad class of probabilistic graphicalmodels, including models with loops. Unlike a standard SMC sampler, theproposed Divide-and-Conquer SMC employs multiple independent populations ofweighted particles, which are resampled, merged, and propagated as the methodprogresses. We illustrate empirically that this approach can outperformstandard methods in terms of the accuracy of the posterior expectation andmarginal likelihood approximations. Divide-and-Conquer SMC also opens up novelparallel implementation options and the possibility of concentrating thecomputational effort on the most challenging sub-problems. We demonstrate itsperformance on a Markov random field and on a hierarchical logistic regressionproblem.
arxiv-11400-244 | Artificial Catalytic Reactions in 2D for Combinatorial Optimization | http://arxiv.org/pdf/1506.09019v1.pdf | author:Jaderick P. Pabico category:cs.ET cs.NE published:2015-06-30 summary:Presented in this paper is a derivation of a 2D catalytic reaction-basedmodel to solve combinatorial optimization problems (COPs). The simulatedcatalytic reactions, a computational metaphor, occurs in an artificial chemicalreactor that finds near-optimal solutions to COPs. The artificial environmentis governed by catalytic reactions that can alter the structure of artificialmolecular elements. Altering the molecular structure means finding newsolutions to the COP. The molecular mass of the elements was considered as ameasure of goodness of fit of the solutions. Several data structures andmatrices were used to record the directions and locations of the molecules.These provided the model the 2D topology. The Traveling Salesperson Problem(TSP) was used as a working example. The performance of the model in finding asolution for the TSP was compared to the performance of a topology-less model.Experimental results show that the 2D model performs better than thetopology-less one.
arxiv-11400-245 | Fast ADMM Algorithm for Distributed Optimization with Adaptive Penalty | http://arxiv.org/pdf/1506.08928v1.pdf | author:Changkyu Song, Sejong Yoon, Vladimir Pavlovic category:cs.LG cs.CV math.OC published:2015-06-30 summary:We propose new methods to speed up convergence of the Alternating DirectionMethod of Multipliers (ADMM), a common optimization tool in the context oflarge scale and distributed learning. The proposed method accelerates the speedof convergence by automatically deciding the constraint penalty needed forparameter consensus in each iteration. In addition, we also propose anextension of the method that adaptively determines the maximum number ofiterations to update the penalty. We show that this approach effectively leadsto an adaptive, dynamic network topology underlying the distributedoptimization. The utility of the new penalty update schemes is demonstrated onboth synthetic and real data, including a computer vision application ofdistributed structure from motion.
arxiv-11400-246 | Learning Single Index Models in High Dimensions | http://arxiv.org/pdf/1506.08910v1.pdf | author:Ravi Ganti, Nikhil Rao, Rebecca M. Willett, Robert Nowak category:stat.ML cs.LG stat.ME published:2015-06-30 summary:Single Index Models (SIMs) are simple yet flexible semi-parametric models forclassification and regression. Response variables are modeled as a nonlinear,monotonic function of a linear combination of features. Estimation in thiscontext requires learning both the feature weights, and the nonlinear function.While methods have been described to learn SIMs in the low dimensional regime,a method that can efficiently learn SIMs in high dimensions has not beenforthcoming. We propose three variants of a computationally and statisticallyefficient algorithm for SIM inference in high dimensions. We establish excessrisk bounds for the proposed algorithms and experimentally validate theadvantages that our SIM learning methods provide relative to Generalized LinearModel (GLM) and low dimensional SIM based learning methods.
arxiv-11400-247 | An Improved BKW Algorithm for LWE with Applications to Cryptography and Lattices | http://arxiv.org/pdf/1506.02717v4.pdf | author:Paul Kirchner, Pierre-Alain Fouque category:cs.CR cs.DS cs.LG I.1.2; F.2.1 published:2015-06-08 summary:In this paper, we study the Learning With Errors problem and its binaryvariant, where secrets and errors are binary or taken in a small interval. Weintroduce a new variant of the Blum, Kalai and Wasserman algorithm, relying ona quantization step that generalizes and fine-tunes modulus switching. Ingeneral this new technique yields a significant gain in the constant in frontof the exponent in the overall complexity. We illustrate this by solving pwithin half a day a LWE instance with dimension n = 128, modulus $q = n^2$,Gaussian noise $\alpha = 1/(\sqrt{n/\pi} \log^2 n)$ and binary secret, using$2^{28}$ samples, while the previous best result based on BKW claims a timecomplexity of $2^{74}$ with $2^{60}$ samples for the same parameters. We thenintroduce variants of BDD, GapSVP and UniqueSVP, where the target point isrequired to lie in the fundamental parallelepiped, and show how the previousalgorithm is able to solve these variants in subexponential time. Moreover, wealso show how the previous algorithm can be used to solve the BinaryLWE problemwith n samples in subexponential time $2^{(\ln 2/2+o(1))n/\log \log n}$. Thisanalysis does not require any heuristic assumption, contrary to other algebraicapproaches; instead, it uses a variant of an idea by Lyubashevsky to generatemany samples from a small number of samples. This makes it possible toasymptotically and heuristically break the NTRU cryptosystem in subexponentialtime (without contradicting its security assumption). We are also able to solvesubset sum problems in subexponential time for density $o(1)$, which is ofindependent interest: for such density, the previous best algorithm requiresexponential time. As a direct application, we can solve in subexponential timethe parameters of a cryptosystem based on this problem proposed at TCC 2010.
arxiv-11400-248 | Machine learning for many-body physics: efficient solution of dynamical mean-field theory | http://arxiv.org/pdf/1506.08858v1.pdf | author:Louis-FranÃ§ois Arsenault, O. Anatole von Lilienfeld, Andrew J. Millis category:stat.ML published:2015-06-29 summary:Machine learning methods for solving the equations of dynamical mean-fieldtheory are developed. The method is demonstrated on the three dimensionalHubbard model. The key technical issues are defining a mapping of an inputfunction to an output function, and distinguishing metallic from insulatingsolutions. Both metallic and Mott insulator solutions can be predicted. Thevalidity of the machine learning scheme is assessed by comparing predictions offull correlation functions, of quasi-particle weight and particle density tovalues directly computed. The results indicate that with modest furtherdevelopment, machine learning approach may be an attractive computationalefficient option for real materials predictions for strongly correlatedsystems.
arxiv-11400-249 | Statistical Inference using the Morse-Smale Complex | http://arxiv.org/pdf/1506.08826v1.pdf | author:Yen-Chi Chen, Christopher R. Genovese, Larry Wasserman category:math.ST stat.ME stat.ML stat.TH published:2015-06-29 summary:The Morse-Smale complex decomposes the sample space into cells where a givenfunction $f$ is increasing or decreasing. When applied to nonparametric densityestimation and regression, it provides a way to represent, visualize andcompare functions, even in high dimensions. In this paper, we study theestimation of the Morse-Smale complex and we use our results for a variety ofstatistical problems including: nonparametric two-sample testing, densityestimation, nonparametric regression and mode clustering.
arxiv-11400-250 | Requirement Tracing using Term Extraction | http://arxiv.org/pdf/1506.08789v1.pdf | author:Najla Al-Saati, Raghda Abdul-Jaleel category:cs.SE cs.CL cs.IR published:2015-06-29 summary:Requirements traceability is an essential step in ensuring the quality ofsoftware during the early stages of its development life cycle. Requirementstracing usually consists of document parsing, candidate link generation andevaluation and traceability analysis. This paper demonstrates the applicabilityof Statistical Term Extraction metrics to generate candidate links. It isapplied and validated using two data sets and four types of filters two foreach data set, 0.2 and 0.25 for MODIS, 0 and 0.05 for CM1. This methodgenerates requirements traceability matrices between textual requirementsartifacts (such as high-level requirements traced to low-level requirements).The proposed method includes ten word frequency metrics divided into three maingroups for calculating the frequency of terms. The results show that theproposed method gives better result when compared with the traditional TF-IDFmethod.
arxiv-11400-251 | Bayesian Nonparametric Kernel-Learning | http://arxiv.org/pdf/1506.08776v1.pdf | author:Junier Oliva, Avinava Dubey, Barnabas Poczos, Jeff Schneider, Eric P. Xing category:stat.ML published:2015-06-29 summary:Kernel methods are ubiquitous tools in machine learning. They have proven tobe effective in many domains and tasks. Yet, kernel methods often require theuser to select a predefined kernel to build an estimator with. However, thereis often little reason for the a priori selection of a kernel. Even if auniversal approximating kernel is selected, the quality of the finite sampleestimator may be greatly effected by the choice of kernel. Furthermore, whendirectly applying kernel methods, one typically needs to compute a $N \times N$Gram matrix of pairwise kernel evaluations to work with a dataset of $N$instances. The computation of this Gram matrix precludes the direct applicationof kernel methods on large datasets. In this paper we introduce Bayesiannonparmetric kernel (BaNK) learning, a generic, data-driven framework forscalable learning of kernels. We show that this framework can be used forperforming both regression and classification tasks and scale to largedatasets. Furthermore, we show that BaNK outperforms several other scalableapproaches for kernel learning on a variety of real world datasets.
arxiv-11400-252 | Spectral Motion Synchronization in SE(3) | http://arxiv.org/pdf/1506.08765v1.pdf | author:Federica Arrigoni, Andrea Fusiello, Beatrice Rossi category:cs.CV published:2015-06-29 summary:This paper addresses the problem of motion synchronization (or averaging) anddescribes a simple, closed-form solution based on a spectral decomposition,which does not consider rotation and translation separately but works straightin SE(3), the manifold of rigid motions. Besides its theoretical interest,being the first closed form solution in SE(3), experimental results show thatit compares favourably with the state of the art both in terms of precision andspeed.
arxiv-11400-253 | S2: An Efficient Graph Based Active Learning Algorithm with Application to Nonparametric Classification | http://arxiv.org/pdf/1506.08760v1.pdf | author:Gautam Dasarathy, Robert Nowak, Xiaojin Zhu category:cs.LG stat.ML published:2015-06-29 summary:This paper investigates the problem of active learning for binary labelprediction on a graph. We introduce a simple and label-efficient algorithmcalled S2 for this task. At each step, S2 selects the vertex to be labeledbased on the structure of the graph and all previously gathered labels.Specifically, S2 queries for the label of the vertex that bisects the *shortestshortest* path between any pair of oppositely labeled vertices. We present atheoretical estimate of the number of queries S2 needs in terms of a novelparametrization of the complexity of binary functions on graphs. We alsopresent experimental results demonstrating the performance of S2 on both realand synthetic data. While other graph-based active learning algorithms haveshown promise in practice, our algorithm is the first with both goodperformance and theoretical guarantees. Finally, we demonstrate theimplications of the S2 algorithm to the theory of nonparametric activelearning. In particular, we show that S2 achieves near minimax optimal excessrisk for an important class of nonparametric classification problems.
arxiv-11400-254 | An automatic and efficient foreground object extraction scheme | http://arxiv.org/pdf/1506.08704v1.pdf | author:Subhajit Adhikari, Joydeep Kar, Jayati Ghosh Dastidar category:cs.CV published:2015-06-29 summary:This paper presents a method to differentiate the foreground objects from thebackground of a color image. Firstly a color image of any size is input forprocessing. The algorithm converts it to a grayscale image. Next we apply cannyedge detector to find the boundary of the foreground object. We concentrate tofind the maximum distance between each boundary pixel column wise and row wiseand we fill the region that is bound by the edges. Thus we are able to extractthe grayscale values of pixels that are in the bounded region and convert thegrayscale image back to original color image containing only the foregroundobject.
arxiv-11400-255 | Tracking Direction of Human Movement - An Efficient Implementation using Skeleton | http://arxiv.org/pdf/1506.08815v1.pdf | author:Merina Kundu, Dhriti Sengupta, Jayati Ghosh Dastidar category:cs.CV published:2015-06-29 summary:Sometimes a simple and fast algorithm is required to detect human presenceand movement with a low error rate in a controlled environment for securitypurposes. Here a light weight algorithm has been presented that generates alerton detection of human presence and its movement towards a certain direction.The algorithm uses fixed angle CCTV camera images taken over time and reliesupon skeleton transformation of successive images and calculation of differencein their coordinates.
arxiv-11400-256 | Portfolio optimization using local linear regression ensembles in RapidMiner | http://arxiv.org/pdf/1506.08690v1.pdf | author:Gabor Nagy, Gergo Barta, Tamas Henk category:q-fin.PM cs.LG stat.ML published:2015-06-29 summary:In this paper we implement a Local Linear Regression Ensemble Committee(LOLREC) to predict 1-day-ahead returns of 453 assets form the S&P500. Theestimates and the historical returns of the committees are used to compute theweights of the portfolio from the 453 stock. The proposed method outperformsbenchmark portfolio selection strategies that optimize the growth rate of thecapital. We investigate the effect of algorithm parameter m: the number ofselected stocks on achieved average annual yields. Results suggest thealgorithm's practical usefulness in everyday trading.
arxiv-11400-257 | Human Shape Variation - An Efficient Implementation using Skeleton | http://arxiv.org/pdf/1506.08682v1.pdf | author:Dhriti Sengupta, Merina Kundu, Jayati Ghosh Dastidar category:cs.CV published:2015-06-29 summary:It is at times important to detect human presence automatically in secureenvironments. This needs a shape recognition algorithm that is robust, fast andhas low error rates. The algorithm needs to process camera images quickly todetect any human in the range of vision, and generate alerts, especially if theobject under scrutiny is moving in certain directions. We present here asimple, efficient and fast algorithm using skeletons of the images, and simplefeatures like posture and length of the object.
arxiv-11400-258 | Automatic Channel Network Extraction from Remotely Sensed Images by Singularity Analysis | http://arxiv.org/pdf/1506.08670v1.pdf | author:F. Isikdogan, A. C. Bovik, P. Passalacqua category:cs.CV published:2015-06-29 summary:Quantitative analysis of channel networks plays an important role in riverstudies. To provide a quantitative representation of channel networks, wepropose a new method that extracts channels from remotely sensed images andestimates their widths. Our fully automated method is based on a recentlyproposed Multiscale Singularity Index that responds strongly to curvilinearstructures but weakly to edges. The algorithm produces a channel map, using asingle image where water and non-water pixels have contrast, such as a Landsatnear-infrared band image or a water index defined on multiple bands. Theproposed method provides a robust alternative to the procedures that are usedin remote sensing of fluvial geomorphology and makes classification andanalysis of channel networks easier. The source code of the algorithm isavailable at: http://live.ece.utexas.edu/research/cne/.
arxiv-11400-259 | A review of landmark articles in the field of co-evolutionary computing | http://arxiv.org/pdf/1506.05082v2.pdf | author:Noe Casas category:cs.NE published:2015-06-10 summary:Coevolution is a powerful tool in evolutionary computing that mitigates someof its endemic problems, namely stagnation in local optima and lack ofconvergence in high dimensionality problems. Since its inception in 1990, thereare multiple articles that have contributed greatly to the development andimprovement of the coevolutionary techniques. In this report we review some ofthose landmark articles dwelving in the techniques they propose and how theyfit to conform robust evolutionary algorithms
arxiv-11400-260 | Regret Lower Bound and Optimal Algorithm in Dueling Bandit Problem | http://arxiv.org/pdf/1506.02550v3.pdf | author:Junpei Komiyama, Junya Honda, Hisashi Kashima, Hiroshi Nakagawa category:stat.ML cs.LG published:2015-06-08 summary:We study the $K$-armed dueling bandit problem, a variation of the standardstochastic bandit problem where the feedback is limited to relative comparisonsof a pair of arms. We introduce a tight asymptotic regret lower bound that isbased on the information divergence. An algorithm that is inspired by theDeterministic Minimum Empirical Divergence algorithm (Honda and Takemura, 2010)is proposed, and its regret is analyzed. The proposed algorithm is found to bethe first one with a regret upper bound that matches the lower bound.Experimental comparisons of dueling bandit algorithms show that the proposedalgorithm significantly outperforms existing ones.
arxiv-11400-261 | Variational Inference for Background Subtraction in Infrared Imagery | http://arxiv.org/pdf/1506.08581v1.pdf | author:Konstantinos Makantasis, Anastasios Doulamis, Nikolaos Doulamis category:cs.CV cs.LG published:2015-06-29 summary:We propose a Gaussian mixture model for background subtraction in infraredimagery. Following a Bayesian approach, our method automatically estimates thenumber of Gaussian components as well as their parameters, while simultaneouslyit avoids over/under fitting. The equations for estimating model parameters areanalytically derived and thus our method does not require any samplingalgorithm that is computationally and memory inefficient. The pixel densityestimate is followed by an efficient and highly accurate updating mechanism,which permits our system to be automatically adapted to dynamically changingoperation conditions. Experimental results and comparisons with other methodsshow that our method outperforms, in terms of precision and recall, while atthe same time it keeps computational cost suitable for real-time applications.
arxiv-11400-262 | Exact and approximate inference in graphical models: variable elimination and beyond | http://arxiv.org/pdf/1506.08544v1.pdf | author:Nathalie Peyrard, Simon de Givry, Alain Franc, StÃ©phane Robin, RÃ©gis Sabbadin, Thomas Schiex, Matthieu Vignes category:stat.ML cs.AI cs.LG published:2015-06-29 summary:Probabilistic graphical models offer a powerful framework to account for thedependence structure between variables, which can be represented as a graph.The dependence between variables may render inference tasks such as computingnormalizing constant, marginalization or optimization intractable. Theobjective of this paper is to review techniques exploiting the graph structurefor exact inference borrowed from optimization and computer science. They arenot yet standard in the statistician toolkit, and we specify under whichconditions they are efficient in practice. They are built on the principle ofvariable elimination whose complexity is dictated in an intricate way by theorder in which variables are eliminated in the graph. The so-called treewidthof the graph characterizes this algorithmic complexity: low-treewidth graphscan be processed efficiently. Algorithmic solutions derived from variableelimination and the notion of treewidth are illustrated on problems oftreewidth computation and inference in challenging benchmarks from optimizationcompetitions. We also review how efficient techniques for approximate inferencesuch as loopy belief propagation and variational approaches can be linked tovariable elimination and we illustrate them in the context ofExpectation-Maximisation procedures for parameter estimation in coupled HiddenMarkov Models.
arxiv-11400-263 | Tell and Predict: Kernel Classifier Prediction for Unseen Visual Classes from Unstructured Text Descriptions | http://arxiv.org/pdf/1506.08529v1.pdf | author:Mohamed Elhoseiny, Ahmed Elgammal, Babak Saleh category:cs.CV published:2015-06-29 summary:In this paper we propose a framework for predicting kernelized classifiers inthe visual domain for categories with no training images where the knowledgecomes from textual description about these categories. Through our optimizationframework, the proposed approach is capable of embedding the class-levelknowledge from the text domain as kernel classifiers in the visual domain. Wealso proposed a distributional semantic kernel between text descriptions whichis shown to be effective in our setting. The proposed framework is notrestricted to textual descriptions, and can also be applied to other formsknowledge representations. Our approach was applied for the challenging task ofzero-shot learning of fine-grained categories from text descriptions of thesecategories.
arxiv-11400-264 | Integrative analysis of gene expression and phenotype data | http://arxiv.org/pdf/1506.08511v1.pdf | author:Min Xu category:q-bio.QM q-bio.GN q-bio.MN stat.ML published:2015-06-29 summary:The linking genotype to phenotype is the fundamental aim of modern genetics.We focus on study of links between gene expression data and phenotype datathrough integrative analysis. We propose three approaches. 1) The inherent complexity of phenotypes makes high-throughput phenotypeprofiling a very difficult and laborious process. We propose a method ofautomated multi-dimensional profiling which uses gene expression similarity.Large-scale analysis show that our method can provide robust profiling thatreveals different phenotypic aspects of samples. This profiling technique isalso capable of interpolation and extrapolation beyond the phenotypeinformation given in training data. It can be used in many applications,including facilitating experimental design and detecting confounding factors. 2) Phenotype association analysis problems are complicated by small samplesize and high dimensionality. Consequently, phenotype-associated gene subsetsobtained from training data are very sensitive to selection of trainingsamples, and the constructed sample phenotype classifiers tend to have poorgeneralization properties. To eliminate these obstacles, we propose a novelapproach that generates sequences of increasingly discriminative gene clustercombinations. Our experiments on both simulated and real datasets show robustand accurate classification performance. 3) Many complex phenotypes, such as cancer, are the product of not only geneexpression, but also gene interaction. We propose an integrative approach tofind gene network modules that activate under different phenotype conditions.Using our method, we discovered cancer subtype-specific network modules, aswell as the ways in which these modules coordinate. In particular, we detecteda breast-cancer specific tumor suppressor network module with a hub gene,PDGFRL, which may play an important role in this module.
arxiv-11400-265 | Compressed Sensing of Multi-Channel EEG Signals: The Simultaneous Cosparsity and Low Rank Optimization | http://arxiv.org/pdf/1506.08499v1.pdf | author:Yipeng Liu, Maarten De Vos, Sabine Van Huffel category:cs.IT math.IT stat.ML published:2015-06-29 summary:Goal: This paper deals with the problems that some EEG signals have no goodsparse representation and single channel processing is not computationallyefficient in compressed sensing of multi-channel EEG signals. Methods: Anoptimization model with L0 norm and Schatten-0 norm is proposed to enforcecosparsity and low rank structures in the reconstructed multi-channel EEGsignals. Both convex relaxation and global consensus optimization withalternating direction method of multipliers are used to compute theoptimization model. Results: The performance of multi-channel EEG signalreconstruction is improved in term of both accuracy and computationalcomplexity. Conclusion: The proposed method is a better candidate than previoussparse signal recovery methods for compressed sensing of EEG signals.Significance: The proposed method enables successful compressed sensing of EEGsignals even when the signals have no good sparse representation. Usingcompressed sensing would much reduce the power consumption of wireless EEGsystem.
arxiv-11400-266 | The Multi-Strand Graph for a PTZ Tracker | http://arxiv.org/pdf/1506.08485v1.pdf | author:Shachaf Melman, Yael Moses, GÃ©rard Medioni, Yinghao Cai category:cs.CV published:2015-06-29 summary:High-resolution images can be used to resolve matching ambiguities betweentrajectory fragments (tracklets), which is one of the main challenges inmultiple target tracking. A PTZ camera, which can pan, tilt and zoom, is apowerful and efficient tool that offers both close-up views and wide areacoverage on demand. The wide-area view makes it possible to track many targetswhile the close-up view allows individuals to be identified fromhigh-resolution images of their faces. A central component of a PTZ trackingsystem is a scheduling algorithm that determines which target to zoom in on. In this paper we study this scheduling problem from a theoreticalperspective, where the high resolution images are also used for trackletmatching. We propose a novel data structure, the Multi-Strand Tracking Graph(MSG), which represents the set of tracklets computed by a tracker and thepossible associations between them. The MSG allows efficient scheduling as wellas resolving -- directly or by elimination -- matching ambiguities betweentracklets. The main feature of the MSG is the auxiliary data saved in eachvertex, which allows efficient computation while avoiding time-consuming graphtraversal. Synthetic data simulations are used to evaluate our schedulingalgorithm and to demonstrate its superiority over a na\"ive one.
arxiv-11400-267 | Nonparametric Estimation of Band-limited Probability Density Functions | http://arxiv.org/pdf/1503.06236v5.pdf | author:Rahul Agarwal, Zhe Chen, Sridevi V. Sarma category:stat.ML math.ST stat.ME stat.TH published:2015-03-20 summary:In this paper, a nonparametric maximum likelihood (ML) estimator forband-limited (BL) probability density functions (pdfs) is proposed. The BLMLestimator is consistent and computationally efficient. To compute the BLMLestimator, three approximate algorithms are presented: a binary quadraticprogramming (BQP) algorithm for medium scale problems, a Trivial algorithm forlarge-scale problems that yields a consistent estimate if the underlying pdf isstrictly positive and BL, and a fast implementation of the Trivial algorithmthat exploits the band-limited assumption and the Nyquist sampling theorem("BLMLQuick"). All three BLML estimators outperform kernel density estimation(KDE) algorithms (adaptive and higher order KDEs) with respect to the meanintegrated squared error for data generated from both BL and infinite-bandpdfs. Further, the BLMLQuick estimate is remarkably faster than the KDalgorithms. Finally, the BLML method is applied to estimate the conditionalintensity function of a neuronal spike train (point process) recorded from arat's entorhinal cortex grid cell, for which it outperforms state-of-the-artestimators used in neuroscience.
arxiv-11400-268 | A Bayesian Model for Generative Transition-based Dependency Parsing | http://arxiv.org/pdf/1506.04334v2.pdf | author:Jan Buys, Phil Blunsom category:cs.CL published:2015-06-13 summary:We propose a simple, scalable, fully generative model for transition-baseddependency parsing with high accuracy. The model, parameterized by HierarchicalPitman-Yor Processes, overcomes the limitations of previous generative modelsby allowing fast and accurate inference. We propose an efficient decodingalgorithm based on particle filtering that can adapt the beam size to theuncertainty in the model while jointly predicting POS tags and parse trees. TheUAS of the parser is on par with that of a greedy discriminative baseline. As alanguage model, it obtains better perplexity than a n-gram model by performingsemi-supervised learning over a large unlabelled corpus. We show that the modelis able to generate locally and syntactically coherent sentences, opening thedoor to further applications in language generation.
arxiv-11400-269 | WYSIWYE: An Algebra for Expressing Spatial and Textual Rules for Information Extraction | http://arxiv.org/pdf/1506.08454v1.pdf | author:Vijil Chenthamarakshan, Prasad M Desphande, Raghu Krishnapuram, Ramakrishna Varadarajan, Knut Stolze category:cs.CL cs.DB cs.IR published:2015-06-28 summary:The visual layout of a webpage can provide valuable clues for certain typesof Information Extraction (IE) tasks. In traditional rule based IE frameworks,these layout cues are mapped to rules that operate on the HTML source of thewebpages. In contrast, we have developed a framework in which the rules can bespecified directly at the layout level. This has many advantages, since thehigher level of abstraction leads to simpler extraction rules that are largelyindependent of the source code of the page, and, therefore, more robust. It canalso enable specification of new types of rules that are not otherwisepossible. To the best of our knowledge, there is no general framework thatallows declarative specification of information extraction rules based onspatial layout. Our framework is complementary to traditional text based rulesframework and allows a seamless combination of spatial layout based rules withtraditional text based rules. We describe the algebra that enables such asystem and its efficient implementation using standard relational and textindexing features of a relational database. We demonstrate the simplicity andefficiency of this system for a task involving the extraction of softwaresystem requirements from software product pages.
arxiv-11400-270 | Oriented Edge Forests for Boundary Detection | http://arxiv.org/pdf/1412.4181v2.pdf | author:Sam Hallman, Charless C. Fowlkes category:cs.CV published:2014-12-13 summary:We present a simple, efficient model for learning boundary detection based ona random forest classifier. Our approach combines (1) efficient clustering oftraining examples based on simple partitioning of the space of local edgeorientations and (2) scale-dependent calibration of individual tree outputprobabilities prior to multiscale combination. The resulting model outperformspublished results on the challenging BSDS500 boundary detection benchmark.Further, on large datasets our model requires substantially less memory fortraining and speeds up training time by a factor of 10 over the structuredforest model.
arxiv-11400-271 | Deep-Plant: Plant Identification with convolutional neural networks | http://arxiv.org/pdf/1506.08425v1.pdf | author:Sue Han Lee, Chee Seng Chan, Paul Wilkin, Paolo Remagnino category:cs.CV cs.AI cs.NE published:2015-06-28 summary:This paper studies convolutional neural networks (CNN) to learn unsupervisedfeature representations for 44 different plant species, collected at the RoyalBotanic Gardens, Kew, England. To gain intuition on the chosen features fromthe CNN model (opposed to a 'black box' solution), a visualisation techniquebased on the deconvolutional networks (DN) is utilized. It is found thatvenations of different order have been chosen to uniquely represent each of theplant species. Experimental results using these CNN features with differentclassifiers show consistency and superiority compared to the state-of-the artsolutions which rely on hand-crafted features.
arxiv-11400-272 | Topic2Vec: Learning Distributed Representations of Topics | http://arxiv.org/pdf/1506.08422v1.pdf | author:Li-Qiang Niu, Xin-Yu Dai category:cs.CL cs.LG published:2015-06-28 summary:Latent Dirichlet Allocation (LDA) mining thematic structure of documentsplays an important role in nature language processing and machine learningareas. However, the probability distribution from LDA only describes thestatistical relationship of occurrences in the corpus and usually in practice,probability is not the best choice for feature representations. Recently,embedding methods have been proposed to represent words and documents bylearning essential concepts and representations, such as Word2Vec and Doc2Vec.The embedded representations have shown more effectiveness than LDA-stylerepresentations in many tasks. In this paper, we propose the Topic2Vec approachwhich can learn topic representations in the same semantic vector space withwords, as an alternative to probability. The experimental results show thatTopic2Vec achieves interesting and meaningful results.
arxiv-11400-273 | The fundamental nature of the log loss function | http://arxiv.org/pdf/1502.06254v2.pdf | author:Vladimir Vovk category:cs.LG stat.ME published:2015-02-22 summary:The standard loss functions used in the literature on probabilisticprediction are the log loss function, the Brier loss function, and thespherical loss function; however, any computable proper loss function can beused for comparison of prediction algorithms. This note shows that the log lossfunction is most selective in that any prediction algorithm that is optimal fora given data sequence (in the sense of the algorithmic theory of randomness)under the log loss function will be optimal under any computable proper mixableloss function; on the other hand, there is a data sequence and a predictionalgorithm that is optimal for that sequence under either of the two otherstandard loss functions but not under the log loss function.
arxiv-11400-274 | Non-Normal Mixtures of Experts | http://arxiv.org/pdf/1506.06707v2.pdf | author:Faicel Chamroukhi category:stat.ME cs.LG stat.ML published:2015-06-22 summary:Mixture of Experts (MoE) is a popular framework for modeling heterogeneity indata for regression, classification and clustering. For continuous data whichwe consider here in the context of regression and cluster analysis, MoE usuallyuse normal experts, that is, expert components following the Gaussiandistribution. However, for a set of data containing a group or groups ofobservations with asymmetric behavior, heavy tails or atypical observations,the use of normal experts may be unsuitable and can unduly affect the fit ofthe MoE model. In this paper, we introduce new non-normal mixture of experts(NNMoE) which can deal with these issues regarding possibly skewed,heavy-tailed data and with outliers. The proposed models are the skew-normalMoE and the robust $t$ MoE and skew $t$ MoE, respectively named SNMoE, TMoE andSTMoE. We develop dedicated expectation-maximization (EM) and expectationconditional maximization (ECM) algorithms to estimate the parameters of theproposed models by monotonically maximizing the observed data log-likelihood.We describe how the presented models can be used in prediction and inmodel-based clustering of regression data. Numerical experiments carried out onsimulated data show the effectiveness and the robustness of the proposed modelsin terms modeling non-linear regression functions as well as in model-basedclustering. Then, to show their usefulness for practical applications, theproposed models are applied to the real-world data of tone perception formusical data analysis, and the one of temperature anomalies for the analysis ofclimate change data.
arxiv-11400-275 | Pedestrian Detection with Spatially Pooled Features and Structured Ensemble Learning | http://arxiv.org/pdf/1409.5209v3.pdf | author:Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel category:cs.CV cs.LG published:2014-09-18 summary:Many typical applications of object detection operate within a prescribedfalse-positive range. In this situation the performance of a detector should beassessed on the basis of the area under the ROC curve over that range, ratherthan over the full curve, as the performance outside the range is irrelevant.This measure is labelled as the partial area under the ROC curve (pAUC). Wepropose a novel ensemble learning method which achieves a maximal detectionrate at a user-defined range of false positive rates by directly optimizing thepartial AUC using structured learning. In order to achieve a high object detection performance, we propose a newapproach to extract low-level visual features based on spatial pooling.Incorporating spatial pooling improves the translational invariance and thusthe robustness of the detection process. Experimental results on both syntheticand real-world data sets demonstrate the effectiveness of our approach, and weshow that it is possible to train state-of-the-art pedestrian detectors usingthe proposed structured ensemble learning method with spatially pooledfeatures. The result is the current best reported performance on theCaltech-USA pedestrian detection dataset.
arxiv-11400-276 | Overlapping Communities Detection via Measure Space Embedding | http://arxiv.org/pdf/1504.06796v2.pdf | author:Mark Kozdoba, Shie Mannor category:cs.LG cs.SI stat.ML published:2015-04-26 summary:We present a new algorithm for community detection. The algorithm uses randomwalks to embed the graph in a space of measures, after which a modification of$k$-means in that space is applied. The algorithm is therefore fast and easilyparallelizable. We evaluate the algorithm on standard random graph benchmarks,including some overlapping community benchmarks, and find its performance to bebetter or at least as good as previously known algorithms. We also prove alinear time (in number of edges) guarantee for the algorithm on a$p,q$-stochastic block model with $p \geq c\cdot N^{-\frac{1}{2} + \epsilon}$and $p-q \geq c' \sqrt{p N^{-\frac{1}{2} + \epsilon} \log N}$.
arxiv-11400-277 | Simultaneously Solving Computational Problems Using an Artificial Chemical Reactor | http://arxiv.org/pdf/1506.08361v1.pdf | author:Jaderick P. Pabico category:cs.ET cs.NE published:2015-06-28 summary:This paper is centered on using chemical reaction as a computational metaphorfor simultaneously solving problems. An artificial chemical reactor that cansimultaneously solve instances of three unrelated problems was created. Thereactor is a distributed stochastic algorithm that simulates a chemicaluniverse wherein the molecular species are being represented either by a humangenomic contig panel, a Hamiltonian cycle, or an aircraft landing schedule. Thechemical universe is governed by reactions that can alter genomic sequences,re-order Hamiltonian cycles, or reschedule an aircraft landing program.Molecular masses were considered as measures of goodness of solutions, andrepresented radiation hybrid (RH) vector similarities, costs of Hamiltoniancycles, and penalty costs for landing an aircraft before and after targetlanding times. This method, tested by solving in tandem with deterministicalgorithms, has been shown to find quality solutions in finding the minima RHvector similarities of genomic data, minima costs in Hamiltonian cycles of thetraveling salesman, and minima costs for landing aircrafts before or aftertarget landing times.
arxiv-11400-278 | Patch-Based Low-Rank Minimization for Image Denoising | http://arxiv.org/pdf/1506.08353v1.pdf | author:Haijuan Hu, Jacques Froment, Quansheng Liu category:cs.CV published:2015-06-28 summary:Patch-based sparse representation and low-rank approximation for imageprocessing attract much attention in recent years. The minimization of thematrix rank coupled with the Frobenius norm data fidelity can be solved by thehard thresholding filter with principle component analysis (PCA) or singularvalue decomposition (SVD). Based on this idea, we propose a patch-basedlow-rank minimization method for image denoising, which learns compactdictionaries from similar patches with PCA or SVD, and applies simple hardthresholding filters to shrink the representation coefficients. Compared torecent patch-based sparse representation methods, experiments demonstrate thatthe proposed method is not only rather rapid, but also effective for a varietyof natural images, especially for texture parts in images.
arxiv-11400-279 | Improved Deep Speaker Feature Learning for Text-Dependent Speaker Recognition | http://arxiv.org/pdf/1506.08349v1.pdf | author:Lantian Li, Yiye Lin, Zhiyong Zhang, Dong Wang category:cs.CL cs.LG cs.NE published:2015-06-28 summary:A deep learning approach has been proposed recently to derive speakeridentifies (d-vector) by a deep neural network (DNN). This approach has beenapplied to text-dependent speaker recognition tasks and shows reasonableperformance gains when combined with the conventional i-vector approach.Although promising, the existing d-vector implementation still can not competewith the i-vector baseline. This paper presents two improvements for the deeplearning approach: a phonedependent DNN structure to normalize phone variation,and a new scoring approach based on dynamic time warping (DTW). Experiments ona text-dependent speaker recognition task demonstrated that the proposedmethods can provide considerable performance improvement over the existingd-vector implementation.
arxiv-11400-280 | Occlusion Coherence: Detecting and Localizing Occluded Faces | http://arxiv.org/pdf/1506.08347v1.pdf | author:Golnaz Ghiasi, Charless C. Fowlkes category:cs.CV published:2015-06-28 summary:The presence of occluders significantly impacts object recognition accuracy.However, occlusion is typically treated as an unstructured source of noise andexplicit models for occluders have lagged behind those for object appearanceand shape. In this paper we describe a hierarchical deformable part model forface detection and keypoint localization that explicitly models part occlusion.The proposed model structure makes it possible to augment positive trainingdata with large numbers of synthetically occluded instances. This allows us toeasily incorporate the statistics of occlusion patterns in a discriminativelytrained model. We test the model on several benchmarks for keypointlocalization and detection including challenging data sets featuringsignificant occlusion. We find that the addition of an explicit model ofocclusion yields a system that outperforms existing approaches in keypointlocalization accuracy and detection performance.
arxiv-11400-281 | A Novel Feature Selection Approach for Analyzing High dimensional Functional MRI Data | http://arxiv.org/pdf/1506.08301v1.pdf | author:Zhiqiang Li, Yilun Wang, Yifeng Wang, Xiaona Wang, Junjie Zheng, Huafu Chen category:cs.CV cs.LG stat.ML I.5.2 published:2015-06-27 summary:Feature selection based on traditional multivariate methods is likely toobtain unstable and unreliable results in case of an extremely high dimensionalspace and very limited training samples. In order to overcome this difficulty,we introduced a novel feature selection method which combines the idea ofstability selection approach and the elastic net approach to detectdiscriminative features in a stable and robust way. This new method is appliedto functional magnetic resonance imaging (fMRI) data, whose discriminativefeatures are often correlated or redundant. Compared with the originalstability selection approach with the pure l_1 -norm regularized model servingas the baseline model, the proposed method achieves a better sensitivityempirically, because elastic net encourages a grouping effect besides sparsity.Compared with the feature selection method based on the plain Elastic Net, ourmethod achieves the finite sample control for certain error rates of falsediscoveries, transparent principle for choosing a proper amount ofregularization and the robustness of the feature selection results, due to theincorporation of the stability selection idea. A simulation study showed thatour approach are less influenced than other methods by label noise. Inaddition, the advantage in terms of better control of false discoveries andmissed discoveries of our approach was verified in a real fMRI experiment.Finally, a multi-center resting-state fMRI data about Attention-deficit/hyperactivity disorder (ADHD) suggested that the resulted classifier based onour feature selection method achieves the best and most robust predictionaccuracy.
arxiv-11400-282 | Occam's Gates | http://arxiv.org/pdf/1506.08251v1.pdf | author:Jonathan Raiman, Szymon Sidor category:cs.LG published:2015-06-27 summary:We present a complimentary objective for training recurrent neural networks(RNN) with gating units that helps with regularization and interpretability ofthe trained model. Attention-based RNN models have shown success in manydifficult sequence to sequence classification problems with long and short termdependencies, however these models are prone to overfitting. In this paper, wedescribe how to regularize these models through an L1 penalty on the activationof the gating units, and show that this technique reduces overfitting on avariety of tasks while also providing to us a human-interpretable visualizationof the inputs used by the network. These tasks include sentiment analysis,paraphrase recognition, and question answering.
arxiv-11400-283 | Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation | http://arxiv.org/pdf/1412.1454v2.pdf | author:Noam Shazeer, Joris Pelemans, Ciprian Chelba category:cs.LG cs.CL published:2014-12-03 summary:We present a novel family of language model (LM) estimation techniques namedSparse Non-negative Matrix (SNM) estimation. A first set of experimentsempirically evaluating it on the One Billion Word Benchmark shows that SNM$n$-gram LMs perform almost as well as the well-established Kneser-Ney (KN)models. When using skip-gram features the models are able to match thestate-of-the-art recurrent neural network (RNN) LMs; combining the two modelingtechniques yields the best known result on the benchmark. The computationaladvantages of SNM over both maximum entropy and RNN LM estimation are probablyits main strength, promising an approach that has the same flexibility incombining arbitrary features effectively and yet should scale to very largeamounts of data as gracefully as $n$-gram LMs do.
arxiv-11400-284 | A geometric alternative to Nesterov's accelerated gradient descent | http://arxiv.org/pdf/1506.08187v1.pdf | author:SÃ©bastien Bubeck, Yin Tat Lee, Mohit Singh category:math.OC cs.DS cs.LG cs.NA published:2015-06-26 summary:We propose a new method for unconstrained optimization of a smooth andstrongly convex function, which attains the optimal rate of convergence ofNesterov's accelerated gradient descent. The new algorithm has a simplegeometric interpretation, loosely inspired by the ellipsoid method. We providesome numerical evidence that the new method can be superior to Nesterov'saccelerated gradient descent.
arxiv-11400-285 | An Empirical Study of Stochastic Variational Algorithms for the Beta Bernoulli Process | http://arxiv.org/pdf/1506.08180v1.pdf | author:Amar Shah, David A. Knowles, Zoubin Ghahramani category:stat.ML cs.LG stat.AP stat.CO stat.ME published:2015-06-26 summary:Stochastic variational inference (SVI) is emerging as the most promisingcandidate for scaling inference in Bayesian probabilistic models to largedatasets. However, the performance of these methods has been assessed primarilyin the context of Bayesian topic models, particularly latent Dirichletallocation (LDA). Deriving several new algorithms, and using synthetic, imageand genomic datasets, we investigate whether the understanding gleaned from LDAapplies in the setting of sparse latent factor models, specifically betaprocess factor analysis (BPFA). We demonstrate that the big picture isconsistent: using Gibbs sampling within SVI to maintain certain posteriordependencies is extremely effective. However, we find that different posteriordependencies are important in BPFA relative to LDA. Particularly,approximations able to model intra-local variable dependence perform best.
arxiv-11400-286 | Online Matrix Completion and Online Robust PCA | http://arxiv.org/pdf/1503.03525v2.pdf | author:Brian Lois, Namrata Vaswani category:cs.IT math.IT stat.ML published:2015-03-11 summary:This work studies two interrelated problems - online robust PCA (RPCA) andonline low-rank matrix completion (MC). In recent work by Cand\`{e}s et al.,RPCA has been defined as a problem of separating a low-rank matrix (true data),$L:=[\ell_1, \ell_2, \dots \ell_{t}, \dots , \ell_{t_{\max}}]$ and a sparsematrix (outliers), $S:=[x_1, x_2, \dots x_{t}, \dots, x_{t_{\max}}]$ from theirsum, $M:=L+S$. Our work uses this definition of RPCA. An important applicationwhere both these problems occur is in video analytics in trying to separatesparse foregrounds (e.g., moving objects) and slowly changing backgrounds. While there has been a large amount of recent work on both developing andanalyzing batch RPCA and batch MC algorithms, the online problem is largelyopen. In this work, we develop a practical modification of our recentlyproposed algorithm to solve both the online RPCA and online MC problems. Themain contribution of this work is that we obtain correctness results for theproposed algorithms under mild assumptions. The assumptions that we need are:(a) a good estimate of the initial subspace is available (easy to obtain usinga short sequence of background-only frames in video surveillance); (b) the$\ell_t$'s obey a `slow subspace change' assumption; (c) the basis vectors forthe subspace from which $\ell_t$ is generated are dense (non-sparse); (d) thesupport of $x_t$ changes by at least a certain amount at least every so often;and (e) algorithm parameters are appropriately set
arxiv-11400-287 | Finding Linear Structure in Large Datasets with Scalable Canonical Correlation Analysis | http://arxiv.org/pdf/1506.08170v1.pdf | author:Zhuang Ma, Yichao Lu, Dean Foster category:stat.ML stat.CO published:2015-06-26 summary:Canonical Correlation Analysis (CCA) is a widely used spectral technique forfinding correlation structures in multi-view datasets. In this paper, we tacklethe problem of large scale CCA, where classical algorithms, usually requiringcomputing the product of two huge matrices and huge matrix decomposition, arecomputationally and storage expensive. We recast CCA from a novel perspectiveand propose a scalable and memory efficient Augmented Approximate Gradient(AppGrad) scheme for finding top $k$ dimensional canonical subspace which onlyinvolves large matrix multiplying a thin matrix of width $k$ and small matrixdecomposition of dimension $k\times k$. Further, AppGrad achieves optimalstorage complexity $O(k(p_1+p_2))$, compared with classical algorithms whichusually require $O(p_1^2+p_2^2)$ space to store two dense whitening matrices.The proposed scheme naturally generalizes to stochastic optimization regime,especially efficient for huge datasets where batch algorithms are prohibitive.The online property of stochastic AppGrad is also well suited to the streamingscenario, where data comes sequentially. To the best of our knowledge, it isthe first stochastic algorithm for CCA. Experiments on four real data sets areprovided to show the effectiveness of the proposed methods.
arxiv-11400-288 | An $O(n\log(n))$ Algorithm for Projecting Onto the Ordered Weighted $\ell_1$ Norm Ball | http://arxiv.org/pdf/1505.00870v3.pdf | author:Damek Davis category:math.OC cs.LG published:2015-05-05 summary:The ordered weighted $\ell_1$ (OWL) norm is a newly developed generalizationof the Octogonal Shrinkage and Clustering Algorithm for Regression (OSCAR)norm. This norm has desirable statistical properties and can be used to performsimultaneous clustering and regression. In this paper, we show how to computethe projection of an $n$-dimensional vector onto the OWL norm ball in$O(n\log(n))$ operations. In addition, we illustrate the performance of ouralgorithm on a synthetic regression test.
arxiv-11400-289 | Humor in Collective Discourse: Unsupervised Funniness Detection in the New Yorker Cartoon Caption Contest | http://arxiv.org/pdf/1506.08126v1.pdf | author:Dragomir Radev, Amanda Stent, Joel Tetreault, Aasish Pappu, Aikaterini Iliakopoulou, Agustin Chanfreau, Paloma de Juan, Jordi Vallmitjana, Alejandro Jaimes, Rahul Jha, Bob Mankoff category:cs.CL cs.AI cs.MM stat.ML published:2015-06-26 summary:The New Yorker publishes a weekly captionless cartoon. More than 5,000readers submit captions for it. The editors select three of them and ask thereaders to pick the funniest one. We describe an experiment that compares adozen automatic methods for selecting the funniest caption. We show thatnegative sentiment, human-centeredness, and lexical centrality most stronglymatch the funniest captions, followed by positive sentiment. These results areuseful for understanding humor and also in the design of more engagingconversational agents in text and multimodal (vision+text) systems. As part ofthis work, a large set of cartoons and captions is being made available to thecommunity.
arxiv-11400-290 | Modelling of directional data using Kent distributions | http://arxiv.org/pdf/1506.08105v1.pdf | author:Parthan Kasarapu category:cs.LG stat.ML published:2015-06-26 summary:The modelling of data on a spherical surface requires the consideration ofdirectional probability distributions. To model asymmetrically distributed dataon a three-dimensional sphere, Kent distributions are often used. The momentestimates of the parameters are typically used in modelling tasks involvingKent distributions. However, these lack a rigorous statistical treatment. Thefocus of the paper is to introduce a Bayesian estimation of the parameters ofthe Kent distribution which has not been carried out in the literature, partlybecause of its complex mathematical form. We employ the Bayesianinformation-theoretic paradigm of Minimum Message Length (MML) to bridge thisgap and derive reliable estimators. The inferred parameters are subsequentlyused in mixture modelling of Kent distributions. The problem of inferring thesuitable number of mixture components is also addressed using the MMLcriterion. We demonstrate the superior performance of the derived MML-basedparameter estimates against the traditional estimators. We apply the MMLprinciple to infer mixtures of Kent distributions to model empirical datacorresponding to protein conformations. We demonstrate the effectiveness ofKent models to act as improved descriptors of protein structural data ascompared to commonly used von Mises-Fisher distributions.
arxiv-11400-291 | Average Distance Queries through Weighted Samples in Graphs and Metric Spaces: High Scalability with Tight Statistical Guarantees | http://arxiv.org/pdf/1503.08528v6.pdf | author:Shiri Chechik, Edith Cohen, Haim Kaplan category:cs.SI cs.LG published:2015-03-30 summary:The average distance from a node to all other nodes in a graph, or from aquery point in a metric space to a set of points, is a fundamental quantity indata analysis. The inverse of the average distance, known as the (classic)closeness centrality of a node, is a popular importance measure in the study ofsocial networks. We develop novel structural insights on the sparsifiability ofthe distance relation via weighted sampling. Based on that, we present highlypractical algorithms with strong statistical guarantees for fundamentalproblems. We show that the average distance (and hence the centrality) for allnodes in a graph can be estimated using $O(\epsilon^{-2})$ single-sourcedistance computations. For a set $V$ of $n$ points in a metric space, we showthat after preprocessing which uses $O(n)$ distance computations we can computea weighted sample $S\subset V$ of size $O(\epsilon^{-2})$ such that the averagedistance from any query point $v$ to $V$ can be estimated from the distancesfrom $v$ to $S$. Finally, we show that for a set of points $V$ in a metricspace, we can estimate the average pairwise distance using $O(n+\epsilon^{-2})$distance computations. The estimate is based on a weighted sample of$O(\epsilon^{-2})$ pairs of points, which is computed using $O(n)$ distancecomputations. Our estimates are unbiased with normalized mean square error(NRMSE) of at most $\epsilon$. Increasing the sample size by a $O(\log n)$factor ensures that the probability that the relative error exceeds $\epsilon$is polynomially small.
arxiv-11400-292 | Spectral Collaborative Representation based Classification for Hand Gestures recognition on Electromyography Signals | http://arxiv.org/pdf/1506.08006v1.pdf | author:Ali Boyali category:cs.CV published:2015-06-26 summary:In this study, we introduce a novel variant and application of theCollaborative Representation based Classification in spectral domain forrecognition of the hand gestures using the raw surface Electromyographysignals. The intuitive use of spectral features are explained via circulantmatrices. The proposed Spectral Collaborative Representation basedClassification (SCRC) is able to recognize gestures with higher levels ofaccuracy for a fairly rich gesture set. The worst recognition result which isthe best in the literature is obtained as 97.3\% among the four sets of theexperiments for each hand gestures. The recognition results are reported with asubstantial number of experiments and labeling computation.
arxiv-11400-293 | ASOC: An Adaptive Parameter-free Stochastic Optimization Techinique for Continuous Variables | http://arxiv.org/pdf/1506.08004v1.pdf | author:Jayanta Basak category:cs.NE published:2015-06-26 summary:Stochastic optimization is an important task in many optimization problemswhere the tasks are not expressible as convex optimization problems. In thecase of non-convex optimization problems, various different stochasticalgorithms like simulated annealing, evolutionary algorithms, and tabu searchare available. Most of these algorithms require user-defined parametersspecific to the problem in order to find out the optimal solution. Moreover, inmany situations, iterative fine-tunings are required for the user-definedparameters, and therefore these algorithms cannot adapt if the search space andthe optima changes over time. In this paper we propose an \underline{a}daptiveparameter-free \underline{s}tochastic \underline{o}ptimization technique for\underline{c}ontinuous random variables called ASOC.
arxiv-11400-294 | Safe Feature Pruning for Sparse High-Order Interaction Models | http://arxiv.org/pdf/1506.08002v1.pdf | author:Kazuya Nakagawa, Shinya Suzumura, Masayuki Karasuyama, Koji Tsuda, Ichiro Takeuchi category:stat.ML published:2015-06-26 summary:Taking into account high-order interactions among covariates is valuable inmany practical regression problems. This is, however, computationallychallenging task because the number of high-order interaction features to beconsidered would be extremely large unless the number of covariates issufficiently small. In this paper, we propose a novel efficient algorithm forLASSO-based sparse learning of such high-order interaction models. Our basicstrategy for reducing the number of features is to employ the idea of recentlyproposed safe feature screening (SFS) rule. An SFS rule has a property that, ifa feature satisfies the rule, then the feature is guaranteed to be non-activein the LASSO solution, meaning that it can be safely screened-out prior to theLASSO training process. If a large number of features can be screened-outbefore training the LASSO, the computational cost and the memory requirment canbe dramatically reduced. However, applying such an SFS rule to each of theextremely large number of high-order interaction features would becomputationally infeasible. Our key idea for solving this computational issueis to exploit the underlying tree structure among high-order interactionfeatures. Specifically, we introduce a pruning condition called safe featurepruning (SFP) rule which has a property that, if the rule is satisfied in acertain node of the tree, then all the high-order interaction featurescorresponding to its descendant nodes can be guaranteed to be non-active at theoptimal solution. Our algorithm is extremely efficient, making it possible towork, e.g., with 3rd order interactions of 10,000 original covariates, wherethe number of possible high-order interaction features is greater than 10^{12}.
arxiv-11400-295 | An Efficient Post-Selection Inference on High-Order Interaction Models | http://arxiv.org/pdf/1506.07997v1.pdf | author:S. Suzumura, K. Nakagawa, K. Tsuda, I. Takeuchi category:stat.ML published:2015-06-26 summary:Finding statistically significant high-order interaction features inpredictive modeling is important but challenging task. The difficulty lies inthe fact that, for a recent applications with high-dimensional covariates, thenumber of possible high-order interaction features would be extremely large.Identifying statistically significant features from such a huge pool ofcandidates would be highly challenging both in computational and statisticalsenses. To work with this problem, we consider a two stage algorithm where wefirst select a set of high-order interaction features by marginal screening,and then make statistical inferences on the regression model fitted only withthe selected features. Such statistical inferences are called post-selectioninference (PSI), and receiving an increasing attention in the literature. Oneof the seminal recent advancements in PSI literature is the works by Lee et al.where the authors presented an algorithmic framework for computing exactsampling distributions in PSI. A main challenge when applying their approach toour high-order interaction models is to cope with the fact that PSI in generaldepends not only on the selected features but also on the unselected features,making it hard to apply to our extremely high-dimensional high-orderinteraction models. The goal of this paper is to overcome this difficulty byintroducing a novel efficient method for PSI. Our key idea is to exploit theunderlying tree structure among high-order interaction features, and to developa pruning method of the tree which enables us to quickly identify a group ofunselected features that are guaranteed to have no influence on PSI. Theexperimental results indicate that the proposed method allows us to reliablyidentify statistically significant high-order interaction features withreasonable computational cost.
arxiv-11400-296 | Java Implementation of a Parameter-less Evolutionary Portfolio | http://arxiv.org/pdf/1506.08867v1.pdf | author:JosÃ© C. Pereira, Fernando G. Lobo category:cs.MS cs.NE I.2.8 published:2015-06-26 summary:The Java implementation of a portfolio of parameter-less evolutionaryalgorithms is presented. The Parameter-less Evolutionary Portfolio implements aheuristic that performs adaptive selection of parameter-less evolutionaryalgorithms in accordance with performance criteria that are measured duringrunning time. At present time, the portfolio includes three parameter-lessevolutionary algorithms: Parameter-less Univariate Marginal DistributionAlgorithm, Parameter-less Extended Compact Genetic Algorithm, andParameter-less Hierarchical Bayesian Optimization Algorithm. Initialexperiments showed that the parameter-less portfolio can solve various classesof problems without the need for any prior parameter setting technique and withan increase in computational effort that can be considered acceptable.
arxiv-11400-297 | A Java Implementation of Parameter-less Evolutionary Algorithms | http://arxiv.org/pdf/1506.08694v1.pdf | author:JosÃ© C. Pereira, Fernando G. Lobo category:cs.MS cs.NE I.2.8 published:2015-06-26 summary:The Parameter-less Genetic Algorithm was first presented by Harik and Lobo in1999 as an alternative to the usual trial-and-error method of finding, for eachgiven problem, an acceptable set-up of the parameter values of the geneticalgorithm. Since then, the same strategy has been successfully applied tocreate parameter-less versions of other population-based search algorithms suchas the Extended Compact Genetic Algorithm and the Hierarchical BayesianOptimization Algorithm. This report describes a Java implementation,Parameter-less Evolutionary Algorithm (P-EAJava), that integrates severalparameter-less evolutionary algorithms into a single platform. Along with abrief description of P-EAJava, we also provide detailed instructions on how touse it, how to implement new problems, and how to generate new parameter-lessversions of evolutionary algorithms. At present time, P-EAJava already includes parameter-less versions of theSimple Genetic Algorithm, the Extended Compact Genetic Algorithm, theUnivariate Marginal Distribution Algorithm, and the Hierarchical BayesianOptimization Algorithm. The source and binary files of the Java implementationof P-EAJava are available for free download athttps://github.com/JoseCPereira/2015ParameterlessEvolutionaryAlgorithmsJava.
arxiv-11400-298 | A Java Implementation of the SGA, UMDA, ECGA, and HBOA | http://arxiv.org/pdf/1506.07980v1.pdf | author:JosÃ© C. Pereira, Fernando G. Lobo category:cs.NE cs.MS I.2.8 published:2015-06-26 summary:The Simple Genetic Algorithm, the Univariate Marginal Distribution Algorithm,the Extended Compact Genetic Algorithm, and the Hierarchical BayesianOptimization Algorithm are all well known Evolutionary Algorithms. In this report we present a Java implementation of these four algorithms withdetailed instructions on how to use each of them to solve a given set ofoptimization problems. Additionally, it is explained how to implement andintegrate new problems within the provided set. The source and binary files ofthe Java implementations are available for free download athttps://github.com/JoseCPereira/2015EvolutionaryAlgorithmsJava.
arxiv-11400-299 | Mining Mid-level Visual Patterns with Deep CNN Activations | http://arxiv.org/pdf/1506.06343v2.pdf | author:Yao Li, Lingqiao Liu, Chunhua Shen, Anton van den Hengel category:cs.CV published:2015-06-21 summary:The purpose of mid-level visual element discovery is to find clusters ofimage patches that are both representative and discriminative. Here we studythis problem from the prospective of pattern mining while relying on therecently popularized Convolutional Neural Networks (CNNs). We observe that afully-connected CNN activation extracted from an image patch typicallypossesses two appealing properties that enable its seamless integration withpattern mining techniques. The marriage between CNN activations and associationrule mining, a well-known pattern mining technique in the literature, leads tofast and effective discovery of representative and discriminative patterns froma huge number of image patches. When we retrieve and visualize image patcheswith the same pattern, surprisingly, they are not only visually similar butalso semantically consistent, and thus give rise to a mid-level visual elementin our work. Given the patterns and retrieved mid-level visual elements, wepropose two methods to generate image feature representations for each. Thefirst method is to use the patterns as codewords in a dictionary, similar tothe Bag-of-Visual-Words model, we compute a Bag-of-Patterns representation. Thesecond one relies on the retrieved mid-level visual elements to construct aBag-of-Elements representation. We evaluate the two encoding methods on sceneand object classification tasks, and demonstrate that our approach outperformsor matches recent works using CNN activations for these tasks.
arxiv-11400-300 | Online Matrix Factorization via Broyden Updates | http://arxiv.org/pdf/1506.04389v2.pdf | author:Ãmer Deniz AkyÄ±ldÄ±z category:stat.ML published:2015-06-14 summary:In this paper, we propose an online algorithm to compute matrixfactorizations. Proposed algorithm updates the dictionary matrix and associatedcoefficients using a single observation at each time. The algorithm performslow-rank updates to dictionary matrix. We derive the algorithm by defining asimple objective function to minimize whenever an observation is arrived. Weextend the algorithm further for handling missing data. We also provide amini-batch extension which enables to compute the matrix factorization on bigdatasets. We demonstrate the efficiency of our algorithm on a real dataset andgive comparisons with well-known algorithms such as stochastic gradient matrixfactorization and nonnegative matrix factorization (NMF).
