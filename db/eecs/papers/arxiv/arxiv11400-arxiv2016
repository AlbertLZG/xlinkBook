arxiv-11400-1 | DopeLearning: A Computational Approach to Rap Lyrics Generation | http://arxiv.org/abs/1505.04771 | author:Eric Malmi, Pyry Takala, Hannu Toivonen, Tapani Raiko, Aristides Gionis category:cs.LG cs.AI cs.CL cs.NE I.2.7; H.3.3 published:2015-05-18 summary:Writing rap lyrics requires both creativity, to construct a meaningful and aninteresting story, and lyrical skills, to produce complex rhyme patterns, whichare the cornerstone of a good flow. We present a method for capturing both ofthese aspects. Our approach is based on two machine-learning techniques: theRankSVM algorithm, and a deep neural network model with a novel structure. Forthe problem of distinguishing the real next line from a randomly selected one,we achieve an 82 % accuracy. We employ the resulting prediction method forcreating new rap lyrics by combining lines from existing songs. In terms ofquantitative rhyme density, the produced lyrics outperform best human rappersby 21 %. The results highlight the benefit of our rhyme density metric and ourinnovative predictor of next lines.
arxiv-11400-2 | Joint Representation Classification for Collective Face Recognition | http://arxiv.org/abs/1505.04617 | author:Liping Wang, Songcan Chen category:cs.CV math.OC published:2015-05-18 summary:Sparse representation based classification (SRC) is popularly used in manyapplications such as face recognition, and implemented in two steps:representation coding and classification. For a given set of testing images,SRC codes every image over the base images as a sparse representation thenclassifies it to the class with the least representation error. This schemeutilizes an individual representation rather than the collective one toclassify such a set of images, doing so obviously ignores the correlation amongthe given images. In this paper, a joint representation classification (JRC)for collective face recognition is proposed. JRC takes the correlation ofmultiple images as well as a single representation into account. Under theassumption that the given face images are generally related to each other, JRCcodes all the testing images over the base images simultaneously to facilitaterecognition. To this end, the testing inputs are aligned into a matrix and thejoint representation coding is formulated to a generalized$l_{2,q}-l_{2,p}$-minimization problem. To uniformly solve the inducedoptimization problems for any $q\in[1,2]$ and $p\in (0,2]$, an iterativequadratic method (IQM) is developed. IQM is proved to be a strict descentalgorithm with convergence to the optimal solution. Moreover, a more practicalIQM is proposed for large-scale case. Experimental results on three publicdatabases show that the JRC with practical IQM no only saves much computationalcost but also achieves better performance in collective face recognition thanthe state-of-the-arts.
arxiv-11400-3 | Fractally-organized Connectionist Networks: Conjectures and Preliminary Results | http://arxiv.org/abs/1505.04618 | author:Vincenzo De Florio category:cs.NE published:2015-05-18 summary:A strict interpretation of connectionism mandates complex networks of simplecomponents. The question here is, is this simplicity to be interpreted inabsolute terms? I conjecture that absolute simplicity might not be an essentialattribute of connectionism, and that it may be effectively exchanged with arequirement for relative simplicity, namely simplicity with respect to thecurrent organizational level. In this paper I provide some elements to theanalysis of the above question. In particular I conjecture that fractallyorganized connectionist networks may provide a convenient means to achive whatLeibniz calls an "art of complication", namely an effective way to encapsulatecomplexity and practically extend the applicability of connectionism to domainssuch as sociotechnical system modeling and design. Preliminary evidence to myclaim is brought by considering the design of the software architecturedesigned for the telemonitoring service of Flemish project "Little Sister".
arxiv-11400-4 | An Asynchronous Mini-Batch Algorithm for Regularized Stochastic Optimization | http://arxiv.org/abs/1505.04824 | author:Hamid Reza Feyzmahdavian, Arda Aytekin, Mikael Johansson category:math.OC cs.SY stat.ML published:2015-05-18 summary:Mini-batch optimization has proven to be a powerful paradigm for large-scalelearning. However, the state of the art parallel mini-batch algorithms assumesynchronous operation or cyclic update orders. When worker nodes areheterogeneous (due to different computational capabilities or differentcommunication delays), synchronous and cyclic operations are inefficient sincethey will leave workers idle waiting for the slower nodes to complete theircomputations. In this paper, we propose an asynchronous mini-batch algorithmfor regularized stochastic optimization problems with smooth loss functionsthat eliminates idle waiting and allows workers to run at their maximal updaterates. We show that by suitably choosing the step-size values, the algorithmachieves a rate of the order $O(1/\sqrt{T})$ for general convex regularizationfunctions, and the rate $O(1/T)$ for strongly convex regularization functions,where $T$ is the number of iterations. In both cases, the impact of asynchronyon the convergence rate of our algorithm is asymptotically negligible, and anear-linear speedup in the number of workers can be expected. Theoreticalresults are confirmed in real implementations on a distributed computinginfrastructure.
arxiv-11400-5 | Hinge-Loss Markov Random Fields and Probabilistic Soft Logic | http://arxiv.org/abs/1505.04406 | author:Stephen H. Bach, Matthias Broecheler, Bert Huang, Lise Getoor category:cs.LG cs.AI stat.ML published:2015-05-17 summary:A fundamental challenge in developing high-impact machine learningtechnologies is balancing the ability to model rich, structured domains withthe ability to scale to big data. Many important problem areas are both richlystructured and large scale, from social and biological networks, to knowledgegraphs and the Web, to images, video, and natural language. In this paper, weintroduce two new formalisms for modeling structured data, distinguished fromprevious approaches by their ability to both capture rich structure and scaleto big data. The first, hinge-loss Markov random fields (HL-MRFs), is a newkind of probabilistic graphical model that generalizes different approaches toconvex inference. We unite three approaches from the randomized algorithms,probabilistic graphical models, and fuzzy logic communities, showing that allthree lead to the same inference objective. We then derive HL-MRFs bygeneralizing this unified objective. The second new formalism, probabilisticsoft logic (PSL), is a probabilistic programming language that makes HL-MRFseasy to define using a syntax based on first-order logic. We next introduce analgorithm for inferring most-probable variable assignments (MAP inference) thatis much more scalable than general-purpose convex optimization software,because it uses message passing to take advantage of sparse dependencystructures. We then show how to learn the parameters of HL-MRFs. The learnedHL-MRFs are as accurate as analogous discrete models, but much more scalable.Together, these algorithms enable HL-MRFs and PSL to model rich, structureddata at scales not previously possible.
arxiv-11400-6 | CCG Parsing and Multiword Expressions | http://arxiv.org/abs/1505.04420 | author:Miryam de Lhoneux category:cs.CL published:2015-05-17 summary:This thesis presents a study about the integration of information aboutMultiword Expressions (MWEs) into parsing with Combinatory Categorial Grammar(CCG). We build on previous work which has shown the benefit of addinginformation about MWEs to syntactic parsing by implementing a similar pipelinewith CCG parsing. More specifically, we collapse MWEs to one token in trainingand test data in CCGbank, a corpus which contains sentences annotated with CCGderivations. Our collapsing algorithm however can only deal with MWEs when theyform a constituent in the data which is one of the limitations of our approach. We study the effect of collapsing training and test data. A parsing effectcan be obtained if collapsed data help the parser in its decisions and atraining effect can be obtained if training on the collapsed data improvesresults. We also collapse the gold standard and show that our modelsignificantly outperforms the baseline model on our gold standard, whichindicates that there is a training effect. We show that the baseline modelperforms significantly better on our gold standard when the data are collapsedbefore parsing than when the data are collapsed after parsing which indicatesthat there is a parsing effect. We show that these results can lead to improvedperformance on the non-collapsed standard benchmark although we fail to showthat it does so significantly. We conclude that despite the limited settings,there are noticeable improvements from using MWEs in parsing. We discuss waysin which the incorporation of MWEs into parsing can be improved and hypothesizethat this will lead to more substantial results. We finally show that turning the MWE recognition part of the pipeline into anexperimental part is a useful thing to do as we obtain different results withdifferent recognizers.
arxiv-11400-7 | Visual Semantic Role Labeling | http://arxiv.org/abs/1505.04474 | author:Saurabh Gupta, Jitendra Malik category:cs.CV published:2015-05-17 summary:In this paper we introduce the problem of Visual Semantic Role Labeling:given an image we want to detect people doing actions and localize the objectsof interaction. Classical approaches to action recognition either study thetask of action classification at the image or video clip level or at bestproduce a bounding box around the person doing the action. We believe such anoutput is inadequate and a complete understanding can only come when we areable to associate objects in the scene to the different semantic roles of theaction. To enable progress towards this goal, we annotate a dataset of 16Kpeople instances in 10K images with actions they are doing and associateobjects in the scene with different semantic roles for each action. Finally, weprovide a set of baseline algorithms for this task and analyze error modesproviding directions for future work.
arxiv-11400-8 | Exploring Nearest Neighbor Approaches for Image Captioning | http://arxiv.org/abs/1505.04467 | author:Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C. Lawrence Zitnick category:cs.CV published:2015-05-17 summary:We explore a variety of nearest neighbor baseline approaches for imagecaptioning. These approaches find a set of nearest neighbor images in thetraining set from which a caption may be borrowed for the query image. Weselect a caption for the query image by finding the caption that bestrepresents the "consensus" of the set of candidate captions gathered from thenearest neighbor images. When measured by automatic evaluation metrics on theMS COCO caption evaluation server, these approaches perform as well as manyrecent approaches that generate novel captions. However, human studies showthat a method that generates novel captions is still preferred over the nearestneighbor approach.
arxiv-11400-9 | Evolving Spiking Networks with Variable Resistive Memories | http://arxiv.org/abs/1505.04357 | author:Gerard David Howard, Larry Bull, Ben de Lacy Costello, Andrew Adamatzky, Ella Gale category:cs.NE published:2015-05-17 summary:Neuromorphic computing is a brainlike information processing paradigm thatrequires adaptive learning mechanisms. A spiking neuro-evolutionary system isused for this purpose; plastic resistive memories are implemented as synapsesin spiking neural networks. The evolutionary design process exploits parameterself-adaptation and allows the topology and synaptic weights to be evolved foreach network in an autonomous manner. Variable resistive memories are the focusof this research; each synapse has its own conductance profile which modifiesthe plastic behaviour of the device and may be altered during evolution. Thesevariable resistive networks are evaluated on a noisy robotic dynamic-rewardscenario against two static resistive memories and a system containing standardconnections only. Results indicate that the extra behavioural degrees offreedom available to the networks incorporating variable resistive memoriesenable them to outperform the comparative synapse types.
arxiv-11400-10 | Sifting Robotic from Organic Text: A Natural Language Approach for Detecting Automation on Twitter | http://arxiv.org/abs/1505.04342 | author:Eric M. Clark, Jake Ryland Williams, Chris A. Jones, Richard A. Galbraith, Christopher M. Danforth, Peter Sheridan Dodds category:cs.CL published:2015-05-17 summary:Twitter, a popular social media outlet, has evolved into a vast source oflinguistic data, rich with opinion, sentiment, and discussion. Due to theincreasing popularity of Twitter, its perceived potential for exerting socialinfluence has led to the rise of a diverse community of automatons, commonlyreferred to as bots. These inorganic and semi-organic Twitter entities canrange from the benevolent (e.g., weather-update bots, help-wanted-alert bots)to the malevolent (e.g., spamming messages, advertisements, or radicalopinions). Existing detection algorithms typically leverage meta-data (timebetween tweets, number of followers, etc.) to identify robotic accounts. Here,we present a powerful classification scheme that exclusively uses the naturallanguage text from organic users to provide a criterion for identifyingaccounts posting automated messages. Since the classifier operates on textalone, it is flexible and may be applied to any textual data beyond theTwitter-sphere.
arxiv-11400-11 | Improved Microaneurysm Detection using Deep Neural Networks | http://arxiv.org/abs/1505.04424 | author:Mrinal Haloi category:cs.CV 68T45 published:2015-05-17 summary:In this work, we propose a novel microaneurysm (MA) detection for earlydieabetic ratinopathy screening using color fundus images. Since MA usually thefirst lesions to appear as a indicator of diabetic retinopathy, accuratedetection of MA is necessary for treatment. Each pixel of the image isclassified as either MA or non-MA using deep neural network with dropouttraining procedure using maxout activation function. No preprocessing step ormanual feature extraction is required. Substantial improvements over standardMA detection method based on pipeline of preprocessing, feature extraction,classification followed by postprocessing is achieved. The presented method isevaluated in publicly available Retinopathy Online Challenge (ROC) andDiaretdb1v2 database and achieved state-of-the-art accuracy.
arxiv-11400-12 | Salient Structure Detection by Context-Guided Visual Search | http://arxiv.org/abs/1505.04364 | author:Kai-Fu Yang, Hui Li, Chao-Yi Li, Yong-Jie Li category:cs.CV published:2015-05-17 summary:We define the task of salient structure (SS) detection to unify thesaliency-related tasks like fixation prediction, salient object detection, andother detection of structures of interest. In this study, we propose a unifiedframework for SS detection by modeling the two-pathway-based guided searchstrategy of biological vision. Firstly, context-based spatial prior (CBSP) isextracted based on the layout of edges in the given scene along a fast visualpathway, called non-selective pathway. This is a rough and non-selectiveestimation of the locations where the potential SSs present. Secondly, anotherflow of local feature extraction is executed in parallel along the selectivepathway. Finally, Bayesian inference is used to integrate local cues guided byCBSP, and to predict the exact locations of SSs in the input scene. Theproposed model is invariant to size and features of objects. Experimentalresults on four datasets (two fixation prediction datasets and two salientobject datasets) demonstrate that our system achieves competitive performancefor SS detection (i.e., both the tasks of fixation prediction and salientobject detection) comparing to the state-of-the-art methods.
arxiv-11400-13 | Robust Visual Knowledge Transfer via EDA | http://arxiv.org/abs/1505.04382 | author:Lei Zhang, David Zhang category:cs.CV published:2015-05-17 summary:We address the problem of visual knowledge adaptation by leveraging labeledpatterns from the source domain and a very limited number of labeled instancesin target domain to learn a robust classifier for visual categorization. Weintroduce a new semi-supervised cross-domain network learning framework,referred to as Extreme Domain Adaptation (EDA), that allows us tosimultaneously learn a category transformation and an extreme classifier byminimizing the L(2,1)-norm of the output weights and the learning error, inwhich the network output weights can be analytically determined. The unlabeledtarget data, as useful knowledge, is also learned as a fidelity term byminimizing the matching error between the extreme classifier and a baseclassifier to guarantee the stability during cross domain learning, into whichmany existing classifiers can be readily incorporated as base classifiers.Additionally, a manifold regularization with Laplacian graph is incorporatedinto EDA, such that it is beneficial to semi-supervised learning. Under theEDA, we also propose an extensive model learned with multiple views.Experiments on three visual data sets for video event recognition and objectrecognition, respectively, demonstrate that our EDA outperforms existingcross-domain learning methods.
arxiv-11400-14 | Harmonic Exponential Families on Manifolds | http://arxiv.org/abs/1505.04413 | author:Taco S. Cohen, Max Welling category:stat.ML published:2015-05-17 summary:In a range of fields including the geosciences, molecular biology, roboticsand computer vision, one encounters problems that involve random variables onmanifolds. Currently, there is a lack of flexible probabilistic models onmanifolds that are fast and easy to train. We define an extremely flexibleclass of exponential family distributions on manifolds such as the torus,sphere, and rotation groups, and show that for these distributions the gradientof the log-likelihood can be computed efficiently using a non-commutativegeneralization of the Fast Fourier Transform (FFT). We discuss applications toBayesian camera motion estimation (where harmonic exponential families serve asconjugate priors), and modelling of the spatial distribution of earthquakes onthe surface of the earth. Our experimental results show that harmonic densitiesyield a significantly higher likelihood than the best competing method, whilebeing orders of magnitude faster to train.
arxiv-11400-15 | Evolutionary Cost-sensitive Extreme Learning Machine and Subspace Extension | http://arxiv.org/abs/1505.04373 | author:Lei Zhang, David Zhang category:cs.CV published:2015-05-17 summary:Conventional extreme learning machines solve a Moore-Penrose generalizedinverse of hidden layer activated matrix and analytically determine the outputweights to achieve generalized performance, by assuming the same loss fromdifferent types of misclassification. The assumption may not hold incost-sensitive recognition tasks, such as face recognition based access controlsystem, where misclassifying a stranger as a family member and allowed to enterthe house may result in more serious disaster than misclassifying a familymember as a stranger and not allowed to enter. Though recent cost-sensitivelearning can reduce the total loss with a given cost matrix that quantifies howsevere one type of mistake against another type of mistake, in many realisticcases the cost matrix is unknown to users. Motivated by these concerns, thispaper proposes an evolutionary cost-sensitive extreme learning machine(ECSELM), with the following merits: 1) to our best knowledge, it is the firstproposal of cost-sensitive ELM; 2) it well addresses the open issue of how todefine the cost matrix in cost-sensitive learning tasks; 3) an evolutionarybacktracking search algorithm is induced for adaptive cost matrix optimization.Extensively, an ECSLDA method is generalized by coupling with cost-sensitivesubspace learning. Experiments in a variety of cost-sensitive tasks welldemonstrate the efficiency and effectiveness of the proposed approaches,specifically, 5%~10% improvements in classification are obtained on severaldatasets compared with ELMs; the computational efficiency is also 10 timesfaster than cost-sensitive subspace learning methods.
arxiv-11400-16 | Shrinkage degree in $L_2$-re-scale boosting for regression | http://arxiv.org/abs/1505.04369 | author:Lin Xu, Shaobo Lin, Yao Wang, Zongben Xu category:cs.LG published:2015-05-17 summary:Re-scale boosting (RBoosting) is a variant of boosting which can essentiallyimprove the generalization performance of boosting learning. The key feature ofRBoosting lies in introducing a shrinkage degree to re-scale the ensembleestimate in each gradient-descent step. Thus, the shrinkage degree determinesthe performance of RBoosting. The aim of this paper is to develop a concrete analysis concerning how todetermine the shrinkage degree in $L_2$-RBoosting. We propose two feasible waysto select the shrinkage degree. The first one is to parameterize the shrinkagedegree and the other one is to develope a data-driven approach of it. Afterrigorously analyzing the importance of the shrinkage degree in $L_2$-RBoostinglearning, we compare the pros and cons of the proposed methods. We find thatalthough these approaches can reach the same learning rates, the structure ofthe final estimate of the parameterized approach is better, which sometimesyields a better generalization capability when the number of sample is finite.With this, we recommend to parameterize the shrinkage degree of$L_2$-RBoosting. To this end, we present an adaptive parameter-selectionstrategy for shrinkage degree and verify its feasibility through boththeoretical analysis and numerical verification. The obtained results enhance the understanding of RBoosting and further giveguidance on how to use $L_2$-RBoosting for regression tasks.
arxiv-11400-17 | Provably Correct Active Sampling Algorithms for Matrix Column Subset Selection with Missing Data | http://arxiv.org/abs/1505.04343 | author:Yining Wang, Aarti Singh category:stat.ML cs.LG published:2015-05-17 summary:We consider the problem of matrix column subset selection, which selects asubset of columns from an input matrix such that the input can be wellapproximated by the span of the selected columns. Column subset selection hasbeen applied to numerous real-world data applications such as populationgenetics summarization, electronic circuits testing and recommendation systems.In many applications the complete data matrix is unavailable and one needs toselect representative columns by inspecting only a small portion of the inputmatrix. In this paper we propose the first provably correct column subsetselection algorithms for partially observed data matrices. Our proposedalgorithms exhibit different merits and drawbacks in terms of statisticalaccuracy, computational efficiency, sample complexity and sampling schemes,which provides a nice exploration of the tradeoff between these desiredproperties for column subset selection. The proposed methods employ the idea offeedback driven sampling and are inspired by several sampling schemespreviously introduced for low-rank matrix approximation tasks [DMM08, FKV04,DV06, KS14]. Our analysis shows that two of the proposed algorithms enjoy arelative error bound, which is preferred for column subset selection and matrixapproximation purposes. We also demonstrate through both theoretical andempirical analysis the power of feedback driven sampling compared to uniformrandom sampling on input matrices with highly correlated columns.
arxiv-11400-18 | Learning Deconvolution Network for Semantic Segmentation | http://arxiv.org/abs/1505.04366 | author:Hyeonwoo Noh, Seunghoon Hong, Bohyung Han category:cs.CV published:2015-05-17 summary:We propose a novel semantic segmentation algorithm by learning adeconvolution network. We learn the network on top of the convolutional layersadopted from VGG 16-layer net. The deconvolution network is composed ofdeconvolution and unpooling layers, which identify pixel-wise class labels andpredict segmentation masks. We apply the trained network to each proposal in aninput image, and construct the final semantic segmentation map by combining theresults from all proposals in a simple manner. The proposed algorithm mitigatesthe limitations of the existing methods based on fully convolutional networksby integrating deep deconvolution network and proposal-wise prediction; oursegmentation method typically identifies detailed structures and handlesobjects in multiple scales naturally. Our network demonstrates outstandingperformance in PASCAL VOC 2012 dataset, and we achieve the best accuracy(72.5%) among the methods trained with no external data through ensemble withthe fully convolutional network.
arxiv-11400-19 | Local identifiability of $l_1$-minimization dictionary learning: a sufficient and almost necessary condition | http://arxiv.org/abs/1505.04363 | author:Siqi Wu, Bin Yu category:stat.ML published:2015-05-17 summary:We study the theoretical properties of learning a dictionary from a set of$N$ signals $\mathbf x_i\in \mathbb R^K$ for $i=1,...,N$ via$l_1$-minimization. We assume that the signals $\mathbf x_i$'s are generated as$i.i.d.$ random linear combinations of the $K$ atoms from a complete referencedictionary $\mathbf D_0 \in \mathbb R^{K\times K}$. For the random linearcoefficients, we consider two generative models: the $s$-sparse Gaussian modelwith $s = 1,..,K$, and the Bernoulli($p$)-Gaussian model with $p\in (0,1]$.First, for the population case and under each of the two generative models, weestablish a sufficient and almost necessary condition for the referencedictionary $\mathbf D_0$ to be locally identifiable, i.e. a local minimum ofthe expected $l_1$-norm objective function. Our condition covers both thesparse and dense cases of signal generation, and significantly improves thesufficient condition by Gribonval and Schnass (2010). It fully describes theinteraction between the collinearity of dictionary atoms $\mathbf M_0 = \mathbfD_0^T\mathbf D_0$, and the sparsity parameter $s$ or $p$ of the randomcoefficients in achieving local identifiability. We also provide sharp andeasy-to-compute lower and upper bounds for the quantities involved in ourconditions. With these bounds, we show that local identifiability is possiblewith sparsity level $s$ or $pK$ up to the order $O(\mu^{-2})$ for a complete$\mu$-coherent reference dictionary, i.e. a dictionary with maximum absolutecollinearity $\mu = \max_{i\neq j} \mathbf M_0[i,j]$. Moreover, our localidentifiability results also translate to the finite sample case with highprobability provided that the number of signals $N$ scales as $O(K\log K)$.
arxiv-11400-20 | The Best of Both Worlds: Combining Data-independent and Data-driven Approaches for Action Recognition | http://arxiv.org/abs/1505.04427 | author:Zhenzhong Lan, Dezhong Yao, Ming Lin, Shoou-I Yu, Alexander Hauptmann category:cs.CV published:2015-05-17 summary:Motivated by the success of data-driven convolutional neural networks (CNNs)in object recognition on static images, researchers are working hard towardsdeveloping CNN equivalents for learning video features. However, learning videofeatures globally has proven to be quite a challenge due to its highdimensionality, the lack of labelled data and the difficulty in processinglarge-scale video data. Therefore, we propose to leverage effective techniquesfrom both data-driven and data-independent approaches to improve actionrecognition system. Our contribution is three-fold. First, we propose a two-stream StackedConvolutional Independent Subspace Analysis (ConvISA) architecture to show thatunsupervised learning methods can significantly boost the performance oftraditional local features extracted from data-independent models. Second, wedemonstrate that by learning on video volumes detected by Improved DenseTrajectory (IDT), we can seamlessly combine our novel local descriptors withhand-crafted descriptors. Thus we can utilize available feature enhancingtechniques developed for hand-crafted descriptors. Finally, similar tomulti-class classification framework in CNNs, we propose a training-freere-ranking technique that exploits the relationship among action classes toimprove the overall performance. Our experimental results on four benchmarkaction recognition datasets show significantly improved performance.
arxiv-11400-21 | Global Convergence of Unmodified 3-Block ADMM for a Class of Convex Minimization Problems | http://arxiv.org/abs/1505.04252 | author:Tianyi Lin, Shiqian Ma, Shuzhong Zhang category:math.OC cs.LG stat.ML published:2015-05-16 summary:The alternating direction method of multipliers (ADMM) has been successfullyapplied to solve structured convex optimization problems due to its superiorpractical performance. The convergence properties of the 2-block ADMM have beenstudied extensively in the literature. Specifically, it has been proven thatthe 2-block ADMM globally converges for any penalty parameter $\gamma>0$. Inthis sense, the 2-block ADMM allows the parameter to be free, i.e., there is noneed to restrict the value for the parameter when implementing this algorithmin order to ensure convergence. However, for the 3-block ADMM, Chen et al.recently constructed a counter-example showing that it can diverge if nofurther condition is imposed. The existing results on studying furthersufficient conditions on guaranteeing the convergence of the 3-block ADMMusually require $\gamma$ to be smaller than a certain bound, which is usuallyeither difficult to compute or too small to make it a practical algorithm. Inthis paper, we show that the 3-block ADMM still globally converges with anypenalty parameter $\gamma>0$ when applied to solve a class of commonlyencountered problems to be called regularized least squares decomposition(RLSD) in this paper, which covers many important applications in practice.
arxiv-11400-22 | A New Perspective on Boosting in Linear Regression via Subgradient Optimization and Relatives | http://arxiv.org/abs/1505.04243 | author:Robert M. Freund, Paul Grigas, Rahul Mazumder category:math.ST cs.LG math.OC stat.ML stat.TH published:2015-05-16 summary:In this paper we analyze boosting algorithms in linear regression from a newperspective: that of modern first-order methods in convex optimization. We showthat classic boosting algorithms in linear regression, namely the incrementalforward stagewise algorithm (FS$_\varepsilon$) and least squares boosting(LS-Boost($\varepsilon$)), can be viewed as subgradient descent to minimize theloss function defined as the maximum absolute correlation between the featuresand residuals. We also propose a modification of FS$_\varepsilon$ that yieldsan algorithm for the Lasso, and that may be easily extended to an algorithmthat computes the Lasso path for different values of the regularizationparameter. Furthermore, we show that these new algorithms for the Lasso mayalso be interpreted as the same master algorithm (subgradient descent), appliedto a regularized version of the maximum absolute correlation loss function. Wederive novel, comprehensive computational guarantees for several boostingalgorithms in linear regression (including LS-Boost($\varepsilon$) andFS$_\varepsilon$) by using techniques of modern first-order methods in convexoptimization. Our computational guarantees inform us about the statisticalproperties of boosting algorithms. In particular they provide, for the firsttime, a precise theoretical description of the amount of data-fidelity andregularization imparted by running a boosting algorithm with a prespecifiedlearning rate for a fixed but arbitrary number of iterations, for any dataset.
arxiv-11400-23 | The color of smiling: computational synaesthesia of facial expressions | http://arxiv.org/abs/1505.04260 | author:Vittorio Cuculo, Raffaella Lanzarotti, Giuseppe Boccignone category:cs.CV published:2015-05-16 summary:This note gives a preliminary account of the transcoding or rechannelingproblem between different stimuli as it is of interest for the naturalinteraction or affective computing fields. By the consideration of a simpleexample, namely the color response of an affective lamp to a sensed facialexpression, we frame the problem within an information- theoretic perspective.A full justification in terms of the Information Bottleneck principle promotesa latent affective space, hitherto surmised as an appealing and intuitivesolution, as a suitable mediator between the different stimuli.
arxiv-11400-24 | Robust Real-time Extraction of Fiducial Facial Feature Points using Haar-like Features | http://arxiv.org/abs/1505.04286 | author:Harry Commin category:cs.CV published:2015-05-16 summary:In this paper, we explore methods of robustly extracting fiducial facialfeature points - an important process for numerous facial image processingtasks. We consider various methods to first detect face, then facial featuresand finally salient facial feature points. Colour-based models are analysed andtheir overall unsuitability for this task is summarised. The bulk of the reportis then dedicated to proposing a learning-based method centred on theViola-Jones algorithm. The specific difficulties and considerations relating tofeature point detection are laid out in this context and a novel approach isestablished to address these issues. On a sequence of clear and unobstructedface images, our proposed system achieves average detection rates of over 90%.Then, using a more varied sample dataset, we identify some possible areas forfuture development of our system.
arxiv-11400-25 | A type-theoretical approach to Universal Grammar | http://arxiv.org/abs/1505.04313 | author:Erkki Luuk category:cs.CL math.LO 03 published:2015-05-16 summary:The idea of Universal Grammar (UG) as the hypothetical linguistic structureshared by all human languages harkens back at least to the 13th century. Thebest known modern elaborations of the idea are due to Chomsky. Following adevastating critique from theoretical, typological and field linguistics, theseelaborations, the idea of UG itself and the more general idea of languageuniversals stand untenable and are largely abandoned. The proposal tackles thehypothetical contents of UG using dependent and polymorphic type theory in aframework very different from the Chomskyan ones. We introduce a type logic fora precise, universal and parsimonious representation of natural languagemorphosyntax and compositional semantics. The logic handles grammaticalambiguity (with polymorphic types), selectional restrictions and diverse kindsof anaphora (with dependent types), and features a partly universal set ofmorphosyntactic types (by the Curry-Howard isomorphism).
arxiv-11400-26 | MCODE: Multivariate Conditional Outlier Detection | http://arxiv.org/abs/1505.04097 | author:Charmgil Hong, Milos Hauskrecht category:cs.AI cs.LG stat.ML published:2015-05-15 summary:Outlier detection aims to identify unusual data instances that deviate fromexpected patterns. The outlier detection is particularly challenging whenoutliers are context dependent and when they are defined by unusualcombinations of multiple outcome variable values. In this paper, we develop andstudy a new conditional outlier detection approach for multivariate outcomespaces that works by (1) transforming the conditional detection to the outlierdetection problem in a new (unconditional) space and (2) defining outlierscores by analyzing the data in the new space. Our approach relies on theclassifier chain decomposition of the multi-dimensional classification problemthat lets us transform the output space into a probability vector, oneprobability for each dimension of the output space. Outlier scores applied tothese transformed vectors are then used to detect the outliers. Experiments onmultiple multi-dimensional classification problems with the different outlierinjection rates show that our methodology is robust and able to successfullyidentify outliers when outliers are either sparse (manifested in one or veryfew dimensions) or dense (affecting multiple dimensions).
arxiv-11400-27 | Optimal Low-Rank Tensor Recovery from Separable Measurements: Four Contractions Suffice | http://arxiv.org/abs/1505.04085 | author:Parikshit Shah, Nikhil Rao, Gongguo Tang category:stat.ML cs.IT cs.LG math.IT math.OC published:2015-05-15 summary:Tensors play a central role in many modern machine learning and signalprocessing applications. In such applications, the target tensor is usually oflow rank, i.e., can be expressed as a sum of a small number of rank onetensors. This motivates us to consider the problem of low rank tensor recoveryfrom a class of linear measurements called separable measurements. As specificexamples, we focus on two distinct types of separable measurement mechanisms(a) Random projections, where each measurement corresponds to an inner productof the tensor with a suitable random tensor, and (b) the completion problemwhere measurements constitute revelation of a random set of entries. We presenta computationally efficient algorithm, with rigorous and order-optimal samplecomplexity results (upto logarithmic factors) for tensor recovery. Our methodis based on reduction to matrix completion sub-problems and adaptation ofLeurgans' method for tensor decomposition. We extend the methodology and samplecomplexity results to higher order tensors, and experimentally validate ourtheoretical results.
arxiv-11400-28 | Safe Screening for Multi-Task Feature Learning with Multiple Data Matrices | http://arxiv.org/abs/1505.04073 | author:Jie Wang, Jieping Ye category:cs.LG published:2015-05-15 summary:Multi-task feature learning (MTFL) is a powerful technique in boosting thepredictive performance by learning multiple relatedclassification/regression/clustering tasks simultaneously. However, solving theMTFL problem remains challenging when the feature dimension is extremely large.In this paper, we propose a novel screening rule---that is based on the dualprojection onto convex sets (DPC)---to quickly identify the inactivefeatures---that have zero coefficients in the solution vectors across alltasks. One of the appealing features of DPC is that: it is safe in the sensethat the detected inactive features are guaranteed to have zero coefficients inthe solution vectors across all tasks. Thus, by removing the inactive featuresfrom the training phase, we may have substantial savings in the computationalcost and memory usage without sacrificing accuracy. To the best of ourknowledge, it is the first screening rule that is applicable to sparse modelswith multiple data matrices. A key challenge in deriving DPC is to solve anonconvex problem. We show that we can solve for the global optimum efficientlyvia a properly chosen parametrization of the constraint set. Moreover, DPC hasvery low computational cost and can be integrated with any existing solvers. Wehave evaluated the proposed DPC rule on both synthetic and real data sets. Theexperiments indicate that DPC is very effective in identifying the inactivefeatures---especially for high dimensional data---which leads to a speedup upto several orders of magnitude.
arxiv-11400-29 | A Video Database of Human Faces under Near Infra-Red Illumination for Human Computer Interaction Aplications | http://arxiv.org/abs/1505.04055 | author:S L Happy, Anirban Dasgupta, Anjith George, Aurobinda Routray category:cs.CV published:2015-05-15 summary:Human Computer Interaction (HCI) is an evolving area of research for coherentcommunication between computers and human beings. Some of the importantapplications of HCI as reported in literature are face detection, face poseestimation, face tracking and eye gaze estimation. Development of algorithmsfor these applications is an active field of research. However, availability ofstandard database to validate such algorithms is insufficient. This paperdiscusses the creation of such a database created under Near Infra-Red (NIR)illumination. NIR illumination has gained its popularity for night modeapplications since prolonged exposure to Infra-Red (IR) lighting may lead tomany health issues. The database contains NIR videos of 60 subjects indifferent head orientations and with different facial expressions, facialocclusions and illumination variation. This new database can be a very valuableresource for development and evaluation of algorithms on face detection, eyedetection, head tracking, eye gaze tracking etc. in NIR lighting.
arxiv-11400-30 | Discovering Attribute Shades of Meaning with the Crowd | http://arxiv.org/abs/1505.04117 | author:Adriana Kovashka, Kristen Grauman category:cs.CV published:2015-05-15 summary:To learn semantic attributes, existing methods typically train onediscriminative model for each word in a vocabulary of nameable properties.However, this "one model per word" assumption is problematic: while a wordmight have a precise linguistic definition, it need not have a precise visualdefinition. We propose to discover shades of attribute meaning. Given anattribute name, we use crowdsourced image labels to discover the latent factorsunderlying how different annotators perceive the named concept. We show thatstructure in those latent factors helps reveal shades, that is, interpretationsfor the attribute shared by some group of annotators. Using these shades, wetrain classifiers to capture the primary (often subtle) variants of theattribute. The resulting models are both semantic and visually precise. Bycatering to users' interpretations, they improve attribute prediction accuracyon novel images. Shades also enable more successful attribute-based imagesearch, by providing robust personalized models for retrieving multi-attributequery results. They are widely applicable to tasks that involve describingvisual content, such as zero-shot category learning and organization of photocollections.
arxiv-11400-31 | Margins, Kernels and Non-linear Smoothed Perceptrons | http://arxiv.org/abs/1505.04123 | author:Aaditya Ramdas, Javier Peña category:cs.LG cs.AI cs.NA math.OC published:2015-05-15 summary:We focus on the problem of finding a non-linear classification function thatlies in a Reproducing Kernel Hilbert Space (RKHS) both from the primal point ofview (finding a perfect separator when one exists) and the dual point of view(giving a certificate of non-existence), with special focus on generalizationsof two classical schemes - the Perceptron (primal) and Von-Neumann (dual)algorithms. We cast our problem as one of maximizing the regularized normalizedhard-margin ($\rho$) in an RKHS and %use the Representer Theorem to rephrase itin terms of a Mahalanobis dot-product/semi-norm associated with the kernel's(normalized and signed) Gram matrix. We derive an accelerated smoothedalgorithm with a convergence rate of $\tfrac{\sqrt {\log n}}{\rho}$ given $n$separable points, which is strikingly similar to the classical kernelizedPerceptron algorithm whose rate is $\tfrac1{\rho^2}$. When no such classifierexists, we prove a version of Gordan's separation theorem for RKHSs, and give areinterpretation of negative margins. This allows us to give guarantees for aprimal-dual algorithm that halts in $\min\{\tfrac{\sqrt n}{\rho},\tfrac{\sqrt n}{\epsilon}\}$ iterations with a perfect separator in the RKHS ifthe primal is feasible or a dual $\epsilon$-certificate of near-infeasibility.
arxiv-11400-32 | Consistent Algorithms for Multiclass Classification with a Reject Option | http://arxiv.org/abs/1505.04137 | author:Harish G. Ramaswamy, Ambuj Tewari, Shivani Agarwal category:cs.LG stat.ML published:2015-05-15 summary:We consider the problem of $n$-class classification ($n\geq 2$), where theclassifier can choose to abstain from making predictions at a given cost, say,a factor $\alpha$ of the cost of misclassification. Designing consistentalgorithms for such $n$-class classification problems with a `reject option' isthe main goal of this paper, thereby extending and generalizing previouslyknown results for $n=2$. We show that the Crammer-Singer surrogate and the onevs all hinge loss, albeit with a different predictor than the standard argmax,yield consistent algorithms for this problem when $\alpha=\frac{1}{2}$. Moreinterestingly, we design a new convex surrogate that is also consistent forthis problem when $\alpha=\frac{1}{2}$ and operates on a much lower dimensionalspace ($\log(n)$ as opposed to $n$). We also generalize all three surrogates tobe consistent for any $\alpha\in[0, \frac{1}{2}]$.
arxiv-11400-33 | Dense Semantic Correspondence where Every Pixel is a Classifier | http://arxiv.org/abs/1505.04143 | author:Hilton Bristow, Jack Valmadre, Simon Lucey category:cs.CV published:2015-05-15 summary:Determining dense semantic correspondences across objects and scenes is adifficult problem that underpins many higher-level computer vision algorithms.Unlike canonical dense correspondence problems which consider images that arespatially or temporally adjacent, semantic correspondence is characterized byimages that share similar high-level structures whose exact appearance andgeometry may differ. Motivated by object recognition literature and recent work on rapidlyestimating linear classifiers, we treat semantic correspondence as aconstrained detection problem, where an exemplar LDA classifier is learned foreach pixel. LDA classifiers have two distinct benefits: (i) they exhibit higheraverage precision than similarity metrics typically used in correspondenceproblems, and (ii) unlike exemplar SVM, can output globally interpretableposterior probabilities without calibration, whilst also being significantlyfaster to train. We pose the correspondence problem as a graphical model, where the unarypotentials are computed via convolution with the set of exemplar classifiers,and the joint potentials enforce smoothly varying correspondence assignment.
arxiv-11400-34 | WhittleSearch: Interactive Image Search with Relative Attribute Feedback | http://arxiv.org/abs/1505.04141 | author:Adriana Kovashka, Devi Parikh, Kristen Grauman category:cs.CV published:2015-05-15 summary:We propose a novel mode of feedback for image search, where a user describeswhich properties of exemplar images should be adjusted in order to more closelymatch his/her mental model of the image sought. For example, perusing imageresults for a query "black shoes", the user might state, "Show me shoe imageslike these, but sportier." Offline, our approach first learns a set of rankingfunctions, each of which predicts the relative strength of a nameable attributein an image (e.g., sportiness). At query time, the system presents the userwith a set of exemplar images, and the user relates them to his/her targetimage with comparative statements. Using a series of such constraints in themulti-dimensional attribute space, our method iteratively updates its relevancefunction and re-ranks the database of images. To determine which exemplarimages receive feedback from the user, we present two variants of the approach:one where the feedback is user-initiated and another where the feedback isactively system-initiated. In either case, our approach allows a user toefficiently "whittle away" irrelevant portions of the visual feature space,using semantic language to precisely communicate her preferences to the system.We demonstrate our technique for refining image search for people, products,and scenes, and we show that it outperforms traditional binary relevancefeedback in terms of search speed and accuracy. In addition, the ordinal natureof relative attributes helps make our active approach efficient -- bothcomputationally for the machine when selecting the reference images, and forthe user by requiring less user interaction than conventional passive andactive methods.
arxiv-11400-35 | Reinforcement Learning applied to Single Neuron | http://arxiv.org/abs/1505.04150 | author:Zhipeng Wang, Mingbo Cai category:cs.AI cs.NE published:2015-05-15 summary:This paper extends the reinforcement learning ideas into the multi-agentssystem, which is far more complicated than the previously studied single-agentsystem. We studied two different multi-agents systems. One is thefully-connected neural network consists of multiple single neurons. Another oneis the simplified mechanical arm system which is controlled by multipleneurons. We suppose that each neuron is like an agent and it can do Gibbssampling of the posterior probability of stimulus features. The policy isoptimized in a way that the cumulative global rewards are maximized. Thealgorithm for the second system is based on the same idea but we incorporatethe physics model into the constraints. The simulation results show that forthe first system our algorithm converges well. For the second system it doesnot converge well in a reasonable simulation time length. In summary, we tookthe initial endeavor to study the reinforcement learning for multi-agentssystem. The computational complexity is always an issue and significant amountof works have to be done in order to better understand the problem.
arxiv-11400-36 | Arabic Inquiry-Answer Dialogue Acts Annotation Schema | http://arxiv.org/abs/1505.04197 | author:AbdelRahim A. Elmadany, Sherif M. Abdou, Mervat Gheith category:cs.CL published:2015-05-15 summary:We present an annotation schema as part of an effort to create a manuallyannotated corpus for Arabic dialogue language understanding including spokendialogue and written "chat" dialogue for inquiry-answer domain. The proposedschema handles mainly the request and response acts that occurs frequently ininquiry-answer debate conversations expressing request services, suggests, andoffers. We applied the proposed schema on 83 Arabic inquiry-answer dialogues.
arxiv-11400-37 | Using Ensemble Models in the Histological Examination of Tissue Abnormalities | http://arxiv.org/abs/1505.03932 | author:Giancarlo Crocetti, Michael Coakley, Phil Dressner, Wanda Kellum, Tamba Lamin category:cs.CV cs.CE cs.LG published:2015-05-15 summary:Classification models for the automatic detection of abnormalities onhistological samples do exists, with an active debate on the cost associatedwith false negative diagnosis (underdiagnosis) and false positive diagnosis(overdiagnosis). Current models tend to underdiagnose, failing to recognize apotentially fatal disease. The objective of this study is to investigate the possibility ofautomatically identifying abnormalities in tissue samples through the use of anensemble model on data generated by histological examination and to minimizethe number of false negative cases.
arxiv-11400-38 | A Real Time Facial Expression Classification System Using Local Binary Patterns | http://arxiv.org/abs/1505.04058 | author:S. L. Happy, Anjith George, Aurobinda Routray category:cs.CV published:2015-05-15 summary:Facial expression analysis is one of the popular fields of research in humancomputer interaction (HCI). It has several applications in next generation userinterfaces, human emotion analysis, behavior and cognitive modeling. In thispaper, a facial expression classification algorithm is proposed which uses Haarclassifier for face detection purpose, Local Binary Patterns (LBP) histogram ofdifferent block sizes of a face image as feature vectors and classifies variousfacial expressions using Principal Component Analysis (PCA). The algorithm isimplemented in real time for expression classification since the computationalcomplexity of the algorithm is small. A customizable approach is proposed forfacial expression analysis, since the various expressions and intensity ofexpressions vary from person to person. The system uses grayscale frontal faceimages of a person to classify six basic emotions namely happiness, sadness,disgust, fear, surprise and anger.
arxiv-11400-39 | Discontinuous Piecewise Polynomial Neural Networks | http://arxiv.org/abs/1505.04211 | author:John Loverich category:cs.NE published:2015-05-15 summary:An artificial neural network is presented where each link is represented by agrid of weights defining a series of piecewise polynomial functions withdiscontinuities between each polynomial. The polynomial order ranges from firstto fifth order. The unit averages the input values from each link. A backpropagation technique that works with discontinuous link functions andactivation functions is presented. The use of discontinuous links functionmeans that only a subset of the network is active for a given input and so onlya subset of the network is trained for a particular training example. Unitdropout is used for regularization and a parameter free weight update is used.Better performance is obtained by moving from piecewise linear links topiecewise quadratic and higher order links. The algorithm is tested on theMNIST data set, using multiple autoencoders, with good results.
arxiv-11400-40 | Biometric Matching and Fusion System for Fingerprints from Non-Distal Phalanges | http://arxiv.org/abs/1505.04028 | author:Mehmet Kayaoglu, Berkay Topcu, Umut Uludag category:cs.CV published:2015-05-15 summary:Market research indicates that fingerprints are still the most popularbiometric modality for personal authentication. Even with the onset of newmodalities (e.g. vein matching), many applications within different domains(e-ID, banking, border control...) and geographies rely on fingerprintsobtained from the distal phalanges (a.k.a. sections, digits) of the human handstructure. Motivated by the problem of poor quality distal fingerprint imagesaffecting a non-trivial portion of the population (which decreases associatedauthentication accuracy), we designed and tested a multifinger, multiphalanxfusion scheme, that combines minutiae matching scores originating fromnon-distal (ie. middle and proximal) phalanges based on (i) simple sum fusion,(ii) NFIQ image-quality-based fusion, and (iii) phalanx-type-based fusion.Utilizing a medium-size (50 individuals, 400 unique fingers, 1600 distinctimages) database collected in our laboratory with a commercial opticalfingerprint sensor, and a commercial minutiae extractor & matcher (without anymodification), allowed us to simulate a real-world fingerprint authenticationsetting. Detailed analyses including ROC curves with statistical confidenceintervals show that the proposed system can be a viable alternative for caseswhere (i) distal phalanx images are not usable (e.g. due to missing digits, orlow quality finger surface due to manual labor), and (ii) switching to a newbiometric modality (e.g. iris) is not possible due to economical orinfrastructure limits. Further, we show that when distal phalanx images are infact usable, combining them with images from other phalanges increases accuracyas well.
arxiv-11400-41 | Algorithmic Connections Between Active Learning and Stochastic Convex Optimization | http://arxiv.org/abs/1505.04214 | author:Aaditya Ramdas, Aarti Singh category:cs.LG cs.AI math.OC stat.ML published:2015-05-15 summary:Interesting theoretical associations have been established by recent papersbetween the fields of active learning and stochastic convex optimization due tothe common role of feedback in sequential querying mechanisms. In this paper,we continue this thread in two parts by exploiting these relations for thefirst time to yield novel algorithms in both fields, further motivating thestudy of their intersection. First, inspired by a recent optimization algorithmthat was adaptive to unknown uniform convexity parameters, we present a newactive learning algorithm for one-dimensional thresholds that can yield minimaxrates by adapting to unknown noise parameters. Next, we show that one canperform $d$-dimensional stochastic minimization of smooth uniformly convexfunctions when only granted oracle access to noisy gradient signs along anycoordinate instead of real-valued gradients, by using a simple randomizedcoordinate descent procedure where each line search can be solved by$1$-dimensional active learning, provably achieving the same error convergencerate as having the entire real-valued gradient. Combining these two partsyields an algorithm that solves stochastic convex optimization of uniformlyconvex and smooth functions using only noisy gradient signs by repeatedlyperforming active learning, achieves optimal rates and is adaptive to allunknown convexity and smoothness parameters.
arxiv-11400-42 | An Analysis of Active Learning With Uniform Feature Noise | http://arxiv.org/abs/1505.04215 | author:Aaditya Ramdas, Barnabas Poczos, Aarti Singh, Larry Wasserman category:stat.ML cs.AI cs.LG math.ST stat.TH published:2015-05-15 summary:In active learning, the user sequentially chooses values for feature $X$ andan oracle returns the corresponding label $Y$. In this paper, we consider theeffect of feature noise in active learning, which could arise either because$X$ itself is being measured, or it is corrupted in transmission to the oracle,or the oracle returns the label of a noisy version of the query point. Instatistics, feature noise is known as "errors in variables" and has beenstudied extensively in non-active settings. However, the effect of featurenoise in active learning has not been studied before. We consider thewell-known Berkson errors-in-variables model with additive uniform noise ofwidth $\sigma$. Our simple but revealing setting is that of one-dimensional binaryclassification setting where the goal is to learn a threshold (point where theprobability of a $+$ label crosses half). We deal with regression functionsthat are antisymmetric in a region of size $\sigma$ around the threshold andalso satisfy Tsybakov's margin condition around the threshold. We prove minimaxlower and upper bounds which demonstrate that when $\sigma$ is smaller than theminimiax active/passive noiseless error derived in \cite{CN07}, then noise hasno effect on the rates and one achieves the same noiseless rates. For larger$\sigma$, the \textit{unflattening} of the regression function on convolutionwith uniform noise, along with its local antisymmetry around the threshold,together yield a behaviour where noise \textit{appears} to be beneficial. Ourkey result is that active learning can buy significant improvement over apassive strategy even in the presence of feature noise.
arxiv-11400-43 | A Multivariate Biomarker for Parkinson's Disease | http://arxiv.org/abs/1602.07264 | author:Giancarlo Crocetti, Michael Coakley, Phil Dressner, Wanda Kellum, Tamba Lamin category:cs.LG published:2015-05-15 summary:In this study, we executed a genomic analysis with the objective of selectinga set of genes (possibly small) that would help in the detection andclassification of samples from patients affected by Parkinson Disease. Weperformed a complete data analysis and during the exploratory phase, weselected a list of differentially expressed genes. Despite their associationwith the diseased state, we could not use them as a biomarker tool. Therefore,our research was extended to include a multivariate analysis approach resultingin the identification and selection of a group of 20 genes that showed a clearpotential in detecting and correctly classify Parkinson Disease samples even inthe presence of other neurodegenerative disorders.
arxiv-11400-44 | Automatic Facial Expression Recognition Using Features of Salient Facial Patches | http://arxiv.org/abs/1505.04026 | author:S L Happy, Aurobinda Routray category:cs.CV published:2015-05-15 summary:Extraction of discriminative features from salient facial patches plays avital role in effective facial expression recognition. The accurate detectionof facial landmarks improves the localization of the salient patches on faceimages. This paper proposes a novel framework for expression recognition byusing appearance features of selected facial patches. A few prominent facialpatches, depending on the position of facial landmarks, are extracted which areactive during emotion elicitation. These active patches are further processedto obtain the salient patches which contain discriminative features forclassification of each pair of expressions, thereby selecting different facialpatches as salient for different pair of expression classes. One-against-oneclassification method is adopted using these features. In addition, anautomated learning-free facial landmark detection technique has been proposed,which achieves similar performances as that of other state-of-art landmarkdetection methods, yet requires significantly less execution time. The proposedmethod is found to perform well consistently in different resolutions, hence,providing a solution for expression recognition in low resolution images.Experiments on CK+ and JAFFE facial expression databases show the effectivenessof the proposed system.
arxiv-11400-45 | Robust Facial Expression Classification Using Shape and Appearance Features | http://arxiv.org/abs/1505.04030 | author:S. L. Happy, Aurobinda Routray category:cs.CV published:2015-05-15 summary:Facial expression recognition has many potential applications which hasattracted the attention of researchers in the last decade. Feature extractionis one important step in expression analysis which contributes toward fast andaccurate expression recognition. This paper represents an approach of combiningthe shape and appearance features to form a hybrid feature vector. We haveextracted Pyramid of Histogram of Gradients (PHOG) as shape descriptors andLocal Binary Patterns (LBP) as appearance features. The proposed frameworkinvolves a novel approach of extracting hybrid features from active facialpatches. The active facial patches are located on the face regions whichundergo a major change during different expressions. After detection of faciallandmarks, the active patches are localized and hybrid features are calculatedfrom these patches. The use of small parts of face instead of the whole facefor extracting features reduces the computational cost and prevents theover-fitting of the features for classification. By using linear discriminantanalysis, the dimensionality of the feature is reduced which is furtherclassified by using the support vector machine (SVM). The experimental resultson two publicly available databases show promising accuracy in recognizing allexpression classes.
arxiv-11400-46 | Achieving Optimal Misclassification Proportion in Stochastic Block Model | http://arxiv.org/abs/1505.03772 | author:Chao Gao, Zongming Ma, Anderson Y. Zhang, Harrison H. Zhou category:math.ST cs.SI stat.ME stat.ML stat.TH published:2015-05-14 summary:Community detection is a fundamental statistical problem in network dataanalysis. Many algorithms have been proposed to tackle this problem. Most ofthese algorithms are not guaranteed to achieve the statistical optimality ofthe problem, while procedures that achieve information theoretic limits forgeneral parameter spaces are not computationally tractable. In this paper, wepresent a computationally feasible two-stage method that achieves optimalstatistical performance in misclassification proportion for stochastic blockmodel under weak regularity conditions. Our two-stage procedure consists of ageneric refinement step that can take a wide range of weakly consistentcommunity detection procedures as initializer, to which the refinement stageapplies and outputs a community assignment achieving optimal misclassificationproportion with high probability. The practical effectiveness of the newalgorithm is demonstrated by competitive numerical results.
arxiv-11400-47 | Pinball Loss Minimization for One-bit Compressive Sensing | http://arxiv.org/abs/1505.03898 | author:Xiaolin Huang, Lei Shi, Ming Yan, Johan A. K. Suykens category:cs.IT math.IT math.NA math.OC stat.ML published:2015-05-14 summary:The one-bit quantization can be implemented by one single comparator, whichoperates at low power and a high rate. Hence one-bit compressive sensing(\emph{1bit-CS}) becomes very attractive in signal processing. When themeasurements are corrupted by noise during signal acquisition and transmission,1bit-CS is usually modeled as minimizing a loss function with a sparsityconstraint. The existing loss functions include the hinge loss and the linearloss. Though 1bit-CS can be regarded as a binary classification problem becausea one-bit measurement only provides the sign information, the choice of thehinge loss over the linear loss in binary classification is not true for1bit-CS. Many experiments show that the linear loss performs better than thehinge loss for 1bit-CS. Motivated by this observation, we consider the pinballloss, which provides a bridge between the hinge loss and the linear loss. Usingthis bridge, two 1bit-CS models and two corresponding algorithms are proposed.Pinball loss iterative hard thresholding improves the performance of the binaryiterative hard theresholding proposed in [6] and is suitable for the case whenthe sparsity of the true signal is given. Elastic-net pinball support vectormachine generalizes the passive model proposed in [11] and is suitable for thecase when the sparsity of the true signal is not given. A fast dual coordinateascent algorithm is proposed to solve the elastic-net pinball support vectormachine problem, and its convergence is proved. The numerical experimentsdemonstrate that the pinball loss, as a trade-off between the hinge loss andthe linear loss, improves the existing 1bit-CS models with better performances.
arxiv-11400-48 | General Riemannian SOM | http://arxiv.org/abs/1505.03917 | author:Jascha A. Schewtschenko category:cs.NE published:2015-05-14 summary:Kohonen's Self-Organizing Maps (SOMs) have proven to be a successfuldata-reduction method to identify the intrinsic lower-dimensional sub-manifoldof a data set that is scattered in the higher-dimensional feature space.Motivated by the possibly non-Euclidian nature of the feature space and of theintrinsic geometry of the data set, we extend the definition of classic SOMs toobtain the General Riemannian SOM (GRiSOM). We additionally provide animplementation as a proof-of-concept for geometries with constant curvature. Wefurthermore perform the analytic and numerical analysis of the stability limitsof certain (GRi)SOM configurations covering the different possible regulartessellation of the map space in each geometry. A deviation between thenumerical and analytic stability limit has been observed for the square andhexagonal Euclidean maps for very small neighbourhoods in the map space as wellas agreement in case of longer-ranged relations between the map nodes.
arxiv-11400-49 | Task-Based Optimization of Computed Tomography Imaging Systems | http://arxiv.org/abs/1505.03891 | author:Adrian A. Sanchez category:physics.med-ph cs.CV published:2015-05-14 summary:The goal of this thesis is to provide a framework for the use of task-basedmetrics of image quality to aid in the design, implementation, and evaluationof CT image reconstruction algorithms and CT systems in general. We support theview that task-based metrics of image quality can be useful in guiding thealgorithm design and implementation process in order to yield images ofobjectively superior quality and higher utility for a given task. Further, webelieve that metrics such as the Hotelling observer (HO) SNR can be used assummary scalar metrics of image quality for the evaluation of images producedby novel reconstruction algorithms. In this work, we aim to construct a conciseand versatile formalism for image reconstruction algorithm design,implementation, and assessment. The bulk of the work focuses on linearanalytical algorithms, specifically the ubiquitous filtered back-projection(FBP) algorithm. However, due to the demonstrated importance ofoptimization-based algorithms in a wide variety of CT applications, we devoteone chapter to the characterization of noise properties in TV-based iterativereconstruction, as the understanding of image statistics in optimization-basedreconstruction is the limiting factor in applying HO metrics.
arxiv-11400-50 | Vanishing Point Attracts Eye Movements in Scene Free-viewing | http://arxiv.org/abs/1505.03578 | author:Ali Borji, Mengyang Feng, Huchuan Lu category:cs.CV published:2015-05-14 summary:Eye movements are crucial in understanding complex scenes. By predictingwhere humans look in natural scenes, we can understand how they percieve scenesand priotriaze information for further high-level processing. Here, we studythe effect of a particular type of scene structural information known asvanishing point and show that human gaze is attracted to vanishing pointregions. We then build a combined model of traditional saliency and vanishingpoint channel that outperforms state of the art saliency models.
arxiv-11400-51 | Improving Image Classification with Location Context | http://arxiv.org/abs/1505.03873 | author:Kevin Tang, Manohar Paluri, Li Fei-Fei, Rob Fergus, Lubomir Bourdev category:cs.CV published:2015-05-14 summary:With the widespread availability of cellphones and cameras that have GPScapabilities, it is common for images being uploaded to the Internet today tohave GPS coordinates associated with them. In addition to research that triesto predict GPS coordinates from visual features, this also opens up the door toproblems that are conditioned on the availability of GPS coordinates. In thiswork, we tackle the problem of performing image classification with locationcontext, in which we are given the GPS coordinates for images in both the trainand test phases. We explore different ways of encoding and extracting featuresfrom the GPS coordinates, and show how to naturally incorporate these featuresinto a Convolutional Neural Network (CNN), the current state-of-the-art formost image classification and recognition problems. We also show how it ispossible to simultaneously learn the optimal pooling radii for a subset of ourfeatures within the CNN framework. To evaluate our model and to help promoteresearch in this area, we identify a set of location-sensitive concepts andannotate a subset of the Yahoo Flickr Creative Commons 100M dataset that hasGPS coordinates with these concepts, which we make publicly available. Byleveraging location context, we are able to achieve almost a 7% gain in meanaverage precision.
arxiv-11400-52 | Distant Supervision for Entity Linking | http://arxiv.org/abs/1505.03823 | author:Miao Fan, Qiang Zhou, Thomas Fang Zheng category:cs.CL cs.IR published:2015-05-14 summary:Entity linking is an indispensable operation of populating knowledgerepositories for information extraction. It studies on aligning a textualentity mention to its corresponding disambiguated entry in a knowledgerepository. In this paper, we propose a new paradigm named distantly supervisedentity linking (DSEL), in the sense that the disambiguated entities that belongto a huge knowledge repository (Freebase) are automatically aligned to thecorresponding descriptive webpages (Wiki pages). In this way, a large scale ofweakly labeled data can be generated without manual annotation and fed to aclassifier for linking more newly discovered entities. Compared withtraditional paradigms based on solo knowledge base, DSEL benefits more viajointly leveraging the respective advantages of Freebase and Wikipedia.Specifically, the proposed paradigm facilitates bridging the disambiguatedlabels (Freebase) of entities and their textual descriptions (Wikipedia) forWeb-scale entities. Experiments conducted on a dataset of 140,000 items and60,000 features achieve a baseline F1-measure of 0.517. Furthermore, we analyzethe feature performance and improve the F1-measure to 0.545.
arxiv-11400-53 | Parametric Regression on the Grassmannian | http://arxiv.org/abs/1505.03832 | author:Yi Hong, Nikhil Singh, Roland Kwitt, Nuno Vasconcelos, Marc Niethammer category:cs.CV published:2015-05-14 summary:We address the problem of fitting parametric curves on the Grassmann manifoldfor the purpose of intrinsic parametric regression. As customary in theliterature, we start from the energy minimization formulation of linearleast-squares in Euclidean spaces and generalize this concept to generalnonflat Riemannian manifolds, following an optimal-control point of view. Wethen specialize this idea to the Grassmann manifold and demonstrate that ityields a simple, extensible and easy-to-implement solution to the parametricregression problem. In fact, it allows us to extend the basic geodesic model to(1) a time-warped variant and (2) cubic splines. We demonstrate the utility ofthe proposed solution on different vision problems, such as shape regression asa function of age, traffic-speed estimation and crowd-counting fromsurveillance video clips. Most notably, these problems can be convenientlysolved within the same framework without any specifically-tailored steps alongthe processing pipeline.
arxiv-11400-54 | Training generative neural networks via Maximum Mean Discrepancy optimization | http://arxiv.org/abs/1505.03906 | author:Gintare Karolina Dziugaite, Daniel M. Roy, Zoubin Ghahramani category:stat.ML cs.LG published:2015-05-14 summary:We consider training a deep neural network to generate samples from anunknown distribution given i.i.d. data. We frame learning as an optimizationminimizing a two-sample test statistic---informally speaking, a good generatornetwork produces samples that cause a two-sample test to fail to reject thenull hypothesis. As our two-sample test statistic, we use an unbiased estimateof the maximum mean discrepancy, which is the centerpiece of the nonparametrickernel two-sample test proposed by Gretton et al. (2012). We compare to theadversarial nets framework introduced by Goodfellow et al. (2014), in whichlearning is a two-player game between a generator network and an adversarialdiscriminator network, both trained to outwit the other. From this perspective,the MMD statistic plays the role of the discriminator. In addition to empiricalcomparisons, we prove bounds on the generalization error incurred by optimizingthe empirical MMD.
arxiv-11400-55 | Fast and numerically stable circle fit | http://arxiv.org/abs/1505.03795 | author:Houssam Abdul-Rahman, Nikolai Chernov category:cs.CV published:2015-05-14 summary:We develop a new algorithm for fitting circles that does not have drawbackscommonly found in existing circle fits. Our fit achieves ultimate accuracy (tomachine precision), avoids divergence, and is numerically stable even whenfitting circles get arbitrary large. Lastly, our algorithm takes less than 10iterations to converge, on average.
arxiv-11400-56 | Unsupervised Object Discovery and Tracking in Video Collections | http://arxiv.org/abs/1505.03825 | author:Suha Kwak, Minsu Cho, Ivan Laptev, Jean Ponce, Cordelia Schmid category:cs.CV published:2015-05-14 summary:This paper addresses the problem of automatically localizing dominant objectsas spatio-temporal tubes in a noisy collection of videos with minimal or evenno supervision. We formulate the problem as a combination of two complementaryprocesses: discovery and tracking. The first one establishes correspondencesbetween prominent regions across videos, and the second one associatessuccessive similar object regions within the same video. Interestingly, ouralgorithm also discovers the implicit topology of frames associated withinstances of the same object class across different videos, a role normallyleft to supervisory information in the form of class labels in conventionalimage and video understanding methods. Indeed, as demonstrated by ourexperiments, our method can handle video collections featuring multiple objectclasses, and substantially outperforms the state of the art in colocalization,even though it tackles a broader problem with much less supervision.
arxiv-11400-57 | Rank diversity of languages: Generic behavior in computational linguistics | http://arxiv.org/abs/1505.03783 | author:Germinal Cocho, Jorge Flores, Carlos Gershenson, Carlos Pineda, Sergio Sánchez category:cs.CL published:2015-05-14 summary:Statistical studies of languages have focused on the rank-frequencydistribution of words. Instead, we introduce here a measure of how word rankschange in time and call this distribution \emph{rank diversity}. We calculatethis diversity for books published in six European languages since 1800, andfind that it follows a universal lognormal distribution. Based on the mean andstandard deviation associated with the lognormal distribution, we define threedifferent word regimes of languages: "heads" consist of words which almost donot change their rank in time, "bodies" are words of general use, while "tails"are comprised by context-specific words and vary their rank considerably intime. The heads and bodies reflect the size of language cores identified bylinguists for basic communication. We propose a Gaussian random walk modelwhich reproduces the rank variation of words in time and thus the diversity.Rank diversity of words can be understood as the result of random variations inrank, where the size of the variation depends on the rank itself. We find thatthe core size is similar for all languages studied.
arxiv-11400-58 | Non-unique games over compact groups and orientation estimation in cryo-EM | http://arxiv.org/abs/1505.03840 | author:Afonso S. Bandeira, Yutong Chen, Amit Singer category:cs.CV cs.DS math.OC published:2015-05-14 summary:Let $\mathcal{G}$ be a compact group and let $f_{ij} \in L^2(\mathcal{G})$.We define the Non-Unique Games (NUG) problem as finding $g_1,\dots,g_n \in\mathcal{G}$ to minimize $\sum_{i,j=1}^n f_{ij} \left( g_i g_j^{-1}\right)$. Wedevise a relaxation of the NUG problem to a semidefinite program (SDP) bytaking the Fourier transform of $f_{ij}$ over $\mathcal{G}$, which can then besolved efficiently. The NUG framework can be seen as a generalization of thelittle Grothendieck problem over the orthogonal group and the Unique Gamesproblem and includes many practically relevant problems, such as the maximumlikelihood estimator} to registering bandlimited functions over the unit spherein $d$-dimensions and orientation estimation in cryo-Electron Microscopy.
arxiv-11400-59 | CAT2000: A Large Scale Fixation Dataset for Boosting Saliency Research | http://arxiv.org/abs/1505.03581 | author:Ali Borji, Laurent Itti category:cs.CV published:2015-05-14 summary:Saliency modeling has been an active research area in computer vision forabout two decades. Existing state of the art models perform very well inpredicting where people look in natural scenes. There is, however, the riskthat these models may have been overfitting themselves to available small scalebiased datasets, thus trapping the progress in a local minimum. To gain adeeper insight regarding current issues in saliency modeling and to bettergauge progress, we recorded eye movements of 120 observers while they freelyviewed a large number of naturalistic and artificial images. Our stimuliincludes 4000 images; 200 from each of 20 categories covering different typesof scenes such as Cartoons, Art, Objects, Low resolution images, Indoor,Outdoor, Jumbled, Random, and Line drawings. We analyze some basic propertiesof this dataset and compare some successful models. We believe that our datasetopens new challenges for the next generation of saliency models and helpsconduct behavioral studies on bottom-up visual attention.
arxiv-11400-60 | A PCA-Based Convolutional Network | http://arxiv.org/abs/1505.03703 | author:Yanhai Gan, Jun Liu, Junyu Dong, Guoqiang Zhong category:cs.LG cs.CV cs.NE published:2015-05-14 summary:In this paper, we propose a novel unsupervised deep learning model, calledPCA-based Convolutional Network (PCN). The architecture of PCN is composed ofseveral feature extraction stages and a nonlinear output stage. Particularly,each feature extraction stage includes two layers: a convolutional layer and afeature pooling layer. In the convolutional layer, the filter banks are simplylearned by PCA. In the nonlinear output stage, binary hashing is applied. Forthe higher convolutional layers, the filter banks are learned from the featuremaps that were obtained in the previous stage. To test PCN, we conductedextensive experiments on some challenging tasks, including handwritten digitsrecognition, face recognition and texture classification. The results show thatPCN performs competitive with or even better than state-of-the-art deeplearning models. More importantly, since there is no back propagation forsupervised finetuning, PCN is much more efficient than existing deep networks.
arxiv-11400-61 | Neural Network with Unbounded Activation Functions is Universal Approximator | http://arxiv.org/abs/1505.03654 | author:Sho Sonoda, Noboru Murata category:cs.NE cs.LG math.FA published:2015-05-14 summary:This paper presents an investigation of the approximation property of neuralnetworks with unbounded activation functions, such as the rectified linear unit(ReLU), which is the new de-facto standard of deep learning. The ReLU networkcan be analyzed by the ridgelet transform with respect to Lizorkindistributions. By showing three reconstruction formulas by using the Fourierslice theorem, the Radon transform, and Parseval's relation, it is shown that aneural network with unbounded activation functions still satisfies theuniversal approximation property. As an additional consequence, the ridgelettransform, or the backprojection filter in the Radon domain, is what thenetwork learns after backpropagation. Subject to a constructive admissibilitycondition, the trained network can be obtained by simply discretizing theridgelet transform, without backpropagation. Numerical examples not onlysupport the consistency of the admissibility condition but also imply that somenon-admissible cases result in low-pass filtering.
arxiv-11400-62 | $k$-center Clustering under Perturbation Resilience | http://arxiv.org/abs/1505.03924 | author:Maria-Florina Balcan, Nika Haghtalab, Colin White category:cs.DS cs.LG published:2015-05-14 summary:The $k$-center problem is a canonical and long-studied facility location andclustering problem with many applications in both its symmetric and asymmetricforms. Both versions of the problem have tight approximation factors on worstcase instances: a $2$-approximation for symmetric $k$-center and an$O(\log^*(k))$-approximation for the asymmetric version. In this work, we go beyond the worst case and provide strong positive resultsboth for the asymmetric and symmetric $k$-center problems under a very naturalinput stability (promise) condition called $\alpha$-perturbation resilience(Bilu & Linial 2012) , which states that the optimal solution does not changeunder any $\alpha$-factor perturbation to the input distances. We show that byassuming 2-perturbation resilience, the exact solution for the asymmetric$k$-center problem can be found in polynomial time. To our knowledge, this isthe first problem that is hard to approximate to any constant factor in theworst case, yet can be optimally solved in polynomial time under perturbationresilience for a constant value of $\alpha$. Furthermore, we prove our resultis tight by showing symmetric $k$-center under $(2-\epsilon)$-perturbationresilience is hard unless $NP=RP$. This is the first tight result for anyproblem under perturbation resilience, i.e., this is the first time the exactvalue of $\alpha$ for which the problem switches from being NP-hard toefficiently computable has been found. Our results illustrate a surprising relationship between symmetric andasymmetric $k$-center instances under perturbation resilience. Unlikeapproximation ratio, for which symmetric $k$-center is easily solved to afactor of $2$ but asymmetric $k$-center cannot be approximated to any constantfactor, both symmetric and asymmetric $k$-center can be solved optimally underresilience to 2-perturbations.
arxiv-11400-63 | Looking outside of the Box: Object Detection and Localization with Multi-scale Patterns | http://arxiv.org/abs/1505.03597 | author:Eshed Ohn-Bar, M. M. Trivedi category:cs.CV published:2015-05-14 summary:Detection and localization of objects at multiple scales often involvessliding a single scale template in order to score windows at different scalesindependently. Nonetheless, multi-scale visual information at a given imagelocation is highly correlated. This fundamental insight allows us to generalizethe traditional multi-scale sliding window technique by jointly consideringimage features at all scales in order to detect and localize objects. Twomax-margin approaches are studied for learning the multi-scale templates andleveraging the highly structured multi-scale information which would have beenignored if a single-scale template was used. The multi-scale formulation isshown to significantly improve general detection performance (measured on thePASCAL VOC dataset). The experimental analysis shows the method to be effectivewith different visual features, both HOG and CNN. Surprisingly, for a givenwindow in a specific scale, visual information from windows at the same imagelocation but other scales (`out-of-scale' information) contains most of thediscriminative information for detection.
arxiv-11400-64 | An Image is Worth More than a Thousand Favorites: Surfacing the Hidden Beauty of Flickr Pictures | http://arxiv.org/abs/1505.03358 | author:Rossano Schifanella, Miriam Redi, Luca Aiello category:cs.SI cs.CV cs.CY cs.MM published:2015-05-13 summary:The dynamics of attention in social media tend to obey power laws. Attentionconcentrates on a relatively small number of popular items and neglecting thevast majority of content produced by the crowd. Although popularity can be anindication of the perceived value of an item within its community, previousresearch has hinted to the fact that popularity is distinct from intrinsicquality. As a result, content with low visibility but high quality lurks in thetail of the popularity distribution. This phenomenon can be particularlyevident in the case of photo-sharing communities, where valuable photographerswho are not highly engaged in online social interactions contribute withhigh-quality pictures that remain unseen. We propose to use a computer visionmethod to surface beautiful pictures from the immense pool ofnear-zero-popularity items, and we test it on a large dataset ofcreative-commons photos on Flickr. By gathering a large crowdsourced groundtruth of aesthetics scores for Flickr images, we show that our method retrievesphotos whose median perceived beauty score is equal to the most popular ones,and whose average is lower by only 1.5%.
arxiv-11400-65 | Noncrossing Ordinal Classification | http://arxiv.org/abs/1505.03442 | author:Xingye Qiao category:stat.ML stat.CO 62H30 published:2015-05-13 summary:Ordinal data are often seen in real applications. Regular multicategoryclassification methods are not designed for this data type and a more propertreatment is needed. We consider a framework of ordinal classification whichpools the results from binary classifiers together. An inherent difficulty ofthis framework is that the class prediction can be ambiguous due to boundarycrossing. To fix this issue, we propose a noncrossing ordinal classificationmethod which materializes the framework by imposing noncrossing constraints. Anasymptotic study of the proposed method is conducted. We show by simulated anddata examples that the proposed method can improve the classificationperformance for ordinal data without the ambiguity caused by boundarycrossings.
arxiv-11400-66 | MRF Optimization by Graph Approximation | http://arxiv.org/abs/1505.03365 | author:Wonsik Kim, Kyoung Mu Lee category:cs.CV published:2015-05-13 summary:Graph cuts-based algorithms have achieved great success in energyminimization for many computer vision applications. These algorithms provideapproximated solutions for multi-label energy functions via move-makingapproach. This approach fuses the current solution with a proposal to generatea lower-energy solution. Thus, generating the appropriate proposals isnecessary for the success of the move-making approach. However, not muchresearch efforts has been done on the generation of "good" proposals,especially for non-metric energy functions. In this paper, we propose anapplication-independent and energy-based approach to generate "good" proposals.With these proposals, we present a graph cuts-based move-making algorithmcalled GA-fusion (fusion with graph approximation-based proposals). Extensiveexperiments support that our proposal generation is effective across differentclasses of energy functions. The proposed algorithm outperforms others both onreal and synthetic problems.
arxiv-11400-67 | Hybrid data clustering approach using K-Means and Flower Pollination Algorithm | http://arxiv.org/abs/1505.03236 | author:R. Jensi, G. Wiselin Jiji category:cs.LG cs.IR cs.NE published:2015-05-13 summary:Data clustering is a technique for clustering set of objects into knownnumber of groups. Several approaches are widely applied to data clustering sothat objects within the clusters are similar and objects in different clustersare far away from each other. K-Means, is one of the familiar center basedclustering algorithms since implementation is very easy and fast convergence.However, K-Means algorithm suffers from initialization, hence trapped in localoptima. Flower Pollination Algorithm (FPA) is the global optimizationtechnique, which avoids trapping in local optimum solution. In this paper, anovel hybrid data clustering approach using Flower Pollination Algorithm andK-Means (FPAKM) is proposed. The proposed algorithm results are compared withK-Means and FPA on eight datasets. From the experimental results, FPAKM isbetter than FPA and K-Means.
arxiv-11400-68 | A Vision Based System for Monitoring the Loss of Attention in Automotive Drivers | http://arxiv.org/abs/1505.03352 | author:Anirban Dasgupta, Anjith George, S. L. Happy, Aurobinda Routray category:cs.CV published:2015-05-13 summary:On board monitoring of the alertness level of an automotive driver has been achallenging research in transportation safety and management. In this paper, wepropose a robust real time embedded platform to monitor the loss of attentionof the driver during day as well as night driving conditions. The PERcentage ofeye CLOSure (PERCLOS) has been used as the indicator of the alertness level. Inthis approach, the face is detected using Haar like features and tracked usinga Kalman Filter. The Eyes are detected using Principal Component Analysis (PCA)during day time and the block Local Binary Pattern (LBP) features during night.Finally the eye state is classified as open or closed using Support VectorMachines(SVM). In plane and off plane rotations of the drivers face have beencompensated using Affine and Perspective Transformation respectively.Compensation in illumination variation is carried out using Bi HistogramEqualization (BHE). The algorithm has been cross validated using brain signalsand finally been implemented on a Single Board Computer (SBC) having Intel Atomprocessor, 1 GB RAM, 1.66 GHz clock, x86 architecture, Windows Embedded XPoperating system. The system is found to be robust under actual drivingconditions.
arxiv-11400-69 | Feature selection using Fisher's ratio technique for automatic speech recognition | http://arxiv.org/abs/1505.03239 | author:Sarika Hegde, K. K. Achary, Surendra Shetty category:cs.CL published:2015-05-13 summary:Automatic Speech Recognition involves mainly two steps; feature extractionand classification . Mel Frequency Cepstral Coefficient is used as one of theprominent feature extraction techniques in ASR. Usually, the set of all 12 MFCCcoefficients is used as the feature vector in the classification step. But thequestion is whether the same or improved classification accuracy can beachieved by using a subset of 12 MFCC as feature vector. In this paper,Fisher's ratio technique is used for selecting a subset of 12 MFCC coefficientsthat contribute more in discriminating a pattern. The selected coefficients areused in classification with Hidden Markov Model algorithm. The classificationaccuracies that we get by using 12 coefficients and by using the selectedcoefficients are compared.
arxiv-11400-70 | APAC: Augmented PAttern Classification with Neural Networks | http://arxiv.org/abs/1505.03229 | author:Ikuro Sato, Hiroki Nishimura, Kensuke Yokoi category:cs.CV published:2015-05-13 summary:Deep neural networks have been exhibiting splendid accuracies in many ofvisual pattern classification problems. Many of the state-of-the-art methodsemploy a technique known as data augmentation at the training stage. This paperaddresses an issue of decision rule for classifiers trained with augmenteddata. Our method is named as APAC: the Augmented PAttern Classification, whichis a way of classification using the optimal decision rule for augmented datalearning. Discussion of methods of data augmentation is not our primary focus.We show clear evidences that APAC gives far better generalization performancethan the traditional way of class prediction in several experiments. Ourconvolutional neural network model with APAC achieved a state-of-the-artaccuracy on the MNIST dataset among non-ensemble classifiers. Even ourmultilayer perceptron model beats some of the convolutional models withrecently invented stochastic regularization techniques on the CIFAR-10 dataset.
arxiv-11400-71 | A Framework for Fast Face and Eye Detection | http://arxiv.org/abs/1505.03344 | author:Anjith George, Anirban Dasgupta, Aurobinda Routray category:cs.CV published:2015-05-13 summary:Face detection is an essential step in many computer vision applications likesurveillance, tracking, medical analysis, facial expression analysis etc.Several approaches have been made in the direction of face detection. Amongthem, Haar-like features based method is a robust method. In spite of therobustness, Haar - like features work with some limitations. However, with somesimple modifications in the algorithm, its performance can be made faster andmore robust. The present work refers to the increase in speed of operation ofthe original algorithm by down sampling the frames and its analysis withdifferent scale factors. It also discusses the detection of tilted faces usingan affine transformation of the input image.
arxiv-11400-72 | A Review Paper: Noise Models in Digital Image Processing | http://arxiv.org/abs/1505.03489 | author:Ajay Kumar Boyat, Brijendra Kumar Joshi category:cs.CV published:2015-05-13 summary:Noise is always presents in digital images during image acquisition, coding,transmission, and processing steps. Noise is very difficult to remove it fromthe digital images without the prior knowledge of noise model. That is why,review of noise models are essential in the study of image denoisingtechniques. In this paper, we express a brief overview of various noise models.These noise models can be selected by analysis of their origin. In this way, wepresent a complete and quantitative analysis of noise models available indigital images.
arxiv-11400-73 | Optimal linear estimation under unknown nonlinear transform | http://arxiv.org/abs/1505.03257 | author:Xinyang Yi, Zhaoran Wang, Constantine Caramanis, Han Liu category:stat.ML cs.IT math.IT published:2015-05-13 summary:Linear regression studies the problem of estimating a model parameter$\beta^* \in \mathbb{R}^p$, from $n$ observations$\{(y_i,\mathbf{x}_i)\}_{i=1}^n$ from linear model $y_i = \langle\mathbf{x}_i,\beta^* \rangle + \epsilon_i$. We consider a significantgeneralization in which the relationship between $\langle \mathbf{x}_i,\beta^*\rangle$ and $y_i$ is noisy, quantized to a single bit, potentially nonlinear,noninvertible, as well as unknown. This model is known as the single-indexmodel in statistics, and, among other things, it represents a significantgeneralization of one-bit compressed sensing. We propose a novel spectral-basedestimation procedure and show that we can recover $\beta^*$ in settings (i.e.,classes of link function $f$) where previous algorithms fail. In general, ouralgorithm requires only very mild restrictions on the (unknown) functionalrelationship between $y_i$ and $\langle \mathbf{x}_i,\beta^* \rangle$. We alsoconsider the high dimensional setting where $\beta^*$ is sparse ,and introducea two-stage nonconvex framework that addresses estimation challenges in highdimensional regimes where $p \gg n$. For a broad class of link functionsbetween $\langle \mathbf{x}_i,\beta^* \rangle$ and $y_i$, we establish minimaxlower bounds that demonstrate the optimality of our estimators in both theclassical and high dimensional regimes.
arxiv-11400-74 | PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Edge-Preserving Coherence | http://arxiv.org/abs/1505.03227 | author:Keze Wang, Liang Lin, Jiangbo Lu, Chenglong Li, Keyang Shi category:cs.CV 68U10 published:2015-05-13 summary:Driven by recent vision and graphics applications such as image segmentationand object recognition, computing pixel-accurate saliency values to uniformlyhighlight foreground objects becomes increasingly important. In this paper, wepropose a unified framework called PISA, which stands for Pixelwise ImageSaliency Aggregating various bottom-up cues and priors. It generates spatiallycoherent yet detail-preserving, pixel-accurate and fine-grained saliency, andovercomes the limitations of previous methods which use homogeneoussuperpixel-based and color only treatment. PISA aggregates multiple saliencycues in a global context such as complementary color and structure contrastmeasures with their spatial priors in the image domain. The saliency confidenceis further jointly modeled with a neighborhood consistence constraint into anenergy minimization formulation, in which each pixel will be evaluated withmultiple hypothetical saliency levels. Instead of using global discreteoptimization methods, we employ the cost-volume filtering technique to solveour formulation, assigning the saliency levels smoothly while preserving theedge-aware structure details. In addition, a faster version of PISA isdeveloped using a gradient-driven image sub-sampling strategy to greatlyimprove the runtime efficiency while keeping comparable detection accuracy.Extensive experiments on a number of public datasets suggest that PISAconvincingly outperforms other state-of-the-art approaches. In addition, withthis work we also create a new dataset containing $800$ commodity images forevaluating saliency detection. The dataset and source code of PISA can bedownloaded at http://vision.sysu.edu.cn/project/PISA/
arxiv-11400-75 | Mind the duality gap: safer rules for the Lasso | http://arxiv.org/abs/1505.03410 | author:Olivier Fercoq, Alexandre Gramfort, Joseph Salmon category:stat.ML cs.LG math.OC stat.CO published:2015-05-13 summary:Screening rules allow to early discard irrelevant variables from theoptimization in Lasso problems, or its derivatives, making solvers faster. Inthis paper, we propose new versions of the so-called $\textit{safe rules}$ forthe Lasso. Based on duality gap considerations, our new rules create safe testregions whose diameters converge to zero, provided that one relies on aconverging solver. This property helps screening out more variables, for awider range of regularization parameter values. In addition to fasterconvergence, we prove that we correctly identify the active sets (supports) ofthe solutions in finite time. While our proposed strategy can cope with anysolver, its performance is demonstrated using a coordinate descent algorithmparticularly adapted to machine learning use cases. Significant computing timereductions are obtained with respect to previous safe rules.
arxiv-11400-76 | On a spatial-temporal decomposition of the optical flow | http://arxiv.org/abs/1505.03505 | author:Aniello Raffale Patrone, Otmar Scherzer category:cs.CV published:2015-05-13 summary:In this paper we present the first variational spatial-temporal decompositionalgorithm for computation of the optical flow of a dynamic sequence. Weconsider several applications, such as the extraction of temporal motionpatterns of different scales and motion detection in dynamic sequences undervarying illumination conditions, such as they appear for instance inpsychological flickering experiments. In order to take into account variableillumination conditions we review the derivation, and modify, the optical flowequation. Concerning the numerical implementation, we propose a relaxationapproach for the adapted model such that the resulting optimality condition isan integro-differential equation, which is numerically solved by a fixed pointiteration. For comparison purposes we use the standard time dependent opticalflow algorithm from Weickert-Schn\"orr, which in contrast to our method,constitutes in solving a spatial-temporal differential equation.
arxiv-11400-77 | COROLA: A Sequential Solution to Moving Object Detection Using Low-rank Approximation | http://arxiv.org/abs/1505.03566 | author:Moein Shakeri, Hong Zhang category:cs.CV cs.RO published:2015-05-13 summary:Extracting moving objects from a video sequence and estimating the backgroundof each individual image are fundamental issues in many practical applicationssuch as visual surveillance, intelligent vehicle navigation, and trafficmonitoring. Recently, some methods have been proposed to detect moving objectsin a video via low-rank approximation and sparse outliers where the backgroundis modeled with the computed low-rank component of the video and the foregroundobjects are detected as the sparse outliers in the low-rank approximation. Allof these existing methods work in a batch manner, preventing them from beingapplied in real time and long duration tasks. In this paper, we present anonline sequential framework, namely contiguous outliers representation viaonline low-rank approximation (COROLA), to detect moving objects and learn thebackground model at the same time. We also show that our model can detectmoving objects with a moving camera. Our experimental evaluation uses simulateddata and real public datasets and demonstrates the superior performance ofCOROLA in terms of both accuracy and execution time.
arxiv-11400-78 | Modified Hausdorff Fractal Dimension (MHFD) | http://arxiv.org/abs/1505.03493 | author:Reza Farrahi Moghaddam, Mohamed Cheriet category:cs.CV published:2015-05-13 summary:The Hausdorff fractal dimension has been a fast-to-calculate method toestimate complexity of fractal shapes. In this work, a modified version of thisfractal dimension is presented in order to make it more robust when applied inestimating complexity of non-fractal images. The modified Hausdorff fractaldimension stands on two features that weaken the requirement of presence of ashape and also reduce the impact of the noise possibly presented in the inputimage. The new algorithm has been evaluated on a set of images of differentcharacter with promising performance.
arxiv-11400-79 | Brain Tumor Segmentation with Deep Neural Networks | http://arxiv.org/abs/1505.03540 | author:Mohammad Havaei, Axel Davy, David Warde-Farley, Antoine Biard, Aaron Courville, Yoshua Bengio, Chris Pal, Pierre-Marc Jodoin, Hugo Larochelle category:cs.CV cs.AI published:2015-05-13 summary:In this paper, we present a fully automatic brain tumor segmentation methodbased on Deep Neural Networks (DNNs). The proposed networks are tailored toglioblastomas (both low and high grade) pictured in MR images. By their verynature, these tumors can appear anywhere in the brain and have almost any kindof shape, size, and contrast. These reasons motivate our exploration of amachine learning solution that exploits a flexible, high capacity DNN whilebeing extremely efficient. Here, we give a description of different modelchoices that we've found to be necessary for obtaining competitive performance.We explore in particular different architectures based on Convolutional NeuralNetworks (CNN), i.e. DNNs specifically adapted to image data. We present a novel CNN architecture which differs from those traditionallyused in computer vision. Our CNN exploits both local features as well as moreglobal contextual features simultaneously. Also, different from mosttraditional uses of CNNs, our networks use a final layer that is aconvolutional implementation of a fully connected layer which allows a 40 foldspeed up. We also describe a 2-phase training procedure that allows us totackle difficulties related to the imbalance of tumor labels. Finally, weexplore a cascade architecture in which the output of a basic CNN is treated asan additional source of information for a subsequent CNN. Results reported onthe 2013 BRATS test dataset reveal that our architecture improves over thecurrently published state-of-the-art while being over 30 times faster.
arxiv-11400-80 | Bootstrapped Adaptive Threshold Selection for Statistical Model Selection and Estimation | http://arxiv.org/abs/1505.03511 | author:Kristofer E. Bouchard category:stat.ML published:2015-05-13 summary:A central goal of neuroscience is to understand how activity in the nervoussystem is related to features of the external world, or to features of thenervous system itself. A common approach is to model neural responses as aweighted combination of external features, or vice versa. The structure of themodel weights can provide insight into neural representations. Often, neuralinput-output relationships are sparse, with only a few inputs contributing tothe output. In part to account for such sparsity, structured regularizers areincorporated into model fitting optimization. However, by imposing priors,structured regularizers can make it difficult to interpret learned modelparameters. Here, we investigate a simple, minimally structured modelestimation method for accurate, unbiased estimation of sparse models based onBootstrapped Adaptive Threshold Selection followed by ordinary least-squaresrefitting (BoATS). Through extensive numerical investigations, we show thatthis method often performs favorably compared to L1 and L2 regularizers. Inparticular, for a variety of model distributions and noise levels, BoATS moreaccurately recovers the parameters of sparse models, leading to moreparsimonious explanations of outputs. Finally, we apply this method to the taskof decoding human speech production from ECoG recordings.
arxiv-11400-81 | Leveraging Image based Prior for Visual Place Recognition | http://arxiv.org/abs/1505.03205 | author:Tsukamoto Taisho, Tanaka Kanji category:cs.CV published:2015-05-13 summary:In this study, we propose a novel scene descriptor for visual placerecognition. Unlike popular bag-of-words scene descriptors which rely on alibrary of vector quantized visual features, our proposed descriptor is basedon a library of raw image data, such as publicly available photo collectionsfrom Google StreetView and Flickr. The library images need not to be associatedwith spatial information regarding the viewpoint and orientation of the scene.As a result, these images are cheaper than the database images; in addition,they are readily available. Our proposed descriptor directly mines the imagelibrary to discover landmarks (i.e., image patches) that suitably match aninput query/database image. The discovered landmarks are then compactlydescribed by their pose and shape (i.e., library image ID, bounding boxes) andused as a compact discriminative scene descriptor for the input image. Weevaluate the effectiveness of our scene description framework by comparing itsperformance to that of previous approaches.
arxiv-11400-82 | Loop-corrected belief propagation for lattice spin models | http://arxiv.org/abs/1505.03504 | author:Hai-Jun Zhou, Wei-Mou Zheng category:cs.CV published:2015-05-13 summary:Belief propagation (BP) is a message-passing method for solving probabilisticgraphical models. It is very successful in treating disordered models (such asspin glasses) on random graphs. On the other hand, finite-dimensional latticemodels have an abundant number of short loops, and the BP method is still farfrom being satisfactory in treating the complicated loop-induced correlationsin these systems. Here we propose a loop-corrected BP method to take intoaccount the effect of short loops in lattice spin models. We demonstrate,through an application to the square-lattice Ising model, that loop-correctedBP improves over the naive BP method significantly. We also implementloop-corrected BP at the coarse-grained region graph level to further boost itsperformance.
arxiv-11400-83 | Removing systematic errors for exoplanet search via latent causes | http://arxiv.org/abs/1505.03036 | author:Bernhard Schölkopf, David W. Hogg, Dun Wang, Daniel Foreman-Mackey, Dominik Janzing, Carl-Johann Simon-Gabriel, Jonas Peters category:stat.ML astro-ph.EP astro-ph.IM cs.LG published:2015-05-12 summary:We describe a method for removing the effect of confounders in order toreconstruct a latent quantity of interest. The method, referred to ashalf-sibling regression, is inspired by recent work in causal inference usingadditive noise models. We provide a theoretical justification and illustratethe potential of the method in a challenging astronomy application.
arxiv-11400-84 | Turn Segmentation into Utterances for Arabic Spontaneous Dialogues and Instance Messages | http://arxiv.org/abs/1505.03081 | author:AbdelRahim A. Elmadany, Sherif M. Abdou, Mervat Gheith category:cs.CL published:2015-05-12 summary:Text segmentation task is an essential processing task for many of NaturalLanguage Processing (NLP) such as text summarization, text translation,dialogue language understanding, among others. Turns segmentation consideredthe key player in dialogue understanding task for building automaticHuman-Computer systems. In this paper, we introduce a novel approach to turnsegmentation into utterances for Egyptian spontaneous dialogues and InstanceMessages (IM) using Machine Learning (ML) approach as a part of automaticunderstanding Egyptian spontaneous dialogues and IM task. Due to the lack ofEgyptian dialect dialogue corpus the system evaluated by our corpus includes3001 turns, which are collected, segmented, and annotated manually fromEgyptian call-centers. The system achieves F1 scores of 90.74% and accuracy of95.98%.
arxiv-11400-85 | A Survey of Arabic Dialogues Understanding for Spontaneous Dialogues and Instant Message | http://arxiv.org/abs/1505.03084 | author:AbdelRahim A. Elmadany, Sherif M. Abdou, Mervat Gheith category:cs.CL published:2015-05-12 summary:Building dialogues systems interaction has recently gained considerableattention, but most of the resources and systems built so far are tailored toEnglish and other Indo-European languages. The need for designing systems forother languages is increasing such as Arabic language. For this reasons, thereare more interest for Arabic dialogue acts classification task because it a keyplayer in Arabic language understanding to building this systems. This papersurveys different techniques for dialogue acts classification for Arabic. Wedescribe the main existing techniques for utterances segmentations andclassification, annotation schemas, and test corpora for Arabic dialoguesunderstanding that have introduced in the literature
arxiv-11400-86 | Incorporating Type II Error Probabilities from Independence Tests into Score-Based Learning of Bayesian Network Structure | http://arxiv.org/abs/1505.02870 | author:Eliot Brenner, David Sontag category:cs.LG stat.ML published:2015-05-12 summary:We give a new consistent scoring function for structure learning of Bayesiannetworks. In contrast to traditional approaches to score-based structurelearning, such as BDeu or MDL, the complexity penalty that we propose isdata-dependent and is given by the probability that a conditional independencetest correctly shows that an edge cannot exist. What really distinguishes thisnew scoring function from earlier work is that it has the property of becomingcomputationally easier to maximize as the amount of data increases. We prove apolynomial sample complexity result, showing that maximizing this score isguaranteed to correctly learn a structure with no false edges and adistribution close to the generating distribution, whenever there exists aBayesian network which is a perfect map for the data generating distribution.Although the new score can be used with any search algorithm, in our relatedUAI 2013 paper [BS13], we have given empirical results showing that it isparticularly effective when used together with a linear programming relaxationapproach to Bayesian network structure learning. The present paper contains alldetails of the proofs of the finite-sample complexity results in [BS13] as wellas detailed explanation of the computation of the certain error probabilitiescalled beta-values, whose precomputation and tabulation is necessary for theimplementation of the algorithm in [BS13].
arxiv-11400-87 | Automatic Script Identification in the Wild | http://arxiv.org/abs/1505.02982 | author:Baoguang Shi, Cong Yao, Chengquan Zhang, Xiaowei Guo, Feiyue Huang, Xiang Bai category:cs.CV published:2015-05-12 summary:With the rapid increase of transnational communication and cooperation,people frequently encounter multilingual scenarios in various situations. Inthis paper, we are concerned with a relatively new problem: scriptidentification at word or line levels in natural scenes. A large-scale datasetwith a great quantity of natural images and 10 types of widely used languagesis constructed and released. In allusion to the challenges in scriptidentification in real-world scenarios, a deep learning based algorithm isproposed. The experiments on the proposed dataset demonstrate that ouralgorithm achieves superior performance, compared with conventional imageclassification methods, such as the original CNN architecture and LLC.
arxiv-11400-88 | Detecting the large entries of a sparse covariance matrix in sub-quadratic time | http://arxiv.org/abs/1505.03001 | author:Ofer Shwartz, Boaz Nadler category:stat.CO cs.LG stat.ML published:2015-05-12 summary:The covariance matrix of a $p$-dimensional random variable is a fundamentalquantity in data analysis. Given $n$ i.i.d. observations, it is typicallyestimated by the sample covariance matrix, at a computational cost of$O(np^{2})$ operations. When $n,p$ are large, this computation may beprohibitively slow. Moreover, in several contemporary applications, thepopulation matrix is approximately sparse, and only its few large entries areof interest. This raises the following question, at the focus of our work:Assuming approximate sparsity of the covariance matrix, can its large entriesbe detected much faster, say in sub-quadratic time, without explicitlycomputing all its $p^{2}$ entries? In this paper, we present and theoreticallyanalyze two randomized algorithms that detect the large entries of anapproximately sparse sample covariance matrix using only $O(np\text{ poly log }p)$ operations. Furthermore, assuming sparsity of the population matrix, wederive sufficient conditions on the underlying random variable and on thenumber of samples $n$, for the sample covariance matrix to satisfy ourapproximate sparsity requirements. Finally, we illustrate the performance ofour algorithms via several simulations.
arxiv-11400-89 | Monocular Object Instance Segmentation and Depth Ordering with CNNs | http://arxiv.org/abs/1505.03159 | author:Ziyu Zhang, Alexander G. Schwing, Sanja Fidler, Raquel Urtasun category:cs.CV published:2015-05-12 summary:In this paper we tackle the problem of instance-level segmentation and depthordering from a single monocular image. Towards this goal, we take advantage ofconvolutional neural nets and train them to directly predict instance-levelsegmentations where the instance ID encodes the depth ordering within imagepatches. To provide a coherent single explanation of an image we develop aMarkov random field which takes as input the predictions of convolutionalneural nets applied at overlapping patches of different resolutions, as well asthe output of a connected component algorithm. It aims to predict accurateinstance-level segmentation and depth ordering. We demonstrate theeffectiveness of our approach on the challenging KITTI benchmark and show goodperformance on both tasks.
arxiv-11400-90 | Comparing methods for Twitter Sentiment Analysis | http://arxiv.org/abs/1505.02973 | author:Evangelos Psomakelis, Konstantinos Tserpes, Dimosthenis Anagnostopoulos, Theodora Varvarigou category:cs.CL cs.IR cs.SI published:2015-05-12 summary:This work extends the set of works which deal with the popular problem ofsentiment analysis in Twitter. It investigates the most popular document("tweet") representation methods which feed sentiment evaluation mechanisms. Inparticular, we study the bag-of-words, n-grams and n-gram graphs approaches andfor each of them we evaluate the performance of a lexicon-based and 7learning-based classification algorithms (namely SVM, Na\"ive BayesianNetworks, Logistic Regression, Multilayer Perceptrons, Best-First Trees,Functional Trees and C4.5) as well as their combinations, using a set of 4451manually annotated tweets. The results demonstrate the superiority oflearning-based methods and in particular of n-gram graphs approaches forpredicting the sentiment of tweets. They also show that the combinatoryapproach has impressive effects on n-grams, raising the confidence up to 83.15%on the 5-Grams, using majority vote and a balanced dataset (equal number ofpositive, negative and neutral tweets for training). In the n-gram graph casesthe improvement was small to none, reaching 94.52% on the 4-gram graphs, usingOrthodromic distance and a threshold of 0.001.
arxiv-11400-91 | Sparse 3D convolutional neural networks | http://arxiv.org/abs/1505.02890 | author:Ben Graham category:cs.CV published:2015-05-12 summary:We have implemented a convolutional neural network designed for processingsparse three-dimensional input data. The world we live in is three dimensionalso there are a large number of potential applications including 3D objectrecognition and analysis of space-time objects. In the quest for efficiency, weexperiment with CNNs on the 2D triangular-lattice and 3D tetrahedral-lattice.
arxiv-11400-92 | Sentiment Analysis For Modern Standard Arabic And Colloquial | http://arxiv.org/abs/1505.03105 | author:Hossam S. Ibrahim, Sherif M. Abdou, Mervat Gheith category:cs.CL published:2015-05-12 summary:The rise of social media such as blogs and social networks has fueledinterest in sentiment analysis. With the proliferation of reviews, ratings,recommendations and other forms of online expression, online opinion has turnedinto a kind of virtual currency for businesses looking to market theirproducts, identify new opportunities and manage their reputations, thereforemany are now looking to the field of sentiment analysis. In this paper, wepresent a feature-based sentence level approach for Arabic sentiment analysis.Our approach is using Arabic idioms/saying phrases lexicon as a key importancefor improving the detection of the sentiment polarity in Arabic sentences aswell as a number of novels and rich set of linguistically motivated featurescontextual Intensifiers, contextual Shifter and negation handling), syntacticfeatures for conflicting phrases which enhance the sentiment classificationaccuracy. Furthermore, we introduce an automatic expandable wide coveragepolarity lexicon of Arabic sentiment words. The lexicon is built withgold-standard sentiment words as a seed which is manually collected andannotated and it expands and detects the sentiment orientation automatically ofnew sentiment words using synset aggregation technique and free online Arabiclexicons and thesauruses. Our data focus on modern standard Arabic (MSA) andEgyptian dialectal Arabic tweets and microblogs (hotel reservation, productreviews, etc.). The experimental results using our resources and techniqueswith SVM classifier indicate high performance levels, with accuracies of over95%.
arxiv-11400-93 | Improving Computer-aided Detection using Convolutional Neural Networks and Random View Aggregation | http://arxiv.org/abs/1505.03046 | author:Holger R. Roth, Le Lu, Jiamin Liu, Jianhua Yao, Ari Seff, Kevin Cherry, Lauren Kim, Ronald M. Summers category:cs.CV published:2015-05-12 summary:Automated computer-aided detection (CADe) in medical imaging has been animportant tool in clinical practice and research. State-of-the-art methodsoften show high sensitivities but at the cost of high false-positives (FP) perpatient rates. We design a two-tiered coarse-to-fine cascade framework thatfirst operates a candidate generation system at sensitivities of $\sim$100% butat high FP levels. By leveraging existing CAD systems, coordinates of regionsor volumes of interest (ROI or VOI) for lesion candidates are generated in thisstep and function as input for a second tier, which is our focus in this study.In this second stage, we generate $N$ 2D (two-dimensional) or 2.5D views viasampling through scale transformations, random translations and rotations withrespect to each ROI's centroid coordinates. These random views are used totrain deep convolutional neural network (ConvNet) classifiers. In testing, thetrained ConvNets are employed to assign class (e.g., lesion, pathology)probabilities for a new set of $N$ random views that are then averaged at eachROI to compute a final per-candidate classification probability. This secondtier behaves as a highly selective process to reject difficult false positiveswhile preserving high sensitivities. The methods are evaluated on threedifferent data sets with different numbers of patients: 59 patients forsclerotic metastases detection, 176 patients for lymph node detection, and1,186 patients for colonic polyp detection. Experimental results show theability of ConvNets to generalize well to different medical imaging CADeapplications and scale elegantly to various data sets. Our proposed methodsimprove CADe performance markedly in all cases. CADe sensitivities improvedfrom 57% to 70%, from 43% to 77% and from 58% to 75% at 3 FPs per patient forsclerotic metastases, lymph nodes and colonic polyps, respectively.
arxiv-11400-94 | The Boundary Forest Algorithm for Online Supervised and Unsupervised Learning | http://arxiv.org/abs/1505.02867 | author:Charles Mathy, Nate Derbinsky, José Bento, Jonathan Rosenthal, Jonathan Yedidia category:cs.LG cs.DS cs.IR stat.ML published:2015-05-12 summary:We describe a new instance-based learning algorithm called the BoundaryForest (BF) algorithm, that can be used for supervised and unsupervisedlearning. The algorithm builds a forest of trees whose nodes store previouslyseen examples. It can be shown data points one at a time and updates itselfincrementally, hence it is naturally online. Few instance-based algorithms havethis property while being simultaneously fast, which the BF is. This is crucialfor applications where one needs to respond to input data in real time. Thenumber of children of each node is not set beforehand but obtained from thetraining procedure, which makes the algorithm very flexible with regards towhat data manifolds it can learn. We test its generalization performance andspeed on a range of benchmark datasets and detail in which settings itoutperforms the state of the art. Empirically we find that training time scalesas O(DNlog(N)) and testing as O(Dlog(N)), where D is the dimensionality and Nthe amount of data,
arxiv-11400-95 | A new Level-set based Protocol for Accurate Bone Segmentation from CT Imaging | http://arxiv.org/abs/1505.03093 | author:Manuel Pinheiro, J. L. Alves category:physics.med-ph cs.CV published:2015-05-12 summary:In this work it is proposed a medical image segmentation pipeline foraccurate bone segmentation from CT imaging. It is a two-step methodology, witha pre-segmentation step and a segmentation refinement step. First, the userperforms a rough segmenting of the desired region of interest. Next, a fullyautomatic refinement step is applied to the pre-segmented data. The automaticsegmentation refinement is composed by several sub-stpng, namely imagedeconvolution, image cropping and interpolation. The user-definedpre-segmentation is then refined over the deconvolved, cropped, and up-sampledversion of the image. The algorithm is applied in the segmentation of CT imagesof a composite femur bone, reconstructed with different reconstructionprotocols. Segmentation outcomes are validated against a gold standard modelobtained with coordinate measuring machine Nikon Metris LK V20 with a digitalline scanner LC60-D that guarantees an accuracy of 28 $\mu m$. High sub-pixelaccuracy models were obtained for all tested Datasets. The algorithm is able toproduce high quality segmentation of the composite femur regardless of thesurface meshing strategy used.
arxiv-11400-96 | Asymptotic Behavior of Minimal-Exploration Allocation Policies: Almost Sure, Arbitrarily Slow Growing Regret | http://arxiv.org/abs/1505.02865 | author:Wesley Cowan, Michael N. Katehakis category:stat.ML cs.LG 62L10 published:2015-05-12 summary:The purpose of this paper is to provide further understanding into thestructure of the sequential allocation ("stochastic multi-armed bandit", orMAB) problem by establishing probability one finite horizon bounds andconvergence rates for the sample (or "pseudo") regret associated with twosimple classes of allocation policies $\pi$. For any slowly increasing function $g$, subject to mild regularityconstraints, we construct two policies (the $g$-Forcing, and the $g$-InflatedSample Mean) that achieve a measure of regret of order $ O(g(n))$ almost surelyas $n \to \infty$, bound from above and below. Additionally, almost sure upperand lower bounds on the remainder term are established. In the constructionsherein, the function $g$ effectively controls the "exploration" of theclassical "exploration/exploitation" tradeoff.
arxiv-11400-97 | Permutational Rademacher Complexity: a New Complexity Measure for Transductive Learning | http://arxiv.org/abs/1505.02910 | author:Ilya Tolstikhin, Nikita Zhivotovskiy, Gilles Blanchard category:stat.ML cs.LG published:2015-05-12 summary:Transductive learning considers situations when a learner observes $m$labelled training points and $u$ unlabelled test points with the final goal ofgiving correct answers for the test points. This paper introduces a newcomplexity measure for transductive learning called Permutational RademacherComplexity (PRC) and studies its properties. A novel symmetrization inequalityis proved, which shows that PRC provides a tighter control over expectedsuprema of empirical processes compared to what happens in the standard i.i.d.setting. A number of comparison results are also provided, which show therelation between PRC and other popular complexity measures used in statisticallearning theory, including Rademacher complexity and Transductive RademacherComplexity (TRC). We argue that PRC is a more suitable complexity measure fortransductive learning. Finally, these results are combined with a standardconcentration argument to provide novel data-dependent risk bounds fortransductive learning.
arxiv-11400-98 | Indonesian Social Media Sentiment Analysis With Sarcasm Detection | http://arxiv.org/abs/1505.03085 | author:Edwin Lunando, Ayu Purwarianti category:cs.CL published:2015-05-12 summary:Sarcasm is considered one of the most difficult problem in sentimentanalysis. In our ob-servation on Indonesian social media, for cer-tain topics,people tend to criticize something using sarcasm. Here, we proposed twoadditional features to detect sarcasm after a common sentiment analysis isconducted. The features are the negativity information and the number ofinterjection words. We also employed translated SentiWordNet in the sentimentclassification. All the classifications were conducted with machine learningalgorithms. The experimental results showed that the additional features arequite effective in the sarcasm detection.
arxiv-11400-99 | How Far Can You Get By Combining Change Detection Algorithms? | http://arxiv.org/abs/1505.02921 | author:Simone Bianco, Gianluigi Ciocca, Raimondo Schettini category:cs.CV I.4.8; G.1.6 published:2015-05-12 summary:In this paper we investigate how state-of-the-art change detection algorithmscan be combined and used to create a more robust change algorithm leveragingtheir individual peculiarities. We exploited Genetic Programming (GP) toautomatically select the best algorithms, combine them in different ways, andperform the most suitable post-processing operations on the outputs of thealgorithms. In particular, algorithms' combination and post-processingoperations are achieved with unary, binary and $n$-ary functions embedded intothe GP framework. Using different experimental settings for combining existingalgorithms we obtained different GP solutions that we termed IUTIS (In UnityThere Is Strength). These solutions are then compared against state-of-the-artchange detection algorithms on the video sequences and ground truth annotationsof the ChandeDetection.net (CDNET 2014) challenge. Results demonstrate thatusing GP, our solutions are able to outperform all the considered singlestate-of-the-art change detection algorithms, as well as other combinationstrategies.
arxiv-11400-100 | Foundational principles for large scale inference: Illustrations through correlation mining | http://arxiv.org/abs/1505.02475 | author:Alfred O. Hero, Bala Rajaratnam category:math.ST stat.ML stat.TH published:2015-05-11 summary:When can reliable inference be drawn in the "Big Data" context? This paperpresents a framework for answering this fundamental question in the context ofcorrelation mining, with implications for general large scale inference. Inlarge scale data applications like genomics, connectomics, and eco-informaticsthe dataset is often variable-rich but sample-starved: a regime where thenumber $n$ of acquired samples (statistical replicates) is far fewer than thenumber $p$ of observed variables (genes, neurons, voxels, or chemicalconstituents). Much of recent work has focused on understanding thecomputational complexity of proposed methods for "Big Data." Sample complexityhowever has received relatively less attention, especially in the setting whenthe sample size $n$ is fixed, and the dimension $p$ grows without bound. Toaddress this gap, we develop a unified statistical framework that explicitlyquantifies the sample complexity of various inferential tasks. Sampling regimescan be divided into several categories: 1) the classical asymptotic regimewhere the variable dimension is fixed and the sample size goes to infinity; 2)the mixed asymptotic regime where both variable dimension and sample size go toinfinity at comparable rates; 3) the purely high dimensional asymptotic regimewhere the variable dimension goes to infinity and the sample size is fixed.Each regime has its niche but only the latter regime applies to exa-scale datadimension. We illustrate this high dimensional framework for the problem ofcorrelation mining, where it is the matrix of pairwise and partial correlationsamong the variables that are of interest. We demonstrate various regimes ofcorrelation mining based on the unifying perspective of high dimensionallearning rates and sample complexity for different structured covariance modelsand different inference tasks.
arxiv-11400-101 | Sample complexity of learning Mahalanobis distance metrics | http://arxiv.org/abs/1505.02729 | author:Nakul Verma, Kristin Branson category:cs.LG cs.AI stat.ML published:2015-05-11 summary:Metric learning seeks a transformation of the feature space that enhancesprediction quality for the given task at hand. In this work we providePAC-style sample complexity rates for supervised metric learning. We givematching lower- and upper-bounds showing that the sample complexity scales withthe representation dimension when no assumptions are made about the underlyingdata distribution. However, by leveraging the structure of the datadistribution, we show that one can achieve rates that are fine-tuned to aspecific notion of intrinsic complexity for a given dataset. Our analysisreveals that augmenting the metric learning optimization criterion with asimple norm-based regularization can help adapt to a dataset's intrinsiccomplexity, yielding better generalization. Experiments on benchmark datasetsvalidate our analysis and show that regularizing the metric can help discernthe signal even when the data contains high amounts of noise.
arxiv-11400-102 | An Online Learning Algorithm for Neuromorphic Hardware Implementation | http://arxiv.org/abs/1505.02495 | author:Chetan Singh Thakur, Runchun Wang, Saeed Afshar, Tara Julia Hamilton, Jonathan Tapson, Andre van Schaik category:cs.NE published:2015-05-11 summary:We propose a sign-based online learning (SOL) algorithm for a neuromorphichardware framework called Trainable Analogue Block (TAB). The TAB frameworkutilises the principles of neural population coding, implying that it encodesthe input stimulus using a large pool of nonlinear neurons. The SOL algorithmis a simple weight update rule that employs the sign of the hidden layeractivation and the sign of the output error, which is the difference betweenthe target output and the predicted output. The SOL algorithm is easilyimplementable in hardware, and can be used in any artificial neural networkframework that learns weights by minimising a convex cost function. We showthat the TAB framework can be trained for various regression tasks using theSOL algorithm.
arxiv-11400-103 | On Markov chain Monte Carlo methods for tall data | http://arxiv.org/abs/1505.02827 | author:Rémi Bardenet, Arnaud Doucet, Chris Holmes category:stat.ME stat.CO stat.ML published:2015-05-11 summary:Markov chain Monte Carlo methods are often deemed too computationallyintensive to be of any practical use for big data applications, and inparticular for inference on datasets containing a large number $n$ ofindividual data points, also known as tall datasets. In scenarios where dataare assumed independent, various approaches to scale up the Metropolis-Hastingsalgorithm in a Bayesian inference context have been recently proposed inmachine learning and computational statistics. These approaches can be groupedinto two categories: divide-and-conquer approaches and, subsampling-basedalgorithms. The aims of this article are as follows. First, we present acomprehensive review of the existing literature, commenting on the underlyingassumptions and theoretical guarantees of each method. Second, by leveragingour understanding of these limitations, we propose an originalsubsampling-based approach which samples from a distribution provably close tothe posterior distribution of interest, yet can require less than $O(n)$ datapoint likelihood evaluations at each iteration for certain statistical modelsin favourable scenarios. Finally, we have only been able so far to proposesubsampling-based methods which display good performance in scenarios where theBernstein-von Mises approximation of the target posterior distribution isexcellent. It remains an open challenge to develop such methods in scenarioswhere the Bernstein-von Mises approximation is poor.
arxiv-11400-104 | Training Deeper Convolutional Networks with Deep Supervision | http://arxiv.org/abs/1505.02496 | author:Liwei Wang, Chen-Yu Lee, Zhuowen Tu, Svetlana Lazebnik category:cs.CV published:2015-05-11 summary:One of the most promising ways of improving the performance of deepconvolutional neural networks is by increasing the number of convolutionallayers. However, adding layers makes training more difficult andcomputationally expensive. In order to train deeper networks, we propose to addauxiliary supervision branches after certain intermediate layers duringtraining. We formulate a simple rule of thumb to determine where these branchesshould be added. The resulting deeply supervised structure makes the trainingmuch easier and also produces better classification results on ImageNet and therecently released, larger MIT Places dataset
arxiv-11400-105 | A Two-Layer Local Constrained Sparse Coding Method for Fine-Grained Visual Categorization | http://arxiv.org/abs/1505.02505 | author:Guo Lihua, Guo Chenggan category:cs.CV 68T45 I.4.10 published:2015-05-11 summary:Fine-grained categories are more difficulty distinguished than genericcategories due to the similarity of inter-class and the diversity ofintra-class. Therefore, the fine-grained visual categorization (FGVC) isconsidered as one of challenge problems in computer vision recently. A newfeature learning framework, which is based on a two-layer local constrainedsparse coding architecture, is proposed in this paper. The two-layerarchitecture is introduced for learning intermediate-level features, and thelocal constrained term is applied to guarantee the local smooth of codingcoefficients. For extracting more discriminative information, local orientationhistograms are the input of sparse coding instead of raw pixels. Moreover, aquick dictionary updating process is derived to further improve the trainingspeed. Two experimental results show that our method achieves 85.29% accuracyon the Oxford 102 flowers dataset and 67.8% accuracy on the CUB-200-2011 birddataset, and the performance of our framework is highly competitive withexisting literatures.
arxiv-11400-106 | Removing Camera Shake via Weighted Fourier Burst Accumulation | http://arxiv.org/abs/1505.02731 | author:Mauricio Delbracio, Guillermo Sapiro category:cs.CV published:2015-05-11 summary:Numerous recent approaches attempt to remove image blur due to camera shake,either with one or multiple input images, by explicitly solving an inverse andinherently ill-posed deconvolution problem. If the photographer takes a burstof images, a modality available in virtually all modern digital cameras, weshow that it is possible to combine them to get a clean sharp version. This isdone without explicitly solving any blur estimation and subsequent inverseproblem. The proposed algorithm is strikingly simple: it performs a weightedaverage in the Fourier domain, with weights depending on the Fourier spectrummagnitude. The method can be seen as a generalization of the align and averageprocedure, with a weighted average, motivated by hand-shake physiology andtheoretically supported, taking place in the Fourier domain. The method'srationale is that camera shake has a random nature and therefore each image inthe burst is generally blurred differently. Experiments with real camera data,and extensive comparisons, show that the proposed Fourier Burst Accumulation(FBA) algorithm achieves state-of-the-art results an order of magnitude faster,with simplicity for on-board implementation on camera phones. Finally, we alsopresent experiments in real high dynamic range (HDR) scenes, showing how themethod can be straightforwardly extended to HDR photography.
arxiv-11400-107 | Improving neural networks with bunches of neurons modeled by Kumaraswamy units: Preliminary study | http://arxiv.org/abs/1505.02581 | author:Jakub Mikolaj Tomczak category:cs.LG cs.NE published:2015-05-11 summary:Deep neural networks have recently achieved state-of-the-art results in manymachine learning problems, e.g., speech recognition or object recognition.Hitherto, work on rectified linear units (ReLU) provides empirical andtheoretical evidence on performance increase of neural networks comparing totypically used sigmoid activation function. In this paper, we investigate a newmanner of improving neural networks by introducing a bunch of copies of thesame neuron modeled by the generalized Kumaraswamy distribution. As a result,we propose novel non-linear activation function which we refer to asKumaraswamy unit which is closely related to ReLU. In the experimental studywith MNIST image corpora we evaluate the Kumaraswamy unit applied tosingle-layer (shallow) neural network and report a significant drop in testclassification error and test cross-entropy in comparison to sigmoid unit, ReLUand Noisy ReLU.
arxiv-11400-108 | Soft-Deep Boltzmann Machines | http://arxiv.org/abs/1505.02462 | author:Taichi Kiwaki category:cs.NE cs.LG stat.ML published:2015-05-11 summary:We present a layered Boltzmann machine (BM) that can better exploit theadvantages of a distributed representation. It is widely believed that deep BMs(DBMs) have far greater representational power than its shallow counterpart,restricted Boltzmann machines (RBMs). However, this expectation on thesupremacy of DBMs over RBMs has not ever been validated in a theoreticalfashion. In this paper, we provide both theoretical and empirical evidencesthat the representational power of DBMs can be actually rather limited intaking advantages of distributed representations. We propose an approximatemeasure for the representational power of a BM regarding to the efficiency of adistributed representation. With this measure, we show a surprising fact thatDBMs can make inefficient use of distributed representations. Based on theseobservations, we propose an alternative BM architecture, which we dub soft-deepBMs (sDBMs). We show that sDBMs can more efficiently exploit the distributedrepresentations in terms of the measure. Experiments demonstrate that sDBMsoutperform several state-of-the-art models, including DBMs, in generative taskson binarized MNIST and Caltech-101 silhouettes.
arxiv-11400-109 | Improved Relation Extraction with Feature-Rich Compositional Embedding Models | http://arxiv.org/abs/1505.02419 | author:Matthew R. Gormley, Mo Yu, Mark Dredze category:cs.CL cs.AI cs.LG published:2015-05-10 summary:Compositional embedding models build a representation (or embedding) for alinguistic structure based on its component word embeddings. We propose aFeature-rich Compositional Embedding Model (FCM) for relation extraction thatis expressive, generalizes to new domains, and is easy-to-implement. The keyidea is to combine both (unlexicalized) hand-crafted features with learned wordembeddings. The model is able to directly tackle the difficulties met bytraditional compositional embeddings models, such as handling arbitrary typesof sentence annotations and utilizing global information for composition. Wetest the proposed model on two relation extraction tasks, and demonstrate thatour model outperforms both previous compositional models and traditionalfeature rich models on the ACE 2005 relation extraction task, and the SemEval2010 relation classification task. The combination of our model and alog-linear classifier with hand-crafted features gives state-of-the-artresults.
arxiv-11400-110 | Bayesian Sparse Tucker Models for Dimension Reduction and Tensor Completion | http://arxiv.org/abs/1505.02343 | author:Qibin Zhao, Liqing Zhang, Andrzej Cichocki category:cs.LG cs.NA stat.ML published:2015-05-10 summary:Tucker decomposition is the cornerstone of modern machine learning ontensorial data analysis, which have attracted considerable attention formultiway feature extraction, compressive sensing, and tensor completion. Themost challenging problem is related to determination of model complexity (i.e.,multilinear rank), especially when noise and missing data are present. Inaddition, existing methods cannot take into account uncertainty information oflatent factors, resulting in low generalization performance. To address theseissues, we present a class of probabilistic generative Tucker models for tensordecomposition and completion with structural sparsity over multilinear latentspace. To exploit structural sparse modeling, we introduce two group sparsityinducing priors by hierarchial representation of Laplace and Student-tdistributions, which facilitates fully posterior inference. For model learning,we derived variational Bayesian inferences over all model (hyper)parameters,and developed efficient and scalable algorithms based on multilinearoperations. Our methods can automatically adapt model complexity and infer anoptimal multilinear rank by the principle of maximum lower bound of modelevidence. Experimental results and comparisons on synthetic, chemometrics andneuroimaging data demonstrate remarkable performance of our models forrecovering ground-truth of multilinear rank and missing entries.
arxiv-11400-111 | Spike and Slab Gaussian Process Latent Variable Models | http://arxiv.org/abs/1505.02434 | author:Zhenwen Dai, James Hensman, Neil Lawrence category:stat.ML cs.LG published:2015-05-10 summary:The Gaussian process latent variable model (GP-LVM) is a popular approach tonon-linear probabilistic dimensionality reduction. One design choice for themodel is the number of latent variables. We present a spike and slab prior forthe GP-LVM and propose an efficient variational inference procedure that givesa lower bound of the log marginal likelihood. The new model provides a moreprincipled approach for selecting latent dimensions than the standard way ofthresholding the length-scale parameters. The effectiveness of our approach isdemonstrated through experiments on real and simulated data. Further, we extendmulti-view Gaussian processes that rely on sharing latent dimensions (known asmanifold relevance determination) with spike and slab priors. This allows amore principled approach for selecting a subset of the latent space for eachview of data. The extended model outperforms the previous state-of-the-art whenapplied to a cross-modal multimedia retrieval task.
arxiv-11400-112 | Towards stability and optimality in stochastic gradient descent | http://arxiv.org/abs/1505.02417 | author:Panos Toulis, Dustin Tran, Edoardo M. Airoldi category:stat.ME cs.LG stat.CO stat.ML published:2015-05-10 summary:Iterative procedures for parameter estimation based on stochastic gradientdescent allow the estimation to scale to massive data sets. However, in boththeory and practice, they suffer from numerical instability. Moreover, they arestatistically inefficient as estimators of the true parameter value. To addressthese two issues, we propose a new iterative procedure termed averaged implicitstochastic gradient descent (AI-SGD). For statistical efficiency, AISGD employsaveraging of the iterates, which achieves the optimal Cram\'{e}r-Rao boundunder strong convexity, i.e., it is an optimal unbiased estimator of the trueparameter value. For numerical stability, AISGD employs an implicit update ateach iteration, which is related to proximal operators in optimization. Inpractice, AISGD achieves competitive performance with state-of-the-artprocedures. Furthermore, it is more stable than averaging procedures that donot employ proximal operators, and is simpler to implement than procedures thatdo employ proximal operators but require careful tuning of severalhyperparameters.
arxiv-11400-113 | Bounded-Distortion Metric Learning | http://arxiv.org/abs/1505.02377 | author:Renjie Liao, Jianping Shi, Ziyang Ma, Jun Zhu, Jiaya Jia category:cs.LG published:2015-05-10 summary:Metric learning aims to embed one metric space into another to benefit taskslike classification and clustering. Although a greatly distorted metric spacehas a high degree of freedom to fit training data, it is prone to overfittingand numerical inaccuracy. This paper presents {\it bounded-distortion metriclearning} (BDML), a new metric learning framework which amounts to finding anoptimal Mahalanobis metric space with a bounded-distortion constraint. Anefficient solver based on the multiplicative weights update method is proposed.Moreover, we generalize BDML to pseudo-metric learning and devise thesemidefinite relaxation and a randomized algorithm to approximately solve it.We further provide theoretical analysis to show that distortion is a keyingredient for stability and generalization ability of our BDML algorithm.Extensive experiments on several benchmark datasets yield promising results.
arxiv-11400-114 | Fast Rhetorical Structure Theory Discourse Parsing | http://arxiv.org/abs/1505.02425 | author:Michael Heilman, Kenji Sagae category:cs.CL published:2015-05-10 summary:In recent years, There has been a variety of research on discourse parsing,particularly RST discourse parsing. Most of the recent work on RST parsing hasfocused on implementing new types of features or learning algorithms in orderto improve accuracy, with relatively little focus on efficiency, robustness, orpractical use. Also, most implementations are not widely available. Here, wedescribe an RST segmentation and parsing system that adapts models and featuresets from various previous work, as described below. Its accuracy is nearstate-of-the-art, and it was developed to be fast, robust, and practical. Forexample, it can process short documents such as news articles or essays in lessthan a second.
arxiv-11400-115 | Deep Learning for Semantic Part Segmentation with High-Level Guidance | http://arxiv.org/abs/1505.02438 | author:S. Tsogkas, I. Kokkinos, G. Papandreou, A. Vedaldi category:cs.CV published:2015-05-10 summary:In this work we address the task of segmenting an object into its parts, orsemantic part segmentation. We start by adapting a state-of-the-art semanticsegmentation system to this task, and show that a combination of afully-convolutional Deep CNN system coupled with Dense CRF labelling providesexcellent results for a broad range of object categories. Still, this approachremains agnostic to high-level constraints between object parts. We introducesuch prior information by means of the Restricted Boltzmann Machine, adapted toour task and train our model in an discriminative fashion, as a hidden CRF,demonstrating that prior information can yield additional improvements. We alsoinvestigate the performance of our approach ``in the wild'', withoutinformation concerning the objects' bounding boxes, using an object detector toguide a multi-scale segmentation scheme. We evaluate the performance of ourapproach on the Penn-Fudan and LFW datasets for the tasks of pedestrian parsingand face labelling respectively. We show superior performance with respect tocompetitive methods that have been extensively engineered on these benchmarks,as well as realistic qualitative results on part segmentation, even foroccluded or deformable objects. We also provide quantitative and extensivequalitative results on three classes from the PASCAL Parts dataset. Finally, weshow that our multi-scale segmentation scheme can boost accuracy, recoveringsegmentations for finer parts.
arxiv-11400-116 | Equitability, interval estimation, and statistical power | http://arxiv.org/abs/1505.02212 | author:Yakir A. Reshef, David N. Reshef, Pardis C. Sabeti, Michael M. Mitzenmacher category:math.ST cs.LG q-bio.QM stat.ME stat.ML stat.TH published:2015-05-09 summary:For analysis of a high-dimensional dataset, a common approach is to test anull hypothesis of statistical independence on all variable pairs using anon-parametric measure of dependence. However, because this approach attemptsto identify any non-trivial relationship no matter how weak, it oftenidentifies too many relationships to be useful. What is needed is a way ofidentifying a smaller set of relationships that merit detailed furtheranalysis. Here we formally present and characterize equitability, a property ofmeasures of dependence that aims to overcome this challenge. Notionally, anequitable statistic is a statistic that, given some measure of noise, assignssimilar scores to equally noisy relationships of different types [Reshef et al.2011]. We begin by formalizing this idea via a new object called theinterpretable interval, which functions as an interval estimate of the amountof noise in a relationship of unknown type. We define an equitable statistic asone with small interpretable intervals. We then draw on the equivalence of interval estimation and hypothesis testingto show that under moderate assumptions an equitable statistic is one thatyields well powered tests for distinguishing not only between trivial andnon-trivial relationships of all kinds but also between non-trivialrelationships of different strengths. This means that equitability allows us tospecify a threshold relationship strength $x_0$ and to search for relationshipsof all kinds with strength greater than $x_0$. Thus, equitability can bethought of as a strengthening of power against independence that enablesfruitful analysis of data sets with a small number of strong, interestingrelationships and a large number of weaker ones. We conclude with ademonstration of how our two equivalent characterizations of equitability canbe used to evaluate the equitability of a statistic in practice.
arxiv-11400-117 | Relations Between Adjacency and Modularity Graph Partitioning | http://arxiv.org/abs/1505.03481 | author:Hansi Jiang, Carl Meyer category:stat.ML published:2015-05-09 summary:In this paper the exact linear relation between the leading eigenvector ofthe unnormalized modularity matrix and the eigenvectors of the adjacency matrixis developed. Based on this analysis a method to approximate the leadingeigenvector of the modularity matrix is given, and the relative error of theapproximation is derived. A complete proof of the equivalence betweennormalized modularity clustering and normalized adjacency clustering is alsogiven. A new metric is defined to describe the agreement of two clusteringmethods, and some applications and experiments are given to illustrate andcorroborate the points that are made in the theoretical development.
arxiv-11400-118 | Probabilistic Cascading for Large Scale Hierarchical Classification | http://arxiv.org/abs/1505.02251 | author:Aris Kosmopoulos, Georgios Paliouras, Ion Androutsopoulos category:cs.LG cs.CL cs.IR published:2015-05-09 summary:Hierarchies are frequently used for the organization of objects. Given ahierarchy of classes, two main approaches are used, to automatically classifynew instances: flat classification and cascade classification. Flatclassification ignores the hierarchy, while cascade classification greedilytraverses the hierarchy from the root to the predicted leaf. In this paper wepropose a new approach, which extends cascade classification to predict theright leaf by estimating the probability of each root-to-leaf path. We provideexperimental results which indicate that, using the same classificationalgorithm, one can achieve better results with our approach, compared to thetraditional flat and cascade classifications.
arxiv-11400-119 | An Empirical Study of Leading Measures of Dependence | http://arxiv.org/abs/1505.02214 | author:David N. Reshef, Yakir A. Reshef, Pardis C. Sabeti, Michael M. Mitzenmacher category:stat.ME cs.IT cs.LG math.IT q-bio.QM stat.ML published:2015-05-09 summary:In exploratory data analysis, we are often interested in identifyingpromising pairwise associations for further analysis while filtering outweaker, less interesting ones. This can be accomplished by computing a measureof dependence on all variable pairs and examining the highest-scoring pairs,provided the measure of dependence used assigns similar scores to equally noisyrelationships of different types. This property, called equitability, isformalized in Reshef et al. [2015b]. In addition to equitability, measures ofdependence can also be assessed by the power of their correspondingindependence tests as well as their runtime. Here we present extensive empirical evaluation of the equitability, poweragainst independence, and runtime of several leading measures of dependence.These include two statistics introduced in Reshef et al. [2015a]: MICe, whichhas equitability as its primary goal, and TICe, which has power againstindependence as its goal. Regarding equitability, our analysis finds that MICeis the most equitable method on functional relationships in most of thesettings we considered, although mutual information estimation proves the mostequitable at large sample sizes in some specific settings. Regarding poweragainst independence, we find that TICe, along with Heller and Gorfine's S^DDP,is the state of the art on the relationships we tested. Our analyses also showa trade-off between power against independence and equitability consistent withthe theory in Reshef et al. [2015b]. In terms of runtime, MICe and TICe aresignificantly faster than many other measures of dependence tested, andcomputing either one makes computing the other trivial. This suggests that afast and useful strategy for achieving a combination of power againstindependence and equitability may be to filter relationships by TICe and thento examine the MICe of only the significant ones.
arxiv-11400-120 | Measuring dependence powerfully and equitably | http://arxiv.org/abs/1505.02213 | author:Yakir A. Reshef, David N. Reshef, Hilary K. Finucane, Pardis C. Sabeti, Michael M. Mitzenmacher category:stat.ME cs.IT cs.LG math.IT q-bio.QM stat.ML published:2015-05-09 summary:For high-dimensional datasets, it is common to evaluate a measure ofdependence on every variable pair and retain the highest-scoring pairs forfollow-up. If the statistic used systematically assigns higher scores to somerelationship types over others, important relationships may be overlooked. Thisdifficulty is avoided if the statistic is equitable [Reshef et al. 2015a],i.e., if, for some measure of noise, it assigns similar scores to equally noisyrelationships regardless of relationship type. In this paper, we introduce and characterize a population measure ofdependence called MIC*. We show three ways that MIC* can be viewed: as thepopulation value of MIC, a highly equitable statistic from [Reshef et al.2011], as a canonical "smoothing" of mutual information, and as the supremum ofan infinite sequence defined in terms of optimal one-dimensional partitions ofthe marginals of the joint distribution. Based on this theory, we introduce anefficient algorithm for computing MIC* from the density of a pair of randomvariables, and we define a new consistent estimator MICe for MIC* that isefficiently computable. (In contrast, there is no known polynomial-timealgorithm for computing MIC.) We show through simulations that MICe has betterbias-variance properties than MIC, and that it has high equitability withrespect to R^2 on a set of functional relationships. While MICe is designed forequitability rather than independence testing, we introduce a relatedstatistic, TICe, that is a trivial side-product of the computation of MICe. Weprove the consistency of independence testing based on TICe and show insimulations that this approach achieves excellent power. This paper is accompanied by a companion paper [Reshef et al. 2015b] focusedon in-depth empirical evaluation of several leading measures of dependence thatfinds that the performance of MICe and TICe is state-of-the-art.
arxiv-11400-121 | Simultaneous Clustering and Model Selection for Multinomial Distribution: A Comparative Study | http://arxiv.org/abs/1505.02324 | author:Md. Abul Hasnat, Julien Velcin, Stéphane Bonnevay, Julien Jacques category:cs.LG stat.ME stat.ML published:2015-05-09 summary:In this paper, we study different discrete data clustering methods, which usethe Model-Based Clustering (MBC) framework with the Multinomial distribution.Our study comprises several relevant issues, such as initialization, modelestimation and model selection. Additionally, we propose a novel MBC method byefficiently combining the partitional and hierarchical clustering techniques.We conduct experiments on both synthetic and real data and evaluate the methodsusing accuracy, stability and computation time. Our study identifiesappropriate strategies to be used for discrete data analysis with the MBCmethods. Moreover, our proposed method is very competitive w.r.t. clusteringaccuracy and better w.r.t. stability and computation time.
arxiv-11400-122 | Subset Feature Learning for Fine-Grained Category Classification | http://arxiv.org/abs/1505.02269 | author:Zongyuan Ge, Christopher Mccool, Conrad Sanderson, Peter Corke category:cs.CV published:2015-05-09 summary:Fine-grained categorisation has been a challenging problem due to smallinter-class variation, large intra-class variation and low number of trainingimages. We propose a learning system which first clusters visually similarclasses and then learns deep convolutional neural network features specific toeach subset. Experiments on the popular fine-grained Caltech-UCSD bird datasetshow that the proposed method outperforms recent fine-grained categorisationmethods under the most difficult setting: no bounding boxes are presented attest time. It achieves a mean accuracy of 77.5%, compared to the previous bestperformance of 73.2%. We also show that progressive transfer learning allows usto first learn domain-generic features (for bird classification) which can thenbe adapted to specific set of bird classes, yielding improvements in accuracy.
arxiv-11400-123 | Estimation with Norm Regularization | http://arxiv.org/abs/1505.02294 | author:Arindam Banerjee, Sheng Chen, Farideh Fazayeli, Vidyashankar Sivakumar category:stat.ML cs.LG published:2015-05-09 summary:Analysis of non-asymptotic estimation error and structured statisticalrecovery based on norm regularized regression, such as Lasso, needs to considerfour aspects: the norm, the loss function, the design matrix, and the noisemodel. This paper presents generalizations of such estimation error analysis onall four aspects compared to the existing literature. We characterize therestricted error set where the estimation error vector lies, establishrelations between error sets for the constrained and regularized problems, andpresent an estimation error bound applicable to any norm. Precisecharacterizations of the bound is presented for isotropic as well asanisotropic subGaussian design matrices, subGaussian noise models, and convexloss functions, including least squares and generalized linear models. Genericchaining and associated results play an important role in the analysis. A keyresult from the analysis is that the sample complexity of all such estimatorsdepends on the Gaussian width of a spherical cap corresponding to therestricted error set. Further, once the number of samples $n$ crosses therequired sample complexity, the estimation error decreases as$\frac{c}{\sqrt{n}}$, where $c$ depends on the Gaussian width of the unit normball.
arxiv-11400-124 | Should we really use post-hoc tests based on mean-ranks? | http://arxiv.org/abs/1505.02288 | author:Alessio Benavoli, Giorgio Corani, Francesca Mangili category:cs.LG math.ST q-bio.QM stat.ML stat.TH published:2015-05-09 summary:The statistical comparison of multiple algorithms over multiple data sets isfundamental in machine learning. This is typically carried out by the Friedmantest. When the Friedman test rejects the null hypothesis, multiple comparisonsare carried out to establish which are the significant differences amongalgorithms. The multiple comparisons are usually performed using the mean-rankstest. The aim of this technical note is to discuss the inconsistencies of themean-ranks post-hoc test with the goal of discouraging its use in machinelearning as well as in medicine, psychology, etc.. We show that the outcome ofthe mean-ranks test depends on the pool of algorithms originally included inthe experiment. In other words, the outcome of the comparison betweenalgorithms A and B depends also on the performance of the other algorithmsincluded in the original experiment. This can lead to paradoxical situations.For instance the difference between A and B could be declared significant ifthe pool comprises algorithms C, D, E and not significant if the pool comprisesalgorithms F, G, H. To overcome these issues, we suggest instead to perform themultiple comparison using a test whose outcome only depends on the twoalgorithms being compared, such as the sign-test or the Wilcoxon signed-ranktest.
arxiv-11400-125 | Performance Evaluation of Vision-Based Algorithms for MAVs | http://arxiv.org/abs/1505.02247 | author:T. Holzmann, R. Prettenthaler, J. Pestana, D. Muschick, G. Graber, C. Mostegel, F. Fraundorfer, H. Bischof category:cs.CV published:2015-05-09 summary:An important focus of current research in the field of Micro Aerial Vehicles(MAVs) is to increase the safety of their operation in general unstructuredenvironments. Especially indoors, where GPS cannot be used for localization,reliable algorithms for localization and mapping of the environment arenecessary in order to keep an MAV airborne safely. In this paper, we comparevision-based real-time capable methods for localization and mapping and pointout their strengths and weaknesses. Additionally, we describe algorithms forstate estimation, control and navigation, which use the localization andmapping results of our vision-based algorithms as input.
arxiv-11400-126 | Newton Sketch: A Linear-time Optimization Algorithm with Linear-Quadratic Convergence | http://arxiv.org/abs/1505.02250 | author:Mert Pilanci, Martin J. Wainwright category:math.OC cs.DS cs.LG stat.ML published:2015-05-09 summary:We propose a randomized second-order method for optimization known as theNewton Sketch: it is based on performing an approximate Newton step using arandomly projected or sub-sampled Hessian. For self-concordant functions, weprove that the algorithm has super-linear convergence with exponentially highprobability, with convergence and complexity guarantees that are independent ofcondition numbers and related problem-dependent quantities. Given a suitableinitialization, similar guarantees also hold for strongly convex and smoothobjectives without self-concordance. When implemented using randomizedprojections based on a sub-sampled Hadamard basis, the algorithm typically hassubstantially lower complexity than Newton's method. We also describeextensions of our methods to programs involving convex constraints that areequipped with self-concordant barriers. We discuss and illustrate applicationsto linear programs, quadratic programs with convex constraints, logisticregression and other generalized linear models, as well as semidefiniteprograms.
arxiv-11400-127 | Learning image representations tied to ego-motion | http://arxiv.org/abs/1505.02206 | author:Dinesh Jayaraman, Kristen Grauman category:cs.CV cs.AI stat.ML published:2015-05-08 summary:Understanding how images of objects and scenes behave in response to specificego-motions is a crucial aspect of proper visual development, yet existingvisual learning methods are conspicuously disconnected from the physical sourceof their images. We propose to exploit proprioceptive motor signals to provideunsupervised regularization in convolutional neural networks to learn visualrepresentations from egocentric video. Specifically, we enforce that ourlearned features exhibit equivariance i.e. they respond predictably totransformations associated with distinct ego-motions. With three datasets, weshow that our unsupervised feature learning approach significantly outperformsprevious approaches on visual recognition and next-best-view prediction tasks.In the most challenging test, we show that features learned from video capturedon an autonomous driving platform improve large-scale scene recognition instatic images from a disjoint domain.
arxiv-11400-128 | Improving Gibbs Sampling Predictions on Unseen Data for Latent Dirichlet Allocation | http://arxiv.org/abs/1505.02065 | author:Yannis Papanikolaou, Timothy N. Rubin, Grigorios Tsoumakas category:stat.ML published:2015-05-08 summary:Latent Dirichlet Allocation (LDA) is a model for discovering the underlyingstructure of a given data set. LDA and its extensions have been used inunsupervised and supervised learning tasks across a variety of data typesincluding textual, image and biological data. Several methods have beenpresented for approximate inference of LDA parameters, including VariationalBayes (VB), Collapsed Gibbs Sampling (CGS) and Collapsed Variational Bayes(CVB) techniques. This work explores three novel methods for generating LDApredictions on unobserved data, given a model trained by CGS. We presentextensive experiments on real-world data sets for both standard unsupervisedLDA and Prior LDA, one of the supervised variants of LDA for multi-label data.In both supervised and unsupervised settings, we perform extensive empiricalcomparison of our prediction methods with the standard predictions generated byCGS and CVB0 (a variant of CVB). The results show a consistent advantage of oneof our methods over CGS under all experimental conditions, and over CVB0 underthe majority of conditions.
arxiv-11400-129 | DeepBox: Learning Objectness with Convolutional Networks | http://arxiv.org/abs/1505.02146 | author:Weicheng Kuo, Bharath Hariharan, Jitendra Malik category:cs.CV published:2015-05-08 summary:Existing object proposal approaches use primarily bottom-up cues to rankproposals, while we believe that objectness is in fact a high level construct.We argue for a data-driven, semantic approach for ranking object proposals. Ourframework, which we call DeepBox, uses convolutional neural networks (CNNs) torerank proposals from a bottom-up method. We use a novel four-layer CNNarchitecture that is as good as much larger networks on the task of evaluatingobjectness while being much faster. We show that DeepBox significantly improvesover the bottom-up ranking, achieving the same recall with 500 proposals asachieved by bottom-up methods with 2000. This improvement generalizes tocategories the CNN has never seen before and leads to a 4.5-point gain indetection mAP. Our implementation achieves this performance while running at260 ms per image.
arxiv-11400-130 | Porting HTM Models to the Heidelberg Neuromorphic Computing Platform | http://arxiv.org/abs/1505.02142 | author:Sebastian Billaudelle, Subutai Ahmad category:q-bio.NC cs.NE published:2015-05-08 summary:Hierarchical Temporal Memory (HTM) is a computational theory of machineintelligence based on a detailed study of the neocortex. The HeidelbergNeuromorphic Computing Platform, developed as part of the Human Brain Project(HBP), is a mixed-signal (analog and digital) large-scale platform for modelingnetworks of spiking neurons. In this paper we present the first effort inporting HTM networks to this platform. We describe a framework for simulatingkey HTM operations using spiking network models. We then describe specificspatial pooling and temporal memory implementations, as well as simulationsdemonstrating that the fundamental properties are maintained. We discuss issuesin implementing the full set of plasticity rules using Spike-Timing DependentPlasticity (STDP), and rough place and route calculations. Although furtherwork is required, our initial studies indicate that it should be possible torun large-scale HTM networks (including plasticity rules) efficiently on theHeidelberg platform. More generally the exercise of porting high level HTMalgorithms to biophysical neuron models promises to be a fruitful area ofinvestigation for future studies.
arxiv-11400-131 | MegaFace: A Million Faces for Recognition at Scale | http://arxiv.org/abs/1505.02108 | author:D. Miller, E. Brossard, S. Seitz, I. Kemelmacher-Shlizerman category:cs.CV published:2015-05-08 summary:Recent face recognition experiments on the LFW benchmark show that facerecognition is performing stunningly well, surpassing human recognition rates.In this paper, we study face recognition at scale. Specifically, we havecollected from Flickr a \textbf{Million} faces and evaluated state of the artface recognition algorithms on this dataset. We found that the performance ofalgorithms varies--while all perform great on LFW, once evaluated at scalerecognition rates drop drastically for most algorithms. Interestingly, deeplearning based approach by \cite{schroff2015facenet} performs much better, butstill gets less robust at scale. We consider both verification andidentification problems, and evaluate how pose affects recognition at scale.Moreover, we ran an extensive human study on Mechanical Turk to evaluate humanrecognition at scale, and report results. All the photos are creative commonsphotos and is released at \small{\url{http://megaface.cs.washington.edu/}} forresearch and further experiments.
arxiv-11400-132 | Bilevel approaches for learning of variational imaging models | http://arxiv.org/abs/1505.02120 | author:Luca Calatroni, Cao Chung, Juan Carlos De Los Reyes, Carola-Bibiane Schönlieb, Tuomo Valkonen category:math.OC cs.CV published:2015-05-08 summary:We review some recent learning approaches in variational imaging, based onbilevel optimisation, and emphasize the importance of their treatment infunction space. The paper covers both analytical and numerical techniques.Analytically, we include results on the existence and structure of minimisers,as well as optimality conditions for their characterisation. Based on thisinformation, Newton type methods are studied for the solution of the problemsat hand, combining them with sampling techniques in case of large databases.The computational verification of the developed techniques is extensivelydocumented, covering instances with different type of regularisers, severalnoise models, spatially dependent weights and large image databases.
arxiv-11400-133 | Deep Learning for Medical Image Segmentation | http://arxiv.org/abs/1505.02000 | author:Matthew Lai category:cs.LG cs.AI cs.CV published:2015-05-08 summary:This report provides an overview of the current state of the art deeplearning architectures and optimisation techniques, and uses the ADNIhippocampus MRI dataset as an example to compare the effectiveness andefficiency of different convolutional architectures on the task of patch-based3-dimensional hippocampal segmentation, which is important in the diagnosis ofAlzheimer's Disease. We found that a slightly unconventional "stacked 2D"approach provides much better classification performance than simple 2D patcheswithout requiring significantly more computational power. We also examined thepopular "tri-planar" approach used in some recently published studies, andfound that it provides much better results than the 2D approaches, but alsowith a moderate increase in computational power requirement. Finally, weevaluated a full 3D convolutional architecture, and found that it providesmarginally better results than the tri-planar approach, but at the cost of avery significant increase in computational power requirement.
arxiv-11400-134 | Evolving Boolean Networks with RNA Editing | http://arxiv.org/abs/1505.01980 | author:Larry Bull category:cs.NE q-bio.MN q-bio.PE published:2015-05-08 summary:The editing of transcribed RNA by other molecules such that the form of thefinal product differs from that specified in the corresponding DNA sequence isubiquitous. This paper uses an abstract, tunable Boolean genetic regulatorynetwork model to explore aspects of RNA editing. In particular, it is shown howdynamically altering expressed sequences via a guide RNA-inspired mechanism canbe selected for by simulated evolution under various single and multicellularscenarios.
arxiv-11400-135 | The structure of optimal parameters for image restoration problems | http://arxiv.org/abs/1505.01953 | author:Juan Carlos De Los Reyes, Carola-Bibiane Schönlieb, Tuomo Valkonen category:math.OC cs.CV published:2015-05-08 summary:We study the qualitative properties of optimal regularisation parameters invariational models for image restoration. The parameters are solutions ofbilevel optimisation problems with the image restoration problem as constraint.A general type of regulariser is considered, which encompasses total variation(TV), total generalized variation (TGV) and infimal-convolution total variation(ICTV). We prove that under certain conditions on the given data optimalparameters derived by bilevel optimisation problems exist. A crucial point inthe existence proof turns out to be the boundedness of the optimal parametersaway from $0$ which we prove in this paper. The analysis is done on theoriginal -- in image restoration typically non-smooth variational problem -- aswell as on a smoothed approximation set in Hilbert space which is the oneconsidered in numerical computations. For the smoothed bilevel problem we alsoprove that it $\Gamma$ converges to the original problem as the smoothingvanishes. All analysis is done in function spaces rather than on thediscretised learning problem.
arxiv-11400-136 | An Asymptotically Optimal Policy for Uniform Bandits of Unknown Support | http://arxiv.org/abs/1505.01918 | author:Wesley Cowan, Michael N. Katehakis category:stat.ML cs.LG published:2015-05-08 summary:Consider the problem of a controller sampling sequentially from a finitenumber of $N \geq 2$ populations, specified by random variables $X^i_k$, $ i =1,\ldots , N,$ and $k = 1, 2, \ldots$; where $X^i_k$ denotes the outcome frompopulation $i$ the $k^{th}$ time it is sampled. It is assumed that for eachfixed $i$, $\{ X^i_k \}_{k \geq 1}$ is a sequence of i.i.d. uniform randomvariables over some interval $[a_i, b_i]$, with the support (i.e., $a_i, b_i$)unknown to the controller. The objective is to have a policy $\pi$ fordeciding, based on available data, from which of the $N$ populations to samplefrom at any time $n=1,2,\ldots$ so as to maximize the expected sum of outcomesof $n$ samples or equivalently to minimize the regret due to lack oninformation of the parameters $\{ a_i \}$ and $\{ b_i \}$. In this paper, wepresent a simple inflated sample mean (ISM) type policy that is asymptoticallyoptimal in the sense of its regret achieving the asymptotic lower bound ofBurnetas and Katehakis (1996). Additionally, finite horizon regret bounds aregiven.
arxiv-11400-137 | Exploring Models and Data for Image Question Answering | http://arxiv.org/abs/1505.02074 | author:Mengye Ren, Ryan Kiros, Richard Zemel category:cs.LG cs.AI cs.CL cs.CV published:2015-05-08 summary:This work aims to address the problem of image-based question-answering (QA)with new models and datasets. In our work, we propose to use neural networksand visual semantic embeddings, without intermediate stages such as objectdetection and image segmentation, to predict answers to simple questions aboutimages. Our model performs 1.8 times better than the only published results onan existing image QA dataset. We also present a question generation algorithmthat converts image descriptions, which are widely available, into QA form. Weused this algorithm to produce an order-of-magnitude larger dataset, with moreevenly distributed answers. A suite of baseline results on this new dataset arealso presented.
arxiv-11400-138 | Noise in Structured-Light Stereo Depth Cameras: Modeling and its Applications | http://arxiv.org/abs/1505.01936 | author:Avishek Chatterjee, Venu Madhav Govindu category:cs.CV published:2015-05-08 summary:Depth maps obtained from commercially available structured-light stereo baseddepth cameras, such as the Kinect, are easy to use but are affected bysignificant amounts of noise. This paper is devoted to a study of the intrinsicnoise characteristics of such depth maps, i.e. the standard deviation of noisein estimated depth varies quadratically with the distance of the object fromthe depth camera. We validate this theoretical model against empiricalobservations and demonstrate the utility of this noise model in three popularapplications: depth map denoising, volumetric scan merging for 3D modeling, andidentification of 3D planes in depth maps.
arxiv-11400-139 | Learning and Optimization with Submodular Functions | http://arxiv.org/abs/1505.01576 | author:Bharath Sankaran, Marjan Ghazvininejad, Xinran He, David Kale, Liron Cohen category:cs.LG published:2015-05-07 summary:In many naturally occurring optimization problems one needs to ensure thatthe definition of the optimization problem lends itself to solutions that aretractable to compute. In cases where exact solutions cannot be computedtractably, it is beneficial to have strong guarantees on the tractableapproximate solutions. In order operate under these criterion most optimizationproblems are cast under the umbrella of convexity or submodularity. In thisreport we will study design and optimization over a common class of functionscalled submodular functions. Set functions, and specifically submodular setfunctions, characterize a wide variety of naturally occurring optimizationproblems, and the property of submodularity of set functions has deeptheoretical consequences with wide ranging applications. Informally, theproperty of submodularity of set functions concerns the intuitive "principle ofdiminishing returns. This property states that adding an element to a smallerset has more value than adding it to a larger set. Common examples ofsubmodular monotone functions are entropies, concave functions of cardinality,and matroid rank functions; non-monotone examples include graph cuts, networkflows, and mutual information. In this paper we will review the formal definition of submodularity; theoptimization of submodular functions, both maximization and minimization; andfinally discuss some applications in relation to learning and reasoning usingsubmodular functions.
arxiv-11400-140 | Blind Compressive Sensing Framework for Collaborative Filtering | http://arxiv.org/abs/1505.01621 | author:Anupriya Gogna, Angshul Majumdar category:cs.IR cs.LG published:2015-05-07 summary:Existing works based on latent factor models have focused on representing therating matrix as a product of user and item latent factor matrices, both beingdense. Latent (factor) vectors define the degree to which a trait is possessedby an item or the affinity of user towards that trait. A dense user matrix is areasonable assumption as each user will like/dislike a trait to certain extent.However, any item will possess only a few of the attributes and never all.Hence, the item matrix should ideally have a sparse structure rather than adense one as formulated in earlier works. Therefore we propose to factor theratings matrix into a dense user matrix and a sparse item matrix which leads usto the Blind Compressed Sensing (BCS) framework. We derive an efficientalgorithm for solving the BCS problem based on Majorization Minimization (MM)technique. Our proposed approach is able to achieve significantly higheraccuracy and shorter run times as compared to existing approaches.
arxiv-11400-141 | Context-Aware Mobility Management in HetNets: A Reinforcement Learning Approach | http://arxiv.org/abs/1505.01625 | author:Meryem Simsek, Mehdi Bennis, Ismail Güvenc category:cs.NI cs.LG published:2015-05-07 summary:The use of small cell deployments in heterogeneous network (HetNet)environments is expected to be a key feature of 4G networks and beyond, andessential for providing higher user throughput and cell-edge coverage. However,due to different coverage sizes of macro and pico base stations (BSs), such aparadigm shift introduces additional requirements and challenges in densenetworks. Among these challenges is the handover performance of user equipment(UEs), which will be impacted especially when high velocity UEs traversepicocells. In this paper, we propose a coordination-based and context-awaremobility management (MM) procedure for small cell networks using tools fromreinforcement learning. Here, macro and pico BSs jointly learn their long-termtraffic loads and optimal cell range expansion, and schedule their UEs based ontheir velocities and historical rates (exchanged among tiers). The proposedapproach is shown to not only outperform the classical MM in terms of UEthroughput, but also to enable better fairness. In average, a gain of up to80\% is achieved for UE throughput, while the handover failure probability isreduced up to a factor of three by the proposed learning based MM approaches.
arxiv-11400-142 | Adaptive Nonparametric Image Parsing | http://arxiv.org/abs/1505.01560 | author:Tam V. Nguyen, Canyi Lu, Jose Sepulveda, Shuicheng Yan category:cs.CV published:2015-05-07 summary:In this paper, we present an adaptive nonparametric solution to the imageparsing task, namely annotating each image pixel with its correspondingcategory label. For a given test image, first, a locality-aware retrieval setis extracted from the training data based on super-pixel matching similarities,which are augmented with feature extraction for better differentiation of localsuper-pixels. Then, the category of each super-pixel is initialized by themajority vote of the $k$-nearest-neighbor super-pixels in the retrieval set.Instead of fixing $k$ as in traditional non-parametric approaches, here wepropose a novel adaptive nonparametric approach which determines thesample-specific k for each test image. In particular, $k$ is adaptively set tobe the number of the fewest nearest super-pixels which the images in theretrieval set can use to get the best category prediction. Finally, the initialsuper-pixel labels are further refined by contextual smoothing. Extensiveexperiments on challenging datasets demonstrate the superiority of the newsolution over other state-of-the-art nonparametric solutions.
arxiv-11400-143 | Learning to See by Moving | http://arxiv.org/abs/1505.01596 | author:Pulkit Agrawal, Joao Carreira, Jitendra Malik category:cs.CV cs.NE cs.RO published:2015-05-07 summary:The dominant paradigm for feature learning in computer vision relies ontraining neural networks for the task of object recognition using millions ofhand labelled images. Is it possible to learn useful features for a diverse setof visual tasks using any other form of supervision? In biology, livingorganisms developed the ability of visual perception for the purpose of movingand acting in the world. Drawing inspiration from this observation, in thiswork we investigate if the awareness of egomotion can be used as a supervisorysignal for feature learning. As opposed to the knowledge of class labels,information about egomotion is freely available to mobile agents. We show thatgiven the same number of training images, features learnt using egomotion assupervision compare favourably to features learnt using class-label assupervision on visual tasks of scene recognition, object recognition, visualodometry and keypoint matching.
arxiv-11400-144 | Bayesian Optimization for Synthetic Gene Design | http://arxiv.org/abs/1505.01627 | author:Javier González, Joseph Longworth, David C. James, Neil D. Lawrence category:stat.ML published:2015-05-07 summary:We address the problem of synthetic gene design using Bayesian optimization.The main issue when designing a gene is that the design space is defined interms of long strings of characters of different lengths, which renders theoptimization intractable. We propose a three-step approach to deal with thisissue. First, we use a Gaussian process model to emulate the behavior of thecell. As inputs of the model, we use a set of biologically meaningful genefeatures, which allows us to define optimal gene designs rules. Based on themodel outputs we define a multi-task acquisition function to optimizesimultaneously severals aspects of interest. Finally, we define an evaluationfunction, which allow us to rank sets of candidate gene sequences that arecoherent with the optimal design strategy. We illustrate the performance ofthis approach in a real gene design experiment with mammalian cells.
arxiv-11400-145 | Jointly Modeling Embedding and Translation to Bridge Video and Language | http://arxiv.org/abs/1505.01861 | author:Yingwei Pan, Tao Mei, Ting Yao, Houqiang Li, Yong Rui category:cs.CV cs.MM published:2015-05-07 summary:Automatically describing video content with natural language is a fundamentalchallenge of multimedia. Recurrent Neural Networks (RNN), which models sequencedynamics, has attracted increasing attention on visual interpretation. However,most existing approaches generate a word locally with given previous words andthe visual content, while the relationship between sentence semantics andvisual content is not holistically exploited. As a result, the generatedsentences may be contextually correct but the semantics (e.g., subjects, verbsor objects) are not true. This paper presents a novel unified framework, named Long Short-Term Memorywith visual-semantic Embedding (LSTM-E), which can simultaneously explore thelearning of LSTM and visual-semantic embedding. The former aims to locallymaximize the probability of generating the next word given previous words andvisual content, while the latter is to create a visual-semantic embedding spacefor enforcing the relationship between the semantics of the entire sentence andvisual content. Our proposed LSTM-E consists of three components: a 2-D and/or3-D deep convolutional neural networks for learning powerful videorepresentation, a deep RNN for generating sentences, and a joint embeddingmodel for exploring the relationships between visual content and sentencesemantics. The experiments on YouTube2Text dataset show that our proposedLSTM-E achieves to-date the best reported performance in generating naturalsentences: 45.3% and 31.0% in terms of BLEU@4 and METEOR, respectively. We alsodemonstrate that LSTM-E is superior in predicting Subject-Verb-Object (SVO)triplets to several state-of-the-art techniques.
arxiv-11400-146 | Integrating K-means with Quadratic Programming Feature Selection | http://arxiv.org/abs/1505.01728 | author:Yamuna Prasad, K. K. Biswas category:cs.CV cs.LG published:2015-05-07 summary:Several data mining problems are characterized by data in high dimensions.One of the popular ways to reduce the dimensionality of the data is to performfeature selection, i.e, select a subset of relevant and non-redundant features.Recently, Quadratic Programming Feature Selection (QPFS) has been proposedwhich formulates the feature selection problem as a quadratic program. It hasbeen shown to outperform many of the existing feature selection methods for avariety of applications. Though, better than many existing approaches, therunning time complexity of QPFS is cubic in the number of features, which canbe quite computationally expensive even for moderately sized datasets. In thispaper we propose a novel method for feature selection by integrating k-meansclustering with QPFS. The basic variant of our approach runs k-means to bringdown the number of features which need to be passed on to QPFS. We then enhancethis idea, wherein we gradually refine the feature space from a very coarseclustering to a fine-grained one, by interleaving steps of QPFS with k-meansclustering. Every step of QPFS helps in identifying the clusters of irrelevantfeatures (which can then be thrown away), whereas every step of k-means furtherrefines the clusters which are potentially relevant. We show that our iterativerefinement of clusters is guaranteed to converge. We provide bounds on thenumber of distance computations involved in the k-means algorithm. Further,each QPFS run is now cubic in number of clusters, which can be much smallerthan actual number of features. Experiments on eight publicly availabledatasets show that our approach gives significant computational gains (both intime and memory), over standard QPFS as well as other state of the art featureselection methods, even while improving the overall accuracy.
arxiv-11400-147 | Data Fusion of Objects Using Techniques Such as Laser Scanning, Structured Light and Photogrammetry for Cultural Heritage Applications | http://arxiv.org/abs/1505.01631 | author:Citlalli Gamez Serna, Ruven Pillay, Alain Tremeau category:cs.CV published:2015-05-07 summary:In this paper we present a semi-automatic 2D-3D local registration pipelinecapable of coloring 3D models obtained from 3D scanners by using uncalibratedimages. The proposed pipeline exploits the Structure from Motion (SfM)technique in order to reconstruct a sparse representation of the 3D object andobtain the camera parameters from image feature matches. We then coarselyregister the reconstructed 3D model to the scanned one through the ScaleIterative Closest Point (SICP) algorithm. SICP provides the global scale,rotation and translation parameters, using minimal manual user intervention. Inthe final processing stage, a local registration refinement algorithm optimizesthe color projection of the aligned photos on the 3D object removing theblurring/ghosting artefacts introduced due to small inaccuracies during theregistration. The proposed pipeline is capable of handling real world caseswith a range of characteristics from objects with low level geometric featuresto complex ones.
arxiv-11400-148 | Fast Spectral Unmixing based on Dykstra's Alternating Projection | http://arxiv.org/abs/1505.01740 | author:Qi Wei, Jose Bioucas-Dias, Nicolas Dobigeon, Jean-Yves Tourneret category:cs.CV published:2015-05-07 summary:This paper presents a fast spectral unmixing algorithm based on Dykstra'salternating projection. The proposed algorithm formulates the fully constrainedleast squares optimization problem associated with the spectral unmixing taskas an unconstrained regression problem followed by a projection onto theintersection of several closed convex sets. This projection is achieved byiteratively projecting onto each of the convex sets individually, followingDyktra's scheme. The sequence thus obtained is guaranteed to converge to thesought projection. Thanks to the preliminary matrix decomposition and variablesubstitution, the projection is implemented intrinsically in a subspace, whosedimension is very often much lower than the number of bands. A benefit of thisstrategy is that the order of the computational complexity for each projectionis decreased from quadratic to linear time. Numerical experiments consideringdiverse spectral unmixing scenarios provide evidence that the proposedalgorithm competes with the state-of-the-art, namely when the number ofendmembers is relatively small, a circumstance often observed in realhyperspectral applications.
arxiv-11400-149 | Webly Supervised Learning of Convolutional Networks | http://arxiv.org/abs/1505.01554 | author:Xinlei Chen, Abhinav Gupta category:cs.CV published:2015-05-07 summary:We present an approach to utilize large amounts of web data for learningCNNs. Specifically inspired by curriculum learning, we present a two-stepapproach for CNN training. First, we use easy images to train an initial visualrepresentation. We then use this initial CNN and adapt it to harder, morerealistic images by leveraging the structure of data and categories. Wedemonstrate that our two-stage CNN outperforms a fine-tuned CNN trained onImageNet on Pascal VOC 2012. We also demonstrate the strength of weblysupervised learning by localizing objects in web images and training a R-CNNstyle detector. It achieves the best performance on VOC 2007 where no VOCtraining data is used. Finally, we show our approach is quite robust to noiseand performs comparably even when we use image search results from March 2013(pre-CNN image search era).
arxiv-11400-150 | Contextual Analysis for Middle Eastern Languages with Hidden Markov Models | http://arxiv.org/abs/1505.01757 | author:Kazem Taghva category:cs.CL cs.AI published:2015-05-07 summary:Displaying a document in Middle Eastern languages requires contextualanalysis due to different presentational forms for each character of thealphabet. The words of the document will be formed by the joining of thecorrect positional glyphs representing corresponding presentational forms ofthe characters. A set of rules defines the joining of the glyphs. As usual,these rules vary from language to language and are subject to interpretation bythe software developers. In this paper, we propose a machine learning approach for contextual analysisbased on the first order Hidden Markov Model. We will design and build a modelfor the Farsi language to exhibit this technology. The Farsi model achieves 94\% accuracy with the training based on a short list of 89 Farsi vocabulariesconsisting of 2780 Farsi characters. The experiment can be easily extended to many languages including Arabic,Urdu, and Sindhi. Furthermore, the advantage of this approach is that the samesoftware can be used to perform contextual analysis without coding complexrules for each specific language. Of particular interest is that the languageswith fewer speakers can have greater representation on the web, since they aretypically ignored by software developers due to lack of financial incentives.
arxiv-11400-151 | Optimal Decision-Theoretic Classification Using Non-Decomposable Performance Metrics | http://arxiv.org/abs/1505.01802 | author:Nagarajan Natarajan, Oluwasanmi Koyejo, Pradeep Ravikumar, Inderjit S. Dhillon category:cs.LG stat.ML published:2015-05-07 summary:We provide a general theoretical analysis of expected out-of-sample utility,also referred to as decision-theoretic classification, for non-decomposablebinary classification metrics such as F-measure and Jaccard coefficient. Ourkey result is that the expected out-of-sample utility for many performancemetrics is provably optimized by a classifier which is equivalent to a signedthresholding of the conditional probability of the positive class. Our analysisbridges a gap in the literature on binary classification, revealed in light ofrecent results for non-decomposable metrics in population utility maximizationstyle classification. Our results identify checkable properties of aperformance metric which are sufficient to guarantee a probability rankingprinciple. We propose consistent estimators for optimal expected out-of-sampleclassification. As a consequence of the probability ranking principle,computational requirements can be reduced from exponential to cubic complexityin the general case, and further reduced to quadratic complexity in specialcases. We provide empirical results on simulated and benchmark datasetsevaluating the performance of the proposed algorithms for decision-theoreticclassification and comparing them to baseline and state-of-the-art methods inpopulation utility maximization for non-decomposable metrics.
arxiv-11400-152 | DART: Dropouts meet Multiple Additive Regression Trees | http://arxiv.org/abs/1505.01866 | author:K. V. Rashmi, Ran Gilad-Bachrach category:cs.LG stat.ML published:2015-05-07 summary:Multiple Additive Regression Trees (MART), an ensemble model of boostedregression trees, is known to deliver high prediction accuracy for diversetasks, and it is widely used in practice. However, it suffers an issue which wecall over-specialization, wherein trees added at later iterations tend toimpact the prediction of only a few instances, and make negligible contributiontowards the remaining instances. This negatively affects the performance of themodel on unseen data, and also makes the model over-sensitive to thecontributions of the few, initially added tress. We show that the commonly usedtool to address this issue, that of shrinkage, alleviates the problem only to acertain extent and the fundamental issue of over-specialization still remains.In this work, we explore a different approach to address the problem that ofemploying dropouts, a tool that has been recently proposed in the context oflearning deep neural networks. We propose a novel way of employing dropouts inMART, resulting in the DART algorithm. We evaluate DART on ranking, regressionand classification tasks, using large scale, publicly available datasets, andshow that DART outperforms MART in each of the tasks, with a significantmargin. We also show that DART overcomes the issue of over-specialization to aconsiderable extent.
arxiv-11400-153 | Optimal Neuron Selection: NK Echo State Networks for Reinforcement Learning | http://arxiv.org/abs/1505.01887 | author:Darrell Whitley, Renato Tinós, Francisco Chicano category:cs.NE I.2.8 published:2015-05-07 summary:This paper introduces the NK Echo State Network. The problem of learning inthe NK Echo State Network is reduced to the problem of optimizing a specialform of a Spin Glass Problem known as an NK Landscape. No weight adjustment isused; all learning is accomplished by spinning up (turning on) or spinning down(turning off) neurons in order to find a combination of neurons that worktogether to achieve the desired computation. For special types of NKLandscapes, an exact global solution can be obtained in polynomial time usingdynamic programming. The NK Echo State Network is applied to a reinforcementlearning problem requiring a recurrent network: balancing two poles on a cartgiven no velocity information. Empirical results shows that the NK Echo StateNetwork learns very rapidly and yields very good generalization.
arxiv-11400-154 | Language Models for Image Captioning: The Quirks and What Works | http://arxiv.org/abs/1505.01809 | author:Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, Margaret Mitchell category:cs.CL cs.AI cs.CV cs.LG published:2015-05-07 summary:Two recent approaches have achieved state-of-the-art results in imagecaptioning. The first uses a pipelined process where a set of candidate wordsis generated by a convolutional neural network (CNN) trained on images, andthen a maximum entropy (ME) language model is used to arrange these words intoa coherent sentence. The second uses the penultimate activation layer of theCNN as input to a recurrent neural network (RNN) that then generates thecaption sequence. In this paper, we compare the merits of these differentlanguage modeling approaches for the first time by using the samestate-of-the-art CNN as input. We examine issues in the different approaches,including linguistic irregularities, caption repetition, and data set overlap.By combining key aspects of the ME and RNN methods, we achieve a new recordperformance over previously published results on the benchmark COCO dataset.However, the gains we see in BLEU do not translate to human judgments.
arxiv-11400-155 | A Survey of Predictive Modelling under Imbalanced Distributions | http://arxiv.org/abs/1505.01658 | author:Paula Branco, Luis Torgo, Rita Ribeiro category:cs.LG I.2.6 published:2015-05-07 summary:Many real world data mining applications involve obtaining predictive modelsusing data sets with strongly imbalanced distributions of the target variable.Frequently, the least common values of this target variable are associated withevents that are highly relevant for end users (e.g. fraud detection, unusualreturns on stock markets, anticipation of catastrophes, etc.). Moreover, theevents may have different costs and benefits, which when associated with therarity of some of them on the available training data creates serious problemsto predictive modelling techniques. This paper presents a survey of existingtechniques for handling these important applications of predictive analytics.Although most of the existing work addresses classification tasks (nominaltarget variables), we also describe methods designed to handle similar problemswithin regression tasks (numeric target variables). In this survey we discussthe main challenges raised by imbalanced distributions, describe the mainapproaches to these problems, propose a taxonomy of these methods and refer tosome related problems within predictive modelling.
arxiv-11400-156 | Filter characteristics in image decomposition with singular spectrum analysis | http://arxiv.org/abs/1505.01599 | author:Kenji Kume, Naoko Nose-Togawa category:cs.CV cs.NA published:2015-05-07 summary:Singular spectrum analysis is developed as a nonparametric spectraldecomposition of a time series. It can be easily extended to the decompositionof multidimensional lattice-like data through the filtering interpretation. Inthis viewpoint, the singular spectrum analysis can be understood as theadaptive and optimal generation of the filters and their two-steppoint-symmetric operation to the original data. In this paper, we point outthat, when applied to the multidimensional data, the adaptively generatedfilters exhibit symmetry properties resulting from the bisymmetric nature ofthe lag-covariance matrices. The eigenvectors of the lag-covariance matrix areeither symmetric or antisymmetric, and for the 2D image data, these lead to thedifferential-type filters with even- or odd-order derivatives. The dominantfilter is a smoothing filter, reflecting the dominance of low-frequencycomponents of the photo images. The others are the edge-enhancement or thenoise filters corresponding to the band-pass or the high-pass filters. Theimplication of the decomposition to the image denoising is briefly discussed.
arxiv-11400-157 | Object detection via a multi-region & semantic segmentation-aware CNN model | http://arxiv.org/abs/1505.01749 | author:Spyros Gidaris, Nikos Komodakis category:cs.CV cs.LG cs.NE published:2015-05-07 summary:We propose an object detection system that relies on a multi-region deepconvolutional neural network (CNN) that also encodes semanticsegmentation-aware features. The resulting CNN-based representation aims atcapturing a diverse set of discriminative appearance factors and exhibitslocalization sensitivity that is essential for accurate object localization. Weexploit the above properties of our recognition module by integrating it on aniterative localization mechanism that alternates between scoring a box proposaland refining its location with a deep CNN regression model. Thanks to theefficient use of our modules, we detect objects with very high localizationaccuracy. On the detection challenges of PASCAL VOC2007 and PASCAL VOC2012 weachieve mAP of 78.2% and 73.9% correspondingly, surpassing any other publishedwork by a significant margin.
arxiv-11400-158 | Shadow Optimization from Structured Deep Edge Detection | http://arxiv.org/abs/1505.01589 | author:Li Shen, Teck Wee Chua, Karianto Leman category:cs.CV published:2015-05-07 summary:Local structures of shadow boundaries as well as complex interactions ofimage regions remain largely unexploited by previous shadow detectionapproaches. In this paper, we present a novel learning-based framework forshadow region recovery from a single image. We exploit the local structures ofshadow edges by using a structured CNN learning framework. We show that usingthe structured label information in the classification can improve the localconsistency of the results and avoid spurious labelling. We further propose andformulate a shadow/bright measure to model the complex interactions among imageregions. The shadow and bright measures of each patch are computed from theshadow edges detected in the image. Using the global interaction constraints onpatches, we formulate a least-square optimization problem for shadow recoverythat can be solved efficiently. Our shadow recovery method achievesstate-of-the-art results on the major shadow benchmark databases collectedunder various conditions.
arxiv-11400-159 | Consistency of Spectral Hypergraph Partitioning under Planted Partition Model | http://arxiv.org/abs/1505.01582 | author:Debarghya Ghoshdastidar, Ambedkar Dukkipati category:stat.ML published:2015-05-07 summary:Hypergraph partitioning lies at the heart of a number of problems in machinelearning and network sciences. Many algorithms for hypergraph partitioning havebeen proposed that extend standard approaches for graph partitioning to thecase of hypergraphs. However, theoretical aspects of such methods have seldomreceived attention in the literature as compared to the extensive studies onthe guarantees of graph partitioning. For instance, consistency results ofspectral graph partitioning under the stochastic block model are well known. Inthis paper, we present a planted partition model for sparse random non-uniformhypergraphs that generalizes the stochastic block model. We derive an errorbound for a spectral hypergraph partitioning algorithm under this model usingmatrix concentration inequalities. To the best of our knowledge, this is thefirst consistency result related to partitioning non-uniform hypergraphs.
arxiv-11400-160 | Graphical Potential Games | http://arxiv.org/abs/1505.01539 | author:Luis E. Ortiz category:cs.GT cs.AI stat.ML published:2015-05-06 summary:Potential games, originally introduced in the early 1990's by Lloyd Shapley,the 2012 Nobel Laureate in Economics, and his colleague Dov Monderer, are avery important class of models in game theory. They have special propertiessuch as the existence of Nash equilibria in pure strategies. This noteintroduces graphical versions of potential games. Special cases of graphicalpotential games have already found applicability in many areas of science andengineering beyond economics, including artificial intelligence, computervision, and machine learning. They have been effectively applied to the studyand solution of important real-world problems such as routing and congestion innetworks, distributed resource allocation (e.g., public goods), andrelaxation-labeling for image segmentation. Implicit use of graphical potentialgames goes back at least 40 years. Several classes of games considered standardin the literature, including coordination games, local interaction games,lattice games, congestion games, and party-affiliation games, are instances ofgraphical potential games. This note provides several characterizations ofgraphical potential games by leveraging well-known results from the literatureon probabilistic graphical models. A major contribution of the work presentedhere that particularly distinguishes it from previous work is establishing thatthe convergence of certain type of game-playing rules implies that theagents/players must be embedded in some graphical potential game.
arxiv-11400-161 | Retaining Experience and Growing Solutions | http://arxiv.org/abs/1505.01474 | author:Robyn Ffrancon category:cs.NE published:2015-05-06 summary:Generally, when genetic programming (GP) is used for function synthesis anyvaluable experience gained by the system is lost from one problem to the next,even when the problems are closely related. With the aim of developing a systemwhich retains beneficial experience from problem to problem, this paperintroduces the novel Node-by-Node Growth Solver (NNGS) algorithm which featuresa component, called the controller, which can be adapted and improved for useacross a set of related problems. NNGS grows a single solution tree from rootto leaves. Using semantic backpropagation and acting locally on each node inturn, the algorithm employs the controller to assign subsequent child nodesuntil a fully formed solution is generated. The aim of this paper is to pave a path towards the use of a neural networkas the controller component and also, separately, towards the use of meta-GP asa mechanism for improving the controller component. A proof-of-conceptcontroller is discussed which demonstrates the success and potential of theNNGS algorithm. In this case, the controller constitutes a set of hand writtenrules which can be used to deterministically and greedily solve standardBoolean function synthesis benchmarks. Even before employing machine learningto improve the controller, the algorithm vastly outperforms other well knownrecent algorithms on run times, maintains comparable solution sizes, and has a100% success rate on all Boolean function synthesis benchmarks tested so far.
arxiv-11400-162 | Estimation from Pairwise Comparisons: Sharp Minimax Bounds with Topology Dependence | http://arxiv.org/abs/1505.01462 | author:Nihar B. Shah, Sivaraman Balakrishnan, Joseph Bradley, Abhay Parekh, Kannan Ramchandran, Martin J. Wainwright category:cs.LG cs.IT math.IT stat.ML published:2015-05-06 summary:Data in the form of pairwise comparisons arises in many domains, includingpreference elicitation, sporting competitions, and peer grading among others.We consider parametric ordinal models for such pairwise comparison datainvolving a latent vector $w^* \in \mathbb{R}^d$ that represents the"qualities" of the $d$ items being compared; this class of models includes thetwo most widely used parametric models--the Bradley-Terry-Luce (BTL) and theThurstone models. Working within a standard minimax framework, we provide tightupper and lower bounds on the optimal error in estimating the quality scorevector $w^*$ under this class of models. The bounds depend on the topology ofthe comparison graph induced by the subset of pairs being compared via itsLaplacian spectrum. Thus, in settings where the subset of pairs may be chosen,our results provide principled guidelines for making this choice. Finally, wecompare these error rates to those under cardinal measurement models and showthat the error rates in the ordinal and cardinal settings have identicalscalings apart from constant pre-factors.
arxiv-11400-163 | Mining Scientific Papers for Bibliometrics: a (very) Brief Survey of Methods and Tools | http://arxiv.org/abs/1505.01393 | author:Iana Atanassova, Marc Bertin, Philipp Mayr category:cs.DL cs.CL published:2015-05-06 summary:The Open Access movement in scientific publishing and search engines likeGoogle Scholar have made scientific articles more broadly accessible. Duringthe last decade, the availability of scientific papers in full text has becomemore and more widespread thanks to the growing number of publications on onlineplatforms such as ArXiv and CiteSeer. The efforts to provide articles inmachine-readable formats and the rise of Open Access publishing have resultedin a number of standardized formats for scientific papers (such as NLM-JATS,TEI, DocBook). Our aim is to stimulate research at the intersection ofBibliometrics and Computational Linguistics in order to study the waysBibliometrics can benefit from large-scale text analytics and sense mining ofscientific papers, thus exploring the interdisciplinarity of Bibliometrics andNatural Language Processing.
arxiv-11400-164 | Re-scale boosting for regression and classification | http://arxiv.org/abs/1505.01371 | author:Shaobo Lin, Yao Wang, Lin Xu category:cs.LG stat.ML F.2.2 published:2015-05-06 summary:Boosting is a learning scheme that combines weak prediction rules to producea strong composite estimator, with the underlying intuition that one can obtainaccurate prediction rules by combining "rough" ones. Although boosting isproved to be consistent and overfitting-resistant, its numerical convergencerate is relatively slow. The aim of this paper is to develop a new boostingstrategy, called the re-scale boosting (RBoosting), to accelerate the numericalconvergence rate and, consequently, improve the learning performance ofboosting. Our studies show that RBoosting possesses the almost optimalnumerical convergence rate in the sense that, up to a logarithmic factor, itcan reach the minimax nonlinear approximation rate. We then use RBoosting totackle both the classification and regression problems, and deduce a tightgeneralization error estimate. The theoretical and experimental results showthat RBoosting outperforms boosting in terms of generalization.
arxiv-11400-165 | Classification of Occluded Objects using Fast Recurrent Processing | http://arxiv.org/abs/1505.01350 | author:Ozgur Yilmaz category:cs.CV published:2015-05-06 summary:Recurrent neural networks are powerful tools for handling incomplete dataproblems in computer vision, thanks to their significant generativecapabilities. However, the computational demand for these algorithms is toohigh to work in real time, without specialized hardware or software solutions.In this paper, we propose a framework for augmenting recurrent processingcapabilities into a feedforward network without sacrificing much fromcomputational efficiency. We assume a mixture model and generate samples of thelast hidden layer according to the class decisions of the output layer, modifythe hidden layer activity using the samples, and propagate to lower layers. Forvisual occlusion problem, the iterative procedure emulates feedforward-feedbackloop, filling-in the missing hidden layer activity with meaningfulrepresentations. The proposed algorithm is tested on a widely used dataset, andshown to achieve 2$\times$ improvement in classification accuracy for occludedobjects. When compared to Restricted Boltzmann Machines, our algorithm showssuperior performance for occluded object classification.
arxiv-11400-166 | Geometry-Aware Neighborhood Search for Learning Local Models for Image Reconstruction | http://arxiv.org/abs/1505.01429 | author:Julio Cesar Ferreira, Elif Vural, Christine Guillemot category:cs.CV cs.IT math.IT math.OC published:2015-05-06 summary:Local learning of sparse image models has proven to be very effective tosolve inverse problems in many computer vision applications. To learn suchmodels, the data samples are often clustered using the K-means algorithm withthe Euclidean distance as a dissimilarity metric. However, the Euclideandistance may not always be a good dissimilarity measure for comparing datasamples lying on a manifold. In this paper, we propose two algorithms fordetermining a local subset of training samples from which a good local modelcan be computed for reconstructing a given input test sample, where we takeinto account the underlying geometry of the data. The first algorithm, calledAdaptive Geometry-driven Nearest Neighbor search (AGNN), is an adaptive schemewhich can be seen as an out-of-sample extension of the replicator graphclustering method for local model learning. The second method, calledGeometry-driven Overlapping Clusters (GOC), is a less complex nonadaptivealternative for training subset selection. The proposed AGNN and GOC methodsare evaluated in image super-resolution, deblurring and denoising applicationsand shown to outperform spectral clustering, soft clustering, and geodesicdistance based subset selection in most settings.
arxiv-11400-167 | Human Social Interaction Modeling Using Temporal Deep Networks | http://arxiv.org/abs/1505.02137 | author:Mohamed R. Amer, Behjat Siddiquie, Amir Tamrakar, David A. Salter, Brian Lande, Darius Mehri, Ajay Divakaran category:cs.CY cs.LG published:2015-05-06 summary:We present a novel approach to computational modeling of social interactionsbased on modeling of essential social interaction predicates (ESIPs) such asjoint attention and entrainment. Based on sound social psychological theory andmethodology, we collect a new "Tower Game" dataset consisting of audio-visualcapture of dyadic interactions labeled with the ESIPs. We expect this datasetto provide a new avenue for research in computational social interactionmodeling. We propose a novel joint Discriminative Conditional RestrictedBoltzmann Machine (DCRBM) model that combines a discriminative component withthe generative power of CRBMs. Such a combination enables us to uncoveractionable constituents of the ESIPs in two steps. First, we train the DCRBMmodel on the labeled data and get accurate (76\%-49\% across various ESIPs)detection of the predicates. Second, we exploit the generative capability ofDCRBMs to activate the trained model so as to generate the lower-level datacorresponding to the specific ESIP that closely matches the actual trainingdata (with mean square error 0.01-0.1 for generating 100 frames). We are thusable to decompose the ESIPs into their constituent actionable behaviors. Such apurely computational determination of how to establish an ESIP such asengagement is unprecedented.
arxiv-11400-168 | A Comprehensive Study On The Applications Of Machine Learning For Diagnosis Of Cancer | http://arxiv.org/abs/1505.01345 | author:Mohnish Chakravarti, Tanay Kothari category:cs.LG published:2015-05-06 summary:Collectively, lung cancer, breast cancer and melanoma was diagnosed in over535,340 people out of which, 209,400 deaths were reported [13]. It is estimatedthat over 600,000 people will be diagnosed with these forms of cancer in 2015.Most of the deaths from lung cancer, breast cancer and melanoma result due tolate detection. All of these cancers, if detected early, are 100% curable. Inthis study, we develop and evaluate algorithms to diagnose Breast cancer,Melanoma, and Lung cancer. In the first part of the study, we employed anormalised Gradient Descent and an Artificial Neural Network to diagnose breastcancer with an overall accuracy of 91% and 95% respectively. In the second partof the study, an artificial neural network coupled with image processing andanalysis algorithms was employed to achieve an overall accuracy of 93% A naivemobile based application that allowed people to take diagnostic tests on theirphones was developed. Finally, a Support Vector Machine algorithm incorporatingimage processing and image analysis algorithms was developed to diagnose lungcancer with an accuracy of 94%. All of the aforementioned systems had very lowfalse positive and false negative rates. We are developing an online networkthat incorporates all of these systems and allows people to collaborateglobally.
arxiv-11400-169 | Comparing persistence diagrams through complex vectors | http://arxiv.org/abs/1505.01335 | author:Barbara Di Fabio, Massimo Ferri category:math.AT cs.CV published:2015-05-06 summary:The natural pseudo-distance of spaces endowed with filtering functions isprecious for shape classification and retrieval; its optimal estimate comingfrom persistence diagrams is the bottleneck distance, which unfortunatelysuffers from combinatorial explosion. A possible algebraic representation ofpersistence diagrams is offered by complex polynomials; since far polynomialsrepresent far persistence diagrams, a fast comparison of the coefficientvectors can reduce the size of the database to be classified by the bottleneckdistance. This article explores experimentally three transformations fromdiagrams to polynomials and three distances between the complex vectors ofcoefficients.
arxiv-11400-170 | A Fixed-Size Encoding Method for Variable-Length Sequences with its Application to Neural Network Language Models | http://arxiv.org/abs/1505.01504 | author:Shiliang Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou, Lirong Dai category:cs.NE cs.CL cs.LG published:2015-05-06 summary:In this paper, we propose the new fixed-size ordinally-forgetting encoding(FOFE) method, which can almost uniquely encode any variable-length sequence ofwords into a fixed-size representation. FOFE can model the word order in asequence using a simple ordinally-forgetting mechanism according to thepositions of words. In this work, we have applied FOFE to feedforward neuralnetwork language models (FNN-LMs). Experimental results have shown that withoutusing any recurrent feedbacks, FOFE based FNN-LMs can significantly outperformnot only the standard fixed-input FNN-LMs but also the popular RNN-LMs.
arxiv-11400-171 | A Deeper Look at Dataset Bias | http://arxiv.org/abs/1505.01257 | author:Tatiana Tommasi, Novi Patricia, Barbara Caputo, Tinne Tuytelaars category:cs.CV published:2015-05-06 summary:The presence of a bias in each image data collection has recently attracted alot of attention in the computer vision community showing the limits ingeneralization of any learning method trained on a specific dataset. At thesame time, with the rapid development of deep learning architectures, theactivation values of Convolutional Neural Networks (CNN) are emerging asreliable and robust image descriptors. In this paper we propose to verify thepotential of the DeCAF features when facing the dataset bias problem. Weconduct a series of analyses looking at how existing datasets differ among eachother and verifying the performance of existing debiasing methods underdifferent representations. We learn important lessons on which part of thedataset bias problem can be considered solved and which open questions stillneed to be tackled.
arxiv-11400-172 | Fast Differentially Private Matrix Factorization | http://arxiv.org/abs/1505.01419 | author:Ziqi Liu, Yu-Xiang Wang, Alexander J. Smola category:cs.LG cs.AI published:2015-05-06 summary:Differentially private collaborative filtering is a challenging task, both interms of accuracy and speed. We present a simple algorithm that is provablydifferentially private, while offering good performance, using a novelconnection of differential privacy to Bayesian posterior sampling viaStochastic Gradient Langevin Dynamics. Due to its simplicity the algorithmlends itself to efficient implementation. By careful systems design and byexploiting the power law behavior of the data to maximize CPU cache bandwidthwe are able to generate 1024 dimensional models at a rate of 8.5 millionrecommendations per second on a single PC.
arxiv-11400-173 | Cats & Co: Categorical Time Series Coclustering | http://arxiv.org/abs/1505.01300 | author:Dominique Gay, Romain Guigourès, Marc Boullé, Fabrice Clérot category:cs.DB stat.ML H.2.8 published:2015-05-06 summary:We suggest a novel method of clustering and exploratory analysis of temporalevent sequences data (also known as categorical time series) based onthree-dimensional data grid models. A data set of temporal event sequences canbe represented as a data set of three-dimensional points, each point is definedby three variables: a sequence identifier, a time value and an event value.Instantiating data grid models to the 3D-points turns the problem into3D-coclustering. The sequences are partitioned into clusters, the time variable is discretizedinto intervals and the events are partitioned into clusters. The cross-productof the univariate partitions forms a multivariate partition of therepresentation space, i.e., a grid of cells and it also represents anonparametric estimator of the joint distribution of the sequences, time andevents dimensions. Thus, the sequences are grouped together because they havesimilar joint distribution of time and events, i.e., similar distribution ofevents along the time dimension. The best data grid is computed using aparameter-free Bayesian model selection approach. We also suggest severalcriteria for exploiting the resulting grid through agglomerative hierarchies,for interpreting the clusters of sequences and characterizing their componentsthrough insightful visualizations. Extensive experiments on both synthetic andreal-world data sets demonstrate that data grid models are efficient, effectiveand discover meaningful underlying patterns of categorical time series data.
arxiv-11400-174 | On the Feasibility of Distributed Kernel Regression for Big Data | http://arxiv.org/abs/1505.00869 | author:Chen Xu, Yongquan Zhang, Runze Li category:stat.ML published:2015-05-05 summary:In modern scientific research, massive datasets with huge numbers ofobservations are frequently encountered. To facilitate the computationalprocess, a divide-and-conquer scheme is often used for the analysis of bigdata. In such a strategy, a full dataset is first split into several manageablesegments; the final output is then averaged from the individual outputs of thesegments. Despite its popularity in practice, it remains largely unknown thatwhether such a distributive strategy provides valid theoretical inferences tothe original data. In this paper, we address this fundamental issue for thedistributed kernel regression (DKR), where the algorithmic feasibility ismeasured by the generalization performance of the resulting estimator. Tojustify DKR, a uniform convergence rate is needed for bounding thegeneralization error over the individual outputs, which brings new andchallenging issues in the big data setup. Under mild conditions, we show that,with a proper number of segments, DKR leads to an estimator that isgeneralization consistent to the unknown regression function. The obtainedresults justify the method of DKR and shed light on the feasibility of usingother distributed algorithms for processing big data. The promising preferenceof the method is supported by both simulation and real data examples.
arxiv-11400-175 | Contextual Action Recognition with R*CNN | http://arxiv.org/abs/1505.01197 | author:Georgia Gkioxari, Ross Girshick, Jitendra Malik category:cs.CV published:2015-05-05 summary:There are multiple cues in an image which reveal what action a person isperforming. For example, a jogger has a pose that is characteristic forjogging, but the scene (e.g. road, trail) and the presence of other joggers canbe an additional source of information. In this work, we exploit the simpleobservation that actions are accompanied by contextual cues to build a strongaction recognition system. We adapt RCNN to use more than one region forclassification while still maintaining the ability to localize the action. Wecall our system R*CNN. The action-specific models and the feature maps aretrained jointly, allowing for action specific representations to emerge. R*CNNachieves 90.2% mean AP on the PASAL VOC Action dataset, outperforming all otherapproaches in the field by a significant margin. Last, we show that R*CNN isnot limited to action recognition. In particular, R*CNN can also be used totackle fine-grained tasks such as attribute classification. We validate thisclaim by reporting state-of-the-art performance on the Berkeley Attributes ofPeople dataset.
arxiv-11400-176 | Empirical Evaluation of Rectified Activations in Convolutional Network | http://arxiv.org/abs/1505.00853 | author:Bing Xu, Naiyan Wang, Tianqi Chen, Mu Li category:cs.LG cs.CV stat.ML published:2015-05-05 summary:In this paper we investigate the performance of different types of rectifiedactivation functions in convolutional neural network: standard rectified linearunit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectifiedlinear unit (PReLU) and a new randomized leaky rectified linear units (RReLU).We evaluate these activation function on standard image classification task.Our experiments suggest that incorporating a non-zero slope for negative partin rectified activation units could consistently improve the results. Thus ourfindings are negative on the common belief that sparsity is the key of goodperformance in ReLU. Moreover, on small scale dataset, using deterministicnegative slope or learning it are both prone to overfitting. They are not aseffective as using their randomized counterpart. By using RReLU, we achieved75.68\% accuracy on CIFAR-100 test set without multiple test or ensemble.
arxiv-11400-177 | Large-scale Classification of Fine-Art Paintings: Learning The Right Metric on The Right Feature | http://arxiv.org/abs/1505.00855 | author:Babak Saleh, Ahmed Elgammal category:cs.CV cs.IR cs.LG cs.MM published:2015-05-05 summary:In the past few years, the number of fine-art collections that are digitizedand publicly available has been growing rapidly. With the availability of suchlarge collections of digitized artworks comes the need to develop multimediasystems to archive and retrieve this pool of data. Measuring the visualsimilarity between artistic items is an essential step for such multimediasystems, which can benefit more high-level multimedia tasks. In order to modelthis similarity between paintings, we should extract the appropriate visualfeatures for paintings and find out the best approach to learn the similaritymetric based on these features. We investigate a comprehensive list of visualfeatures and metric learning approaches to learn an optimized similaritymeasure between paintings. We develop a machine that is able to makeaesthetic-related semantic-level judgments, such as predicting a painting'sstyle, genre, and artist, as well as providing similarity measures optimizedbased on the knowledge available in the domain of art historicalinterpretation. Our experiments show the value of using this similarity measurefor the aforementioned prediction tasks.
arxiv-11400-178 | A Feature-based Classification Technique for Answering Multi-choice World History Questions | http://arxiv.org/abs/1505.00863 | author:Shuangyong Song, Yao Meng, Zhongguang Zheng, Jun Sun category:cs.IR cs.AI cs.CL 68T50 H.3.4 published:2015-05-05 summary:Our FRDC_QA team participated in the QA-Lab English subtask of the NTCIR-11.In this paper, we describe our system for solving real-world universityentrance exam questions, which are related to world history. Wikipedia is usedas the main external resource for our system. Since problems with choosingright/wrong sentence from multiple sentence choices account for abouttwo-thirds of the total, we individually design a classification based modelfor solving this type of questions. For other types of questions, we alsodesign some simple methods.
arxiv-11400-179 | Learning Style Similarity for Searching Infographics | http://arxiv.org/abs/1505.01214 | author:Babak Saleh, Mira Dontcheva, Aaron Hertzmann, Zhicheng Liu category:cs.GR cs.CV cs.HC cs.IR cs.MM published:2015-05-05 summary:Infographics are complex graphic designs integrating text, images, charts andsketches. Despite the increasing popularity of infographics and the rapidgrowth of online design portfolios, little research investigates how we cantake advantage of these design resources. In this paper we present a method formeasuring the style similarity between infographics. Based on human perceptiondata collected from crowdsourced experiments, we use computer vision andmachine learning algorithms to learn a style similarity metric for infographicdesigns. We evaluate different visual features and learning algorithms and findthat a combination of color histograms and Histograms-of-Gradients (HoG)features is most effective in characterizing the style of infographics. Wedemonstrate our similarity metric on a preliminary image retrieval test.
arxiv-11400-180 | Accurate estimation of influenza epidemics using Google search data via ARGO | http://arxiv.org/abs/1505.00864 | author:Shihao Yang, Mauricio Santillana, S. C. Kou category:stat.AP cs.SI stat.ML published:2015-05-05 summary:Accurate real-time tracking of influenza outbreaks helps public healthofficials make timely and meaningful decisions that could save lives. Wepropose an influenza tracking model, ARGO (AutoRegression with GOogle searchdata), that uses publicly available online search data. In addition to having arigorous statistical foundation, ARGO outperforms all previously availableGoogle-search-based tracking models, including the latest version of Google FluTrends, even though it uses only low-quality search data as input from publiclyavailable Google Trends and Google Correlate websites. ARGO not onlyincorporates the seasonality in influenza epidemics but also captures changesin people's online search behavior over time. ARGO is also flexible,self-correcting, robust, and scalable, making it a potentially powerful toolthat can be used for real-time tracking of other social events at multipletemporal and spatial resolutions.
arxiv-11400-181 | Support Vector Machines for Current Status Data | http://arxiv.org/abs/1505.00991 | author:Yael Travis-Lumer, Yair Goldberg category:math.ST stat.ML stat.TH published:2015-05-05 summary:Current status data is a data format where the time to event is restricted toknowledge of whether or not the failure time exceeds a random monitoring time.We develop a support vector machine learning method for current status datathat estimates the failure time expectation as a function of the covariates. Inorder to obtain the support vector machine decision function, we minimize aregularized version of the empirical risk with respect to a data-dependentloss. We show that the decision function has a closed form. Using finite samplebounds and novel oracle inequalities, we prove that the obtained decisionfunction converges to the true conditional expectation for a large family ofprobability measures and study the associated learning rates. Finally wepresent a simulation study that compares the performance of the proposedapproach to current state of the art.
arxiv-11400-182 | Fast Guided Filter | http://arxiv.org/abs/1505.00996 | author:Kaiming He, Jian Sun category:cs.CV published:2015-05-05 summary:The guided filter is a technique for edge-aware image filtering. Because ofits nice visual quality, fast speed, and ease of implementation, the guidedfilter has witnessed various applications in real products, such as imageediting apps in phones and stereo reconstruction, and has been included inofficial MATLAB and OpenCV. In this note, we remind that the guided filter canbe simply sped up from O(N) time to O(N/s^2) time for a subsampling ratio s. Ina variety of applications, this leads to a speedup of >10x with almost novisible degradation. We hope this acceleration will improve performance ofcurrent applications and further popularize this filter. Code is released.
arxiv-11400-183 | The Configurable SAT Solver Challenge (CSSC) | http://arxiv.org/abs/1505.01221 | author:Frank Hutter, Marius Lindauer, Adrian Balint, Sam Bayless, Holger Hoos, Kevin Leyton-Brown category:cs.AI cs.LG published:2015-05-05 summary:It is well known that different solution strategies work well for differenttypes of instances of hard combinatorial problems. As a consequence, mostsolvers for the propositional satisfiability problem (SAT) expose parametersthat allow them to be customized to a particular family of instances. In theinternational SAT competition series, these parameters are ignored: solvers arerun using a single default parameter setting (supplied by the authors) for allbenchmark instances in a given track. While this competition format rewardssolvers with robust default settings, it does not reflect the situation facedby a practitioner who only cares about performance on one particularapplication and can invest some time into tuning solver parameters for thisapplication. The new Configurable SAT Solver Competition (CSSC) comparessolvers in this latter setting, scoring each solver by the performance itachieved after a fully automated configuration step. This article describes theCSSC in more detail, and reports the results obtained in its two instantiationsso far, CSSC 2013 and 2014.
arxiv-11400-184 | Deep Learning for Object Saliency Detection and Image Segmentation | http://arxiv.org/abs/1505.01173 | author:Hengyue Pan, Bo Wang, Hui Jiang category:cs.CV published:2015-05-05 summary:In this paper, we propose several novel deep learning methods for objectsaliency detection based on the powerful convolutional neural networks. In ourapproach, we use a gradient descent method to iteratively modify an input imagebased on the pixel-wise gradients to reduce a cost function measuring theclass-specific objectness of the image. The pixel-wise gradients can beefficiently computed using the back-propagation algorithm. The discrepancybetween the modified image and the original one may be used as a saliency mapfor the image. Moreover, we have further proposed several new training methodsto learn saliency-specific convolutional nets for object saliency detection, inorder to leverage the available pixel-wise segmentation information. Ourmethods are extremely computationally efficient (processing 20-40 images persecond in one GPU). In this work, we use the computed saliency maps for imagesegmentation. Experimental results on two benchmark tasks, namely MicrosoftCOCO and Pascal VOC 2012, have shown that our proposed methods can generatehigh-quality salience maps, clearly outperforming many existing methods. Inparticular, our approaches excel in handling many difficult images, whichcontain complex background, highly-variable salient objects, multiple objects,and/or very small salient objects.
arxiv-11400-185 | Mining Measured Information from Text | http://arxiv.org/abs/1505.01072 | author:Arun S. Maiya, Dale Visser, Andrew Wan category:cs.CL cs.IR I.2.7; H.3.3 published:2015-05-05 summary:We present an approach to extract measured information from text (e.g., a1370 degrees C melting point, a BMI greater than 29.9 kg/m^2 ). Suchextractions are critically important across a wide range of domains -especially those involving search and exploration of scientific and technicaldocuments. We first propose a rule-based entity extractor to mine measuredquantities (i.e., a numeric value paired with a measurement unit), whichsupports a vast and comprehensive set of both common and obscure measurementunits. Our method is highly robust and can correctly recover valid measuredquantities even when significant errors are introduced through the process ofconverting document formats like PDF to plain text. Next, we describe anapproach to extracting the properties being measured (e.g., the property "pixelpitch" in the phrase "a pixel pitch as high as 352 {\mu}m"). Finally, wepresent MQSearch: the realization of a search engine with full support formeasured information.
arxiv-11400-186 | In Defense of the Direct Perception of Affordances | http://arxiv.org/abs/1505.01085 | author:David F. Fouhey, Xiaolong Wang, Abhinav Gupta category:cs.CV published:2015-05-05 summary:The field of functional recognition or affordance estimation from images hasseen a revival in recent years. As originally proposed by Gibson, theaffordances of a scene were directly perceived from the ambient light: in otherwords, functional properties like sittable were estimated directly fromincoming pixels. Recent work, however, has taken a mediated approach in whichaffordances are derived by first estimating semantics or geometry and thenreasoning about the affordances. In a tribute to Gibson, this paper exploreshis theory of affordances as originally proposed. We propose two approaches fordirect perception of affordances and show that they obtain good results and canout-perform mediated approaches. We hope this paper can rekindle discussionaround direct perception and its implications in the long term.
arxiv-11400-187 | Trees Assembling Mann Whitney Approach for Detecting Genome-wide Joint Association among Low Marginal Effect loci | http://arxiv.org/abs/1505.01206 | author:Changshuai Wei, Daniel J. Schaid, Qing Lu category:q-bio.QM stat.CO stat.ML published:2015-05-05 summary:Common complex diseases are likely influenced by the interplay of hundreds,or even thousands, of genetic variants. Converging evidence shows that geneticvariants with low marginal effects (LME) play an important role in diseasedevelopment. Despite their potential significance, discovering LME geneticvariants and assessing their joint association on high dimensional data (e.g.,genome wide association studies) remain a great challenge. To facilitate jointassociation analysis among a large ensemble of LME genetic variants, weproposed a computationally efficient and powerful approach, which we call TreesAssembling Mann whitney (TAMW). Through simulation studies and an empiricaldata application, we found that TAMW outperformed multifactor dimensionalityreduction (MDR) and the likelihood ratio based Mann whitney approach (LRMW)when the underlying complex disease involves multiple LME loci and theirinteractions. For instance, in a simulation with 20 interacting LME loci, TAMWattained a higher power (power=0.931) than both MDR (power=0.599) and LRMW(power=0.704). In an empirical study of 29 known Crohn's disease (CD) loci,TAMW also identified a stronger joint association with CD than those detectedby MDR and LRMW. Finally, we applied TAMW to Wellcome Trust CD GWAS to conducta genome wide analysis. The analysis of 459K single nucleotide polymorphismswas completed in 40 hours using parallel computing, and revealed a jointassociation predisposing to CD (p-value=2.763e-19). Further analysis of thenewly discovered association suggested that 13 genes, such as ATG16L1 andLACC1, may play an important role in CD pathophysiological and etiologicalprocesses.
arxiv-11400-188 | Adaptive diffusion constrained total variation scheme with application to `cartoon + texture + edge' image decomposition | http://arxiv.org/abs/1505.00866 | author:Juan C. Moreno, V. B. Surya Prasath, D. Vorotnikov, H. Proenca, K. Palaniappan category:cs.CV 68U10 published:2015-05-05 summary:We consider an image decomposition model involving a variational(minimization) problem and an evolutionary partial differential equation (PDE).We utilize a linear inhomogenuous diffusion constrained and weighted totalvariation (TV) scheme for image adaptive decomposition. An adaptive weightalong with TV regularization splits a given image into three componentsrepresenting the geometrical (cartoon), textural (small scale - microtextures),and edges (big scale - macrotextures). We study the wellposedness of thecoupled variational-PDE scheme along with an efficient numerical scheme basedon Chambolle's dual minimization method. We provide extensive experimentalresults in cartoon-texture-edges decomposition, and denoising as well comparewith other related variational, coupled anisotropic diffusion PDE basedmethods.
arxiv-11400-189 | Reinforced Decision Trees | http://arxiv.org/abs/1505.00908 | author:Aurélia Léon, Ludovic Denoyer category:cs.LG published:2015-05-05 summary:In order to speed-up classification models when facing a large number ofcategories, one usual approach consists in organizing the categories in aparticular structure, this structure being then used as a way to speed-up theprediction computation. This is for example the case when usingerror-correcting codes or even hierarchies of categories. But in the majorityof approaches, this structure is chosen \textit{by hand}, or during apreliminary step, and not integrated in the learning process. We propose a newmodel called Reinforced Decision Tree which simultaneously learns how toorganize categories in a tree structure and how to classify any input based onthis structure. This approach keeps the advantages of existing techniques (lowinference complexity) but allows one to build efficient classifiers in onelearning step. The learning algorithm is inspired by reinforcement learning andpolicy-gradient techniques which allows us to integrate the two steps (buildingthe tree, and learning the classifier) in one single algorithm.
arxiv-11400-190 | Ask Your Neurons: A Neural-based Approach to Answering Questions about Images | http://arxiv.org/abs/1505.01121 | author:Mateusz Malinowski, Marcus Rohrbach, Mario Fritz category:cs.CV cs.AI cs.CL published:2015-05-05 summary:We address a question answering task on real-world images that is set up as aVisual Turing Test. By combining latest advances in image representation andnatural language processing, we propose Neural-Image-QA, an end-to-endformulation to this problem for which all parts are trained jointly. Incontrast to previous efforts, we are facing a multi-modal problem where thelanguage output (answer) is conditioned on visual and natural language input(image and question). Our approach Neural-Image-QA doubles the performance ofthe previous best approach on this problem. We provide additional insights intothe problem by analyzing how much information is contained only in the languagepart for which we provide a new human baseline. To study human consensus, whichis related to the ambiguities inherent in this challenging task, we propose twonovel metrics and collect additional answers which extends the original DAQUARdataset to DAQUAR-Consensus.
arxiv-11400-191 | Prediction and Quantification of Individual Athletic Performance | http://arxiv.org/abs/1505.01147 | author:Duncan A. J. Blythe, Franz J. Király category:stat.AP stat.ML published:2015-05-05 summary:We provide scientific foundations for athletic performance prediction on anindividual level, exposing the phenomenology of individual athletic runningperformance in the form of a low-rank model dominated by an individual powerlaw. We present, evaluate, and compare a selection of methods for prediction ofindividual running performance, including our own, \emph{local matrixcompletion} (LMC), which we show to perform best. We also show that manydocumented phenomena in quantitative sports science, such as the form ofscoring tables, the success of existing prediction methods including Riegel'sformula, the Purdy points scheme, the power law for world records performancesand the broken power law for world record speeds may be explained on the basisof our findings in a unified way.
arxiv-11400-192 | Visual Summary of Egocentric Photostreams by Representative Keyframes | http://arxiv.org/abs/1505.01130 | author:Marc Bolaños, Ricard Mestre, Estefanía Talavera, Xavier Giró-i-Nieto, Petia Radeva category:cs.CV cs.IR published:2015-05-05 summary:Building a visual summary from an egocentric photostream captured by alifelogging wearable camera is of high interest for different applications(e.g. memory reinforcement). In this paper, we propose a new summarizationmethod based on keyframes selection that uses visual features extracted bymeans of a convolutional neural network. Our method applies an unsupervisedclustering for dividing the photostreams into events, and finally extracts themost relevant keyframe for each event. We assess the results by applying ablind-taste test on a group of 20 people who assessed the quality of thesummaries.
arxiv-11400-193 | Achieving a Hyperlocal Housing Price Index: Overcoming Data Sparsity by Bayesian Dynamical Modeling of Multiple Data Streams | http://arxiv.org/abs/1505.01164 | author:You Ren, Emily B. Fox, Andrew Bruce category:stat.AP stat.ME stat.ML published:2015-05-05 summary:Understanding how housing values evolve over time is important to policymakers, consumers and real estate professionals. Existing methods forconstructing housing indices are computed at a coarse spatial granularity, suchas metropolitan regions, which can mask or distort price dynamics apparent inlocal markets, such as neighborhoods and census tracts. A challenge in movingto estimates at, for example, the census tract level is the sparsity ofspatiotemporally localized house sales observations. Our work aims ataddressing this challenge by leveraging observations from multiple censustracts discovered to have correlated valuation dynamics. Our proposed Bayesiannonparametric approach builds on the framework of latent factor models toenable a flexible, data-driven method for inferring the clustering ofcorrelated census tracts. We explore methods for scalability andparallelizability of computations, yielding a housing valuation index at thelevel of census tract rather than zip code, and on a monthly basis rather thanquarterly. Our analysis is provided on a large Seattle metropolitan housingdataset.
arxiv-11400-194 | Multi-view Convolutional Neural Networks for 3D Shape Recognition | http://arxiv.org/abs/1505.00880 | author:Hang Su, Subhransu Maji, Evangelos Kalogerakis, Erik Learned-Miller category:cs.CV cs.GR published:2015-05-05 summary:A longstanding question in computer vision concerns the representation of 3Dshapes for recognition: should 3D shapes be represented with descriptorsoperating on their native 3D formats, such as voxel grid or polygon mesh, orcan they be effectively represented with view-based descriptors? We addressthis question in the context of learning to recognize 3D shapes from acollection of their rendered views on 2D images. We first present a standardCNN architecture trained to recognize the shapes' rendered views independentlyof each other, and show that a 3D shape can be recognized even from a singleview at an accuracy far higher than using state-of-the-art 3D shapedescriptors. Recognition rates further increase when multiple views of theshapes are provided. In addition, we present a novel CNN architecture thatcombines information from multiple views of a 3D shape into a single andcompact shape descriptor offering even better recognition performance. The samearchitecture can be applied to accurately recognize human hand-drawn sketchesof shapes. We conclude that a collection of 2D views can be highly informativefor 3D shape recognition and is amenable to emerging CNN architectures andtheir derivatives.
arxiv-11400-195 | An $O(n\log(n))$ Algorithm for Projecting Onto the Ordered Weighted $\ell_1$ Norm Ball | http://arxiv.org/abs/1505.00870 | author:Damek Davis category:math.OC cs.LG published:2015-05-05 summary:The ordered weighted $\ell_1$ (OWL) norm is a newly developed generalizationof the Octogonal Shrinkage and Clustering Algorithm for Regression (OSCAR)norm. This norm has desirable statistical properties and can be used to performsimultaneous clustering and regression. In this paper, we show how to computethe projection of an $n$-dimensional vector onto the OWL norm ball in$O(n\log(n))$ operations. In addition, we illustrate the performance of ouralgorithm on a synthetic regression test.
arxiv-11400-196 | Autoencoding Time Series for Visualisation | http://arxiv.org/abs/1505.00936 | author:Nikolaos Gianniotis, Dennis Kügler, Peter Tino, Kai Polsterer, Ranjeev Misra category:astro-ph.IM cs.NE published:2015-05-05 summary:We present an algorithm for the visualisation of time series. To that end weemploy echo state networks to convert time series into a suitable vectorrepresentation which is capable of capturing the latent dynamics of the timeseries. Subsequently, the obtained vector representations are put through anautoencoder and the visualisation is constructed using the activations of thebottleneck. The crux of the work lies with defining an objective function thatquantifies the reconstruction error of these representations in a principledmanner. We demonstrate the method on synthetic and real data.
arxiv-11400-197 | fastFM: A Library for Factorization Machines | http://arxiv.org/abs/1505.00641 | author:Immanuel Bayer category:cs.LG cs.IR published:2015-05-04 summary:Factorization Machines (FM) are only used in a narrow range of applicationsand are not part of the standard toolbox of machine learning models. This is apity, because even though FMs are recognized as being very successful forrecommender system type applications they are a general model to deal withsparse and high dimensional features. Our Factorization Machine implementationprovides easy access to many solvers and supports regression, classificationand ranking tasks. Such an implementation simplifies the use of FM's for a widefield of applications. This implementation has the potential to improve ourunderstanding of the FM model and drive new development.
arxiv-11400-198 | Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database for Automated Image Interpretation | http://arxiv.org/abs/1505.00670 | author:Hoo-Chang Shin, Le Lu, Lauren Kim, Ari Seff, Jianhua Yao, Ronald M. Summers category:cs.CV cs.LG published:2015-05-04 summary:Despite tremendous progress in computer vision, there has not been an attemptfor machine learning on very large-scale medical image databases. We present aninterleaved text/image deep learning system to extract and mine the semanticinteractions of radiology images and reports from a national researchhospital's Picture Archiving and Communication System. With natural languageprocessing, we mine a collection of representative ~216K two-dimensional keyimages selected by clinicians for diagnostic reference, and match the imageswith their descriptions in an automated manner. Our system interleaves betweenunsupervised learning and supervised learning on document- and sentence-leveltext collections, to generate semantic labels and to predict them given animage. Given an image of a patient scan, semantic topics in radiology levelsare predicted, and associated key-words are generated. Also, a number offrequent disease types are detected as present or absent, to provide morespecific interpretation of a patient scan. This shows the potential oflarge-scale learning and prediction in electronic patient records available inmost modern clinical institutions.
arxiv-11400-199 | A Gaussian Scale Space Approach For Exudates Detection, Classification And Severity Prediction | http://arxiv.org/abs/1505.00737 | author:Mrinal Haloi, Samarendra Dandapat, Rohit Sinha category:cs.CV 68T45 published:2015-05-04 summary:In the context of Computer Aided Diagnosis system for diabetic retinopathy,we present a novel method for detection of exudates and their classificationfor disease severity prediction. The method is based on Gaussian scale spacebased interest map and mathematical morphology. It makes use of support vectormachine for classification and location information of the optic disc and themacula region for severity prediction. It can efficiently handle luminancevariation and it is suitable for varied sized exudates. The method has beenprobed in publicly available DIARETDB1V2 and e-ophthaEX databases. For exudatedetection the proposed method achieved a sensitivity of 96.54% and predictionof 98.35% in DIARETDB1V2 database.
arxiv-11400-200 | Optimal Learning via the Fourier Transform for Sums of Independent Integer Random Variables | http://arxiv.org/abs/1505.00662 | author:Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart category:cs.DS cs.IT cs.LG math.IT math.ST stat.TH published:2015-05-04 summary:We study the structure and learnability of sums of independent integer randomvariables (SIIRVs). For $k \in \mathbb{Z}_{+}$, a $k$-SIIRV of order $n \in\mathbb{Z}_{+}$ is the probability distribution of the sum of $n$ independentrandom variables each supported on $\{0, 1, \dots, k-1\}$. We denote by ${\calS}_{n,k}$ the set of all $k$-SIIRVs of order $n$. In this paper, we tightly characterize the sample and computationalcomplexity of learning $k$-SIIRVs. More precisely, we design a computationallyefficient algorithm that uses $\widetilde{O}(k/\epsilon^2)$ samples, and learnsan arbitrary $k$-SIIRV within error $\epsilon,$ in total variation distance.Moreover, we show that the {\em optimal} sample complexity of this learningproblem is $\Theta((k/\epsilon^2)\sqrt{\log(1/\epsilon)}).$ Our algorithmproceeds by learning the Fourier transform of the target $k$-SIIRV in itseffective support. Its correctness relies on the {\em approximate sparsity} ofthe Fourier transform of $k$-SIIRVs -- a structural property that we establish,roughly stating that the Fourier transform of $k$-SIIRVs has small magnitudeoutside a small set. Along the way we prove several new structural results about $k$-SIIRVs. Asone of our main structural contributions, we give an efficient algorithm toconstruct a sparse {\em proper} $\epsilon$-cover for ${\cal S}_{n,k},$ in totalvariation distance. We also obtain a novel geometric characterization of thespace of $k$-SIIRVs. Our characterization allows us to prove a tight lowerbound on the size of $\epsilon$-covers for ${\cal S}_{n,k}$, and is the keyingredient in our tight sample complexity lower bound. Our approach of exploiting the sparsity of the Fourier transform indistribution learning is general, and has recently found additionalapplications.
arxiv-11400-201 | Self-Expressive Decompositions for Matrix Approximation and Clustering | http://arxiv.org/abs/1505.00824 | author:Eva L. Dyer, Tom A. Goldstein, Raajen Patel, Konrad P. Kording, Richard G. Baraniuk category:cs.IT cs.CV cs.LG math.IT stat.ML published:2015-05-04 summary:Data-aware methods for dimensionality reduction and matrix decomposition aimto find low-dimensional structure in a collection of data. Classical approachesdiscover such structure by learning a basis that can efficiently express thecollection. Recently, "self expression", the idea of using a small subset ofdata vectors to represent the full collection, has been developed as analternative to learning. Here, we introduce a scalable method for computingsparse SElf-Expressive Decompositions (SEED). SEED is a greedy method thatconstructs a basis by sequentially selecting incoherent vectors from thedataset. After forming a basis from a subset of vectors in the dataset, SEEDthen computes a sparse representation of the dataset with respect to thisbasis. We develop sufficient conditions under which SEED exactly represents lowrank matrices and vectors sampled from a unions of independent subspaces. Weshow how SEED can be used in applications ranging from matrix approximation anddenoising to clustering, and apply it to numerous real-world datasets. Ourresults demonstrate that SEED is an attractive low-complexity alternative toother sparse matrix factorization approaches such as sparse PCA andself-expressive methods for clustering.
arxiv-11400-202 | A novel plasticity rule can explain the development of sensorimotor intelligence | http://arxiv.org/abs/1505.00835 | author:Ralf Der, Georg Martius category:cs.RO cs.LG q-bio.NC I.2.9; I.2.6 published:2015-05-04 summary:Grounding autonomous behavior in the nervous system is a fundamentalchallenge for neuroscience. In particular, the self-organized behavioraldevelopment provides more questions than answers. Are there special functionalunits for curiosity, motivation, and creativity? This paper argues that thesefeatures can be grounded in synaptic plasticity itself, without requiring anyhigher level constructs. We propose differential extrinsic plasticity (DEP) asa new synaptic rule for self-learning systems and apply it to a number ofcomplex robotic systems as a test case. Without specifying any purpose or goal,seemingly purposeful and adaptive behavior is developed, displaying a certainlevel of sensorimotor intelligence. These surprising results require no systemspecific modifications of the DEP rule but arise rather from the underlyingmechanism of spontaneous symmetry breaking due to the tightbrain-body-environment coupling. The new synaptic rule is biologicallyplausible and it would be an interesting target for a neurobiolocalinvestigation. We also argue that this neuronal mechanism may have been acatalyst in natural evolution.
arxiv-11400-203 | Reinforcement Learning Neural Turing Machines - Revised | http://arxiv.org/abs/1505.00521 | author:Wojciech Zaremba, Ilya Sutskever category:cs.LG published:2015-05-04 summary:The Neural Turing Machine (NTM) is more expressive than all previouslyconsidered models because of its external memory. It can be viewed as a broadereffort to use abstract external Interfaces and to learn a parametric model thatinteracts with them. The capabilities of a model can be extended by providing it with properInterfaces that interact with the world. These external Interfaces includememory, a database, a search engine, or a piece of software such as a theoremverifier. Some of these Interfaces are provided by the developers of the model.However, many important existing Interfaces, such as databases and searchengines, are discrete. We examine feasibility of learning models to interact with discreteInterfaces. We investigate the following discrete Interfaces: a memory Tape, aninput Tape, and an output Tape. We use a Reinforcement Learning algorithm totrain a neural network that interacts with such Interfaces to solve simplealgorithmic tasks. Our Interfaces are expressive enough to make our modelTuring complete.
arxiv-11400-204 | Modeling Representation of Videos for Anomaly Detection using Deep Learning: A Review | http://arxiv.org/abs/1505.00523 | author:Yong Shean Chong, Yong Haur Tay category:cs.CV published:2015-05-04 summary:This review article surveys the current progresses made toward video-basedanomaly detection. We address the most fundamental aspect for video anomalydetection, that is, video feature representation. Much research works have beendone in finding the right representation to perform anomaly detection in videostreams accurately with an acceptable false alarm rate. However, this is verychallenging due to large variations in environment and human movement, and highspace-time complexity due to huge dimensionality of video data. The weaklysupervised nature of deep learning algorithms can help in learningrepresentations from the video data itself instead of manually designing theright feature for specific scenes. In this paper, we would like to review theexisting methods of modeling video representations using deep learningtechniques for the task of anomaly detection and action recognition.
arxiv-11400-205 | An Explicit Sampling Dependent Spectral Error Bound for Column Subset Selection | http://arxiv.org/abs/1505.00526 | author:Tianbao Yang, Lijun Zhang, Rong Jin, Shenghuo Zhu category:math.NA stat.ML published:2015-05-04 summary:In this paper, we consider the problem of column subset selection. We presenta novel analysis of the spectral norm reconstruction for a simple randomizedalgorithm and establish a new bound that depends explicitly on the samplingprobabilities. The sampling dependent error bound (i) allows us to betterunderstand the tradeoff in the reconstruction error due to samplingprobabilities, (ii) exhibits more insights than existing error bounds thatexploit specific probability distributions, and (iii) implies better samplingdistributions. In particular, we show that a sampling distribution withprobabilities proportional to the square root of the statistical leveragescores is always better than uniform sampling and is better than leverage-basedsampling when the statistical leverage scores are very nonuniform. And bysolving a constrained optimization problem related to the error bound with anefficient bisection search we are able to achieve better performance than usingeither the leverage-based distribution or that proportional to the square rootof the statistical leverage scores. Numerical simulations demonstrate thebenefits of the new sampling distributions for low-rank matrix approximationand least square approximation compared to state-of-the art algorithms.
arxiv-11400-206 | Learning Document Image Binarization from Data | http://arxiv.org/abs/1505.00529 | author:Yue Wu, Stephen Rawls, Wael AbdAlmageed, Premkumar Natarajan category:cs.CV published:2015-05-04 summary:In this paper we present a fully trainable binarization solution for degradeddocument images. Unlike previous attempts that often used simple features witha series of pre- and post-processing, our solution encodes all heuristics aboutwhether or not a pixel is foreground text into a high-dimensional featurevector and learns a more complicated decision function. In particular, weprepare features of three types: 1) existing features for binarization such asintensity [1], contrast [2], [3], and Laplacian [4], [5]; 2) reformulatedfeatures from existing binarization decision functions such those in [6] and[7]; and 3) our newly developed features, namely the Logarithm IntensityPercentile (LIP) and the Relative Darkness Index (RDI). Our initialexperimental results show that using only selected samples (about 1.5% of allavailable training data), we can achieve a binarization performance comparableto those fine-tuned (typically by hand), state-of-the-art methods.Additionally, the trained document binarization classifier shows goodgeneralization capabilities on out-of-domain data.
arxiv-11400-207 | On Regret-Optimal Learning in Decentralized Multi-player Multi-armed Bandits | http://arxiv.org/abs/1505.00553 | author:Naumaan Nayyar, Dileep Kalathil, Rahul Jain category:stat.ML cs.LG published:2015-05-04 summary:We consider the problem of learning in single-player and multiplayermultiarmed bandit models. Bandit problems are classes of online learningproblems that capture exploration versus exploitation tradeoffs. In amultiarmed bandit model, players can pick among many arms, and each play of anarm generates an i.i.d. reward from an unknown distribution. The objective isto design a policy that maximizes the expected reward over a time horizon for asingle player setting and the sum of expected rewards for the multiplayersetting. In the multiplayer setting, arms may give different rewards todifferent players. There is no separate channel for coordination among theplayers. Any attempt at communication is costly and adds to regret. We proposetwo decentralizable policies, $\tt E^3$ ($\tt E$-$\tt cubed$) and $\ttE^3$-$\tt TS$, that can be used in both single player and multiplayer settings.These policies are shown to yield expected regret that grows at most asO($\log^{1+\epsilon} T$). It is well known that $\log T$ is the lower bound onthe rate of growth of regret even in a centralized case. The proposedalgorithms improve on prior work where regret grew at O($\log^2 T$). Morefundamentally, these policies address the question of additional cost incurredin decentralized online learning, suggesting that there is at most an$\epsilon$-factor cost in terms of order of regret. This solves a problem ofrelevance in many domains and had been open for a while.
arxiv-11400-208 | Light-field Microscopy with a Consumer Light-field Camera | http://arxiv.org/abs/1508.03590 | author:Lois Mignard-Debise, Ivo Ihrke category:cs.GR cs.CV published:2015-05-04 summary:We explore the use of inexpensive consumer light- field camera technology forthe purpose of light-field mi- croscopy. Our experiments are based on the Lytro(first gen- eration) camera. Unfortunately, the optical systems of the Lytroand those of microscopes are not compatible, lead- ing to a loss of light-fieldinformation due to angular and spatial vignetting when directly recordingmicroscopic pic- tures. We therefore consider an adaptation of the Lytro op-tical system. We demonstrate that using the Lytro directly as an oc- ularreplacement, leads to unacceptable spatial vignetting. However, we also found asetting that allows the use of the Lytro camera in a virtual imaging mode whichprevents the information loss to a large extent. We analyze the new vir- tualimaging mode and use it in two different setups for im- plementing light-fieldmicroscopy using a Lytro camera. As a practical result, we show that the cameracan be used for low magnification work, as e.g. common in quality control,surface characterization, etc. We achieve a maximum spa- tial resolution ofabout 6.25{\mu}m, albeit at a limited SNR for the side views.
arxiv-11400-209 | See the Difference: Direct Pre-Image Reconstruction and Pose Estimation by Differentiating HOG | http://arxiv.org/abs/1505.00663 | author:Wei-Chen Chiu, Mario Fritz category:cs.CV published:2015-05-04 summary:The Histogram of Oriented Gradient (HOG) descriptor has led to many advancesin computer vision over the last decade and is still part of many state of theart approaches. We realize that the associated feature computation is piecewisedifferentiable and therefore many pipelines which build on HOG can be madedifferentiable. This lends to advanced introspection as well as opportunitiesfor end-to-end optimization. We present our implementation of $\nabla$HOG basedon the auto-differentiation toolbox Chumpy and show applications to pre-imagevisualization and pose estimation which extends the existing differentiablerenderer OpenDR pipeline. Both applications improve on the respectivestate-of-the-art HOG approaches.
arxiv-11400-210 | Unsupervised Learning of Visual Representations using Videos | http://arxiv.org/abs/1505.00687 | author:Xiaolong Wang, Abhinav Gupta category:cs.CV published:2015-05-04 summary:Is strong supervision necessary for learning a good visual representation? Dowe really need millions of semantically-labeled images to train a ConvolutionalNeural Network (CNN)? In this paper, we present a simple yet surprisinglypowerful approach for unsupervised learning of CNN. Specifically, we usehundreds of thousands of unlabeled videos from the web to learn visualrepresentations. Our key idea is that visual tracking provides the supervision.That is, two patches connected by a track should have similar visualrepresentation in deep feature space since they probably belong to the sameobject or object part. We design a Siamese-triplet network with a ranking lossfunction to train this CNN representation. Without using a single image fromImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we trainan ensemble of unsupervised networks that achieves 52% mAP (no bounding boxregression). This performance comes tantalizingly close to itsImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. Wealso show that our unsupervised network can perform competitively in othertasks such as surface-normal estimation.
arxiv-11400-211 | Activity recognition from videos with parallel hypergraph matching on GPUs | http://arxiv.org/abs/1505.00581 | author:Eric Lombardi, Christian Wolf, Oya Celiktutan, Bülent Sankur category:cs.CV published:2015-05-04 summary:In this paper, we propose a method for activity recognition from videos basedon sparse local features and hypergraph matching. We benefit from specialproperties of the temporal domain in the data to derive a sequential and fastgraph matching algorithm for GPUs. Traditionally, graphs and hypergraphs are frequently used to recognizecomplex and often non-rigid patterns in computer vision, either through graphmatching or point-set matching with graphs. Most formulations resort to theminimization of a difficult discrete energy function mixing geometric orstructural terms with data attached terms involving appearance features.Traditional methods solve this minimization problem approximately, for instancewith spectral techniques. In this work, instead of solving the problem approximatively, the exactsolution for the optimal assignment is calculated in parallel on GPUs. Thegraphical structure is simplified and regularized, which allows to derive anefficient recursive minimization algorithm. The algorithm distributessubproblems over the calculation units of a GPU, which solves them in parallel,allowing the system to run faster than real-time on medium-end GPUs.
arxiv-11400-212 | Higher Order Maximum Persistency and Comparison Theorems | http://arxiv.org/abs/1505.00571 | author:Alexander Shekhovtsov category:cs.CV cs.DM math.CO published:2015-05-04 summary:We address combinatorial problems that can be formulated as minimization of apartially separable function of discrete variables (energy minimization ingraphical models, weighted constraint satisfaction, pseudo-Booleanoptimization, 0-1 polynomial programming). For polyhedral relaxations of suchproblems it is generally not true that variables integer in the relaxedsolution will retain the same values in the optimal discrete solution. Thosewhich do are called persistent. Such persistent variables define a part of aglobally optimal solution. Once identified, they can be excluded from theproblem, reducing its size. To any polyhedral relaxation we associate a sufficient condition provingpersistency of a subset of variables. We set up a specially constructed linearprogram which determines the set of persistent variables maximal with respectto the relaxation. The condition improves as the relaxation is tightened andpossesses all its invariances. The proposed framework explains a variety ofexisting methods originating from different areas of research and based ondifferent principles. A theoretical comparison is established that relatesthese methods to the standard linear relaxation and proves that the proposedtechnique identifies same or larger set of persistent variables.
arxiv-11400-213 | Risk Bounds For Mode Clustering | http://arxiv.org/abs/1505.00482 | author:Martin Azizyan, Yen-Chi Chen, Aarti Singh, Larry Wasserman category:math.ST cs.LG stat.ML stat.TH published:2015-05-03 summary:Density mode clustering is a nonparametric clustering method. The clustersare the basins of attraction of the modes of a density estimator. We study therisk of mode-based clustering. We show that the clustering risk over thecluster cores --- the regions where the density is high --- is very small evenin high dimensions. And under a low noise condition, the overall cluster riskis small even beyond the cores, in high dimensions.
arxiv-11400-214 | Kernel Spectral Clustering and applications | http://arxiv.org/abs/1505.00477 | author:Rocco Langone, Raghvendra Mall, Carlos Alzate, Johan A. K. Suykens category:cs.LG stat.ML published:2015-05-03 summary:In this chapter we review the main literature related to kernel spectralclustering (KSC), an approach to clustering cast within a kernel-basedoptimization setting. KSC represents a least-squares support vector machinebased formulation of spectral clustering described by a weighted kernel PCAobjective. Just as in the classifier case, the binary clustering model isexpressed by a hyperplane in a high dimensional space induced by a kernel. Inaddition, the multi-way clustering can be obtained by combining a set of binarydecision functions via an Error Correcting Output Codes (ECOC) encoding scheme.Because of its model-based nature, the KSC method encompasses three main steps:training, validation, testing. In the validation stage model selection isperformed to obtain tuning parameters, like the number of clusters present inthe data. This is a major advantage compared to classical spectral clusteringwhere the determination of the clustering parameters is unclear and relies onheuristics. Once a KSC model is trained on a small subset of the entire data,it is able to generalize well to unseen test points. Beyond the basicformulation, sparse KSC algorithms based on the Incomplete CholeskyDecomposition (ICD) and $L_0$, $L_1, L_0 + L_1$, Group Lasso regularization arereviewed. In that respect, we show how it is possible to handle large scaledata. Also, two possible ways to perform hierarchical clustering and a softclustering method are presented. Finally, real-world applications such as imagesegmentation, power load time-series clustering, document clustering and bigdata learning are considered.
arxiv-11400-215 | Sequence to Sequence -- Video to Text | http://arxiv.org/abs/1505.00487 | author:Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor Darrell, Kate Saenko category:cs.CV published:2015-05-03 summary:Real-world videos often have complex dynamics; and methods for generatingopen-domain video descriptions should be sensitive to temporal structure andallow both input (sequence of frames) and output (sequence of words) ofvariable length. To approach this problem, we propose a novel end-to-endsequence-to-sequence model to generate captions for videos. For this we exploitrecurrent neural networks, specifically LSTMs, which have demonstratedstate-of-the-art performance in image caption generation. Our LSTM model istrained on video-sentence pairs and learns to associate a sequence of videoframes to a sequence of words in order to generate a description of the eventin the video clip. Our model naturally is able to learn the temporal structureof the sequence of frames as well as the sequence model of the generatedsentences, i.e. a language model. We evaluate several variants of our modelthat exploit different visual features on a standard set of YouTube videos andtwo movie description datasets (M-VAD and MPII-MD).
arxiv-11400-216 | Some Theoretical Properties of a Network of Discretely Firing Neurons | http://arxiv.org/abs/1505.00444 | author:Stephen Luttrell category:cs.NE I.2.6; I.5.1 published:2015-05-03 summary:The problem of optimising a network of discretely firing neurons isaddressed. An objective function is introduced which measures the averagenumber of bits that are needed for the network to encode its state. When thisis minimised, it is shown that this leads to a number of results, such astopographic mappings, piecewise linear dependence on the input of theprobability of a neuron firing, and factorial encoder networks.
arxiv-11400-217 | Object Class Detection and Classification using Multi Scale Gradient and Corner Point based Shape Descriptors | http://arxiv.org/abs/1505.00432 | author:Basura Fernando, Sezer Karaoglu, Sajib Kumar Saha category:cs.CV published:2015-05-03 summary:This paper presents a novel multi scale gradient and a corner point basedshape descriptors. The novel multi scale gradient based shape descriptor iscombined with generic Fourier descriptors to extract contour and region basedshape information. Shape information based object class detection andclassification technique with a random forest classifier has been optimized.Proposed integrated descriptor in this paper is robust to rotation, scale,translation, affine deformations, noisy contours and noisy shapes. The newcorner point based interpolated shape descriptor has been exploited for fastobject detection and classification with higher accuracy.
arxiv-11400-218 | Electron Neutrino Classification in Liquid Argon Time Projection Chamber Detector | http://arxiv.org/abs/1505.00424 | author:Piotr Płoński, Dorota Stefan, Robert Sulej, Krzysztof Zaremba category:cs.CV published:2015-05-03 summary:Neutrinos are one of the least known elementary particles. The detection ofneutrinos is an extremely difficult task since they are affected only by weaksub-atomic force or gravity. Therefore large detectors are constructed toreveal neutrino's properties. Among them the Liquid Argon Time ProjectionChamber (LAr-TPC) detectors provide excellent imaging and particleidentification ability for studying neutrinos. The computerized methods forautomatic reconstruction and identification of particles are needed to fullyexploit the potential of the LAr-TPC technique. Herein, the novel method forelectron neutrino classification is presented. The method constructs a featuredescriptor from images of observed event. It characterizes the signaldistribution propagated from vertex of interest, where the particle interactswith the detector medium. The classifier is learned with a constructed featuredescriptor to decide whether the images represent the electron neutrino orcascade produced by photons. The proposed approach assumes that the position ofprimary interaction vertex is known. The method's performance in dependency tothe noise in a primary vertex position and deposited energy of particles isstudied.
arxiv-11400-219 | Detail-preserving and Content-aware Variational Multi-view Stereo Reconstruction | http://arxiv.org/abs/1505.00389 | author:Zhaoxin Li, Kuanquan Wang, Wangmeng Zuo, Deyu Meng, Lei Zhang category:cs.CV published:2015-05-03 summary:Accurate recovery of 3D geometrical surfaces from calibrated 2D multi-viewimages is a fundamental yet active research area in computer vision. Despitethe steady progress in multi-view stereo reconstruction, most existing methodsare still limited in recovering fine-scale details and sharp features whilesuppressing noises, and may fail in reconstructing regions with few textures.To address these limitations, this paper presents a Detail-preserving andContent-aware Variational (DCV) multi-view stereo method, which reconstructsthe 3D surface by alternating between reprojection error minimization and meshdenoising. In reprojection error minimization, we propose a novel inter-imagesimilarity measure, which is effective to preserve fine-scale details of thereconstructed surface and builds a connection between guided image filteringand image registration. In mesh denoising, we propose a content-aware$\ell_{p}$-minimization algorithm by adaptively estimating the $p$ value andregularization parameters based on the current input. It is much more promisingin suppressing noise while preserving sharp features than conventionalisotropic mesh smoothing. Experimental results on benchmark datasetsdemonstrate that our DCV method is capable of recovering more surface details,and obtains cleaner and more accurate reconstructions than state-of-the-artmethods. In particular, our method achieves the best results among allpublished methods on the Middlebury dino ring and dino sparse ring datasets interms of both completeness and accuracy.
arxiv-11400-220 | Structured Block Basis Factorization for Scalable Kernel Matrix Evaluation | http://arxiv.org/abs/1505.00398 | author:Ruoxi Wang, Yingzhou Li, Michael W. Mahoney, Eric Darve category:stat.ML cs.LG cs.NA published:2015-05-03 summary:Kernel matrices are popular in machine learning and scientific computing, butthey are limited by their quadratic complexity in both construction andstorage. It is well-known that as one varies the kernel parameter, e.g., thewidth parameter in radial basis function kernels, the kernel matrix changesfrom a smooth low-rank kernel to a diagonally-dominant and then fully-diagonalkernel. Low-rank approximation methods have been widely-studied, mostly in thefirst case, to reduce the memory storage and the cost of computingmatrix-vector products. Here, we use ideas from scientific computing to proposean extension of these methods to situations where the matrix is notwell-approximated by a low-rank matrix. In particular, we construct anefficient block low-rank approximation method---which we call the Block BasisFactorization---and we show that it has $\mathcal{O}(n)$ complexity in bothtime and memory. Our method works for a wide range of kernel parameters,extending the domain of applicability of low-rank approximation methods, andour empirical results demonstrate the stability (small standard deviation inerror) and superiority over current state-of-art kernel approximationalgorithms.
arxiv-11400-221 | ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks | http://arxiv.org/abs/1505.00393 | author:Francesco Visin, Kyle Kastner, Kyunghyun Cho, Matteo Matteucci, Aaron Courville, Yoshua Bengio category:cs.CV published:2015-05-03 summary:In this paper, we propose a deep neural network architecture for objectrecognition based on recurrent neural networks. The proposed network, calledReNet, replaces the ubiquitous convolution+pooling layer of the deepconvolutional neural network with four recurrent neural networks that sweephorizontally and vertically in both directions across the image. We evaluatethe proposed ReNet on three widely-used benchmark datasets; MNIST, CIFAR-10 andSVHN. The result suggests that ReNet is a viable alternative to the deepconvolutional neural network, and that further investigation is needed.
arxiv-11400-222 | A Linear-Time Particle Gibbs Sampler for Infinite Hidden Markov Models | http://arxiv.org/abs/1505.00428 | author:Nilesh Tripuraneni, Shane Gu, Hong Ge, Zoubin Ghahramani category:stat.ML published:2015-05-03 summary:Infinite Hidden Markov Models (iHMM's) are an attractive, nonparametricgeneralization of the classical Hidden Markov Model which can automaticallyinfer the number of hidden states in the system. However, due to theinfinite-dimensional nature of transition dynamics performing inference in theiHMM is difficult. In this paper, we present an infinite-state Particle Gibbs(PG) algorithm to resample state trajectories for the iHMM. The proposedalgorithm uses an efficient proposal optimized for iHMMs and leverages ancestorsampling to suppress degeneracy of the standard PG algorithm. Our algorithmdemonstrates significant convergence improvements on synthetic and real worlddata sets. Additionally, the infinite-state PG algorithm has linear-timecomplexity in the number of states in the sampler, while competing methodsscale quadratically.
arxiv-11400-223 | Highway Networks | http://arxiv.org/abs/1505.00387 | author:Rupesh Kumar Srivastava, Klaus Greff, Jürgen Schmidhuber category:cs.LG cs.NE 68T01 I.2.6; G.1.6 published:2015-05-03 summary:There is plenty of theoretical and empirical evidence that depth of neuralnetworks is a crucial ingredient for their success. However, network trainingbecomes more difficult with increasing depth and training of very deep networksremains an open problem. In this extended abstract, we introduce a newarchitecture designed to ease gradient-based training of very deep networks. Werefer to networks with this architecture as highway networks, since they allowunimpeded information flow across several layers on "information highways". Thearchitecture is characterized by the use of gating units which learn toregulate the flow of information through a network. Highway networks withhundreds of layers can be trained directly using stochastic gradient descentand with a variety of activation functions, opening up the possibility ofstudying extremely deep and efficient architectures.
arxiv-11400-224 | Visualization of Tradeoff in Evaluation: from Precision-Recall & PN to LIFT, ROC & BIRD | http://arxiv.org/abs/1505.00401 | author:David M. W. Powers category:cs.LG cs.AI cs.IR stat.ME stat.ML published:2015-05-03 summary:Evaluation often aims to reduce the correctness or error characteristics of asystem down to a single number, but that always involves trade-offs. Anotherway of dealing with this is to quote two numbers, such as Recall and Precision,or Sensitivity and Specificity. But it can also be useful to see more thanthis, and a graphical approach can explore sensitivity to cost, prevalence,bias, noise, parameters and hyper-parameters. Moreover, most techniques are implicitly based on two balanced classes, andour ability to visualize graphically is intrinsically two dimensional, but weoften want to visualize in a multiclass context. We review the dichotomousapproaches relating to Precision, Recall, and ROC as well as the related LIFTchart, exploring how they handle unbalanced and multiclass data, and derivingnew probabilistic and information theoretic variants of LIFT that help dealwith the issues associated with the handling of multiple and unbalancedclasses.
arxiv-11400-225 | Making Sense of Hidden Layer Information in Deep Networks by Learning Hierarchical Targets | http://arxiv.org/abs/1505.00384 | author:Abhinav Tushar category:cs.NE cs.LG published:2015-05-03 summary:This paper proposes an architecture for deep neural networks with hiddenlayer branches that learn targets of lower hierarchy than final layer targets.The branches provide a channel for enforcing useful information in hidden layerwhich helps in attaining better accuracy, both for the final layer and hiddenlayers. The shared layers modify their weights using the gradients of all costfunctions higher than the branching layer. This model provides a flexibleinference system with many levels of targets which is modular and can be usedefficiently in situations requiring different levels of results according tocomplexity. This paper applies the idea to a text classification task on 20Newsgroups data set with two level of hierarchical targets and a comparison ismade with training without the use of hidden layer branches.
arxiv-11400-226 | VQA: Visual Question Answering | http://arxiv.org/abs/1505.00468 | author:Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence Zitnick, Dhruv Batra, Devi Parikh category:cs.CL cs.CV published:2015-05-03 summary:We propose the task of free-form and open-ended Visual Question Answering(VQA). Given an image and a natural language question about the image, the taskis to provide an accurate natural language answer. Mirroring real-worldscenarios, such as helping the visually impaired, both the questions andanswers are open-ended. Visual questions selectively target different areas ofan image, including background details and underlying context. As a result, asystem that succeeds at VQA typically needs a more detailed understanding ofthe image and complex reasoning than a system producing generic image captions.Moreover, VQA is amenable to automatic evaluation, since many open-endedanswers contain only a few words or a closed set of answers that can beprovided in a multiple-choice format. We provide a dataset containing ~0.25Mimages, ~0.76M questions, and ~10M answers (www.visualqa.org), and discuss theinformation it provides. Numerous baselines and methods for VQA are providedand compared with human performance.
arxiv-11400-227 | Order-Revealing Encryption and the Hardness of Private Learning | http://arxiv.org/abs/1505.00388 | author:Mark Bun, Mark Zhandry category:cs.CR cs.CC cs.LG published:2015-05-03 summary:An order-revealing encryption scheme gives a public procedure by which twociphertexts can be compared to reveal the ordering of their underlyingplaintexts. We show how to use order-revealing encryption to separatecomputationally efficient PAC learning from efficient $(\epsilon,\delta)$-differentially private PAC learning. That is, we construct a conceptclass that is efficiently PAC learnable, but for which every efficient learnerfails to be differentially private. This answers a question of Kasiviswanathanet al. (FOCS '08, SIAM J. Comput. '11). To prove our result, we give a generic transformation from an order-revealingencryption scheme into one with strongly correct comparison, which enables theconsistent comparison of ciphertexts that are not obtained as the validencryption of any message. We believe this construction may be of independentinterest.
arxiv-11400-228 | Optimal Time-Series Motifs | http://arxiv.org/abs/1505.00423 | author:Josif Grabocka, Nicolas Schilling, Lars Schmidt-Thieme category:cs.AI cs.LG published:2015-05-03 summary:Motifs are the most repetitive/frequent patterns of a time-series. Thediscovery of motifs is crucial for practitioners in order to understand andinterpret the phenomena occurring in sequential data. Currently, motifs aresearched among series sub-sequences, aiming at selecting the most frequentlyoccurring ones. Search-based methods, which try out series sub-sequence asmotif candidates, are currently believed to be the best methods in finding themost frequent patterns. However, this paper proposes an entirely new perspective in finding motifs.We demonstrate that searching is non-optimal since the domain of motifs isrestricted, and instead we propose a principled optimization approach able tofind optimal motifs. We treat the occurrence frequency as a function andtime-series motifs as its parameters, therefore we \textit{learn} the optimalmotifs that maximize the frequency function. In contrast to searching, ourmethod is able to discover the most repetitive patterns (hence optimal), evenin cases where they do not explicitly occur as sub-sequences. Experiments onseveral real-life time-series datasets show that the motifs found by our methodare highly more frequent than the ones found through searching, for exactly thesame distance threshold.
arxiv-11400-229 | On a fast bilateral filtering formulation using functional rearrangements | http://arxiv.org/abs/1505.00412 | author:Gonzalo Galiano, Julián Velasco category:cs.CV 68U10 published:2015-05-03 summary:We introduce an exact reformulation of a broad class of neighborhood filters,among which the bilateral filters, in terms of two functional rearrangements:the decreasing and the relative rearrangements. Independently of the image spatial dimension (one-dimensional signal, image,volume of images, etc.), we reformulate these filters as integral operatorsdefined in a one-dimensional space corresponding to the level sets measures. We prove the equivalence between the usual pixel-based version and therearranged version of the filter. When restricted to the discrete setting, ourreformulation of bilateral filters extends previous results for the so-calledfast bilateral filtering. We, in addition, prove that the solution of thediscrete setting, understood as constant-wise interpolators, converges to thesolution of the continuous setting. Finally, we numerically illustrate computational aspects concerning qualityapproximation and execution time provided by the rearranged formulation.
arxiv-11400-230 | Joint Multi-Leaf Segmentation, Alignment and Tracking from Fluorescence Plant Videos | http://arxiv.org/abs/1505.00353 | author:Xi Yin, Xiaoming Liu, Jin Chen, David M. Kramer category:cs.CV published:2015-05-02 summary:This paper proposes a novel framework for fluorescence plant videoprocessing. Biologists are interested in the leaf level photosynthetic analysiswithin a plant. A prerequisite for such analysis is to segment all leaves,estimate their structures and track them over time. We treat this as a jointmulti-leaf segmentation, alignment, and tracking problem. First, leafsegmentation and alignment are applied on the last frame of a plant video tofind a number of well-aligned leaf candidates. Second, leaf tracking is appliedon the remaining frames with leaf candidate transformation from the previousframe. We form two optimization problems with shared terms in their objectivefunctions for leaf alignment and tracking respectively. Gradient descent isused to solve the proposed optimization problems. A quantitative evaluationframework is formulated to evaluate the performance of our algorithm with threemetrics. Two models are learned to predict the alignment accuracy and detecttracking failure respectively. We also study the limitation of our proposedalignment and tracking framework. Experimental results show the effectiveness,efficiency, and robustness of the proposed method.
arxiv-11400-231 | Learning Temporal Embeddings for Complex Video Analysis | http://arxiv.org/abs/1505.00315 | author:Vignesh Ramanathan, Kevin Tang, Greg Mori, Li Fei-Fei category:cs.CV published:2015-05-02 summary:In this paper, we propose to learn temporal embeddings of video frames forcomplex video analysis. Large quantities of unlabeled video data can be easilyobtained from the Internet. These videos possess the implicit weak label thatthey are sequences of temporally and semantically coherent images. We leveragethis information to learn temporal embeddings for video frames by associatingframes with the temporal context that they appear in. To do this, we propose ascheme for incorporating temporal context based on past and future frames invideos, and compare this to other contextual representations. In addition, weshow how data augmentation using multi-resolution samples and hard negativeshelps to significantly improve the quality of the learned embeddings. Weevaluate various design decisions for learning temporal embeddings, and showthat our embeddings can improve performance for multiple video tasks such asretrieval, classification, and temporal order recovery in unconstrainedInternet video.
arxiv-11400-232 | Deconstructing Principal Component Analysis Using a Data Reconciliation Perspective | http://arxiv.org/abs/1505.00314 | author:Shankar Narasimhan, Nirav Bhatt category:cs.LG cs.SY stat.ME I.2 published:2015-05-02 summary:Data reconciliation (DR) and Principal Component Analysis (PCA) are twopopular data analysis techniques in process industries. Data reconciliation isused to obtain accurate and consistent estimates of variables and parametersfrom erroneous measurements. PCA is primarily used as a method for reducing thedimensionality of high dimensional data and as a preprocessing technique fordenoising measurements. These techniques have been developed and deployedindependently of each other. The primary purpose of this article is toelucidate the close relationship between these two seemingly disparatetechniques. This leads to a unified framework for applying PCA and DR. Further,we show how the two techniques can be deployed together in a collaborative andconsistent manner to process data. The framework has been extended to deal withpartially measured systems and to incorporate partial knowledge available aboutthe process model.
arxiv-11400-233 | Can deep learning help you find the perfect match? | http://arxiv.org/abs/1505.00359 | author:Harm de Vries, Jason Yosinski category:cs.LG published:2015-05-02 summary:Is he/she my type or not? The answer to this question depends on the personalpreferences of the one asking it. The individual process of obtaining a fullanswer may generally be difficult and time consuming, but often an approximateanswer can be obtained simply by looking at a photo of the potential match.Such approximate answers based on visual cues can be produced in a fraction ofa second, a phenomenon that has led to a series of recently successful datingapps in which users rate others positively or negatively using primarily asingle photo. In this paper we explore using convolutional networks to create amodel of an individual's personal preferences based on rated photos. Thisintroduced task is difficult due to the large number of variations in profilepictures and the noise in attractiveness labels. Toward this task we collect adataset comprised of $9364$ pictures and binary labels for each. We compareperformance of convolutional models trained in three ways: first directly onthe collected dataset, second with features transferred from a network trainedto predict gender, and third with features transferred from a network trainedon ImageNet. Our findings show that ImageNet features transfer best, producinga model that attains $68.1\%$ accuracy on the test set and is moderatelysuccessful at predicting matches.
arxiv-11400-234 | Using PCA to Efficiently Represent State Spaces | http://arxiv.org/abs/1505.00322 | author:William Curran, Tim Brys, Matthew Taylor, William Smart category:cs.LG cs.RO published:2015-05-02 summary:Reinforcement learning algorithms need to deal with the exponential growth ofstates and actions when exploring optimal control in high-dimensional spaces.This is known as the curse of dimensionality. By projecting the agent's stateonto a low-dimensional manifold, we can represent the state space in a smallerand more efficient representation. By using this representation duringlearning, the agent can converge to a good policy much faster. We test thisapproach in the Mario Benchmarking Domain. When using dimensionality reductionin Mario, learning converges much faster to a good policy. But, there is acritical convergence-performance trade-off. By projecting onto alow-dimensional manifold, we are ignoring important data. In this paper, weexplore this trade-off of convergence and performance. We find that learning inas few as 4 dimensions (instead of 9), we can improve performance past learningin the full dimensional space at a faster convergence rate.
arxiv-11400-235 | Multi-Object Classification and Unsupervised Scene Understanding Using Deep Learning Features and Latent Tree Probabilistic Models | http://arxiv.org/abs/1505.00308 | author:Tejaswi Nimmagadda, Anima Anandkumar category:cs.CV cs.LG published:2015-05-02 summary:Deep learning has shown state-of-art classification performance on datasetssuch as ImageNet, which contain a single object in each image. However,multi-object classification is far more challenging. We present a unifiedframework which leverages the strengths of multiple machine learning methods,viz deep learning, probabilistic models and kernel methods to obtainstate-of-art performance on Microsoft COCO, consisting of non-iconic images. Weincorporate contextual information in natural images through a conditionallatent tree probabilistic model (CLTM), where the object co-occurrences areconditioned on the extracted fc7 features from pre-trained Imagenet CNN asinput. We learn the CLTM tree structure using conditional pairwiseprobabilities for object co-occurrences, estimated through kernel methods, andwe learn its node and edge potentials by training a new 3-layer neural network,which takes fc7 features as input. Object classification is carried out viainference on the learnt conditional tree model, and we obtain significant gainin precision-recall and F-measures on MS-COCO, especially for difficult objectcategories. Moreover, the latent variables in the CLTM capture sceneinformation: the images with top activations for a latent node have commonthemes such as being a grasslands or a food scene, and on on. In addition, weshow that a simple k-means clustering of the inferred latent nodes alonesignificantly improves scene classification performance on the MIT-Indoordataset, without the need for any retraining, and without using scene labelsduring training. Thus, we present a unified framework for multi-objectclassification and unsupervised scene understanding.
arxiv-11400-236 | Object-Scene Convolutional Neural Networks for Event Recognition in Images | http://arxiv.org/abs/1505.00296 | author:Limin Wang, Zhe Wang, Wenbin Du, Yu Qiao category:cs.CV published:2015-05-02 summary:Event recognition from still images is of great importance for imageunderstanding. However, compared with event recognition in videos, there aremuch fewer research works on event recognition in images. This paper addressesthe issue of event recognition from images and proposes an effective methodwith deep neural networks. Specifically, we design a new architecture, calledObject-Scene Convolutional Neural Network (OS-CNN). This architecture isdecomposed into object net and scene net, which extract useful information forevent understanding from the perspective of objects and scene context,respectively. Meanwhile, we investigate different network architectures forOS-CNN design, and adapt the deep (AlexNet) and very-deep (GoogLeNet) networksto the task of event recognition. Furthermore, we find that the deep andvery-deep networks are complementary to each other. Finally, based on theproposed OS-CNN and comparative study of different network architectures, wecome up with a solution of five-stream CNN for the track of cultural eventrecognition at the ChaLearn Looking at People (LAP) challenge 2015. Our methodobtains the performance of 85.5% and ranks the $1^{st}$ place in thischallenge.
arxiv-11400-237 | Dense Optical Flow Prediction from a Static Image | http://arxiv.org/abs/1505.00295 | author:Jacob Walker, Abhinav Gupta, Martial Hebert category:cs.CV published:2015-05-02 summary:Given a scene, what is going to move, and in what direction will it move?Such a question could be considered a non-semantic form of action prediction.In this work, we present a convolutional neural network (CNN) based approachfor motion prediction. Given a static image, this CNN predicts the futuremotion of each and every pixel in the image in terms of optical flow. Our CNNmodel leverages the data in tens of thousands of realistic videos to train ourmodel. Our method relies on absolutely no human labeling and is able to predictmotion based on the context of the scene. Because our CNN model makes noassumptions about the underlying scene, it can predict future optical flow on adiverse set of scenarios. We outperform all previous approaches by largemargins.
arxiv-11400-238 | Pose Induction for Novel Object Categories | http://arxiv.org/abs/1505.00066 | author:Shubham Tulsiani, João Carreira, Jitendra Malik category:cs.CV published:2015-05-01 summary:We address the task of predicting pose for objects of unannotated objectcategories from a small seed set of annotated object classes. We present ageneralized classifier that can reliably induce pose given a single instance ofa novel category. In case of availability of a large collection of novelinstances, our approach then jointly reasons over all instances to improve theinitial estimates. We empirically validate the various components of ouralgorithm and quantitatively show that our method produces reliable poseestimates. We also show qualitative results on a diverse set of classes andfurther demonstrate the applicability of our system for learning shape modelsof novel object classes.
arxiv-11400-239 | DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving | http://arxiv.org/abs/1505.00256 | author:Chenyi Chen, Ari Seff, Alain Kornhauser, Jianxiong Xiao category:cs.CV published:2015-05-01 summary:Today, there are two major paradigms for vision-based autonomous drivingsystems: mediated perception approaches that parse an entire scene to make adriving decision, and behavior reflex approaches that directly map an inputimage to a driving action by a regressor. In this paper, we propose a thirdparadigm: a direct perception approach to estimate the affordance for driving.We propose to map an input image to a small number of key perception indicatorsthat directly relate to the affordance of a road/traffic state for driving. Ourrepresentation provides a set of compact yet complete descriptions of the sceneto enable a simple controller to drive autonomously. Falling in between the twoextremes of mediated perception and behavior reflex, we argue that our directperception representation provides the right level of abstraction. Todemonstrate this, we train a deep Convolutional Neural Network using recordingfrom 12 hours of human driving in a video game and show that our model can workwell to drive a car in a very diverse set of virtual environments. We alsotrain a model for car distance estimation on the KITTI dataset. Results showthat our direct perception approach can generalize well to real driving images.Source code and data are available on our project website.
arxiv-11400-240 | Hierarchy of Scales in Language Dynamics | http://arxiv.org/abs/1505.00122 | author:Richard A. Blythe category:physics.soc-ph cs.CL published:2015-05-01 summary:Methods and insights from statistical physics are finding an increasingvariety of applications where one seeks to understand the emergent propertiesof a complex interacting system. One such area concerns the dynamics oflanguage at a variety of levels of description, from the behaviour ofindividual agents learning simple artificial languages from each other, up tochanges in the structure of languages shared by large groups of speakers overhistorical timescales. In this Colloquium, we survey a hierarchy of scales atwhich language and linguistic behaviour can be described, along with the mainprogress in understanding that has been made at each of them---much of whichhas come from the statistical physics community. We argue that futuredevelopments may arise by linking the different levels of the hierarchytogether in a more coherent fashion, in particular where this allows moreeffective use of rich empirical data sets.
arxiv-11400-241 | Segmentation and Restoration of Images on Surfaces by Parametric Active Contours with Topology Changes | http://arxiv.org/abs/1505.00193 | author:Heike Benninghoff, Harald Garcke category:cs.CV math.AP math.NA published:2015-05-01 summary:In this article, a new method for segmentation and restoration of images ontwo-dimensional surfaces is given. Active contour models for image segmentationare extended to images on surfaces. The evolving curves on the surfaces aremathematically described using a parametric approach. For image restoration, adiffusion equation with Neumann boundary conditions is solved in apostprocessing step in the individual regions. Numerical schemes are presentedwhich allow to efficiently compute segmentations and denoised versions ofimages on surfaces. Also topology changes of the evolving curves are detectedand performed using a fast sub-routine. Finally, several experiments arepresented where the developed methods are applied on different artificial andreal images defined on different surfaces.
arxiv-11400-242 | Volumetric Bias in Segmentation and Reconstruction: Secrets and Solutions | http://arxiv.org/abs/1505.00218 | author:Yuri Boykov, Hossam Isack, Carl Olsson, Ismail Ben Ayed category:cs.CV published:2015-05-01 summary:Many standard optimization methods for segmentation and reconstructioncompute ML model estimates for appearance or geometry of segments, e.g.Zhu-Yuille 1996, Torr 1998, Chan-Vese 2001, GrabCut 2004, Delong et al. 2012.We observe that the standard likelihood term in these formulations correspondsto a generalized probabilistic K-means energy. In learning it is well knownthat this energy has a strong bias to clusters of equal size, which can beexpressed as a penalty for KL divergence from a uniform distribution ofcardinalities. However, this volumetric bias has been mostly ignored incomputer vision. We demonstrate significant artifacts in standard segmentationand reconstruction methods due to this bias. Moreover, we propose binary andmulti-label optimization techniques that either (a) remove this bias or (b)replace it by a KL divergence term for any given target volume distribution.Our general ideas apply to many continuous or discrete energy formulations insegmentation, stereo, and other reconstruction problems.
arxiv-11400-243 | Theory of Optimizing Pseudolinear Performance Measures: Application to F-measure | http://arxiv.org/abs/1505.00199 | author:Shameem A Puthiya Parambath, Nicolas Usunier, Yves Grandvalet category:cs.LG published:2015-05-01 summary:Non-linear performance measures are widely used for the evaluation oflearning algorithms. For example, $F$-measure is a commonly used performancemeasure for classification problems in machine learning and informationretrieval community. We study the theoretical properties of a subset ofnon-linear performance measures called pseudo-linear performance measures whichincludes $F$-measure, \emph{Jaccard Index}, among many others. We establishthat many notions of $F$-measures and \emph{Jaccard Index} are pseudo-linearfunctions of the per-class false negatives and false positives for binary,multiclass and multilabel classification. Based on this observation, we presenta general reduction of such performance measure optimization problem tocost-sensitive classification problem with unknown costs. We then propose analgorithm with provable guarantees to obtain an approximately optimalclassifier for the $F$-measure by solving a series of cost-sensitiveclassification problems. The strength of our analysis is to be valid on anydataset and any class of classifiers, extending the existing theoreticalresults on pseudo-linear measures, which are asymptotic in nature. We alsoestablish the multi-objective nature of the $F$-score maximization problem bylinking the algorithm with the weighted-sum approach used in multi-objectiveoptimization. We present numerical experiments to illustrate the relativeimportance of cost asymmetry and thresholding when learning linear classifierson various $F$-measure optimization tasks.
arxiv-11400-244 | The Cross-Depiction Problem: Computer Vision Algorithms for Recognising Objects in Artwork and in Photographs | http://arxiv.org/abs/1505.00110 | author:Hongping Cai, Qi Wu, Tadeo Corradi, Peter Hall category:cs.CV 68745 I.2.10 published:2015-05-01 summary:The cross-depiction problem is that of recognising visual objects regardlessof whether they are photographed, painted, drawn, etc. It is a potentiallysignificant yet under-researched problem. Emulating the remarkable humanability to recognise objects in an astonishingly wide variety of depictiveforms is likely to advance both the foundations and the applications ofComputer Vision. In this paper we benchmark classification, domain adaptation, and deeplearning methods; demonstrating that none perform consistently well in thecross-depiction problem. Given the current interest in deep learning, the factsuch methods exhibit the same behaviour as all but one other method: they showa significant fall in performance over inhomogeneous databases compared totheir peak performance, which is always over data comprising photographs only.Rather, we find the methods that have strong models of spatial relationsbetween parts tend to be more robust and therefore conclude that suchinformation is important in modelling object classes regardless of appearancedetails.
arxiv-11400-245 | Quality Control in Crowdsourced Object Segmentation | http://arxiv.org/abs/1505.00145 | author:Ferran Cabezas, Axel Carlier, Amaia Salvador, Xavier Giró-i-Nieto, Vincent Charvillat category:cs.CV cs.HC published:2015-05-01 summary:This paper explores processing techniques to deal with noisy data incrowdsourced object segmentation tasks. We use the data collected with"Click'n'Cut", an online interactive segmentation tool, and we perform severalexperiments towards improving the segmentation results. First, we introducedifferent superpixel-based techniques to filter users' traces, and assess theirimpact on the segmentation result. Second, we present different criteria todetect and discard the traces from potential bad users, resulting in aremarkable increase in performance. Finally, we show a novel superpixel-basedsegmentation algorithm which does not require any prior filtering and is basedon weighting each user's contribution according to his/her level of expertise.
arxiv-11400-246 | Image Segmentation by Size-Dependent Single Linkage Clustering of a Watershed Basin Graph | http://arxiv.org/abs/1505.00249 | author:Aleksandar Zlateski, H. Sebastian Seung category:cs.CV published:2015-05-01 summary:We present a method for hierarchical image segmentation that defines adisaffinity graph on the image, over-segments it into watershed basins, definesa new graph on the basins, and then merges basins with a modified,size-dependent version of single linkage clustering. The quasilinear runtime ofthe method makes it suitable for segmenting large images. We illustrate themethod on the challenging problem of segmenting 3D electron microscopic brainimages.
arxiv-11400-247 | Joint Object and Part Segmentation using Deep Learned Potentials | http://arxiv.org/abs/1505.00276 | author:Peng Wang, Xiaohui Shen, Zhe Lin, Scott Cohen, Brian Price, Alan Yuille category:cs.CV published:2015-05-01 summary:Segmenting semantic objects from images and parsing them into theirrespective semantic parts are fundamental steps towards detailed objectunderstanding in computer vision. In this paper, we propose a joint solutionthat tackles semantic object and part segmentation simultaneously, in whichhigher object-level context is provided to guide part segmentation, and moredetailed part-level localization is utilized to refine object segmentation.Specifically, we first introduce the concept of semantic compositional parts(SCP) in which similar semantic parts are grouped and shared among differentobjects. A two-channel fully convolutional network (FCN) is then trained toprovide the SCP and object potentials at each pixel. At the same time, acompact set of segments can also be obtained from the SCP predictions of thenetwork. Given the potentials and the generated segments, in order to explorelong-range context, we finally construct an efficient fully connectedconditional random field (FCRF) to jointly predict the final object and partlabels. Extensive evaluation on three different datasets shows that ourapproach can mutually enhance the performance of object and part segmentation,and outperforms the current state-of-the-art on both tasks.
arxiv-11400-248 | A Cooperative Framework for Fireworks Algorithm | http://arxiv.org/abs/1505.00075 | author:Shaoqiu Zheng, Junzhi Li, Andreas Janecek, Ying Tan category:cs.NE published:2015-05-01 summary:This paper presents a cooperative framework for fireworks algorithm (CoFFWA).A detailed analysis of existing fireworks algorithm (FWA) and its recentlydeveloped variants has revealed that (i) the selection strategy lead to thecontribution of the firework with the best fitness (core firework) for theoptimization overwhelms the contributions of the rest of fireworks (non-corefireworks) in the explosion operator, (ii) the Gaussian mutation operator isnot as effective as it is designed to be. To overcome these limitations, theCoFFWA is proposed, which can greatly enhance the exploitation ability ofnon-core fireworks by using independent selection operator and increase theexploration capacity by crowdness-avoiding cooperative strategy among thefireworks. Experimental results on the CEC2013 benchmark functions suggest thatCoFFWA outperforms the state-of-the-art FWA variants, artificial bee colony,differential evolution, the standard particle swarm optimization (SPSO) in 2007and the most recent SPSO in 2011 in term of convergence performance.
arxiv-11400-249 | SynthCam3D: Semantic Understanding With Synthetic Indoor Scenes | http://arxiv.org/abs/1505.00171 | author:Ankur Handa, Viorica Patraucean, Vijay Badrinarayanan, Simon Stent, Roberto Cipolla category:cs.CV published:2015-05-01 summary:We are interested in automatic scene understanding from geometric cues. Tothis end, we aim to bring semantic segmentation in the loop of real-timereconstruction. Our semantic segmentation is built on a deep autoencoder stacktrained exclusively on synthetic depth data generated from our novel 3D scenelibrary, SynthCam3D. Importantly, our network is able to segment real worldscenes without any noise modelling. We present encouraging preliminary results.
arxiv-11400-250 | Monotonous (Semi-)Nonnegative Matrix Factorization | http://arxiv.org/abs/1505.00294 | author:Nirav Bhatt, Arun Ayyar category:cs.LG stat.ML I.2 published:2015-05-01 summary:Nonnegative matrix factorization (NMF) factorizes a non-negative matrix intoproduct of two non-negative matrices, namely a signal matrix and a mixingmatrix. NMF suffers from the scale and ordering ambiguities. Often, the sourcesignals can be monotonous in nature. For example, in source separation problem,the source signals can be monotonously increasing or decreasing while themixing matrix can have nonnegative entries. NMF methods may not be effectivefor such cases as it suffers from the ordering ambiguity. This paper proposesan approach to incorporate notion of monotonicity in NMF, labeled as monotonousNMF. An algorithm based on alternating least-squares is proposed for recoveringmonotonous signals from a data matrix. Further, the assumption on mixing matrixis relaxed to extend monotonous NMF for data matrix with real numbers asentries. The approach is illustrated using synthetic noisy data. The resultsobtained by monotonous NMF are compared with standard NMF algorithms in theliterature, and it is shown that monotonous NMF estimates source signals wellin comparison to standard NMF algorithms when the underlying sources signalsare monotonous.
arxiv-11400-251 | Compositional Distributional Semantics with Compact Closed Categories and Frobenius Algebras | http://arxiv.org/abs/1505.00138 | author:Dimitri Kartsaklis category:cs.CL cs.AI math.CT math.QA quant-ph published:2015-05-01 summary:This thesis contributes to ongoing research related to the categoricalcompositional model for natural language of Coecke, Sadrzadeh and Clark inthree ways: Firstly, I propose a concrete instantiation of the abstractframework based on Frobenius algebras (joint work with Sadrzadeh). The theoryimproves shortcomings of previous proposals, extends the coverage of thelanguage, and is supported by experimental work that improves existing results.The proposed framework describes a new class of compositional models that findintuitive interpretations for a number of linguistic phenomena. Secondly, Ipropose and evaluate in practice a new compositional methodology whichexplicitly deals with the different levels of lexical ambiguity (joint workwith Pulman). A concrete algorithm is presented, based on the separation ofvector disambiguation from composition in an explicit prior step. Extensiveexperimental work shows that the proposed methodology indeed results in moreaccurate composite representations for the framework of Coecke et al. inparticular and every other class of compositional models in general. As a lastcontribution, I formalize the explicit treatment of lexical ambiguity in thecontext of the categorical framework by resorting to categorical quantummechanics (joint work with Coecke). In the proposed extension, the concept of adistributional vector is replaced with that of a density matrix, whichcompactly represents a probability distribution over the potential differentmeanings of the specific word. Composition takes the form of quantummeasurements, leading to interesting analogies between quantum physics andlinguistics.
arxiv-11400-252 | Grounded Discovery of Coordinate Term Relationships between Software Entities | http://arxiv.org/abs/1505.00277 | author:Dana Movshovitz-Attias, William W. Cohen category:cs.CL cs.AI cs.LG cs.SE published:2015-05-01 summary:We present an approach for the detection of coordinate-term relationshipsbetween entities from the software domain, that refer to Java classes. Usually,relations are found by examining corpus statistics associated with textentities. In some technical domains, however, we have access to additionalinformation about the real-world objects named by the entities, suggesting thatcoupling information about the "grounded" entities with corpus statistics mightlead to improved methods for relation discovery. To this end, we develop asimilarity measure for Java classes using distributional information about howthey are used in software, which we combine with corpus statistics on thedistribution of contexts in which the classes appear in text. Using ourapproach, cross-validation accuracy on this dataset can be improveddramatically, from around 60% to 88%. Human labeling results show that ourclassifier has an F1 score of 86% over the top 1000 predicted pairs.
arxiv-11400-253 | Embedding Semantic Relations into Word Representations | http://arxiv.org/abs/1505.00161 | author:Danushka Bollegala, Takanori Maehara, Ken-ichi Kawarabayashi category:cs.CL published:2015-05-01 summary:Learning representations for semantic relations is important for varioustasks such as analogy detection, relational search, and relationclassification. Although there have been several proposals for learningrepresentations for individual words, learning word representations thatexplicitly capture the semantic relations between words remains underdeveloped. We propose an unsupervised method for learning vectorrepresentations for words such that the learnt representations are sensitive tothe semantic relations that exist between two words. First, we extract lexicalpatterns from the co-occurrence contexts of two words in a corpus to representthe semantic relations that exist between those two words. Second, we representa lexical pattern as the weighted sum of the representations of the words thatco-occur with that lexical pattern. Third, we train a binary classifier todetect relationally similar vs. non-similar lexical pattern pairs. The proposedmethod is unsupervised in the sense that the lexical pattern pairs we use astrain data are automatically sampled from a corpus, without requiring anymanual intervention. Our proposed method statistically significantlyoutperforms the current state-of-the-art word representations on threebenchmark datasets for proportional analogy detection, demonstrating itsability to accurately capture the semantic relations among words.
arxiv-11400-254 | Stick-Breaking Policy Learning in Dec-POMDPs | http://arxiv.org/abs/1505.00274 | author:Miao Liu, Christopher Amato, Xuejun Liao, Lawrence Carin, Jonathan P. How category:cs.AI cs.SY stat.ML published:2015-05-01 summary:Expectation maximization (EM) has recently been shown to be an efficientalgorithm for learning finite-state controllers (FSCs) in large decentralizedPOMDPs (Dec-POMDPs). However, current methods use fixed-size FSCs and oftenconverge to maxima that are far from optimal. This paper considers avariable-size FSC to represent the local policy of each agent. Thesevariable-size FSCs are constructed using a stick-breaking prior, leading to anew framework called \emph{decentralized stick-breaking policy representation}(Dec-SBPR). This approach learns the controller parameters with a variationalBayesian algorithm without having to assume that the Dec-POMDP model isavailable. The performance of Dec-SBPR is demonstrated on several benchmarkproblems, showing that the algorithm scales to large problems whileoutperforming other state-of-the-art methods.
arxiv-11400-255 | Thompson Sampling for Budgeted Multi-armed Bandits | http://arxiv.org/abs/1505.00146 | author:Yingce Xia, Haifang Li, Tao Qin, Nenghai Yu, Tie-Yan Liu category:cs.LG published:2015-05-01 summary:Thompson sampling is one of the earliest randomized algorithms formulti-armed bandits (MAB). In this paper, we extend the Thompson sampling toBudgeted MAB, where there is random cost for pulling an arm and the total costis constrained by a budget. We start with the case of Bernoulli bandits, inwhich the random rewards (costs) of an arm are independently sampled from aBernoulli distribution. To implement the Thompson sampling algorithm in thiscase, at each round, we sample two numbers from the posterior distributions ofthe reward and cost for each arm, obtain their ratio, select the arm with themaximum ratio, and then update the posterior distributions. We prove that thedistribution-dependent regret bound of this algorithm is $O(\ln B)$, where $B$denotes the budget. By introducing a Bernoulli trial, we further extend thisalgorithm to the setting that the rewards (costs) are drawn from generaldistributions, and prove that its regret bound remains almost the same. Oursimulation results demonstrate the effectiveness of the proposed algorithm.
arxiv-11400-256 | Algorithms for Lipschitz Learning on Graphs | http://arxiv.org/abs/1505.00290 | author:Rasmus Kyng, Anup Rao, Sushant Sachdeva, Daniel A. Spielman category:cs.LG cs.DS math.MG published:2015-05-01 summary:We develop fast algorithms for solving regression problems on graphs whereone is given the value of a function at some vertices, and must find itssmoothest possible extension to all vertices. The extension we compute is theabsolutely minimal Lipschitz extension, and is the limit for large $p$ of$p$-Laplacian regularization. We present an algorithm that computes a minimalLipschitz extension in expected linear time, and an algorithm that computes anabsolutely minimal Lipschitz extension in expected time $\widetilde{O} (m n)$.The latter algorithm has variants that seem to run much faster in practice.These extensions are particularly amenable to regularization: we can perform$l_{0}$-regularization on the given values in polynomial time and$l_{1}$-regularization on the initial function values and on graph edge weightsin time $\widetilde{O} (m^{3/2})$.
arxiv-11400-257 | Image Denoising using Optimally Weighted Bilateral Filters: A Sure and Fast Approach | http://arxiv.org/abs/1505.00074 | author:Kunal N. Chaudhury, Kollipara Rithwik category:cs.CV published:2015-05-01 summary:The bilateral filter is known to be quite effective in denoising imagescorrupted with small dosages of additive Gaussian noise. The denoisingperformance of the filter, however, is known to degrade quickly with theincrease in noise level. Several adaptations of the filter have been proposedin the literature to address this shortcoming, but often at a substantialcomputational overhead. In this paper, we report a simple pre-processing stepthat can substantially improve the denoising performance of the bilateralfilter, at almost no additional cost. The modified filter is designed to berobust at large noise levels, and often tends to perform poorly below a certainnoise threshold. To get the best of the original and the modified filter, wepropose to combine them in a weighted fashion, where the weights are chosen tominimize (a surrogate of) the oracle mean-squared-error (MSE). Theoptimally-weighted filter is thus guaranteed to perform better than either ofthe component filters in terms of the MSE, at all noise levels. We also providea fast algorithm for the weighted filtering. Visual and quantitative denoisingresults on standard test images are reported which demonstrate that theimprovement over the original filter is significant both visually and in termsof PSNR. Moreover, the denoising performance of the optimally-weightedbilateral filter is competitive with the computation-intensive non-local meansfilter.
arxiv-11400-258 | Fast and Accurate Bilateral Filtering using Gauss-Polynomial Decomposition | http://arxiv.org/abs/1505.00077 | author:Kunal N. Chaudhury category:cs.CV published:2015-05-01 summary:The bilateral filter is a versatile non-linear filter that has found diverseapplications in image processing, computer vision, computer graphics, andcomputational photography. A widely-used form of the filter is the Gaussianbilateral filter in which both the spatial and range kernels are Gaussian. Adirect implementation of this filter requires $O(\sigma^2)$ operations perpixel, where $\sigma$ is the standard deviation of the spatial Gaussian. Inthis paper, we propose an accurate approximation algorithm that can cut downthe computational complexity to $O(1)$ per pixel for any arbitrary $\sigma$(constant-time implementation). This is based on the observation that the rangekernel operates via the translations of a fixed Gaussian over the range space,and that these translated Gaussians can be accurately approximated using theso-called Gauss-polynomials. The overall algorithm emerging from thisapproximation involves a series of spatial Gaussian filtering, which can beimplemented in constant-time using separability and recursion. We present somepreliminary results to demonstrate that the proposed algorithm comparesfavorably with some of the existing fast algorithms in terms of speed andaccuracy.
arxiv-11400-259 | Semi-Orthogonal Multilinear PCA with Relaxed Start | http://arxiv.org/abs/1504.08142 | author:Qiquan Shi, Haiping Lu category:stat.ML cs.CV cs.LG I.2.6 published:2015-04-30 summary:Principal component analysis (PCA) is an unsupervised method for learninglow-dimensional features with orthogonal projections. Multilinear PCA methodsextend PCA to deal with multidimensional data (tensors) directly viatensor-to-tensor projection or tensor-to-vector projection (TVP). However,under the TVP setting, it is difficult to develop an effective multilinear PCAmethod with the orthogonality constraint. This paper tackles this problem byproposing a novel Semi-Orthogonal Multilinear PCA (SO-MPCA) approach. SO-MPCAlearns low-dimensional features directly from tensors via TVP by imposing theorthogonality constraint in only one mode. This formulation results in morecaptured variance and more learned features than full orthogonality. For bettergeneralization, we further introduce a relaxed start (RS) strategy to getSO-MPCA-RS by fixing the starting projection vectors, which increases the biasand reduces the variance of the learning model. Experiments on both face (2D)and gait (3D) data demonstrate that SO-MPCA-RS outperforms other competingalgorithms on the whole, and the relaxed start strategy is also effective forother TVP-based PCA methods.
arxiv-11400-260 | Efficient Image-Space Extraction and Representation of 3D Surface Topography | http://arxiv.org/abs/1504.08308 | author:Matthias Zeppelzauer, Markus Seidl category:cs.CV published:2015-04-30 summary:Surface topography refers to the geometric micro-structure of a surface anddefines its tactile characteristics (typically in the sub-millimeter range).High-resolution 3D scanning techniques developed recently enable the 3Dreconstruction of surfaces including their surface topography. In his paper, wepresent an efficient image-space technique for the extraction of surfacetopography from high-resolution 3D reconstructions. Additionally, we filternoise and enhance topographic attributes to obtain an improved representationfor subsequent topography classification. Comprehensive experiments show thatthe our representation captures well topographic attributes and significantlyimproves classification performance compared to alternative 2D and 3Drepresentations.
arxiv-11400-261 | A new kernel-based approach for overparameterized Hammerstein system identification | http://arxiv.org/abs/1504.08190 | author:Riccardo Sven Risuleo, Giulio Bottegal, Håkan Hjalmarsson category:cs.SY stat.ML published:2015-04-30 summary:In this paper we propose a new identification scheme for Hammerstein systems,which are dynamic systems consisting of a static nonlinearity and a lineartime-invariant dynamic system in cascade. We assume that the nonlinear functioncan be described as a linear combination of $p$ basis functions. We reconstructthe $p$ coefficients of the nonlinearity together with the first $n$ samples ofthe impulse response of the linear system by estimating an $np$-dimensionaloverparameterized vector, which contains all the combinations of the unknownvariables. To avoid high variance in these estimates, we adopt a regularizedkernel-based approach and, in particular, we introduce a new kernel tailoredfor Hammerstein system identification. We show that the resulting schemeprovides an estimate of the overparameterized vector that can be uniquelydecomposed as the combination of an impulse response and $p$ coefficients ofthe static nonlinearity. We also show, through several numerical experiments,that the proposed method compares very favorably with two standard methods forHammerstein system identification.
arxiv-11400-262 | Neural Activation Constellations: Unsupervised Part Model Discovery with Convolutional Networks | http://arxiv.org/abs/1504.08289 | author:Marcel Simon, Erik Rodner category:cs.CV published:2015-04-30 summary:Part models of object categories are essential for challenging recognitiontasks, where differences in categories are subtle and only reflected inappearances of small parts of the object. We present an approach that is ableto learn part models in a completely unsupervised manner, without partannotations and even without given bounding boxes during learning. The key ideais to find constellations of neural activation patterns computed usingconvolutional neural networks. In our experiments, we outperform existingapproaches for fine-grained recognition on the CUB200-2011, NA birds, OxfordPETS, and Oxford Flowers dataset in case no part or bounding box annotationsare available and achieve state-of-the-art performance for the Stanford Dogdataset. We also show the benefits of neural constellation models as a dataaugmentation technique for fine-tuning. Furthermore, our paper unites the areasof generic and fine-grained classification, since our approach is suitable forboth scenarios. The source code of our method is available online athttp://www.inf-cv.uni-jena.de/part_discovery
arxiv-11400-263 | Fast R-CNN | http://arxiv.org/abs/1504.08083 | author:Ross Girshick category:cs.CV published:2015-04-30 summary:This paper proposes a Fast Region-based Convolutional Network method (FastR-CNN) for object detection. Fast R-CNN builds on previous work to efficientlyclassify object proposals using deep convolutional networks. Compared toprevious work, Fast R-CNN employs several innovations to improve training andtesting speed while also increasing detection accuracy. Fast R-CNN trains thevery deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, andachieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trainsVGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN isimplemented in Python and C++ (using Caffe) and is available under theopen-source MIT License at https://github.com/rbgirshick/fast-rcnn.
arxiv-11400-264 | On the estimation of initial conditions in kernel-based system identification | http://arxiv.org/abs/1504.08196 | author:Riccardo Sven Risuleo, Giulio Bottegal, Håkan Hjalmarsson category:cs.SY stat.ML published:2015-04-30 summary:Recent developments in system identification have brought attention toregularized kernel-based methods, where, adopting the recently introducedstable spline kernel, prior information on the unknown process is enforced.This reduces the variance of the estimates and thus makes kernel-based methodsparticularly attractive when few input-output data samples are available. Insuch cases however, the influence of the system initial conditions may have asignificant impact on the output dynamics. In this paper, we specificallyaddress this point. We propose three methods that deal with the estimation ofinitial conditions using different types of information. The methods consist invarious mixed maximum likelihood--a posteriori estimators which estimate theinitial conditions and tune the hyperparameters characterizing the stablespline kernel. To solve the related optimization problems, we resort to theexpectation-maximization method, showing that the solutions can be attained byiterating among simple update steps. Numerical experiments show the advantages,in terms of accuracy in reconstructing the system impulse response, of theproposed strategies, compared to other kernel-based schemes not accounting forthe effect initial conditions.
arxiv-11400-265 | Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy? | http://arxiv.org/abs/1504.08291 | author:Raja Giryes, Guillermo Sapiro, Alex M. Bronstein category:cs.NE cs.LG stat.ML 62M45 I.5.1 published:2015-04-30 summary:Three important properties of a classification machinery are: (i) the systempreserves the core information of the input data; (ii) the training examplesconvey information about unseen data; and (iii) the system is able to treatdifferently points from different classes. In this work we show that thesefundamental properties are satisfied by the architecture of deep neuralnetworks. We formally prove that these networks with random Gaussian weightsperform a distance-preserving embedding of the data, with a special treatmentfor in-class and out-of-class data. Similar points at the input of the networkare likely to have a similar output. The theoretical analysis of deep networkshere presented exploits tools used in the compressed sensing and dictionarylearning literature, thereby making a formal connection between these importanttopics. The derived results allow drawing conclusions on the metric learningproperties of the network and their relation to its structure, as well asproviding bounds on the required size of the training set such that thetraining examples would represent faithfully the unseen data. The results arevalidated with state-of-the-art trained networks.
arxiv-11400-266 | Overlapping and Non-overlapping Camera Layouts for Robot Pose Estimation | http://arxiv.org/abs/1505.00040 | author:Mohammad Ehab Ragab category:cs.CV published:2015-04-30 summary:We study the use of overlapping and non-overlapping camera layouts inestimating the ego-motion of a moving robot. To estimate the location andorientation of the robot, we investigate using four cameras as non-overlappingindividuals, and as two stereo pairs. The pros and cons of the two approachesare elucidated. The cameras work independently and can have larger field ofview in the non-overlapping layout. However, a scale factor ambiguity should bedealt with. On the other hand, stereo systems provide more accuracy but requireestablishing feature correspondence with more computational demand. For bothapproaches, the extended Kalman filter is used as a real-time recursiveestimator. The approaches studied are verified with synthetic and realexperiments alike.
arxiv-11400-267 | Application of S-Transform on Hyper kurtosis based Modified Duo Histogram Equalized DIC images for Pre-cancer Detection | http://arxiv.org/abs/1505.00192 | author:Sabyasachi Mukhopadhyay, Soham Mandal, Sawon Pratiher, Ritwik Barman, M. Venkatesh, Nirmalya Ghosh, Prasanta K. Panigrahi category:cs.CV published:2015-04-30 summary:Our proposed hyper kurtosis based histogram equalized DIC images enhances thecontrast by preserving the brightness. The evolution and development ofprecancerous activity among tissues are studied through S-transform (ST). Thesignificant variations of amplitude spectra can be observed due to increasedmedium roughness from normal tissue were observed in time-frequency domain. Therandomness and inhomogeneity of the tissue structures among human normal anddifferent grades of DIC tissues is recognized by ST based timefrequencyanalysis. This study offers a simpler and better way to recognize thesubstantial changes among different stages of DIC tissues, which are reflectedby spatial information containing within the inhomogeneity structures ofdifferent types of tissue.
arxiv-11400-268 | Model Selection and Overfitting in Genetic Programming: Empirical Study [Extended Version] | http://arxiv.org/abs/1504.08168 | author:Jan Žegklitz, Petr Pošík category:cs.NE cs.LG published:2015-04-30 summary:Genetic Programming has been very successful in solving a large area ofproblems but its use as a machine learning algorithm has been limited so far.One of the reasons is the problem of overfitting which cannot be solved orsuppresed as easily as in more traditional approaches. Another problem, closelyrelated to overfitting, is the selection of the final model from thepopulation. In this article we present our research that addresses both problems:overfitting and model selection. We compare several ways of dealing withovefitting, based on Random Sampling Technique (RST) and on using a validationset, all with an emphasis on model selection. We subject each approach to athorough testing on artificial and real--world datasets and compare them withthe standard approach, which uses the full training data, as a baseline.
arxiv-11400-269 | Hierarchical Subquery Evaluation for Active Learning on a Graph | http://arxiv.org/abs/1504.08219 | author:Oisin Mac Aodha, Neill D. F. Campbell, Jan Kautz, Gabriel J. Brostow category:cs.CV cs.LG stat.ML published:2015-04-30 summary:To train good supervised and semi-supervised object classifiers, it iscritical that we not waste the time of the human experts who are providing thetraining labels. Existing active learning strategies can have unevenperformance, being efficient on some datasets but wasteful on others, orinconsistent just between runs on the same dataset. We propose perplexity basedgraph construction and a new hierarchical subquery evaluation algorithm tocombat this variability, and to release the potential of Expected ErrorReduction. Under some specific circumstances, Expected Error Reduction has been one ofthe strongest-performing informativeness criteria for active learning. Untilnow, it has also been prohibitively costly to compute for sizeable datasets. Wedemonstrate our highly practical algorithm, comparing it to other activelearning measures on classification datasets that vary in sparsity,dimensionality, and size. Our algorithm is consistent over multiple runs andachieves high accuracy, while querying the human expert for labels at afrequency that matches their desired time budget.
arxiv-11400-270 | Lateral Connections in Denoising Autoencoders Support Supervised Learning | http://arxiv.org/abs/1504.08215 | author:Antti Rasmus, Harri Valpola, Tapani Raiko category:cs.LG cs.NE stat.ML published:2015-04-30 summary:We show how a deep denoising autoencoder with lateral connections can be usedas an auxiliary unsupervised learning task to support supervised learning. Theproposed model is trained to minimize simultaneously the sum of supervised andunsupervised cost functions by back-propagation, avoiding the need forlayer-wise pretraining. It improves the state of the art significantly in thepermutation-invariant MNIST classification task.
arxiv-11400-271 | Texts in, meaning out: neural language models in semantic similarity task for Russian | http://arxiv.org/abs/1504.08183 | author:Andrey Kutuzov, Igor Andreev category:cs.CL published:2015-04-30 summary:Distributed vector representations for natural language vocabulary get a lotof attention in contemporary computational linguistics. This paper summarizesthe experience of applying neural network language models to the task ofcalculating semantic similarity for Russian. The experiments were performed inthe course of Russian Semantic Similarity Evaluation track, where our modelstook from the 2nd to the 5th position, depending on the task. We introduce the tools and corpora used, comment on the nature of the sharedtask and describe the achieved results. It was found out that ContinuousSkip-gram and Continuous Bag-of-words models, previously successfully appliedto English material, can be used for semantic modeling of Russian as well.Moreover, we show that texts in Russian National Corpus (RNC) provide anexcellent training material for such models, outperforming other, much largercorpora. It is especially true for semantic relatedness tasks (althoughstacking models trained on larger corpora on top of RNC models improvesperformance even more). High-quality semantic vectors learned in such a way can be used in a varietyof linguistic tasks and promise an exciting field for further study.
arxiv-11400-272 | Parsing Linear Context-Free Rewriting Systems with Fast Matrix Multiplication | http://arxiv.org/abs/1504.08342 | author:Shay B. Cohen, Daniel Gildea category:cs.CL cs.FL published:2015-04-30 summary:We describe a matrix multiplication recognition algorithm for a subset ofbinary linear context-free rewriting systems (LCFRS) with running time$O(n^{\omega d})$ where $M(m) = O(m^{\omega})$ is the running time for $m\times m$ matrix multiplication and $d$ is the "contact rank" of the LCFRS --the maximal number of combination and non-combination points that appear in thegrammar rules. We also show that this algorithm can be used as a subroutine toget a recognition algorithm for general binary LCFRS with running time$O(n^{\omega d + 1})$. The currently best known $\omega$ is smaller than$2.38$. Our result provides another proof for the best known result for parsingmildly context sensitive formalisms such as combinatory categorial grammars,head grammars, linear indexed grammars, and tree adjoining grammars, which canbe parsed in time $O(n^{4.76})$. It also shows that inversion transductiongrammars can be parsed in time $O(n^{5.76})$. In addition, binary LCFRSsubsumes many other formalisms and types of grammars, for some of which we alsoimprove the asymptotic complexity of parsing.
arxiv-11400-273 | Proceedings of The 39th Annual Workshop of the Austrian Association for Pattern Recognition (OAGM), 2015 | http://arxiv.org/abs/1505.01065 | author:Sebastian Hegenbart, Roland Kwitt, Andreas Uhl category:cs.CV published:2015-04-30 summary:The 39th annual workshop of the Austrian Association for Pattern Recognition(OAGM/AAPR) provides a platform for presentation and discussion of researchprogress as well as research projects within the OAGM/AAPR community.
arxiv-11400-274 | On the Structure, Covering, and Learning of Poisson Multinomial Distributions | http://arxiv.org/abs/1504.08363 | author:Constantinos Daskalakis, Gautam Kamath, Christos Tzamos category:cs.DS cs.LG math.PR math.ST stat.TH published:2015-04-30 summary:An $(n,k)$-Poisson Multinomial Distribution (PMD) is the distribution of thesum of $n$ independent random vectors supported on the set ${\calB}_k=\{e_1,\ldots,e_k\}$ of standard basis vectors in $\mathbb{R}^k$. We provea structural characterization of these distributions, showing that, for all$\varepsilon >0$, any $(n, k)$-Poisson multinomial random vector is$\varepsilon$-close, in total variation distance, to the sum of a discretizedmultidimensional Gaussian and an independent $(\text{poly}(k/\varepsilon),k)$-Poisson multinomial random vector. Our structural characterization extendsthe multi-dimensional CLT of Valiant and Valiant, by simultaneously applying toall approximation requirements $\varepsilon$. In particular, it overcomesfactors depending on $\log n$ and, importantly, the minimum eigenvalue of thePMD's covariance matrix from the distance to a multidimensional Gaussian randomvariable. We use our structural characterization to obtain an $\varepsilon$-cover, intotal variation distance, of the set of all $(n, k)$-PMDs, significantlyimproving the cover size of Daskalakis and Papadimitriou, and obtaining thesame qualitative dependence of the cover size on $n$ and $\varepsilon$ as the$k=2$ cover of Daskalakis and Papadimitriou. We further exploit this structureto show that $(n,k)$-PMDs can be learned to within $\varepsilon$ in totalvariation distance from $\tilde{O}_k(1/\varepsilon^2)$ samples, which isnear-optimal in terms of dependence on $\varepsilon$ and independent of $n$. Inparticular, our result generalizes the single-dimensional result of Daskalakis,Diakonikolas, and Servedio for Poisson Binomials to arbitrary dimension.
arxiv-11400-275 | Predicting People's 3D Poses from Short Sequences | http://arxiv.org/abs/1504.08200 | author:Bugra Tekin, Xiaolu Sun, Xinchao Wang, Vincent Lepetit, Pascal Fua category:cs.CV published:2015-04-30 summary:We propose an efficient approach to exploiting motion information fromconsecutive frames of a video sequence to recover the 3D pose of people.Instead of computing candidate poses in individual frames and then linkingthem, as is often done, we regress directly from a spatio-temporal block offrames to a 3D pose in the central one. We will demonstrate that this approachallows us to effectively overcome ambiguities and to improve upon thestate-of-the-art on challenging sequences.
arxiv-11400-276 | Average Convergence Rate of Evolutionary Algorithms | http://arxiv.org/abs/1504.08117 | author:Jun He, Guangming Lin category:cs.NE published:2015-04-30 summary:In evolutionary optimization, it is important to understand how fastevolutionary algorithms converge to the optimum per generation, or theirconvergence rate. This paper proposes a new measure of the convergence rate,called average convergence rate. It is a normalised geometric mean of thereduction ratio of the fitness difference per generation. The calculation ofthe average convergence rate is very simple and it is applicable for mostevolutionary algorithms on both continuous and discrete optimization. Atheoretical study of the average convergence rate is conducted for discreteoptimization. Lower bounds on the average convergence rate are derived. Thelimit of the average convergence rate is analysed and then the asymptoticaverage convergence rate is proposed.
arxiv-11400-277 | Detecting and ordering adjectival scalemates | http://arxiv.org/abs/1504.08102 | author:Emiel van Miltenburg category:cs.CL published:2015-04-30 summary:This paper presents a pattern-based method that can be used to inferadjectival scales, such as <lukewarm, warm, hot>, from a corpus. Specifically,the proposed method uses lexical patterns to automatically identify and orderpairs of scalemates, followed by a filtering phase in which unrelated pairs arediscarded. For the filtering phase, several different similarity measures areimplemented and compared. The model presented in this paper is evaluated usingthe current standard, along with a novel evaluation set, and shown to be atleast as good as the current state-of-the-art.
arxiv-11400-278 | Detecting Concept-level Emotion Cause in Microblogging | http://arxiv.org/abs/1504.08050 | author:Shuangyong Song, Yao Meng category:cs.CL cs.AI 68P20 H.2.8 published:2015-04-30 summary:In this paper, we propose a Concept-level Emotion Cause Model (CECM), insteadof the mere word-level models, to discover causes of microblogging users'diversified emotions on specific hot event. A modified topic-supervised bitermtopic model is utilized in CECM to detect emotion topics' in event-relatedtweets, and then context-sensitive topical PageRank is utilized to detectmeaningful multiword expressions as emotion causes. Experimental results on adataset from Sina Weibo, one of the largest microblogging websites in China,show CECM can better detect emotion causes than baseline methods.
arxiv-11400-279 | PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions | http://arxiv.org/abs/1504.08362 | author:Michael Figurnov, Dmitry Vetrov, Pushmeet Kohli category:cs.CV published:2015-04-30 summary:We propose a novel approach to reduce the computational cost of evaluation ofconvolutional neural networks, a factor that has hindered their deployment inlow-power devices such as mobile phones. Inspired by the loop perforationtechnique from source code optimization, we speed up the bottleneckconvolutional layers by skipping their evaluation in some of the spatialpositions. We propose and analyze several strategies of choosing thesepositions. Our method allows to reduce the evaluation time of modernconvolutional neural networks by 50% with a small decrease in accuracy.
arxiv-11400-280 | Multi-user lax communications: a multi-armed bandit approach | http://arxiv.org/abs/1504.08167 | author:Orly Avner, Shie Mannor category:cs.LG cs.MA published:2015-04-30 summary:Inspired by cognitive radio networks, we consider a setting where multipleusers share several channels modeled as a multi-user multi-armed bandit (MAB)problem. The characteristics of each channel are unknown and are different foreach user. Each user can choose between the channels, but her success dependson the particular channel chosen as well as on the selections of other users:if two users select the same channel their messages collide and none of themmanages to send any data. Our setting is fully distributed, so there is nocentral control. As in many communication systems, the users cannot set up adirect communication protocol, so information exchange must be limited to aminimum. We develop an algorithm for learning a stable configuration for themulti-user MAB problem. We further offer both convergence guarantees andexperiments inspired by real communication networks, including comparison tostate-of-the-art algorithms.
arxiv-11400-281 | Bilinear CNN Models for Fine-grained Visual Recognition | http://arxiv.org/abs/1504.07889 | author:Tsung-Yu Lin, Aruni RoyChowdhury, Subhransu Maji category:cs.CV published:2015-04-29 summary:We propose bilinear models, a recognition architecture that consists of twofeature extractors whose outputs are multiplied using outer product at eachlocation of the image and pooled to obtain an image descriptor. Thisarchitecture can model local pairwise feature interactions in a translationallyinvariant manner which is particularly useful for fine-grained categorization.It also generalizes various orderless texture descriptors such as the Fishervector, VLAD and O2P. We present experiments with bilinear models where thefeature extractors are based on convolutional neural networks. The bilinearform simplifies gradient computation and allows end-to-end training of bothnetworks using image labels only. Using networks initialized from the ImageNetdataset followed by domain specific fine-tuning we obtain 84.1% accuracy of theCUB-200-2011 dataset requiring only category labels at training time. Wepresent experiments and visualizations that analyze the effects of fine-tuningand the choice two networks on the speed and accuracy of the models. Resultsshow that the architecture compares favorably to the existing state of the arton a number of fine-grained datasets while being substantially simpler andeasier to train. Moreover, our most accurate model is fairly efficient runningat 8 frames/sec on a NVIDIA Tesla K40 GPU. The source code for the completesystem will be made available at http://vis-www.cs.umass.edu/bcnn.
arxiv-11400-282 | Projected Iterative Soft-thresholding Algorithm for Tight Frames in Compressed Sensing Magnetic Resonance Imaging | http://arxiv.org/abs/1504.07786 | author:Yunsong Liu, Zhifang Zhan, Jian-Feng Cai, Di Guo, Zhong Chen, Xiaobo Qu category:physics.med-ph cs.CV math.OC published:2015-04-29 summary:Compressed sensing has shown great potentials in accelerating magneticresonance imaging. Fast image reconstruction and high image quality are twomain issues faced by this new technology. It has been shown that, redundantimage representations, e.g. tight frames, can significantly improve the imagequality. But how to efficiently solve the reconstruction problem with theseredundant representation systems is still challenging. This paper attempts toaddress the problem of applying iterative soft-thresholding algorithm (ISTA) totight frames based magnetic resonance image reconstruction. By introducing thecanonical dual frame to construct the orthogonal projection operator on therange of the analysis sparsity operator, we propose a projected iterativesoft-thresholding algorithm (pISTA) and further accelerate it by incorporatingthe strategy proposed by Beck and Teboulle in 2009. We theoretically prove thatpISTA converges to the minimum of a function with a balanced tight framesparsity. Experimental results demonstrate that the proposed algorithm achievesbetter reconstruction than the widely used synthesis sparse model and theaccelerated pISTA converges faster or comparable to the state-of-art smoothingFISTA. One major advantage of pISTA is that only one extra parameter, the stepsize, is introduced and the numerical solution is stable to it in terms ofimage reconstruction errors, thus allowing easily setting in many fast magneticresonance imaging applications.
arxiv-11400-283 | Patch-based Convolutional Neural Network for Whole Slide Tissue Image Classification | http://arxiv.org/abs/1504.07947 | author:Le Hou, Dimitris Samaras, Tahsin M. Kurc, Yi Gao, James E. Davis, Joel H. Saltz category:cs.CV J.3; I.4; I.5 published:2015-04-29 summary:Convolutional Neural Networks (CNN) are state-of-the-art models for manyimage classification tasks. However, to recognize cancer subtypesautomatically, training a CNN on gigapixel resolution Whole Slide Tissue Images(WSI) is currently computationally impossible. The differentiation of cancersubtypes is based on cellular-level visual features observed on image patchscale. Therefore, we argue that in this situation, training a patch-levelclassifier on image patches will perform better than or similar to animage-level classifier. The challenge becomes how to intelligently combinepatch-level classification results and model the fact that not all patches willbe discriminative. We propose to train a decision fusion model to aggregatepatch-level predictions given by patch-level CNNs, which to the best of ourknowledge has not been shown before. Furthermore, we formulate a novelExpectation-Maximization (EM) based method that automatically locatesdiscriminative patches robustly by utilizing the spatial relationships ofpatches. We apply our method to the classification of glioma and non-small-celllung carcinoma cases into subtypes. The classification accuracy of our methodis similar to the inter-observer agreement between pathologists. Although it isimpossible to train CNNs on WSIs, we experimentally demonstrate using acomparable non-cancer dataset of smaller images that a patch-based CNN canoutperform an image-based CNN.
arxiv-11400-284 | Incorporating Road Networks into Territory Design | http://arxiv.org/abs/1504.07846 | author:Nitin Ahuja, Matthias Bender, Peter Sanders, Christian Schulz, Andreas Wagner category:math.OC cs.DS cs.NE published:2015-04-29 summary:Given a set of basic areas, the territory design problem asks to create apredefined number of territories, each containing at least one basic area, suchthat an objective function is optimized. Desired properties of territoriesoften include a reasonable balance, compact form, contiguity and small averagejourney times which are usually encoded in the objective function or formulatedas constraints. We address the territory design problem by developing graphtheoretic models that also consider the underlying road network. The derivedgraph models enable us to tackle the territory design problem by modifyinggraph partitioning algorithms and mixed integer programming formulations sothat the objective of the planning problem is taken into account. We test andcompare the algorithms on several real world instances.
arxiv-11400-285 | Who Spoke What? A Latent Variable Framework for the Joint Decoding of Multiple Speakers and their Keywords | http://arxiv.org/abs/1504.08021 | author:Harshavardhan Sundar, Thippur V. Sreenivas category:cs.SD cs.LG published:2015-04-29 summary:In this paper, we present a latent variable (LV) framework to identify allthe speakers and their keywords given a multi-speaker mixture signal. Weintroduce two separate LVs to denote active speakers and the keywords uttered.The dependency of a spoken keyword on the speaker is modeled through aconditional probability mass function. The distribution of the mixture signalis expressed in terms of the LV mass functions and speaker-specific-keywordmodels. The proposed framework admits stochastic models, representing theprobability density function of the observation vectors given that a particularspeaker uttered a specific keyword, as speaker-specific-keyword models. The LVmass functions are estimated in a Maximum Likelihood framework using theExpectation Maximization (EM) algorithm. The active speakers and their keywordsare detected as modes of the joint distribution of the two LVs. In mixturesignals, containing two speakers uttering the keywords simultaneously, theproposed framework achieves an accuracy of 82% for detecting both the speakersand their respective keywords, using Student's-t mixture models asspeaker-specific-keyword models.
arxiv-11400-286 | Anticipating the future by watching unlabeled video | http://arxiv.org/abs/1504.08023 | author:Carl Vondrick, Hamed Pirsiavash, Antonio Torralba category:cs.CV published:2015-04-29 summary:In many computer vision applications, machines will need to reason beyond thepresent, and predict the future. This task is challenging because it requiresleveraging extensive commonsense knowledge of the world that is difficult towrite down. We believe that a promising resource for efficiently obtaining thisknowledge is through the massive amounts of readily available unlabeled video.In this paper, we present a large scale framework that capitalizes on temporalstructure in unlabeled video to learn to anticipate both actions and objects inthe future. The key idea behind our approach is that we can train deep networksto predict the visual representation of images in the future. We experimentallyvalidate this idea on two challenging "in the wild" video datasets, and ourresults suggest that learning with unlabeled videos significantly helpsforecast actions and anticipate objects.
arxiv-11400-287 | Probabilistic Depth Image Registration incorporating Nonvisual Information | http://arxiv.org/abs/1504.07857 | author:Manuel Wüthrich, Peter Pastor, Ludovic Righetti, Aude Billard, Stefan Schaal category:cs.RO cs.CV published:2015-04-29 summary:In this paper, we derive a probabilistic registration algorithm for objectmodeling and tracking. In many robotics applications, such as manipulationtasks, nonvisual information about the movement of the object is available,which we will combine with the visual information. Furthermore we do not onlyconsider observations of the object, but we also take space into account whichhas been observed to not be part of the object. Furthermore we are computing aposterior distribution over the relative alignment and not a point estimate astypically done in for example Iterative Closest Point (ICP). To our knowledgeno existing algorithm meets these three conditions and we thus derive a novelregistration algorithm in a Bayesian framework. Experimental results suggestthat the proposed methods perform favorably in comparison to PCLimplementations of feature mapping and ICP, especially if nonvisual informationis available.
arxiv-11400-288 | A Flexible Tensor Block Coordinate Ascent Scheme for Hypergraph Matching | http://arxiv.org/abs/1504.07907 | author:Quynh Nguyen, Antoine Gautier, Matthias Hein category:cs.CV published:2015-04-29 summary:The estimation of correspondences between two images resp. point sets is acore problem in computer vision. One way to formulate the problem is graphmatching leading to the quadratic assignment problem which is NP-hard. Severalso called second order methods have been proposed to solve this problem. Inrecent years hypergraph matching leading to a third order problem becamepopular as it allows for better integration of geometric information. For mostof these third order algorithms no theoretical guarantees are known. In thispaper we propose a general framework for tensor block coordinate ascent methodsfor hypergraph matching. We propose two algorithms which both come along withthe guarantee of monotonic ascent in the matching score on the set of discreteassignment matrices. In the experiments we show that our new algorithmsoutperform previous work both in terms of achieving better matching scores andmatching accuracy. This holds in particular for very challenging settings whereone has a high number of outliers and other forms of noise.
arxiv-11400-289 | Dual Averaging on Compactly-Supported Distributions And Application to No-Regret Learning on a Continuum | http://arxiv.org/abs/1504.07720 | author:Walid Krichene category:cs.LG math.OC published:2015-04-29 summary:We consider an online learning problem on a continuum. A decision maker isgiven a compact feasible set $S$, and is faced with the following sequentialproblem: at iteration~$t$, the decision maker chooses a distribution $x^{(t)}\in \Delta(S)$, then a loss function $\ell^{(t)} : S \to \mathbb{R}_+$ isrevealed, and the decision maker incurs expected loss $\langle \ell^{(t)},x^{(t)} \rangle = \mathbb{E}_{s \sim x^{(t)}} \ell^{(t)}(s)$. We view theproblem as an online convex optimization problem on the space $\Delta(S)$ ofLebesgue-continnuous distributions on $S$. We prove a general regret bound forthe Dual Averaging method on $L^2(S)$, then prove that dual averaging with$\omega$-potentials (a class of strongly convex regularizers) achievessublinear regret when $S$ is uniformly fat (a condition weaker than convexity).
arxiv-11400-290 | On the universal structure of human lexical semantics | http://arxiv.org/abs/1504.07843 | author:Hyejin Youn, Logan Sutton, Eric Smith, Cristopher Moore, Jon F. Wilkins, Ian Maddieson, William Croft, Tanmoy Bhattacharya category:physics.soc-ph cs.CL published:2015-04-29 summary:How universal is human conceptual structure? The way concepts are organizedin the human brain may reflect distinct features of cultural, historical, andenvironmental background in addition to properties universal to humancognition. Semantics, or meaning expressed through language, provides directaccess to the underlying conceptual structure, but meaning is notoriouslydifficult to measure, let alone parameterize. Here we provide an empiricalmeasure of semantic proximity between concepts using cross-linguisticdictionaries. Across languages carefully selected from a phylogenetically andgeographically stratified sample of genera, translations of words reveal caseswhere a particular language uses a single polysemous word to express conceptsrepresented by distinct words in another. We use the frequency of polysemieslinking two concepts as a measure of their semantic proximity, and representthe pattern of such linkages by a weighted network. This network is highlyuneven and fragmented: certain concepts are far more prone to polysemy thanothers, and there emerge naturally interpretable clusters loosely connected toeach other. Statistical analysis shows such structural properties areconsistent across different language groups, largely independent of geography,environment, and literacy. It is therefore possible to conclude the conceptualstructure connecting basic vocabulary studied is primarily due to universalfeatures of human cognition and language use.
arxiv-11400-291 | Intelligent Health Recommendation System for Computer Users | http://arxiv.org/abs/1504.07858 | author:Qi Guo, Zixuan Wang, Ming Li, Hamid Aghajan category:cs.CV published:2015-04-29 summary:The time people spend in front of computers has been increasing steadily dueto the role computers play in modern society. Individuals who sit in front ofcomputers for an extended period of time, specifically with improper posturesmay incur various health issues. In this work, individuals' behaviors in frontof computers are studied using web cameras. By means of non-rigid face trackingsystem, data are analyzed to determine the 3D head pose, blink rate and yawnfrequency of computer users. When combining these visual cues, a system ofintelligent personal assistants for computer users is proposed.
arxiv-11400-292 | ASTROMLSKIT: A New Statistical Machine Learning Toolkit: A Platform for Data Analytics in Astronomy | http://arxiv.org/abs/1504.07865 | author:Snehanshu Saha, Surbhi Agrawal, Manikandan. R, Kakoli Bora, Swati Routh, Anand Narasimhamurthy category:cs.CE astro-ph.IM cs.LG published:2015-04-29 summary:Astroinformatics is a new impact area in the world of astronomy, occasionallycalled the final frontier, where several astrophysicists, statisticians andcomputer scientists work together to tackle various data intensive astronomicalproblems. Exponential growth in the data volume and increased complexity of thedata augments difficult questions to the existing challenges. Classicalproblems in Astronomy are compounded by accumulation of astronomical volume ofcomplex data, rendering the task of classification and interpretationincredibly laborious. The presence of noise in the data makes analysis andinterpretation even more arduous. Machine learning algorithms and data analytictechniques provide the right platform for the challenges posed by theseproblems. A diverse range of open problem like star-galaxy separation,detection and classification of exoplanets, classification of supernovae isdiscussed. The focus of the paper is the applicability and efficacy of variousmachine learning algorithms like K Nearest Neighbor (KNN), random forest (RF),decision tree (DT), Support Vector Machine (SVM), Na\"ive Bayes and LinearDiscriminant Analysis (LDA) in analysis and inference of the decision theoreticproblems in Astronomy. The machine learning algorithms, integrated intoASTROMLSKIT, a toolkit developed in the course of the work, have been used toanalyze HabCat data and supernovae data. Accuracy has been found to beappreciably good.
arxiv-11400-293 | Market forecasting using Hidden Markov Models | http://arxiv.org/abs/1504.07829 | author:Sara Rebagliati, Emanuela Sasso, Samuele Soraggi category:stat.ML cs.LG 91B84 published:2015-04-29 summary:Working on the daily closing prices and logreturns, in this paper we dealwith the use of Hidden Markov Models (HMMs) to forecast the price of theEUR/USD Futures. The aim of our work is to understand how the HMMs describedifferent financial time series depending on their structure. Subsequently, weanalyse the forecasting methods exposed in the previous literature, putting onevidence their pros and cons.
arxiv-11400-294 | Visual Information Retrieval in Endoscopic Video Archives | http://arxiv.org/abs/1504.07874 | author:Jennifer Roldan-Carlos, Mathias Lux, Xavier Giró-i-Nieto, Pia Muñoz, Nektarios Anagnostopoulos category:cs.IR cs.CV cs.MM published:2015-04-29 summary:In endoscopic procedures, surgeons work with live video streams from theinside of their subjects. A main source for documentation of procedures arestill frames from the video, identified and taken during the surgery. However,with growing demands and technical means, the streams are saved to storageservers and the surgeons need to retrieve parts of the videos on demand. Inthis submission we present a demo application allowing for video retrievalbased on visual features and late fusion, which allows surgeons to re-findshots taken during the procedure.
arxiv-11400-295 | Technical Note on Equivalence Between Recurrent Neural Network Time Series Models and Variational Bayesian Models | http://arxiv.org/abs/1504.08025 | author:Jascha Sohl-Dickstein, Diederik P. Kingma category:cs.LG published:2015-04-29 summary:We observe that the standard log likelihood training objective for aRecurrent Neural Network (RNN) model of time series data is equivalent to avariational Bayesian training objective, given the proper choice of generativeand inference models. This perspective may motivate extensions to both RNNs andvariational Bayesian models. We propose one such extension, where multipleparticles are used for the hidden state of an RNN, allowing a naturalrepresentation of uncertainty or multimodality.
arxiv-11400-296 | Comparative study of image registration techniques for bladder video-endoscopy | http://arxiv.org/abs/1504.07901 | author:Achraf Ben-Hamadou, Charles Soussen, Walter Blondel, Christian Daul, Didier Wolf category:cs.CV published:2015-04-29 summary:Bladder cancer is widely spread in the world. Many adequate diagnosistechniques exist. Video-endoscopy remains the standard clinical procedure forvisual exploration of the bladder internal surface. However, video-endoscopypresents the limit that the imaged area for each image is about nearly 1cm2.And, lesions are, typically, spread over several images. The aim of thiscontribution is to assess the performance of two mosaicing algorithms leadingto the construction of panoramic maps (one unique image) of bladder walls. Thequantitative comparison study is performed on a set of real endoscopic examdata and on simulated data relative to bladder phantom.
arxiv-11400-297 | Robust hyperspectral image classification with rejection fields | http://arxiv.org/abs/1504.07918 | author:Filipe Condessa, Jose Bioucas-Dias, Jelena Kovacevic category:cs.CV 68 published:2015-04-29 summary:In this paper we present a novel method for robust hyperspectral imageclassification using context and rejection. Hyperspectral image classificationis generally an ill-posed image problem where pixels may belong to unknownclasses, and obtaining representative and complete training sets is costly.Furthermore, the need for high classification accuracies is frequently greaterthan the need to classify the entire image. We approach this problem with a robust classification method that combinesclassification with context with classification with rejection. A rejectionfield that will guide the rejection is derived from the classification withcontextual information obtained by using the SegSALSA algorithm. We validateour method in real hyperspectral data and show that the performance gainsobtained from the rejection fields are equivalent to an increase the dimensionof the training sets.
arxiv-11400-298 | A Deep Learning Model for Structured Outputs with High-order Interaction | http://arxiv.org/abs/1504.08022 | author:Hongyu Guo, Xiaodan Zhu, Martin Renqiang Min category:cs.LG cs.NE published:2015-04-29 summary:Many real-world applications are associated with structured data, where notonly input but also output has interplay. However, typical classification andregression models often lack the ability of simultaneously exploring high-orderinteraction within input and that within output. In this paper, we present adeep learning model aiming to generate a powerful nonlinear functional mappingfrom structured input to structured output. More specifically, we propose tointegrate high-order hidden units, guided discriminative pretraining, andhigh-order auto-encoders for this purpose. We evaluate the model with threedatasets, and obtain state-of-the-art performances among competitive methods.Our current work focuses on structured output regression, which is a lessexplored area, although the model can be extended to handle structured labelclassification.
arxiv-11400-299 | Exploring Integral Image Word Length Reduction Techniques for SURF Detector | http://arxiv.org/abs/1504.07958 | author:Shoaib Ehsan, Klaus D. McDonald-Maier category:cs.CV published:2015-04-29 summary:Speeded Up Robust Features (SURF) is a state of the art computer visionalgorithm that relies on integral image representation for performing fastdetection and description of image features that are scale and rotationinvariant. Integral image representation, however, has major draw back of largebinary word length that leads to substantial increase in memory size. Whendesigning a dedicated hardware to achieve real-time performance for the SURFalgorithm, it is imperative to consider the adverse effects of integral imageon memory size, bus width and computational resources. With the objective ofminimizing hardware resources, this paper presents a novel implementationconcept of a reduced word length integral image based SURF detector. Itevaluates two existing word length reduction techniques for the particular caseof SURF detector and extends one of these to achieve more reduction in wordlength. This paper also introduces a novel method to achieve integral imageword length reduction for SURF detector.
arxiv-11400-300 | Hardware based Scale- and Rotation-Invariant Feature Extraction: A Retrospective Analysis and Future Directions | http://arxiv.org/abs/1504.07962 | author:Shoaib Ehsan, Adrian F. Clark, Klaus D. McDonald-Maier category:cs.CV published:2015-04-29 summary:Computer Vision techniques represent a class of algorithms that are highlycomputation and data intensive in nature. Generally, performance of thesealgorithms in terms of execution speed on desktop computers is far fromreal-time. Since real-time performance is desirable in many applications,special-purpose hardware is required in most cases to achieve this goal. Scale-and rotation-invariant local feature extraction is a low level computer visiontask with very high computational complexity. The state-of-the-art algorithmsthat currently exist in this domain, like SIFT and SURF, suffer from slowexecution speeds and at best can only achieve rates of 2-3 Hz on modern desktopcomputers. Hardware-based scale- and rotation-invariant local featureextraction is an emerging trend enabling real-time performance for thesecomputationally complex algorithms. This paper takes a retrospective look atthe advances made so far in this field, discusses the hardware designstrategies employed and results achieved, identifies current research gaps andsuggests future research directions.
