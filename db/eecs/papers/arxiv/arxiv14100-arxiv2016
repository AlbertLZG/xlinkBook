arxiv-1511-02196 | Evaluating Protein-protein Interaction Predictors with a Novel 3-Dimensional Metric |  http://arxiv.org/abs/1511.02196  | author:Haohan Wang, Madhavi K. Ganapathiraju category:cs.LG published:2015-11-06 summary:In order for the predicted interactions to be directly adopted by biologists,the ma- chine learning predictions have to be of high precision, regardless ofrecall. This aspect cannot be evaluated or numerically represented well bytraditional metrics like accuracy, ROC, or precision-recall curve. In thiswork, we start from the alignment in sensitivity of ROC and recall ofprecision-recall curve, and propose an evaluation metric focusing on theability of a model to be adopted by biologists. This metric evaluates theability of a machine learning algorithm to predict only new interactions,meanwhile, it eliminates the influence of test dataset. In the experiment ofevaluating different classifiers with a same data set and evaluating the samepredictor with different datasets, our new metric fulfills the evaluation taskof our interest while two widely recognized metrics, ROC and precision-recallcurve fail the tasks for different reasons.
arxiv-1511-02204 | An Extended Frank-Wolfe Method with "In-Face" Directions, and its Application to Low-Rank Matrix Completion |  http://arxiv.org/abs/1511.02204  | author:Robert M. Freund, Paul Grigas, Rahul Mazumder category:math.OC stat.CO stat.ML 90C25 G.1.6 published:2015-11-06 summary:Motivated principally by the low-rank matrix completion problem, we presentan extension of the Frank-Wolfe method that is designed to induce near-optimalsolutions on low-dimensional faces of the feasible region. This is accomplishedby a new approach to generating ``in-face" directions at each iteration, aswell as through new choice rules for selecting between in-face and ``regular"Frank-Wolfe steps. Our framework for generating in-face directions generalizesthe notion of away-steps introduced by Wolfe. In particular, the in-facedirections always keep the next iterate within the minimal face containing thecurrent iterate. We present computational guarantees for the new method thattrade off efficiency in computing near-optimal solutions with upper bounds onthe dimension of minimal faces of iterates. We apply the new method to thematrix completion problem, where low-dimensional faces correspond to low-rankmatrices. We present computational results that demonstrate the effectivenessof our methodological approach at producing nearly-optimal solutions of verylow rank. On both artificial and real datasets, we demonstrate significantspeed-ups in computing very low-rank nearly-optimal solutions as compared toeither the Frank-Wolfe method or its traditional away-step variant.
arxiv-1511-02126 | Pooling the Convolutional Layers in Deep ConvNets for Action Recognition |  http://arxiv.org/abs/1511.02126  | author:Shichao Zhao, Yanbin Liu, Yahong Han, Richang Hong category:cs.CV published:2015-11-06 summary:Deep ConvNets have shown its good performance in image classification tasks.However it still remains as a problem in deep video representation for actionrecognition. The problem comes from two aspects: on one hand, current videoConvNets are relatively shallow compared with image ConvNets, which limits itscapability of capturing the complex video action information; on the otherhand, temporal information of videos is not properly utilized to pool andencode the video sequences. Towards these issues, in this paper, we utilize twostate-of-the-art ConvNets, i.e., the very deep spatial net (VGGNet) and thetemporal net from Two-Stream ConvNets, for action representation. Theconvolutional layers and the proposed new layer, called frame-diff layer, areextracted and pooled with two temporal pooling strategy: Trajectory pooling andline pooling. The pooled local descriptors are then encoded with VLAD to formthe video representations. In order to verify the effectiveness of the proposedframework, we conduct experiments on UCF101 and HMDB51 datasets. It achievesthe accuracy of 93.78\% on UCF101 which is the state-of-the-art and theaccuracy of 65.62\% on HMDB51 which is comparable to the state-of-the-art.
arxiv-1511-01776 | Computational Intractability of Dictionary Learning for Sparse Representation |  http://arxiv.org/abs/1511.01776  | author:Meisam Razaviyayn, Hung-Wei Tseng, Zhi-Quan Luo category:cs.LG stat.ML published:2015-11-05 summary:In this paper we consider the dictionary learning problem for sparserepresentation. We first show that this problem is NP-hard by polynomial timereduction of the densest cut problem. Then, using successive convexapproximation strategies, we propose efficient dictionary learning schemes tosolve several practical formulations of this problem to stationary points.Unlike many existing algorithms in the literature, such as K-SVD, our proposeddictionary learning scheme is theoretically guaranteed to converge to the setof stationary points under certain mild assumptions. For the image denoisingapplication, the performance and the efficiency of the proposed dictionarylearning scheme are comparable to that of K-SVD algorithm in simulation.
arxiv-1511-01942 | Stop Wasting My Gradients: Practical SVRG |  http://arxiv.org/abs/1511.01942  | author:Reza Babanezhad, Mohamed Osama Ahmed, Alim Virani, Mark Schmidt, Jakub Konečný, Scott Sallinen category:cs.LG math.OC stat.CO stat.ML published:2015-11-05 summary:We present and analyze several strategies for improving the performance ofstochastic variance-reduced gradient (SVRG) methods. We first show that theconvergence rate of these methods can be preserved under a decreasing sequenceof errors in the control variate, and use this to derive variants of SVRG thatuse growing-batch strategies to reduce the number of gradient calculationsrequired in the early iterations. We further (i) show how to exploit supportvectors to reduce the number of gradient computations in the later iterations,(ii) prove that the commonly-used regularized SVRG iteration is justified andimproves the convergence rate, (iii) consider alternate mini-batch selectionstrategies, and (iv) consider the generalization error of the method.
arxiv-1511-01954 | Recovering hard-to-find object instances by sampling context-based object proposals |  http://arxiv.org/abs/1511.01954  | author:Jose Oramas M., Tinne Tuytelaars category:cs.CV published:2015-11-05 summary:In this paper we focus on improving object detection performance in terms ofrecall. We propose a post-detection stage during which we explore the imagewith the objective of recovering missed detections. This exploration isperformed by sampling object proposals in the image. We analyse four differentstrategies to perform this sampling, giving special attention to strategiesthat exploit spatial relations between objects. In addition, we propose a novelmethod to discover higher-order relations between groups of objects.Experiments on the challenging KITTI dataset show that our proposedrelations-based proposal generation strategies can help improving recall at thecost of a relatively low amount of object proposals.
arxiv-1511-01844 | A note on the evaluation of generative models |  http://arxiv.org/abs/1511.01844  | author:Lucas Theis, Aäron van den Oord, Matthias Bethge category:stat.ML cs.LG published:2015-11-05 summary:Probabilistic generative models can be used for compression, denoising,inpainting, texture synthesis, semi-supervised learning, unsupervised featurelearning, and other tasks. Given this wide range of applications, it is notsurprising that a lot of heterogeneity exists in the way these models areformulated, trained, and evaluated. As a consequence, direct comparison betweenmodels is often difficult. This article reviews mostly known but oftenunderappreciated properties relating to the evaluation and interpretation ofgenerative models with a focus on image models. In particular, we show thatthree of the currently most commonly used criteria---average log-likelihood,Parzen window estimates, and visual fidelity of samples---are largelyindependent of each other when the data is high-dimensional. Good performancewith respect to one criterion therefore need not imply good performance withrespect to the other criteria. Our results show that extrapolation from onecriterion to another is not warranted and generative models need to beevaluated directly with respect to the application(s) they were intended for.In addition, we provide examples demonstrating that Parzen window estimatesshould generally be avoided.
arxiv-1511-01764 | Discrete Rényi Classifiers |  http://arxiv.org/abs/1511.01764  | author:Meisam Razaviyayn, Farzan Farnia, David Tse category:cs.LG published:2015-11-05 summary:Consider the binary classification problem of predicting a target variable$Y$ from a discrete feature vector $X = (X_1,...,X_d)$. When the probabilitydistribution $\mathbb{P}(X,Y)$ is known, the optimal classifier, leading to theminimum misclassification rate, is given by the Maximum A-posterioriProbability decision rule. However, estimating the complete joint distribution$\mathbb{P}(X,Y)$ is computationally and statistically impossible for largevalues of $d$. An alternative approach is to first estimate some low ordermarginals of $\mathbb{P}(X,Y)$ and then design the classifier based on theestimated low order marginals. This approach is also helpful when the completetraining data instances are not available due to privacy concerns. In thiswork, we consider the problem of finding the optimum classifier based on someestimated low order marginals of $(X,Y)$. We prove that for a given set ofmarginals, the minimum Hirschfeld-Gebelein-Renyi (HGR) correlation principleintroduced in [1] leads to a randomized classification rule which is shown tohave a misclassification rate no larger than twice the misclassification rateof the optimal classifier. Then, under a separability condition, we show thatthe proposed algorithm is equivalent to a randomized linear regressionapproach. In addition, this method naturally results in a robust featureselection method selecting a subset of features having the maximum worst caseHGR correlation with the target variable. Our theoretical upper-bound issimilar to the recent Discrete Chebyshev Classifier (DCC) approach [2], whilethe proposed algorithm has significant computational advantages since it onlyrequires solving a least square optimization problem. Finally, we numericallycompare our proposed algorithm with the DCC classifier and show that theproposed algorithm results in better misclassification rate over variousdatasets.
arxiv-1511-01957 | False Discoveries Occur Early on the Lasso Path |  http://arxiv.org/abs/1511.01957  | author:Weijie Su, Malgorzata Bogdan, Emmanuel Candes category:math.ST cs.IT math.IT stat.ML stat.TH published:2015-11-05 summary:In regression settings where explanatory variables have very low correlationsand where there are relatively few effects each of large magnitude, it iscommonly believed that the Lasso shall be able to find the important variableswith few errors---if any. In contrast, this paper shows that this is not thecase even when the design variables are stochastically independent. In a regimeof linear sparsity, we demonstrate that true features and null features arealways interspersed on the Lasso path, and that this phenomenon occurs nomatter how strong the effect sizes are. We derive a sharp asymptotic trade-offbetween false and true positive rates or, equivalently, between measures oftype I and type II errors along the Lasso path. This trade-off states that ifwe ever want to achieve a type II error (false negative rate) under a giventhreshold, then anywhere on the Lasso path the type I error (false positiverate) will need to exceed a given threshold so that we can never have botherrors at a low level at the same time. Our analysis uses tools fromapproximate message passing (AMP) theory as well as novel elements to deal witha possibly adaptive selection of the Lasso regularizing parameter.
arxiv-1511-01756 | "Pale as death" or "pâle comme la mort": Frozen similes used as literary clichés |  http://arxiv.org/abs/1511.01756  | author:Suzanne Mpouli, Jean-Gabriel Ganascia category:cs.CL published:2015-11-05 summary:The present study is focused on the automatic identification and descriptionof frozen similes in British and French novels written between the 19 thcentury and the beginning of the 20 th century. Two main patterns of frozensimiles were considered: adjectival ground + simile marker + nominal vehicle(e.g. happy as a lark) and eventuality + simile marker + nominal vehicle (e.g.sleep like a top). All potential similes and their components were firstextracted using a rule-based algorithm. Then, frozen similes were identifiedbased on reference lists of existing similes and semantic distance between thetenor and the vehicle. The results obtained tend to confirm the fact thatfrozen similes are not used haphazardly in literary texts. In addition,contrary to how they are often presented, frozen similes often go beyond theground or the eventuality and the vehicle to also include the tenor.
arxiv-1511-01726 | Multi-Target Tracking and Occlusion Handling with Learned Variational Bayesian Clusters and a Social Force Model |  http://arxiv.org/abs/1511.01726  | author:Ata-ur-Rehman, Syed Mohsen Naqvi, Lyudmila Mihaylova, Jonathon Chambers category:cs.CV published:2015-11-05 summary:This paper considers the problem of multiple human target tracking in asequence of video data. A solution is proposed which is able to deal with thechallenges of a varying number of targets, interactions and when every targetgives rise to multiple measurements. The developed novel algorithm comprisesvariational Bayesian clustering combined with a social force model, integratedwithin a particle filter with an enhanced prediction step. It performsmeasurement-to-target association by automatically detecting the measurementrelevance. The performance of the developed algorithm is evaluated over severalsequences from publicly available data sets: AV16.3, CAVIAR and PETS2006, whichdemonstrates that the proposed algorithm successfully initializes and tracks avariable number of targets in the presence of complex occlusions. A comparisonwith state-of-the-art techniques due to Khan et al., Laet et al. and Czyz etal. shows improved tracking performance.
arxiv-1511-01870 | Thoughts on Massively Scalable Gaussian Processes |  http://arxiv.org/abs/1511.01870  | author:Andrew Gordon Wilson, Christoph Dann, Hannes Nickisch category:cs.LG cs.AI stat.ME stat.ML published:2015-11-05 summary:We introduce a framework and early results for massively scalable Gaussianprocesses (MSGP), significantly extending the KISS-GP approach of Wilson andNickisch (2015). The MSGP framework enables the use of Gaussian processes (GPs)on billions of datapoints, without requiring distributed inference, or severeassumptions. In particular, MSGP reduces the standard $O(n^3)$ complexity of GPlearning and inference to $O(n)$, and the standard $O(n^2)$ complexity per testpoint prediction to $O(1)$. MSGP involves 1) decomposing covariance matrices asKronecker products of Toeplitz matrices approximated by circulant matrices.This multi-level circulant approximation allows one to unify the orthogonalcomputational benefits of fast Kronecker and Toeplitz approaches, and issignificantly faster than either approach in isolation; 2) local kernelinterpolation and inducing points to allow for arbitrarily located data inputs,and $O(1)$ test time predictions; 3) exploiting block-Toeplitz Toeplitz-blockstructure (BTTB), which enables fast inference and learning whenmultidimensional Kronecker structure is not present; and 4) projections of theinput space to flexibly model correlated inputs and high dimensional data. Theability to handle many ($m \approx n$) inducing points allows for near-exactaccuracy and large scale kernel learning.
arxiv-1511-01706 | Image classification based on support vector machine and the fusion of complementary features |  http://arxiv.org/abs/1511.01706  | author:Huilin Gao, Wenjie Chen, Lihua Dou category:cs.CV published:2015-11-05 summary:Image Classification based on BOW (Bag-of-words) has broad applicationprospect in pattern recognition field but the shortcomings are existed becauseof single feature and low classification accuracy. To this end we combine threeingredients: (i) Three features with functions of mutual complementation areadopted to describe the images, including PHOW (Pyramid Histogram of Words),PHOC (Pyramid Histogram of Color) and PHOG (Pyramid Histogram of OrientatedGradients). (ii) The improvement of traditional BOW model is presented by usingdense sample and an improved K-means clustering method for constructing thevisual dictionary. (iii) An adaptive feature-weight adjusted imagecategorization algorithm based on the SVM and the fusion of multiple featuresis adopted. Experiments carried out on Caltech 101 database confirm thevalidity of the proposed approach. From the experimental results can be seenthat the classification accuracy rate of the proposed method is improved by7%-17% higher than that of the traditional BOW methods. This algorithm makesfull use of global, local and spatial information and has significantimprovements to the classification accuracy.
arxiv-1511-01666 | Comparing Writing Styles using Word Embedding and Dynamic Time Warping |  http://arxiv.org/abs/1511.01666  | author:Abhinav Tushar, Abhinav Dahiya category:cs.CL published:2015-11-05 summary:The development of plot or story in novels is reflected in the content andthe words used. The flow of sentiments, which is one aspect of writing style,can be quantified by analyzing the flow of words. This study explores literaryworks as signals in word embedding space and tries to compare writing styles ofpopular classic novels using dynamic time warping.
arxiv-1511-01665 | An Empirical Study on Sentiment Classification of Chinese Review using Word Embedding |  http://arxiv.org/abs/1511.01665  | author:Yiou Lin, Hang Lei, Jia Wu, Xiaoyu Li category:cs.CL published:2015-11-05 summary:In this article, how word embeddings can be used as features in Chinesesentiment classification is presented. Firstly, a Chinese opinion corpus isbuilt with a million comments from hotel review websites. Then the wordembeddings which represent each comment are used as input in different machinelearning methods for sentiment classification, including SVM, LogisticRegression, Convolutional Neural Network (CNN) and ensemble methods. Thesemethods get better performance compared with N-gram models using Naive Bayes(NB) and Maximum Entropy (ME). Finally, a combination of machine learningmethods is proposed which presents an outstanding performance in precision,recall and F1 score. After selecting the most useful methods to construct thecombinational model and testing over the corpus, the final F1 score is 0.920.
arxiv-1511-01846 | Sparse approximation by greedy algorithms |  http://arxiv.org/abs/1511.01846  | author:Vladimir Temlyakov category:math.NA stat.ML published:2015-11-05 summary:It is a survey on recent results in constructive sparse approximation. Threedirections are discussed here: (1) Lebesgue-type inequalities for greedyalgorithms with respect to a special class of dictionaries, (2) constructivesparse approximation with respect to the trigonometric system, (3) sparseapproximation with respect to dictionaries with tensor product structure. Inall three cases constructive ways are provided for sparse approximation. Thetechnique used is based on fundamental results from the theory of greedyapproximation. In particular, results in the direction (1) are based on deepmethods developed recently in compressed sensing. We present some of theseresults with detailed proofs.
arxiv-1511-01644 | Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model |  http://arxiv.org/abs/1511.01644  | author:Benjamin Letham, Cynthia Rudin, Tyler H. McCormick, David Madigan category:stat.AP cs.LG stat.ML published:2015-11-05 summary:We aim to produce predictive models that are not only accurate, but are alsointerpretable to human experts. Our models are decision lists, which consist ofa series of if...then... statements (e.g., if high blood pressure, then stroke)that discretize a high-dimensional, multivariate feature space into a series ofsimple, readily interpretable decision statements. We introduce a generativemodel called Bayesian Rule Lists that yields a posterior distribution overpossible decision lists. It employs a novel prior structure to encouragesparsity. Our experiments show that Bayesian Rule Lists has predictive accuracyon par with the current top algorithms for prediction in machine learning. Ourmethod is motivated by recent developments in personalized medicine, and can beused to produce highly accurate and interpretable medical scoring systems. Wedemonstrate this by producing an alternative to the CHADS$_2$ score, activelyused in clinical practice for estimating the risk of stroke in patients thathave atrial fibrillation. Our model is as interpretable as CHADS$_2$, but moreaccurate.
arxiv-1511-01865 | Convolutional Neural Network for Stereotypical Motor Movement Detection in Autism |  http://arxiv.org/abs/1511.01865  | author:Nastaran Mohammadian Rad, Andrea Bizzego, Seyed Mostafa Kia, Giuseppe Jurman, Paola Venuti, Cesare Furlanello category:cs.NE cs.CV cs.LG published:2015-11-05 summary:Autism Spectrum Disorders (ASDs) are often associated with specific atypicalpostural or motor behaviors, of which Stereotypical Motor Movements (SMMs) havea specific visibility. While the identification and the quantification of SMMpatterns remain complex, its automation would provide support to accuratetuning of the intervention in the therapy of autism. Therefore, it is essentialto develop automatic SMM detection systems in a real world setting, taking careof strong inter-subject and intra-subject variability. Wireless accelerometersensing technology can provide a valid infrastructure for real-time SMMdetection, however such variability remains a problem also for machine learningmethods, in particular whenever handcrafted features extracted fromaccelerometer signal are considered. Here, we propose to employ the deeplearning paradigm in order to learn discriminating features from multi-sensoraccelerometer signals. Our results provide preliminary evidence that featurelearning and transfer learning embedded in the deep architecture achieve higheraccurate SMM detectors in longitudinal scenarios.
arxiv-1511-01754 | Symmetry-invariant optimization in deep networks |  http://arxiv.org/abs/1511.01754  | author:Vijay Badrinarayanan, Bamdev Mishra, Roberto Cipolla category:cs.LG cs.AI cs.CV published:2015-11-05 summary:Recent works have highlighted scale invariance or symmetry that is present inthe weight space of a typical deep network and the adverse effect that it hason the Euclidean gradient based stochastic gradient descent optimization. Inthis work, we show that these and other commonly used deep networks, such asthose which use a max-pooling and sub-sampling layer, possess more complexforms of symmetry arising from scaling based reparameterization of the networkweights. We then propose two symmetry-invariant gradient based weight updatesfor stochastic gradient descent based learning. Our empirical evidence based onthe MNIST dataset shows that these updates improve the test performance withoutsacrificing the computational efficiency of the weight updates. We also showthe results of training with one of the proposed weight updates on an imagesegmentation problem.
arxiv-1511-01707 | Getting started with particle Metropolis-Hastings for inference in nonlinear dynamical models |  http://arxiv.org/abs/1511.01707  | author:Johan Dahlin, Thomas B. Schön category:stat.CO q-fin.ST stat.ML published:2015-11-05 summary:We provide a gentle introduction to the particle Metropolis-Hastings (PMH)algorithm for parameter inference in nonlinear state space models (SSMs)together with a software implementation in the statistical programming languageR. Throughout this tutorial, we develop an implementation of the PMH algorithm(and the integrated particle filter), which is distributed as the packagepmhtutorial available from the CRAN repository. Moreover, we provide the readerwith some intuition for how the algorithm operates and discuss some solutionsto numerical problems that might occur in practice. To illustrate the use ofPMH, we consider parameter inference in a linear Gaussian SSM with syntheticdata and a nonlinear stochastic volatility model with real-world data. Weconclude the tutorial by discussing important possible improvements to thealgorithm and we also list suitable references for further study.
arxiv-1511-01631 | Background Modeling Using Adaptive Pixelwise Kernel Variances in a Hybrid Feature Space |  http://arxiv.org/abs/1511.01631  | author:Manjunath Narayana, Allen Hanson, Erik Learned-Miller category:cs.CV published:2015-11-05 summary:Recent work on background subtraction has shown developments on two majorfronts. In one, there has been increasing sophistication of probabilisticmodels, from mixtures of Gaussians at each pixel [7], to kernel densityestimates at each pixel [1], and more recently to joint domainrange densityestimates that incorporate spatial information [6]. Another line of work hasshown the benefits of increasingly complex feature representations, includingthe use of texture information, local binary patterns, and recentlyscale-invariant local ternary patterns [4]. In this work, we use jointdomain-range based estimates for background and foreground scores and show thatdynamically choosing kernel variances in our kernel estimates at eachindividual pixel can significantly improve results. We give a heuristic methodfor selectively applying the adaptive kernel calculations which is nearly asaccurate as the full procedure but runs much faster. We combine these modelingimprovements with recently developed complex features [4] and show significantimprovements on a standard backgrounding benchmark.
arxiv-1511-01627 | Background subtraction - separating the modeling and the inference |  http://arxiv.org/abs/1511.01627  | author:Manjunath Narayana, Allen Hanson, Erik Learned-Miller category:cs.CV published:2015-11-05 summary:In its early implementations, background modeling was a process of building amodel for the background of a video with a stationary camera, and identifyingpixels that did not conform well to this model. The pixels that were notwell-described by the background model were assumed to be moving objects. Manysystems today maintain models for the foreground as well as the background, andthese models compete to explain the pixels in a video. In this paper, we arguethat the logical endpoint of this evolution is to simply use Bayes' rule toclassify pixels. In particular, it is essential to have a backgroundlikelihood, a foreground likelihood, and a prior at each pixel. A simpleapplication of Bayes' rule then gives a posterior probability over the label.The only remaining question is the quality of the component models: thebackground likelihood, the foreground likelihood, and the prior. We describe amodel for the likelihoods that is built by using not only the past observationsat a given pixel location, but by also including observations in a spatialneighborhood around the location. This enables us to model the influencebetween neighboring pixels and is an improvement over earlier pixelwise modelsthat do not allow for such influence. Although similar in spirit to the jointdomain-range model, we show that our model overcomes certain deficiencies inthat model. We use a spatially dependent prior for the background andforeground. The background and foreground labels from the previous frame, afterspatial smoothing to account for movement of objects,are used to build theprior for the current frame.
arxiv-1511-01853 | Autoregressive Model for Individual Consumption Data - LASSO Selection and Significance Test |  http://arxiv.org/abs/1511.01853  | author:Pan Li, Baosen Zhang, Yang Weng, Ram Rajagopal category:stat.ML cs.SY math.OC published:2015-11-05 summary:Understanding user flexibility and behavior patterns is becoming increasinglyvital to the design of robust and efficient energy saving programs. Accurateprediction of consumption is a key part to this understanding. Existingprediction methods usually have high relative errors that can be larger than30\%. In this paper, we explore sparsity in users' past data and relationshipbetween different users to increase prediction accuracy. We show that usingLASSO and significance test techniques, prediction accuracy can besignificantly compared to standard existing algorithms. We use mean absolutepercentage error (MAPE) as the criteria.
arxiv-1511-01664 | Stochastic Proximal Gradient Descent for Nuclear Norm Regularization |  http://arxiv.org/abs/1511.01664  | author:Lijun Zhang, Tianbao Yang, Rong Jin, Zhi-Hua Zhou category:cs.LG published:2015-11-05 summary:In this paper, we utilize stochastic optimization to reduce the spacecomplexity of convex composite optimization with a nuclear norm regularizer,where the variable is a matrix of size $m \times n$. By constructing a low-rankestimate of the gradient, we propose an iterative algorithm based on stochasticproximal gradient descent (SPGD), and take the last iterate of SPGD as thefinal solution. The main advantage of the proposed algorithm is that its spacecomplexity is $O(m+n)$, in contrast, most of previous algorithms have a $O(mn)$space complexity. Theoretical analysis shows that it achieves $O(\logT/\sqrt{T})$ and $O(\log T/T)$ convergence rates for general convex functionsand strongly convex functions, respectively.
arxiv-1511-01804 | Wood Species Recognition Based on SIFT Keypoint Histogram |  http://arxiv.org/abs/1511.01804  | author:Shuaiqi Hu, Ke Li, Xudong Bao category:cs.CV published:2015-11-05 summary:Traditionally, only experts who are equipped with professional knowledge andrich experience are able to recognize different species of wood. Applying imageprocessing techniques for wood species recognition can not only reduce theexpense to train qualified identifiers, but also increase the recognitionaccuracy. In this paper, a wood species recognition technique base on ScaleInvariant Feature Transformation (SIFT) keypoint histogram is proposed. We usefirst the SIFT algorithm to extract keypoints from wood cross section images,and then k-means and k-means++ algorithms are used for clustering. Using theclustering results, an SIFT keypoints histogram is calculated for each woodimage. Furthermore, several classification models, including Artificial NeuralNetworks (ANN), Support Vector Machine (SVM) and K-Nearest Neighbor (KNN) areused to verify the performance of the method. Finally, through comparing withother prevalent wood recognition methods such as GLCM and LBP, results showthat our scheme achieves higher accuracy.
arxiv-1511-01574 | Multinomial Loss on Held-out Data for the Sparse Non-negative Matrix Language Model |  http://arxiv.org/abs/1511.01574  | author:Ciprian Chelba, Fernando Pereira category:cs.CL published:2015-11-05 summary:We describe Sparse Non-negative Matrix (SNM) language model estimation usingmultinomial loss on held-out data. Being able to train on held-out data is important in practical situationswhere the training data is usually mismatched from the held-out/test data. Itis also less constrained than the previous training algorithm usingleave-one-out on training data: it allows the use of richer meta-features inthe adjustment model, e.g. the diversity counts used by Kneser-Ney smoothingwhich would be difficult to deal with correctly in leave-one-out training. In experiments on the one billion words language modeling benchmark, we areable to slightly improve on our previous results which use a different lossfunction, and employ leave-one-out training on a subset of the main trainingset. Surprisingly, an adjustment model with meta-features that discard alllexical information can perform as well as lexicalized meta-features. We findthat fairly small amounts of held-out data (on the order of 30-70 thousandwords) are sufficient for training the adjustment model. In a real-life scenario where the training data is a mix of data sources thatare imbalanced in size, and of different degrees of relevance to the held-outand test data, taking into account the data source for a given skip-/n-gramfeature and combining them for best performance on held-out/test data improvesover skip-/n-gram SNM models trained on pooled data by about 8% in the SMTsetup, or as much as 15% in the ASR/IME setup. The ability to mix various data sources based on how relevant they are to amismatched held-out set is probably the most attractive feature of the newestimation method for SNM LM.
arxiv-1511-01887 | Radon-Nikodym approximation in application to image analysis |  http://arxiv.org/abs/1511.01887  | author:Vladislav Gennadievich Malyshkin category:cs.CV published:2015-11-05 summary:For an image pixel information can be converted to the moments of some basis$Q_k$, e.g. Fourier-Mellin, Zernike, monomials, etc. Given sufficient number ofmoments pixel information can be completely recovered, for insufficient numberof moments only partial information can be recovered and the imagereconstruction is, at best, of interpolatory type. Standard approach is topresent interpolated value as a linear combination of basis functions, what isequivalent to least squares expansion. However, recent progress in numericalstability of moments estimation allows image information to be recovered frommoments in a completely different manner, applying Radon-Nikodym type ofexpansion, what gives the result as a ratio of two quadratic forms of basisfunctions. In contrast with least squares the Radon-Nikodym approach hasoscillation near the boundaries very much suppressed and does not divergeoutside of basis support. While least squares theory operate with vectors$<fQ_k>$, Radon-Nikodym theory operates with matrices $<fQ_jQ_k>$, what makethe approach much more suitable to image transforms and statistical propertyestimation.
arxiv-1511-01619 | Coherent Motion Segmentation in Moving Camera Videos using Optical Flow Orientations |  http://arxiv.org/abs/1511.01619  | author:Manjunath Narayana, Allen Hanson, Erik Learned-Miller category:cs.CV published:2015-11-05 summary:In moving camera videos, motion segmentation is commonly performed using theimage plane motion of pixels, or optical flow. However, objects that are atdifferent depths from the camera can exhibit different optical flows even ifthey share the same real-world motion. This can cause a depth-dependentsegmentation of the scene. Our goal is to develop a segmentation algorithm thatclusters pixels that have similar real-world motion irrespective of their depthin the scene. Our solution uses optical flow orientations instead of thecomplete vectors and exploits the well-known property that under cameratranslation, optical flow orientations are independent of object depth. Weintroduce a probabilistic model that automatically estimates the number ofobserved independent motions and results in a labeling that is consistent withreal-world motion in the scene. The result of our system is that static objectsare correctly identified as one segment, even if they are at different depths.Color features and information from previous frames in the video sequence areused to correct occasional errors due to the orientation-based segmentation. Wepresent results on more than thirty videos from different benchmarks. Thesystem is particularly robust on complex background scenes containing objectsat significantly different depths
arxiv-1511-01559 | Color Aesthetics and Social Networks in Complete Tang Poems: Explorations and Discoveries |  http://arxiv.org/abs/1511.01559  | author:Chao-Lin Liu, Hongsu Wang, Wen-Huei Cheng, Chu-Ting Hsu, Wei-Yun Chiu category:cs.CL cs.DL cs.IR published:2015-11-05 summary:The Complete Tang Poems (CTP) is the most important source to study Tangpoems. We look into CTP with computational tools from specific linguisticperspectives, including distributional semantics and collocational analysis.From such quantitative viewpoints, we compare the usage of "wind" and "moon" inthe poems of Li Bai and Du Fu. Colors in poems function like sounds in movies,and play a crucial role in the imageries of poems. Thus, words for colors arestudied, and "white" is the main focus because it is the most frequent color inCTP. We also explore some cases of using colored words in antithesis pairs thatwere central for fostering the imageries of the poems. CTP also contains usefulhistorical information, and we extract person names in CTP to study the socialnetworks of the Tang poets. Such information can then be integrated with theChina Biographical Database of Harvard University.
arxiv-1511-01293 | Towards a tracking algorithm based on the clustering of spatio-temporal clouds of points |  http://arxiv.org/abs/1511.01293  | author:Andrea Cavagna, Chiara Creato, Lorenzo Del Castello, Stefania Melillo, Leonardo Parisi, Massimiliano Viale category:cs.CV published:2015-11-04 summary:The interest in 3D dynamical tracking is growing in fields such as robotics,biology and fluid dynamics. Recently, a major source of progress in 3D trackinghas been the study of collective behaviour in biological systems, where thetrajectories of individual animals moving within large and dense groups need tobe reconstructed to understand the behavioural interaction rules. Experimentaldata in this field are generally noisy and at low spatial resolution, so thatindividuals appear as small featureless objects and trajectories must beretrieved by making use of epipolar information only. Moreover, opticalocclusions often occur: in a multi-camera system one or more objects becomeindistinguishable in one view, potentially jeopardizing the conservation ofidentity over long-time trajectories. The most advanced 3D tracking algorithmsovercome optical occlusions making use of set-cover techniques, which howeverhave to solve NP-hard optimization problems. Moreover, current methods are notable to cope with occlusions arising from actual physical proximity of objectsin 3D space. Here, we present a new method designed to work directly in 3Dspace and time, creating (3D+1) clouds of points representing the fullspatio-temporal evolution of the moving targets. We can then use a simpleconnected components labeling routine, which is linear in time, to solveoptical occlusions, hence lowering from NP to P the complexity of the problem.Finally, we use normalized cut spectral clustering to tackle 3D physicalproximity.
arxiv-1511-01289 | Data-Driven Learning of a Union of Sparsifying Transforms Model for Blind Compressed Sensing |  http://arxiv.org/abs/1511.01289  | author:Saiprasad Ravishankar, Yoram Bresler category:stat.ML cs.LG published:2015-11-04 summary:Compressed sensing is a powerful tool in applications such as magneticresonance imaging (MRI). It enables accurate recovery of images from highlyundersampled measurements by exploiting the sparsity of the images or imagepatches in a transform domain or dictionary. In this work, we focus on blindcompressed sensing (BCS), where the underlying sparse signal model is a prioriunknown, and propose a framework to simultaneously reconstruct the underlyingimage as well as the unknown model from highly undersampled measurements.Specifically, our model is that the patches of the underlying image(s) areapproximately sparse in a transform domain. We also extend this model to aunion of transforms model that better captures the diversity of features innatural images. The proposed block coordinate descent type algorithms for blindcompressed sensing are highly efficient, and are guaranteed to converge to atleast the partial global and partial local minimizers of the highly non-convexBCS problems. Our numerical experiments show that the proposed frameworkusually leads to better quality of image reconstructions in MRI compared toseveral recent image reconstruction methods. Importantly, the learning of aunion of sparsifying transforms leads to better image reconstructions than asingle adaptive transform.
arxiv-1511-01258 | Learn on Source, Refine on Target:A Model Transfer Learning Framework with Random Forests |  http://arxiv.org/abs/1511.01258  | author:Noam Segev, Maayan Harel, Shie Mannor, Koby Crammer, Ran El-Yaniv category:cs.LG published:2015-11-04 summary:We propose novel model transfer-learning methods that refine a decisionforest model M learned within a "source" domain using a training set sampledfrom a "target" domain, assumed to be a variation of the source. We present tworandom forest transfer algorithms. The first algorithm searches greedily forlocally optimal modifications of each tree structure by trying to locallyexpand or reduce the tree around individual nodes. The second algorithm doesnot modify structure, but only the parameter (thresholds) associated withdecision nodes. We also propose to combine both methods by considering anensemble that contains the union of the two forests. The proposed methodsexhibit impressive experimental results over a range of problems.
arxiv-1511-01543 | Regularization and Bayesian Learning in Dynamical Systems: Past, Present and Future |  http://arxiv.org/abs/1511.01543  | author:A. Chiuso category:cs.SY stat.ML published:2015-11-04 summary:Regularization and Bayesian methods for system identification have beenrepopularized in the recent years, and proved to be competitive w.r.t.classical parametric approaches. In this paper we shall make an attempt toillustrate how the use of regularization in system identification has evolvedover the years, starting from the early contributions both in the AutomaticControl as well as Econometrics and Statistics literature. In particular weshall discuss some fundamental issues such as compound estimation problems andexchangeability which play and important role in regularization and Bayesianapproaches, as also illustrated in early publications in Statistics. Thehistorical and foundational issues will be given more emphasis (and space), atthe expense of the more recent developments which are only briefly discussed.The main reason for such a choice is that, while the recent literature isreadily available, and surveys have already been published on the subject, inthe author's opinion a clear link with past work had not been completelyclarified.
arxiv-1511-01556 | Mining Local Gazetteers of Literary Chinese with CRF and Pattern based Methods for Biographical Information in Chinese History |  http://arxiv.org/abs/1511.01556  | author:Chao-Lin Liu, Chih-Kai Huang, Hongsu Wang, Peter K. Bol category:cs.CL cs.DL cs.IR cs.LG published:2015-11-04 summary:Person names and location names are essential building blocks for identifyingevents and social networks in historical documents that were written inliterary Chinese. We take the lead to explore the research on algorithmicallyrecognizing named entities in literary Chinese for historical studies withlanguage-model based and conditional-random-field based methods, and extend ourwork to mining the document structures in historical documents. Practicalevaluations were conducted with texts that were extracted from more than 220volumes of local gazetteers (Difangzhi). Difangzhi is a huge and the singlemost important collection that contains information about officers who servedin local government in Chinese history. Our methods performed very well onthese realistic tests. Thousands of names and addresses were identified fromthe texts. A good portion of the extracted names match the biographicalinformation currently recorded in the China Biographical Database (CBDB) ofHarvard University, and many others can be verified by historians and willbecome as new additions to CBDB.
arxiv-1511-01284 | Lasso based feature selection for malaria risk exposure prediction |  http://arxiv.org/abs/1511.01284  | author:Bienvenue Kouwayè, Noël Fonton, Fabrice Rossi category:stat.ML published:2015-11-04 summary:In life sciences, the experts generally use empirical knowledge to recodevariables, choose interactions and perform selection by classical approach. Theaim of this work is to perform automatic learning algorithm for variablesselection which can lead to know if experts can be help in they decision orsimply replaced by the machine and improve they knowledge and results. TheLasso method can detect the optimal subset of variables for estimation andprediction under some conditions. In this paper, we propose a novel approachwhich uses automatically all variables available and all interactions. By adouble cross-validation combine with Lasso, we select a best subset ofvariables and with GLM through a simple cross-validation perform predictions.The algorithm assures the stability and the the consistency of estimators.
arxiv-1511-01427 | Turing Computation with Recurrent Artificial Neural Networks |  http://arxiv.org/abs/1511.01427  | author:Giovanni S Carmantini, Peter beim Graben, Mathieu Desroches, Serafim Rodrigues category:cs.NE published:2015-11-04 summary:We improve the results by Siegelmann & Sontag (1995) by providing a novel andparsimonious constructive mapping between Turing Machines and RecurrentArtificial Neural Networks, based on recent developments of Nonlinear DynamicalAutomata. The architecture of the resulting R-ANNs is simple and elegant,stemming from its transparent relation with the underlying NDAs. Thesecharacteristics yield promise for developments in machine learning methods andsymbolic computation with continuous time dynamical systems. A framework isprovided to directly program the R-ANNs from Turing Machine descriptions, inabsence of network training. At the same time, the network can potentially betrained to perform algorithmic tasks, with exciting possibilities in theintegration of approaches akin to Google DeepMind's Neural Turing Machines.
arxiv-1511-01304 | Dictionary descent in optimization |  http://arxiv.org/abs/1511.01304  | author:Vladimir Temlyakov category:stat.ML math.NA published:2015-11-04 summary:The problem of convex optimization is studied. Usually in convex optimizationthe minimization is over a d-dimensional domain. Very often the convergencerate of an optimization algorithm depends on the dimension d. The algorithmsstudied in this paper utilize dictionaries instead of a canonical basis used inthe coordinate descent algorithms. We show how this approach allows us toreduce dimensionality of the problem. Also, we investigate which properties ofa dictionary are beneficial for the convergence rate of typical greedy-typealgorithms.
arxiv-1511-01245 | Decomposition into Low-rank plus Additive Matrices for Background/Foreground Separation: A Review for a Comparative Evaluation with a Large-Scale Dataset |  http://arxiv.org/abs/1511.01245  | author:Thierry Bouwmans, Andrews Sobral, Sajid Javed, Soon Ki Jung, El-Hadi Zahzah category:cs.CV published:2015-11-04 summary:Recent research on problem formulations based on decomposition into low-rankplus sparse matrices shows a suitable framework to separate moving objects fromthe background. The most representative problem formulation is the RobustPrincipal Component Analysis (RPCA) solved via Principal Component Pursuit(PCP) which decomposes a data matrix in a low-rank matrix and a sparse matrix.However, similar robust implicit or explicit decompositions can be made in thefollowing problem formulations: Robust Non-negative Matrix Factorization(RNMF), Robust Matrix Completion (RMC), Robust Subspace Recovery (RSR), RobustSubspace Tracking (RST) and Robust Low-Rank Minimization (RLRM). The main goalof these similar problem formulations is to obtain explicitly or implicitly adecomposition into low-rank matrix plus additive matrices. In this context,this work aims to initiate a rigorous and comprehensive review of the similarproblem formulations in robust subspace learning and tracking based ondecomposition into low-rank plus additive matrices for testing and rankingexisting algorithms for background/foreground separation. For this, we firstprovide a preliminary review of the recent developments in the differentproblem formulations which allows us to define a unified view that we calledDecomposition into Low-rank plus Additive Matrices (DLAM). Then, we examinecarefully each method in each robust subspace learning/tracking frameworks withtheir decomposition, their loss functions, their optimization problem and theirsolvers. Furthermore, we investigate if incremental algorithms and real-timeimplementations can be achieved for background/foreground separation. Finally,experimental results on a large-scale dataset called Background ModelsChallenge (BMC 2012) show the comparative performance of 32 different robustsubspace learning/tracking methods.
arxiv-1511-01282 | Factorizing LambdaMART for cold start recommendations |  http://arxiv.org/abs/1511.01282  | author:Phong Nguyen, Jun Wang, Alexandros Kalousis category:cs.LG cs.IR published:2015-11-04 summary:Recommendation systems often rely on point-wise loss metrics such as the meansquared error. However, in real recommendation settings only few items arepresented to a user. This observation has recently encouraged the use ofrank-based metrics. LambdaMART is the state-of-the-art algorithm in learning torank which relies on such a metric. Despite its success it does not have aprincipled regularization mechanism relying in empirical approaches to controlmodel complexity leaving it thus prone to overfitting. Motivated by the fact that very often the users' and items' descriptions aswell as the preference behavior can be well summarized by a small number ofhidden factors, we propose a novel algorithm, LambdaMART Matrix Factorization(LambdaMART-MF), that learns a low rank latent representation of users anditems using gradient boosted trees. The algorithm factorizes lambdaMART bydefining relevance scores as the inner product of the learned representationsof the users and items. The low rank is essentially a model complexitycontroller; on top of it we propose additional regularizers to constraint thelearned latent representations that reflect the user and item manifolds asthese are defined by their original feature based descriptors and thepreference behavior. Finally we also propose to use a weighted variant of NDCGto reduce the penalty for similar items with large rating discrepancy. We experiment on two very different recommendation datasets, meta-mining andmovies-users, and evaluate the performance of LambdaMART-MF, with and withoutregularization, in the cold start setting as well as in the simpler matrixcompletion setting. In both cases it outperforms in a significant mannercurrent state of the art algorithms.
arxiv-1511-01443 | A Distributed One-Step Estimator |  http://arxiv.org/abs/1511.01443  | author:Cheng Huang, Xiaoming Huo category:stat.ME cs.DC stat.ML published:2015-11-04 summary:Distributed statistical inference has recently attracted enormous attention.Many existing work focuses on the averaging estimator. We propose a one-stepapproach to enhance a simple-averaging based distributed estimator. We derivethe corresponding asymptotic properties of the newly proposed estimator. Wefind that the proposed one-step estimator enjoys the same asymptotic propertiesas the centralized estimator. The proposed one-step approach merely requiresone additional round of communication in relative to the averaging estimator;so the extra communication burden is insignificant. In finite sample cases,numerical examples show that the proposed estimator outperforms the simpleaveraging estimator with a large margin in terms of the mean squared errors. Apotential application of the one-step approach is that one can use multiplemachines to speed up large scale statistical inference with little compromisein the quality of estimators. The proposed method becomes more valuable whendata can only be available at distributed machines with limited communicationbandwidth.
arxiv-1511-01419 | Train and Test Tightness of LP Relaxations in Structured Prediction |  http://arxiv.org/abs/1511.01419  | author:Ofer Meshi, Mehrdad Mahdavi, Adrian Weller, David Sontag category:stat.ML cs.AI cs.LG published:2015-11-04 summary:Structured prediction is used in areas such as computer vision and naturallanguage processing to predict structured outputs such as segmentations orparse trees. In these settings, prediction is performed by MAP inference or,equivalently, by solving an integer linear program. Because of the complexscoring functions required to obtain accurate predictions, both learning andinference typically require the use of approximate solvers. We propose atheoretical explanation to the striking observation that approximations basedon linear programming (LP) relaxations are often tight on real-world instances.In particular, we show that learning with LP relaxed inference encouragesintegrality of training instances, and that tightness generalizes from train totest data.
arxiv-1511-01442 | Low-Rank Approximation of Weighted Tree Automata |  http://arxiv.org/abs/1511.01442  | author:Guillaume Rabusseau, Borja Balle, Shay B. Cohen category:cs.LG cs.FL published:2015-11-04 summary:We describe a technique to minimize weighted tree automata (WTA), a powerfulformalisms that subsumes probabilistic context-free grammars (PCFGs) andlatent-variable PCFGs. Our method relies on a singular value decomposition ofthe underlying Hankel matrix defined by the WTA. Our main theoretical result isan efficient algorithm for computing the SVD of an infinite Hankel matriximplicitly represented as a WTA. We provide an analysis of the approximationerror induced by the minimization, and we evaluate our method on real-worlddata originating in newswire treebank. We show that the model achieves lowerperplexity than previous methods for PCFG minimization, and also is much morestable due to the absence of local optima.
arxiv-1511-01473 | How Robust are Reconstruction Thresholds for Community Detection? |  http://arxiv.org/abs/1511.01473  | author:Ankur Moitra, William Perry, Alexander S. Wein category:cs.DS cs.IT cs.LG math.IT math.PR stat.ML published:2015-11-04 summary:The stochastic block model is one of the oldest and most ubiquitous modelsfor studying clustering and community detection. In an exciting sequence ofdevelopments, motivated by deep but non-rigorous ideas from statisticalphysics, Decelle et al. conjectured a sharp threshold for when communitydetection is possible in the sparse regime. Mossel, Neeman and Sly andMassoulie proved the conjecture and gave matching algorithms and lower bounds. Here we revisit the stochastic block model from the perspective of semirandommodels where we allow an adversary to make `helpful' changes that strengthenties within each community and break ties between them. We show a surprisingresult that these `helpful' changes can shift the information-theoreticthreshold, making the community detection problem strictly harder. Wecomplement this by showing that an algorithm based on semidefinite programming(which was known to get close to the threshold) continues to work in thesemirandom model (even for partial recovery). This suggests that algorithmsbased on semidefinite programming are robust in ways that any algorithm meetingthe information-theoretic threshold cannot be. These results point to an interesting new direction: Can we find robust,semirandom analogues to some of the classical, average-case thresholds instatistics? We also explore this question in the broadcast tree model, and weshow that the viewpoint of semirandom models can help explain why somealgorithms are preferred to others in practice, in spite of the gaps in theirstatistical performance on random models.
arxiv-1511-01281 | Co-Clustering Network-Constrained Trajectory Data |  http://arxiv.org/abs/1511.01281  | author:Mohamed Khalil El Mahrsi, Romain Guigourès, Fabrice Rossi, Marc Boullé category:stat.ML cs.DB cs.LG published:2015-11-04 summary:Recently, clustering moving object trajectories kept gaining interest fromboth the data mining and machine learning communities. This problem, however,was studied mainly and extensively in the setting where moving objects can movefreely on the euclidean space. In this paper, we study the problem ofclustering trajectories of vehicles whose movement is restricted by theunderlying road network. We model relations between these trajectories and roadsegments as a bipartite graph and we try to cluster its vertices. Wedemonstrate our approaches on synthetic data and show how it could be useful ininferring knowledge about the flow dynamics and the behavior of the driversusing the road network.
arxiv-1511-01280 | Study of a bias in the offline evaluation of a recommendation algorithm |  http://arxiv.org/abs/1511.01280  | author:Arnaud De Myttenaere, Boris Golden, Bénédicte Le Grand, Fabrice Rossi category:cs.IR cs.LG stat.ML published:2015-11-04 summary:Recommendation systems have been integrated into the majority of large onlinesystems to filter and rank information according to user profiles. It thusinfluences the way users interact with the system and, as a consequence, biasthe evaluation of the performance of a recommendation algorithm computed usinghistorical data (via offline evaluation). This paper describes this bias anddiscuss the relevance of a weighted offline evaluation to reduce this bias fordifferent classes of recommendation algorithms.
arxiv-1511-01259 | Transforming Wikipedia into a Search Engine for Local Experts |  http://arxiv.org/abs/1511.01259  | author:Gregory Grefenstette, Karima Rafes category:cs.IR cs.CL published:2015-11-04 summary:Finding experts for a given problem is recognized as a difficult task. Evenwhen a taxonomy of subject expertise exists, and is associated with a group ofexperts, it can be hard to exploit by users who have not internalized thetaxonomy. Here we present a method for both attaching experts to a domainontology, and hiding this fact from the end user looking for an expert. Bylinking Wikipedia to this same pivot ontology, we describe how a user canbrowse Wikipedia, as they normally do to search for information, and use thisbrowsing behavior to find experts. Experts are characterized by their textualproductions (webpages, publications, reports), and these textual productionsare attached to concepts in the pivot ontology. When the user finds theWikipedia page characterizing their need, a list of experts is displayed. Inthis way we transform Wikipedia into a search engine for experts.
arxiv-1511-01432 | Semi-supervised Sequence Learning |  http://arxiv.org/abs/1511.01432  | author:Andrew M. Dai, Quoc V. Le category:cs.LG cs.CL published:2015-11-04 summary:We present two approaches that use unlabeled data to improve sequencelearning with recurrent networks. The first approach is to predict what comesnext in a sequence, which is a conventional language model in natural languageprocessing. The second approach is to use a sequence autoencoder, which readsthe input sequence into a vector and predicts the input sequence again. Thesetwo algorithms can be used as a "pretraining" step for a later supervisedsequence learning algorithm. In other words, the parameters obtained from theunsupervised step can be used as a starting point for other supervised trainingmodels. In our experiments, we find that long short term memory recurrentnetworks after being pretrained with the two approaches are more stable andgeneralize better. With pretraining, we are able to train long short termmemory recurrent networks up to a few hundred timesteps, thereby achievingstrong performance in many text classification tasks, such as IMDB, DBpedia and20 Newsgroups.
arxiv-1511-01480 | Approximation of the truncated Zeta distribution and Zipf's law |  http://arxiv.org/abs/1511.01480  | author:Maurizio Naldi category:stat.AP cs.CL cs.SI published:2015-11-04 summary:Zipf's law appears in many application areas but does not have a closed formexpression, which may make its use cumbersome. Since it coincides with thetruncated version of the Zeta distribution, in this paper we propose threeapproximate closed form expressions for the truncated Zeta distribution, whichmay be employed for Zipf's law as well. The three approximations are based onthe replacement of the sum occurring in Zipf's law with an integral, and arenamed respectively the integral approximation, the average integralapproximation, and the trapezoidal approximation. While the first one is shownto be of little use, the trapezoidal approximation exhibits an error which istypically lower than 1\%, but is as low as 0.1\% for the range of values of theZipf parameter below 1.
arxiv-1512-04354 | A proposal project for a blind image quality assessment by learning distortions from the full reference image quality assessments |  http://arxiv.org/abs/1512.04354  | author:Stéfane Paris category:cs.MM cs.CV published:2015-11-04 summary:This short paper presents a perspective plan to build a null reference imagequality assessment. Its main goal is to deliver both the objective score andthe distortion map for a given distorted image without the knowledge of itsreference image.
arxiv-1511-01186 | Face Aging Effect Simulation using Hidden Factor Analysis Joint Sparse Representation |  http://arxiv.org/abs/1511.01186  | author:Hongyu Yang, Di Huang, Yunhong Wang, Heng Wang, Yuanyan Tang category:cs.CV published:2015-11-04 summary:Face aging simulation has received rising investigations nowadays, whereas itstill remains a challenge to generate convincing and natural age-progressedface images. In this paper, we present a novel approach to such an issue byusing hidden factor analysis joint sparse representation. In contrast to themajority of tasks in the literature that handle the facial texture integrally,the proposed aging approach separately models the person-specific facialproperties that tend to be stable in a relatively long period and theage-specific clues that change gradually over time. It then merely transformsthe age component to a target age group via sparse reconstruction, yieldingaging effects, which is finally combined with the identity component to achievethe aged face. Experiments are carried out on three aging databases, and theresults achieved clearly demonstrate the effectiveness and robustness of theproposed method in rendering a face with aging effects. Additionally, a seriesof evaluations prove its validity with respect to identity preservation andaging effect generation.
arxiv-1511-01168 | Cell identification in whole-brain multiview images of neural activation |  http://arxiv.org/abs/1511.01168  | author:Marco Paciscopi, Ludovico Silvestri, Francesco Saverio Pavone, Paolo Frasconi category:cs.CV J.3 published:2015-11-04 summary:We present a scalable method for brain cell identification in multiviewconfocal light sheet microscopy images. Our algorithmic pipeline includes ahierarchical registration approach and a novel multiview version of semanticdeconvolution that simultaneously enhance visibility of fluorescent cellbodies, equalize their contrast, and fuses adjacent views into a single 3Dimages on which cell identification is performed with mean shift. We present empirical results on a whole-brain image of an adult Arc-dVenusmouse acquired at 4micron resolution. Based on an annotated test volumecontaining 3278 cells, our algorithm achieves an $F_1$ measure of 0.89.
arxiv-1511-01169 | adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs |  http://arxiv.org/abs/1511.01169  | author:Nitish Shirish Keskar, Albert S. Berahas category:cs.LG math.OC stat.ML published:2015-11-04 summary:Recurrent Neural Networks (RNNs) are powerful models that achieve exceptionalperformance on several pattern recognition problems. However, the training ofRNNs is a computationally difficult task owing to the well-known"vanishing/exploding" gradient problem. Algorithms proposed for training RNNseither exploit no (or limited) curvature information and have cheapper-iteration complexity, or attempt to gain significant curvature informationat the cost of increased per-iteration cost. The former set includesdiagonally-scaled first-order methods such as ADAGRAD and ADAM, while thelatter consists of second-order algorithms like Hessian-Free Newton and K-FAC.In this paper, we present adaQN, a stochastic quasi-Newton algorithm fortraining RNNs. Our approach retains a low per-iteration cost while allowing fornon-diagonal scaling through a stochastic L-BFGS updating scheme. The methoduses a novel L-BFGS scaling initialization scheme and is judicious in storingand retaining L-BFGS curvature pairs. We present numerical experiments on twolanguage modeling tasks and show that adaQN is competitive with popular RNNtraining algorithms.
arxiv-1511-01512 | Mean-field inference of Hawkes point processes |  http://arxiv.org/abs/1511.01512  | author:Emmanuel Bacry, Stéphane Gaïffas, Iacopo Mastromatteo, Jean-François Muzy category:cs.LG published:2015-11-04 summary:We propose a fast and efficient estimation method that is able to accuratelyrecover the parameters of a d-dimensional Hawkes point-process from a set ofobservations. We exploit a mean-field approximation that is valid when thefluctuations of the stochastic intensity are small. We show that this isnotably the case in situations when interactions are sufficiently weak, whenthe dimension of the system is high or when the fluctuations are self-averagingdue to the large number of past events they involve. In such a regime theestimation of a Hawkes process can be mapped on a least-squares problem forwhich we provide an analytic solution. Though this estimator is biased, we showthat its precision can be comparable to the one of the Maximum LikelihoodEstimator while its computation speed is shown to be improved considerably. Wegive a theoretical control on the accuracy of our new approach and illustrateits efficiency using synthetic datasets, in order to assess the statisticalestimation error of the parameters.
arxiv-1511-01508 | Enhancing Feature Tracking With Gyro Regularization |  http://arxiv.org/abs/1511.01508  | author:Bryan Poling, Gilad Lerman category:cs.CV 68T45 published:2015-11-04 summary:We present a deeply integrated method of exploiting low-cost gyroscopes toimprove general purpose feature tracking. Most previous methods use gyroscopesto initialize and bound the search for features. In contrast, we use them toregularize the tracking energy function so that they can directly assist in thetracking of ambiguous and poor-quality features. We demonstrate that our simpletechnique offers significant improvements in performance over conventionaltemplate-based tracking methods, and is in fact competitive with more complexand computationally expensive state-of-the-art trackers, but at a fraction ofthe computational cost. Additionally, we show that the practice of initializingtemplate-based feature trackers like KLT (Kanade-Lucas-Tomasi) usinggyro-predicted optical flow offers no advantage over using a carefuloptical-only initialization method, suggesting that some deeper level ofintegration, like the method we propose, is needed in order to realize agenuine improvement in tracking performance from these inertial sensors.
arxiv-1511-01411 | Learning in Auctions: Regret is Hard, Envy is Easy |  http://arxiv.org/abs/1511.01411  | author:Constantinos Daskalakis, Vasilis Syrgkanis category:cs.GT cs.AI cs.CC cs.LG published:2015-11-04 summary:A line of recent work provides welfare guarantees of simple combinatorialauction formats, such as selling m items via simultaneous second price auctions(SiSPAs) (Christodoulou et al. 2008, Bhawalkar and Roughgarden 2011, Feldman etal. 2013). These guarantees hold even when the auctions are repeatedly executedand players use no-regret learning algorithms. Unfortunately, off-the-shelfno-regret algorithms for these auctions are computationally inefficient as thenumber of actions is exponential. We show that this obstacle is insurmountable:there are no polynomial-time no-regret algorithms for SiSPAs, unlessRP$\supseteq$ NP, even when the bidders are unit-demand. Our lower bound raisesthe question of how good outcomes polynomially-bounded bidders may discover insuch auctions. To answer this question, we propose a novel concept of learning in auctions,termed "no-envy learning." This notion is founded upon Walrasian equilibrium,and we show that it is both efficiently implementable and results inapproximately optimal welfare, even when the bidders have fractionallysubadditive (XOS) valuations (assuming demand oracles) or coverage valuations(without demand oracles). No-envy learning outcomes are a relaxation ofno-regret outcomes, which maintain their approximate welfare optimality whileendowing them with computational tractability. Our results extend to otherauction formats that have been studied in the literature via the smoothnessparadigm. Our results for XOS valuations are enabled by a novelFollow-The-Perturbed-Leader algorithm for settings where the number of expertsis infinite, and the payoff function of the learner is non-linear. Thisalgorithm has applications outside of auction settings, such as in securitygames. Our result for coverage valuations is based on a novel use of convexrounding schemes and a reduction to online convex optimization.
arxiv-1511-01214 | Quantifying the information of the prior and likelihood in parametric Bayesian modeling |  http://arxiv.org/abs/1511.01214  | author:Giri Gopalan category:stat.ML cs.IT math.IT stat.AP stat.ME published:2015-11-04 summary:I suggest using a pair of metrics to quantify the information of the priorand likelihood functions within a parametric Bayesian model, one of which isclosely related to the reference priors of Berger and Bernardo (Bernardo 1979,Berger and Bernardo 2009) and information measure introduced by Lindley(Lindley 1956). A Monte Carlo algorithm to estimate these metrics is developedand their properties are explored via a combination of theoretical results,simulations, and applications to public medical data sets. This combination oftheoretical, empirical, and computational support provides evidence that thesemetrics may be useful diagnostic tools when performing a Bayesian analysis.
arxiv-1511-01032 | TribeFlow: Mining & Predicting User Trajectories |  http://arxiv.org/abs/1511.01032  | author:Flavio Figueiredo, Bruno Ribeiro, Jussara Almeida, Christos Faloutsos category:cs.SI physics.soc-ph stat.ML published:2015-11-03 summary:Which song will Smith listen to next? Which restaurant will Alice go totomorrow? Which product will John click next? These applications have in commonthe prediction of user trajectories that are in a constant state of flux over ahidden network (e.g. website links, geographic location). What users are doingnow may be unrelated to what they will be doing in an hour from now. Mindful ofthese challenges we propose TribeFlow, a method designed to cope with thecomplex challenges of learning personalized predictive models ofnon-stationary, transient, and time-heterogeneous user trajectories. TribeFlowis a general method that can perform next product recommendation, next songrecommendation, next location prediction, and general arbitrary-length usertrajectory prediction without domain-specific knowledge. TribeFlow is moreaccurate and up to 413x faster than top competitors.
arxiv-1511-00830 | The Variational Fair Autoencoder |  http://arxiv.org/abs/1511.00830  | author:Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, Richard Zemel category:stat.ML cs.LG published:2015-11-03 summary:We investigate the problem of learning representations that are invariant tocertain nuisance or sensitive factors of variation in the data while retainingas much of the remaining information as possible. Our model is based on avariational autoencoding architecture with priors that encourage independencebetween sensitive and latent factors of variation. Any subsequent processing,such as classification, can then be performed on this purged latentrepresentation. To remove any remaining dependencies we incorporate anadditional penalty term based on the "Maximum Mean Discrepancy" (MMD) measure.We discuss how these architectures can be efficiently trained on data and showin experiments that this method is more effective than previous work inremoving unwanted sources of variation while maintaining informative latentrepresentations.
arxiv-1511-00754 | PAC Learning-Based Verification and Model Synthesis |  http://arxiv.org/abs/1511.00754  | author:Yu-Fang Chen, Chiao Hsieh, Ondřej Lengál, Tsung-Ju Lii, Ming-Hsien Tsai, Bow-Yaw Wang, Farn Wang category:cs.SE cs.LG cs.LO published:2015-11-03 summary:We introduce a novel technique for verification and model synthesis ofsequential programs. Our technique is based on learning a regular model of theset of feasible paths in a program, and testing whether this model contains anincorrect behavior. Exact learning algorithms require checking equivalencebetween the model and the program, which is a difficult problem, in generalundecidable. Our learning procedure is therefore based on the framework ofprobably approximately correct (PAC) learning, which uses sampling instead andprovides correctness guarantees expressed using the terms error probability andconfidence. Besides the verification result, our procedure also outputs themodel with the said correctness guarantees. Obtained preliminary experimentsshow encouraging results, in some cases even outperforming mature softwareverifiers.
arxiv-1511-00925 | Do Prices Coordinate Markets? |  http://arxiv.org/abs/1511.00925  | author:Justin Hsu, Jamie Morgenstern, Ryan Rogers, Aaron Roth, Rakesh Vohra category:cs.GT cs.LG published:2015-11-03 summary:Walrasian equilibrium prices can be said to coordinate markets: They supporta welfare optimal allocation in which each buyer is buying bundle of goods thatis individually most preferred. However, this clean story has two caveats.First, the prices alone are not sufficient to coordinate the market, and buyersmay need to select among their most preferred bundles in a coordinated way tofind a feasible allocation. Second, we don't in practice expect to encounterexact equilibrium prices tailored to the market, but instead only approximateprices, somehow encoding "distributional" information about the market. Howwell do prices work to coordinate markets when tie-breaking is not coordinated,and they encode only distributional information? We answer this question. First, we provide a genericity condition such thatfor buyers with Matroid Based Valuations, overdemand with respect toequilibrium prices is at most 1, independent of the supply of goods, even whentie-breaking is done in an uncoordinated fashion. Second, we providelearning-theoretic results that show that such prices are robust to changingthe buyers in the market, so long as all buyers are sampled from the same(unknown) distribution.
arxiv-1511-00792 | Scalable Recommendation from Web Usage Mining using Method of Moments |  http://arxiv.org/abs/1511.00792  | author:Sayantan Dasgupta category:cs.LG published:2015-11-03 summary:With the advent of mass-available Internet, twenty-first century observed asteady growth in web based commercial services and technology companies. Mostof them are based on web applications that receive huge amount of usertraffics, and generate massive amount of web usage data containing user-iteminteractions. We attempt to build a recommendation algorithm based on such webusage data. It is essential that recommendation algorithms for suchapplications are highly scalable in nature. Existing algorithms such as matrixfactorization run several iterations through the dataset, and therefore may notbe suitable for large web-scale datasets. Here we propose a highly scalablerecommendation algorithm based on recently proposed Method of Moments (alsoknown as Spectral Method). Our method takes only two to three passes throughthe entire dataset to extract the model parameters during the training phase.We demonstrate the competitive performance of our algorithm in comparison withthe existing algorithms on various publicly available datasets through severalempirical measures.
arxiv-1511-00831 | PCA-Based Out-of-Sample Extension for Dimensionality Reduction |  http://arxiv.org/abs/1511.00831  | author:Yariv Aizenbud, Amit Bermanis, Amir Averbuch category:stat.ML published:2015-11-03 summary:Dimensionality reduction methods are very common in the field of highdimensional data analysis. Typically, algorithms for dimensionality reductionare computationally expensive. Therefore, their applications for the analysisof massive amounts of data are impractical. For example, repeated computationsdue to accumulated data are computationally prohibitive. In this paper, anout-of-sample extension scheme, which is used as a complementary method fordimensionality reduction, is presented. We describe an algorithm which performsan out-of-sample extension to newly-arrived data points. Unlike other extensionalgorithms such as Nystr\"om algorithm, the proposed algorithm uses theintrinsic geometry of the data and properties for dimensionality reduction map.We prove that the error of the proposed algorithm is bounded. Additionally tothe out-of-sample extension, the algorithm provides a degree of the abnormalityof any newly-arrived data point.
arxiv-1511-01161 | Image based compensation for thickness variation in microscopy section series |  http://arxiv.org/abs/1511.01161  | author:Philipp Hanslovsky, John A. Bogovic, C. Shan Xu, Kenneth J. Hayworth, Zhiyuan Lu, Harald F. Hess, Stephan Saalfeld category:cs.CV published:2015-11-03 summary:Serial block face scanning electron microscopy in combination with focusedion beam milling (FIB-SEM) has become a popular method for nanometer-resolutionisotropic imaging of neural and other cellular tissue with a planar field ofview of up to 100um. While FIB-SEM is particularly attractive for its highin-plane resolution, ion beam milling generates non-planar block faces andinhomogeneous z-spacing leading to distorted volume acquisitions. We extend ourprevious work on image-based z-spacing correction for serial section series todetermine a deformation field that varies within the xy-plane to account fornon-planarity. We show that our method identifies and corrects thesedistortions in real world FIB-SEM acquisitions and quantitatively assess itsprecision on virtual ground truth. Our method is available as an open sourceimplementation that is parallelized using the Spark framework enabling rapidprocessing of very large volumes.
arxiv-1601-03809 | Artificial neural network approach for condition-based maintenance |  http://arxiv.org/abs/1601.03809  | author:Mostafa Sayyed category:cs.NE cs.CY published:2015-11-03 summary:In this research, computerized maintenance management will be investigated.The rise of maintenance cost forced the research community to look for moreeffective ways to schedule maintenance operations. Using computerized models tocome up with optimal maintenance policy has led to better equipment utilizationand lower costs. This research adopts Condition-Based Maintenance model wherethe maintenance decision is generated based on equipment conditions. ArtificialNeural Network technique is proposed to capture and analyze equipment conditionsignals which lead to higher level of knowledge gathering. This knowledge isused to accurately estimate equipment failure time. Based on these estimations,an optimal maintenance management policy can be achieved.
arxiv-1511-00758 | High-Performance and Tunable Stereo Reconstruction |  http://arxiv.org/abs/1511.00758  | author:Sudeep Pillai, Srikumar Ramalingam, John J. Leonard category:cs.RO cs.CV published:2015-11-03 summary:Traditional stereo algorithms have focused their efforts on reconstructionquality and have largely avoided prioritizing for run time performance. Robots,on the other hand, require quick maneuverability and effective computation toobserve its immediate environment and perform tasks within it. In this work, wepropose a high-performance and tunable stereo disparity estimation method, witha peak frame-rate of 120Hz (VGA resolution, on a single CPU-thread), that canpotentially enable robots to quickly reconstruct their immediate surroundingsand maneuver at high-speeds. Our key contribution is a disparity estimationalgorithm that iteratively approximates the scene depth via a piece-wise planarmesh from stereo imagery, with a fast depth validation step for semi-densereconstruction. The mesh is initially seeded with sparsely matched keypoints,and is recursively tessellated and refined as needed (via a resampling stage),to provide the desired stereo disparity accuracy. The inherent simplicity andspeed of our approach, with the ability to tune it to a desired reconstructionquality and runtime performance makes it a compelling solution for applicationsin high-speed vehicles.
arxiv-1511-00871 | Properties of the Sample Mean in Graph Spaces and the Majorize-Minimize-Mean Algorithm |  http://arxiv.org/abs/1511.00871  | author:Brijnesh J. Jain category:cs.CV cs.LG stat.ML published:2015-11-03 summary:One of the most fundamental concepts in statistics is the concept of samplemean. Properties of the sample mean that are well-defined in Euclidean spacesbecome unwieldy or even unclear in graph spaces. Open problems related to thesample mean of graphs include: non-existence, non-uniqueness, statisticalinconsistency, lack of convergence results of mean algorithms, non-existence ofmidpoints, and disparity to midpoints. We present conditions to resolve all sixproblems and propose a Majorize-Minimize-Mean (MMM) Algorithm. Experiments ongraph datasets representing images and molecules show that the MMM-Algorithmbest approximates a sample mean of graphs compared to six other meanalgorithms.
arxiv-1511-01158 | Distributed Deep Learning for Answer Selection |  http://arxiv.org/abs/1511.01158  | author:Minwei Feng, Bing Xiang, Bowen Zhou category:cs.LG cs.CL cs.DC published:2015-11-03 summary:This paper is an empirical study of the distributed deep learning for aquestion answering subtask: answer selection. Comparison studies of SGD, MSGD,DOWNPOUR and EASGD/EAMSGD algorithms have been presented. Experimental resultsshow that the message passing interface based distributed framework canaccelerate the convergence speed at a sublinear scale. This paper demonstratesthe importance of distributed training: with 120 workers, an 83x speedup isachievable and running time is decreased from 107.9 hours to 1.3 hours, whichwill benefit the productivity significantly.
arxiv-1511-01156 | Robust Large-Scale Localization in 3D Point Clouds Revisited |  http://arxiv.org/abs/1511.01156  | author:Fabian Tschopp, Marco Zorzi category:cs.CV published:2015-11-03 summary:We tackle the problem of getting a full 6-DOF pose estimation of a queryimage inside a given point cloud. This technical report re-evaluates thealgorithms proposed by Y. Li et al. "Worldwide Pose Estimation using 3D PointCloud". Our code computes poses from 3 or 4 points, with both known and unknownfocal length. The results can easily be displayed and analyzed with Meshlab. Wefound both advantages and shortcomings of the methods proposed. Furthermore,additional priors and parameters for point selection, RANSAC and pose qualityestimate (inlier test) are proposed and applied.
arxiv-1511-00971 | Data Stream Classification using Random Feature Functions and Novel Method Combinations |  http://arxiv.org/abs/1511.00971  | author:Diego Marrón, Jesse Read, Albert Bifet, Nacho Navarro category:cs.LG cs.NE published:2015-11-03 summary:Big Data streams are being generated in a faster, bigger, and morecommonplace. In this scenario, Hoeffding Trees are an established method forclassification. Several extensions exist, including high-performing ensemblesetups such as online and leveraging bagging. Also, $k$-nearest neighbors is apopular choice, with most extensions dealing with the inherent performancelimitations over a potentially-infinite stream. At the same time, gradient descent methods are becoming increasingly popular,owing in part to the successes of deep learning. Although deep neural networkscan learn incrementally, they have so far proved too sensitive tohyper-parameter options and initial conditions to be considered an effective`off-the-shelf' data-streams solution. In this work, we look at combinations of Hoeffding-trees, nearest neighbour,and gradient descent methods with a streaming preprocessing approach in theform of a random feature functions filter for additional predictive power. We further extend the investigation to implementing methods on GPUs, which wetest on some large real-world datasets, and show the benefits of using GPUs fordata-stream learning due to their high scalability. Our empirical evaluation yields positive results for the novel approachesthat we experiment with, highlighting important issues, and shed light onpromising future directions in approaches to data-stream classification.
arxiv-1511-01154 | Robust Registration of Calcium Images by Learned Contrast Synthesis |  http://arxiv.org/abs/1511.01154  | author:John A. Bogovic, Philipp Hanslovsky, Allan Wong, Stephan Saalfeld category:cs.CV published:2015-11-03 summary:Multi-modal image registration is a challenging task that is vital to fusecomplementary signals for subsequent analyses. Despite much research into costfunctions addressing this challenge, there exist cases in which these areineffective. In this work, we show that (1) this is true for the registrationof in-vivo Drosophila brain volumes visualizing genetically encoded calciumindicators to an nc82 atlas and (2) that machine learning based contrastsynthesis can yield improvements. More specifically, the number of subjects forwhich the registration outright failed was greatly reduced (from 40% to 15%) byusing a synthesized image.
arxiv-1511-01088 | There is no fast lunch: an examination of the running speed of evolutionary algorithms in several languages |  http://arxiv.org/abs/1511.01088  | author:Juan-J. Merelo, Pablo García-Sánchez, Mario García-Valdez, Israel Blancas category:cs.NE cs.PF published:2015-11-03 summary:It is quite usual when an evolutionary algorithm tool or library uses alanguage other than C, C++, Java or Matlab that a reviewer or the audiencequestions its usefulness based on the speed of those other languages,purportedly slower than the aforementioned ones. Despite speed being noteverything needed to design a useful evolutionary algorithm application, inthis paper we will measure the speed for several very basic evolutionaryalgorithm operations in several languages which use different virtual machinesand approaches, and prove that, in fact, there is no big difference in speedbetween interpreted and compiled languages, and that in some cases, interpretedlanguages such as JavaScript or Python can be faster than compiled languagessuch as Scala, making them worthy of use for evolutionary algorithmexperimentation.
arxiv-1511-01029 | Understanding symmetries in deep networks |  http://arxiv.org/abs/1511.01029  | author:Vijay Badrinarayanan, Bamdev Mishra, Roberto Cipolla category:cs.LG cs.AI cs.CV published:2015-11-03 summary:Recent works have highlighted scale invariance or symmetry present in theweight space of a typical deep network and the adverse effect it has on theEuclidean gradient based stochastic gradient descent optimization. In thiswork, we show that a commonly used deep network, which uses convolution, batchnormalization, reLU, max-pooling, and sub-sampling pipeline, possess morecomplex forms of symmetry arising from scaling-based reparameterization of thenetwork weights. We propose to tackle the issue of the weight space symmetry byconstraining the filters to lie on the unit-norm manifold. Consequently,training the network boils down to using stochastic gradient descent updates onthe unit-norm manifold. Our empirical evidence based on the MNIST dataset showsthat the proposed updates improve the test performance beyond what is achievedwith batch normalization and without sacrificing the computational efficiencyof the weight updates.
arxiv-1511-01042 | Detecting Interrogative Utterances with Recurrent Neural Networks |  http://arxiv.org/abs/1511.01042  | author:Junyoung Chung, Jacob Devlin, Hany Hassan Awadalla category:cs.CL cs.LG cs.NE published:2015-11-03 summary:In this paper, we explore different neural network architectures that canpredict if a speaker of a given utterance is asking a question or making astatement. We com- pare the outcomes of regularization methods that arepopularly used to train deep neural networks and study how different contextfunctions can affect the classification performance. We also compare theefficacy of gated activation functions that are favorably used in recurrentneural networks and study how to combine multimodal inputs. We evaluate ourmodels on two multimodal datasets: MSR-Skype and CALLHOME.
arxiv-1511-01017 | Consistent Parameter Estimation for LASSO and Approximate Message Passing |  http://arxiv.org/abs/1511.01017  | author:Ali Mousavi, Arian Maleki, Richard G. Baraniuk category:math.ST cs.IT math.IT math.OC stat.ML stat.TH published:2015-11-03 summary:We consider the problem of recovering a vector $\beta_o \in \mathbb{R}^p$from $n$ random and noisy linear observations $y= X\beta_o + w$, where $X$ isthe measurement matrix and $w$ is noise. The LASSO estimate is given by thesolution to the optimization problem $\hat{\beta}_{\lambda} = \arg \min_{\beta}\frac{1}{2} \y-X\beta\_2^2 + \lambda \ \beta \_1$. Among the iterativealgorithms that have been proposed for solving this optimization problem,approximate message passing (AMP) has attracted attention for its fastconvergence. Despite significant progress in the theoretical analysis of theestimates of LASSO and AMP, little is known about their behavior as a functionof the regularization parameter $\lambda$, or the thereshold parameters$\tau^t$. For instance the following basic questions have not yet been studiedin the literature: (i) How does the size of the active set$\\hat{\beta}^\lambda\_0/p$ behave as a function of $\lambda$? (ii) How doesthe mean square error $\\hat{\beta}_{\lambda} - \beta_o\_2^2/p$ behave as afunction of $\lambda$? (iii) How does $\\beta^t - \beta_o \_2^2/p$ behave asa function of $\tau^1, \ldots, \tau^{t-1}$? Answering these questions will helpin addressing practical challenges regarding the optimal tuning of $\lambda$ or$\tau^1, \tau^2, \ldots$. This paper answers these questions in the asymptoticsetting and shows how these results can be employed in deriving simple andtheoretically optimal approaches for tuning the parameters $\tau^1, \ldots,\tau^t$ for AMP or $\lambda$ for LASSO. It also explores the connection betweenthe optimal tuning of the parameters of AMP and the optimal tuning of LASSO.
arxiv-1511-00360 | Automatic Prosody Prediction for Chinese Speech Synthesis using BLSTM-RNN and Embedding Features |  http://arxiv.org/abs/1511.00360  | author:Chuang Ding, Lei Xie, Jie Yan, Weini Zhang, Yang Liu category:cs.CL cs.SD published:2015-11-02 summary:Prosody affects the naturalness and intelligibility of speech. However,automatic prosody prediction from text for Chinese speech synthesis is still agreat challenge and the traditional conditional random fields (CRF) basedmethod always heavily relies on feature engineering. In this paper, we proposeto use neural networks to predict prosodic boundary labels directly fromChinese characters without any feature engineering. Experimental results showthat stacking feed-forward and bidirectional long short-term memory (BLSTM)recurrent network layers achieves superior performance over the CRF-basedmethod. The embedding features learned from raw text further enhance theperformance.
arxiv-1511-00622 | On the Number of Many-to-Many Alignments of N Sequences |  http://arxiv.org/abs/1511.00622  | author:Steffen Eger category:math.CO cs.CL cs.DM published:2015-11-02 summary:We count the number of alignments of $N \ge 1$ sequences when match-up typesare from a specified set $S\subseteq \mathbb{N}^N$. Equivalently, we count thenumber of nonnegative integer matrices whose rows sum to a given fixed vectorand each of whose columns lie in $S$. We provide a new asymptotic formula forthe case $S=\{(s_1,\ldots,s_N) \:\: 1\le s_i\le 2\}$.
arxiv-1511-00573 | From random walks to distances on unweighted graphs |  http://arxiv.org/abs/1511.00573  | author:Tatsunori B. Hashimoto, Yi Sun, Tommi S. Jaakkola category:stat.ML cs.AI cs.SI published:2015-11-02 summary:Large unweighted directed graphs are commonly used to capture relationsbetween entities. A fundamental problem in the analysis of such networks is toproperly define the similarity or dissimilarity between any two vertices.Despite the significance of this problem, statistical characterization of theproposed metrics has been limited. We introduce and develop a class oftechniques for analyzing random walks on graphs using stochastic calculus.Using these techniques we generalize results on the degeneracy of hitting timesand analyze a metric based on the Laplace transformed hitting time (LTHT). Themetric serves as a natural, provably well-behaved alternative to the expectedhitting time. We establish a general correspondence between hitting times ofthe Brownian motion and analogous hitting times on the graph. We show that theLTHT is consistent with respect to the underlying metric of a geometric graph,preserves clustering tendency, and remains robust against random addition ofnon-geometric edges. Tests on simulated and real-world data show that the LTHTmatches theoretical predictions and outperforms alternatives.
arxiv-1511-00725 | Galaxy-X: A Novel Approach for Multi-class Classification in an Open Universe |  http://arxiv.org/abs/1511.00725  | author:Wajdi Dhifli, Abdoulaye Baniré Diallo category:cs.LG cs.AI cs.DB cs.IR published:2015-11-02 summary:Classification is a fundamental task in machine learning and artificialintelligence. Existing classification methods are designed to classify unknowninstances within a set of previously known classes that are seen in training.Such classification takes the form of prediction within a closed-set. However,a more realistic scenario that fits the ground truth of real world applicationsis to consider the possibility of encountering instances that do not belong toany of the classes that are seen in training, $i.e.$, an open-setclassification. In such situation, existing closed-set classification methodswill assign a training label to these instances resulting in amisclassification. In this paper, we introduce Galaxy-X, a novel multi-classclassification method for open-set problem. For each class of the training set,Galaxy-X creates a minimum bounding hyper-sphere that encompasses thedistribution of the class by enclosing all of its instances. In such manner,our method is able to distinguish instances resembling previously seen classesfrom those that are of unseen classes. To adequately evaluate open-setclassification, we introduce a novel evaluation procedure. Experimental resultson benchmark datasets as well as on synthetic datasets show the efficiency ofour approach in classifying novel instances from known as well as unknownclasses.
arxiv-1511-00472 | Water Detection through Spatio-Temporal Invariant Descriptors |  http://arxiv.org/abs/1511.00472  | author:Pascal Mettes, Robby T. Tan, Remco C. Veltkamp category:cs.CV published:2015-11-02 summary:In this work, we aim to segment and detect water in videos. Water detectionis beneficial for appllications such as video search, outdoor surveillance, andsystems such as unmanned ground vehicles and unmanned aerial vehicles. Thespecific problem, however, is less discussed compared to general texturerecognition. Here, we analyze several motion properties of water. First, wedescribe a video pre-processing step, to increase invariance against waterreflections and water colours. Second, we investigate the temporal and spatialproperties of water and derive corresponding local descriptors. The descriptorsare used to locally classify the presence of water and a binary water detectionmask is generated through spatio-temporal Markov Random Field regularization ofthe local classifications. Third, we introduce the Video Water Database,containing several hours of water and non-water videos, to validate ouralgorithm. Experimental evaluation on the Video Water Database and the DynTexdatabase indicates the effectiveness of the proposed algorithm, outperformingmultiple algorithms for dynamic texture recognition and material recognition byca. 5% and 15% respectively.
arxiv-1511-00513 | Pixel-wise Segmentation of Street with Neural Networks |  http://arxiv.org/abs/1511.00513  | author:Sebastian Bittel, Vitali Kaiser, Marvin Teichmann, Martin Thoma category:cs.CV published:2015-11-02 summary:Pixel-wise street segmentation of photographs taken from a driversperspective is important for self-driving cars and can also support otherobject recognition tasks. A framework called SST was developed to examine theaccuracy and execution time of different neural networks. The best neuralnetwork achieved an $F_1$-score of 89.5% with a simple feedforward neuralnetwork which trained to solve a regression task.
arxiv-1511-00394 | Submodular Functions: from Discrete to Continous Domains |  http://arxiv.org/abs/1511.00394  | author:Francis Bach category:cs.LG math.OC published:2015-11-02 summary:Submodular set-functions have many applications in combinatorialoptimization, as they can be minimized and approximately maximized inpolynomial time. A key element in many of the algorithms and analyses is thepossibility of extending the submodular set-function to a convex function,which opens up tools from convex optimization. Submodularity goes beyondset-functions and has naturally been considered for problems with multiplelabels or for functions defined on continuous domains, where it correspondsessentially to cross second-derivatives being nonpositive. In this paper, weshow that most results relating submodularity and convexity for set-functionscan be extended to all submodular functions. In particular, (a) we naturallydefine a continuous extension in a set of probability measures, (b) show thatthe extension is convex if and only if the original function is submodular, (c)prove that the problem of minimizing a submodular function is equivalent to atypically non-smooth convex optimization problem, and (d) propose anotherconvex optimization problem with better computational properties (e.g., asmooth dual problem). Most of these extensions from the set-function situationare obtained by drawing links with the theory of multi-marginal optimaltransport, which provides also a new interpretation of existing results forset-functions. We then provide practical algorithms to minimize genericsubmodular functions on discrete domains, with associated convergence rates.
arxiv-1511-00352 | Spatial Semantic Scan: Detecting Subtle, Spatially Localized Events in Text Streams |  http://arxiv.org/abs/1511.00352  | author:Abhinav Maurya category:cs.LG cs.CL stat.ML published:2015-11-02 summary:Many methods have been proposed for detecting emerging events in text streamsusing topic modeling. However, these methods have shortcomings that make themunsuitable for rapid detection of locally emerging events on massive textstreams. We describe Spatially Compact Semantic Scan (SCSS) that has beendeveloped specifically to overcome the shortcomings of current methods indetecting new spatially compact events in text streams. SCSS employsalternating optimization between using semantic scan to estimate contrastiveforeground topics in documents, and discovering spatial neighborhoods with highoccurrence of documents containing the foreground topics. We evaluate ourmethod on Emergency Department chief complaints dataset (ED dataset) to verifythe effectiveness of our method in detecting real-world disease outbreaks fromfree-text ED chief complaint data.
arxiv-1511-00736 | ProtNN: Fast and Accurate Nearest Neighbor Protein Function Prediction based on Graph Embedding in Structural and Topological Space |  http://arxiv.org/abs/1511.00736  | author:Wajdi Dhifli, Abdoulaye Baniré Diallo category:cs.LG cs.SI published:2015-11-02 summary:Studying the function of proteins is important for understanding themolecular mechanisms of life. The number of publicly available proteinstructures has increasingly become extremely large. Still, the determination ofthe function of a protein structure remains a difficult, costly, and timeconsuming task. The difficulties are often due to the essential role of spatialand topological structures in the determination of protein functions in livingcells. In this paper, we propose ProtNN, a novel approach for protein functionprediction. Given an unannotated protein structure and a set of annotatedproteins, ProtNN finds the nearest neighbor annotated structures based onprotein-graph pairwise similarities. Given a query protein, ProtNN finds thenearest neighbor reference proteins based on a graph representation model and apairwise similarity between vector embedding of both query and referenceprotein-graphs in structural and topological spaces. ProtNN assigns to thequery protein the function with the highest number of votes across the set of knearest neighbor reference proteins, where k is a user-defined parameter.Experimental evaluation demonstrates that ProtNN is able to accurately classifyseveral datasets in an extremely fast runtime compared to state-of-the-artapproaches. We further show that ProtNN is able to scale up to a whole PDBdataset in a single-process mode with no parallelization, with a gain ofthousands order of magnitude of runtime compared to state-of-the-artapproaches.
arxiv-1511-00438 | Semantic Summarization of Egocentric Photo Stream Events |  http://arxiv.org/abs/1511.00438  | author:Aniol Lidon, Marc Bolaños, Mariella Dimiccoli, Petia Radeva, Maite Garolera, Xavier Giró-i-Nieto category:cs.CV published:2015-11-02 summary:With the rapid increase of users of wearable cameras in recent years and ofthe amount of data they produce, there is a strong need for automatic retrievaland summarization techniques. This work addresses the problem of automaticallysummarizing egocentric photo streams captured through a wearable camera bytaking an image retrieval perspective. After removing non-informative images bya new CNN-based filter, images are ranked by relevance to ensure semanticdiversity and finally re-ranked by a novelty criterion to reduce redundancy. Toassess the results, a new evaluation metric is proposed which takes intoaccount the non-uniqueness of the solution. Experimental results applied on adatabase of 7,110 images from 6 different subjects and evaluated by expertsgave 95.74% of experts satisfaction and a Mean Opinion Score of 4.57 out of5.0.
arxiv-1511-00540 | Spiking Analog VLSI Neuron Assemblies as Constraint Satisfaction Problem Solvers |  http://arxiv.org/abs/1511.00540  | author:Jonathan Binas, Giacomo Indiveri, Michael Pfeiffer category:cs.NE published:2015-11-02 summary:Solving constraint satisfaction problems (CSPs) is a notoriously expensivecomputational task. Recently, it has been proposed that efficient stochasticsolvers can be obtained through appropriately configured spiking neuralnetworks performing Markov Chain Monte Carlo (MCMC) sampling. The possibilityto run such models on massively parallel, low-power neuromorphic hardware holdsgreat promise; however, previously proposed networks are based onprobabilistically spiking neurons, and thus rely on random number generators orexternal noise sources to achieve the necessary stochasticity, leading tosignificant overhead in the implementation. Here we show how stochasticity canbe achieved by implementing deterministic models of integrate and fire neuronsusing subthreshold analog circuits that are affected by thermal noise. Wepresent an efficient implementation of spike-based CSP solvers using areconfigurable neural network VLSI device, and the device's intrinsic noise asa source of randomness. To illustrate the overall concept, we implement ageneric Sudoku solver based on our approach and demonstrate its operation. Weestablish a link between the neuron parameters and the system dynamics,allowing for a simple temperature control mechanism.
arxiv-1511-00461 | Circle detection using isosceles triangles sampling |  http://arxiv.org/abs/1511.00461  | author:Hanqing Zhang, Krister Wiklund, Magnus Andersson category:cs.CV I.5.4 published:2015-11-02 summary:Detection of circular objects in digital images is an important problem inseveral vision applications. Circle detection using randomized sampling hasbeen developed in recent years to reduce the computational intensity.Randomized sampling, however, is sensitive to noise that can lead to reducedaccuracy and false-positive candidates. This paper presents a new circledetection method based upon randomized isosceles triangles sampling to improvethe robustness of randomized circle detection in noisy conditions. It is shownthat the geometrical property of isosceles triangles provide a robust criterionto find relevant edge pixels and thereby efficiently provide an estimation ofthe circle center and radii. The estimated results given by the isoscelestriangles sampling from each connected component of edge map were analyzedusing a simple clustering approach for efficiency. To further improve on theaccuracy we applied a two-step refinement process using chords and linear errorcompensation with gradient information of the edge pixels. Extensiveexperiments using both synthetic and real images were presented and resultswere compared to leading state-of-the-art algorithms and showed that theproposed algorithm: are efficient in finding circles with a low number ofiterations; has high rejection rate of false-positive circle candidates; andhas high robustness against noise, making it adaptive and useful in many visionapplications.
arxiv-1511-00740 | Learning Unfair Trading: a Market Manipulation Analysis From the Reinforcement Learning Perspective |  http://arxiv.org/abs/1511.00740  | author:Enrique Martínez-Miranda, Peter McBurney, Matthew J. Howard category:q-fin.TR cs.LG published:2015-11-02 summary:Market manipulation is a strategy used by traders to alter the price offinancial securities. One type of manipulation is based on the process ofbuying or selling assets by using several trading strategies, among themspoofing is a popular strategy and is considered illegal by market regulators.Some promising tools have been developed to detect manipulation, but cases canstill be found in the markets. In this paper we model spoofing and pingingtrading, two strategies that differ in the legal background but share the sameelemental concept of market manipulation. We use a reinforcement learningframework within the full and partial observability of Markov decisionprocesses and analyse the underlying behaviour of the manipulators by findingthe causes of what encourages the traders to perform fraudulent activities.This reveals procedures to counter the problem that may be helpful to marketregulators as our model predicts the activity of spoofers.
arxiv-1511-00423 | Reading Hidden Emotions: Spontaneous Micro-expression Spotting and Recognition |  http://arxiv.org/abs/1511.00423  | author:Xiaobai Li, Xiaopeng Hong, Antti Moilanen, Xiaohua Huang, Tomas Pfister, Guoying Zhao, Matti Pietikäinen category:cs.CV published:2015-11-02 summary:Micro-expressions (MEs) are rapid, involuntary facial expressions whichreveal emotions that people do not intend to show. Studying MEs is valuable asrecognizing them has many important applications, particularly in forensicscience and psychotherapy. However, analyzing spontaneous MEs is verychallenging due to their short duration and low intensity. Automatic MEanalysis includes two tasks: ME spotting and ME recognition.For ME spotting,previous studies have focused on posed rather than spontaneous videos. For MErecognition, the performance of previous studies is low. To address thesechallenges, we make the following contributions: (i) We propose the firstmethod for spotting spontaneous MEs in long videos (by exploiting featuredifference contrast). This method is training free and works on arbitraryunseen videos. (ii) We present an advanced ME recognition framework, whichoutperforms previous work by a large margin on two challenging spontaneous MEdatabases (SMIC and CASMEII). (iii) We propose the first automatic ME analysissystem (MESR), which can spot and recognize MEs from spontaneous video data.Finally, we show that our method achieves comparable performance to humans atthis very challenging task, and outperforms humans in the ME recognition taskby a large margin.
arxiv-1511-00561 | SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation |  http://arxiv.org/abs/1511.00561  | author:Vijay Badrinarayanan, Alex Kendall, Roberto Cipolla category:cs.CV cs.LG cs.NE published:2015-11-02 summary:We present a novel and practical deep fully convolutional neural networkarchitecture for semantic pixel-wise segmentation termed SegNet. This coretrainable segmentation engine consists of an encoder network, a correspondingdecoder network followed by a pixel-wise classification layer. The architectureof the encoder network is topologically identical to the 13 convolutionallayers in the VGG16 network . The role of the decoder network is to map the lowresolution encoder feature maps to full input resolution feature maps forpixel-wise classification. The novelty of SegNet lies is in the manner in whichthe decoder upsamples its lower resolution input feature map(s). Specifically,the decoder uses pooling indices computed in the max-pooling step of thecorresponding encoder to perform non-linear upsampling. This eliminates theneed for learning to upsample. The upsampled maps are sparse and are thenconvolved with trainable filters to produce dense feature maps. We compare ourproposed architecture with the fully convolutional network (FCN) architectureand its variants. This comparison reveals the memory versus accuracy trade-offinvolved in achieving good segmentation performance. The design of SegNet wasprimarily motivated by road scene understanding applications. Hence, it isefficient both in terms of memory and computational time during inference. Itis also significantly smaller in the number of trainable parameters thancompeting architectures and can be trained end-to-end using stochastic gradientdescent. We also benchmark the performance of SegNet on Pascal VOC12 salientobject segmentation and the recent SUN RGB-D indoor scene understandingchallenge. We show that SegNet provides competitive performance although it issignificantly smaller than other architectures. We also provide a Caffeimplementation of SegNet and a webdemo athttp://mi.eng.cam.ac.uk/projects/segnet/
arxiv-1511-00363 | BinaryConnect: Training Deep Neural Networks with binary weights during propagations |  http://arxiv.org/abs/1511.00363  | author:Matthieu Courbariaux, Yoshua Bengio, Jean-Pierre David category:cs.LG cs.CV cs.NE published:2015-11-02 summary:Deep Neural Networks (DNN) have achieved state-of-the-art results in a widerange of tasks, with the best results obtained with large training sets andlarge models. In the past, GPUs enabled these breakthroughs because of theirgreater computational speed. In the future, faster computation at both trainingand test time is likely to be crucial for further progress and for consumerapplications on low-power devices. As a result, there is much interest inresearch and development of dedicated hardware for Deep Learning (DL). Binaryweights, i.e., weights which are constrained to only two possible values (e.g.-1 or 1), would bring great benefits to specialized DL hardware by replacingmany multiply-accumulate operations by simple accumulations, as multipliers arethe most space and power-hungry components of the digital implementation ofneural networks. We introduce BinaryConnect, a method which consists intraining a DNN with binary weights during the forward and backwardpropagations, while retaining precision of the stored weights in whichgradients are accumulated. Like other dropout schemes, we show thatBinaryConnect acts as regularizer and we obtain near state-of-the-art resultswith BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.
arxiv-1511-00221 | LM-CMA: an Alternative to L-BFGS for Large Scale Black-box Optimization |  http://arxiv.org/abs/1511.00221  | author:Ilya Loshchilov category:cs.NE math.OC published:2015-11-01 summary:The limited memory BFGS method (L-BFGS) of Liu and Nocedal (1989) is oftenconsidered to be the method of choice for continuous optimization when first-and/or second- order information is available. However, the use of L-BFGS canbe complicated in a black-box scenario where gradient information is notavailable and therefore should be numerically estimated. The accuracy of thisestimation, obtained by finite difference methods, is often problem-dependentthat may lead to premature convergence of the algorithm. In this paper, we demonstrate an alternative to L-BFGS, the limited memoryCovariance Matrix Adaptation Evolution Strategy (LM-CMA) proposed by Loshchilov(2014). The LM-CMA is a stochastic derivative-free algorithm for numericaloptimization of non-linear, non-convex optimization problems. Inspired by theL-BFGS, the LM-CMA samples candidate solutions according to a covariance matrixreproduced from $m$ direction vectors selected during the optimization process.The decomposition of the covariance matrix into Cholesky factors allows toreduce the memory complexity to $O(mn)$, where $n$ is the number of decisionvariables. The time complexity of sampling one candidate solution is also$O(mn)$, but scales as only about 25 scalar-vector multiplications in practice.The algorithm has an important property of invariance w.r.t. strictlyincreasing transformations of the objective function, such transformations donot compromise its ability to approach the optimum. The LM-CMA outperforms theoriginal CMA-ES and its large scale versions on non-separable ill-conditionedproblems with a factor increasing with problem dimension. Invariance propertiesof the algorithm do not prevent it from demonstrating a comparable performanceto L-BFGS on non-trivial large scale smooth and nonsmooth optimizationproblems.
arxiv-1511-00271 | Stochastic Top-k ListNet |  http://arxiv.org/abs/1511.00271  | author:Tianyi Luo, Dong Wang, Rong Liu, Yiqiao Pan category:cs.IR cs.LG published:2015-11-01 summary:ListNet is a well-known listwise learning to rank model and has gained muchattention in recent years. A particular problem of ListNet, however, is thehigh computation complexity in model training, mainly due to the large numberof object permutations involved in computing the gradients. This paper proposesa stochastic ListNet approach which computes the gradient within a boundedpermutation subset. It significantly reduces the computation complexity ofmodel training and allows extension to Top-k models, which is impossible withthe conventional implementation based on full-set permutations. Meanwhile, thenew approach utilizes partial ranking information of human labels, which helpsimprove model quality. Our experiments demonstrated that the stochastic ListNetmethod indeed leads to better ranking performance and speeds up the modeltraining remarkably.
arxiv-1511-00215 | A Unified Tagging Solution: Bidirectional LSTM Recurrent Neural Network with Word Embedding |  http://arxiv.org/abs/1511.00215  | author:Peilu Wang, Yao Qian, Frank K. Soong, Lei He, Hai Zhao category:cs.CL published:2015-11-01 summary:Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTM-RNN) hasbeen shown to be very effective for modeling and predicting sequential data,e.g. speech utterances or handwritten documents. In this study, we propose touse BLSTM-RNN for a unified tagging solution that can be applied to varioustagging tasks including part-of-speech tagging, chunking and named entityrecognition. Instead of exploiting specific features carefully optimized foreach task, our solution only uses one set of task-independent features andinternal representations learnt from unlabeled text for all tasks.Requiring notask specific knowledge or sophisticated feature engineering, our approach getsnearly state-of-the-art performance in all these three tagging tasks.
arxiv-1511-00296 | Limiting fitness distributions in evolutionary dynamics |  http://arxiv.org/abs/1511.00296  | author:Matteo Smerlak, Ahmed Youssef category:q-bio.PE cs.NE published:2015-11-01 summary:Darwinian evolution can be modeled in general terms as a flow in the space offitness (i.e. reproductive rate) distributions. In the diffusion approximation,Tsimring et al. have showed that this flow admits "fitness wave" solutions:Gaussian-shape fitness distributions moving towards higher fitness values atconstant speed. Here we show more generally that evolving fitness distributionsare attracted to a one-parameter family of distributions with a fixed parabolicrelationship between skewness and kurtosis. Unlike fitness waves, thisstatistical pattern encompasses both positive and negative (a.k.a. purifying)selection and is not restricted to rapidly adapting populations. Moreover wefind that the mean fitness of a population under the selection of pre-existingvariation is a power-law function of time, as observed in microbiologicalevolution experiments but at variance with fitness wave theory. At theconceptual level, our results can be viewed as the resolution of the "dynamicinsufficiency" of Fisher's fundamental theorem of natural selection. Ourpredictions are in good agreement with numerical simulations.
arxiv-1511-00213 | Large-scale probabilistic predictors with and without guarantees of validity |  http://arxiv.org/abs/1511.00213  | author:Vladimir Vovk, Ivan Petej, Valentina Fedorova category:cs.LG 68T05 published:2015-11-01 summary:This paper studies theoretically and empirically a method of turningmachine-learning algorithms into probabilistic predictors that automaticallyenjoys a property of validity (perfect calibration) and is computationallyefficient. The price to pay for perfect calibration is that these probabilisticpredictors produce imprecise (in practice, almost precise for large data sets)probabilities. When these imprecise probabilities are merged into preciseprobabilities, the resulting predictors, while losing the theoretical propertyof perfect calibration, are consistently more accurate than the existingmethods in empirical studies.
arxiv-1511-00060 | Top-down Tree Long Short-Term Memory Networks |  http://arxiv.org/abs/1511.00060  | author:Xingxing Zhang, Liang Lu, Mirella Lapata category:cs.CL cs.LG published:2015-10-31 summary:Long Short-Term Memory (LSTM) networks, a type of recurrent neural networkwith a more complex computational unit, have been successfully applied to avariety of sequence modeling tasks. In this paper we develop Tree LongShort-Term Memory (TreeLSTM), a neural network model based on LSTM, which isdesigned to predict a tree rather than a linear sequence. TreeLSTM defines theprobability of a sentence by estimating the generation probability of itsdependency tree. At each time step, a node is generated based on therepresentation of the generated sub-tree. We further enhance the modeling powerof TreeLSTM by explicitly representing the correlations between left and rightdependents. Application of our model to the MSR sentence completion challengeachieves results beyond the current state of the art. We also report results ondependency parsing reranking achieving competitive performance.
arxiv-1511-00152 | Preconditioned Data Sparsification for Big Data with Applications to PCA and K-means |  http://arxiv.org/abs/1511.00152  | author:Farhad Pourkamali-Anaraki, Stephen Becker category:stat.ML cs.LG published:2015-10-31 summary:We analyze a compression scheme for large data sets that randomly keeps asmall percentage of the components of each data sample. The benefit is that theoutput is a sparse matrix and therefore subsequent processing, such as PCA orK-means, is significantly faster, especially in a distributed-data setting.Furthermore, the sampling is single-pass and applicable to streaming data. Thesampling mechanism is a variant of previous methods proposed in the literaturecombined with a randomized preconditioning to smooth the data. We provideguarantees for PCA in terms of the covariance matrix, and guarantees forK-means in terms of the error in the center estimators at a given step. Wepresent numerical evidence to show both that our bounds are nearly tight andthat our algorithms provide a real benefit when applied to standard test datasets, as well as providing certain benefits over related sampling approaches.
arxiv-1511-01064 | Color Space Transformation Network |  http://arxiv.org/abs/1511.01064  | author:Alexandros Karargyris category:cs.CV published:2015-10-31 summary:Deep networks have become very popular over the past few years. The mainreason for this widespread use is their excellent ability to learn and predictknowledge in a very easy and efficient way. Convolutional neural networks andauto-encoders have become the normal in the area of imaging and computer visionachieving unprecedented accuracy levels in many applications. The most commonstrategy is to build and train networks with many layers by tuning theirhyper-parameters. While this approach has proven to be a successful way tobuild robust deep learning schemes it suffers from high complexity. In thispaper we introduce a module that learns color space transformations within anetwork. Given a large dataset of colored images the color space transformationmodule tries to learn color space transformations that increase overallclassification accuracy. This module has shown to increase overall accuracy forthe same network design and to achieve faster convergence. It is part of abroader family of image transformations (e.g. spatial transformer network).
arxiv-1511-00054 | Gaussian Process Random Fields |  http://arxiv.org/abs/1511.00054  | author:David A. Moore, Stuart J. Russell category:cs.LG stat.ML published:2015-10-31 summary:Gaussian processes have been successful in both supervised and unsupervisedmachine learning tasks, but their computational complexity has constrainedpractical applications. We introduce a new approximation for large-scaleGaussian processes, the Gaussian Process Random Field (GPRF), in which localGPs are coupled via pairwise potentials. The GPRF likelihood is a simple,tractable, and parallelizeable approximation to the full GP marginallikelihood, enabling latent variable modeling and hyperparameter selection onlarge datasets. We demonstrate its effectiveness on synthetic spatial data aswell as a real-world application to seismic event location.
arxiv-1511-00096 | Bioinspired Visual Motion Estimation |  http://arxiv.org/abs/1511.00096  | author:Garrick Orchard, Ralph Etienne-Cummings category:cs.CV published:2015-10-31 summary:Visual motion estimation is a computationally intensive, but important taskfor sighted animals. Replicating the robustness and efficiency of biologicalvisual motion estimation in artificial systems would significantly enhance thecapabilities of future robotic agents. 25 years ago, in this very journal,Carver Mead outlined his argument for replicating biological processing insilicon circuits. His vision served as the foundation for the field ofneuromorphic engineering, which has experienced a rapid growth in interest overrecent years as the ideas and technologies mature. Replicating biologicalvisual sensing was one of the first tasks attempted in the neuromorphic field.In this paper we focus specifically on the task of visual motion estimation. Wedescribe the task itself, present the progression of works from the early firstattempts through to the modern day state-of-the-art, and provide an outlook forfuture directions in the field.
arxiv-1511-00158 | Support Vector Regression, Smooth Splines, and Time Series Prediction |  http://arxiv.org/abs/1511.00158  | author:Raymundo Navarrete, Divakar Viswanath category:stat.ML cs.LG published:2015-10-31 summary:Delay coordinates and support vector regression are among the techniquescommonly used for time series prediction. We show that the combination of thesetwo techniques leads to systematic error that obstructs convergence. Apreliminary step of spline smoothing restores convergence and leads topredictions that are consistently more accurate, typically by about a factor of$2$ or so. Since the algorithm without spline smoothing is not convergent, theimprovement in accuracy can even be as high as a factor of $100$. Assuminglocal isotropy, the systematic error in the absence of spline smoothing isestimated to be $d\sigma^{2}L/2$, where $d$ is the embedding dimension,$\sigma^{2}$ is the variance of Gaussian noise in the signal, and $L$ is aglobal bound on the Hessian of the exact predictor. The smooth spline, althoughvery effective, is shown not to have even first order accuracy, unless thenoise is unusually mild. The lack of order of accuracy implies that attempts totake advantage of invariance in time to enhance fidelity of learning areunlikely to be successful.
arxiv-1511-00098 | Semantic Cross-View Matching |  http://arxiv.org/abs/1511.00098  | author:Francesco Castaldo, Amir Zamir, Roland Angst, Francesco Palmieri, Silvio Savarese category:cs.CV published:2015-10-31 summary:Matching cross-view images is challenging because the appearance andviewpoints are significantly different. While low-level features based ongradient orientations or filter responses can drastically vary with suchchanges in viewpoint, semantic information of images however shows an invariantcharacteristic in this respect. Consequently, semantically labeled regions canbe used for performing cross-view matching. In this paper, we therefore explorethis idea and propose an automatic method for detecting and representing thesemantic information of an RGB image with the goal of performing cross-viewmatching with a (non-RGB) geographic information system (GIS). A segmentedimage forms the input to our system with segments assigned to semantic conceptssuch as traffic signs, lakes, roads, foliage, etc. We design a descriptor torobustly capture both, the presence of semantic concepts and the spatial layoutof those segments. Pairwise distances between the descriptors extracted fromthe GIS map and the query image are then used to generate a shortlist of themost promising locations with similar semantic concepts in a consistent spatiallayout. An experimental evaluation with challenging query images and a largeurban area shows promising results.
arxiv-1511-00146 | Convergence of Proximal-Gradient Stochastic Variational Inference under Non-Decreasing Step-Size Sequence |  http://arxiv.org/abs/1511.00146  | author:Mohammad Emtiyaz Khan, Reza Babanezhad, Wu Lin, Mark Schmidt, Masashi Sugiyama category:stat.ML cs.LG stat.CO published:2015-10-31 summary:Stochastic approximation methods have recently gained popularity forvariational inference, but many existing approaches treat them as "black-box"tools. Thus, they often do not take advantage of the geometry of the posteriorand usually require a decreasing sequence of step-sizes (which converges slowlyin practice). We introduce a new stochastic-approximation method that uses aproximal-gradient framework. Our method exploits the geometry and structure ofthe variational lower bound, and contains many existing methods, such asstochastic variational inference, as a special case. We establish theconvergence of our method under a "non-decreasing" step-size schedule, whichhas both theoretical and practical advantages. We consider setting thestep-size based on the continuity of the objective and the geometry of theposterior, and show that our method gives a faster rate of convergence forvariational-Gaussian inference than existing stochastic methods.
arxiv-1511-00111 | Regional Active Contours based on Variational level sets and Machine Learning for Image Segmentation |  http://arxiv.org/abs/1511.00111  | author:M. Abdelsamea category:cs.CV published:2015-10-31 summary:Image segmentation is the problem of partitioning an image into differentsubsets, where each subset may have a different characterization in terms ofcolor, intensity, texture, and/or other features. Segmentation is a fundamentalcomponent of image processing, and plays a significant role in computer vision,object recognition, and object tracking. Active Contour Models (ACMs)constitute a powerful energy-based minimization framework for imagesegmentation, which relies on the concept of contour evolution. Starting froman initial guess, the contour is evolved with the aim of approximating betterand better the actual object boundary. Handling complex images in an efficient,effective, and robust way is a real challenge, especially in the presence ofintensity inhomogeneity, overlap between the foreground/background intensitydistributions, objects characterized by many different intensities, and/oradditive noise. In this thesis, to deal with these challenges, we propose anumber of image segmentation models relying on variational level set methodsand specific kinds of neural networks, to handle complex images in bothsupervised and unsupervised ways. Experimental results demonstrate the highaccuracy of the segmentation results, obtained by the proposed models onvarious benchmark synthetic and real images compared with state-of-the-artactive contour models.
arxiv-1511-00100 | Fast Neuromimetic Object Recognition using FPGA Outperforms GPU Implementations |  http://arxiv.org/abs/1511.00100  | author:Garrick Orchard, Jacob G. Martin, R. Jacob Vogelstein, Ralph Etienne-Cummings category:cs.CV published:2015-10-31 summary:Recognition of objects in still images has traditionally been regarded as adifficult computational problem. Although modern automated methods for visualobject recognition have achieved steadily increasing recognition accuracy, eventhe most advanced computational vision approaches are unable to obtainperformance equal to that of humans. This has led to the creation of manybiologically-inspired models of visual object recognition, among them the HMAXmodel. HMAX is traditionally known to achieve high accuracy in visual objectrecognition tasks at the expense of significant computational complexity.Increasing complexity, in turn, increases computation time, reducing the numberof images that can be processed per unit time. In this paper we describe howthe computationally intensive, biologically inspired HMAX model for visualobject recognition can be modified for implementation on a commercial FieldProgrammable Gate Array, specifically the Xilinx Virtex 6 ML605 evaluationboard with XC6VLX240T FPGA. We show that with minor modifications to thetraditional HMAX model we can perform recognition on images of size 128x128pixels at a rate of 190 images per second with a less than 1% loss inrecognition accuracy in both binary and multi-class visual object recognitiontasks.
arxiv-1511-00099 | Sketch-based Image Retrieval from Millions of Images under Rotation, Translation and Scale Variations |  http://arxiv.org/abs/1511.00099  | author:Sarthak Parui, Anurag Mittal category:cs.CV cs.IR published:2015-10-31 summary:Proliferation of touch-based devices has made sketch-based image retrievalpractical. While many methods exist for sketch-based object detection/imageretrieval on small datasets, relatively less work has been done on large(web)-scale image retrieval. In this paper, we present an efficient approachfor image retrieval from millions of images based on user-drawn sketches.Unlike existing methods for this problem which are sensitive to eventranslation or scale variations, our method handles rotation, translation,scale (i.e. a similarity transformation) and small deformations. The objectboundaries are represented as chains of connected segments and the databaseimages are pre-processed to obtain such chains that have a high chance ofcontaining the object. This is accomplished using two approaches in this work:a) extracting long chains in contour segment networks and b) extractingboundaries of segmented object proposals. These chains are then represented bysimilarity-invariant variable length descriptors. Descriptor similarities arecomputed by a fast Dynamic Programming-based partial matching algorithm. Thismatching mechanism is used to generate a hierarchical k-medoids based indexingstructure for the extracted chains of all database images in an offline processwhich is used to efficiently retrieve a small set of possible matched imagesfor query chains. Finally, a geometric verification step is employed to testgeometric consistency of multiple chain matches to improve results. Qualitativeand quantitative results clearly demonstrate superiority of the approach overexisting methods.
arxiv-1511-00175 | FireCaffe: near-linear acceleration of deep neural network training on compute clusters |  http://arxiv.org/abs/1511.00175  | author:Forrest N. Iandola, Khalid Ashraf, Matthew W. Moskewicz, Kurt Keutzer category:cs.CV published:2015-10-31 summary:Long training times for high-accuracy deep neural networks (DNNs) impederesearch into new DNN architectures and slow the development of high-accuracyDNNs. In this paper we present FireCaffe, which successfully scales deep neuralnetwork training across a cluster of GPUs. We also present a number of bestpractices to aid in comparing advancements in methods for scaling andaccelerating the training of deep neural networks. The speed and scalability ofdistributed algorithms is almost always limited by the overhead ofcommunicating between servers; DNN training is not an exception to this rule.Therefore, the key consideration here is to reduce communication overheadwherever possible, while not degrading the accuracy of the DNN models that wetrain. Our approach has three key pillars. First, we select network hardwarethat achieves high bandwidth between GPU servers -- Infiniband or Crayinterconnects are ideal for this. Second, we consider a number of communicationalgorithms, and we find that reduction trees are more efficient and scalablethan the traditional parameter server approach. Third, we optionally increasethe batch size to reduce the total quantity of communication during DNNtraining, and we identify hyperparameters that allow us to reproduce thesmall-batch accuracy while training with large batch sizes. When trainingGoogLeNet and Network-in-Network on ImageNet, we achieve a 47x and 39x speedup,respectively, when training on a cluster of 128 GPUs.
arxiv-1510-08983 | Highway Long Short-Term Memory RNNs for Distant Speech Recognition |  http://arxiv.org/abs/1510.08983  | author:Yu Zhang, Guoguo Chen, Dong Yu, Kaisheng Yao, Sanjeev Khudanpur, James Glass category:cs.NE cs.AI cs.CL cs.LG published:2015-10-30 summary:In this paper, we extend the deep long short-term memory (DLSTM) recurrentneural networks by introducing gated direct connections between memory cells inadjacent layers. These direct links, called highway connections, enableunimpeded information flow across different layers and thus alleviate thegradient vanishing problem when building deeper LSTMs. We further introduce thelatency-controlled bidirectional LSTMs (BLSTMs) which can exploit the wholehistory while keeping the latency under control. Efficient algorithms areproposed to train these novel networks using both frame and sequencediscriminative criteria. Experiments on the AMI distant speech recognition(DSR) task indicate that we can train deeper LSTMs and achieve betterimprovement from sequence training with highway LSTMs (HLSTMs). Our novel modelobtains $43.9/47.7\%$ WER on AMI (SDM) dev and eval sets, outperforming allprevious works. It beats the strong DNN and DLSTM baselines with $15.7\%$ and$5.3\%$ relative improvement respectively.
arxiv-1510-09083 | Deep Cascaded Regression for Face Alignment |  http://arxiv.org/abs/1510.09083  | author:Hanjiang Lai, Shengtao Xiao, Zhen Cui, Yan Pan, Chunyan Xu, Shuicheng Yan category:cs.CV published:2015-10-30 summary:We propose a novel cascaded regression framework for face alignment based ona deep convolutional neural network (CNN). In most existing cascaded regressionmethods, the shape-indexed features are either obtained by hand-crafted visualdescriptors or by leaning from the shallow models. This setting may besuboptimal for the face alignment task. To solve this problem, we propose anend-to-end CNN architecture to learn highly discriminative shape-indexedfeatures. First, our deep architecture encodes the image into high-levelfeature maps in the same size of the image via three main operations:convolution, pooling and deconvolution. Then, we propose "Shape-IndexedPooling" to extract the deep features from these high level descriptors. Werefine the shape via sequential regressions by using the deep shape-indexedfeatures, which demonstrates outstanding performance. We also propose to learnthe probability mask for each landmark that can be used to choose theinitialization from the shape space. Extensive evaluations conducted on severalbenchmark datasets demonstrate that the proposed deep framework showssignificant improvement over the state-of-the-art methods.
arxiv-1511-00048 | The Pareto Regret Frontier for Bandits |  http://arxiv.org/abs/1511.00048  | author:Tor Lattimore category:cs.LG published:2015-10-30 summary:Given a multi-armed bandit problem it may be desirable to achieve asmaller-than-usual worst-case regret for some special actions. I show that theprice for such unbalanced worst-case regret guarantees is rather high.Specifically, if an algorithm enjoys a worst-case regret of B with respect tosome action, then there must exist another action for which the worst-caseregret is at least {\Omega}(nK/B), where n is the horizon and K the number ofactions. I also give upper bounds in both the stochastic and adversarialsettings showing that this result cannot be improved. For the stochastic casethe pareto regret frontier is characterised exactly up to constant factors.
arxiv-1511-00040 | Quantifying the Cognitive Extent of Science |  http://arxiv.org/abs/1511.00040  | author:Staša Milojević category:cs.DL astro-ph.IM cs.CL physics.soc-ph published:2015-10-30 summary:While the modern science is characterized by an exponential growth inscientific literature, the increase in publication volume clearly does notreflect the expansion of the cognitive boundaries of science. Nevertheless,most of the metrics for assessing the vitality of science or for making fundingand policy decisions are based on productivity. Similarly, the increasing levelof knowledge production by large science teams, whose results often enjoygreater visibility, does not necessarily mean that "big science" leads tocognitive expansion. Here we present a novel, big-data method to quantify theextents of cognitive domains of different bodies of scientific literatureindependently from publication volume, and apply it to 20 million articlespublished over 60-130 years in physics, astronomy, and biomedicine. The methodis based on the lexical diversity of titles of fixed quotas of researcharticles. Owing to large size of quotas, the method overcomes the inherentstochasticity of article titles to achieve <1% precision. We show that theperiods of cognitive growth do not necessarily coincide with the trends inpublication volume. Furthermore, we show that the articles produced by largerteams cover significantly smaller cognitive territory than (the same quota of)articles from smaller teams. Our findings provide a new perspective on the roleof small teams and individual researchers in expanding the cognitive boundariesof science. The proposed method of quantifying the extent of the cognitiveterritory can also be applied to study many other aspects of "science ofscience."
arxiv-1511-00041 | Learning Causal Graphs with Small Interventions |  http://arxiv.org/abs/1511.00041  | author:Karthikeyan Shanmugam, Murat Kocaoglu, Alexandros G. Dimakis, Sriram Vishwanath category:cs.AI cs.IT cs.LG math.IT stat.ML published:2015-10-30 summary:We consider the problem of learning causal networks with interventions, wheneach intervention is limited in size under Pearl's Structural Equation Modelwith independent errors (SEM-IE). The objective is to minimize the number ofexperiments to discover the causal directions of all the edges in a causalgraph. Previous work has focused on the use of separating systems for completegraphs for this task. We prove that any deterministic adaptive algorithm needsto be a separating system in order to learn complete graphs in the worst case.In addition, we present a novel separating system construction, whose size isclose to optimal and is arguably simpler than previous work in combinatorics.We also develop a novel information theoretic lower bound on the number ofinterventions that applies in full generality, including for randomizedadaptive learning algorithms. For general chordal graphs, we derive worst case lower bounds on the numberof interventions. Building on observations about induced trees, we give a newdeterministic adaptive algorithm to learn directions on any chordal skeletoncompletely. In the worst case, our achievable scheme is an$\alpha$-approximation algorithm where $\alpha$ is the independence number ofthe graph. We also show that there exist graph classes for which the sufficientnumber of experiments is close to the lower bound. In the other extreme, thereare graph classes for which the required number of experiments ismultiplicatively $\alpha$ away from our lower bound. In simulations, our algorithm almost always performs very close to the lowerbound, while the approach based on separating systems for complete graphs issignificantly worse for random chordal graphs.
arxiv-1510-09219 | Submatrix localization via message passing |  http://arxiv.org/abs/1510.09219  | author:Bruce Hajek, Yihong Wu, Jiaming Xu category:stat.ML cs.IT cs.SI math.IT math.PR math.ST stat.TH published:2015-10-30 summary:The principal submatrix localization problem deals with recovering a $K\timesK$ principal submatrix of elevated mean $\mu$ in a large $n\times n$ symmetricmatrix subject to additive standard Gaussian noise. This problem serves as aprototypical example for community detection, in which the communitycorresponds to the support of the submatrix. The main result of this paper isthat in the regime $\Omega(\sqrt{n}) \leq K \leq o(n)$, the support of thesubmatrix can be weakly recovered (with $o(K)$ misclassification errors onaverage) by an optimized message passing algorithm if $\lambda = \mu^2K^2/n$,the signal-to-noise ratio, exceeds $1/e$. This extends a result by Deshpandeand Montanari previously obtained for $K=\Theta(\sqrt{n}).$ In addition, thealgorithm can be extended to provide exact recovery wheneverinformation-theoretically possible and achieve the information limit of exactrecovery as long as $K \geq \frac{n}{\log n} (\frac{1}{8e} + o(1))$. The totalrunning time of the algorithm is $O(n^2\log n)$. Another version of the submatrix localization problem, known as noisybiclustering, aims to recover a $K_1\times K_2$ submatrix of elevated mean$\mu$ in a large $n_1\times n_2$ Gaussian matrix. The optimized message passingalgorithm and its analysis are adapted to the bicluster problem assuming$\Omega(\sqrt{n_i}) \leq K_i \leq o(n_i)$ and $K_1\asymp K_2.$ A sharpinformation-theoretic condition for the weak recovery of both clusters is alsoidentified.
arxiv-1510-09202 | Generating Text with Deep Reinforcement Learning |  http://arxiv.org/abs/1510.09202  | author:Hongyu Guo category:cs.CL cs.LG cs.NE published:2015-10-30 summary:We introduce a novel schema for sequence to sequence learning with a DeepQ-Network (DQN), which decodes the output sequence iteratively. The aim here isto enable the decoder to first tackle easier portions of the sequences, andthen turn to cope with difficult parts. Specifically, in each iteration, anencoder-decoder Long Short-Term Memory (LSTM) network is employed to, from theinput sequence, automatically create features to represent the internal statesof and formulate a list of potential actions for the DQN. Take rephrasing anatural sentence as an example. This list can contain ranked potential words.Next, the DQN learns to make decision on which action (e.g., word) will beselected from the list to modify the current decoded sequence. The newlymodified output sequence is subsequently used as the input to the DQN for thenext decoding iteration. In each iteration, we also bias the reinforcementlearning's attention to explore sequence portions which are previouslydifficult to be decoded. For evaluation, the proposed strategy was trained todecode ten thousands natural sentences. Our experiments indicate that, whencompared to a left-to-right greedy beam search LSTM decoder, the proposedmethod performed competitively well when decoding sentences from the trainingset, but significantly outperformed the baseline when decoding unseensentences, in terms of BLEU score obtained.
arxiv-1510-08949 | Testing Visual Attention in Dynamic Environments |  http://arxiv.org/abs/1510.08949  | author:Philip Bachman, David Krueger, Doina Precup category:cs.LG published:2015-10-30 summary:We investigate attention as the active pursuit of useful information. Thiscontrasts with attention as a mechanism for the attenuation of irrelevantinformation. We also consider the role of short-term memory, whose use iscritical to any model incapable of simultaneously perceiving all information onwhich its output depends. We present several simple synthetic tasks, whichbecome considerably more interesting when we impose strong constraints on how amodel can interact with its input, and on how long it can take to produce itsoutput. We develop a model with a different structure from those seen inprevious work, and we train it using stochastic variational inference with alearned proposal distribution.
arxiv-1510-08956 | Principal Differences Analysis: Interpretable Characterization of Differences between Distributions |  http://arxiv.org/abs/1510.08956  | author:Jonas Mueller, Tommi Jaakkola category:stat.ML cs.LG stat.ME published:2015-10-30 summary:We introduce principal differences analysis (PDA) for analyzing differencesbetween high-dimensional distributions. The method operates by finding theprojection that maximizes the Wasserstein divergence between the resultingunivariate populations. Relying on the Cramer-Wold device, it requires noassumptions about the form of the underlying distributions, nor the nature oftheir inter-class differences. A sparse variant of the method is introduced toidentify features responsible for the differences. We provide algorithms forboth the original minimax formulation as well as its semidefinite relaxation.In addition to deriving some convergence results, we illustrate how theapproach may be applied to identify differences between cell populations in thesomatosensory cortex and hippocampus as manifested by single cell RNA-seq. Ourbroader framework extends beyond the specific choice of Wasserstein divergence.
arxiv-1510-08971 | Robust Subspace Clustering via Tighter Rank Approximation |  http://arxiv.org/abs/1510.08971  | author:Zhao Kang, Chong Peng, Qiang Cheng category:cs.CV cs.AI cs.LG stat.ML published:2015-10-30 summary:Matrix rank minimization problem is in general NP-hard. The nuclear norm isused to substitute the rank function in many recent studies. Nevertheless, thenuclear norm approximation adds all singular values together and theapproximation error may depend heavily on the magnitudes of singular values.This might restrict its capability in dealing with many practical problems. Inthis paper, an arctangent function is used as a tighter approximation to therank function. We use it on the challenging subspace clustering problem. Forthis nonconvex minimization problem, we develop an effective optimizationprocedure based on a type of augmented Lagrange multipliers (ALM) method.Extensive experiments on face clustering and motion segmentation show that theproposed method is effective for rank approximation.
arxiv-1510-08973 | VISALOGY: Answering Visual Analogy Questions |  http://arxiv.org/abs/1510.08973  | author:Fereshteh Sadeghi, C. Lawrence Zitnick, Ali Farhadi category:cs.CV published:2015-10-30 summary:In this paper, we study the problem of answering visual analogy questions.These questions take the form of image A is to image B as image C is to what.Answering these questions entails discovering the mapping from image A to imageB and then extending the mapping to image C and searching for the image D suchthat the relation from A to B holds for C to D. We pose this problem aslearning an embedding that encourages pairs of analogous images with similartransformations to be close together using convolutional neural networks with aquadruple Siamese architecture. We introduce a dataset of visual analogyquestions in natural images, and show first results of its kind on solvinganalogy questions on natural images.
arxiv-1510-08974 | CONQUER: Confusion Queried Online Bandit Learning |  http://arxiv.org/abs/1510.08974  | author:Daniel Barsky, Koby Crammer category:cs.LG stat.ML published:2015-10-30 summary:We present a new recommendation setting for picking out two items from agiven set to be highlighted to a user, based on contextual input. These twoitems are presented to a user who chooses one of them, possibly stochastically,with a bias that favours the item with the higher value. We propose asecond-order algorithm framework that members of it use uses relativeupper-confidence bounds to trade off exploration and exploitation, and someexplore via sampling. We analyze one algorithm in this framework in anadversarial setting with only mild assumption on the data, and prove a regretbound of $O(Q_T + \sqrt{TQ_T\log T} + \sqrt{T}\log T)$, where $T$ is the numberof rounds and $Q_T$ is the cumulative approximation error of item values usinga linear model. Experiments with product reviews from 33 domains show theadvantage of our methods over algorithms designed for related settings, andthat UCB based algorithms are inferior to greed or sampling based algorithms.
arxiv-1510-08985 | Prediction-Adaptation-Correction Recurrent Neural Networks for Low-Resource Language Speech Recognition |  http://arxiv.org/abs/1510.08985  | author:Yu Zhang, Ekapol Chuangsuwanich, James Glass, Dong Yu category:cs.CL cs.LG cs.NE published:2015-10-30 summary:In this paper, we investigate the use of prediction-adaptation-correctionrecurrent neural networks (PAC-RNNs) for low-resource speech recognition. APAC-RNN is comprised of a pair of neural networks in which a {\it correction}network uses auxiliary information given by a {\it prediction} network to helpestimate the state probability. The information from the correction network isalso used by the prediction network in a recurrent loop. Our model outperformsother state-of-the-art neural networks (DNNs, LSTMs) on IARPA-Babel tasks.Moreover, transfer learning from a language that is similar to the targetlanguage can help improve performance further.
arxiv-1510-08986 | A Unified Theory of Confidence Regions and Testing for High Dimensional Estimating Equations |  http://arxiv.org/abs/1510.08986  | author:Matey Neykov, Yang Ning, Jun S. Liu, Han Liu category:math.ST stat.ME stat.ML stat.TH published:2015-10-30 summary:We propose a new inferential framework of constructing confidence regions andtesting hypotheses for statistical models specified by a system of highdimensional estimating equations. The key ingredient of this framework is aninfluence function constructed by projecting the fitted estimating equations toa sparse direction obtained by solving a large-scale linear program. The main feature of our framework which makes it different from the existingones is that the specification of the loglikelihood and other types of lossfunctions is not needed. The main theoretical contribution is to establish aunified Z-estimation theory of confidence regions for high dimensionalproblems. We further apply our general framework to a number of examples includingnoisy compressed sensing, undirected graphical models, discriminant analysisand vector autoregression models. We provide thorough numerical simulations toback up the developed theoretical results.
arxiv-1510-09184 | Estimating Target Signatures with Diverse Density |  http://arxiv.org/abs/1510.09184  | author:Taylor Glenn, Alina Zare category:cs.CV published:2015-10-30 summary:Hyperspectral target detection algorithms rely on knowing the desired targetsignature in advance. However, obtaining an effective target signature can bedifficult; signatures obtained from laboratory measurements orhand-spectrometers in the field may not transfer to airborne imageryeffectively. One approach to dealing with this difficulty is to learn aneffective target signature from training data. An approach for learning targetsignatures from training data is presented. The proposed approach addressesuncertainty and imprecision in groundtruth in the training data using amultiple instance learning, diverse density (DD) based objective function.After learning the target signature given data with uncertain and imprecisegroundtruth, target detection can be applied on test data. Results are shown onsimulated and real data.
arxiv-1510-09005 | A Study of the Spatio-Temporal Correlations in Mobile Calls Networks |  http://arxiv.org/abs/1510.09005  | author:Romain Guigourès, Marc Boullé, Fabrice Rossi category:stat.ML cs.SI published:2015-10-30 summary:For the last few years, the amount of data has significantly increased in thecompanies. It is the reason why data analysis methods have to evolve to meetnew demands. In this article, we introduce a practical analysis of a largedatabase from a telecommunication operator. The problem is to segment aterritory and characterize the retrieved areas owing to their inhabitantbehavior in terms of mobile telephony. We have call detail records collectedduring five months in France. We propose a two stages analysis. The first oneaims at grouping source antennas which originating calls are similarlydistributed on target antennas and conversely for target antenna w.r.t. sourceantenna. A geographic projection of the data is used to display the results ona map of France. The second stage discretizes the time into periods betweenwhich we note changes in distributions of calls emerging from the clusters ofsource antennas. This enables an analysis of temporal changes of inhabitantsbehavior in every area of the country.
arxiv-1510-09079 | SentiWords: Deriving a High Precision and High Coverage Lexicon for Sentiment Analysis |  http://arxiv.org/abs/1510.09079  | author:Lorenzo Gatti, Marco Guerini, Marco Turchi category:cs.CL published:2015-10-30 summary:Deriving prior polarity lexica for sentiment analysis - where positive ornegative scores are associated with words out of context - is a challengingtask. Usually, a trade-off between precision and coverage is hard to find, andit depends on the methodology used to build the lexicon. Manually annotatedlexica provide a high precision but lack in coverage, whereas automaticderivation from pre-existing knowledge guarantees high coverage at the cost ofa lower precision. Since the automatic derivation of prior polarities is lesstime consuming than manual annotation, there has been a great bloom of theseapproaches, in particular based on the SentiWordNet resource. In this paper, wecompare the most frequently used techniques based on SentiWordNet with newerones and blend them in a learning framework (a so called 'ensemble method'). Bytaking advantage of manually built prior polarity lexica, our ensemble methodis better able to predict the prior value of unseen words and to outperform allthe other SentiWordNet approaches. Using this technique we have builtSentiWords, a prior polarity lexicon of approximately 155,000 words, that hasboth a high precision and a high coverage. We finally show that in sentimentanalysis tasks, using our lexicon allows us to outperform both the singlemetrics derived from SentiWordNet and popular manually annotated sentimentlexica.
arxiv-1510-09123 | Subsampling in Smoothed Range Spaces |  http://arxiv.org/abs/1510.09123  | author:Jeff M. Phillips, Yan Zheng category:cs.CG cs.LG published:2015-10-30 summary:We consider smoothed versions of geometric range spaces, so an element of theground set (e.g. a point) can be contained in a range with a non-binary valuein $[0,1]$. Similar notions have been considered for kernels; we extend them tomore general types of ranges. We then consider approximations of these rangespaces through $\varepsilon $-nets and $\varepsilon $-samples (aka$\varepsilon$-approximations). We characterize when size bounds for$\varepsilon $-samples on kernels can be extended to these more generalsmoothed range spaces. We also describe new generalizations for $\varepsilon$-nets to these range spaces and show when results from binary range spaces cancarry over to these smoothed ones.
arxiv-1510-09130 | Latent Bayesian melding for integrating individual and population models |  http://arxiv.org/abs/1510.09130  | author:Mingjun Zhong, Nigel Goddard, Charles Sutton category:stat.ML cs.AI stat.AP stat.ME published:2015-10-30 summary:In many statistical problems, a more coarse-grained model may be suitable forpopulation-level behaviour, whereas a more detailed model is appropriate foraccurate modelling of individual behaviour. This raises the question of how tointegrate both types of models. Methods such as posterior regularization followthe idea of generalized moment matching, in that they allow matchingexpectations between two models, but sometimes both models are mostconveniently expressed as latent variable models. We propose latent Bayesianmelding, which is motivated by averaging the distributions over populationsstatistics of both the individual-level and the population-level models under alogarithmic opinion pool framework. In a case study on electricitydisaggregation, which is a type of single-channel blind source separationproblem, we show that latent Bayesian melding leads to significantly moreaccurate predictions than an approach based solely on generalized momentmatching.
arxiv-1510-09142 | Learning Continuous Control Policies by Stochastic Value Gradients |  http://arxiv.org/abs/1510.09142  | author:Nicolas Heess, Greg Wayne, David Silver, Timothy Lillicrap, Yuval Tassa, Tom Erez category:cs.LG cs.NE published:2015-10-30 summary:We present a unified framework for learning continuous control policies usingbackpropagation. It supports stochastic control by treating stochasticity inthe Bellman equation as a deterministic function of exogenous noise. Theproduct is a spectrum of general policy gradient algorithms that range frommodel-free methods with value functions to model-based methods without valuefunctions. We use learned models but only require observations from theenvironment in- stead of observations from model-predicted trajectories,minimizing the impact of compounded model errors. We apply these algorithmsfirst to a toy stochastic control problem and then to several physics-basedcontrol problems in simulation. One of these variants, SVG(1), shows theeffectiveness of learning models, value functions, and policies simultaneouslyin continuous domains.
arxiv-1510-09161 | Streaming, Distributed Variational Inference for Bayesian Nonparametrics |  http://arxiv.org/abs/1510.09161  | author:Trevor Campbell, Julian Straub, John W. Fisher III, Jonathan P. How category:cs.LG stat.ML published:2015-10-30 summary:This paper presents a methodology for creating streaming, distributedinference algorithms for Bayesian nonparametric (BNP) models. In the proposedframework, processing nodes receive a sequence of data minibatches, compute avariational posterior for each, and make asynchronous streaming updates to acentral model. In contrast to previous algorithms, the proposed framework istruly streaming, distributed, asynchronous, learning-rate-free, andtruncation-free. The key challenge in developing the framework, arising fromthe fact that BNP models do not impose an inherent ordering on theircomponents, is finding the correspondence between minibatch and central BNPposterior components before performing each update. To address this, the paperdevelops a combinatorial optimization problem over component correspondences,and provides an efficient solution technique. The paper concludes with anapplication of the methodology to the DP mixture model, with experimentalresults demonstrating its practical scalability and performance.
arxiv-1510-09041 | Postprocessing of Compressed Images via Sequential Denoising |  http://arxiv.org/abs/1510.09041  | author:Yehuda Dar, Alfred M. Bruckstein, Michael Elad, Raja Giryes category:cs.CV published:2015-10-30 summary:In this work we propose a novel postprocessing technique forcompression-artifact reduction. Our approach is based on posing this task as aninverse problem, with a regularization that leverages on existingstate-of-the-art image denoising algorithms. We rely on the recently proposedPlug-and-Play Prior framework, suggesting the solution of general inverseproblems via Alternating Direction Method of Multipliers (ADMM), leading to asequence of Gaussian denoising steps. A key feature in our scheme is alinearization of the compression-decompression process, so as to get aformulation that can be optimized. In addition, we supply a thorough analysisof this linear approximation for several basic compression procedures. Theproposed method is suitable for diverse compression techniques that rely ontransform coding. Specifically, we demonstrate impressive gains in imagequality for several leading compression methods - JPEG, JPEG2000, and HEVC.
arxiv-1510-09171 | Accurate Vision-based Vehicle Localization using Satellite Imagery |  http://arxiv.org/abs/1510.09171  | author:Hang Chu, Hongyuan Mei, Mohit Bansal, Matthew R. Walter category:cs.RO cs.CV published:2015-10-30 summary:We propose a method for accurately localizing ground vehicles with the aid ofsatellite imagery. Our approach takes a ground image as input, and outputs thelocation from which it was taken on a georeferenced satellite image. We performvisual localization by estimating the co-occurrence probabilities between theground and satellite images based on a ground-satellite feature dictionary. Themethod is able to estimate likelihoods over arbitrary locations without theneed for a dense ground image database. We present a ranking-loss basedalgorithm that learns location-discriminative feature projection matrices thatresult in further improvements in accuracy. We evaluate our method on theMalaga and KITTI public datasets and demonstrate significant improvements overa baseline that performs exhaustive search.
arxiv-1511-00043 | Learning Adversary Behavior in Security Games: A PAC Model Perspective |  http://arxiv.org/abs/1511.00043  | author:Arunesh Sinha, Debarun Kar, Milind Tambe category:cs.AI cs.GT cs.LG published:2015-10-30 summary:Recent applications of Stackelberg Security Games (SSG), from wildlife crimeto urban crime, have employed machine learning tools to learn and predictadversary behavior using available data about defender-adversary interactions.Given these recent developments, this paper commits to an approach of directlylearning the response function of the adversary. Using the PAC model, thispaper lays a firm theoretical foundation for learning in SSGs (e.g.,theoretically answer questions about the numbers of samples required to learnadversary behavior) and provides utility guarantees when the learned adversarymodel is used to plan the defender's strategy. The paper also aims to answerpractical questions such as how much more data is needed to improve anadversary model's accuracy. Additionally, we explain a recently observedphenomenon that prediction accuracy of learned adversary behavior is not enoughto discover the utility maximizing defender strategy. We provide four maincontributions: (1) a PAC model of learning adversary response functions inSSGs; (2) PAC-model analysis of the learning of key, existing boundedrationality models in SSGs; (3) an entirely new approach to adversary modelingbased on a non-parametric class of response functions with PAC-model analysisand (4) identification of conditions under which computing the best defenderstrategy against the learned adversary behavior is indeed the optimal strategy.Finally, we conduct experiments with real-world data from a national park inUganda, showing the benefit of our new adversary modeling approach andverification of our PAC model predictions.
arxiv-1510-08896 | Robust Shift-and-Invert Preconditioning: Faster and More Sample Efficient Algorithms for Eigenvector Computation |  http://arxiv.org/abs/1510.08896  | author:Chi Jin, Sham M. Kakade, Cameron Musco, Praneeth Netrapalli, Aaron Sidford category:cs.DS cs.LG math.NA math.OC published:2015-10-29 summary:We provide faster algorithms and improved sample complexities forapproximating the top eigenvector of a matrix. Offline Setting: Given an $n \times d$ matrix $A$, we show how to compute an$\epsilon$ approximate top eigenvector in time $\tilde O ( [nnz(A) + \frac{d\cdot sr(A)}{gap^2}]\cdot \log 1/\epsilon )$ and $\tilde O([\frac{nnz(A)^{3/4}(d \cdot sr(A))^{1/4}}{\sqrt{gap}}]\cdot \log1/\epsilon )$. Here $sr(A)$ is thestable rank and $gap$ is the multiplicative eigenvalue gap. By separating the$gap$ dependence from $nnz(A)$ we improve on the classic power and Lanczosmethods. We also improve prior work using fast subspace embeddings andstochastic optimization, giving significantly improved dependencies on $sr(A)$and $\epsilon$. Our second running time improves this further when $nnz(A) \le\frac{d\cdot sr(A)}{gap^2}$. Online Setting: Given a distribution $D$ with covariance matrix $\Sigma$ anda vector $x_0$ which is an $O(gap)$ approximate top eigenvector for $\Sigma$,we show how to refine to an $\epsilon$ approximation using $\tildeO(\frac{v(D)}{gap^2} + \frac{v(D)}{gap \cdot \epsilon})$ samples from $D$. Here$v(D)$ is a natural variance measure. Combining our algorithm with previouswork to initialize $x_0$, we obtain a number of improved sample complexity andruntime results. For general distributions, we achieve asymptotically optimalaccuracy as a function of sample size as the number of samples grows large. Our results center around a robust analysis of the classic method ofshift-and-invert preconditioning to reduce eigenvector computation toapproximately solving a sequence of linear systems. We then apply fast SVRGbased approximate system solvers to achieve our claims. We believe our resultssuggest the general effectiveness of shift-and-invert based approaches andimply that further computational gains may be reaped in practice.
arxiv-1510-08893 | A Deep Siamese Network for Scene Detection in Broadcast Videos |  http://arxiv.org/abs/1510.08893  | author:Lorenzo Baraldi, Costantino Grana, Rita Cucchiara category:cs.CV cs.MM published:2015-10-29 summary:We present a model that automatically divides broadcast videos into coherentscenes by learning a distance measure between shots. Experiments are performedto demonstrate the effectiveness of our approach by comparing our algorithmagainst recent proposals for automatic scene segmentation. We also propose animproved performance measure that aims to reduce the gap between numericalevaluation and expected results, and propose and release a new benchmarkdataset.
arxiv-1510-08865 | Mixed Robust/Average Submodular Partitioning: Fast Algorithms, Guarantees, and Applications |  http://arxiv.org/abs/1510.08865  | author:Kai Wei, Rishabh Iyer, Shengjie Wang, Wenruo Bai, Jeff Bilmes category:cs.DS cs.DM cs.LG published:2015-10-29 summary:We investigate two novel mixed robust/average-case submodular datapartitioning problems that we collectively call \emph{Submodular Partitioning}.These problems generalize purely robust instances of the problem, namely\emph{max-min submodular fair allocation} (SFA) and \emph{min-max submodularload balancing} (SLB), and also average-case instances, that is the\emph{submodular welfare problem} (SWP) and \emph{submodular multiwaypartition} (SMP). While the robust versions have been studied in the theorycommunity, existing work has focused on tight approximation guarantees, and theresultant algorithms are not generally scalable to large real-worldapplications. This is in contrast to the average case, where most of thealgorithms are scalable. In the present paper, we bridge this gap, by proposingseveral new algorithms (including greedy, majorization-minimization,minorization-maximization, and relaxation algorithms) that not only scale tolarge datasets but that also achieve theoretical approximation guaranteescomparable to the state-of-the-art. We moreover provide new scalable algorithmsthat apply to additive combinations of the robust and average-case objectives.We show that these problems have many applications in machine learning (ML),including data partitioning and load balancing for distributed ML, dataclustering, and image segmentation. We empirically demonstrate the efficacy ofour algorithms on real-world problems involving data partitioning fordistributed optimization (of convex and deep neural network objectives), andalso purely unsupervised image segmentation.
arxiv-1510-08565 | Attention with Intention for a Neural Network Conversation Model |  http://arxiv.org/abs/1510.08565  | author:Kaisheng Yao, Geoffrey Zweig, Baolin Peng category:cs.NE cs.AI cs.HC cs.LG published:2015-10-29 summary:In a conversation or a dialogue process, attention and intention playintrinsic roles. This paper proposes a neural network based approach thatmodels the attention and intention processes. It essentially consists of threerecurrent networks. The encoder network is a word-level model representingsource side sentences. The intention network is a recurrent network that modelsthe dynamics of the intention process. The decoder network is a recurrentnetwork produces responses to the input from the source side. It is a languagemodel that is dependent on the intention and has an attention mechanism toattend to particular source side words, when predicting a symbol in theresponse. The model is trained end-to-end without labeling data. Experimentsshow that this model generates natural responses to user inputs.
arxiv-1510-08829 | Spiking Deep Networks with LIF Neurons |  http://arxiv.org/abs/1510.08829  | author:Eric Hunsberger, Chris Eliasmith category:cs.LG cs.NE published:2015-10-29 summary:We train spiking deep networks using leaky integrate-and-fire (LIF) neurons,and achieve state-of-the-art results for spiking networks on the CIFAR-10 andMNIST datasets. This demonstrates that biologically-plausible spiking LIFneurons can be integrated into deep networks can perform as well as otherspiking models (e.g. integrate-and-fire). We achieved this result by softeningthe LIF response function, such that its derivative remains bounded, and bytraining the network with noise to provide robustness against the variabilityintroduced by spikes. Our method is general and could be applied to otherneuron types, including those used on modern neuromorphic hardware. Our workbrings more biological realism into modern image classification models, withthe hope that these models can inform how the brain performs this difficulttask. It also provides new methods for training deep networks to run onneuromorphic hardware, with the aim of fast, power-efficient imageclassification for robotics applications.
arxiv-1510-08583 | Privacy Prediction of Images Shared on Social Media Sites Using Deep Features |  http://arxiv.org/abs/1510.08583  | author:Ashwini Tonge, Cornelia Caragea category:cs.CV cs.CY published:2015-10-29 summary:Online image sharing in social media sites such as Facebook, Flickr, andInstagram can lead to unwanted disclosure and privacy violations, when privacysettings are used inappropriately. With the exponential increase in the numberof images that are shared online every day, the development of effective andefficient prediction methods for image privacy settings are highly needed. Theperformance of models critically depends on the choice of the featurerepresentation. In this paper, we present an approach to image privacyprediction that uses deep features and deep image tags as featurerepresentations. Specifically, we explore deep features at various neuralnetwork layers and use the top layer (probability) as an auto-annotationmechanism. The results of our experiments show that models trained on theproposed deep features and deep image tags substantially outperform baselinessuch as those based on SIFT and GIST as well as those that use "bag of tags" asfeatures.
arxiv-1510-08692 | Covariance-Controlled Adaptive Langevin Thermostat for Large-Scale Bayesian Sampling |  http://arxiv.org/abs/1510.08692  | author:Xiaocheng Shang, Zhanxing Zhu, Benedict Leimkuhler, Amos J. Storkey category:stat.ML cs.LG published:2015-10-29 summary:Monte Carlo sampling for Bayesian posterior inference is a common approachused in machine learning. The Markov Chain Monte Carlo procedures that are usedare often discrete-time analogues of associated stochastic differentialequations (SDEs). These SDEs are guaranteed to leave invariant the requiredposterior distribution. An area of current research addresses the computationalbenefits of stochastic gradient methods in this setting. Existing techniquesrely on estimating the variance or covariance of the subsampling error, andtypically assume constant variance. In this article, we propose acovariance-controlled adaptive Langevin thermostat that can effectivelydissipate parameter-dependent noise while maintaining a desired targetdistribution. The proposed method achieves a substantial speedup over popularalternative schemes for large-scale machine learning applications.
arxiv-1510-08568 | Feature-Based Diversity Optimization for Problem Instance Classification |  http://arxiv.org/abs/1510.08568  | author:Wanru Gao, Samadhi Nallaperuma, Frank Neumann category:cs.NE cs.AI published:2015-10-29 summary:Understanding the behaviour of heuristic search methods is a challenge. Thiseven holds for simple local search methods such as 2-OPT for the TravelingSalesperson problem. In this paper, we present a general framework that is ableto construct a diverse set of instances that are hard or easy for a givensearch heuristic. Such a diverse set is obtained by using an evolutionaryalgorithm for constructing hard or easy instances that are diverse with respectto different features of the underlying problem. Examining the constructedinstance sets, we show that many combinations of two or three features give agood classification of the TSP instances in terms of whether they are hard tobe solved by 2-OPT.
arxiv-1510-08906 | Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning |  http://arxiv.org/abs/1510.08906  | author:Christoph Dann, Emma Brunskill category:stat.ML cs.AI cs.LG published:2015-10-29 summary:Recently, there has been significant progress in understanding reinforcementlearning in discounted infinite-horizon Markov decision processes (MDPs) byderiving tight sample complexity bounds. However, in many real-worldapplications, an interactive learning agent operates for a fixed or boundedperiod of time, for example tutoring students for exams or handling customerservice requests. Such scenarios can often be better treated as episodicfixed-horizon MDPs, for which only looser bounds on the sample complexityexist. A natural notion of sample complexity in this setting is the number ofepisodes required to guarantee a certain performance with high probability (PACguarantee). In this paper, we derive an upper PAC bound $\tildeO(\frac{\mathcal S^2 \mathcal A H^2}{\epsilon^2} \ln\frac 1 \delta)$ and alower PAC bound $\tilde \Omega(\frac{\mathcal S \mathcal A H^2}{\epsilon^2}\ln \frac 1 {\delta + c})$ that match up to log-terms and an additional lineardependency on the number of states $\mathcal S$. The lower bound is the firstof its kind for this setting. Our upper bound leverages Bernstein's inequalityto improve on previous bounds for episodic finite-horizon MDPs which have atime-horizon dependency of at least $H^3$.
arxiv-1510-08660 | RATM: Recurrent Attentive Tracking Model |  http://arxiv.org/abs/1510.08660  | author:Samira Ebrahimi Kahou, Vincent Michalski, Roland Memisevic category:cs.LG published:2015-10-29 summary:We present an attention-based modular neural framework for computer vision.The framework uses a soft attention mechanism allowing models to be trainedwith gradient descent. It consists of three modules: a recurrent attentionmodule controlling where to look in an image or video frame, afeature-extraction module providing a representation of what is seen, and anobjective module formalizing why the model learns its attentive behavior. Theattention module allows the model to focus computation on task-relatedinformation in the input. We apply the framework to several object trackingtasks and explore various design choices. We experiment with three data sets,bouncing ball, moving digits and the real-world KTH data set. The proposedRecurrent Attentive Tracking Model performs well on all three tasks and cangeneralize to related but previously unseen sequences from a challengingtracking data set.
arxiv-1510-08633 | Nonconvex Penalization in Sparse Estimation: An Approach Based on the Bernstein Function |  http://arxiv.org/abs/1510.08633  | author:Zhihua Zhang category:stat.ML published:2015-10-29 summary:In this paper we study nonconvex penalization using Bernstein functions whosefirst-order derivatives are completely monotone. The Bernstein function caninduce a class of nonconvex penalty functions for high-dimensional sparseestimation problems. We derive a thresholding function based on the Bernsteinpenalty and discuss some important mathematical properties in sparsitymodeling. We show that a coordinate descent algorithm is especially appropriatefor regression problems penalized by the Bernstein function. We also considerthe application of the Bernstein penalty in classification problems and devisea proximal alternating linearized minimization method. Based on theory of theKurdyka-Lojasiewicz inequality, we conduct convergence analysis of thesealternating iteration procedures. We particularly exemplify a family ofBernstein nonconvex penalties based on a generalized Gamma measure and conductempirical analysis for this family.
arxiv-1510-08532 | The Singular Value Decomposition, Applications and Beyond |  http://arxiv.org/abs/1510.08532  | author:Zhihua Zhang category:cs.LG published:2015-10-29 summary:The singular value decomposition (SVD) is not only a classical theory inmatrix computation and analysis, but also is a powerful tool in machinelearning and modern data analysis. In this tutorial we first study the basicnotion of SVD and then show the central role of SVD in matrices. Usingmajorization theory, we consider variational principles of singular values andeigenvalues. Built on SVD and a theory of symmetric gauge functions, we discussunitarily invariant norms, which are then used to formulate general results formatrix low rank approximation. We study the subdifferentials of unitarilyinvariant norms. These results would be potentially useful in many machinelearning problems such as matrix completion and matrix data classification.Finally, we discuss matrix low rank approximation and its recent developmentssuch as randomized SVD, approximate matrix multiplication, CUR decomposition,and Nystrom approximation. Randomized algorithms are important approaches tolarge scale SVD as well as fast matrix computations.
arxiv-1510-08628 | WarpLDA: a Cache Efficient O(1) Algorithm for Latent Dirichlet Allocation |  http://arxiv.org/abs/1510.08628  | author:Jianfei Chen, Kaiwei Li, Jun Zhu, Wenguang Chen category:stat.ML cs.DC cs.IR cs.LG published:2015-10-29 summary:Developing efficient and scalable algorithms for Latent Dirichlet Allocation(LDA) is of wide interest for many applications. Previous work has developed anO(1) Metropolis-Hastings sampling method for each token. However, theperformance is far from being optimal due to random accesses to the parametermatrices and frequent cache misses. In this paper, we first carefully analyze the memory access efficiency ofexisting algorithms for LDA by the scope of random access, which is the size ofthe memory region in which random accesses fall, within a short period of time.We then develop WarpLDA, an LDA sampler which achieves both the best O(1) timecomplexity per token and the best O(K) scope of random access. Our empiricalresults in a wide range of testing conditions demonstrate that WarpLDA isconsistently 5-15x faster than the state-of-the-art Metropolis-Hastings basedLightLDA, and is comparable or faster than the sparsity aware F+LDA. WithWarpLDA, users can learn up to one million topics from hundreds of millions ofdocuments in a few hours, at an unprecedentedly throughput of 11G tokens persecond.
arxiv-1510-08389 | Universal Dependency Analysis |  http://arxiv.org/abs/1510.08389  | author:Hoang-Vu Nguyen, Jilles Vreeken category:stat.ML cs.LG published:2015-10-28 summary:Most data is multi-dimensional. Discovering whether any subset of dimensions,or subspaces, of such data is significantly correlated is a core task in datamining. To do so, we require a measure that quantifies how correlated asubspace is. For practical use, such a measure should be universal in the sensethat it captures correlation in subspaces of any dimensionality and allows tomeaningfully compare correlation scores across different subspaces, regardlesshow many dimensions they have and what specific statistical properties theirdimensions possess. Further, it would be nice if the measure cannon-parametrically and efficiently capture both linear and non-linearcorrelations. In this paper, we propose UDS, a multivariate correlation measure thatfulfills all of these desiderata. In short, we define \uds based on cumulativeentropy and propose a principled normalization scheme to bring its scoresacross different subspaces to the same domain, enabling universal correlationassessment. UDS is purely non-parametric as we make no assumption on datadistributions nor types of correlation. To compute it on empirical data, weintroduce an efficient and non-parametric method. Extensive experiments showthat UDS outperforms state of the art.
arxiv-1510-08406 | Fast Landmark Subspace Clustering |  http://arxiv.org/abs/1510.08406  | author:Xu Wang, Gilad Lerman category:stat.ML published:2015-10-28 summary:Kernel methods obtain superb performance in terms of accuracy for variousmachine learning tasks since they can effectively extract nonlinear relations.However, their time complexity can be rather large especially for clusteringtasks. In this paper we define a general class of kernels that can be easilyapproximated by randomization. These kernels appear in various applications, inparticular, traditional spectral clustering, landmark-based spectral clusteringand landmark-based subspace clustering. We show that for $n$ data points from$K$ clusters with $D$ landmarks, the randomization procedure results in analgorithm of complexity $O(KnD)$. Furthermore, we bound the error between theoriginal clustering scheme and its randomization. To illustrate the power ofthis framework, we propose a new fast landmark subspace (FLS) clusteringalgorithm. Experiments over synthetic and real datasets demonstrate thesuperior performance of FLS in accelerating subspace clustering with marginalsacrifice of accuracy.
arxiv-1510-08418 | Fast k-best Sentence Compression |  http://arxiv.org/abs/1510.08418  | author:Katja Filippova, Enrique Alfonseca category:cs.CL published:2015-10-28 summary:A popular approach to sentence compression is to formulate the task as aconstrained optimization problem and solve it with integer linear programming(ILP) tools. Unfortunately, dependence on ILP may make the compressorprohibitively slow, and thus approximation techniques have been proposed whichare often complex and offer a moderate gain in speed. As an alternativesolution, we introduce a novel compression algorithm which generates k-bestcompressions relying on local deletion decisions. Our algorithm is two ordersof magnitude faster than a recent ILP-based method while producing bettercompressions. Moreover, an extensive evaluation demonstrates that the qualityof compressions does not degrade much as we move from single best to top-fiveresults.
arxiv-1510-08440 | Priors on exchangeable directed graphs |  http://arxiv.org/abs/1510.08440  | author:Diana Cai, Nathanael Ackerman, Cameron Freer category:math.ST stat.ME stat.ML stat.TH published:2015-10-28 summary:Directed graphs occur throughout statistical modeling of networks, andexchangeability is a natural assumption when the ordering of vertices does notmatter. There is a deep structural theory for exchangeable undirected graphs,which extends to the directed case, but with additional complexities that arisefrom the need to consider the joint distribution over both edge directions on apair of vertices. Exchangeable directed graphs are characterized by a samplingprocedure given by the Aldous-Hoover theorem, which can be described in termsof a distribution on measurable objects known as digraphons. Most existing workon exchangeable graph models has focused on undirected graphs, and littleattention has been placed on priors for exchangeable directed graphs.Currently, many directed network models generalize the undirected case bytreating each edge direction as independent, rather than considering both edgedirections jointly. By placing priors on digraphons one can capture dependencein the edge directions in exchangeable directed graphs, which we demonstrate isnot captured by models that consider the edge directions independently. Weconstruct priors on exchangeable directed graphs using digraphons, includingspecial cases such as tournaments, linear orderings, directed acyclic graphs,and partial orderings. We also present a Bayesian nonparametric block model forexchangeable directed graphs and demonstrate inference for these models onsynthetic data.
arxiv-1510-08470 | Toward Long Distance, Sub-diffraction Imaging Using Coherent Camera Arrays |  http://arxiv.org/abs/1510.08470  | author:Jason Holloway, M. Salman Asif, Manoj Kumar Sharma, Nathan Matsuda, Roarke Horstmeyer, Oliver Cossairt, Ashok Veeraraghavan category:cs.CV physics.optics published:2015-10-28 summary:In this work, we propose using camera arrays coupled with coherentillumination as an effective method of improving spatial resolution in longdistance images by a factor of ten and beyond. Recent advances in ptychographyhave demonstrated that one can image beyond the diffraction limit of theobjective lens in a microscope. We demonstrate a similar imaging system toimage beyond the diffraction limit in long range imaging. We emulate a cameraarray with a single camera attached to an X-Y translation stage. We show thatan appropriate phase retrieval based reconstruction algorithm can be used toeffectively recover the lost high resolution details from the multiple lowresolution acquired images. We analyze the effects of noise, required degree ofimage overlap, and the effect of increasing synthetic aperture size on thereconstructed image quality. We show that coherent camera arrays have thepotential to greatly improve imaging performance. Our simulations showresolution gains of 10x and more are achievable. Furthermore, experimentalresults from our proof-of-concept systems show resolution gains of 4x-7x forreal scenes. Finally, we introduce and analyze in simulation a new strategy tocapture macroscopic Fourier Ptychography images in a single snapshot, albeitusing a camera array.
arxiv-1510-08480 | Emoticons vs. Emojis on Twitter: A Causal Inference Approach |  http://arxiv.org/abs/1510.08480  | author:Umashanthi Pavalanathan, Jacob Eisenstein category:cs.CL published:2015-10-28 summary:Online writing lacks the non-verbal cues present in face-to-facecommunication, which provide additional contextual information about theutterance, such as the speaker's intention or affective state. To fill thisvoid, a number of orthographic features, such as emoticons, expressivelengthening, and non-standard punctuation, have become popular in social mediaservices including Twitter and Instagram. Recently, emojis have been introducedto social media, and are increasingly popular. This raises the question ofwhether these predefined pictographic characters will come to replace earlierorthographic methods of paralinguistic communication. In this abstract, weattempt to shed light on this question, using a matching approach from causalinference to test whether the adoption of emojis causes individual users toemploy fewer emoticons in their text on Twitter.
arxiv-1510-08512 | Robust Gaussian Graphical Modeling with the Trimmed Graphical Lasso |  http://arxiv.org/abs/1510.08512  | author:Eunho Yang, Aurélie C. Lozano category:stat.ML published:2015-10-28 summary:Gaussian Graphical Models (GGMs) are popular tools for studying networkstructures. However, many modern applications such as gene network discoveryand social interactions analysis often involve high-dimensional noisy data withoutliers or heavier tails than the Gaussian distribution. In this paper, wepropose the Trimmed Graphical Lasso for robust estimation of sparse GGMs. Ourmethod guards against outliers by an implicit trimming mechanism akin to thepopular Least Trimmed Squares method used for linear regression. We provide arigorous statistical analysis of our estimator in the high-dimensional setting.In contrast, existing approaches for robust sparse GGMs estimation lackstatistical guarantees. Our theoretical results are complemented by experimentson simulated and real gene expression data which further demonstrate the valueof our approach.
arxiv-1510-08382 | Flexibly Mining Better Subgroups |  http://arxiv.org/abs/1510.08382  | author:Hoang-Vu Nguyen, Jilles Vreeken category:stat.ML cs.LG published:2015-10-28 summary:In subgroup discovery, also known as supervised pattern mining, discoveringhigh quality one-dimensional subgroups and refinements of these is a crucialtask. For nominal attributes, this is relatively straightforward, as we canconsider individual attribute values as binary features. For numericalattributes, the task is more challenging as individual numeric values are notreliable statistics. Instead, we can consider combinations of adjacent values,i.e. bins. Existing binning strategies, however, are not tailored for subgroupdiscovery. That is, they do not directly optimize for the quality of subgroups,therewith potentially degrading the mining result. To address this issue, we propose FLEXI. In short, with FLEXI we propose touse optimal binning to find high quality binary features for both numeric andordinal attributes. We instantiate FLEXI with various quality measures and showhow to achieve efficiency accordingly. Experiments on both synthetic andreal-world data sets show that FLEXI outperforms state of the art with up to 25times improvement in subgroup quality.
arxiv-1510-08370 | Canonical Divergence Analysis |  http://arxiv.org/abs/1510.08370  | author:Hoang-Vu Nguyen, Jilles Vreeken category:stat.ML cs.LG published:2015-10-28 summary:We aim to analyze the relation between two random vectors that maypotentially have both different number of attributes as well as realizations,and which may even not have a joint distribution. This problem arises in manypractical domains, including biology and architecture. Existing techniquesassume the vectors to have the same domain or to be jointly distributed, andhence are not applicable. To address this, we propose Canonical DivergenceAnalysis (CDA). We introduce three instantiations, each of which permitspractical implementation. Extensive empirical evaluation shows the potential ofour method.
arxiv-1510-08160 | Scale-aware Fast R-CNN for Pedestrian Detection |  http://arxiv.org/abs/1510.08160  | author:Jianan Li, Xiaodan Liang, ShengMei Shen, Tingfa Xu, Shuicheng Yan category:cs.CV published:2015-10-28 summary:Intuitively, instances of the same object category with different spatialscales may exhibit dramatically different features. Thus, large variance ininstance scales, which results in undesirable large intra-category variance infeatures, may severely hurt the performance of modern object instance detectionmethods. We argue that this issue can be substantially alleviated by thedivide-and-conquer philosophy. Taking pedestrian detection as an example, weillustrate how we can leverage this philosophy to develop a Scale-Aware FastR-CNN (SAF R-CNN) framework. The model introduces multiple built-insub-networks which detect pedestrians with scales from disjoint ranges. Outputsfrom all the sub-networks are then adaptively combined to generate the finaldetection results that are shown to be robust to large variance in instancescales, via a gate function defined over the sizes of object proposals.Extensive evaluations on the challenging Caltech pedestrian detectiondataset~\cite{dollar2012pedestrian} well demonstrate the superiority of theproposed SAF R-CNN over the state-of-the-arts. Particularly, the miss rate isreduced to $9.68\%$, which is significantly smaller than $11.75\%$ byCompACT-Deep~\cite{compact}, $20.86\%$ by TA-CNN~\cite{ta_cnn} and $12.86\%$ bythe vanilla Fast R-CNN model~\cite{girshick2015fast}.
arxiv-1510-08291 | Linear Shape Deformation Models with Local Support Using Graph-based Structured Matrix Factorisation |  http://arxiv.org/abs/1510.08291  | author:Florian Bernard, Peter Gemmar, Frank Hertel, Jorge Goncalves, Johan Thunberg category:cs.CV math.OC stat.ML published:2015-10-28 summary:Representing 3D shape deformations by linear models in high-dimensional spacehas many applications in computer vision and medical imaging, such asshape-based interpolation or segmentation. Commonly, using Principal ComponentsAnalysis a low-dimensional (affine) subspace of the high-dimensional shapespace is determined. However, the resulting factors (the most dominanteigenvectors of the covariance matrix) have global support, i.e. changing thecoefficient of a single factor deforms the entire shape. In this paper, amethod to obtain deformation factors with local support is presented. Thebenefits of such models include better flexibility and interpretability as wellas the possibility of interactively deforming shapes locally. For that, basedon a well-grounded theoretical motivation, we formulate a matrix factorisationproblem employing sparsity and graph-based regularisation terms. We demonstratethat for brain shapes our method outperforms the state of the art in localsupport models with respect to generalisation ability and sparse shapereconstruction, whereas for human body shapes our method gives more realisticdeformations.
arxiv-1510-08231 | Operator-valued Kernels for Learning from Functional Response Data |  http://arxiv.org/abs/1510.08231  | author:Hachem Kadri, Emmanuel Duflos, Philippe Preux, Stéphane Canu, Alain Rakotomamonjy, Julien Audiffren category:cs.LG stat.ML published:2015-10-28 summary:In this paper we consider the problems of supervised classification andregression in the case where attributes and labels are functions: a data isrepresented by a set of functions, and the label is also a function. We focuson the use of reproducing kernel Hilbert space theory to learn from suchfunctional data. Basic concepts and properties of kernel-based learning areextended to include the estimation of function-valued functions. In thissetting, the representer theorem is restated, a set of rigorously definedinfinite-dimensional operator-valued kernels that can be valuably applied whenthe data are functions is described, and a learning algorithm for nonlinearfunctional data analysis is introduced. The methodology is illustrated throughspeech and audio signal processing experiments.
arxiv-1510-08174 | Visual Quality Enhancement in Optoacoustic Tomography using Active Contour Segmentation Priors |  http://arxiv.org/abs/1510.08174  | author:Subhamoy Mandal, Xosé Luís Deán-Ben, Daniel Razansky category:physics.med-ph cs.CV physics.optics published:2015-10-28 summary:Segmentation of biomedical images is essential for studying andcharacterizing anatomical structures, detection and evaluation of pathologicaltissues. Segmentation has been further shown to enhance the reconstructionperformance in many tomographic imaging modalities by accounting forheterogeneities of the excitation field and tissue properties in the imagedregion. This is particularly relevant in optoacoustic tomography, wherediscontinuities in the optical and acoustic tissue properties, if not properlyaccounted for, may result in deterioration of the imaging performance.Efficient segmentation of optoacoustic images is often hampered by therelatively low intrinsic contrast of large anatomical structures, which isfurther impaired by the limited angular coverage of some commonly employedtomographic imaging configurations. Herein, we analyze the performance ofactive contour models for boundary segmentation in cross-sectional optoacoustictomography. The segmented mask is employed to construct a two compartment modelfor the acoustic and optical parameters of the imaged tissues, which issubsequently used to improve accuracy of the image reconstruction routines. Theperformance of the suggested segmentation and modeling approach are showcasedin tissue-mimicking phantoms and small animal imaging experiments.
arxiv-1510-08520 | Learning with $\ell^{0}$-Graph: $\ell^{0}$-Induced Sparse Subspace Clustering |  http://arxiv.org/abs/1510.08520  | author:Yingzhen Yang, Jiashi Feng, Jianchao Yang, Thomas S. Huang category:cs.LG cs.CV published:2015-10-28 summary:Sparse subspace clustering methods, such as Sparse Subspace Clustering (SSC)\cite{ElhamifarV13} and $\ell^{1}$-graph \cite{YanW09,ChengYYFH10}, areeffective in partitioning the data that lie in a union of subspaces. Most ofthose methods use $\ell^{1}$-norm or $\ell^{2}$-norm with thresholding toimpose the sparsity of the constructed sparse similarity graph, and certainassumptions, e.g. independence or disjointness, on the subspaces are requiredto obtain the subspace-sparse representation, which is the key to theirsuccess. Such assumptions are not guaranteed to hold in practice and they limitthe application of sparse subspace clustering on subspaces with generallocation. In this paper, we propose a new sparse subspace clustering methodnamed $\ell^{0}$-graph. In contrast to the required assumptions on subspacesfor most existing sparse subspace clustering methods, it is proved thatsubspace-sparse representation can be obtained by $\ell^{0}$-graph forarbitrary distinct underlying subspaces almost surely under the mild i.i.d.assumption on the data generation. We develop a proximal method to obtain thesub-optimal solution to the optimization problem of $\ell^{0}$-graph withproved guarantee of convergence. Moreover, we propose a regularized$\ell^{0}$-graph that encourages nearby data to have similar neighbors so thatthe similarity graph is more aligned within each cluster and the graphconnectivity issue is alleviated. Extensive experimental results on variousdata sets demonstrate the superiority of $\ell^{0}$-graph compared to othercompeting clustering methods, as well as the effectiveness of regularized$\ell^{0}$-graph.
arxiv-1510-08385 | Linear-time Detection of Non-linear Changes in Massively High Dimensional Time Series |  http://arxiv.org/abs/1510.08385  | author:Hoang-Vu Nguyen, Jilles Vreeken category:stat.ML cs.LG published:2015-10-28 summary:Change detection in multivariate time series has applications in manydomains, including health care and network monitoring. A common approach todetect changes is to compare the divergence between the distributions of areference window and a test window. When the number of dimensions is verylarge, however, the naive approach has both quality and efficiency issues: toensure robustness the window size needs to be large, which not only leads tomissed alarms but also increases runtime. To this end, we propose LIGHT, a linear-time algorithm for robustly detectingnon-linear changes in massively high dimensional time series. Importantly,LIGHT provides high flexibility in choosing the window size, allowing thedomain expert to fit the level of details required. To do such, we 1) performscalable PCA to reduce dimensionality, 2) perform scalable factorization of thejoint distribution, and 3) scalably compute divergences between these lowerdimensional distributions. Extensive empirical evaluation on both synthetic andreal-world data show that LIGHT outperforms state of the art with up to 100%improvement in both quality and efficiency.
arxiv-1510-07727 | Statistically efficient thinning of a Markov chain sampler |  http://arxiv.org/abs/1510.07727  | author:Art B. Owen category:stat.CO cs.LG stat.ML 65C40, 62M05 published:2015-10-27 summary:It is common to subsample Markov chain samples to reduce the storage burdenof the output. It is also well known that discarding $k-1$ out of every $k$observations will not improve statistical efficiency. It is less frequentlyremarked that subsampling a Markov chain allows one to omit some of thecomputation beyond that needed to simply advance the chain. When this reducedcomputation is accounted for, thinning the Markov chain by subsampling it canimprove statistical efficiency. Given an autocorrelation parameter $\rho$ and acost ratio $\theta$, this paper shows how to compute the most efficientsubsampling frequency $k$. The optimal $k$ grows rapidly as $\rho$ increasestowards $1$. The resulting efficiency gain depends primarily on $\theta$, not$\rho$. Taking $k=1$ (no thinning) is optimal when $\rho\le0$. For $\rho>0$ itis optimal if and only if $\theta \le (1-\rho)^2/(2\rho)$. The efficiency gainnever exceeds $1+\theta$. The derivations are exact for an AR(1)autocorrelation which is often a good approximation to the autocorrelations onesees in practice.
arxiv-1510-07965 | Blitzkriging: Kronecker-structured Stochastic Gaussian Processes |  http://arxiv.org/abs/1510.07965  | author:Thomas Nickson, Tom Gunter, Chris Lloyd, Michael A Osborne, Stephen Roberts category:stat.ML published:2015-10-27 summary:We present Blitzkriging, a new approach to fast inference for Gaussianprocesses, applicable to regression, optimisation and classification.State-of-the-art (stochastic) inference for Gaussian processes on very largedatasets scales cubically in the number of 'inducing inputs', variablesintroduced to factorise the model. Blitzkriging shares state-of-the-art scalingwith data, but reduces the scaling in the number of inducing points toapproximately linear. Further, in contrast to other methods, Blitzkriging: doesnot force the data to conform to any particular structure (includinggrid-like); reduces reliance on error-prone optimisation of inducing pointlocations; and is able to learn rich (covariance) structure from the data. Wedemonstrate the benefits of our approach on real data in regression,time-series prediction and signal-interpolation experiments.
arxiv-1510-08012 | ENFT: Efficient Non-Consecutive Feature Tracking for Robust Structure-from-Motion |  http://arxiv.org/abs/1510.08012  | author:Guofeng Zhang, Haomin Liu, Zilong Dong, Jiaya Jia, Tien-Tsin Wong, Hujun Bao category:cs.CV published:2015-10-27 summary:Structure-from-motion (SfM) largely relies on the quality of featuretracking. In image sequences, if disjointed tracks caused by objects moving inand out of the view, occasional occlusion, or image noise, are not handledwell, the corresponding SfM could be significantly affected. This problembecomes more serious for accurate SfM of large-scale scenes, which typicallyrequires to capture multiple sequences to cover the whole scene. In this paper,we propose an efficient non-consecutive feature tracking (ENFT) framework tomatch the interrupted tracks distributed in different subsequences or even indifferent videos. Our framework consists of steps of solving the feature`dropout' problem when indistinctive structures, noise or even large imagedistortion exist, and of rapidly recognizing and joining common featureslocated in different subsequences. In addition, we contribute an effectivesegment-based coarse-to-fine SfM estimation algorithm for efficiently androbustly handling large datasets. Experimental results on several challengingand large video datasets demonstrate the effectiveness of the proposed system.
arxiv-1510-07957 | Increasing Behavioral Complexity for Evolved Virtual Creatures with the ESP Method |  http://arxiv.org/abs/1510.07957  | author:Dan Lessin, Don Fussell, Risto Miikkulainen, Sebastian Risi category:cs.NE published:2015-10-27 summary:Since their introduction in 1994 (Sims), evolved virtual creatures (EVCs)have employed the coevolution of morphology and control to produce high-impactwork in multiple fields, including graphics, evolutionary computation,robotics, and artificial life. However, in contrast to fixed-morphologycreatures, there has been no clear increase in the behavioral complexity ofEVCs in those two decades. This paper describes a method for moving beyond thislimit, making use of high-level human input in the form of a syllabus ofintermediate learning tasks--along with mechanisms for preservation, reuse, andcombination of previously learned tasks. This method--named ESP for its threecomponents: encapsulation, syllabus, and pandemonium--is presented in twocomplementary versions: Fast ESP, which constrains later morphological changesto achieve linear growth in computation time as behavioral complexity is added,and General ESP, which allows this restriction to be removed when sufficientcomputational resources are available. Experiments demonstrate that the ESPmethod allows evolved virtual creatures to reach new levels of behavioralcomplexity in the co-evolution of morphology and control, approximatelydoubling the previous state of the art.
arxiv-1510-08110 | Spectral Convergence Rate of Graph Laplacian |  http://arxiv.org/abs/1510.08110  | author:Xu Wang category:stat.ML published:2015-10-27 summary:Laplacian Eigenvectors of the graph constructed from a data set are used inmany spectral manifold learning algorithms such as diffusion maps and spectralclustering. Given a graph constructed from a random sample of a $d$-dimensionalcompact submanifold $M$ in $\mathbb{R}^D$, we establish the spectralconvergence rate of the graph Laplacian. It implies the consistency of thespectral clustering algorithm via a standard perturbation argument. A simplenumerical study indicates the necessity of a denoising step before applyingspectral algorithms.
arxiv-1510-07925 | Exclusive Sparsity Norm Minimization with Random Groups via Cone Projection |  http://arxiv.org/abs/1510.07925  | author:Yijun Huang, Ji Liu category:stat.ML cs.LG published:2015-10-27 summary:Many practical applications such as gene expression analysis, multi-tasklearning, image recognition, signal processing, and medical data analysispursue a sparse solution for the feature selection purpose and particularlyfavor the nonzeros \emph{evenly} distributed in different groups. The exclusivesparsity norm has been widely used to serve to this purpose. However, it stilllacks systematical studies for exclusive sparsity norm optimization. This paperoffers two main contributions from the optimization perspective: 1) We provideseveral efficient algorithms to solve exclusive sparsity norm minimization witheither smooth loss or hinge loss (non-smooth loss). All algorithms achieve theoptimal convergence rate $O(1/k^2)$ ($k$ is the iteration number). To the bestof our knowledge, this is the first time to guarantee such convergence rate forthe general exclusive sparsity norm minimization; 2) When the group informationis unavailable to define the exclusive sparsity norm, we propose to use therandom grouping scheme to construct groups and prove that if the number ofgroups is appropriately chosen, the nonzeros (true features) would be groupedin the ideal way with high probability. Empirical studies validate theefficiency of proposed algorithms, and the effectiveness of random groupingscheme on the proposed exclusive SVM formulation.
arxiv-1510-07945 | Learning Multi-Domain Convolutional Neural Networks for Visual Tracking |  http://arxiv.org/abs/1510.07945  | author:Hyeonseob Nam, Bohyung Han category:cs.CV published:2015-10-27 summary:We propose a novel visual tracking algorithm based on the representationsfrom a discriminatively trained Convolutional Neural Network (CNN). Ouralgorithm pretrains a CNN using a large set of videos with trackingground-truths to obtain a generic target representation. Our network iscomposed of shared layers and multiple branches of domain-specific layers,where domains correspond to individual training sequences and each branch isresponsible for binary classification to identify the target in each domain. Wetrain the network with respect to each domain iteratively to obtain generictarget representations in the shared layers. When tracking a target in a newsequence, we construct a new network by combining the shared layers in thepretrained CNN with a new binary classification layer, which is updated online.Online tracking is performed by evaluating the candidate windows randomlysampled around the previous target state. The proposed algorithm illustratesoutstanding performance compared with state-of-the-art methods in existingtracking benchmarks.
arxiv-1510-08108 | Online Learning with Gaussian Payoffs and Side Observations |  http://arxiv.org/abs/1510.08108  | author:Yifan Wu, András György, Csaba Szepesvári category:stat.ML cs.LG published:2015-10-27 summary:We consider a sequential learning problem with Gaussian payoffs and sideinformation: after selecting an action $i$, the learner receives informationabout the payoff of every action $j$ in the form of Gaussian observations whosemean is the same as the mean payoff, but the variance depends on the pair$(i,j)$ (and may be infinite). The setup allows a more refined informationtransfer from one action to another than previous partial monitoring setups,including the recently introduced graph-structured feedback case. For the firsttime in the literature, we provide non-asymptotic problem-dependent lowerbounds on the regret of any algorithm, which recover existing asymptoticproblem-dependent lower bounds and finite-time minimax lower bounds availablein the literature. We also provide algorithms that achieve theproblem-dependent lower bound (up to some universal constant factor) or theminimax lower bounds (up to logarithmic factors).
arxiv-1510-07867 | Some like it hot - visual guidance for preference prediction |  http://arxiv.org/abs/1510.07867  | author:Rasmus Rothe, Radu Timofte, Luc Van Gool category:cs.CV published:2015-10-27 summary:For people first impressions of someone are of determining importance. Theyare hard to alter through further information. This begs the question if acomputer can reach the same judgement. Earlier research has already pointed outthat age, gender, and average attractiveness can be estimated with reasonableprecision. We improve the state-of-the-art, but also predict - based onsomeone's known preferences - how much that particular person is attracted to anovel face. Our computational pipeline comprises a face detector, convolutionalneural networks for the extraction of deep features, standard support vectorregression for gender, age and facial beauty, and - as the main novelties -visual regularized collaborative filtering to infer inter-person preferences aswell as a novel regression technique for handling visual queries without ratinghistory. We validate the method using a very large dataset from a dating siteas well as images from celebrities. Our experiments yield convincing results,i.e. we predict 76% of the ratings correctly solely based on an image, andreveal some sociologically relevant conclusions. We also validate ourcollaborative filtering solution on the standard MovieLens rating dataset,augmented with movie posters, to predict an individual's movie rating. Wedemonstrate our algorithms on howhot.io which went viral around the Internetwith more than 50 million pictures evaluated in the first month.
arxiv-1510-07786 | A Framework to Adjust Dependency Measure Estimates for Chance |  http://arxiv.org/abs/1510.07786  | author:Simone Romano, Nguyen Xuan Vinh, James Bailey, Karin Verspoor category:stat.ML published:2015-10-27 summary:Estimating the strength of dependency between two variables is fundamentalfor exploratory analysis and many other applications in data mining. Forexample: non-linear dependencies between two continuous variables can beexplored with the Maximal Information Coefficient (MIC); and categoricalvariables that are dependent to the target class are selected using Gini gainin random forests. Nonetheless, because dependency measures are estimated onfinite samples, the interpretability of their quantification and the accuracywhen ranking dependencies become challenging. Dependency estimates are notequal to 0 when variables are independent, cannot be compared if computed ondifferent sample size, and they are inflated by chance on variables with morecategories. In this paper, we propose a framework to adjust dependency measureestimates on finite samples. Our adjustments, which are simple and applicableto any dependency measure, are helpful in improving interpretability whenquantifying dependency and in improving accuracy on the task of rankingdependencies. In particular, we demonstrate that our approach enhances theinterpretability of MIC when used as a proxy for the amount of noise betweenvariables, and to gain accuracy when ranking variables during the splittingprocedure in random forests.
arxiv-1510-07740 | The Wilson Machine for Image Modeling |  http://arxiv.org/abs/1510.07740  | author:Saeed Saremi, Terrence J. Sejnowski category:stat.ML cs.CV cs.LG published:2015-10-27 summary:Learning the distribution of natural images is one of the hardest and mostimportant problems in machine learning. The problem remains open, because theenormous complexity of the structures in natural images spans all lengthscales. We break down the complexity of the problem and show that the hierarchyof structures in natural images fuels a new class of learning algorithms basedon the theory of critical phenomena and stochastic processes. We approach thisproblem from the perspective of the theory of critical phenomena, which wasdeveloped in condensed matter physics to address problems with infinitelength-scale fluctuations, and build a framework to integrate the criticalityof natural images into a learning algorithm. The problem is broken down bymapping images into a hierarchy of binary images, called bitplanes. In thisrepresentation, the top bitplane is critical, having fluctuations in structuresover a vast range of scales. The bitplanes below go through a gradualstochastic heating process to disorder. We turn this representation into adirected probabilistic graphical model, transforming the learning problem intothe unsupervised learning of the distribution of the critical bitplane and thesupervised learning of the conditional distributions for the remainingbitplanes. We learnt the conditional distributions by logistic regression in aconvolutional architecture. Conditioned on the critical binary image, thissimple architecture can generate large, natural-looking images, with manyshades of gray, without the use of hidden units, unprecedented in the studiesof natural images. The framework presented here is a major step in bringingcriticality and stochastic processes to machine learning and in studyingnatural image statistics.
arxiv-1510-07851 | Standards for language resources in ISO -- Looking back at 13 fruitful years |  http://arxiv.org/abs/1510.07851  | author:Laurent Romary category:cs.CL published:2015-10-27 summary:This paper provides an overview of the various projects carried out withinISO committee TC 37/SC 4 dealing with the management of language (digital)resources. On the basis of the technical experience gained in the committee andthe wider standardization landscape the paper identifies some possible trendsfor the future.
arxiv-1510-07748 | Computational models: Bottom-up and top-down aspects |  http://arxiv.org/abs/1510.07748  | author:Laurent Itti, Ali Borji category:cs.CV published:2015-10-27 summary:Computational models of visual attention have become popular over the pastdecade, we believe primarily for two reasons: First, models make testablepredictions that can be explored by experimentalists as well as theoreticians,second, models have practical and technological applications of interest to theapplied science and engineering communities. In this chapter, we take acritical look at recent attention modeling efforts. We focus on {\emcomputational models of attention} as defined by Tsotsos \& Rothenstein\shortcite{Tsotsos_Rothenstein11}: Models which can process any visual stimulus(typically, an image or video clip), which can possibly also be given some taskdefinition, and which make predictions that can be compared to human or animalbehavioral or physiological responses elicited by the same stimulus and task.Thus, we here place less emphasis on abstract models, phenomenological models,purely data-driven fitting or extrapolation models, or models specificallydesigned for a single task or for a restricted class of stimuli. Fortheoretical models, we refer the reader to a number of previous reviews thataddress attention theories and models more generally\cite{Itti_Koch01nrn,Paletta_etal05,Frintrop_etal10,Rothenstein_Tsotsos08,Gottlieb_Balan10,Toet11,Borji_Itti12pami}.
arxiv-1510-08039 | Hybrid One-Shot 3D Hand Pose Estimation by Exploiting Uncertainties |  http://arxiv.org/abs/1510.08039  | author:Georg Poier, Konstantinos Roditakis, Samuel Schulter, Damien Michel, Horst Bischof, Antonis A. Argyros category:cs.CV published:2015-10-27 summary:Model-based approaches to 3D hand tracking have been shown to perform well ina wide range of scenarios. However, they require initialisation and cannotrecover easily from tracking failures that occur due to fast hand motions.Data-driven approaches, on the other hand, can quickly deliver a solution, butthe results often suffer from lower accuracy or missing anatomical validitycompared to those obtained from model-based approaches. In this work we proposea hybrid approach for hand pose estimation from a single depth image. First, alearned regressor is employed to deliver multiple initial hypotheses for the 3Dposition of each hand joint. Subsequently, the kinematic parameters of a 3Dhand model are found by deliberately exploiting the inherent uncertainty of theinferred joint proposals. This way, the method provides anatomically valid andaccurate solutions without requiring manual initialisation or suffering fromtrack losses. Quantitative results on several standard datasets demonstratethat the proposed method outperforms state-of-the-art representatives of themodel-based, data-driven and hybrid paradigms.
arxiv-1510-07609 | Efficient Learning by Directed Acyclic Graph For Resource Constrained Prediction |  http://arxiv.org/abs/1510.07609  | author:Joseph Wang, Kirill Trapeznikov, Venkatesh Saligrama category:stat.ML cs.LG published:2015-10-26 summary:We study the problem of reducing test-time acquisition costs inclassification systems. Our goal is to learn decision rules that adaptivelyselect sensors for each example as necessary to make a confident prediction. Wemodel our system as a directed acyclic graph (DAG) where internal nodescorrespond to sensor subsets and decision functions at each node choose whetherto acquire a new sensor or classify using the available measurements. Thisproblem can be naturally posed as an empirical risk minimization over trainingdata. Rather than jointly optimizing such a highly coupled and non-convexproblem over all decision nodes, we propose an efficient algorithm motivated bydynamic programming. We learn node policies in the DAG by reducing the globalobjective to a series of cost sensitive learning problems. Our approach iscomputationally efficient and has proven guarantees of convergence to theoptimal system for a fixed architecture. In addition, we present an extensionto map other budgeted learning problems with large number of sensors to our DAGarchitecture and demonstrate empirical performance exceeding state-of-the-artalgorithms for data composed of both few and many sensors.
arxiv-1510-07586 | Parser for Abstract Meaning Representation using Learning to Search |  http://arxiv.org/abs/1510.07586  | author:Sudha Rao, Yogarshi Vyas, Hal Daume III, Philip Resnik category:cs.CL published:2015-10-26 summary:We develop a novel technique to parse English sentences into Abstract MeaningRepresentation (AMR) using SEARN, a Learning to Search approach, by modelingthe concept and the relation learning in a unified framework. We evaluate ourparser on multiple datasets from varied domains and show an absoluteimprovement of 2% to 6% over the state-of-the-art. Additionally we show thatusing the most frequent concept gives us a baseline that is stronger than thestate-of-the-art for concept prediction. We plan to release our parser forpublic use.
arxiv-1510-07389 | The Human Kernel |  http://arxiv.org/abs/1510.07389  | author:Andrew Gordon Wilson, Christoph Dann, Christopher G. Lucas, Eric P. Xing category:cs.LG cs.AI stat.ML published:2015-10-26 summary:Bayesian nonparametric models, such as Gaussian processes, provide acompelling framework for automatic statistical modelling: these models have ahigh degree of flexibility, and automatically calibrated complexity. However,automating human expertise remains elusive; for example, Gaussian processeswith standard kernels struggle on function extrapolation problems that aretrivial for human learners. In this paper, we create function extrapolationproblems and acquire human responses, and then design a kernel learningframework to reverse engineer the inductive biases of human learners across aset of behavioral experiments. We use the learned kernels to gain psychologicalinsights and to extrapolate in human-like ways that go beyond traditionalstationary and polynomial kernels. Finally, we investigate Occam's razor inhuman and Gaussian process based function learning.
arxiv-1510-07482 | Edge-Linear First-Order Dependency Parsing with Undirected Minimum Spanning Tree Inference |  http://arxiv.org/abs/1510.07482  | author:Effi Levi, Roi Reichart, Ari Rappoport category:cs.CL published:2015-10-26 summary:The run time complexity of state-of-the-art inference algorithms ingraph-based dependency parsing is super-linear in the number of input words(n). Recently, pruning algorithms for these models have shown to cut a largeportion of the graph edges, with minimal damage to the resulting parse trees.Solving the inference problem in run time complexity determined solely by thenumber of edges (m) is hence of obvious importance. We propose such an inference algorithm for first-order models, which encodesthe problem as a minimum spanning tree (MST) problem in an undirected graph.This allows us to utilize state-of-the-art undirected MST algorithms whose runtime is O(m) at expectation and with a very high probability. A directed parsetree is then inferred from the undirected MST and is subsequently improved withrespect to the directed parsing model through local greedy updates, both stepsrunning in O(n) time. In experiments with 18 languages, a variant of thefirst-order MSTParser (McDonald et al., 2005b) that employs our algorithmperforms very similarly to the original parser that runs an O(n^2) directed MSTinference.
arxiv-1510-07526 | Empirical Study on Deep Learning Models for Question Answering |  http://arxiv.org/abs/1510.07526  | author:Yang Yu, Wei Zhang, Chung-Wei Hang, Bing Xiang, Bowen Zhou category:cs.CL cs.AI cs.LG published:2015-10-26 summary:In this paper we explore deep learning models with memory component orattention mechanism for question answering task. We combine and compare threemodels, Neural Machine Translation, Neural Turing Machine, and Memory Networksfor a simulated QA data set. This paper is the first one that uses NeuralMachine Translation and Neural Turing Machines for solving QA tasks. Ourresults suggest that the combination of attention and memory have potential tosolve certain QA problem.
arxiv-1510-07471 | A Parallel algorithm for $\mathcal{X}$-Armed bandits |  http://arxiv.org/abs/1510.07471  | author:Cheng Chen, Shuang Liu, Zhihua Zhang, Wu-Jun Li category:stat.ML cs.LG published:2015-10-26 summary:The target of $\mathcal{X}$-armed bandit problem is to find the globalmaximum of an unknown stochastic function $f$, given a finite budget of $n$evaluations. Recently, $\mathcal{X}$-armed bandits have been widely used inmany situations. Many of these applications need to deal with large-scale datasets. To deal with these large-scale data sets, we study a distributed settingof $\mathcal{X}$-armed bandits, where $m$ players collaborate to find themaximum of the unknown function. We develop a novel anytime distributed$\mathcal{X}$-armed bandit algorithm. Compared with prior work on$\mathcal{X}$-armed bandits, our algorithm uses a quite different searchingstrategy so as to fit distributed learning scenarios. Our theoretical analysisshows that our distributed algorithm is $m$ times faster than the classicalsingle-player algorithm. Moreover, the number of communication rounds of ouralgorithm is only logarithmic in $mn$. The numerical results show that ourmethod can make effective use of every players to minimize the loss. Thus, ourdistributed approach is attractive and useful.
arxiv-1510-07474 | A Markov Random Field and Active Contour Image Segmentation Model for Animal Spots Patterns |  http://arxiv.org/abs/1510.07474  | author:Alexander Gómez, German Díez, Jhony Giraldo, Augusto Salazar, Juan M. Daza category:cs.CV published:2015-10-26 summary:Non-intrusive biometrics of animals using images allows to analyze phenotypicpopulations and individuals with patterns like stripes and spots withoutaffecting the studied subjects. However, non-intrusive biometrics demand a welltrained subject or the development of computer vision algorithms that ease theidentification task. In this work, an analysis of classic segmentationapproaches that require a supervised tuning of their parameters such asthreshold, adaptive threshold, histogram equalization, and saturationcorrection is presented. In contrast, a general unsupervised algorithm usingMarkov Random Fields (MRF) for segmentation of spots patterns is proposed.Active contours are used to boost results using MRF output as seeds. As studysubject the Diploglossus millepunctatus lizard is used. The proposed methodachieved a maximum efficiency of $91.11\%$.
arxiv-1510-07493 | Aggregating Deep Convolutional Features for Image Retrieval |  http://arxiv.org/abs/1510.07493  | author:Artem Babenko, Victor Lempitsky category:cs.CV published:2015-10-26 summary:Several recent works have shown that image descriptors produced by deepconvolutional neural networks provide state-of-the-art performance for imageclassification and retrieval problems. It has also been shown that theactivations from the convolutional layers can be interpreted as local featuresdescribing particular image regions. These local features can be aggregatedusing aggregation approaches developed for local features (e.g. Fishervectors), thus providing new powerful global descriptors. In this paper we investigate possible ways to aggregate local deep featuresto produce compact global descriptors for image retrieval. First, we show thatdeep features and traditional hand-engineered features have quite differentdistributions of pairwise similarities, hence existing aggregation methods haveto be carefully re-evaluated. Such re-evaluation reveals that in contrast toshallow features, the simple aggregation method based on sum pooling providesarguably the best performance for deep convolutional features. This method isefficient, has few parameters, and bears little risk of overfitting when e.g.learning the PCA matrix. Overall, the new compact global descriptor improvesthe state-of-the-art on four common benchmarks considerably.
arxiv-1510-07439 | Object Oriented Analysis using Natural Language Processing concepts: A Review |  http://arxiv.org/abs/1510.07439  | author:Abinash Tripathy, Santanu Kumar Rath category:cs.SE cs.CL published:2015-10-26 summary:The Software Development Life Cycle (SDLC) starts with eliciting requirementsof the customers in the form of Software Requirement Specification (SRS). SRSdocument needed for software development is mostly written in NaturalLanguage(NL) convenient for the client. From the SRS document only, the classname, its attributes and the functions incorporated in the body of the classare traced based on pre-knowledge of analyst. The paper intends to present areview on Object Oriented (OO) analysis using Natural Language Processing (NLP)techniques. This analysis can be manual where domain expert helps to generatethe required diagram or automated system, where the system generates therequired diagram, from the input in the form of SRS.
arxiv-1510-07545 | Using Shortlists to Support Decision Making and Improve Recommender System Performance |  http://arxiv.org/abs/1510.07545  | author:Tobias Schnabel, Paul N. Bennett, Susan T. Dumais, Thorsten Joachims category:cs.HC cs.IR cs.LG published:2015-10-26 summary:In this paper, we study shortlists as an interface component for recommendersystems with the dual goal of supporting the user's decision process, as wellas improving implicit feedback elicitation for increased recommendationquality. A shortlist is a temporary list of candidates that the user iscurrently considering, e.g., a list of a few movies the user is currentlyconsidering for viewing. From a cognitive perspective, shortlists serve asdigital short-term memory where users can off-load the items underconsideration -- thereby decreasing their cognitive load. From a machinelearning perspective, adding items to the shortlist generates a new implicitfeedback signal as a by-product of exploration and decision making which canimprove recommendation quality. Shortlisting therefore provides additional datafor training recommendation systems without the increases in cognitive loadthat requesting explicit feedback would incur. We perform an user study with a movie recommendation setup to compareinterfaces that offer shortlist support with those that do not. From the userstudies we conclude: (i) users make better decisions with a shortlist; (ii)users prefer an interface with shortlist support; and (iii) the additionalimplicit feedback from sessions with a shortlist improves the quality ofrecommendations by nearly a factor of two.
arxiv-1510-08713 | How good is good enough? Re-evaluating the bar for energy disaggregation |  http://arxiv.org/abs/1510.08713  | author:Nipun Batra, Rishi Baijal, Amarjeet Singh, Kamin Whitehouse category:cs.LG published:2015-10-26 summary:Since the early 1980s, the research community has developed ever moresophisticated algorithms for the problem of energy disaggregation, but despitedecades of research, there is still a dearth of applications with demonstratedvalue. In this work, we explore a question that is highly pertinent to thisresearch community: how good does energy disaggregation need to be in order toinfer characteristics of a household? We present novel techniques that useunsupervised energy disaggregation to predict both household occupancy andstatic properties of the household such as size of the home and number ofoccupants. Results show that basic disaggregation approaches performs up to 30%better at occupancy estimation than using aggregate power data alone, and areup to 10% better at estimating static household characteristics. These resultsshow that even rudimentary energy disaggregation techniques are sufficient forimproved inference of household characteristics. To conclude, we re-evaluatethe bar set by the community for energy disaggregation accuracy and try toanswer the question "how good is good enough?"
arxiv-1510-07573 | Generalized Regressive Motion: a Visual Cue to Collision |  http://arxiv.org/abs/1510.07573  | author:Krzysztof Chalupka, Michael Dickinson, Pietro Perona category:cs.RO cs.CV cs.MA cs.SY published:2015-10-26 summary:Brains and sensory systems evolved to guide motion. Central to this task iscontrolling the approach to stationary obstacles and detecting movingorganisms. Looming has been proposed as the main monocular visual cue fordetecting the approach of other animals and avoiding collisions with stationaryobstacles. Elegant neural mechanisms for looming detection have been found inthe brain of insects and vertebrates. However, looming has not been analyzed inthe context of collisions between two moving animals. We propose an alternativestrategy, Generalized Regressive Motion (GRM), which is consistent withrecently observed behavior in fruit flies. Geometric analysis proves that GRMis a reliable cue to collision among conspecifics, whereas agent-based modelingsuggests that GRM is a better cue than looming as a means to detect approach,prevent collisions and maintain mobility.
arxiv-1510-07391 | Vehicle Color Recognition using Convolutional Neural Network |  http://arxiv.org/abs/1510.07391  | author:Reza Fuad Rachmadi, I Ketut Eddy Purnama category:cs.CV published:2015-10-26 summary:Vehicle color information is one of the important elements in ITS(Intelligent Traffic System). In this paper, we present a vehicle colorrecognition method using convolutional neural network (CNN). Naturally, CNN isdesigned to learn classification method based on shape information, but weproved that CNN can also learn classification based on color distribution. Inour method, we convert the input image to two different color spaces, HSV andCIE Lab, and run it to some CNN architecture. The training process followprocedure introduce by Krizhevsky, that learning rate is decreasing by factorof 10 after some iterations. To test our method, we use publicly vehicle colorrecognition dataset provided by Chen. The results, our model outperform theoriginal system provide by Chen with 2% higher overall accuracy.
arxiv-1510-07390 | Pan-Tilt Camera and PIR Sensor Fusion Based Moving Object Detection for Mobile Security Robots |  http://arxiv.org/abs/1510.07390  | author:YongChol Sin, MyongSong Choe, GyongIl Ryang category:cs.RO cs.CV published:2015-10-26 summary:One of fundamental issues for security robots is to detect and track peoplein the surroundings. The main problems of this task are real-time constraints,a changing background, varying illumination conditions and a non-rigid shape ofthe person to be tracked. In this paper, we propose a solution for trackingwith a pan-tilt camera and a passive infrared range (PIR) sensor to detect themoving object based on consecutive frame difference. The proposed method isexcellent in real-time performance because it requires only a little memory andcomputation. Experiment results show that this method can detect the movingobject such as human efficiently and accurately in non-stationary and complexindoor environment.
arxiv-1510-07385 | How to merge three different methods for information filtering ? |  http://arxiv.org/abs/1510.07385  | author:Jean-Valère Cossu, Ludovic Bonnefoy, Xavier Bost, Marc El Bèze category:cs.CL cs.IR published:2015-10-26 summary:Twitter is now a gold marketing tool for entities concerned with onlinereputation. To automatically monitor online reputation of entities , systemshave to deal with ambiguous entity names, polarity detection and topicdetection. We propose three approaches to tackle the first issue: monitoringTwitter in order to find relevant tweets about a given entity. Evaluated withinthe framework of the RepLab-2013 Filtering task, each of them has been showncompetitive with state-of-the-art approaches. Mainly we investigate on how muchmerging strategies may impact performances on a filtering task according to theevaluation measure.
arxiv-1510-07712 | Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks |  http://arxiv.org/abs/1510.07712  | author:Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, Wei Xu category:cs.CV published:2015-10-26 summary:We present an approach that exploits hierarchical Recurrent Neural Networks(RNNs) to tackle the video captioning problem, i.e., generating one or multiplesentences to describe a realistic video. Our hierarchical framework contains asentence generator and a paragraph generator. The sentence generator producesone simple short sentence that describes a specific short video interval. Itexploits both temporal- and spatial-attention mechanisms to selectively focuson visual elements during generation. The paragraph generator captures theinter-sentence dependency by taking as input the sentential embedding producedby the sentence generator, combining it with the paragraph history, andoutputting the new initial state for the sentence generator. We evaluate ourapproach on two large-scale benchmark datasets: YouTubeClips andTACoS-MultiLevel. The experiments demonstrate that our approach significantlyoutperforms the current state-of-the-art methods with BLEU@4 scores 0.499 and0.305 respectively.
arxiv-1511-02900 | Neighbourhood NILM: A Big-data Approach to Household Energy Disaggregation |  http://arxiv.org/abs/1511.02900  | author:Nipun Batra, Amarjeet Singh, Kamin Whitehouse category:cs.LG published:2015-10-26 summary:In this paper, we investigate whether "big-data" is more valuable than"precise" data for the problem of energy disaggregation: the process ofbreaking down aggregate energy usage on a per-appliance basis. Existingtechniques for disaggregation rely on energy metering at a resolution of 1minute or higher, but most power meters today only provide a reading once permonth, and at most once every 15 minutes. In this paper, we propose a newtechnique called Neighbourhood NILM that leverages data from 'neighbouring'homes to disaggregate energy given only a single energy reading per month. Thekey intuition behind our approach is that 'similar' homes have 'similar' energyconsumption on a per-appliance basis. Neighbourhood NILM matches every homewith a set of 'neighbours' that have direct submetering infrastructure, i.e.power meters on individual circuits or loads. Many such homes already exist.Then, it estimates the appliance-level energy consumption of the target home tobe the average of its K neighbours. We evaluate this approach using 25 homesand results show that our approach gives comparable or better disaggregation incomparison to state-of-the-art accuracy reported in the literature that dependon manual model training, high frequency power metering, or both. Results showthat Neighbourhood NILM can achieve 83% and 79% accuracy disaggregating fridgeand heating/cooling loads, compared to 74% and 73% for a technique called FHMM.Furthermore, it achieves up to 64% accuracy on washing machine, dryer,dishwasher, and lighting loads, which is higher than previously reportedresults. Many existing techniques are not able to disaggregate these loads atall. These results indicate a potentially substantial advantage to installingsubmetering infrastructure in a select few homes rather than installing newhigh-frequency smart metering infrastructure in all homes.
arxiv-1510-07641 | Phenotyping of Clinical Time Series with LSTM Recurrent Neural Networks |  http://arxiv.org/abs/1510.07641  | author:Zachary C. Lipton, David C. Kale, Randall C. Wetzell category:cs.LG published:2015-10-26 summary:We present a novel application of LSTM recurrent neural networks tomultilabel classification of diagnoses given variable-length time series ofclinical measurements. Our method outperforms a strong baseline on a variety ofmetrics.
arxiv-1510-07323 | Finding Temporally Consistent Occlusion Boundaries in Videos using Geometric Context |  http://arxiv.org/abs/1510.07323  | author:S. Hussain Raza, Ahmad Humayun, Matthias Grundmann, David Anderson, Irfan Essa category:cs.CV published:2015-10-25 summary:We present an algorithm for finding temporally consistent occlusionboundaries in videos to support segmentation of dynamic scenes. We learnocclusion boundaries in a pairwise Markov random field (MRF) framework. Wefirst estimate the probability of an spatio-temporal edge being an occlusionboundary by using appearance, flow, and geometric features. Next, we enforceocclusion boundary continuity in a MRF model by learning pairwise occlusionprobabilities using a random forest. Then, we temporally smooth boundaries toremove temporal inconsistencies in occlusion boundary estimation. Our proposedframework provides an efficient approach for finding temporally consistentocclusion boundaries in video by utilizing causality, redundancy in videos, andsemantic layout of the scene. We have developed a dataset with fully annotatedground-truth occlusion boundaries of over 30 videos ($5000 frames). Thisdataset is used to evaluate temporal occlusion boundaries and provides a muchneeded baseline for future studies. We perform experiments to demonstrate therole of scene layout, and temporal information for occlusion reasoning indynamic scenes.
arxiv-1510-07208 | Vehicle Speed Prediction using Deep Learning |  http://arxiv.org/abs/1510.07208  | author:Joe Lemieux, Yuan Ma category:cs.LG cs.NE published:2015-10-25 summary:Global optimization of the energy consumption of dual power source vehiclessuch as hybrid electric vehicles, plug-in hybrid electric vehicles, and plug infuel cell electric vehicles requires knowledge of the complete routecharacteristics at the beginning of the trip. One of the main characteristicsis the vehicle speed profile across the route. The profile will translatedirectly into energy requirements for a given vehicle. However, the vehiclespeed that a given driver chooses will vary from driver to driver and from timeto time, and may be slower, equal to, or faster than the average traffic flow.If the specific driver speed profile can be predicted, the energy usage can beoptimized across the route chosen. The purpose of this paper is to research theapplication of Deep Learning techniques to this problem to identify at thebeginning of a drive cycle the driver specific vehicle speed profile for anindividual driver repeated drive cycle, which can be used in an optimizationalgorithm to minimize the amount of fossil fuel energy used during the trip.
arxiv-1510-07320 | Geometric Context from Videos |  http://arxiv.org/abs/1510.07320  | author:S. Hussain Raza, Matthias Grundmann, Irfan Essa category:cs.CV published:2015-10-25 summary:We present a novel algorithm for estimating the broad 3D geometric structureof outdoor video scenes. Leveraging spatio-temporal video segmentation, wedecompose a dynamic scene captured by a video into geometric classes, based onpredictions made by region-classifiers that are trained on appearance andmotion features. By examining the homogeneity of the prediction, we combinepredictions across multiple segmentation hierarchy levels alleviating the needto determine the granularity a priori. We built a novel, extensive dataset ongeometric context of video to evaluate our method, consisting of over 100ground-truth annotated outdoor videos with over 20,000 frames. To further scalebeyond this dataset, we propose a semi-supervised learning framework to expandthe pool of labeled data with high confidence predictions obtained fromunlabeled data. Our system produces an accurate prediction of geometric contextof video achieving 96% accuracy across main geometric classes.
arxiv-1510-07211 | On End-to-End Program Generation from User Intention by Deep Neural Networks |  http://arxiv.org/abs/1510.07211  | author:Lili Mou, Rui Men, Ge Li, Lu Zhang, Zhi Jin category:cs.SE cs.LG published:2015-10-25 summary:This paper envisions an end-to-end program generation scenario usingrecurrent neural networks (RNNs): Users can express their intention in naturallanguage; an RNN then automatically generates corresponding code in acharacterby-by-character fashion. We demonstrate its feasibility through a casestudy and empirical analysis. To fully make such technique useful in practice,we also point out several cross-disciplinary challenges, including modelinguser intention, providing datasets, improving model architectures, etc.Although much long-term research shall be addressed in this new field, webelieve end-to-end program generation would become a reality in future decades,and we are looking forward to its practice.
arxiv-1510-07317 | Depth Extraction from Videos Using Geometric Context and Occlusion Boundaries |  http://arxiv.org/abs/1510.07317  | author:S. Hussain Raza, Omar Javed, Aveek Das, Harpreet Sawhney, Hui Cheng, Irfan Essa category:cs.CV published:2015-10-25 summary:We present an algorithm to estimate depth in dynamic video scenes. We proposeto learn and infer depth in videos from appearance, motion, occlusionboundaries, and geometric context of the scene. Using our method, depth can beestimated from unconstrained videos with no requirement of camera poseestimation, and with significant background/foreground motions. We start bydecomposing a video into spatio-temporal regions. For each spatio-temporalregion, we learn the relationship of depth to visual appearance, motion, andgeometric classes. Then we infer the depth information of new scenes usingpiecewise planar parametrization estimated within a Markov random field (MRF)framework by combining appearance to depth learned mappings and occlusionboundary guided smoothness constraints. Subsequently, we perform temporalsmoothing to obtain temporally consistent depth maps. To evaluate our depthestimation algorithm, we provide a novel dataset with ground truth depth foroutdoor video scenes. We present a thorough evaluation of our algorithm on ournew dataset and the publicly available Make3d static image dataset.
arxiv-1510-07303 | A Framework for Distributed Deep Learning Layer Design in Python |  http://arxiv.org/abs/1510.07303  | author:Clay McLeod category:cs.LG published:2015-10-25 summary:In this paper, a framework for testing Deep Neural Network (DNN) design inPython is presented. First, big data, machine learning (ML), and ArtificialNeural Networks (ANNs) are discussed to familiarize the reader with theimportance of such a system. Next, the benefits and detriments of implementingsuch a system in Python are presented. Lastly, the specifics of the system areexplained, and some experimental results are presented to prove theeffectiveness of the system.
arxiv-1510-07234 | Seam Puckering Objective Evaluation Method for Sewing Process |  http://arxiv.org/abs/1510.07234  | author:Raluca Brad, Eugen HĂloiu, Remus Brad category:cs.CV cs.CE published:2015-10-25 summary:The paper presents an automated method for the assessment and classificationof puckering defects detected during the preproduction control stage of thesewing machine or product inspection. In this respect, we have presented thepossible causes and remedies of the wrinkle nonconformities. Subjective factorsrelated to the control environment and operators during the seams evaluationcan be reduced using an automated system whose operation is based on imageprocessing. Our implementation involves spectral image analysis using Fouriertransform and an unsupervised neural network, the Kohonen Map, employed toclassify material specimens, the input images, into five discrete degrees ofquality, from grade 5 (best) to grade 1 (the worst).
arxiv-1510-07905 | Defect Detection Techniques for Airbag Production Sewing Stages |  http://arxiv.org/abs/1510.07905  | author:Raluca Brad, Lavinia Barac, Remus Brad category:cs.CV published:2015-10-25 summary:Airbags are subject to strict quality control in order to ensure passengerssafety. The quality of fabric and sewing thread influence the final product andtherefore, sewing defects must be early and accurately detected, in order toremove the item from production. Airbag seams assembly can take various forms,using linear and circle primitives, with threads of different colors and lengthdensities, creating lockstitch or double threads chainstitch. The paperpresents a framework for the automatic detection of defects occurring duringthe airbag sewing stage. Types of defects as skipped stitch, missed stitch orsuperimposed seam for lockstitch and two threads chainstitch are detected andmarked. Using image processing methods, the proposed framework follows theseams path and determines if a color pattern of the considered stitches isvalid.
arxiv-1510-07193 | Statistical Parsing by Machine Learning from a Classical Arabic Treebank |  http://arxiv.org/abs/1510.07193  | author:Kais Dukes category:cs.CL published:2015-10-25 summary:Research into statistical parsing for English has enjoyed over a decade ofsuccessful results. However, adapting these models to other languages has metwith difficulties. Previous comparative work has shown that Modern Arabic isone of the most difficult languages to parse due to rich morphology and freeword order. Classical Arabic is the ancient form of Arabic, and is understudiedin computational linguistics, relative to its worldwide reach as the languageof the Quran. The thesis is based on seven publications that make significantcontributions to knowledge relating to annotating and parsing Classical Arabic. A central argument of this thesis is that using a hybrid representationclosely aligned to traditional grammar leads to improved parsing for Arabic. Totest this hypothesis, two approaches are compared. As a reference, a puredependency parser is adapted using graph transformations, resulting in an87.47% F1-score. This is compared to an integrated parsing model with anF1-score of 89.03%, demonstrating that joint dependency-constituency parsing isbetter suited to Classical Arabic.
arxiv-1510-07182 | Computational models of attention |  http://arxiv.org/abs/1510.07182  | author:Laurent Itti, Ali Borji category:cs.CV published:2015-10-24 summary:This chapter reviews recent computational models of visual attention. Webegin with models for the bottom-up or stimulus-driven guidance of attention tosalient visual items, which we examine in seven different broad categories. Wethen examine more complex models which address the top-down or goal-orientedguidance of attention towards items that are more relevant to the task at hand.
arxiv-1510-07169 | Fast and Scalable Lasso via Stochastic Frank-Wolfe Methods with a Convergence Guarantee |  http://arxiv.org/abs/1510.07169  | author:Emanuele Frandi, Ricardo Nanculef, Stefano Lodi, Claudio Sartori, Johan A. K. Suykens category:stat.ML cs.LG math.OC published:2015-10-24 summary:Frank-Wolfe (FW) algorithms have been often proposed over the last few yearsas efficient solvers for a variety of optimization problems arising in thefield of Machine Learning. The ability to work with cheap projection-freeiterations and the incremental nature of the method make FW a very effectivechoice for many large-scale problems where computing a sparse model isdesirable. In this paper, we present a high-performance implementation of the FW methodtailored to solve large-scale Lasso regression problems, based on a randomizediteration, and prove that the convergence guarantees of the standard FW methodare preserved in the stochastic setting. We show experimentally that ouralgorithm outperforms several existing state of the art methods, including theCoordinate Descent algorithm by Friedman et al. (one of the fastest known Lassosolvers), on several benchmark datasets with a very large number of features,without sacrificing the accuracy of the model. Our results illustrate that thealgorithm is able to generate the complete regularization path on problems ofsize up to four million variables in less than one minute.
arxiv-1510-07136 | Image Parsing with a Wide Range of Classes and Scene-Level Context |  http://arxiv.org/abs/1510.07136  | author:Marian George category:cs.CV published:2015-10-24 summary:This paper presents a nonparametric scene parsing approach that improves theoverall accuracy, as well as the coverage of foreground classes in sceneimages. We first improve the label likelihood estimates at superpixels bymerging likelihood scores from different probabilistic classifiers. This booststhe classification performance and enriches the representation ofless-represented classes. Our second contribution consists of incorporatingsemantic context in the parsing process through global label costs. Our methoddoes not rely on image retrieval sets but rather assigns a global likelihoodestimate to each label, which is plugged into the overall energy function. Weevaluate our system on two large-scale datasets, SIFTflow and LMSun. We achievestate-of-the-art performance on the SIFTflow dataset and near-record results onLMSun.
arxiv-1510-07112 | Predicting Performance of a Face Recognition System Based on Image Quality |  http://arxiv.org/abs/1510.07112  | author:Abhishek Dutta category:cs.CV published:2015-10-24 summary:In this dissertation, we present a generative model to capture the relationbetween facial image quality features (like pose, illumination direction, etc)and face recognition performance. Such a model can be used to predict theperformance of a face recognition system. Since the model is based solely onimage quality features, performance predictions can be done even before theactual recognition has taken place thereby facilitating many preemptive action.A practical limitation of such a data driven generative model is the limitednature of training data set. To address this limitation, we have developed aBayesian approach to model the distribution of recognition performance measurebased on the number of match and non-match scores in small regions of the imagequality space. Random samples drawn from these models provide the initial dataessential for training the generative model. Experiment results based on sixface recognition systems operating on three independent data sets show that theproposed performance prediction model can accurately predict face recognitionperformance using an accurate and unbiased Image Quality Assessor (IQA).Furthermore, our results show that variability in the unaccounted quality space-- the image quality features not considered by the IQA -- is the major factorcausing inaccuracies in predicted performance.
arxiv-1510-07146 | Data-driven detrending of nonstationary fractal time series with echo state networks |  http://arxiv.org/abs/1510.07146  | author:Enrico Maiorino, Filippo Maria Bianchi, Lorenzo Livi, Antonello Rizzi, Alireza Sadeghian category:cs.LG cs.NE published:2015-10-24 summary:In this paper, we propose a data-driven approach to the problem of detrendingfractal and multifractal time series. We consider a time series as themeasurements elaborated from a dynamical process over time. We assume that sucha dynamical process is predictable to a certain degree, by means of a class ofrecurrent networks called echo state networks. Such networks have been shown tobe able to predict the outcome of a number of dynamical processes. Here wepropose to perform a data-driven detrending of nonstationary, fractal andmultifractal time series by using an echo state network operating as a filter.Notably, we predict the trend component of a given input time series, which issuperimposed to the (multi)fractal component of interest. Such a (estimated)trend is then removed from the original time series and the residual signal isanalyzed with the Multifractal Detrended Fluctuation Analysis for aquantitative verification of the correctness of the proposed detrendingprocedure. In order to demonstrate the effectiveness of the proposed technique,we consider several synthetic time series having a self-similar noise componentwith known characteristics. Such synthetic time series contain different typesof trends. We also process a real-world dataset, the sunspot time series, whichis well-known for its multifractal features and it has recently gainedattention in the complex systems field. Results demonstrate the validity andgenerality of the proposed detrending method based on echo state networks.
arxiv-1510-07099 | Combine CRF and MMSEG to Boost Chinese Word Segmentation in Social Media |  http://arxiv.org/abs/1510.07099  | author:Yao Yushi, Huang Zheng category:cs.CL published:2015-10-24 summary:In this paper, we propose a joint algorithm for the word segmentation onChinese social media. Previous work mainly focus on word segmentation for plainChinese text, in order to develop a Chinese social media processing tool, weneed to take the main features of social media into account, whose grammaticalstructure is not rigorous, and the tendency of using colloquial and Internetterms makes the existing Chinese-processing tools inefficient to obtain goodperformance on social media. In our approach, we combine CRF and MMSEG algorithm and extend features oftraditional CRF algorithm to train the model for word segmentation, We useInternet lexicon in order to improve the performance of our model on Chinesesocial media. Our experimental result on Sina Weibo shows that our approachoutperforms the state-of-the-art model.
arxiv-1510-07119 | Predicting Face Recognition Performance Using Image Quality |  http://arxiv.org/abs/1510.07119  | author:Abhishek Dutta, Raymond Veldhuis, Luuk Spreeuwers category:cs.CV published:2015-10-24 summary:This paper proposes a data driven model to predict the performance of a facerecognition system based on image quality features. We model the relationshipbetween image quality features (e.g. pose, illumination, etc.) and recognitionperformance measures using a probability density function. To address the issueof limited nature of practical training data inherent in most data drivenmodels, we have developed a Bayesian approach to model the distribution ofrecognition performance measures in small regions of the quality space. Sincethe model is based solely on image quality features, it can predict performanceeven before the actual recognition has taken place. We evaluate the performancepredictive capabilities of the proposed model for six face recognition systems(two commercial and four open source) operating on three independent data sets:MultiPIE, FRGC and CAS-PEAL. Our results show that the proposed model canaccurately predict performance using an accurate and unbiased Image QualityAssessor (IQA). Furthermore, our experiments highlight the impact of theunaccounted quality space -- the image quality features not considered by IQA-- in contributing to performance prediction errors.
arxiv-1510-07163 | Evolutionary Landscape and Management of Population Diversity |  http://arxiv.org/abs/1510.07163  | author:Maumita Bhattacharya category:cs.NE 68T99 published:2015-10-24 summary:The search ability of an Evolutionary Algorithm (EA) depends on the variationamong the individuals in the population [3, 4, 8]. Maintaining an optimal levelof diversity in the EA population is imperative to ensure that progress of theEA search is unhindered by premature convergence to suboptimal solutions.Clearer understanding of the concept of population diversity, in the context ofevolutionary search and premature convergence in particular, is the key todesigning efficient EAs. To this end, this paper first presents a briefanalysis of the EA population diversity issues. Next we present aninvestigation on a counter-niching EA technique [4] that introduces andmaintains constructive diversity in the population. The proposed approach usesinformed genetic operations to reach promising, but unexplored orunder-explored areas of the search space, while discouraging premature localconvergence. Simulation runs on a suite of standard benchmark test functionswith Genetic Algorithm (GA) implementation shows promising results.
arxiv-1510-06895 | Nonconvex Nonsmooth Low-Rank Minimization via Iteratively Reweighted Nuclear Norm |  http://arxiv.org/abs/1510.06895  | author:Canyi Lu, Jinhui Tang, Shuicheng Yan, Zhouchen Lin category:cs.LG cs.CV cs.NA published:2015-10-23 summary:The nuclear norm is widely used as a convex surrogate of the rank function incompressive sensing for low rank matrix recovery with its applications in imagerecovery and signal processing. However, solving the nuclear norm based relaxedconvex problem usually leads to a suboptimal solution of the original rankminimization problem. In this paper, we propose to perform a family ofnonconvex surrogates of $L_0$-norm on the singular values of a matrix toapproximate the rank function. This leads to a nonconvex nonsmooth minimizationproblem. Then we propose to solve the problem by Iteratively Reweighted NuclearNorm (IRNN) algorithm. IRNN iteratively solves a Weighted Singular ValueThresholding (WSVT) problem, which has a closed form solution due to thespecial properties of the nonconvex surrogate functions. We also extend IRNN tosolve the nonconvex problem with two or more blocks of variables. In theory, weprove that IRNN decreases the objective function value monotonically, and anylimit point is a stationary point. Extensive experiments on both synthesizeddata and real images demonstrate that IRNN enhances the low-rank matrixrecovery compared with state-of-the-art convex algorithms.
arxiv-1510-06920 | On the complexity of switching linear regression |  http://arxiv.org/abs/1510.06920  | author:Fabien Lauer category:stat.ML cs.CC cs.LG published:2015-10-23 summary:This technical note extends recent results on the computational complexity ofglobally minimizing the error of piecewise-affine models to the related problemof minimizing the error of switching linear regression models. In particular,we show that, on the one hand the problem is NP-hard, but on the other hand, itadmits a polynomial-time algorithm with respect to the number of data for anyfixed data dimension and number of modes.
arxiv-1510-07035 | Fast Latent Variable Models for Inference and Visualization on Mobile Devices |  http://arxiv.org/abs/1510.07035  | author:Joseph W Robinson, Aaron Q Li category:cs.LG cs.CL cs.DC cs.IR published:2015-10-23 summary:In this project we outline Vedalia, a high performance distributed networkfor performing inference on latent variable models in the context of Amazonreview visualization. We introduce a new model, RLDA, which extends LatentDirichlet Allocation (LDA) [Blei et al., 2003] for the review space byincorporating auxiliary data available in online reviews to improve modelingwhile simultaneously remaining compatible with pre-existing fast samplingtechniques such as [Yao et al., 2009; Li et al., 2014a] to achieve highperformance. The network is designed such that computation is efficientlyoffloaded to the client devices using the Chital system [Robinson & Li, 2015],improving response times and reducing server costs. The resulting system isable to rapidly compute a large number of specialized latent variable modelswhile requiring minimal server resources.
arxiv-1510-06807 | Learning in the Rational Speech Acts Model |  http://arxiv.org/abs/1510.06807  | author:Will Monroe, Christopher Potts category:cs.CL published:2015-10-23 summary:The Rational Speech Acts (RSA) model treats language use as a recursiveprocess in which probabilistic speaker and listener agents reason about eachother's intentions to enrich the literal semantics of their language alongbroadly Gricean lines. RSA has been shown to capture many kinds ofconversational implicature, but it has been criticized as an unrealistic modelof speakers, and it has so far required the manual specification of a semanticlexicon, preventing its use in natural language processing applications thatlearn lexical knowledge from data. We address these concerns by showing how todefine and optimize a trained statistical classifier that uses the intermediateagents of RSA as hidden layers of representation forming a non-linearactivation function. This treatment opens up new application domains and newpossibilities for learning effectively from data. We validate the model on areferential expression generation task, showing that the best performance isachieved by incorporating features approximating well-established insightsabout natural language generation into RSA.
arxiv-1510-06939 | Objects2action: Classifying and localizing actions without any video example |  http://arxiv.org/abs/1510.06939  | author:Mihir Jain, Jan C. van Gemert, Thomas Mensink, Cees G. M. Snoek category:cs.CV published:2015-10-23 summary:The goal of this paper is to recognize actions in video without the need forexamples. Different from traditional zero-shot approaches we do not demand thedesign and specification of attribute classifiers and class-to-attributemappings to allow for transfer from seen classes to unseen classes. Our keycontribution is objects2action, a semantic word embedding that is spanned by askip-gram model of thousands of object categories. Action labels are assignedto an object encoding of unseen video based on a convex combination of actionand object affinities. Our semantic embedding has three main characteristics toaccommodate for the specifics of actions. First, we propose a mechanism toexploit multiple-word descriptions of actions and objects. Second, weincorporate the automated selection of the most responsive objects per action.And finally, we demonstrate how to extend our zero-shot approach to thespatio-temporal localization of actions in video. Experiments on four actiondatasets demonstrate the potential of our approach.
arxiv-1510-07025 | Modeling User Exposure in Recommendation |  http://arxiv.org/abs/1510.07025  | author:Dawen Liang, Laurent Charlin, James McInerney, David M. Blei category:stat.ML cs.IR cs.LG published:2015-10-23 summary:Collaborative filtering analyzes user preferences for items (e.g., books,movies, restaurants, academic papers) by exploiting the similarity patternsacross users. In implicit feedback settings, all the items, including the onesthat a user did not consume, are taken into consideration. But this assumptiondoes not accord with the common sense understanding that users have a limitedscope and awareness of items. For example, a user might not have heard of acertain paper, or might live too far away from a restaurant to experience it.In the language of causal analysis, the assignment mechanism (i.e., the itemsthat a user is exposed to) is a latent variable that may change for varioususer/item combinations. In this paper, we propose a new probabilistic approachthat directly incorporates user exposure to items into collaborative filtering.The exposure is modeled as a latent variable and the model infers its valuefrom data. In doing so, we recover one of the most successful state-of-the-artapproaches as a special case of our model, and provide a plug-in method forconditioning exposure on various forms of exposure covariates (e.g., topics intext, venue locations). We show that our scalable inference algorithmoutperforms existing benchmarks in four different domains both with and withoutexposure covariates.
arxiv-1510-06925 | Confusing Deep Convolution Networks by Relabelling |  http://arxiv.org/abs/1510.06925  | author:Leigh Robinson, Benjamin Graham category:cs.CV cs.NE published:2015-10-23 summary:Deep convolutional neural networks have become the gold standard for imagerecognition tasks, demonstrating many current state-of-the-art results and evenachieving near-human level performance on some tasks. Despite this fact it hasbeen shown that their strong generalisation qualities can be fooled tomisclassify previously correctly classified natural images and give erroneoushigh confidence classifications to nonsense synthetic images. In this paper weextend that work, by presenting a straightforward way to perturb an image insuch a way as to cause it to acquire any other label from within the datasetwhile leaving this perturbed image visually indistinguishable from theoriginal.
arxiv-1510-06915 | Semi-Automatic Segmentation of Autosomal Dominant Polycystic Kidneys using Random Forests |  http://arxiv.org/abs/1510.06915  | author:Kanishka Sharma, Loic Peter, Christian Rupprecht, Anna Caroli, Lichao Wang, Andrea Remuzzi, Maximilian Baust, Nassir Navab category:cs.CV published:2015-10-23 summary:This paper presents a method for 3D segmentation of kidneys from patientswith autosomal dominant polycystic kidney disease (ADPKD) and severe renalinsufficiency, using computed tomography (CT) data. ADPKD severely alters theshape of the kidneys due to non-uniform formation of cysts. As a consequence,fully automatic segmentation of such kidneys is very challenging. We present asegmentation method with minimal user interaction based on a random forestclassifier. One of the major novelties of the proposed approach is the usage ofgeodesic distance volumes as additional source of information. These volumescontain the intensity weighted distance to a manual outline of the respectivekidney in only one slice (for each kidney) of the CT volume. We evaluate ourmethod qualitatively and quantitatively on 55 CT acquisitions using groundtruth annotations from clinical experts.
arxiv-1510-06646 | A 'Gibbs-Newton' Technique for Enhanced Inference of Multivariate Polya Parameters and Topic Models |  http://arxiv.org/abs/1510.06646  | author:Osama Khalifa, David Wolfe Corne, Mike Chantler category:cs.LG cs.CL stat.ML published:2015-10-22 summary:Hyper-parameters play a major role in the learning and inference process oflatent Dirichlet allocation (LDA). In order to begin the LDA latent variableslearning process, these hyper-parameters values need to be pre-determined. Wepropose an extension for LDA that we call 'Latent Dirichlet allocation GibbsNewton' (LDA-GN), which places non-informative priors over thesehyper-parameters and uses Gibbs sampling to learn appropriate values for them.At the heart of LDA-GN is our proposed 'Gibbs-Newton' algorithm, which is a newtechnique for learning the parameters of multivariate Polya distributions. Wereport Gibbs-Newton performance results compared with two prominent existingapproaches to the latter task: Minka's fixed-point iteration method and theMoments method. We then evaluate LDA-GN in two ways: (i) by comparing it withstandard LDA in terms of the ability of the resulting topic models togeneralize to unseen documents; (ii) by comparing it with standard LDA in itsperformance on a binary classification task.
arxiv-1510-06706 | ZNN - A Fast and Scalable Algorithm for Training 3D Convolutional Networks on Multi-Core and Many-Core Shared Memory Machines |  http://arxiv.org/abs/1510.06706  | author:Aleksandar Zlateski, Kisuk Lee, H. Sebastian Seung category:cs.NE cs.CV cs.DC cs.LG published:2015-10-22 summary:Convolutional networks (ConvNets) have become a popular approach to computervision. It is important to accelerate ConvNet training, which iscomputationally costly. We propose a novel parallel algorithm based ondecomposition into a set of tasks, most of which are convolutions or FFTs.Applying Brent's theorem to the task dependency graph implies that linearspeedup with the number of processors is attainable within the PRAM model ofparallel computation, for wide network architectures. To attain suchperformance on real shared-memory machines, our algorithm computes convolutionsconverging on the same node of the network with temporal locality to reducecache misses, and sums the convergent convolution outputs via an almostwait-free concurrent method to reduce time spent in critical sections. Weimplement the algorithm with a publicly available software package called ZNN.Benchmarking with multi-core CPUs shows that ZNN can attain speedup roughlyequal to the number of physical cores. We also show that ZNN can attain over90x speedup on a many-core CPU (Xeon Phi Knights Corner). These speedups areachieved for network architectures with widths that are in common use. The taskparallelism of the ZNN algorithm is suited to CPUs, while the SIMD parallelismof previous algorithms is compatible with GPUs. Through examples, we show thatZNN can be either faster or slower than certain GPU implementations dependingon specifics of the network architecture, kernel sizes, and density and size ofthe output patch. ZNN may be less costly to develop and maintain, due to therelative ease of general-purpose CPU programming.
arxiv-1510-06688 | Partitioning Data on Features or Samples in Communication-Efficient Distributed Optimization? |  http://arxiv.org/abs/1510.06688  | author:Chenxin Ma, Martin Takáč category:math.OC cs.LG published:2015-10-22 summary:In this paper we study the effect of the way that the data is partitioned indistributed optimization. The original DiSCO algorithm [Communication-EfficientDistributed Optimization of Self-Concordant Empirical Loss, Yuchen Zhang andLin Xiao, 2015] partitions the input data based on samples. We describe how theoriginal algorithm has to be modified to allow partitioning on features andshow its efficiency both in theory and also in practice.
arxiv-1510-06684 | Dual Free SDCA for Empirical Risk Minimization with Adaptive Probabilities |  http://arxiv.org/abs/1510.06684  | author:Xi He, Martin Takáč category:math.OC cs.LG published:2015-10-22 summary:In this paper we develop dual free SDCA with adaptive probabilities forregularized empirical risk minimization. This extends recent work of ShaiShalev-Shwartz [SDCA without Duality, arXiv:1502.06177] to allow non-uniformselection of "dual" coordinate in SDCA. Moreover, the probability can changeover time, making it more efficient than uniform selection. Our work focuses ongenerating adaptive probabilities through iterative process, preferring tochoose coordinate with highest potential to decrease sub-optimality. We alsopropose a practical variant Algorithm adfSDCA+ which is more aggressive. Thework is concluded with multiple experiments which shows efficiency of proposedalgorithms.
arxiv-1510-06595 | Efficient Unsupervised Temporal Segmentation of Motion Data |  http://arxiv.org/abs/1510.06595  | author:Björn Krüger, Anna Vögele, Tobias Willig, Angela Yao, Reinhard Klein, Andreas Weber category:cs.CV published:2015-10-22 summary:We introduce a method for automated temporal segmentation of human motiondata into distinct actions and compositing motion primitives based onself-similar structures in the motion sequence. We use neighbourhood graphs forthe partitioning and the similarity information in the graph is furtherexploited to cluster the motion primitives into larger entities of semanticsignificance. The method requires no assumptions about the motion sequences athand and no user interaction is required for the segmentation or clustering. Inaddition, we introduce a feature bundling preprocessing technique to make thesegmentation more robust to noise, as well as a notion of motion symmetry formore refined primitive detection. We test our method on several sensormodalities, including markered and markerless motion capture as well as onelectromyograph and accelerometer recordings. The results highlight oursystem's capabilities for both segmentation and for analysis of the finerstructures of motion data, all in a completely unsupervised manner.
arxiv-1510-06582 | Collective Prediction of Individual Mobility Traces with Exponential Weights |  http://arxiv.org/abs/1510.06582  | author:Bartosz Hawelka, Izabela Sitko, Pavlos Kazakopoulos, Euro Beinat category:physics.soc-ph cs.CY cs.LG stat.ML published:2015-10-22 summary:We present and test a sequential learning algorithm for the short-termprediction of human mobility. This novel approach pairs the Exponential Weightsforecaster with a very large ensemble of experts. The experts are individualsequence prediction algorithms constructed from the mobility traces of 10million roaming mobile phone users in a European country. Average predictionaccuracy is significantly higher than that of individual sequence predictionalgorithms, namely constant order Markov models derived from the user's owndata, that have been shown to achieve high accuracy in previous studies ofhuman mobility prediction. The algorithm uses only time stamped location data,and accuracy depends on the completeness of the expert ensemble, which shouldcontain redundant records of typical mobility patterns. The proposed algorithmis applicable to the prediction of any sufficiently large dataset of sequences.
arxiv-1510-06567 | Generalized conditional gradient: analysis of convergence and applications |  http://arxiv.org/abs/1510.06567  | author:Alain Rakotomamonjy, Rémi Flamary, Nicolas Courty category:cs.LG math.OC stat.ML published:2015-10-22 summary:The objectives of this technical report is to provide additional results onthe generalized conditional gradient methods introduced by Bredies et al.[BLM05]. Indeed , when the objective function is smooth, we provide a novelcertificate of optimality and we show that the algorithm has a linearconvergence rate. Applications of this algorithm are also discussed.
arxiv-1510-06549 | Multi-GPU Distributed Parallel Bayesian Differential Topic Modelling |  http://arxiv.org/abs/1510.06549  | author:Aaron Q Li category:cs.CL cs.DC cs.LG published:2015-10-22 summary:There is an explosion of data, documents, and other content, and peoplerequire tools to analyze and interpret these, tools to turn the content intoinformation and knowledge. Topic modeling have been developed to solve theseproblems. Topic models such as LDA [Blei et. al. 2003] allow salient patternsin data to be extracted automatically. When analyzing texts, these patterns arecalled topics. Among numerous extensions of LDA, few of them can reliablyanalyze multiple groups of documents and extract topic similarities. Recently,the introduction of differential topic modeling (SPDP) [Chen et. al. 2012]performs uniformly better than many topic models in a discriminative setting. There is also a need to improve the sampling speed for topic models. Whilesome effort has been made for distributed algorithms, there is no workcurrently done using graphical processing units (GPU). Note the GPU frameworkhas already become the most cost-efficient platform for many problems. In this thesis, I propose and implement a scalable multi-GPU distributedparallel framework which approximates SPDP. Through experiments, I have shownmy algorithms have a gain in speed of about 50 times while being almost asaccurate, with only one single cheap laptop GPU. Furthermore, I have shown thespeed improvement is sublinearly scalable when multiple GPUs are used, whilefairly maintaining the accuracy. Therefore on a medium-sized GPU cluster, thespeed improvement could potentially reach a factor of a thousand. Note SPDP is just a representative of other extensions of LDA. Although myalgorithm is implemented to work with SPDP, it is designed to be a generalenough to work with other topic models. The speed-up on smaller collections(i.e., 1000s of documents), means that these more complex LDA extensions couldnow be done in real-time, thus opening up a new way of using these LDA modelsin industry.
arxiv-1510-06767 | Order-Fractal transition in abstract paintings |  http://arxiv.org/abs/1510.06767  | author:E. M. De la Calleja, F. Cervantes, J. De la Calleja category:cs.CV published:2015-10-22 summary:We report the degree of order of twenty-two Jackson Pollock's paintings using\emph{Hausdorff-Besicovitch fractal dimension}. Through the maximum value ofeach multi-fractal spectrum, the artworks are classify by the year in whichthey were painted. It has been reported that Pollock's paintings are fractaland it increased on his latest works. However our results show that fractaldimension of the paintings are on a range of fractal dimension with valuesclose to two. We identify this behavior as a fractal-order transition. Based onthe study of disorder-order transition in physical systems, we interpreted thefractal-order transition through its dark paint strokes in Pollocks' paintings,as structured lines following a power law measured by fractal dimension. Weobtain self-similarity in some specific Pollock's paintings, that reveal animportant dependence on the scale of observation. We also characterize by itsfractal spectrum, the called \emph{Teri's Find}. We obtained similar spectrumsbetween \emph{Teri's Find} and \emph{Number 5} from Pollock, suggesting thatfractal dimension cannot be completely rejected as a quantitative parameter toauthenticate this kind of artworks.
arxiv-1510-06507 | Modelling, Measuring and Compensating Color Weak Vision |  http://arxiv.org/abs/1510.06507  | author:Satoshi Oshima, Rica Mochizuki, Reiner Lenz, Jinhui Chao category:cs.CV published:2015-10-22 summary:We use methods from Riemann geometry to investigate transformations betweenthe color spaces of color-normal and color weak observers. The two mainapplications are the simulation of the perception of a color weak observer fora color normal observer and the compensation of color images in a way that acolor weak observer has approximately the same perception as a color normalobserver. The metrics in the color spaces of interest are characterized withthe help of ellipsoids defined by the just-noticable-differences between colorwhich are measured with the help of color-matching experiments. The constructedmappings are isometries of Riemann spaces that preserve the perceivedcolor-differences for both observers. Among the two approaches to build such anisometry, we introduce normal coordinates in Riemann spaces as a tool toconstruct a global color-weak compensation map. Compared to previously usedmethods this method is free from approximation errors due to locallinearizations and it avoids the problem of shifting locations of the origin ofthe local coordinate system. We analyse the variations of the Riemann metricsfor different observers obtained from new color matching experiments anddescribe three variations of the basic method. The performance of the methodsis evaluated with the help of semantic differential (SD) tests.
arxiv-1510-06503 | Personalized Age Progression with Aging Dictionary |  http://arxiv.org/abs/1510.06503  | author:Xiangbo Shu, Jinhui Tang, Hanjiang Lai, Luoqi Liu, Shuicheng Yan category:cs.CV published:2015-10-22 summary:In this paper, we aim to automatically render aging faces in a personalizedway. Basically, a set of age-group specific dictionaries are learned, where thedictionary bases corresponding to the same index yet from differentdictionaries form a particular aging process pattern cross different agegroups, and a linear combination of these patterns expresses a particularpersonalized aging process. Moreover, two factors are taken into considerationin the dictionary learning process. First, beyond the aging dictionaries, eachsubject may have extra personalized facial characteristics, e.g. mole, whichare invariant in the aging process. Second, it is challenging or evenimpossible to collect faces of all age groups for a particular subject, yetmuch easier and more practical to get face pairs from neighboring age groups.Thus a personality-aware coupled reconstruction loss is utilized to learn thedictionaries based on face pairs from neighboring age groups. Extensiveexperiments well demonstrate the advantages of our proposed solution over otherstate-of-the-arts in term of personalized aging progression, as well as theperformance gain for cross-age face verification by synthesizing aging faces.
arxiv-1510-06492 | Generalized Shortest Path Kernel on Graphs |  http://arxiv.org/abs/1510.06492  | author:Linus Hermansson, Fredrik D. Johansson, Osamu Watanabe category:cs.DS cs.LG published:2015-10-22 summary:We consider the problem of classifying graphs using graph kernels. We definea new graph kernel, called the generalized shortest path kernel, based on thenumber and length of shortest paths between nodes. For our exampleclassification problem, we consider the task of classifying random graphs fromtwo well-known families, by the number of clusters they contain. We verifyempirically that the generalized shortest path kernel outperforms the originalshortest path kernel on a number of datasets. We give a theoretical analysisfor explaining our experimental results. In particular, we estimatedistributions of the expected feature vectors for the shortest path kernel andthe generalized shortest path kernel, and we show some evidence explaining whyour graph kernel outperforms the shortest path kernel for our graphclassification problem.
arxiv-1510-06779 | Cascaded High Dimensional Histograms: A Generative Approach to Density Estimation |  http://arxiv.org/abs/1510.06779  | author:Siong Thye Goh, Cynthia Rudin category:stat.ML 62 published:2015-10-22 summary:We present tree- and list- structured density estimation methods for highdimensional binary/categorical data. Our density estimation models are highdimensional analogies to variable bin width histograms. In each leaf of thetree (or list), the density is constant, similar to the flat density within thebin of a histogram. Histograms, however, cannot easily be visualized in higherdimensions, whereas our models can. The accuracy of histograms fades asdimensions increase, whereas our models have priors that help withgeneralization. Our models are sparse, unlike high-dimensional histograms. Wepresent three generative models, where the first one allows the user to specifythe number of desired leaves in the tree within a Bayesian prior. The secondmodel allows the user to specify the desired number of branches within theprior. The third model returns lists (rather than trees) and allows the user tospecify the desired number of rules and the length of rules within the prior.Our results indicate that the new approaches yield a better balance betweensparsity and accuracy of density estimates than other methods for this task.
arxiv-1510-06664 | Random Projections through multiple optical scattering: Approximating kernels at the speed of light |  http://arxiv.org/abs/1510.06664  | author:Alaa Saade, Francesco Caltagirone, Igor Carron, Laurent Daudet, Angélique Drémeau, Sylvain Gigan, Florent Krzakala category:cs.ET cs.LG physics.optics published:2015-10-22 summary:Random projections have proven extremely useful in many signal processing andmachine learning applications. However, they often require either to store avery large random matrix, or to use a different, structured matrix to reducethe computational and memory costs. Here, we overcome this difficulty byproposing an analog, optical device, that performs the random projectionsliterally at the speed of light without having to store any matrix in memory.This is achieved using the physical properties of multiple coherent scatteringof coherent light in random media. We use this device on a simple task ofclassification with a kernel machine, and we show that, on the MNIST database,the experimental results closely match the theoretical performance of thecorresponding kernel. This framework can help make kernel methods practical forapplications that have large training sets and/or require real-time prediction.We discuss possible extensions of the method in terms of a class of kernels,speed, memory consumption and different problems.
arxiv-1510-06463 | Inventory Control Involving Unknown Demand of Discrete Nonperishable Items - Analysis of a Newsvendor-based Policy |  http://arxiv.org/abs/1510.06463  | author:Michael N. Katehakis, Jian Yang, Tingting Zhou category:stat.ML published:2015-10-22 summary:Inventory control with unknown demand distribution is considered, withemphasis placed on the case involving discrete nonperishable items. We focus onan adaptive policy which in every period uses, as much as possible, the optimalnewsvendor ordering quantity for the empirical distribution learned up to thatperiod. The policy is assessed using the regret criterion, which measures theprice paid for ambiguity on demand distribution over $T$ periods. When thereare guarantees on the latter's separation from the critical newsvendorparameter $\beta=b/(h+b)$, a constant upper bound on regret can be found.Without any prior information on the demand distribution, we show that theregret does not grow faster than the rate $T^{1/2+\epsilon}$ for any$\epsilon>0$. In view of a known lower bound, this is almost the best one couldhope for. Simulation studies involving this along with other policies are alsoconducted.
arxiv-1510-06479 | Generic decoding of seen and imagined objects using hierarchical visual features |  http://arxiv.org/abs/1510.06479  | author:Tomoyasu Horikawa, Yukiyasu Kamitani category:q-bio.NC cs.CV published:2015-10-22 summary:Object recognition is a key function in both human and machine vision. Whilerecent studies have achieved fMRI decoding of seen and imagined contents, theprediction is limited to training examples. We present a decoding approach forarbitrary objects, using the machine vision principle that an object categoryis represented by a set of features rendered invariant through hierarchicalprocessing. We show that visual features including those from a convolutionalneural network can be predicted from fMRI patterns and that greater accuracy isachieved for low/high-level features with lower/higher-level visual areas,respectively. Predicted features are used to identify the target object(extending beyond decoder training) from a set of computed features fornumerous objects. Furthermore, we demonstrate the identification of imaginedobjects, suggesting the recruitment of intermediate image representations intop-down processing. Our results demonstrate a tight link between human andmachine vision and its utility for brain-based information retrieval.
arxiv-1510-06786 | Freshman or Fresher? Quantifying the Geographic Variation of Internet Language |  http://arxiv.org/abs/1510.06786  | author:Vivek Kulkarni, Bryan Perozzi, Steven Skiena category:cs.CL cs.IR cs.LG published:2015-10-22 summary:We present a new computational technique to detect and analyze statisticallysignificant geographic variation in language. Our meta-analysis approachcaptures statistical properties of word usage across geographical regions anduses statistical methods to identify significant changes specific to regions.While previous approaches have primarily focused on lexical variation betweenregions, our method identifies words that demonstrate semantic and syntacticvariation as well. We extend recently developed techniques for neural language models to learnword representations which capture differing semantics across geographicalregions. In order to quantify this variation and ensure robust detection oftrue regional differences, we formulate a null model to determine whetherobserved changes are statistically significant. Our method is the first suchapproach to explicitly account for random variation due to chance whiledetecting regional variation in word meaning. To validate our model, we study and analyze two different massive online datasets: millions of tweets from Twitter spanning not only four differentcountries but also fifty states, as well as millions of phrases contained inthe Google Book Ngrams. Our analysis reveals interesting facets of languagechange at multiple scales of geographic resolution -- from neighboring statesto distant continents. Finally, using our model, we propose a measure of semantic distance betweenlanguages. Our analysis of British and American English over a period of 100years reveals that semantic variation between these dialects is shrinking.
arxiv-1510-06356 | Application of Quantum Annealing to Training of Deep Neural Networks |  http://arxiv.org/abs/1510.06356  | author:Steven H. Adachi, Maxwell P. Henderson category:quant-ph cs.LG stat.ML published:2015-10-21 summary:In Deep Learning, a well-known approach for training a Deep Neural Networkstarts by training a generative Deep Belief Network model, typically usingContrastive Divergence (CD), then fine-tuning the weights using backpropagationor other discriminative techniques. However, the generative training can betime-consuming due to the slow mixing of Gibbs sampling. We investigated analternative approach that estimates model expectations of Restricted BoltzmannMachines using samples from a D-Wave quantum annealing machine. We tested thismethod on a coarse-grained version of the MNIST data set. In our tests we foundthat the quantum sampling-based training approach achieves comparable or betteraccuracy with significantly fewer iterations of generative training thanconventional CD-based training. Further investigation is needed to determinewhether similar improvements can be achieved for other data sets, and to whatextent these improvements can be attributed to quantum effects.
arxiv-1510-06096 | When Are Nonconvex Problems Not Scary? |  http://arxiv.org/abs/1510.06096  | author:Ju Sun, Qing Qu, John Wright category:math.OC cs.IT math.IT stat.ML published:2015-10-21 summary:In this note, we focus on smooth nonconvex optimization problems that obey:(1) all local minimizers are also global; and (2) around any saddle point orlocal maximizer, the objective has a negative directional curvature. Concreteapplications such as dictionary learning, generalized phase retrieval, andorthogonal tensor decomposition are known to induce such structures. Wedescribe a second-order trust-region algorithm that provably converges to aglobal minimizer efficiently, without special initializations. Finally wehighlight alternatives, and open problems in this direction.
arxiv-1510-06342 | Prevalence and recoverability of syntactic parameters in sparse distributed memories |  http://arxiv.org/abs/1510.06342  | author:Jeong Joon Park, Ronnel Boettcher, Andrew Zhao, Alex Mun, Kevin Yuh, Vibhor Kumar, Matilde Marcolli category:cs.CL cs.IT math.IT 91F20 published:2015-10-21 summary:We propose a new method, based on Sparse Distributed Memory (KanervaNetworks), for studying dependency relations between different syntacticparameters in the Principles and Parameters model of Syntax. We store data ofsyntactic parameters of world languages in a Kanerva Network and we check therecoverability of corrupted parameter data from the network. We find thatdifferent syntactic parameters have different degrees of recoverability. Weidentify two different effects: an overall underlying relation between theprevalence of parameters across languages and their degree of recoverability,and a finer effect that makes some parameters more easily recoverable beyondwhat their prevalence would indicate. We interpret a higher recoverability fora syntactic parameter as an indication of the existence of a dependencyrelation, through which the given parameter can be determined using theremaining uncorrupted data.
arxiv-1510-06299 | GLASSES: Relieving The Myopia Of Bayesian Optimisation |  http://arxiv.org/abs/1510.06299  | author:Javier González, Michael Osborne, Neil D. Lawrence category:stat.ML published:2015-10-21 summary:We present GLASSES: Global optimisation with Look-Ahead through StochasticSimulation and Expected-loss Search. The majority of global optimisationapproaches in use are myopic, in only considering the impact of the nextfunction value; the non-myopic approaches that do exist are able to consideronly a handful of future evaluations. Our novel algorithm, GLASSES, permits theconsideration of dozens of evaluations into the future. This is done byapproximating the ideal look-ahead loss function, which is expensive toevaluate, by a cheaper alternative in which the future steps of the algorithmare simulated beforehand. An Expectation Propagation algorithm is used tocompute the expected value of the loss.We show that the far-horizon planningthus enabled leads to substantive performance gains in empirical tests.
arxiv-1510-06143 | High Performance Latent Variable Models |  http://arxiv.org/abs/1510.06143  | author:Aaron Q. Li, Amr Ahmed, Mu Li, Vanja Josifovski category:cs.LG cs.AI published:2015-10-21 summary:Latent variable models have accumulated a considerable amount of interestfrom the industry and academia for their versatility in a wide range ofapplications. A large amount of effort has been made to develop systems that isable to extend the systems to a large scale, in the hope to make use of them onindustry scale data. In this paper, we describe a system that operates at ascale orders of magnitude higher than previous works, and an order of magnitudefaster than state-of-the-art system at the same scale, at the same time showingmore robustness and more accurate results. Our system uses a number of advances in distributed inference: highperformance in synchronization of sufficient statistics with relaxedconsistency model; fast sampling, using the Metropolis-Hastings-Walker methodto overcome dense generative models; statistical modeling, moving beyond LatentDirichlet Allocation (LDA) to Pitman-Yor distributions (PDP) and HierarchicalDirichlet Process (HDP) models; sophisticated parameter projection schemes, toresolve the conflicts within the constraint between parameters arising from therelaxed consistency model. This work significantly extends the domain of applicability of what iscommonly known as the Parameter Server. We obtain results with up to hundredsbillion oftokens, thousands of topics, and a vocabulary of a few milliontoken-types, using up to 60,000 processor cores operating on a productioncluster of a large Internet company. This demonstrates the feasibility to scaleto problems orders of magnitude larger than any previously published work.
arxiv-1510-06168 | Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network |  http://arxiv.org/abs/1510.06168  | author:Peilu Wang, Yao Qian, Frank K. Soong, Lei He, Hai Zhao category:cs.CL published:2015-10-21 summary:Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTM-RNN) hasbeen shown to be very effective for tagging sequential data, e.g. speechutterances or handwritten documents. While word embedding has been demoed as apowerful representation for characterizing the statistical properties ofnatural language. In this study, we propose to use BLSTM-RNN with wordembedding for part-of-speech (POS) tagging task. When tested on Penn TreebankWSJ test set, a state-of-the-art performance of 97.40 tagging accuracy isachieved. Without using morphological features, this approach can also achievea good performance comparable with the Stanford POS tagger.
arxiv-1512-04582 | Interactive Volumetry Of Liver Ablation Zones |  http://arxiv.org/abs/1512.04582  | author:Jan Egger, Harald Busse, Philipp Brandmaier, Daniel Seider, Matthias Gawlitza, Steffen Strocka, Philip Voglreiter, Mark Dokter, Michael Hofmann, Bernhard Kainz, Alexander Hann, Xiaojun Chen, Tuomas Alhonnoro, Mika Pollari, Dieter Schmalstieg, Michael Moche category:cs.CV cs.GR cs.HC physics.med-ph published:2015-10-21 summary:Percutaneous radiofrequency ablation (RFA) is a minimally invasive techniquethat destroys cancer cells by heat. The heat results from focusing energy inthe radiofrequency spectrum through a needle. Amongst others, this can enablethe treatment of patients who are not eligible for an open surgery. However,the possibility of recurrent liver cancer due to incomplete ablation of thetumor makes post-interventional monitoring via regular follow-up scansmandatory. These scans have to be carefully inspected for any conspicuousness.Within this study, the RF ablation zones from twelve post-interventional CTacquisitions have been segmented semi-automatically to support the visualinspection. An interactive, graph-based contouring approach, which prefersspherically shaped regions, has been applied. For the quantitative andqualitative analysis of the algorithm's results, manual slice-by-slicesegmentations produced by clinical experts have been used as the gold standard(which have also been compared among each other). As evaluation metric for thestatistical validation, the Dice Similarity Coefficient (DSC) has beencalculated. The results show that the proposed tool provides lesionsegmentation with sufficient accuracy much faster than manual segmentation. Thevisual feedback and interactivity make the proposed tool well suitable for theclinical workflow.
arxiv-1510-06423 | Optimization as Estimation with Gaussian Processes in Bandit Settings |  http://arxiv.org/abs/1510.06423  | author:Zi Wang, Bolei Zhou, Stefanie Jegelka category:stat.ML cs.LG published:2015-10-21 summary:Recently, there has been rising interest in Bayesian optimization -- theoptimization of an unknown function with assumptions usually expressed by aGaussian Process (GP) prior. We study an optimization strategy that directlyuses an estimate of the argmax of the function. This strategy offers bothpractical and theoretical advantages: no tradeoff parameter needs to beselected, and, moreover, we establish close connections to the popular GP-UCBand GP-PI strategies. Our approach can be understood as automatically andadaptively trading off exploration and exploitation in GP-UCB and GP-PI. Weillustrate the effects of this adaptive tuning via bounds on the regret as wellas an extensive empirical evaluation on robotics and vision tasks,demonstrating the robustness of this strategy for a range of performancecriteria.
arxiv-1510-06138 | Multiple co-clustering based on nonparametric mixture models with heterogeneous marginal distributions |  http://arxiv.org/abs/1510.06138  | author:Tomoki Tokuda, Junichiro Yoshimoto, Yu Shimizu, Shigeru Toki, Go Okada, Masahiro Takamura, Tetsuya Yamamoto, Shinpei Yoshimura, Yasumasa Okamoto, Shigeto Yamawaki, Kenji Doya category:stat.ML published:2015-10-21 summary:We propose a novel method for multiple clustering that assumes aco-clustering structure (partitions in both rows and columns of the datamatrix) in each view. The new method is applicable to high-dimensional data. Itis based on a nonparametric Bayesian approach in which the number of views andthe number of feature-/subject clusters are inferred in a data-driven manner.We simultaneously model different distribution families, such as Gaussian,Poisson, and multinomial distributions in each cluster block. This makes ourmethod applicable to datasets consisting of both numerical and categoricalvariables, which biomedical data typically do. Clustering solutions are basedon variational inference with mean field approximation. We apply the proposedmethod to synthetic and real data, and show that our method outperforms othermultiple clustering methods both in recovering true cluster structures and incomputation time. Finally, we apply our method to a depression dataset with notrue cluster structure available, from which useful inferences are drawn aboutpossible clustering structures of the data.
arxiv-1510-06112 | Dimensionality Reduction for Binary Data through the Projection of Natural Parameters |  http://arxiv.org/abs/1510.06112  | author:Andrew J. Landgraf, Yoonkyung Lee category:stat.ML stat.ME published:2015-10-21 summary:Principal component analysis (PCA) for binary data, known as logistic PCA,has become a popular alternative to dimensionality reduction of binary data. Itis motivated as an extension of ordinary PCA by means of a matrixfactorization, akin to the singular value decomposition, that maximizes theBernoulli log-likelihood. We propose a new formulation of logistic PCA whichextends Pearson's formulation of a low dimensional data representation withminimum error to binary data. Our formulation does not require a matrixfactorization, as previous methods do, but instead looks for projections of thenatural parameters from the saturated model. Due to this difference, the numberof parameters does not grow with the number of observations and the principalcomponent scores on new data can be computed with simple matrix multiplication.We derive explicit solutions for data matrices of special structure and providecomputationally efficient algorithms for solving for the principal componentloadings. Through simulation experiments and an analysis of medical diagnosesdata, we compare our formulation of logistic PCA to the previous formulation aswell as ordinary PCA to demonstrate its benefits.
arxiv-1510-06093 | Content adaptive screen image scaling |  http://arxiv.org/abs/1510.06093  | author:Yao Zhai, Qifei Wang, Yan Lu, Shipeng Li category:cs.CV published:2015-10-21 summary:This paper proposes an efficient content adaptive screen image scaling schemefor the real-time screen applications like remote desktop and screen sharing.In the proposed screen scaling scheme, a screen content classification step isfirst introduced to classify the screen image into text and pictorial regions.Afterward, we propose an adaptive shift linear interpolation algorithm topredict the new pixel values with the shift offset adapted to the content typeof each pixel. The shift offset for each screen content type is offlineoptimized by minimizing the theoretical interpolation error based on thetraining samples respectively. The proposed content adaptive screen imagescaling scheme can achieve good visual quality and also keep the low complexityfor real-time applications.
arxiv-1510-06223 | Predicting popularity of online videos using Support Vector Regression |  http://arxiv.org/abs/1510.06223  | author:Tomasz Trzcinski, Przemyslaw Rokita category:cs.SI cs.CV published:2015-10-21 summary:In this work, we propose a regression method to predict the popularity of anonline video based on temporal and visual cues. Our method uses Support VectorRegression with Gaussian Radial Basis Functions. We show that modellingpopularity patterns with this approach provides higher and more stableprediction results, mainly thanks to the non-linearity character of theproposed method as well as its resistance against overfitting. We compare ourmethod with the state of the art on datasets containing over 14,000 videos fromYouTube and Facebook. Furthermore, we show that results obtained relying onlyon the early distribution patterns, can be improved by adding social and visualmetadata.
arxiv-1510-06188 | Learning-based Compressive Subsampling |  http://arxiv.org/abs/1510.06188  | author:Luca Baldassarre, Yen-Huan Li, Jonathan Scarlett, Baran Gözcü, Ilija Bogunovic, Volkan Cevher category:cs.IT cs.LG math.IT stat.ML published:2015-10-21 summary:The problem of recovering a structured signal $\mathbf{x} \in \mathbb{C}^p$from a set of dimensionality-reduced linear measurements $\mathbf{b} = \mathbf{A}\mathbf {x}$ arises in a variety of applications, such as medical imaging,spectroscopy, Fourier optics, and computerized tomography. Due to computationaland storage complexity or physical constraints imposed by the problem, themeasurement matrix $\mathbf{A} \in \mathbb{C}^{n \times p}$ is often of theform $\mathbf{A} = \mathbf{P}_{\Omega}\boldsymbol{\Psi}$ for some orthonormalbasis matrix $\boldsymbol{\Psi}\in \mathbb{C}^{p \times p}$ and subsamplingoperator $\mathbf{P}_{\Omega}: \mathbb{C}^{p} \rightarrow \mathbb{C}^{n}$ thatselects the rows indexed by $\Omega$. This raises the fundamental question ofhow best to choose the index set $\Omega$ in order to optimize the recoveryperformance. Previous approaches to addressing this question rely onnon-uniform \emph{random} subsampling using application-specific knowledge ofthe structure of $\mathbf{x}$. In this paper, we instead take a principledlearning-based approach in which a \emph{fixed} index set is chosen based on aset of training signals $\mathbf{x}_1,\dotsc,\mathbf{x}_m$. We formulatecombinatorial optimization problems seeking to maximize the energy captured inthese signals in an average-case or worst-case sense, and we show that thesecan be efficiently solved either exactly or approximately via theidentification of modularity and submodularity structures. We provide bothdeterministic and statistical theoretical guarantees showing how the resultingmeasurement matrices perform on signals differing from the training signals,and we provide numerical examples showing our approach to be effective on avariety of data sets.
arxiv-1510-06335 | Time-Sensitive Bayesian Information Aggregation for Crowdsourcing Systems |  http://arxiv.org/abs/1510.06335  | author:Matteo Venanzi, John Guiver, Pushmeet Kohli, Nick Jennings category:cs.AI cs.LG published:2015-10-21 summary:Crowdsourcing systems commonly face the problem of aggregating multiplejudgments provided by potentially unreliable workers. In addition, severalaspects of the design of efficient crowdsourcing processes, such as definingworker's bonuses, fair prices and time limits of the tasks, involve knowledgeof the likely duration of the task at hand. Bringing this together, in thiswork we introduce a new time--sensitive Bayesian aggregation method thatsimultaneously estimates a task's duration and obtains reliable aggregations ofcrowdsourced judgments. Our method, called BCCTime, builds on the key insightthat the time taken by a worker to perform a task is an important indicator ofthe likely quality of the produced judgment. To capture this, BCCTime useslatent variables to represent the uncertainty about the workers' completiontime, the tasks' duration and the workers' accuracy. To relate the quality of ajudgment to the time a worker spends on a task, our model assumes that eachtask is completed within a latent time window within which all workers with apropensity to genuinely attempt the labelling task (i.e., no spammers) areexpected to submit their judgments. In contrast, workers with a lowerpropensity to valid labeling, such as spammers, bots or lazy labelers, areassumed to perform tasks considerably faster or slower than the time requiredby normal workers. Specifically, we use efficient message-passing Bayesianinference to learn approximate posterior probabilities of (i) the confusionmatrix of each worker, (ii) the propensity to valid labeling of each worker,(iii) the unbiased duration of each task and (iv) the true label of each task.Using two real-world public datasets for entity linking tasks, we show thatBCCTime produces up to 11% more accurate classifications and up to 100% moreinformative estimates of a task's duration compared to state-of-the-artmethods.
arxiv-1510-06375 | Towards Direct Medical Image Analysis without Segmentation |  http://arxiv.org/abs/1510.06375  | author:Xiantong Zhen, Shuo Li category:cs.CV published:2015-10-21 summary:Direct methods have recently emerged as an effective and efficient tool inautomated medical image analysis and become a trend to solve diversechallenging tasks in clinical practise. Compared to traditional methods, directmethods are of much more clinical significance by straightly targeting to thefinal clinical goal rather than relying on any intermediate steps. Theseintermediate steps, e.g., segmentation, registration and tracking, are actuallynot necessary and only limited to very constrained tasks far from being used inpractical clinical applications; moreover they are computationally expensiveand time-consuming, which causes a high waste of research resources. Theadvantages of direct methods stem from \textbf{1)} removal of intermediatesteps, e.g., segmentation, tracking and registration; \textbf{2)} avoidance ofuser inputs and initialization; \textbf{3)} reformulation of conventionalchallenging problems, e.g., inversion problem, with efficient solutions.
arxiv-1510-05976 | Transductive Optimization of Top k Precision |  http://arxiv.org/abs/1510.05976  | author:Li-Ping Liu, Thomas G. Dietterich, Nan Li, Zhi-Hua Zhou category:cs.LG published:2015-10-20 summary:Consider a binary classification problem in which the learner is given alabeled training set, an unlabeled test set, and is restricted to choosingexactly $k$ test points to output as positive predictions. Problems of thiskind---{\it transductive precision@$k$}---arise in information retrieval,digital advertising, and reserve design for endangered species. Previousmethods separate the training of the model from its use in scoring the testpoints. This paper introduces a new approach, Transductive Top K (TTK), thatseeks to minimize the hinge loss over all training instances under theconstraint that exactly $k$ test instances are predicted as positive. The paperpresents two optimization methods for this challenging problem. Experiments andanalysis confirm the importance of incorporating the knowledge of $k$ into thelearning process. Experimental evaluations of the TTK approach show that theperformance of TTK matches or exceeds existing state-of-the-art methods on 7UCI datasets and 3 reserve design problem instances.
arxiv-1510-06083 | Regularization vs. Relaxation: A conic optimization perspective of statistical variable selection |  http://arxiv.org/abs/1510.06083  | author:Hongbo Dong, Kun Chen, Jeff Linderoth category:cs.LG math.NA math.OC stat.ML G.1.3; G.1.6 published:2015-10-20 summary:Variable selection is a fundamental task in statistical data analysis.Sparsity-inducing regularization methods are a popular class of methods thatsimultaneously perform variable selection and model estimation. The centralproblem is a quadratic optimization problem with an l0-norm penalty. Exactlyenforcing the l0-norm penalty is computationally intractable for larger scaleproblems, so dif- ferent sparsity-inducing penalty functions that approximatethe l0-norm have been introduced. In this paper, we show that viewing theproblem from a convex relaxation perspective offers new insights. Inparticular, we show that a popular sparsity-inducing concave penalty functionknown as the Minimax Concave Penalty (MCP), and the reverse Huber penaltyderived in a recent work by Pilanci, Wainwright and Ghaoui, can both be derivedas special cases of a lifted convex relaxation called the perspectiverelaxation. The optimal perspective relaxation is a related minimax problemthat balances the overall convexity and tightness of approximation to the l0norm. We show it can be solved by a semidefinite relaxation. Moreover, aprobabilistic interpretation of the semidefinite relaxation reveals connectionswith the boolean quadric polytope in combinatorial optimization. Finally byreformulating the l0-norm pe- nalized problem as a two-level problem, with theinner level being a Max-Cut problem, our proposed semidefinite relaxation canbe realized by replacing the inner level problem with its semidefiniterelaxation studied by Goemans and Williamson. This interpretation suggestsusing the Goemans-Williamson rounding procedure to find approximate solutionsto the l0-norm penalized problem. Numerical experiments demonstrate thetightness of our proposed semidefinite relaxation, and the effectiveness offinding approximate solutions by Goemans-Williamson rounding.
arxiv-1510-05956 | Optimal Cluster Recovery in the Labeled Stochastic Block Model |  http://arxiv.org/abs/1510.05956  | author:Se-Young Yun, Alexandre Proutiere category:math.PR cs.LG cs.SI stat.ML published:2015-10-20 summary:We consider the problem of community detection or clustering in the labeledStochastic Block Model (labeled SBM) with a finite number $K$ of clusters ofsizes linearly growing with the global population of items $n$. Every pair ofitems is labeled independently at random, and label $\ell$ appears withprobability $p(i,j,\ell)$ between two items in clusters indexed by $i$ and $j$,respectively. The objective is to reconstruct the clusters from the observationof these random labels. Clustering under the SBM and their extensions has attracted much attentionrecently. Most existing work aimed at characterizing the set of parameters suchthat it is possible to infer clusters either positively correlated with thetrue clusters, or with a vanishing proportion of misclassified items, orexactly matching the true clusters. We address the finer and more challenging question of determining, under thegeneral LSBM and for any $s$, the set of parameters such that there exists apolynomial-time clustering algorithm with at most $s$ misclassified items inaverage. We prove that in the regime where it is possible to recover theclusters with a vanishing proportion of misclassified items, a necessary andsufficient condition to get $s=o(n)$ misclassified items in average is $\frac{nD(\alpha,p)}{ \log (n/s)} \ge 1$, where $D(\alpha,p)$ is an appropriatelydefined function of the parameters $p=(p(i,j,\ell), i,j, \ell)$, and $\alpha$defining the sizes of the clusters. We further develop an algorithm, based onsimple spectral methods, that achieves this fundamental performance limit. Theanalysis presented in this paper allows us to recover existing results forasymptotically accurate and exact cluster recovery in the SBM, but has muchbroader applications. For example, it implies that the minimal number ofmisclassified items under the LSBM considered scales as$n\exp(-nD(\alpha,p)(1+o(1)))$.
arxiv-1510-05937 | Binary Speaker Embedding |  http://arxiv.org/abs/1510.05937  | author:Lantian Li, Dong Wang, Chao Xing, Kaimin Yu, Thomas Fang Zheng category:cs.SD cs.LG published:2015-10-20 summary:The popular i-vector model represents speakers as low-dimensional continuousvectors (i-vectors), and hence it is a way of continuous speaker embedding. Inthis paper, we investigate binary speaker embedding, which transforms i-vectorsto binary vectors (codes) by a hash function. We start from locality sensitivehashing (LSH), a simple binarization approach where binary codes are derivedfrom a set of random hash functions. A potential problem of LSH is that therandomly sampled hash functions might be suboptimal. We therefore propose animproved Hamming distance learning approach, where the hash function is learnedby a variable-sized block training that projects each dimension of the originali-vectors to variable-sized binary codes independently. Our experiments showthat binary speaker embedding can deliver competitive or even better results onboth speaker verification and identification tasks, while the memory usage andthe computation cost are significantly reduced.
arxiv-1510-05830 | Unsupervised Ensemble Learning with Dependent Classifiers |  http://arxiv.org/abs/1510.05830  | author:Ariel Jaffe, Ethan Fetaya, Boaz Nadler, Tingting Jiang, Yuval Kluger category:cs.LG stat.ML published:2015-10-20 summary:In unsupervised ensemble learning, one obtains predictions from multiplesources or classifiers, yet without knowing the reliability and expertise ofeach source, and with no labeled data to assess it. The task is to combinethese possibly conflicting predictions into an accurate meta-learner. Mostworks to date assumed perfect diversity between the different sources, aproperty known as conditional independence. In realistic scenarios, however,this assumption is often violated, and ensemble learners based on it can beseverely sub-optimal. The key challenges we address in this paper are:\ (i) howto detect, in an unsupervised manner, strong violations of conditionalindependence; and (ii) construct a suitable meta-learner. To this end weintroduce a statistical model that allows for dependencies between classifiers.Our main contributions are the development of novel unsupervised methods todetect strongly dependent classifiers, better estimate their accuracies, andconstruct an improved meta-learner. Using both artificial and real datasets, weshowcase the importance of taking classifier dependencies into account and thecompetitive performance of our approach.
arxiv-1510-05893 | Online Unmixing of Multitemporal Hyperspectral Images accounting for Spectral Variability |  http://arxiv.org/abs/1510.05893  | author:Pierre-Antoine Thouvenin, Nicolas Dobigeon, Jean-Yves Tourneret category:cs.CV stat.ME published:2015-10-20 summary:Hyperspectral unmixing is aimed at identifying the reference spectralsignatures composing an hyperspectral image and their relative abundancefractions in each pixel. In practice, the identified signatures may varyspectrally from an image to another due to varying acquisition conditions, thusinducing possibly significant estimation errors. Against this background,hyperspectral unmixing of several images acquired over the same area is ofconsiderable interest. Indeed, such an analysis enables the endmembers of thescene to be tracked and the corresponding endmember variability to becharacterized. Sequential endmember estimation from a set of hyperspectralimages is expected to provide improved performance when compared to methodsanalyzing the images independently. However, the significant size ofhyperspectral data precludes the use of batch procedures to jointly estimatethe mixture parameters of a sequence of hyperspectral images. Provided thateach elementary component is present in at least one image of the sequence, wepropose to perform an online hyperspectral unmixing accounting for temporalendmember variability. The online hyperspectral unmixing is formulated as atwo-stage stochastic program, which can be solved using a stochasticapproximation. The performance of the proposed method is evaluated on syntheticand real data. A comparison with independent unmixings of each image byestablished methods finally illustrates the interest of the proposed strategy.
arxiv-1510-05940 | Max-margin Metric Learning for Speaker Recognition |  http://arxiv.org/abs/1510.05940  | author:Lantian Li, Dong Wang, Chao Xing, Thomas Fang Zheng category:cs.SD cs.LG published:2015-10-20 summary:Probabilistic linear discriminant analysis (PLDA) is a popular normalizationapproach for the i-vector model, and has delivered state-of-the-art performancein speaker recognition. A potential problem of the PLDA model, however, is thatit essentially assumes Gaussian distributions over speaker vectors, which isnot always true in practice. Additionally, the objective function is notdirectly related to the goal of the task, e.g., discriminating true speakersand imposters. In this paper, we propose a max-margin metric learning approachto solve the problems. It learns a linear transform with a criterion that themargin between target and imposter trials are maximized. Experiments conductedon the SRE08 core test show that compared to PLDA, the new approach can obtaincomparable or even better performance, though the scoring is simply a cosinecomputation.
arxiv-1510-05879 | What's the point? Frame-wise Pointing Gesture Recognition with Latent-Dynamic Conditional Random Fields |  http://arxiv.org/abs/1510.05879  | author:Christian Wittner, Boris Schauerte, Rainer Stiefelhagen category:cs.HC cs.CV cs.RO published:2015-10-20 summary:We use Latent-Dynamic Conditional Random Fields to perform skeleton-basedpointing gesture classification at each time instance of a video sequence,where we achieve a frame-wise pointing accuracy of roughly 83%. Subsequently,we determine continuous time sequences of arbitrary length that form individualpointing gestures and this way reliably detect pointing gestures at a falsepositive detection rate of 0.63%.
arxiv-1510-05822 | Sequential Score Adaptation with Extreme Value Theory for Robust Railway Track Inspection |  http://arxiv.org/abs/1510.05822  | author:Xavier Gibert, Vishal M. Patel, Rama Chellappa category:cs.CV published:2015-10-20 summary:Periodic inspections are necessary to keep railroad tracks in state of goodrepair and prevent train accidents. Automatic track inspection using machinevision technology has become a very effective inspection tool. Because of itsnon-contact nature, this technology can be deployed on virtually any railwayvehicle to continuously survey the tracks and send exception reports to trackmaintenance personnel. However, as appearance and imaging conditions vary,false alarm rates can dramatically change, making it difficult to select a goodoperating point. In this paper, we use extreme value theory (EVT) within aBayesian framework to optimally adjust the sensitivity of anomaly detectors. Weshow that by approximating the lower tail of the probability density function(PDF) of the scores with an Exponential distribution (a special case of theGeneralized Pareto distribution), and using the Gamma conjugate prior learnedfrom the training data, it is possible to reduce the variability in false alarmrate and improve the overall performance. This method has shown an increase inthe defect detection rate of rail fasteners in the presence of clutter (at PFA0.1%) from 95.40% to 99.26% on the 85-mile Northeast Corridor (NEC) 2012-2013concrete tie dataset.
arxiv-1510-06002 | Fast and Scalable Structural SVM with Slack Rescaling |  http://arxiv.org/abs/1510.06002  | author:Heejin Choi, Ofer Meshi, Nathan Srebro category:cs.LG published:2015-10-20 summary:We present an efficient method for training slack-rescaled structural SVM.Although finding the most violating label in a margin-rescaled formulation isoften easy since the target function decomposes with respect to the structure,this is not the case for a slack-rescaled formulation, and finding the mostviolated label might be very difficult. Our core contribution is an efficientmethod for finding the most-violating-label in a slack-rescaled formulation,given an oracle that returns the most-violating-label in a (slightly modified)margin-rescaled formulation. We show that our method enables accurate andscalable training for slack-rescaled SVMs, reducing runtime by an order ofmagnitude compared to previous approaches to slack-rescaled SVMs.
arxiv-1510-05970 | Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches |  http://arxiv.org/abs/1510.05970  | author:Jure Žbontar, Yann LeCun category:cs.CV cs.LG cs.NE published:2015-10-20 summary:We present a method for extracting depth information from a rectified imagepair. Our approach focuses on the first stage of many stereo algorithms: thematching cost computation. We approach the problem by learning a similaritymeasure on small image patches using a convolutional neural network. Trainingis carried out in a supervised manner by constructing a binary classificationdata set with examples of similar and dissimilar pairs of patches. We examinetwo network architectures for this task: one tuned for speed, the other foraccuracy. The output of the convolutional neural network is used to initializethe stereo matching cost. A series of post-processing steps follow: cross-basedcost aggregation, semiglobal matching, a left-right consistency check, subpixelenhancement, a median filter, and a bilateral filter. We evaluate our method onthe KITTI 2012, KITTI 2015, and Middlebury stereo data sets and show that itoutperforms other approaches on all three data sets.
arxiv-1510-05981 | A latent shared-component generative model for real-time disease surveillance using Twitter data |  http://arxiv.org/abs/1510.05981  | author:Roberto C. S. N. P. Souza, Denise E. F de Brito, Renato M. Assunção, Wagner Meira Jr category:cs.SI stat.ML published:2015-10-20 summary:Exploiting the large amount of available data for addressing relevant socialproblems has been one of the key challenges in data mining. Such efforts havebeen recently named "data science for social good" and attracted the attentionof several researchers and institutions. We give a contribution in thisobjective in this paper considering a difficult public health problem, thetimely monitoring of dengue epidemics in small geographical areas. We develop agenerative simple yet effective model to connect the fluctuations of diseasecases and disease-related Twitter posts. We considered a hidden Markov processdriving both, the fluctuations in dengue reported cases and the tweets issuedin each region. We add a stable but random source of tweets to represent theposts when no disease cases are recorded. The model is learned through a Markovchain Monte Carlo algorithm that produces the posterior distribution of therelevant parameters. Using data from a significant number of large Braziliantowns, we demonstrate empirically that our model is able to predict well thenext weeks of the disease counts using the tweets and disease cases jointly.
arxiv-1510-05613 | PERCH: Perception via Search for Multi-Object Recognition and Localization |  http://arxiv.org/abs/1510.05613  | author:Venkatraman Narayanan, Maxim Likhachev category:cs.CV cs.AI cs.RO published:2015-10-19 summary:In many robotic domains such as flexible automated manufacturing or personalassistance, a fundamental perception task is that of identifying and localizingobjects whose 3D models are known. Canonical approaches to this problem includediscriminative methods that find correspondences between feature descriptorscomputed over the model and observed data. While these methods have beenemployed successfully, they can be unreliable when the feature descriptors failto capture variations in observed data; a classic cause being occlusion. As astep towards deliberative reasoning, we present PERCH: PErception via SeaRCH,an algorithm that seeks to find the best explanation of the observed sensordata by hypothesizing possible scenes in a generative fashion. Ourcontributions are: i) formulating the multi-object recognition and localizationtask as an optimization problem over the space of hypothesized scenes, ii)exploiting structure in the optimization to cast it as a combinatorial searchproblem on what we call the Monotone Scene Generation Tree, and iii) leveragingparallelization and recent advances in multi-heuristic search in makingcombinatorial search tractable. We prove that our system can guaranteedlyproduce the best explanation of the scene under the chosen cost function, andvalidate our claims on real world RGB-D test data. Our experimental resultsshow that we can identify and localize objects under heavy occlusion--caseswhere state-of-the-art methods struggle.
arxiv-1510-05492 | Modularity Component Analysis versus Principal Component Analysis |  http://arxiv.org/abs/1510.05492  | author:Hansi Jiang, Carl Meyer category:stat.ML published:2015-10-19 summary:In this paper the exact linear relation between the leading eigenvectors ofthe modularity matrix and the singular vectors of an uncentered data matrix isdeveloped. Based on this analysis the concept of a modularity component isdefined, and its properties are developed. It is shown that modularitycomponent analysis can be used to cluster data similar to how traditionalprincipal component analysis is used except that modularity component analysisdoes not require data centering.
arxiv-1510-05711 | Qualitative Projection Using Deep Neural Networks |  http://arxiv.org/abs/1510.05711  | author:Andrew J. R. Simpson category:cs.NE cs.LG 68Txx published:2015-10-19 summary:Deep neural networks (DNN) abstract by demodulating the output of linearfilters. In this article, we refine this definition of abstraction to show thatthe inputs of a DNN are abstracted with respect to the filters. Or, to restate,the abstraction is qualified by the filters. This leads us to introduce thenotion of qualitative projection. We use qualitative projection to abstractMNIST hand-written digits with respect to the various dogs, horses, planes andcars of the CIFAR dataset. We then classify the MNIST digits according to themagnitude of their dogness, horseness, planeness and carness qualities,illustrating the generality of qualitative projection.
arxiv-1510-05484 | DeepSaliency: Multi-Task Deep Neural Network Model for Salient Object Detection |  http://arxiv.org/abs/1510.05484  | author:Xi Li, Liming Zhao, Lina Wei, MingHsuan Yang, Fei Wu, Yueting Zhuang, Haibin Ling, Jingdong Wang category:cs.CV published:2015-10-19 summary:A key problem in salient object detection is how to effectively model thesemantic properties of salient objects in a data-driven manner. In this paper,we propose a multi-task deep saliency model based on a fully convolutionalneural network (FCNN) with global input (whole raw images) and global output(whole saliency maps). In principle, the proposed saliency model takes adata-driven strategy for encoding the underlying saliency prior information,and then sets up a multi-task learning scheme for exploring the intrinsiccorrelations between saliency detection and semantic image segmentation.Through collaborative feature learning from such two correlated tasks, theshared fully convolutional layers produce effective features for objectperception. Moreover, it is capable of capturing the semantic information onsalient objects across different levels using the fully convolutional layers,which investigates the feature-sharing properties of salient object detectionwith great feature redundancy reduction. Finally, we present a graph Laplacianregularized nonlinear regression model for saliency refinement. Experimentalresults demonstrate the effectiveness of our approach in comparison with thestate-of-the-art approaches.
arxiv-1510-05336 | Clustering is Easy When ....What? |  http://arxiv.org/abs/1510.05336  | author:Shai Ben-David category:stat.ML cs.LG published:2015-10-19 summary:It is well known that most of the common clustering objectives are NP-hard tooptimize. In practice, however, clustering is being routinely carried out. Oneapproach for providing theoretical understanding of this seeming discrepancy isto come up with notions of clusterability that distinguish realisticallyinteresting input data from worst-case data sets. The hope is that there willbe clustering algorithms that are provably efficient on such "clusterable"instances. This paper addresses the thesis that the computational hardness ofclustering tasks goes away for inputs that one really cares about. In otherwords, that "Clustering is difficult only when it does not matter" (the\emph{CDNM thesis} for short). I wish to present a a critical bird's eye overview of the results publishedon this issue so far and to call attention to the gap between available anddesirable results on this issue. A longer, more detailed version of this noteis available as arXiv:1507.05307. I discuss which requirements should be met in order to provide formal supportto the the CDNM thesis and then examine existing results in view of theserequirements and list some significant unsolved research challenges in thatdirection.
arxiv-1510-05577 | Application of Machine Learning Techniques in Human Activity Recognition |  http://arxiv.org/abs/1510.05577  | author:Jitenkumar Babubhai Rana, Rashmi Shetty, Tanya Jha category:cs.LG published:2015-10-19 summary:Human activity detection has seen a tremendous growth in the last decadeplaying a major role in the field of pervasive computing. This emergingpopularity can be attributed to its myriad of real-life applications primarilydealing with human-centric problems like healthcare and elder care. Manyresearch attempts with data mining and machine learning techniques have beenundergoing to accurately detect human activities for e-health systems. Thispaper reviews some of the predictive data mining algorithms and compares theaccuracy and performances of these models. A discussion on the future researchdirections is subsequently offered.
arxiv-1510-05610 | Stochastically Transitive Models for Pairwise Comparisons: Statistical and Computational Issues |  http://arxiv.org/abs/1510.05610  | author:Nihar B. Shah, Sivaraman Balakrishnan, Adityanand Guntuboyina, Martin J. Wainwright category:stat.ML cs.IT cs.LG math.IT published:2015-10-19 summary:There are various parametric models for analyzing pairwise comparison data,including the Bradley-Terry-Luce (BTL) and Thurstone models, but their relianceon strong parametric assumptions is limiting. In this work, we study a flexiblemodel for pairwise comparisons, under which the probabilities of outcomes arerequired only to satisfy a natural form of stochastic transitivity. This classincludes several parametric models including the BTL and Thurstone models asspecial cases, but is considerably more general. We provide various examples ofmodels in this broader stochastically transitive class for which classicalparametric models provide poor fits. Despite this greater flexibility, we showthat the matrix of probabilities can be estimated at the same rate as instandard parametric models. On the other hand, unlike in the BTL and Thurstonemodels, computing the minimax-optimal estimator in the stochasticallytransitive model is non-trivial, and we explore various computationallytractable alternatives. We show that a simple singular value thresholdingalgorithm is statistically consistent but does not achieve the minimax rate. Wethen propose and study algorithms that achieve the minimax rate overinteresting sub-classes of the full stochastically transitive class. Wecomplement our theoretical results with thorough numerical simulations.
arxiv-1510-05491 | Clustering with Beta Divergences |  http://arxiv.org/abs/1510.05491  | author:Mehmet Emin Basbug, Barbara Engelhardt category:cs.LG published:2015-10-19 summary:Clustering algorithms start with a fixed divergence metric, which capturesthe possibly asymmetric distance between two samples. In a mixture model, thesample distribution plays the role of a divergence metric. It is often the casethat the distributional assumption is not validated, which calls for anadaptive approach. We consider a richer model where the underlying distributionbelongs to a parametrized exponential family, called Tweedie Models. We firstshow the connection between the Tweedie models and beta divergences, and derivethe corresponding hard-assignment clustering algorithm. We exploit thisconnection to identify moment conditions and use Generalized Method ofMoments(GMoM) to learn the data distribution. Based on this adaptive approach,we propose four new hard clustering algorithms and compare them to theclassical k-means and DP-means on synthetic data as well as seven UCI datasetsand one large gene expression dataset. We further compare the GMoM routine toan approximate maximum likelihood routine and validate the computationalbenefits of the GMoM approach.
arxiv-1510-06024 | Robust Semi-Supervised Classification for Multi-Relational Graphs |  http://arxiv.org/abs/1510.06024  | author:Junting Ye, Leman Akoglu category:cs.LG published:2015-10-19 summary:Graph-regularized semi-supervised learning has been used effectively forclassification when (i) instances are connected through a graph, and (ii)labeled data is scarce. If available, using multiple relations (or graphs)between the instances can improve the prediction performance. On the otherhand, when these relations have varying levels of veracity and exhibit varyingrelevance for the task, very noisy and/or irrelevant relations may deterioratethe performance. As a result, an effective weighing scheme needs to be put inplace. In this work, we propose a robust and scalable approach formulti-relational graph-regularized semi-supervised classification. Under aconvex optimization scheme, we simultaneously infer weights for the multiplegraphs as well as a solution. We provide a careful analysis of the inferredweights, based on which we devise an algorithm that filters out irrelevant andnoisy graphs and produces weights proportional to the informativeness of theremaining graphs. Moreover, the proposed method is linearly scalable w.r.t. thenumber of edges in the union of the multiple graphs. Through extensiveexperiments we show that our method yields superior results under differentnoise models, and under increasing number of noisy graphs and intensity ofnoise, as compared to a list of baselines and state-of-the-art approaches.
arxiv-1510-05559 | Sparse + Low Rank Decomposition of Annihilating Filter-based Hankel Matrix for Impulse Noise Removal |  http://arxiv.org/abs/1510.05559  | author:Kyong Hwan Jin, Jong Chul Ye category:cs.CV published:2015-10-19 summary:Recently, so called annihilating filer-based low rank Hankel matrix (ALOHA)approach was proposed as a powerful image inpainting method. Based on theobservation that smoothness or textures within an image patch corresponds tosparse spectral components in the frequency domain, ALOHA exploits theexistence of annihilating filters and the associated rank-deficient Hankelmatrices in the image domain to estimate the missing pixels. By extending thisidea, here we propose a novel impulse noise removal algorithm using sparse +low rank decomposition of an annihilating filter-based Hankel matrix. The newapproach, what we call the robust ALOHA, is motivated by the observation thatan image corrupted with impulse noises has intact pixels; so the impulse noisescan be modeled as sparse components, whereas the underlying image can be stillmodeled using a low-rank Hankel structured matrix. To solve the sparse + lowrank decomposition problem, we propose an alternating direction method ofmultiplier (ADMM) method with initial factorized matrices coming from low rankmatrix fitting (LMaFit) algorithm. To adapt the local image statistics thathave distinct spectral distributions, the robust ALOHA is applied patch bypatch. Experimental results from two types of impulse noises - random valuedimpulse noises and salt/pepper noises - for both single channel andmulti-channel color images demonstrate that the robust ALOHA outperforms theexisting algorithms up to 8dB in terms of the peak signal to noise ratio(PSNR).
arxiv-1510-05705 | Single Memristor Logic Gates: From NOT to a Full Adder |  http://arxiv.org/abs/1510.05705  | author:Ella Gale category:cs.ET cs.NE 03B-02, 68U02 published:2015-10-19 summary:Memristors have been suggested as a novel route to neuromorphic computingbased on the similarity between them and neurons (specifically synapses and ionpumps). The d.c. action of the memristor is a current spike which imparts ashort-term memory to the device. Here it is demonstrated that this short-termmemory works exactly like habituation (e.g. in \emph{Aplysia}). We elucidatethe physical rules, based on energy conservation, governing the interaction ofthese current spikes: summation, `bounce-back', directionality and `diminishingreturns'. Using these rules, we introduce 4 different logical systems toimplement sequential logic in the memristor and demonstrate how sequentiallogic works by instantiating a NOT gate, an AND gate, an XOR gate and a FullAdder with a single memristor. The Full Adder makes use of the memristor'sshort-term memory to add together three binary values and outputs the sum, thecarry digit and even the order they were input in. A memristor full adder alsooutputs the arithmetical sum of bits, allowing for a logically (but notphysically) reversible system. Essentially, we can replace an input/output portwith an extra time-step, allowing a single memristor to do a hither-tounexpectedly large amount of computation. This makes up for the memristor'sslow operation speed and may relate to how neurons do a similarly-largecomputation with such slow operations speeds. We propose that using spikinglogic, either in gates or as neuron-analogues, with plastic rewritableconnections between them, would allow the building of a neuromorphic computer.
arxiv-1510-05682 | Protein Structure Prediction by Protein Alignments |  http://arxiv.org/abs/1510.05682  | author:Jianzhu Ma category:cs.CE cs.LG q-bio.BM published:2015-10-19 summary:Proteins are the basic building blocks of life. They usually performfunctions by folding to a particular structure. Understanding the foldingprocess could help the researchers to understand the functions of proteins andcould also help to develop supplemental proteins for people with deficienciesand gain more insight into diseases associated with troublesome foldingproteins. Experimental methods are both expensive and time consuming. In thisthesis I introduce a new machine learning based method to predict the proteinstructure. The new method improves the performance from two directions:creating accurate protein alignments and predicting accurate protein contacts.First, I present an alignment framework MRFalign which goes beyondstate-of-the-art methods and uses Markov Random Fields to model a proteinfamily and align two proteins by aligning two MRFs together. Compared to othermethods, that can only model local-range residue correlation, MRFs can modellong-range residue interactions and thus, encodes global information in aprotein. Secondly, I present a Group Graphical Lasso method for contactprediction that integrates joint multi-family Evolutionary Coupling analysisand supervised learning to improve accuracy on proteins without many sequencehomologs. Different from single-family EC analysis that uses residueco-evolution information in only the target protein family, our joint ECanalysis uses residue co-evolution in both the target family and its relatedfamilies, which may have divergent sequences but similar folds. Our method canalso integrate supervised learning methods to further improve accuracy. Weevaluate the performance of both methods including each of its components onlarge public benchmarks. Experiments show that our methods can achieve betteraccuracy than existing state-of-the-art methods under all the measurements onmost of the protein classes.
arxiv-1510-05407 | Bayesian Inference of Online Social Network Statistics via Lightweight Random Walk Crawls |  http://arxiv.org/abs/1510.05407  | author:Konstantin Avrachenkov, Bruno Ribeiro, Jithin K. Sreedharan category:cs.SI physics.soc-ph stat.ML published:2015-10-19 summary:Online social networks (OSN) contain extensive amount of information aboutthe underlying society that is yet to be explored. One of the most feasibletechnique to fetch information from OSN, crawling through ApplicationProgramming Interface (API) requests, poses serious concerns over the theguarantees of the estimates. In this work, we focus on making reliablestatistical inference with limited API crawls. Based on regenerative propertiesof the random walks, we propose an unbiased estimator for the aggregated sum offunctions over edges and proved the connection between variance of theestimator and spectral gap. In order to facilitate Bayesian inference on thetrue value of the estimator, we derive the approximate posterior distributionof the estimate. Later the proposed ideas are validated with numericalexperiments on inference problems in real-world networks.
arxiv-1510-05436 | Color graph based wavelet transform with perceptual information |  http://arxiv.org/abs/1510.05436  | author:Mohamed Malek, David Helbert, Philippe Carre category:cs.CV published:2015-10-19 summary:In this paper, we propose a numerical strategy to define a multiscaleanalysis for color and multicomponent images based on the representation ofdata on a graph. Our approach consists in computing the graph of an image usingthe psychovisual information and analysing it by using the spectral graphwavelet transform. We suggest introducing color dimension into the computationof the weights of the graph and using the geodesic distance as a means ofdistance measurement. We thus have defined a wavelet transform based on a graphwith perceptual information by using the CIELab color distance. This newrepresentation is illustrated with denoising and inpainting applications.Overall, by introducing psychovisual information in the graph computation forthe graph wavelet transform we obtain very promising results. Therefore resultsin image restoration highlight the interest of the appropriate use of colorinformation.
arxiv-1510-05461 | Confidence Sets for the Source of a Diffusion in Regular Trees |  http://arxiv.org/abs/1510.05461  | author:Justin Khim, Po-Ling Loh category:math.ST cs.DM cs.SI math.PR stat.ML stat.TH 62M99 published:2015-10-19 summary:We study the problem of identifying the source of a diffusion spreading overa regular tree. When the degree of each node is at least three, we show that itis possible to construct confidence sets for the diffusion source with sizeindependent of the number of infected nodes. Our estimators are motivated byanalogous results in the literature concerning identification of the root nodein preferential attachment and uniform attachment trees. At the core of ourproofs is a probabilistic analysis of P\'{o}lya urns corresponding to thenumber of uninfected neighbors in specific subtrees of the infection tree. Wealso provide an example illustrating the shortcomings of source estimationtechniques in settings where the underlying graph is asymmetric.
arxiv-1510-05328 | Exploring the Space of Adversarial Images |  http://arxiv.org/abs/1510.05328  | author:Pedro Tabacof, Eduardo Valle category:cs.NE published:2015-10-19 summary:Adversarial examples have raised questions regarding the robustness andsecurity of deep neural networks. In this work we formalize the problem ofadversarial images given a pretrained classifier, showing that even in thelinear case the resulting optimization problem is nonconvex. We generateadversarial images using shallow and deep classifiers on the MNIST and ImageNetdatasets. We probe the pixel space of adversarial images using noise of varyingintensity and distribution. We bring novel visualizations that showcase thephenomenon and its high variability. We show that adversarial images appear inlarge regions in the pixel space, but that, for the same task, a shallowclassifier seems more robust to adversarial images than a deep convolutionalnetwork.
arxiv-1510-05576 | Optimization for Gaussian Processes via Chaining |  http://arxiv.org/abs/1510.05576  | author:Emile Contal, Cédric Malherbe, Nicolas Vayatis category:stat.ML published:2015-10-19 summary:In this paper, we consider the problem of stochastic optimization under abandit feedback model. We generalize the GP-UCB algorithm [Srinivas and al.,2012] to arbitrary kernels and search spaces. To do so, we use a notion oflocalized chaining to control the supremum of a Gaussian process, and provide anovel optimization scheme based on the computation of covering numbers. Thetheoretical bounds we obtain on the cumulative regret are more generic andpresent the same convergence rates as the GP-UCB algorithm. Finally, thealgorithm is shown to be empirically more efficient than its naturalcompetitors on simple and complex input spaces.
arxiv-1510-05477 | Accelerometer based Activity Classification with Variational Inference on Sticky HDP-SLDS |  http://arxiv.org/abs/1510.05477  | author:Mehmet Emin Basbug, Koray Ozcan, Senem Velipasalar category:cs.LG stat.ML published:2015-10-19 summary:As part of daily monitoring of human activities, wearable sensors and devicesare becoming increasingly popular sources of data. With the advent ofsmartphones equipped with acceloremeter, gyroscope and camera; it is nowpossible to develop activity classification platforms everyone can useconveniently. In this paper, we propose a fast inference method for anunsupervised non-parametric time series model namely variational inference forsticky HDP-SLDS(Hierarchical Dirichlet Process Switching Linear DynamicalSystem). We show that the proposed algorithm can differentiate various indooractivities such as sitting, walking, turning, going up/down the stairs andtaking the elevator using only the acceloremeter of an Android smartphoneSamsung Galaxy S4. We used the front camera of the smartphone to annotateactivity types precisely. We compared the proposed method with Hidden MarkovModels with Gaussian emission probabilities on a dataset of 10 subjects. Weshowed that the efficacy of the stickiness property. We further compared thevariational inference to the Gibbs sampler on the same model and show thatvariational inference is faster in one order of magnitude.
arxiv-1510-05684 | NYTRO: When Subsampling Meets Early Stopping |  http://arxiv.org/abs/1510.05684  | author:Tomas Angles, Raffaello Camoriano, Alessandro Rudi, Lorenzo Rosasco category:stat.ML published:2015-10-19 summary:Early stopping is a well known approach to reduce the time complexity forperforming training and model selection of large scale learning machines. Onthe other hand, memory/space (rather than time) complexity is the mainconstraint in many applications, and randomized subsampling techniques havebeen proposed to tackle this issue. In this paper we ask whether early stoppingand subsampling ideas can be combined in a fruitful way. We consider thequestion in a least squares regression setting and propose a form of randomizediterative regularization based on early stopping and subsampling. In thiscontext, we analyze the statistical and computational properties of theproposed method. Theoretical results are complemented and validated by athorough experimental analysis.
arxiv-1510-05417 | Piecewise-Linear Approximation for Feature Subset Selection in a Sequential Logit Model |  http://arxiv.org/abs/1510.05417  | author:Toshiki Sato, Yuichi Takano, Ryuhei Miyashiro category:stat.ME cs.LG math.OC stat.ML published:2015-10-19 summary:This paper concerns a method of selecting a subset of features for asequential logit model. Tanaka and Nakagawa (2014) proposed a mixed integerquadratic optimization formulation for solving the problem based on a quadraticapproximation of the logistic loss function. However, since there is asignificant gap between the logistic loss function and its quadraticapproximation, their formulation may fail to find a good subset of features. Toovercome this drawback, we apply a piecewise-linear approximation to thelogistic loss function. Accordingly, we frame the feature subset selectionproblem of minimizing an information criterion as a mixed integer linearoptimization problem. The computational results demonstrate that ourpiecewise-linear approximation approach found a better subset of features thanthe quadratic approximation approach.
arxiv-1510-05237 | Large Enforced Sparse Non-Negative Matrix Factorization |  http://arxiv.org/abs/1510.05237  | author:Brendan Gavin, Vijay Gadepally, Jeremy Kepner category:cs.LG cs.NA cs.SI published:2015-10-18 summary:Non-negative matrix factorization (NMF) is a common method for generatingtopic models from text data. NMF is widely accepted for producing good resultsdespite its relative simplicity of implementation and ease of computation. Onechallenge with applying NMF to large datasets is that intermediate matrixproducts often become dense, stressing the memory and compute elements of asystem. In this article, we investigate a simple but powerful modification of acommon NMF algorithm that enforces the generation of sparse intermediate andoutput matrices. This method enables the application of NMF to large datasetsthrough improved memory and compute performance. Further, we demonstrateempirically that this method of enforcing sparsity in the NMF either preservesor improves both the accuracy of the resulting topic model and the convergencerate of the underlying algorithm.
arxiv-1510-05257 | Scalable inference for a full multivariate stochastic volatility model |  http://arxiv.org/abs/1510.05257  | author:P. Dellaportas, A. Plataniotis, M. K. Titsias category:stat.ML published:2015-10-18 summary:We introduce a multivariate stochastic volatility model for asset returnsthat imposes no restrictions to the structure of the volatility matrix andtreats all its elements as functions of latent stochastic processes. When thenumber of assets is prohibitively large, we propose a factor multivariatestochastic volatility model in which the variances and correlations of thefactors evolve stochastically over time. Inference is achieved via a carefullydesigned feasible and scalable Markov chain Monte Carlo algorithm that combinestwo computationally important ingredients: it utilizes invariant to the priorMetropolis proposal densities for simultaneously updating all latent paths andhas quadratic, rather than cubic, computational complexity when evaluating themultivariate normal densities required. We apply our modelling andcomputational methodology to $571$ stock daily returns of Euro STOXX index fordata over a period of $10$ years.
arxiv-1510-05275 | Real-time Tracking Based on Neuromrophic Vision |  http://arxiv.org/abs/1510.05275  | author:Hongmin Li, Pei Jing, Guoqi Li category:cs.CV published:2015-10-18 summary:Real-time tracking is an important problem in computer vision in which mostmethods are based on the conventional cameras. Neuromorphic vision is a conceptdefined by incorporating neuromorphic vision sensors such as silicon retinas invision processing system. With the development of the silicon technology,asynchronous event-based silicon retinas that mimic neuro-biologicalarchitectures has been developed in recent years. In this work, we combine thevision tracking algorithm of computer vision with the information encodingmechanism of event-based sensors which is inspired from the neural rate codingmechanism. The real-time tracking of single object with the advantage of highspeed of 100 time bins per second is successfully realized. Our methoddemonstrates that the computer vision methods could be used for theneuromorphic vision processing and we can realize fast real-time tracking usingneuromorphic vision sensors compare to the conventional camera.
arxiv-1510-05318 | Latent Space Model for Multi-Modal Social Data |  http://arxiv.org/abs/1510.05318  | author:Yoon-Sik Cho, Greg Ver Steeg, Emilio Ferrara, Aram Galstyan category:cs.SI cs.LG physics.soc-ph published:2015-10-18 summary:With the emergence of social networking services, researchers enjoy theincreasing availability of large-scale heterogenous datasets capturing onlineuser interactions and behaviors. Traditional analysis of techno-social systemsdata has focused mainly on describing either the dynamics of socialinteractions, or the attributes and behaviors of the users. However,overwhelming empirical evidence suggests that the two dimensions affect oneanother, and therefore they should be jointly modeled and analyzed in amulti-modal framework. The benefits of such an approach include the ability tobuild better predictive models, leveraging social network information as wellas user behavioral signals. To this purpose, here we propose the ConstrainedLatent Space Model (CLSM), a generalized framework that combines MixedMembership Stochastic Blockmodels (MMSB) and Latent Dirichlet Allocation (LDA)incorporating a constraint that forces the latent space to concurrentlydescribe the multiple data modalities. We derive an efficient inferencealgorithm based on Variational Expectation Maximization that has acomputational cost linear in the size of the network, thus making it feasibleto analyze massive social datasets. We validate the proposed framework on twoproblems: prediction of social interactions from user attributes and behaviors,and behavior prediction exploiting network information. We perform experimentswith a variety of multi-modal social systems, spanning location-based socialnetworks (Gowalla), social media services (Instagram, Orkut), e-commerce andreview sites (Amazon, Ciao), and finally citation networks (Cora). The resultsindicate significant improvement in prediction accuracy over state of the artmethods, and demonstrate the flexibility of the proposed approach foraddressing a variety of different learning problems commonly occurring withmulti-modal social data.
arxiv-1510-05203 | Neural Reranking Improves Subjective Quality of Machine Translation: NAIST at WAT2015 |  http://arxiv.org/abs/1510.05203  | author:Graham Neubig, Makoto Morishita, Satoshi Nakamura category:cs.CL published:2015-10-18 summary:This year, the Nara Institute of Science and Technology (NAIST)'s submissionto the 2015 Workshop on Asian Translation was based on syntax-based statisticalmachine translation, with the addition of a reranking component using neuralattentional machine translation models. Experiments re-confirmed results fromprevious work stating that neural MT reranking provides a large gain inobjective evaluation measures such as BLEU, and also confirmed for the firsttime that these results also carry over to manual evaluation. We furtherperform a detailed analysis of reasons for this increase, finding that the maincontributions of the neural models lie in improvement of the grammaticalcorrectness of the output, as opposed to improvements in lexical choice ofcontent words.
arxiv-1510-05198 | Learning multi-faceted representations of individuals from heterogeneous evidence using neural networks |  http://arxiv.org/abs/1510.05198  | author:Jiwei Li, Alan Ritter, Dan Jurafsky category:cs.SI cs.CL published:2015-10-18 summary:Inferring latent attributes of people online is an important social computingtask, but requires integrating the many heterogeneous sources of informationavailable on the web. We propose to learn individual representations of peopleusing neural nets to integrate information from social media. The algorithm isable to combine any kind of cues, such as the text a person writes, theperson's attributes (e.g. gender, employer, school, location) and socialrelations to other people (e.g., friendship, marriage), using global inferenceto infer missing attributes from noisy cues. The resulting latentrepresentations capture homophily: people who have similar attributes, arerelated socially, or write similar text are closer in vector space. We showthat these learned representations offer good performance at solving fourimportant tasks in social media inference on Twitter: predicting (1) gender,(2) occupation, (3) location, and (4) friendships for users, and that weachieve the best performance by integrating all these signals. Our approachscales to large datasets, using parallel stochastic gradient descent forlearning. The resulting representations can be used as general features in andhave the potential to benefit a large number of downstream tasks like linkprediction, community detection, or reasoning over social networks, discoveringfor example the high probability that a New York City resident is a fan of theNew York Knicks, or the greater preference for iPhones by computerprofessionals than legal professionals.
arxiv-1510-05214 | Clustering Noisy Signals with Structured Sparsity Using Time-Frequency Representation |  http://arxiv.org/abs/1510.05214  | author:Tom Hope, Avishai Wagner, Or Zuk category:cs.LG stat.ML 62H30, 65T60 published:2015-10-18 summary:We propose a simple and efficient time-series clustering frameworkparticularly suited for low Signal-to-Noise Ratio (SNR), by simultaneoussmoothing and dimensionality reduction aimed at preserving clusteringinformation. We extend the sparse K-means algorithm by incorporating structuredsparsity, and use it to exploit the multi-scale property of wavelets and groupstructure in multivariate signals. Finally, we extract features invariant totranslation and scaling with the scattering transform, which corresponds to aconvolutional network with filters given by a wavelet operator, and use thenetwork's structure in sparse clustering. By promoting sparsity, this transformcan yield a low-dimensional representation of signals that gives improvedclustering results on several real datasets.
arxiv-1510-05067 | How Important is Weight Symmetry in Backpropagation? |  http://arxiv.org/abs/1510.05067  | author:Qianli Liao, Joel Z. Leibo, Tomaso Poggio category:cs.LG published:2015-10-17 summary:Gradient backpropagation (BP) requires symmetric feedforward and feedbackconnections -- the same weights must be used for forward and backward passes.This "weight transport problem" (Grossberg 1987) is thought to be one of themain reasons to doubt BP's biologically plausibility. Using 15 differentclassification datasets, we systematically investigate to what extent BP reallydepends on weight symmetry. In a study that turned out to be surprisinglysimilar in spirit to Lillicrap et al.'s demonstration (Lillicrap et al. 2014)but orthogonal in its results, our experiments indicate that: (1) themagnitudes of feedback weights do not matter to performance (2) the signs offeedback weights do matter -- the more concordant signs between feedforward andtheir corresponding feedback connections, the better (3) with feedback weightshaving random magnitudes and 100% concordant signs, we were able to achieve thesame or even better performance than SGD. (4) somenormalizations/stabilizations are indispensable for such asymmetric BP to work,namely Batch Normalization (BN) (Ioffe and Szegedy 2015) and/or a "BatchManhattan" (BM) update rule.
arxiv-1510-05145 | Rapid Online Analysis of Local Feature Detectors and Their Complementarity |  http://arxiv.org/abs/1510.05145  | author:Shoaib Ehsan, Adrian F. Clark, Klaus D. McDonald-Maier category:cs.CV published:2015-10-17 summary:A vision system that can assess its own performance and take appropriateactions online to maximize its effectiveness would be a step towards achievingthe long-cherished goal of imitating humans. This paper proposes a method forperforming an online performance analysis of local feature detectors, theprimary stage of many practical vision systems. It advocates the spatialdistribution of local image features as a good performance indicator andpresents a metric that can be calculated rapidly, concurs with human visualassessments and is complementary to existing offline measures such asrepeatability. The metric is shown to provide a measure of complementarity forcombinations of detectors, correctly reflecting the underlying principles ofindividual detectors. Qualitative results on well-established datasets forseveral state-of-the-art detectors are presented based on the proposed measure.Using a hypothesis testing approach and a newly-acquired, larger imagedatabase, statistically-significant performance differences are identified.Different detector pairs and triplets are examined quantitatively and theresults provide a useful guideline for combining detectors in applications thatrequire a reasonable spatial distribution of image features. A principledframework for combining feature detectors in these applications is alsopresented. Timing results reveal the potential of the metric for onlineapplications.
arxiv-1510-05142 | Memory-Efficient Design Strategy for a Parallel Embedded Integral Image Computation Engine |  http://arxiv.org/abs/1510.05142  | author:Shoaib Ehsan, Adrian F. Clark, Wah M. Cheung, Arjunsingh M. Bais, Bayar I. Menzat, Nadia Kanwal, Klaus D. McDonald-Maier category:cs.CV published:2015-10-17 summary:In embedded vision systems, parallel computation of the integral imagepresents several design challenges in terms of hardware resources, speed andpower consumption. Although recursive equations significantly reduce the numberof operations for computing the integral image, the required internal memorybecomes prohibitively large for an embedded integral image computation enginefor increasing image sizes. With the objective of achieving high-throughputwith minimum hardware resources, this paper proposes a memory-efficient designstrategy for a parallel embedded integral image computation engine. Resultsshow that the design achieves nearly 35% reduction in memory for common HDvideo.
arxiv-1510-05138 | Integral Images: Efficient Algorithms for Their Computation and Storage in Resource-Constrained Embedded Vision Systems |  http://arxiv.org/abs/1510.05138  | author:Shoaib Ehsan, Adrian F. Clark, Naveed ur Rehman, Klaus D. McDonald-Maier category:cs.CV published:2015-10-17 summary:The integral image, an intermediate image representation, has found extensiveuse in multi-scale local feature detection algorithms, such as Speeded-UpRobust Features (SURF), allowing fast computation of rectangular features atconstant speed, independent of filter size. For resource-constrained real-timeembedded vision systems, computation and storage of integral image presentsseveral design challenges due to strict timing and hardware limitations.Although calculation of the integral image only consists of simple additionoperations, the total number of operations is large owing to the generallylarge size of image data. Recursive equations allow substantial decrease in thenumber of operations but require calculation in a serial fashion. This paperpresents two new hardware algorithms that are based on the decomposition ofthese recursive equations, allowing calculation of up to four integral imagevalues in a row-parallel way without significantly increasing the number ofoperations. An efficient design strategy is also proposed for a parallelintegral image computation unit to reduce the size of the required internalmemory (nearly 35% for common HD video). Addressing the storage problem ofintegral image in embedded vision systems, the paper presents two algorithmswhich allow substantial decrease (at least 44.44%) in the memory requirements.Finally, the paper provides a case study that highlights the utility of theproposed architectures in embedded vision systems.
arxiv-1510-05157 | Performance Characterization of Image Feature Detectors in Relation to the Scene Content Utilizing a Large Image Database |  http://arxiv.org/abs/1510.05157  | author:Bruno Ferrarini, Shoaib Ehsan, Naveed Ur Rehman, Klaus D. McDonald-Maier category:cs.CV published:2015-10-17 summary:Selecting the most suitable local invariant feature detector for a particularapplication has rendered the task of evaluating feature detectors a criticalissue in vision research. No state-of-the-art image feature detector workssatisfactorily under all types of image transformations. Although theliterature offers a variety of comparison works focusing on performanceevaluation of image feature detectors under several types of imagetransformation, the influence of the scene content on the performance of localfeature detectors has received little attention so far. This paper aims tobridge this gap with a new framework for determining the type of scenes, whichmaximize and minimize the performance of detectors in terms of repeatabilityrate. Several state-of-the-art feature detectors have been assessed utilizing alarge database of 12936 images generated by applying uniform light and blurchanges to 539 scenes captured from the real world. The results obtainedprovide new insights into the behaviour of feature detectors.
arxiv-1510-05078 | A General Method for Robust Bayesian Modeling |  http://arxiv.org/abs/1510.05078  | author:Chong Wang, David M. Blei category:stat.ML published:2015-10-17 summary:Robust Bayesian models are appealing alternatives to standard models,providing protection from data that contains outliers or other departures fromthe model assumptions. Historically, robust models were mostly developed on acase-by-case basis; examples include robust linear regression, robust mixturemodels, and bursty topic models. In this paper we develop a general approach torobust Bayesian modeling. We show how to turn an existing Bayesian model into arobust model, and then develop a generic strategy for computing with it. We useour method to study robust variants of several models, including linearregression, Poisson regression, logistic regression, and probabilistic topicmodels. We discuss the connections between our methods and existing approaches,especially empirical Bayes and James-Stein estimation.
arxiv-1510-05058 | A Distance Measure for the Analysis of Polar Opinion Dynamics in Social Networks |  http://arxiv.org/abs/1510.05058  | author:Victor Amelkin, Ambuj Singh, Petko Bogdanov category:cs.SI cs.DM cs.DS stat.ML published:2015-10-17 summary:Analysis of opinion dynamics in social networks plays an important role intoday's life. For applications such as predicting users' political preference,it is particularly important to be able to analyze the dynamics of competingopinions. While observing the evolution of polar opinions of a social network'susers over time, can we tell when the network "behaved" abnormally?Furthermore, can we predict how the opinions of the users will change in thefuture? Do opinions evolve according to existing network opinion dynamicsmodels? To answer such questions, it is not sufficient to study individual userbehavior, since opinions can spread far beyond users' egonets. We need a methodto analyze opinion dynamics of all network users simultaneously and capture theeffect of individuals' behavior on the global evolution pattern of the socialnetwork. In this work, we introduce Social Network Distance (SND) - a distance measurethat quantifies the "cost" of evolution of one snapshot of a social networkinto another snapshot under various models of polar opinion propagation. SNDhas a rich semantics of a transportation problem, yet, is computable in timelinear in the number of users, which makes SND applicable to the analysis oflarge-scale online social networks. In our experiments with synthetic andreal-world Twitter data, we demonstrate the utility of our distance measure foranomalous event detection. It achieves a true positive rate of 0.83, twice ashigh as that of alternatives. When employed for opinion prediction in Twitter,our method's accuracy is 75.63%, which is 7.5% higher than that of the nextbest method. Source Code: https://cs.ucsb.edu/~victor/pub/ucsb/dbl/snd/
arxiv-1510-05149 | Robust Non-linear Wiener-Granger Causality For Large High-dimensional Data |  http://arxiv.org/abs/1510.05149  | author:Mehrdad Jafari-Mamaghani category:stat.ML stat.ME published:2015-10-17 summary:Wiener-Granger causality is a widely used framework of causal analysis fortemporally resolved events. We introduce a new measure of Wiener-Grangercausality based on kernelization of partial canonical correlation analysis withspecific advantages in the context of large high-dimensional data. Theintroduced measure is able to detect non-linear and non-monotonous signals, isdesigned to be immune to noise, and offers tunability in terms of computationalcomplexity in its estimations. Furthermore, we show that, under specifiedconditions, the introduced measure can be regarded as an estimate ofconditional mutual information (transfer entropy). The functionality of thismeasure is assessed using comparative simulations where it outperforms otherexisting methods. The paper is concluded with an application to climatologicaldata.
arxiv-1510-05154 | A Historical Analysis of the Field of OR/MS using Topic Models |  http://arxiv.org/abs/1510.05154  | author:Christopher J. Gatti, James D. Brooks, Sarah G. Nurre category:stat.ML cs.DL stat.AP published:2015-10-17 summary:This study investigates the content of the published scientific literature inthe fields of operations research and management science (OR/MS) since theearly 1950s. Our study is based on 80,757 published journal abstracts from 37of the leading OR/MS journals. We have developed a topic model, using LatentDirichlet Allocation (LDA), and extend this analysis to reveal the temporaldynamics of the field, journals, and topics. Our analysis shows the generalityor specificity of each of the journals, and we identify groups of journals withsimilar content, which are both consistent and inconsistent with intuition. Wealso show how journals have become more or less unique in their scope. A moredetailed analysis of each journals' topics over time shows significant temporaldynamics, especially for journals with niche content. This study presents anobservational, yet objective, view of the published literature from OR/MS thatwould be of interest to authors, editors, journals, and publishers.Furthermore, this work can be used by new entrants to the fields of OR/MS tounderstand the content landscape, as a starting point for discussions andinquiry of the field at large, or as a model for other fields to performsimilar analyses.
arxiv-1510-05156 | Assessing The Performance Bounds Of Local Feature Detectors: Taking Inspiration From Electronics Design Practices |  http://arxiv.org/abs/1510.05156  | author:Shoaib Ehsan, Adrian F. Clark, Bruno Ferrarini, Naveed Ur Rehman, Klaus D. McDonald-Maier category:cs.CV published:2015-10-17 summary:Since local feature detection has been one of the most active research areasin computer vision, a large number of detectors have been proposed. This hasrendered the task of characterizing the performance of various featuredetection methods an important issue in vision research. Inspired by the goodpractices of electronic system design, a generic framework based on theimproved repeatability measure is presented in this paper that allowsassessment of the upper and lower bounds of detector performance in an effortto design more reliable and effective vision systems. This framework is thenemployed to establish operating and guarantee regions for several state-of-theart detectors for JPEG compression and uniform light changes. The results areobtained using a newly acquired, large image database (15092 images) with 539different scenes. These results provide new insights into the behavior ofdetectors and are also useful from the vision systems design perspective.
arxiv-1510-04850 | Change Detection in Multivariate Datastreams: Likelihood and Detectability Loss |  http://arxiv.org/abs/1510.04850  | author:Cesare Alippi, Giacomo Boracchi, Diego Carrera, Manuel Roveri category:stat.ML published:2015-10-16 summary:We address the problem of detecting changes in multivariate datastreams, andwe investigate the intrinsic difficulty that change-detection methods have toface when the data dimension scales. In particular, we consider a generalapproach where changes are detected by comparing the distribution of thelog-likelihood of the datastream over different time windows. Despite the factthat this approach constitutes the frame of several change-detection methods,its effectiveness when data dimension scales has never been investigated, whichis indeed the goal of our paper. We show that the magnitude of the change canbe naturally measured by the symmetric Kullback-Leibler divergence between thepre- and post-change distributions, and that the detectability of a change of agiven magnitude worsens when the data dimension increases. This problem, whichwe refer to as \emph{detectability loss}, is due to the linear relationshipbetween the variance of the log-likelihood and the data dimension. Weanalytically derive the detectability loss on Gaussian-distributed datastreams,and empirically demonstrate that this problem holds also on real-world datasetsand that can be harmful even at low data-dimensions (say, 10).
arxiv-1510-05043 | A cost function for similarity-based hierarchical clustering |  http://arxiv.org/abs/1510.05043  | author:Sanjoy Dasgupta category:cs.DS cs.LG stat.ML published:2015-10-16 summary:The development of algorithms for hierarchical clustering has been hamperedby a shortage of precise objective functions. To help address this situation,we introduce a simple cost function on hierarchies over a set of points, givenpairwise similarities between those points. We show that this criterion behavessensibly in canonical instances and that it admits a top-down constructionprocedure with a provably good approximation ratio.
arxiv-1510-04863 | An Extension to Hough Transform Based on Gradient Orientation |  http://arxiv.org/abs/1510.04863  | author:Tomislav Petković, Sven Lončarić category:cs.CV published:2015-10-16 summary:The Hough transform is one of the most common methods for line detection. Inthis paper we propose a novel extension of the regular Hough transform. Theproposed extension combines the extension of the accumulator space and thelocal gradient orientation resulting in clutter reduction and yielding moreprominent peaks, thus enabling better line identification. We demonstratebenefits in applications such as visual quality inspection and rectangledetection.
