arxiv-14100-1 | Deep-Spying: Spying using Smartwatch and Deep Learning | http://arxiv.org/pdf/1512.05616v1.pdf | author:Tony Beltramelli, Sebastian Risi category:cs.CR cs.CY cs.LG published:2015-12-17 summary:Wearable technologies are today on the rise, becoming more common and broadlyavailable to mainstream users. In fact, wristband and armband devices such assmartwatches and fitness trackers already took an important place in theconsumer electronics market and are becoming ubiquitous. By their very natureof being wearable, these devices, however, provide a new pervasive attacksurface threatening users privacy, among others. In the meantime, advances in machine learning are providing unprecedentedpossibilities to process complex data efficiently. Allowing patterns to emergefrom high dimensional unavoidably noisy data. The goal of this work is to raise awareness about the potential risks relatedto motion sensors built-in wearable devices and to demonstrate abuseopportunities leveraged by advanced neural network architectures. The LSTM-based implementation presented in this research can performtouchlogging and keylogging on 12-keys keypads with above-average accuracy evenwhen confronted with raw unprocessed data. Thus demonstrating that deep neuralnetworks are capable of making keystroke inference attacks based on motionsensors easier to achieve by removing the need for non-trivial pre-processingpipelines and carefully engineered feature extraction strategies. Our resultssuggest that the complete technological ecosystem of a user can be compromisedwhen a wearable wristband device is worn.
arxiv-14100-2 | Classification of weak multi-view signals by sharing factors in a mixture of Bayesian group factor analyzers | http://arxiv.org/pdf/1512.05610v1.pdf | author:Sami Remes, Tommi Mononen, Samuel Kaski category:stat.ML published:2015-12-17 summary:We propose a novel classification model for weak signal data, building upon arecent model for Bayesian multi-view learning, Group Factor Analysis (GFA).Instead of assuming all data to come from a single GFA model, we allow latentclusters, each having a different GFA model and producing a different classdistribution. We show that sharing information across the clusters, by sharingfactors, increases the classification accuracy considerably; the shared factorsessentially form a flexible noise model that explains away the part of data notrelated to classification. Motivation for the setting comes from single-trialfunctional brain imaging data, having a very low signal-to-noise ratio and anatural multi-view setting, with the different sensors, measurement modalities(EEG, MEG, fMRI) and possible auxiliary information as views. We demonstrateour model on a MEG dataset.
arxiv-14100-3 | Reconstruction of Enhanced Ultrasound Images From Compressed Measurements Using Simultaneous Direction Method of Multipliers | http://arxiv.org/pdf/1512.05586v1.pdf | author:Zhouye Chen, Adrian Basarab, Denis Kouamé category:cs.CV published:2015-12-17 summary:High resolution ultrasound image reconstruction from a reduced number ofmeasurements is of great interest in ultrasound imaging, since it could enhanceboth the frame rate and image resolution. Compressive deconvolution, combiningcompressed sensing and image deconvolution, represents an interestingpossibility to consider this challenging task. The model of compressivedeconvolution includes, in addition to the compressive sampling matrix, a 2Dconvolution operator carrying the information on the system point spreadfunction. Through this model, the resolution of reconstructed ultrasound imagesfrom compressed measurements mainly depends on three aspects: the acquisitionsetup, i.e. the incoherence of the sampling matrix, the image regularization,i.e. the sparsity prior, and the optimization technique. In this paper, wemainly focused on the last two aspects. We proposed a novel simultaneousdirection method of multipliers-based optimization scheme to invert the linearmodel, including two regularization terms expressing the sparsity of the RFimages in a given basis and the generalized Gaussian statistical assumption ontissue reflectivity functions. The performance of the method is evaluated onboth simulated and in vivo data.
arxiv-14100-4 | An Empirical Comparison of Neural Architectures for Reinforcement Learning in Partially Observable Environments | http://arxiv.org/pdf/1512.05509v1.pdf | author:Denis Steckelmacher, Peter Vrancx category:cs.NE cs.AI cs.LG published:2015-12-17 summary:This paper explores the performance of fitted neural Q iteration forreinforcement learning in several partially observable environments, usingthree recurrent neural network architectures: Long Short-Term Memory, GatedRecurrent Unit and MUT1, a recurrent neural architecture evolved from a pool ofseveral thousands candidate architectures. A variant of fitted Q iteration,based on Advantage values instead of Q values, is also explored. The resultsshow that GRU performs significantly better than LSTM and MUT1 for most of theproblems considered, requiring less training episodes and less CPU time beforelearning a very good policy. Advantage learning also tends to produce betterresults.
arxiv-14100-5 | Inferring the Causal Direction Privately | http://arxiv.org/pdf/1512.05469v1.pdf | author:Matt J. Kusner, Yu Sun, Karthik Sridharan, Kilian Q. Weinberger category:stat.ML published:2015-12-17 summary:Causal inference deals with identifying which random variables "cause" orcontrol other random variables. Recent advances on the topic of causalinference based on tools from statistical estimation and machine learning haveresulted in practical algorithms for causal inference. Causal inference has thepotential to have significant impact on medical research, prevention andcontrol of diseases and identifying influential factors that impact society toname just a few. However, these promising applications for causal inference areoften ones that involve sensitive or personal data of users that need to bekept private (e.g. medical records, personal finances). Therefore, there is aneed for the development of causal inference methods that preserve dataprivacy. We study the problem of inferring causality using the current, popularcausal inference framework, the additive noise model (ANM) while simultaneouslyensuring privacy of the users. We derive a framework that provides differentialprivacy guarantees for a variety of ANM variants. We run extensive experiments,and demonstrate that our techniques are practical and easy to implement.
arxiv-14100-6 | Clustering and Inference From Pairwise Comparisons | http://arxiv.org/pdf/1502.04631v2.pdf | author:Rui Wu, Jiaming Xu, R. Srikant, Laurent Massoulié, Marc Lelarge, Bruce Hajek category:stat.ML published:2015-02-16 summary:Given a set of pairwise comparisons, the classical ranking problem computes asingle ranking that best represents the preferences of all users. In thispaper, we study the problem of inferring individual preferences, arising in thecontext of making personalized recommendations. In particular, we assume thatthere are $n$ users of $r$ types; users of the same type provide similarpairwise comparisons for $m$ items according to the Bradley-Terry model. Wepropose an efficient algorithm that accurately estimates the individualpreferences for almost all users, if there are $r \max \{m, n\}\log m \log^2 n$pairwise comparisons per type, which is near optimal in sample complexity when$r$ only grows logarithmically with $m$ or $n$. Our algorithm has three steps:first, for each user, compute the \emph{net-win} vector which is a projectionof its $\binom{m}{2}$-dimensional vector of pairwise comparisons onto an$m$-dimensional linear subspace; second, cluster the users based on the net-winvectors; third, estimate a single preference for each cluster separately. Thenet-win vectors are much less noisy than the high dimensional vectors ofpairwise comparisons and clustering is more accurate after the projection asconfirmed by numerical experiments. Moreover, we show that, when a cluster isonly approximately correct, the maximum likelihood estimation for theBradley-Terry model is still close to the true preference.
arxiv-14100-7 | Unsupervised Feature Construction for Improving Data Representation and Semantics | http://arxiv.org/pdf/1512.05467v1.pdf | author:Marian-Andrei Rizoiu, Julien Velcin, Stéphane Lallich category:cs.AI cs.LG published:2015-12-17 summary:Feature-based format is the main data representation format used by machinelearning algorithms. When the features do not properly describe the initialdata, performance starts to degrade. Some algorithms address this problem byinternally changing the representation space, but the newly-constructedfeatures are rarely comprehensible. We seek to construct, in an unsupervisedway, new features that are more appropriate for describing a given dataset and,at the same time, comprehensible for a human user. We propose two algorithmsthat construct the new features as conjunctions of the initial primitivefeatures or their negations. The generated feature sets have reducedcorrelations between features and succeed in catching some of the hiddenrelations between individuals in a dataset. For example, a feature like $sky\wedge \neg building \wedge panorama$ would be true for non-urban images and ismore informative than simple features expressing the presence or the absence ofan object. The notion of Pareto optimality is used to evaluate feature sets andto obtain a balance between total correlation and the complexity of theresulted feature set. Statistical hypothesis testing is used in order toautomatically determine the values of the parameters used for constructing adata-dependent feature set. We experimentally show that our approaches achievethe construction of informative feature sets for multiple datasets.
arxiv-14100-8 | Dense Optical Flow Prediction from a Static Image | http://arxiv.org/pdf/1505.00295v2.pdf | author:Jacob Walker, Abhinav Gupta, Martial Hebert category:cs.CV published:2015-05-02 summary:Given a scene, what is going to move, and in what direction will it move?Such a question could be considered a non-semantic form of action prediction.In this work, we present a convolutional neural network (CNN) based approachfor motion prediction. Given a static image, this CNN predicts the futuremotion of each and every pixel in the image in terms of optical flow. Our CNNmodel leverages the data in tens of thousands of realistic videos to train ourmodel. Our method relies on absolutely no human labeling and is able to predictmotion based on the context of the scene. Because our CNN model makes noassumptions about the underlying scene, it can predict future optical flow on adiverse set of scenarios. We outperform all previous approaches by largemargins.
arxiv-14100-9 | Riemannian Dictionary Learning and Sparse Coding for Positive Definite Matrices | http://arxiv.org/pdf/1507.02772v2.pdf | author:Anoop Cherian, Suvrit Sra category:cs.CV published:2015-07-10 summary:Data encoded as symmetric positive definite (SPD) matrices frequently arisein many areas of computer vision and machine learning. While these matricesform an open subset of the Euclidean space of symmetric matrices, viewing themthrough the lens of non-Euclidean Riemannian geometry often turns out to bebetter suited in capturing several desirable data properties. However,formulating classical machine learning algorithms within such a geometry isoften non-trivial and computationally expensive. Inspired by the great successof dictionary learning and sparse coding for vector-valued data, our goal inthis paper is to represent data in the form of SPD matrices as sparse coniccombinations of SPD atoms from a learned dictionary via a Riemannian geometricapproach. To that end, we formulate a novel Riemannian optimization objectivefor dictionary learning and sparse coding in which the representation loss ischaracterized via the affine invariant Riemannian metric. We also present acomputationally simple algorithm for optimizing our model. Experiments onseveral computer vision datasets demonstrate superior classification andretrieval performance using our approach when compared to sparse coding viaalternative non-Riemannian formulations.
arxiv-14100-10 | Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture | http://arxiv.org/pdf/1411.4734v4.pdf | author:David Eigen, Rob Fergus category:cs.CV published:2014-11-18 summary:In this paper we address three different computer vision tasks using a singlebasic architecture: depth prediction, surface normal estimation, and semanticlabeling. We use a multiscale convolutional network that is able to adapteasily to each task using only small modifications, regressing from the inputimage to the output map directly. Our method progressively refines predictionsusing a sequence of scales, and captures many image details without anysuperpixels or low-level segmentation. We achieve state-of-the-art performanceon benchmarks for all three tasks.
arxiv-14100-11 | A convergence and asymptotic analysis of the generalized symmetric FastICA algorithm | http://arxiv.org/pdf/1408.0145v2.pdf | author:Tianwen Wei category:stat.ML published:2014-08-01 summary:This contribution deals with the generalized symmetric FastICA algorithm inthe domain of Independent Component Analysis (ICA). The generalized symmetricversion of FastICA has been shown to have the potential to achieve theCram\'er-Rao Bound (CRB) by allowing the usage of different nonlinearityfunctions in its parallel implementations of one-unit FastICA. In spite of thisappealing property, a rigorous study of the asymptotic error of the generalizedsymmetric FastICA algorithm is still missing in the community. In fact, all theexisting results exhibit certain limitations, such as ignoring the impact ofdata standardization on the asymptotic statistics or being based on a heuristicapproach. In this work, we aim at filling this blank. The first result of this contribution is the characterization of the limitsof the generalized symmetric FastICA. It is shown that the algorithm optimizesa function that is a sum of the contrast functions used by traditional one-unitFastICA with a correction of the sign. Based on this characterization, wederive a closed-form analytic expression of the asymptotic covariance matrix ofthe generalized symmetric FastICA estimator using the method of estimatingequation and M-estimator.
arxiv-14100-12 | Cross-Modal Similarity Learning : A Low Rank Bilinear Formulation | http://arxiv.org/pdf/1411.4738v2.pdf | author:Cuicui Kang, Shengcai Liao, Yonghao He, Jian Wang, Wenjia Niu, Shiming Xiang, Chunhong Pan category:cs.MM cs.IR cs.LG published:2014-11-18 summary:The cross-media retrieval problem has received much attention in recent yearsdue to the rapid increasing of multimedia data on the Internet. A new approachto the problem has been raised which intends to match features of differentmodalities directly. In this research, there are two critical issues: how toget rid of the heterogeneity between different modalities and how to match thecross-modal features of different dimensions. Recently metric learning methodsshow a good capability in learning a distance metric to explore therelationship between data points. However, the traditional metric learningalgorithms only focus on single-modal features, which suffer difficulties inaddressing the cross-modal features of different dimensions. In this paper, wepropose a cross-modal similarity learning algorithm for the cross-modal featurematching. The proposed method takes a bilinear formulation, and with thenuclear-norm penalization, it achieves low-rank representation. Accordingly,the accelerated proximal gradient algorithm is successfully imported to findthe optimal solution with a fast convergence rate O(1/t^2). Experiments onthree well known image-text cross-media retrieval databases show that theproposed method achieves the best performance compared to the state-of-the-artalgorithms.
arxiv-14100-13 | Numerical Demultiplexing of Color Image Sensor Measurements via Non-linear Random Forest Modeling | http://arxiv.org/pdf/1512.05421v1.pdf | author:Jason Deglint, Farnoud Kazemzadeh, Daniel Cho, David A. Clausi, Alexander Wong category:cs.CV published:2015-12-17 summary:The simultaneous capture of imaging data at multiple wavelengths across theelectromagnetic spectrum is highly challenging, requiring complex and costlymultispectral image sensors. In this study, we introduce a comprehensiveframework for performing simultaneous multispectral imaging using conventionalimage sensors with color filter arrays via numerical demultiplexing of thecolor image sensor measurements. A numerical forward model characterizing theformation of sensor measurements from light spectra hitting the sensor isconstructed based on a comprehensive spectral characterization of the sensor. Anumerical demultiplexer is then learned via non-linear random forest modelingbased on the forward model. Given the learned numerical demultiplexer, one canthen demultiplex simultaneously-acquired measurements made by the image sensorinto reflectance intensities at discrete selectable wavelengths, resulting in ahigher resolution reflectance spectrum. Simulation and real-world experimentalresults demonstrate the efficacy of such a method for simultaneousmultispectral imaging.
arxiv-14100-14 | Double or Nothing: Multiplicative Incentive Mechanisms for Crowdsourcing | http://arxiv.org/pdf/1408.1387v3.pdf | author:Nihar B. Shah, Dengyong Zhou category:cs.GT cs.HC cs.LG published:2014-08-06 summary:Crowdsourcing has gained immense popularity in machine learning applicationsfor obtaining large amounts of labeled data. Crowdsourcing is cheap and fast,but suffers from the problem of low-quality data. To address this fundamentalchallenge in crowdsourcing, we propose a simple payment mechanism toincentivize workers to answer only the questions that they are sure of and skipthe rest. We show that surprisingly, under a mild and natural "no-free-lunch"requirement, this mechanism is the one and only incentive-compatible paymentmechanism possible. We also show that among all possible incentive-compatiblemechanisms (that may or may not satisfy no-free-lunch), our mechanism makes thesmallest possible payment to spammers. We further extend our results to a moregeneral setting in which workers are required to provide a quantized confidencefor each question. Interestingly, this unique mechanism takes a"multiplicative" form. The simplicity of the mechanism is an added benefit. Inpreliminary experiments involving over 900 worker-task pairs, we observe asignificant drop in the error rates under this unique mechanism for the same orlower monetary expenditure.
arxiv-14100-15 | A Restricted Visual Turing Test for Deep Scene and Event Understanding | http://arxiv.org/pdf/1512.01715v2.pdf | author:Hang Qi, Tianfu Wu, Mun-Wai Lee, Song-Chun Zhu category:cs.CV cs.AI published:2015-12-06 summary:This paper presents a restricted visual Turing test (VTT) for story-linebased deep understanding in long-term and multi-camera captured videos. Given aset of videos of a scene (such as a multi-room office, a garden, and a parkinglot.) and a sequence of story-line based queries, the task is to provideanswers either simply in binary form "true/false" (to a polar query) or in anaccurate natural language description (to a non-polar query). Queries, polar ornon-polar, consist of view-based queries which can be answered from aparticular camera view and scene-centered queries which involves jointinference across different cameras. The story lines are collected to coverspatial, temporal and causal understanding of input videos. The data andqueries distinguish our VTT from recently proposed visual question answering inimages and video captioning. A vision system is proposed to perform joint videoand query parsing which integrates different vision modules, a knowledge baseand a query engine. The system provides unified interfaces for differentmodules so that individual modules can be reconfigured to test a new method. Weprovide a benchmark dataset and a toolkit for ontology guided story-line querygeneration which consists of about 93.5 hours videos captured in four differentlocations and 3,426 queries split into 127 story lines. We also provide abaseline implementation and result analyses.
arxiv-14100-16 | Shape and Spatially-Varying Reflectance Estimation From Virtual Exemplars | http://arxiv.org/pdf/1512.05278v1.pdf | author:Zhuo Hui, Aswin C Sankaranarayanan category:cs.CV published:2015-12-16 summary:This paper addresses the problem of estimating the shape of objects thatexhibit spatially-varying reflectance. We assume that multiple images of theobject are obtained under a fixed view-point and varying illumination, i.e.,the setting of photometric stereo. At the core of our techniques is theassumption that the BRDF at each pixel lies in the non-negative span of a knownBRDF dictionary.This assumption enables a per-pixel surface normal and BRDFestimation framework that is computationally tractable and requires noinitialization in spite of the underlying problem being non-convex. Ourestimation framework first solves for the surface normal at each pixel using avariant of example-based photometric stereo. We design an efficient multi-scalesearch strategy for estimating the surface normal and subsequently, refine thisestimate using a gradient descent procedure. Given the surface normal estimate,we solve for the spatially-varying BRDF by constraining the BRDF at each pixelto be in the span of the BRDF dictionary, here, we use additional priors tofurther regularize the solution. A hallmark of our approach is that it does notrequire iterative optimization techniques nor the need for carefulinitialization, both of which are endemic to most state-of-the-art techniques.We showcase the performance of our technique on a wide range of simulated andreal scenes where we outperform competing methods.
arxiv-14100-17 | Deep Reinforcement Learning with Attention for Slate Markov Decision Processes with High-Dimensional States and Actions | http://arxiv.org/pdf/1512.01124v2.pdf | author:Peter Sunehag, Richard Evans, Gabriel Dulac-Arnold, Yori Zwols, Daniel Visentin, Ben Coppin category:cs.AI cs.HC cs.LG published:2015-12-03 summary:Many real-world problems come with action spaces represented as featurevectors. Although high-dimensional control is a largely unsolved problem, therehas recently been progress for modest dimensionalities. Here we report on asuccessful attempt at addressing problems of dimensionality as high as $2000$,of a particular form. Motivated by important applications such asrecommendation systems that do not fit the standard reinforcement learningframeworks, we introduce Slate Markov Decision Processes (slate-MDPs). ASlate-MDP is an MDP with a combinatorial action space consisting of slates(tuples) of primitive actions of which one is executed in an underlying MDP.The agent does not control the choice of this executed action and the actionmight not even be from the slate, e.g., for recommendation systems for whichall recommendations can be ignored. We use deep Q-learning based on featurerepresentations of both the state and action to learn the value of wholeslates. Unlike existing methods, we optimize for both the combinatorial andsequential aspects of our tasks. The new agent's superiority over agents thateither ignore the combinatorial or sequential long-term value aspect isdemonstrated on a range of environments with dynamics from a real-worldrecommendation system. Further, we use deep deterministic policy gradients tolearn a policy that for each position of the slate, guides attention towardsthe part of the action space in which the value is the highest and we onlyevaluate actions in this area. The attention is used within a sequentiallygreedy procedure leveraging submodularity. Finally, we show how introducingrisk-seeking can dramatically improve the agents performance and ability todiscover more far reaching strategies.
arxiv-14100-18 | Blockout: Dynamic Model Selection for Hierarchical Deep Networks | http://arxiv.org/pdf/1512.05246v1.pdf | author:Calvin Murdock, Zhen Li, Howard Zhou, Tom Duerig category:cs.CV cs.LG published:2015-12-16 summary:Most deep architectures for image classification--even those that are trainedto classify a large number of diverse categories--learn shared imagerepresentations with a single model. Intuitively, however, categories that aremore similar should share more information than those that are very different.While hierarchical deep networks address this problem by learning separatefeatures for subsets of related categories, current implementations requiresimplified models using fixed architectures specified via heuristic clusteringmethods. Instead, we propose Blockout, a method for regularization and modelselection that simultaneously learns both the model architecture andparameters. A generalization of Dropout, our approach gives a novelparametrization of hierarchical architectures that allows for structurelearning via back-propagation. To demonstrate its utility, we evaluate Blockouton the CIFAR and ImageNet datasets, demonstrating improved classificationaccuracy, better regularization performance, faster training, and the clearemergence of hierarchical network structures.
arxiv-14100-19 | Symphony from Synapses: Neocortex as a Universal Dynamical Systems Modeller using Hierarchical Temporal Memory | http://arxiv.org/pdf/1512.05245v1.pdf | author:Fergal Byrne category:cs.NE cs.AI q-bio.NC published:2015-12-16 summary:Reverse engineering the brain is proving difficult, perhaps impossible. Whilemany believe that this is just a matter of time and effort, a differentapproach might help. Here, we describe a very simple idea which explains thepower of the brain as well as its structure, exploiting complex dynamics ratherthan abstracting it away. Just as a Turing Machine is a Universal DigitalComputer operating in a world of symbols, we propose that the brain is aUniversal Dynamical Systems Modeller, evolved bottom-up (itself using nestednetworks of interconnected, self-organised dynamical systems) to prosper in aworld of dynamical systems. Recent progress in Applied Mathematics has produced startling evidence ofwhat happens when abstract Dynamical Systems interact. Key latent informationdescribing system A can be extracted by system B from very simple signals, andsignals can be used by one system to control and manipulate others. Using thesefacts, we show how a region of the neocortex uses its dynamics to intrinsically"compute" about the external and internal world. Building on an existing "static" model of cortical computation (Hawkins'Hierarchical Temporal Memory - HTM), we describe how a region of neocortex canbe viewed as a network of components which together form a Dynamical Systemsmodelling module, connected via sensory and motor pathways to the externalworld, and forming part of a larger dynamical network in the brain. Empirical modelling and simulations of Dynamical HTM are possible with simpleextensions and combinations of currently existing open source software. We lista number of relevant projects.
arxiv-14100-20 | Learning a Hybrid Architecture for Sequence Regression and Annotation | http://arxiv.org/pdf/1512.05219v1.pdf | author:Yizhe Zhang, Ricardo Henao, Lawrence Carin, Jianling Zhong, Alexander J. Hartemink category:stat.ML published:2015-12-16 summary:When learning a hidden Markov model (HMM), sequen- tial observations canoften be complemented by real-valued summary response variables generated fromthe path of hid- den states. Such settings arise in numerous domains, includ-ing many applications in biology, like motif discovery and genome annotation.In this paper, we present a flexible frame- work for jointly modeling bothlatent sequence features and the functional mapping that relates the summaryresponse variables to the hidden state sequence. The algorithm is com- patiblewith a rich set of mapping functions. Results show that the availability ofadditional continuous response vari- ables can simultaneously improve theannotation of the se- quential observations and yield good predictionperformance in both synthetic data and real-world datasets.
arxiv-14100-21 | DNA-Level Splice Junction Prediction using Deep Recurrent Neural Networks | http://arxiv.org/pdf/1512.05135v1.pdf | author:Byunghan Lee, Taehoon Lee, Byunggook Na, Sungroh Yoon category:cs.LG q-bio.GN published:2015-12-16 summary:A eukaryotic gene consists of multiple exons (protein coding regions) andintrons (non-coding regions), and a splice junction refers to the boundarybetween a pair of exon and intron. Precise identification of spice junctions ona gene is important for deciphering its primary structure, function, andinteraction. Experimental techniques for determining exon/intron boundariesinclude RNA-seq, which is often accompanied by computational approaches.Canonical splicing signals are known, but computational junction predictionstill remains challenging because of a large number of false positives andother complications. In this paper, we exploit deep recurrent neural networks(RNNs) to model DNA sequences and to detect splice junctions thereon. We testvarious RNN units and architectures including long short-term memory units,gated recurrent units, and recently proposed iRNN for in-depth design spaceexploration. According to our experimental results, the proposed approachsignificantly outperforms not only conventional machine learning-based methodsbut also a recent state-of-the-art deep belief network-based technique in termsof prediction accuracy.
arxiv-14100-22 | Effects of GIMP Retinex Filtering Evaluated by the Image Entropy | http://arxiv.org/pdf/1512.05653v1.pdf | author:A. C. Sparavigna, R. Marazzato category:cs.CV published:2015-12-16 summary:A GIMP Retinex filtering can be used for enhancing images, with good resultson foggy images, as recently discussed. Since this filter has some parametersthat can be adjusted to optimize the output image, several approaches can bedecided according to desired results. Here, as a criterion for optimizing thefiltering parameters, we consider the maximization of the image entropy. Weuse, besides the Shannon entropy, also a generalized entropy.
arxiv-14100-23 | A Novel Minimum Divergence Approach to Robust Speaker Identification | http://arxiv.org/pdf/1512.05073v1.pdf | author:Ayanendranath Basu, Smarajit Bose, Amita Pal, Anish Mukherjee, Debasmita Das category:stat.ML cs.SD stat.AP published:2015-12-16 summary:In this work, a novel solution to the speaker identification problem isproposed through minimization of statistical divergences between theprobability distribution (g). of feature vectors from the test utterance andthe probability distributions of the feature vector corresponding to thespeaker classes. This approach is made more robust to the presence of outliers,through the use of suitably modified versions of the standard divergencemeasures. The relevant solutions to the minimum distance methods are referredto as the minimum rescaled modified distance estimators (MRMDEs). Threemeasures were considered - the likelihood disparity, the Hellinger distance andPearson's chi-square distance. The proposed approach is motivated by theobservation that, in the case of the likelihood disparity, when the empiricaldistribution function is used to estimate g, it becomes equivalent to maximumlikelihood classification with Gaussian Mixture Models (GMMs) for speakerclasses, a highly effective approach used, for example, by Reynolds [22] basedon Mel Frequency Cepstral Coefficients (MFCCs) as features. Significantimprovement in classification accuracy is observed under this approach on thebenchmark speech corpus NTIMIT and a new bilingual speech corpus NISIS, withMFCC features, both in isolation and in combination with delta MFCC features.Moreover, the ubiquitous principal component transformation, by itself and inconjunction with the principle of classifier combination, is found to furtherenhance the performance.
arxiv-14100-24 | Scalable Gradient-Based Tuning of Continuous Regularization Hyperparameters | http://arxiv.org/pdf/1511.06727v2.pdf | author:Jelena Luketina, Mathias Berglund, Tapani Raiko category:cs.LG published:2015-11-20 summary:Hyperparameter selection generally relies on running multiple full trainingtrials, with hyperparameter selection based on validation set performance. Wepropose a gradient-based approach for locally adjusting hyperparameters on thefly in which we adjust the hyperparameters so as to make the model parametergradients, and hence updates, more advantageous for the validation cost. Weexplore the approach for tuning regularization hyperparameters and find that inexperiments on MNIST the resulting regularization levels are within the optimalregions. The method is less computationally demanding compared to similargradient-based approaches to hyperparameter selection, only requires a fewtrials, and consistently finds solid hyperparameter values which makes it auseful tool for training neural network models.
arxiv-14100-25 | Streaming Kernel Principal Component Analysis | http://arxiv.org/pdf/1512.05059v1.pdf | author:Mina Ghashami, Daniel Perry, Jeff M. Phillips category:cs.DS cs.LG stat.ML published:2015-12-16 summary:Kernel principal component analysis (KPCA) provides a concise set of basisvectors which capture non-linear structures within large data sets, and is acentral tool in data analysis and learning. To allow for non-linear relations,typically a full $n \times n$ kernel matrix is constructed over $n$ datapoints, but this requires too much space and time for large values of $n$.Techniques such as the Nystr\"om method and random feature maps can helptowards this goal, but they do not explicitly maintain the basis vectors in astream and take more space than desired. We propose a new approach forstreaming KPCA which maintains a small set of basis elements in a stream,requiring space only logarithmic in $n$, and also improves the dependence onthe error parameter. Our technique combines together random feature maps withrecent advances in matrix sketching, it has guaranteed spectral norm errorbounds with respect to the original kernel matrix, and it compares favorably inpractice to state-of-the-art approaches.
arxiv-14100-26 | Inferring Gene Regulatory Network Using An Evolutionary Multi-Objective Method | http://arxiv.org/pdf/1512.05055v1.pdf | author:Yu Chen, Xiufen Zou category:cs.CE cs.NE q-bio.QM 92B99 published:2015-12-16 summary:Inference of gene regulatory networks (GRNs) based on experimental data is achallenging task in bioinformatics. In this paper, we present a bi-objectiveminimization model (BoMM) for inference of GRNs, where one objective is thefitting error of derivatives, and the other is the number of connections in thenetwork. To solve the BoMM efficiently, we propose a multi-objectiveevolutionary algorithm (MOEA), and utilize the separable parameter estimationmethod (SPEM) decoupling the ordinary differential equation (ODE) system. Then,the Akaike Information Criterion (AIC) is employed to select one inferenceresult from the obtained Pareto set. Taking the S-system as the investigatedGRN model, our method can properly identify the topologies and parameter valuesof benchmark systems. There is no need to preset problem-dependent parametervalues to obtain appropriate results, and thus, our method could be applicableto inference of various GRNs models.
arxiv-14100-27 | Wood Species Recognition Based on SIFT Keypoint Histogram | http://arxiv.org/pdf/1511.01804v4.pdf | author:Shuaiqi Hu, Ke Li, Xudong Bao category:cs.CV published:2015-11-05 summary:Traditionally, only experts who are equipped with professional knowledge andrich experience are able to recognize different species of wood. Applying imageprocessing techniques for wood species recognition can not only reduce theexpense to train qualified identifiers, but also increase the recognitionaccuracy. In this paper, a wood species recognition technique base on ScaleInvariant Feature Transformation (SIFT) keypoint histogram is proposed. We usefirst the SIFT algorithm to extract keypoints from wood cross section images,and then k-means and k-means++ algorithms are used for clustering. Using theclustering results, an SIFT keypoints histogram is calculated for each woodimage. Furthermore, several classification models, including Artificial NeuralNetworks (ANN), Support Vector Machine (SVM) and K-Nearest Neighbor (KNN) areused to verify the performance of the method. Finally, through comparing withother prevalent wood recognition methods such as GLCM and LBP, results showthat our scheme achieves higher accuracy.
arxiv-14100-28 | Zero-Shot Event Detection by Multimodal Distributional Semantic Embedding of Videos | http://arxiv.org/pdf/1512.00818v2.pdf | author:Mohamed Elhoseiny, Jingen Liu, Hui Cheng, Harpreet Sawhney, Ahmed Elgammal category:cs.CV cs.CL cs.LG published:2015-12-02 summary:We propose a new zero-shot Event Detection method by Multi-modalDistributional Semantic embedding of videos. Our model embeds object and actionconcepts as well as other available modalities from videos into adistributional semantic space. To our knowledge, this is the first Zero-Shotevent detection model that is built on top of distributional semantics andextends it in the following directions: (a) semantic embedding of multimodalinformation in videos (with focus on the visual modalities), (b) automaticallydetermining relevance of concepts/attributes to a free text query, which couldbe useful for other applications, and (c) retrieving videos by free text eventquery (e.g., "changing a vehicle tire") based on their content. We embed videosinto a distributional semantic space and then measure the similarity betweenvideos and the event query in a free text form. We validated our method on thelarge TRECVID MED (Multimedia Event Detection) challenge. Using only the eventtitle as a query, our method outperformed the state-of-the-art that uses bigdescriptions from 12.6% to 13.5% with MAP metric and 0.73 to 0.83 with ROC-AUCmetric. It is also an order of magnitude faster.
arxiv-14100-29 | Admissibility of a posterior predictive decision rule | http://arxiv.org/pdf/1507.06350v3.pdf | author:Giri Gopalan category:stat.ML published:2015-07-22 summary:Recent decades have seen an interest in prediction problems for whichBayesian methodology has been extremely useful. Sampling from or approximatingthe posterior predictive distribution in a Bayesian model allows one to makeinference about potentially observable random quantities given observed data.The purpose of this note is to use statistical decision theory as a basis tojustify the use of a posterior predictive distribution for making a pointprediction.
arxiv-14100-30 | Multiple penalized principal curves: analysis and computation | http://arxiv.org/pdf/1512.05010v1.pdf | author:Slav Kirov, Dejan Slepčev category:math.AP cs.CV stat.ML published:2015-12-15 summary:We study the problem of finding the one-dimensional structure in a given dataset. In other words we consider ways to approximate a given measure (data) bycurves. We consider an objective functional whose minimizers are aregularization of principal curves and introduce a new functional which allowsfor multiple curves. We prove the existence of minimizers and establish theirbasic properties. We develop an efficient algorithm for obtaining (near)minimizers of the functional. While both of the functionals used are nonconvex,we argue that enlarging the configuration space to allow for multiple curvesleads to a simpler energy landscape with fewer undesirable (high-energy) localminima. Furthermore we note that the approach proposed is able to find theone-dimensional structure even for data with considerable amount of noise.
arxiv-14100-31 | An Average Classification Algorithm | http://arxiv.org/pdf/1506.01520v3.pdf | author:Brendan van Rooyen, Aditya Krishna Menon, Robert C. Williamson category:stat.ML cs.LG published:2015-06-04 summary:Many classification algorithms produce a classifier that is a weightedaverage of kernel evaluations. When working with a high or infinite dimensionalkernel, it is imperative for speed of evaluation and storage issues that as fewtraining samples as possible are used in the kernel expansion. Popular existingapproaches focus on altering standard learning algorithms, such as the SupportVector Machine, to induce sparsity, as well as post-hoc procedures for sparseapproximations. Here we adopt the latter approach. We begin with a very simpleclassifier, given by the kernel mean $$ f(x) = \frac{1}{n}\sum\limits_{i=i}^{n} y_i K(x_i,x) $$ We then find a sparse approximation tothis kernel mean via herding. The result is an accurate, easily parallelizedalgorithm for learning classifiers.
arxiv-14100-32 | Adopting Robustness and Optimality in Fitting and Learning | http://arxiv.org/pdf/1510.03826v3.pdf | author:Zhiguang Wang, Tim Oates, James Lo category:cs.LG cs.NE math.OC published:2015-10-13 summary:We generalized a modified exponentialized estimator by pushing therobust-optimal (RO) index $\lambda$ to $-\infty$ for achieving robustness tooutliers by optimizing a quasi-Minimin function. The robustness is realized andcontrolled adaptively by the RO index without any predefined threshold.Optimality is guaranteed by expansion of the convexity region in the Hessianmatrix to largely avoid local optima. Detailed quantitative analysis on bothrobustness and optimality are provided. The results of proposed experiments onfitting tasks for three noisy non-convex functions and the digits recognitiontask on the MNIST dataset consolidate the conclusions.
arxiv-14100-33 | An Operator for Entity Extraction in MapReduce | http://arxiv.org/pdf/1512.04973v1.pdf | author:Ndapandula Nakashole category:cs.DB cs.CL 68T50 published:2015-12-15 summary:Dictionary-based entity extraction involves finding mentions of dictionaryentities in text. Text mentions are often noisy, containing spurious or missingwords. Efficient algorithms for detecting approximate entity mentions followone of two general techniques. The first approach is to build an index on theentities and perform index lookups of document substrings. The second approachrecognizes that the number of substrings generated from documents can explodeto large numbers, to get around this, they use a filter to prune many suchsubstrings which do not match any dictionary entity and then only verify theremaining substrings if they are entity mentions of dictionary entities, bymeans of a text join. The choice between the index-based approach and thefilter & verification-based approach is a case-to-case decision as the bestapproach depends on the characteristics of the input entity dictionary, forexample frequency of entity mentions. Choosing the right approach for thesetting can make a substantial difference in execution time. Making this choiceis however non-trivial as there are parameters within each of the approachesthat make the space of possible approaches very large. In this paper, wepresent a cost-based operator for making the choice among execution plans forentity extraction. Since we need to deal with large dictionaries and evenlarger large datasets, our operator is developed for implementations ofMapReduce distributed algorithms.
arxiv-14100-34 | A Light Touch for Heavily Constrained SGD | http://arxiv.org/pdf/1512.04960v1.pdf | author:Andrew Cotter, Maya Gupta, Jan Pfeifer category:cs.LG published:2015-12-15 summary:Projected stochastic gradient descent (SGD) is often the default choice forlarge-scale optimization in machine learning, but requires a projection aftereach update. For heavily-constrained objectives, we propose an efficientextension of SGD that stays close to the feasible region while only applyingconstraints probabilistically at each iteration. Theoretical analysis shows agood trade-off between per-iteration work and the number of iterations needed,indicating compelling advantages on problems with a large number of constraintsonto which projecting is expensive. In MATLAB experiments, our algorithmsuccessfully handles a large-scale real-world video ranking problem with tensof thousands of linear inequality constraints that was too large for projectedSGD and stochastic Frank-Wolfe.
arxiv-14100-35 | Context Driven Label Fusion for segmentation of Subcutaneous and Visceral Fat in CT Volumes | http://arxiv.org/pdf/1512.04958v1.pdf | author:Sarfaraz Hussein, Aileen Green, Arjun Watane, Georgios Papadakis, Medhat Osman, Ulas Bagci category:cs.CV published:2015-12-15 summary:Quantification of adipose tissue (fat) from computed tomography (CT) scans isconducted mostly through manual or semi-automated image segmentation algorithmswith limited efficacy. In this work, we propose a completely unsupervised andautomatic method to identify adipose tissue, and then separate SubcutaneousAdipose Tissue (SAT) from Visceral Adipose Tissue (VAT) at the abdominalregion. We offer a three-phase pipeline consisting of (1) Initial boundaryestimation using gradient points, (2) boundary refinement using GeometricMedian Absolute Deviation and Appearance based Local Outlier Scores (3) Contextdriven label fusion using Conditional Random Fields (CRF) to obtain the finalboundary between SAT and VAT. We evaluate the proposed method on 151 abdominalCT scans and obtain state-of-the-art 94% and 91% dice similarity scores for SATand VAT segmentation, as well as significant reduction in fat quantificationerror measure.
arxiv-14100-36 | Relative Density and Exact Recovery in Heterogeneous Stochastic Block Models | http://arxiv.org/pdf/1512.04937v1.pdf | author:Amin Jalali, Qiyang Han, Ioana Dumitriu, Maryam Fazel category:stat.ML published:2015-12-15 summary:The Stochastic Block Model (SBM) is a widely used random graph model fornetworks with communities. Despite the recent burst of interest in recoveringcommunities in the SBM from statistical and computational points of view, thereare still gaps in understanding the fundamental information theoretic andcomputational limits of recovery. In this paper, we consider the SBM in itsfull generality, where there is no restriction on the number and sizes ofcommunities or how they grow with the number of nodes, as well as on theconnection probabilities inside or across communities. This generality allowsus to move past the artifacts of homogenous SBM, and understand the rightparameters (such as the relative densities of communities) that define thevarious recovery thresholds. We outline the implications of our generalizationsvia a set of illustrative examples. For instance, $\log n$ is considered to bethe standard lower bound on the cluster size for exact recovery via convexmethods, for homogenous SBM. We show that it is possible, in the rightcircumstances (when sizes are spread and the smaller the cluster, the denser),to recover very small clusters (up to $\sqrt{\log n}$ size), if there are justa few of them (at most polylogarithmic in $n$).
arxiv-14100-37 | Strategies for Training Large Vocabulary Neural Language Models | http://arxiv.org/pdf/1512.04906v1.pdf | author:Welin Chen, David Grangier, Michael Auli category:cs.CL cs.LG published:2015-12-15 summary:Training neural network language models over large vocabularies is stillcomputationally very costly compared to count-based models such as Kneser-Ney.At the same time, neural language models are gaining popularity for manyapplications such as speech recognition and machine translation whose successdepends on scalability. We present a systematic comparison of strategies torepresent and train large vocabularies, including softmax, hierarchicalsoftmax, target sampling, noise contrastive estimation and self normalization.We further extend self normalization to be a proper estimator of likelihood andintroduce an efficient variant of softmax. We evaluate each method on threepopular benchmarks, examining performance on rare words, the speed/accuracytrade-off and complementarity to Kneser-Ney.
arxiv-14100-38 | Fast Steerable Principal Component Analysis | http://arxiv.org/pdf/1412.0781v5.pdf | author:Zhizhen Zhao, Yoel Shkolnisky, Amit Singer category:cs.CV published:2014-12-02 summary:Cryo-electron microscopy nowadays often requires the analysis of hundreds ofthousands of 2D images as large as a few hundred pixels in each direction. Herewe introduce an algorithm that efficiently and accurately performs principalcomponent analysis (PCA) for a large set of two-dimensional images, and, foreach image, the set of its uniform rotations in the plane and theirreflections. For a dataset consisting of $n$ images of size $L \times L$pixels, the computational complexity of our algorithm is $O(nL^3 + L^4)$, whileexisting algorithms take $O(nL^4)$. The new algorithm computes the expansioncoefficients of the images in a Fourier-Bessel basis efficiently using thenon-uniform fast Fourier transform. We compare the accuracy and efficiency ofthe new algorithm with traditional PCA and existing algorithms for steerablePCA.
arxiv-14100-39 | Radon-Nikodym approximation in application to image analysis | http://arxiv.org/pdf/1511.01887v2.pdf | author:Vladislav Gennadievich Malyshkin category:cs.CV published:2015-11-05 summary:For an image pixel information can be converted to the moments of some basis$Q_k$, e.g. Fourier-Mellin, Zernike, monomials, etc. Given sufficient number ofmoments pixel information can be completely recovered, for insufficient numberof moments only partial information can be recovered and the imagereconstruction is, at best, of interpolatory type. Standard approach is topresent interpolated value as a linear combination of basis functions, what isequivalent to least squares expansion. However, recent progress in numericalstability of moments estimation allows image information to be recovered frommoments in a completely different manner, applying Radon-Nikodym type ofexpansion, what gives the result as a ratio of two quadratic forms of basisfunctions. In contrast with least squares the Radon-Nikodym approach hasoscillation near the boundaries very much suppressed and does not divergeoutside of basis support. While least squares theory operate with vectors$<fQ_k>$, Radon-Nikodym theory operates with matrices $<fQ_jQ_k>$, what makethe approach much more suitable to image transforms and statistical propertyestimation.
arxiv-14100-40 | Norm-Free Radon-Nikodym Approach to Machine Learning | http://arxiv.org/pdf/1512.03219v2.pdf | author:Vladislav Gennadievich Malyshkin category:cs.LG stat.ML published:2015-12-10 summary:For Machine Learning (ML) classification problem, where a vector of$\mathbf{x}$--observations (values of attributes) is mapped to a single $y$value (class label), a generalized Radon--Nikodym type of solution is proposed.Quantum--mechanics --like probability states $\psi^2(\mathbf{x})$ areconsidered and "Cluster Centers", corresponding to the extremums of$<y\psi^2(\mathbf{x})>/<\psi^2(\mathbf{x})>$, are found from generalizedeigenvalues problem. The eigenvalues give possible $y^{[i]}$ outcomes andcorresponding to them eigenvectors $\psi^{[i]}(\mathbf{x})$ define "ClusterCenters". The projection of a $\psi$ state, localized at given $\mathbf{x}$ toclassify, on these eigenvectors define the probability of $y^{[i]}$ outcome,thus avoiding using a norm ($L^2$ or other types), required for "qualitycriteria" in a typical Machine Learning technique. A coverage of each `ClusterCenter" is calculated, what potentially allows to separate system properties(described by $y^{[i]}$ outcomes) and system testing conditions (described by$C^{[i]}$ coverage). As an example of such application $y$ distributionestimator is proposed in a form of pairs $(y^{[i]},C^{[i]})$, that can beconsidered as Gauss quadratures generalization. This estimator allows toperform $y$ probability distribution estimation in a strongly non--Gaussiancase.
arxiv-14100-41 | Increasing the Action Gap: New Operators for Reinforcement Learning | http://arxiv.org/pdf/1512.04860v1.pdf | author:Marc G. Bellemare, Georg Ostrovski, Arthur Guez, Philip S. Thomas, Rémi Munos category:cs.AI cs.LG published:2015-12-15 summary:This paper introduces new optimality-preserving operators on Q-functions. Wefirst describe an operator for tabular representations, the consistent Bellmanoperator, which incorporates a notion of local policy consistency. We show thatthis local consistency leads to an increase in the action gap at each state;increasing this gap, we argue, mitigates the undesirable effects ofapproximation and estimation errors on the induced greedy policies. Thisoperator can also be applied to discretized continuous space and time problems,and we provide empirical results evidencing superior performance in thiscontext. Extending the idea of a locally consistent operator, we then derivesufficient conditions for an operator to preserve optimality, leading to afamily of operators which includes our consistent Bellman operator. Ascorollaries we provide a proof of optimality for Baird's advantage learningalgorithm and derive other gap-increasing operators with interestingproperties. We conclude with an empirical study on 60 Atari 2600 gamesillustrating the strong potential of these new operators.
arxiv-14100-42 | Energy-Efficient Classification for Anomaly Detection: The Wireless Channel as a Helper | http://arxiv.org/pdf/1512.04857v1.pdf | author:Kiril Ralinovski, Mario Goldenbaum, Sławomir Stańczak category:cs.IT cs.LG math.IT published:2015-12-15 summary:Anomaly detection has various applications including condition monitoring andfault diagnosis. The objective is to sense the environment, learn the normalsystem state, and then periodically classify whether the instantaneous statedeviates from the normal one or not. A flexible and cost-effective way ofmonitoring a system state is to use a wireless sensor network. In thetraditional approach, the sensors encode their observations and transmit themto a fusion center by means of some interference avoiding channel accessmethod. The fusion center then decodes all the data and classifies thecorresponding system state. As this approach can be highly inefficient in termsof energy consumption, in this paper we propose a transmission scheme thatexploits interference for carrying out the anomaly detection directly in theair. In other words, the wireless channel helps the fusion center to retrievethe sought classification outcome immediately from the channel output. Toachieve this, the chosen learning model is linear support vector machines.After discussing the proposed scheme and proving its reliability, we presentnumerical examples demonstrating that the scheme reduces the energy consumptionfor anomaly detection by up to 53% compared to a strategy that uses timedivision multiple-access.
arxiv-14100-43 | Data Driven Resource Allocation for Distributed Learning | http://arxiv.org/pdf/1512.04848v1.pdf | author:Travis Dick, Mu Li, Venkata Krishna Pillutla, Colin White, Maria Florina Balcan, Alex Smola category:cs.LG cs.DS stat.ML published:2015-12-15 summary:In distributed machine learning, data is dispatched to multiple machines forprocessing. Motivated by the fact that similar data points are often belongingto the same or similar classes, and more generally, classification rules ofhigh accuracy tend to be "locally simple but globally complex", we propose datadependent dispatching that takes advantage of such structures. Our maintechnical contribution is to provide algorithms with provable guarantees fordata-dependent dispatching, that partition the data in a way that satisfiesimportant conditions for accurate distributed learning, including faulttolerance and balancedness. We show the effectiveness of our method over thewidely used random partitioning scheme in several real world image andadvertising datasets.
arxiv-14100-44 | Feature-Level Domain Adaptation | http://arxiv.org/pdf/1512.04829v1.pdf | author:Wouter M. Kouw, Jesse H. Krijthe, Marco Loog, Laurens J. P. van der Maaten category:stat.ML cs.LG published:2015-12-15 summary:Domain adaptation is the supervised learning setting in which the trainingand test data originate from different domains: the so-called source and targetdomains. In this paper, we propose and study a domain adaption approach, calledfeature-level domain adaptation (flda), that models the dependence between twodomains by means of a feature-level transfer distribution. The domain adaptedclassifier is trained by minimizing the expected loss under this transferdistribution. Our empirical evaluation of flda focuses on problems with binaryand count features in which the domain adaptation can be naturally modeled viaa dropout distribution, which allows the final classifier to adapt to theimportance of specific features in the target data. Our experimental evaluationsuggests that under certain conditions, flda converges to the classifiertrained on the target distribution. Experiments with our domain adaptationapproach on several real-world problems show that flda performs on par withstate-of-the-art techniques in domain adaptation.
arxiv-14100-45 | Causal and anti-causal learning in pattern recognition for neuroimaging | http://arxiv.org/pdf/1512.04808v1.pdf | author:Sebastian Weichwald, Bernhard Schölkopf, Tonio Ball, Moritz Grosse-Wentrup category:stat.ML cs.LG q-bio.NC stat.ME published:2015-12-15 summary:Pattern recognition in neuroimaging distinguishes between two types ofmodels: encoding- and decoding models. This distinction is based on the insightthat brain state features, that are found to be relevant in an experimentalparadigm, carry a different meaning in encoding- than in decoding models. Inthis paper, we argue that this distinction is not sufficient: Relevant featuresin encoding- and decoding models carry a different meaning depending on whetherthey represent causal- or anti-causal relations. We provide a theoreticaljustification for this argument and conclude that causal inference is essentialfor interpretation in neuroimaging.
arxiv-14100-46 | On Deep Representation Learning from Noisy Web Images | http://arxiv.org/pdf/1512.04785v1.pdf | author:Phong D. Vo, Alexandru Ginsca, Hervé Le Borgne, Adrian Popescu category:cs.CV cs.MM published:2015-12-15 summary:The keep-growing content of Web images may be the next important data sourceto scale up deep neural networks, which recently obtained a great success inthe ImageNet classification challenge and related tasks. This prospect,however, has not been validated on convolutional networks (convnet) -- one ofbest performing deep models -- because of their supervised regime. Whileunsupervised alternatives are not so good as convnet in generalizing thelearned model to new domains, we use convnet to leverage semi-supervisedrepresentation learning. Our approach is to use massive amounts of unlabeledand noisy Web images to train convnets as general feature detectors despitechallenges coming from data such as high level of mislabeled data, outliers,and data biases. Extensive experiments are conducted at several data scales,different network architectures, and data reranking techniques. The learnedrepresentations are evaluated on nine public datasets of various topics. Thebest results obtained by our convnets, trained on 3.14 million Web images,outperform AlexNet trained on 1.2 million clean images of ILSVRC 2012 and isclosing the gap with VGG-16. These prominent results suggest a budget solutionto use deep learning in practice and motivate more research in semi-supervisedrepresentation learning.
arxiv-14100-47 | Learning optimal nonlinearities for iterative thresholding algorithms | http://arxiv.org/pdf/1512.04754v1.pdf | author:Ulugbek S. Kamilov, Hassan Mansour category:cs.LG stat.ML published:2015-12-15 summary:Iterative shrinkage/thresholding algorithm (ISTA) is a well-studied methodfor finding sparse solutions to ill-posed inverse problems. In this letter, wepresent a data-driven scheme for learning optimal thresholding functions forISTA. The proposed scheme is obtained by relating iterations of ISTA to layersof a simple deep neural network (DNN) and developing a corresponding errorbackpropagation algorithm that allows to fine-tune the thresholding functions.Simulations on sparse statistical signals illustrate potential gains inestimation quality due to the proposed data adaptive ISTA.
arxiv-14100-48 | Multimodal, high-dimensional, model-based, Bayesian inverse problems with applications in biomechanics | http://arxiv.org/pdf/1512.04481v2.pdf | author:Isabell M. Franck, P. S. Koutsourelakis category:stat.CO stat.ML published:2015-12-14 summary:This paper is concerned with the numerical solution of model-based, Bayesianinverse problems. We are particularly interested in cases where the cost ofeach likelihood evaluation (forward-model call) is expensive and the number ofunknown (latent) variables is high. This is the setting of many problems incomputational physics where forward models with nonlinear PDEs are used and theparameters to be calibrated involve spatio-temporarily varying coefficients,which upon discretization give rise to a high-dimensional vector of unknowns. One of the consequences of the well-documented, ill-posedness of inverseproblems is the possibility of multiple solutions. While such information iscontained in the posterior density in Bayesian formulations, the discovery of asingle mode, let alone multiple, is a formidable task. The goal of the presentpaper is two-fold. On one hand, we propose approximate, adaptive inferencestrategies using mixture densities to capture multimodal posteriors, and on theother, to extend our work in \cite{franck_sparse_2015} with regards toeffective dimensionality reduction techniques that reveal low-dimensionalsubspaces where the posterior variance is mostly concentrated. We validate theproposed methodology in a problem in nonlinear elastography where theidentification of the mechanical properties of biological materials can informnon-invasive, medical diagnosis. The discovery of multiple modes (solutions) insuch problems is critical in achieving the diagnostic objectives.
arxiv-14100-49 | Joint Image-Text News Topic Detection and Tracking with And-Or Graph Representation | http://arxiv.org/pdf/1512.04701v1.pdf | author:Weixin Li, Jungseock Joo, Hang Qi, Song-Chun Zhu category:cs.IR cs.CL cs.SI published:2015-12-15 summary:In this paper, we aim to develop a method for automatically detecting andtracking topics in broadcast news. We present a hierarchical And-Or graph (AOG)to jointly represent the latent structure of both texts and visuals. The AOGembeds a context sensitive grammar that can describe the hierarchicalcomposition of news topics by semantic elements about people involved, relatedplaces and what happened, and model contextual relationships between elementsin the hierarchy. We detect news topics through a cluster sampling processwhich groups stories about closely related events. Swendsen-Wang Cuts (SWC), aneffective cluster sampling algorithm, is adopted for traversing the solutionspace and obtaining optimal clustering solutions by maximizing a Bayesianposterior probability. Topics are tracked to deal with the continuously updatednews streams. We generate topic trajectories to show how topics emerge, evolveand disappear over time. The experimental results show that our method canexplicitly describe the textual and visual data in news videos and producemeaningful topic trajectories. Our method achieves superior performancecompared to state-of-the-art methods on both a public dataset Reuters-21578 anda self-collected dataset named UCLA Broadcast News Dataset.
arxiv-14100-50 | Learning Discrete Bayesian Networks from Continuous Data | http://arxiv.org/pdf/1512.02406v2.pdf | author:Yi-Chun Chen, Tim Allan Wheeler, Mykel John Kochenderfer category:cs.AI cs.LG published:2015-12-08 summary:Real data often contains a mixture of discrete and continuous variables, butmany Bayesian network structure learning and inference algorithms assume allrandom variables are discrete. Continuous variables are often discretized, butthe choice of discretization policy has significant impact on the accuracy,speed, and interpretability of the resulting models. This paper introduces aprincipled Bayesian discretization method for continuous variables in Bayesiannetworks with quadratic complexity instead of the cubic complexity of otherstandard techniques. Empirical demonstrations show that the proposed method issuperior to the state of the art. In addition, this paper shows how toincorporate existing methods into the structure learning process to discretizeall continuous variables and simultaneously learn Bayesian network structures.
arxiv-14100-51 | Computational Models for Multiview Dense Depth Maps of Dynamic Scene | http://arxiv.org/pdf/1512.02329v2.pdf | author:Qifei Wang category:cs.CV published:2015-12-08 summary:This paper reviews the recent progresses of the depth map generation fordynamic scene and its corresponding computational models. This paper mainlycovers the homogeneous ambiguity models in depth sensing, resolution models indepth processing, and consistency models in depth optimization. We alsosummarize the future work in the depth map generation.
arxiv-14100-52 | Simple Baseline for Visual Question Answering | http://arxiv.org/pdf/1512.02167v2.pdf | author:Bolei Zhou, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, Rob Fergus category:cs.CV cs.CL published:2015-12-07 summary:We describe a very simple bag-of-words baseline for visual questionanswering. This baseline concatenates the word features from the question andCNN features from the image to predict the answer. When evaluated on thechallenging VQA dataset [2], it shows comparable performance to many recentapproaches using recurrent neural networks. To explore the strength andweakness of the trained model, we also provide an interactive web demo andopen-source code. .
arxiv-14100-53 | Neural Network Matrix Factorization | http://arxiv.org/pdf/1511.06443v2.pdf | author:Gintare Karolina Dziugaite, Daniel M. Roy category:cs.LG stat.ML published:2015-11-19 summary:Data often comes in the form of an array or matrix. Matrix factorizationtechniques attempt to recover missing or corrupted entries by assuming that thematrix can be written as the product of two low-rank matrices. In other words,matrix factorization approximates the entries of the matrix by a simple, fixedfunction---namely, the inner product---acting on the latent feature vectors forthe corresponding row and column. Here we consider replacing the inner productby an arbitrary function that we learn from the data at the same time as welearn the latent feature vectors. In particular, we replace the inner productby a multi-layer feed-forward neural network, and learn by alternating betweenoptimizing the network for fixed latent features, and optimizing the latentfeatures for a fixed network. The resulting approach---which we call neuralnetwork matrix factorization or NNMF, for short---dominates standard low-ranktechniques on a suite of benchmark but is dominated by some recent proposalsthat take advantage of the graph features. Given the vast range ofarchitectures, activation functions, regularizers, and optimization techniquesthat could be used within the NNMF framework, it seems likely the truepotential of the approach has yet to be reached.
arxiv-14100-54 | Linear Models of Computation and Program Learning | http://arxiv.org/pdf/1512.04639v1.pdf | author:Michael Bukatin, Steve Matthews category:cs.LO cs.NE published:2015-12-15 summary:We consider two classes of computations which admit taking linearcombinations of execution runs: probabilistic sampling and generalizedanimation. We argue that the task of program learning should be more tractablefor these architectures than for conventional deterministic programs. We lookat the recent advances in the "sampling the samplers" paradigm in higher-orderprobabilistic programming. We also discuss connections between partialinconsistency, non-monotonic inference, and vector semantics.
arxiv-14100-55 | Noise-Compensated, Bias-Corrected Diffusion Weighted Endorectal Magnetic Resonance Imaging via a Stochastically Fully-Connected Joint Conditional Random Field Model | http://arxiv.org/pdf/1512.04636v1.pdf | author:Ameneh Boroomand, Mohammad Javad Shafiee, Farzad Khalvati, Masoom A. Haider, Alexander Wong category:stat.ME cs.CV physics.med-ph stat.AP published:2015-12-15 summary:Diffusion weighted magnetic resonance imaging (DW-MRI) is a powerful tool inimaging-based prostate cancer (PCa) screening and detection. Endorectal coilsare commonly used in DW-MRI to improve the signal-to-noise ratio (SNR) of theacquisition, at the expense of significant intensity inhomogeneities (biasfield) that worsens as we move away from the endorectal coil. The presence ofbias field can have a significant negative impact on the accuracy of differentimage analysis tasks, as well as the accuracy of PCa tumor localization, thusleading to increased inter- and intra-observer variability. The previouslyproposed bias field correction methods often suffer from undesired noiseamplification that can reduce the image quality of the resulting bias-correctedDW-MRI data. Here, we propose a unified data reconstruction approach thatenables joint compensation of bias field as well as data noise in diffusionweighted endorectal magnetic resonance (DW-EMR) imaging. The proposednoise-compensated, bias-corrected (NCBC) data reconstruction method takesadvantage of a novel stochastically fully connected joint conditional randomfield (SFC-JCRF) model to mitigate the effects of data noise and bias field inthe reconstructed DW-EMR prostate imaging data. The proposed NCBCreconstruction method was tested on synthetic DW-EMR data, physical DW-EMRphantom, as well as real DW-EMR imaging data. Both qualitative and quantitativeanalysis illustrated that the proposed NCBC method can achieve improved imagequality when compared to other tested bias correction methods. As such, theproposed NCBC method can have strong potential for improving the consistency ofimage interpretations, thus leading to more accurate PCa diagnosis.
arxiv-14100-56 | Max-margin Deep Generative Models | http://arxiv.org/pdf/1504.06787v4.pdf | author:Chongxuan Li, Jun Zhu, Tianlin Shi, Bo Zhang category:cs.LG cs.CV published:2015-04-26 summary:Deep generative models (DGMs) are effective on learning multilayeredrepresentations of complex data and performing inference of input data byexploring the generative ability. However, little work has been done onexamining or empowering the discriminative ability of DGMs on making accuratepredictions. This paper presents max-margin deep generative models (mmDGMs),which explore the strongly discriminative principle of max-margin learning toimprove the discriminative power of DGMs, while retaining the generativecapability. We develop an efficient doubly stochastic subgradient algorithm forthe piecewise linear objective. Empirical results on MNIST and SVHN datasetsdemonstrate that (1) max-margin learning can significantly improve theprediction performance of DGMs and meanwhile retain the generative ability; and(2) mmDGMs are competitive to the state-of-the-art fully discriminativenetworks by employing deep convolutional neural networks (CNNs) as bothrecognition and generative models.
arxiv-14100-57 | Semantic-enriched Visual Vocabulary Construction in a Weakly Supervised Context | http://arxiv.org/pdf/1512.04605v1.pdf | author:Marian-Andrei Rizoiu, Julien Velcin, Stéphane Lallich category:cs.CV published:2015-12-14 summary:One of the prevalent learning tasks involving images is content-based imageclassification. This is a difficult task especially because the low-levelfeatures used to digitally describe images usually capture little informationabout the semantics of the images. In this paper, we tackle this difficulty byenriching the semantic content of the image representation by using externalknowledge. The underlying hypothesis of our work is that creating a moresemantically rich representation for images would yield higher machine learningperformances, without the need to modify the learning algorithms themselves.The external semantic information is presented under the form of non-positionalimage labels, therefore positioning our work in a weakly supervised context.Two approaches are proposed: the first one leverages the labels into the visualvocabulary construction algorithm, the result being dedicated visualvocabularies. The second approach adds a filtering phase as a pre-processing ofthe vocabulary construction. Known positive and known negative sets areconstructed and features that are unlikely to be associated with the objectsdenoted by the labels are filtered. We apply our proposition to the task ofcontent-based image classification and we show that semantically enriching theimage representation yields higher classification performances than thebaseline representation.
arxiv-14100-58 | Relaxed Linearized Algorithms for Faster X-Ray CT Image Reconstruction | http://arxiv.org/pdf/1512.04564v1.pdf | author:Hung Nien, Jeffrey A. Fessler category:math.OC cs.LG stat.ML published:2015-12-14 summary:Statistical image reconstruction (SIR) methods are studied extensively forX-ray computed tomography (CT) due to the potential of acquiring CT scans withreduced X-ray dose while maintaining image quality. However, the longerreconstruction time of SIR methods hinders their use in X-ray CT in practice.To accelerate statistical methods, many optimization techniques have beeninvestigated. Over-relaxation is a common technique to speed up convergence ofiterative algorithms. For instance, using a relaxation parameter that is closeto two in alternating direction method of multipliers (ADMM) has been shown tospeed up convergence significantly. This paper proposes a relaxed linearizedaugmented Lagrangian (AL) method that shows theoretical faster convergence ratewith over-relaxation and applies the proposed relaxed linearized AL method toX-ray CT image reconstruction problems. Experimental results with bothsimulated and real CT scan data show that the proposed relaxed algorithm (withordered-subsets [OS] acceleration) is about twice as fast as the existingunrelaxed fast algorithms, with negligible computation and memory overhead.
arxiv-14100-59 | Minimal Realization Problems for Hidden Markov Models | http://arxiv.org/pdf/1411.3698v2.pdf | author:Qingqing Huang, Rong Ge, Sham Kakade, Munther Dahleh category:cs.LG published:2014-11-13 summary:Consider a stationary discrete random process with alphabet size d, which isassumed to be the output process of an unknown stationary Hidden Markov Model(HMM). Given the joint probabilities of finite length strings of the process,we are interested in finding a finite state generative model to describe theentire process. In particular, we focus on two classes of models: HMMs andquasi-HMMs, which is a strictly larger class of models containing HMMs. In themain theorem, we show that if the random process is generated by an HMM oforder less or equal than k, and whose transition and observation probabilitymatrix are in general position, namely almost everywhere on the parameterspace, both the minimal quasi-HMM realization and the minimal HMM realizationcan be efficiently computed based on the joint probabilities of all the lengthN strings, for N > 4 lceil log_d(k) rceil +1. In this paper, we also aim tocompare and connect the two lines of literature: realization theory of HMMs,and the recent development in learning latent variable models with tensordecomposition techniques.
arxiv-14100-60 | Learning Directed Acyclic Graphs with Penalized Neighbourhood Regression | http://arxiv.org/pdf/1511.08963v2.pdf | author:Bryon Aragam, Arash A. Amini, Qing Zhou category:math.ST cs.LG stat.ML stat.TH published:2015-11-29 summary:We consider the problem of estimating a directed acyclic graph (DAG) for amultivariate normal distribution from high-dimensional data with $p\gg n$. Ourmain results establish nonasymptotic deviation bounds on the estimation error,sparsity bounds, and model selection consistency for a penalized least squaresestimator under concave regularization. The proofs rely on interpreting thegraphical model as a recursive linear structural equation model, which reducesthe estimation problem to a series of tractable neighbourhood regressions andallows us to avoid making any assumptions regarding faithfulness. In doing so,we provide some novel techniques for handling general nonidentifiable andnonconvex problems. These techniques are used to guarantee uniform control overa superexponential number of neighbourhood regression problems by exploitingvarious notions of monotonicity among them. Our results apply to a wide varietyof practical situations that allow for arbitrary nondegenerate covariancestructures as well as many popular regularizers including the MCP, SCAD,$\ell_{0}$ and $\ell_{1}$.
arxiv-14100-61 | Dropout Training of Matrix Factorization and Autoencoder for Link Prediction in Sparse Graphs | http://arxiv.org/pdf/1512.04483v1.pdf | author:Shuangfei Zhai, Zhongfei Zhang category:cs.LG published:2015-12-14 summary:Matrix factorization (MF) and Autoencoder (AE) are among the most successfulapproaches of unsupervised learning. While MF based models have beenextensively exploited in the graph modeling and link prediction literature, theAE family has not gained much attention. In this paper we investigate both MFand AE's application to the link prediction problem in sparse graphs. We showthe connection between AE and MF from the perspective of multiview learning,and further propose MF+AE: a model training MF and AE jointly with sharedparameters. We apply dropout to training both the MF and AE parts, and showthat it can significantly prevent overfitting by acting as an adaptiveregularization. We conduct experiments on six real world sparse graph datasets,and show that MF+AE consistently outperforms the competing methods, especiallyon datasets that demonstrate strong non-cohesive structures.
arxiv-14100-62 | Semisupervised Autoencoder for Sentiment Analysis | http://arxiv.org/pdf/1512.04466v1.pdf | author:Shuangfei Zhai, Zhongfei Zhang category:cs.LG published:2015-12-14 summary:In this paper, we investigate the usage of autoencoders in modeling textualdata. Traditional autoencoders suffer from at least two aspects: scalabilitywith the high dimensionality of vocabulary size and dealing withtask-irrelevant words. We address this problem by introducing supervision viathe loss function of autoencoders. In particular, we first train a linearclassifier on the labeled data, then define a loss for the autoencoder with theweights learned from the linear classifier. To reduce the bias brought by onesingle classifier, we define a posterior probability distribution on theweights of the classifier, and derive the marginalized loss of the autoencoderwith Laplace approximation. We show that our choice of loss function can berationalized from the perspective of Bregman Divergence, which justifies thesoundness of our model. We evaluate the effectiveness of our model on sixsentiment analysis datasets, and show that our model significantly outperformsall the competing methods with respect to classification accuracy. We also showthat our model is able to take advantage of unlabeled dataset and get improvedperformance. We further show that our model successfully learns highlydiscriminative feature maps, which explains its superior performance.
arxiv-14100-63 | Memory-based control with recurrent neural networks | http://arxiv.org/pdf/1512.04455v1.pdf | author:Nicolas Heess, Jonathan J Hunt, Timothy P Lillicrap, David Silver category:cs.LG published:2015-12-14 summary:Partially observed control problems are a challenging aspect of reinforcementlearning. We extend two related, model-free algorithms for continuous control-- deterministic policy gradient and stochastic value gradient -- to solvepartially observed domains using recurrent neural networks trained withbackpropagation through time. We demonstrate that this approach, coupled with long-short term memory isable to solve a variety of physical control problems exhibiting an assortmentof memory requirements. These include the short-term integration of informationfrom noisy sensors and the identification of system parameters, as well aslong-term memory problems that require preserving information over many timesteps. We also demonstrate success on a combined exploration and memory problemin the form of a simplified version of the well-known Morris water maze task.Finally, we show that our approach can deal with high-dimensional observationsby learning directly from pixels. We find that recurrent deterministic and stochastic policies are able tolearn similarly good solutions to these tasks, including the water maze wherethe agent must learn effective search strategies.
arxiv-14100-64 | Near-Optimal Bounds for Binary Embeddings of Arbitrary Sets | http://arxiv.org/pdf/1512.04433v1.pdf | author:Samet Oymak, Ben Recht category:cs.LG cs.DS math.FA published:2015-12-14 summary:We study embedding a subset $K$ of the unit sphere to the Hamming cube$\{-1,+1\}^m$. We characterize the tradeoff between distortion and samplecomplexity $m$ in terms of the Gaussian width $\omega(K)$ of the set. Forsubspaces and several structured sets we show that Gaussian maps provide theoptimal tradeoff $m\sim \delta^{-2}\omega^2(K)$, in particular for $\delta$distortion one needs $m\approx\delta^{-2}{d}$ where $d$ is the subspacedimension. For general sets, we provide sharp characterizations which reducesto $m\approx{\delta^{-4}}{\omega^2(K)}$ after simplification. We provideimproved results for local embedding of points that are in close proximity ofeach other which is related to locality sensitive hashing. We also discussfaster binary embedding where one takes advantage of an initial sketchingprocedure based on Fast Johnson-Lindenstauss Transform. Finally, we listseveral numerical observations and discuss open problems.
arxiv-14100-65 | Sentence Entailment in Compositional Distributional Semantics | http://arxiv.org/pdf/1512.04419v1.pdf | author:Esma Balkir, Dimitri Kartsaklis, Mehrnoosh Sadrzadeh category:cs.CL cs.AI math.CT I.2.7 published:2015-12-14 summary:Distributional semantic models provide vector representations for words bygathering co-occurrence frequencies from corpora of text. Compositionaldistributional models extend these representations from words to phrases andsentences. In categorical compositional distributional semantics theserepresentations are built in such a manner that meanings of phrases andsentences are functions of their grammatical structure and the meanings of thewords therein. These models have been applied to reasoning about phrase andsentence level similarity. In this paper, we argue for and prove that thesemodels can also be used to reason about phrase and sentence level entailment.We provide preliminary experimental results on a toy entailment dataset.
arxiv-14100-66 | Sparse Representation of a Blur Kernel for Blind Image Restoration | http://arxiv.org/pdf/1512.04418v1.pdf | author:Chia-Chen Lee, Wen-Liang Hwang category:cs.CV published:2015-12-14 summary:Blind image restoration is a non-convex problem which involves restoration ofimages from an unknown blur kernel. The factors affecting the performance ofthis restoration are how much prior information about an image and a blurkernel are provided and what algorithm is used to perform the restoration task.Prior information on images is often employed to restore the sharpness of theedges of an image. By contrast, no consensus is still present regarding whatprior information to use in restoring from a blur kernel due to complex imageblurring processes. In this paper, we propose modelling of a blur kernel as asparse linear combinations of basic 2-D patterns. Our approach has acompetitive edge over the existing blur kernel modelling methods because ourmethod has the flexibility to customize the dictionary design, which makes itwell-adaptive to a variety of applications. As a demonstration, we construct adictionary formed by basic patterns derived from the Kronecker product ofGaussian sequences. We also compare our results with those derived by otherstate-of-the-art methods, in terms of peak signal to noise ratio (PSNR).
arxiv-14100-67 | Instance-aware Semantic Segmentation via Multi-task Network Cascades | http://arxiv.org/pdf/1512.04412v1.pdf | author:Jifeng Dai, Kaiming He, Jian Sun category:cs.CV published:2015-12-14 summary:Semantic segmentation research has recently witnessed rapid progress, butmany leading methods are unable to identify object instances. In this paper, wepresent Multi-task Network Cascades for instance-aware semantic segmentation.Our model consists of three networks, respectively differentiating instances,estimating masks, and categorizing objects. These networks form a cascadedstructure, and are designed to share their convolutional features. We developan algorithm for the nontrivial end-to-end training of this causal, cascadedstructure. Our solution is a clean, single-step training framework and can begeneralized to cascades that have more stages. We demonstrate state-of-the-artinstance-aware semantic segmentation accuracy on PASCAL VOC. Meanwhile, ourmethod takes only 360ms testing an image using VGG-16, which is two orders ofmagnitude faster than previous systems for this challenging problem. As a byproduct, our method also achieves compelling object detection results whichsurpass the competitive Fast/Faster R-CNN systems. The method described in this paper is the foundation of our submissions tothe MS COCO 2015 segmentation competition, where we won the 1st place.
arxiv-14100-68 | Image patch analysis of sunspots and active regions. I. Intrinsic dimension and correlation analysis | http://arxiv.org/pdf/1503.04127v2.pdf | author:Kevin R. Moon, Jimmy J. Li, Veronique Delouille, Ruben De Visscher, Fraser Watson, Alfred O. Hero III category:astro-ph.SR cs.CV published:2015-03-13 summary:The flare-productivity of an active region is observed to be related to itsspatial complexity. Mount Wilson or McIntosh sunspot classifications measuresuch complexity but in a categorical way, and may therefore not use all theinformation present in the observations. Moreover, such categorical schemeshinder a systematic study of an active region's evolution for example. Wepropose fine-scale quantitative descriptors for an active region's complexityand relate them to the Mount Wilson classification. We analyze the localcorrelation structure within continuum and magnetogram data, as well as thecross-correlation between continuum and magnetogram data. We compute theintrinsic dimension, partial correlation, and canonical correlation analysis(CCA) of image patches of continuum and magnetogram active region images takenfrom the SOHO-MDI instrument. We use masks of sunspots derived from continuumas well as larger masks of magnetic active regions derived from the magnetogramto analyze separately the core part of an active region from its surroundingpart. We find the relationship between complexity of an active region asmeasured by Mount Wilson and the intrinsic dimension of its image patches.Partial correlation patterns exhibit approximately a third-order Markovstructure. CCA reveals different patterns of correlation between continuum andmagnetogram within the sunspots and in the region surrounding the sunspots.These results also pave the way for patch-based dictionary learning with a viewtowards automatic clustering of active regions.
arxiv-14100-69 | Decoding index finger position from EEG using random forests | http://arxiv.org/pdf/1512.04274v1.pdf | author:Sebastian Weichwald, Timm Meyer, Bernhard Schölkopf, Tonio Ball, Moritz Grosse-Wentrup category:stat.ML q-bio.NC q-bio.QM published:2015-12-14 summary:While invasively recorded brain activity is known to provide detailedinformation on motor commands, it is an open question at what level of detailinformation about positions of body parts can be decoded from non-invasivelyacquired signals. In this work it is shown that index finger positions can bedifferentiated from non-invasive electroencephalographic (EEG) recordings inhealthy human subjects. Using a leave-one-subject-out cross-validationprocedure, a random forest distinguished different index finger positions on anumerical keyboard above chance-level accuracy. Among the different spectralfeatures investigated, high $\beta$-power (20-30 Hz) over contralateralsensorimotor cortex carried most information about finger position. Thus, thesefindings indicate that finger position is in principle decodable fromnon-invasive features of brain activity that generalize across individuals.
arxiv-14100-70 | A Proximal Approach for Sparse Multiclass SVM | http://arxiv.org/pdf/1501.03669v5.pdf | author:G. Chierchia, Nelly Pustelnik, Jean-Christophe Pesquet, B. Pesquet-Popescu category:cs.LG published:2015-01-15 summary:Sparsity-inducing penalties are useful tools to design multiclass supportvector machines (SVMs). In this paper, we propose a convex optimizationapproach for efficiently and exactly solving the multiclass SVM learningproblem involving a sparse regularization and the multiclass hinge lossformulated by Crammer and Singer. We provide two algorithms: the first onedealing with the hinge loss as a penalty term, and the other one addressing thecase when the hinge loss is enforced through a constraint. The related convexoptimization problems can be efficiently solved thanks to the flexibilityoffered by recent primal-dual proximal algorithms and epigraphical splittingtechniques. Experiments carried out on several datasets demonstrate theinterest of considering the exact expression of the hinge loss rather than asmooth approximation. The efficiency of the proposed algorithms w.r.t. severalstate-of-the-art methods is also assessed through comparisons of executiontimes.
arxiv-14100-71 | On the Relation between two Rotation Metrics | http://arxiv.org/pdf/1512.04219v1.pdf | author:Thomas Ruland category:cs.CV published:2015-12-14 summary:In their work "Global Optimization through Rotation Space Search", RichardHartley and Fredrik Kahl introduce a global optimization strategy for problemsin geometric computer vision, based on rotation space search using abranch-and-bound algorithm. In its core, Lemma 2 of their publication is theimportant foundation for a class of global optimization algorithms, which isadopted over a wide range of problems in subsequent publications. This lemmarelates a metric on rotations represented by rotation matrices with a metric onrotations in axis-angle representation. This work focuses on a proof for thisrelationship, which is based on Rodrigues' Rotation Theorem for the compositionof rotations in axis-angle representation.
arxiv-14100-72 | Watch-Bot: Unsupervised Learning for Reminding Humans of Forgotten Actions | http://arxiv.org/pdf/1512.04208v1.pdf | author:Chenxia Wu, Jiemi Zhang, Bart Selman, Silvio Savarese, Ashutosh Saxena category:cs.RO cs.CV published:2015-12-14 summary:We present a robotic system that watches a human using a Kinect v2 RGB-Dsensor, detects what he forgot to do while performing an activity, and ifnecessary reminds the person using a laser pointer to point out the relatedobject. Our simple setup can be easily deployed on any assistive robot. Our approach is based on a learning algorithm trained in a purelyunsupervised setting, which does not require any human annotations. This makesour approach scalable and applicable to variant scenarios. Our model learns theaction/object co-occurrence and action temporal relations in the activity, anduses the learned rich relationships to infer the forgotten action and therelated object. We show that our approach not only improves the unsupervisedaction segmentation and action cluster assignment performance, but alsoeffectively detects the forgotten actions on a challenging human activity RGB-Dvideo dataset. In robotic experiments, we show that our robot is able to remindpeople of forgotten actions successfully.
arxiv-14100-73 | Compressed Dynamic Mode Decomposition for Real-Time Object Detection | http://arxiv.org/pdf/1512.04205v1.pdf | author:N. Benjamin Erichson, Steven L. Brunton, J. Nathan Kutz category:cs.CV published:2015-12-14 summary:We introduce the method of compressive dynamic mode decomposition (cDMD) forrobustly performing real-time foreground/background separation inhigh-definition video. The DMD method provides a regression technique forleast-square fitting of video snapshots to a linear dynamical system. Themethod integrates two of the leading data analysis methods in use today:Fourier transforms and Principal Components. DMD modes with temporal Fourierfrequencies near the origin (zero-modes) are interpreted as background(low-rank) portions of the given video frames, and the terms with Fourierfrequencies bounded away from the origin are their foreground (sparse)counterparts. When combined with compression techniques, the resulting cDMD canprocess full HD video feeds in real-time on CPU computing platforms while stillmaintaining competitive video decomposition quality, quantified by F-measure,Recall and Precision. On a GPU architecture, the method is significantly fasterthan real-time, allowing for further video processing to improve the separationquality and/or enacting further computer vision processes such as objectrecognition.
arxiv-14100-74 | Preconditioned Stochastic Gradient Descent | http://arxiv.org/pdf/1512.04202v1.pdf | author:Xi-Lin Li category:stat.ML cs.LG published:2015-12-14 summary:Stochastic gradient descent (SGD) still is the workhorse for many practicalproblems. However, it converges slow, and can be difficult to tune. It ispossible to precondition SGD to accelerate its convergence remarkably. But manyattempts in this direction either aim at solving specialized problems, orresult in significantly more complicated methods than SGD. This paper proposesa new way to estimate a preconditioner by equalizing the amplitudes ofparameter changes and the amplitudes of associated gradient changes. Unlike theHessian inverse like preconditioners based on secant equation fitting as donein deterministic quasi-Newton methods, which work the best when the Hessian ispositive definite, the new preconditioner works equally well for both convexand non-convex optimizations. When stochastic gradient is used, it cannaturally damp the gradient noise to stabilize SGD. Efficient preconditionerestimation methods are developed, and with reasonable simplifications, they areapplicable to large scaled problems. Experimental results demonstrate thatequipped with the new preconditioner, without any tuning effort, preconditionedSGD can efficiently solve many challenging problems like the training of a deepneural network or a recurrent neural network requiring extremely long termmemories.
arxiv-14100-75 | Controlled Experiments for Word Embeddings | http://arxiv.org/pdf/1510.02675v2.pdf | author:Benjamin J. Wilson, Adriaan M. J. Schakel category:cs.CL 68T50 I.2.7 published:2015-10-09 summary:An experimental approach to studying the properties of word embeddings isproposed. Controlled experiments, achieved through modifications of thetraining corpus, permit the demonstration of direct relations between wordproperties and word vector direction and length. The approach is demonstratedusing the word2vec CBOW model with experiments that independently vary wordfrequency and word co-occurrence noise. The experiments reveal that word vectorlength depends more or less linearly on both word frequency and the level ofnoise in the co-occurrence distribution of the word. The coefficients oflinearity depend upon the word. The special point in feature space, defined bythe (artificial) word with pure noise in its co-occurrence distribution, isfound to be small but non-zero.
arxiv-14100-76 | Understanding Human-Centric Images: From Geometry to Fashion | http://arxiv.org/pdf/1604.08164v1.pdf | author:Edgar Simo-Serra category:cs.CV published:2015-12-14 summary:Understanding humans from photographs has always been a fundamental goal ofcomputer vision. In this thesis we have developed a hierarchy of tools thatcover a wide range of topics with the objective of understanding humans frommonocular RGB image: from low level feature point descriptors to high levelfashion-aware conditional random fields models. In order to build these highlevel models it is paramount to have a battery of robust and reliable low andmid level cues. Along these lines, we have proposed two low-level keypointdescriptors: one based on the theory of the heat diffusion on images, and theother that uses a convolutional neural network to learn discriminative imagepatch representations. We also introduce distinct low-level generative modelsfor representing human pose: in particular we present a discrete model based ona directed acyclic graph and a continuous model that consists of posesclustered on a Riemannian manifold. As mid level cues we propose two 3D humanpose estimation algorithms: one that estimates the 3D pose given a noisy 2Destimation, and an approach that simultaneously estimates both the 2D and 3Dpose. Finally, we formulate higher level models built upon low and mid levelcues for understanding humans from single images. Concretely, we focus on twodifferent tasks in the context of fashion: semantic segmentation of clothing,and predicting the fashionability from images with metadata to ultimatelyprovide fashion advice to the user. For all presented approaches we presentextensive results and comparisons against the state-of-the-art and showsignificant improvements on the entire variety of tasks we tackle.
arxiv-14100-77 | Cross-validation of matching correlation analysis by resampling matching weights | http://arxiv.org/pdf/1503.08471v3.pdf | author:Hidetoshi Shimodaira category:stat.ML cs.LG published:2015-03-29 summary:The strength of association between a pair of data vectors is represented bya nonnegative real number, called matching weight. For dimensionalityreduction, we consider a linear transformation of data vectors, and define amatching error as the weighted sum of squared distances between transformedvectors with respect to the matching weights. Given data vectors and matchingweights, the optimal linear transformation minimizing the matching error issolved by the spectral graph embedding of Yan et al. (2007). This method is ageneralization of the canonical correlation analysis, and will be called asmatching correlation analysis (MCA). In this paper, we consider a novelsampling scheme where the observed matching weights are randomly sampled fromunderlying true matching weights with small probability, whereas the datavectors are treated as constants. We then investigate a cross-validation byresampling the matching weights. Our asymptotic theory shows that thecross-validation, if rescaled properly, computes an unbiased estimate of thematching error with respect to the true matching weights. Existing ideas ofcross-validation for resampling data vectors, instead of resampling matchingweights, are not applicable here. MCA can be used for data vectors frommultiple domains with different dimensions via an embarrassingly simple idea ofcoding the data vectors. This method will be called as cross-domain matchingcorrelation analysis (CDMCA), and an interesting connection to the classicalassociative memory model of neural networks is also discussed.
arxiv-14100-78 | Spectral Smoothing via Random Matrix Perturbations | http://arxiv.org/pdf/1507.03032v2.pdf | author:Jacob Abernethy, Chansoo Lee, Ambuj Tewari category:cs.LG published:2015-07-10 summary:We consider stochastic smoothing of spectral functions of matrices usingperturbations commonly studied in random matrix theory. We show that a spectralfunction remains spectral when smoothed using a unitarily invariantperturbation distribution. We then derive state-of-the-art smoothing bounds forthe maximum eigenvalue function using the Gaussian Orthogonal Ensemble (GOE).Smoothing the maximum eigenvalue function is important for applications insemidefinite optimization and online learning. As a direct consequence of ourGOE smoothing results, we obtain an $O((N \log N)^{1/4} \sqrt{T})$ expectedregret bound for the online variance minimization problem using an algorithmthat performs only a single maximum eigenvector computation per time step. Here$T$ is the number of rounds and $N$ is the matrix dimension. Our algorithm andits analysis also extend to the more general online PCA problem where thelearner has to output a rank $k$ subspace. The algorithm just requirescomputing $k$ maximum eigenvectors per step and enjoys an $O(k (N \log N)^{1/4}\sqrt{T})$ expected regret bound.
arxiv-14100-79 | Fighting Bandits with a New Kind of Smoothness | http://arxiv.org/pdf/1512.04152v1.pdf | author:Jacob Abernethy, Chansoo Lee, Ambuj Tewari category:cs.LG cs.GT stat.ML published:2015-12-14 summary:We define a novel family of algorithms for the adversarial multi-armed banditproblem, and provide a simple analysis technique based on convex smoothing. Weprove two main results. First, we show that regularization via the\emph{Tsallis entropy}, which includes EXP3 as a special case, achieves the$\Theta(\sqrt{TN})$ minimax regret. Second, we show that a wide class ofperturbation methods achieve a near-optimal regret as low as $O(\sqrt{TN \logN})$ if the perturbation distribution has a bounded hazard rate. For example,the Gumbel, Weibull, Frechet, Pareto, and Gamma distributions all satisfy thiskey property.
arxiv-14100-80 | Learning Deep Features for Discriminative Localization | http://arxiv.org/pdf/1512.04150v1.pdf | author:Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba category:cs.CV published:2015-12-14 summary:In this work, we revisit the global average pooling layer proposed in [13],and shed light on how it explicitly enables the convolutional neural network tohave remarkable localization ability despite being trained on image-levellabels. While this technique was previously proposed as a means forregularizing training, we find that it actually builds a generic localizabledeep representation that can be applied to a variety of tasks. Despite theapparent simplicity of global average pooling, we are able to achieve 37.1%top-5 error for object localization on ILSVRC 2014, which is remarkably closeto the 34.2% top-5 error achieved by a fully supervised CNN approach. Wedemonstrate that our network is able to localize the discriminative imageregions on a variety of tasks despite not being trained for them
arxiv-14100-81 | Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks | http://arxiv.org/pdf/1512.04143v1.pdf | author:Sean Bell, C. Lawrence Zitnick, Kavita Bala, Ross Girshick category:cs.CV published:2015-12-14 summary:It is well known that contextual and multi-scale representations areimportant for accurate visual recognition. In this paper we present theInside-Outside Net (ION), an object detector that exploits information bothinside and outside the region of interest. Contextual information outside theregion of interest is integrated using spatial recurrent neural networks.Inside, we use skip pooling to extract information at multiple scales andlevels of abstraction. Through extensive experiments we evaluate the designspace and provide readers with an overview of what tricks of the trade areimportant. ION improves state-of-the-art on PASCAL VOC 2012 object detectionfrom 73.9% to 76.4% mAP. On the new and more challenging MS COCO dataset, weimprove state-of-art-the from 19.7% to 33.1% mAP. In the 2015 MS COCO DetectionChallenge, our ION model won the Best Student Entry and finished 3rd placeoverall. As intuition suggests, our detection results provide strong evidencethat context and multi-scale representations improve small object detection.
arxiv-14100-82 | Maximum Persistency via Iterative Relaxed Inference with Graphical Models | http://arxiv.org/pdf/1508.07902v2.pdf | author:Alexander Shekhovtsov, Paul Swoboda, Bogdan Savchynskyy category:cs.CV cs.DS published:2015-08-31 summary:We consider the NP-hard problem of MAP-inference for graphical models. Wepropose a polynomial time practically efficient algorithm for finding a part ofits optimal solution. Specifically, our algorithm marks each label in each nodeof the considered graphical model either as (i) optimal, meaning that itbelongs to all optimal solutions of the inference problem; (ii) non-optimal ifit provably does not belong to any solution; or (iii) undefined, which meansour algorithm can not make a decision regarding the label. Moreover, we proveoptimality of our approach: it delivers in a certain sense the largest totalnumber of labels marked as optimal or non-optimal. We demonstrate superiorityof our approach on problems from machine learning and computer visionbenchmarks.
arxiv-14100-83 | SentiCap: Generating Image Descriptions with Sentiments | http://arxiv.org/pdf/1510.01431v2.pdf | author:Alexander Mathews, Lexing Xie, Xuming He category:cs.CV cs.CL published:2015-10-06 summary:The recent progress on image recognition and language modeling is makingautomatic description of image content a reality. However, stylized,non-factual aspects of the written description are missing from the currentsystems. One such style is descriptions with emotions, which is commonplace ineveryday communication, and influences decision-making and interpersonalrelationships. We design a system to describe an image with emotions, andpresent a model that automatically generates captions with positive or negativesentiments. We propose a novel switching recurrent neural network withword-level regularization, which is able to produce emotional image captionsusing only 2000+ training sentences containing sentiments. We evaluate thecaptions with different automatic and crowd-sourcing metrics. Our modelcompares favourably in common quality metrics for image captioning. In 84.6% ofcases the generated positive captions were judged as being at least asdescriptive as the factual captions. Of these positive captions 88% wereconfirmed by the crowd-sourced workers as having the appropriate sentiment.
arxiv-14100-84 | Evaluation of Pose Tracking Accuracy in the First and Second Generations of Microsoft Kinect | http://arxiv.org/pdf/1512.04134v1.pdf | author:Qifei Wang, Gregorij Kurillo, Ferda Ofli, Ruzena Bajcsy category:cs.CV cs.AI published:2015-12-13 summary:Microsoft Kinect camera and its skeletal tracking capabilities have beenembraced by many researchers and commercial developers in various applicationsof real-time human movement analysis. In this paper, we evaluate the accuracyof the human kinematic motion data in the first and second generation of theKinect system, and compare the results with an optical motion capture system.We collected motion data in 12 exercises for 10 different subjects and fromthree different viewpoints. We report on the accuracy of the joint localizationand bone length estimation of Kinect skeletons in comparison to the motioncapture. We also analyze the distribution of the joint localization offsets byfitting a mixture of Gaussian and uniform distribution models to determine theoutliers in the Kinect motion data. Our analysis shows that overall Kinect 2has more robust and more accurate tracking of human pose as compared to Kinect1.
arxiv-14100-85 | A Person Re-Identification System For Mobile Devices | http://arxiv.org/pdf/1512.04133v1.pdf | author:George Cushen category:cs.CV cs.CR cs.IR published:2015-12-13 summary:Person re-identification is a critical security task for recognizing a personacross spatially disjoint sensors. Previous work can be computationallyintensive and is mainly based on low-level cues extracted from RGB data andimplemented on a PC for a fixed sensor network (such as traditional CCTV). Wepresent a practical and efficient framework for mobile devices (such as smartphones and robots) where high-level semantic soft biometrics are extracted fromRGB and depth data. By combining these cues, our approach attempts to providerobustness to noise, illumination, and minor variations in clothing. Thismobile approach may be particularly useful for the identification of persons inareas ill-served by fixed sensors or for tasks where the sensor position anddirection need to dynamically adapt to a target. Results on the BIWI datasetare preliminary but encouraging. Further evaluation and demonstration of thesystem will be available on our website.
arxiv-14100-86 | Dimensionality-reduced subspace clustering | http://arxiv.org/pdf/1507.07105v2.pdf | author:Reinhard Heckel, Michael Tschannen, Helmut Bölcskei category:stat.ML cs.IT cs.LG math.IT published:2015-07-25 summary:Subspace clustering refers to the problem of clustering unlabeledhigh-dimensional data points into a union of low-dimensional linear subspaces,whose number, orientations, and dimensions are all unknown. In practice one mayhave access to dimensionality-reduced observations of the data only, resulting,e.g., from undersampling due to complexity and speed constraints on theacquisition device or mechanism. More pertinently, even if the high-dimensionaldata set is available it is often desirable to first project the data pointsinto a lower-dimensional space and to perform clustering there; this reducesstorage requirements and computational cost. The purpose of this paper is toquantify the impact of dimensionality reduction through random projection onthe performance of three subspace clustering algorithms, all of which are basedon principles from sparse signal recovery. Specifically, we analyze thethresholding based subspace clustering (TSC) algorithm, the sparse subspaceclustering (SSC) algorithm, and an orthogonal matching pursuit variant thereof(SSC-OMP). We find, for all three algorithms, that dimensionality reductiondown to the order of the subspace dimensions is possible without incurringsignificant performance degradation. Moreover, these results are order-wiseoptimal in the sense that reducing the dimensionality further leads to afundamentally ill-posed clustering problem. Our findings carry over to thenoisy case as illustrated through analytical results for TSC and simulationsfor SSC and SSC-OMP. Extensive experiments on synthetic and real datacomplement our theoretical findings.
arxiv-14100-87 | Articulated Pose Estimation Using Hierarchical Exemplar-Based Models | http://arxiv.org/pdf/1512.04118v1.pdf | author:Jiongxin Liu, Yinxiao Li, Peter Allen, Peter Belhumeur category:cs.CV published:2015-12-13 summary:Exemplar-based models have achieved great success on localizing the parts ofsemi-rigid objects. However, their efficacy on highly articulated objects suchas humans is yet to be explored. Inspired by hierarchical object representationand recent application of Deep Convolutional Neural Networks (DCNNs) on humanpose estimation, we propose a novel formulation that incorporates bothhierarchical exemplar-based models and DCNNs in the spatial terms.Specifically, we obtain more expressive spatial models by assuming independencebetween exemplars at different levels in the hierarchy; we also obtain strongerspatial constraints by inferring the spatial relations between parts at thesame level. As our method strikes a good balance between expressiveness andstrength of spatial models, it is both effective and generalizable, achievingstate-of-the-art results on different benchmarks: Leeds Sports Dataset andCUB-200-2011.
arxiv-14100-88 | Unsupervised Temporal Segmentation of Repetitive Human Actions Based on Kinematic Modeling and Frequency Analysis | http://arxiv.org/pdf/1512.04115v1.pdf | author:Qifei Wang, Gregorij Kurillo, Ferda Ofli, Ruzena Bajcsy category:cs.CV published:2015-12-13 summary:In this paper, we propose a method for temporal segmentation of humanrepetitive actions based on frequency analysis of kinematic parameters,zero-velocity crossing detection, and adaptive k-means clustering. Since thehuman motion data may be captured with different modalities which havedifferent temporal sampling rate and accuracy (e.g., optical motion capturesystems vs. Microsoft Kinect), we first apply a generic full-body kinematicmodel with an unscented Kalman filter to convert the motion data into a unifiedrepresentation that is robust to noise. Furthermore, we extract the mostrepresentative kinematic parameters via the primary frequency analysis. Thesequences are segmented based on zero-velocity crossing of the selectedparameters followed by an adaptive k-means clustering to identify therepetition segments. Experimental results demonstrate that for the motion datacaptured by both the motion capture system and the Microsoft Kinect, ourproposed algorithm obtains robust segmentation of repetitive action sequences.
arxiv-14100-89 | Policy Gradient Methods for Off-policy Control | http://arxiv.org/pdf/1512.04105v1.pdf | author:Lucas Lehnert, Doina Precup category:cs.AI cs.LG published:2015-12-13 summary:Off-policy learning refers to the problem of learning the value function of away of behaving, or policy, while following a different policy. Gradient-basedoff-policy learning algorithms, such as GTD and TDC/GQ, converge even whenusing function approximation and incremental updates. However, they have beendeveloped for the case of a fixed behavior policy. In control problems, onewould like to adapt the behavior policy over time to become more greedy withrespect to the existing value function. In this paper, we present the firstgradient-based learning algorithms for this problem, which rely on theframework of policy gradient in order to modify the behavior policy. We presentderivations of the algorithms, a convergence theorem, and empirical evidenceshowing that they compare favorably to existing approaches.
arxiv-14100-90 | Deep Relative Attributes | http://arxiv.org/pdf/1512.04103v1.pdf | author:Yaser Souri, Erfan Noury, Ehsan Adeli-Mosabbeb category:cs.CV published:2015-12-13 summary:Visual attributes are great means of describing images or scenes, in a wayboth humans and computers understand. In order to establish a correspondencebetween images and to be able to compare the strength of each property betweenimages, relative attributes were introduced. However, since their introduction,hand-crafted and engineered features were used to learn complex models for theproblem of relative attributes. This limits the applicability of those methodsfor more realistic cases. We introduce a two part deep learning architecturefor the task of relative attribute prediction. A convolutional neural network(ConvNet) architecture is adopted to learn the features with addition of anadditional layer (ranking layer) that learns to rank the images based on thesefeatures. Also an appropriate ranking loss is adapted to train the wholenetwork in an end-to-end fashion. Our proposed method outperforms the baselineand state-of-the-art methods in relative attribute prediction on variousdatasets. Our qualitative results also show that the network is able to learneffective features for the task. Furthermore, we use our trained models tovisualize saliency maps for each attribute.
arxiv-14100-91 | Stack Exchange Tagger | http://arxiv.org/pdf/1512.04092v1.pdf | author:Sanket Mehta, Shagun Sodhani category:cs.CL cs.LG published:2015-12-13 summary:The goal of our project is to develop an accurate tagger for questions postedon Stack Exchange. Our problem is an instance of the more general problem ofdeveloping accurate classifiers for large scale text datasets. We are tacklingthe multilabel classification problem where each item (in this case, question)can belong to multiple classes (in this case, tags). We are predicting the tags(or keywords) for a particular Stack Exchange post given only the question textand the title of the post. In the process, we compare the performance ofSupport Vector Classification (SVC) for different kernel functions, lossfunction, etc. We found linear SVC with Crammer Singer technique produces bestresults.
arxiv-14100-92 | True Online Temporal-Difference Learning | http://arxiv.org/pdf/1512.04087v1.pdf | author:Harm van Seijen, A. Rupam Mahmood, Patrick M. Pilarski, Marlos C. Machado, Richard S. Sutton category:cs.AI cs.LG published:2015-12-13 summary:The temporal-difference methods TD($\lambda$) and Sarsa($\lambda$) form acore part of modern reinforcement learning. Their appeal comes from their goodperformance, low computational cost, and their simple interpretation, given bytheir forward view. Recently, new versions of these methods were introduced,called true online TD($\lambda$) and true online Sarsa($\lambda$), respectively(van Seijen and Sutton, 2014). Algorithmically, these true online methods onlymake two small changes to the update rules of the regular methods, and theextra computational cost is negligible in most cases. However, they follow theideas underlying the forward view much more closely. In particular, theymaintain an exact equivalence with the forward view at all times, whereas thetraditional versions only approximate it for small step-sizes. We hypothesizethat these true online methods not only have better theoretical properties, butalso dominate the regular methods empirically. In this article, we put thishypothesis to the test by performing an extensive empirical comparison.Specifically, we compare the performance of true onlineTD($\lambda$)/Sarsa($\lambda$) with regular TD($\lambda$)/Sarsa($\lambda$) onrandom MRPs, a real-world myoelectric prosthetic arm, and a domain from theArcade Learning Environment. We use linear function approximation with tabular,binary, and non-binary features. Our results suggest that the true onlinemethods indeed dominate the regular methods. Across all domains/representationsthe learning speed of the true online methods are often better, but never worsethan that of the regular methods. An additional advantage is that no choicebetween traces has to be made for the true online methods. We show that newtrue online temporal-difference methods can be derived by making changes to thereal-time forward view and then rewriting the update equations.
arxiv-14100-93 | Cross-dimensional Weighting for Aggregated Deep Convolutional Features | http://arxiv.org/pdf/1512.04065v1.pdf | author:Yannis Kalantidis, Clayton Mellina, Simon Osindero category:cs.CV published:2015-12-13 summary:We propose a simple and straightforward way of creating powerful imagerepresentations via cross-dimensional weighting and aggregation of deepconvolutional neural network layer outputs. We first present a generalizedframework that encompasses a broad family of approaches and includescross-dimensional pooling and weighting steps. We then propose specificnon-parametric schemes for both spatial- and channel-wise weighting, that boostthe effect of highly active spatial responses and at the same time regulateburstiness effects. We experiment on four public datasets for image search andunsupervised fine-grained classification and show that our approachconsistently outperforms the current state-of-the-art by a large margin.
arxiv-14100-94 | Big Data Scaling through Metric Mapping: Exploiting the Remarkable Simplicity of Very High Dimensional Spaces using Correspondence Analysis | http://arxiv.org/pdf/1512.04052v1.pdf | author:Fionn Murtagh category:stat.ML cs.LG 62H25 published:2015-12-13 summary:We present new findings in regard to data analysis in very high dimensionalspaces. We use dimensionalities up to around one million. A particular benefitof Correspondence Analysis is its suitability for carrying out an orthonormalmapping, or scaling, of power law distributed data. Power law distributed dataare found in many domains. Correspondence factor analysis provides a latentsemantic or principal axes mapping. Our experiments use data from digitalchemistry and finance, and other statistically generated data.
arxiv-14100-95 | Distributed Optimization with Arbitrary Local Solvers | http://arxiv.org/pdf/1512.04039v1.pdf | author:Chenxin Ma, Jakub Konečný, Martin Jaggi, Virginia Smith, Michael I. Jordan, Peter Richtárik, Martin Takáč category:cs.LG math.OC published:2015-12-13 summary:With the growth of data and necessity for distributed optimization methods,solvers that work well on a single machine must be re-designed to leveragedistributed computation. Recent work in this area has been limited by focusingheavily on developing highly specific methods for the distributed environment.These special-purpose methods are often unable to fully leverage thecompetitive performance of their well-tuned and customized single machinecounterparts. Further, they are unable to easily integrate improvements thatcontinue to be made to single machine methods. To this end, we present aframework for distributed optimization that both allows the flexibility ofarbitrary solvers to be used on each (single) machine locally, and yetmaintains competitive performance against other state-of-the-artspecial-purpose distributed methods. We give strong primal-dual convergencerate guarantees for our framework that hold for arbitrary local solvers. Wedemonstrate the impact of local solver selection both theoretically and in anextensive experimental comparison. Finally, we provide thorough implementationdetails for our framework, highlighting areas for practical performance gains.
arxiv-14100-96 | Tracking Idea Flows between Social Groups | http://arxiv.org/pdf/1512.04036v1.pdf | author:Yangxin Zhong, Shixia Liu, Xiting Wang, Jiannan Xiao, Yangqiu Song category:cs.SI cs.LG published:2015-12-13 summary:In many applications, ideas that are described by a set of words often flowbetween different groups. To facilitate users in analyzing the flow, we presenta method to model the flow behaviors that aims at identifying the lead-lagrelationships between word clusters of different user groups. In particular, animproved Bayesian conditional cointegration based on dynamic time warping isemployed to learn links between words in different groups. A tensor-basedtechnique is developed to cluster these linked words into different clusters(ideas) and track the flow of ideas. The main feature of the tensorrepresentation is that we introduce two additional dimensions to represent bothtime and lead-lag relationships. Experiments on both synthetic and realdatasets show that our method is more effective than methods based ontraditional clustering techniques and achieves better accuracy. A case studywas conducted to demonstrate the usefulness of our method in helping usersunderstand the flow of ideas between different user groups on social media
arxiv-14100-97 | Direct Prediction of 3D Body Poses from Motion Compensated Sequences | http://arxiv.org/pdf/1511.06692v2.pdf | author:Bugra Tekin, Artem Rozantsev, Vincent Lepetit, Pascal Fua category:cs.CV published:2015-11-20 summary:We propose an efficient approach to exploiting motion information fromconsecutive frames of a video sequence to recover the 3D pose of people.Previous approaches typically compute candidate poses in individual frames andthen link them in a post-processing step to resolve ambiguities. By contrast,we directly regress from a spatio-temporal volume of bounding boxes to a 3Dpose in the central frame. We further show that, for this approach to achieve its full potential, it isessential to compensate for the motion in consecutive frames so that thesubject remains centered. This then allows us to effectively overcomeambiguities and improve upon the state-of-the-art by a large margin on theHuman3.6m, HumanEva, and KTH Multiview Football 3D human pose estimationbenchmarks.
arxiv-14100-98 | L1-Regularized Distributed Optimization: A Communication-Efficient Primal-Dual Framework | http://arxiv.org/pdf/1512.04011v1.pdf | author:Virginia Smith, Simone Forte, Michael I. Jordan, Martin Jaggi category:cs.LG published:2015-12-13 summary:Despite the importance of sparsity in many big data applications, there arefew existing methods for efficient distributed optimization ofsparsely-regularized objectives. In this paper, we present acommunication-efficient framework for L1-regularized optimization indistributed environments. By taking a non-traditional view of classicalobjectives as part of a more general primal-dual setting, we obtain a new classof methods that can be efficiently distributed and is applicable to commonL1-regularized regression and classification objectives, such as Lasso, sparselogistic regression, and elastic net regression. We provide convergenceguarantees for this framework and demonstrate strong empirical performance ascompared to other state-of-the-art methods on several real-world distributeddatasets.
arxiv-14100-99 | Deep Tracking: Visual Tracking Using Deep Convolutional Networks | http://arxiv.org/pdf/1512.03993v1.pdf | author:Meera Hahn, Si Chen, Afshin Dehghan category:cs.CV published:2015-12-13 summary:In this paper, we study a discriminatively trained deep convolutional networkfor the task of visual tracking. Our tracker utilizes both motion andappearance features that are extracted from a pre-trained dual stream deepconvolution network. We show that the features extracted from our dual-streamnetwork can provide rich information about the target and this leads tocompetitive performance against state of the art tracking methods on a visualtracking benchmark.
arxiv-14100-100 | Cloud-based Electronic Health Records for Real-time, Region-specific Influenza Surveillance | http://arxiv.org/pdf/1512.03990v1.pdf | author:Mauricio Santillana, Andre Nguyen, Tamara Louie, Anna Zink, Josh Gray, Iyue Sung, John S. Brownstein category:stat.AP stat.ML published:2015-12-13 summary:Accurate real-time monitoring systems of influenza outbreaks help publichealth officials make informed decisions that may help save lives. We show thatinformation extracted from cloud-based electronic health records databases, incombination with machine learning techniques and historical epidemiologicalinformation, have the potential to accurately and reliably provide nearreal-time regional predictions of flu outbreaks in the United States.
arxiv-14100-101 | Action Recognition with Image Based CNN Features | http://arxiv.org/pdf/1512.03980v1.pdf | author:Mahdyar Ravanbakhsh, Hossein Mousavi, Mohammad Rastegari, Vittorio Murino, Larry S. Davis category:cs.CV published:2015-12-13 summary:Most of human actions consist of complex temporal compositions of more simpleactions. Action recognition tasks usually relies on complex handcraftedstructures as features to represent the human action model. ConvolutionalNeural Nets (CNN) have shown to be a powerful tool that eliminate the need fordesigning handcrafted features. Usually, the output of the last layer in CNN (alayer before the classification layer -known as fc7) is used as a genericfeature for images. In this paper, we show that fc7 features, per se, can notget a good performance for the task of action recognition, when the network istrained only on images. We present a feature structure on top of fc7 features,which can capture the temporal variation in a video. To represent the temporalcomponents, which is needed to capture motion information, we introduced ahierarchical structure. The hierarchical model enables to capture sub-actionsfrom a complex action. At the higher levels of the hierarchy, it represents acoarse capture of action sequence and lower levels represent fine actionelements. Furthermore, we introduce a method for extracting key-frames usingbinary coding of each frame in a video, which helps to improve the performanceof our hierarchical model. We experimented our method on several actiondatasets and show that our method achieves superior results compared to otherstate-of-the-arts methods.
arxiv-14100-102 | RNN Fisher Vectors for Action Recognition and Image Annotation | http://arxiv.org/pdf/1512.03958v1.pdf | author:Guy Lev, Gil Sadeh, Benjamin Klein, Lior Wolf category:cs.CV published:2015-12-12 summary:Recurrent Neural Networks (RNNs) have had considerable success in classifyingand predicting sequences. We demonstrate that RNNs can be effectively used inorder to encode sequences and provide effective representations. Themethodology we use is based on Fisher Vectors, where the RNNs are thegenerative probabilistic models and the partial derivatives are computed usingbackpropagation. State of the art results are obtained in two central butdistant tasks, which both rely on sequences: video action recognition and imageannotation. We also show a surprising transfer learning result from the task ofimage annotation to the task of video action recognition.
arxiv-14100-103 | Active Distance-Based Clustering using K-medoids | http://arxiv.org/pdf/1512.03953v1.pdf | author:Mehrdad Ghadiri, Amin Aghaee, Mahdieh Soleymani Baghshah category:cs.LG published:2015-12-12 summary:k-medoids algorithm is a partitional, centroid-based clustering algorithmwhich uses pairwise distances of data points and tries to directly decomposethe dataset with $n$ points into a set of $k$ disjoint clusters. However,k-medoids itself requires all distances between data points that are not soeasy to get in many applications. In this paper, we introduce a new methodwhich requires only a small proportion of the whole set of distances and makesan effort to estimate an upper-bound for unknown distances using the inquiredones. This algorithm makes use of the triangle inequality to calculate anupper-bound estimation of the unknown distances. Our method is built upon arecursive approach to cluster objects and to choose some points actively fromeach bunch of data and acquire the distances between these prominent pointsfrom oracle. Experimental results show that the proposed method using only asmall subset of the distances can find proper clustering on many real-world andsynthetic datasets.
arxiv-14100-104 | A mathematical motivation for complex-valued convolutional networks | http://arxiv.org/pdf/1503.03438v3.pdf | author:Joan Bruna, Soumith Chintala, Yann LeCun, Serkan Piantino, Arthur Szlam, Mark Tygert category:cs.LG cs.NE stat.ML published:2015-03-11 summary:A complex-valued convolutional network (convnet) implements the repeatedapplication of the following composition of three operations, recursivelyapplying the composition to an input vector of nonnegative real numbers: (1)convolution with complex-valued vectors followed by (2) taking the absolutevalue of every entry of the resulting vectors followed by (3) local averaging.For processing real-valued random vectors, complex-valued convnets can beviewed as "data-driven multiscale windowed power spectra," "data-drivenmultiscale windowed absolute spectra," "data-driven multiwavelet absolutevalues," or (in their most general configuration) "data-driven nonlinearmultiwavelet packets." Indeed, complex-valued convnets can calculate multiscalewindowed spectra when the convnet filters are windowed complex-valuedexponentials. Standard real-valued convnets, using rectified linear units(ReLUs), sigmoidal (for example, logistic or tanh) nonlinearities, max.pooling, etc., do not obviously exhibit the same exact correspondence withdata-driven wavelets (whereas for complex-valued convnets, the correspondenceis much more than just a vague analogy). Courtesy of the exact correspondence,the remarkably rich and rigorous body of mathematical analysis for waveletsapplies directly to (complex-valued) convnets.
arxiv-14100-105 | A Hidden Markov Model Based System for Entity Extraction from Social Media English Text at FIRE 2015 | http://arxiv.org/pdf/1512.03950v1.pdf | author:Kamal Sarkar category:cs.CL 68T50 published:2015-12-12 summary:This paper presents the experiments carried out by us at Jadavpur Universityas part of the participation in FIRE 2015 task: Entity Extraction from SocialMedia Text - Indian Languages (ESM-IL). The tool that we have developed for thetask is based on Trigram Hidden Markov Model that utilizes information likegazetteer list, POS tag and some other word level features to enhance theobservation probabilities of the known tokens as well as unknown tokens. Wesubmitted runs for English only. A statistical HMM (Hidden Markov Models) basedmodel has been used to implement our system. The system has been trained andtested on the datasets released for FIRE 2015 task: Entity Extraction fromSocial Media Text - Indian Languages (ESM-IL). Our system is the best performerfor English language and it obtains precision, recall and F-measures of 61.96,39.46 and 48.21 respectively.
arxiv-14100-106 | A Novel Approach to Document Classification using WordNet | http://arxiv.org/pdf/1510.02755v2.pdf | author:Koushiki Sarkar, Ritwika Law category:cs.IR cs.CL published:2015-10-04 summary:Content based Document Classification is one of the biggest challenges in thecontext of free text mining. Current algorithms on document classificationsmostly rely on cluster analysis based on bag-of-words approach. However thatmethod is still being applied to many modern scientific dilemmas. It hasestablished a strong presence in fields like economics and social science tomerit serious attention from the researchers. In this paper we would like topropose and explore an alternative grounded more securely on the dictionaryclassification and correlatedness of words and phrases. It is expected thatapplication of our existing knowledge about the underlying classificationstructure may lead to improvement of the classifier's performance.
arxiv-14100-107 | Quantum assisted Gaussian process regression | http://arxiv.org/pdf/1512.03929v1.pdf | author:Zhikuan Zhao, Jack K. Fitzsimons, Joseph F. Fitzsimons category:quant-ph cs.LG stat.ML published:2015-12-12 summary:Gaussian processes (GP) are a widely used model for regression problems insupervised machine learning. Implementation of GP regression typically requires$O(n^3)$ logic gates. We show that the quantum linear systems algorithm [Harrowet al., Phys. Rev. Lett. 103, 150502 (2009)] can be applied to Gaussian processregression (GPR), leading to an exponential reduction in computation time insome instances. We show that even in some cases not ideally suited to thequantum linear systems algorithm, a polynomial increase in efficiency stilloccurs.
arxiv-14100-108 | Active Sampler: Light-weight Accelerator for Complex Data Analytics at Scale | http://arxiv.org/pdf/1512.03880v1.pdf | author:Jinyang Gao, H. V. Jagadish, Beng Chin Ooi category:cs.DB cs.LG stat.ML published:2015-12-12 summary:Recent years have witnessed amazing outcomes from "Big Models" trained by"Big Data". Most popular algorithms for model training are iterative. Due tothe surging volumes of data, we can usually afford to process only a fractionof the training data in each iteration. Typically, the data are eitheruniformly sampled or sequentially accessed. In this paper, we study how the data access pattern can affect modeltraining. We propose an Active Sampler algorithm, where training data with more"learning value" to the model are sampled more frequently. The goal is to focustraining effort on valuable instances near the classification boundaries,rather than evident cases, noisy data or outliers. We show the correctness andoptimality of Active Sampler in theory, and then develop a light-weightvectorized implementation. Active Sampler is orthogonal to most approachesoptimizing the efficiency of large-scale data analytics, and can be applied tomost analytics models trained by stochastic gradient descent (SGD) algorithm.Extensive experimental evaluations demonstrate that Active Sampler can speed upthe training procedure of SVM, feature selection and deep learning, forcomparable training quality by 1.6-2.2x.
arxiv-14100-109 | Minimal Perceptrons for Memorizing Complex Patterns | http://arxiv.org/pdf/1512.03850v1.pdf | author:Marissa Pastor, Juyong Song, Danh-Tai Hoang, Junghyo Jo category:q-bio.NC cs.NE published:2015-12-12 summary:Feedforward neural networks have been investigated to understand learning andmemory, as well as applied to numerous practical problems in patternclassification. It is a rule of thumb that more complex tasks require largernetworks. However, the design of optimal network architectures for specifictasks is still an unsolved fundamental problem. In this study, we considerthree-layered neural networks for memorizing binary patterns. We developed anew complexity measure of binary patterns, and estimated the minimal networksize for memorizing them as a function of their complexity. We formulated theminimal network size for regular, random, and complex patterns. In particular,the minimal size for complex patterns, which are neither ordered nordisordered, was predicted by measuring their Hamming distances from knownordered patterns. Our predictions agreed with simulations based on theback-propagation algorithm.
arxiv-14100-110 | Multispectral Palmprint Recognition Using a Hybrid Feature | http://arxiv.org/pdf/1112.5997v3.pdf | author:Sina Akbari Mistani, Shervin Minaee, Emad Fatemizadeh category:cs.CV published:2011-12-27 summary:Personal identification problem has been a major field of research in recentyears. Biometrics-based technologies that exploit fingerprints, iris, face,voice and palmprints, have been in the center of attention to solve thisproblem. Palmprints can be used instead of fingerprints that have been of theearliest of these biometrics technologies. A palm is covered with the same skinas the fingertips but has a larger surface, giving us more information than thefingertips. The major features of the palm are palm-lines, including principallines, wrinkles and ridges. Using these lines is one of the most popularapproaches towards solving the palmprint recognition problem. Another robustfeature is the wavelet energy of palms. In this paper we used a hybrid featurewhich combines both of these features. %Moreover, multispectral analysis isapplied to improve the performance of the system. At the end, minimum distanceclassifier is used to match test images with one of the training samples. Theproposed algorithm has been tested on a well-known multispectral palmprintdataset and achieved an average accuracy of 98.8\%.
arxiv-14100-111 | Efficient Deep Feature Learning and Extraction via StochasticNets | http://arxiv.org/pdf/1512.03844v1.pdf | author:Mohammad Javad Shafiee, Parthipan Siva, Paul Fieguth, Alexander Wong category:cs.LG stat.ML published:2015-12-11 summary:Deep neural networks are a powerful tool for feature learning and extractiongiven their ability to model high-level abstractions in highly complex data.One area worth exploring in feature learning and extraction using deep neuralnetworks is efficient neural connectivity formation for faster feature learningand extraction. Motivated by findings of stochastic synaptic connectivityformation in the brain as well as the brain's uncanny ability to efficientlyrepresent information, we propose the efficient learning and extraction offeatures via StochasticNets, where sparsely-connected deep neural networks canbe formed via stochastic connectivity between neurons. To evaluate thefeasibility of such a deep neural network architecture for feature learning andextraction, we train deep convolutional StochasticNets to learn abstractfeatures using the CIFAR-10 dataset, and extract the learned features fromimages to perform classification on the SVHN and STL-10 datasets. Experimentalresults show that features learned using deep convolutional StochasticNets,with fewer neural connections than conventional deep convolutional neuralnetworks, can allow for better or comparable classification accuracy thanconventional deep neural networks: relative test error decrease of ~4.5% forclassification on the STL-10 dataset and ~1% for classification on the SVHNdataset. Furthermore, it was shown that the deep features extracted using deepconvolutional StochasticNets can provide comparable classification accuracyeven when only 10% of the training data is used for feature learning. Finally,it was also shown that significant gains in feature extraction speed can beachieved in embedded applications using StochasticNets. As such, StochasticNetsallow for faster feature learning and extraction performance while facilitatefor better or comparable accuracy performances.
arxiv-14100-112 | Distributed Training of Deep Neural Networks with Theoretical Analysis: Under SSP Setting | http://arxiv.org/pdf/1512.02728v2.pdf | author:Abhimanu Kumar, Pengtao Xie, Junming Yin, Eric P. Xing category:stat.ML cs.LG math.OC published:2015-12-09 summary:We propose a distributed approach to train deep neural networks (DNNs), whichhas guaranteed convergence theoretically and great scalability empirically:close to 6 times faster on instance of ImageNet data set when run with 6machines. The proposed scheme is close to optimally scalable in terms of numberof machines, and guaranteed to converge to the same optima as the undistributedsetting. The convergence and scalability of the distributed setting is shownempirically across di?erent datasets (TIMIT and ImageNet) and machine learningtasks (image classi?cation and phoneme extraction). The convergence analysisprovides novel insights into this complex learning scheme, including: 1)layerwise convergence, and 2) convergence of the weights in probability.
arxiv-14100-113 | Color Space Transformation Network | http://arxiv.org/pdf/1511.01064v2.pdf | author:Alexandros Karargyris category:cs.CV published:2015-10-31 summary:Deep networks have become very popular over the past few years. The mainreason for this widespread use is their excellent ability to learn and predictknowledge in a very easy and efficient way. Convolutional neural networks andauto-encoders have become the normal in the area of imaging and computer visionachieving unprecedented accuracy levels in many applications. The most commonstrategy is to build and train networks with many layers by tuning theirhyper-parameters. While this approach has proven to be a successful way tobuild robust deep learning schemes it suffers from high complexity. In thispaper we introduce a module that learns color space transformations within anetwork. Given a large dataset of colored images the color space transformationmodule tries to learn color space transformations that increase overallclassification accuracy. This module has shown to increase overall accuracy forthe same network design and to achieve faster convergence. It is part of abroader family of image transformations (e.g. spatial transformer network).
arxiv-14100-114 | Rethinking the Inception Architecture for Computer Vision | http://arxiv.org/pdf/1512.00567v3.pdf | author:Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna category:cs.CV published:2015-12-02 summary:Convolutional networks are at the core of most state-of-the-art computervision solutions for a wide variety of tasks. Since 2014 very deepconvolutional networks started to become mainstream, yielding substantial gainsin various benchmarks. Although increased model size and computational costtend to translate to immediate quality gains for most tasks (as long as enoughlabeled data is provided for training), computational efficiency and lowparameter count are still enabling factors for various use cases such as mobilevision and big-data scenarios. Here we explore ways to scale up networks inways that aim at utilizing the added computation as efficiently as possible bysuitably factorized convolutions and aggressive regularization. We benchmarkour methods on the ILSVRC 2012 classification challenge validation setdemonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6%top-5 error for single frame evaluation using a network with a computationalcost of 5 billion multiply-adds per inference and with using less than 25million parameters. With an ensemble of 4 models and multi-crop evaluation, wereport 3.5% top-5 error on the validation set (3.6% error on the test set) and17.3% top-1 error on the validation set.
arxiv-14100-115 | Improving Human Activity Recognition Through Ranking and Re-ranking | http://arxiv.org/pdf/1512.03740v1.pdf | author:Zhenzhong Lan, Shoou-I Yu, Alexander G. Hauptmann category:cs.CV published:2015-12-11 summary:We propose two well-motivated ranking-based methods to enhance theperformance of current state-of-the-art human activity recognition systems.First, as an improvement over the classic power normalization method, wepropose a parameter-free ranking technique called rank normalization (RaN). RaNnormalizes each dimension of the video features to address the sparse andbursty distribution problems of Fisher Vectors and VLAD. Second, inspired bycurriculum learning, we introduce a training-free re-ranking technique calledmulti-class iterative re-ranking (MIR). MIR captures relationships among actionclasses by separating easy and typical videos from difficult ones andre-ranking the prediction scores of classifiers accordingly. We demonstratethat our methods significantly improve the performance of state-of-the-artmotion features on six real-world datasets.
arxiv-14100-116 | A New Approach of Gray Images Binarization with Threshold Methods | http://arxiv.org/pdf/1512.03706v1.pdf | author:Andrei Hossu, Daniela Andone category:cs.CV published:2015-12-11 summary:The paper presents some aspects of the (gray level) image binarizationmethods used in artificial vision systems. It is introduced a new approach ofgray level image binarization for artificial vision systems dedicated toindustrial automation temporal thresholding. In the first part of the paper areextracted some limitations of using the global optimum thresholding in graylevel image binarization. In the second part of this paper are presented someaspects of the dynamic optimum thresholding method for gray level imagebinarization. Starting from classic methods of global and dynamic optimalthresholding of the gray level images in the next section are introduced theconcepts of temporal histogram and temporal thresholding. In the final sectionare presented some practical aspects of the temporal thresholding method inartificial vision applications form the moving scene in robotic automationclass; pointing out the influence of the acquisition frequency on the methodsresults.
arxiv-14100-117 | Single and Multiple Illuminant Estimation Using Convolutional Neural Networks | http://arxiv.org/pdf/1508.00998v2.pdf | author:Simone Bianco, Claudio Cusano, Raimondo Schettini category:cs.CV published:2015-08-05 summary:In this paper we present a method for the estimation of the color of theilluminant in RAW images. The method includes a Convolutional Neural Networkthat has been specially designed to produce multiple local estimates. Amultiple illuminant detector determines whether or not the local outputs of thenetwork must be aggregated into a single estimate. We evaluated our method onstandard datasets with single and multiple illuminants, obtaining lowerestimation errors with respect to those obtained by other general purposemethods in the state of the art.
arxiv-14100-118 | Deep Feature Learning with Relative Distance Comparison for Person Re-identification | http://arxiv.org/pdf/1512.03622v1.pdf | author:Shengyong Ding, Liang Lin, Guangrun Wang, Hongyang Chao category:cs.CV published:2015-12-11 summary:Identifying the same individual across different scenes is an important yetdifficult task in intelligent video surveillance. Its main difficulty lies inhow to preserve similarity of the same person against large appearance andstructure variation while discriminating different individuals. In this paper,we present a scalable distance driven feature learning framework based on thedeep neural network for person re-identification, and demonstrate itseffectiveness to handle the existing challenges. Specifically, given thetraining images with the class labels (person IDs), we first produce a largenumber of triplet units, each of which contains three images, i.e. one personwith a matched reference and a mismatched reference. Treating the units as theinput, we build the convolutional neural network to generate the layeredrepresentations, and follow with the $L2$ distance metric. By means ofparameter optimization, our framework tends to maximize the relative distancebetween the matched pair and the mismatched pair for each triplet unit.Moreover, a nontrivial issue arising with the framework is that the tripletorganization cubically enlarges the number of training triplets, as one imagecan be involved into several triplet units. To overcome this problem, wedevelop an effective triplet generation scheme and an optimized gradientdescent algorithm, making the computational load mainly depends on the numberof original images instead of the number of triplets. On several challengingdatabases, our approach achieves very promising results and outperforms otherstate-of-the-art approaches.
arxiv-14100-119 | Robust Dictionary based Data Representation | http://arxiv.org/pdf/1512.03617v1.pdf | author:Wei-Ya Ren category:cs.CV published:2015-12-11 summary:The robustness to noise and outliers is an important issue in linearrepresentation in real applications. We focus on the problem that samples aregrossly corrupted, which is also the 'sample specific' corruptions problem. Areasonable assumption is that corrupted samples cannot be represented by thedictionary while clean samples can be well represented. This assumption isenforced in this paper by investigating the coefficients of corrupted samples.Concretely, we require the coefficients of corrupted samples be zero. In thisway, the representation quality of clean data can be assured without the effectof corrupted data. At last, a robust dictionary based data representationapproach and its sparse representation version are proposed, which havedirective significance for future applications.
arxiv-14100-120 | Decoding Hidden Markov Models Faster Than Viterbi Via Online Matrix-Vector (max, +)-Multiplication | http://arxiv.org/pdf/1512.00077v2.pdf | author:Massimo Cairo, Gabriele Farina, Romeo Rizzi category:cs.LG cs.DS cs.IT math.IT published:2015-11-30 summary:In this paper, we present a novel algorithm for the maximum a posterioridecoding (MAPD) of time-homogeneous Hidden Markov Models (HMM), improving theworst-case running time of the classical Viterbi algorithm by a logarithmicfactor. In our approach, we interpret the Viterbi algorithm as a repeatedcomputation of matrix-vector $(\max, +)$-multiplications. On time-homogeneousHMMs, this computation is online: a matrix, known in advance, has to bemultiplied with several vectors revealed one at a time. Our main contributionis an algorithm solving this version of matrix-vector $(\max,+)$-multiplicationin subquadratic time, by performing a polynomial preprocessing of the matrix.Employing this fast multiplication algorithm, we solve the MAPD problem in$O(mn^2/ \log n)$ time for any time-homogeneous HMM of size $n$ and observationsequence of length $m$, with an extra polynomial preprocessing cost negligiblefor $m > n$. To the best of our knowledge, this is the first algorithm for theMAPD problem requiring subquadratic time per observation, under the onlyassumption -- usually verified in practice -- that the transition probabilitymatrix does not change with time.
arxiv-14100-121 | Words are not Equal: Graded Weighting Model for building Composite Document Vectors | http://arxiv.org/pdf/1512.03549v1.pdf | author:Pranjal Singh, Amitabha Mukerjee category:cs.CL cs.LG cs.NE published:2015-12-11 summary:Despite the success of distributional semantics, composing phrases from wordvectors remains an important challenge. Several methods have been tried forbenchmark tasks such as sentiment classification, including word vectoraveraging, matrix-vector approaches based on parsing, and on-the-fly learningof paragraph vectors. Most models usually omit stop words from the composition.Instead of such an yes-no decision, we consider several graded schemes wherewords are weighted according to their discriminatory relevance with respect toits use in the document (e.g., idf). Some of these methods (particularlytf-idf) are seen to result in a significant improvement in performance overprior state of the art. Further, combining such approaches into an ensemblebased on alternate classifiers such as the RNN model, results in an 1.6%performance improvement on the standard IMDB movie review dataset, and a 7.01%improvement on Amazon product reviews. Since these are language free models andcan be obtained in an unsupervised manner, they are of interest also forunder-resourced languages such as Hindi as well and many more languages. Wedemonstrate the language free aspects by showing a gain of 12% for two reviewdatasets over earlier results, and also release a new larger dataset for futuretesting (Singh,2015).
arxiv-14100-122 | Distilling Knowledge from Deep Networks with Applications to Healthcare Domain | http://arxiv.org/pdf/1512.03542v1.pdf | author:Zhengping Che, Sanjay Purushotham, Robinder Khemani, Yan Liu category:stat.ML cs.LG published:2015-12-11 summary:Exponential growth in Electronic Healthcare Records (EHR) has resulted in newopportunities and urgent needs for discovery of meaningful data-drivenrepresentations and patterns of diseases in Computational Phenotyping research.Deep Learning models have shown superior performance for robust prediction incomputational phenotyping tasks, but suffer from the issue of modelinterpretability which is crucial for clinicians involved in decision-making.In this paper, we introduce a novel knowledge-distillation approach calledInterpretable Mimic Learning, to learn interpretable phenotype features formaking robust prediction while mimicking the performance of deep learningmodels. Our framework uses Gradient Boosting Trees to learn interpretablefeatures from deep learning models such as Stacked Denoising Autoencoder andLong Short-Term Memory. Exhaustive experiments on a real-world clinicaltime-series dataset show that our method obtains similar or better performancethan the deep learning models, and it provides interpretable phenotypes forclinical decision making.
arxiv-14100-123 | Randomized Low-Rank Dynamic Mode Decomposition for Motion Detection | http://arxiv.org/pdf/1512.03526v1.pdf | author:N. Benjamin Erichson, Carl Donovan category:cs.CV published:2015-12-11 summary:This paper introduces a fast algorithm for randomized computation of alow-rank Dynamic Mode Decomposition (DMD) of a matrix. Here we consider thismatrix to represent the development of a spatial grid through time e.g. datafrom a static video source. DMD was originally introduced in the fluidmechanics community, but is also suitable for motion detection in video streamsand its use for background subtraction has received little previousinvestigation. In this study we present a comprehensive evaluation ofbackground subtraction, using the randomized DMD and compare the results withleading robust principal component analysis algorithms. The results areconvincing and show the random DMD is an efficient and powerful approach forbackground modeling, allowing processing of high resolution videos inreal-time. Supplementary materials include implementations of the algorithms inPython.
arxiv-14100-124 | A Unified Approach to Error Bounds for Structured Convex Optimization Problems | http://arxiv.org/pdf/1512.03518v1.pdf | author:Zirui Zhou, Anthony Man-Cho So category:math.OC cs.LG math.NA stat.ML published:2015-12-11 summary:Error bounds, which refer to inequalities that bound the distance of vectorsin a test set to a given set by a residual function, have proven to beextremely useful in analyzing the convergence rates of a host of iterativemethods for solving optimization problems. In this paper, we present a newframework for establishing error bounds for a class of structured convexoptimization problems, in which the objective function is the sum of a smoothconvex function and a general closed proper convex function. Such a classencapsulates not only fairly general constrained minimization problems but alsovarious regularized loss minimization formulations in machine learning, signalprocessing, and statistics. Using our framework, we show that a number ofexisting error bound results can be recovered in a unified and transparentmanner. To further demonstrate the power of our framework, we apply it to aclass of nuclear-norm regularized loss minimization problems and establish anew error bound for this class under a strict complementarity-type regularitycondition. We then complement this result by constructing an example to showthat the said error bound could fail to hold without the regularity condition.Consequently, we obtain a rather complete answer to a question raised by Tseng.We believe that our approach will find further applications in the study oferror bounds for structured convex optimization problems.
arxiv-14100-125 | Protein secondary structure prediction using deep convolutional neural fields | http://arxiv.org/pdf/1512.00843v3.pdf | author:Sheng Wang, Jian Peng, Jianzhu Ma, Jinbo Xu category:q-bio.BM cs.LG q-bio.QM published:2015-12-02 summary:Protein secondary structure (SS) prediction is important for studying proteinstructure and function. When only the sequence (profile) information is used asinput feature, currently the best predictors can obtain ~80% Q3 accuracy, whichhas not been improved in the past decade. Here we present DeepCNF (DeepConvolutional Neural Fields) for protein SS prediction. DeepCNF is a DeepLearning extension of Conditional Neural Fields (CNF), which is an integrationof Conditional Random Fields (CRF) and shallow neural networks. DeepCNF canmodel not only complex sequence-structure relationship by a deep hierarchicalarchitecture, but also interdependency between adjacent SS labels, so it ismuch more powerful than CNF. Experimental results show that DeepCNF can obtain~84% Q3 accuracy, ~85% SOV score, and ~72% Q8 accuracy, respectively, on theCASP and CAMEO test proteins, greatly outperforming currently popularpredictors. As a general framework, DeepCNF can be used to predict otherprotein structure properties such as contact number, disorder regions, andsolvent accessibility.
arxiv-14100-126 | Computing factorized approximations of Pareto-fronts using mNM-landscapes and Boltzmann distributions | http://arxiv.org/pdf/1512.03466v1.pdf | author:Roberto Santana, Alexander Mendiburu, Jose A. Lozano category:cs.NE published:2015-12-10 summary:NM-landscapes have been recently introduced as a class of tunable ruggedmodels. They are a subset of the general interaction models where all theinteractions are of order less or equal $M$. The Boltzmann distribution hasbeen extensively applied in single-objective evolutionary algorithms toimplement selection and study the theoretical properties of model-buildingalgorithms. In this paper we propose the combination of the multi-objectiveNM-landscape model and the Boltzmann distribution to obtain Pareto-frontapproximations. We investigate the joint effect of the parameters of theNM-landscapes and the probabilistic factorizations in the shape of the Paretofront approximations.
arxiv-14100-127 | Measuring Semantic Relatedness using Mined Semantic Analysis | http://arxiv.org/pdf/1512.03465v1.pdf | author:Walid Shalaby, Wlodek Zadrozny category:cs.CL H.3.1 published:2015-12-10 summary:Mined Semantic Analysis (MSA) is a novel distributional semantics approachwhich employs data mining techniques. MSA embraces knowledge-driven analysis ofnatural languages. It uncovers implicit relations between concepts by miningfor their associations in target encyclopedic corpora. MSA exploits not onlytarget corpus content but also its knowledge graph (e.g., "See also" link graphof Wikipedia). Empirical results show competitive performance of MSA comparedto prior state-of-the-art methods for measuring semantic relatedness onbenchmark data sets. Additionally, we introduce the first analytical study toexamine statistical significance of results reported by different semanticrelatedness methods. Our study shows that, top performing results could bestatistically equivalent though mathematically different. The study positionsMSA as one of state-of-the-art methods for measuring semantic relatedness.
arxiv-14100-128 | Neural Self Talk: Image Understanding via Continuous Questioning and Answering | http://arxiv.org/pdf/1512.03460v1.pdf | author:Yezhou Yang, Yi Li, Cornelia Fermuller, Yiannis Aloimonos category:cs.CV cs.CL cs.RO I.2.10 published:2015-12-10 summary:In this paper we consider the problem of continuously discovering imagecontents by actively asking image based questions and subsequently answeringthe questions being asked. The key components include a Visual QuestionGeneration (VQG) module and a Visual Question Answering module, in whichRecurrent Neural Networks (RNN) and Convolutional Neural Network (CNN) areused. Given a dataset that contains images, questions and their answers, bothmodules are trained at the same time, with the difference being VQG uses theimages as input and the corresponding questions as output, while VQA usesimages and questions as input and the corresponding answers as output. Weevaluate the self talk process subjectively using Amazon Mechanical Turk, whichshow effectiveness of the proposed method.
arxiv-14100-129 | Fast Convergence of Regularized Learning in Games | http://arxiv.org/pdf/1507.00407v5.pdf | author:Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, Robert E. Schapire category:cs.GT cs.AI cs.LG published:2015-07-02 summary:We show that natural classes of regularized learning algorithms with a formof recency bias achieve faster convergence rates to approximate efficiency andto coarse correlated equilibria in multiplayer normal form games. When eachplayer in a game uses an algorithm from our class, their individual regretdecays at $O(T^{-3/4})$, while the sum of utilities converges to an approximateoptimum at $O(T^{-1})$--an improvement upon the worst case $O(T^{-1/2})$ rates.We show a black-box reduction for any algorithm in the class to achieve$\tilde{O}(T^{-1/2})$ rates against an adversary, while maintaining the fasterrates against algorithms in the class. Our results extend those of [Rakhlin andShridharan 2013] and [Daskalakis et al. 2014], who only analyzed two-playerzero-sum games for specific algorithms.
arxiv-14100-130 | Cross-Validated Variable Selection in Tree-Based Methods Improves Predictive Performance | http://arxiv.org/pdf/1512.03444v1.pdf | author:Amichai Painsky, Saharon Rosset category:stat.ML published:2015-12-10 summary:Recursive partitioning approaches producing tree-like models are a longstanding staple of predictive modeling, in the last decade mostly as``sub-learners'' within state of the art ensemble methods like Boosting andRandom Forest. However, a fundamental flaw in the partitioning (or splitting)rule of commonly used tree building methods precludes them from treatingdifferent types of variables equally. This most clearly manifests in thesemethods' inability to properly utilize categorical variables with a largenumber of categories, which are ubiquitous in the new age of big data. Suchvariables can often be very informative, but current tree methods essentiallyleave us a choice of either not using them, or exposing our models to severeoverfitting. We propose a conceptual framework to splitting using leave-one-out(LOO) cross validation for selecting the splitting variable, then performing aregular split (in our case, following CART's approach) for the selectedvariable. The most important consequence of our approach is that categoricalvariables with many categories can be safely used in tree building and are onlychosen if they contribute to predictive power. We demonstrate in extensivesimulation and real data analysis that our novel splitting approachsignificantly improves the performance of both single tree models and ensemblemethods that utilize trees. Importantly, we design an algorithm for LOOsplitting variable selection which under reasonable assumptions does notincrease the overall computational complexity compared to CART for two-classclassification. For regression tasks, our approach carries an increasedcomputational burden, replacing a O(log(n)) factor in CART splitting rulesearch with an O(n) term.
arxiv-14100-131 | Convex Analysis of Mixtures for Separating Non-negative Well-grounded Sources | http://arxiv.org/pdf/1406.7349v3.pdf | author:Yitan Zhu, Niya Wang, David J. Miller, Yue Wang category:stat.ML q-bio.QM published:2014-06-28 summary:Blind Source Separation (BSS) has proven to be a powerful tool for theanalysis of composite patterns in engineering and science. We introduce ConvexAnalysis of Mixtures (CAM) for separating non-negative well-grounded sources,which learns the mixing matrix by identifying the lateral edges of the convexdata scatter plot. We prove a sufficient and necessary condition foridentifying the mixing matrix through edge detection, which also serves as thefoundation for CAM to be applied not only to the exact-determined andover-determined cases, but also to the under-determined case. We show theoptimality of the edge detection strategy, even for cases where sourcewell-groundedness is not strictly satisfied. The CAM algorithm integratesplug-in noise filtering using sector-based clustering, an efficient geometricconvex analysis scheme, and stability-based model order selection. Wedemonstrate the principle of CAM on simulated data and numerically mixednatural images. The superior performance of CAM against a panel of benchmarkBSS techniques is demonstrated on numerically mixed gene expression data. Wethen apply CAM to dissect dynamic contrast-enhanced magnetic resonance imagingdata taken from breast tumors and time-course microarray gene expression dataderived from in-vivo muscle regeneration in mice, both producing biologicallyplausible decomposition results.
arxiv-14100-132 | Scalable Modeling of Conversational-role based Self-presentation Characteristics in Large Online Forums | http://arxiv.org/pdf/1512.03443v1.pdf | author:Abhimanu Kumar, Shriphani Palakodety, Chong Wang, Carolyn P. Rose, Eric P. Xing, Miaomiao Wen category:stat.ML cs.SI published:2015-12-10 summary:Online discussion forums are complex webs of overlapping subcommunities(macrolevel structure, across threads) in which users enact different rolesdepending on which subcommunity they are participating in within a particulartime point (microlevel structure, within threads). This sub-network structureis implicit in massive collections of threads. To uncover this structure, wedevelop a scalable algorithm based on stochastic variational inference andleverage topic models (LDA) along with mixed membership stochastic block (MMSB)models. We evaluate our model on three large-scale datasets,Cancer-ThreadStarter (22K users and 14.4K threads), Cancer-NameMention(15.1Kusers and 12.4K threads) and StackOverFlow (1.19 million users and 4.55 millionthreads). Qualitatively, we demonstrate that our model can provide usefulexplanations of microlevel and macrolevel user presentation characteristics indifferent communities using the topics discovered from posts. Quantitatively,we show that our model does better than MMSB and LDA in predicting user replystructure within threads. In addition, we demonstrate via synthetic dataexperiments that the proposed active sub-network discovery model is stable andrecovers the original parameters of the experimental setup with highprobability.
arxiv-14100-133 | Boosted Sparse Non-linear Distance Metric Learning | http://arxiv.org/pdf/1512.03396v1.pdf | author:Yuting Ma, Tian Zheng category:stat.ML cs.LG published:2015-12-10 summary:This paper proposes a boosting-based solution addressing metric learningproblems for high-dimensional data. Distance measures have been used as naturalmeasures of (dis)similarity and served as the foundation of various learningmethods. The efficiency of distance-based learning methods heavily depends onthe chosen distance metric. With increasing dimensionality and complexity ofdata, however, traditional metric learning methods suffer from poor scalabilityand the limitation due to linearity as the true signals are usually embeddedwithin a low-dimensional nonlinear subspace. In this paper, we propose anonlinear sparse metric learning algorithm via boosting. We restructure aglobal optimization problem into a forward stage-wise learning of weak learnersbased on a rank-one decomposition of the weight matrix in the Mahalanobisdistance metric. A gradient boosting algorithm is devised to obtain a sparserank-one update of the weight matrix at each step. Nonlinear features arelearned by a hierarchical expansion of interactions incorporated within theboosting algorithm. Meanwhile, an early stopping rule is imposed to control theoverall complexity of the learned metric. As a result, our approach guaranteesthree desirable properties of the final metric: positive semi-definiteness, lowrank and element-wise sparsity. Numerical experiments show that our learningmodel compares favorably with the state-of-the-art methods in the currentliterature of metric learning.
arxiv-14100-134 | Deep Residual Learning for Image Recognition | http://arxiv.org/pdf/1512.03385v1.pdf | author:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun category:cs.CV published:2015-12-10 summary:Deeper neural networks are more difficult to train. We present a residuallearning framework to ease the training of networks that are substantiallydeeper than those used previously. We explicitly reformulate the layers aslearning residual functions with reference to the layer inputs, instead oflearning unreferenced functions. We provide comprehensive empirical evidenceshowing that these residual networks are easier to optimize, and can gainaccuracy from considerably increased depth. On the ImageNet dataset we evaluateresidual nets with a depth of up to 152 layers---8x deeper than VGG nets butstill having lower complexity. An ensemble of these residual nets achieves3.57% error on the ImageNet test set. This result won the 1st place on theILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100and 1000 layers. The depth of representations is of central importance for many visualrecognition tasks. Solely due to our extremely deep representations, we obtaina 28% relative improvement on the COCO object detection dataset. Deep residualnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,where we also won the 1st places on the tasks of ImageNet detection, ImageNetlocalization, COCO detection, and COCO segmentation.
arxiv-14100-135 | Speeding Up Distributed Machine Learning Using Codes | http://arxiv.org/pdf/1512.02673v2.pdf | author:Kangwook Lee, Maximilian Lam, Ramtin Pedarsani, Dimitris Papailiopoulos, Kannan Ramchandran category:cs.DC cs.IT cs.LG cs.PF math.IT published:2015-12-08 summary:Codes are widely used in many engineering applications to offer some form ofreliability and fault tolerance. The high-level idea of coding is to exploitresource redundancy to deliver higher robustness against system noise. Inlarge-scale systems there are several types of "noise" that can affect theperformance of distributed machine learning algorithms: straggler nodes, systemfailures, or communication bottlenecks. Moreover, redundancy is abundant: aplethora of nodes, a lot of spare storage, etc. In this work, scratching the surface of "codes for distributed computation,"we provide theoretical insights on how coded solutions can achieve significantgains compared to uncoded ones. We focus on two of the most basic buildingblocks of distributed learning algorithms: matrix multiplication and datashuffling. For matrix multiplication, we use codes to leverage the plethora ofnodes and alleviate the effects of stragglers. We show that if the number ofworkers is $n$, and the runtime of each subtask has an exponential tail, theoptimal coded matrix multiplication is $\Theta(\log n)$ times faster than theuncoded matrix multiplication. In data shuffling, we use codes to exploit theexcess in storage and reduce communication bottlenecks. We show that when$\alpha$ is the fraction of the data matrix that can be cached at each worker,and $n$ is the number of workers, coded shuffling reduces the communicationcost by a factor $\Theta(\alpha \gamma(n))$ compared to uncoded shuffling,where $\gamma(n)$ is the ratio of the cost of unicasting $n$ messages to $n$users to broadcasting a common message (of the same size) to $n$ users. Oursynthetic and Open MPI experiments on Amazon EC2 show that coded distributedalgorithms can achieve significant speedups of up to 40% compared to uncodeddistributed algorithms.
arxiv-14100-136 | Convolutional Monte Carlo Rollouts in Go | http://arxiv.org/pdf/1512.03375v1.pdf | author:Peter H. Jin, Kurt Keutzer category:cs.LG cs.AI published:2015-12-10 summary:In this work, we present a MCTS-based Go-playing program which usesconvolutional networks in all parts. Our method performs MCTS in batches,explores the Monte Carlo search tree using Thompson sampling and aconvolutional network, and evaluates convnet-based rollouts on the GPU. Weachieve strong win rates against open source Go programs and attain competitiveresults against state of the art convolutional net-based Go-playing programs.
arxiv-14100-137 | Fine-grained Image Classification by Exploring Bipartite-Graph Labels | http://arxiv.org/pdf/1512.02665v2.pdf | author:Feng Zhou, Yuanqing Lin category:cs.CV published:2015-12-08 summary:Given a food image, can a fine-grained object recognition engine tell "whichrestaurant which dish" the food belongs to? Such ultra-fine grained imagerecognition is the key for many applications like search by images, but it isvery challenging because it needs to discern subtle difference between classeswhile dealing with the scarcity of training data. Fortunately, the ultra-finegranularity naturally brings rich relationships among object classes. Thispaper proposes a novel approach to exploit the rich relationships throughbipartite-graph labels (BGL). We show how to model BGL in an overallconvolutional neural networks and the resulting system can be optimized throughback-propagation. We also show that it is computationally efficient ininference thanks to the bipartite structure. To facilitate the study, weconstruct a new food benchmark dataset, which consists of 37,885 food imagescollected from 6 restaurants and totally 975 menus. Experimental results onthis new food and three other datasets demonstrates BGL advances previous worksin fine-grained object recognition. An online demo is available athttp://www.f-zhou.com/fg_demo/.
arxiv-14100-138 | Guaranteed algorithms for inference in topic models | http://arxiv.org/pdf/1512.03308v1.pdf | author:Khoat Than, Tung Doan category:stat.ML published:2015-12-10 summary:One of the core problems in statistical models is the estimation of aposterior distribution. For topic models, the problem of posterior inferencefor individual texts is particularly important, especially when dealing withdata streams, but is often intractable in the worst case \citep{SontagR11}. Asa consequence, existing methods for posterior inference are approximate and donot have any guarantee on neither quality nor convergence rate. In this paper,we introduce a provably fast algorithm, namely \textit{Online Maximum aPosterior Estimation (OPE)}, for posterior inference in topic models. OPE hasmore attractive properties than existing inference approaches, includingtheoretical guarantees on quality and fast convergence rate. The discussionsabout OPE are very general and hence can be easily employed in a wide class ofprobabilistic models. Finally, we employ OPE to design three novel methods forlearning Latent Dirichlet allocation from text streams or large corpora.Extensive experiments demonstrate some superior behaviors of OPE and of our newlearning methods.
arxiv-14100-139 | Image patch analysis of sunspots and active regions. II. Clustering via matrix factorization | http://arxiv.org/pdf/1504.02762v2.pdf | author:Kevin R. Moon, Veronique Delouille, Jimmy J. Li, Ruben De Visscher, Fraser Watson, Alfred O. Hero III category:astro-ph.SR cs.CV published:2015-04-10 summary:Separating active regions that are quiet from potentially eruptive ones is akey issue in Space Weather applications. Traditional classification schemessuch as Mount Wilson and McIntosh have been effective in relating an activeregion large scale magnetic configuration to its ability to produce eruptiveevents. However, their qualitative nature prevents systematic studies of anactive region's evolution for example. We introduce a new clustering of activeregions that is based on the local geometry observed in Line of Sightmagnetogram and continuum images. We use a reduced-dimension representation ofan active region that is obtained by factoring the corresponding data matrixcomprised of local image patches. Two factorizations can be compared via thedefinition of appropriate metrics on the resulting factors. The distancesobtained from these metrics are then used to cluster the active regions. Wefind that these metrics result in natural clusterings of active regions. Theclusterings are related to large scale descriptors of an active region such asits size, its local magnetic field distribution, and its complexity as measuredby the Mount Wilson classification scheme. We also find that including datafocused on the neutral line of an active region can result in an increasedcorrespondence between our clustering results and other active regiondescriptors such as the Mount Wilson classifications and the $R$ value. Weprovide some recommendations for which metrics, matrix factorizationtechniques, and regions of interest to use to study active regions.
arxiv-14100-140 | Inference in topic models: sparsity and trade-off | http://arxiv.org/pdf/1512.03300v1.pdf | author:Khoat Than, Tu Bao Ho category:stat.ML published:2015-12-10 summary:Topic models are popular for modeling discrete data (e.g., texts, images,videos, links), and provide an efficient way to discover hiddenstructures/semantics in massive data. One of the core problems in this field isthe posterior inference for individual data instances. This problem isparticularly important in streaming environments, but is often intractable. Inthis paper, we investigate the use of the Frank-Wolfe algorithm (FW) forrecovering sparse solutions to posterior inference. From detailed elucidationof both theoretical and practical aspects, FW exhibits many interestingproperties which are beneficial to topic modeling. We then employ FW to designfast methods, including ML-FW, for learning latent Dirichlet allocation (LDA)at large scales. Extensive experiments show that to reach the samepredictiveness level, ML-FW can perform tens to thousand times faster thanexisting state-of-the-art methods for learning LDA from massive/streaming data.
arxiv-14100-141 | THCHS-30 : A Free Chinese Speech Corpus | http://arxiv.org/pdf/1512.01882v2.pdf | author:Dong Wang, Xuewei Zhang category:cs.CL cs.SD published:2015-12-07 summary:Speech data is crucially important for speech recognition research. There arequite some speech databases that can be purchased at prices that are reasonablefor most research institutes. However, for young people who just start researchactivities or those who just gain initial interest in this direction, the costfor data is still an annoying barrier. We support the `free data' movement inspeech recognition: research institutes (particularly supported by publicfunds) publish their data freely so that new researchers can obtain sufficientdata to kick of their career. In this paper, we follow this trend and release afree Chinese speech database THCHS-30 that can be used to build a full- edgedChinese speech recognition system. We report the baseline system establishedwith this database, including the performance under highly noisy conditions.
arxiv-14100-142 | DISC: Deep Image Saliency Computing via Progressive Representation Learning | http://arxiv.org/pdf/1511.04192v2.pdf | author:Tianshui Chen, Liang Lin, Lingbo Liu, Xiaonan Luo, Xuelong Li category:cs.CV published:2015-11-13 summary:Salient object detection increasingly receives attention as an importantcomponent or step in several pattern recognition and image processing tasks.Although a variety of powerful saliency models have been intensively proposed,they usually involve heavy feature (or model) engineering based on priors (orassumptions) about the properties of objects and backgrounds. Inspired by theeffectiveness of recently developed feature learning, we provide a novel DeepImage Saliency Computing (DISC) framework for fine-grained image saliencycomputing. In particular, we model the image saliency from both the coarse- andfine-level observations, and utilize the deep convolutional neural network(CNN) to learn the saliency representation in a progressive manner.Specifically, our saliency model is built upon two stacked CNNs. The first CNNgenerates a coarse-level saliency map by taking the overall image as the input,roughly identifying saliency regions in the global context. Furthermore, weintegrate superpixel-based local context information in the first CNN to refinethe coarse-level saliency map. Guided by the coarse saliency map, the secondCNN focuses on the local context to produce fine-grained and accurate saliencymap while preserving object details. For a testing image, the two CNNscollaboratively conduct the saliency computing in one shot. Our DISC frameworkis capable of uniformly highlighting the objects-of-interest from complexbackground while preserving well object details. Extensive experiments onseveral standard benchmarks suggest that DISC outperforms otherstate-of-the-art methods and it also generalizes well across datasets withoutadditional training. The executable version of DISC is available online:http://vision.sysu.edu.cn/projects/DISC.
arxiv-14100-143 | A Population Background for Nonparametric Density-Based Clustering | http://arxiv.org/pdf/1408.1381v2.pdf | author:José E. Chacón category:math.ST math.CA math.DG math.GT stat.ML stat.TH published:2014-08-06 summary:Despite its popularity, it is widely recognized that the investigation ofsome theoretical aspects of clustering has been relatively sparse. One of themain reasons for this lack of theoretical results is surely the fact that,whereas for other statistical problems the theoretical population goal isclearly defined (as in regression or classification), for some of theclustering methodologies it is difficult to specify the population goal towhich the data-based clustering algorithms should try to get close. This paperaims to provide some insight into the theoretical foundations of clustering byfocusing on two main objectives: to provide an explicit formulation for theideal population goal of the modal clustering methodology, which understandsclusters as regions of high density; and to present two new loss functions,applicable in fact to any clustering methodology, to evaluate the performanceof a data-based clustering algorithm with respect to the ideal population goal.In particular, it is shown that only mild conditions on a sequence of densityestimators are needed to ensure that the sequence of modal clusterings thatthey induce is consistent.
arxiv-14100-144 | On Russian Roulette Estimates for Bayesian Inference with Doubly-Intractable Likelihoods | http://arxiv.org/pdf/1306.4032v4.pdf | author:Anne-Marie Lyne, Mark Girolami, Yves Atchadé, Heiko Strathmann, Daniel Simpson category:stat.ME stat.CO stat.ML published:2013-06-17 summary:A large number of statistical models are "doubly-intractable": the likelihoodnormalising term, which is a function of the model parameters, is intractable,as well as the marginal likelihood (model evidence). This means that standardinference techniques to sample from the posterior, such as Markov chain MonteCarlo (MCMC), cannot be used. Examples include, but are not confined to,massive Gaussian Markov random fields, autologistic models and Exponentialrandom graph models. A number of approximate schemes based on MCMC techniques,Approximate Bayesian computation (ABC) or analytic approximations to theposterior have been suggested, and these are reviewed here. Exact MCMC schemes,which can be applied to a subset of doubly-intractable distributions, have alsobeen developed and are described in this paper. As yet, no general methodexists which can be applied to all classes of models with doubly-intractableposteriors. In addition, taking inspiration from the Physics literature, westudy an alternative method based on representing the intractable likelihood asan infinite series. Unbiased estimates of the likelihood can then be obtainedby finite time stochastic truncation of the series via Russian Roulettesampling, although the estimates are not necessarily positive. Results from theQuantum Chromodynamics literature are exploited to allow the use of possiblynegative estimates in a pseudo-marginal MCMC scheme such that expectations withrespect to the posterior distribution are preserved. The methodology isreviewed on well-known examples such as the parameters in Ising models, theposterior for Fisher-Bingham distributions on the $d$-Sphere and a large-scaleGaussian Markov Random Field model describing the Ozone Column data. This leadsto a critical assessment of the strengths and weaknesses of the methodologywith pointers to ongoing research.
arxiv-14100-145 | Gated networks: an inventory | http://arxiv.org/pdf/1512.03201v1.pdf | author:Olivier Sigaud, Clément Masson, David Filliat, Freek Stulp category:cs.LG published:2015-12-10 summary:Gated networks are networks that contain gating connections, in which theoutputs of at least two neurons are multiplied. Initially, gated networks wereused to learn relationships between two input sources, such as pixels from twoimages. More recently, they have been applied to learning activity recognitionor multi-modal representations. The aims of this paper are threefold: 1) toexplain the basic computations in gated networks to the non-expert, whileadopting a standpoint that insists on their symmetric nature. 2) to serve as aquick reference guide to the recent literature, by providing an inventory ofapplications of these networks, as well as recent extensions to the basicarchitecture. 3) to suggest future research directions and applications.
arxiv-14100-146 | 3D Reconstruction of Crime Scenes and Design Considerations for an Interactive Investigation Tool | http://arxiv.org/pdf/1512.03156v1.pdf | author:Erkan Bostanci category:cs.CV published:2015-12-10 summary:Crime Scene Investigation (CSI) is a carefully planned systematic processwith the purpose of acquiring physical evidences to shed light upon thephysical reality of the crime and eventually detect the identity of thecriminal. Capturing images and videos of the crime scene is an important partof this process in order to conduct a deeper analysis on the digital evidencefor possible hints. This work brings this idea further to use the acquiredfootage for generating a 3D model of the crime scene. Results show thatrealistic reconstructions can be obtained using sophisticated computer visiontechniques. The paper also discusses a number of important designconsiderations describing key features that should be present in a powerfulinteractive CSI analysis tool.
arxiv-14100-147 | Enhanced image feature coverage: Key-point selection using genetic algorithms | http://arxiv.org/pdf/1512.03155v1.pdf | author:Erkan Bostanci category:cs.CV published:2015-12-10 summary:Coverage of image features play an important role in many vision algorithmssince their distribution affect the estimated homography. This paper presents aGenetic Algorithm (GA) in order to select the optimal set of features yieldingmaximum coverage of the image which is measured by a robust method based onspatial statistics. It is shown with statistical tests on two datasets that themetric yields better coverage and this is also confirmed by an accuracy test onthe computed homography for the original set and the newly selected set offeatures. Results have demonstrated that the new set has similar performance interms of the accuracy of the computed homography with the original one with anextra benefit of using fewer number of features ultimately reducing the timerequired for descriptor calculation and matching.
arxiv-14100-148 | Evaluation of Object Detection Proposals Under Condition Variations | http://arxiv.org/pdf/1512.03424v1.pdf | author:Fahimeh Rezazadegan, Sareh Shirazi, Michael Milford, Ben Upcroft category:cs.CV published:2015-12-10 summary:Object detection is a fundamental task in many computer vision applications,therefore the importance of evaluating the quality of object detection is wellacknowledged in this domain. This process gives insight into the capabilitiesof methods in handling environmental changes. In this paper, a new method forobject detection is introduced that combines the Selective Search andEdgeBoxes. We tested these three methods under environmental variations. Ourexperiments demonstrate the outperformance of the combination method underillumination and view point variations.
arxiv-14100-149 | Predicting proximity with ambient mobile sensors for non-invasive health diagnostics | http://arxiv.org/pdf/1512.03423v1.pdf | author:Sylvester Olubolu Orimaye, Foo Chuan Leong, Chen Hui Lee, Eddy Cheng Han Ng category:cs.CY cs.LG published:2015-12-10 summary:Modern smart phones are becoming helpful in the areas of Internet-Of-Things(IoT) and ambient health intelligence. By learning data from several mobilesensors, we detect nearness of the human body to a mobile device in athree-dimensional space with no physical contact with the device fornon-invasive health diagnostics. We show that the human body generates wavepatterns that interact with other naturally occurring ambient signals thatcould be measured by mobile sensors, such as, temperature, humidity, magneticfield, acceleration, gravity, and light. This interaction consequentiallyalters the patterns of the naturally occurring signals, and thus, exhibitscharacteristics that could be learned to predict the nearness of the human bodyto a mobile device, hence provide diagnostic information for medicalpractitioners. Our prediction technique achieved 88.75% accuracy and 88.3%specificity.
arxiv-14100-150 | Approximate Message Passing with Restricted Boltzmann Machine Priors | http://arxiv.org/pdf/1502.06470v3.pdf | author:Eric W. Tramel, Angélique Drémeau, Florent Krzakala category:cs.IT math.IT stat.ML published:2015-02-23 summary:Approximate Message Passing (AMP) has been shown to be an excellentstatistical approach to signal inference and compressed sensing problem. TheAMP framework provides modularity in the choice of signal prior; here wepropose a hierarchical form of the Gauss-Bernouilli prior which utilizes aRestricted Boltzmann Machine (RBM) trained on the signal support to pushreconstruction performance beyond that of simple iid priors for signals whosesupport can be well represented by a trained binary RBM. We present and analyzetwo methods of RBM factorization and demonstrate how these affect signalreconstruction performance within our proposed algorithm. Finally, using theMNIST handwritten digit dataset, we show experimentally that using an RBMallows AMP to approach oracle-support performance.
arxiv-14100-151 | Learning Linguistic Biomarkers for Predicting Mild Cognitive Impairment using Compound Skip-grams | http://arxiv.org/pdf/1511.02436v2.pdf | author:Sylvester Olubolu Orimaye, Kah Yee Tai, Jojo Sze-Meng Wong, Chee Piau Wong category:cs.CL cs.AI published:2015-11-08 summary:Predicting Mild Cognitive Impairment (MCI) is currently a challenge asexisting diagnostic criteria rely on neuropsychological examinations. AutomatedMachine Learning (ML) models that are trained on verbal utterances of MCIpatients can aid diagnosis. Using a combination of skip-gram features, ourmodel learned several linguistic biomarkers to distinguish between 19 patientswith MCI and 19 healthy control individuals from the DementiaBank languagetranscript clinical dataset. Results show that a model with compound ofskip-grams has better AUC and could help ML prediction on small MCI datasample.
arxiv-14100-152 | Deep Learning Algorithms with Applications to Video Analytics for A Smart City: A Survey | http://arxiv.org/pdf/1512.03131v1.pdf | author:Li Wang, Dennis Sng category:cs.CV published:2015-12-10 summary:Deep learning has recently achieved very promising results in a wide range ofareas such as computer vision, speech recognition and natural languageprocessing. It aims to learn hierarchical representations of data by using deeparchitecture models. In a smart city, a lot of data (e.g. videos captured frommany distributed sensors) need to be automatically processed and analyzed. Inthis paper, we review the deep learning algorithms applied to video analyticsof smart city in terms of different research topics: object detection, objecttracking, face recognition, image classification and scene labeling.
arxiv-14100-153 | How the Voynich Manuscript was created | http://arxiv.org/pdf/1407.6639v3.pdf | author:Torsten Timm category:cs.CR cs.CL published:2014-07-24 summary:The Voynich manuscript is a medieval book written in an unknown script. Thispaper studies the relation between similarly spelled words in the Voynichmanuscript. By means of a detailed analysis of similar spelled words it waspossible to reveal the text generation method used for the Voynich manuscript.
arxiv-14100-154 | Minimax Correlation Clustering and Biclustering: Bounding Errors Locally | http://arxiv.org/pdf/1506.08189v2.pdf | author:Gregory J. Puleo, Olgica Milenkovic category:cs.DS cs.LG published:2015-06-26 summary:We introduce a new agnostic clustering model, \emph{minimax correlationclustering}, and a rounding algorithm tailored to the needs of this model.Given a graph whose edges are labeled with $+$ or $-$, we wish to partition thegraph into clusters while trying to avoid errors: $+$ edges between clusters or$-$ edges within clusters. Unlike classical correlation clustering, which seeksto minimize the total number of errors, minimax clustering instead seeks tominimize the number of errors at the \emph{worst vertex}, that is, at thevertex with the greatest number of incident errors. This minimax objectivefunction may be seen as a way to enforce individual-level quality of partitionconstraints for vertices in a graph. We study this problem on complete graphsand complete bipartite graphs, proving that the problem is NP-hard on thesegraph classes and giving polynomial-time constant-factor approximationalgorithms. The approximation algorithms rely on LP relaxation and roundingprocedures. We also discuss the broader applicability of our rounding algorithmto other (nonlinear) objective functions for correlation clustering.
arxiv-14100-155 | Gamma Belief Networks | http://arxiv.org/pdf/1512.03081v1.pdf | author:Mingyuan Zhou, Yulai Cong, Bo Chen category:stat.ML stat.ME published:2015-12-09 summary:To infer multilayer deep representations of high-dimensional discrete andnonnegative real vectors, we propose the gamma belief network (GBN) thatfactorizes each of its hidden layers into the product of a sparse connectionweight matrix and the nonnegative real hidden units of the next layer. TheGBN's hidden layers are jointly trained with an upward-downward Gibbs samplerthat solves each layer with the same subroutine. The gamma-negative binomialprocess combined with a layer-wise training strategy allows inferring the widthof each layer given a fixed budget on the width of the first layer. Exampleresults illustrate interesting relationships between the width of the firstlayer and the inferred network structure, and demonstrate that the GBN can addmore layers to improve its performance in both unsupervisedly extractingfeatures and predicting heldout data. For exploratory data analysis, we extracttrees and subnetworks from the learned deep network to visualize how the veryspecific factors discovered at the first hidden layer and the increasingly moregeneral factors discovered at deeper hidden layers are related to each other,and we generate synthetic data by propagating random variables through the deepnetwork from the top hidden layer back to the bottom data layer.
arxiv-14100-156 | Partial Reinitialisation for Optimisers | http://arxiv.org/pdf/1512.03025v1.pdf | author:Ilia Zintchenko, Matthew Hastings, Nathan Wiebe, Ethan Brown, Matthias Troyer category:stat.ML cs.LG cs.NE math.OC published:2015-12-09 summary:Heuristic optimisers which search for an optimal configuration of variablesrelative to an objective function often get stuck in local optima where thealgorithm is unable to find further improvement. The standard approach tocircumvent this problem involves periodically restarting the algorithm fromrandom initial configurations when no further improvement can be found. Wepropose a method of partial reinitialization, whereby, in an attempt to find abetter solution, only sub-sets of variables are re-initialised rather than thewhole configuration. Much of the information gained from previous runs is henceretained. This leads to significant improvements in the quality of the solutionfound in a given time for a variety of optimisation problems in machinelearning.
arxiv-14100-157 | Minimally Supervised Feature Selection for Classification (Master's Thesis, University Politehnica of Bucharest) | http://arxiv.org/pdf/1512.03019v1.pdf | author:Alexandra Maria Radu category:cs.CV published:2015-12-09 summary:In the context of the highly increasing number of features that are availablenowadays we design a robust and fast method for feature selection. The methodtries to select the most representative features that are independent from eachother, but are strong together. We propose an algorithm that requires verylimited labeled data (as few as one labeled frame per class) and canaccommodate as many unlabeled samples. We also present here the supervisedapproach from which we started. We compare our two formulations withestablished methods like AdaBoost, SVM, Lasso, Elastic Net and FoBa and showthat our method is much faster and it has constant training time. Moreover, theunsupervised approach outperforms all the methods with which we compared andthe difference might be quite prominent. The supervised approach is in mostcases better than the other methods, especially when the number of trainingshots is very limited. All that the algorithm needs is to choose from a pool ofpositively correlated features. The methods are evaluated on theYoutube-Objects dataset of videos and on MNIST digits dataset, while attraining time we also used features obtained on CIFAR10 dataset and otherspre-trained on ImageNet dataset. Thereby, we also proved that transfer learningis useful, even though the datasets differ very much: from low-resolutioncentered images from 10 classes, to high-resolution images with objects from1000 classes occurring in different regions of the images or to very difficultvideos with very high intraclass variance. 7
arxiv-14100-158 | ShapeNet: An Information-Rich 3D Model Repository | http://arxiv.org/pdf/1512.03012v1.pdf | author:Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, Fisher Yu category:cs.GR cs.AI cs.CG cs.CV cs.RO published:2015-12-09 summary:We present ShapeNet: a richly-annotated, large-scale repository of shapesrepresented by 3D CAD models of objects. ShapeNet contains 3D models from amultitude of semantic categories and organizes them under the WordNet taxonomy.It is a collection of datasets providing many semantic annotations for each 3Dmodel such as consistent rigid alignments, parts and bilateral symmetry planes,physical sizes, keywords, as well as other planned annotations. Annotations aremade available through a public web-based interface to enable datavisualization of object attributes, promote data-driven geometric analysis, andprovide a large-scale quantitative benchmark for research in computer graphicsand vision. At the time of this technical report, ShapeNet has indexed morethan 3,000,000 models, 220,000 models out of which are classified into 3,135categories (WordNet synsets). In this report we describe the ShapeNet effort asa whole, provide details for all currently available datasets, and summarizefuture plans.
arxiv-14100-159 | Feature-based Attention in Convolutional Neural Networks | http://arxiv.org/pdf/1511.06408v2.pdf | author:Grace W. Lindsay category:cs.CV published:2015-11-19 summary:Convolutional neural networks (CNNs) have proven effective for imageprocessing tasks, such as object recognition and classification. Recently, CNNshave been enhanced with concepts of attention, similar to those found inbiology. Much of this work on attention has focused on effective serial spatialprocessing. In this paper, I introduce a simple procedure for applyingfeature-based attention (FBA) to CNNs and compare multiple implementationoptions. FBA is a top-down signal applied globally to an input image whichaides in detecting chosen objects in cluttered or noisy settings. The conceptof FBA and the implementation details tested here were derived from what isknown (and debated) about biological object- and feature-based attention. Theimplementations of FBA described here increase performance on challengingobject detection tasks using a procedure that is simple, fast, and does notrequire additional iterative training. Furthermore, the comparisons performedhere suggest that a proposed model of biological FBA (the "feature similaritygain model") is effective in increasing performance.
arxiv-14100-160 | Get More With Less: Near Real-Time Image Clustering on Mobile Phones | http://arxiv.org/pdf/1512.02972v1.pdf | author:Jorge Ortiz, Chien-Chin Huang, Supriyo Chakraborty category:cs.CV cs.DC cs.PF published:2015-12-09 summary:Machine learning algorithms, in conjunction with user data, hold the promiseof revolutionizing the way we interact with our phones, and indeed theirwidespread adoption in the design of apps bear testimony to this promise.However, currently, the computationally expensive segments of the learningpipeline, such as feature extraction and model training, are offloaded to thecloud, resulting in an over-reliance on the network and under-utilization ofcomputing resources available on mobile platforms. In this paper, we show thatby combining the computing power distributed over a number of phones, judiciousoptimization choices, and contextual information it is possible to execute theend-to-end pipeline entirely on the phones at the edge of the network,efficiently. We also show that by harnessing the power of this combination, itis possible to execute a computationally expensive pipeline at near real-time. To demonstrate our approach, we implement an end-to-end image-processingpipeline -- that includes feature extraction, vocabulary learning,vectorization, and image clustering -- on a set of mobile phones. Our resultsshow a 75% improvement over the standard, full pipeline implementation runningon the phones without modification -- reducing the time to one minute undercertain conditions. We believe that this result is a promising indication thatfully distributed, infrastructure-less computing is possible on networks ofmobile phones; enabling a new class of mobile applications that are lessreliant on the cloud.
arxiv-14100-161 | Scaling Up Distributed Stochastic Gradient Descent Using Variance Reduction | http://arxiv.org/pdf/1512.02970v1.pdf | author:Soham De, Gavin Taylor, Tom Goldstein category:cs.LG cs.DC math.OC stat.ML published:2015-12-09 summary:Variance reduction stochastic gradient descent methods enable minimization ofmodel fitting problems involving big datasets with low iteration complexity andfast asymptotic convergence rates. However, they scale poorly in distributedsettings. In this paper, we propose a highly parallel variance reductionmethod, CentralVR, with performance that scales linearly with the number ofworker nodes. We also propose distributed versions of popular variancereduction methods that support a high degree of parallelization. Unlikeexisting distributed stochastic gradient schemes, CentralVR exhibits linearperformance gains up to thousands of cores for massive datasets.
arxiv-14100-162 | Video captioning with recurrent networks based on frame- and video-level features and visual content classification | http://arxiv.org/pdf/1512.02949v1.pdf | author:Rakshith Shetty, Jorma Laaksonen category:cs.CV published:2015-12-09 summary:In this paper, we describe the system for generating textual descriptions ofshort video clips using recurrent neural networks (RNN), which we used whileparticipating in the Large Scale Movie Description Challenge 2015 in ICCV 2015.Our work builds on static image captioning systems with RNN based languagemodels and extends this framework to videos utilizing both static imagefeatures and video-specific features. In addition, we study the usefulness ofvisual content classifiers as a source of additional information for captiongeneration. With experimental results we show that utilizing keyframe basedfeatures, dense trajectory video features and content classifier outputstogether gives better performance than any one of them individually.
arxiv-14100-163 | Stochastic Interpretation of Quasi-periodic Event-based Systems | http://arxiv.org/pdf/1512.02930v1.pdf | author:Hesham Mostafa, Giacomo Indiveri category:cs.NE cs.ET q-bio.NC published:2015-12-09 summary:Many networks used in machine learning and as models of biological neuralnetworks make use of stochastic neurons or neuron-like units. We show thatstochastic artificial neurons can be realized on silicon chips by exploitingthe quasi-periodic behavior of mismatched analog oscillators to approximate theneuron's stochastic activation function. We represent neurons by finite statemachines (FSMs) that communicate using digital events and whose transitions areevent-triggered. The event generation times of each neuron are controlled by ananalog oscillator internal to that neuron/FSM and the frequencies of theoscillators in different FSMs are incommensurable. We show that within thisquasi-periodic system, the transition graph of a FSM can be interpreted as thetransition graph of a Markov chain and we show that by using different FSMs, wecan obtain approximations of different stochastic activation functions. Weinvestigate the quality of the stochastic interpretation of such adeterministic system and we use the system to realize and sample from arestricted Boltzmann machine. We implemented the quasi-periodic event-basedsystem on a custom silicon chip and we show that the chip behavior can be usedto closely approximate a stochastic sampling task.
arxiv-14100-164 | Yet Another Statistical Analysis of Bob Ross Paintings | http://arxiv.org/pdf/1512.02914v1.pdf | author:Christopher Steven Marcum category:stat.AP cs.CV published:2015-12-09 summary:In this paper, we analyze a sample of clippings from paintings by the lateartist Bob Ross. Previous work focused on the qualitative themes of hispaintings (Hickey, 2014); here, we expand on that line of research byconsidering the colorspace and luminosity values as our data. Our resultsdemonstrate the subtle aesthetics of the average Ross painting, the commonvariation shared by his paintings, and the structure of the relationshipsbetween each painting in our sample. We reveal, for the first time, renderingsof the average paintings and introduce "eigenross" components to identify andevaluate shared variance. Additionally, all data and code are embedded in thisdocument to encourage future research, and, in the spirit of Bob Ross, to teachothers how to do so.
arxiv-14100-165 | MovieQA: Understanding Stories in Movies through Question-Answering | http://arxiv.org/pdf/1512.02902v1.pdf | author:Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, Sanja Fidler category:cs.CV cs.CL published:2015-12-09 summary:We introduce the MovieQA dataset which aims to evaluate automatic storycomprehension from both video and text. The dataset consists of 7702 questionsabout 294 movies with high semantic diversity. The questions range from simpler"Who" did "What" to "Whom", to "Why" and "How" certain events occurred. Eachquestion comes with a set of five possible answers; a correct one and fourdeceiving answers provided by human annotators. Our dataset is unique in thatit contains multiple sources of information -- full-length movies, plots,subtitles, scripts and for a subset DVS. We analyze our data through variousstatistics and intelligent baselines. We further extend existing QA techniquesto show that question-answering with such open-ended semantics is hard. We planto create a benchmark with an active leader board, to encourage inspiring workin this challenging domain.
arxiv-14100-166 | Where You Are Is Who You Are: User Identification by Matching Statistics | http://arxiv.org/pdf/1512.02896v1.pdf | author:Farid M. Naini, Jayakrishnan Unnikrishnan, Patrick Thiran, Martin Vetterli category:cs.LG cs.CR cs.SI stat.AP stat.ML published:2015-12-09 summary:Most users of online services have unique behavioral or usage patterns. Thesebehavioral patterns can be exploited to identify and track users by using onlythe observed patterns in the behavior. We study the task of identifying usersfrom statistics of their behavioral patterns. Specifically, we focus on thesetting in which we are given histograms of users' data collected during twodifferent experiments. We assume that, in the first dataset, the users'identities are anonymized or hidden and that, in the second dataset, theiridentities are known. We study the task of identifying the users by matchingthe histograms of their data in the first dataset with the histograms from thesecond dataset. In recent works, the optimal algorithm for this useridentification task is introduced. In this paper, we evaluate the effectivenessof this method on three different types of datasets and in multiple scenarios.Using datasets such as call data records, web browsing histories, and GPStrajectories, we show that a large fraction of users can be easily identifiedgiven only histograms of their data; hence these histograms can act as users'fingerprints. We also verify that simultaneous identification of users achievesbetter performance compared to one-by-one user identification. We show thatusing the optimal method for identification gives higher identificationaccuracy than heuristics-based approaches in practical scenarios. The accuracyobtained under this optimal method can thus be used to quantify the maximumlevel of user identification that is possible in such settings. We show thatthe key factors affecting the accuracy of the optimal identification algorithmare the duration of the data collection, the number of users in the anonymizeddataset, and the resolution of the dataset. We analyze the effectiveness ofk-anonymization in resisting user identification attacks on these datasets.
arxiv-14100-167 | DETRAC: A New Benchmark and Protocol for Multi-Object Tracking | http://arxiv.org/pdf/1511.04136v2.pdf | author:Longyin Wen, Dawei Du, Zhaowei Cai, Zhen Lei, Ming-Ching Chang, Honggang Qi, Jongwoo Lim, Ming-Hsuan Yang, Siwei Lyu category:cs.CV published:2015-11-13 summary:In recent years, most effective multi-object tracking (MOT) methods are basedon the tracking-by-detection framework. Existing performance evaluations of MOTmethods usually separate the target association step from the object detectionstep by using the same object detection results for comparisons. In this work,we perform a comprehensive quantitative study on the effect of object detectionaccuracy to the overall MOT performance. This is based on a new large-scaleDETection and tRACking (DETRAC) benchmark dataset. The DETRAC benchmark datasetconsists of 100 challenging video sequences captured from real-world trafficscenes (over 140 thousand frames and 1.2 million labeled bounding boxes ofobjects) for both object detection and MOT. We evaluate complete MOT systemsconstructed from combinations of state-of-the-art target association methodsand object detection schemes. Our analysis shows the complex effects of objectdetection accuracy on MOT performance. Based on these observations, we proposenew evaluation tools and metrics for MOT systems that consider both objectdetection and target association for comprehensive analysis.
arxiv-14100-168 | Minimum Risk Training for Neural Machine Translation | http://arxiv.org/pdf/1512.02433v2.pdf | author:Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, Yang Liu category:cs.CL published:2015-12-08 summary:We propose minimum risk training for end-to-end neural machine translation.Unlike conventional maximum likelihood estimation, minimum risk training iscapable of optimizing model parameters directly with respect to evaluationmetrics. Experiments on Chinese-English and English-French translation showthat our approach achieves significant improvements over maximum likelihoodestimation on a state-of-the-art neural machine translation system.
arxiv-14100-169 | Multi-Player Bandits -- a Musical Chairs Approach | http://arxiv.org/pdf/1512.02866v1.pdf | author:Jonathan Rosenski, Ohad Shamir, Liran Szlak category:cs.LG stat.ML published:2015-12-09 summary:We consider a variant of the stochastic multi-armed bandit problem, wheremultiple players simultaneously choose from the same set of arms and maycollide, receiving no reward. This setting has been motivated by problemsarising in cognitive radio networks, and is especially challenging under therealistic assumption that communication between players is limited. We providea communication-free algorithm (Musical Chairs) which attains constant regretwith high probability, as well as a sublinear-regret, communication-freealgorithm (Dynamic Musical Chairs) for the more difficult setting of playersdynamically entering and leaving throughout the game. Moreover, both algorithmsdo not require prior knowledge of the number of players. To the best of ourknowledge, these are the first communication-free algorithms with these typesof formal guarantees. We also rigorously compare our algorithms to previousworks, and complement our theoretical findings with experiments.
arxiv-14100-170 | Bigger Buffer k-d Trees on Multi-Many-Core Systems | http://arxiv.org/pdf/1512.02831v1.pdf | author:Fabian Gieseke, Cosmin Eugen Oancea, Ashish Mahabal, Christian Igel, Tom Heskes category:cs.DC cs.DS cs.LG published:2015-12-09 summary:A buffer k-d tree is a k-d tree variant for massively-parallel nearestneighbor search. While providing valuable speed-ups on modern many-core devicesin case both a large number of reference and query points are given, buffer k-dtrees are limited by the amount of points that can fit on a single device. Inthis work, we show how to modify the original data structure and the associatedworkflow to make the overall approach capable of dealing with massive datasets. We further provide a simple yet efficient way of using multiple devicesgiven in a single workstation. The applicability of the modified framework isdemonstrated in the context of astronomy, a field that is faced with hugeamounts of data.
arxiv-14100-171 | Risk Minimization in Structured Prediction using Orbit Loss | http://arxiv.org/pdf/1512.02033v2.pdf | author:Danny Karmon, Joseph Keshet category:cs.LG published:2015-12-07 summary:We introduce a new surrogate loss function called orbit loss in thestructured prediction framework, which has good theoretical and practicaladvantages. While the orbit loss is not convex, it has a simple analyticalgradient and a simple perceptron-like learning rule. We analyze the new losstheoretically and state a PAC-Bayesian generalization bound. We also prove thatthe new loss is consistent in the strong sense; namely, the risk achieved bythe set of the trained parameters approaches the infimum risk achievable by anylinear decoder over the given features. Methods that are aimed at riskminimization, such as the structured ramp loss, the structured probit loss andthe direct loss minimization require at least two inference operations pertraining iteration. In this sense, the orbit loss is more efficient as itrequires only one inference operation per training iteration, while yieldssimilar performance. We conclude the paper with an empirical comparison of theproposed loss function to the structured hinge loss, the structured ramp loss,the structured probit loss and the direct loss minimization method on severalbenchmark datasets and tasks.
arxiv-14100-172 | Sensor Fusion of Camera, GPS and IMU using Fuzzy Adaptive Multiple Motion Models | http://arxiv.org/pdf/1512.02766v1.pdf | author:Erkan Bostanci, Betul Bostanci, Nadia Kanwal, Adrian F. Clark category:cs.RO cs.CV published:2015-12-09 summary:A tracking system that will be used for Augmented Reality (AR) applicationshas two main requirements: accuracy and frame rate. The first requirement isrelated to the performance of the pose estimation algorithm and how accuratelythe tracking system can find the position and orientation of the user in theenvironment. Accuracy problems of current tracking devices, considering thatthey are low-cost devices, cause static errors during this motion estimationprocess. The second requirement is related to dynamic errors (the end-to-endsystem delay; occurring because of the delay in estimating the motion of theuser and displaying images based on this estimate. This paper investigatescombining the vision-based estimates with measurements from other sensors, GPSand IMU, in order to improve the tracking accuracy in outdoor environments. Theidea of using Fuzzy Adaptive Multiple Models (FAMM) was investigated using anovel fuzzy rule-based approach to decide on the model that results in improvedaccuracy and faster convergence for the fusion filter. Results show that thedeveloped tracking system is more accurate than a conventional GPS-IMU fusionapproach due to additional estimates from a camera and fuzzy motion models. Thepaper also presents an application in cultural heritage context.
arxiv-14100-173 | Return of Frustratingly Easy Domain Adaptation | http://arxiv.org/pdf/1511.05547v2.pdf | author:Baochen Sun, Jiashi Feng, Kate Saenko category:cs.CV cs.AI cs.LG cs.NE published:2015-11-17 summary:Unlike human learning, machine learning often fails to handle changes betweentraining (source) and test (target) input distributions. Such domain shifts,common in practical scenarios, severely damage the performance of conventionalmachine learning methods. Supervised domain adaptation methods have beenproposed for the case when the target data have labels, including some thatperform very well despite being "frustratingly easy" to implement. However, inpractice, the target domain is often unlabeled, requiring unsupervisedadaptation. We propose a simple, effective, and efficient method forunsupervised domain adaptation called CORrelation ALignment (CORAL). CORALminimizes domain shift by aligning the second-order statistics of source andtarget distributions, without requiring any target labels. Even though it isextraordinarily simple--it can be implemented in four lines of Matlabcode--CORAL performs remarkably well in extensive evaluations on standardbenchmark datasets.
arxiv-14100-174 | Hinge-Loss Markov Random Fields and Probabilistic Soft Logic | http://arxiv.org/pdf/1505.04406v2.pdf | author:Stephen H. Bach, Matthias Broecheler, Bert Huang, Lise Getoor category:cs.LG cs.AI stat.ML published:2015-05-17 summary:A fundamental challenge in developing high-impact machine learningtechnologies is balancing the ability to model rich, structured domains withthe ability to scale to big data. Many important problem areas are both richlystructured and large scale, from social and biological networks, to knowledgegraphs and the Web, to images, video, and natural language. In this paper, weintroduce two new formalisms for modeling structured data, distinguished fromprevious approaches by their ability to both capture rich structure and scaleto big data. The first, hinge-loss Markov random fields (HL-MRFs), is a newkind of probabilistic graphical model that generalizes different approaches toconvex inference. We unite three approaches from the randomized algorithms,probabilistic graphical models, and fuzzy logic communities, showing that allthree lead to the same inference objective. We then derive HL-MRFs bygeneralizing this unified objective. The second new formalism, probabilisticsoft logic (PSL), is a probabilistic programming language that makes HL-MRFseasy to define using a syntax based on first-order logic. We next introduce analgorithm for inferring most-probable variable assignments (MAP inference) thatis much more scalable than general-purpose convex optimization software,because it uses message passing to take advantage of sparse dependencystructures. We then show how to learn the parameters of HL-MRFs. The learnedHL-MRFs are as accurate as analogous discrete models, but much more scalable.Together, these algorithms enable HL-MRFs and PSL to model rich, structureddata at scales not previously possible.
arxiv-14100-175 | Scalable, High-Quality Object Detection | http://arxiv.org/pdf/1412.1441v3.pdf | author:Christian Szegedy, Scott Reed, Dumitru Erhan, Dragomir Anguelov, Sergey Ioffe category:cs.CV published:2014-12-03 summary:Current high-quality object detection approaches use the scheme ofsalience-based object proposal methods followed by post-classification usingdeep convolutional features. This spurred recent research in improving objectproposal methods. However, domain agnostic proposal generation has theprincipal drawback that the proposals come unranked or with very weak ranking,making it hard to trade-off quality for running time. This raises the morefundamental question of whether high-quality proposal generation requirescareful engineering or can be derived just from data alone. We demonstrate thatlearning-based proposal methods can effectively match the performance ofhand-engineered methods while allowing for very efficient runtime-qualitytrade-offs. Using the multi-scale convolutional MultiBox (MSC-MultiBox)approach, we substantially advance the state-of-the-art on the ILSVRC 2014detection challenge data set, with $0.5$ mAP for a single model and $0.52$ mAPfor an ensemble of two models. MSC-Multibox significantly improves the proposalquality over its predecessor MultiBox~method: AP increases from $0.42$ to$0.53$ for the ILSVRC detection challenge. Finally, we demonstrate improvedbounding-box recall compared to Multiscale Combinatorial Grouping with lessproposals on the Microsoft-COCO data set.
arxiv-14100-176 | Window-Object Relationship Guided Representation Learning for Generic Object Detections | http://arxiv.org/pdf/1512.02736v1.pdf | author:Xingyu Zeng, Wanli Ouyang, Xiaogang Wang category:cs.CV cs.LG cs.MM published:2015-12-09 summary:In existing works that learn representation for object detection, therelationship between a candidate window and the ground truth bounding box of anobject is simplified by thresholding their overlap. This paper showsinformation loss in this simplification and picks up the relative location/sizeinformation discarded by thresholding. We propose a representation learningpipeline to use the relationship as supervision for improving the learnedrepresentation in object detection. Such relationship is not limited to objectof the target category, but also includes surrounding objects of othercategories. We show that image regions with multiple contexts and multiplerotations are effective in capturing such relationship during therepresentation learning process and in handling the semantic and visualvariation caused by different window-object configurations. Experimentalresults show that the representation learned by our approach can improve theobject detection accuracy by 6.4% in mean average precision (mAP) onILSVRC2014. On the challenging ILSVRC2014 test dataset, 48.6% mAP is achievedby our single model and it is the best among published results. On PASCAL VOC,it outperforms the state-of-the-art result of Fast RCNN by 3.3% in absolutemAP.
arxiv-14100-177 | Level-Based Analysis of Genetic Algorithms for Combinatorial Optimization | http://arxiv.org/pdf/1512.02047v2.pdf | author:Duc-Cuong Dang, Anton V. Eremeev, Per Kristian Lehre category:cs.NE published:2015-12-07 summary:The paper is devoted to upper bounds on run-time of Non-Elitist GeneticAlgorithms until some target subset of solutions is visited for the first time.In particular, we consider the sets of optimal solutions and the sets of localoptima as the target subsets. Previously known upper bounds are improved bymeans of drift analysis. Finally, we propose conditions ensuring that aNon-Elitist Genetic Algorithm efficiently finds approximate solutions withconstant approximation ratio on the class of combinatorial optimizationproblems with guaranteed local optima (GLO).
arxiv-14100-178 | Reinforcement Control with Hierarchical Backpropagated Adaptive Critics | http://arxiv.org/pdf/1512.02693v1.pdf | author:John W. Jameson category:cs.NE cs.LG cs.SY I.2.6 published:2015-12-08 summary:Present incremental learning methods are limited in the ability to achievereliable credit assignment over a large number time steps (or events). However,this situation is typical for cases where the dynamical system to be controlledrequires relatively frequent control updates in order to maintain stability orrobustness yet has some action-consequences which must be established overrelatively long periods of time. To address this problem, the learningcapabilities of a control architecture comprised of two Backpropagated AdaptiveCritics (BACs) in a two-level hierarchy with continuous actions are explored.The high-level BAC updates less frequently than the low-level BAC and controlsthe latter to some degree. The response of the low-level to high-level signalscan either be determined a priori or it can emerge during learning. A generalapproach called Response Induction Learning is introduced to address the lattercase.
arxiv-14100-179 | Deep Reinforcement Learning with Double Q-learning | http://arxiv.org/pdf/1509.06461v3.pdf | author:Hado van Hasselt, Arthur Guez, David Silver category:cs.LG published:2015-09-22 summary:The popular Q-learning algorithm is known to overestimate action values undercertain conditions. It was not previously known whether, in practice, suchoverestimations are common, whether they harm performance, and whether they cangenerally be prevented. In this paper, we answer all these questionsaffirmatively. In particular, we first show that the recent DQN algorithm,which combines Q-learning with a deep neural network, suffers from substantialoverestimations in some games in the Atari 2600 domain. We then show that theidea behind the Double Q-learning algorithm, which was introduced in a tabularsetting, can be generalized to work with large-scale function approximation. Wepropose a specific adaptation to the DQN algorithm and show that the resultingalgorithm not only reduces the observed overestimations, as hypothesized, butthat this also leads to much better performance on several games.
arxiv-14100-180 | Optimal Testing for Properties of Distributions | http://arxiv.org/pdf/1507.05952v3.pdf | author:Jayadev Acharya, Constantinos Daskalakis, Gautam Kamath category:cs.DS cs.IT cs.LG math.IT math.ST stat.TH published:2015-07-21 summary:Given samples from an unknown distribution $p$, is it possible to distinguishwhether $p$ belongs to some class of distributions $\mathcal{C}$ versus $p$being far from every distribution in $\mathcal{C}$? This fundamental questionhas received tremendous attention in statistics, focusing primarily onasymptotic analysis, and more recently in information theory and theoreticalcomputer science, where the emphasis has been on small sample size andcomputational complexity. Nevertheless, even for basic properties ofdistributions such as monotonicity, log-concavity, unimodality, independence,and monotone-hazard rate, the optimal sample complexity is unknown. We provide a general approach via which we obtain sample-optimal andcomputationally efficient testers for all these distribution families. At thecore of our approach is an algorithm which solves the following problem: Givensamples from an unknown distribution $p$, and a known distribution $q$, are $p$and $q$ close in $\chi^2$-distance, or far in total variation distance? The optimality of our testers is established by providing matching lowerbounds with respect to both $n$ and $\varepsilon$. Finally, a necessarybuilding block for our testers and an important byproduct of our work are thefirst known computationally efficient proper learners for discrete log-concaveand monotone hazard rate distributions.
arxiv-14100-181 | Deep Speech 2: End-to-End Speech Recognition in English and Mandarin | http://arxiv.org/pdf/1512.02595v1.pdf | author:Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni Hannun, Billy Jun, Patrick LeGresley, Libby Lin, Sharan Narang, Andrew Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh, David Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yogatama, Jun Zhan, Zhenyao Zhu category:cs.CL published:2015-12-08 summary:We show that an end-to-end deep learning approach can be used to recognizeeither English or Mandarin Chinese speech--two vastly different languages.Because it replaces entire pipelines of hand-engineered components with neuralnetworks, end-to-end learning allows us to handle a diverse variety of speechincluding noisy environments, accents and different languages. Key to ourapproach is our application of HPC techniques, resulting in a 7x speedup overour previous system. Because of this efficiency, experiments that previouslytook weeks now run in days. This enables us to iterate more quickly to identifysuperior architectures and algorithms. As a result, in several cases, oursystem is competitive with the transcription of human workers when benchmarkedon standard datasets. Finally, using a technique called Batch Dispatch withGPUs in the data center, we show that our system can be inexpensively deployedin an online setting, delivering low latency when serving users at scale.
arxiv-14100-182 | Distributed Adaptive LMF Algorithm for Sparse Parameter Estimation in Gaussian Mixture Noise | http://arxiv.org/pdf/1512.02567v1.pdf | author:Mojtaba Hajiabadi category:cs.IT cs.CL math.IT published:2015-12-08 summary:A distributed adaptive algorithm for estimation of sparse unknown parametersin the presence of nonGaussian noise is proposed in this paper based onnormalized least mean fourth (NLMF) criterion. At the first step, localadaptive NLMF algorithm is modified by zero norm in order to speed up theconvergence rate and also to reduce the steady state error power in sparseconditions. Then, the proposed algorithm is extended for distributed scenarioin which more improvement in estimation performance is achieved due tocooperation of local adaptive filters. Simulation results show the superiorityof the proposed algorithm in comparison with conventional NLMF algorithms.
arxiv-14100-183 | Selective Sequential Model Selection | http://arxiv.org/pdf/1512.02565v1.pdf | author:William Fithian, Jonathan Taylor, Robert Tibshirani, Ryan Tibshirani category:stat.ME stat.ML published:2015-12-08 summary:Many model selection algorithms produce a path of fits specifying a sequenceof increasingly complex models. Given such a sequence and the data used toproduce them, we consider the problem of choosing the least complex model thatis not falsified by the data. Extending the selected-model tests of Fithian etal. (2014), we construct p-values for each step in the path which account forthe adaptive selection of the model path using the data. In the case of linearregression, we propose two specific tests, the max-t test for forward stepwiseregression (generalizing a proposal of Buja and Brown (2014)), and thenext-entry test for the lasso. These tests improve on the power of thesaturated-model test of Tibshirani et al. (2014), sometimes dramatically. Inaddition, our framework extends beyond linear regression to a much more generalclass of parametric and nonparametric model selection problems. To select a model, we can feed our single-step p-values as inputs intosequential stopping rules such as those proposed by G'Sell et al. (2013) and Liand Barber (2015), achieving control of the familywise error rate or falsediscovery rate (FDR) as desired. The FDR-controlling rules require the nullp-values to be independent of each other and of the non-null p-values, acondition not satisfied by the saturated-model p-values of Tibshirani et al.(2014). We derive intuitive and general sufficient conditions for independence,and show that our proposed constructions yield independent p-values.
arxiv-14100-184 | Deep Learning for Single and Multi-Session i-Vector Speaker Recognition | http://arxiv.org/pdf/1512.02560v1.pdf | author:Omid Ghahabi, Javier Hernando category:cs.SD cs.LG published:2015-12-08 summary:The promising performance of Deep Learning (DL) in speech recognition hasmotivated the use of DL in other speech technology applications such as speakerrecognition. Given i-vectors as inputs, the authors proposed an impostorselection algorithm and a universal model adaptation process in a hybrid systembased on Deep Belief Networks (DBN) and Deep Neural Networks (DNN) todiscriminatively model each target speaker. In order to have more insight intothe behavior of DL techniques in both single and multi-session speakerenrollment tasks, some experiments have been carried out in this paper in bothscenarios. Additionally, the parameters of the global model, referred to asuniversal DBN (UDBN), are normalized before adaptation. UDBN normalizationfacilitates training DNNs specifically with more than one hidden layer.Experiments are performed on the NIST SRE 2006 corpus. It is shown that theproposed impostor selection algorithm and UDBN adaptation process enhance theperformance of conventional DNNs 8-20 % and 16-20 % in terms of EER for thesingle and multi-session tasks, respectively. In both scenarios, the proposedarchitectures outperform the baseline systems obtaining up to 17 % reduction inEER.
arxiv-14100-185 | Gibbs-type Indian buffet processes | http://arxiv.org/pdf/1512.02543v1.pdf | author:Creighton Heaukulani, Daniel M. Roy category:stat.ML published:2015-12-08 summary:We investigate a class of feature allocation models that generalize theIndian buffet process and are parameterized by Gibbs-type random measures. Twoexisting classes are contained as special cases: the original two-parameterIndian buffet process, corresponding to the Dirichlet process, and the stable(or three-parameter) Indian buffet process, corresponding to the Pitman-Yorprocess. Asymptotic behavior of the Gibbs-type partitions, such as power lawsholding for the number of latent clusters, translates into analogouscharacteristics for this class of Gibbs-type feature allocation models. Despitecontaining several different distinct subclasses, the properties of Gibbs-typepartitions allow us to develop a black-box procedure for posterior inferencewithin any subclass of models. Through numerical experiments, we compare andcontrast a few of these subclasses and highlight the utility of varyingpower-law behaviors in the latent features.
arxiv-14100-186 | Alternating direction method of multipliers for penalized zero-variance discriminant analysis | http://arxiv.org/pdf/1401.5492v4.pdf | author:Brendan P. W. Ames, Mingyi Hong category:stat.ML math.OC published:2014-01-21 summary:We consider the task of classification in the high dimensional setting wherethe number of features of the given data is significantly greater than thenumber of observations. To accomplish this task, we propose a heuristic, calledsparse zero-variance discriminant analysis (SZVD), for simultaneouslyperforming linear discriminant analysis and feature selection on highdimensional data. This method combines classical zero-variance discriminantanalysis, where discriminant vectors are identified in the null space of thesample within-class covariance matrix, with penalization applied to inducesparse structures in the resulting vectors. To approximately solve theresulting nonconvex problem, we develop a simple algorithm based on thealternating direction method of multipliers. Further, we show that thisalgorithm is applicable to a larger class of penalized generalized eigenvalueproblems, including a particular relaxation of the sparse principal componentanalysis problem. Finally, we establish theoretical guarantees for convergenceof our algorithm to stationary points of the original nonconvex problem, andempirically demonstrate the effectiveness of our heuristic for classifyingsimulated data and data drawn from applications in time-series classification.
arxiv-14100-187 | Approximated and User Steerable tSNE for Progressive Visual Analytics | http://arxiv.org/pdf/1512.01655v2.pdf | author:Nicola Pezzotti, Boudewijn P. F. Lelieveldt, Laurens van der Maaten, Thomas Höllt, Elmar Eisemann, Anna Vilanova category:cs.CV cs.LG published:2015-12-05 summary:Progressive Visual Analytics aims at improving the interactivity in existinganalytics techniques by means of visualization as well as interaction withintermediate results. One key method for data analysis is dimensionalityreduction, for example, to produce 2D embeddings that can be visualized andanalyzed efficiently. t-Distributed Stochastic Neighbor Embedding (tSNE) is awell-suited technique for the visualization of several high-dimensional data.tSNE can create meaningful intermediate results but suffers from a slowinitialization that constrains its application in Progressive Visual Analytics.We introduce a controllable tSNE approximation (A-tSNE), which trades off speedand accuracy, to enable interactive data exploration. We offer real-timevisualization techniques, including a density-based solution and a Magic Lensto inspect the degree of approximation. With this feedback, the user can decideon local refinements and steer the approximation level during the analysis. Wedemonstrate our technique with several datasets, in a real-world researchscenario and for the real-time analysis of high-dimensional streams toillustrate its effectiveness for interactive data analysis.
arxiv-14100-188 | Explaining NonLinear Classification Decisions with Deep Taylor Decomposition | http://arxiv.org/pdf/1512.02479v1.pdf | author:Grégoire Montavon, Sebastian Bach, Alexander Binder, Wojciech Samek, Klaus-Robert Müller category:cs.LG stat.ML published:2015-12-08 summary:Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standardfor various challenging machine learning problems, e.g., image classification,natural language processing or human action recognition. Although these methodsperform impressively well, they have a significant disadvantage, the lack oftransparency, limiting the interpretability of the solution and thus the scopeof application in practice. Especially DNNs act as black boxes due to theirmultilayer nonlinear structure. In this paper we introduce a novel methodologyfor interpreting generic multilayer neural networks by decomposing the networkclassification decision into contributions of its input elements. Although ourfocus is on image classification, the method is applicable to a broad set ofinput data, learning tasks and network architectures. Our method is based ondeep Taylor decomposition and efficiently utilizes the structure of the networkby backpropagating the explanations from the output to the input layer. Weevaluate the proposed method empirically on the MNIST and ILSVRC data sets.
arxiv-14100-189 | SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation | http://arxiv.org/pdf/1511.00561v2.pdf | author:Vijay Badrinarayanan, Alex Kendall, Roberto Cipolla category:cs.CV cs.LG cs.NE published:2015-11-02 summary:We present a novel and practical deep fully convolutional neural networkarchitecture for semantic pixel-wise segmentation termed SegNet. This coretrainable segmentation engine consists of an encoder network, a correspondingdecoder network followed by a pixel-wise classification layer. The architectureof the encoder network is topologically identical to the 13 convolutionallayers in the VGG16 network . The role of the decoder network is to map the lowresolution encoder feature maps to full input resolution feature maps forpixel-wise classification. The novelty of SegNet lies is in the manner in whichthe decoder upsamples its lower resolution input feature map(s). Specifically,the decoder uses pooling indices computed in the max-pooling step of thecorresponding encoder to perform non-linear upsampling. This eliminates theneed for learning to upsample. The upsampled maps are sparse and are thenconvolved with trainable filters to produce dense feature maps. We compare ourproposed architecture with the fully convolutional network (FCN) architectureand its variants. This comparison reveals the memory versus accuracy trade-offinvolved in achieving good segmentation performance. The design of SegNet wasprimarily motivated by road scene understanding applications. Hence, it isefficient both in terms of memory and computational time during inference. Itis also significantly smaller in the number of trainable parameters thancompeting architectures and can be trained end-to-end using stochastic gradientdescent. We also benchmark the performance of SegNet on Pascal VOC12 salientobject segmentation and the recent SUN RGB-D indoor scene understandingchallenge. We show that SegNet provides competitive performance although it issignificantly smaller than other architectures. We also provide a Caffeimplementation of SegNet and a webdemo athttp://mi.eng.cam.ac.uk/projects/segnet/
arxiv-14100-190 | Convolutional Neural Networks over Tree Structures for Programming Language Processing | http://arxiv.org/pdf/1409.5718v2.pdf | author:Lili Mou, Ge Li, Lu Zhang, Tao Wang, Zhi Jin category:cs.LG cs.NE cs.SE published:2014-09-18 summary:Programming language processing (similar to natural language processing) is ahot research topic in the field of software engineering; it has also arousedgrowing interest in the artificial intelligence community. However, differentfrom a natural language sentence, a program contains rich, explicit, andcomplicated structural information. Hence, traditional NLP models may beinappropriate for programs. In this paper, we propose a novel tree-basedconvolutional neural network (TBCNN) for programming language processing, inwhich a convolution kernel is designed over programs' abstract syntax trees tocapture structural information. TBCNN is a generic architecture for programminglanguage processing; our experiments show its effectiveness in two differentprogram analysis tasks: classifying programs according to functionality, anddetecting code snippets of certain patterns. TBCNN outperforms baselinemethods, including several neural models for NLP.
arxiv-14100-191 | A Novel Paradigm for Calculating Ramsey Number via Artificial Bee Colony Algorithm | http://arxiv.org/pdf/1512.01613v2.pdf | author:Wei-Hao Mao, Fei Gao, Yi-Jin Dong, Wen-Ming Li category:cs.AI cs.NE math.CO math.OC published:2015-12-05 summary:The Ramsey number is of vital importance in Ramsey's theorem. This paperproposed a novel methodology for constructing Ramsey graphs about R(3,10),which uses Artificial Bee Colony optimization(ABC) to raise the lower bound ofRamsey number R(3,10). The r(3,10)-graph contains two limitations, that is,neither complete graphs of order 3 nor independent sets of order 10. To resolvethese limitations, a special mathematical model is put in the paradigm toconvert the problems into discrete optimization whose smaller minimizers arecorrespondent to bigger lower bound as approximation of inf R(3,10). Todemonstrate the potential of the proposed method, simulations are done to tominimize the amount of these two types of graphs. For the first time, fourr(3,9,39) graphs with best approximation for inf R(3,10) are reported insimulations to support the current lower bound for R(3,10). The experiments'results show that the proposed paradigm for Ramsey number's calculation drivenby ABC is a successful method with the advantages of high precision androbustness.
arxiv-14100-192 | Online Gradient Descent in Function Space | http://arxiv.org/pdf/1512.02394v1.pdf | author:Changbo Zhu, Huan Xu category:cs.LG published:2015-12-08 summary:In many problems in machine learning and operations research, we need tooptimize a function whose input is a random variable or a probability densityfunction, i.e. to solve optimization problems in an in?nite dimensional space.On the other hand, online learning has the advantage of dealing with streamingexamples, and better model a changing environ- ment. In this paper, we extendthe celebrated online gradient descent algorithm to Hilbert spaces (functionspaces), and analyze the convergence guarantee of the algorithm. Finally, wedemonstrate that our algorithms can be useful in several important problems.
arxiv-14100-193 | Online Crowdsourcing | http://arxiv.org/pdf/1512.02393v1.pdf | author:Changbo Zhu, Huan Xu, Shuicheng Yan category:cs.LG published:2015-12-08 summary:With the success of modern internet based platform, such as Amazon MechanicalTurk, it is now normal to collect a large number of hand labeled samples fromnon-experts. The Dawid- Skene algorithm, which is based on Expectation-Maximization update, has been widely used for inferring the true labels fromnoisy crowdsourced labels. However, Dawid-Skene scheme requires all the data toperform each EM iteration, and can be infeasible for streaming data or largescale data. In this paper, we provide an online version of Dawid- Skenealgorithm that only requires one data frame for each iteration. Further, weprove that under mild conditions, the online Dawid-Skene scheme with projectionconverges to a stationary point of the marginal log-likelihood of the observeddata. Our experiments demonstrate that the online Dawid- Skene scheme achievesstate of the art performance comparing with other methods based on the Dawid-Skene scheme.
arxiv-14100-194 | Sentiment of Emojis | http://arxiv.org/pdf/1509.07761v2.pdf | author:Petra Kralj Novak, Jasmina Smailović, Borut Sluban, Igor Mozetič category:cs.CL published:2015-09-25 summary:There is a new generation of emoticons, called emojis, that is increasinglybeing used in mobile communications and social media. In the past two years,over ten billion emojis were used on Twitter. Emojis are Unicode graphicsymbols, used as a shorthand to express concepts and ideas. In contrast to thesmall number of well-known emoticons that carry clear emotional contents, thereare hundreds of emojis. But what are their emotional contents? We provide thefirst emoji sentiment lexicon, called the Emoji Sentiment Ranking, and draw asentiment map of the 751 most frequently used emojis. The sentiment of theemojis is computed from the sentiment of the tweets in which they occur. Weengaged 83 human annotators to label over 1.6 million tweets in 13 Europeanlanguages by the sentiment polarity (negative, neutral, or positive). About 4%of the annotated tweets contain emojis. The sentiment analysis of the emojisallows us to draw several interesting conclusions. It turns out that most ofthe emojis are positive, especially the most popular ones. The sentimentdistribution of the tweets with and without emojis is significantly different.The inter-annotator agreement on the tweets with emojis is higher. Emojis tendto occur at the end of the tweets, and their sentiment polarity increases withthe distance. We observe no significant differences in the emoji rankingsbetween the 13 languages and the Emoji Sentiment Ranking. Consequently, wepropose our Emoji Sentiment Ranking as a European language-independent resourcefor automated sentiment analysis. Finally, the paper provides a formalizationof sentiment and a novel visualization in the form of a sentiment bar.
arxiv-14100-195 | Cost Sensitive Learning of Deep Feature Representations from Imbalanced Data | http://arxiv.org/pdf/1508.03422v2.pdf | author:Salman H. Khan, Mohammed Bennamoun, Ferdous Sohel, Roberto Togneri category:cs.CV published:2015-08-14 summary:Class imbalance is a common problem in the case of real-world objectdetection and classification tasks. Data of some classes is abundant makingthem an over-represented majority, and data of other classes is scarce, makingthem an under-represented minority. This imbalance makes it challenging for aclassifier to appropriately learn the discriminating boundaries of the majorityand minority classes. In this work, we propose a cost sensitive deep neuralnetwork which can automatically learn robust feature representations for boththe majority and minority classes. During training, our learning procedurejointly optimizes the class dependent costs and the neural network parameters.The proposed approach is applicable to both binary and multi-class problemswithout any modification. Moreover, as opposed to data level approaches, we donot alter the original data distribution which results in a lower computationalcost during the training process. We report the results of our experiments onsix major image classification datasets and show that the proposed approachsignificantly outperforms the baseline algorithms. Comparisons with populardata sampling techniques and cost sensitive classifiers demonstrate thesuperior performance of our proposed method.
arxiv-14100-196 | Towards the Application of Linear Programming Methods For Multi-Camera Pose Estimation | http://arxiv.org/pdf/1512.02357v1.pdf | author:Masoud Aghamohamadian-Sharbaf, Ahmadreza Heravi, Hamidreza Pourreza category:cs.CV published:2015-12-08 summary:We presented a separation based optimization algorithm which, rather thanoptimization the entire variables altogether, This would allow us to employ: 1)a class of nonlinear functions with three variables and 2) a convex quadraticmultivariable polynomial, for minimization of reprojection error. Neglectingthe inversion required to minimize the nonlinear functions, in this paper wedemonstrate how separation allows eradication of matrix inversion.
arxiv-14100-197 | Is Hamming distance the only way for matching binary image feature descriptors? | http://arxiv.org/pdf/1512.02355v1.pdf | author:Erkan Bostanci category:cs.CV published:2015-12-08 summary:Brute force matching of binary image feature descriptors is conventionallyperformed using the Hamming distance. This paper assesses the use ofalternative metrics in order to see whether they can produce featurecorrespondences that yield more accurate homography matrices. Two statisticaltests, namely ANOVA (Analysis of Variance) and McNemar's test were employed forevaluation. Results show that Jackard-Needham and Dice metrics can displaybetter performance for some descriptors. Yet, these performance differenceswere not found to be statistically significant.
arxiv-14100-198 | Learning to Point and Count | http://arxiv.org/pdf/1512.02326v1.pdf | author:Jie Shao, Dequan Wang, Xiangyang Xue, Zheng Zhang category:cs.CV published:2015-12-08 summary:This paper proposes the problem of point-and-count as a test case to breakthe what-and-where deadlock. Different from the traditional detection problem,the goal is to discover key salient points as a way to localize and count thenumber of objects simultaneously. We propose two alternatives, one that countsfirst and then point, and another that works the other way around.Fundamentally, they pivot around whether we solve "what" or "where" first. Weevaluate their performance on dataset that contains multiple instances of thesame class, demonstrating the potentials and their synergies. The experiencesderive a few important insights that explains why this is a much harder problemthan classification, including strong data bias and the inability to deal withobject scales robustly in state-of-art convolutional neural networks.
arxiv-14100-199 | Direct Intrinsics: Learning Albedo-Shading Decomposition by Convolutional Regression | http://arxiv.org/pdf/1512.02311v1.pdf | author:Takuya Narihira, Michael Maire, Stella X. Yu category:cs.CV published:2015-12-08 summary:We introduce a new approach to intrinsic image decomposition, the task ofdecomposing a single image into albedo and shading components. Our strategy,which we term direct intrinsics, is to learn a convolutional neural network(CNN) that directly predicts output albedo and shading channels from an inputRGB image patch. Direct intrinsics is a departure from classical techniques forintrinsic image decomposition, which typically rely on physically-motivatedpriors and graph-based inference algorithms. The large-scale synthetic ground-truth of the MPI Sintel dataset plays a keyrole in training direct intrinsics. We demonstrate results on both thesynthetic images of Sintel and the real images of the classic MIT intrinsicimage dataset. On Sintel, direct intrinsics, using only RGB input, outperformsall prior work, including methods that rely on RGB+Depth input. Directintrinsics also generalizes across modalities; it produces quite reasonabledecompositions on the real images of the MIT dataset. Our results indicate thatthe marriage of CNNs with synthetic training data may be a powerful newtechnique for tackling classic problems in computer vision.
arxiv-14100-200 | Nonparametric Reduced-Rank Regression for Multi-SNP, Multi-Trait Association Mapping | http://arxiv.org/pdf/1512.02306v1.pdf | author:Ashlee Valente, Geoffrey Ginsburg, Barbara E Engelhardt category:stat.AP q-bio.GN stat.ML published:2015-12-08 summary:Genome-wide association studies have proven to be essential for understandingthe genetic basis of disease. However, many complex traits---personalitytraits, facial features, disease subtyping---are inherently high-dimensional,impeding simple approaches to association mapping. We developed a nonparametricBayesian reduced rank regression model for multi-SNP, multi-trait associationmapping that does not require the rank of the linear subspace to be specified.We show in simulations and real data that our model shares strength over SNPsand over correlated traits, improving statistical power to identify geneticassociations with an interpretable, SNP-supervised low-dimensional linearprojection of the high-dimensional phenotype. On the HapMap phase 3 geneexpression QTL study data, we identify pleiotropic expression QTLs thatclassical univariate tests are underpowered to find and that two stepapproaches cannot recover. Our Python software, BERRRI, is publicly availableat GitHub: https://github.com/ashlee1031/BERRRI.
arxiv-14100-201 | Optimal strategies for the control of autonomous vehicles in data assimilation | http://arxiv.org/pdf/1512.02271v1.pdf | author:Damon McDougall, Richard Moore category:math.OC stat.CO stat.ML published:2015-12-07 summary:We propose a method to compute optimal control paths for autonomous vehiclesdeployed for the purpose of inferring a velocity field. In addition to beingadvected by the flow, the vehicles are able to effect a fixed relative speedwith arbitrary control over direction. It is this direction that is used as thebasis for the locally optimal control algorithm presented here, with objectiveformed from the variance trace of the expected posterior distribution. Wepresent results for linear flows near hyperbolic fixed points.
arxiv-14100-202 | Learning FRAME Models Using CNN Filters | http://arxiv.org/pdf/1509.08379v3.pdf | author:Yang Lu, Song-Chun Zhu, Ying Nian Wu category:cs.CV published:2015-09-28 summary:The convolutional neural network (ConvNet or CNN) has proven to be verysuccessful in many tasks such as those in computer vision. In this conceptualpaper, we study the generative perspective of the discriminative CNN. Inparticular, we propose to learn the generative FRAME (Filters, Random field,And Maximum Entropy) model using the highly expressive filters pre-learned bythe CNN at the convolutional layers. We show that the learning algorithm cangenerate realistic and rich object and texture patterns in natural scenes. Weexplain that each learned model corresponds to a new CNN unit at a layer abovethe layer of filters employed by the model. We further show that it is possibleto learn a new layer of CNN units using a generative CNN model, which is aproduct of experts model, and the learning algorithm admits an EMinterpretation with binary latent variables.
arxiv-14100-203 | New Design Criteria for Robust PCA and a Compliant Bayesian-Inspired Algorithm | http://arxiv.org/pdf/1512.02188v1.pdf | author:Tae-Hyun Oh, David Wipf, Yasuyuki Matsushita, In So Kweon category:cs.CV cs.LG stat.ML published:2015-12-07 summary:Commonly used in computer vision and other applications, robust PCArepresents an algorithmic attempt to reduce the sensitivity of classical PCA tooutliers. The basic idea is to learn a decomposition of some data matrix ofinterest into low rank and sparse components, the latter representing unwantedoutliers. Although the resulting optimization problem is typically NP-hard,convex relaxations provide a computationally-expedient alternative withtheoretical support. However, in practical regimes performance guarantees breakdown and a variety of non-convex alternatives, including Bayesian-inspiredmodels, have been proposed to boost estimation quality. Unfortunately though,without additional a priori knowledge none of these methods can significantlyexpand the critical operational range such that exact principal subspacerecovery is possible. Into this mix we propose a novel pseudo-Bayesianalgorithm that explicitly compensates for design weaknesses in many existingnon-convex approaches leading to state-of-the-art performance with a soundanalytical foundation.
arxiv-14100-204 | The Teaching Dimension of Linear Learners | http://arxiv.org/pdf/1512.02181v1.pdf | author:Ji Liu, Xiaojin Zhu category:cs.LG published:2015-12-07 summary:Teaching dimension is a learning theoretic quantity that specifies theminimum training set size to teach a target model to a learner. Previousstudies on teaching dimension focused on version-space learners which maintainall hypotheses consistent with the training data, and cannot be applied tomodern machine learners which select a specific hypothesis via optimization.This paper presents the first known teaching dimension for ridge regression,support vector machines, and logistic regression. We also exhibit optimaltraining sets that match these teaching dimensions. Our approach generalizes toother linear learners.
arxiv-14100-205 | Color Constancy by Learning to Predict Chromaticity from Luminance | http://arxiv.org/pdf/1506.02167v2.pdf | author:Ayan Chakrabarti category:cs.CV published:2015-06-06 summary:Color constancy is the recovery of true surface color from observed color,and requires estimating the chromaticity of scene illumination to correct forthe bias it induces. In this paper, we show that the per-pixel color statisticsof natural scenes---without any spatial or semantic context---can by themselvesbe a powerful cue for color constancy. Specifically, we describe an illuminantestimation method that is built around a "classifier" for identifying the truechromaticity of a pixel given its luminance (absolute brightness across colorchannels). During inference, each pixel's observed color restricts its truechromaticity to those values that can be explained by one of a candidate set ofilluminants, and applying the classifier over these values yields adistribution over the corresponding illuminants. A global estimate for thescene illuminant is computed through a simple aggregation of thesedistributions across all pixels. We begin by simply defining theluminance-to-chromaticity classifier by computing empirical histograms overdiscretized chromaticity and luminance values from a training set of naturalimages. These histograms reflect a preference for hues corresponding to smoothreflectance functions, and for achromatic colors in brighter pixels. Despiteits simplicity, the resulting estimation algorithm outperforms currentstate-of-the-art color constancy methods. Next, we propose a method to learnthe luminance-to-chromaticity classifier "end-to-end". Using stochasticgradient descent, we set chromaticity-luminance likelihoods to minimize errorsin the final scene illuminant estimates on a training set. This leads tofurther improvements in accuracy, most significantly in the tail of the errordistribution.
arxiv-14100-206 | Fast ConvNets Using Group-wise Brain Damage | http://arxiv.org/pdf/1506.02515v2.pdf | author:Vadim Lebedev, Victor Lempitsky category:cs.CV published:2015-06-08 summary:We revisit the idea of brain damage, i.e. the pruning of the coefficients ofa neural network, and suggest how brain damage can be modified and used tospeedup convolutional layers. The approach uses the fact that many efficientimplementations reduce generalized convolutions to matrix multiplications. Thesuggested brain damage process prunes the convolutional kernel tensor in agroup-wise fashion by adding group-sparsity regularization to the standardtraining process. After such group-wise pruning, convolutions can be reduced tomultiplications of thinned dense matrices, which leads to speedup. In thecomparison on AlexNet, the method achieves very competitive performance.
arxiv-14100-207 | Holographic Embeddings of Knowledge Graphs | http://arxiv.org/pdf/1510.04935v2.pdf | author:Maximilian Nickel, Lorenzo Rosasco, Tomaso Poggio category:cs.AI cs.LG stat.ML I.2.6; I.2.4 published:2015-10-16 summary:Learning embeddings of entities and relations is an efficient and versatilemethod to perform machine learning on relational data such as knowledge graphs.In this work, we propose holographic embeddings (HolE) to learn compositionalvector space representations of entire knowledge graphs. The proposed method isrelated to holographic models of associative memory in that it employs circularcorrelation to create compositional representations. By using correlation asthe compositional operator HolE can capture rich interactions butsimultaneously remains efficient to compute, easy to train, and scalable tovery large datasets. In extensive experiments we show that holographicembeddings are able to outperform state-of-the-art methods for link predictionin knowledge graphs and relational learning benchmark datasets.
arxiv-14100-208 | A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation | http://arxiv.org/pdf/1512.02134v1.pdf | author:Nikolaus Mayer, Eddy Ilg, Philip Häusser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, Thomas Brox category:cs.CV cs.LG stat.ML published:2015-12-07 summary:Recent work has shown that optical flow estimation can be formulated as asupervised learning task and can be successfully solved with convolutionalnetworks. Training of the so-called FlowNet was enabled by a largesynthetically generated dataset. The present paper extends the concept ofoptical flow estimation via convolutional networks to disparity and scene flowestimation. To this end, we propose three synthetic stereo video datasets withsufficient realism, variation, and size to successfully train large networks.Our datasets are the first large-scale datasets to enable training andevaluating scene flow methods. Besides the datasets, we present a convolutionalnetwork for real-time disparity estimation that provides state-of-the-artresults. By combining a flow and disparity estimation network and training itjointly, we demonstrate the first scene flow estimation with a convolutionalnetwork.
arxiv-14100-209 | In-situ multi-scattering tomography | http://arxiv.org/pdf/1512.02110v1.pdf | author:Vadim Holodovsky, Yoav Y. Schechner, Anat Levin, Aviad Levis, Amit Aides category:cs.CV published:2015-12-07 summary:To recover the three dimensional (3D) volumetric distribution of matter in anobject, images of the object are captured from multiple directions andlocations. Using these images tomographic computations extract thedistribution. In highly scattering media and constrained, natural irradiance,tomography must explicitly account for off-axis scattering. Furthermore, thetomographic model and recovery must function when imaging is done in-situ, asoccurs in medical imaging and ground-based atmospheric sensing. We formulatetomography that handles arbitrary orders of scattering, using a monte-carlomodel. Moreover, the model is highly parallelizable in our formulation. Thisenables large scale rendering and recovery of volumetric scenes having a largenumber of variables. We solve stability and conditioning problems that stemfrom radiative transfer (RT) modeling in-situ.
arxiv-14100-210 | Digital Genesis: Computers, Evolution and Artificial Life | http://arxiv.org/pdf/1512.02100v1.pdf | author:Tim Taylor, Alan Dorin, Kevin Korb category:cs.NE published:2015-12-07 summary:The application of evolution in the digital realm, with the goal of creatingartificial intelligence and artificial life, has a history as long as that ofthe digital computer itself. We illustrate the intertwined history of theseideas, starting with the early theoretical work of John von Neumann and thepioneering experimental work of Nils Aall Barricelli. We argue thatevolutionary thinking and artificial life will continue to play an integralrole in the future development of the digital world.
arxiv-14100-211 | Time-causal and time-recursive spatio-temporal receptive fields | http://arxiv.org/pdf/1504.02648v2.pdf | author:Tony Lindeberg category:cs.CV q-bio.NC published:2015-04-10 summary:We present an improved model and theory for time-causal and time-recursivespatio-temporal receptive fields, based on a combination of Gaussian receptivefields over the spatial domain and first-order integrators or equivalentlytruncated exponential filters coupled in cascade over the temporal domain. Compared to previous spatio-temporal scale-space formulations in terms ofnon-enhancement of local extrema or scale invariance, these receptive fieldsare based on different scale-space axiomatics over time by ensuringnon-creation of new local extrema or zero-crossings with increasing temporalscale. Specifically, extensions are presented about (i) parameterizing theintermediate temporal scale levels, (ii) analysing the resulting temporaldynamics, (iii) transferring the theory to a discrete implementation, (iv)computing scale-normalized spatio-temporal derivative expressions forspatio-temporal feature detection and (v) computational modelling of receptivefields in the lateral geniculate nucleus (LGN) and the primary visual cortex(V1) in biological vision. We show that by distributing the intermediate temporal scale levels accordingto a logarithmic distribution, we obtain much faster temporal responseproperties (shorter temporal delays) compared to a uniform distribution.Specifically, these kernels converge very rapidly to a limit kernel possessingtrue self-similar scale-invariant properties over temporal scales, therebyallowing for true scale invariance over variations in the temporal scale,although the underlying temporal scale-space representation is based on adiscretized temporal scale parameter. We show how scale-normalized temporal derivatives can be defined for thesetime-causal scale-space kernels and how the composed theory can be used forcomputing basic types of scale-normalized spatio-temporal derivativeexpressions in a computationally efficient manner.
arxiv-14100-212 | Clustering by Deep Nearest Neighbor Descent (D-NND): A Density-based Parameter-Insensitive Clustering Method | http://arxiv.org/pdf/1512.02097v1.pdf | author:Teng Qiu, Yongjie Li category:stat.ML cs.CV cs.LG stat.CO stat.ME published:2015-12-07 summary:Most density-based clustering methods largely rely on how well the underlyingdensity is estimated. However, density estimation itself is also a challengingproblem, especially the determination of the kernel bandwidth. A largebandwidth could lead to the over-smoothed density estimation in which thenumber of density peaks could be less than the true clusters, while a smallbandwidth could lead to the under-smoothed density estimation in which spuriousdensity peaks, or called the "ripple noise", would be generated in theestimated density. In this paper, we propose a density-based hierarchicalclustering method, called the Deep Nearest Neighbor Descent (D-NND), whichcould learn the underlying density structure layer by layer and capture thecluster structure at the same time. The over-smoothed density estimation couldbe largely avoided and the negative effect of the under-estimated cases couldbe also largely reduced. Overall, D-NND presents not only the strong capabilityof discovering the underlying cluster structure but also the remarkablereliability due to its insensitivity to parameters.
arxiv-14100-213 | On The Continuous Steering of the Scale of Tight Wavelet Frames | http://arxiv.org/pdf/1512.02072v1.pdf | author:Zsuzsanna Püspöki, John Paul Ward, Daniel Sage, Michael Unser category:cs.CV published:2015-12-07 summary:In analogy with steerable wavelets, we present a general construction ofadaptable tight wavelet frames, with an emphasis on scaling operations. Inparticular, the derived wavelets can be "dilated" by a procedure comparable tothe operation of steering steerable wavelets. The fundamental aspects of theconstruction are the same: an admissible collection of Fourier multipliers isused to extend a tight wavelet frame, and the "scale" of the wavelets isadapted by scaling the multipliers. As an application, the proposed waveletscan be used to improve the frequency localization. Importantly, the localizedfrequency bands specified by this construction can be scaled efficiently usingmatrix multiplication.
arxiv-14100-214 | Piecewise Linear Activation Functions For More Efficient Deep Networks | http://arxiv.org/pdf/1511.03650v3.pdf | author:Cheng-Yang Fu, Alexander C. Berg category:cs.CV published:2015-11-11 summary:This submission has been withdrawn by arXiv administrators because it isintentionally incomplete, which is in violation of our policies.
arxiv-14100-215 | A Parallel Framework for Parametric Maximum Flow Problems in Image Segmentation | http://arxiv.org/pdf/1509.06004v2.pdf | author:Vlad Olaru, Mihai Florea, Cristian Sminchisescu category:cs.CV published:2015-09-20 summary:This paper presents a framework that supports the implementation of parallelsolutions for the widespread parametric maximum flow computational routinesused in image segmentation algorithms. The framework is based on supergraphs, aspecial construction combining several image graphs into a larger one, andworks on various architectures (multi-core or GPU), either locally or remotelyin a cluster of computing nodes. The framework can also be used for performanceevaluation of parallel implementations of maximum flow algorithms. We presentthe case study of a state-of-the-art image segmentation algorithm based ongraph cuts, Constrained Parametric Min-Cut (CPMC), that uses the parallelframework to solve parametric maximum flow problems, based on a GPUimplementation of the well-known push-relabel algorithm. Our results indicatethat real-time implementations based on the proposed techniques are possible.
arxiv-14100-216 | Fast Online EM for Big Topic Modeling | http://arxiv.org/pdf/1210.2179v3.pdf | author:Jia Zeng, Zhi-Qiang Liu, Xiao-Qin Cao category:cs.LG published:2012-10-08 summary:The expectation-maximization (EM) algorithm can compute themaximum-likelihood (ML) or maximum a posterior (MAP) point estimate of themixture models or latent variable models such as latent Dirichlet allocation(LDA), which has been one of the most popular probabilistic topic modelingmethods in the past decade. However, batch EM has high time and spacecomplexities to learn big LDA models from big data streams. In this paper, wepresent a fast online EM (FOEM) algorithm that infers the topic distributionfrom the previously unseen documents incrementally with constant memoryrequirements. Within the stochastic approximation framework, we show that FOEMcan converge to the local stationary point of the LDA's likelihood function. Bydynamic scheduling for the fast speed and parameter streaming for the lowmemory usage, FOEM is more efficient for some lifelong topic modeling tasksthan the state-of-the-art online LDA algorithms to handle both big data and bigmodels (aka, big topic modeling) on just a PC.
arxiv-14100-217 | Discriminative Nonparametric Latent Feature Relational Models with Data Augmentation | http://arxiv.org/pdf/1512.02016v1.pdf | author:Bei Chen, Ning Chen, Jun Zhu, Jiaming Song, Bo Zhang category:cs.LG stat.ML published:2015-12-07 summary:We present a discriminative nonparametric latent feature relational model(LFRM) for link prediction to automatically infer the dimensionality of latentfeatures. Under the generic RegBayes (regularized Bayesian inference)framework, we handily incorporate the prediction loss with probabilisticinference of a Bayesian model; set distinct regularization parameters fordifferent types of links to handle the imbalance issue in real networks; andunify the analysis of both the smooth logistic log-loss and the piecewiselinear hinge loss. For the nonconjugate posterior inference, we present asimple Gibbs sampler via data augmentation, without making restrictingassumptions as done in variational methods. We further develop an approximatesampler using stochastic gradient Langevin dynamics to handle large networkswith hundreds of thousands of entities and millions of links, orders ofmagnitude larger than what existing LFRM models can process. Extensive studieson various real networks show promising performance.
arxiv-14100-218 | Scalable domain adaptation of convolutional neural networks | http://arxiv.org/pdf/1512.02013v1.pdf | author:Adrian Popescu, Etienne Gadeski, Hervé Le Borgne category:cs.CV published:2015-12-07 summary:Convolutional neural networks (CNNs) tend to become a standard approach tosolve a wide array of computer vision problems. Besides important theoreticaland practical advances in their design, their success is built on the existenceof manually labeled visual resources, such as ImageNet. The creation of suchdatasets is cumbersome and here we focus on alternatives to manual labeling. Wehypothesize that new resources are of uttermost importance in domains which arenot or weakly covered by ImageNet, such as tourism photographs. We firstcollect noisy Flickr images for tourist points of interest and apply automaticor weakly-supervised reranking techniques to reduce noise. Then, we learndomain adapted models with a standard CNN architecture and compare them to ageneric model obtained from ImageNet. Experimental validation is conducted withpublicly available datasets, including Oxford5k, INRIA Holidays and Div150Cred.Results show that low-cost domain adaptation improves results compared to theuse of generic models but also compared to strong non-CNN baselines such astriangulation embedding.
arxiv-14100-219 | Jointly Modeling Topics and Intents with Global Order Structure | http://arxiv.org/pdf/1512.02009v1.pdf | author:Bei Chen, Jun Zhu, Nan Yang, Tian Tian, Ming Zhou, Bo Zhang category:cs.CL cs.IR cs.LG published:2015-12-07 summary:Modeling document structure is of great importance for discourse analysis andrelated applications. The goal of this research is to capture the documentintent structure by modeling documents as a mixture of topic words andrhetorical words. While the topics are relatively unchanged through onedocument, the rhetorical functions of sentences usually change followingcertain orders in discourse. We propose GMM-LDA, a topic modeling basedBayesian unsupervised model, to analyze the document intent structurecooperated with order information. Our model is flexible that has the abilityto combine the annotations and do supervised learning. Additionally, entropicregularization can be introduced to model the significant divergence betweentopics and intents. We perform experiments in both unsupervised and supervisedsettings, results show the superiority of our model over severalstate-of-the-art baselines.
arxiv-14100-220 | A Novel Approach to Distributed Multi-Class SVM | http://arxiv.org/pdf/1512.01993v1.pdf | author:Aruna Govada, Shree Ranjani, Aditi Viswanathan, S. K. Sahay category:cs.LG cs.DC published:2015-12-07 summary:With data sizes constantly expanding, and with classical machine learningalgorithms that analyze such data requiring larger and larger amounts ofcomputation time and storage space, the need to distribute computation andmemory requirements among several computers has become apparent. Althoughsubstantial work has been done in developing distributed binary SVM algorithmsand multi-class SVM algorithms individually, the field of multi-classdistributed SVMs remains largely unexplored. This research proposes a novelalgorithm that implements the Support Vector Machine over a multi-class datasetand is efficient in a distributed environment (here, Hadoop). The idea is todivide the dataset into half recursively and thus compute the optimal SupportVector Machine for this half during the training phase, much like a divide andconquer approach. While testing, this structure has been effectively exploitedto significantly reduce the prediction time. Our algorithm has shown bettercomputation time during the prediction phase than the traditional sequentialSVM methods (One vs. One, One vs. Rest) and out-performs them as the size ofthe dataset grows. This approach also classifies the data with higher accuracythan the traditional multi-class algorithms.
arxiv-14100-221 | Hyperspectral Chemical Plume Detection Algorithms Based On Multidimensional Iterative Filtering Decomposition | http://arxiv.org/pdf/1512.01979v1.pdf | author:Antonio Cicone, Jingfang Liu, Haomin Zhou category:math.NA cs.CV published:2015-12-07 summary:Chemicals released in the air can be extremely dangerous for human beings andthe environment. Hyperspectral images can be used to identify chemical plumes,however the task can be extremely challenging. Assuming we know a priori thatsome chemical plume, with a known frequency spectrum, has been photographedusing a hyperspectral sensor, we can use standard techniques like the so calledmatched filter or adaptive cosine estimator, plus a properly chosen thresholdvalue, to identify the position of the chemical plume. However, due to noiseand sensors fault, the accurate identification of chemical pixels is not easyeven in this apparently simple situation. In this paper we present apost-processing tool that, in a completely adaptive and data driven fashion,allows to improve the performance of any classification methods in identifyingthe boundaries of a plume. This is done using the Multidimensional IterativeFiltering (MIF) algorithm (arXiv:1411.6051, arXiv:1507.07173), which is anon-stationary signal decomposition method like the pioneering Empirical ModeDecomposition (EMD) method. Moreover, based on the MIF technique, we proposealso a pre-processing method that allows to decorrelate and mean-center ahyperspectral dataset. The Cosine Similarity measure, which often fails inpractice, appears to become a successful and outperforming classifier whenequipped with such pre-processing method. We show some examples of the proposedmethods when applied to real life problems.
arxiv-14100-222 | Light-field Microscopy with a Consumer Light-field Camera | http://arxiv.org/pdf/1508.03590v2.pdf | author:Lois Mignard-Debise, Ivo Ihrke category:cs.GR cs.CV published:2015-05-04 summary:We explore the use of inexpensive consumer light- field camera technology forthe purpose of light-field mi- croscopy. Our experiments are based on the Lytro(first gen- eration) camera. Unfortunately, the optical systems of the Lytroand those of microscopes are not compatible, lead- ing to a loss of light-fieldinformation due to angular and spatial vignetting when directly recordingmicroscopic pic- tures. We therefore consider an adaptation of the Lytro op-tical system. We demonstrate that using the Lytro directly as an oc- ularreplacement, leads to unacceptable spatial vignetting. However, we also found asetting that allows the use of the Lytro camera in a virtual imaging mode whichprevents the information loss to a large extent. We analyze the new vir- tualimaging mode and use it in two different setups for im- plementing light-fieldmicroscopy using a Lytro camera. As a practical result, we show that the cameracan be used for low magnification work, as e.g. common in quality control,surface characterization, etc. We achieve a maximum spa- tial resolution ofabout 6.25{\mu}m, albeit at a limited SNR for the side views.
arxiv-14100-223 | Learning population and subject-specific brain connectivity networks via Mixed Neighborhood Selection | http://arxiv.org/pdf/1512.01947v1.pdf | author:Ricardo Pio Monti, Christoforos Anagnostopoulos, Giovanni Montana category:stat.ML published:2015-12-07 summary:In neuroimaging data analysis, Gaussian graphical models are often used tomodel statistical dependencies across spatially remote brain regions known asfunctional connectivity. Typically, data is collected across a cohort ofsubjects and the scientific objectives consist of estimating population andsubject-specific graphical models. A third objective that is often overlookedinvolves quantifying inter-subject variability and thus identifying regions orsub-networks that demonstrate heterogeneity across subjects. Such informationis fundamental in order to thoroughly understand the human connectome. Wepropose Mixed Neighborhood Selection in order to simultaneously address thethree aforementioned objectives. By recasting covariance selection as aneighborhood selection problem we are able to efficiently learn the topology ofeach node. We introduce an additional mixed effect component to neighborhoodselection in order to simultaneously estimate a graphical model for thepopulation of subjects as well as for each individual subject. The proposedmethod is validated empirically through a series of simulations and applied toresting state data for healthy subjects taken from the ABIDE consortium.
arxiv-14100-224 | Fast Optimization Algorithm on Riemannian Manifolds and Its Application in Low-Rank Representation | http://arxiv.org/pdf/1512.01927v1.pdf | author:Haoran Chen, Yanfeng Sun, Junbin Gao, Yongli Hu category:cs.NA cs.CV cs.LG published:2015-12-07 summary:The paper addresses the problem of optimizing a class of composite functionson Riemannian manifolds and a new first order optimization algorithm (FOA) witha fast convergence rate is proposed. Through the theoretical analysis for FOA,it has been proved that the algorithm has quadratic convergence. Theexperiments in the matrix completion task show that FOA has better performancethan other first order optimization methods on Riemannian manifolds. A fastsubspace pursuit method based on FOA is proposed to solve the low-rankrepresentation model based on augmented Lagrange method on the low rank matrixvariety. Experimental results on synthetic and real data sets are presented todemonstrate that both FOA and SP-RPRG(ALM) can achieve superior performance interms of faster convergence and higher accuracy.
arxiv-14100-225 | Thinking Required | http://arxiv.org/pdf/1512.01926v1.pdf | author:Kamil Rocki category:cs.LG cs.AI cs.CL published:2015-12-07 summary:There exists a theory of a single general-purpose learning algorithm whichcould explain the principles its operation. It assumes the initial rougharchitecture, a small library of simple innate circuits which are prewired atbirth. and proposes that all significant mental algorithms are learned. Givencurrent understanding and observations, this paper reviews and lists theingredients of such an algorithm from architectural and functionalperspectives.
arxiv-14100-226 | Rademacher Complexity of the Restricted Boltzmann Machine | http://arxiv.org/pdf/1512.01914v1.pdf | author:Xiao Zhang category:cs.LG published:2015-12-07 summary:Boltzmann machine, as a fundamental construction block of deep belief networkand deep Boltzmann machines, is widely used in deep learning community andgreat success has been achieved. However, theoretical understanding of manyaspects of it is still far from clear. In this paper, we studied the Rademachercomplexity of both the asymptotic restricted Boltzmann machine and thepractical implementation with single-step contrastive divergence (CD-1)procedure. Our results disclose the fact that practical implementation trainingprocedure indeed increased the Rademacher complexity of restricted Boltzmannmachines. A further research direction might be the investigation of the VCdimension of a compositional function used in the CD-1 procedure.
arxiv-14100-227 | Bounds on bilinear inverse forms via Gaussian quadrature with applications | http://arxiv.org/pdf/1512.01904v1.pdf | author:Chengtao Li, Suvrit Sra, Stefanie Jegelka category:stat.ML cs.NA published:2015-12-07 summary:We address quadrature-based approximations of the bilinear inverse form$u^\top A^{-1} u$, where $A$ is a real symmetric positive definite matrix, andanalyze properties of the Gauss, Gauss-Radau, and Gauss-Lobatto quadrature. Inparticular, we establish monotonicity of the bounds given by these quadraturerules, compare the tightness of these bounds, and derive associated convergencerates. To our knowledge, this is the first work to establish these propertiesof Gauss-type quadrature for computing bilinear inverse forms, thus filling atheoretical gap regarding this classical topic. We illustrate the empiricalbenefits of our theoretical results by applying quadrature to speed up twoMarkov Chain sampling procedures for (discrete) determinantal point processes.
arxiv-14100-228 | Sparsifying Neural Network Connections for Face Recognition | http://arxiv.org/pdf/1512.01891v1.pdf | author:Yi Sun, Xiaogang Wang, Xiaoou Tang category:cs.CV published:2015-12-07 summary:This paper proposes to learn high-performance deep ConvNets with sparseneural connections, referred to as sparse ConvNets, for face recognition. Thesparse ConvNets are learned in an iterative way, each time one additional layeris sparsified and the entire model is re-trained given the initial weightslearned in previous iterations. One important finding is that directly trainingthe sparse ConvNet from scratch failed to find good solutions for facerecognition, while using a previously learned denser model to properlyinitialize a sparser model is critical to continue learning effective featuresfor face recognition. This paper also proposes a new neural correlation-basedweight selection criterion and empirically verifies its effectiveness inselecting informative connections from previously learned models in eachiteration. When taking a moderately sparse structure (26%-76% of weights in thedense model), the proposed sparse ConvNet model significantly improves the facerecognition performance of the previous state-of-the-art DeepID2+ models giventhe same training data, while it keeps the performance of the baseline modelwith only 12% of the original parameters.
arxiv-14100-229 | Using SVM to pre-classify government purchases | http://arxiv.org/pdf/1601.02680v1.pdf | author:Thiago Marzagão category:cs.LG published:2015-12-07 summary:The Brazilian government often misclassifies the goods it buys. That makes ithard to audit government expenditures. We cannot know whether the price paidfor a ballpoint pen (code #7510) was reasonable if the pen was misclassified asa technical drawing pen (code #6675) or as any other good. This paper shows howwe can use machine learning to reduce misclassification. I trained a supportvector machine (SVM) classifier that takes a product description as input andreturns the most likely category codes as output. I trained the classifierusing 20 million goods purchased by the Brazilian government between 1999-04-01and 2015-04-02. In 83.3% of the cases the correct category code was one of thethree most likely category codes identified by the classifier. I used thetrained classifier to develop a web app that might help the government reducemisclassification. I open sourced the code on GitHub; anyone can use and modifyit.
arxiv-14100-230 | Fixation prediction with a combined model of bottom-up saliency and vanishing point | http://arxiv.org/pdf/1512.01858v1.pdf | author:Mengyang Feng, Ali Borji, Huchuan Lu category:cs.CV published:2015-12-06 summary:By predicting where humans look in natural scenes, we can understand how theyperceive complex natural scenes and prioritize information for furtherhigh-level visual processing. Several models have been proposed for thispurpose, yet there is a gap between best existing saliency models and humanperformance. While many researchers have developed purely computational modelsfor fixation prediction, less attempts have been made to discover cognitivefactors that guide gaze. Here, we study the effect of a particular type ofscene structural information, known as the vanishing point, and show that humangaze is attracted to the vanishing point regions. We record eye movements of 10observers over 532 images, out of which 319 have vanishing points. We thenconstruct a combined model of traditional saliency and a vanishing pointchannel and show that our model outperforms state of the art saliency modelsusing three scores on our dataset.
arxiv-14100-231 | Explaining reviews and ratings with PACO: Poisson Additive Co-Clustering | http://arxiv.org/pdf/1512.01845v1.pdf | author:Chao-Yuan Wu, Alex Beutel, Amr Ahmed, Alexander J. Smola category:cs.LG stat.ML published:2015-12-06 summary:Understanding a user's motivations provides valuable information beyond theability to recommend items. Quite often this can be accomplished by perusingboth ratings and review texts, since it is the latter where the reasoning forspecific preferences is explicitly expressed. Unfortunately matrix factorization approaches to recommendation result inlarge, complex models that are difficult to interpret and give recommendationsthat are hard to clearly explain to users. In contrast, in this paper, weattack this problem through succinct additive co-clustering. We devise a novelBayesian technique for summing co-clusterings of Poisson distributions. Withthis novel technique we propose a new Bayesian model for joint collaborativefiltering of ratings and text reviews through a sum of simple co-clusterings.The simple structure of our model yields easily interpretable recommendations.Even with a simple, succinct structure, our model outperforms competitors interms of predicting ratings with reviews.
arxiv-14100-232 | Frank-Wolfe Bayesian Quadrature: Probabilistic Integration with Theoretical Guarantees | http://arxiv.org/pdf/1506.02681v3.pdf | author:François-Xavier Briol, Chris J. Oates, Mark Girolami, Michael A. Osborne category:stat.ML published:2015-06-08 summary:There is renewed interest in formulating integration as an inference problem,motivated by obtaining a full distribution over numerical error that can bepropagated through subsequent computation. Current methods, such as BayesianQuadrature, demonstrate impressive empirical performance but lack theoreticalanalysis. An important challenge is to reconcile these probabilisticintegrators with rigorous convergence guarantees. In this paper, we present thefirst probabilistic integrator that admits such theoretical treatment, calledFrank-Wolfe Bayesian Quadrature (FWBQ). Under FWBQ, convergence to the truevalue of the integral is shown to be exponential and posterior contractionrates are proven to be superexponential. In simulations, FWBQ is competitivewith state-of-the-art methods and out-performs alternatives based onFrank-Wolfe optimisation. Our approach is applied to successfully quantifynumerical error in the solution to a challenging model choice problem incellular biology.
arxiv-14100-233 | Classification of Manifolds by Single-Layer Neural Networks | http://arxiv.org/pdf/1512.01834v1.pdf | author:SueYeon Chung, Daniel D. Lee, Haim Sompolinsky category:cs.NE q-bio.NC stat.ML published:2015-12-06 summary:The neuronal representation of objects exhibit enormous variability due tochanges in the object's physical features such as location, size, orientation,and intensity. How the brain copes with the variability across these manifoldsof neuronal states and generates invariant perception of objects remains poorlyunderstood. Here we present a theory of neuronal classification of manifolds,extending Gardner's replica theory of classification of isolated points by asingle layer perceptron. We evaluate how the perceptron capacity depends on thedimensionality, size and shape of the classified manifolds.
arxiv-14100-234 | Information Exchange and Learning Dynamics over Weakly-Connected Adaptive Networks | http://arxiv.org/pdf/1412.1523v2.pdf | author:Bicheng Ying, Ali H. Sayed category:cs.MA cs.IT cs.LG math.IT published:2014-12-04 summary:The paper examines the learning mechanism of adaptive agents overweakly-connected graphs and reveals an interesting behavior on how informationflows through such topologies. The results clarify how asymmetries in theexchange of data can mask local information at certain agents and make themtotally dependent on other agents. A leader-follower relationship develops withthe performance of some agents being fully determined by the performance ofother agents that are outside their domain of influence. This scenario canarise, for example, due to intruder attacks by malicious agents or as theresult of failures by some critical links. The findings in this work helpexplain why strong-connectivity of the network topology, adaptation of thecombination weights, and clustering of agents are important ingredients toequalize the learning abilities of all agents against such disturbances. Theresults also clarify how weak-connectivity can be helpful in reducing theeffect of outlier data on learning performance.
arxiv-14100-235 | On The Direct Maximization of Quadratic Weighted Kappa | http://arxiv.org/pdf/1509.07107v3.pdf | author:David Vaughn, Derek Justice category:cs.LG published:2015-09-23 summary:In recent years, quadratic weighted kappa has been growing in popularity inthe machine learning community as an evaluation metric in domains where thetarget labels to be predicted are drawn from integer ratings, usually obtainedfrom human experts. For example, it was the metric of choice in several recent,high profile machine learning contests hosted on Kaggle :https://www.kaggle.com/c/asap-aes , https://www.kaggle.com/c/asap-sas ,https://www.kaggle.com/c/diabetic-retinopathy-detection . Yet, little isunderstood about the nature of this metric, its underlying mathematicalproperties, where it fits among other common evaluation metrics such as meansquared error (MSE) and correlation, or if it can be optimized analytically,and if so, how. Much of this is due to the cumbersome way that this metric iscommonly defined. In this paper we first derive an equivalent but much simpler,and more useful, definition for quadratic weighted kappa, and then employ thisalternate form to address the above issues.
arxiv-14100-236 | The Next Best Underwater View | http://arxiv.org/pdf/1512.01789v1.pdf | author:Mark Sheinin, Yoav Y. Schechner category:cs.CV published:2015-12-06 summary:To image in high resolution large and occlusion-prone scenes, a camera mustmove above and around. Degradation of visibility due to geometric occlusionsand distances is exacerbated by scattering, when the scene is in aparticipating medium. Moreover, underwater and in other media, artificiallighting is needed. Overall, data quality depends on the observed surface,medium and the time-varying poses of the camera and light source. This workproposes to optimize camera/light poses as they move, so that the surface isscanned efficiently and the descattered recovery has the highest quality. Thework generalizes the next best view concept of robot vision to scattering mediaand cooperative movable lighting. It also extends descattering to platformsthat move optimally. The optimization criterion is information gain, taken frominformation theory. We exploit the existence of a prior rough 3D model, sinceunderwater such a model is routinely obtained using sonar. We demonstrate thisprinciple in a scaled-down setup.
arxiv-14100-237 | Image reconstruction from dense binary pixels | http://arxiv.org/pdf/1512.01774v1.pdf | author:Or Litany, Tal Remez, Alex Bronstein category:cs.CV published:2015-12-06 summary:Recently, the dense binary pixel Gigavision camera had been introduced,emulating a digital version of the photographic film. While seems to be apromising solution for HDR imaging, its output is not directly usable andrequires an image reconstruction process. In this work, we formulate thisproblem as the minimization of a convex objective combining amaximum-likelihood term with a sparse synthesis prior. We present MLNet - anovel feed-forward neural network, producing acceptable output quality at afixed complexity and is two orders of magnitude faster than iterativealgorithms. We present state of the art results in the abstract.
arxiv-14100-238 | Want Answers? A Reddit Inspired Study on How to Pose Questions | http://arxiv.org/pdf/1512.01768v1.pdf | author:Danish, Yogesh Dahiya, Partha Talukdar category:cs.CL published:2015-12-06 summary:Questions form an integral part of our everyday communication, both offlineand online. Getting responses to our questions from others is fundamental tosatisfying our information need and in extending our knowledge boundaries. Aquestion may be represented using various factors such as social, syntactic,semantic, etc. We hypothesize that these factors contribute with varyingdegrees towards getting responses from others for a given question. We performa thorough empirical study to measure effects of these factors using a novelquestion and answer dataset from the website Reddit.com. To the best of ourknowledge, this is the first such analysis of its kind on this important topic.We also use a sparse nonnegative matrix factorization technique toautomatically induce interpretable semantic factors from the question dataset.We also document various patterns on response prediction we observe during ouranalysis in the data. For instance, we found that preference-probing questionsare scantily answered. Our method is robust to capture such latent responsefactors. We hope to make our code and datasets publicly available uponpublication of the paper.
arxiv-14100-239 | Separated by an Un-common Language: Towards Judgment Language Informed Vector Space Modeling | http://arxiv.org/pdf/1508.00106v5.pdf | author:Ira Leviant, Roi Reichart category:cs.CL published:2015-08-01 summary:A common evaluation practice in the vector space models (VSMs) literature isto measure the models' ability to predict human judgments about lexicalsemantic relations between word pairs. Most existing evaluation sets, however,consist of scores collected for English word pairs only, ignoring the potentialimpact of the judgment language in which word pairs are presented on the humanscores. In this paper we translate two prominent evaluation sets, wordsim353(association) and SimLex999 (similarity), from English to Italian, German andRussian and collect scores for each dataset from crowdworkers fluent in itslanguage. Our analysis reveals that human judgments are strongly impacted bythe judgment language. Moreover, we show that the predictions of monolingualVSMs do not necessarily best correlate with human judgments made with thelanguage used for model training, suggesting that models and humans areaffected differently by the language they use when making semantic judgments.Finally, we show that in a large number of setups, multilingual VSM combinationresults in improved correlations with human judgments, suggesting thatmultilingualism may partially compensate for the judgment language effect onhuman judgments.
arxiv-14100-240 | Variational Particle Approximations | http://arxiv.org/pdf/1402.5715v3.pdf | author:Ardavan Saeedi, Tejas D Kulkarni, Vikash Mansinghka, Samuel Gershman category:stat.ML cs.LG published:2014-02-24 summary:Approximate inference in high-dimensional, discrete probabilistic models is acentral problem in computational statistics and machine learning. This paperdescribes discrete particle variational inference (DPVI), a new approach thatcombines key strengths of Monte Carlo, variational and search-based techniques.DPVI is based on a novel family of particle-based variational approximationsthat can be fit using simple, fast, deterministic search techniques. Like MonteCarlo, DPVI can handle multiple modes, and yields exact results in awell-defined limit. Like unstructured mean-field, DPVI is based on optimizing alower bound on the partition function; when this quantity is not of intrinsicinterest, it facilitates convergence assessment and debugging. Like both MonteCarlo and combinatorial search, DPVI can take advantage of factorization,sequential structure, and custom search operators. This paper defines DPVIparticle-based approximation family and partition function lower bounds, alongwith the sequential DPVI and local DPVI algorithm templates for optimizingthem. DPVI is illustrated and evaluated via experiments on lattice MarkovRandom Fields, nonparametric Bayesian mixtures and block-models, and parametricas well as non-parametric hidden Markov models. Results include applications toreal-world spike-sorting and relational modeling problems, and show that DPVIcan offer appealing time/accuracy trade-offs as compared to multiplealternatives.
arxiv-14100-241 | Similarity Learning via Adaptive Regression and Its Application to Image Retrieval | http://arxiv.org/pdf/1512.01728v1.pdf | author:Qi Qian, Inci M. Baytas, Rong Jin, Anil Jain, Shenghuo Zhu category:cs.LG published:2015-12-06 summary:We study the problem of similarity learning and its application to imageretrieval with large-scale data. The similarity between pairs of images can bemeasured by the distances between their high dimensional representations, andthe problem of learning the appropriate similarity is often addressed bydistance metric learning. However, distance metric learning requires thelearned metric to be a PSD matrix, which is computational expensive and notnecessary for retrieval ranking problem. On the other hand, the bilinear modelis shown to be more flexible for large-scale image retrieval task, hence, weadopt it to learn a matrix for estimating pairwise similarities under theregression framework. By adaptively updating the target matrix in regression,we can mimic the hinge loss, which is more appropriate for similarity learningproblem. Although the regression problem can have the closed-form solution, thecomputational cost can be very expensive. The computational challenges comefrom two aspects: the number of images can be very large and image featureshave high dimensionality. We address the first challenge by compressing thedata by a randomized algorithm with the theoretical guarantee. For the highdimensional issue, we address it by taking low rank assumption and applyingalternating method to obtain the partial matrix, which has a global optimalsolution. Empirical studies on real world image datasets (i.e., Caltech andImageNet) demonstrate the effectiveness and efficiency of the proposed method.
arxiv-14100-242 | Generating News Headlines with Recurrent Neural Networks | http://arxiv.org/pdf/1512.01712v1.pdf | author:Konstantin Lopyrev category:cs.CL cs.LG cs.NE published:2015-12-05 summary:We describe an application of an encoder-decoder recurrent neural networkwith LSTM units and attention to generating headlines from the text of newsarticles. We find that the model is quite effective at concisely paraphrasingnews articles. Furthermore, we study how the neural network decides which inputwords to pay attention to, and specifically we identify the function of thedifferent neurons in a simplified attention mechanism. Interestingly, oursimplified attention mechanism performs better that the more complex attentionmechanism on a held out set of articles.
arxiv-14100-243 | Iteratively reweighted adaptive lasso for conditional heteroscedastic time series with applications to AR-ARCH type processes | http://arxiv.org/pdf/1502.06557v2.pdf | author:Florian Ziel category:stat.ME q-fin.CP stat.AP stat.CO stat.ML published:2015-02-23 summary:Shrinkage algorithms are of great importance in almost every area ofstatistics due to the increasing impact of big data. Especially time seriesanalysis benefits from efficient and rapid estimation techniques such as thelasso. However, currently lasso type estimators for autoregressive time seriesmodels still focus on models with homoscedastic residuals. Therefore, aniteratively reweighted adaptive lasso algorithm for the estimation of timeseries models under conditional heteroscedasticity is presented in ahigh-dimensional setting. The asymptotic behaviour of the resulting estimatoris analysed. It is found that the proposed estimation procedure performssubstantially better than its homoscedastic counterpart. A special case of thealgorithm is suitable to compute the estimated multivariate AR-ARCH type modelsefficiently. Extensions to the model like periodic AR-ARCH, threshold AR-ARCHor ARMA-GARCH are discussed. Finally, different simulation results andapplications to electricity market data and returns of metal prices are shown.
arxiv-14100-244 | Variance Reduction for Distributed Stochastic Gradient Descent | http://arxiv.org/pdf/1512.01708v1.pdf | author:Soham De, Gavin Taylor, Tom Goldstein category:cs.LG cs.DC math.OC stat.ML published:2015-12-05 summary:Variance reduction (VR) methods boost the performance of stochastic gradientdescent (SGD) by enabling the use of larger stepsizes and preserving linearconvergence rates. However, current variance reduced SGD methods require eitherhigh memory usage or require a full pass over the (large) data set at the endof each epoch to calculate the exact gradient of the objective function. Thismakes current VR methods impractical in distributed or parallel settings. Inthis paper, we propose a variance reduction method, called VR-lite, that doesnot require full gradient computations or extra storage. We explore distributedsynchronous and asynchronous variants with both high and low communicationlatency. We find that our distributed algorithms scale linearly with the numberof local workers and remain stable even with low communication frequency. Weempirically compare both the sequential and distributed algorithms tostate-of-the-art stochastic optimization methods, and find that our proposedalgorithms consistently converge faster than other stochastic methods.
arxiv-14100-245 | BDgraph: An R Package for Bayesian Structure Learning in Graphical Models | http://arxiv.org/pdf/1501.05108v4.pdf | author:Abdolreza Mohammadi, Ernst C. Wit category:stat.ML published:2015-01-21 summary:Graphical models provide powerful tools to uncover complicated patterns inmultivariate data and are commonly used in Bayesian statistics and machinelearning. In this paper, we introduce an R package BDgraph which performsBayesian structure learning for general undirected graphical models with eithercontinuous or discrete variables. The package efficiently implements recentimprovements in the Bayesian literature. To speed up computations, thecomputationally intensive tasks have been implemented in C++ and interfacedwith R. In addition, the package contains several functions for simulation andvisualization, as well as two multivariate datasets taken from the literatureand are used to describe the package capabilities. The paper includes a briefoverview of the statistical methods which have been implemented in the package.The main body of the paper explains how to use the package. Furthermore, weillustrate the package's functionality in both real and artificial examples, aswell as in an extensive simulation study.
arxiv-14100-246 | Canonical Correlation Forests | http://arxiv.org/pdf/1507.05444v5.pdf | author:Tom Rainforth, Frank Wood category:stat.ML cs.LG published:2015-07-20 summary:We introduce canonical correlation forests (CCFs), a new decision treeensemble method for classification. Individual canonical correlation trees arebinary decision trees with hyperplane splits based on canonical correlationcomponents. Unlike axis-aligned alternatives, the decision surfaces of CCFs arenot restricted to the coordinate system of the input features and thereforemore naturally represent data with correlation between the features.Additionally we introduce a novel alternative to bagging, the projectionbootstrap, which maintains use of the full dataset in selecting split points.CCFs do not require parameter tuning and our experiments show that theyout-perform axis-aligned random forests, other state-of-the-art tree ensemblemethods and all of the 179 popular classifiers considered in a recent extensivesurvey.
arxiv-14100-247 | Deep Attention Recurrent Q-Network | http://arxiv.org/pdf/1512.01693v1.pdf | author:Ivan Sorokin, Alexey Seleznev, Mikhail Pavlov, Aleksandr Fedorov, Anastasiia Ignateva category:cs.LG published:2015-12-05 summary:A deep learning approach to reinforcement learning led to a general learnerable to train on visual input to play a variety of arcade games at the humanand superhuman levels. Its creators at the Google DeepMind's team called theapproach: Deep Q-Network (DQN). We present an extension of DQN by "soft" and"hard" attention mechanisms. Tests of the proposed Deep Attention RecurrentQ-Network (DARQN) algorithm on multiple Atari 2600 games show level ofperformance superior to that of DQN. Moreover, built-in attention mechanismsallow a direct online monitoring of the training process by highlighting theregions of the game screen the agent is focusing on when making decisions.
arxiv-14100-248 | Feature Selection for Ridge Regression with Provable Guarantees | http://arxiv.org/pdf/1506.05173v2.pdf | author:Saurabh Paul, Petros Drineas category:stat.ML cs.IT cs.LG math.IT published:2015-06-17 summary:We introduce single-set spectral sparsification as a deterministic samplingbased feature selection technique for regularized least squares classification,which is the classification analogue to ridge regression. The method isunsupervised and gives worst-case guarantees of the generalization power of theclassification function after feature selection with respect to theclassification function obtained using all features. We also introduceleverage-score sampling as an unsupervised randomized feature selection methodfor ridge regression. We provide risk bounds for both single-set spectralsparsification and leverage-score sampling on ridge regression in the fixeddesign setting and show that the risk in the sampled space is comparable to therisk in the full-feature space. We perform experiments on synthetic andreal-world datasets, namely a subset of TechTC-300 datasets, to support ourtheory. Experimental results indicate that the proposed methods perform betterthan the existing feature selection methods.
arxiv-14100-249 | Maximum Entropy Binary Encoding for Face Template Protection | http://arxiv.org/pdf/1512.01691v1.pdf | author:Rohit Kumar Pandey, Yingbo Zhou, Bhargava Urala Kota, Venu Govindaraju category:cs.CV published:2015-12-05 summary:In this paper we present a framework for secure identification using deepneural networks, and apply it to the task of template protection for faceauthentication. We use deep convolutional neural networks (CNNs) to learn amapping from face images to maximum entropy binary (MEB) codes. The mapping isrobust enough to tackle the problem of exact matching, yielding the same codefor new samples of a user as the code assigned during training. These codes arethen hashed using any hash function that follows the random oracle model (likeSHA-512) to generate protected face templates (similar to text based passwordprotection). The algorithm makes no unrealistic assumptions and offers hightemplate security, cancelability, and state-of-the-art matching performance.The efficacy of the approach is shown on CMU-PIE, Extended Yale B, andMulti-PIE face databases. We achieve high (~95%) genuine accept rates (GAR) atzero false accept rate (FAR) with up to 1024 bits of template security.
arxiv-14100-250 | Neural Activation Constellations: Unsupervised Part Model Discovery with Convolutional Networks | http://arxiv.org/pdf/1504.08289v3.pdf | author:Marcel Simon, Erik Rodner category:cs.CV published:2015-04-30 summary:Part models of object categories are essential for challenging recognitiontasks, where differences in categories are subtle and only reflected inappearances of small parts of the object. We present an approach that is ableto learn part models in a completely unsupervised manner, without partannotations and even without given bounding boxes during learning. The key ideais to find constellations of neural activation patterns computed usingconvolutional neural networks. In our experiments, we outperform existingapproaches for fine-grained recognition on the CUB200-2011, NA birds, OxfordPETS, and Oxford Flowers dataset in case no part or bounding box annotationsare available and achieve state-of-the-art performance for the Stanford Dogdataset. We also show the benefits of neural constellation models as a dataaugmentation technique for fine-tuning. Furthermore, our paper unites the areasof generic and fine-grained classification, since our approach is suitable forboth scenarios. The source code of our method is available online athttp://www.inf-cv.uni-jena.de/part_discovery
arxiv-14100-251 | A Shapley Value Solution to Game Theoretic-based Feature Reduction in False Alarm Detection | http://arxiv.org/pdf/1512.01680v1.pdf | author:Fatemeh Afghah, Abolfazl Razi, Kayvan Najarian category:cs.CV published:2015-12-05 summary:False alarm is one of the main concerns in intensive care units and canresult in care disruption, sleep deprivation, and insensitivity of care-giversto alarms. Several methods have been proposed to suppress the false alarm ratethrough improving the quality of physiological signals by filtering, anddeveloping more accurate sensors. However, significant intrinsic correlationamong the extracted features limits the performance of most currently availabledata mining techniques, as they often discard the predictors with lowindividual impact that may potentially have strong discriminatory power whengrouped with others. We propose a model based on coalition game theory thatconsiders the inter-features dependencies in determining the salient predictorsin respect to false alarm, which results in improved classification accuracy.The superior performance of this method compared to current methods is shown insimulation results using PhysionNet's MIMIC II database.
arxiv-14100-252 | A Survey of the Trends in Facial and Expression Recognition Databases and Methods | http://arxiv.org/pdf/1511.02407v2.pdf | author:Sohini Roychowdhury, Michelle Emmons category:cs.CV published:2015-11-07 summary:Automated facial identification and facial expression recognition have beentopics of active research over the past few decades. Facial and expressionrecognition find applications in human-computer interfaces, subject tracking,real-time security surveillance systems and social networking. Several holisticand geometric methods have been developed to identify faces and expressionsusing public and local facial image databases. In this work we present theevolution in facial image data sets and the methodologies for facialidentification and recognition of expressions such as anger, sadness,happiness, disgust, fear and surprise. We observe that most of the earliermethods for facial and expression recognition aimed at improving therecognition rates for facial feature-based methods using static images.However, the recent methodologies have shifted focus towards robustimplementation of facial/expression recognition from large image databases thatvary with space (gathered from the internet) and time (video recordings). Theevolution trends in databases and methodologies for facial and expressionrecognition can be useful for assessing the next-generation topics that mayhave applications in security systems or personal identification systems thatinvolve "Quantitative face" assessments.
arxiv-14100-253 | Stochastic Collapsed Variational Inference for Sequential Data | http://arxiv.org/pdf/1512.01666v1.pdf | author:Pengyu Wang, Phil Blunsom category:stat.ML published:2015-12-05 summary:Stochastic variational inference for collapsed models has recently beensuccessfully applied to large scale topic modelling. In this paper, we proposea stochastic collapsed variational inference algorithm in the sequential datasetting. Our algorithm is applicable to both finite hidden Markov models andhierarchical Dirichlet process hidden Markov models, and to any datasetsgenerated by emission distributions in the exponential family. Our experimentresults on two discrete datasets show that our inference is both more efficientand more accurate than its uncollapsed version, stochastic variationalinference.
arxiv-14100-254 | Stochastic Collapsed Variational Inference for Hidden Markov Models | http://arxiv.org/pdf/1512.01665v1.pdf | author:Pengyu Wang, Phil Blunsom category:stat.ML published:2015-12-05 summary:Stochastic variational inference for collapsed models has recently beensuccessfully applied to large scale topic modelling. In this paper, we proposea stochastic collapsed variational inference algorithm for hidden Markovmodels, in a sequential data setting. Given a collapsed hidden Markov Model, webreak its long Markov chain into a set of short subchains. We propose a novelsum-product algorithm to update the posteriors of the subchains, taking intoaccount their boundary transitions due to the sequential dependencies. Ourexperiments on two discrete datasets show that our collapsed algorithm isscalable to very large datasets, memory efficient and significantly moreaccurate than the existing uncollapsed algorithm.
arxiv-14100-255 | A Picture is Worth a Billion Bits: Real-Time Image Reconstruction from Dense Binary Pixels | http://arxiv.org/pdf/1510.04601v2.pdf | author:Tal Remez, Or Litany, Alex Bronstein category:cs.CV published:2015-10-15 summary:The pursuit of smaller pixel sizes at ever increasing resolution in digitalimage sensors is mainly driven by the stringent price and form-factorrequirements of sensors and optics in the cellular phone market. Recently, EricFossum proposed a novel concept of an image sensor with dense sub-diffractionlimit one-bit pixels jots, which can be considered a digital emulation ofsilver halide photographic film. This idea has been recently embodied as theEPFL Gigavision camera. A major bottleneck in the design of such sensors is theimage reconstruction process, producing a continuous high dynamic range imagefrom oversampled binary measurements. The extreme quantization of the Poissonstatistics is incompatible with the assumptions of most standard imageprocessing and enhancement frameworks. The recently proposed maximum-likelihood(ML) approach addresses this difficulty, but suffers from image artifacts andhas impractically high computational complexity. In this work, we study avariant of a sensor with binary threshold pixels and propose a reconstructionalgorithm combining an ML data fitting term with a sparse synthesis prior. Wealso show an efficient hardware-friendly real-time approximation of thisinverse operator.Promising results are shown on synthetic data as well as onHDR data emulated using multiple exposures of a regular CMOS sensor.
arxiv-14100-256 | Spatially Coherent Random Forests | http://arxiv.org/pdf/1511.02911v2.pdf | author:Tal Remez, Shai Avidan category:cs.CV published:2015-11-09 summary:Spatially Coherent Random Forest (SCRF) extends Random Forest to createspatially coherent labeling. Each split function in SCRF is evaluated based ona traditional information gain measure that is regularized by a spatialcoherency term. This way, SCRF is encouraged to choose split functions thatcluster pixels both in appearance space and in image space. In particular, weuse SCRF to detect contours in images, where contours are taken to be theboundaries between different regions. Each tree in the forest produces asegmentation of the image plane and the boundaries of the segmentations of alltrees are aggregated to produce a final hierarchical contour map. We show thatthis modification improves the performance of regular Random Forest by about10% on the standard Berkeley Segmentation Datasets. We believe that SCRF can beused in other settings as well.
arxiv-14100-257 | Regularized EM Algorithms: A Unified Framework and Statistical Guarantees | http://arxiv.org/pdf/1511.08551v2.pdf | author:Xinyang Yi, Constantine Caramanis category:cs.LG stat.ML published:2015-11-27 summary:Latent variable models are a fundamental modeling tool in machine learningapplications, but they present significant computational and analyticalchallenges. The popular EM algorithm and its variants, is a much usedalgorithmic tool; yet our rigorous understanding of its performance is highlyincomplete. Recently, work in Balakrishnan et al. (2014) has demonstrated thatfor an important class of problems, EM exhibits linear local convergence. Inthe high-dimensional setting, however, the M-step may not be well defined. Weaddress precisely this setting through a unified treatment usingregularization. While regularization for high-dimensional problems is by nowwell understood, the iterative EM algorithm requires a careful balancing ofmaking progress towards the solution while identifying the right structure(e.g., sparsity or low-rank). In particular, regularizing the M-step using thestate-of-the-art high-dimensional prescriptions (e.g., Wainwright (2014)) isnot guaranteed to provide this balance. Our algorithm and analysis are linkedin a way that reveals the balance between optimization and statistical errors.We specialize our general framework to sparse gaussian mixture models,high-dimensional mixed regression, and regression with missing variables,obtaining statistical guarantees for each of these examples.
arxiv-14100-258 | A Deep Structured Model with Radius-Margin Bound for 3D Human Activity Recognition | http://arxiv.org/pdf/1512.01642v1.pdf | author:Liang Lin, Keze Wang, Wangmeng Zuo, Meng Wang, Jiebo Luo, Lei Zhang category:cs.CV published:2015-12-05 summary:Understanding human activity is very challenging even with the recentlydeveloped 3D/depth sensors. To solve this problem, this work investigates anovel deep structured model, which adaptively decomposes an activity instanceinto temporal parts using the convolutional neural networks (CNNs). Our modeladvances the traditional deep learning approaches in two aspects. First, { weincorporate latent temporal structure into the deep model, accounting for largetemporal variations of diverse human activities. In particular, we utilize thelatent variables to decompose the input activity into a number of temporallysegmented sub-activities, and accordingly feed them into the parts (i.e.sub-networks) of the deep architecture}. Second, we incorporate a radius-marginbound as a regularization term into our deep model, which effectively improvesthe generalization performance for classification. For model training, wepropose a principled learning algorithm that iteratively (i) discovers theoptimal latent variables (i.e. the ways of activity decomposition) for alltraining instances, (ii) { updates the classifiers} based on the generatedfeatures, and (iii) updates the parameters of multi-layer neural networks. Inthe experiments, our approach is validated on several complex scenarios forhuman activity recognition and demonstrates superior performances over otherstate-of-the-art approaches.
arxiv-14100-259 | Unsupervised comparable corpora preparation and exploration for bi-lingual translation equivalents | http://arxiv.org/pdf/1512.01641v1.pdf | author:Krzysztof Wołk, Krzysztof Marasek category:cs.CL stat.ML published:2015-12-05 summary:The multilingual nature of the world makes translation a crucial requirementtoday. Parallel dictionaries constructed by humans are a widely-availableresource, but they are limited and do not provide enough coverage for goodquality translation purposes, due to out-of-vocabulary words and neologisms.This motivates the use of statistical translation systems, which areunfortunately dependent on the quantity and quality of training data. Suchsystems have a very limited availability especially for some languages and verynarrow text domains. In this research we present our improvements to currentcomparable corpora mining methodologies by re- implementation of the comparisonalgorithms (using Needleman-Wunch algorithm), introduction of a tuning scriptand computation time improvement by GPU acceleration. Experiments are carriedout on bilingual data extracted from the Wikipedia, on various domains. For theWikipedia itself, additional cross-lingual comparison heuristics wereintroduced. The modifications made a positive impact on the quality andquantity of mined data and on the translation quality.
arxiv-14100-260 | PJAIT Systems for the IWSLT 2015 Evaluation Campaign Enhanced by Comparable Corpora | http://arxiv.org/pdf/1512.01639v1.pdf | author:Krzysztof Wołk, Krzysztof Marasek category:cs.CL stat.ML published:2015-12-05 summary:In this paper, we attempt to improve Statistical Machine Translation (SMT)systems on a very diverse set of language pairs (in both directions): Czech -English, Vietnamese - English, French - English and German - English. Toaccomplish this, we performed translation model training, created adaptationsof training settings for each language pair, and obtained comparable corporafor our SMT systems. Innovative tools and data adaptation techniques wereemployed. The TED parallel text corpora for the IWSLT 2015 evaluation campaignwere used to train language models, and to develop, tune, and test the system.In addition, we prepared Wikipedia-based comparable corpora for use with ourSMT system. This data was specified as permissible for the IWSLT 2015evaluation. We explored the use of domain adaptation techniques, symmetrizedword alignment models, the unsupervised transliteration models and the KenLMlanguage modeling tool. To evaluate the effects of different preparations ontranslation results, we conducted experiments and used the BLEU, NIST and TERmetrics. Our results indicate that our approach produced a positive impact onSMT quality.
arxiv-14100-261 | Stochastic Proximal Gradient Descent for Nuclear Norm Regularization | http://arxiv.org/pdf/1511.01664v2.pdf | author:Lijun Zhang, Tianbao Yang, Rong Jin, Zhi-Hua Zhou category:cs.LG published:2015-11-05 summary:In this paper, we utilize stochastic optimization to reduce the spacecomplexity of convex composite optimization with a nuclear norm regularizer,where the variable is a matrix of size $m \times n$. By constructing a low-rankestimate of the gradient, we propose an iterative algorithm based on stochasticproximal gradient descent (SPGD), and take the last iterate of SPGD as thefinal solution. The main advantage of the proposed algorithm is that its spacecomplexity is $O(m+n)$, in contrast, most of previous algorithms have a $O(mn)$space complexity. Theoretical analysis shows that it achieves $O(\logT/\sqrt{T})$ and $O(\log T/T)$ convergence rates for general convex functionsand strongly convex functions, respectively.
arxiv-14100-262 | Hierarchical Sparse Modeling: A Choice of Two Regularizers | http://arxiv.org/pdf/1512.01631v1.pdf | author:Xiaohan Yan, Jacob Bien category:stat.ME math.ST stat.CO stat.ML stat.TH published:2015-12-05 summary:Demanding sparsity in estimated models has become a routine practice instatistics. In many situations, we wish to demand that the sparsity patternsattained honor certain problem-specific constraints. Hierarchical sparsemodeling (HSM) refers to situations in which these constraints specify that oneset of parameters be set to zero whenever another is set to zero. In recentyears, numerous papers have developed convex regularizers for this form ofsparsity structure arising in areas including interaction modeling, timeseries, and covariance estimation. In this paper, we observe that these methodsfall into two frameworks, which have not been systematically compared in thecontext of HSM. The purpose of this paper is to provide a side-by-sidecomparison of these two frameworks for HSM in terms of their statisticalproperties and computational efficiency. We call attention to a problem withthe more commonly used framework and provide new insights into the other, whichcan greatly improve its computational performance. Finally, we compare the twomethods in the context of covariance estimation, where we introduce a newsparsely-banded estimator, which we show achieves the statistical advantages ofan existing method but is simpler to compute.
arxiv-14100-263 | Risk-Constrained Reinforcement Learning with Percentile Risk Criteria | http://arxiv.org/pdf/1512.01629v1.pdf | author:Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, Marco Pavone category:cs.AI cs.LG math.OC published:2015-12-05 summary:In many sequential decision-making problems one is interested in minimizingan expected cumulative cost while taking into account \emph{risk}, i.e.,increased awareness of events of small probability and high consequences.Accordingly, the objective of this paper is to present efficient reinforcementlearning algorithms for risk-constrained Markov decision processes (MDPs),where risk is represented via a chance constraint or a constraint on theconditional value-at-risk (CVaR) of the cumulative cost. We collectively referto such problems as percentile risk-constrained MDPs. Specifically, we first derive a formula for computing the gradient of theLagrangian function for percentile risk-constrained MDPs. Then, we devisepolicy gradient and actor-critic algorithms that (1) estimate such gradient,(2) update the policy parameters in the descent direction, and (3) update theLagrange multiplier in the ascent direction. For these algorithms we proveconvergence to locally-optimal policies. Finally, we demonstrate theeffectiveness of our algorithms in an optimal stopping problem and an onlinemarketing application.
arxiv-14100-264 | On Binary Embedding using Circulant Matrices | http://arxiv.org/pdf/1511.06480v2.pdf | author:Felix X. Yu, Aditya Bhaskara, Sanjiv Kumar, Yunchao Gong, Shih-Fu Chang category:cs.DS cs.LG published:2015-11-20 summary:Binary embeddings provide efficient and powerful ways to perform operationson large scale data. However binary embedding typically requires long codes inorder to preserve the discriminative power of the input space. Thus binarycoding methods traditionally suffer from high computation and storage costs insuch a scenario. To address this problem, we propose Circulant Binary Embedding(CBE) which generates binary codes by projecting the data with a circulantmatrix. The circulant structure allows us to use Fast Fourier Transformalgorithms to speed up the computation. For obtaining $k$-bit binary codes from$d$-dimensional data, this improves the time complexity from $O(dk)$ to$O(d\log{d})$, and the space complexity from $O(dk)$ to $O(d)$. We study two settings, which differ in the way we choose the parameters ofthe circulant matrix. In the first, the parameters are chosen randomly and inthe second, the parameters are learned using the data. For randomized CBE, wegive a theoretical analysis comparing it with binary embedding using anunstructured random projection matrix. The challenge here is to show that thedependencies in the entries of the circulant matrix do not lead to a loss inperformance. In the second setting, we design a novel time-frequencyalternating optimization to learn data-dependent circulant projections, whichalternatively minimizes the objective in original and Fourier domains. In boththe settings, we show by extensive experiments that the CBE approach gives muchbetter performance than the state-of-the-art approaches if we fix a runningtime, and provides much faster computation with negligible performancedegradation if we fix the number of bits in the embedding.
arxiv-14100-265 | Extracting Biomolecular Interactions Using Semantic Parsing of Biomedical Text | http://arxiv.org/pdf/1512.01587v1.pdf | author:Sahil Garg, Aram Galstyan, Ulf Hermjakob, Daniel Marcu category:cs.CL cs.AI cs.IR cs.IT cs.LG math.IT published:2015-12-04 summary:We advance the state of the art in biomolecular interaction extraction withthree contributions: (i) We show that deep, Abstract Meaning Representations(AMR) significantly improve the accuracy of a biomolecular interactionextraction system when compared to a baseline that relies solely on surface-and syntax-based features; (ii) In contrast with previous approaches that inferrelations on a sentence-by-sentence basis, we expand our framework to enableconsistent predictions over sets of sentences (documents); (iii) We furthermodify and expand a graph kernel learning framework to enable concurrentexploitation of automatically induced AMR (semantic) and dependency structure(syntactic) representations. Our experiments show that our approach yieldsinteraction extraction systems that are more robust in environments where thereis a significant mismatch between training and test conditions.
arxiv-14100-266 | Learning with Group Invariant Features: A Kernel Perspective | http://arxiv.org/pdf/1506.02544v2.pdf | author:Youssef Mroueh, Stephen Voinea, Tomaso Poggio category:cs.LG cs.CV stat.ML published:2015-06-08 summary:We analyze in this paper a random feature map based on a theory of invarianceI-theory introduced recently. More specifically, a group invariant signalsignature is obtained through cumulative distributions of group transformedrandom projections. Our analysis bridges invariant feature learning with kernelmethods, as we show that this feature map defines an expected Haar integrationkernel that is invariant to the specified group action. We show how thisnon-linear random feature map approximates this group invariant kerneluniformly on a set of $N$ points. Moreover, we show that it defines a functionspace that is dense in the equivalent Invariant Reproducing Kernel HilbertSpace. Finally, we quantify error rates of the convergence of the empiricalrisk minimization, as well as the reduction in the sample complexity of alearning algorithm using such an invariant representation for signalclassification, in a classical supervised learning setting.
arxiv-14100-267 | Representational Distance Learning for Deep Neural Networks | http://arxiv.org/pdf/1511.03979v5.pdf | author:Patrick McClure, Nikolaus Kriegeskorte category:cs.NE cs.CV published:2015-11-12 summary:We propose representational distance learning (RDL), a technique that allowstransferring knowledge from an arbitrary model with task related information toa deep neural network (DNN). This method seeks to maximize the similaritybetween the representational distance matrices (RDMs) of a model with desiredknowledge, the teacher, and a DNN currently being trained, the student. Theknowledge contained in the information transformations performed by the teacherare transferred to a student using auxiliary error functions. This allows a DNNto simultaneously learn from a teacher model and learn to perform some taskwithin the framework of backpropagation. We test the use of RDL for knowledgedistillation, also known as model compression, from a large teacher DNN to asmall student DNN using the MNIST and CIFAR-10 datasets. Also, we test the useof RDL for knowledge transfer between tasks using the CIFAR-10 and CIFAR-100datasets. For each test, RDL significantly improves performance when comparedto traditional backpropagation alone and performs similarly to, or better than,recently proposed methods for model compression and knowledge transfer.
arxiv-14100-268 | Reuse of Neural Modules for General Video Game Playing | http://arxiv.org/pdf/1512.01537v1.pdf | author:Alexander Braylan, Mark Hollenbeck, Elliot Meyerson, Risto Miikkulainen category:cs.NE cs.AI published:2015-12-04 summary:A general approach to knowledge transfer is introduced in which an agentcontrolled by a neural network adapts how it reuses existing networks as itlearns in a new domain. Networks trained for a new domain can improve theirperformance by routing activation selectively through previously learned neuralstructure, regardless of how or for what it was learned. A neuroevolutionimplementation of this approach is presented with application tohigh-dimensional sequential decision-making domains. This approach is moregeneral than previous approaches to neural transfer for reinforcement learning.It is domain-agnostic and requires no prior assumptions about the nature oftask relatedness or mappings. The method is analyzed in a stochastic version ofthe Arcade Learning Environment, demonstrating that it improves performance insome of the more complex Atari 2600 games, and that the success of transfer canbe predicted based on a high-level characterization of game dynamics.
arxiv-14100-269 | Motion trails from time-lapse video | http://arxiv.org/pdf/1512.01533v1.pdf | author:Camille Goudeseune category:cs.CV I.3.3; I.4.6 published:2015-12-04 summary:From an image sequence captured by a stationary camera, backgroundsubtraction can detect moving foreground objects in the scene. Distinguishingforeground from background is further improved by various heuristics. Then eachobject's motion can be emphasized by duplicating its positions as a motiontrail. These trails clarify the objects' spatial relationships. Also, addingmotion trails to a video before previewing it at high speed reduces the risk ofoverlooking transient events.
arxiv-14100-270 | Detecting Road Surface Wetness from Audio: A Deep Learning Approach | http://arxiv.org/pdf/1511.07035v2.pdf | author:Irman Abdić, Lex Fridman, Erik Marchi, Daniel E Brown, William Angell, Bryan Reimer, Björn Schuller category:cs.LG cs.NE cs.SD published:2015-11-22 summary:We introduce a recurrent neural network architecture for automated roadsurface wetness detection from audio of tire-surface interaction. Therobustness of our approach is evaluated on 785,826 bins of audio that span anextensive range of vehicle speeds, noises from the environment, road surfacetypes, and pavement conditions including international roughness index (IRI)values from 25 in/mi to 1400 in/mi. The training and evaluation of the modelare performed on different roads to minimize the impact of environmental andother external factors on the accuracy of the classification. We achieve anunweighted average recall (UAR) of 93.2% across all vehicle speeds including 0mph. The classifier still works at 0 mph because the discriminating signal ispresent in the sound of other vehicles driving by.
arxiv-14100-271 | Learning the Semantics of Manipulation Action | http://arxiv.org/pdf/1512.01525v1.pdf | author:Yezhou Yang, Yiannis Aloimonos, Cornelia Fermuller, Eren Erdal Aksoy category:cs.RO cs.CL cs.CV published:2015-12-04 summary:In this paper we present a formal computational framework for modelingmanipulation actions. The introduced formalism leads to semantics ofmanipulation action and has applications to both observing and understandinghuman manipulation actions as well as executing them with a robotic mechanism(e.g. a humanoid robot). It is based on a Combinatory Categorial Grammar. Thegoal of the introduced framework is to: (1) represent manipulation actions withboth syntax and semantic parts, where the semantic part employs$\lambda$-calculus; (2) enable a probabilistic semantic parsing schema to learnthe $\lambda$-calculus representation of manipulation action from an annotatedaction corpus of videos; (3) use (1) and (2) to develop a system that visuallyobserves manipulation actions and understands their meaning while it can reasonbeyond observations using propositional logic and axiom schemata. Theexperiments conducted on a public available large manipulation action datasetvalidate the theoretical framework and our implementation.
arxiv-14100-272 | ASIST: Automatic Semantically Invariant Scene Transformation | http://arxiv.org/pdf/1512.01515v1.pdf | author:Or Litany, Tal Remez, Daniel Freedman, Lior Shapira, Alex Bronstein, Ran Gal category:cs.CV published:2015-12-04 summary:We present ASIST, a technique for transforming point clouds by replacingobjects with their semantically equivalent counterparts. Transformations ofthis kind have applications in virtual reality, repair of fused scans, androbotics. ASIST is based on a unified formulation of semantic labeling andobject replacement; both result from minimizing a single objective. We presentnumerical tools for the e?cient solution of this optimization problem. Themethod is experimentally assessed on new datasets of both synthetic and realpoint clouds, and is additionally compared to two recent works on objectreplacement on data from the corresponding papers.
arxiv-14100-273 | Necessary and Sufficient Conditions and a Provably Efficient Algorithm for Separable Topic Discovery | http://arxiv.org/pdf/1508.05565v2.pdf | author:Weicong Ding, Prakash Ishwar, Venkatesh Saligrama category:cs.LG cs.CL cs.IR stat.ML published:2015-08-23 summary:We develop necessary and sufficient conditions and a novel provablyconsistent and efficient algorithm for discovering topics (latent factors) fromobservations (documents) that are realized from a probabilistic mixture ofshared latent factors that have certain properties. Our focus is on the classof topic models in which each shared latent factor contains a novel word thatis unique to that factor, a property that has come to be known as separability.Our algorithm is based on the key insight that the novel words correspond tothe extreme points of the convex hull formed by the row-vectors of a suitablynormalized word co-occurrence matrix. We leverage this geometric insight toestablish polynomial computation and sample complexity bounds based on a fewisotropic random projections of the rows of the normalized word co-occurrencematrix. Our proposed random-projections-based algorithm is naturally amenableto an efficient distributed implementation and is attractive for modernweb-scale distributed data mining applications.
arxiv-14100-274 | Hand-held Video Deblurring via Efficient Fourier Aggregation | http://arxiv.org/pdf/1509.05251v3.pdf | author:Mauricio Delbracio, Guillermo Sapiro category:cs.CV published:2015-09-17 summary:Videos captured with hand-held cameras often suffer from a significant amountof blur, mainly caused by the inevitable natural tremor of the photographer'shand. In this work, we present an algorithm that removes blur due to camerashake by combining information in the Fourier domain from nearby frames in avideo. The dynamic nature of typical videos with the presence of multiplemoving objects and occlusions makes this problem of camera shake removalextremely challenging, in particular when low complexity is needed. Given aninput video frame, we first create a consistent registered version oftemporally adjacent frames. Then, the set of consistently registered frames isblock-wise fused in the Fourier domain with weights depending on the Fourierspectrum magnitude. The method is motivated from the physiological fact thatcamera shake blur has a random nature and therefore, nearby video frames aregenerally blurred differently. Experiments with numerous videos recorded in thewild, along with extensive comparisons, show that the proposed algorithmachieves state-of-the-art results while at the same time being much faster thanits competitors.
arxiv-14100-275 | Removing Camera Shake via Weighted Fourier Burst Accumulation | http://arxiv.org/pdf/1505.02731v2.pdf | author:Mauricio Delbracio, Guillermo Sapiro category:cs.CV published:2015-05-11 summary:Numerous recent approaches attempt to remove image blur due to camera shake,either with one or multiple input images, by explicitly solving an inverse andinherently ill-posed deconvolution problem. If the photographer takes a burstof images, a modality available in virtually all modern digital cameras, weshow that it is possible to combine them to get a clean sharp version. This isdone without explicitly solving any blur estimation and subsequent inverseproblem. The proposed algorithm is strikingly simple: it performs a weightedaverage in the Fourier domain, with weights depending on the Fourier spectrummagnitude. The method can be seen as a generalization of the align and averageprocedure, with a weighted average, motivated by hand-shake physiology andtheoretically supported, taking place in the Fourier domain. The method'srationale is that camera shake has a random nature and therefore each image inthe burst is generally blurred differently. Experiments with real camera data,and extensive comparisons, show that the proposed Fourier Burst Accumulation(FBA) algorithm achieves state-of-the-art results an order of magnitude faster,with simplicity for on-board implementation on camera phones. Finally, we alsopresent experiments in real high dynamic range (HDR) scenes, showing how themethod can be straightforwardly extended to HDR photography.
arxiv-14100-276 | Compressive Deconvolution in Medical Ultrasound Imaging | http://arxiv.org/pdf/1507.00136v2.pdf | author:Zhouye Chen, Adrian Basarab, Denis Kouamé category:cs.CV published:2015-07-01 summary:The interest of compressive sampling in ultrasound imaging has been recentlyextensively evaluated by several research teams. Following the differentapplication setups, it has been shown that the RF data may be reconstructedfrom a small number of measurements and/or using a reduced number of ultrasoundpulse emissions. Nevertheless, RF image spatial resolution, contrast and signalto noise ratio are affected by the limited bandwidth of the imaging transducerand the physical phenomenon related to US wave propagation. To overcome theselimitations, several deconvolution-based image processing techniques have beenproposed to enhance the ultrasound images. In this paper, we propose a novelframework, named compressive deconvolution, that reconstructs enhanced RFimages from compressed measurements. Exploiting an unified formulation of thedirect acquisition model, combining random projections and 2D convolution witha spatially invariant point spread function, the benefit of our approach is thejoint data volume reduction and image quality improvement. The proposedoptimization method, based on the Alternating Direction Method of Multipliers,is evaluated on both simulated and in vivo data.
arxiv-14100-277 | Stochastic Expectation Propagation for Large Scale Gaussian Process Classification | http://arxiv.org/pdf/1511.03249v3.pdf | author:Daniel Hernández-Lobato, José Miguel Hernández-Lobato, Yingzhen Li, Thang Bui, Richard E. Turner category:stat.ML published:2015-11-10 summary:A method for large scale Gaussian process classification has been recentlyproposed based on expectation propagation (EP). Such a method allows Gaussianprocess classifiers to be trained on very large datasets that were out of thereach of previous deployments of EP and has been shown to be competitive withrelated techniques based on stochastic variational inference. Nevertheless, thememory resources required scale linearly with the dataset size, unlike invariational methods. This is a severe limitation when the number of instancesis very large. Here we show that this problem is avoided when stochastic EP isused to train the model.
arxiv-14100-278 | Computational Imaging for VLBI Image Reconstruction | http://arxiv.org/pdf/1512.01413v1.pdf | author:Katherine L. Bouman, Michael D. Johnson, Daniel Zoran, Vincent L. Fish, Sheperd S. Doeleman, William T. Freeman category:astro-ph.IM astro-ph.GA cs.CV published:2015-12-04 summary:Very long baseline interferometry (VLBI) is a technique for imaging celestialradio emissions by simultaneously observing a source from telescopesdistributed across Earth. The challenges in reconstructing images from fineangular resolution VLBI data are immense. The data is extremely sparse andnoisy, thus requiring statistical image models such as those designed in thecomputer vision community. In this paper we present a novel Bayesian approachfor VLBI image reconstruction. While other methods require careful tuning andparameter selection for different types of images, our method is robust andproduces good results under different settings such as low SNR or extendedemissions. The success of our method is demonstrated on realistic syntheticexperiments as well as publicly available real data. We present this problem ina way that is accessible to members of the computer vision community, andprovide a dataset website (vlbiimaging.csail.mit.edu) to allow for controlledcomparisons across algorithms. This dataset can foster development of newmethods by making VLBI easily approachable to computer vision researchers.
arxiv-14100-279 | What Makes it Difficult to Understand a Scientific Literature? | http://arxiv.org/pdf/1512.01409v1.pdf | author:Mengyun Cao, Jiao Tian, Dezhi Cheng, Jin Liu, Xiaoping Sun category:cs.CL published:2015-12-04 summary:In the artificial intelligence area, one of the ultimate goals is to makecomputers understand human language and offer assistance. In order to achievethis ideal, researchers of computer science have put forward a lot of modelsand algorithms attempting at enabling the machine to analyze and process humannatural language on different levels of semantics. Although recent progress inthis field offers much hope, we still have to ask whether current research canprovide assistance that people really desire in reading and comprehension. Tothis end, we conducted a reading comprehension test on two scientific paperswhich are written in different styles. We use the semantic link models toanalyze the understanding obstacles that people will face in the process ofreading and figure out what makes it difficult for human to understand ascientific literature. Through such analysis, we summarized somecharacteristics and problems which are reflected by people with differentlevels of knowledge on the comprehension of difficult science and technologyliterature, which can be modeled in semantic link network. We believe thatthese characteristics and problems will help us re-examine the existing machinemodels and are helpful in the designing of new one.
arxiv-14100-280 | Neuron's Eye View: Inferring Features of Complex Stimuli from Neural Responses | http://arxiv.org/pdf/1512.01408v1.pdf | author:John M Pearson, Jeffrey M Beck category:stat.ML q-bio.NC published:2015-12-04 summary:Experiments that study neural encoding of stimuli at the level of individualneurons typically choose a small set of features present in the world ---contrast and luminance for vision, pitch and intensity for sound --- andassemble a stimulus set that systematically (and preferably exhaustively)varies along these dimensions. Neuronal responses in the form of firing ratesare then examined for modulation with respect to these features via some formof regression. This approach requires that experimenters know (or guess) inadvance the relevant features coded by a given population of neurons.Unfortunately, for domains as complex as social interaction or naturalmovement, the relevant feature space is poorly understood, and an arbitrary\emph{a priori} choice of feature sets may give rise to confirmation bias.Here, we present a Bayesian model for exploratory data analysis that is capableof automatically identifying the features present in unstructured stimuli basedsolely on neuronal responses. Our approach is unique within the class of latentstate space models of neural activity in that it assumes that firing rates ofneurons are sensitive to multiple discrete time-varying features tied to the\emph{stimulus}, each of which has Markov (or semi-Markov) dynamics. That is,we are modeling stimulus dynamics as driven by neural activity, rather thanintrinsic neural dynamics. We derive a fast variational Bayesian inferencealgorithm and show that it correctly recovers hidden features in syntheticdata, as well as ground-truth stimulus features in a prototypical neuraldataset. To demonstrate the utility of the algorithm, we also apply it to anexploratory analysis of prefrontal cortex recordings performed while monkeyswatched naturalistic videos of primate social activity.
arxiv-14100-281 | Model Validation for Vision Systems via Graphics Simulation | http://arxiv.org/pdf/1512.01401v1.pdf | author:V S R Veeravasarapu, Rudra Narayan Hota, Constantin Rothkopf, Ramesh Visvanathan category:cs.CV published:2015-12-04 summary:Rapid advances in computation, combined with latest advances in computergraphics simulations have facilitated the development of vision systems andtraining them in virtual environments. One major stumbling block is incertification of the designs and tuned parameters of these systems to work inreal world. In this paper, we begin to explore the fundamental question: Whichtype of information transfer is more analogous to real world? Inspired from theperformance characterization methodology outlined in the 90's, we note thatinsights derived from simulations can be qualitative or quantitative dependingon the degree of the fidelity of models used in simulations and the nature ofthe questions posed by the experimenter. We adapt the methodology in thecontext of current graphics simulation tools for modeling data generationprocesses and, for systematic performance characterization and trade-offanalysis for vision system design leading to qualitative and quantitativeinsights. In concrete, we examine invariance assumptions used in visionalgorithms for video surveillance settings as a case study and assess thedegree to which those invariance assumptions deviate as a function ofcontextual variables on both graphics simulations and in real data. As computergraphics rendering quality improves, we believe teasing apart the degree towhich model assumptions are valid via systematic graphics simulation can be asignificant aid to assisting more principled ways of approaching vision systemdesign and performance modeling.
arxiv-14100-282 | Max-Pooling Dropout for Regularization of Convolutional Neural Networks | http://arxiv.org/pdf/1512.01400v1.pdf | author:Haibing Wu, Xiaodong Gu category:cs.LG cs.CV cs.NE published:2015-12-04 summary:Recently, dropout has seen increasing use in deep learning. For deepconvolutional neural networks, dropout is known to work well in fully-connectedlayers. However, its effect in pooling layers is still not clear. This paperdemonstrates that max-pooling dropout is equivalent to randomly pickingactivation based on a multinomial distribution at training time. In light ofthis insight, we advocate employing our proposed probabilistic weightedpooling, instead of commonly used max-pooling, to act as model averaging attest time. Empirical evidence validates the superiority of probabilisticweighted pooling. We also compare max-pooling dropout and stochastic pooling,both of which introduce stochasticity based on multinomial distributions atpooling stage.
arxiv-14100-283 | Topic segmentation via community detection in complex networks | http://arxiv.org/pdf/1512.01384v1.pdf | author:Henrique F. de Arruda, Luciano da F. Costa, Diego R. Amancio category:cs.CL cs.SI published:2015-12-04 summary:Many real systems have been modelled in terms of network concepts, andwritten texts are a particular example of information networks. In recentyears, the use of network methods to analyze language has allowed the discoveryof several interesting findings, including the proposition of novel models toexplain the emergence of fundamental universal patterns. While syntacticalnetworks, one of the most prevalent networked models of written texts, displayboth scale-free and small-world properties, such representation fails incapturing other textual features, such as the organization in topics orsubjects. In this context, we propose a novel network representation whose mainpurpose is to capture the semantical relationships of words in a simple way. Todo so, we link all words co-occurring in the same semantic context, which isdefined in a threefold way. We show that the proposed representations favoursthe emergence of communities of semantically related words, and this featuremay be used to identify relevant topics. The proposed methodology to detecttopics was applied to segment selected Wikipedia articles. We have found that,in general, our methods outperform traditional bag-of-words representations,which suggests that a high-level textual representation may be useful to studysemantical features of texts.
arxiv-14100-284 | Sublabel-Accurate Relaxation of Nonconvex Energies | http://arxiv.org/pdf/1512.01383v1.pdf | author:Thomas Möllenhoff, Emanuel Laude, Michael Moeller, Jan Lellmann, Daniel Cremers category:cs.CV published:2015-12-04 summary:We propose a novel spatially continuous framework for convex relaxationsbased on functional lifting. Our method can be interpreted as asublabel-accurate solution to multilabel problems. We show that previouslyproposed functional lifting methods optimize an energy which is linear betweentwo labels and hence require (often infinitely) many labels for a faithfulapproximation. In contrast, the proposed formulation is based on a piecewiseconvex approximation and therefore needs far fewer labels. In comparison torecent MRF-based approaches, our method is formulated in a spatially continuoussetting and shows less grid bias. Moreover, in a local sense, our formulationis the tightest possible convex relaxation. It is easy to implement and allowsan efficient primal-dual optimization on GPUs. We show the effectiveness of ourapproach on several computer vision problems.
arxiv-14100-285 | Completely random measures for modelling block-structured networks | http://arxiv.org/pdf/1507.02925v3.pdf | author:Tue Herlau, Mikkel N. Schmidt, Morten Mørup category:stat.ML published:2015-07-10 summary:Many statistical methods for network data parameterize the edge-probabilityby attributing latent traits to the vertices such as block structure and assumeexchangeability in the sense of the Aldous-Hoover representation theorem.Empirical studies of networks indicate that many real-world networks have apower-law distribution of the vertices which in turn implies the number ofedges scale slower than quadratically in the number of vertices. Theseassumptions are fundamentally irreconcilable as the Aldous-Hoover theoremimplies quadratic scaling of the number of edges. Recently Caron and Fox (2014)proposed the use of a different notion of exchangeability due to Kallenberg(2009) and obtained a network model which admits power-law behaviour whileretaining desirable statistical properties, however this model does not capturelatent vertex traits such as block-structure. In this work we re-introduce theuse of block-structure for network models obeying Kallenberg's notion ofexchangeability and thereby obtain a model which admits the inference ofblock-structure and edge inhomogeneity. We derive a simple expression for thelikelihood and an efficient sampling method. The obtained model is notsignificantly more difficult to implement than existing approaches toblock-modelling and performs well on real network datasets.
arxiv-14100-286 | On Sparse variational methods and the Kullback-Leibler divergence between stochastic processes | http://arxiv.org/pdf/1504.07027v2.pdf | author:Alexander G. de G. Matthews, James Hensman, Richard E. Turner, Zoubin Ghahramani category:stat.ML published:2015-04-27 summary:The variational framework for learning inducing variables (Titsias, 2009a)has had a large impact on the Gaussian process literature. The framework may beinterpreted as minimizing a rigorously defined Kullback-Leibler divergencebetween the approximating and posterior processes. To our knowledge thisconnection has thus far gone unremarked in the literature. In this paper wegive a substantial generalization of the literature on this topic. We give anew proof of the result for infinite index sets which allows inducing pointsthat are not data points and likelihoods that depend on all function values. Wethen discuss augmented index sets and show that, contrary to previous works,marginal consistency of augmentation is not enough to guarantee consistency ofvariational inference with the original model. We then characterize an extracondition where such a guarantee is obtainable. Finally we show how ourframework sheds light on interdomain sparse approximations and sparseapproximations for Cox processes.
arxiv-14100-287 | Locally Adaptive Translation for Knowledge Graph Embedding | http://arxiv.org/pdf/1512.01370v1.pdf | author:Yantao Jia, Yuanzhuo Wang, Hailun Lin, Xiaolong Jin, Xueqi Cheng category:cs.AI cs.CL I.2.4; I.2.6 published:2015-12-04 summary:Knowledge graph embedding aims to represent entities and relations in alarge-scale knowledge graph as elements in a continuous vector space. Existingmethods, e.g., TransE and TransH, learn embedding representation by defining aglobal margin-based loss function over the data. However, the optimal lossfunction is determined during experiments whose parameters are examined among aclosed set of candidates. Moreover, embeddings over two knowledge graphs withdifferent entities and relations share the same set of candidate lossfunctions, ignoring the locality of both graphs. This leads to the limitedperformance of embedding related applications. In this paper, we propose alocally adaptive translation method for knowledge graph embedding, calledTransA, to find the optimal loss function by adaptively determining its marginover different knowledge graphs. Experiments on two benchmark data setsdemonstrate the superiority of the proposed method, as compared tothe-state-of-the-art ones.
arxiv-14100-288 | Proposition of a Theoretical Model for Missing Data Imputation using Deep Learning and Evolutionary Algorithms | http://arxiv.org/pdf/1512.01362v1.pdf | author:Collins Leke, Tshilidzi Marwala, Satyakama Paul category:cs.NE cs.LG published:2015-12-04 summary:In the last couple of decades, there has been major advancements in thedomain of missing data imputation. The techniques in the domain include amongstothers: Expectation Maximization, Neural Networks with Evolutionary Algorithmsor optimization techniques and K-Nearest Neighbor approaches to solve theproblem. The presence of missing data entries in databases render the tasks ofdecision-making and data analysis nontrivial. As a result this area hasattracted a lot of research interest with the aim being to yield accurate andtime efficient and sensitive missing data imputation techniques especially whentime sensitive applications are concerned like power plants and windingprocesses. In this article, considering arbitrary and monotone missing datapatterns, we hypothesize that the use of deep neural networks built usingautoencoders and denoising autoencoders in conjunction with genetic algorithms,swarm intelligence and maximum likelihood estimator methods as novel dataimputation techniques will lead to better imputed values than existingtechniques. Also considered are the missing at random, missing completely atrandom and missing not at random missing data mechanisms. We also intend to usefuzzy logic in tandem with deep neural networks to perform the missing dataimputation tasks, as well as different building blocks for the deep neuralnetworks like Stacked Restricted Boltzmann Machines and Deep Belief Networks totest our hypothesis. The motivation behind this article is the need for missingdata imputation techniques that lead to better imputed values than existingmethods with higher accuracies and lower errors.
arxiv-14100-289 | Q-Networks for Binary Vector Actions | http://arxiv.org/pdf/1512.01332v1.pdf | author:Naoto Yoshida category:cs.NE cs.LG published:2015-12-04 summary:In this paper reinforcement learning with binary vector actions wasinvestigated. We suggest an effective architecture of the neural networks forapproximating an action-value function with binary vector actions. The proposedarchitecture approximates the action-value function by a linear function withrespect to the action vector, but is still non-linear with respect to the stateinput. We show that this approximation method enables the efficient calculationof greedy action selection and softmax action selection. Using thisarchitecture, we suggest an online algorithm based on Q-learning. The empiricalresults in the grid world and the blocker task suggest that our approximationarchitecture would be effective for the RL problems with large discrete actionsets.
arxiv-14100-290 | Asynchronous Distributed Semi-Stochastic Gradient Optimization | http://arxiv.org/pdf/1508.01633v2.pdf | author:Ruiliang Zhang, Shuai Zheng, James T. Kwok category:cs.LG cs.DC published:2015-08-07 summary:With the recent proliferation of large-scale learning problems,there havebeen a lot of interest on distributed machine learning algorithms, particularlythose that are based on stochastic gradient descent (SGD) and its variants.However, existing algorithms either suffer from slow convergence due to theinherent variance of stochastic gradients, or have a fast linear convergencerate but at the expense of poorer solution quality. In this paper, we combinetheir merits by proposing a fast distributed asynchronous SGD-based algorithmwith variance reduction. A constant learning rate can be used, and it is alsoguaranteed to converge linearly to the optimal solution. Experiments on theGoogle Cloud Computing Platform demonstrate that the proposed algorithmoutperforms state-of-the-art distributed asynchronous algorithms in terms ofboth wall clock time and solution quality.
arxiv-14100-291 | Toward a Taxonomy and Computational Models of Abnormalities in Images | http://arxiv.org/pdf/1512.01325v1.pdf | author:Babak Saleh, Ahmed Elgammal, Jacob Feldman, Ali Farhadi category:cs.CV cs.AI cs.HC cs.IT cs.LG math.IT published:2015-12-04 summary:The human visual system can spot an abnormal image, and reason about whatmakes it strange. This task has not received enough attention in computervision. In this paper we study various types of atypicalities in images in amore comprehensive way than has been done before. We propose a new dataset ofabnormal images showing a wide range of atypicalities. We design human subjectexperiments to discover a coarse taxonomy of the reasons for abnormality. Ourexperiments reveal three major categories of abnormality: object-centric,scene-centric, and contextual. Based on this taxonomy, we propose acomprehensive computational model that can predict all different types ofabnormality in images and outperform prior arts in abnormality recognition.
arxiv-14100-292 | An Online Unsupervised Structural Plasticity Algorithm for Spiking Neural Networks | http://arxiv.org/pdf/1512.01314v1.pdf | author:Subhrajit Roy, Arindam Basu category:cs.NE published:2015-12-04 summary:In this article, we propose a novel Winner-Take-All (WTA) architectureemploying neurons with nonlinear dendrites and an online unsupervisedstructural plasticity rule for training it. Further, to aid hardwareimplementations, our network employs only binary synapses. The proposedlearning rule is inspired by spike time dependent plasticity (STDP) but differsfor each dendrite based on its activation level. It trains the WTA networkthrough formation and elimination of connections between inputs and synapses.To demonstrate the performance of the proposed network and learning rule, weemploy it to solve two, four and six class classification of random Poissonspike time inputs. The results indicate that by proper tuning of the inhibitorytime constant of the WTA, a trade-off between specificity and sensitivity ofthe network can be achieved. We use the inhibitory time constant to set thenumber of subpatterns per pattern we want to detect. We show that while thepercentage of successful trials are 92%, 88% and 82% for two, four and sixclass classification when no pattern subdivisions are made, it increases to100% when each pattern is subdivided into 5 or 10 subpatterns. However, theformer scenario of no pattern subdivision is more jitter resilient than thelater ones.
arxiv-14100-293 | Predicting psychological attributions from face photographs with a deep neural network | http://arxiv.org/pdf/1512.01289v1.pdf | author:Edward Grant, Stephan Sahm, Mariam Zabihi, Marcel van Gerven category:cs.CV cs.LG cs.NE published:2015-12-04 summary:Judgements about personality based on facial appearance are strong effectorsin social decision making and are known to impact on areas from presidentialelections to jury decisions. Recent work has shown that it is possible topredict perception of memorability, trustworthiness, intelligence and otherattributes in human face images. The most successful of these approachesrequires face images expertly annotated with key facial landmarks. Wedemonstrate a Convolutional Neural Network (CNN) model that is able perform thesame task without the need for landmark features thereby greatly increasingefficiency. The model has high accuracy, surpassing human level performance insome cases. Furthermore, we use a deconvolutional approach to visualizeimportant features for perception of 22 attributes and show that these can bedescribed as a composites of their positive and negative components byseparately visualizing both.
arxiv-14100-294 | Adjusting for Chance Clustering Comparison Measures | http://arxiv.org/pdf/1512.01286v1.pdf | author:Simone Romano, Nguyen Xuan Vinh, James Bailey, Karin Verspoor category:stat.ML published:2015-12-03 summary:Adjusted for chance measures are widely used to comparepartitions/clusterings of the same data set. In particular, the Adjusted RandIndex (ARI) based on pair-counting, and the Adjusted Mutual Information (AMI)based on Shannon information theory are very popular in the clusteringcommunity. Nonetheless it is an open problem as to what are the bestapplication scenarios for each measure and guidelines in the literature fortheir usage are sparse, with the result that users often resort to using both.Generalized Information Theoretic (IT) measures based on the Tsallis entropyhave been shown to link pair-counting and Shannon IT measures. In this paper,we aim to bridge the gap between adjustment of measures based on pair-countingand measures based on information theory. We solve the key technical challengeof analytically computing the expected value and variance of generalized ITmeasures. This allows us to propose adjustments of generalized IT measures,which reduce to well known adjusted clustering comparison measures as specialcases. Using the theory of generalized IT measures, we are able to propose thefollowing guidelines for using ARI and AMI as external validation indices: ARIshould be used when the reference clustering has large equal sized clusters;AMI should be used when the reference clustering is unbalanced and there existsmall clusters.
arxiv-14100-295 | Predicting the top and bottom ranks of billboard songs using Machine Learning | http://arxiv.org/pdf/1512.01283v1.pdf | author:Vivek Datla, Abhinav Vishnu category:cs.CL cs.LG published:2015-12-03 summary:The music industry is a $130 billion industry. Predicting whether a songcatches the pulse of the audience impacts the industry. In this paper weanalyze language inside the lyrics of the songs using several computationallinguistic algorithms and predict whether a song would make to the top orbottom of the billboard rankings based on the language features. We trained andtested an SVM classifier with a radial kernel function on the linguisticfeatures. Results indicate that we can classify whether a song belongs to topand bottom of the billboard charts with a precision of 0.76.
arxiv-14100-296 | MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems | http://arxiv.org/pdf/1512.01274v1.pdf | author:Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, Zheng Zhang category:cs.DC cs.LG cs.MS cs.NE published:2015-12-03 summary:MXNet is a multi-language machine learning (ML) library to ease thedevelopment of ML algorithms, especially for deep neural networks. Embedded inthe host language, it blends declarative symbolic expression with imperativetensor computation. It offers auto differentiation to derive gradients. MXNetis computation and memory efficient and runs on various heterogeneous systems,ranging from mobile devices to distributed GPU clusters. This paper describes both the API design and the system implementation ofMXNet, and explains how embedding of both symbolic expression and tensoroperation is handled in a unified fashion. Our preliminary experiments revealpromising results on large scale deep neural network applications usingmultiple GPU machines.
arxiv-14100-297 | CrossCat: A Fully Bayesian Nonparametric Method for Analyzing Heterogeneous, High Dimensional Data | http://arxiv.org/pdf/1512.01272v1.pdf | author:Vikash Mansinghka, Patrick Shafto, Eric Jonas, Cap Petschulat, Max Gasner, Joshua B. Tenenbaum category:cs.AI stat.CO stat.ML published:2015-12-03 summary:There is a widespread need for statistical methods that can analyzehigh-dimensional datasets with- out imposing restrictive or opaque modelingassumptions. This paper describes a domain-general data analysis method calledCrossCat. CrossCat infers multiple non-overlapping views of the data, eachconsisting of a subset of the variables, and uses a separate nonparametricmixture to model each view. CrossCat is based on approximately Bayesianinference in a hierarchical, nonparamet- ric model for data tables. This modelconsists of a Dirichlet process mixture over the columns of a data table inwhich each mixture component is itself an independent Dirichlet process mixtureover the rows; the inner mixture components are simple parametric models whoseform depends on the types of data in the table. CrossCat combines strengths ofmixture modeling and Bayesian net- work structure learning. Like mixturemodeling, CrossCat can model a broad class of distributions by positing latentvariables, and produces representations that can be efficiently conditioned andsampled from for prediction. Like Bayesian networks, CrossCat represents thedependencies and independencies between variables, and thus remains accuratewhen there are multiple statistical signals. Inference is done via a scalableGibbs sampling scheme; this paper shows that it works well in practice. Thispaper also includes empirical results on heterogeneous tabular data of up to 10million cells, such as hospital cost and quality measures, voting records,unemployment rates, gene expression measurements, and images of handwrittendigits. CrossCat infers structure that is consistent with accepted findings andcommon-sense knowledge in multiple domains and yields predictive accuracycompetitive with generative, discriminative, and model-free alternatives.
arxiv-14100-298 | MERLiN: Mixture Effect Recovery in Linear Networks | http://arxiv.org/pdf/1512.01255v1.pdf | author:Sebastian Weichwald, Moritz Grosse-Wentrup, Arthur Gretton category:stat.ME q-bio.NC stat.AP stat.ML published:2015-12-03 summary:Causal inference concerns the identification of cause-effect relationshipsbetween variables. However, often only a linear combination of variablesconstitutes a meaningful causal variable. We propose to construct causalvariables from non-causal variables such that the resulting statisticalproperties guarantee meaningful cause-effect relationships. Exploiting thisnovel idea, MERLiN is able to recover a causal variable from an observed linearmixture that is an effect of another given variable. We illustrate how to adaptthe algorithm to a particular domain and how to incorporate a priori knowledge.Evaluation on both synthetic and experimental EEG data indicates MERLiN's powerto infer cause-effect relationships.
arxiv-14100-299 | Mind the duality gap: safer rules for the Lasso | http://arxiv.org/pdf/1505.03410v3.pdf | author:Olivier Fercoq, Alexandre Gramfort, Joseph Salmon category:stat.ML cs.LG math.OC stat.CO published:2015-05-13 summary:Screening rules allow to early discard irrelevant variables from theoptimization in Lasso problems, or its derivatives, making solvers faster. Inthis paper, we propose new versions of the so-called $\textit{safe rules}$ forthe Lasso. Based on duality gap considerations, our new rules create safe testregions whose diameters converge to zero, provided that one relies on aconverging solver. This property helps screening out more variables, for awider range of regularization parameter values. In addition to fasterconvergence, we prove that we correctly identify the active sets (supports) ofthe solutions in finite time. While our proposed strategy can cope with anysolver, its performance is demonstrated using a coordinate descent algorithmparticularly adapted to machine learning use cases. Significant computing timereductions are obtained with respect to previous safe rules.
arxiv-14100-300 | Prototypical Priors: From Improving Classification to Zero-Shot Learning | http://arxiv.org/pdf/1512.01192v1.pdf | author:Saumya Jetley, Bernardino Romera-Paredes, Sadeep Jayasumana, Philip Torr category:cs.CV published:2015-12-03 summary:Recent works on zero-shot learning make use of side information such asvisual attributes or natural language semantics to define the relations betweenoutput visual classes and then use these relationships to draw inference on newunseen classes at test time. In a novel extension to this idea, we propose theuse of visual prototypical concepts as side information. For most real-worldvisual object categories, it may be difficult to establish a unique prototype.However, in cases such as traffic signs, brand logos, flags, and even naturallanguage characters, these prototypical templates are available and can beleveraged for an improved recognition performance. The present work proposes away to incorporate this prototypical information in a deep learning framework.Using prototypes as prior information, the deepnet pipeline learns the inputimage projections into the prototypical embedding space subject to minimizationof the final classification loss. Based on our experiments with two differentdatasets of traffic signs and brand logos, prototypical embeddings incorporatedin a conventional convolutional neural network improve the recognitionperformance. Recognition accuracy on the Belga logo dataset is especiallynoteworthy and establishes a new state-of-the-art. In zero-shot learningscenarios, the same system can be directly deployed to draw inference on unseenclasses by simply adding the prototypical information for these new classes attest time. Thus, unlike earlier approaches, testing on seen and unseen classesis handled using the same pipeline, and the system can be tuned for a trade-offof seen and unseen class performance as per task requirement. Comparison withone of the latest works in the zero-shot learning domain yields top results onthe two datasets mentioned above.
