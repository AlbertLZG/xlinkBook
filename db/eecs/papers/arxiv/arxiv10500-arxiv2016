arxiv-10500-1 | LSTM: A Search Space Odyssey | http://arxiv.org/pdf/1503.04069v1.pdf | author:Klaus Greff, Rupesh Kumar Srivastava, Jan Koutník, Bas R. Steunebrink, Jürgen Schmidhuber category:cs.NE cs.LG 68T10 published:2015-03-13 summary:Several variants of the Long Short-Term Memory (LSTM) architecture forrecurrent neural networks have been proposed since its inception in 1995. Inrecent years, these networks have become the state-of-the-art models for avariety of machine learning problems. This has led to a renewed interest inunderstanding the role and utility of various computational components oftypical LSTM variants. In this paper, we present the first large-scale analysisof eight LSTM variants on three representative tasks: speech recognition,handwriting recognition, and polyphonic music modeling. The hyperparameters ofall LSTM variants for each task were optimized separately using random searchand their importance was assessed using the powerful fANOVA framework. Intotal, we summarize the results of 5400 experimental runs (about 15 years ofCPU time), which makes our study the largest of its kind on LSTM networks. Ourresults show that none of the variants can improve upon the standard LSTMarchitecture significantly, and demonstrate the forget gate and the outputactivation function to be its most critical components. We further observe thatthe studied hyperparameters are virtually independent and derive guidelines fortheir efficient adjustment.
arxiv-10500-2 | Exploiting Image-trained CNN Architectures for Unconstrained Video Classification | http://arxiv.org/pdf/1503.04144v3.pdf | author:Shengxin Zha, Florian Luisier, Walter Andrews, Nitish Srivastava, Ruslan Salakhutdinov category:cs.CV published:2015-03-13 summary:We conduct an in-depth exploration of different strategies for doing eventdetection in videos using convolutional neural networks (CNNs) trained forimage classification. We study different ways of performing spatial andtemporal pooling, feature normalization, choice of CNN layers as well as choiceof classifiers. Making judicious choices along these dimensions led to a verysignificant increase in performance over more naive approaches that have beenused till now. We evaluate our approach on the challenging TRECVID MED'14dataset with two popular CNN architectures pretrained on ImageNet. On thisMED'14 dataset, our methods, based entirely on image-trained CNN features, canoutperform several state-of-the-art non-CNN models. Our proposed late fusion ofCNN- and motion-based features can further increase the mean average precision(mAP) on MED'14 from 34.95% to 38.74%. The fusion approach achieves thestate-of-the-art classification performance on the challenging UCF-101 dataset.
arxiv-10500-3 | Sparse Code Formation with Linear Inhibition | http://arxiv.org/pdf/1503.04115v1.pdf | author:Nam Do-Hoang Le category:cs.CV published:2015-03-13 summary:Sparse code formation in the primary visual cortex (V1) has been inspirationfor many state-of-the-art visual recognition systems. To stimulate thisbehavior, networks are trained networks under mathematical constraint ofsparsity or selectivity. In this paper, the authors exploit another approachwhich uses lateral interconnections in feature learning networks. However,instead of adding direct lateral interconnections among neurons, we introducean inhibitory layer placed right after normal encoding layer. This ideaovercomes the challenge of computational cost and complexity on lateralnetworks while preserving crucial objective of sparse code formation. Todemonstrate this idea, we use sparse autoencoder as normal encoding layer andapply inhibitory layer. Early experiments in visual recognition show relativeimprovements over traditional approach on CIFAR-10 dataset. Moreover, simpleinstallment and training process using Hebbian rule allow inhibitory layer tobe integrated into existing networks, which enables further analysis in thefuture.
arxiv-10500-4 | Hybrid multi-layer Deep CNN/Aggregator feature for image classification | http://arxiv.org/pdf/1503.04065v1.pdf | author:Praveen Kulkarni, Joaquin Zepeda, Frederic Jurie, Patrick Perez, Louis Chevallier category:cs.CV published:2015-03-13 summary:Deep Convolutional Neural Networks (DCNN) have established a remarkableperformance benchmark in the field of image classification, displacingclassical approaches based on hand-tailored aggregations of local descriptors.Yet DCNNs impose high computational burdens both at training and at testingtime, and training them requires collecting and annotating large amounts oftraining data. Supervised adaptation methods have been proposed in theliterature that partially re-learn a transferred DCNN structure from a newtarget dataset. Yet these require expensive bounding-box annotations and arestill computationally expensive to learn. In this paper, we address theseshortcomings of DCNN adaptation schemes by proposing a hybrid approach thatcombines conventional, unsupervised aggregators such as Bag-of-Words (BoW),with the DCNN pipeline by treating the output of intermediate layers as denselyextracted local descriptors. We test a variant of our approach that uses only intermediate DCNN layers onthe standard PASCAL VOC 2007 dataset and show performance significantly higherthan the standard BoW model and comparable to Fisher vector aggregation butwith a feature that is 150 times smaller. A second variant of our approach thatincludes the fully connected DCNN layers significantly outperforms Fishervector schemes and performs comparably to DCNN approaches adapted to Pascal VOC2007, yet at only a small fraction of the training and testing cost.
arxiv-10500-5 | Characterizing driving behavior using automatic visual analysis | http://arxiv.org/pdf/1503.04036v1.pdf | author:Mrinal Haloi, Dinesh Babu Jayagopi category:cs.CV H.4.3 published:2015-03-13 summary:In this work, we present the problem of rash driving detection algorithmusing a single wide angle camera sensor, particularly useful in the Indiancontext. To our knowledge this rash driving problem has not been addressedusing Image processing techniques (existing works use other sensors such asaccelerometer). Car Image processing literature, though rich and mature, doesnot address the rash driving problem. In this work-in-progress paper, wepresent the need to address this problem, our approach and our future plans tobuild a rash driving detector.
arxiv-10500-6 | Image patch analysis of sunspots and active regions. I. Intrinsic dimension and correlation analysis | http://arxiv.org/pdf/1503.04127v2.pdf | author:Kevin R. Moon, Jimmy J. Li, Veronique Delouille, Ruben De Visscher, Fraser Watson, Alfred O. Hero III category:astro-ph.SR cs.CV published:2015-03-13 summary:The flare-productivity of an active region is observed to be related to itsspatial complexity. Mount Wilson or McIntosh sunspot classifications measuresuch complexity but in a categorical way, and may therefore not use all theinformation present in the observations. Moreover, such categorical schemeshinder a systematic study of an active region's evolution for example. Wepropose fine-scale quantitative descriptors for an active region's complexityand relate them to the Mount Wilson classification. We analyze the localcorrelation structure within continuum and magnetogram data, as well as thecross-correlation between continuum and magnetogram data. We compute theintrinsic dimension, partial correlation, and canonical correlation analysis(CCA) of image patches of continuum and magnetogram active region images takenfrom the SOHO-MDI instrument. We use masks of sunspots derived from continuumas well as larger masks of magnetic active regions derived from the magnetogramto analyze separately the core part of an active region from its surroundingpart. We find the relationship between complexity of an active region asmeasured by Mount Wilson and the intrinsic dimension of its image patches.Partial correlation patterns exhibit approximately a third-order Markovstructure. CCA reveals different patterns of correlation between continuum andmagnetogram within the sunspots and in the region surrounding the sunspots.These results also pave the way for patch-based dictionary learning with a viewtowards automatic clustering of active regions.
arxiv-10500-7 | Interactive Restless Multi-armed Bandit Game and Swarm Intelligence Effect | http://arxiv.org/pdf/1503.03964v1.pdf | author:Shunsuke Yoshida, Masato Hisakado, Shintaro Mori category:cs.AI cs.LG stat.ML published:2015-03-13 summary:We obtain the conditions for the emergence of the swarm intelligence effectin an interactive game of restless multi-armed bandit (rMAB). A player competeswith multiple agents. Each bandit has a payoff that changes with a probability$p_{c}$ per round. The agents and player choose one of three options: (1)Exploit (a good bandit), (2) Innovate (asocial learning for a good bandit among$n_{I}$ randomly chosen bandits), and (3) Observe (social learning for a goodbandit). Each agent has two parameters $(c,p_{obs})$ to specify the decision:(i) $c$, the threshold value for Exploit, and (ii) $p_{obs}$, the probabilityfor Observe in learning. The parameters $(c,p_{obs})$ are uniformlydistributed. We determine the optimal strategies for the player using completeknowledge about the rMAB. We show whether or not social or asocial learning ismore optimal in the $(p_{c},n_{I})$ space and define the swarm intelligenceeffect. We conduct a laboratory experiment (67 subjects) and observe the swarmintelligence effect only if $(p_{c},n_{I})$ are chosen so that social learningis far more optimal than asocial learning.
arxiv-10500-8 | An implementation of Apertium based Assamese morphological analyzer | http://arxiv.org/pdf/1503.03989v1.pdf | author:Mirzanur Rahman, Shikhar Kumar Sarma category:cs.CL published:2015-03-13 summary:Morphological Analysis is an important branch of linguistics for any NaturalLanguage Processing Technology. Morphology studies the word structure andformation of word of a language. In current scenario of NLP research,morphological analysis techniques have become more popular day by day. Forprocessing any language, morphology of the word should be first analyzed.Assamese language contains very complex morphological structure. In our work wehave used Apertium based Finite-State-Transducers for developing morphologicalanalyzer for Assamese Language with some limited domain and we get 72.7%accuracy
arxiv-10500-9 | Deep Unsupervised Learning using Nonequilibrium Thermodynamics | http://arxiv.org/pdf/1503.03585v8.pdf | author:Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli category:cs.LG q-bio.NC stat.ML published:2015-03-12 summary:A central problem in machine learning involves modeling complex data-setsusing highly flexible families of probability distributions in which learning,sampling, inference, and evaluation are still analytically or computationallytractable. Here, we develop an approach that simultaneously achieves bothflexibility and tractability. The essential idea, inspired by non-equilibriumstatistical physics, is to systematically and slowly destroy structure in adata distribution through an iterative forward diffusion process. We then learna reverse diffusion process that restores structure in data, yielding a highlyflexible and tractable generative model of the data. This approach allows us torapidly learn, sample from, and evaluate probabilities in deep generativemodels with thousands of layers or time steps, as well as to computeconditional and posterior probabilities under the learned model. Weadditionally release an open source reference implementation of the algorithm.
arxiv-10500-10 | Diagnosing Heterogeneous Dynamics for CT Scan Images of Human Brain in Wavelet and MFDFA domain | http://arxiv.org/pdf/1503.03913v1.pdf | author:Sabyasachi Mukhopadhyay, Soham Mandal, Nandan K Das, Subhadip Dey, Asish Mitra, Nirmalya Ghosh, Prasanta K Panigrahi category:cs.CV published:2015-03-12 summary:CT scan images of human brain of a particular patient in different crosssections are taken, on which wavelet transform and multi-fractal analysis areapplied. The vertical and horizontal unfolding of images are done beforeanalyzing these images. A systematic investigation of de-noised CT scan imagesof human brain in different cross-sections are carried out through waveletnormalized energy and wavelet semi-log plots, which clearly points out themismatch between results of vertical and horizontal unfolding. The mismatch ofresults confirms the heterogeneity in spatial domain. Using the multi-fractalde-trended fluctuation analysis (MFDFA), the mismatch between the values ofHurst exponent and width of singularity spectrum by vertical and horizontalunfolding confirms the same.
arxiv-10500-11 | Approximating Sparse PCA from Incomplete Data | http://arxiv.org/pdf/1503.03903v1.pdf | author:Abhisek Kundu, Petros Drineas, Malik Magdon-Ismail category:cs.LG cs.IT cs.NA math.IT stat.ML published:2015-03-12 summary:We study how well one can recover sparse principal components of a datamatrix using a sketch formed from a few of its elements. We show that for awide class of optimization problems, if the sketch is close (in the spectralnorm) to the original data matrix, then one can recover a near optimal solutionto the optimization problem by using the sketch. In particular, we use thisapproach to obtain sparse principal components and show that for \math{m} datapoints in \math{n} dimensions, \math{O(\epsilon^{-2}\tilde k\max\{m,n\})}elements gives an \math{\epsilon}-additive approximation to the sparse PCAproblem (\math{\tilde k} is the stable rank of the data matrix). We demonstrateour algorithms extensively on image, text, biological and financial data. Theresults show that not only are we able to recover the sparse PCAs from theincomplete data, but by using our sparse sketch, the running time drops by afactor of five or more.
arxiv-10500-12 | Training Binary Multilayer Neural Networks for Image Classification using Expectation Backpropagation | http://arxiv.org/pdf/1503.03562v3.pdf | author:Zhiyong Cheng, Daniel Soudry, Zexi Mao, Zhenzhong Lan category:cs.NE cs.CV cs.LG published:2015-03-12 summary:Compared to Multilayer Neural Networks with real weights, Binary MultilayerNeural Networks (BMNNs) can be implemented more efficiently on dedicatedhardware. BMNNs have been demonstrated to be effective on binary classificationtasks with Expectation BackPropagation (EBP) algorithm on high dimensional textdatasets. In this paper, we investigate the capability of BMNNs using the EBPalgorithm on multiclass image classification tasks. The performances of binaryneural networks with multiple hidden layers and different numbers of hiddenunits are examined on MNIST. We also explore the effectiveness of image spatialfilters and the dropout technique in BMNNs. Experimental results on MNISTdataset show that EBP can obtain 2.12% test error with binary weights and 1.66%test error with real weights, which is comparable to the results of standardBackPropagation algorithm on fully connected MNNs.
arxiv-10500-13 | Compact Nonlinear Maps and Circulant Extensions | http://arxiv.org/pdf/1503.03893v1.pdf | author:Felix X. Yu, Sanjiv Kumar, Henry Rowley, Shih-Fu Chang category:stat.ML cs.LG published:2015-03-12 summary:Kernel approximation via nonlinear random feature maps is widely used inspeeding up kernel machines. There are two main challenges for the conventionalkernel approximation methods. First, before performing kernel approximation, agood kernel has to be chosen. Picking a good kernel is a very challengingproblem in itself. Second, high-dimensional maps are often required in order toachieve good performance. This leads to high computational cost in bothgenerating the nonlinear maps, and in the subsequent learning and predictionprocess. In this work, we propose to optimize the nonlinear maps directly withrespect to the classification objective in a data-dependent fashion. Theproposed approach achieves kernel approximation and kernel learning in a jointframework. This leads to much more compact maps without hurting theperformance. As a by-product, the same framework can also be used to achievemore compact kernel maps to approximate a known kernel. We also introduceCirculant Nonlinear Maps, which uses a circulant-structured projection matrixto speed up the nonlinear maps for high-dimensional data.
arxiv-10500-14 | Hierarchical learning of grids of microtopics | http://arxiv.org/pdf/1503.03701v3.pdf | author:Nebojsa Jojic, Alessandro Perina, Dongwoo Kim category:stat.ML cs.IR cs.LG published:2015-03-12 summary:The counting grid is a grid of microtopics, sparse word/featuredistributions. The generative model associated with the grid does not use thesemicrotopics individually. Rather, it groups them in overlapping rectangularwindows and uses these grouped microtopics as either mixture or admixturecomponents. This paper builds upon the basic counting grid model and it showsthat hierarchical reasoning helps avoid bad local minima, produces betterclassification accuracy and, most interestingly, allows for extraction of largenumbers of coherent microtopics even from small datasets. We evaluate this interms of consistency, diversity and clarity of the indexed content, as well asin a user study on word intrusion tasks. We demonstrate that these models workwell as a technique for embedding raw images and discuss interesting parallelsbetween hierarchical CG models and other deep architectures.
arxiv-10500-15 | Qualitative inequalities for squared partial correlations of a Gaussian random vector | http://arxiv.org/pdf/1503.03879v1.pdf | author:Sanjay Chaudhuri category:math.ST stat.AP stat.ME stat.ML stat.TH published:2015-03-12 summary:We describe various sets of conditional independence relationships,sufficient for qualitatively comparing non-vanishing squared partialcorrelations of a Gaussian random vector. These sufficient conditions aresatisfied by several graphical Markov models. Rules for comparing degree ofassociation among the vertices of such Gaussian graphical models are alsodeveloped. We apply these rules to compare conditional dependencies on Gaussiantrees. In particular for trees, we show that such dependence can be completelycharacterized by the length of the paths joining the dependent vertices to eachother and to the vertices conditioned on. We also apply our results topostulate rules for model selection for polytree models. Our rules apply tomutual information of Gaussian random vectors as well.
arxiv-10500-16 | Learning to Detect Vehicles by Clustering Appearance Patterns | http://arxiv.org/pdf/1503.03771v1.pdf | author:Eshed Ohn-Bar, Mohan M. Trivedi category:cs.CV published:2015-03-12 summary:This paper studies efficient means for dealing with intra-category diversityin object detection. Strategies for occlusion and orientation handling areexplored by learning an ensemble of detection models from visual andgeometrical clusters of object instances. An AdaBoost detection scheme isemployed with pixel lookup features for fast detection. The analysis providesinsight into the design of a robust vehicle detection system, showing promisein terms of detection performance and orientation estimation accuracy.
arxiv-10500-17 | 2D Face Recognition System Based on Selected Gabor Filters and Linear Discriminant Analysis LDA | http://arxiv.org/pdf/1503.03741v1.pdf | author:Samir F. Hafez, Mazen M. Selim, Hala H. Zayed category:cs.CV published:2015-03-12 summary:We present a new approach for face recognition system. The method is based on2D face image features using subset of non-correlated and Orthogonal GaborFilters instead of using the whole Gabor Filter Bank, then compressing theoutput feature vector using Linear Discriminant Analysis (LDA). The face imagehas been enhanced using multi stage image processing technique to normalize itand compensate for illumination variation. Experimental results show that theproposed system is effective for both dimension reduction and good recognitionperformance when compared to the complete Gabor filter bank. The system hasbeen tested using CASIA, ORL and Cropped YaleB 2D face images Databases andachieved average recognition rate of 98.9 %.
arxiv-10500-18 | Starting engagement detection towards a companion robot using multimodal features | http://arxiv.org/pdf/1503.03732v1.pdf | author:Dominique Vaufreydaz, Wafa Johal, Claudine Combe category:cs.RO cs.CV published:2015-03-12 summary:Recognition of intentions is a subconscious cognitive process vital to humancommunication. This skill enables anticipation and increases the quality ofinteractions between humans. Within the context of engagement, non-verbalsignals are used to communicate the intention of starting the interaction witha partner. In this paper, we investigated methods to detect these signals inorder to allow a robot to know when it is about to be addressed. Originality ofour approach resides in taking inspiration from social and cognitive sciencesto perform our perception task. We investigate meaningful features, i.e. humanreadable features, and elicit which of these are important for recognizingsomeone's intention of starting an interaction. Classically, spatialinformation like the human position and speed, the human-robot distance areused to detect the engagement. Our approach integrates multimodal featuresgathered using a companion robot equipped with a Kinect. The evaluation on ourcorpus collected in spontaneous conditions highlights its robustness andvalidates the use of such a technique in a real environment. Experimentalvalidation shows that multimodal features set gives better precision and recallthan using only spatial and speed features. We also demonstrate that 7 selectedfeatures are sufficient to provide a good starting engagement detection score.In our last investigation, we show that among our full 99 features set, thespace reduction is not a solved task. This result opens new researchesperspectives on multimodal engagement detection.
arxiv-10500-19 | Functional Inverse Regression in an Enlarged Dimension Reduction Space | http://arxiv.org/pdf/1503.03673v1.pdf | author:Ting-Li Chen, Su-Yun Huang, Yanyuan Ma, I-Ping Tu category:math.ST stat.ML stat.TH published:2015-03-12 summary:We consider an enlarged dimension reduction space in functional inverseregression. Our operator and functional analysis based approach facilitates acompact and rigorous formulation of the functional inverse regression problem.It also enables us to expand the possible space where the dimension reductionfunctions belong. Our formulation provides a unified framework so that theclassical notions, such as covariance standardization, Mahalanobis distance,SIR and linear discriminant analysis, can be naturally and smoothly carried outin our enlarged space. This enlarged dimension reduction space also links tothe linear discriminant space of Gaussian measures on a separable Hilbertspace.
arxiv-10500-20 | Designing A Composite Dictionary Adaptively From Joint Examples | http://arxiv.org/pdf/1503.03621v2.pdf | author:Zhangyang Wang, Yingzhen Yang, Jianchao Yang, Thomas S. Huang category:cs.CV published:2015-03-12 summary:We study the complementary behaviors of external and internal examples inimage restoration, and are motivated to formulate a composite dictionary designframework. The composite dictionary consists of the global part learned fromexternal examples, and the sample-specific part learned from internal examples.The dictionary atoms in both parts are further adaptively weighted to emphasizetheir model statistics. Experiments demonstrate that the joint utilization ofexternal and internal examples leads to substantial improvements, withsuccessful applications in image denoising and super resolution.
arxiv-10500-21 | Single image super-resolution by approximated Heaviside functions | http://arxiv.org/pdf/1503.03630v1.pdf | author:Liang-Jian Deng, Weihong Guo, Ting-Zhu Huang category:cs.CV cs.IT math.IT math.OC published:2015-03-12 summary:Image super-resolution is a process to enhance image resolution. It is widelyused in medical imaging, satellite imaging, target recognition, etc. In thispaper, we conduct continuous modeling and assume that the unknown imageintensity function is defined on a continuous domain and belongs to a spacewith a redundant basis. We propose a new iterative model for single imagesuper-resolution based on an observation: an image is consisted of smoothcomponents and non-smooth components, and we use two classes of approximatedHeaviside functions (AHFs) to represent them respectively. Due to sparsity ofthe non-smooth components, a $L_{1}$ model is employed. In addition, we applythe proposed iterative model to image patches to reduce computation andstorage. Comparisons with some existing competitive methods show theeffectiveness of the proposed method.
arxiv-10500-22 | On the Impossibility of Learning the Missing Mass | http://arxiv.org/pdf/1503.03613v1.pdf | author:Elchanan Mossel, Mesrob I. Ohannessian category:stat.ML cs.IT cs.LG math.IT math.PR math.ST stat.TH published:2015-03-12 summary:This paper shows that one cannot learn the probability of rare events withoutimposing further structural assumptions. The event of interest is that ofobtaining an outcome outside the coverage of an i.i.d. sample from a discretedistribution. The probability of this event is referred to as the "missingmass". The impossibility result can then be stated as: the missing mass is notdistribution-free PAC-learnable in relative error. The proof issemi-constructive and relies on a coupling argument using a dithered geometricdistribution. This result formalizes the folklore that in order to predict rareevents, one necessarily needs distributions with "heavy tails".
arxiv-10500-23 | Low-Level Features for Image Retrieval Based on Extraction of Directional Binary Patterns and Its Oriented Gradients Histogram | http://arxiv.org/pdf/1503.03606v1.pdf | author:Nagaraja S., Prabhakar C. J. category:cs.CV cs.IR published:2015-03-12 summary:In this paper, we present a novel approach for image retrieval based onextraction of low level features using techniques such as Directional BinaryCode, Haar Wavelet transform and Histogram of Oriented Gradients. The DBCtexture descriptor captures the spatial relationship between any pair ofneighbourhood pixels in a local region along a given direction, while LocalBinary Patterns descriptor considers the relationship between a given pixel andits surrounding neighbours. Therefore, DBC captures more spatial informationthan LBP and its variants, also it can extract more edge information than LBP.Hence, we employ DBC technique in order to extract grey level texture featurefrom each RGB channels individually and computed texture maps are furthercombined which represents colour texture features of an image. Then, wedecomposed the extracted colour texture map and original image using Haarwavelet transform. Finally, we encode the shape and local features of wavelettransformed images using Histogram of Oriented Gradients for content basedimage retrieval. The performance of proposed method is compared with existingmethods on two databases such as Wang's corel image and Caltech 256. Theevaluation results show that our approach outperforms the existing methods forimage retrieval.
arxiv-10500-24 | Efficient Learning of Linear Separators under Bounded Noise | http://arxiv.org/pdf/1503.03594v1.pdf | author:Pranjal Awasthi, Maria-Florina Balcan, Nika Haghtalab, Ruth Urner category:cs.LG cs.CC published:2015-03-12 summary:We study the learnability of linear separators in $\Re^d$ in the presence ofbounded (a.k.a Massart) noise. This is a realistic generalization of the randomclassification noise model, where the adversary can flip each example $x$ withprobability $\eta(x) \leq \eta$. We provide the first polynomial time algorithmthat can learn linear separators to arbitrarily small excess error in thisnoise model under the uniform distribution over the unit ball in $\Re^d$, forsome constant value of $\eta$. While widely studied in the statistical learningtheory community in the context of getting faster convergence rates,computationally efficient algorithms in this model had remained elusive. Ourwork provides the first evidence that one can indeed design algorithmsachieving arbitrarily small excess error in polynomial time under thisrealistic noise model and thus opens up a new and exciting line of research. We additionally provide lower bounds showing that popular algorithms such ashinge loss minimization and averaging cannot lead to arbitrarily small excesserror under Massart noise, even under the uniform distribution. Our workinstead, makes use of a margin based technique developed in the context ofactive learning. As a result, our algorithm is also an active learningalgorithm with label complexity that is only a logarithmic the desired excesserror $\epsilon$.
arxiv-10500-25 | LINE: Large-scale Information Network Embedding | http://arxiv.org/pdf/1503.03578v1.pdf | author:Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, Qiaozhu Mei category:cs.LG published:2015-03-12 summary:This paper studies the problem of embedding very large information networksinto low-dimensional vector spaces, which is useful in many tasks such asvisualization, node classification, and link prediction. Most existing graphembedding methods do not scale for real world information networks whichusually contain millions of nodes. In this paper, we propose a novel networkembedding method called the "LINE," which is suitable for arbitrary types ofinformation networks: undirected, directed, and/or weighted. The methodoptimizes a carefully designed objective function that preserves both the localand global network structures. An edge-sampling algorithm is proposed thataddresses the limitation of the classical stochastic gradient descent andimproves both the effectiveness and the efficiency of the inference. Empiricalexperiments prove the effectiveness of the LINE on a variety of real-worldinformation networks, including language networks, social networks, andcitation networks. The algorithm is very efficient, which is able to learn theembedding of a network with millions of vertices and billions of edges in a fewhours on a typical single machine. The source code of the LINE is availableonline.
arxiv-10500-26 | On Computing the Translations Norm in the Epipolar Graph | http://arxiv.org/pdf/1503.03637v3.pdf | author:Federica Arrigoni, Beatrice Rossi, Andrea Fusiello category:cs.CV published:2015-03-12 summary:This paper deals with the problem of recovering the unknown norm of relativetranslations between cameras based on the knowledge of relative rotations andtranslation directions. We provide theoretical conditions for the solvabilityof such a problem, and we propose a two-stage method to solve it. First, acycle basis for the epipolar graph is computed, then all the scaling factorsare recovered simultaneously by solving a homogeneous linear system. Wedemonstrate the accuracy of our solution by means of synthetic and realexperiments.
arxiv-10500-27 | On Graduated Optimization for Stochastic Non-Convex Problems | http://arxiv.org/pdf/1503.03712v2.pdf | author:Elad Hazan, Kfir Y. Levy, Shai Shalev-Shwartz category:cs.LG math.OC 68 published:2015-03-12 summary:The graduated optimization approach, also known as the continuation method,is a popular heuristic to solving non-convex problems that has received renewedinterest over the last decade. Despite its popularity, very little is known interms of theoretical convergence analysis. In this paper we describe a newfirst-order algorithm based on graduated optimiza- tion and analyze itsperformance. We characterize a parameterized family of non- convex functionsfor which this algorithm provably converges to a global optimum. In particular,we prove that the algorithm converges to an {\epsilon}-approximate solutionwithin O(1/\epsilon^2) gradient-based steps. We extend our algorithm andanalysis to the setting of stochastic non-convex optimization with noisygradient feedback, attaining the same convergence rate. Additionally, wediscuss the setting of zero-order optimization, and devise a a variant of ouralgorithm which converges at rate of O(d^2/\epsilon^4).
arxiv-10500-28 | FaceNet: A Unified Embedding for Face Recognition and Clustering | http://arxiv.org/pdf/1503.03832v3.pdf | author:Florian Schroff, Dmitry Kalenichenko, James Philbin category:cs.CV published:2015-03-12 summary:Despite significant recent advances in the field of face recognition,implementing face verification and recognition efficiently at scale presentsserious challenges to current approaches. In this paper we present a system,called FaceNet, that directly learns a mapping from face images to a compactEuclidean space where distances directly correspond to a measure of facesimilarity. Once this space has been produced, tasks such as face recognition,verification and clustering can be easily implemented using standard techniqueswith FaceNet embeddings as feature vectors. Our method uses a deep convolutional network trained to directly optimize theembedding itself, rather than an intermediate bottleneck layer as in previousdeep learning approaches. To train, we use triplets of roughly aligned matching/ non-matching face patches generated using a novel online triplet miningmethod. The benefit of our approach is much greater representationalefficiency: we achieve state-of-the-art face recognition performance using only128-bytes per face. On the widely used Labeled Faces in the Wild (LFW) dataset, our systemachieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves95.12%. Our system cuts the error rate in comparison to the best publishedresult by 30% on both datasets. We also introduce the concept of harmonic embeddings, and a harmonic tripletloss, which describe different versions of face embeddings (produced bydifferent networks) that are compatible to each other and allow for directcomparison between each other.
arxiv-10500-29 | A mathematical motivation for complex-valued convolutional networks | http://arxiv.org/pdf/1503.03438v3.pdf | author:Joan Bruna, Soumith Chintala, Yann LeCun, Serkan Piantino, Arthur Szlam, Mark Tygert category:cs.LG cs.NE stat.ML published:2015-03-11 summary:A complex-valued convolutional network (convnet) implements the repeatedapplication of the following composition of three operations, recursivelyapplying the composition to an input vector of nonnegative real numbers: (1)convolution with complex-valued vectors followed by (2) taking the absolutevalue of every entry of the resulting vectors followed by (3) local averaging.For processing real-valued random vectors, complex-valued convnets can beviewed as "data-driven multiscale windowed power spectra," "data-drivenmultiscale windowed absolute spectra," "data-driven multiwavelet absolutevalues," or (in their most general configuration) "data-driven nonlinearmultiwavelet packets." Indeed, complex-valued convnets can calculate multiscalewindowed spectra when the convnet filters are windowed complex-valuedexponentials. Standard real-valued convnets, using rectified linear units(ReLUs), sigmoidal (for example, logistic or tanh) nonlinearities, max.pooling, etc., do not obviously exhibit the same exact correspondence withdata-driven wavelets (whereas for complex-valued convnets, the correspondenceis much more than just a vague analogy). Courtesy of the exact correspondence,the remarkably rich and rigorous body of mathematical analysis for waveletsapplies directly to (complex-valued) convnets.
arxiv-10500-30 | Deep Convolutional Inverse Graphics Network | http://arxiv.org/pdf/1503.03167v4.pdf | author:Tejas D. Kulkarni, Will Whitney, Pushmeet Kohli, Joshua B. Tenenbaum category:cs.CV cs.GR cs.LG cs.NE published:2015-03-11 summary:This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), amodel that learns an interpretable representation of images. Thisrepresentation is disentangled with respect to transformations such asout-of-plane rotations and lighting variations. The DC-IGN model is composed ofmultiple layers of convolution and de-convolution operators and is trainedusing the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose atraining procedure to encourage neurons in the graphics code layer to representa specific transformation (e.g. pose or light). Given a single input image, ourmodel can generate new images of the same object with variations in pose andlighting. We present qualitative and quantitative results of the model'sefficacy at learning a 3D rendering engine.
arxiv-10500-31 | Optimal prediction for sparse linear models? Lower bounds for coordinate-separable M-estimators | http://arxiv.org/pdf/1503.03188v2.pdf | author:Yuchen Zhang, Martin J. Wainwright, Michael I. Jordan category:math.ST stat.ML stat.TH published:2015-03-11 summary:For the problem of high-dimensional sparse linear regression, it is knownthat an $\ell_0$-based estimator can achieve a $1/n$ "fast" rate on theprediction error without any conditions on the design matrix, whereas inabsence of restrictive conditions on the design matrix, popular polynomial-timemethods only guarantee the $1/\sqrt{n}$ "slow" rate. In this paper, we showthat the slow rate is intrinsic to a broad class of M-estimators. Inparticular, for estimators based on minimizing a least-squares cost functiontogether with a (possibly non-convex) coordinate-wise separable regularizer,there is always a "bad" local optimum such that the associated prediction erroris lower bounded by a constant multiple of $1/\sqrt{n}$. For convexregularizers, this lower bound applies to all global optima. The theory isapplicable to many popular estimators, including convex $\ell_1$-based methodsas well as M-estimators based on nonconvex regularizers, including the SCADpenalty or the MCP regularizer. In addition, for a broad class of nonconvexregularizers, we show that the bad local optima are very common, in that abroad class of local minimization algorithms with random initialization willtypically converge to a bad solution.
arxiv-10500-32 | Convolutional Neural Network Architectures for Matching Natural Language Sentences | http://arxiv.org/pdf/1503.03244v1.pdf | author:Baotian Hu, Zhengdong Lu, Hang Li, Qingcai Chen category:cs.CL cs.LG cs.NE published:2015-03-11 summary:Semantic matching is of central importance to many natural language tasks\cite{bordes2014semantic,RetrievalQA}. A successful matching algorithm needs toadequately model the internal structures of language objects and theinteraction between them. As a step toward this goal, we propose convolutionalneural network models for matching two sentences, by adapting the convolutionalstrategy in vision and speech. The proposed models not only nicely representthe hierarchical structures of sentences with their layer-by-layer compositionand pooling, but also capture the rich matching patterns at different levels.Our models are rather generic, requiring no prior knowledge on language, andcan hence be applied to matching tasks of different nature and in differentlanguages. The empirical study on a variety of matching tasks demonstrates theefficacy of the proposed model on a variety of matching tasks and itssuperiority to competitor models.
arxiv-10500-33 | Learning Classifiers from Synthetic Data Using a Multichannel Autoencoder | http://arxiv.org/pdf/1503.03163v1.pdf | author:Xi Zhang, Yanwei Fu, Andi Zang, Leonid Sigal, Gady Agam category:cs.CV cs.LG published:2015-03-11 summary:We propose a method for using synthetic data to help learning classifiers.Synthetic data, even is generated based on real data, normally results in ashift from the distribution of real data in feature space. To bridge the gapbetween the real and synthetic data, and jointly learn from synthetic and realdata, this paper proposes a Multichannel Autoencoder(MCAE). We show that bysuing MCAE, it is possible to learn a better feature representation forclassification. To evaluate the proposed approach, we conduct experiments ontwo types of datasets. Experimental results on two datasets validate theefficiency of our MCAE model and our methodology of generating synthetic data.
arxiv-10500-34 | Benchmarking NLopt and state-of-art algorithms for Continuous Global Optimization via Hybrid IACO$_\mathbb{R}$ | http://arxiv.org/pdf/1503.03175v1.pdf | author:Udit Kumar, Sumit Soman, Jayadeva category:cs.NE 80M50 G.1.6 published:2015-03-11 summary:This paper presents a comparative analysis of the performance of theIncremental Ant Colony algorithm for continuous optimization($IACO_\mathbb{R}$), with different algorithms provided in the NLopt library.The key objective is to understand how the various algorithms in the NLoptlibrary perform in combination with the Multi Trajectory Local Search (Mtsls1)technique. A hybrid approach has been introduced in the local search strategyby the use of a parameter which allows for probabilistic selection betweenMtsls1 and a NLopt algorithm. In case of stagnation, the algorithm switch ismade based on the algorithm being used in the previous iteration. The paperpresents an exhaustive comparison on the performance of these approaches onSoft Computing (SOCO) and Congress on Evolutionary Computation (CEC) 2014benchmarks. For both benchmarks, we conclude that the best performing algorithmis a hybrid variant of Mtsls1 with BFGS for local search.
arxiv-10500-35 | Dense image registration and deformable surface reconstruction in presence of occlusions and minimal texture | http://arxiv.org/pdf/1503.03429v3.pdf | author:Dat Tien Ngo, Sanghuyk Park, Anne Jorstad, Alberto Crivellaro, Chang Yoo, Pascal Fua category:cs.CV published:2015-03-11 summary:Deformable surface tracking from monocular images is well-known to beunder-constrained. Occlusions often make the task even more challenging, andcan result in failure if the surface is not sufficiently textured. In thiswork, we explicitly address the problem of 3D reconstruction of poorlytextured, occluded surfaces, proposing a framework based on a template-matchingapproach that scales dense robust features by a relevancy score. Our approachis extensively compared to current methods employing both local featurematching and dense template alignment. We test on standard datasets as well ason a new dataset (that will be made publicly available) of a sparsely textured,occluded surface. Our framework achieves state-of-the-art results for both welland poorly textured, occluded surfaces.
arxiv-10500-36 | Scalable Discovery of Time-Series Shapelets | http://arxiv.org/pdf/1503.03238v1.pdf | author:Josif Grabocka, Martin Wistuba, Lars Schmidt-Thieme category:cs.LG published:2015-03-11 summary:Time-series classification is an important problem for the data miningcommunity due to the wide range of application domains involving time-seriesdata. A recent paradigm, called shapelets, represents patterns that are highlypredictive for the target variable. Shapelets are discovered by measuring theprediction accuracy of a set of potential (shapelet) candidates. The candidatestypically consist of all the segments of a dataset, therefore, the discovery ofshapelets is computationally expensive. This paper proposes a novel method thatavoids measuring the prediction accuracy of similar candidates in Euclideandistance space, through an online clustering pruning technique. In addition,our algorithm incorporates a supervised shapelet selection that filters outonly those candidates that improve classification accuracy. Empirical evidenceon 45 datasets from the UCR collection demonstrate that our method is 3-4orders of magnitudes faster than the fastest existing shapelet-discoverymethod, while providing better prediction accuracy.
arxiv-10500-37 | Adaptive-Rate Sparse Signal Reconstruction With Application in Compressive Background Subtraction | http://arxiv.org/pdf/1503.03231v1.pdf | author:Joao F. C. Mota, Nikos Deligiannis, Aswin C. Sankaranarayanan, Volkan Cevher, Miguel R. D. Rodrigues category:math.OC cs.CV cs.IT math.IT stat.ML published:2015-03-11 summary:We propose and analyze an online algorithm for reconstructing a sequence ofsignals from a limited number of linear measurements. The signals are assumedsparse, with unknown support, and evolve over time according to a genericnonlinear dynamical model. Our algorithm, based on recent theoretical resultsfor $\ell_1$-$\ell_1$ minimization, is recursive and computes the number ofmeasurements to be taken at each time on-the-fly. As an example, we apply thealgorithm to compressive video background subtraction, a problem that can bestated as follows: given a set of measurements of a sequence of images with astatic background, simultaneously reconstruct each image while separating itsforeground from the background. The performance of our method is illustrated onsequences of real images: we observe that it allows a dramatic reduction in thenumber of measurements with respect to state-of-the-art compressive backgroundsubtraction schemes.
arxiv-10500-38 | A Multi-Gene Genetic Programming Application for Predicting Students Failure at School | http://arxiv.org/pdf/1503.03211v1.pdf | author:J. O. Orove, N. E. Osegi, B. O. Eke category:cs.CY cs.AI cs.NE published:2015-03-11 summary:Several efforts to predict student failure rate (SFR) at school accuratelystill remains a core problem area faced by many in the educational sector. Theprocedure for forecasting SFR are rigid and most often times require datascaling or conversion into binary form such as is the case of the logisticmodel which may lead to lose of information and effect size attenuation. Also,the high number of factors, incomplete and unbalanced dataset, and black boxingissues as in Artificial Neural Networks and Fuzzy logic systems exposes theneed for more efficient tools. Currently the application of Genetic Programming(GP) holds great promises and has produced tremendous positive results indifferent sectors. In this regard, this study developed GPSFARPS, a softwareapplication to provide a robust solution to the prediction of SFR using anevolutionary algorithm known as multi-gene genetic programming. The approach isvalidated by feeding a testing data set to the evolved GP models. Resultobtained from GPSFARPS simulations show its unique ability to evolve a suitablefailure rate expression with a fast convergence at 30 generations from amaximum specified generation of 500. The multi-gene system was also able tominimize the evolved model expression and accurately predict student failurerate using a subset of the original expression
arxiv-10500-39 | Stochastic Texture Difference for Scale-Dependent Data Analysis | http://arxiv.org/pdf/1503.03278v3.pdf | author:Nicolas Brodu, Hussein Yahia category:cs.CV published:2015-03-11 summary:This article introduces the Stochastic Texture Difference method foranalyzing data at prescribed spatial and value scales. This method relies onconstrained random walks around each pixel, describing how nearby image valuestypically evolve on each side of this pixel. Textures are represented asprobability distributions of such random walks, so a texture differenceoperator is statistically defined as a distance between these distributions ina suitable reproducing kernel Hilbert space. The method is thus not limited toscalar pixel values: any data type for which a kernel is available may beconsidered, from color triplets and multispectral vector data to strings,graphs, and more. By adjusting the size of the neighborhoods that are compared,the method is implicitly scale-dependent. It is also able to focus on eithersmall changes or large gradients. We demonstrate how it can be used to inferspatial and data value characteristic scales in measured signals and naturalimages.
arxiv-10500-40 | On Using Monolingual Corpora in Neural Machine Translation | http://arxiv.org/pdf/1503.03535v2.pdf | author:Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares, Holger Schwenk, Yoshua Bengio category:cs.CL published:2015-03-11 summary:Recent work on end-to-end neural network-based architectures for machinetranslation has shown promising results for En-Fr and En-De translation.Arguably, one of the major factors behind this success has been theavailability of high quality parallel corpora. In this work, we investigate howto leverage abundant monolingual corpora for neural machine translation.Compared to a phrase-based and hierarchical baseline, we obtain up to $1.96$BLEU improvement on the low-resource language pair Turkish-English, and $1.59$BLEU on the focused domain task of Chinese-English chat messages. While ourmethod was initially targeted toward such tasks with less parallel data, weshow that it also extends to high resource languages such as Cs-En and De-Enwhere we obtain an improvement of $0.39$ and $0.47$ BLEU scores over the neuralmachine translation baselines, respectively.
arxiv-10500-41 | L_1-regularized Boltzmann machine learning using majorizer minimization | http://arxiv.org/pdf/1503.03132v1.pdf | author:Masayuki Ohzeki category:stat.ML cs.LG published:2015-03-11 summary:We propose an inference method to estimate sparse interactions and biasesaccording to Boltzmann machine learning. The basis of this method is $L_1$regularization, which is often used in compressed sensing, a technique forreconstructing sparse input signals from undersampled outputs. $L_1$regularization impedes the simple application of the gradient method, whichoptimizes the cost function that leads to accurate estimations, owing to thecost function's lack of smoothness. In this study, we utilize the majorizerminimization method, which is a well-known technique implemented inoptimization problems, to avoid the non-smoothness of the cost function. Byusing the majorizer minimization method, we elucidate essentially relevantbiases and interactions from given data with seemingly strongly-correlatedcomponents.
arxiv-10500-42 | Multigrid with rough coefficients and Multiresolution operator decomposition from Hierarchical Information Games | http://arxiv.org/pdf/1503.03467v4.pdf | author:Houman Owhadi category:math.NA cs.AI math.ST stat.ML stat.TH published:2015-03-11 summary:We introduce a near-linear complexity (geometric and meshless/algebraic)multigrid/multiresolution method for PDEs with rough ($L^\infty$) coefficientswith rigorous a-priori accuracy and performance estimates. The method isdiscovered through a decision/game theory formulation of the problems of (1)identifying restriction and interpolation operators (2) recovering a signalfrom incomplete measurements based on norm constraints on its image under alinear operator (3) gambling on the value of the solution of the PDE based on ahierarchy of nested measurements of its solution or source term. The resultingelementary gambles form a hierarchy of (deterministic) basis functions of$H^1_0(\Omega)$ (gamblets) that (1) are orthogonal across subscales/subbandswith respect to the scalar product induced by the energy norm of the PDE (2)enable sparse compression of the solution space in $H^1_0(\Omega)$ (3) inducean orthogonal multiresolution operator decomposition. The operating diagram ofthe multigrid method is that of an inverted pyramid in which gamblets arecomputed locally (by virtue of their exponential decay), hierarchically (fromfine to coarse scales) and the PDE is decomposed into a hierarchy ofindependent linear systems with uniformly bounded condition numbers. Theresulting algorithm is parallelizable both in space (via localization) and inbandwith/subscale (subscales can be computed independently from each other).Although the method is deterministic it has a natural Bayesian interpretationunder the measure of probability emerging (as a mixed strategy) from theinformation game formulation and multiresolution approximations form amartingale with respect to the filtration induced by the hierarchy of nestedmeasurements.
arxiv-10500-43 | A Neurodynamical System for finding a Minimal VC Dimension Classifier | http://arxiv.org/pdf/1503.03148v1.pdf | author:Jayadeva, Sumit Soman, Amit Bhaya category:cs.LG stat.ML 70G660, 68T05 published:2015-03-11 summary:The recently proposed Minimal Complexity Machine (MCM) finds a hyperplaneclassifier by minimizing an exact bound on the Vapnik-Chervonenkis (VC)dimension. The VC dimension measures the capacity of a learning machine, and asmaller VC dimension leads to improved generalization. On many benchmarkdatasets, the MCM generalizes better than SVMs and uses far fewer supportvectors than the number used by SVMs. In this paper, we describe a neuralnetwork based on a linear dynamical system, that converges to the MCM solution.The proposed MCM dynamical system is conducive to an analogue circuitimplementation on a chip or simulation using Ordinary Differential Equation(ODE) solvers. Numerical experiments on benchmark datasets from the UCIrepository show that the proposed approach is scalable and accurate, as weobtain improved accuracies and fewer number of support vectors (upto 74.3%reduction) with the MCM dynamical system.
arxiv-10500-44 | Online Matrix Completion and Online Robust PCA | http://arxiv.org/pdf/1503.03525v2.pdf | author:Brian Lois, Namrata Vaswani category:cs.IT math.IT stat.ML published:2015-03-11 summary:This work studies two interrelated problems - online robust PCA (RPCA) andonline low-rank matrix completion (MC). In recent work by Cand\`{e}s et al.,RPCA has been defined as a problem of separating a low-rank matrix (true data),$L:=[\ell_1, \ell_2, \dots \ell_{t}, \dots , \ell_{t_{\max}}]$ and a sparsematrix (outliers), $S:=[x_1, x_2, \dots x_{t}, \dots, x_{t_{\max}}]$ from theirsum, $M:=L+S$. Our work uses this definition of RPCA. An important applicationwhere both these problems occur is in video analytics in trying to separatesparse foregrounds (e.g., moving objects) and slowly changing backgrounds. While there has been a large amount of recent work on both developing andanalyzing batch RPCA and batch MC algorithms, the online problem is largelyopen. In this work, we develop a practical modification of our recentlyproposed algorithm to solve both the online RPCA and online MC problems. Themain contribution of this work is that we obtain correctness results for theproposed algorithms under mild assumptions. The assumptions that we need are:(a) a good estimate of the initial subspace is available (easy to obtain usinga short sequence of background-only frames in video surveillance); (b) the$\ell_t$'s obey a `slow subspace change' assumption; (c) the basis vectors forthe subspace from which $\ell_t$ is generated are dense (non-sparse); (d) thesupport of $x_t$ changes by at least a certain amount at least every so often;and (e) algorithm parameters are appropriately set
arxiv-10500-45 | Is language evolution grinding to a halt?: Exploring the life and death of words in English fiction | http://arxiv.org/pdf/1503.03512v3.pdf | author:Eitan Adam Pechenick, Christopher M. Danforth, Peter Sheridan Dodds category:cs.CL cs.IT math.IT physics.soc-ph stat.AP published:2015-03-11 summary:The Google Books corpus, derived from millions of books in a range of majorlanguages, would seem to offer many possibilities for research into cultural,social, and linguistic evolution. In a previous work, we found that the 2009and 2012 versions of the unfiltered English data set as well as the 2009version of the English Fiction data set are all heavily saturated withscientific and medical literature, rendering them unsuitable for rigorousanalysis [Pechenick, Danforth and Dodds, PLoS ONE, 10, e0137041, 2015]. Bycontrast, the 2012 version of English Fiction appeared to be uncompromised, andwe use this data set to explore language dynamics for English from 1820--2000.We critique an earlier method for measuring birth and death rates of words, andprovide a robust, principled approach to examining the volume of word fluxacross various relative frequency usage thresholds. We use the contributions tothe Jensen-Shannon divergence of words crossing thresholds between consecutivedecades to illuminate the major driving factors behind the flux. We find thatwhile individual word usage may vary greatly, the overall statistical structureof the language appears to remain fairly stable. We also find indications thatscholarly works about fiction are strongly represented in the 2012 EnglishFiction corpus, and suggest that a future revision of the corpus should attemptto separate critical works from fiction itself.
arxiv-10500-46 | Automatic Unsupervised Tensor Mining with Quality Assessment | http://arxiv.org/pdf/1503.03355v1.pdf | author:Evangelos E. Papalexakis category:stat.ML cs.LG cs.NA stat.AP published:2015-03-11 summary:A popular tool for unsupervised modelling and mining multi-aspect data istensor decomposition. In an exploratory setting, where and no labels or groundtruth are available how can we automatically decide how many components toextract? How can we assess the quality of our results, so that a domain expertcan factor this quality measure in the interpretation of our results? In thispaper, we introduce AutoTen, a novel automatic unsupervised tensor miningalgorithm with minimal user intervention, which leverages and improves uponheuristics that assess the result quality. We extensively evaluate AutoTen'sperformance on synthetic data, outperforming existing baselines on this veryhard problem. Finally, we apply AutoTen on a variety of real datasets,providing insights and discoveries. We view this work as a step towards a fullyautomated, unsupervised tensor mining tool that can be easily adopted bypractitioners in academia and industry.
arxiv-10500-47 | Properties of simple sets in digital spaces. Contractions of simple sets preserving the homotopy type of a digital space | http://arxiv.org/pdf/1503.03491v1.pdf | author:Alexander V. Evako category:cs.CV cs.DM math.AT published:2015-03-11 summary:A point of a digital space is called simple if it can be deleted from thespace without altering topology. This paper introduces the notion simple set ofpoints of a digital space. The definition is based on contractible spaces andcontractible transformations. A set of points in a digital space is calledsimple if it can be contracted to a point without changing topology of thespace. It is shown that contracting a simple set of points does not change thehomotopy type of a digital space, and the number of points in a digital spacewithout simple points can be reduces by contracting simple sets. Using theprocess of contracting, we can substantially compress a digital space whilepreserving the topology. The paper proposes a method for thinning a digitalspace which shows that this approach can contribute to computer science such asmedical imaging, computer graphics and pattern analysis.
arxiv-10500-48 | A Novel Hybrid CNN-AIS Visual Pattern Recognition Engine | http://arxiv.org/pdf/1503.03270v1.pdf | author:Vandna Bhalla, Santanu Chaudhury, Arihant Jain category:cs.CV published:2015-03-11 summary:Machine learning methods are used today for most recognition problems.Convolutional Neural Networks (CNN) have time and again proved successful formany image processing tasks primarily for their architecture. In this paper wepropose to apply CNN to small data sets like for example, personal albums orother similar environs where the size of training dataset is a limitation,within the framework of a proposed hybrid CNN-AIS model. We use ArtificialImmune System Principles to enhance small size of training data set. A layer ofClonal Selection is added to the local filtering and max pooling of CNNArchitecture. The proposed Architecture is evaluated using the standard MNISTdataset by limiting the data size and also with a small personal data samplebelonging to two different classes. Experimental results show that the proposedhybrid CNN-AIS based recognition engine works well when the size of trainingdata is limited in size
arxiv-10500-49 | Diverse Landmark Sampling from Determinantal Point Processes for Scalable Manifold Learning | http://arxiv.org/pdf/1503.03506v1.pdf | author:Christian Wachinger, Polina Golland category:cs.LG cs.AI cs.CV published:2015-03-11 summary:High computational costs of manifold learning prohibit its application forlarge point sets. A common strategy to overcome this problem is to performdimensionality reduction on selected landmarks and to successively embed theentire dataset with the Nystr\"om method. The two main challenges that ariseare: (i) the landmarks selected in non-Euclidean geometries must result in alow reconstruction error, (ii) the graph constructed from sparsely sampledlandmarks must approximate the manifold well. We propose the sampling oflandmarks from determinantal distributions on non-Euclidean spaces. Sincecurrent determinantal sampling algorithms have the same complexity as those formanifold learning, we present an efficient approximation running in lineartime. Further, we recover the local geometry after the sparsification byassigning each landmark a local covariance matrix, estimated from the originalpoint set. The resulting neighborhood selection based on the Bhattacharyyadistance improves the embedding of sparsely sampled manifolds. Our experimentsshow a significant performance improvement compared to state-of-the-artlandmark selection techniques.
arxiv-10500-50 | Appearance-based indoor localization: A comparison of patch descriptor performance | http://arxiv.org/pdf/1503.03514v1.pdf | author:Jose Rivera-Rubio, Ioannis Alexiou, Anil A. Bharath category:cs.CV cs.RO 68T45, 68T40 published:2015-03-11 summary:Vision is one of the most important of the senses, and humans use itextensively during navigation. We evaluated different types of image and videoframe descriptors that could be used to determine distinctive visual landmarksfor localizing a person based on what is seen by a camera that they carry. Todo this, we created a database containing over 3 km of video-sequences withground-truth in the form of distance travelled along different corridors. Usingthis database, the accuracy of localization - both in terms of knowing whichroute a user is on - and in terms of position along a certain route, can beevaluated. For each type of descriptor, we also tested different techniques toencode visual structure and to search between journeys to estimate a user'sposition. The techniques include single-frame descriptors, those usingsequences of frames, and both colour and achromatic descriptors. We found thatsingle-frame indexing worked better within this particular dataset. This mightbe because the motion of the person holding the camera makes the video toodependent on individual steps and motions of one particular journey. Ourresults suggest that appearance-based information could be an additional sourceof navigational data indoors, augmenting that provided by, say, radio signalstrength indicators (RSSIs). Such visual information could be collected bycrowdsourcing low-resolution video feeds, allowing journeys made by differentusers to be associated with each other, and location to be inferred withoutrequiring explicit mapping. This offers a complementary approach to methodsbased on simultaneous localization and mapping (SLAM) algorithms.
arxiv-10500-51 | Simple, Accurate, and Robust Nonparametric Blind Super-Resolution | http://arxiv.org/pdf/1503.03187v2.pdf | author:Wen-Ze Shao, Michael Elad category:cs.CV published:2015-03-11 summary:This paper proposes a simple, accurate, and robust approach to single imagenonparametric blind Super-Resolution (SR). This task is formulated as afunctional to be minimized with respect to both an intermediate super-resolvedimage and a nonparametric blur-kernel. The proposed approach includes aconvolution consistency constraint which uses a non-blind learning-based SRresult to better guide the estimation process. Another key component is theunnatural bi-l0-l2-norm regularization imposed on the super-resolved, sharpimage and the blur-kernel, which is shown to be quite beneficial for estimatingthe blur-kernel accurately. The numerical optimization is implemented bycoupling the splitting augmented Lagrangian and the conjugate gradient (CG).Using the pre-estimated blur-kernel, we finally reconstruct the SR image by avery simple non-blind SR method that uses a natural image prior. The proposedapproach is demonstrated to achieve better performance than the recent methodby Michaeli and Irani [2] in both terms of the kernel estimation accuracy andimage SR quality.
arxiv-10500-52 | A model-based approach to recovering the structure of a plant from images | http://arxiv.org/pdf/1503.03191v2.pdf | author:Ben Ward, John Bastian, Anton van den Hengel, Daniel Pooley, Rajendra Bari, Bettina Berger, Mark Tester category:cs.CV published:2015-03-11 summary:We present a method for recovering the structure of a plant directly from asmall set of widely-spaced images. Structure recovery is more complex thanshape estimation, but the resulting structure estimate is more closely relatedto phenotype than is a 3D geometric model. The method we propose is applicableto a wide variety of plants, but is demonstrated on wheat. Wheat is made up ofthin elements with few identifiable features, making it difficult to analyseusing standard feature matching techniques. Our method instead analyses thestructure of plants using only their silhouettes. We employ a generate-and-testmethod, using a database of manually modelled leaves and a model for theircomposition to synthesise plausible plant structures which are evaluatedagainst the images. The method is capable of efficiently recovering accurateestimates of plant structure in a wide variety of imaging scenarios, with nomanual intervention.
arxiv-10500-53 | Describing and Understanding Neighborhood Characteristics through Online Social Media | http://arxiv.org/pdf/1503.03524v1.pdf | author:Mohamed Kafsi, Henriette Cramer, Bart Thomee, David A. Shamma category:stat.ML cs.SI published:2015-03-11 summary:Geotagged data can be used to describe regions in the world and discoverlocal themes. However, not all data produced within a region is necessarilyspecifically descriptive of that area. To surface the content that ischaracteristic for a region, we present the geographical hierarchy model (GHM),a probabilistic model based on the assumption that data observed in a region isa random mixture of content that pertains to different levels of a hierarchy.We apply the GHM to a dataset of 8 million Flickr photos in order todiscriminate between content (i.e., tags) that specifically characterizes aregion (e.g., neighborhood) and content that characterizes surrounding areas ormore general themes. Knowledge of the discriminative and non-discriminativeterms used throughout the hierarchy enables us to quantify the uniqueness of agiven region and to compare similar but distant regions. Our evaluationdemonstrates that our model improves upon traditional Naive Bayesclassification by 47% and hierarchical TF-IDF by 27%. We further highlight thedifferences and commonalities with human reasoning about what is locallycharacteristic for a neighborhood, distilled from ten interviews and a surveythat covered themes such as time, events, and prior regional knowledge
arxiv-10500-54 | Switching to Learn | http://arxiv.org/pdf/1503.03517v1.pdf | author:Shahin Shahrampour, Mohammad Amin Rahimian, Ali Jadbabaie category:cs.LG math.OC stat.ML published:2015-03-11 summary:A network of agents attempt to learn some unknown state of the world drawn bynature from a finite set. Agents observe private signals conditioned on thetrue state, and form beliefs about the unknown state accordingly. Each agentmay face an identification problem in the sense that she cannot distinguish thetruth in isolation. However, by communicating with each other, agents are ableto benefit from side observations to learn the truth collectively. Unlike manydistributed algorithms which rely on all-time communication protocols, wepropose an efficient method by switching between Bayesian and non-Bayesianregimes. In this model, agents exchange information only when their privatesignals are not informative enough; thence, by switching between the tworegimes, agents efficiently learn the truth using only a few rounds ofcommunications. The proposed algorithm preserves learnability while incurring alower communication cost. We also verify our theoretical findings by simulationexamples.
arxiv-10500-55 | Parallel Statistical Multi-resolution Estimation | http://arxiv.org/pdf/1503.03492v1.pdf | author:Jan Lebert, Lutz Künneke, Johannes Hagemann, Stephan C. Kramer category:cs.CV published:2015-03-10 summary:We discuss several strategies to implement Dykstra's projection algorithm onNVIDIA's compute unified device architecture (CUDA). Dykstra's algorithm is thecentral step in and the computationally most expensive part of statisticalmulti-resolution methods. It projects a given vector onto the intersection ofconvex sets. Compared with a CPU implementation our CUDA implementation is oneorder of magnitude faster. For a further speed up and to reduce memoryconsumption we have developed a new variant, which we call incomplete Dykstra'salgorithm. Implemented in CUDA it is one order of magnitude faster than theCUDA implementation of the standard Dykstra algorithm. As sample application wediscuss using the incomplete Dykstra's algorithm as preprocessor for therecently developed super-resolution optical fluctuation imaging (SOFI) method(Dertinger et al. 2009). We show that statistical multi-resolution estimationcan enhance the resolution improvement of the plain SOFI algorithm just as theFourier-reweighting of SOFI. The results are compared in terms of their powerspectrum and their Fourier ring correlation (Saxton and Baumeister 1982). TheFourier ring correlation indicates that the resolution for typical second orderSOFI images can be improved by about 30 per cent. Our results show that acareful parallelization of Dykstra's algorithm enables its use in large-scalestatistical multi-resolution analyses.
arxiv-10500-56 | Minimax Optimal Rates of Estimation in High Dimensional Additive Models: Universal Phase Transition | http://arxiv.org/pdf/1503.02817v1.pdf | author:Ming Yuan, Ding-Xuan Zhou category:math.ST cs.IT math.IT stat.ML stat.TH published:2015-03-10 summary:We establish minimax optimal rates of convergence for estimation in a highdimensional additive model assuming that it is approximately sparse. Ourresults reveal an interesting phase transition behavior universal to this classof high dimensional problems. In the {\it sparse regime} when the componentsare sufficiently smooth or the dimensionality is sufficiently large, theoptimal rates are identical to those for high dimensional linear regression,and therefore there is no additional cost to entertain a nonparametric model.Otherwise, in the so-called {\it smooth regime}, the rates coincide with theoptimal rates for estimating a univariate function, and therefore they areimmune to the "curse of dimensionality".
arxiv-10500-57 | Short Text Hashing Improved by Integrating Multi-Granularity Topics and Tags | http://arxiv.org/pdf/1503.02801v1.pdf | author:Jiaming Xu, Bo Xu, Guanhua Tian, Jun Zhao, Fangyuan Wang, Hongwei Hao category:cs.IR cs.CL published:2015-03-10 summary:Due to computational and storage efficiencies of compact binary codes,hashing has been widely used for large-scale similarity search. Unfortunately,many existing hashing methods based on observed keyword features are noteffective for short texts due to the sparseness and shortness. Recently, someresearchers try to utilize latent topics of certain granularity to preservesemantic similarity in hash codes beyond keyword matching. However, topics ofcertain granularity are not adequate to represent the intrinsic semanticinformation. In this paper, we present a novel unified approach for short textHashing using Multi-granularity Topics and Tags, dubbed HMTT. In particular, wepropose a selection method to choose the optimal multi-granularity topicsdepending on the type of dataset, and design two distinct hashing strategies toincorporate multi-granularity topics. We also propose a simple and effectivemethod to exploit tags to enhance the similarity of related texts. We carry outextensive experiments on one short text dataset as well as on one normal textdataset. The results demonstrate that our approach is effective andsignificantly outperforms baselines on several evaluation metrics.
arxiv-10500-58 | Single stream parallelization of generalized LSTM-like RNNs on a GPU | http://arxiv.org/pdf/1503.02852v1.pdf | author:Kyuyeon Hwang, Wonyong Sung category:cs.NE cs.LG published:2015-03-10 summary:Recurrent neural networks (RNNs) have shown outstanding performance onprocessing sequence data. However, they suffer from long training time, whichdemands parallel implementations of the training procedure. Parallelization ofthe training algorithms for RNNs are very challenging because internalrecurrent paths form dependencies between two different time frames. In thispaper, we first propose a generalized graph-based RNN structure that covers themost popular long short-term memory (LSTM) network. Then, we present aparallelization approach that automatically explores parallelisms of arbitraryRNNs by analyzing the graph structure. The experimental results show that theproposed approach shows great speed-up even with a single training stream, andfurther accelerates the training when combined with multiple parallel trainingstreams.
arxiv-10500-59 | Robust recovery of complex exponential signals from random Gaussian projections via low rank Hankel matrix reconstruction | http://arxiv.org/pdf/1503.02893v1.pdf | author:Jian-Feng Cai, Xiaobo Qu, Weiyu Xu, Gui-Bo Ye category:cs.IT math.IT math.NA math.OC stat.ML published:2015-03-10 summary:This paper explores robust recovery of a superposition of $R$ distinctcomplex exponential functions from a few random Gaussian projections. We assumethat the signal of interest is of $2N-1$ dimensional and $R<<2N-1$. Thisframework covers a large class of signals arising from real applications inbiology, automation, imaging science, etc. To reconstruct such a signal, ouralgorithm is to seek a low-rank Hankel matrix of the signal by minimizing itsnuclear norm subject to the consistency on the sampled data. Our theoreticalresults show that a robust recovery is possible as long as the number ofprojections exceeds $O(R\ln^2N)$. No incoherence or separation condition isrequired in our proof. Our method can be applied to spectral compressed sensingwhere the signal of interest is a superposition of $R$ complex sinusoids.Compared to existing results, our result here does not need any separationcondition on the frequencies, while achieving better or comparable bounds onthe number of measurements. Furthermore, our method provides theoreticalguidance on how many samples are required in the state-of-the-art non-uniformsampling in NMR spectroscopy. The performance of our algorithm is furtherdemonstrated by numerical experiments.
arxiv-10500-60 | Post-Regularization Confidence Bands for High Dimensional Nonparametric Models with Local Sparsity | http://arxiv.org/pdf/1503.02978v1.pdf | author:Junwei Lu, Mladen Kolar, Han Liu category:stat.ML math.ST stat.TH published:2015-03-10 summary:We propose a novel high dimensional nonparametric model named ATLAS whichnaturally generlizes the sparse additive model. Given a covariate of interest$X_j$, the ATLAS model assumes the mean function can be locally approximated bya sparse additive function whose sparsity pattern may vary from the globalperspective. We propose to infer the marginal influence function $f_j^*(z) =\mathbb{E}[f(X_1,\ldots, X_d) \mid X_j = z]$ using a new kernel-sieve approachthat combines the local kernel regression with the B-spline basisapproximation. We prove the rate of convergence for estimating $f_j^*$ underthe supremum norm. We also propose two types of confidence bands for $f_j^*$and illustrate their statistical-comptuational tradeoffs. Thorough numericalresults on both synthetic data and real-world genomic data are provided todemonstrate the efficacy of the theory.
arxiv-10500-61 | Technical Analysis on Financial Forecasting | http://arxiv.org/pdf/1503.03011v1.pdf | author:S. Gopal Krishna Patro, Pragyan Parimita Sahoo, Ipsita Panda, Kishore Kumar Sahu category:cs.NE published:2015-03-10 summary:Financial forecasting is an estimation of future financial outcomes for acompany, industry, country using historical internal accounting and sales data.We may predict the future outcome of BSE_SENSEX practically by some softcomputing techniques and can also optimized using PSO (Particle SwarmOptimization), EA (Evolutionary Algorithm) or DEA (Differential EvolutionaryAlgorithm) etc. PSO is a biologically inspired computational search &optimization method developed in 1995 by Dr. Eberhart and Dr. Kennedy based onthe social behaviors of fish schooling or birds flocking. PSO is a promisingmethod to train Artificial Neural Network (ANN). It is easy to implement thenGenetic Algorithm except few parameters are adjusted. PSO is a random & patternsearch technique based on populating of particle. In PSO, the particles arehaving some position and velocity in the search space. Two terms are used inPSO one is Local Best and another one is Global Best. To optimize problems thatare like Irregular, Noisy, Change over time, Static etc. PSO uses a classicoptimization method such as Gradient Decent & Quasi-Newton Methods. Theobservation and review of few related studies in the last few years, focusingon function of PSO, modification of PSO and operation that have implementedusing PSO like function optimization, ANN Training & Fuzzy Control etc.Differential Evolution is an efficient EA technique for optimization ofnumerical problems, financial problems etc. PSO technique is introduced due tothe swarming behavior of animals which is the collective behavior of similarsize that aggregates together.
arxiv-10500-62 | Fast Multi-class Dictionaries Learning with Geometrical Directions in MRI Reconstruction | http://arxiv.org/pdf/1503.02945v2.pdf | author:Zhifang Zhan, Jian-Feng Cai, Di Guo, Yunsong Liu, Zhong Chen, Xiaobo Qu category:cs.CV math.OC physics.med-ph published:2015-03-10 summary:Objective: Improve the reconstructed image with fast and multi-classdictionaries learning when magnetic resonance imaging is accelerated byundersampling the k-space data. Methods: A fast orthogonal dictionary learningmethod is introduced into magnetic resonance image reconstruction to providingadaptive sparse representation of images. To enhance the sparsity, image isdivided into classified patches according to the same geometrical direction anddictionary is trained within each class. A new sparse reconstruction model withthe multi-class dictionaries is proposed and solved using a fast alternatingdirection method of multipliers. Results: Experiments on phantom and brainimaging data with acceleration factor up to 10 and various undersamplingpatterns are conducted. The proposed method is compared with state-of-the-artmagnetic resonance image reconstruction methods. Conclusion: Artifacts arebetter suppressed and image edges are better preserved than the comparedmethods. Besides, the computation of the proposed approach is much faster thanthe typical K-SVD dictionary learning method in magnetic resonance imagereconstruction. Significance: The proposed method can be exploited inundersapmled magnetic resonance imaging to reduce data acquisition time andreconstruct images with better image quality.
arxiv-10500-63 | Fast and Robust Fixed-Rank Matrix Recovery | http://arxiv.org/pdf/1503.03004v3.pdf | author:German Ros, Julio Guerrero category:cs.CV cs.NA published:2015-03-10 summary:We address the problem of efficient sparse fixed-rank (S-FR) matrixdecomposition, i.e., splitting a corrupted matrix $M$ into an uncorruptedmatrix $L$ of rank $r$ and a sparse matrix of outliers $S$. Fixed-rankconstraints are usually imposed by the physical restrictions of the systemunder study. Here we propose a method to perform accurate and very efficientS-FR decomposition that is more suitable for large-scale problems than existingapproaches. Our method is a grateful combination of geometrical and algebraicaltechniques, which avoids the bottleneck caused by the Truncated SVD (TSVD).Instead, a polar factorization is used to exploit the manifold structure offixed-rank problems as the product of two Stiefel and an SPD manifold, leadingto a better convergence and stability. Then, closed-form projectors help tospeed up each iteration of the method. We introduce a novel and fast projectorfor the $\text{SPD}$ manifold and a proof of its validity. Further accelerationis achieved using a Nystrom scheme. Extensive experiments with synthetic andreal data in the context of robust photometric stereo and spectral clusteringshow that our proposals outperform the state of the art.
arxiv-10500-64 | Learning the Structure for Structured Sparsity | http://arxiv.org/pdf/1503.03082v2.pdf | author:Nino Shervashidze, Francis Bach category:stat.ML published:2015-03-10 summary:Structured sparsity has recently emerged in statistics, machine learning andsignal processing as a promising paradigm for learning in high-dimensionalsettings. All existing methods for learning under the assumption of structuredsparsity rely on prior knowledge on how to weight (or how to penalize)individual subsets of variables during the subset selection process, which isnot available in general. Inferring group weights from data is a key openresearch problem in structured sparsity.In this paper, we propose a Bayesianapproach to the problem of group weight learning. We model the group weights ashyperparameters of heavy-tailed priors on groups of variables and derive anapproximate inference scheme to infer these hyperparameters. We empiricallyshow that we are able to recover the model hyperparameters when the data aregenerated from the model, and we demonstrate the utility of learning weights insynthetic and real denoising problems.
arxiv-10500-65 | Novel Bernstein-like Concentration Inequalities for the Missing Mass | http://arxiv.org/pdf/1503.02768v2.pdf | author:Bahman Yari Saeed Khanloo, Gholamreza Haffari category:stat.ML published:2015-03-10 summary:We are concerned with obtaining novel concentration inequalities for themissing mass, i.e. the total probability mass of the outcomes not observed inthe sample. We not only derive - for the first time - distribution-freeBernstein-like deviation bounds with sublinear exponents in deviation size formissing mass, but also improve the results of McAllester and Ortiz (2003)andBerend and Kontorovich (2013, 2012) for small deviations which is the mostinteresting case in learning theory. It is known that the majority of standardinequalities cannot be directly used to analyze heterogeneous sums i.e. sumswhose terms have large difference in magnitude. Our generic and intuitiveapproach shows that the heterogeneity issue introduced in McAllester and Ortiz(2003) is resolvable at least in the case of missing mass via regulating theterms using our novel thresholding technique.
arxiv-10500-66 | An Adaptive Online HDP-HMM for Segmentation and Classification of Sequential Data | http://arxiv.org/pdf/1503.02761v2.pdf | author:Ava Bargi, Richard Yi Da Xu, Massimo Piccardi category:stat.ML cs.LG published:2015-03-10 summary:In the recent years, the desire and need to understand sequential data hasbeen increasing, with particular interest in sequential contexts such aspatient monitoring, understanding daily activities, video surveillance, stockmarket and the like. Along with the constant flow of data, it is critical toclassify and segment the observations on-the-fly, without being limited to arigid number of classes. In addition, the model needs to be capable of updatingits parameters to comply with possible evolutions. This interesting problem,however, is not adequately addressed in the literature since many studies focuson offline classification over a pre-defined class set. In this paper, wepropose a principled solution to this gap by introducing an adaptive onlinesystem based on Markov switching models with hierarchical Dirichlet processpriors. This infinite adaptive online approach is capable of segmenting andclassifying the sequential data over unlimited number of classes, while meetingthe memory and delay constraints of streaming contexts. The model is furtherenhanced by introducing a learning rate, responsible for balancing the extentto which the model sustains its previous learning (parameters) or adapts to thenew streaming observations. Experimental results on several variants ofstationary and evolving synthetic data and two video datasets, TUM AssistiveKitchen and collatedWeizmann, show remarkable performance in segmentation andclassification, particularly for evolutionary sequences with changingdistributions and/or containing new, unseen classes.
arxiv-10500-67 | apsis - Framework for Automated Optimization of Machine Learning Hyper Parameters | http://arxiv.org/pdf/1503.02946v2.pdf | author:Frederik Diehl, Andreas Jauch category:cs.LG published:2015-03-10 summary:The apsis toolkit presented in this paper provides a flexible framework forhyperparameter optimization and includes both random search and a bayesianoptimizer. It is implemented in Python and its architecture featuresadaptability to any desired machine learning code. It can easily be used withcommon Python ML frameworks such as scikit-learn. Published under the MITLicense other researchers are heavily encouraged to check out the code,contribute or raise any suggestions. The code can be found atgithub.com/FrederikDiehl/apsis.
arxiv-10500-68 | Remarks on pointed digital homotopy | http://arxiv.org/pdf/1503.03016v2.pdf | author:Laurence Boxer, P. Christopher Staecker category:math.CO cs.CV math.GN 55P10, 68R10 I.4.m published:2015-03-10 summary:We present and explore in detail a pair of digital images with$c_u$-adjacencies that are homotopic but not pointed homotopic. For two digitalloops $f,g: [0,m]_Z \rightarrow X$ with the same basepoint, we introduce thenotion of {\em tight at the basepoint (TAB)} pointed homotopy, which is morerestrictive than ordinary pointed homotopy and yields some different results. We present a variant form of the digital fundamental group. Based on what wecall {\em eventually constant} loops, this version of the fundamental group isequivalent to that of Boxer (1999), but offers the advantage that eventuallyconstant maps are often easier to work with than the trivial extensions thatare key to the development of the fundamental group in Boxer (1999) and manysubsequent papers. We show that homotopy equivalent digital images have isomorphic fundamentalgroups, even when the homotopy equivalence does not preserve the basepoint.This assertion appeared in Boxer (2005), but there was an error in the proof;here, we correct the error.
arxiv-10500-69 | Scalable Nuclear-norm Minimization by Subspace Pursuit Proximal Riemannian Gradient | http://arxiv.org/pdf/1503.02828v2.pdf | author:Mingkui Tan, Shijie Xiao, Junbin Gao, Dong Xu, Anton Van Den Hengel, Qinfeng Shi category:cs.LG cs.NA published:2015-03-10 summary:Nuclear-norm regularization plays a vital role in many learning tasks, suchas low-rank matrix recovery (MR), and low-rank representation (LRR). Solvingthis problem directly can be computationally expensive due to the unknown rankof variables or large-rank singular value decompositions (SVDs). To addressthis, we propose a proximal Riemannian gradient (PRG) scheme which canefficiently solve trace-norm regularized problems defined on real-algebraicvariety $\mMLr$ of real matrices of rank at most $r$. Based on PRG, we furtherpresent a simple and novel subspace pursuit (SP) paradigm for generaltrace-norm regularized problems without the explicit rank constraint $\mMLr$.The proposed paradigm is very scalable by avoiding large-rank SVDs. Empiricalstudies on several tasks, such as matrix completion and LRR based subspaceclustering, demonstrate the superiority of the proposed paradigms over existingmethods.
arxiv-10500-70 | Fully Connected Deep Structured Networks | http://arxiv.org/pdf/1503.02351v1.pdf | author:Alexander G. Schwing, Raquel Urtasun category:cs.CV cs.LG published:2015-03-09 summary:Convolutional neural networks with many layers have recently been shown toachieve excellent results on many high-level tasks such as imageclassification, object detection and more recently also semantic segmentation.Particularly for semantic segmentation, a two-stage procedure is oftenemployed. Hereby, convolutional networks are trained to provide good localpixel-wise features for the second step being traditionally a more globalgraphical model. In this work we unify this two-stage process into a singlejoint training algorithm. We demonstrate our method on the semantic imagesegmentation task and show encouraging results on the challenging PASCAL VOC2012 dataset.
arxiv-10500-71 | Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs | http://arxiv.org/pdf/1503.02424v2.pdf | author:Yarin Gal, Richard Turner category:stat.ML published:2015-03-09 summary:Standard sparse pseudo-input approximations to the Gaussian process (GP)cannot handle complex functions well. Sparse spectrum alternatives attempt toanswer this but are known to over-fit. We suggest the use of variationalinference for the sparse spectrum approximation to avoid both issues. We modelthe covariance function with a finite Fourier series approximation and treat itas a random variable. The random covariance function has a posterior, on whicha variational distribution is placed. The variational distribution transformsthe random covariance function to fit the data. We study the properties of ourapproximate inference, compare it to alternative ones, and extend it to thedistributed and stochastic domains. Our approximation captures complexfunctions better than standard approaches and avoids over-fitting.
arxiv-10500-72 | Representation Learning with Deep Extreme Learning Machines for Efficient Image Set Classification | http://arxiv.org/pdf/1503.02445v3.pdf | author:Muhammad Uzair, Faisal Shafait, Bernard Ghanem, Ajmal Mian category:cs.CV published:2015-03-09 summary:Efficient and accurate joint representation of a collection of images, thatbelong to the same class, is a major research challenge for practical image setclassification. Existing methods either make prior assumptions about the datastructure, or perform heavy computations to learn structure from the dataitself. In this paper, we propose an efficient image set representation thatdoes not make any prior assumptions about the structure of the underlying data.We learn the non-linear structure of image sets with Deep Extreme LearningMachines (DELM) that are very efficient and generalize well even on a limitednumber of training samples. Extensive experiments on a broad range of publicdatasets for image set classification (Honda/UCSD, CMU Mobo, YouTubeCelebrities, Celebrity-1000, ETH-80) show that the proposed algorithmconsistently outperforms state-of-the-art image set classification methods bothin terms of speed and accuracy.
arxiv-10500-73 | Global 6DOF Pose Estimation from Untextured 2D City Models | http://arxiv.org/pdf/1503.02675v2.pdf | author:Clemens Arth, Christian Pirchheim, Jonathan Ventura, Vincent Lepetit category:cs.CV published:2015-03-09 summary:We propose a method for estimating the 3D pose for the camera of a mobiledevice in outdoor conditions, using only an untextured 2D model. Previousmethods compute only a relative pose using a SLAM algorithm, or require manyregistered images, which are cumbersome to acquire. By contrast, our methodreturns an accurate, absolute camera pose in an absolute referential usingsimple 2D+height maps, which are broadly available, to refine a first estimateof the pose provided by the device's sensors. We show how to first estimate thecamera absolute orientation from straight line segments, and then how toestimate the translation by aligning the 2D map with a semantic segmentation ofthe input image. We demonstrate the robustness and accuracy of our approach ona challenging dataset.
arxiv-10500-74 | Brain Tumor Segmentation: A Comparative Analysis | http://arxiv.org/pdf/1503.02466v1.pdf | author:Muhammad Ali Qadar, Yan Zhaowen category:cs.CV published:2015-03-09 summary:Five different threshold segmentation based approaches have been reviewed andcompared over here to extract the tumor from set of brain images. This researchfocuses on the analysis of image segmentation methods, a comparison of fivesemi-automated methods have been undertaken for evaluating their relativeperformance in the segmentation of tumor. Consequently, results are compared onthe basis of quantitative and qualitative analysis of respective methods. Thepurpose of this study was to analytically identify the methods, most suitablefor application for a particular genre of problems. The results show that ofthe region growing segmentation performed better than rest in most cases.
arxiv-10500-75 | Kernel-Based Just-In-Time Learning for Passing Expectation Propagation Messages | http://arxiv.org/pdf/1503.02551v2.pdf | author:Wittawat Jitkrittum, Arthur Gretton, Nicolas Heess, S. M. Ali Eslami, Balaji Lakshminarayanan, Dino Sejdinovic, Zoltán Szabó category:stat.ML cs.LG G.3; I.2.6 published:2015-03-09 summary:We propose an efficient nonparametric strategy for learning a messageoperator in expectation propagation (EP), which takes as input the set ofincoming messages to a factor node, and produces an outgoing message as output.This learned operator replaces the multivariate integral required in classicalEP, which may not have an analytic expression. We use kernel-based regression,which is trained on a set of probability distributions representing theincoming messages, and the associated outgoing messages. The kernel approachhas two main advantages: first, it is fast, as it is implemented using a noveltwo-layer random feature representation of the input message distributions;second, it has principled uncertainty estimates, and can be cheaply updatedonline, meaning it can request and incorporate new training data when itencounters inputs on which it is uncertain. In experiments, our approach isable to solve learning problems where a single message operator is required formultiple, substantially different data sets (logistic regression for a varietyof classification problems), where it is essential to accurately assessuncertainty and to efficiently and robustly update the message operator.
arxiv-10500-76 | Context-Dependent Translation Selection Using Convolutional Neural Network | http://arxiv.org/pdf/1503.02357v2.pdf | author:Zhaopeng Tu, Baotian Hu, Zhengdong Lu, Hang Li category:cs.CL cs.LG cs.NE published:2015-03-09 summary:We propose a novel method for translation selection in statistical machinetranslation, in which a convolutional neural network is employed to judge thesimilarity between a phrase pair in two languages. The specifically designedconvolutional architecture encodes not only the semantic similarity of thetranslation pair, but also the context containing the phrase in the sourcelanguage. Therefore, our approach is able to capture context-dependent semanticsimilarities of translation pairs. We adopt a curriculum learning strategy totrain the model: we classify the training examples into easy, medium, anddifficult categories, and gradually build the ability of representing phraseand sentence level context by using training examples from easy to difficult.Experimental results show that our approach significantly outperforms thebaseline system by up to 1.4 BLEU points.
arxiv-10500-77 | Neural Responding Machine for Short-Text Conversation | http://arxiv.org/pdf/1503.02364v2.pdf | author:Lifeng Shang, Zhengdong Lu, Hang Li category:cs.CL cs.AI cs.NE published:2015-03-09 summary:We propose Neural Responding Machine (NRM), a neural network-based responsegenerator for Short-Text Conversation. NRM takes the general encoder-decoderframework: it formalizes the generation of response as a decoding process basedon the latent representation of the input text, while both encoding anddecoding are realized with recurrent neural networks (RNN). The NRM is trainedwith a large amount of one-round conversation data collected from amicroblogging service. Empirical study shows that NRM can generategrammatically correct and content-wise appropriate responses to over 75% of theinput text, outperforming state-of-the-arts in the same setting, includingretrieval-based and SMT-based models.
arxiv-10500-78 | Learning Co-Sparse Analysis Operators with Separable Structures | http://arxiv.org/pdf/1503.02398v5.pdf | author:Matthias Seibert, Julian Wörmann, Rémi Gribonval, Martin Kleinsteuber category:cs.LG stat.ML published:2015-03-09 summary:In the co-sparse analysis model a set of filters is applied to a signal outof the signal class of interest yielding sparse filter responses. As such, itmay serve as a prior in inverse problems, or for structural analysis of signalsthat are known to belong to the signal class. The more the model is adapted tothe class, the more reliable it is for these purposes. The task of learningsuch operators for a given class is therefore a crucial problem. In manyapplications, it is also required that the filter responses are obtained in atimely manner, which can be achieved by filters with a separable structure. Notonly can operators of this sort be efficiently used for computing the filterresponses, but they also have the advantage that less training samples arerequired to obtain a reliable estimate of the operator. The first contributionof this work is to give theoretical evidence for this claim by providing anupper bound for the sample complexity of the learning process. The second is astochastic gradient descent (SGD) method designed to learn an analysis operatorwith separable structures, which includes a novel and efficient step sizeselection rule. Numerical experiments are provided that link the samplecomplexity to the convergence speed of the SGD algorithm.
arxiv-10500-79 | Compositional Distributional Semantics with Long Short Term Memory | http://arxiv.org/pdf/1503.02510v2.pdf | author:Phong Le, Willem Zuidema category:cs.CL cs.AI cs.LG published:2015-03-09 summary:We are proposing an extension of the recursive neural network that makes useof a variant of the long short-term memory architecture. The extension allowsinformation low in parse trees to be stored in a memory register (the `memorycell') and used much later higher up in the parse tree. This provides asolution to the vanishing gradient problem and allows the network to capturelong range dependencies. Experimental results show that our compositionoutperformed the traditional neural-network composition on the StanfordSentiment Treebank.
arxiv-10500-80 | Distilling the Knowledge in a Neural Network | http://arxiv.org/pdf/1503.02531v1.pdf | author:Geoffrey Hinton, Oriol Vinyals, Jeff Dean category:stat.ML cs.LG cs.NE published:2015-03-09 summary:A very simple way to improve the performance of almost any machine learningalgorithm is to train many different models on the same data and then toaverage their predictions. Unfortunately, making predictions using a wholeensemble of models is cumbersome and may be too computationally expensive toallow deployment to a large number of users, especially if the individualmodels are large neural nets. Caruana and his collaborators have shown that itis possible to compress the knowledge in an ensemble into a single model whichis much easier to deploy and we develop this approach further using a differentcompression technique. We achieve some surprising results on MNIST and we showthat we can significantly improve the acoustic model of a heavily usedcommercial system by distilling the knowledge in an ensemble of models into asingle model. We also introduce a new type of ensemble composed of one or morefull models and many specialist models which learn to distinguish fine-grainedclasses that the full models confuse. Unlike a mixture of experts, thesespecialist models can be trained rapidly and in parallel.
arxiv-10500-81 | Modeling State-Conditional Observation Distribution using Weighted Stereo Samples for Factorial Speech Processing Models | http://arxiv.org/pdf/1503.02578v1.pdf | author:Mahdi Khademian, Mohammad Mehdi Homayounpour category:cs.LG cs.AI cs.SD published:2015-03-09 summary:This paper investigates the role of factorial speech processing models innoise-robust automatic speech recognition tasks. Factorial models can embednon-stationary noise models using Markov chains as one of its source chain. Thepaper proposes a modeling scheme for modeling state-conditional observationdistribution of factorial models based on weighted stereo samples. This schemeis an extension to previous single pass retraining for ideal model compensationand here we used it to construct ideal state-conditional observationdistributions. Experiments of this paper over the set A of the Aurora 2 datasetshows that by considering noise models with multiple states, system performancecan be improved especially in low SNR conditions up to 4% absolute wordrecognition performance. In addition to its power in accurate representation ofstate-conditional observation distribution, it has an important advantage overprevious methods by providing the opportunity to independently select featurespaces for both source and corrupted features. This opens a new window forseeking better feature spaces appropriate for noise-robust tasks independentfrom clean speech feature space.
arxiv-10500-82 | Bayesian Model-Averaged Regularization for Gaussian Graphical Models | http://arxiv.org/pdf/1503.02698v1.pdf | author:Zhe Liu category:stat.ML published:2015-03-09 summary:Graphical models are an intuitive way of exploring and modeling therelationships between variables. The graphical lasso has now become as a usefultool to estimate high-dimensional Gaussian graphical models, but its practicalapplications suffer from the problem of choosing regularization parameters in adata-dependent way. In this paper, we propose and analyze a model-averagedmethod for estimating sparse inverse covariance matrices for Gaussian graphicalmodels. We consider the graphical lasso regularization path as the model spacefor Bayesian model averaging and use Markov chain Monte Carlo techniques forthe regularization path point selection. Numerical performance of our method isinvestigated using both simulated and real datasets, in comparison with somestate-of-art model selection procedures.
arxiv-10500-83 | MODS: Fast and Robust Method for Two-View Matching | http://arxiv.org/pdf/1503.02619v2.pdf | author:Dmytro Mishkin, Jiri Matas, Michal Perdoch category:cs.CV published:2015-03-09 summary:A novel algorithm for wide-baseline matching called MODS - Matching On Demandwith view Synthesis - is presented. The MODS algorithm is experimentally shownto solve a broader range of wide-baseline problems than the state of the artwhile being nearly as fast as standard matchers on simple problems. Theapparent robustness vs. speed trade-off is finessed by the use of progressivelymore time-consuming feature detectors and by on-demand generation ofsynthesized images that is performed until a reliable estimate of geometry isobtained. We introduce an improved method for tentative correspondence selection,applicable both with and without view synthesis. A modification of the standardfirst to second nearest distance rule increases the number of correct matchesby 5-20% at no additional computational cost. Performance of the MODS algorithm is evaluated on several standard publiclyavailable datasets, and on a new set of geometrically challenging wide baselineproblems that is made public together with the ground truth. Experiments showthat the MODS outperforms the state-of-the-art in robustness and speed.Moreover, MODS performs well on other classes of difficult two-view problemslike matching of images from different modalities, with wide temporal baselineor with significant lighting changes.
arxiv-10500-84 | Deep Hierarchical Parsing for Semantic Segmentation | http://arxiv.org/pdf/1503.02725v2.pdf | author:Abhishek Sharma, Oncel Tuzel, David W. Jacobs category:cs.CV published:2015-03-09 summary:This paper proposes a learning-based approach to scene parsing inspired bythe deep Recursive Context Propagation Network (RCPN). RCPN is a deepfeed-forward neural network that utilizes the contextual information from theentire image, through bottom-up followed by top-down context propagation viarandom binary parse trees. This improves the feature representation of everysuper-pixel in the image for better classification into semantic categories. Weanalyze RCPN and propose two novel contributions to further improve the model.We first analyze the learning of RCPN parameters and discover the presence ofbypass error paths in the computation graph of RCPN that can hinder contextualpropagation. We propose to tackle this problem by including the classificationloss of the internal nodes of the random parse trees in the original RCPN lossfunction. Secondly, we use an MRF on the parse tree nodes to model thehierarchical dependency present in the output. Both modifications provideperformance boosts over the original RCPN and the new system achievesstate-of-the-art performance on Stanford Background, SIFT-Flow and Daimlerurban datasets.
arxiv-10500-85 | Video Compressive Sensing for Spatial Multiplexing Cameras using Motion-Flow Models | http://arxiv.org/pdf/1503.02727v2.pdf | author:Aswin C. Sankaranarayanan, Lina Xu, Christoph Studer, Yun Li, Kevin Kelly, Richard G. Baraniuk category:cs.CV published:2015-03-09 summary:Spatial multiplexing cameras (SMCs) acquire a (typically static) scenethrough a series of coded projections using a spatial light modulator (e.g., adigital micro-mirror device) and a few optical sensors. This approach finds usein imaging applications where full-frame sensors are either too expensive(e.g., for short-wave infrared wavelengths) or unavailable. Existing SMCsystems reconstruct static scenes using techniques from compressive sensing(CS). For videos, however, existing acquisition and recovery methods deliverpoor quality. In this paper, we propose the CS multi-scale video (CS-MUVI)sensing and recovery framework for high-quality video acquisition and recoveryusing SMCs. Our framework features novel sensing matrices that enable theefficient computation of a low-resolution video preview, while enablinghigh-resolution video recovery using convex optimization. To further improvethe quality of the reconstructed videos, we extract optical-flow estimates fromthe low-resolution previews and impose them as constraints in the recoveryprocedure. We demonstrate the efficacy of our CS-MUVI framework for a host ofsynthetic and real measured SMC video data, and we show that high-qualityvideos can be recovered at roughly $60\times$ compression.
arxiv-10500-86 | Deep Human Parsing with Active Template Regression | http://arxiv.org/pdf/1503.02391v1.pdf | author:Xiaodan Liang, Si Liu, Xiaohui Shen, Jianchao Yang, Luoqi Liu, Jian Dong, Liang Lin, Shuicheng Yan category:cs.CV published:2015-03-09 summary:In this work, the human parsing task, namely decomposing a human image intosemantic fashion/body regions, is formulated as an Active Template Regression(ATR) problem, where the normalized mask of each fashion/body item is expressedas the linear combination of the learned mask templates, and then morphed to amore precise mask with the active shape parameters, including position, scaleand visibility of each semantic region. The mask template coefficients and theactive shape parameters together can generate the human parsing results, andare thus called the structure outputs for human parsing. The deep ConvolutionalNeural Network (CNN) is utilized to build the end-to-end relation between theinput human image and the structure outputs for human parsing. Morespecifically, the structure outputs are predicted by two separate networks. Thefirst CNN network is with max-pooling, and designed to predict the templatecoefficients for each label mask, while the second CNN network is withoutmax-pooling to preserve sensitivity to label mask position and accuratelypredict the active shape parameters. For a new image, the structure outputs ofthe two networks are fused to generate the probability of each label for eachpixel, and super-pixel smoothing is finally used to refine the human parsingresult. Comprehensive evaluations on a large dataset well demonstrate thesignificant superiority of the ATR framework over other state-of-the-arts forhuman parsing. In particular, the F1-score reaches $64.38\%$ by our ATRframework, significantly higher than $44.76\%$ based on the state-of-the-artalgorithm.
arxiv-10500-87 | A Characterization of Deterministic Sampling Patterns for Low-Rank Matrix Completion | http://arxiv.org/pdf/1503.02596v2.pdf | author:Daniel L. Pimentel-Alarcón, Nigel Boston, Robert D. Nowak category:stat.ML cs.LG math.AG published:2015-03-09 summary:Low-rank matrix completion (LRMC) problems arise in a wide variety ofapplications. Previous theory mainly provides conditions for completion undermissing-at-random samplings. An incomplete $d \times N$ matrix is$\textit{finitely completable}$ if there are at most finitely many rank-$r$matrices that agree with all its observed entries. Finite completability is thetipping point in LRMC, as a few additional samples of a finitely completablematrix guarantee its $\textit{unique}$ completability. The main contribution ofthis paper is a full characterization of finitely completable observation sets.We use this characterization to derive sufficient deterministic samplingconditions for unique completability. We also show that under uniform randomsampling schemes, these conditions are satisfied with high probability if atleast $\mathscr{O}(\max\{r,\log d \})$ entries per column are observed.
arxiv-10500-88 | Syntax-based Deep Matching of Short Texts | http://arxiv.org/pdf/1503.02427v6.pdf | author:Mingxuan Wang, Zhengdong Lu, Hang Li, Qun Liu category:cs.CL cs.LG cs.NE published:2015-03-09 summary:Many tasks in natural language processing, ranging from machine translationto question answering, can be reduced to the problem of matching two sentencesor more generally two short texts. We propose a new approach to the problem,called Deep Match Tree (DeepMatch$_{tree}$), under a general setting. Theapproach consists of two components, 1) a mining algorithm to discover patternsfor matching two short-texts, defined in the product space of dependency trees,and 2) a deep neural network for matching short texts using the mined patterns,as well as a learning algorithm to build the network having a sparse structure.We test our algorithm on the problem of matching a tweet and a response insocial media, a hard matching problem proposed in [Wang et al., 2013], and showthat DeepMatch$_{tree}$ can outperform a number of competitor models includingone without using dependency trees and one based on word-embedding, all withlarge margins
arxiv-10500-89 | Deep Learning and the Information Bottleneck Principle | http://arxiv.org/pdf/1503.02406v1.pdf | author:Naftali Tishby, Noga Zaslavsky category:cs.LG published:2015-03-09 summary:Deep Neural Networks (DNNs) are analyzed via the theoretical framework of theinformation bottleneck (IB) principle. We first show that any DNN can bequantified by the mutual information between the layers and the input andoutput variables. Using this representation we can calculate the optimalinformation theoretic limits of the DNN and obtain finite sample generalizationbounds. The advantage of getting closer to the theoretical limit isquantifiable both by the generalization bound and by the network's simplicity.We argue that both the optimal architecture, number of layers andfeatures/connections at each layer, are related to the bifurcation points ofthe information bottleneck tradeoff, namely, relevant compression of the inputlayer with respect to the output layer. The hierarchical representations at thelayered network naturally correspond to the structural phase transitions alongthe information curve. We believe that this new insight can lead to newoptimality bounds and deep learning algorithms.
arxiv-10500-90 | Mathematical understanding of detailed balance condition violation and its application to Langevin dynamics | http://arxiv.org/pdf/1503.02356v1.pdf | author:M. Ohzeki, A. Ichiki category:stat.ML published:2015-03-09 summary:We develop an efficient sampling method by simulating Langevin dynamics withan artificial force rather than a natural force by using the gradient of thepotential energy. The standard technique for sampling following thepredetermined distribution such as the Gibbs-Boltzmann one is performed underthe detailed balance condition. In the present study, we propose a modifiedLangevin dynamics violating the detailed balance condition on thetransition-probability formulation. We confirm that the numericalimplementation of the proposed method actually demonstrates two majorbeneficial improvements: acceleration of the relaxation to the predetermineddistribution and reduction of the correlation time between two differentrealizations in the steady state.
arxiv-10500-91 | A Smoothed Dual Approach for Variational Wasserstein Problems | http://arxiv.org/pdf/1503.02533v2.pdf | author:Marco Cuturi, Gabriel Peyré category:stat.ML math.OC published:2015-03-09 summary:Variational problems that involve Wasserstein distances have been recentlyproposed to summarize and learn from probability measures. Despite beingconceptually simple, such problems are computationally challenging because theyinvolve minimizing over quantities (Wasserstein distances) that are themselveshard to compute. We show that the dual formulation of Wasserstein variationalproblems introduced recently by Carlier et al. (2014) can be regularized usingan entropic smoothing, which leads to smooth, differentiable, convexoptimization problems that are simpler to implement and numerically morestable. We illustrate the versatility of this approach by applying it to thecomputation of Wasserstein barycenters and gradient flows of spacialregularization functionals.
arxiv-10500-92 | Structured Prediction of Sequences and Trees using Infinite Contexts | http://arxiv.org/pdf/1503.02417v1.pdf | author:Ehsan Shareghi, Gholamreza Haffari, Trevor Cohn, Ann Nicholson category:cs.LG cs.CL published:2015-03-09 summary:Linguistic structures exhibit a rich array of global phenomena, howevercommonly used Markov models are unable to adequately describe these phenomenadue to their strong locality assumptions. We propose a novel hierarchical modelfor structured prediction over sequences and trees which exploits globalcontext by conditioning each generation decision on an unbounded context ofprior decisions. This builds on the success of Markov models but withoutimposing a fixed bound in order to better represent global phenomena. Tofacilitate learning of this large and unbounded model, we use a hierarchicalPitman-Yor process prior which provides a recursive form of smoothing. Wepropose prediction algorithms based on A* and Markov Chain Monte Carlosampling. Empirical results demonstrate the potential of our model compared tobaseline finite-context Markov models on part-of-speech tagging and syntacticparsing.
arxiv-10500-93 | Fitting 3D Morphable Models using Local Features | http://arxiv.org/pdf/1503.02330v1.pdf | author:Patrik Huber, Zhen-Hua Feng, William Christmas, Josef Kittler, Matthias Rätsch category:cs.CV 68T45 I.4.8; I.2.10 published:2015-03-08 summary:In this paper, we propose a novel fitting method that uses local imagefeatures to fit a 3D Morphable Model to 2D images. To overcome the obstacle ofoptimising a cost function that contains a non-differentiable featureextraction operator, we use a learning-based cascaded regression method thatlearns the gradient direction from data. The method allows to simultaneouslysolve for shape and pose parameters. Our method is thoroughly evaluated onMorphable Model generated data and first results on real data are presented.Compared to traditional fitting methods, which use simple raw features likepixel colour or edge maps, local features have been shown to be much morerobust against variations in imaging conditions. Our approach is unique in thatwe are the first to use local features to fit a Morphable Model. Because of the speed of our method, it is applicable for realtimeapplications. Our cascaded regression framework is available as an open sourcelibrary (https://github.com/patrikhuber).
arxiv-10500-94 | Financial Market Prediction | http://arxiv.org/pdf/1503.02328v1.pdf | author:Mike Wu category:cs.CE cs.LG published:2015-03-08 summary:Given financial data from popular sites like Yahoo and the London Exchange,the presented paper attempts to model and predict stocks that can be considered"good investments". Stocks are characterized by 125 features ranging from grossdomestic product to EDIBTA, and are labeled by discrepancies between stock andmarket price returns. An artificial neural network (Self-Organizing Map) isfitted to train on more than a million data points to predict "goodinvestments" given testing stocks from 2013 and after.
arxiv-10500-95 | DESAT: an SSW tool for SDO/AIA image de-saturation | http://arxiv.org/pdf/1503.02302v1.pdf | author:Richard A Schwartz, Gabriele Torre, Anna Maria Massone, Michele Piana category:astro-ph.IM cs.CV 85-08, 68U10 published:2015-03-08 summary:Saturation affects a significant rate of images recorded by the AtmosphericImaging Assembly on the Solar Dynamics Observatory. This paper describes acomputational method and a technological pipeline for the de-saturation of suchimages, based on several mathematical ingredients like ExpectationMaximization, image correlation and interpolation. An analysis of thecomputational properties and demands of the pipeline, together with anassessment of its reliability are performed against a set of data recorded fromthe Feburary 25 2014 flaring event.
arxiv-10500-96 | An Unsupervised Method for Uncovering Morphological Chains | http://arxiv.org/pdf/1503.02335v1.pdf | author:Karthik Narasimhan, Regina Barzilay, Tommi Jaakkola category:cs.CL published:2015-03-08 summary:Most state-of-the-art systems today produce morphological analysis based onlyon orthographic patterns. In contrast, we propose a model for unsupervisedmorphological analysis that integrates orthographic and semantic views ofwords. We model word formation in terms of morphological chains, from basewords to the observed words, breaking the chains into parent-child relations.We use log-linear models with morpheme and word-level features to predictpossible parents, including their modifications, for each word. The limited setof candidate parents for each word render contrastive estimation feasible. Ourmodel consistently matches or outperforms five state-of-the-art systems onArabic, English and Turkish.
arxiv-10500-97 | Understanding Image Virality | http://arxiv.org/pdf/1503.02318v3.pdf | author:Arturo Deza, Devi Parikh category:cs.SI cs.CV published:2015-03-08 summary:Virality of online content on social networking websites is an important butesoteric phenomenon often studied in fields like marketing, psychology and datamining. In this paper we study viral images from a computer vision perspective.We introduce three new image datasets from Reddit, and define a virality scoreusing Reddit metadata. We train classifiers with state-of-the-art imagefeatures to predict virality of individual images, relative virality in pairsof images, and the dominant topic of a viral image. We also compare machineperformance to human performance on these tasks. We find that computers performpoorly with low level features, and high level information is critical forpredicting virality. We encode semantic information through relativeattributes. We identify the 5 key visual attributes that correlate withvirality. We create an attribute-based characterization of images that canpredict relative virality with 68.10% accuracy (SVM+Deep Relative Attributes)-- better than humans at 60.12%. Finally, we study how human prediction ofimage virality varies with different `contexts' in which the images are viewed,such as the influence of neighbouring images, images recently viewed, as wellas the image title or caption. This work is a first step in understanding thecomplex but important phenomenon of image virality. Our datasets andannotations will be made publicly available.
arxiv-10500-98 | One Scan 1-Bit Compressed Sensing | http://arxiv.org/pdf/1503.02346v2.pdf | author:Ping Li category:stat.ME cs.IT cs.LG math.IT published:2015-03-08 summary:Based on $\alpha$-stable random projections with small $\alpha$, we develop asimple algorithm for compressed sensing (sparse signal recovery) by utilizingonly the signs (i.e., 1-bit) of the measurements. Using only 1-bit informationof the measurements results in substantial cost reduction in collection,storage, communication, and decoding for compressed sensing. The proposedalgorithm is efficient in that the decoding procedure requires only one scan ofthe coordinates. Our analysis can precisely show that, for a $K$-sparse signalof length $N$, $12.3K\log N/\delta$ measurements (where $\delta$ is theconfidence) would be sufficient for recovering the support and the signs of thesignal. While the method is very robust against typical measurement noises, wealso provide the analysis of the scheme under random flipping of the signs ofthe measurements. \noindent Compared to the well-known work on 1-bit marginal regression (whichcan also be viewed as a one-scan method), the proposed algorithm requiresorders of magnitude fewer measurements. Compared to 1-bit Iterative HardThresholding (IHT) (which is not a one-scan algorithm), our method is stillsignificantly more accurate. Furthermore, the proposed method is reasonablyrobust against random sign flipping while IHT is known to be very sensitive tothis type of noise.
arxiv-10500-99 | TED: A Tolerant Edit Distance for Segmentation Evaluation | http://arxiv.org/pdf/1503.02291v3.pdf | author:Jan Funke, Francesc Moreno-Noguer, Albert Cardona, Matthew Cook category:cs.CV published:2015-03-08 summary:In this paper, we present a novel error measure to compare a segmentationagainst ground truth. This measure, which we call Tolerant Edit Distance (TED),is motivated by two observations: (1) Some errors, like small boundary shifts,are tolerable in practice. Which errors are tolerable is application dependentand should be a parameter of the measure. (2) Non-tolerable errors have to becorrected manually. The time needed to do so should be reflected by the errormeasure. Using integer linear programming, the TED finds the minimal weightedsum of split and merge errors exceeding a given tolerance criterion, and thusprovides a time-to-fix estimate. In contrast to commonly used measures likeRand index or variation of information, the TED (1) does not count small, buttolerable, differences, (2) provides intuitive numbers, (3) gives a time-to-fixestimate, and (4) can localize and classify the type of errors. By supportingboth isotropic and anisotropic volumes and having a flexible tolerancecriterion, the TED can be adapted to different requirements. On examplesegmentations for 3D neuron segmentation, we demonstrate that the TED iscapable of counting topological errors, while ignoring small boundary shifts.
arxiv-10500-100 | A Nonconvex Approach for Structured Sparse Learning | http://arxiv.org/pdf/1503.02164v1.pdf | author:Shubao Zhang, Hui Qian, Zhihua Zhang category:cs.IT cs.LG math.IT published:2015-03-07 summary:Sparse learning is an important topic in many areas such as machine learning,statistical estimation, signal processing, etc. Recently, there emerges agrowing interest on structured sparse learning. In this paper we focus on the$\ell_q$-analysis optimization problem for structured sparse learning ($0< q\leq 1$). Compared to previous work, we establish weaker conditions for exactrecovery in noiseless case and a tighter non-asymptotic upper bound of estimateerror in noisy case. We further prove that the nonconvex $\ell_q$-analysisoptimization can do recovery with a lower sample complexity and in a widerrange of cosparsity than its convex counterpart. In addition, we develop aniteratively reweighted method to solve the optimization problem under thevariational framework. Theoretical analysis shows that our method is capable ofpursuing a local minima close to the global minima. Also, empirical results ofpreliminary computational experiments illustrate that our nonconvex methodoutperforms both its convex counterpart and other state-of-the-art methods.
arxiv-10500-101 | Sparse Bayesian Dictionary Learning with a Gaussian Hierarchical Model | http://arxiv.org/pdf/1503.02144v1.pdf | author:Linxiao Yang, Jun Fang, Hong Cheng, Hongbin Li category:cs.LG cs.IT math.IT published:2015-03-07 summary:We consider a dictionary learning problem whose objective is to design adictionary such that the signals admits a sparse or an approximate sparserepresentation over the learned dictionary. Such a problem finds a variety ofapplications such as image denoising, feature extraction, etc. In this paper,we propose a new hierarchical Bayesian model for dictionary learning, in whicha Gaussian-inverse Gamma hierarchical prior is used to promote the sparsity ofthe representation. Suitable priors are also placed on the dictionary and thenoise variance such that they can be reasonably inferred from the data. Basedon the hierarchical model, a variational Bayesian method and a Gibbs samplingmethod are developed for Bayesian inference. The proposed methods have theadvantage that they do not require the knowledge of the noise variance \emph{apriori}. Numerical results show that the proposed methods are able to learn thedictionary with an accuracy better than existing methods, particularly for thecase where there is a limited number of training signals.
arxiv-10500-102 | Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data | http://arxiv.org/pdf/1503.02182v1.pdf | author:Yarin Gal, Yutian Chen, Zoubin Ghahramani category:stat.ML published:2015-03-07 summary:Multivariate categorical data occur in many applications of machine learning.One of the main difficulties with these vectors of categorical variables issparsity. The number of possible observations grows exponentially with vectorlength, but dataset diversity might be poor in comparison. Recent models havegained significant improvement in supervised tasks with this data. These modelsembed observations in a continuous space to capture similarities between them.Building on these ideas we propose a Bayesian model for the unsupervised taskof distribution estimation of multivariate categorical data. We model vectorsof categorical variables as generated from a non-linear transformation of acontinuous latent space. Non-linearity captures multi-modality in thedistribution. The continuous representation addresses sparsity. Our model tiestogether many existing models, linking the linear categorical latent Gaussianmodel, the Gaussian process latent variable model, and Gaussian processclassification. We derive inference for our model based on recent developmentsin sampling based variational inference. We show empirically that the modeloutperforms its linear and discrete counterparts in imputation tasks of sparsedata.
arxiv-10500-103 | Model selection of polynomial kernel regression | http://arxiv.org/pdf/1503.02143v1.pdf | author:Shaobo Lin, Xingping Sun, Zongben Xu, Jinshan Zeng category:cs.LG F.2.2 published:2015-03-07 summary:Polynomial kernel regression is one of the standard and state-of-the-artlearning strategies. However, as is well known, the choices of the degree ofpolynomial kernel and the regularization parameter are still open in the realmof model selection. The first aim of this paper is to develop a strategy toselect these parameters. On one hand, based on the worst-case learning rateanalysis, we show that the regularization term in polynomial kernel regressionis not necessary. In other words, the regularization parameter can decreasearbitrarily fast when the degree of the polynomial kernel is suitable tuned. Onthe other hand,taking account of the implementation of the algorithm, theregularization term is required. Summarily, the effect of the regularizationterm in polynomial kernel regression is only to circumvent the " ill-condition"of the kernel matrix. Based on this, the second purpose of this paper is topropose a new model selection strategy, and then design an efficient learningalgorithm. Both theoretical and experimental analysis show that the newstrategy outperforms the previous one. Theoretically, we prove that the newlearning strategy is almost optimal if the regression function is smooth.Experimentally, it is shown that the new strategy can significantly reduce thecomputational burden without loss of generalization capability.
arxiv-10500-104 | An Improved Image Mosaicing Algorithm for Damaged Documents | http://arxiv.org/pdf/1503.02136v1.pdf | author:Waheeda Dhokley, Khan Munifa, Shaikh Nazia, Shaikh Saiqua category:cs.CV published:2015-03-07 summary:It is a common phenomenon in day to day life; where in some of the documentgets damaged. Out of several reasons, the main reason for documents gettingdamaged is shredding by hands. Recovery of such documents is essential. Manualrecovery of such damaged document is tedious and time consuming task. In thispaper, we are describing an algorithm which recovers the original document fromsuch shredded pieces of the same. In order to implement this, we are using asimple technique called Image Mosaicing. In this technique a complete new imageis developed using two or more torn fragments. For simplicity ofimplementation, we are considering only two torn pieces of a document that willbe mosaiced together. The successful implementation of this algorithm wouldlead to recovery of important information which in turn would be beneficial invarious fields such as forensic sciences, archival study, etc
arxiv-10500-105 | Community Detection and Classification in Hierarchical Stochastic Blockmodels | http://arxiv.org/pdf/1503.02115v3.pdf | author:Vince Lyzinski, Minh Tang, Avanti Athreya, Youngser Park, Carey E. Priebe category:stat.ML stat.AP published:2015-03-07 summary:We propose a robust, scalable, integrated methodology for community detectionand community comparison in graphs. In our procedure, we first embed a graphinto an appropriate Euclidean space to obtain a low-dimensional representation,and then cluster the vertices into communities. We next employ nonparametricgraph inference techniques to identify structural similarity among thesecommunities. These two steps are then applied recursively on the communities,allowing us to detect more fine-grained structure. We describe a hierarchicalstochastic blockmodel---namely, a stochastic blockmodel with a naturalhierarchical structure---and establish conditions under which our algorithmyields consistent estimates of model parameters and motifs, which we define tobe stochastically similar groups of subgraphs. Finally, we demonstrate theeffectiveness of our algorithm in both simulated and real data. Specifically,we address the problem of locating similar subcommunities in a partiallyreconstructed Drosophila connectome and in the social network Friendster.
arxiv-10500-106 | Higher order Matching Pursuit for Low Rank Tensor Learning | http://arxiv.org/pdf/1503.02216v1.pdf | author:Yuning Yang, Siamak Mehrkanoon, Johan A. K. Suykens category:stat.ML cs.LG math.OC published:2015-03-07 summary:Low rank tensor learning, such as tensor completion and multilinear multitasklearning, has received much attention in recent years. In this paper, wepropose higher order matching pursuit for low rank tensor learning problemswith a convex or a nonconvex cost function, which is a generalization of thematching pursuit type methods. At each iteration, the main cost of the proposedmethods is only to compute a rank-one tensor, which can be done efficiently,making the proposed methods scalable to large scale problems. Moreover, storingthe resulting rank-one tensors is of low storage requirement, which can help tobreak the curse of dimensionality. The linear convergence rate of the proposedmethods is established in various circumstances. Along with the main methods,we also provide a method of low computational complexity for approximatelycomputing the rank-one tensors, with provable approximation ratio, which helpsto improve the efficiency of the main methods and to analyze the convergencerate. Experimental results on synthetic as well as real datasets verify theefficiency and effectiveness of the proposed methods.
arxiv-10500-107 | Estimating the Mean Number of K-Means Clusters to Form | http://arxiv.org/pdf/1503.03488v2.pdf | author:Robert A. Murphy category:cs.LG 60D05 published:2015-03-07 summary:Utilizing the sample size of a dataset, the random cluster model is employedin order to derive an estimate of the mean number of K-Means clusters to formduring classification of a dataset.
arxiv-10500-108 | Identifying missing dictionary entries with frequency-conserving context models | http://arxiv.org/pdf/1503.02120v3.pdf | author:Jake Ryland Williams, Eric M. Clark, James P. Bagrow, Christopher M. Danforth, Peter Sheridan Dodds category:cs.CL cs.IT math.IT stat.ML published:2015-03-07 summary:In an effort to better understand meaning from natural language texts, weexplore methods aimed at organizing lexical objects into contexts. A number ofthese methods for organization fall into a family defined by word ordering.Unlike demographic or spatial partitions of data, these collocation models areof special importance for their universal applicability. While we areinterested here in text and have framed our treatment appropriately, our workis potentially applicable to other areas of research (e.g., speech, genomics,and mobility patterns) where one has ordered categorical data, (e.g., sounds,genes, and locations). Our approach focuses on the phrase (whether word orlarger) as the primary meaning-bearing lexical unit and object of study. To doso, we employ our previously developed framework for generating word-conservingphrase-frequency data. Upon training our model with the Wiktionary---anextensive, online, collaborative, and open-source dictionary that contains over100,000 phrasal-definitions---we develop highly effective filters for theidentification of meaningful, missing phrase-entries. With our predictions wethen engage the editorial community of the Wiktionary and propose short listsof potential missing entries for definition, developing a breakthrough, lexicalextraction technique, and expanding our knowledge of the defined Englishlexicon of phrases.
arxiv-10500-109 | Learning Scale-Free Networks by Dynamic Node-Specific Degree Prior | http://arxiv.org/pdf/1503.02129v3.pdf | author:Qingming Tang, Siqi Sun, Jinbo Xu category:cs.LG cs.AI stat.ML published:2015-03-07 summary:Learning the network structure underlying data is an important problem inmachine learning. This paper introduces a novel prior to study the inference ofscale-free networks, which are widely used to model social and biologicalnetworks. The prior not only favors a desirable global node degreedistribution, but also takes into consideration the relative strength of allthe possible edges adjacent to the same node and the estimated degree of eachindividual node. To fulfill this, ranking is incorporated into the prior, which makes theproblem challenging to solve. We employ an ADMM (alternating direction methodof multipliers) framework to solve the Gaussian Graphical model regularized bythis prior. Our experiments on both synthetic and real data show that our priornot only yields a scale-free network, but also produces many more correctlypredicted edges than the others such as the scale-free inducing prior, thehub-inducing prior and the $l_1$ norm.
arxiv-10500-110 | Exact Hybrid Covariance Thresholding for Joint Graphical Lasso | http://arxiv.org/pdf/1503.02128v2.pdf | author:Qingming Tang, Chao Yang, Jian Peng, Jinbo Xu category:cs.LG cs.AI stat.ML published:2015-03-07 summary:This paper considers the problem of estimating multiple related Gaussiangraphical models from a $p$-dimensional dataset consisting of differentclasses. Our work is based upon the formulation of this problem as groupgraphical lasso. This paper proposes a novel hybrid covariance thresholdingalgorithm that can effectively identify zero entries in the precision matricesand split a large joint graphical lasso problem into small subproblems. Ourhybrid covariance thresholding method is superior to existing uniformthresholding methods in that our method can split the precision matrix of eachindividual class using different partition schemes and thus split groupgraphical lasso into much smaller subproblems, each of which can be solved veryfast. In addition, this paper establishes necessary and sufficient conditionsfor our hybrid covariance thresholding algorithm. The superior performance ofour thresholding method is thoroughly analyzed and illustrated by a fewexperiments on simulated data and real gene expression data.
arxiv-10500-111 | Label optimal regret bounds for online local learning | http://arxiv.org/pdf/1503.02193v2.pdf | author:Pranjal Awasthi, Moses Charikar, Kevin A. Lai, Andrej Risteski category:cs.LG published:2015-03-07 summary:We resolve an open question from (Christiano, 2014b) posed in COLT'14regarding the optimal dependency of the regret achievable for online locallearning on the size of the label set. In this framework the algorithm is showna pair of items at each step, chosen from a set of $n$ items. The learner thenpredicts a label for each item, from a label set of size $L$ and receives areal valued payoff. This is a natural framework which captures many interestingscenarios such as collaborative filtering, online gambling, and online max cutamong others. (Christiano, 2014a) designed an efficient online learningalgorithm for this problem achieving a regret of $O(\sqrt{nL^3T})$, where $T$is the number of rounds. Information theoretically, one can achieve a regret of$O(\sqrt{n \log L T})$. One of the main open questions left in this frameworkconcerns closing the above gap. In this work, we provide a complete answer to the question above via two mainresults. We show, via a tighter analysis, that the semi-definite programmingbased algorithm of (Christiano, 2014a), in fact achieves a regret of$O(\sqrt{nLT})$. Second, we show a matching computational lower bound. Namely,we show that a polynomial time algorithm for online local learning with lowerregret would imply a polynomial time algorithm for the planted clique problemwhich is widely believed to be hard. We prove a similar hardness result under arelated conjecture concerning planted dense subgraphs that we put forth. Unlikeplanted clique, the planted dense subgraph problem does not have any knownquasi-polynomial time algorithms. Computational lower bounds for online learning are relatively rare, and wehope that the ideas developed in this work will lead to lower bounds for otheronline learning scenarios as well.
arxiv-10500-112 | Hamiltonian ABC | http://arxiv.org/pdf/1503.01916v1.pdf | author:Edward Meeds, Robert Leenders, Max Welling category:stat.ML cs.LG q-bio.QM published:2015-03-06 summary:Approximate Bayesian computation (ABC) is a powerful and elegant frameworkfor performing inference in simulation-based models. However, due to thedifficulty in scaling likelihood estimates, ABC remains useful for relativelylow-dimensional problems. We introduce Hamiltonian ABC (HABC), a set oflikelihood-free algorithms that apply recent advances in scaling Bayesianlearning using Hamiltonian Monte Carlo (HMC) and stochastic gradients. We findthat a small number forward simulations can effectively approximate the ABCgradient, allowing Hamiltonian dynamics to efficiently traverse parameterspaces. We also describe a new simple yet general approach of incorporatingrandom seeds into the state of the Markov chain, further reducing the randomwalk behavior of HABC. We demonstrate HABC on several typical ABC problems, andshow that HABC samples comparably to regular Bayesian inference using truegradients on a high-dimensional problem from machine learning.
arxiv-10500-113 | Sequential Relevance Maximization with Binary Feedback | http://arxiv.org/pdf/1503.01910v1.pdf | author:Vijay Kamble, Nadia Fawaz, Fernando Silveira category:cs.LG cs.AI published:2015-03-06 summary:Motivated by online settings where users can provide explicit feedback aboutthe relevance of products that are sequentially presented to them, we look atthe recommendation process as a problem of dynamically optimizing thisrelevance feedback. Such an algorithm optimizes the fine tradeoff betweenpresenting the products that are most likely to be relevant, and learning thepreferences of the user so that more relevant recommendations can be made inthe future. We assume a standard predictive model inspired by collaborative filtering, inwhich a user is sampled from a distribution over a set of possible types. Forevery product category, each type has an associated relevance feedback that isassumed to be binary: the category is either relevant or irrelevant. Assumingthat the user stays for each additional recommendation opportunity withprobability $\beta$ independent of the past, the problem is to find a policythat maximizes the expected number of recommendations that are deemed relevantin a session. We analyze this problem and prove key structural properties of the optimalpolicy. Based on these properties, we first present an algorithm that strikes abalance between recursion and dynamic programming to compute this policy. Wefurther propose and analyze two heuristic policies: a `farsighted' greedypolicy that attains at least $1-\beta$ factor of the optimal payoff, and anaive greedy policy that attains at least $\frac{1-\beta}{1+\beta}$ factor ofthe optimal payoff in the worst case. Extensive simulations show that theseheuristics are very close to optimal in practice.
arxiv-10500-114 | Escaping From Saddle Points --- Online Stochastic Gradient for Tensor Decomposition | http://arxiv.org/pdf/1503.02101v1.pdf | author:Rong Ge, Furong Huang, Chi Jin, Yang Yuan category:cs.LG math.OC stat.ML published:2015-03-06 summary:We analyze stochastic gradient descent for optimizing non-convex functions.In many cases for non-convex functions the goal is to find a reasonable localminimum, and the main concern is that gradient updates are trapped in saddlepoints. In this paper we identify strict saddle property for non-convex problemthat allows for efficient optimization. Using this property we show thatstochastic gradient descent converges to a local minimum in a polynomial numberof iterations. To the best of our knowledge this is the first work that givesglobal convergence guarantees for stochastic gradient descent on non-convexfunctions with exponentially many local minima and saddle points. Our analysiscan be applied to orthogonal tensor decomposition, which is widely used inlearning a rich class of latent variable models. We propose a new optimizationformulation for the tensor decomposition problem that has strict saddleproperty. As a result we get the first online algorithm for orthogonal tensordecomposition with global convergence guarantee.
arxiv-10500-115 | Encoding Source Language with Convolutional Neural Network for Machine Translation | http://arxiv.org/pdf/1503.01838v5.pdf | author:Fandong Meng, Zhengdong Lu, Mingxuan Wang, Hang Li, Wenbin Jiang, Qun Liu category:cs.CL cs.LG cs.NE published:2015-03-06 summary:The recently proposed neural network joint model (NNJM) (Devlin et al., 2014)augments the n-gram target language model with a heuristically chosen sourcecontext window, achieving state-of-the-art performance in SMT. In this paper,we give a more systematic treatment by summarizing the relevant sourceinformation through a convolutional architecture guided by the targetinformation. With different guiding signals during decoding, our specificallydesigned convolution+gating architectures can pinpoint the parts of a sourcesentence that are relevant to predicting a target word, and fuse them with thecontext of entire source sentence to form a unified representation. Thisrepresentation, together with target language words, are fed to a deep neuralnetwork (DNN) to form a stronger NNJM. Experiments on two NIST Chinese-Englishtranslation tasks show that the proposed model can achieve significantimprovements over the previous NNJM by up to +1.08 BLEU points on average
arxiv-10500-116 | Maximum a Posteriori Adaptation of Network Parameters in Deep Models | http://arxiv.org/pdf/1503.02108v2.pdf | author:Zhen Huang, Sabato Marco Siniscalchi, I-Fan Chen, Jiadong Wu, Chin-Hui Lee category:cs.LG cs.CL cs.NE published:2015-03-06 summary:We present a Bayesian approach to adapting parameters of a well-trainedcontext-dependent, deep-neural-network, hidden Markov model (CD-DNN-HMM) toimprove automatic speech recognition performance. Given an abundance of DNNparameters but with only a limited amount of data, the effectiveness of theadapted DNN model can often be compromised. We formulate maximum a posteriori(MAP) adaptation of parameters of a specially designed CD-DNN-HMM with anaugmented linear hidden networks connected to the output tied states, orsenones, and compare it to feature space MAP linear regression previouslyproposed. Experimental evidences on the 20,000-word open vocabulary Wall StreetJournal task demonstrate the feasibility of the proposed framework. Insupervised adaptation, the proposed MAP adaptation approach provides more than10% relative error reduction and consistently outperforms the conventionaltransformation based methods. Furthermore, we present an initial attempt togenerate hierarchical priors to improve adaptation efficiency and effectivenesswith limited adaptation data by exploiting similarities among senones.
arxiv-10500-117 | Partial light field tomographic reconstruction from a fixed-camera focal stack | http://arxiv.org/pdf/1503.01903v1.pdf | author:A. Mousnier, E. Vural, C. Guillemot category:cs.CV cs.GR published:2015-03-06 summary:This paper describes a novel approach to partially reconstructhigh-resolution 4D light fields from a stack of differently focused photographstaken with a fixed camera. First, a focus map is calculated from this stackusing a simple approach combining gradient detection and region expansion withgraph-cut. Then, this focus map is converted into a depth map thanks to thecalibration of the camera. We proceed after this with the tomographicreconstruction of the epipolar images by back-projecting the focused regions ofthe scene only. We call it masked back-projection. The angles ofback-projection are calculated from the depth map. Thanks to the high angularresolution we achieve by suitably exploiting the image content captured over alarge interval of focus distances, we are able to render puzzling perspectiveshifts although the original photographs were taken from a single fixed cameraat a fixed position.
arxiv-10500-118 | Ranking and significance of variable-length similarity-based time series motifs | http://arxiv.org/pdf/1503.01883v1.pdf | author:Joan Serrà, Isabel Serra, Álvaro Corral, Josep Lluis Arcos category:cs.LG published:2015-03-06 summary:The detection of very similar patterns in a time series, commonly calledmotifs, has received continuous and increasing attention from diversescientific communities. In particular, recent approaches for discoveringsimilar motifs of different lengths have been proposed. In this work, we showthat such variable-length similarity-based motifs cannot be directly compared,and hence ranked, by their normalized dissimilarities. Specifically, we findthat length-normalized motif dissimilarities still have intrinsic dependencieson the motif length, and that lowest dissimilarities are particularly affectedby this dependency. Moreover, we find that such dependencies are generallynon-linear and change with the considered data set and dissimilarity measure.Based on these findings, we propose a solution to rank those motifs and measuretheir significance. This solution relies on a compact but accurate model of thedissimilarity space, using a beta distribution with three parameters thatdepend on the motif length in a non-linear way. We believe the incomparabilityof variable-length dissimilarities could go beyond the field of time series,and that similar modeling strategies as the one used here could be of help in amore broad context.
arxiv-10500-119 | Estimation of the parameters of an infectious disease model using neural networks | http://arxiv.org/pdf/1503.01847v1.pdf | author:V. Sree Hari Rao, M. Naresh Kumar category:cs.NE published:2015-03-06 summary:In this paper, we propose a realistic mathematical model taking into accountthe mutual interference among the interacting populations. This model attemptsto describe the control (vaccination) function as a function of the number ofinfective individuals, which is an improvement over the existing susceptible?infective epidemic models. Regarding the growth of the epidemic as a nonlinearphenomenon we have developed a neural network architecture to estimate thevital parameters associated with this model. This architecture is based on arecently developed new class of neural networks known as co-operative andsupportive neural networks. The application of this architecture to the presentstudy involves preprocessing of the input data, and this renders an efficientestimation of the rate of spread of the epidemic. It is observed that theproposed new neural network outperforms a simple feed-forward neural networkand polynomial regression.
arxiv-10500-120 | Deep Clustered Convolutional Kernels | http://arxiv.org/pdf/1503.01824v1.pdf | author:Minyoung Kim, Luca Rigazio category:cs.LG cs.NE published:2015-03-06 summary:Deep neural networks have recently achieved state of the art performancethanks to new training algorithms for rapid parameter estimation and newregularization methods to reduce overfitting. However, in practice the networkarchitecture has to be manually set by domain experts, generally by a costlytrial and error procedure, which often accounts for a large portion of thefinal system performance. We view this as a limitation and propose a noveltraining algorithm that automatically optimizes network architecture, byprogressively increasing model complexity and then eliminating model redundancyby selectively removing parameters at training time. For convolutional neuralnetworks, our method relies on iterative split/merge clustering ofconvolutional kernels interleaved by stochastic gradient descent. We present atraining algorithm and experimental results on three different vision tasks,showing improved performance compared to similarly sized hand-craftedarchitectures.
arxiv-10500-121 | Fast image-based obstacle detection from unmanned surface vehicles | http://arxiv.org/pdf/1503.01918v1.pdf | author:Matej Kristan, Vildana Sulic, Stanislav Kovacic, Janez Pers category:cs.CV published:2015-03-06 summary:Obstacle detection plays an important role in unmanned surface vehicles(USV). The USVs operate in highly diverse environments in which an obstacle maybe a floating piece of wood, a scuba diver, a pier, or a part of a shoreline,which presents a significant challenge to continuous detection from imagestaken onboard. This paper addresses the problem of online detection byconstrained unsupervised segmentation. To this end, a new graphical model isproposed that affords a fast and continuous obstacle image-map estimation froma single video stream captured onboard a USV. The model accounts for thesemantic structure of marine environment as observed from USV by imposing weakstructural constraints. A Markov random field framework is adopted and a highlyefficient algorithm for simultaneous optimization of model parameters andsegmentation mask estimation is derived. Our approach does not requirecomputationally intensive extraction of texture features and comfortably runsin real-time. The algorithm is tested on a new, challenging, dataset forsegmentation and obstacle detection in marine environments, which is thelargest annotated dataset of its kind. Results on this dataset show that ourmodel outperforms the related approaches, while requiring a fraction ofcomputational effort.
arxiv-10500-122 | On the Invariance of Dictionary Learning and Sparse Representation to Projecting Data to a Discriminative Space | http://arxiv.org/pdf/1503.02041v2.pdf | author:Mehrdad J. Gangeh, Ali Ghodsi category:cs.CV published:2015-03-06 summary:In this paper, it is proved that dictionary learning and sparserepresentation is invariant to a linear transformation. It subsumes the specialcase of transforming/projecting the data into a discriminative space. This isimportant because recently, supervised dictionary learning algorithms have beenproposed, which suggest to include the category information into the learningof dictionary to improve its discriminative power. Among them, there are someapproaches that propose to learn the dictionary in a discriminative projectedspace. To this end, two approaches have been proposed: first, assigning thediscriminative basis as the dictionary and second, perform dictionary learningin the projected space. Based on the invariance of dictionary learning to anytransformation in general, and to a discriminative space in particular, weadvocate the first approach.
arxiv-10500-123 | Convolutional LSTM Networks for Subcellular Localization of Proteins | http://arxiv.org/pdf/1503.01919v1.pdf | author:Søren Kaae Sønderby, Casper Kaae Sønderby, Henrik Nielsen, Ole Winther category:q-bio.QM cs.NE published:2015-03-06 summary:Machine learning is widely used to analyze biological sequence data.Non-sequential models such as SVMs or feed-forward neural networks are oftenused although they have no natural way of handling sequences of varying length.Recurrent neural networks such as the long short term memory (LSTM) model onthe other hand are designed to handle sequences. In this study we demonstratethat LSTM networks predict the subcellular location of proteins given only theprotein sequence with high accuracy (0.902) outperforming current state of theart algorithms. We further improve the performance by introducing convolutionalfilters and experiment with an attention mechanism which lets the LSTM focus onspecific parts of the protein. Lastly we introduce new visualizations of boththe convolutional filters and the attention mechanisms and show how they can beused to extract biological relevant knowledge from the LSTM networks.
arxiv-10500-124 | Tomographic Image Reconstruction using Training images | http://arxiv.org/pdf/1503.01993v2.pdf | author:Sara Soltani, Martin S. Andersen, Per Christian Hansen category:cs.CV math.NA 65F22, 65K10 published:2015-03-06 summary:We describe and examine an algorithm for tomographic image reconstructionwhere prior knowledge about the solution is available in the form of trainingimages. We first construct a nonnegative dictionary based on prototype elementsfrom the training images; this problem is formulated as a regularizednon-negative matrix factorization. Incorporating the dictionary as a prior in aconvex reconstruction problem, we then find an approximate solution with asparse representation in the dictionary. The dictionary is applied tonon-overlapping patches of the image, which reduces the computationalcomplexity compared to other algorithms. Computational experiments clarify thechoice and interplay of the model parameters and the regularization parameters,and we show that in few-projection low-dose settings our algorithm iscompetitive with total variation regularization and tends to include moretexture and more correct edges.
arxiv-10500-125 | To Drop or Not to Drop: Robustness, Consistency and Differential Privacy Properties of Dropout | http://arxiv.org/pdf/1503.02031v1.pdf | author:Prateek Jain, Vivek Kulkarni, Abhradeep Thakurta, Oliver Williams category:cs.LG cs.NE stat.ML published:2015-03-06 summary:Training deep belief networks (DBNs) requires optimizing a non-convexfunction with an extremely large number of parameters. Naturally, existinggradient descent (GD) based methods are prone to arbitrarily poor local minima.In this paper, we rigorously show that such local minima can be avoided (uptoan approximation error) by using the dropout technique, a widely used heuristicin this domain. In particular, we show that by randomly dropping a few nodes ofa one-hidden layer neural network, the training objective function, up to acertain approximation error, decreases by a multiplicative factor. On the flip side, we show that for training convex empirical risk minimizers(ERM), dropout in fact acts as a "stabilizer" or regularizer. That is, a simpledropout based GD method for convex ERMs is stable in the face of arbitrarychanges to any one of the training points. Using the above assertion, we showthat dropout provides fast rates for generalization error in learning (convex)generalized linear models (GLM). Moreover, using the above mentioned stabilityproperties of dropout, we design dropout based differentially privatealgorithms for solving ERMs. The learned GLM thus, preserves privacy of each ofthe individual training points while providing accurate predictions for newtest points. Finally, we empirically validate our stability assertions fordropout in the context of convex ERMs and show that surprisingly, dropoutsignificantly outperforms (in terms of prediction accuracy) the L2regularization based methods for several benchmark datasets.
arxiv-10500-126 | Denoising Autoencoders for fast Combinatorial Black Box Optimization | http://arxiv.org/pdf/1503.01954v2.pdf | author:Malte Probst category:cs.NE I.2.6; I.2.8 published:2015-03-06 summary:Estimation of Distribution Algorithms (EDAs) require flexible probabilitymodels that can be efficiently learned and sampled. Autoencoders (AE) aregenerative stochastic networks with these desired properties. We integrate aspecial type of AE, the Denoising Autoencoder (DAE), into an EDA and evaluatethe performance of DAE-EDA on several combinatorial optimization problems witha single objective. We asses the number of fitness evaluations as well as therequired CPU times. We compare the results to the performance to the BayesianOptimization Algorithm (BOA) and RBM-EDA, another EDA which is based on agenerative neural network which has proven competitive with BOA. For theconsidered problem instances, DAE-EDA is considerably faster than BOA andRBM-EDA, sometimes by orders of magnitude. The number of fitness evaluations ishigher than for BOA, but competitive with RBM-EDA. These results show that DAEscan be useful tools for problems with low but non-negligible fitness evaluationcosts.
arxiv-10500-127 | Latent Hierarchical Model for Activity Recognition | http://arxiv.org/pdf/1503.01820v1.pdf | author:Ninghang Hu, Gwenn Englebienne, Zhongyu Lou, Ben Kröse category:cs.RO cs.AI cs.CV cs.LG published:2015-03-06 summary:We present a novel hierarchical model for human activity recognition. Incontrast to approaches that successively recognize actions and activities, ourapproach jointly models actions and activities in a unified framework, andtheir labels are simultaneously predicted. The model is embedded with a latentlayer that is able to capture a richer class of contextual information in bothstate-state and observation-state pairs. Although loops are present in themodel, the model has an overall linear-chain structure, where the exactinference is tractable. Therefore, the model is very efficient in bothinference and learning. The parameters of the graphical model are learned witha Structured Support Vector Machine (Structured-SVM). A data-driven approach isused to initialize the latent variables; therefore, no manual labeling for thelatent states is required. The experimental results from using two benchmarkdatasets show that our model outperforms the state-of-the-art approach, and ourmodel is computationally more efficient.
arxiv-10500-128 | Convex Color Image Segmentation with Optimal Transport Distances | http://arxiv.org/pdf/1503.01986v2.pdf | author:Julien Rabin, Nicolas Papadakis category:cs.CV published:2015-03-06 summary:This work is about the use of regularized optimal-transport distances forconvex, histogram-based image segmentation. In the considered framework, fixedexemplar histograms define a prior on the statistical features of the tworegions in competition. In this paper, we investigate the use of varioustransport-based cost functions as discrepancy measures and rely on aprimal-dual algorithm to solve the obtained convex optimization problem.
arxiv-10500-129 | Band selection in RKHS for fast nonlinear unmixing of hyperspectral images | http://arxiv.org/pdf/1503.02090v1.pdf | author:T. Imbiriba, J. C. M. Bermudez, C. Richard, J. -Y. Tourneret category:cs.CV published:2015-03-06 summary:The profusion of spectral bands generated by the acquisition process ofhyperspectral images generally leads to high computational costs. Suchdifficulties arise in particular with nonlinear unmixing methods, which arenaturally more complex than linear ones. This complexity, associated with thehigh redundancy of information within the complete set of bands, make thesearch of band selection algorithms relevant. With this work, we propose a bandselection strategy in reproducing kernel Hilbert spaces that allows todrastically reduce the processing time required by nonlinear unmixingtechniques. Simulation results show a complexity reduction of two orders ofmagnitude without compromising unmixing performance.
arxiv-10500-130 | Linear Global Translation Estimation with Feature Tracks | http://arxiv.org/pdf/1503.01832v2.pdf | author:Zhaopeng Cui, Nianjuan Jiang, Chengzhou Tang, Ping Tan category:cs.CV published:2015-03-06 summary:This paper derives a novel linear position constraint for cameras seeing acommon scene point, which leads to a direct linear method for global cameratranslation estimation. Unlike previous solutions, this method deals withcollinear camera motion and weak image association at the same time. The finallinear formulation does not involve the coordinates of scene points, whichmakes it efficient even for large scale data. We solve the linear equationbased on $L_1$ norm, which makes our system more robust to outliers inessential matrices and feature correspondences. We experiment this method onboth sequentially captured images and unordered Internet images. Theexperiments demonstrate its strength in robustness, accuracy, and efficiency.
arxiv-10500-131 | A Novel Tensor Robust PCA Approach for Background Subtraction from Compressive Measurements | http://arxiv.org/pdf/1503.01868v3.pdf | author:Wenfei Cao, Yao Wang, Jian Sun, Deyu Meng, Can Yang, Andrzej Cichocki, Zongben Xu category:cs.CV published:2015-03-06 summary:Background subtraction has been a fundamental and widely studied task invideo analysis, with a wide range of applications in video surveillance,teleconferencing and 3D modeling. Recently, motivated by compressive imaging,background subtraction from compressive measurements (BSCM) is becoming anactive research task in video surveillance. In this paper, we propose a noveltensor-based robust PCA (TenRPCA) approach for BSCM by decomposing video framesinto backgrounds with spatial-temporal correlations and foregrounds withspatio-temporal continuity in a tensor framework. In this approach, we use 3Dtotal variation to enhance the spatio-temporal continuity of foregrounds, andTucker decomposition to model the spatio-temporal correlations of videobackground. Based on this idea, we design a basic tensor RPCA model over thevideo frames, dubbed holistic TenRPCA model (H-TenRPCA). To characterize thecorrelations among the groups of similar 3D patches of video background, wefurther design a patch-group-based tensor RPCA model (PG-TenRPCA) by jointtensor Tucker decompositions of 3D patch groups for modeling the videobackground. Efficient algorithms using alternating direction method ofmultipliers (ADMM) are developed to solve the proposed models. Extensiveexperiments on simulated and real-world videos demonstrate the superiority ofthe proposed models over the existing state-of-the-art approaches.
arxiv-10500-132 | Spectral Clustering by Ellipsoid and Its Connection to Separable Nonnegative Matrix Factorization | http://arxiv.org/pdf/1503.01531v1.pdf | author:Tomohiko Mizutani category:cs.CV published:2015-03-05 summary:This paper proposes a variant of the normalized cut algorithm for spectralclustering. Although the normalized cut algorithm applies the K-means algorithmto the eigenvectors of a normalized graph Laplacian for finding clusters, ouralgorithm instead uses a minimum volume enclosing ellipsoid for them. We showthat the algorithm shares similarity with the ellipsoidal rounding algorithmfor separable nonnegative matrix factorization. Our theoretical insight impliesthat the algorithm can serve as a bridge between spectral clustering andseparable NMF. The K-means algorithm has the issues in that the choice ofinitial points affects the construction of clusters and certain choices resultin poor clustering performance. The normalized cut algorithm inherits theseissues since K-means is incorporated in it, whereas the algorithm proposed heredoes not. An empirical study is presented to examine the performance of thealgorithm.
arxiv-10500-133 | Pyrcca: regularized kernel canonical correlation analysis in Python and its applications to neuroimaging | http://arxiv.org/pdf/1503.01538v1.pdf | author:Natalia Y. Bilenko, Jack L. Gallant category:q-bio.QM cs.CV stat.ML published:2015-03-05 summary:Canonical correlation analysis (CCA) is a valuable method for interpretingcross-covariance across related datasets of different dimensionality. There aremany potential applications of CCA to neuroimaging data analysis. For instance,CCA can be used for finding functional similarities across fMRI datasetscollected from multiple subjects without resampling individual datasets to atemplate anatomy. In this paper, we introduce Pyrcca, an open-source Pythonmodule for executing CCA between two or more datasets. Pyrcca can be used toimplement CCA with or without regularization, and with or without linear or aGaussian kernelization of the datasets. We demonstrate an application of CCAimplemented with Pyrcca to neuroimaging data analysis. We use CCA to find adata-driven set of functional response patterns that are similar acrossindividual subjects in a natural movie experiment. We then demonstrate how thisset of response patterns discovered by CCA can be used to accurately predictsubject responses to novel natural movie stimuli.
arxiv-10500-134 | Genetic optimization of the Hyperloop route through the Grapevine | http://arxiv.org/pdf/1503.01524v1.pdf | author:Casey J. Handmer category:cs.NE published:2015-03-05 summary:We demonstrate a genetic algorithm that employs a versatile fitness functionto optimize route selection for the Hyperloop, a proposed high speed passengertransportation system.
arxiv-10500-135 | Deep Temporal Appearance-Geometry Network for Facial Expression Recognition | http://arxiv.org/pdf/1503.01532v1.pdf | author:Heechul Jung, Sihaeng Lee, Sunjeong Park, Injae Lee, Chunghyun Ahn, Junmo Kim category:cs.CV published:2015-03-05 summary:Temporal information can provide useful features for recognizing facialexpressions. However, to manually design useful features requires a lot ofeffort. In this paper, to reduce this effort, a deep learning technique whichis regarded as a tool to automatically extract useful features from raw data,is adopted. Our deep network is based on two different models. The first deepnetwork extracts temporal geometry features from temporal facial landmarkpoints, while the other deep network extracts temporal appearance features fromimage sequences . These two models are combined in order to boost theperformance of the facial expression recognition. Through several experiments,we showed that the two models cooperate with each other. As a result, weachieved superior performance to other state-of-the-art methods in CK+ andOulu-CASIA databases. Furthermore, one of the main contributions of this paperis that our deep network catches the facial action points automatically.
arxiv-10500-136 | Do We Need More Training Data? | http://arxiv.org/pdf/1503.01508v1.pdf | author:Xiangxin Zhu, Carl Vondrick, Charless Fowlkes, Deva Ramanan category:cs.CV published:2015-03-05 summary:Datasets for training object recognition systems are steadily increasing insize. This paper investigates the question of whether existing detectors willcontinue to improve as data grows, or saturate in performance due to limitedmodel complexity and the Bayes risk associated with the feature spaces in whichthey operate. We focus on the popular paradigm of discriminatively trainedtemplates defined on oriented gradient features. We investigate the performanceof mixtures of templates as the number of mixture components and the amount oftraining data grows. Surprisingly, even with proper treatment of regularizationand "outliers", the performance of classic mixture models appears to saturatequickly ($\sim$10 templates and $\sim$100 positive training examples pertemplate). This is not a limitation of the feature space as compositionalmixtures that share template parameters via parts and that can synthesize newtemplates not encountered during training yield significantly betterperformance. Based on our analysis, we conjecture that the greatest gains indetection performance will continue to derive from improved representations andlearning algorithms that can make efficient use of large datasets.
arxiv-10500-137 | High Dimensional Bayesian Optimisation and Bandits via Additive Models | http://arxiv.org/pdf/1503.01673v3.pdf | author:Kirthevasan Kandasamy, Jeff Schneider, Barnabas Poczos category:stat.ML cs.LG published:2015-03-05 summary:Bayesian Optimisation (BO) is a technique used in optimising a$D$-dimensional function which is typically expensive to evaluate. While therehave been many successes for BO in low dimensions, scaling it to highdimensions has been notoriously difficult. Existing literature on the topic areunder very restrictive settings. In this paper, we identify two key challengesin this endeavour. We tackle these challenges by assuming an additive structurefor the function. This setting is substantially more expressive and contains aricher class of functions than previous work. We prove that, for additivefunctions the regret has only linear dependence on $D$ even though the functiondepends on all $D$ dimensions. We also demonstrate several other statisticaland computational benefits in our framework. Via synthetic examples, ascientific simulation and a face detection problem we demonstrate that ourmethod outperforms naive BO on additive functions and on several examples wherethe function is not additive.
arxiv-10500-138 | Frequency Domain TOF: Encoding Object Depth in Modulation Frequency | http://arxiv.org/pdf/1503.01804v1.pdf | author:Achuta Kadambi, Vage Taamazyan, Suren Jayasuriya, Ramesh Raskar category:cs.CV cs.GR published:2015-03-05 summary:Time of flight cameras may emerge as the 3-D sensor of choice. Today, time offlight sensors use phase-based sampling, where the phase delay between emittedand received, high-frequency signals encodes distance. In this paper, wepresent a new time of flight architecture that relies only on frequency---werefer to this technique as frequency-domain time of flight (FD-TOF). Inspiredby optical coherence tomography (OCT), FD-TOF excels when frequency bandwidthis high. With the increasing frequency of TOF sensors, new challenges to timeof flight sensing continue to emerge. At high frequencies, FD-TOF offersseveral potential benefits over phase-based time of flight methods.
arxiv-10500-139 | BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation | http://arxiv.org/pdf/1503.01640v2.pdf | author:Jifeng Dai, Kaiming He, Jian Sun category:cs.CV published:2015-03-05 summary:Recent leading approaches to semantic segmentation rely on deep convolutionalnetworks trained with human-annotated, pixel-level segmentation masks. Suchpixel-accurate supervision demands expensive labeling effort and limits theperformance of deep networks that usually benefit from more training data. Inthis paper, we propose a method that achieves competitive accuracy but onlyrequires easily obtained bounding box annotations. The basic idea is to iteratebetween automatically generating region proposals and training convolutionalnetworks. These two steps gradually recover segmentation masks for improvingthe networks, and vise versa. Our method, called BoxSup, produces competitiveresults supervised by boxes only, on par with strong baselines fully supervisedby masks under the same setting. By leveraging a large amount of boundingboxes, BoxSup further unleashes the power of deep convolutional networks andyields state-of-the-art results on PASCAL VOC 2012 and PASCAL-CONTEXT.
arxiv-10500-140 | What's Cookin'? Interpreting Cooking Videos using Text, Speech and Vision | http://arxiv.org/pdf/1503.01558v3.pdf | author:Jonathan Malmaud, Jonathan Huang, Vivek Rathod, Nick Johnston, Andrew Rabinovich, Kevin Murphy category:cs.CL cs.CV cs.IR published:2015-03-05 summary:We present a novel method for aligning a sequence of instructions to a videoof someone carrying out a task. In particular, we focus on the cooking domain,where the instructions correspond to the recipe. Our technique relies on an HMMto align the recipe steps to the (automatically generated) speech transcript.We then refine this alignment using a state-of-the-art visual food detector,based on a deep convolutional neural network. We show that our techniqueoutperforms simpler techniques based on keyword spotting. It also enablesinteresting applications, such as automatically illustrating recipes withkeyframes, and searching within a video for events of interest.
arxiv-10500-141 | Scalable Iterative Algorithm for Robust Subspace Clustering | http://arxiv.org/pdf/1503.01578v2.pdf | author:Sanghyuk Chun, Yung-Kyun Noh, Jinwoo Shin category:cs.DS cs.LG published:2015-03-05 summary:Subspace clustering (SC) is a popular method for dimensionality reduction ofhigh-dimensional data, where it generalizes Principal Component Analysis (PCA).Recently, several methods have been proposed to enhance the robustness of PCAand SC, while most of them are computationally very expensive, in particular,for high dimensional large-scale data. In this paper, we develop much fasteriterative algorithms for SC, incorporating robustness using a {\em non-squared}$\ell_2$-norm objective. The known implementations for optimizing the objectivewould be costly due to the alternative optimization of two separate objectives:optimal cluster-membership assignment and robust subspace selection, while thesubstitution of one process to a faster surrogate can cause failure inconvergence. To address the issue, we use a simplified procedure requiringefficient matrix-vector multiplications for subspace update instead of solvingan expensive eigenvector problem at each iteration, in addition to releasenested robust PCA loops. We prove that the proposed algorithm monotonicallyconverges to a local minimum with approximation guarantees, e.g., it achieves2-approximation for the robust PCA objective. In our experiments, the proposedalgorithm is shown to converge at an order of magnitude faster than knownalgorithms optimizing the same objective, and have outperforms prior subspaceclustering methods in accuracy and running time for MNIST dataset.
arxiv-10500-142 | Jointly Learning Multiple Measures of Similarities from Triplet Comparisons | http://arxiv.org/pdf/1503.01521v3.pdf | author:Liwen Zhang, Subhransu Maji, Ryota Tomioka category:stat.ML cs.AI cs.CV cs.LG published:2015-03-05 summary:Similarity between objects is multi-faceted and it can be easier for humanannotators to measure it when the focus is on a specific aspect. We considerthe problem of mapping objects into view-specific embeddings where the distancebetween them is consistent with the similarity comparisons of the form "fromthe t-th view, object A is more similar to B than to C". Our framework jointlylearns view-specific embeddings exploiting correlations between views.Experiments on a number of datasets, including one of multi-view crowdsourcedcomparison on bird images, show the proposed method achieves lower tripletgeneralization error when compared to both learning embeddings independentlyfor each view and all views pooled into one view. Our method can also be usedto learn multiple measures of similarity over input features taking classlabels into account and compares favorably to existing approaches formulti-task metric learning on the ISOLET dataset.
arxiv-10500-143 | Learning to rank in person re-identification with metric ensembles | http://arxiv.org/pdf/1503.01543v1.pdf | author:Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel category:cs.CV published:2015-03-05 summary:We propose an effective structured learning based approach to the problem ofperson re-identification which outperforms the current state-of-the-art on mostbenchmark data sets evaluated. Our framework is built on the basis of multiplelow-level hand-crafted and high-level visual features. We then formulate twooptimization algorithms, which directly optimize evaluation measures commonlyused in person re-identification, also known as the Cumulative MatchingCharacteristic (CMC) curve. Our new approach is practical to many real-worldsurveillance applications as the re-identification performance can beconcentrated in the range of most practical importance. The combination ofthese factors leads to a person re-identification system which outperforms mostexisting algorithms. More importantly, we advance state-of-the-art results onperson re-identification by improving the rank-$1$ recognition rates from$40\%$ to $50\%$ on the iLIDS benchmark, $16\%$ to $18\%$ on the PRID2011benchmark, $43\%$ to $46\%$ on the VIPeR benchmark, $34\%$ to $53\%$ on theCUHK01 benchmark and $21\%$ to $62\%$ on the CUHK03 benchmark.
arxiv-10500-144 | Correct-by-synthesis reinforcement learning with temporal logic constraints | http://arxiv.org/pdf/1503.01793v1.pdf | author:Min Wen, Ruediger Ehlers, Ufuk Topcu category:cs.LO cs.GT cs.LG cs.SY published:2015-03-05 summary:We consider a problem on the synthesis of reactive controllers that optimizesome a priori unknown performance criterion while interacting with anuncontrolled environment such that the system satisfies a given temporal logicspecification. We decouple the problem into two subproblems. First, we extracta (maximally) permissive strategy for the system, which encodes multiple(possibly all) ways in which the system can react to the adversarialenvironment and satisfy the specifications. Then, we quantify the a prioriunknown performance criterion as a (still unknown) reward function and computean optimal strategy for the system within the operating envelope allowed by thepermissive strategy by using the so-called maximin-Q learning algorithm. Weestablish both correctness (with respect to the temporal logic specifications)and optimality (with respect to the a priori unknown performance criterion) ofthis two-step technique for a fragment of temporal logic specifications. Forspecifications beyond this fragment, correctness can still be preserved, butthe learned strategy may be sub-optimal. We present an algorithm to the overallproblem, and demonstrate its use and computational requirements on a set ofrobot motion planning examples.
arxiv-10500-145 | Optimally Combining Classifiers Using Unlabeled Data | http://arxiv.org/pdf/1503.01811v3.pdf | author:Akshay Balsubramani, Yoav Freund category:cs.LG stat.ML published:2015-03-05 summary:We develop a worst-case analysis of aggregation of classifier ensembles forbinary classification. The task of predicting to minimize error is formulatedas a game played over a given set of unlabeled data (a transductive setting),where prior label information is encoded as constraints on the game. Theminimax solution of this game identifies cases where a weighted combination ofthe classifiers can perform significantly better than any single classifier.
arxiv-10500-146 | Supervised Discrete Hashing | http://arxiv.org/pdf/1503.01557v3.pdf | author:Fumin Shen, Chunhua Shen, Wei Liu, Heng Tao Shen category:cs.CV published:2015-03-05 summary:This paper has been withdrawn by the authour.
arxiv-10500-147 | Visualization of Clandestine Labs from Seizure Reports: Thematic Mapping and Data Mining Research Directions | http://arxiv.org/pdf/1503.01549v1.pdf | author:William Hsu, Mohammed Abduljabbar, Ryuichi Osuga, Max Lu, Wesam Elshamy category:cs.IR cs.CL published:2015-03-05 summary:The problem of spatiotemporal event visualization based on reports entailssubtasks ranging from named entity recognition to relationship extraction andmapping of events. We present an approach to event extraction that is driven bydata mining and visualization goals, particularly thematic mapping and trendanalysis. This paper focuses on bridging the information extraction andvisualization tasks and investigates topic modeling approaches. We develop astatic, finite topic model and examine the potential benefits and feasibilityof extending this to dynamic topic modeling with a large number of topics andcontinuous time. We describe an experimental test bed for event mapping thatuses this end-to-end information retrieval system, and report preliminaryresults on a geoinformatics problem: tracking of methamphetamine lab seizureevents across time and space.
arxiv-10500-148 | Convex Optimization for Parallel Energy Minimization | http://arxiv.org/pdf/1503.01563v1.pdf | author:K. S. Sesh Kumar, Alvaro Barbero, Stefanie Jegelka, Suvrit Sra, Francis Bach category:cs.CV math.OC published:2015-03-05 summary:Energy minimization has been an intensely studied core problem in computervision. With growing image sizes (2D and 3D), it is now highly desirable to runenergy minimization algorithms in parallel. But many existing algorithms, inparticular, some efficient combinatorial algorithms, are difficult topar-allelize. By exploiting results from convex and submodular theory, wereformulate the quadratic energy minimization problem as a total variationdenoising problem, which, when viewed geometrically, enables the use ofprojection and reflection based convex methods. The resulting min-cut algorithm(and code) is conceptually very simple, and solves a sequence of TV denoisingproblems. We perform an extensive empirical evaluation comparingstate-of-the-art combinatorial algorithms and convex optimization techniques.On small problems the iterative convex methods match the combinatorial max-flowalgorithms, while on larger problems they offer other flexibility and importantgains: (a) their memory footprint is small; (b) their straightforwardparallelizability fits multi-core platforms; (c) they can easily bewarm-started; and (d) they quickly reach approximately good solutions, therebyenabling faster "inexact" solutions. A key consequence of our approach based onsubmodularity and convexity is that it is allows to combine any arbitrarycombinatorial or convex methods as subroutines, which allows one to obtainhybrid combinatorial and convex optimization algorithms that benefit from thestrengths of both.
arxiv-10500-149 | Video-Based Facial Expression Recognition Using Local Directional Binary Pattern | http://arxiv.org/pdf/1503.01646v1.pdf | author:Sahar Hooshmand, Ali Jamali Avilaq, Amir Hossein Rezaie category:cs.CV published:2015-03-05 summary:Automatic facial expression analysis is a challenging issue and influenced somany areas such as human computer interaction. Due to the uncertainties of thelight intensity and light direction, the face gray shades are uneven and theexpression recognition rate under simple Local Binary Pattern is not ideal andpromising. In this paper we propose two state-of-the-art descriptors forperson-independent facial expression recognition. First the face regions of thewhole images in a video sequence are modeled with Volume Local DirectionalBinary pattern (VLDBP), which is an extended version of the LDBP operator,incorporating movement and appearance together. To make the surveycomputationally simple and easy to expand, only the co-occurrences of the LocalDirectional Binary Pattern on three orthogonal planes (LDBP-TOP) are debated.After extracting the feature vectors the K-Nearest Neighbor classifier was usedto recognize the expressions. The proposed methods are applied to the videos ofthe Extended Cohn-Kanade database (CK+) and the experimental outcomesdemonstrate that the offered techniques achieve more accuracy in comparisonwith the classic and traditional algorithms.
arxiv-10500-150 | Color Image Classification via Quaternion Principal Component Analysis Network | http://arxiv.org/pdf/1503.01657v1.pdf | author:Rui Zeng, Jiasong Wu, Zhuhong Shao, Yang Chen, Lotfi Senhadji, Huazhong Shu category:cs.CV published:2015-03-05 summary:The Principal Component Analysis Network (PCANet), which is one of therecently proposed deep learning architectures, achieves the state-of-the-artclassification accuracy in various databases. However, the performance ofPCANet may be degraded when dealing with color images. In this paper, aQuaternion Principal Component Analysis Network (QPCANet), which is anextension of PCANet, is proposed for color images classification. Compared toPCANet, the proposed QPCANet takes into account the spatial distributioninformation of color images and ensures larger amount of intra-class invarianceof color images. Experiments conducted on different color image datasets suchas Caltech-101, UC Merced Land Use, Georgia Tech face and CURet have revealedthat the proposed QPCANet achieves higher classification accuracy than PCANet.
arxiv-10500-151 | EmoNets: Multimodal deep learning approaches for emotion recognition in video | http://arxiv.org/pdf/1503.01800v2.pdf | author:Samira Ebrahimi Kahou, Xavier Bouthillier, Pascal Lamblin, Caglar Gulcehre, Vincent Michalski, Kishore Konda, Sébastien Jean, Pierre Froumenty, Yann Dauphin, Nicolas Boulanger-Lewandowski, Raul Chandias Ferrari, Mehdi Mirza, David Warde-Farley, Aaron Courville, Pascal Vincent, Roland Memisevic, Christopher Pal, Yoshua Bengio category:cs.LG cs.CV published:2015-03-05 summary:The task of the emotion recognition in the wild (EmotiW) Challenge is toassign one of seven emotions to short video clips extracted from Hollywoodstyle movies. The videos depict acted-out emotions under realistic conditionswith a large degree of variation in attributes such as pose and illumination,making it worthwhile to explore approaches which consider combinations offeatures from multiple modalities for label assignment. In this paper wepresent our approach to learning several specialist models using deep learningtechniques, each focusing on one modality. Among these are a convolutionalneural network, focusing on capturing visual information in detected faces, adeep belief net focusing on the representation of the audio stream, a K-Meansbased "bag-of-mouths" model, which extracts visual features around the mouthregion and a relational autoencoder, which addresses spatio-temporal aspects ofvideos. We explore multiple methods for the combination of cues from thesemodalities into one common classifier. This achieves a considerably greateraccuracy than predictions from our strongest single-modality classifier. Ourmethod was the winning submission in the 2013 EmotiW challenge and achieved atest set accuracy of 47.67% on the 2014 dataset.
arxiv-10500-152 | Studying the Wikipedia Hyperlink Graph for Relatedness and Disambiguation | http://arxiv.org/pdf/1503.01655v2.pdf | author:Eneko Agirre, Ander Barrena, Aitor Soroa category:cs.CL published:2015-03-05 summary:Hyperlinks and other relations in Wikipedia are a extraordinary resourcewhich is still not fully understood. In this paper we study the different typesof links in Wikipedia, and contrast the use of the full graph with respect tojust direct links. We apply a well-known random walk algorithm on two tasks,word relatedness and named-entity disambiguation. We show that using the fullgraph is more effective than just direct links by a large margin, thatnon-reciprocal links harm performance, and that there is no benefit fromcategories and infoboxes, with coherent results on both tasks. We set newstate-of-the-art figures for systems based on Wikipedia links, comparable tosystems exploiting several information sources and/or supervised machinelearning. Our approach is open source, with instruction to reproduce results,and amenable to be integrated with complementary text-based methods.
arxiv-10500-153 | Min-Max Kernels | http://arxiv.org/pdf/1503.01737v1.pdf | author:Ping Li category:stat.ML cs.LG stat.CO published:2015-03-05 summary:The min-max kernel is a generalization of the popular resemblance kernel(which is designed for binary data). In this paper, we demonstrate, through anextensive classification study using kernel machines, that the min-max kerneloften provides an effective measure of similarity for nonnegative data. As themin-max kernel is nonlinear and might be difficult to be used for industrialapplications with massive data, we show that the min-max kernel can belinearized via hashing techniques. This allows practitioners to apply min-maxkernel to large-scale applications using well matured linear algorithms such aslinear SVM or logistic regression. The previous remarkable work on consistent weighted sampling (CWS) producessamples in the form of ($i^*, t^*$) where the $i^*$ records the location (andin fact also the weights) information analogous to the samples produced byclassical minwise hashing on binary data. Because the $t^*$ is theoreticallyunbounded, it was not immediately clear how to effectively implement CWS forbuilding large-scale linear classifiers. In this paper, we provide a simplesolution by discarding $t^*$ (which we refer to as the "0-bit" scheme). Via anextensive empirical study, we show that this 0-bit scheme does not loseessential information. We then apply the "0-bit" CWS for building linearclassifiers to approximate min-max kernel classifiers, as extensively validatedon a wide range of publicly available classification datasets. We expect thiswork will generate interests among data mining practitioners who would like toefficiently utilize the nonlinear information of non-binary and nonnegativedata.
arxiv-10500-154 | Inference of hidden structures in complex physical systems by multi-scale clustering | http://arxiv.org/pdf/1503.01626v2.pdf | author:Z. Nussinov, P. Ronhovde, Dandan Hu, S. Chakrabarty, M. Sahu, Bo Sun, N. A. Mauro, K. K. Sahu category:cs.CV published:2015-03-05 summary:We survey the application of a relatively new branch of statisticalphysics--"community detection"-- to data mining. In particular, we focus on thediagnosis of materials and automated image segmentation. Community detectiondescribes the quest of partitioning a complex system involving many elementsinto optimally decoupled subsets or communities of such elements. We review amultiresolution variant which is used to ascertain structures at differentspatial and temporal scales. Significant patterns are obtained by examining thecorrelations between different independent solvers. Similar to othercombinatorial optimization problems in the NP complexity class, communitydetection exhibits several phases. Typically, illuminating orders are revealedby choosing parameters that lead to extremal information theory correlations.
arxiv-10500-155 | Large-Scale Distributed Bayesian Matrix Factorization using Stochastic Gradient MCMC | http://arxiv.org/pdf/1503.01596v2.pdf | author:Sungjin Ahn, Anoop Korattikara, Nathan Liu, Suju Rajan, Max Welling category:cs.LG stat.ML published:2015-03-05 summary:Despite having various attractive qualities such as high prediction accuracyand the ability to quantify uncertainty and avoid over-fitting, Bayesian MatrixFactorization has not been widely adopted because of the prohibitive cost ofinference. In this paper, we propose a scalable distributed Bayesian matrixfactorization algorithm using stochastic gradient MCMC. Our algorithm, based onDistributed Stochastic Gradient Langevin Dynamics, can not only match theprediction accuracy of standard MCMC methods like Gibbs sampling, but at thesame time is as fast and simple as stochastic gradient descent. In ourexperiments, we show that our algorithm can achieve the same level ofprediction accuracy as Gibbs sampling an order of magnitude faster. We alsoshow that our method reduces the prediction error as fast as distributedstochastic gradient descent, achieving a 4.1% improvement in RMSE for theNetflix dataset and an 1.8% for the Yahoo music dataset.
arxiv-10500-156 | Quantifying Uncertainty in Stochastic Models with Parametric Variability | http://arxiv.org/pdf/1503.01401v1.pdf | author:Kyle S. Hickmann, James M. Hyman, Sara Y. Del Valle category:stat.ML stat.ME published:2015-03-04 summary:We present a method to quantify uncertainty in the predictions made bysimulations of mathematical models that can be applied to a broad class ofstochastic, discrete, and differential equation models. Quantifying uncertaintyis crucial for determining how accurate the model predictions are andidentifying which input parameters affect the outputs of interest. Most of theexisting methods for uncertainty quantification require many samples togenerate accurate results, are unable to differentiate where the uncertainty iscoming from (e.g., parameters or model assumptions), or require a lot ofcomputational resources. Our approach addresses these challenges andopportunities by allowing different types of uncertainty, that is, uncertaintyin input parameters as well as uncertainty created through stochastic modelcomponents. This is done by combining the Karhunen-Loeve decomposition,polynomial chaos expansion, and Bayesian Gaussian process regression to createa statistical surrogate for the stochastic model. The surrogate separates theanalysis of variation arising through stochastic simulation and variationarising through uncertainty in the model parameterization. We illustrate ourapproach by quantifying the uncertainty in a stochastic ordinary differentialequation epidemic model. Specifically, we estimate four quantities of interestfor the epidemic model and show agreement between the surrogate and the actualmodel results.
arxiv-10500-157 | Bethe Projections for Non-Local Inference | http://arxiv.org/pdf/1503.01397v2.pdf | author:Luke Vilnis, David Belanger, Daniel Sheldon, Andrew McCallum category:stat.ML cs.CL cs.LG published:2015-03-04 summary:Many inference problems in structured prediction are naturally solved byaugmenting a tractable dependency structure with complex, non-local auxiliaryobjectives. This includes the mean field family of variational inferencealgorithms, soft- or hard-constrained inference using Lagrangian relaxation orlinear programming, collective graphical models, and forms of semi-supervisedlearning such as posterior regularization. We present a method todiscriminatively learn broad families of inference objectives, capturingpowerful non-local statistics of the latent variables, while maintainingtractable and provably fast inference using non-Euclidean projected gradientdescent with a distance-generating function given by the Bethe entropy. Wedemonstrate the performance and flexibility of our method by (1) extractingstructured citations from research papers by learning soft global constraints,(2) achieving state-of-the-art results on a widely-used handwriting recognitiontask using a novel learned non-convex inference procedure, and (3) providing afast and highly scalable algorithm for the challenging problem of inference ina collective graphical model applied to bird migration.
arxiv-10500-158 | Partial Sum Minimization of Singular Values in Robust PCA: Algorithm and Applications | http://arxiv.org/pdf/1503.01444v2.pdf | author:Tae-Hyun Oh, Yu-Wing Tai, Jean-Charles Bazin, Hyeongwoo Kim, In So Kweon category:cs.CV cs.AI published:2015-03-04 summary:Robust Principal Component Analysis (RPCA) via rank minimization is apowerful tool for recovering underlying low-rank structure of clean datacorrupted with sparse noise/outliers. In many low-level vision problems, notonly it is known that the underlying structure of clean data is low-rank, butthe exact rank of clean data is also known. Yet, when applying conventionalrank minimization for those problems, the objective function is formulated in away that does not fully utilize a priori target rank information about theproblems. This observation motivates us to investigate whether there is abetter alternative solution when using rank minimization. In this paper,instead of minimizing the nuclear norm, we propose to minimize the partial sumof singular values, which implicitly encourages the target rank constraint. Ourexperimental analyses show that, when the number of samples is deficient, ourapproach leads to a higher success rate than conventional rank minimization,while the solutions obtained by the two approaches are almost identical whenthe number of samples is more than sufficient. We apply our approach to variouslow-level vision problems, e.g. high dynamic range imaging, motion edgedetection, photometric stereo, image alignment and recovery, and show that ourresults outperform those obtained by the conventional nuclear norm rankminimization method.
arxiv-10500-159 | Toxicity Prediction using Deep Learning | http://arxiv.org/pdf/1503.01445v1.pdf | author:Thomas Unterthiner, Andreas Mayr, Günter Klambauer, Sepp Hochreiter category:stat.ML cs.LG cs.NE q-bio.BM published:2015-03-04 summary:Everyday we are exposed to various chemicals via food additives, cleaning andcosmetic products and medicines -- and some of them might be toxic. Howevertesting the toxicity of all existing compounds by biological experiments isneither financially nor logistically feasible. Therefore the governmentagencies NIH, EPA and FDA launched the Tox21 Data Challenge within the"Toxicology in the 21st Century" (Tox21) initiative. The goal of this challengewas to assess the performance of computational methods in predicting thetoxicity of chemical compounds. State of the art toxicity prediction methodsbuild upon specifically-designed chemical descriptors developed over decades.Though Deep Learning is new to the field and was never applied to toxicityprediction before, it clearly outperformed all other participating methods. Inthis application paper we show that deep nets automatically learn featuresresembling well-established toxicophores. In total, our Deep Learning approachwon both of the panel-challenges (nuclear receptors and stress response) aswell as the overall Grand Challenge, and thereby sets a new standard in toxprediction.
arxiv-10500-160 | Local Expectation Gradients for Doubly Stochastic Variational Inference | http://arxiv.org/pdf/1503.01494v1.pdf | author:Michalis K. Titsias category:stat.ML published:2015-03-04 summary:We introduce local expectation gradients which is a general purposestochastic variational inference algorithm for constructing stochasticgradients through sampling from the variational distribution. This algorithmdivides the problem of estimating the stochastic gradients over multiplevariational parameters into smaller sub-tasks so that each sub-task exploitsintelligently the information coming from the most relevant part of thevariational distribution. This is achieved by performing an exact expectationover the single random variable that mostly correlates with the variationalparameter of interest resulting in a Rao-Blackwellized estimate that has lowvariance and can work efficiently for both continuous and discrete randomvariables. Furthermore, the proposed algorithm has interesting similaritieswith Gibbs sampling but at the same time, unlike Gibbs sampling, it can betrivially parallelized.
arxiv-10500-161 | Probabilistic Label Relation Graphs with Ising Models | http://arxiv.org/pdf/1503.01428v3.pdf | author:Nan Ding, Jia Deng, Kevin Murphy, Hartmut Neven category:cs.LG published:2015-03-04 summary:We consider classification problems in which the label space has structure. Acommon example is hierarchical label spaces, corresponding to the case whereone label subsumes another (e.g., animal subsumes dog). But labels can also bemutually exclusive (e.g., dog vs cat) or unrelated (e.g., furry, carnivore). Tojointly model hierarchy and exclusion relations, the notion of a HEX (hierarchyand exclusion) graph was introduced in [7]. This combined a conditional randomfield (CRF) with a deep neural network (DNN), resulting in state of the artresults when applied to visual object classification problems where thetraining labels were drawn from different levels of the ImageNet hierarchy(e.g., an image might be labeled with the basic level category "dog", ratherthan the more specific label "husky"). In this paper, we extend the HEX modelto allow for soft or probabilistic relations between labels, which is usefulwhen there is uncertainty about the relationship between two labels (e.g., anantelope is "sort of" furry, but not to the same degree as a grizzly bear). Wecall our new model pHEX, for probabilistic HEX. We show that the pHEX graph canbe converted to an Ising model, which allows us to use existing off-the-shelfinference methods (in contrast to the HEX method, which needed specializedinference algorithms). Experimental results show significant improvements in anumber of large-scale visual object classification tasks, outperforming theprevious HEX model.
arxiv-10500-162 | All Who Wander: On the Prevalence and Characteristics of Multi-community Engagement | http://arxiv.org/pdf/1503.01180v2.pdf | author:Chenhao Tan, Lillian Lee category:cs.SI cs.CL physics.soc-ph J.4; H.2.8 published:2015-03-04 summary:Although analyzing user behavior within individual communities is an activeand rich research domain, people usually interact with multiple communitiesboth on- and off-line. How do users act in such multi-community environments?Although there are a host of intriguing aspects to this question, it hasreceived much less attention in the research community in comparison to theintra-community case. In this paper, we examine three aspects ofmulti-community engagement: the sequence of communities that users post to, thelanguage that users employ in those communities, and the feedback that usersreceive, using longitudinal posting behavior on Reddit as our main data source,and DBLP for auxiliary experiments. We also demonstrate the effectiveness offeatures drawn from these aspects in predicting users' future level ofactivity. One might expect that a user's trajectory mimics the "settling-down" processin real life: an initial exploration of sub-communities before settling downinto a few niches. However, we find that the users in our data continually postin new communities; moreover, as time goes on, they post increasingly evenlyamong a more diverse set of smaller communities. Interestingly, it seems thatusers that eventually leave the community are "destined" to do so from the verybeginning, in the sense of showing significantly different "wandering" patternsvery early on in their trajectories; this finding has potentially importantdesign implications for community maintainers. Our multi-community perspectivealso allows us to investigate the "situation vs. personality" debate fromlanguage usage across different communities.
arxiv-10500-163 | A Novel Performance Evaluation Methodology for Single-Target Trackers | http://arxiv.org/pdf/1503.01313v3.pdf | author:Matej Kristan, Jiri Matas, Ales Leonardis, Tomas Vojir, Roman Pflugfelder, Gustavo Fernandez, Georg Nebehay, Fatih Porikli, Luka Cehovin category:cs.CV published:2015-03-04 summary:This paper addresses the problem of single-target tracker performanceevaluation. We consider the performance measures, the dataset and theevaluation system to be the most important components of tracker evaluation andpropose requirements for each of them. The requirements are the basis of a newevaluation methodology that aims at a simple and easily interpretable trackercomparison. The ranking-based methodology addresses tracker equivalence interms of statistical significance and practical differences. A fully-annotateddataset with per-frame annotations with several visual attributes isintroduced. The diversity of its visual properties is maximized in a novel wayby clustering a large number of videos according to their visual attributes.This makes it the most sophistically constructed and annotated dataset to date.A multi-platform evaluation system allowing easy integration of third-partytrackers is presented as well. The proposed evaluation methodology was testedon the VOT2014 challenge on the new dataset and 38 trackers, making it thelargest benchmark to date. Most of the tested trackers are indeedstate-of-the-art since they outperform the standard baselines, resulting in ahighly-challenging benchmark. An exhaustive analysis of the dataset from theperspective of tracking difficulty is carried out. To facilitate trackercomparison a new performance visualization technique is proposed.
arxiv-10500-164 | Class Probability Estimation via Differential Geometric Regularization | http://arxiv.org/pdf/1503.01436v7.pdf | author:Qinxun Bai, Steven Rosenberg, Zheng Wu, Stan Sclaroff category:cs.LG cs.CG stat.ML published:2015-03-04 summary:We study the problem of supervised learning for both binary and multiclassclassification from a unified geometric perspective. In particular, we proposea geometric regularization technique to find the submanifold corresponding to arobust estimator of the class probability $P(y\pmb{x})$. The regularizationterm measures the volume of this submanifold, based on the intuition thatoverfitting produces rapid local oscillations and hence large volume of theestimator. This technique can be applied to regularize any classificationfunction that satisfies two requirements: firstly, an estimator of the classprobability can be obtained; secondly, first and second derivatives of theclass probability estimator can be calculated. In experiments, we apply ourregularization technique to standard loss functions for classification, ourRBF-based implementation compares favorably to widely used regularizationmethods for both binary and multiclass classification.
arxiv-10500-165 | A General Hybrid Clustering Technique | http://arxiv.org/pdf/1503.01183v2.pdf | author:Saeid Amiri, Bertrand Clarke, Jennifer Clarke, Hoyt A. Koepke category:stat.ML cs.LG published:2015-03-04 summary:Here, we propose a clustering technique for general clustering problemsincluding those that have non-convex clusters. For a given desired number ofclusters $K$, we use three stages to find a clustering. The first stage uses ahybrid clustering technique to produce a series of clusterings of various sizes(randomly selected). They key steps are to find a $K$-means clustering using$K_\ell$ clusters where $K_\ell \gg K$ and then joins these small clusters byusing single linkage clustering. The second stage stabilizes the result ofstage one by reclustering via the `membership matrix' under Hamming distance togenerate a dendrogram. The third stage is to cut the dendrogram to get $K^*$clusters where $K^* \geq K$ and then prune back to $K$ to give a finalclustering. A variant on our technique also gives a reasonable estimate for$K_T$, the true number of clusters. We provide a series of arguments to justify the steps in the stages of ourmethods and we provide numerous examples involving real and simulated data tocompare our technique with other related techniques.
arxiv-10500-166 | A Differential Equation for Modeling Nesterov's Accelerated Gradient Method: Theory and Insights | http://arxiv.org/pdf/1503.01243v2.pdf | author:Weijie Su, Stephen Boyd, Emmanuel J. Candes category:stat.ML math.CA math.OC published:2015-03-04 summary:We derive a second-order ordinary differential equation (ODE) which is thelimit of Nesterov's accelerated gradient method. This ODE exhibits approximateequivalence to Nesterov's scheme and thus can serve as a tool for analysis. Weshow that the continuous time ODE allows for a better understanding ofNesterov's scheme. As a byproduct, we obtain a family of schemes with similarconvergence rates. The ODE interpretation also suggests restarting Nesterov'sscheme leading to an algorithm, which can be rigorously proven to converge at alinear rate whenever the objective is strongly convex.
arxiv-10500-167 | A Hierarchical Approach for Joint Multi-view Object Pose Estimation and Categorization | http://arxiv.org/pdf/1503.01393v1.pdf | author:Mete Ozay, Krzysztof Walas, Ales Leonardis category:cs.CV cs.RO published:2015-03-04 summary:We propose a joint object pose estimation and categorization approach whichextracts information about object poses and categories from the object partsand compositions constructed at different layers of a hierarchical objectrepresentation algorithm, namely Learned Hierarchy of Parts (LHOP). In theproposed approach, we first employ the LHOP to learn hierarchical partlibraries which represent entity parts and compositions across different objectcategories and views. Then, we extract statistical and geometric features fromthe part realizations of the objects in the images in order to represent theinformation about object pose and category at each different layer of thehierarchy. Unlike the traditional approaches which consider specific layers ofthe hierarchies in order to extract information to perform specific tasks, wecombine the information extracted at different layers to solve a joint objectpose estimation and categorization problem using distributed optimizationalgorithms. We examine the proposed generative-discriminative learning approachand the algorithms on two benchmark 2-D multi-view image datasets. The proposedapproach and the algorithms outperform state-of-the-art classification,regression and feature extraction algorithms. In addition, the experimentalresults shed light on the relationship between object categorization, poseestimation and the part realizations observed at different layers of thehierarchy.
arxiv-10500-168 | The concept "altruism" for sociological research: from conceptualization to operationalization | http://arxiv.org/pdf/1503.01258v1.pdf | author:Oleg V. Pavenkov, Vladimir G. Pavenkov, Mariia V. Rubtcova category:cs.CY cs.CL published:2015-03-04 summary:This article addresses the question of the relevant conceptualization of{\guillemotleft}altruism{\guillemotright} in Russian from the perspectivesociological research operationalization. It investigates the spheres of socialapplication of the word {\guillemotleft}altruism{\guillemotright}, includeRussian equivalent {\guillemotleft}vzaimopomoshh`{\guillemotright} (mutualhelp). The data for the study comes from Russian National Corpus (Russian). Thetheoretical framework consists of Paul F. Lazarsfeld`s Theory of SociologicalResearch Methodology and the Natural Semantic Metalanguage (NSM). Quantitativeanalysis shows features in the representation of altruism in Russian thatsociologists need to know in the preparation of questionnaires, interviewguides and analysis of transcripts.
arxiv-10500-169 | Hierarchies of Relaxations for Online Prediction Problems with Evolving Constraints | http://arxiv.org/pdf/1503.01212v2.pdf | author:Alexander Rakhlin, Karthik Sridharan category:cs.LG cs.DS stat.ML published:2015-03-04 summary:We study online prediction where regret of the algorithm is measured againsta benchmark defined via evolving constraints. This framework captures onlineprediction on graphs, as well as other prediction problems with combinatorialstructure. A key aspect here is that finding the optimal benchmark predictor(even in hindsight, given all the data) might be computationally hard due tothe combinatorial nature of the constraints. Despite this, we providepolynomial-time \emph{prediction} algorithms that achieve low regret againstcombinatorial benchmark sets. We do so by building improper learning algorithmsbased on two ideas that work together. The first is to alleviate part of thecomputational burden through random playout, and the second is to employLasserre semidefinite hierarchies to approximate the resulting integer program.Interestingly, for our prediction algorithms, we only need to compute thevalues of the semidefinite programs and not the rounded solutions. However, theintegrality gap for Lasserre hierarchy \emph{does} enter the generic regretbound in terms of Rademacher complexity of the benchmark set. This establishesa trade-off between the computation time and the regret bound of the algorithm.
arxiv-10500-170 | Sparse multi-view matrix factorisation: a multivariate approach to multiple tissue comparisons | http://arxiv.org/pdf/1503.01291v2.pdf | author:Zi Wang, Wei Yuan, Giovanni Montana category:stat.ML stat.AP published:2015-03-04 summary:Gene expression levels in a population vary extensively across tissues. Suchheterogeneity is caused by genetic variability and environmental factors, andis expected to be linked to disease development. The abundance of experimentaldata now enables the identification of features of gene expression profilesthat are shared across tissues, and those that are tissue-specific. While mostcurrent research is concerned with characterising differential expression bycomparing mean expression profiles across tissues, it is also believed that asignificant difference in a gene expression's variance across tissues may alsobe associated to molecular mechanisms that are important for tissue developmentand function. We propose a sparse multi-view matrix factorisation (sMVMF)algorithm to jointly analyse gene expression measurements in multiple tissues,where each tissue provides a different "view" of the underlying organism. Theproposed methodology can be interpreted as an extension of principal componentanalysis in that it provides the means to decompose the total sample variancein each tissue into the sum of two components: one capturing the variance thatis shared across tissues, and one isolating the tissue-specific variances.sMVMF has been used to jointly model mRNA expression profiles in three tissues- adipose, skin and LCL - which are available for a large and well-phenotypedtwins cohort, TwinsUK. Using sMVMF, we are able to prioritise genes based onwhether their variation patterns are specific to each tissue. Furthermore,using DNA methylation profiles available, we provide supporting evidence thatadipose-specific gene expression patterns may be driven by epigenetic effects.
arxiv-10500-171 | Low-dimensional Models in Spatio-Temporal Wind Speed Forecasting | http://arxiv.org/pdf/1503.01210v1.pdf | author:Borhan M. Sanandaji, Akin Tascikaraoglu, Kameshwar Poolla, Pravin Varaiya category:cs.SY stat.ML published:2015-03-04 summary:Integrating wind power into the grid is challenging because of its randomnature. Integration is facilitated with accurate short-term forecasts of windpower. The paper presents a spatio-temporal wind speed forecasting algorithmthat incorporates the time series data of a target station and data ofsurrounding stations. Inspired by Compressive Sensing (CS) andstructured-sparse recovery algorithms, we claim that there usually exists anintrinsic low-dimensional structure governing a large collection of stationsthat should be exploited. We cast the forecasting problem as recovery of ablock-sparse signal $\boldsymbol{x}$ from a set of linear equations$\boldsymbol{b} = A\boldsymbol{x}$ for which we propose novel structure-sparserecovery algorithms. Results of a case study in the east coast show that theproposed Compressive Spatio-Temporal Wind Speed Forecasting (CST-WSF) algorithmsignificantly improves the short-term forecasts compared to a set ofwidely-used benchmark models.
arxiv-10500-172 | Large Dimensional Analysis of Robust M-Estimators of Covariance with Outliers | http://arxiv.org/pdf/1503.01245v1.pdf | author:David Morales-Jimenez, Romain Couillet, Matthew R. McKay category:math.ST cs.IT math.IT stat.ML stat.TH published:2015-03-04 summary:A large dimensional characterization of robust M-estimators of covariance (orscatter) is provided under the assumption that the dataset comprisesindependent (essentially Gaussian) legitimate samples as well as arbitrarydeterministic samples, referred to as outliers. Building upon recent randommatrix advances in the area of robust statistics, we specifically show that theso-called Maronna M-estimator of scatter asymptotically behaves similar towell-known random matrices when the population and sample sizes grow togetherto infinity. The introduction of outliers leads the robust estimator to behaveasymptotically as the weighted sum of the sample outer products, with aconstant weight for all legitimate samples and different weights for theoutliers. A fine analysis of this structure reveals importantly that thepropensity of the M-estimator to attenuate (or enhance) the impact of outliersis mostly dictated by the alignment of the outliers with the inverse populationcovariance matrix of the legitimate samples. Thus, robust M-estimators canbring substantial benefits over more simplistic estimators such as theper-sample normalized version of the sample covariance matrix, which is notcapable of differentiating the outlying samples. The analysis shows that,within the class of Maronna's estimators of scatter, the Huber estimator ismost favorable for rejecting outliers. On the contrary, estimators more similarto Tyler's scale invariant estimator (often preferred in the literature) runthe risk of inadvertently enhancing some outliers.
arxiv-10500-173 | Active Sample Learning and Feature Selection: A Unified Approach | http://arxiv.org/pdf/1503.01239v1.pdf | author:Changsheng Li, Xiangfeng Wang, Weishan Dong, Junchi Yan, Qingshan Liu, Hongyuan Zha category:cs.LG published:2015-03-04 summary:This paper focuses on the problem of simultaneous sample and featureselection for machine learning in a fully unsupervised setting. Though mostexisting works tackle these two problems separately that derives twowell-studied sub-areas namely active learning and feature selection, a unifiedapproach is inspirational since they are often interleaved with each other.Noisy and high-dimensional features will bring adverse effect on sampleselection, while `good' samples will be beneficial to feature selection. Wepresent a unified framework to conduct active learning and feature selectionsimultaneously. From the data reconstruction perspective, both the selectedsamples and features can best approximate the original dataset respectively,such that the selected samples characterized by the selected features are veryrepresentative. Additionally our method is one-shot without iterativelyselecting samples for progressive labeling. Thus our model is especiallysuitable when the initial labeled samples are scarce or totally absent, whichexisting works hardly address particularly for simultaneous feature selection.To alleviate the NP-hardness of the raw problem, the proposed formulationinvolves a convex but non-smooth optimization problem. We solve it efficientlyby an iterative algorithm, and prove its global convergence. Experiments onpublicly available datasets validate that our method is promising compared withthe state-of-the-arts.
arxiv-10500-174 | Statistical Limits of Convex Relaxations | http://arxiv.org/pdf/1503.01442v2.pdf | author:Zhaoran Wang, Quanquan Gu, Han Liu category:stat.ML published:2015-03-04 summary:Many high dimensional sparse learning problems are formulated as nonconvexoptimization. A popular approach to solve these nonconvex optimization problemsis through convex relaxations such as linear and semidefinite programming. Inthis paper, we study the statistical limits of convex relaxations.Particularly, we consider two problems: Mean estimation for sparse principalsubmatrix and edge probability estimation for stochastic block model. Weexploit the sum-of-squares relaxation hierarchy to sharply characterize thelimits of a broad class of convex relaxations. Our result shows statisticaloptimality needs to be compromised for achieving computational tractabilityusing convex relaxations. Compared with existing results on computational lowerbounds for statistical problems, which consider general polynomial-timealgorithms and rely on computational hardness hypotheses on problems likeplanted clique detection, our theory focuses on a broad class of convexrelaxations and does not rely on unproven hypotheses.
arxiv-10500-175 | Bethe Learning of Conditional Random Fields via MAP Decoding | http://arxiv.org/pdf/1503.01228v1.pdf | author:Kui Tang, Nicholas Ruozzi, David Belanger, Tony Jebara category:cs.LG cs.CV stat.ML published:2015-03-04 summary:Many machine learning tasks can be formulated in terms of predictingstructured outputs. In frameworks such as the structured support vector machine(SVM-Struct) and the structured perceptron, discriminative functions arelearned by iteratively applying efficient maximum a posteriori (MAP) decoding.However, maximum likelihood estimation (MLE) of probabilistic models over thesesame structured spaces requires computing partition functions, which isgenerally intractable. This paper presents a method for learning discreteexponential family models using the Bethe approximation to the MLE. Remarkably,this problem also reduces to iterative (MAP) decoding. This connection emergesby combining the Bethe approximation with a Frank-Wolfe (FW) algorithm on aconvex dual objective which circumvents the intractable partition function. Theresult is a new single loop algorithm MLE-Struct, which is substantially moreefficient than previous double-loop methods for approximate maximum likelihoodestimation. Our algorithm outperforms existing methods in experiments involvingimage segmentation, matching problems from vision, and a new dataset ofuniversity roommate assignments.
arxiv-10500-176 | Temporal Pyramid Pooling Based Convolutional Neural Networks for Action Recognition | http://arxiv.org/pdf/1503.01224v2.pdf | author:Peng Wang, Yuanzhouhan Cao, Chunhua Shen, Lingqiao Liu, Heng Tao Shen category:cs.CV published:2015-03-04 summary:Encouraged by the success of Convolutional Neural Networks (CNNs) in imageclassification, recently much effort is spent on applying CNNs to video basedaction recognition problems. One challenge is that video contains a varyingnumber of frames which is incompatible to the standard input format of CNNs.Existing methods handle this issue either by directly sampling a fixed numberof frames or bypassing this issue by introducing a 3D convolutional layer whichconducts convolution in spatial-temporal domain. To solve this issue, here we propose a novel network structure which allowsan arbitrary number of frames as the network input. The key of our solution isto introduce a module consisting of an encoding layer and a temporal pyramidpooling layer. The encoding layer maps the activation from previous layers to afeature vector suitable for pooling while the temporal pyramid pooling layerconverts multiple frame-level activations into a fixed-length video-levelrepresentation. In addition, we adopt a feature concatenation layer whichcombines appearance information and motion information. Compared with the framesampling strategy, our method avoids the risk of missing any important frames.Compared with the 3D convolutional method which requires a huge video datasetfor network training, our model can be learned on a small target datasetbecause we can leverage the off-the-shelf image-level CNN for model parameterinitialization. Experiments on two challenging datasets, Hollywood2 and HMDB51,demonstrate that our method achieves superior performance over state-of-the-artmethods while requiring much fewer training data.
arxiv-10500-177 | Statistical modality tagging from rule-based annotations and crowdsourcing | http://arxiv.org/pdf/1503.01190v1.pdf | author:Vinodkumar Prabhakaran, Michael Bloodgood, Mona Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko, Owen Rambow, Benjamin Van Durme category:cs.CL cs.LG stat.ML published:2015-03-04 summary:We explore training an automatic modality tagger. Modality is the attitudethat a speaker might have toward an event or state. One of the main hurdles fortraining a linguistic tagger is gathering training data. This is particularlyproblematic for training a tagger for modality because modality triggers aresparse for the overwhelming majority of sentences. We investigate an approachto automatically training a modality tagger where we first gathered sentencesbased on a high-recall simple rule-based modality tagger and then providedthese sentences to Mechanical Turk annotators for further annotation. We usedthe resulting set of training data to train a precise modality tagger using amulti-class SVM that delivers good performance.
arxiv-10500-178 | Multiscale Combinatorial Grouping for Image Segmentation and Object Proposal Generation | http://arxiv.org/pdf/1503.00848v4.pdf | author:Jordi Pont-Tuset, Pablo Arbelaez, Jonathan T. Barron, Ferran Marques, Jitendra Malik category:cs.CV published:2015-03-03 summary:We propose a unified approach for bottom-up hierarchical image segmentationand object proposal generation for recognition, called Multiscale CombinatorialGrouping (MCG). For this purpose, we first develop a fast normalized cutsalgorithm. We then propose a high-performance hierarchical segmenter that makeseffective use of multiscale information. Finally, we propose a groupingstrategy that combines our multiscale regions into highly-accurate objectproposals by exploring efficiently their combinatorial space. We also presentSingle-scale Combinatorial Grouping (SCG), a faster version of MCG thatproduces competitive proposals in under five second per image. We conduct anextensive and comprehensive empirical validation on the BSDS500, SegVOC12, SBD,and COCO datasets, showing that MCG produces state-of-the-art contours,hierarchical regions, and object proposals.
arxiv-10500-179 | Normalization based K means Clustering Algorithm | http://arxiv.org/pdf/1503.00900v1.pdf | author:Deepali Virmani, Shweta Taneja, Geetika Malhotra category:cs.LG cs.DB published:2015-03-03 summary:K-means is an effective clustering technique used to separate similar datainto groups based on initial centroids of clusters. In this paper,Normalization based K-means clustering algorithm(N-K means) is proposed.Proposed N-K means clustering algorithm applies normalization prior toclustering on the available data as well as the proposed approach calculatesinitial centroids based on weights. Experimental results prove the bettermentof proposed N-K means clustering algorithm over existing K-means clusteringalgorithm in terms of complexity and overall performance.
arxiv-10500-180 | Anisotropic Diffusion in ITK | http://arxiv.org/pdf/1503.00992v1.pdf | author:Jean-Marie Mirebeau, Jérôme Fehrenbach, Laurent Risser, Shaza Tobji category:cs.CV math.AP published:2015-03-03 summary:Anisotropic Non-Linear Diffusion is a powerful image processing technique,which allows to simultaneously remove the noise and enhance sharp features intwo or three dimensional images. Anisotropic Diffusion is understood here inthe sense of Weickert, meaning that diffusion tensors are anisotropic andreflect the local orientation of image features. This is in contrast with thenon-linear diffusion filter of Perona and Malik, which only involves scalardiffusion coefficients, in other words isotropic diffusion tensors. In thispaper, we present an anisotropic non-linear diffusion technique we implementedin ITK. This technique is based on a recent adaptive scheme making thediffusion stable and requiring limited numerical resources. (See supplementarydata.)
arxiv-10500-181 | A Survey On Video Forgery Detection | http://arxiv.org/pdf/1503.00843v1.pdf | author:Sowmya K. N., H. R. Chennamma category:cs.MM cs.CV published:2015-03-03 summary:The Digital Forgeries though not visibly identifiable to human perception itmay alter or meddle with underlying natural statistics of digital content.Tampering involves fiddling with video content in order to cause damage or makeunauthorized alteration/modification. Tampering detection in video iscumbersome compared to image when considering the properties of the video.Tampering impacts need to be studied and the applied technique/method is usedto establish the factual information for legal course in judiciary. In thispaper we give an overview of the prior literature and challenges involved invideo forgery detection where passive approach is found.
arxiv-10500-182 | Robustly Leveraging Prior Knowledge in Text Classification | http://arxiv.org/pdf/1503.00841v1.pdf | author:Biao Liu, Minlie Huang category:cs.CL cs.AI cs.IR cs.LG published:2015-03-03 summary:Prior knowledge has been shown very useful to address many natural languageprocessing tasks. Many approaches have been proposed to formalise a variety ofknowledge, however, whether the proposed approach is robust or sensitive to theknowledge supplied to the model has rarely been discussed. In this paper, wepropose three regularization terms on top of generalized expectation criteria,and conduct extensive experiments to justify the robustness of the proposedmethods. Experimental results demonstrate that our proposed methods obtainremarkable improvements and are much more robust than baselines.
arxiv-10500-183 | The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification | http://arxiv.org/pdf/1503.01161v1.pdf | author:Been Kim, Cynthia Rudin, Julie Shah category:stat.ML cs.LG published:2015-03-03 summary:We present the Bayesian Case Model (BCM), a general framework for Bayesiancase-based reasoning (CBR) and prototype classification and clustering. BCMbrings the intuitive power of CBR to a Bayesian generative framework. The BCMlearns prototypes, the "quintessential" observations that best representclusters in a dataset, by performing joint inference on cluster labels,prototypes and important features. Simultaneously, BCM pursues sparsity bylearning subspaces, the sets of features that play important roles in thecharacterization of the prototypes. The prototype and subspace representationprovides quantitative benefits in interpretability while preservingclassification accuracy. Human subject experiments verify statisticallysignificant improvements to participants' understanding when using explanationsproduced by BCM, compared to those given by prior art.
arxiv-10500-184 | Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets | http://arxiv.org/pdf/1503.01007v4.pdf | author:Armand Joulin, Tomas Mikolov category:cs.NE cs.LG published:2015-03-03 summary:Despite the recent achievements in machine learning, we are still very farfrom achieving real artificial intelligence. In this paper, we discuss thelimitations of standard deep learning approaches and show that some of theselimitations can be overcome by learning how to grow the complexity of a modelin a structured way. Specifically, we study the simplest sequence predictionproblems that are beyond the scope of what is learnable with standard recurrentnetworks, algorithmically generated sequences which can only be learned bymodels which have the capacity to count and to memorize sequences. We show thatsome basic algorithms can be learned from sequential data using a recurrentnetwork associated with a trainable memory.
arxiv-10500-185 | Learning Super-Resolution Jointly from External and Internal Examples | http://arxiv.org/pdf/1503.01138v3.pdf | author:Zhangyang Wang, Yingzhen Yang, Zhaowen Wang, Shiyu Chang, Jianchao Yang, Thomas S. Huang category:cs.CV published:2015-03-03 summary:Single image super-resolution (SR) aims to estimate a high-resolution (HR)image from a lowresolution (LR) input. Image priors are commonly learned toregularize the otherwise seriously ill-posed SR problem, either using externalLR-HR pairs or internal similar patterns. We propose joint SR to adaptivelycombine the advantages of both external and internal SR methods. We define twoloss functions using sparse coding based external examples, and epitomicmatching based on internal examples, as well as a corresponding adaptive weightto automatically balance their contributions according to their reconstructionerrors. Extensive SR results demonstrate the effectiveness of the proposedmethod over the existing state-of-the-art methods, and is also verified by oursubjective evaluation study.
arxiv-10500-186 | Systematic Construction of Anomaly Detection Benchmarks from Real Data | http://arxiv.org/pdf/1503.01158v1.pdf | author:Andrew Emmott, Shubhomoy Das, Thomas Dietterich, Alan Fern, Weng-Keen Wong category:cs.AI cs.LG stat.ML published:2015-03-03 summary:Research in anomaly detection suffers from a lack of realistic andpublicly-available data sets. Because of this, most published experiments inanomaly detection validate their algorithms with application-specific casestudies or benchmark datasets of the researchers' construction. This makes itdifficult to compare different methods or to measure progress in the field. Italso limits our ability to understand the factors that determine theperformance of anomaly detection algorithms. This article proposes a newmethodology for empirical analysis and evaluation of anomaly detectionalgorithms. It is based on generating thousands of benchmark datasets bytransforming existing supervised learning benchmark datasets and manipulatingproperties relevant to anomaly detection. The paper identifies and validatesfour important dimensions: (a) point difficulty, (b) relative frequency ofanomalies, (c) clusteredness of anomalies, and (d) relevance of features. Weapply our generated datasets to analyze several leading anomaly detectionalgorithms. The evaluation verifies the importance of these dimensions andshows that, while some algorithms are clearly superior to others, anomalydetection accuracy is determined more by variation in the four dimensions thanby the choice of algorithm.
arxiv-10500-187 | Complexity and universality in the long-range order of words | http://arxiv.org/pdf/1503.01129v1.pdf | author:Marcelo A Montemurro, Damián H Zanette category:cs.CL physics.soc-ph published:2015-03-03 summary:As is the case of many signals produced by complex systems, language presentsa statistical structure that is balanced between order and disorder. Here wereview and extend recent results from quantitative characterisations of thedegree of order in linguistic sequences that give insights into two relevantaspects of language: the presence of statistical universals in word ordering,and the link between semantic information and the statistical linguisticstructure. We first analyse a measure of relative entropy that assesses howmuch the ordering of words contributes to the overall statistical structure oflanguage. This measure presents an almost constant value close to 3.5 bits/wordacross several linguistic families. Then, we show that a direct application ofinformation theory leads to an entropy measure that can quantify and extractsemantic structures from linguistic samples, even without prior knowledge ofthe underlying language.
arxiv-10500-188 | Context Forest for efficient object detection with large mixture models | http://arxiv.org/pdf/1503.00787v1.pdf | author:Davide Modolo, Alexander Vezhnevets, Vittorio Ferrari category:cs.CV published:2015-03-03 summary:We present Context Forest (ConF), a technique for predicting properties ofthe objects in an image based on its global appearance. Compared to standardnearest-neighbour techniques, ConF is more accurate, fast and memory efficient.We train ConF to predict which aspects of an object class are likely to appearin a given image (e.g. which viewpoint). This enables to speed-upmulti-component object detectors, by automatically selecting the most relevantcomponents to run on that image. This is particularly useful for detectorstrained from large datasets, which typically need many components to fullyabsorb the data and reach their peak performance. ConF provides a speed-up of2x for the DPM detector [1] and of 10x for the EE-SVM detector [2]. To showConF's generality, we also train it to predict at which locations objects arelikely to appear in an image. Incorporating this information in the detectorscore improves mAP performance by about 2% by removing false positivedetections in unlikely locations.
arxiv-10500-189 | Weakly Supervised Object Localization with Multi-fold Multiple Instance Learning | http://arxiv.org/pdf/1503.00949v3.pdf | author:Ramazan Gokberk Cinbis, Jakob Verbeek, Cordelia Schmid category:cs.CV published:2015-03-03 summary:Object category localization is a challenging problem in computer vision.Standard supervised training requires bounding box annotations of objectinstances. This time-consuming annotation process is sidestepped in weaklysupervised learning. In this case, the supervised information is restricted tobinary labels that indicate the absence/presence of object instances in theimage, without their locations. We follow a multiple-instance learning approachthat iteratively trains the detector and infers the object locations in thepositive training images. Our main contribution is a multi-fold multipleinstance learning procedure, which prevents training from prematurely lockingonto erroneous object locations. This procedure is particularly important whenusing high-dimensional representations, such as Fisher vectors andconvolutional neural network features. We also propose a window refinementmethod, which improves the localization accuracy by incorporating an objectnessprior. We present a detailed experimental evaluation using the PASCAL VOC 2007dataset, which verifies the effectiveness of our approach.
arxiv-10500-190 | Projection onto the capped simplex | http://arxiv.org/pdf/1503.01002v1.pdf | author:Weiran Wang, Canyi Lu category:cs.LG published:2015-03-03 summary:We provide a simple and efficient algorithm for computing the Euclideanprojection of a point onto the capped simplex---a simplex with an additionaluniform bound on each coordinate---together with an elementary proof. Both theMATLAB and C++ implementations of the proposed algorithm can be downloaded athttps://eng.ucmerced.edu/people/wwang5.
arxiv-10500-191 | Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP) | http://arxiv.org/pdf/1503.01057v1.pdf | author:Andrew Gordon Wilson, Hannes Nickisch category:cs.LG stat.ML published:2015-03-03 summary:We introduce a new structured kernel interpolation (SKI) framework, whichgeneralises and unifies inducing point methods for scalable Gaussian processes(GPs). SKI methods produce kernel approximations for fast computations throughkernel interpolation. The SKI framework clarifies how the quality of aninducing point approach depends on the number of inducing (aka interpolation)points, interpolation strategy, and GP covariance kernel. SKI also provides amechanism to create new scalable kernel methods, through choosing differentkernel interpolation strategies. Using SKI, with local cubic kernelinterpolation, we introduce KISS-GP, which is 1) more scalable than inducingpoint alternatives, 2) naturally enables Kronecker and Toeplitz algebra forsubstantial additional gains in scalability, without requiring any grid data,and 3) can be used for fast and expressive kernel learning. KISS-GP costs O(n)time and storage for GP inference. We evaluate KISS-GP for kernel matrixapproximation, kernel learning, and natural sound modelling.
arxiv-10500-192 | Using Descriptive Video Services to Create a Large Data Source for Video Annotation Research | http://arxiv.org/pdf/1503.01070v1.pdf | author:Atousa Torabi, Christopher Pal, Hugo Larochelle, Aaron Courville category:cs.CV cs.AI published:2015-03-03 summary:In this work, we introduce a dataset of video annotated with high qualitynatural language phrases describing the visual content in a given segment oftime. Our dataset is based on the Descriptive Video Service (DVS) that is nowencoded on many digital media products such as DVDs. DVS is an audio narrationdescribing the visual elements and actions in a movie for the visuallyimpaired. It is temporally aligned with the movie and mixed with the originalmovie soundtrack. We describe an automatic DVS segmentation and alignmentmethod for movies, that enables us to scale up the collection of a DVS-deriveddataset with minimal human intervention. Using this method, we have collectedthe largest DVS-derived dataset for video description of which we are aware.Our dataset currently includes over 84.6 hours of paired video/sentences from92 DVDs and is growing.
arxiv-10500-193 | Simple, Efficient, and Neural Algorithms for Sparse Coding | http://arxiv.org/pdf/1503.00778v1.pdf | author:Sanjeev Arora, Rong Ge, Tengyu Ma, Ankur Moitra category:cs.LG cs.DS cs.NE stat.ML published:2015-03-02 summary:Sparse coding is a basic task in many fields including signal processing,neuroscience and machine learning where the goal is to learn a basis thatenables a sparse representation of a given set of data, if one exists. Itsstandard formulation is as a non-convex optimization problem which is solved inpractice by heuristics based on alternating minimization. Re- cent work hasresulted in several algorithms for sparse coding with provable guarantees, butsomewhat surprisingly these are outperformed by the simple alternatingminimization heuristics. Here we give a general framework for understandingalternating minimization which we leverage to analyze existing heuristics andto design new ones also with provable guarantees. Some of these algorithms seemimplementable on simple neural architectures, which was the original motivationof Olshausen and Field (1997a) in introducing sparse coding. We also give thefirst efficient algorithm for sparse coding that works almost up to theinformation theoretic limit for sparse recovery on incoherent dictionaries. Allprevious algorithms that approached or surpassed this limit run in timeexponential in some natural parameter. Finally, our algorithms improve upon thesample complexity of existing approaches. We believe that our analysisframework will have applications in other settings where simple iterativealgorithms are used.
arxiv-10500-194 | Bayesian Optimization of Text Representations | http://arxiv.org/pdf/1503.00693v1.pdf | author:Dani Yogatama, Noah A. Smith category:cs.CL cs.LG stat.ML published:2015-03-02 summary:When applying machine learning to problems in NLP, there are many choices tomake about how to represent input texts. These choices can have a big effect onperformance, but they are often uninteresting to researchers or practitionerswho simply need a module that performs well. We propose an approach tooptimizing over this space of choices, formulating the problem as globaloptimization. We apply a sequential model-based optimization technique and showthat our method makes standard linear models competitive with moresophisticated, expensive state-of-the-art methods based on latent variablemodels or neural networks on various topic classification and sentimentanalysis problems. Our approach is a first step towards black-box NLP systemsthat work with raw text and do not require manual tuning.
arxiv-10500-195 | Recovering PCA from Hybrid-$(\ell_1,\ell_2)$ Sparse Sampling of Data Elements | http://arxiv.org/pdf/1503.00547v1.pdf | author:Abhisek Kundu, Petros Drineas, Malik Magdon-Ismail category:cs.IT cs.LG math.IT stat.ML published:2015-03-02 summary:This paper addresses how well we can recover a data matrix when only given afew of its elements. We present a randomized algorithm that element-wisesparsifies the data, retaining only a few its elements. Our new algorithmindependently samples the data using sampling probabilities that depend on boththe squares ($\ell_2$ sampling) and absolute values ($\ell_1$ sampling) of theentries. We prove that the hybrid algorithm recovers a near-PCA reconstructionof the data from a sublinear sample-size: hybrid-($\ell_1,\ell_2$) inherits the$\ell_2$-ability to sample the important elements as well as the regularizationproperties of $\ell_1$ sampling, and gives strictly better performance thaneither $\ell_1$ or $\ell_2$ on their own. We also give a one-pass version ofour algorithm and show experiments to corroborate the theory.
arxiv-10500-196 | A neuromorphic hardware framework based on population coding | http://arxiv.org/pdf/1503.00505v1.pdf | author:Chetan Singh Thakur, Tara Julia Hamilton, Runchun Wang, Jonathan Tapson, André van Schaik category:cs.NE published:2015-03-02 summary:In the biological nervous system, large neuronal populations workcollaboratively to encode sensory stimuli. These neuronal populations arecharacterised by a diverse distribution of tuning curves, ensuring that theentire range of input stimuli is encoded. Based on these principles, we havedesigned a neuromorphic system called a Trainable Analogue Block (TAB), whichencodes given input stimuli using a large population of neurons with aheterogeneous tuning curve profile. Heterogeneity of tuning curves is achievedusing random device mismatches in VLSI (Very Large Scale Integration) processand by adding a systematic offset to each hidden neuron. Here, we presentmeasurement results of a single test cell fabricated in a 65nm technology toverify the TAB framework. We have mimicked a large population of neurons byre-using measurement results from the test cell by varying offset. We thusdemonstrate the learning capability of the system for various regression tasks.The TAB system may pave the way to improve the design of analogue circuits forcommercial applications, by rendering circuits insensitive to random mismatchthat arises due to the manufacturing process.
arxiv-10500-197 | A review of mean-shift algorithms for clustering | http://arxiv.org/pdf/1503.00687v1.pdf | author:Miguel Á. Carreira-Perpiñán category:cs.LG cs.CV stat.ML published:2015-03-02 summary:A natural way to characterize the cluster structure of a dataset is byfinding regions containing a high density of data. This can be done in anonparametric way with a kernel density estimate, whose modes and henceclusters can be found using mean-shift algorithms. We describe the theory andpractice behind clustering based on kernel density estimates and mean-shiftalgorithms. We discuss the blurring and non-blurring versions of mean-shift;theoretical results about mean-shift algorithms and Gaussian mixtures;relations with scale-space theory, spectral clustering and other algorithms;extensions to tracking, to manifold and graph data, and to manifold denoising;K-modes and Laplacian K-modes algorithms; acceleration strategies for largedatasets; and applications to image segmentation, manifold denoising andmultivalued regression.
arxiv-10500-198 | A Hebbian/Anti-Hebbian Network Derived from Online Non-Negative Matrix Factorization Can Cluster and Discover Sparse Features | http://arxiv.org/pdf/1503.00680v1.pdf | author:Cengiz Pehlevan, Dmitri B. Chklovskii category:q-bio.NC cs.NE stat.ML published:2015-03-02 summary:Despite our extensive knowledge of biophysical properties of neurons, thereis no commonly accepted algorithmic theory of neuronal function. Here weexplore the hypothesis that single-layer neuronal networks perform onlinesymmetric nonnegative matrix factorization (SNMF) of the similarity matrix ofthe streamed data. By starting with the SNMF cost function we derive an onlinealgorithm, which can be implemented by a biologically plausible network withlocal learning rules. We demonstrate that such network performs soft clusteringof the data as well as sparse feature discovery. The derived algorithmreplicates many known aspects of sensory anatomy and biophysical properties ofneurons including unipolar nature of neuronal activity and synaptic weights,local synaptic plasticity rules and the dependence of learning rate oncumulative neuronal activity. Thus, we make a step towards an algorithmictheory of neuronal function, which should facilitate large-scale neural circuitsimulations and biologically inspired artificial intelligence.
arxiv-10500-199 | Utility-Theoretic Ranking for Semi-Automated Text Classification | http://arxiv.org/pdf/1503.00491v1.pdf | author:Giacomo Berardi, Andrea Esuli, Fabrizio Sebastiani category:cs.LG published:2015-03-02 summary:\emph{Semi-Automated Text Classification} (SATC) may be defined as the taskof ranking a set $\mathcal{D}$ of automatically labelled textual documents insuch a way that, if a human annotator validates (i.e., inspects and correctswhere appropriate) the documents in a top-ranked portion of $\mathcal{D}$ withthe goal of increasing the overall labelling accuracy of $\mathcal{D}$, theexpected increase is maximized. An obvious SATC strategy is to rank$\mathcal{D}$ so that the documents that the classifier has labelled with thelowest confidence are top-ranked. In this work we show that this strategy issuboptimal. We develop new utility-theoretic ranking methods based on thenotion of \emph{validation gain}, defined as the improvement in classificationeffectiveness that would derive by validating a given automatically labelleddocument. We also propose a new effectiveness measure for SATC-oriented rankingmethods, based on the expected reduction in classification error brought aboutby partially validating a list generated by a given ranking method. We reportthe results of experiments showing that, with respect to the baseline methodabove, and according to the proposed measure, our utility-theoretic rankingmethods can achieve substantially higher expected reductions in classificationerror.
arxiv-10500-200 | A Review of Relational Machine Learning for Knowledge Graphs | http://arxiv.org/pdf/1503.00759v3.pdf | author:Maximilian Nickel, Kevin Murphy, Volker Tresp, Evgeniy Gabrilovich category:stat.ML cs.LG published:2015-03-02 summary:Relational machine learning studies methods for the statistical analysis ofrelational, or graph-structured, data. In this paper, we provide a review ofhow such statistical models can be "trained" on large knowledge graphs, andthen used to predict new facts about the world (which is equivalent topredicting new edges in the graph). In particular, we discuss two fundamentallydifferent kinds of statistical relational models, both of which can scale tomassive datasets. The first is based on latent feature models such as tensorfactorization and multiway neural networks. The second is based on miningobservable patterns in the graph. We also show how to combine these latent andobservable models to get improved modeling power at decreased computationalcost. Finally, we discuss how such statistical models of graphs can be combinedwith text-based information extraction methods for automatically constructingknowledge graphs from the Web. To this end, we also discuss Google's KnowledgeVault project as an example of such combination.
arxiv-10500-201 | A Hebbian/Anti-Hebbian Neural Network for Linear Subspace Learning: A Derivation from Multidimensional Scaling of Streaming Data | http://arxiv.org/pdf/1503.00669v1.pdf | author:Cengiz Pehlevan, Tao Hu, Dmitri B. Chklovskii category:q-bio.NC cs.NE stat.ML published:2015-03-02 summary:Neural network models of early sensory processing typically reduce thedimensionality of streaming input data. Such networks learn the principalsubspace, in the sense of principal component analysis (PCA), by adjustingsynaptic weights according to activity-dependent learning rules. When derivedfrom a principled cost function these rules are nonlocal and hence biologicallyimplausible. At the same time, biologically plausible local rules have beenpostulated rather than derived from a principled cost function. Here, to bridgethis gap, we derive a biologically plausible network for subspace learning onstreaming data by minimizing a principled cost function. In a departure fromprevious work, where cost was quantified by the representation, orreconstruction, error, we adopt a multidimensional scaling (MDS) cost functionfor streaming data. The resulting algorithm relies only on biologicallyplausible Hebbian and anti-Hebbian local learning rules. In a stochasticsetting, synaptic weights converge to a stationary state which projects theinput data onto the principal subspace. If the data are generated by anonstationary distribution, the network can track the principal subspace. Thus,our result makes a step towards an algorithmic theory of neural computation.
arxiv-10500-202 | Deep Transfer Network: Unsupervised Domain Adaptation | http://arxiv.org/pdf/1503.00591v1.pdf | author:Xu Zhang, Felix Xinnan Yu, Shih-Fu Chang, Shengjin Wang category:cs.CV published:2015-03-02 summary:Domain adaptation aims at training a classifier in one dataset and applyingit to a related but not identical dataset. One successfully used framework ofdomain adaptation is to learn a transformation to match both the distributionof the features (marginal distribution), and the distribution of the labelsgiven features (conditional distribution). In this paper, we propose a newdomain adaptation framework named Deep Transfer Network (DTN), where the highlyflexible deep neural networks are used to implement such a distributionmatching process. This is achieved by two types of layers in DTN: the shared feature extractionlayers which learn a shared feature subspace in which the marginaldistributions of the source and the target samples are drawn close, and thediscrimination layers which match conditional distributions by classifiertransduction. We also show that DTN has a computation complexity linear to thenumber of training samples, making it suitable to large-scale problems. Bycombining the best paradigms in both worlds (deep neural networks inrecognition, and matching marginal and conditional distributions in domainadaptation), we demonstrate by extensive experiments that DTN improvessignificantly over former methods in both execution time and classificationaccuracy.
arxiv-10500-203 | An $\mathcal{O}(n\log n)$ projection operator for weighted $\ell_1$-norm regularization with sum constraint | http://arxiv.org/pdf/1503.00600v1.pdf | author:Weiran Wang category:cs.LG published:2015-03-02 summary:We provide a simple and efficient algorithm for the projection operator forweighted $\ell_1$-norm regularization subject to a sum constraint, togetherwith an elementary proof. The implementation of the proposed algorithm can bedownloaded from the author's homepage.
arxiv-10500-204 | FPGA Implementation of the CAR Model of the Cochlea | http://arxiv.org/pdf/1503.00504v1.pdf | author:Chetan Singh Thakur, Tara Julia Hamilton, Jonathan Tapson, Richard F. Lyon, André van Schaik category:cs.NE cs.AR published:2015-03-02 summary:The front end of the human auditory system, the cochlea, converts soundsignals from the outside world into neural impulses transmitted along theauditory pathway for further processing. The cochlea senses and separates soundin a nonlinear active fashion, exhibiting remarkable sensitivity and frequencydiscrimination. Although several electronic models of the cochlea have beenproposed and implemented, none of these are able to reproduce all thecharacteristics of the cochlea, including large dynamic range, large gain andsharp tuning at low sound levels, and low gain and broad tuning at intensesound levels. Here, we implement the Cascade of Asymmetric Resonators (CAR)model of the cochlea on an FPGA. CAR represents the basilar membrane filter inthe Cascade of Asymmetric Resonators with Fast-Acting Compression (CAR-FAC)cochlear model. CAR-FAC is a neuromorphic model of hearing based on a pole-zerofilter cascade model of auditory filtering. It uses simple nonlinear extensionsof conventional digital filter stages that are well suited to FPGAimplementations, so that we are able to implement up to 1224 cochlear sectionson Virtex-6 FPGA to process sound data in real time. The FPGA implementation ofthe electronic cochlea described here may be used as a front-end sound analyserfor various machine-hearing applications.
arxiv-10500-205 | Grouping and Recognition of Dot Patterns with Straight Offset Polygons | http://arxiv.org/pdf/1503.00769v1.pdf | author:Toshiro Kubota category:cs.CV published:2015-03-02 summary:When the boundary of a familiar object is shown by a series of isolated dots,humans can often recognize the object with ease. This ability can be sustainedwith addition of distracting dots around the object. However, such capabilityhas not been reproduced algorithmically on computers. We introduce a newalgorithm that groups a set of dots into multiple non-disjoint subsets. Itconnects the dots into a spanning tree using the proximity cue. It then appliesthe straight polygon transformation to an initial polygon derived from thespanning tree. The straight polygon divides the space into polygons recursivelyand each polygon can be viewed as grouping of a subset of the dots. The numberof polygons generated is O($n$). We also introduce simple shape selection andrecognition algorithms that can be applied to the grouping result. We used bothnatural and synthetic images to show effectiveness of these algorithms.
arxiv-10500-206 | Constrained $H^1$-regularization schemes for diffeomorphic image registration | http://arxiv.org/pdf/1503.00757v2.pdf | author:Andreas Mang, George Biros category:math.OC cs.CV published:2015-03-02 summary:We propose regularization schemes for deformable registration and efficientalgorithms for its numerical approximation. We treat image registration as avariational optimal control problem. The deformation map is parametrized by itsvelocity. Tikhonov regularization ensures well-posedness. Our scheme augmentsstandard smoothness regularization operators based on $H^1$- and$H^2$-seminorms with a constraint on the divergence of the velocity field,which resembles variational formulations for Stokes incompressible flows. Inour formulation, we invert for a stationary velocity field and a mass sourcemap. This allows us to explicitly control the compressibility of thedeformation map and by that the determinant of the deformation gradient. Wealso introduce a new regularization scheme that allows us to control shear. We use a globalized, preconditioned, matrix-free, reduced spaceGauss-Newton-Krylov scheme for numerical optimization. We exploit variableelimination techniques to reduce the number of unknowns of our system; we onlyiterate on the reduced space of the velocity field. The numerical experimentsdemonstrate that we can control the determinant of the deformation gradientwithout compromising registration quality. This additional control allows us toavoid oversmoothing of the deformation map. We also demonstrate that we canpromote or penalize shear whilst controlling the determinant of the deformationgradient.
arxiv-10500-207 | Unregularized Online Learning Algorithms with General Loss Functions | http://arxiv.org/pdf/1503.00623v2.pdf | author:Yiming Ying, Ding-Xuan Zhou category:cs.LG stat.ML published:2015-03-02 summary:In this paper, we consider unregularized online learning algorithms in aReproducing Kernel Hilbert Spaces (RKHS). Firstly, we derive explicitconvergence rates of the unregularized online learning algorithms forclassification associated with a general gamma-activating loss (see Definition1 in the paper). Our results extend and refine the results in Ying and Pontil(2008) for the least-square loss and the recent result in Bach and Moulines(2011) for the loss function with a Lipschitz-continuous gradient. Moreover, weestablish a very general condition on the step sizes which guarantees theconvergence of the last iterate of such algorithms. Secondly, we establish, forthe first time, the convergence of the unregularized pairwise learningalgorithm with a general loss function and derive explicit rates under theassumption of polynomially decaying step sizes. Concrete examples are used toillustrate our main results. The main techniques are tools from convexanalysis, refined inequalities of Gaussian averages, and an induction approach.
arxiv-10500-208 | Joint calibration of Ensemble of Exemplar SVMs | http://arxiv.org/pdf/1503.00783v2.pdf | author:Davide Modolo, Alexander Vezhnevets, Olga Russakovsky, Vittorio Ferrari category:cs.CV published:2015-03-02 summary:We present a method for calibrating the Ensemble of Exemplar SVMs model.Unlike the standard approach, which calibrates each SVM independently, ourmethod optimizes their joint performance as an ensemble. We formulate jointcalibration as a constrained optimization problem and devise an efficientoptimization algorithm to find its global optimum. The algorithm dynamicallydiscards parts of the solution space that cannot contain the optimum early on,making the optimization computationally feasible. We experiment with EE-SVMtrained on state-of-the-art CNN descriptors. Results on the ILSVRC 2014 andPASCAL VOC 2007 datasets show that (i) our joint calibration procedureoutperforms independent calibration on the task of classifying windows asbelonging to an object class or not; and (ii) this improved window classifierleads to better performance on the object detection task.
arxiv-10500-209 | Learning a Convolutional Neural Network for Non-uniform Motion Blur Removal | http://arxiv.org/pdf/1503.00593v3.pdf | author:Jian Sun, Wenfei Cao, Zongben Xu, Jean Ponce category:cs.CV I.4 published:2015-03-02 summary:In this paper, we address the problem of estimating and removing non-uniformmotion blur from a single blurry image. We propose a deep learning approach topredicting the probabilistic distribution of motion blur at the patch levelusing a convolutional neural network (CNN). We further extend the candidate setof motion kernels predicted by the CNN using carefully designed imagerotations. A Markov random field model is then used to infer a densenon-uniform motion blur field enforcing motion smoothness. Finally, motion bluris removed by a non-uniform deblurring model using patch-level image prior.Experimental evaluations show that our approach can effectively estimate andremove complex non-uniform motion blur that is not handled well by previousapproaches.
arxiv-10500-210 | Matrix Product State for Feature Extraction of Higher-Order Tensors | http://arxiv.org/pdf/1503.00516v4.pdf | author:Johann A. Bengua, Ho N. Phien, Hoang D. Tuan, Minh N. Do category:cs.CV cs.DS cs.LG published:2015-03-02 summary:This paper introduces matrix product state (MPS) decomposition as acomputational tool for extracting features of multidimensional data representedby higher-order tensors. Regardless of tensor order, MPS extracts its relevantfeatures to the so-called core tensor of maximum order three which can be usedfor classification. Mainly based on a successive sequence of singular valuedecompositions (SVD), MPS is quite simple to implement without any recursiveprocedure needed for optimizing local tensors. Thus, it leads to substantialcomputational savings compared to other tensor feature extraction methods suchas higher-order orthogonal iteration (HOOI) underlying the Tucker decomposition(TD). Benchmark results show that MPS can reduce significantly the featurespace of data while achieving better classification performance compared toHOOI.
arxiv-10500-211 | A Hebbian/Anti-Hebbian Network for Online Sparse Dictionary Learning Derived from Symmetric Matrix Factorization | http://arxiv.org/pdf/1503.00690v2.pdf | author:Tao Hu, Cengiz Pehlevan, Dmitri B. Chklovskii category:q-bio.NC cs.NE stat.ML published:2015-03-02 summary:Olshausen and Field (OF) proposed that neural computations in the primaryvisual cortex (V1) can be partially modeled by sparse dictionary learning. Byminimizing the regularized representation error they derived an onlinealgorithm, which learns Gabor-filter receptive fields from a natural imageensemble in agreement with physiological experiments. Whereas the OF algorithmcan be mapped onto the dynamics and synaptic plasticity in a single-layerneural network, the derived learning rule is nonlocal - the synaptic weightupdate depends on the activity of neurons other than just pre- and postsynapticones - and hence biologically implausible. Here, to overcome this problem, wederive sparse dictionary learning from a novel cost-function - a regularizederror of the symmetric factorization of the input's similarity matrix. Ouralgorithm maps onto a neural network of the same architecture as OF but usingonly biologically plausible local learning rules. When trained on naturalimages our network learns Gabor-filter receptive fields and reproduces thecorrelation among synaptic weights hard-wired in the OF network. Therefore,online symmetric matrix factorization may serve as an algorithmic theory ofneural computation.
arxiv-10500-212 | Graphical Representation for Heterogeneous Face Recognition | http://arxiv.org/pdf/1503.00488v3.pdf | author:Chunlei Peng, Xinbo Gao, Nannan Wang, Jie Li category:cs.CV published:2015-03-02 summary:Heterogeneous face recognition (HFR) refers to matching face images acquiredfrom different sources (i.e., different sensors or different wavelengths) foridentification. HFR plays an important role in both biometrics research andindustry. In spite of promising progresses achieved in recent years, HFR isstill a challenging problem due to the difficulty to represent twoheterogeneous images in a homogeneous manner. Existing HFR methods eitherrepresent an image ignoring the spatial information, or rely on atransformation procedure which complicates the recognition task. Consideringthese problems, we propose a novel graphical representation based HFR method(G-HFR) in this paper. Markov networks are employed to represent heterogeneousimage patches separately, which takes the spatial compatibility betweenneighboring image patches into consideration. A coupled representationsimilarity metric (CRSM) is designed to measure the similarity between obtainedgraphical representations. Extensive experiments conducted on multiple HFRscenarios (viewed sketch, forensic sketch, near infrared image, and thermalinfrared image) show that the proposed method outperforms state-of-the-artmethods.
arxiv-10500-213 | Learning Mixtures of Gaussians in High Dimensions | http://arxiv.org/pdf/1503.00424v2.pdf | author:Rong Ge, Qingqing Huang, Sham M. Kakade category:cs.LG published:2015-03-02 summary:Efficiently learning mixture of Gaussians is a fundamental problem instatistics and learning theory. Given samples coming from a random one out of kGaussian distributions in Rn, the learning problem asks to estimate the meansand the covariance matrices of these Gaussians. This learning problem arises inmany areas ranging from the natural sciences to the social sciences, and hasalso found many machine learning applications. Unfortunately, learning mixtureof Gaussians is an information theoretically hard problem: in order to learnthe parameters up to a reasonable accuracy, the number of samples required isexponential in the number of Gaussian components in the worst case. In thiswork, we show that provided we are in high enough dimensions, the class ofGaussian mixtures is learnable in its most general form under a smoothedanalysis framework, where the parameters are randomly perturbed from anadversarial starting point. In particular, given samples from a mixture ofGaussians with randomly perturbed parameters, when n > {\Omega}(k^2), we givean algorithm that learns the parameters with polynomial running time and usingpolynomial number of samples. The central algorithmic ideas consist of new waysto decompose the moment tensor of the Gaussian mixture by exploiting itsstructural properties. The symmetries of this tensor are derived from thecombinatorial structure of higher order moments of Gaussian distributions(sometimes referred to as Isserlis' theorem or Wick's theorem). We also developnew tools for bounding smallest singular values of structured random matrices,which could be useful in other smoothed analysis settings.
arxiv-10500-214 | Contrastive Pessimistic Likelihood Estimation for Semi-Supervised Classification | http://arxiv.org/pdf/1503.00269v2.pdf | author:Marco Loog category:stat.ML cs.LG stat.ME I.2.6; I.5.1 published:2015-03-01 summary:Improvement guarantees for semi-supervised classifiers can currently only begiven under restrictive conditions on the data. We propose a general way toperform semi-supervised parameter estimation for likelihood-based classifiersfor which, on the full training set, the estimates are never worse than thesupervised solution in terms of the log-likelihood. We argue, moreover, that wemay expect these solutions to really improve upon the supervised classifier inparticular cases. In a worked-out example for LDA, we take it one step furtherand essentially prove that its semi-supervised version is strictly better thanits supervised counterpart. The two new concepts that form the core of ourestimation principle are contrast and pessimism. The former refers to the factthat our objective function takes the supervised estimates into account,enabling the semi-supervised solution to explicitly control the potentialimprovements over this estimate. The latter refers to the fact that ourestimates are conservative and therefore resilient to whatever form the truelabeling of the unlabeled data takes on. Experiments demonstrate theimprovements in terms of both the log-likelihood and the classification errorrate on independent test sets.
arxiv-10500-215 | An Online Convex Optimization Approach to Blackwell's Approachability | http://arxiv.org/pdf/1503.00255v1.pdf | author:Nahum Shimkin category:cs.GT cs.LG published:2015-03-01 summary:The notion of approachability in repeated games with vector payoffs wasintroduced by Blackwell in the 1950s, along with geometric conditions forapproachability and corresponding strategies that rely on computing {\emsteering directions} as projections from the current average payoff vector tothe (convex) target set. Recently, Abernethy, Batlett and Hazan (2011) proposeda class of approachability algorithms that rely on the no-regret properties ofOnline Linear Programming for computing a suitable sequence of steeringdirections. This is first carried out for target sets that are convex cones,and then generalized to any convex set by embedding it in a higher-dimensionalconvex cone. In this paper we present a more direct formulation that relies onthe support function of the set, along with suitable Online Convex Optimizationalgorithms, which leads to a general class of approachability algorithms. Wefurther show that Blackwell's original algorithm and its convergence follow asa special case.
arxiv-10500-216 | Phase Transitions in Sparse PCA | http://arxiv.org/pdf/1503.00338v1.pdf | author:Thibault Lesieur, Florent Krzakala, Lenka Zdeborova category:cs.IT math.IT stat.ML published:2015-03-01 summary:We study optimal estimation for sparse principal component analysis when thenumber of non-zero elements is small but on the same order as the dimension ofthe data. We employ approximate message passing (AMP) algorithm and its stateevolution to analyze what is the information theoretically minimal mean-squarederror and the one achieved by AMP in the limit of large sizes. For a specialcase of rank one and large enough density of non-zeros Deshpande and Montanari[1] proved that AMP is asymptotically optimal. We show that both for lowdensity and for large rank the problem undergoes a series of phase transitionssuggesting existence of a region of parameters where estimation is informationtheoretically possible, but AMP (and presumably every other polynomialalgorithm) fails. The analysis of the large rank limit is particularlyinstructive.
arxiv-10500-217 | JUMP-Means: Small-Variance Asymptotics for Markov Jump Processes | http://arxiv.org/pdf/1503.00332v3.pdf | author:Jonathan H. Huggins, Karthik Narasimhan, Ardavan Saeedi, Vikash K. Mansinghka category:stat.ML cs.LG published:2015-03-01 summary:Markov jump processes (MJPs) are used to model a wide range of phenomena fromdisease progression to RNA path folding. However, maximum likelihood estimationof parametric models leads to degenerate trajectories and inferentialperformance is poor in nonparametric models. We take a small-varianceasymptotics (SVA) approach to overcome these limitations. We derive thesmall-variance asymptotics for parametric and nonparametric MJPs for bothdirectly observed and hidden state models. In the parametric case we obtain anovel objective function which leads to non-degenerate trajectories. To derivethe nonparametric version we introduce the gamma-gamma process, a novelextension to the gamma-exponential process. We propose algorithms for each ofthese formulations, which we call \emph{JUMP-means}. Our experimentsdemonstrate that JUMP-means is competitive with or outperforms widely used MJPinference approaches in terms of both speed and reconstruction accuracy.
arxiv-10500-218 | Matrix Completion with Noisy Entries and Outliers | http://arxiv.org/pdf/1503.00214v1.pdf | author:Raymond K. W. Wong, Thomas C. M. Lee category:stat.ML published:2015-03-01 summary:This paper considers the problem of matrix completion when the observedentries are noisy and contain outliers. It begins with introducing a newoptimization criterion for which the recovered matrix is defined as itssolution. This criterion uses the celebrated Huber function from the robuststatistics literature to downweigh the effects of outliers. A practicalalgorithm is developed to solve the optimization involved. This algorithm isfast, straightforward to implement, and monotonic convergent. Furthermore, theproposed methodology is theoretically shown to be stable in a well definedsense. Its promising empirical performance is demonstrated via a sequence ofsimulation experiments, including image inpainting.
arxiv-10500-219 | Sparse Approximation of a Kernel Mean | http://arxiv.org/pdf/1503.00323v1.pdf | author:E. Cruz Cortés, C. Scott category:stat.ML cs.LG published:2015-03-01 summary:Kernel means are frequently used to represent probability distributions inmachine learning problems. In particular, the well known kernel densityestimator and the kernel mean embedding both have the form of a kernel mean.Unfortunately, kernel means are faced with scalability issues. A single pointevaluation of the kernel density estimator, for example, requires a computationtime linear in the training sample size. To address this challenge, we presenta method to efficiently construct a sparse approximation of a kernel mean. Wedo so by first establishing an incoherence-based bound on the approximationerror, and then noticing that, for the case of radial kernels, the bound can beminimized by solving the $k$-center problem. The outcome is a linear timeconstruction of a sparse kernel mean, which also lends itself naturally to anautomatic sparsity selection scheme. We show the computational gains of ourmethod by looking at three problems involving kernel means: Euclidean embeddingof distributions, class proportion estimation, and clustering using themean-shift algorithm.
arxiv-10500-220 | Constructive sparse trigonometric approximation for functions with small mixed smoothness | http://arxiv.org/pdf/1503.00282v1.pdf | author:V. N. Temlyakov category:math.NA stat.ML published:2015-03-01 summary:The paper gives a constructive method, based on greedy algorithms, thatprovides for the classes of functions with small mixed smoothness the bestpossible in the sense of order approximation error for the $m$-termapproximation with respect to the trigonometric system.
arxiv-10500-221 | Variation of word frequencies in Russian literary texts | http://arxiv.org/pdf/1503.00339v2.pdf | author:Vladislav Kargin category:cs.CL physics.soc-ph stat.AP published:2015-03-01 summary:We study the variation of word frequencies in Russian literary texts. Ourfindings indicate that the standard deviation of a word's frequency acrosstexts depends on its average frequency according to a power law with exponent$0.62,$ showing that the rarer words have a relatively larger degree offrequency volatility (i.e., "burstiness"). Several latent factors models have been estimated to investigate thestructure of the word frequency distribution. The dependence of a word'sfrequency volatility on its average frequency can be explained by the asymmetryin the distribution of latent factors.
arxiv-10500-222 | Analysis of Crowdsourced Sampling Strategies for HodgeRank with Sparse Random Graphs | http://arxiv.org/pdf/1503.00164v2.pdf | author:Braxton Osting, Jiechao Xiong, Qianqian Xu, Yuan Yao category:stat.ML cs.LG published:2015-02-28 summary:Crowdsourcing platforms are now extensively used for conducting subjectivepairwise comparison studies. In this setting, a pairwise comparison dataset istypically gathered via random sampling, either \emph{with} or \emph{without}replacement. In this paper, we use tools from random graph theory to analyzethese two random sampling methods for the HodgeRank estimator. Using theFiedler value of the graph as a measurement for estimator stability(informativeness), we provide a new estimate of the Fiedler value for these tworandom graph models. In the asymptotic limit as the number of vertices tends toinfinity, we prove the validity of the estimate. Based on our findings, for asmall number of items to be compared, we recommend a two-stage samplingstrategy where a greedy sampling method is used initially and random sampling\emph{without} replacement is used in the second stage. When a large number ofitems is to be compared, we recommend random sampling with replacement as thisis computationally inexpensive and trivially parallelizable. Experiments onsynthetic and real-world datasets support our analysis.
arxiv-10500-223 | Macroblock Classification Method for Video Applications Involving Motions | http://arxiv.org/pdf/1503.00087v1.pdf | author:Weiyao Lin, Ming-Ting Sun, Hongxiang Li, Zhenzhong Chen, Wei Li, Bing Zhou category:cs.MM cs.CV published:2015-02-28 summary:In this paper, a macroblock classification method is proposed for variousvideo processing applications involving motions. Based on the analysis of theMotion Vector field in the compressed video, we propose to classify Macroblocksof each video frame into different classes and use this class information todescribe the frame content. We demonstrate that this low-computation-complexitymethod can efficiently catch the characteristics of the frame. Based on theproposed macroblock classification, we further propose algorithms for differentvideo processing applications, including shot change detection, motiondiscontinuity detection, and outlier rejection for global motion estimation.Experimental results demonstrate that the methods based on the proposedapproach can work effectively on these applications.
arxiv-10500-224 | Efficient Upsampling of Natural Images | http://arxiv.org/pdf/1503.00040v1.pdf | author:Chinmay Hegde, Oncel Tuzel, Fatih Porikli category:cs.CV cs.GR published:2015-02-28 summary:We propose a novel method of efficient upsampling of a single natural image.Current methods for image upsampling tend to produce high-resolution imageswith either blurry salient edges, or loss of fine textural detail, or spuriousnoise artifacts. In our method, we mitigate these effects by modeling the input image as a sumof edge and detail layers, operating upon these layers separately, and mergingthe upscaled results in an automatic fashion. We formulate the upsampled outputimage as the solution to a non-convex energy minimization problem, and proposean algorithm to obtain a tractable approximate solution. Our algorithmcomprises two main stages. 1) For the edge layer, we use a nonparametricapproach by constructing a dictionary of patches from a given image, andsynthesize edge regions in a higher-resolution version of the image. 2) For thedetail layer, we use a global parametric texture enhancement approach tosynthesize detail regions across the image. We demonstrate that our method is able to accurately reproduce sharp edges aswell as synthesize photorealistic textures, while avoiding common artifactssuch as ringing and haloing. In addition, our method involves no training phaseor estimation of model parameters, and is easily parallelizable. We demonstratethe utility of our method on a number of challenging standard test photos.
arxiv-10500-225 | Generating Multi-Sentence Lingual Descriptions of Indoor Scenes | http://arxiv.org/pdf/1503.00064v1.pdf | author:Dahua Lin, Chen Kong, Sanja Fidler, Raquel Urtasun category:cs.CV cs.CL published:2015-02-28 summary:This paper proposes a novel framework for generating lingual descriptions ofindoor scenes. Whereas substantial efforts have been made to tackle thisproblem, previous approaches focusing primarily on generating a single sentencefor each image, which is not sufficient for describing complex scenes. Weattempt to go beyond this, by generating coherent descriptions with multiplesentences. Our approach is distinguished from conventional ones in severalaspects: (1) a 3D visual parsing system that jointly infers objects,attributes, and relations; (2) a generative grammar learned automatically fromtraining text; and (3) a text generation algorithm that takes into account thecoherence among sentences. Experiments on the augmented NYU-v2 dataset showthat our framework can generate natural descriptions with substantially higherROGUE scores compared to those produced by the baseline.
arxiv-10500-226 | DeepTrack: Learning Discriminative Feature Representations Online for Robust Visual Tracking | http://arxiv.org/pdf/1503.00072v1.pdf | author:Hanxi Li, Yi Li, Fatih Porikli category:cs.CV published:2015-02-28 summary:Deep neural networks, albeit their great success on feature learning invarious computer vision tasks, are usually considered as impractical for onlinevisual tracking because they require very long training time and a large numberof training samples. In this work, we present an efficient and very robusttracking algorithm using a single Convolutional Neural Network (CNN) forlearning effective feature representations of the target object, in a purelyonline manner. Our contributions are multifold: First, we introduce a noveltruncated structural loss function that maintains as many training samples aspossible and reduces the risk of tracking error accumulation. Second, weenhance the ordinary Stochastic Gradient Descent approach in CNN training witha robust sample selection mechanism. The sampling mechanism randomly generatespositive and negative samples from different temporal distributions, which aregenerated by taking the temporal relations and label noise into account.Finally, a lazy yet effective updating scheme is designed for CNN training.Equipped with this novel updating algorithm, the CNN model is robust to somelong-existing difficulties in visual tracking such as occlusion or incorrectdetections, without loss of the effective adaption for significant appearancechanges. In the experiment, our CNN tracker outperforms all comparedstate-of-the-art methods on two recently proposed benchmarks which in totalinvolve over 60 video sequences. The remarkable performance improvement overthe existing trackers illustrates the superiority of the featurerepresentations which are learned
arxiv-10500-227 | Improved Image Deblurring based on Salient-region Segmentation | http://arxiv.org/pdf/1503.00090v1.pdf | author:Chongyang Zhang, Weiyao Lin, Wei Li, Bing Zhou, Jun Xie, Jijia Li category:cs.CV published:2015-02-28 summary:Image deblurring techniques play important roles in many image processingapplications. As the blur varies spatially across the image plane, it calls forrobust and effective methods to deal with the spatially-variant blur problem.In this paper, a Saliency-based Deblurring (SD) approach is proposed based onthe saliency detection for salient-region segmentation and a correspondingcompensate method for image deblurring. We also propose a PDE-based deblurringmethod which introduces an anisotropic Partial Differential Equation (PDE)model for latent image prediction and employs an adaptive optimization model inthe kernel estimation and deconvolution steps. Experimental results demonstratethe effectiveness of the proposed algorithm.
arxiv-10500-228 | Non-linear Learning for Statistical Machine Translation | http://arxiv.org/pdf/1503.00107v1.pdf | author:Shujian Huang, Huadong Chen, Xinyu Dai, Jiajun Chen category:cs.CL cs.NE published:2015-02-28 summary:Modern statistical machine translation (SMT) systems usually use a linearcombination of features to model the quality of each translation hypothesis.The linear combination assumes that all the features are in a linearrelationship and constrains that each feature interacts with the rest featuresin an linear manner, which might limit the expressive power of the model andlead to a under-fit model on the current data. In this paper, we propose anon-linear modeling for the quality of translation hypotheses based on neuralnetworks, which allows more complex interaction between features. A learningframework is presented for training the non-linear models. We also discusspossible heuristics in designing the network structure which may improve thenon-linear learning performance. Experimental results show that with the basicfeatures of a hierarchical phrase-based machine translation system, our methodproduce translations that are better than a linear model.
arxiv-10500-229 | Supervised learning sets benchmark for robust spike detection from calcium imaging signals | http://arxiv.org/pdf/1503.00135v1.pdf | author:Lucas Theis, Philipp Berens, Emmanouil Froudarakis, Jacob Reimer, Miroslav Román Rosón, Tom Baden, Thomas Euler, Andreas Tolias, Matthias Bethge category:stat.ML stat.AP published:2015-02-28 summary:A fundamental challenge in calcium imaging has been to infer the timing ofaction potentials from the measured noisy calcium fluorescence traces. Wesystematically evaluate a range of spike inference algorithms on a largebenchmark dataset recorded from varying neural tissue (V1 and retina) usingdifferent calcium indicators (OGB-1 and GCamp6). We show that a new algorithmbased on supervised learning in flexible probabilistic models outperforms allpreviously published techniques, setting a new standard for spike inferencefrom calcium signals. Importantly, it performs better than other algorithmseven on datasets not seen during training. Future data acquired in newexperimental conditions can easily be used to further improve its spikeprediction accuracy and generalization performance. Finally, we show thatcomparing algorithms on artificial data is not informative about performance onreal population imaging data, suggesting that a benchmark dataset may greatlyfacilitate future algorithmic developments.
arxiv-10500-230 | Group Event Detection with a Varying Number of Group Members for Video Surveillance | http://arxiv.org/pdf/1503.00082v1.pdf | author:Weiyao Lin, Ming-Ting Sun, Radha Poovendran, Zhengyou Zhang category:cs.CV cs.AI cs.MM published:2015-02-28 summary:This paper presents a novel approach for automatic recognition of groupactivities for video surveillance applications. We propose to use a grouprepresentative to handle the recognition with a varying number of groupmembers, and use an Asynchronous Hidden Markov Model (AHMM) to model therelationship between people. Furthermore, we propose a group activity detectionalgorithm which can handle both symmetric and asymmetric group activities, anddemonstrate that this approach enables the detection of hierarchicalinteractions between people. Experimental results show the effectiveness of ourapproach.
arxiv-10500-231 | Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks | http://arxiv.org/pdf/1503.00075v3.pdf | author:Kai Sheng Tai, Richard Socher, Christopher D. Manning category:cs.CL cs.AI cs.LG published:2015-02-28 summary:Because of their superior ability to preserve sequence information over time,Long Short-Term Memory (LSTM) networks, a type of recurrent neural network witha more complex computational unit, have obtained strong results on a variety ofsequence modeling tasks. The only underlying LSTM structure that has beenexplored so far is a linear chain. However, natural language exhibits syntacticproperties that would naturally combine words to phrases. We introduce theTree-LSTM, a generalization of LSTMs to tree-structured network topologies.Tree-LSTMs outperform all existing systems and strong LSTM baselines on twotasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task1) and sentiment classification (Stanford Sentiment Treebank).
arxiv-10500-232 | Activity Recognition Using A Combination of Category Components And Local Models for Video Surveillance | http://arxiv.org/pdf/1503.00081v1.pdf | author:Weiyao Lin, Ming-Ting Sun, Radha Poovendran, Zhengyou Zhang category:cs.CV cs.MM published:2015-02-28 summary:This paper presents a novel approach for automatic recognition of humanactivities for video surveillance applications. We propose to represent anactivity by a combination of category components, and demonstrate that thisapproach offers flexibility to add new activities to the system and an abilityto deal with the problem of building models for activities lacking trainingdata. For improving the recognition accuracy, a Confident-Frame- basedRecognition algorithm is also proposed, where the video frames with highconfidence for recognizing an activity are used as a specialized local model tohelp classify the remainder of the video frames. Experimental results show theeffectiveness of the proposed approach.
arxiv-10500-233 | Task-Oriented Learning of Word Embeddings for Semantic Relation Classification | http://arxiv.org/pdf/1503.00095v3.pdf | author:Kazuma Hashimoto, Pontus Stenetorp, Makoto Miwa, Yoshimasa Tsuruoka category:cs.CL published:2015-02-28 summary:We present a novel learning method for word embeddings designed for relationclassification. Our word embeddings are trained by predicting words betweennoun pairs using lexical relation-specific features on a large unlabeledcorpus. This allows us to explicitly incorporate relation-specific informationinto the word embeddings. The learned word embeddings are then used toconstruct feature vectors for a relation classification model. On awell-established semantic relation classification task, our methodsignificantly outperforms a baseline based on a previously introduced wordembedding method, and compares favorably to previous state-of-the-art modelsthat use syntactic information or manually constructed external resources.
arxiv-10500-234 | Sequential Feature Explanations for Anomaly Detection | http://arxiv.org/pdf/1503.00038v1.pdf | author:Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, Weng-Keen Wong category:cs.AI cs.LG stat.ML published:2015-02-28 summary:In many applications, an anomaly detection system presents the most anomalousdata instance to a human analyst, who then must determine whether the instanceis truly of interest (e.g. a threat in a security setting). Unfortunately, mostanomaly detectors provide no explanation about why an instance was consideredanomalous, leaving the analyst with no guidance about where to begin theinvestigation. To address this issue, we study the problems of computing andevaluating sequential feature explanations (SFEs) for anomaly detectors. An SFEof an anomaly is a sequence of features, which are presented to the analyst oneat a time (in order) until the information contained in the highlightedfeatures is enough for the analyst to make a confident judgement about theanomaly. Since analyst effort is related to the amount of information that theyconsider in an investigation, an explanation's quality is related to the numberof features that must be revealed to attain confidence. One of our maincontributions is to present a novel framework for large scale quantitativeevaluations of SFEs, where the quality measure is based on analyst effort. Todo this we construct anomaly detection benchmarks from real data sets alongwith artificial experts that can be simulated for evaluation. Our secondcontribution is to evaluate several novel explanation approaches within theframework and on traditional anomaly detection benchmarks, offering severalinsights into the approaches.
arxiv-10500-235 | Signal Processing on Graphs: Causal Modeling of Big Data | http://arxiv.org/pdf/1503.00173v2.pdf | author:Jonathan Mei, José M. F. Moura category:cs.IT math.IT stat.ML published:2015-02-28 summary:Often, Big Data applications collect a large number of time series, forexample, the financial data of companies quoted in a stock exchange, the healthcare data of all patients that visit the emergency room of a hospital, or thetemperature sequences continuously measured by weather stations across the US.A first task in the analytics of these data is to derive a low dimensionalrepresentation, a graph or discrete manifold, that describes well theinterrelations among the time series and their intrarelations across time. Thispaper presents a computationally tractable algorithm for estimating this graphstructure from the available data. This graph is directed and weighted,possibly representing causal relations, not just reciprocal correlations as inmany existing approaches in the literature. A detailed convergence analysis iscarried out. The algorithm is demonstrated on random graph and real networktime series datasets, and its performance is compared to that of relatedmethods. The adjacency matrices estimated with the new method are close to thetrue graph in the simulated data and consistent with prior physical knowledgein the real dataset tested.
arxiv-10500-236 | The NLP Engine: A Universal Turing Machine for NLP | http://arxiv.org/pdf/1503.00168v1.pdf | author:Jiwei Li, Eduard Hovy category:cs.CL published:2015-02-28 summary:It is commonly accepted that machine translation is a more complex task thanpart of speech tagging. But how much more complex? In this paper we make anattempt to develop a general framework and methodology for computing theinformational and/or processing complexity of NLP applications and tasks. Wedefine a universal framework akin to a Turning Machine that attempts to fit(most) NLP tasks into one paradigm. We calculate the complexities of variousNLP tasks using measures of Shannon Entropy, and compare `simple' ones such aspart of speech tagging to `complex' ones such as machine translation. Thispaper provides a first, though far from perfect, attempt to quantify NLP tasksunder a uniform paradigm. We point out current deficiencies and suggest someavenues for fruitful research.
arxiv-10500-237 | When Are Tree Structures Necessary for Deep Learning of Representations? | http://arxiv.org/pdf/1503.00185v5.pdf | author:Jiwei Li, Minh-Thang Luong, Dan Jurafsky, Eudard Hovy category:cs.AI cs.CL published:2015-02-28 summary:Recursive neural models, which use syntactic parse trees to recursivelygenerate representations bottom-up, are a popular architecture. But there havenot been rigorous evaluations showing for exactly which tasks this syntax-basedmethod is appropriate. In this paper we benchmark {\bf recursive} neural modelsagainst sequential {\bf recurrent} neural models (simple recurrent and LSTMmodels), enforcing apples-to-apples comparison as much as possible. Weinvestigate 4 tasks: (1) sentiment classification at the sentence level andphrase level; (2) matching questions to answer-phrases; (3) discourse parsing;(4) semantic relation extraction (e.g., {\em component-whole} between nouns). Our goal is to understand better when, and why, recursive models canoutperform simpler models. We find that recursive models help mainly on tasks(like semantic relation extraction) that require associating headwords across along distance, particularly on very long sequences. We then introduce a methodfor allowing recurrent models to achieve similar performance: breaking longsentences into clause-like units at punctuation and processing them separatelybefore combining. Our results thus help understand the limitations of bothclasses of models, and suggest directions for improving recurrent models.
arxiv-10500-238 | Sensitivity Analysis for additive STDP rule | http://arxiv.org/pdf/1503.07490v2.pdf | author:Subhajit Sengupta, Karthik S. Gurumoorthy, Arunava Banerjee category:q-bio.NC cs.NE published:2015-02-28 summary:Spike Timing Dependent Plasticity (STDP) is a Hebbian like synaptic learningrule. The basis of STDP has strong experimental evidences and it depends onprecise input and output spike timings. In this paper we show that underbiologically plausible spiking regime, slight variability in the spike timingleads to drastically different evolution of synaptic weights when its dynamicsare governed by the additive STDP rule.
arxiv-10500-239 | Author Name Disambiguation by Using Deep Neural Network | http://arxiv.org/pdf/1502.08030v1.pdf | author:Hung Nghiep Tran, Tin Huynh, Tien Do category:cs.DL cs.CL cs.LG published:2015-02-27 summary:Author name ambiguity decreases the quality and reliability of informationretrieved from digital libraries. Existing methods have tried to solve thisproblem by predefining a feature set based on expert's knowledge for a specificdataset. In this paper, we propose a new approach which uses deep neuralnetwork to learn features automatically from data. Additionally, we propose thegeneral system architecture for author name disambiguation on any dataset. Inthis research, we evaluate the proposed method on a dataset containingVietnamese author names. The results show that this method significantlyoutperforms other methods that use predefined feature set. The proposed methodachieves 99.31% in terms of accuracy. Prediction error rate decreases from1.83% to 0.69%, i.e., it decreases by 1.14%, or 62.3% relatively compared withother methods that use predefined feature set (Table 3).
arxiv-10500-240 | SciRecSys: A Recommendation System for Scientific Publication by Discovering Keyword Relationships | http://arxiv.org/pdf/1502.08033v1.pdf | author:Vu Le Anh, Vo Hoang Hai, Hung Nghiep Tran, Jason J. Jung category:cs.DL cs.CL cs.IR published:2015-02-27 summary:In this work, we propose a new approach for discovering various relationshipsamong keywords over the scientific publications based on a Markov Chain model.It is an important problem since keywords are the basic elements forrepresenting abstract objects such as documents, user profiles, topics and manythings else. Our model is very effective since it combines four importantfactors in scientific publications: content, publicity, impact and randomness.Particularly, a recommendation system (called SciRecSys) has been presented tosupport users to efficiently find out relevant articles.
arxiv-10500-241 | Second-order Quantile Methods for Experts and Combinatorial Games | http://arxiv.org/pdf/1502.08009v1.pdf | author:Wouter M. Koolen, Tim van Erven category:cs.LG stat.ML published:2015-02-27 summary:We aim to design strategies for sequential decision making that adjust to thedifficulty of the learning problem. We study this question both in the settingof prediction with expert advice, and for more general combinatorial decisiontasks. We are not satisfied with just guaranteeing minimax regret rates, but wewant our algorithms to perform significantly better on easy data. Two popularways to formalize such adaptivity are second-order regret bounds and quantilebounds. The underlying notions of 'easy data', which may be paraphrased as "thelearning problem has small variance" and "multiple decisions are useful", aresynergetic. But even though there are sophisticated algorithms that exploit oneof the two, no existing algorithm is able to adapt to both. In this paper we outline a new method for obtaining such adaptive algorithms,based on a potential function that aggregates a range of learning rates (whichare essential tuning parameters). By choosing the right prior we constructefficient algorithms and show that they reap both benefits by proving the firstbounds that are both second-order and incorporate quantiles.
arxiv-10500-242 | Describing Videos by Exploiting Temporal Structure | http://arxiv.org/pdf/1502.08029v5.pdf | author:Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, Aaron Courville category:stat.ML cs.AI cs.CL cs.CV cs.LG published:2015-02-27 summary:Recent progress in using recurrent neural networks (RNNs) for imagedescription has motivated the exploration of their application for videodescription. However, while images are static, working with videos requiresmodeling their dynamic temporal structure and then properly integrating thatinformation into a natural language description. In this context, we propose anapproach that successfully takes into account both the local and globaltemporal structure of videos to produce descriptions. First, our approachincorporates a spatial temporal 3-D convolutional neural network (3-D CNN)representation of the short temporal dynamics. The 3-D CNN representation istrained on video action recognition tasks, so as to produce a representationthat is tuned to human motion and behavior. Second we propose a temporalattention mechanism that allows to go beyond local temporal modeling and learnsto automatically select the most relevant temporal segments given thetext-generating RNN. Our approach exceeds the current state-of-art for bothBLEU and METEOR metrics on the Youtube2Text dataset. We also present results ona new, larger and more challenging dataset of paired video and natural languagedescriptions.
arxiv-10500-243 | Error-Correcting Factorization | http://arxiv.org/pdf/1502.07976v2.pdf | author:Miguel Angel Bautista, Oriol Pujol, Fernando de la Torre, Sergio Escalera category:cs.CV cs.LG published:2015-02-27 summary:Error Correcting Output Codes (ECOC) is a successful technique in multi-classclassification, which is a core problem in Pattern Recognition and MachineLearning. A major advantage of ECOC over other methods is that the multi- classproblem is decoupled into a set of binary problems that are solvedindependently. However, literature defines a general error-correctingcapability for ECOCs without analyzing how it distributes among classes,hindering a deeper analysis of pair-wise error-correction. To address theselimitations this paper proposes an Error-Correcting Factorization (ECF) method,our contribution is three fold: (I) We propose a novel representation of theerror-correction capability, called the design matrix, that enables us to buildan ECOC on the basis of allocating correction to pairs of classes. (II) Wederive the optimal code length of an ECOC using rank properties of the designmatrix. (III) ECF is formulated as a discrete optimization problem, and arelaxed solution is found using an efficient constrained block coordinatedescent approach. (IV) Enabled by the flexibility introduced with the designmatrix we propose to allocate the error-correction on classes that are prone toconfusion. Experimental results in several databases show that when allocatingthe error-correction to confusable classes ECF outperforms state-of-the-artapproaches.
arxiv-10500-244 | Modelling Local Deep Convolutional Neural Network Features to Improve Fine-Grained Image Classification | http://arxiv.org/pdf/1502.07802v1.pdf | author:ZongYuan Ge, Chris McCool, Conrad Sanderson, Peter Corke category:cs.CV published:2015-02-27 summary:We propose a local modelling approach using deep convolutional neuralnetworks (CNNs) for fine-grained image classification. Recently, deep CNNstrained from large datasets have considerably improved the performance ofobject recognition. However, to date there has been limited work using thesedeep CNNs as local feature extractors. This partly stems from CNNs havinginternal representations which are high dimensional, thereby making suchrepresentations difficult to model using stochastic models. To overcome thisissue, we propose to reduce the dimensionality of one of the internal fullyconnected layers, in conjunction with layer-restricted retraining to avoidretraining the entire network. The distribution of low-dimensional featuresobtained from the modified layer is then modelled using a Gaussian mixturemodel. Comparative experiments show that considerable performance improvementscan be achieved on the challenging Fish and UEC FOOD-100 datasets.
arxiv-10500-245 | Non-stochastic Best Arm Identification and Hyperparameter Optimization | http://arxiv.org/pdf/1502.07943v1.pdf | author:Kevin Jamieson, Ameet Talwalkar category:cs.LG stat.ML published:2015-02-27 summary:Motivated by the task of hyperparameter optimization, we introduce thenon-stochastic best-arm identification problem. Within the multi-armed banditliterature, the cumulative regret objective enjoys algorithms and analyses forboth the non-stochastic and stochastic settings while to the best of ourknowledge, the best-arm identification framework has only been considered inthe stochastic setting. We introduce the non-stochastic setting under thisframework, identify a known algorithm that is well-suited for this setting, andanalyze its behavior. Next, by leveraging the iterative nature of standardmachine learning algorithms, we cast hyperparameter optimization as an instanceof non-stochastic best-arm identification, and empirically evaluate ourproposed algorithm on this task. Our empirical results show that, by allocatingmore resources to promising hyperparameter settings, we typically achievecomparable test accuracies an order of magnitude faster than baseline methods.
arxiv-10500-246 | Local Translation Prediction with Global Sentence Representation | http://arxiv.org/pdf/1502.07920v1.pdf | author:Jiajun Zhang category:cs.CL published:2015-02-27 summary:Statistical machine translation models have made great progress in improvingthe translation quality. However, the existing models predict the targettranslation with only the source- and target-side local context information. Inpractice, distinguishing good translations from bad ones does not only dependon the local features, but also rely on the global sentence-level information.In this paper, we explore the source-side global sentence-level features fortarget-side local translation prediction. We propose a novelbilingually-constrained chunk-based convolutional neural network to learnsentence semantic representations. With the sentence-level featurerepresentation, we further design a feed-forward neural network to betterpredict translations using both local and global information. The large-scaleexperiments show that our method can obtain substantial improvements intranslation quality over the strong baseline: the hierarchical phrase-basedtranslation model augmented with the neural network joint model.
arxiv-10500-247 | Influence Maximization with Bandits | http://arxiv.org/pdf/1503.00024v4.pdf | author:Sharan Vaswani, Laks. V. S. Lakshmanan, Mark Schmidt category:cs.SI cs.LG stat.ML published:2015-02-27 summary:We consider the problem of \emph{influence maximization}, the problem ofmaximizing the number of people that become aware of a product by finding the`best' set of `seed' users to expose the product to. Most prior work on thistopic assumes that we know the probability of each user influencing each otheruser, or we have data that lets us estimate these influences. However, thisinformation is typically not initially available or is difficult to obtain. Toavoid this assumption, we adopt a combinatorial multi-armed bandit paradigmthat estimates the influence probabilities as we sequentially try differentseed sets. We establish bounds on the performance of this procedure under theexisting edge-level feedback as well as a novel and more realistic node-levelfeedback. Beyond our theoretical results, we describe a practicalimplementation and experimentally demonstrate its efficiency and effectivenesson four real datasets.
arxiv-10500-248 | Probabilistic Zero-shot Classification with Semantic Rankings | http://arxiv.org/pdf/1502.08039v1.pdf | author:Jihun Hamm, Mikhail Belkin category:cs.LG cs.AI cs.CV published:2015-02-27 summary:In this paper we propose a non-metric ranking-based representation ofsemantic similarity that allows natural aggregation of semantic informationfrom multiple heterogeneous sources. We apply the ranking-based representationto zero-shot learning problems, and present deterministic and probabilisticzero-shot classifiers which can be built from pre-trained classifiers withoutretraining. We demonstrate their the advantages on two large real-world imagedatasets. In particular, we show that aggregating different sources of semanticinformation, including crowd-sourcing, leads to more accurate classification.
arxiv-10500-249 | Minimum message length estimation of mixtures of multivariate Gaussian and von Mises-Fisher distributions | http://arxiv.org/pdf/1502.07813v1.pdf | author:Parthan Kasarapu, Lloyd Allison category:cs.LG stat.ML published:2015-02-27 summary:Mixture modelling involves explaining some observed evidence using acombination of probability distributions. The crux of the problem is theinference of an optimal number of mixture components and their correspondingparameters. This paper discusses unsupervised learning of mixture models usingthe Bayesian Minimum Message Length (MML) criterion. To demonstrate theeffectiveness of search and inference of mixture parameters using the proposedapproach, we select two key probability distributions, each handlingfundamentally different types of data: the multivariate Gaussian distributionto address mixture modelling of data distributed in Euclidean space, and themultivariate von Mises-Fisher (vMF) distribution to address mixture modellingof directional data distributed on a unit hypersphere. The key contributions ofthis paper, in addition to the general search and inference methodology,include the derivation of MML expressions for encoding the data usingmultivariate Gaussian and von Mises-Fisher distributions, and the analyticalderivation of the MML estimates of the parameters of the two distributions. Ourapproach is tested on simulated and real world data sets. For instance, weinfer vMF mixtures that concisely explain experimentally determinedthree-dimensional protein conformations, providing an effective null modeldescription of protein structures that is central to many inference problems instructural bioinformatics. The experimental results demonstrate that theperformance of our proposed search and inference method along with the encodingschemes improve on the state of the art mixture modelling techniques.
arxiv-10500-250 | DistancePPG: Robust non-contact vital signs monitoring using a camera | http://arxiv.org/pdf/1502.08040v2.pdf | author:Mayank Kumar, Ashok Veeraraghavan, Ashutosh Sabharval category:cs.CV published:2015-02-27 summary:Vital signs such as pulse rate and breathing rate are currently measuredusing contact probes. But, non-contact methods for measuring vital signs aredesirable both in hospital settings (e.g. in NICU) and for ubiquitous in-situhealth tracking (e.g. on mobile phone and computers with webcams). Recently,camera-based non-contact vital sign monitoring have been shown to be feasible.However, camera-based vital sign monitoring is challenging for people withdarker skin tone, under low lighting conditions, and/or during movement of anindividual in front of the camera. In this paper, we propose distancePPG, a newcamera-based vital sign estimation algorithm which addresses these challenges.DistancePPG proposes a new method of combining skin-color change signals fromdifferent tracked regions of the face using a weighted average, where theweights depend on the blood perfusion and incident light intensity in theregion, to improve the signal-to-noise ratio (SNR) of camera-based estimate.One of our key contributions is a new automatic method for determining theweights based only on the video recording of the subject. The gains in SNR ofcamera-based PPG estimated using distancePPG translate into reduction of theerror in vital sign estimation, and thus expand the scope of camera-based vitalsign monitoring to potentially challenging scenarios. Further, a dataset willbe released, comprising of synchronized video recordings of face and pulseoximeter based ground truth recordings from the earlobe for people withdifferent skin tones, under different lighting conditions and for variousmotion scenarios.
arxiv-10500-251 | Image Segmentation in Liquid Argon Time Projection Chamber Detector | http://arxiv.org/pdf/1502.08046v1.pdf | author:Piotr Płoński, Dorota Stefan, Robert Sulej, Krzysztof Zaremba category:cs.CV hep-ex published:2015-02-27 summary:The Liquid Argon Time Projection Chamber (LAr-TPC) detectors provideexcellent imaging and particle identification ability for studying neutrinos.An efficient and automatic reconstruction procedures are required to exploitpotential of this imaging technology. Herein, a novel method for segmentationof images from LAr-TPC detectors is presented. The proposed approach computes afeature descriptor for each pixel in the image, which characterizes amplitudedistribution in pixel and its neighbourhood. The supervised classifier isemployed to distinguish between pixels representing particle's track and noise.The classifier is trained and evaluated on the hand-labeled dataset. Theproposed approach can be a preprocessing step for reconstructing algorithmsworking directly on detector images.
arxiv-10500-252 | Puzzle Imaging: Using Large-scale Dimensionality Reduction Algorithms for Localization | http://arxiv.org/pdf/1502.07816v3.pdf | author:Joshua I. Glaser, Bradley M. Zamft, George M. Church, Konrad P. Kording category:q-bio.NC cs.CE cs.CV q-bio.QM published:2015-02-27 summary:Current high-resolution imaging techniques require an intact sample thatpreserves spatial relationships. We here present a novel approach, "puzzleimaging," that allows imaging a spatially scrambled sample. This techniquetakes many spatially disordered samples, and then pieces them back togetherusing local properties embedded within the sample. We show that puzzle imagingcan efficiently produce high-resolution images using dimensionality reductionalgorithms. We demonstrate the theoretical capabilities of puzzle imaging inthree biological scenarios, showing that (1) relatively precise 3-dimensionalbrain imaging is possible; (2) the physical structure of a neural network canoften be recovered based only on the neural connectivity matrix; and (3) achemical map could be reproduced using bacteria with chemosensitive DNA andconjugative transfer. The ability to reconstruct scrambled images promises toenable imaging based on DNA sequencing of homogenized tissue samples.
arxiv-10500-253 | Norm-Based Capacity Control in Neural Networks | http://arxiv.org/pdf/1503.00036v2.pdf | author:Behnam Neyshabur, Ryota Tomioka, Nathan Srebro category:cs.LG cs.AI cs.NE stat.ML published:2015-02-27 summary:We investigate the capacity, convexity and characterization of a generalfamily of norm-constrained feed-forward networks.
arxiv-10500-254 | Stochastic Dual Coordinate Ascent with Adaptive Probabilities | http://arxiv.org/pdf/1502.08053v1.pdf | author:Dominik Csiba, Zheng Qu, Peter Richtárik category:math.OC cs.LG stat.ML published:2015-02-27 summary:This paper introduces AdaSDCA: an adaptive variant of stochastic dualcoordinate ascent (SDCA) for solving the regularized empirical riskminimization problems. Our modification consists in allowing the methodadaptively change the probability distribution over the dual variablesthroughout the iterative process. AdaSDCA achieves provably better complexitybound than SDCA with the best fixed probability distribution, known asimportance sampling. However, it is of a theoretical character as it isexpensive to implement. We also propose AdaSDCA+: a practical variant which inour experiments outperforms existing non-adaptive methods.
arxiv-10500-255 | Hybrid coding of visual content and local image features | http://arxiv.org/pdf/1502.07828v1.pdf | author:Luca Baroffio, Matteo Cesana, Alessandro Redondi, Marco Tagliasacchi, Stefano Tubaro category:cs.MM cs.CV published:2015-02-27 summary:Distributed visual analysis applications, such as mobile visual search orVisual Sensor Networks (VSNs) require the transmission of visual content on abandwidth-limited network, from a peripheral node to a processing unit.Traditionally, a Compress-Then-Analyze approach has been pursued, in whichsensing nodes acquire and encode the pixel-level representation of the visualcontent, that is subsequently transmitted to a sink node in order to beprocessed. This approach might not represent the most effective solution, sinceseveral analysis applications leverage a compact representation of the content,thus resulting in an inefficient usage of network resources. Furthermore,coding artifacts might significantly impact the accuracy of the visual task athand. To tackle such limitations, an orthogonal approach namedAnalyze-Then-Compress has been proposed. According to such a paradigm, sensingnodes are responsible for the extraction of visual features, that are encodedand transmitted to a sink node for further processing. In spite of improvedtask efficiency, such paradigm implies the central processing node not beingable to reconstruct a pixel-level representation of the visual content. In thispaper we propose an effective compromise between the two paradigms, namelyHybrid-Analyze-Then-Compress (HATC) that aims at jointly encoding visualcontent and local image features. Furthermore, we show how a target tradeoffbetween image quality and task accuracy might be achieved by accuratelyallocating the bitrate to either visual content or local features.
arxiv-10500-256 | Parsing as Reduction | http://arxiv.org/pdf/1503.00030v1.pdf | author:Daniel Fernández-González, André F. T. Martins category:cs.CL published:2015-02-27 summary:We reduce phrase-representation parsing to dependency parsing. Our reductionis grounded on a new intermediate representation, "head-ordered dependencytrees", shown to be isomorphic to constituent trees. By encoding orderinformation in the dependency labels, we show that any off-the-shelf, trainabledependency parser can be used to produce constituents. When this parser isnon-projective, we can perform discontinuous parsing in a very natural manner.Despite the simplicity of our approach, experiments show that the resultingparsers are on par with strong baselines, such as the Berkeley parser forEnglish and the best single system in the SPMRL-2014 shared task. Results areparticularly striking for discontinuous parsing of German, where we surpass thecurrent state of the art by a wide margin.
arxiv-10500-257 | ROCKET: Robust Confidence Intervals via Kendall's Tau for Transelliptical Graphical Models | http://arxiv.org/pdf/1502.07641v2.pdf | author:Rina Foygel Barber, Mladen Kolar category:math.ST cs.LG stat.TH published:2015-02-26 summary:Undirected graphical models are used extensively in the biological and socialsciences to encode a pattern of conditional independences between variables,where the absence of an edge between two nodes $a$ and $b$ indicates that thecorresponding two variables $X_a$ and $X_b$ are believed to be conditionallyindependent, after controlling for all other measured variables. In theGaussian case, conditional independence corresponds to a zero entry in theprecision matrix $\Omega$ (the inverse of the covariance matrix $\Sigma$). Realdata often exhibits heavy tail dependence between variables, which cannot becaptured by the commonly-used Gaussian or nonparanormal (Gaussian copula)graphical models. In this paper, we study the transelliptical model, anelliptical copula model that generalizes Gaussian and nonparanormal models to abroader family of distributions. We propose the ROCKET method, which constructsan estimator of $\Omega_{ab}$ that we prove to be asymptotically normal undermild assumptions. Empirically, ROCKET outperforms the nonparanormal andGaussian models in terms of achieving accurate inference on simulated data. Wealso compare the three methods on real data (daily stock returns), and findthat the ROCKET estimator is the only method whose behavior across subsamplesagrees with the distribution predicted by the theory.
arxiv-10500-258 | Learning Depth from Single Monocular Images Using Deep Convolutional Neural Fields | http://arxiv.org/pdf/1502.07411v6.pdf | author:Fayao Liu, Chunhua Shen, Guosheng Lin, Ian Reid category:cs.CV published:2015-02-26 summary:In this article, we tackle the problem of depth estimation from singlemonocular images. Compared with depth estimation using multiple images such asstereo depth perception, depth from monocular images is much more challenging.Prior work typically focuses on exploiting geometric priors or additionalsources of information, most using hand-crafted features. Recently, there ismounting evidence that features from deep convolutional neural networks (CNN)set new records for various vision applications. On the other hand, consideringthe continuous characteristic of the depth values, depth estimations can benaturally formulated as a continuous conditional random field (CRF) learningproblem. Therefore, here we present a deep convolutional neural field model forestimating depths from single monocular images, aiming to jointly explore thecapacity of deep CNN and continuous CRF. In particular, we propose a deepstructured learning scheme which learns the unary and pairwise potentials ofcontinuous CRF in a unified deep CNN framework. We then further propose anequally effective model based on fully convolutional networks and a novelsuperpixel pooling method, which is $\sim 10$ times faster, to speedup thepatch-wise convolutions in the deep model. With this more efficient model, weare able to design deeper networks to pursue better performance. Experiments onboth indoor and outdoor scene datasets demonstrate that the proposed methodoutperforms state-of-the-art depth estimation approaches.
arxiv-10500-259 | Concept for a CMOS Image Sensor Suited for Analog Image Pre-Processing | http://arxiv.org/pdf/1502.07449v1.pdf | author:Lan Shi, Christopher Soell, Andreas Baenisch, Robert Weigel, Jürgen Seiler, Thomas Ussmueller category:cs.ET cs.AR cs.CV published:2015-02-26 summary:A concept for a novel CMOS image sensor suited for analog imagepre-processing is presented in this paper. As an example, an image restorationalgorithm for reducing image noise is applied as image pre-processing in theanalog domain. To supply low-latency data input for analog image preprocessing,the proposed concept for a CMOS image sensor offers a new sensor signalacquisition method in 2D. In comparison to image pre-processing in the digitaldomain, the proposed analog image pre-processing promises an improved imagequality. Furthermore, the image noise at the stage of analog sensor signalacquisition can be used to select the most effective restoration algorithmapplied to the analog circuit due to image processing prior to the A/Dconverter.
arxiv-10500-260 | A Holistic Approach for Modeling and Synthesis of Image Processing Applications for Heterogeneous Computing Architectures | http://arxiv.org/pdf/1502.07453v1.pdf | author:Christian Hartmann, Anna Yupatova, Marc Reichenbach, Dietmar Fey, Reinhard German category:cs.CV published:2015-02-26 summary:Image processing applications are common in every field of our daily life.However, most of them are very complex and contain several tasks with differentcomplexities which result in varying requirements for computing architectures.Nevertheless, a general processing scheme in every image processing applicationhas a similar structure, called image processing pipeline: (1) capturing animage, (2) pre-processing using local operators, (3) processing with globaloperators and (4) post-processing using complex operations. Therefore,application-specialized hardware solutions based on heterogeneous architecturesare used for image processing. Unfortunately the development of applicationsfor heterogeneous hardware architectures is challenging due to the distributionof computational tasks among processors and programmable logic units. Nowadays,image processing systems are started from scratch which is time-consuming,error-prone and inflexible. A new methodology for modeling and implementing isneeded in order to reduce the development time of heterogenous image processingsystems. This paper introduces a new holistic top down approach for imageprocessing systems. Two challenges have to be investigated. First, designersought to be able to model their complete image processing pipeline on anabstract layer using UML. Second, we want to close the gap between the abstractsystem and the system architecture.
arxiv-10500-261 | Estimating the Potential Speedup of Computer Vision Applications on Embedded Multiprocessors | http://arxiv.org/pdf/1502.07446v1.pdf | author:Vítor Schwambach, Sébastien Cleyet-Merle, Alain Issard, Stéphane Mancini category:cs.CV cs.DC cs.PF published:2015-02-26 summary:Computer vision applications constitute one of the key drivers for embeddedmulticore architectures. Although the number of available cores is increasingin new architectures, designing an application to maximize the utilization ofthe platform is still a challenge. In this sense, parallel performanceprediction tools can aid developers in understanding the characteristics of anapplication and finding the most adequate parallelization strategy. In thiswork, we present a method for early parallel performance estimation on embeddedmultiprocessors from sequential application traces. We describe itsimplementation in Parana, a fast trace-driven simulator targeting OpenMPapplications on the STMicroelectronics' STxP70 Application-SpecificMultiprocessor (ASMP). Results for the FAST key point detector application showan error margin of less than 10% compared to the reference cycle-approximatesimulator, with lower modeling effort and up to 20x faster execution time.
arxiv-10500-262 | Achieving Exact Cluster Recovery Threshold via Semidefinite Programming: Extensions | http://arxiv.org/pdf/1502.07738v2.pdf | author:Bruce Hajek, Yihong Wu, Jiaming Xu category:stat.ML cs.SI math.PR published:2015-02-26 summary:Resolving a conjecture of Abbe, Bandeira and Hall, the authors have recentlyshown that the semidefinite programming (SDP) relaxation of the maximumlikelihood estimator achieves the sharp threshold for exactly recovering thecommunity structure under the binary stochastic block model of two equal-sizedclusters. The same was shown for the case of a single cluster and outliers.Extending the proof techniques, in this paper it is shown that SDP relaxationsalso achieve the sharp recovery threshold in the following cases: (1) Binarystochastic block model with two clusters of sizes proportional to network sizebut not necessarily equal; (2) Stochastic block model with a fixed number ofequal-sized clusters; (3) Binary censored block model with the background graphbeing Erd\H{o}s-R\'enyi. Furthermore, a sufficient condition is given for anSDP procedure to achieve exact recovery for the general case of a fixed numberof clusters plus outliers. These results demonstrate the versatility of SDPrelaxation as a simple, general purpose, computationally feasible methodologyfor community detection.
arxiv-10500-263 | A Chaining Algorithm for Online Nonparametric Regression | http://arxiv.org/pdf/1502.07697v2.pdf | author:Pierre Gaillard, Sébastien Gerchinovitz category:stat.ML cs.LG published:2015-02-26 summary:We consider the problem of online nonparametric regression with arbitrarydeterministic sequences. Using ideas from the chaining technique, we design analgorithm that achieves a Dudley-type regret bound similar to the one obtainedin a non-constructive fashion by Rakhlin and Sridharan (2014). Our regret boundis expressed in terms of the metric entropy in the sup norm, which yieldsoptimal guarantees when the metric and sequential entropies are of the sameorder of magnitude. In particular our algorithm is the first one that achievesoptimal rates for online regression over H{\"o}lder balls. In addition we showfor this example how to adapt our chaining algorithm to get a reasonablecomputational efficiency with similar regret guarantees (up to a log factor).
arxiv-10500-264 | A Dictionary Approach to EBSD Indexing | http://arxiv.org/pdf/1502.07436v2.pdf | author:Yu-Hui Chen, Se Un Park, Dennis Wei, Gregory Newstadt, Michael Jackson, Jeff P. Simmons, Marc De Graef, Alfred O. Hero category:cs.CV stat.AP published:2015-02-26 summary:We propose a framework for indexing of grain and sub-grain structures inelectron backscatter diffraction (EBSD) images of polycrystalline materials.The framework is based on a previously introduced physics-based forward modelby Callahan and De Graef (2013) relating measured patterns to grainorientations (Euler angle). The forward model is tuned to the microscope andthe sample symmetry group. We discretize the domain of the forward model onto adense grid of Euler angles and for each measured pattern we identify the mostsimilar patterns in the dictionary. These patterns are used to identifyboundaries, detect anomalies, and index crystal orientations. The statisticaldistribution of these closest matches is used in an unsupervised binarydecision tree (DT) classifier to identify grain boundaries and anomalousregions. The DT classifies a pattern as an anomaly if it has an abnormally lowsimilarity to any pattern in the dictionary. It classifies a pixel as beingnear a grain boundary if the highly ranked patterns in the dictionary differsignificantly over the pixels 3x3 neighborhood. Indexing is accomplished bycomputing the mean orientation of the closest dictionary matches to eachpattern. The mean orientation is estimated using a maximum likelihood approachthat models the orientation distribution as a mixture of Von Mises-Fisherdistributions over the quaternionic 3-sphere. The proposed dictionary matchingapproach permits segmentation, anomaly detection, and indexing to be performedin a unified manner with the additional benefit of uncertainty quantification.We demonstrate the proposed dictionary-based approach on a Ni-base IN100 alloy.
arxiv-10500-265 | Rational Kernels for Arabic Stemming and Text Classification | http://arxiv.org/pdf/1502.07504v1.pdf | author:Attia Nehar, Djelloul Ziadi, Hadda Cherroun category:cs.CL published:2015-02-26 summary:In this paper, we address the problems of Arabic Text Classification andstemming using Transducers and Rational Kernels. We introduce a new stemmingtechnique based on the use of Arabic patterns (Pattern Based Stemmer). Patternsare modelled using transducers and stemming is done without depending on anydictionary. Using transducers for stemming, documents are transformed intofinite state transducers. This document representation allows us to use andexplore rational kernels as a framework for Arabic Text Classification.Stemming experiments are conducted on three word collections and classificationexperiments are done on the Saudi Press Agency dataset. Results show that ourapproach, when compared with other approaches, is promising specially in termsof Accuracy, Recall and F1.
arxiv-10500-266 | Automatic Optimization of Hardware Accelerators for Image Processing | http://arxiv.org/pdf/1502.07448v1.pdf | author:Oliver Reiche, Konrad Häublein, Marc Reichenbach, Frank Hannig, Jürgen Teich, Dietmar Fey category:cs.PL cs.CV published:2015-02-26 summary:In the domain of image processing, often real-time constraints are required.In particular, in safety-critical applications, such as X-ray computedtomography in medical imaging or advanced driver assistance systems in theautomotive domain, timing is of utmost importance. A common approach tomaintain real-time capabilities of compute-intensive applications is to offloadthose computations to dedicated accelerator hardware, such as FieldProgrammable Gate Arrays (FPGAs). Programming such architectures is achallenging task, with respect to the typical FPGA-specific design criteria:Achievable overall algorithm latency and resource usage of FPGA primitives(BRAM, FF, LUT, and DSP). High-Level Synthesis (HLS) dramatically simplifiesthis task by enabling the description of algorithms in well-known higherlanguages (C/C++) and its automatic synthesis that can be accomplished by HLStools. However, algorithm developers still need expert knowledge about thetarget architecture, in order to achieve satisfying results. Therefore, inprevious work, we have shown that elevating the description of image algorithmsto an even higher abstraction level, by using a Domain-Specific Language (DSL),can significantly cut down the complexity for designing such algorithms forFPGAs. To give the developer even more control over the common trade-off,latency vs. resource usage, we will present an automatic optimization processwhere these criteria are analyzed and fed back to the DSL compiler, in order togenerate code that is closer to the desired design specifications. Finally, wegenerate code for stereo block matching algorithms and compare it withhandwritten implementations to quantify the quality of our results.
arxiv-10500-267 | The conjugated null space method of blind PSF estimation and deconvolution optimization | http://arxiv.org/pdf/1502.07781v1.pdf | author:Yuriy A. Bunyak, Roman N. Kvetnyy, Olga Yu. Sofina category:cs.CV published:2015-02-26 summary:We have shown that the vector of the point spread function (PSF)lexicographical presentation belongs to the left side conjugated null space(NS) of the autoregression (AR) matrix operator on condition the AR parametersare common for original and blurred images. The method of the PSF and inversePSF (IPSF) evaluation in the basis of the NS eigenfunctions is offered. Theoptimization of the PSF and IPSF shape with the aim of fluctuation eliminationis considered in NS spectral domain and image space domain. The function ofsurface area was used as the regularization functional. Two methods of originalimage estimate optimization were designed basing on maximum entropygeneralization of sought and blurred images conditional probability density andregularization. The first method uses balanced variations of convolutions withthe PSF and IPSF to obtaining iterative schema of image optimization. Thevariations balance is providing by dynamic regularization basing on conditionof the iteration process convergence. The regularization has dynamic characterbecause depends on current and previous image estimate variations. The secondmethod implements the regularization of the deconvolution optimization incurved space with metric defined on image estimate surface. The given iterativeschemas have fast convergence and therefore can be used for reconstruction ofhigh resolution images series in real time. The NS can be used for design ofdenoising bilateral linear filter which does not introduce image smoothing.
arxiv-10500-268 | Privacy for Free: Posterior Sampling and Stochastic Gradient Monte Carlo | http://arxiv.org/pdf/1502.07645v2.pdf | author:Yu-Xiang Wang, Stephen E. Fienberg, Alex Smola category:stat.ML cs.LG published:2015-02-26 summary:We consider the problem of Bayesian learning on sensitive datasets andpresent two simple but somewhat surprising results that connect Bayesianlearning to "differential privacy:, a cryptographic approach to protectindividual-level privacy while permiting database-level utility. Specifically,we show that that under standard assumptions, getting one single sample from aposterior distribution is differentially private "for free". We will see thatestimator is statistically consistent, near optimal and computationallytractable whenever the Bayesian model of interest is consistent, optimal andtractable. Similarly but separately, we show that a recent line of works thatuse stochastic gradient for Hybrid Monte Carlo (HMC) sampling also preservedifferentially privacy with minor or no modifications of the algorithmicprocedure at all, these observations lead to an "anytime" algorithm forBayesian learning under privacy constraint. We demonstrate that it performsmuch better than the state-of-the-art differential private methods on syntheticand real datasets.
arxiv-10500-269 | Dynamic Belief Fusion for Object Detection | http://arxiv.org/pdf/1502.07643v3.pdf | author:Ryan Robinson category:cs.CV published:2015-02-26 summary:A novel approach for the fusion of detection scores from disparate objectdetection methods is proposed. In order to effectively integrate the outputs ofmultiple detectors, the level of ambiguity in each individual detection score(called "uncertainty") is estimated using the precision/recall relationship ofthe corresponding detector. The proposed fusion method, called Dynamic BeliefFusion (DBF), dynamically assigns basic probabilities to propositions (target,non-target, uncertain) based on confidence levels in the detection results ofindividual approaches. A joint basic probability assignment, containinginformation from all detectors, is determined using Dempster's combinationrule, and is easily reduced to a single fused detection score. Experiments onARL and PASCAL VOC 07 datasets demonstrate that the detection accuracy of DBFis considerably greater than conventional fusion approaches as well asstate-of-the-art individual detectors.
arxiv-10500-270 | Total variation on a tree | http://arxiv.org/pdf/1502.07770v3.pdf | author:Vladimir Kolmogorov, Thomas Pock, Michal Rolinek category:cs.CV published:2015-02-26 summary:We consider the problem of minimizing the continuous valued total variationsubject to different unary terms on trees and propose fast direct algorithmsbased on dynamic programming to solve these problems. We treat both the convexand the non-convex case and derive worst case complexities that are equal orbetter than existing methods. We show applications to total variation based 2Dimage processing and computer vision problems based on a Lagrangiandecomposition approach. The resulting algorithms are very efficient, offer ahigh degree of parallelism and come along with memory requirements which areonly in the order of the number of image pixels.
arxiv-10500-271 | Coercive Region-level Registration for Multi-modal Images | http://arxiv.org/pdf/1502.07432v3.pdf | author:Yu-Hui Chen, Dennis Wei, Gregory Newstadt, Jeffrey Simmons, Alfred Hero category:cs.CV published:2015-02-26 summary:We propose a coercive approach to simultaneously register and segmentmulti-modal images which share similar spatial structure. Registration is doneat the region level to facilitate data fusion while avoiding the need forinterpolation. The algorithm performs alternating minimization of an objectivefunction informed by statistical models for pixel values in differentmodalities. Hypothesis tests are developed to determine whether to refinesegmentations by splitting regions. We demonstrate that our approach hassignificantly better performance than the state-of-the-art registration andsegmentation methods on microscopy images.
arxiv-10500-272 | Connections Between Nuclear Norm and Frobenius Norm Based Representation | http://arxiv.org/pdf/1502.07423v1.pdf | author:Xi Peng, Canyi Lu, Zhang Yi, Huajin Tang category:cs.CV published:2015-02-26 summary:Several recent works have shown that Frobenius-Norm based Representation(FNR) is comparable with Sparse Representation (SR) and Nuclear-Norm basedRepresentation (NNR) in face recognition and subspace clustering. Despite thesuccess of FNR in experimental studies, less theoretical analysis is providedto understand its working mechanism. In this paper, we fill this gap bybridging FNR and NNR. More specially, we prove that: 1) when the dictionary canprovide enough representative capacity, FNR is exactly the NNR; 2) Otherwise,FNR and NNR are two solutions on the column space of the dictionary. The firstresult provides a novel theoretical explanation towards some existing FNR basedmethods by crediting their success to low rank property. The second resultprovides a new insight to understand FNR and NNR under a unified framework.
arxiv-10500-273 | Efficient Geometric-based Computation of the String Subsequence Kernel | http://arxiv.org/pdf/1502.07776v1.pdf | author:Slimane Bellaouar, Hadda Cherroun, Djelloul Ziadi category:cs.LG cs.CG published:2015-02-26 summary:Kernel methods are powerful tools in machine learning. They have to becomputationally efficient. In this paper, we present a novel Geometric-basedapproach to compute efficiently the string subsequence kernel (SSK). Our mainidea is that the SSK computation reduces to range query problem. We started bythe construction of a match list $L(s,t)=\{(i,j):s_{i}=t_{j}\}$ where $s$ and$t$ are the strings to be compared; such match list contains only the requireddata that contribute to the result. To compute efficiently the SSK, we extendedthe layered range tree data structure to a layered range sum tree, arange-aggregation data structure. The whole process takes $ O(pL\logL)$time and $O(L\logL)$ space, where $L$ is the size of the match list and$p$ is the length of the SSK. We present empiric evaluations of our approachagainst the dynamic and the sparse programming approaches both on syntheticallygenerated data and on newswire article data. Such experiments show theefficiency of our approach for large alphabet size except for very shortstrings. Moreover, compared to the sparse dynamic approach, the proposedapproach outperforms absolutely for long strings.
arxiv-10500-274 | A hypothesize-and-verify framework for Text Recognition using Deep Recurrent Neural Networks | http://arxiv.org/pdf/1502.07540v1.pdf | author:Anupama Ray, Sai Rajeswar, Santanu Chaudhury category:cs.CV published:2015-02-26 summary:Deep LSTM is an ideal candidate for text recognition. However textrecognition involves some initial image processing steps like segmentation oflines and words which can induce error to the recognition system. Withoutsegmentation, learning very long range context is difficult and becomescomputationally intractable. Therefore, alternative soft decisions are neededat the pre-processing level. This paper proposes a hybrid text recognizer usinga deep recurrent neural network with multiple layers of abstraction and longrange context along with a language model to verify the performance of the deepneural network. In this paper we construct a multi-hypotheses tree architecturewith candidate segments of line sequences from different segmentationalgorithms at its different branches. The deep neural network is trained onperfectly segmented data and tests each of the candidate segments, generatingunicode sequences. In the verification step, these unicode sequences arevalidated using a sub-string match with the language model and best firstsearch is used to find the best possible combination of alternative hypothesisfrom the tree structure. Thus the verification framework using language modelseliminates wrong segmentation outputs and filters recognition errors.
arxiv-10500-275 | Online Learning with Feedback Graphs: Beyond Bandits | http://arxiv.org/pdf/1502.07617v1.pdf | author:Noga Alon, Nicolò Cesa-Bianchi, Ofer Dekel, Tomer Koren category:cs.LG published:2015-02-26 summary:We study a general class of online learning problems where the feedback isspecified by a graph. This class includes online prediction with expert adviceand the multi-armed bandit problem, but also several learning problems wherethe online player does not necessarily observe his own loss. We analyze how thestructure of the feedback graph controls the inherent difficulty of the induced$T$-round learning problem. Specifically, we show that any feedback graphbelongs to one of three classes: strongly observable graphs, weakly observablegraphs, and unobservable graphs. We prove that the first class induces learningproblems with $\widetilde\Theta(\alpha^{1/2} T^{1/2})$ minimax regret, where$\alpha$ is the independence number of the underlying graph; the second classinduces problems with $\widetilde\Theta(\delta^{1/3}T^{2/3})$ minimax regret,where $\delta$ is the domination number of a certain portion of the graph; andthe third class induces problems with linear minimax regret. Our resultssubsume much of the previous work on learning with feedback graphs and revealnew connections to partial monitoring games. We also show how the regret isaffected if the graphs are allowed to vary with time.
arxiv-10500-276 | Coding local and global binary visual features extracted from video sequences | http://arxiv.org/pdf/1502.07939v1.pdf | author:Luca Baroffio, Antonio Canclini, Matteo Cesana, Alessandro Redondi, Marco Tagliasacchi, Stefano Tubaro category:cs.MM cs.CV published:2015-02-26 summary:Binary local features represent an effective alternative to real-valueddescriptors, leading to comparable results for many visual analysis tasks,while being characterized by significantly lower computational complexity andmemory requirements. When dealing with large collections, a more compactrepresentation based on global features is often preferred, which can beobtained from local features by means of, e.g., the Bag-of-Visual-Word (BoVW)model. Several applications, including for example visual sensor networks andmobile augmented reality, require visual features to be transmitted over abandwidth-limited network, thus calling for coding techniques that aim atreducing the required bit budget, while attaining a target level of efficiency.In this paper we investigate a coding scheme tailored to both local and globalbinary features, which aims at exploiting both spatial and temporal redundancyby means of intra- and inter-frame coding. In this respect, the proposed codingscheme can be conveniently adopted to support the Analyze-Then-Compress (ATC)paradigm. That is, visual features are extracted from the acquired content,encoded at remote nodes, and finally transmitted to a central controller thatperforms visual analysis. This is in contrast with the traditional approach, inwhich visual content is acquired at a node, compressed and then sent to acentral unit for further processing, according to the Compress-Then-Analyze(CTA) paradigm. In this paper we experimentally compare ATC and CTA by means ofrate-efficiency curves in the context of two different visual analysis tasks:homography estimation and content-based retrieval. Our results show that thenovel ATC paradigm based on the proposed coding primitives can be competitivewith CTA, especially in bandwidth limited scenarios.
arxiv-10500-277 | Covariance Matrices and Influence Scores for Mean Field Variational Bayes | http://arxiv.org/pdf/1502.07685v1.pdf | author:Ryan Giordano, Tamara Broderick category:stat.ML stat.ME published:2015-02-26 summary:Mean field variational Bayes (MFVB) is a popular posterior approximationmethod due to its fast runtime on large-scale data sets. However, it is wellknown that a major failing of MFVB is that it underestimates the uncertainty ofmodel variables (sometimes severely) and provides no information about modelvariable covariance. We develop a fast, general methodology for exponentialfamilies that augments MFVB to deliver accurate uncertainty estimates for modelvariables -- both for individual variables and coherently across variables.MFVB for exponential families defines a fixed-point equation in the means ofthe approximating posterior, and our approach yields a covariance estimate byperturbing this fixed point. Inspired by linear response theory, we call ourmethod linear response variational Bayes (LRVB). We also show how LRVB can beused to quickly calculate a measure of the influence of individual data pointson parameter point estimates. We demonstrate the accuracy and scalability ofour method by learning Gaussian mixture models for both simulated and realdata.
arxiv-10500-278 | Topic-adjusted visibility metric for scientific articles | http://arxiv.org/pdf/1502.07190v3.pdf | author:Linda S. L. Tan, Aik Hui Chan, Tian Zheng category:stat.ML cs.LG published:2015-02-25 summary:Measuring the impact of scientific articles is important for evaluating theresearch output of individual scientists, academic institutions and journals.While citations are raw data for constructing impact measures, there existbiases and potential issues if factors affecting citation patterns are notproperly accounted for. In this work, we address the problem of field variationand introduce an article level metric useful for evaluating individualarticles' visibility. This measure derives from joint probabilistic modeling ofthe content in the articles and the citations amongst them using latentDirichlet allocation (LDA) and the mixed membership stochastic blockmodel(MMSB). Our proposed model provides a visibility metric for individual articlesadjusted for field variation in citation rates, a structural understanding ofcitation behavior in different fields, and article recommendations which takeinto account article visibility and citation patterns. We develop an efficientalgorithm for model fitting using variational methods. To scale up to largenetworks, we develop an online variant using stochastic gradient methods andcase-control likelihood approximation. We apply our methods to the benchmarkKDD Cup 2003 dataset with approximately 30,000 high energy physics papers.
arxiv-10500-279 | Describing Colors, Textures and Shapes for Content Based Image Retrieval - A Survey | http://arxiv.org/pdf/1502.07041v1.pdf | author:Jamil Ahmad, Muhammad Sajjad, Irfan Mehmood, Seungmin Rho, Sung Wook Baik category:cs.IR cs.CV published:2015-02-25 summary:Visual media has always been the most enjoyed way of communication. From theadvent of television to the modern day hand held computers, we have witnessedthe exponential growth of images around us. Undoubtedly it's a fact that theycarry a lot of information in them which needs be utilized in an effectivemanner. Hence intense need has been felt to efficiently index and store largeimage collections for effective and on- demand retrieval. For this purposelow-level features extracted from the image contents like color, texture andshape has been used. Content based image retrieval systems employing thesefeatures has proven very successful. Image retrieval has promising applicationsin numerous fields and hence has motivated researchers all over the world. Newand improved ways to represent visual content are being developed each day.Tremendous amount of research has been carried out in the last decade. In thispaper we will present a detailed overview of some of the powerful color,texture and shape descriptors for content based image retrieval. A comparativeanalysis will also be carried out for providing an insight into outstandingchallenges in this field.
arxiv-10500-280 | A Note on the Kullback-Leibler Divergence for the von Mises-Fisher distribution | http://arxiv.org/pdf/1502.07104v1.pdf | author:Tom Diethe category:stat.ML published:2015-02-25 summary:We present a derivation of the Kullback Leibler (KL)-Divergence (also knownas Relative Entropy) for the von Mises Fisher (VMF) Distribution in$d$-dimensions.
arxiv-10500-281 | Exploiting a comparability mapping to improve bi-lingual data categorization: a three-mode data analysis perspective | http://arxiv.org/pdf/1502.07157v2.pdf | author:Pierre-François Marteau, Guiyao Ke category:cs.IR cs.CL published:2015-02-25 summary:We address in this paper the co-clustering and co-classification of bilingualdata laying in two linguistic similarity spaces when a comparability measuredefining a mapping between these two spaces is available. A new approach thatwe can characterized as a three-mode analysis scheme, is proposed to mix thecomparability measure with the two similarity measures. Our aim is to improvejointly the accuracy of classification and clustering tasks performed in eachof the two linguistic spaces, as well as the quality of the final alignment ofcomparable clusters that can be obtained. We used first some purely syntheticrandom data sets to assess our formal similarity-comparability mixing model. Wethen propose two variants of the comparability measure that has been defined by(Li and Gaussier 2010) in the context of bilingual lexicon extraction to adaptit to clustering or categorizing tasks. These two variant measures aresubsequently used to evaluate our similarity-comparability mixing model in thecontext of the co-classification and co-clustering of comparable textual datasets collected from Wikipedia categories for the English and French languages.Our experiments show clear improvements in clustering and classificationaccuracies when mixing comparability with similarity measures, with, asexpected, a higher robustness obtained when the two comparability variantmeasures that we propose are used. We believe that this approach isparticularly well suited for the construction of thematic comparable corpora ofcontrollable quality.
arxiv-10500-282 | Web-scale Surface and Syntactic n-gram Features for Dependency Parsing | http://arxiv.org/pdf/1502.07038v1.pdf | author:Dominick Ng, Mohit Bansal, James R. Curran category:cs.CL published:2015-02-25 summary:We develop novel first- and second-order features for dependency parsingbased on the Google Syntactic Ngrams corpus, a collection of subtree counts ofparsed sentences from scanned books. We also extend previous work on surface$n$-gram features from Web1T to the Google Books corpus and from first-order tosecond-order, comparing and analysing performance over newswire and webtreebanks. Surface and syntactic $n$-grams both produce substantial and complementarygains in parsing accuracy across domains. Our best system combines the twofeature sets, achieving up to 0.8% absolute UAS improvements on newswire and1.4% on web text.
arxiv-10500-283 | Building with Drones: Accurate 3D Facade Reconstruction using MAVs | http://arxiv.org/pdf/1502.07019v1.pdf | author:Shreyansh Daftry, Christof Hoppe, Horst Bischof category:cs.RO cs.AI cs.CV published:2015-02-25 summary:Automatic reconstruction of 3D models from images using multi-viewStructure-from-Motion methods has been one of the most fruitful outcomes ofcomputer vision. These advances combined with the growing popularity of MicroAerial Vehicles as an autonomous imaging platform, have made 3D vision toolsubiquitous for large number of Architecture, Engineering and Constructionapplications among audiences, mostly unskilled in computer vision. However, toobtain high-resolution and accurate reconstructions from a large-scale objectusing SfM, there are many critical constraints on the quality of image data,which often become sources of inaccuracy as the current 3D reconstructionpipelines do not facilitate the users to determine the fidelity of input dataduring the image acquisition. In this paper, we present and advocate aclosed-loop interactive approach that performs incremental reconstruction inreal-time and gives users an online feedback about the quality parameters likeGround Sampling Distance (GSD), image redundancy, etc on a surface mesh. Wealso propose a novel multi-scale camera network design to prevent scene driftcaused by incremental map building, and release the first multi-scale imagesequence dataset as a benchmark. Further, we evaluate our system on realoutdoor scenes, and show that our interactive pipeline combined with amulti-scale camera network approach provides compelling accuracy in multi-viewreconstruction tasks when compared against the state-of-the-art methods.
arxiv-10500-284 | Sparse Multivariate Factor Regression | http://arxiv.org/pdf/1502.07334v5.pdf | author:Milad Kharratzadeh, Mark Coates category:stat.ML published:2015-02-25 summary:We consider the problem of multivariate regression in a setting where therelevant predictors could be shared among different responses. We propose analgorithm which decomposes the coefficient matrix into the product of a longmatrix and a wide matrix, with an elastic net penalty on the former and an$\ell_1$ penalty on the latter. The first matrix linearly transforms thepredictors to a set of latent factors, and the second one regresses theresponses on these factors. Our algorithm simultaneously performs dimensionreduction and coefficient estimation and automatically estimates the number oflatent factors from the data. Our formulation results in a non-convexoptimization problem, which despite its flexibility to impose effectivelow-dimensional structure, is difficult, or even impossible, to solve exactlyin a reasonable time. We specify an optimization algorithm based on alternatingminimization with three different sets of updates to solve this non-convexproblem and provide theoretical results on its convergence and optimality.Finally, we demonstrate the effectiveness of our algorithm via experiments onsimulated and real data.
arxiv-10500-285 | On Convolutional Approximations to Linear Dimensionality Reduction Operators for Large Scale Data Processing | http://arxiv.org/pdf/1502.07017v1.pdf | author:Swayambhoo Jain, Jarvis Haupt category:stat.ML published:2015-02-25 summary:In this paper, we examine the problem of approximating a general lineardimensionality reduction (LDR) operator, represented as a matrix $A \in\mathbb{R}^{m \times n}$ with $m < n$, by a partial circulant matrix with rowsrelated by circular shifts. Partial circulant matrices admit fastimplementations via Fourier transform methods and subsampling operations; ourinvestigation here is motivated by a desire to leverage these potentialcomputational improvements in large-scale data processing tasks. We establish afundamental result, that most large LDR matrices (whose row spaces areuniformly distributed) in fact cannot be well approximated by partial circulantmatrices. Then, we propose a natural generalization of the partial circulantapproximation framework that entails approximating the range space of a givenLDR operator $A$ over a restricted domain of inputs, using a matrix formed as aproduct of a partial circulant matrix having $m '> m$ rows and a $m \times k$'post processing' matrix. We introduce a novel algorithmic technique, based onsparse matrix factorization, for identifying the factors comprising suchapproximations, and provide preliminary evidence to demonstrate the potentialof this approach.
arxiv-10500-286 | The VC-Dimension of Similarity Hypotheses Spaces | http://arxiv.org/pdf/1502.07143v1.pdf | author:Mark Herbster, Paul Rubenstein, James Townsend category:cs.LG published:2015-02-25 summary:Given a set $X$ and a function $h:X\longrightarrow\{0,1\}$ which labels eachelement of $X$ with either $0$ or $1$, we may define a function $h^{(s)}$ tomeasure the similarity of pairs of points in $X$ according to $h$.Specifically, for $h\in \{0,1\}^X$ we define $h^{(s)}\in \{0,1\}^{X\times X}$by $h^{(s)}(w,x):= \mathbb{1}[h(w) = h(x)]$. This idea can be extended to a setof functions, or hypothesis space $\mathcal{H} \subseteq \{0,1\}^X$ by defininga similarity hypothesis space $\mathcal{H}^{(s)}:=\{h^{(s)}:h\in\mathcal{H}\}$.We show that ${{vc-dimension}}(\mathcal{H}^{(s)}) \in\Theta({{vc-dimension}}(\mathcal{H}))$.
arxiv-10500-287 | Exploiting Feature and Class Relationships in Video Categorization with Regularized Deep Neural Networks | http://arxiv.org/pdf/1502.07209v1.pdf | author:Yu-Gang Jiang, Zuxuan Wu, Jun Wang, Xiangyang Xue, Shih-Fu Chang category:cs.CV cs.MM published:2015-02-25 summary:In this paper, we study the challenging problem of categorizing videosaccording to high-level semantics such as the existence of a particular humanaction or a complex event. Although extensive efforts have been devoted inrecent years, most existing works combined multiple video features using simplefusion strategies and neglected the utilization of inter-class semanticrelationships. This paper proposes a novel unified framework that jointlyexploits the feature relationships and the class relationships for improvedcategorization performance. Specifically, these two types of relationships areestimated and utilized by rigorously imposing regularizations in the learningprocess of a deep neural network (DNN). Such a regularized DNN (rDNN) can beefficiently realized using a GPU-based implementation with an affordabletraining cost. Through arming the DNN with better capability of harnessing boththe feature and the class relationships, the proposed rDNN is more suitable formodeling video semantics. With extensive experimental evaluations, we show thatrDNN produces superior performance over several state-of-the-art approaches. Onthe well-known Hollywood2 and Columbia Consumer Video benchmarks, we obtainvery competitive results: 66.9\% and 73.5\% respectively in terms of meanaverage precision. In addition, to substantially evaluate our rDNN andstimulate future research on large scale video categorization, we collect andrelease a new benchmark dataset, called FCVID, which contains 91,223 Internetvideos and 239 manually annotated categories.
arxiv-10500-288 | Strongly Adaptive Online Learning | http://arxiv.org/pdf/1502.07073v3.pdf | author:Amit Daniely, Alon Gonen, Shai Shalev-Shwartz category:cs.LG published:2015-02-25 summary:Strongly adaptive algorithms are algorithms whose performance on every timeinterval is close to optimal. We present a reduction that can transformstandard low-regret algorithms to strongly adaptive. As a consequence, wederive simple, yet efficient, strongly adaptive algorithms for a handful ofproblems.
arxiv-10500-289 | Breaking Sticks and Ambiguities with Adaptive Skip-gram | http://arxiv.org/pdf/1502.07257v2.pdf | author:Sergey Bartunov, Dmitry Kondrashkin, Anton Osokin, Dmitry Vetrov category:cs.CL published:2015-02-25 summary:Recently proposed Skip-gram model is a powerful method for learninghigh-dimensional word representations that capture rich semantic relationshipsbetween words. However, Skip-gram as well as most prior work on learning wordrepresentations does not take into account word ambiguity and maintain onlysingle representation per word. Although a number of Skip-gram modificationswere proposed to overcome this limitation and learn multi-prototype wordrepresentations, they either require a known number of word meanings or learnthem using greedy heuristic approaches. In this paper we propose the AdaptiveSkip-gram model which is a nonparametric Bayesian extension of Skip-gramcapable to automatically learn the required number of representations for allwords at desired semantic resolution. We derive efficient online variationallearning algorithm for the model and empirically demonstrate its efficiency onword-sense induction task.
arxiv-10500-290 | Online Pairwise Learning Algorithms with Kernels | http://arxiv.org/pdf/1502.07229v1.pdf | author:Yiming Ying, Ding-Xuan Zhou category:stat.ML cs.LG published:2015-02-25 summary:Pairwise learning usually refers to a learning task which involves a lossfunction depending on pairs of examples, among which most notable ones includeranking, metric learning and AUC maximization. In this paper, we study anonline algorithm for pairwise learning with a least-square loss function in anunconstrained setting of a reproducing kernel Hilbert space (RKHS), which werefer to as the Online Pairwise lEaRning Algorithm (OPERA). In contrast toexisting works \cite{Kar,Wang} which require that the iterates are restrictedto a bounded domain or the loss function is strongly-convex, OPERA isassociated with a non-strongly convex objective function and learns the targetfunction in an unconstrained RKHS. Specifically, we establish a general theoremwhich guarantees the almost surely convergence for the last iterate of OPERAwithout any assumptions on the underlying distribution. Explicit convergencerates are derived under the condition of polynomially decaying step sizes. Wealso establish an interesting property for a family of widely-used kernels inthe setting of pairwise learning and illustrate the above convergence resultsusing such kernels. Our methodology mainly depends on the characterization ofRKHSs using its associated integral operators and probability inequalities forrandom variables with values in a Hilbert space.
arxiv-10500-291 | Proceedings of the DATE Friday Workshop on Heterogeneous Architectures and Design Methods for Embedded Image Systems (HIS 2015) | http://arxiv.org/pdf/1502.07241v2.pdf | author:Frank Hannig, Dietmar Fey, Anton Lokhmotov category:cs.AR cs.CV cs.DC published:2015-02-25 summary:This volume contains the papers accepted at the DATE Friday Workshop onHeterogeneous Architectures and Design Methods for Embedded Image Systems (HIS2015), held in Grenoble, France, March 13, 2015. HIS 2015 was co-located withthe Conference on Design, Automation and Test in Europe (DATE).
arxiv-10500-292 | Highly corrupted image inpainting through hypoelliptic diffusion | http://arxiv.org/pdf/1502.07331v1.pdf | author:Dario Prandi, Alexey Remizov, Roman Chertovskih, Ugo Boscain, Jean-Paul Gauthier category:cs.CV math.AP published:2015-02-25 summary:We present a new image inpainting algorithm, the Averaging and HypoellipticEvolution (AHE) algorithm, inspired by the one presented in [1] and based upona (semi-discrete) variation of the Citti--Petitot--Sarti model of the primaryvisual cortex V1. In particular, we focus on reconstructing highly corruptedimages (i.e. where more than the 80% of the image is missing). [1] U. Boscain, R. A. Chertovskih, J. P. Gauthier, and A. O. Remizov,Hypoelliptic diffusion and human vision: a semidiscrete new twist, SIAM J.Imaging Sci., vol. 7, no. 2, pp. 669--695, 2014.
arxiv-10500-293 | On aggregation for heavy-tailed classes | http://arxiv.org/pdf/1502.07097v1.pdf | author:Shahar Mendelson category:math.ST stat.ML stat.TH I.2.6 published:2015-02-25 summary:We introduce an alternative to the notion of `fast rate' in Learning Theory,which coincides with the optimal error rate when the given class happens to beconvex and regular in some sense. While it is well known that such a ratecannot always be attained by a learning procedure (i.e., a procedure thatselects a function in the given class), we introduce an aggregation procedurethat attains that rate under rather minimal assumptions -- for example, thatthe $L_q$ and $L_2$ norms are equivalent on the linear span of the class forsome $q>2$, and the target random variable is square-integrable.
arxiv-10500-294 | Evaluation of Deep Convolutional Nets for Document Image Classification and Retrieval | http://arxiv.org/pdf/1502.07058v1.pdf | author:Adam W. Harley, Alex Ufkes, Konstantinos G. Derpanis category:cs.CV cs.IR cs.LG cs.NE published:2015-02-25 summary:This paper presents a new state-of-the-art for document image classificationand retrieval, using features learned by deep convolutional neural networks(CNNs). In object and scene analysis, deep neural nets are capable of learninga hierarchical chain of abstraction from pixel inputs to concise anddescriptive representations. The current work explores this capacity in therealm of document analysis, and confirms that this representation strategy issuperior to a variety of popular hand-crafted alternatives. Experiments alsoshow that (i) features extracted from CNNs are robust to compression, (ii) CNNstrained on non-document images transfer well to document analysis tasks, and(iii) enforcing region-specific feature-learning is unnecessary givensufficient training data. This work also makes available a new labelled subsetof the IIT-CDIP collection, containing 400,000 document images across 16categories, useful for training new CNNs for document analysis.
arxiv-10500-295 | Reified Context Models | http://arxiv.org/pdf/1502.06665v1.pdf | author:Jacob Steinhardt, Percy Liang category:cs.LG published:2015-02-24 summary:A classic tension exists between exact inference in a simple model andapproximate inference in a complex model. The latter offers expressivity andthus accuracy, but the former provides coverage of the space, an importantproperty for confidence estimation and learning with indirect supervision. Inthis work, we introduce a new approach, reified context models, to reconcilethis tension. Specifically, we let the amount of context (the arity of thefactors in a graphical model) be chosen "at run-time" by reifying it---that is,letting this choice itself be a random variable inside the model. Empirically,we show that our approach obtains expressivity and coverage on three naturallanguage tasks.
arxiv-10500-296 | Online Tracking by Learning Discriminative Saliency Map with Convolutional Neural Network | http://arxiv.org/pdf/1502.06796v1.pdf | author:Seunghoon Hong, Tackgeun You, Suha Kwak, Bohyung Han category:cs.CV published:2015-02-24 summary:We propose an online visual tracking algorithm by learning discriminativesaliency map using Convolutional Neural Network (CNN). Given a CNN pre-trainedon a large-scale image repository in offline, our algorithm takes outputs fromhidden layers of the network as feature descriptors since they show excellentrepresentation performance in various general visual recognition problems. Thefeatures are used to learn discriminative target appearance models using anonline Support Vector Machine (SVM). In addition, we construct target-specificsaliency map by backpropagating CNN features with guidance of the SVM, andobtain the final tracking result in each frame based on the appearance modelgeneratively constructed with the saliency map. Since the saliency mapvisualizes spatial configuration of target effectively, it improves targetlocalization accuracy and enable us to achieve pixel-level target segmentation.We verify the effectiveness of our tracking algorithm through extensiveexperiment on a challenging benchmark, where our method illustrates outstandingperformance compared to the state-of-the-art tracking algorithms.
arxiv-10500-297 | Personalising Mobile Advertising Based on Users Installed Apps | http://arxiv.org/pdf/1503.00587v1.pdf | author:Jenna Reps, Uwe Aickelin, Jonathan Garibaldi, Chris Damski category:cs.CY cs.LG published:2015-02-24 summary:Mobile advertising is a billion pound industry that is rapidly expanding. Thesuccess of an advert is measured based on how users interact with it. In thispaper we investigate whether the application of unsupervised learning andassociation rule mining could be used to enable personalised targeting ofmobile adverts with the aim of increasing the interaction rate. Over May andJune 2014 we recorded advert interactions such as tapping the advert orwatching the whole advert video along with the set of apps a user has installedat the time of the interaction. Based on the apps that the users have installedwe applied k-means clustering to profile the users into one of ten classes. Dueto the large number of apps considered we implemented dimension reduction toreduced the app feature space by mapping the apps to their iTunes category andclustered users based on the percentage of their apps that correspond to eachiTunes app category. The clustering was externally validated by investigatingdifferences between the way the ten profiles interact with the various advertsgenres (lifestyle, finance and entertainment adverts). In addition associationrule mining was performed to find whether the time of the day that the advertis served and the number of apps a user has installed makes certain profilesmore likely to interact with the advert genres. The results showed there wereclear differences in the way the profiles interact with the different advertgenres and the results of this paper suggest that mobile advert targeting wouldimprove the frequency that users interact with an advert.
arxiv-10500-298 | New HSL Distance Based Colour Clustering Algorithm | http://arxiv.org/pdf/1505.05819v1.pdf | author:Vasile Patrascu category:cs.CV published:2015-02-24 summary:In this paper, we define a distance for the HSL colour system. Next, theproposed distance is used for a fuzzy colour clustering algorithm construction.The presented algorithm is related to the well-known fuzzy c-means algorithm.Finally, the clustering algorithm is used as colour reduction method. Theobtained experimental results are presented to demonstrate the effectiveness ofour approach.
arxiv-10500-299 | On the consistency theory of high dimensional variable screening | http://arxiv.org/pdf/1502.06895v3.pdf | author:Xiangyu Wang, Chenlei Leng, David B. Dunson category:math.ST cs.LG stat.ML stat.TH published:2015-02-24 summary:Variable screening is a fast dimension reduction technique for assisting highdimensional feature selection. As a preselection method, it selects a moderatesize subset of candidate variables for further refining via feature selectionto produce the final model. The performance of variable screening depends onboth computational efficiency and the ability to dramatically reduce the numberof variables without discarding the important ones. When the data dimension $p$is substantially larger than the sample size $n$, variable screening becomescrucial as 1) Faster feature selection algorithms are needed; 2) Conditionsguaranteeing selection consistency might fail to hold. This article studies aclass of linear screening methods and establishes consistency theory for thisspecial class. In particular, we prove the restricted diagonally dominant (RDD)condition is a necessary and sufficient condition for strong screeningconsistency. As concrete examples, we show two screening methods $SIS$ and$HOLP$ are both strong screening consistent (subject to additional constraints)with large probability if $n > O((\rho s + \sigma/\tau)^2\log p)$ under randomdesigns. In addition, we relate the RDD condition to the irrepresentablecondition, and highlight limitations of $SIS$.
arxiv-10500-300 | Hands Deep in Deep Learning for Hand Pose Estimation | http://arxiv.org/pdf/1502.06807v1.pdf | author:Markus Oberweger, Paul Wohlhart, Vincent Lepetit category:cs.CV published:2015-02-24 summary:We introduce and evaluate several architectures for Convolutional NeuralNetworks to predict the 3D joint locations of a hand given a depth map. Wefirst show that a prior on the 3D pose can be easily introduced andsignificantly improves the accuracy and reliability of the predictions. We alsoshow how to use context efficiently to deal with ambiguities between fingers.These two contributions allow us to significantly outperform thestate-of-the-art on several challenging benchmarks, both in terms of accuracyand computation times.
